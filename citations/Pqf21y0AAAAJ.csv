Data fusion,Jens Bleiholder; Felix Naumann,Abstract The development of the Internet in recent years has made it possible and useful toaccess many different information systems anywhere in the world to obtain information.While there is much research on the integration of heterogeneous information systems; mostcommercial systems stop short of the actual integration of available data. Data fusion is theprocess of fusing multiple records representing the same real-world object into a single;consistent; and clean representation. This article places data fusion into the greater contextof data integration; precisely defines the goals of data fusion; namely; complete; concise;and consistent data; and highlights the challenges of data fusion; namely; uncertain andconflicting data values. We give an overview and classification of different ways of fusingdata and present several techniques based on standard and advanced operators of the …,ACM Computing Surveys (CSUR),2009,512
Assessment methods for information quality criteria,Felix Naumann; Claudia Rolker,*,International Conference in Information Quality (ICIQ),2000,381
The Stratosphere platform for big data analytics,Alexander Alexandrov; Rico Bergmann; Stephan Ewen; Johann-Christoph Freytag; Fabian Hueske; Arvid Heise; Odej Kao; Marcus Leich; Ulf Leser; Volker Markl; Felix Naumann; Mathias Peters; Astrid Rheinländer; Matthias J Sax; Sebastian Schelter; Mareike Höger; Kostas Tzoumas; Daniel Warneke,Abstract We present Stratosphere; an open-source software stack for parallel data analysis.Stratosphere brings together a unique set of features that allow the expressive; easy; andefficient programming of analytical applications at very large scale. Stratosphere's featuresinclude “in situ” data processing; a declarative query language; treatment of user-definedfunctions as first-class citizens; automatic program parallelization and optimization; supportfor iterative programs; and a scalable and efficient execution engine. Stratosphere covers avariety of “Big Data” use cases; such as data warehousing; information extraction andintegration; data cleansing; graph analysis; and statistical analysis applications. In thispaper; we present the overall system architecture design decisions; introduce Stratospherethrough example queries; and then dive into the internal workings of the system's …,The VLDB Journal,2014,303
Quality-driven integration of heterogeneous information systems,Felix Naumann; Ulf Leser; Johann Christoph Freytag,*,International Conference on Very Large Data (VLDB),1999,301
Quality-driven query answering for integrated information systems,Felix Naumann,The Internet and the World Wide Web (WWW) are becoming more and more important in ourhighly interconnected world as more and more data and information is made available foronline access. Many individuals and governmental; commercial; cultural; and scientificorganizations increasingly depend on information sources that can be accessed andqueried over the Web. For example; accessing flight schedules or retrieving stockinformation has become common practice in todays world. When accessing this data; manypeople assume that the information accessed is accurate and that the data source can beaccessed reliably. These two examples clearly demonstrate that not only the informationcontent is important; the information about the quality of the data becomes an even morecrucial and critical aspect for individuals and organizations when they make plans or take …,*,2002,291
Schema matching using duplicates,Alexander Bilke; Felix Naumann,Most data integration applications require a matching between the schemas of therespective data sets. We show how the existence of duplicates within these data sets can beexploited to automatically identify matching attributes. We describe an algorithm that firstdiscovers duplicates among data sets with unaligned schemas and then uses theseduplicates to perform schema matching between schemas with opaque column names.Discovering duplicates among data sets with unaligned schemas is more difficult than in theusual setting; because it is not clear which fields in one object should be compared withwhich fields in the other. We have developed a new algorithm that efficiently finds the mostlikely duplicates in such a setting. Now; our schema matching algorithm is able to identifycorresponding attributes by comparing data values within those duplicate records. An …,International Conference on Data Engineering (ICDE),2005,257
An introduction to duplicate detection,Felix Naumann; Melanie Herschel,Abstract With the ever increasing volume of data; data quality problems abound. Multiple; yetdifferent representations of the same real-world objects in data; duplicates; are one of themost intriguing data quality problems. The effects of such duplicates are detrimental; forinstance; bank customers can obtain duplicate identities; inventory levels are monitoredincorrectly; catalogs are mailed multiple times to the same household; etc. Automaticallydetecting duplicates is difficult: First; duplicate representations are usually not identical butslightly differ in their values. Second; in principle all pairs of records should be compared;which is infeasible for large volumes of data. This lecture examines closely the two maincomponents to overcome these difficulties:(i) Similarity measures are used to automaticallyidentify duplicates when comparing two records. Well-chosen similarity measures …,Synthesis Lectures on Data Management,2010,222
DogmatiX tracks down duplicates in XML,Melanie Weis; Felix Naumann,Abstract Duplicate detection is the problem of detecting different entries in a data sourcerepresenting the same real-world entity. While research abounds in the realm of duplicatedetection in relational data; there is yet little work for duplicates in other; more complex datamodels; such as XML. In this paper; we present a generalized framework for duplicatedetection; dividing the problem into three components: candidate definition defining whichobjects are to be compared; duplicate definition defining when two duplicate candidates arein fact duplicates; and duplicate detection specifying how to efficiently find those duplicates.Using this framework; we propose an XML duplicate detection method; DogmatiX; whichcompares XML elements based not only on their direct data values; but also on the similarityof their parents; children; structure; etc. We propose heuristics to determine which of these …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,179
Semantic overlay clusters within super-peer networks,Alexander Löser; Felix Naumann; Wolf Siberski; Wolfgang Nejdl; Uwe Thaden,Abstract When joining information provider peers to a peer-to-peer network; an arbitrarydistribution is sub-optimal. In fact; clustering peers by their characteristics; enhances searchand integration significantly. Currently super-peer networks; such as the Edutella network;provide no sophisticated means for such a” semantic clustering” of peers. We introduce theconcept of semantic overlay clusters (SOC) for super-peer networks enabling a controlleddistribution of peers to clusters. In contrast to the recently announced semantic overlaynetwork approach designed for flat; pure peer-to-peer topologies and for limited meta datasets; such as simple filenames; we allow a clustering of complex heterogeneous schemesknown from relational databases and use advantages of super-peer networks; such asefficient search and broadcast of messages. Our approach is based on predefined …,International Workshop on Databases; Information Systems; and Peer-to-Peer Computing,2003,174
Data fusion: resolving data conflicts for integration,Xin Luna Dong; Felix Naumann,Abstract The amount of information produced in the world increases by 30% every year andthis rate will only go up. With advanced network technology; more and more sources areavailable either over the Internet or in enterprise intranets. Modern data managementapplications; such as setting up Web portals; managing enterprise data; managingcommunity data; and sharing scientific data; often require integrating available data sourcesand providing a uniform interface for users to access data from different sources; suchrequirements have been driving fruitful research on data integration over the last twodecades [11; 13].,Proceedings of the VLDB Endowment,2009,167
Completeness of integrated information sources,Felix Naumann; Johann-Christoph Freytag; Ulf Leser,Abstract For many information domains there are numerous World Wide Web data sources.The sources vary both in their extension and their intension: They represent different real-world entities with possible overlap and provide different attributes of these entities. Mediator-based information systems allow integrated access to such sources by providing a commonschema against which the user can pose queries. Given a query; the mediator mustdetermine which participating sources to access and how to integrate the incoming results.This article describes how to support mediators in their source selection and query planningprocess. We propose three new merge operators; which formalize the integration of multiplesource responses. A completeness model describes the usefulness of a source to answer aquery. The completeness measure incorporates both extensional value (called coverage) …,Information Systems,2004,165
Informationsintegration: Architekturen und Methoden zur Integration verteilter und heterogener Datenquellen,Ulf Leser; Felix Naumann,*,*,2006,141
Method for schema mapping and data transformation,*,A computer program product is provided that uses data examples as a basis forunderstanding and refining declarative schema mappings. The system of the presentinvention identifies a set of intuitive operators for manipulating examples includingestablishing value correspondences; data linking; data trimming; data walking; and datachasing. These operators allow a user to follow and refine an example by walking through adata source. In addition; these operators can identify a large class of schema mappings anddistinguish effectively between alternative schema mappings. With these operators; a user isable to quickly and intuitively build and refine complex data transformation queries that mapone data source into another while continuously verifying that the mapping is accurate andappropriate.,*,2006,104
Data Fusion in Three Steps: Resolving Schema; Tuple; and Value Inconsistencies.,Felix Naumann; Alexander Bilke; Jens Bleiholder; Melanie Weis,Abstract Heterogeneous and dirty data is abundant. It is stored under different; often opaqueschemata; it represents identical real-world objects multiple times; causing duplicates; and ithas missing values and conflicting values. Without suitable techniques for integrating andfusing such data; the data quality of an integrated system remains low. We present a suite ofmethods; combined in a single tool; that allows ad-hoc; declarative fusion of such data byemploying schema matching; duplicate detection and data fusion. Guided by a SQL-likequery against one or more tables; we proceed in three fully automated steps: First; instance-based schema matching bridges schematic heterogeneity of the tables by aligningcorresponding attributes. Next; duplicate detection techniques find multiple representationsof identical real-world objects. Finally; data fusion and conflict resolution merges each …,IEEE Data Eng. Bull.,2006,103
Detecting duplicate objects in XML documents,Melanie Weis; Felix Naumann,Abstract The problem of detecting duplicate entities that describe the same real-world object(and purging them) is an important data cleansing task; necessary to improve data quality.For data stored in a flat relation; numerous solutions to this problem exist. As XML becomesincreasingly popular for data representation; algorithms to detect duplicates in nested XMLdocuments are required. In this paper; we present a domain-independent algorithm thateffectively identifies duplicates in an XML document. The solution adopts a top-downtraversal of the XML tree structure to identify duplicate elements on each level. Pairs ofduplicate elements are detected using a thresholded similarity function; and are thenclustered by computing the transitive closure. To minimize the number of pairwise elementcomparisons; an appropriate filter function is used. The similarity measure involves string …,Proceedings of the 2004 international workshop on Information quality in information systems,2004,98
Adaptive windows for duplicate detection,Uwe Draisbach; Felix Naumann; Sascha Szott; Oliver Wonneberg,Duplicate detection is the task of identifying all groups of records within a data set thatrepresent the same real-world entity; respectively. This task is difficult; because (i)representations might differ slightly; so some similarity measure must be defined to comparepairs of records and (ii) data sets might have a high volume making a pair-wise comparisonof all records infeasible. To tackle the second problem; many algorithms have beensuggested that partition the data set and compare all record pairs only within each partition.One well-known such approach is the Sorted Neighborhood Method (SNM); which sorts thedata according to some key and then advances a window over the data comparing onlyrecords that appear within the same window. We propose with the Duplicate Count Strategy(DCS) a variation of SNM that uses a varying window size. It is based on the intuition that …,International Conference on Data Engineering (ICDE),2012,85
Approximate tree embedding for querying XML data,Torsten Schlieder; Felix Naumann,Querying heterogeneous collections of data-centric XML documents requires a combinationof database languages and concepts used in information retrieval; in particular similaritysearch and ranking. In this paper we present an approach to find approximate answers toformal user queries. We reduce the problem of answering queries against XML documentcollections to the well-known unordered tree inclusion problem. We extend this problem toan optimization problem by applying a cost model to the embeddings. Thereby we are ableto determine how close parts of the XML document match a user query. We present anefficient algorithm that finds all approximate matches and ranks them according to theirsimilarity to the query.,*,2000,85
Conflict handling strategies in an integrated information system,Jens Bleiholder; Felix Naumann,Integrated information systems provide users and applications with a unified view ofheterogeneous data sources. To provide a single consistent result for every objectrepresented in these data sources; data fusion is concerned with resolving datainconsistencies present in and among the sources. We present a classification of conflictresolution strategies and show how these are realized using conflict handling functions. Acatalog of such functions is given; together with a description of some of their properties. Wefurther show how the functions are used within an integrated information system; theHumboldt-Merger (HumMer).,*,2006,82
XStruct: Efficient schema extraction from multiple and large XML documents,Jan Hegewald; Felix Naumann; Melanie Weis,XML is the de facto standard format for data exchange on the Web. While it is fairly simple togenerate XML data; it is a complex task to design a schema and then guarantee that thegenerated data is valid according to that schema. As a consequence much XML data doesnot have a schema or is not accompanied by its schema. In order to gain the benefits ofhaving a schema-efficient querying and storage of XML data; semantic verification; dataintegration; etc.-this schema must be extracted. In this paper we present an automatictechnique; XStruct; for XML Schema extraction. Based on ideas of [5]; XStruct extracts aschema for XML data by applying several heuristics to deduce regular expressions that are1-unambiguous and describe each element's contents correctly but generalized to areasonable degree. Our approach features several advantages over known techniques …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,81
Fusion cubes: towards self-service business intelligence,Alberto Abelló; Jérôme Darmont; Lorena Etcheverry; Matteo Golfarelli; José Norberto Mazón López; Felix Naumann; Torben Bach Pedersen; Stefano Rizzi; Juan Carlos Trujillo Mondéjar; Panos Vassiliadis; Gottfried Vossen,Self-service business intelligence is about enabling non-expert users to make well-informeddecisions by enriching the decision process with situational data; ie; data that have a narrowfocus on a specific business problem and; typically; a short lifespan for a small group ofusers. Often; these data are not owned and controlled by the decision maker; their search;extraction; integration; and storage for reuse or sharing should be accomplished by decisionmakers without any intervention by designers or programmers. The goal of this paper is topresent the framework we envision to support self-service business intelligence and therelated research challenges; the underlying core idea is the notion of fusion cubes; ie;multidimensional cubes that can be dynamically extended both in their schema and theirinstances; and in which situational data and metadata are associated with quality and …,International Journal of Data Warehousing and Mining (IJDWM),2012,79
Mapping XML and relational schemas with Clio,Lucian Popa; Mauricio A Hernandez; Yannis Velegrakis; Renée J Miller; Felix Naumann; Howard Ho,Merging and coalescing data from multiple and diverse sources into different data formatscontinues to be an important problem in modern information systems. Schema matching (theprocess of matching elements of a source schema with elements of a target schema) andschema mapping (the process of creating a query that maps between two disparateschemas) are at the heart of data integration systems. We demonstrate Clio; a semi-automatic schema mapping tool developed at the IBM Almaden Research Center. In thispaper; we showcase Clio's mapping engine which allows mapping to and from relationaland XML schemas; and takes advantage of data constraints in order to preserve dataassociations.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,79
Data profiling revisited,Felix Naumann,Abstract Data profiling comprises a broad range of methods to efficiently analyze a givendata set. In a typical scenario; which mirrors the capabilities of commercial data profilingtools; tables of a relational database are scanned to derive metadata; such as data typesand value patterns; completeness and uniqueness of columns; keys and foreign keys; andoccasionally functional dependencies and association rules. Individual research projectshave proposed several additional profiling tasks; such as the discovery of inclusiondependencies or conditional functional dependencies. Data profiling deserves a fresh lookfor two reasons: First; the area itself is neither established nor defined in any principled way;despite significant research activity on individual parts in the past. Second; more and moredata beyond the traditional relational databases are being created and beg to be profiled …,ACM SIGMOD Record,2014,78
Analyzing and predicting viral tweets,Maximilian Jenders; Gjergji Kasneci; Felix Naumann,Abstract Twitter and other microblogging services have become indispensable sources ofinformation in today's web. Understanding the main factors that make certain pieces ofinformation spread quickly in these platforms can be decisive for the analysis of opinionformation and many other opinion mining tasks. This paper addresses important questionsconcerning the spread of information on Twitter. What makes Twitter users retweet a tweet?Is it possible to predict whether a tweet will become" viral"; ie; will be frequently retweeted?To answer these questions we provide an extensive analysis of a wide range of tweet anduser features regarding their influence on the spread of tweets. The most impactful featuresare chosen to build a learning model that predicts viral tweets with high accuracy. Allexperiments are performed on a real-world dataset; extracted through a public Twitter API …,Proceedings of the 22nd international conference on World Wide Web,2013,75
Automatic data fusion with HumMer,Alexander Bilke; Jens Bleiholder; Felix Naumann; Christoph Böhm; Karsten Draba; Melanie Weis,Abstract Heterogeneous and dirty data is abundant. It is stored under different; often opaqueschemata; it represents identical real-world objects multiple times; causing duplicates; and ithas missing values and conflicting values. The Humboldt Merger (HumMer) is a tool thatallows ad-hoc; declarative fusion of such data using a simple extension to SQL. Guided by aquery against multiple tables; HumMer proceeds in three fully automated steps: First;instance-based schema matching bridges schematic heterogeneity of the tables by aligningcorresponding attributes. Next; duplicate detection techniques find multiple representationsof identical real-world objects. Finally; data fusion and conflict resolution merges duplicatesinto a single; consistent; and clean representation.,Proceedings of the 31st international conference on Very large data bases,2005,72
Data fusion and data quality,Felix Naumann,Summary The recent development of the Internet has made an increasing number ofinformation sources available to users. This makes it necessary to submit queries only to themost appropriate sources. When gathering and combining information from these sourcesthe quality offered can and must be a criterion for source selection. However; informationquality has many dimensions and it is thus difficult to directly compare sources with oneanother or give a ranking of sources. Selecting the best sources is thus a multiple attributedecision problem. After introducing four techniques of multiple attribute decision making weapply them to the problem of quality-driven selection of sources: The Simple AdditiveWeighting method (SAW); the TOPSIS method; the Analytical Hierarchy Process method(AHP) and the Data Envelopment Analysis method (DEA). We analyze and compare …,New Techniques & Technologies for Statistics (NTTS),1998,72
Creating void descriptions for web-scale data,Christoph Böhm; Johannes Lorey; Felix Naumann,Abstract When working with large amounts of crawled semantic data as provided by theBillion Triple Challenge (BTC); it is desirable to present the data in a manner best suited forend users. This includes conceiving and presenting explanatory metainformation. TheVocabulary of Interlinked Data (voiD) has been proposed as a means to annotate sets ofRDF resources to facilitate not only human understanding; but also query optimization. Inthis article we introduce tools that automatically generate voiD descriptions for largedatasets. Our approach comprises different means to identify (sub) datasets and annotatethe derived subsets according to the voiD specification. Due to the complexity of Web-scaleLinked Data; all algorithms used for partitioning and augmenting are implemented in a cloudenvironment utilizing the MapReduce paradigm. We employed the Billion Triple …,Web Semantics: Science; Services and Agents on the World Wide Web,2011,64
LINDA: distributed web-of-data-scale entity matching,Christoph Böhm; Gerard de Melo; Felix Naumann; Gerhard Weikum,Abstract Linked Data has emerged as a powerful way of interconnecting structured data onthe Web. However; the cross-linkage between Linked Data sources is not as extensive asone would hope for. In this paper; we formalize the task of automatically creating" sameAs"links across data sources in a globally consistent manner. Our algorithm; presented in amulti-core as well as a distributed version; achieves this link generation by accounting forjoint evidence of a match. Experiments confirm that our system scales beyond 100 millionentities and delivers highly accurate results despite the vast heterogeneity and dauntingscale.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,62
Declarative data fusion–syntax; semantics; and implementation,Jens Bleiholder; Felix Naumann,Abstract In today's integrating information systems data fusion; ie; the merging of multipletuples about the same real-world object into a single tuple; is left to ETL tools and otherspecialized software. While much attention has been paid to architecture; query languages;and query execution; the final step of actually fusing data from multiple sources into aconsistent and homogeneous set is often ignored. This paper states the formal problem ofdata fusion in relational databases and discusses which parts of the problem can already besolved with standard Sql. To bridge the final gap; we propose the SQL Fuse By statementand define its syntax and semantics. A first implementation of the statement in a prototypicaldatabase system shows the usefulness and feasibility of the new operator.,East European Conference on Advances in Databases and Information Systems,2005,58
Data quality in genome databases,Heiko Müller; Felix Naumann; Johann-Christoph Freytag,Genome databases store data about molecular biological entities such as genes; proteins;diseases; etc. The main purpose of creating and maintaining such databases in commercialorganizations is their importance in the process of drug discovery. Genome data is analyzedand interpreted to gain so-called leads; ie; promising structures for new drugs. Following alead through the process of drug development; testing; and finally sev-eral stages of clinicaltrials is extremely expensive. Thus; an underlying high quality data-base is of utmostimportance. Due to the exploratory nature of genome databases; commer-cial and public;they are inaccurate; incomplete; outdated and in an overall poor state. This paper highlightsthe important challenges of determining and improving data quality for databases storingmolecular biological data. We examine the production process for ge-nome data in detail …,International Conference on Information Quality (ICIQ),2003,58
Industry-scale duplicate detection,Melanie Weis; Felix Naumann; Ulrich Jehle; Jens Lufter; Holger Schuster,Abstract Duplicate detection is the process of identifying multiple representations of a samereal-world object in a data source. Duplicate detection is a problem of critical importance inmany applications; including customer relationship management; personal informationmanagement; or data mining. In this paper; we present how a research prototype; namelyDogmatiX; which was designed to detect duplicates in hierarchical XML data; wassuccessfully extended and applied on a large scale industrial relational database incooperation with Schufa Holding AG. Schufa's main business line is to store and retrievecredit histories of over 60 million individuals. Here; correctly identifying duplicates is criticalboth for individuals and companies: On the one hand; an incorrectly identified duplicatepotentially results in a false negative credit history for an individual; who will then not be …,Proceedings of the VLDB Endowment,2008,57
Attribute classification using feature analysis,Felix Naumann; Ching-Tien Ho; Xuqing Tian; Laura M Haas; Nimrod Megiddo,Abstract Database integration and migration are important; but labor-intensive tasks. Totransform data from one representation to another; an expert user must identify and expresscorrespondences between different attributes of different schemata. There are potentiallymany attributes in a source schema that might correspond to a particular target attribute. Ouraim is to ease the burden of the user by classifying source attributes so that they can beautomatically and intelligently matched to target attributes. For categorical data; we presenta novel variation of existing Naяve Bayes classiication techniques based on domain-independent feature selection. For numerical data; we use a quantile-based classiicationmethod; discovering characteristic distributions of the data. We show through extensiveexperiments that automatic classiication of attributes is both feasible and useful for …,icde,2002,57
Profiling relational data: a survey,Ziawasch Abedjan; Lukasz Golab; Felix Naumann,Abstract Profiling data to determine metadata about a given dataset is an important andfrequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata.Among the simpler results are statistics; such as the number of null values and distinctvalues in a column; its data type; or the most frequent patterns of its data values. Metadatathat are more difficult to compute involve multiple columns; namely correlations; uniquecolumn combinations; functional dependencies; and inclusion dependencies. Furthertechniques detect conditional properties of the dataset at hand. This survey provides aclassification of data profiling tasks and comprehensively reviews the state of the art for eachclass. In addition; we review data profiling tools and systems from research and industry …,The VLDB Journal,2015,56
Efficiently detecting inclusion dependencies,Jana Bauckmann; Ulf Leser; Felix Naumann; Véronique Tietz,Data sources for data integration often come with spurious schema definitions such asundefined foreign key constraints. Such metadata are important for querying the databaseand for database integration. We present our algorithm SPIDER (single pass inclusiondependency recognition) for detecting inclusion dependencies; as these are theautomatically testable part of a foreign key constraint. For IND detection all pairs of attributesmust be tested. SPIDER solves this task very efficiently by testing all attribute pairs inparallel. It analyzes a 2 GB database in~ 20 min and a 21 GB database in~ 4 h.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,55
Functional dependency discovery: An experimental evaluation of seven algorithms,Thorsten Papenbrock; Jens Ehrlich; Jannik Marten; Tommy Neubert; Jan-Peer Rudolph; Martin Schönberg; Jakob Zwiener; Felix Naumann,Abstract Functional dependencies are important metadata used for schema normalization;data cleansing and many other tasks. The efficient discovery of functional dependencies intables is a well-known challenge in database research and has seen several approaches.Because no comprehensive comparison between these algorithms exist at the time; it ishard to choose the best algorithm for a given dataset. In this experimental paper; wedescribe; evaluate; and compare the seven most cited and most important algorithms; allsolving this same problem. First; we classify the algorithms into three different categories;explaining their commonalities. We then describe all algorithms with their main ideas. Thedescriptions provide additional details where the original papers were ambiguous orincomplete. Our evaluation of careful re-implementations of all algorithms spans a broad …,Proceedings of the VLDB Endowment,2015,54
Profiling linked open data with ProLOD,Christoph Böhm; Felix Naumann; Ziawasch Abedjan; Dandy Fenz; Toni Grütze; Daniel Hefenbrock; Matthias Pohl; David Sonnabend,Linked open data (LOD); as provided by a quickly growing number of sources constitutes awealth of easily accessible information. However; this data is not easy to understand. It isusually provided as a set of (RDF) triples; often enough in the form of enormous filescovering many domains. What is more; the data usually has a loose structure when it isderived from end-user generated sources; such as Wikipedia. Finally; the quality of theactual data is also worrisome; because it may be incomplete; poorly formatted; inconsistent;etc. To understand and profile such linked open data; traditional data profiling methods donot suffice. With ProLOD; we propose a suite of methods ranging from the domain level(clustering; labeling); via the schema level (matching; disambiguation); to the data level(data type detection; pattern detection; value distribution). Packaged into an interactive …,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,53
XML duplicate detection using sorted neighborhoods,Sven Puhlmann; Melanie Weis; Felix Naumann,Abstract Detecting duplicates is a problem with a long tradition in many domains; such ascustomer relationship management and data warehousing. The problem is twofold: Firstdefine a suitable similarity measure; and second efficiently apply the measure to all pairs ofobjects. With the advent and pervasion of the XML data model; it is necessary to find newsimilarity measures and to develop efficient methods to detect duplicate elements in nestedXML data. A classical approach to duplicate detection in flat relational data is the sortedneighborhood method; which draws its efficiency from sliding a window over the relation andcomparing only tuples within that window. We extend the algorithm to cover not only a singlerelation but nested XML elements. To compare objects we make use of XML parent andchild relationships. For efficiency; we apply the windowing technique in a bottom-up …,International Conference on Extending Database Technology,2006,53
Links and Paths through Life Sciences,Zoé Lacroix; Hyma Murthy; Felix Naumann; Louiqa Raschid,*,Data Integration in the Life Sciences: First International Workshop; DILS 2004; Leipzig; Germany; March 25-26; 2004; Proceedings,2004,50
Declarative data merging with conflict resolution,Felix Naumann; Matthias Häussler,Database integration is a growing and increasingly important field in both research andindustry. Integration requires many steps from initial schema integration and schemamapping; to data scrubbing and cleansing; and finally to data merging. While much researchhas concentrated on the first steps performed at schema level; there are only fewpublications about actual; practical merging of the data in an integrated database or in aquery against multiple databases. When merging data; especially data from autonomoussources; there is a large potential for decreasing the quality of the merged data; even belowthe level of the original sources. The main reasons for decreased quality are data conflictsamong the sources. To address this problem; we define resolution functions mergingconflicting data. We present several alternatives of merging relational data sources with …,International Conference on Information Quality (ICIQ),2002,47
From Databases to Information Systems - Information Quality Makes the Difference.,Felix Naumann,*,International Conference on Information Quality (ICIQ),2001,47
Scalable discovery of unique column combinations,Arvid Heise; Jorge-Arnulfo Quiané-Ruiz; Ziawasch Abedjan; Anja Jentzsch; Felix Naumann,Abstract The discovery of all unique (and non-unique) column combinations in a givendataset is at the core of any data profiling effort. The results are useful for a large number ofareas of data management; such as anomaly detection; data integration; data modeling;duplicate detection; indexing; and query optimization. However; discovering all unique andnon-unique column combinations is an NP-hard problem; which in principle requires toverify an exponential number of column combinations for uniqueness on all data values.Thus; achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper; we devise Ducc; a scalable and efficient approach to the problem of finding allunique and non-unique column combinations in big datasets. We first model the problem asa graph coloring problem and analyze the pruning effect of individual combinations. We …,Proceedings of the VLDB Endowment,2013,46
GovWILD: integrating open government data for transparency,Christoph Böhm; Markus Freitag; Arvid Heise; Claudia Lehmann; Andrina Mascher; Felix Naumann; Vuk Ercegovac; Mauricio Hernandez; Peter Haase; Michael Schmidt,Abstract Many government organizations publish a variety of data on the web to enabletransparency; foster applications; and to satisfy legal obligations. Data content; format;structure; and quality vary widely; even in cases where the data is published using the wide-spread linked data principles. Yet within this data and their integration lies much value: Wedemonstrate GovWILD; a web-based prototype that integrates and cleanses OpenGovernment Data at a large scale. Apart from the web-based interface that presents a usecase of the created dataset at govwild. org; we provide all integrated data as a download.This data can be used to answer questions about politicians; companies; and governmentfunding.,Proceedings of the 21st International Conference on World Wide Web,2012,44
Extracting structured information from Wikipedia articles to populate infoboxes,Dustin Lange; Christoph Böhm; Felix Naumann,Abstract Roughly every third Wikipedia article contains an infobox-a table that displaysimportant facts about the subject in attribute-value form. The schema of an infobox; ie; theattributes that can be expressed for a concept; is defined by an infobox template. Often;authors do not specify all template attributes; resulting in incomplete infoboxes. WithiPopulator; we introduce a system that automatically populates infoboxes of Wikipediaarticles by extracting attribute values from the article's text. In contrast to prior work;iPopulator detects and exploits the structure of attribute values to independently extractvalue parts. We have tested iPopulator on the entire set of infobox templates and provide adetailed analysis of its effectiveness. For instance; we achieve an average extractionprecision of 91% for 1;727 distinct infobox template attributes.,Proceedings of the International Conference on Information and Knowledge Management (CIKM),2010,44
A comparison and generalization of blocking and windowing algorithms for duplicate detection,Uwe Draisbach; Felix Naumann,ABSTRACT Duplicate detection is the problem of identifying pairs of records that representthe same real world object; and could thus be merged into a single record. To avoid aprohibitively expensive comparison of all pairs of records; a common technique is tocarefully partition the records into smaller subsets. If duplicate records appear in the samepartition; only all pairs within each partition must be compared. Two competing approachesare often cited: Blocking methods strictly partition records into disjoint subsets; for instanceusing zip-codes as partitioning key. Windowing methods; in particular the Sorted-Neighborhood method; sort the data according to some key; such as zip-code; and thenslide a window of fixed size across the sorted data and compare pairs only within thewindow. Herein we compare both approaches qualitatively and experimentally. Further …,Proceedings of the International Workshop on Quality in Databases (QDB),2009,44
A machine learning approach to foreign key discovery.,Alexandra Rostin; Oliver Albrecht; Jana Bauckmann; Felix Naumann; Ulf Leser,ABSTRACT We study the problem of automatically discovering semantic associationsbetween schema elements; namely foreign keys. This problem is important in allapplications where data sets need to be integrated that are structured in tables but withoutexplicit foreign key constraints. If such constraints could be recovered automatically;querying and integrating such databases would become much easier. Clearly; one may findcandidates for foreign key constraints in a given database instance by computing allinclusion dependencies (IND) between attributes. However; this set usually contains manyfalse positives due to spurious set inclusions. We present a machine learning approach totackle this problem. We first compute all INDs of a given schema and let each be judged by abinary classification algorithm using a small set of features that can be derived efficiently …,WebDB,2009,43
A generalization of blocking and windowing algorithms for duplicate detection,Uwe Draisbach; Felix Naumann,Duplicate detection is the process of finding multiple records in a dataset that represent thesame real-world entity. Due to the enormous costs of an exhaustive comparison; typicalalgorithms select only promising record pairs for comparison. Two competing approachesare blocking and windowing. Blocking methods partition records into disjoint subsets; whilewindowing methods; in particular the Sorted Neighborhood Method; slide a window over thesorted records and compare records only within the window. We present a new algorithmcalled Sorted Blocks in several variants; which generalizes both approaches. To evaluateSorted Blocks; we have conducted extensive experiments with different datasets. Theseshow that our new algorithm needs fewer comparisons to find the same number ofduplicates.,Data and Knowledge Engineering (ICDKE); 2011 International Conference on,2011,41
Progressive duplicate detection,Thorsten Papenbrock; Arvid Heise; Felix Naumann,Duplicate detection is the process of identifying multiple representations of same real worldentities. Today; duplicate detection methods need to process ever larger datasets in evershorter time: maintaining the quality of a dataset becomes increasingly difficult. We presenttwo novel; progressive duplicate detection algorithms that significantly increase theefficiency of finding duplicates if the execution time is limited: They maximize the gain of theoverall process within the time available by reporting most results much earlier thantraditional approaches. Comprehensive experiments show that our progressive algorithmscan double the efficiency over time of traditional duplicate detection and significantlyimprove upon related work.,IEEE Transactions on knowledge and data engineering,2015,40
A duplicate detection benchmark for XML (and relational) data,Melanie Weis; Felix Naumann; Franziska Brosy,Abstract Duplicate detection; which is an important subtask of data cleaning; is the task ofidentifying multiple representations of a same real-world object. Numerous approaches bothfor relational and XML data exist. Their goals are either on improving the quality of thedetected duplicates (effectiveness) or on saving computation time (efficiency). In particularfor the first goal; the “goodness” of an approach is usually evaluated based on experimentalstudies. Although some methods and data sets have gained popularity; it is still difficult tocompare different approaches or to assess the quality of one own's approach. This difficultyof comparison is mainly due to lack of documentation of algorithms and the data; softwareand hardware used and/or limited resources not allowing to rebuild systems described byothers. In this paper; we propose a benchmark for duplicate detection; specialized to XML …,Proc. of Workshop on Information Quality for Information Systems (IQIS),2006,39
Quality-driven source selection using Data Envelopment Analysis,Felix Naumann; Johann Christoph Freytag; Myra Spiliopoulou,x0y {z 厊}  2€ d|  c Sm wns w 坒Sr v S 厒dSUT 僺doQe 係Ufuw  sd 橯w 坒S  剏fuwnS厔噁S 厀  w 坒S 厔圫r VW 唴adf  VWf2 噲剤SUad 唸VWf q fgc2erqtS 厔{sd 櫒VWf 檇sd剣e7a 憌圴僺uf 唍suc 剣噲SU 唌aU 倯adVWTWa 憅QT 僑wns  c2 唍S 厔噯厴  姄f2VW唌e7a 慹dSU 唌V 僿mf SU 噲SU 唸唸a 憚坧7 wns  実c S 厔坧  suf2T 僷  w 坒S e 俿u唍w  a 憃2o2 剤sdo2 剣VWa 憌nS 唍suc 剣噲SU 唴槂姄f S  VWf 檇sd 剣e7a 憌圴僺uf実c2adTWV 僿乸s 憦tS 厔圫Uv  qgp w 坒SU 唍S  唍suc 剣噲SU 唟噮adf  adf2v  e 哻2唍wj qtS  a  噲剣V 僿nS 厔嘨僺uf  檇sd /nobr> 唍suc 剣噲S  唍SUT 僑U 噲w 圴僺ufjpws 恦朣厒dS 厔U 恵VWf 檇sd 剣e7a 憌圴僺uf  実c2adTWV 僿乸  f2ad 哹e7adfupv2VWe 係Uf2 唸V 僺uf2 唴怮qtsdw 坒  唸cq 抷SU 噲w 圴儌dS adf2v  sdq 抷SU 噲w圴儌dSd 悡adf2vV 僿VW 唚w 坒gc2 唚v2V  c2T 僿w wns0 v2V 儎圫U 噲w 圱僷噲sue 俹 …,Proc. of the 3rd Conference on Information Quality (IQ); Cambridge; MA,1998,39
Profiling and mining RDF data with ProLOD++,Ziawasch Abedjan; Toni Gruetze; Anja Jentzsch; Felix Naumann,Before reaping the benefits of open data to add value to an organizations internal data; suchnew; external datasets must be analyzed and understood already at the basic level of datatypes; constraints; value patterns etc. Such data profiling; already difficult for large relationaldata sources; is even more challenging for RDF datasets; the preferred data model for linkedopen data. We present ProLod++; a novel tool for various profiling and mining tasks tounderstand and ultimately improve open RDF data. ProLod++ comprises various traditionaldata profiling tasks; adapted to the RDF data model. In addition; it features many specificprofiling results for open data; such as schema discovery for user-generated attributes;association rule discovery to uncover synonymous predicates; and uniqueness discoveryalong ontology hierarchies. ProLod++ is highly efficient; allowing interactive profiling for …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,38
Meteor/sopremo: An extensible query language and operator model,Arvid Heise; Astrid Rheinländer; Marcus Leich; Ulf Leser; Felix Naumann,ABSTRACT Recently; quite a few query and scripting languages for Map-Reduce-basedsystems have been developed to ease formulating complex data analysis tasks. However;existing tools mainly provide basic operators for rather simple analyses; such asaggregating or filtering. Analytic functionality for advanced applications; such as datacleansing or information extraction can only be embedded in user-defined functions wherethe semantics is hidden from the query compiler and optimizer. In this paper; we present alanguage that treats application-specific functions as first-class operators; so that operatorsemantics can be evaluated and exploited for optimization at compile time. We presentSopremo; a semantically rich operator model; and Meteor; an extensible query languagethat is grounded in Sopremo. Sopremo also provides a programming framework that …,Workshop on End-to-end Management of Big Data; Istanbul; Turkey,2012,38
A Classification of Schema Mappings and Analysis of Mapping Tools.,Frank Legler; Felix Naumann,Abstract: Schema mapping techniques for data exchange have become popular and usefultools both in research and industry. A schema mapping relates a source schema with atarget schema via correspondences; which are specified by a domain expert possiblysupported by automated schema matching algorithms. The set of correspondences; ie; themapping; is interpreted as a data transformation usually expressed as a query. Thesequeries transform data from the source schema to conform to the target schema. They canbe used to materialize data at the target or used as views in a virtually integrated system. Wepresent a classification of mapping situations that can occur when mapping between tworelational or nested (XML) schemata. Our classification takes into consideration 1: 1 and n:m correspondences; attribute-level and higher-level mappings; and special constructs …,BTW,2007,38
Query planning in the presence of overlapping sources,Jens Bleiholder; Samir Khuller; Felix Naumann; Louiqa Raschid; Yao Wu,Abstract Navigational queries on Web-accessible life science sources pose unique queryoptimization challenges. The objects in these sources are interconnected to objects in othersources; forming a large and complex graph; and there is an overlap of objects in thesources. Answering a query requires the traversal of multiple alternate paths through thesesources. Each path can be associated with the benefit or the cardinality of the target objectset (TOS) of objects reached in the result. There is also an evaluation cost of reaching theTOS. We present dual problems in selecting the best set of paths. The first problem is toselect a set of paths that satisfy a constraint on the evaluation cost while maximizing thebenefit (number of distinct objects in the TOS). The dual problem is to select a set of pathsthat satisfies a threshold of the TOS benefit with minimal evaluation cost. The two …,*,2006,37
Cross-lingual entity matching and infobox alignment in Wikipedia,Daniel Rinser; Dustin Lange; Felix Naumann,Abstract Wikipedia has grown to a huge; multi-lingual source of encyclopedic knowledge.Apart from textual content; a large and ever-increasing number of articles feature so-calledinfoboxes; which provide factual information about the articles' subjects. As the differentlanguage versions evolve independently; they provide different information on the sametopics. Correspondences between infobox attributes in different language editions can beleveraged for several use cases; such as automatic detection and resolution ofinconsistencies in infobox data across language versions; or the automatic augmentation ofinfoboxes in one language with data from other language versions. We present an instance-based schema matching technique that exploits information overlap in infoboxes acrossdifferent language editions. As a prerequisite we present a graph-based approach to …,Information Systems,2013,36
Managing ETL Processes,Alexander Albrecht; Felix Naumann,ABSTRACT ETL tools allow the definition of sometimes complex processes to extract;transform; and load heterogeneous data into a data warehouse or to perform other datamigration tasks. In larger organizations many ETL processes of different data integrationprojects are accumulated. Such processes can encompass common sub-processes; shareddata sources and targets; and same or similar operations. However; there is no commonmethod or approach to systematically manage such ETL processes. We propose thehighlevel management of such processes as a generic approach to enable their flexible re-use; optimization; and rapid development. To this end we introduce a set of basic operatorson ETL processes; such as merge or invert; and motivate their use in several scenarios.,New Trends in Information Integration (NTII),2008,36
Detecting duplicates in complex XML data,Melanie Weis; Felix Naumann,Recent work both in the relational and the XML world have shown that the efficacy andefficiency of duplicate detection is enhanced by regarding relationships between entities.However; most approaches for XML data rely on 1: n parent/child relationships; and do notapply to XML data that represents m: n relationships. We present a novel comparisonstrategy; which performs duplicate detection effectively for all kinds of parent/childrelationships; given dependencies between different XML elements. Due to cyclicdependencies; it is possible that a pairwise classification is performed more than once;which compromises efficiency. We propose an order that reduces the number of suchreclassifications and apply it to two algorithms. The first algorithm performs reclassifications;and efficiency is increased by using the order reducing the number of reclassifications …,Proceedings of the International Conference on Data Engineering (ICDE),2006,36
Integrating open government data with stratosphere for more transparency,Arvid Heise; Felix Naumann,Abstract Governments are increasingly publishing their data to enable organizations andcitizens to browse and analyze the data. However; the heterogeneity of this OpenGovernment Data hinders meaningful search; analysis; and integration and thus limits thedesired transparency. In this article; we present the newly developed data integrationoperators of the Stratosphere parallel data analysis framework to overcome theheterogeneity. With declaratively specified queries; we demonstrate the integration of well-known government data sources and other large open data sets at technical; structural; andsemantic levels. Furthermore; we publish the integrated data on the Web in a form thatenables users to discover relationships between persons; government agencies; funds; andcompanies. The evaluation shows that linking person entities of different data sets results …,Web Semantics: Science; Services and Agents on the World Wide Web,2012,34
Assessing the completeness of sensor data,Jit Biswas; Felix Naumann; Qiang Qiu,Abstract In this paper we present a quality model highlighting the completeness of sensordata with respect to its application. The model allows consistent handling of information lossas data propagates through a sensor network. The tradeoffs between various factors thatinfluence completeness are quantified thereby allowing an integrated view of completenessat various levels in a system. The paper is presented in the context of the fast emerging fieldof smart spaces. All concepts in the paper have a foundation in real-life problems arising inthis context. Preliminary implementation results are presented to illustrate the value of thecompleteness based approach versus one that does not use completeness.,International Conference on Database Systems for Advanced Applications,2006,32
Improving RDF data through association rule mining,Ziawasch Abedjan; Felix Naumann,Abstract Linked Open Data comprises very many and often large public data sets; which aremostly presented in the Rdf triple structure of subject; predicate; and object. However; theheterogeneity of available open data requires significant integration steps before it can beused in applications. A promising and novel technique to explore such data is the use ofassociation rule mining. We introduce “mining configurations”; which allow us to mine Rdfdata sets in various ways. Different configurations enable us to identify schema and valuedependencies that in combination result in interesting use cases. We present rule-basedapproaches for predicate suggestion; data enrichment; ontology improvement; and queryrelaxation. On the one hand we prevent inconsistencies in the data through predicatesuggestion; enrichment with missing facts; and alignment of the corresponding ontology …,Datenbank-Spektrum,2013,31
Scalable iterative graph duplicate detection,Melanie Herschel; Felix Naumann; Sascha Szott; Maik Taubert,Duplicate detection determines different representations of real-world objects in a database.Recent research has considered the use of relationships among object representations toimprove duplicate detection. In the general case where relationships form a graph; researchhas mainly focused on duplicate detection quality/effectiveness. Scalability has beenneglected so far; even though it is crucial for large real-world duplicate detection tasks. Wescale-up duplicate detection in graph data (DDG) to large amounts of data and pairwisecomparisons; using the support of a relational database management system. To this end;we first present a framework that generalizes the DDG process. We then present algorithmsto scale DDG in space (amount of data processed with bounded main memory) and in time.Finally; we extend our framework to allow batched and parallel DDG; thus further …,IEEE Transactions on Knowledge and Data Engineering,2012,31
DuDe: The duplicate detection toolkit,Uwe Draisbach; Felix Naumann,ABSTRACT Duplicate detection; also known as entity matching or record linkage; was firstdefined by Newcombe et al.[19] and has been a research topic for several decades. Thechallenge is to effectively and efficiently identify pairs of records that represent the same realworld entity. Researchers have developed and described a variety of methods to measurethe similarity of records and/or to reduce the number of required comparisons. Comparingthese methods to each other is essential to assess their quality and efficiency. However; it isstill difficult to compare results; as there usually are differences in the evaluated datasets; thesimilarity measures; the implementation of the algorithms; or simply the hardware on whichthe code is executed. To face this challenge; we are developing the comprehensiveduplicate detection toolkit “DuDe”. DuDe already provides multiple methods and datasets …,Proceedings of the International Workshop on Quality in Databases (QDB),2010,31
Self-extending peer data management,Ralf Heese; Sven Herschel; Felix Naumann; Armin Roth,Peer data management systems (PDMS) are the natural extension of integrated informationsystems. Conventionally; a single integrating system manages an integrated schema;distributes queries to appropriate sources; and integrates incoming data to a common result.In contrast; a PDMS consists of a set of peers; each of which can play the role of anintegrating component. A peer knows about its neighboring peers by mappings; which helpto translate queries and transform data. Queries submitted to one peer are answered by dataresiding at that peer and by data that is reached along paths of mappings through thenetwork of peers. The only restriction for PDMS to cover unbounded data is the need toformulate at least one mapping from some known peer to a new data source. We propose aSemantic Web based method that overcomes this restriction; albeit at a price. As sources …,Datenbanksysteme in Business; Technologie und Web (BTW),2005,30
Completeness of information sources,Felix Naumann; Johann-Christoph Freytag; Ulf Leser,Information quality plays a crucial role in every application that integrates data fromautonomous sources. However; information quality is hard to measure and complex toconsider for the tasks of information integration; even if the integrating sources cooperate.We present a systematic and formal approach to the measurement of information quality andthe combination of such measurements for information integration. Our approach is basedon a value model that incorporates both extensional value (coverage) and intensional value(density) of information. For both aspects we provide merge functions for adequately scoringintegrated results. Also; we combine the two criteria to an overall completeness criterion thatformalizes the intuitive notion of completeness of query results. This completeness measureis a valuable tool to assess source size and to predict result sizes of queries in integrated …,*,2003,29
Do Metadata Models meet IQ Requirements?,Felix Naumann; Claudia Rolker,Abstract Research has recognized the importance ofanalyzing information quality (IQ) formany different applications: The success of data integration greatly depends on the qualityof the individual data. In statistical applications poor data quality often leads to wrongconclusions. High information quality is literally a vital property of hospital informationsystems. Poor dataquality of stock price information services can lead to economically wrongdecisions. Several projects have analyzed this need for IQ metadata and have proposed aset of IQ criteria or attributes which can be used to properly assess information quality. In thispaper we survey and compare these approaches. In a second step we take a look at existingprominent proposals of metadata models; especially those on the Internet. Then; we matchthese models to the requirements of information quality modeling. Finally; we propose a …,International Conference on Information Quality (ICIQ),1999,29
Query planning with information quality bounds,Ulf Leser; Felix Naumann,Abstract Query planning for information integration using a local-as-view approach isexponential in the size of the user query. Furthermore; it may generate an exponentialnumber of plans; many of which will produce results of very poor quality. We propose to useinformation quality reasoning to speed up query planning. We construct tight upper qualitybounds for a branch & bound algorithm. The algorithm uses these quality scores to filter outnon-promising plans early on. Experiments show that this approach dramatically improvesplanning time without compromising the quality of the result.,*,2001,28
Data profiling with metanome,Thorsten Papenbrock; Tanja Bergmann; Moritz Finke; Jakob Zwiener; Felix Naumann,Abstract Data profiling is the discipline of discovering metadata about given datasets. Themetadata itself serve a variety of use cases; such as data integration; data cleansing; orquery optimization. Due to the importance of data profiling in practice; many tools haveemerged that support data scientists and IT professionals in this task. These tools providegood support for profiling statistics that are easy to compute; but they are usually lackingautomatic and efficient discovery of complex statistics; such as inclusion dependencies;unique column combinations; or functional dependencies. We present Metanome; anextensible profiling platform that incorporates many state-of-the-art profiling algorithms.While Metanome is able to calculate simple profiling statistics in relational data; its focus lieson the automatic discovery of complex metadata. Metanome's goal is to provide novel …,Proceedings of the VLDB Endowment,2015,26
Advancing the discovery of unique column combinations,Ziawasch Abedjan; Felix Naumann,Abstract Unique column combinations of a relational database table are sets of columns thatcontain only unique values. Discovering such combinations is a fundamental researchproblem and has many different data management and knowledge discovery applications.Existing discovery algorithms are either brute force or have a high memory load and canthus be applied only to small datasets or samples. In this paper; the well-known Gordianalgorithm [9] and" Apriori-based" algorithms [4] are compared and analyzed for furtheroptimization. We greatly improve the Apriori algorithms through efficient candidategeneration and statistics-based pruning methods. A hybrid solution HCA-Gordian combinesthe advantages of Gordian and our new algorithm HCA; and it outperforms all previous workin many situations.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,26
Object identification quality,Mattis Neiling; Steffen Jurk; Hans-J Lenz; Felix Naumann,Research and industry has tackled the object identification problem of data integration inmany different ways. This paper presents a framework; that allows the evaluation ofcompeting approaches. To this end; complexity measures and data characteristics areintroduced; which reflect the hardness of a given object identification problem. Allcharacteristics can be estimated by use of simple SQL queries and simple calculations.Following the principle of benchmark definitions we specify a test framework. It consists of atest database and its characteristics; quality criteria; and a test specification. Adequatemeasures needed for the correctness criterion of the benchmark are given. A runningexample of the Berlin Online Apartment-Advertisements database (BOA) illustrates theapproach. The BOA-database is freely available at www. wiwiss. fu-berlin. de/lenz/boa/.,International Workshop on Data Quality in Cooperative Information Systsems (DQCIS),2003,26
Linking open government data: what journalists wish they had known,Christoph Böhm; Felix Naumann; Markus Freitag; Stefan George; Norman Höfler; Martin Köppelmann; Claudia Lehmann; Andrina Mascher; Tobias Schmidt,Abstract Many government organizations publish a variety of data on the web to facilitatetransparency. The multitude of sources has resulted in heterogeneous structures andformats as well as varying quality of such data. We report on a project dubbed GovWild (< u>Gov</u> ernment< u> W</u> eb Data< u> I</u> ntegration for< u> L</u> inked< u> D</u>ata) that integrates and cleanses open government data at a large scale. Also; we point tothe unified and clean integration result; published as Linked Open Data at govwild. hpi-web.de; and feature our web application to showcase the usability of the created dataset.,Proceedings of the 6th International Conference on Semantic Systems,2010,25
Subsumption and complementation as data fusion operators,Jens Bleiholder; Sascha Szott; Melanie Herschel; Frank Kaufer; Felix Naumann,Abstract The goal of data fusion is to combine several representations of one real worldobject into a single; consistent representation; eg; in data integration. A very popularoperator to perform data fusion is the minimum union operator. It is defined as the outerunion and the subsequent removal of subsumed tuples. Minimum union is used in otherapplications as well; for instance in database query optimization to rewrite outer join queries;in the semantic web community in implementing Sparql's optional operator; etc. Despite itswide applicability; there are only few efficient implementations; and until now; minimumunion is not a relational database primitive. This paper fills this gap as we presentimplementations of subsumption that serve as a building block for minimum union.Furthermore; we consider this operator as database primitive and show how to perform …,Proceedings of the 13th International Conference on Extending Database Technology,2010,25
A research agenda for query processing in large-scale peer data management systems,Katja Hose; Armin Roth; André Zeitz; Kai-Uwe Sattler; Felix Naumann,Abstract Peer Data Management Systems (P dms) are a novel; useful; but challengingparadigm for distributed data management and query processing. Conventional integratedinformation systems have a hierarchical structure with an integration component thatmanages a global schema and distributes queries against this schema to the underlyingdata sources. P dms are a natural extension to this architecture by allowing eachparticipating system (peer) to act both as a data source and as an integrator. Peers areinterconnected by schema mappings; which guide the rewriting of queries between theheterogeneous schemas; and thus form a P2P (peer-to-peer)-like network. Despite severalyears of research; the development of efficient P dms still holds many challenges. In thisarticle we first survey the state of the art on peer data management: We classify P dms by …,Information Systems,2008,25
Efficiently computing inclusion dependencies for schema discovery,Jana Bauckmann; Ulf Leser; Felix Naumann,Large data integration projects must often cope with undocumented data sources. Schemadiscovery aims at automatically finding structures in such cases. An important class ofrelationships between attributes that can be detected automatically are inclusiondependencies (IND); which provide an excellent basis for guessing foreign key constraints.INDs can be discovered by comparing the sets of distinct values of pairs of attributes. In thispaper we present efficient algorithms for finding unary INDs. We first show that (and why)SQL is not suitable for this task. We then develop two algorithms that compute inclusiondependencies outside of the database. Both are much faster than the SQL-based methods;in fact; for larger schemas they are the only feasible solution. Our experiments show that wecan compute all unary INDs in a schema of 1; 680 attributes with a total database size of …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,25
Latent topics in graph-structured data,Christoph Böhm; Gjergji Kasneci; Felix Naumann,Abstract Large amounts of graph-structured data are emerging from various avenues;ranging from natural and life sciences to social and semantic web communities. We addressthe problem of discovering subgraphs of entities that reflect latent topics in graph-structureddata. These topics are structured meta-information providing further insights into the data.The presented approach effectively detects such topics by exploiting only the structure of theunderlying graph; thus avoiding the dependency on textual labels; which are a scarce assetin prevalent graph datasets. The viability of our approach is demonstrated in experiments onreal-world datasets.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,24
Context and target configurations for mining RDF data,Ziawasch Abedjan; Felix Naumann,Abstract Association rule mining has been widely studied in the context of basket analysisand sale recommendations [1]. In fact; the concept can be applied to any domain with manyitems or events in which interesting relationships can be inferred from co-occurrence ofthose items or events in existing subsets (transactions). The increasing amount of LinkedOpen Data (LOD) in the World Wide Web raises new opportunities and challenges for thedata mining community [5]. LOD is often represented in the Resource DescriptionFramework (RDF) data model. In RDF; data is represented by a triple structure consisting ofsubject; predicate; and object (SPO). Each triple represents a statement/fact. We propose anapproach that applies association rule mining at statement level by introducing the conceptof mining configurations.,Proceedings of the 1st international workshop on Search and mining entity-relationship data,2011,24
FuSem: exploring different semantics of data fusion,Jens Bleiholder; Karsten Draba; Felix Naumann,Abstract Data fusion is the final step of a typical data integration process; after schematicconflicts have been overcome and after duplicates have been correctly identified. Wepresent the relational data fusion system FuSem; which uses schema mappings andinformation about duplicates to decide what to fuse; ie; which tuples to merge into one. Theaspect emphasized by the demo is how to fuse the duplicates with FuSem. First; it offersseveral conflict resolution functions to handle data conflicts among duplicates. Furthermore;different fusion semantics proposed in the literature; such as MatchJoin or ConQuer; can becompared and visually explored. Optimized execution allows interactive access to the dataand thus to explore the different data fusion procedures.,Proceedings of the 33rd international conference on Very large data bases,2007,24
(Almost) Hands-Off Information Integration for the Life Sciences,Ulf Leser; Felix Naumann,Data integration in complex domains; such as the life sciences; involves either manual datacuration; offering highest information quality at highest price; or follows a schema integrationand mapping approach; leading to moderate information quality at a moderate price. Wesuggest a radically differ-ent integration approach; called ALADIN; for the life sciencesapplication domain. The predominant feature of the ALADIN system is an architecture thatallows almost automatic integration of new data sources into the system; ie; it offers data in-tegration at almost no cost. We suggest a novel combination of data and text mining; schemamatching; and duplicate detection to combat the reduction in information quality that seemsinevitable when demanding a high degree of automatism. These heuristics can also lead tothe detection of previously unknown or unseen rela-tionships between objects; thus …,Conference on Innovative Database Research (CIDR),2005,24
DFD: Efficient functional dependency discovery,Ziawasch Abedjan; Patrick Schulze; Felix Naumann,Abstract The discovery of unknown functional dependencies in a dataset is of greatimportance for database redesign; anomaly detection and data cleansing applications.However; as the nature of the problem is exponential in the number of attributes none of theexisting approaches can be applied on large datasets. We present a new algorithm DFD fordiscovering all functional dependencies in a dataset following a depth-first traversal strategyof the attribute lattice that combines aggressive pruning and efficient result verification. Ourapproach is able to scale far beyond existing algorithms for up to 7.5 million tuples; and is upto three orders of magnitude faster than existing approaches on smaller datasets.,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,23
Collecting; annotating; and classifying public web services,Mohammed AbuJarour; Felix Naumann; Mircea Craculeac,Abstract The limitations of the traditional SOA operational model; such as the lack of richservice descriptions; weaken the role of service registries. Their removal from the modelviolates the basic principles of SOA; namely; dynamic binding and loose coupling. Currently;most service providers publish their Web Services on their websites instead of publishingthem in service registries. This results in poor usability of these Web Services especially wrt.service discovery and service composition. To handle this problem; we propose to increasethe usability of public Web Services by collecting them automatically from the websites oftheir providers with the help of web crawling techniques. Additionally; the collected servicesare annotated with descriptions that are extracted from the crawled web pages and tags thatare generated from the same web pages. These annotations are then used to derive a …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2010,23
Graph-based concept identification and disambiguation for enterprise search,Falk Brauer; Michael Huber; Gregor Hackenbroich; Ulf Leser; Felix Naumann; Wojciech M Barczynski,Abstract Enterprise Search (ES) is different from traditional IR due to a number of reasons;among which the high level of ambiguity of terms in queries and documents and existence ofgraph-structured enterprise data (ontologies) that describe the concepts of interest and theirrelationships to each other; are the most important ones. Our method identifies conceptsfrom the enterprise ontology in the query and corpus. We propose a ranking scheme forontology sub-graphs on top of approximately matched token q-grams. The rankingleverages the graph-structure of the ontology to incorporate not explicitly mentionedconcepts. It improves previous solutions by using a fine-grained ranking function that isspecifically designed to cope with high levels of ambiguity. This method is able to capturemuch more of the semantics of queries and documents than previous techniques. We …,Proceedings of the 19th international conference on World wide web,2010,23
Discovering conditional inclusion dependencies,Jana Bauckmann; Ziawasch Abedjan; Ulf Leser; Heiko Müller; Felix Naumann,Abstract Data dependencies are used to improve the quality of a database schema; tooptimize queries; and to ensure consistency in a database. Conditional dependencies havebeen introduced to analyze and improve data quality. A conditional dependency is adependency with a limited scope defined by conditions over one or more attributes. Only thematching part of the instance must adhere to the dependency. In this paper we focus onconditional inclusion dependencies (CINDs). We generalize the definition of CINDs;distinguishing covering and completeness conditions. We present a new use case for suchCINDs showing their value for solving complex data quality tasks. Further; we proposeefficient algorithms that identify covering and completeness conditions conforming to givenquality thresholds. Our algorithms choose not only the condition values but also the …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,22
Datenqualität,Felix Naumann,Motivation Das Customer-Relationship Management (CRM) ist eines der primären Betätigungsfelderfür die Datenreinigung; denn elektronisch erfasste Kun- dendaten haben viele Fehlerquellenund Fehler in Kundendaten können vielfältige negative Wirkun- gen haben. Bei der Erfassungvon Kundendaten via Telefon entstehen ua Tippfehler; Verständnisfehler und fehlendeWerte; da Kunden oft nicht bereit sind; alle Daten anzugeben oder bewusst falsche Datennennen. Zudem entstehen leicht Dublet- ten (auch “Duplikate”); zB wenn Kundendaten mehrfachoder über verschiedene Kanäle (Telefon; www-Formular; Brief) mit dem Unternehmen in Kontakttreten. Auf der anderen Seite verursachen fehlerhafte Daten Kosten. Als einfaches Beispiel dientdas Nichterreichen eines Kunden oder das doppelte Versenden eines aufwändigenKatalogs. Schwerer wiegen nicht erkannte Gefahren; etwa wenn ein Kunde unter …,Informatik-Spektrum,2007,22
A hybrid approach to functional dependency discovery,Thorsten Papenbrock; Felix Naumann,Abstract Functional dependencies are structural metadata that can be used for schemanormalization; data integration; data cleansing; and many other data management tasks.Despite their importance; the functional dependencies of a specific dataset are usuallyunknown and almost impossible to discover manually. For this reason; database researchhas proposed various algorithms for functional dependency discovery. None; however; areable to process datasets of typical real-world size; eg; datasets with more than 50 attributesand a million records. We present a hybrid discovery algorithm called HyFD; whichcombines fast approximation techniques with efficient validation techniques in order to findall minimal functional dependencies in a given dataset. While operating on compact datastructures; HyFD not only outperforms all existing approaches; it also scales to much …,Proceedings of the 2016 International Conference on Management of Data (SIGMOD),2016,21
Divide & conquer-based inclusion dependency discovery,Thorsten Papenbrock; Sebastian Kruse; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,Abstract The discovery of all inclusion dependencies (INDs) in a dataset is an important partof any data profiling effort. Apart from the detection of foreign key relationships; INDs canhelp to perform data integration; query optimization; integrity checking; or schema (re-)design. However; the detection of INDs gets harder as datasets become larger in terms ofnumber of tuples as well as attributes. To this end; we propose Binder; an IND detectionsystem that is capable of detecting both unary and n-ary INDs. It is based on a divide &conquer approach; which allows to handle very large datasets--an important property on theface of the ever increasing size of today's data. In contrast to most related works; we do notrely on existing database functionality nor assume that inspected datasets fit into mainmemory. This renders Binder an efficient and scalable competitor. Our exhaustive …,Proceedings of the VLDB Endowment,2015,21
Synonym analysis for predicate expansion,Ziawasch Abedjan; Felix Naumann,Abstract Despite unified data models; such as the Resource Description Framework (Rdf) onstructural level and the corresponding query language Sparql; the integration and usage ofLinked Open Data faces major heterogeneity challenges on the semantic level. Incorrect useof ontology concepts and class properties impede the goal of machine readability andknowledge discovery. For example; users searching for movies with a certain artist cannotrely on a single given property artist; because some movies may be connected to that artistby the predicate starring. In addition; the information need of a data consumer may notalways be clear and her interpretation of given schemata may differ from the intentions of theontology engineer or data publisher. It is thus necessary to either support users during queryformulation or to incorporate implicitly related facts through predicate expansion. To this …,Extended Semantic Web Conference,2013,19
Reconciling ontologies and the web of data,Ziawasch Abedjan; Johannes Lorey; Felix Naumann,Abstract To integrate Linked Open Data; which originates from various and heterogeneoussources; the use of well-defined ontologies is essential. However; oftentimes the utilizationof these ontologies by data publishers differs from the intended application envisioned byontology engineers. This may lead to unspecified properties being used ad-hoc aspredicates in RDF triples or it may result in infrequent usage of specified properties. Thesemismatches impede the goals and propagation of the Web of Data as data consumers facedifficulties when trying to discover and integrate domain-specific information. In this work; weidentify and classify common misusage patterns by employing frequency analysis and rulemining. Based on this analysis; we introduce an algorithm to propose suggestions for a data-driven ontology re-engineering workflow; which we evaluate on two large-scale RDF …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,19
BioFast: challenges in exploring linked life sciences sources,Jens Bleiholder; Felix Naumann; Zoé Lacroix; Louiqa Raschid; Hyma Murthy; Maria-Esther Vidal,Abstract An abundance of life sciences data sources contain data about scientific entitiessuch as genes and sequences. Scientists are interested in exploring relationships betweenscientific objects; eg; between genes and bibliographic citations. A scientist may choose theOMIM source; which contains information related to human genetic diseases; as a startingpoint for her exploration; and wish to eventually retrieve all related citations from thePUBMED source. Starting with a keyword search on a certain disease; she can explore allpossible relationships between genes in OMIM and citations in PUBMED. This correspondsto the following query:" Return all citations of PUBMED that are linked to an OMIM entry thatis related to some disease or condition.",ACM SIGMOD Record,2004,19
Schema management,Periklis Andritsos; Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Phokion G Kolaitis; Renée J Miller; Felix Naumann; Lucian Popa; Yannis Velegrakis,Abstract Clio is a management system for heterogeneous data that couples a traditionaldatabase management engine with additional tools for managing schemas (models of data)and mappings between schemas. In this article; we provide a brief overview of Clio andplace our solutions in the context of the rich research literature on data integration andtransformation. Clio is the result of an on-going collaboration between the University ofToronto and IBM Almaden Research Center in which we are addressing both foundationaland systems issues related to heterogeneous data; schema; and integration management.,IEEE Data Engineering Bulletin,2002,19
Holistic and Scalable Ontology Alignment for Linked Open Data.,Toni Gruetze; Christoph Böhm; Felix Naumann,ABSTRACT The Linked Open Data community continuously releases massive amounts ofRDF data that shall be used to easily create applications that incorporate data from differentsources. Inter-operability across different sources requires links at instance-and at schema-level; thus connecting entities on the one hand and relating concepts on the other hand.State-of-the-art entity-and ontology-alignment methods produce high quality alignments fortwo “nicely structured” individual sources; where an identification of relevant and meaningfulpairs of ontologies is a precondition. Thus; these methods cannot deal with heterogeneousdata from many sources simultaneously; eg; data from a linked open data web crawl. To thisend we propose Holistic Concept Matching (HCM). HCM aligns thousands of concepts fromhundreds of ontologies (from many sources) simultaneously; while maintaining scalability …,LDOW,2012,18
Benefit and cost of query answering in PDMS,Armin Roth; Felix Naumann,Abstract Peer data management systems (PDMS) are a natural extension to integratedinformation systems. They consist of a dynamic set of autonomous peers; each of which canmediate between heterogenous schemas of other peers. A new data source joins a PDMSby defining a semantic mapping to one or more other peers; thus forming a network of peers.Queries submitted to a peer are answered with data residing at that peer and by data that isreached along paths of mappings through the network of peers. However; withoutoptimization methods query reformulation in PDMS is very inefficient due to redundancy inmapping paths. We present a decentral strategy that guides peers in their decision alongwhich further mappings the query should be sent. The strategy uses statistics of the peersown data and statistics of mappings to neighboring peers to predict whether it is …,*,2007,18
SOFA: An extensible logical optimizer for UDF-heavy data flows,Astrid Rheinländer; Arvid Heise; Fabian Hueske; Ulf Leser; Felix Naumann,Abstract Recent years have seen an increased interest in large-scale analytical data flowson non-relational data. These data flows are compiled into execution graphs scheduled onlarge compute clusters. In many novel application areas the predominant building blocks ofsuch data flows are user-defined predicates or functions (U df s). However; the heavy use ofU df s is not well taken into account for data flow optimization in current systems. S ofa is anovel and extensible optimizer for U df-heavy data flows. It builds on a concise set ofproperties for describing the semantics of Map/Reduce-style U df s and a small set of rewriterules; which use these properties to find a much larger number of semantically equivalentplan rewrites than possible with traditional techniques. A salient feature of our approach isextensibility: we arrange user-defined operators and their properties into a subsumption …,Information Systems,2015,16
BEL: Bagging for entity linking,Zhe Zuo; Gjergji Kasneci; Toni Gruetze; Felix Naumann,Abstract With recent advances in the areas of knowledge engineering and informationextraction; the task of linking textual mentions of named entities to corresponding ones in aknowledge base has received much attention. The rich; structured information in state-of-the-art knowledge bases can be leveraged to facilitate this task. Although recent approachesachieve satisfactory accuracy results; they typically suffer from at least one of the followingissues:(1) the linking quality is highly sensitive to the amount of textual information; typically;long textual fragments are needed to capture the context of a mention;(2) the disambiguationuncertainty is not explicitly addressed and often only implicitly represented by the ranking ofentities to which a mention could be linked;(3) complex; joint reasoning negatively affectsthe efficiency. We propose an entity linking technique that addresses the above issues by …,Proceedings of COLING 2014; the 25th International Conference on Computational Linguistics: Technical Papers,2014,16
Detecting SPARQL query templates for data prefetching,Johannes Lorey; Felix Naumann,Abstract Publicly available Linked Data repositories provide a multitude of information. Byutilizing Sparql; Web sites and services can consume this data and present it in a user-friendly form; eg; in mash-ups. To gather RDF triples for this task; machine agents typicallyissue similarly structured queries with recurring patterns against the Sparql endpoint. Thesequeries usually differ only in a small number of individual triple pattern parts; such asresource labels or literals in objects. We present an approach to detect such recurringpatterns in queries and introduce the notion of query templates; which represent clusters ofsimilar queries exhibiting these recurrences. We describe a matching algorithm to extractquery templates and illustrate the benefits of prefetching data by utilizing these templates.Finally; we comment on the applicability of our approach using results from real-world …,Extended Semantic Web Conference,2013,16
Scaling up duplicate detection in graph data,Melanie Herschel; Felix Naumann,Abstract Duplicate detection determines different representations of real-world objects in adatabase. Recent research has considered the use of relationships among objectrepresentations to improve duplicate detection. In the general case where relationships forma graph; research has mainly focused on duplicate detection quality/effectiveness.Scalability has been neglected so far; even though it is crucial for large real-world duplicatedetection tasks. We scale up duplicate detection in graph data (DDG) to large amounts ofdata using the support of a relational database system. We first generalize the process ofDDG and then present how to scale DDG in space (amount of data processed with limitedmain memory) and in time. Finally; we explore how complex similarity computation can beperformed efficiently. Experiments on data an order of magnitude larger than data …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,16
Caching and prefetching strategies for sparql queries,Johannes Lorey; Felix Naumann,Abstract Linked Data repositories offer a wealth of structured facts; useful for a wide array ofapplication scenarios. However; retrieving this data using Sparql queries yields a number ofchallenges; such as limited endpoint capabilities and availability; or high latency forconnecting to it. To cope with these challenges; we argue that it is advantageous to cachedata that is relevant for future information needs. However; instead of retaining only resultsof previously issued queries; we aim at retrieving data that is potentially interesting forsubsequent requests in advance. To this end; we present different methods to modify thestructure of a query so that the altered query can be used to retrieve additional relatedinformation. We evaluate these approaches by applying them to requests found in real-worldSparql query logs.,Extended Semantic Web Conference,2013,15
Efficient similarity search in very large string sets,Dandy Fenz; Dustin Lange; Astrid Rheinländer; Felix Naumann; Ulf Leser,Abstract String similarity search is required by many real-life applications; such as spellchecking; data cleansing; fuzzy keyword search; or comparison of DNA sequences. Given avery large string set and a query string; the string similarity search problem is to efficientlyfind all strings in the string set that are similar to the query string. Similarity is defined using asimilarity (or distance) measure; such as edit distance or Hamming distance. In this paper;we introduce the State Set Index (SSI) as an efficient solution for this search problem. SSI isbased on a trie (prefix index) that is interpreted as a nondeterministic finite automaton. SSIimplements a novel state labeling strategy making the index highly space-efficient.Furthermore; SSI's space consumption can be gracefully traded against search time. Weevaluated SSI on different sets of person names with up to 170 million strings from a …,International Conference on Scientific and Statistical Database Management,2012,15
Efficient similarity search: arbitrary similarity measures; arbitrary composition,Dustin Lange; Felix Naumann,Abstract Given a (large) set of objects and a query; similarity search aims to find all objectssimilar to the query. A frequent approach is to define a set of base similarity measures for thedifferent aspects of the objects; and to build light-weight similarity indexes on thesemeasures. To determine the overall similarity of two objects; the results of these basemeasures are composed; eg; using simple aggregates or more involved machine learningtechniques. We propose the first solution to this search problem that does not place anyrestrictions on the similarity measures; the composition technique; or the data set size. Wedefine the query plan optimization problem to determine the best query plan using thesimilarity indexes. A query plan must choose which individual indexes to access and whichthresholds to apply. The plan result should be as complete as possible within some cost …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,15
Super-Fast XML wrapper generation in DB2: A demonstration,Vanja Josifovski; Sabine Massmann; Felix Naumann,The XML wrapper is a new feature of the federated database capabilities of DB2/UDB v8. Itenables users and applications to issue SQL queries against XML data from a variety ofsources; including files and Web services. The XML wrapper assumes hierarchical XMLdocuments modeled as families of virtual relational tables in a federated schema; which canthen be queried to extract information from the XML and combine it with data from othersources. Due to the nature of the problem; using the XML wrapper is complex and severaldifficult steps must be undertaken:(i) The hierarchical schema of the source must beflattened to a relational form;(ii) Each relation of the flattened schema must be registered inDB2 as a NICKNAME-a complex virtual table definition containing several XPaths asspecialized options.(iii) Each NICKNAME must be accompanied by a VIEW-again a …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,15
Topic modeling for expert finding using latent Dirichlet allocation,Saeedeh Momtazi; Felix Naumann,Abstract The task of expert finding is to rank the experts in the search space given a field ofexpertise as an input query. In this paper; we propose a topic modeling approach for thistask. The proposed model uses latent Dirichlet allocation (LDA) to induce probabilistictopics. In the first step of our algorithm; the main topics of a document collection areextracted using LDA. The extracted topics present the connection between expertcandidates and user queries. In the second step; the topics are used as a bridge to find theprobability of selecting each candidate for a given query. The candidates are then rankedbased on these probabilities. The experimental results on the Text REtrieval Conference(TREC) Enterprise track for 2005 and 2006 show that the proposed topic-based approachoutperforms the state-of-the-art profile-and document-based models; which use …,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,2013,14
System P: Completeness-driven Query Answering in Peer Data Management Systems.,Armin Roth; Felix Naumann,*,BTW,2007,14
System P: Query answering in PDMS under limited resources,Armin Roth; Felix Naumann; Martin Schweigert,Abstract Peer data management systems (Pdms) consist of autonom-ous peers withmappings to other peers. Queries submitted at a peer are answered with data residing atthat peer and by data that is reached along paths of mappings through the network of peers.System P is a full-fledged relational Pdms featuring peers distributed over the network; rela-tional schemata and multiple local sources at each peer; LaV and GaV mappings betweenpeer schemata; fully localized query planning and execution. In addition; it serves as a testbed with automated deployment of schemata and data across the network and anindependent monitor peer to per-form scalability measurements. We have observed that thenumber of mappings and peers that must be traversed to obtain a complete query answer,In IIWeb,2006,14
Information Quality: How Good Are Off-The-Shelf DBMS?,Felix Naumann; Mary Roth,*,International Conference on Information Quality (ICIQ),2004,14
Efficient and exact computation of inclusion dependencies for data integration,Jana Bauckmann; Ulf Leser; Felix Naumann,Data obtained from foreign data sources often come with only superficial structuralinformation; such as relation names and attribute names. Other types of metadata that areimportant for effective integration and meaningful querying of such data sets are missing. Inparticular; relationships among attributes; such as foreign keys; are crucial metadata forunderstanding the structure of an unknown database. The discovery of such relationships isdifficult; because in principle for each pair of attributes in the database each pair of datavalues must be compared. A precondition for a foreign key is an inclusion dependency (IND)between the key and the foreign key attributes. We present with Spider an algorithm thatefficiently finds all INDs in a given relational database. It leverages the sorting facilities ofDBMS but performs the actual comparisons outside of the database to save computation …,*,2010,13
METL: Managing and Integrating ETL Processes.,Alexander Albrecht; Felix Naumann,ABSTRACT Companies use Extract-Transform-Load (Etl) tools to save time and costs whendeveloping and maintaining data migration tasks. Etl tools allow the definition of oftencomplex processes to extract; transform; and load heterogeneous data into a datawarehouse or to perform other data migration tasks. In larger organizations many Etlprocesses of different data integration and warehouse projects accumulate. Such processesencompass common sub-processes; shared data sources and targets; and same or similaroperations. However; there is no common method or approach to systematically managelarge collections of Etl processes. With Metl (Managing Etl) we present an Etl managementapproach that supports high-level Etl management. To this end we establish and implementa set of basic management operators on Etl processes; such as Match; Merge or Invert.,VLDB PhD workshop,2009,13
LODOP-Multi-Query Optimization for Linked Data Profiling Queries.,Benedikt Forchhammer; Anja Jentzsch; Felix Naumann,Abstract. The Web of Data contains a large number of different; openlyavailable datasets. Inorder to effectively integrate them into existing applications; meta information on statisticaland structural properties is needed. Examples include information about cardinalities; valuepatterns; or co-occurring properties. For Linked Datasets such information is currently verylimited or not available at all. Data profiling techniques are needed to compute respectivestatistics and meta information. However; current state of the art approaches can either notbe applied to Linked Data; or exhibit considerable performance problems. We presentLodop; a framework for computing; optimizing; and benchmarking data profiling techniquesbased on MapReduce with Apache Pig. We implemented 15 of the most important dataprofiling tasks; optimized their simultaneous execution; and evaluate them with four …,PROFILES@ ESWC,2014,11
Frequency-aware similarity measures: why Arnold Schwarzenegger is always a duplicate,Dustin Lange; Felix Naumann,Abstract Measuring the similarity of two records is a challenging problem; but necessary forfundamental tasks; such as duplicate detection and similarity search. By exploitingfrequencies of attribute values; many similarity measures can be improved: In a person tablewith US citizens; Arnold Schwarzenegger is a very rare name. If we find several ArnoldSchwarzeneggers in it; it is very likely that these are duplicates. We are then less strict whencomparing other attribute values; such as birth date or address. We put this intuition to useby partitioning compared record pairs according to frequencies of attribute values. Forexample; we could create three partitions from our data: Partition 1 contains all pairs withrare names; Partition 2 all pairs with medium frequent names; and Partition 3 all pairs withfrequent names. For each partition; we learn a different similarity measure: we apply …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,11
Clio: A schema mapping tool for information integration,Mauricio Hernandez; Howard Ho; Felix Naumann; Lucian Popa,The summary form only given. Information integration typically requires the construction ofcomplex artifacts like federated databases; ETL scripts; data warehouses; applications foraccessing multiple data sources; and applications that ingest or publish XML. For manycompanies; it is one of the most complicated IT tasks they face today. To reduce the overallcost; intelligent tools are needed to simplify this difficult task. Clio is a semi-automatic tool forschema mapping and data integration developed at IBM Almaden Research Center over thepast few years. It takes source and target schemas as input; which may describe relational orXML data models. Via a graphical Schema Viewer; a user can then interactively specifyattribute correspondences between the source and target schemas. An AttributeMatchercomponent helps suggest such correspondences; based on the similarity of both attribute …,Parallel Architectures; Algorithms and Networks; 2005. ISPAN 2005. Proceedings. 8th International Symposium on,2005,11
RDF ontology (re-) engineering through large-scale data mining,Johannes Lorey; Ziawasch Abedjan; Felix Naumann; Christoph Böhm,Abstract. As Linked Open Data originates from various sources; leveraging well-definedontologies aids integration. However; oftentimes the utilization of RDF vocabularies by datapublishers differs from the intended application envisioned by ontology engineers.Especially in largescale datasets as presented in the Billion Triple Challenge a significantdivergence between vocabulary specification and usage patterns can be observed. Thismay impede the goals of the Web of Data in terms of discovering domain-specificinformation in the Semantic Web. In this work; we identify common misusage patterns byemploying frequency analysis and rule mining and propose reengineering suggestions.,Semantic Web Challenge,2011,10
Data fusion and conflict resolution in integrated information systems.,Jens Bleiholder; F Naumann,Summary e development of the Internet in recent years has made it possible and useful toaccess many different information systems anywhere in the world to obtain information.While there is much research on the integration of heterogeneous information systems; moststops short of the actual integration of available data. Data fusion is the process ofcombining multiple records representing the same real-world object into a single; consistent;and clean representation. e problem that is considered when fusing data is the problem ofhandling data conflicts (uncertainties and contradictions) that may exist among the differentrepresentations. In preliminary steps; duplicate detection techniques help in identifyingdifferent representations of same real-world objects and schema mappings are used toidentify common representations if data originates in different sources. is thesis first …,*,2010,10
Rule-Based Measurement Of Data Quality In Nominal Data.,Jochen Hipp; Markus Müller; Johannes Hohendorff; Felix Naumann,Abstract: Sufficiently high data quality is crucial for almost every application. Nonetheless;data quality issues are nearly omnipresent. The reasons for poor quality cannot simply beblamed on software issues or insufficiently implemented business processes. Based on ourexperiences the main reason is that data quality shows the strong tendency to convergedown to a level that is inherent to the existing applications. As soon as applications and dataare used for other than the established tasks they were originally designed for; problemsarise. In this paper we extend and evaluate an approach to measure the accuracydimension of data quality based on association rules. The rules are used to build a modelthat is intended to capture normality. Then; this model is employed to divide the databaserecords into three subsets:“potentially incorrect”;“no decision”; and “probably correct”. We …,ICIQ,2007,10
Maximizing coverage of mediated web queries,Ramana Yerneni; Felix Naumann; Hector Garcia-Molina,Abstract Over the Web; mediators are built on large collections of sources to provideintegrated access to Web content (eg; meta-search engines). In order to minimize theexpense of visiting a large number of sources; mediators need to choose a subset ofsources to contact when processing queries. As fewer sources participate in processing amediated query; the coverage of the query goes down. In this paper; we study this trade-oand develop techniques for mediators to maximize the coverage for their queries while at thesame time visiting a subset of their sources. We formalize the problem; study its complexity;propose algorithms to solve it; and analyze the theoretical performance guarantees of thealgorithms. We also study the performance of our algorithms through simulationexperiments.,Techn. Ber.; Stanford University; CA,2000,10
Estimating Data Integration and Cleaning Effort.,Sebastian Kruse; Paolo Papotti; Felix Naumann,ABSTRACT Data cleaning and data integration have been the topic of intensive research forat least the past thirty years; resulting in a multitude of specialized methods and integratedtool suites. All of them require at least some and in most cases significant human input intheir configuration; during processing; and for evaluation. For managers (and for developersand scientists) it would be therefore of great value to be able to estimate the effort ofcleaning and integrating some given data sets and to know the pitfalls of such an integrationproject in advance. This helps deciding about an integration project using cost/benefitanalysis; budgeting a team with funds and manpower; and monitoring its progress. Further;knowledge of how well a data source fits into a given data ecosystem improves sourceselection. We present an extensible framework for the automatic effort estimation for …,EDBT,2015,9
Duplicate detection on GPUs,Benedikt Forchhammer; Thorsten Papenbrock; Thomas Stening; Sven Viehmeier; Uwe Draisbach; Felix Naumann,Abstract Duplicate detection is an integralpart ofdata cleansing. In this project we developeda complete system to detect duplicates in very large datasets; using the capabilities ofmodern graphics processing units (GPUs). Our solution covers several algorithms for thetasks of pair selection; similarity-comparison of attribute values; aggregation of pairs; andclustering. We describe how each algorithm can be designed to run memory ejﬁcient andparallel on the GPU. Thereby; we exploit the increasing capabilities ofmodern graphicscards with their many cores. Our similarity-comparisons are based on strings with variablelengths. This is a dijﬁcult taskfor GPUs; because they cannot handle variable sized datastructures and lose synchronism; but in return the duplicate detection process achieveshigher precision values. Experiments demonstrate that our solution outperforms an …,HPI Future SOC Lab: proceedings 2011,2013,9
ECIR-A Lightweight Approach for Entity-Centric Information Retrieval.,Alexander Hold; Michael Leben; Benjamin Emde; Christoph Thiele; Felix Naumann; Wojciech M Barczynski; Falk Brauer,Abstract—This paper describes our system developed for the TREC 2010 Entity track. Inparticular we study the exploitation of advanced features of different Web search engines toachieve high quality answers for the 'related entity finding'-task. Our system preprocesses auser query using part-ofspeech tagging and synonym dictionaries; and generates anenriched keyword query employing advanced features of particular Web search engines.After retrieving a corpus of documents; the system constructs rules to extract candidateentities. Potentially related entities are deduplicated and scored for each document withrespect to the distance to the source entity that is defined in the query. Finally; these scoresare aggregated across the corpus by incorporating the rank position of a document. Forhomepage retrieval we further employ advanced features of Web search engines; for …,TREC,2010,9
Information integration and disaster data management (disdm),Felix Naumann; Louiqa Raschid,Recent disasters such as the 2003 SARS outbreak; the 2004 Asian tsunami; the 2005Kashmir/Pakistan earthquake and 2005 hurricanes Katrina and Rita clearly identified theshortcomings of IT solutions for disaster rescue and recovery. Of particular note was theaccompanying disaster of the lack of viable or deployed data management solutions;specifically information integration and information sharing solutions. In this position paper;we identify some specific challenges to information integration and sharing that is driven bythe requirements of disaster data management (DisDM). We then identify a brief summary ofneeded technologies and methods for DisDM. DisDM Challenges All major disasters affectthe lives and the welfare of many individuals; in many other aspects they differ significantly.These differences include the demographics of the affected people and the affected area …,University of Maryland; Tech. Rep,2006,9
Information quality: Fundamentals; techniques; and use,Felix Naumann; Kai-Uwe Sattler,Page 1. Information Quality: Fundamentals; Techniques; and Use Felix NaumannHumboldt-Universität zu Berlin Kai-Uwe Sattler TU Ilmenau EDBT Tutorial; Munich; March 282006 Page 2. March 28; 2006 Felix Naumann; Kai-Uwe Sattler 2 Our Personal Motivation ●Now: Motivation ● IQ is big business ● IQ is (also) a database topic ● This tutorial: The past ●Where we are now ● The future: Open Problems ● Much to do 1.5 hours ⇒ no details Page3. March 28; 2006 Felix Naumann; Kai-Uwe Sattler 3 Tutorial Overview ● Motivation ● DefiningIQ ● IQ Dimensions ● IQ Models ● IQ Assessment ● Assessment techniques ● IQ aggregationand ranking ● IQ Improvement ● Profiling & Data Scrubbing ● Outlier Detection ● DuplicateDetection ● Wrapup Page 4. Information Quality: Fundamentals; Techniques; and Use Part1: Motivation Felix Naumann Humboldt-Universität zu Berlin …,Tutorial at EDBT,2006,9
Cooperative query answering with density scores,Felix Naumann; Ulf Leser,Abstract Mediator-based information systems answer global queries by rewriting them into acombination of queries against physical data sources. One assumption in most systems isthat only such combinations are considered as valid that obtain values for each selectedattribute of the query. Another assumption is that systems must compute and execute allvalid combinations; ie; they strive to retrieve all possible answers. These assumptionsfrequently lead to user frustration: First; in many scenarios an incomplete answer is muchmore appreciated than no answer at all. Second; if many valid combinations exist; it is verytime-consuming to execute them all. We present a cooperative query planning method thatavoids both problems. First; it treats incomplete and complete source combinations in alogically equivalent manner; ie; incomplete answers are also considered. Second; it ranks …,*,2000,9
Quality-driven Query Planning.,Felix Naumann,*,EDBT PhD Workshop,2000,9
Which answer is best?: Predicting accepted answers in mooc forums,Maximilian Jenders; Ralf Krestel; Felix Naumann,Abstract Massive Open Online Courses (MOOCs) have grown in reach and importance overthe last few years; enabling a vast userbase to enroll in online courses. Besides watchingvideos; user participate in discussion forums to further their understanding of the coursematerial. As in other community-based question-answering communities; in many MOOCforums a user posting a question can mark the answer they are most satisfied with. In thispaper; we present a machine learning model that predicts this accepted answer to a forumquestion using historical forum data.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,8
CohEEL: Coherent and efficient named entity linking through random walks,Toni Gruetze; Gjergji Kasneci; Zhe Zuo; Felix Naumann,Abstract In recent years; the ever-growing amount of documents on the Web as well as indigital libraries led to a considerable increase of valuable textual information about entities.Harvesting entity knowledge from these large text collections is a major challenge. Itrequires the linkage of textual mentions within the documents with their real-world entities.This process is called entity linking. Solutions to this entity linking problem have typicallyaimed at balancing the rate of linking correctness (precision) and the linking coverage rate(recall). While entity links in texts could be used to improve various Information Retrievaltasks; such as text summarization; document classification; or topic-based clustering; thelinking precision is the decisive factor. For example; for topic-based clustering a method thatproduces mostly correct links would be more desirable than a high-coverage method that …,Web Semantics: Science; Services and Agents on the World Wide Web,2016,8
Estimating the number and sizes of fuzzy-duplicate clusters,Arvid Heise; Gjergji Kasneci; Felix Naumann,Abstract Duplicates in a dataset are multiple representations of the same real-world entityand constitute a major data quality problem. This paper investigates the problem ofestimating the number and sizes of duplicate record clusters in advance and describes asampling-based method for solving this problem. In extensive experiments; on multipledatasets; we show that the proposed method reliably estimates the number of duplicateclusters; while being highly efficient.,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,8
Exploring Life Sciences Data Sources Zoée Lacroix Arizona State University zoe. lacroix@ asu. edu,Louiqa Raschid; Maria Esther Vidal,1 Introduction There has been an explosion of the data that is available to the biomolecularresearcher. A recent estimate suggests up to 600 public data sources! While this explosionpresents an opportunity; it is accompanied by difficulties in harnessing and exploring thisdata. An average research group can (simultaneously) utilize up to 40 databases many ofwhich are publicly available on the Web. Public life science data sources represent acomplex link-driven federation of sources. A fundamental problem facing the researchertoday is correctly identifying a specific instance of a biological entity; eg; a specific gene orprotein; and then obtaining a complete functional characterization of this entity instance byexploring a multiplicity of inter-related and inter-linked sources. An example question thatcould be answered by such a correct and complete characterization is as follows: What …,*,2003,8
Scaling out the discovery of inclusion dependencies,Sebastian Kruse; Thorsten Papenbrock; Felix Naumann,Inclusion dependencies are among the most important database dependencies. In additionto their most prominent application-foreign key discovery-inclusion dependencies are animportant input to data integration; query optimization; and schema redesign. With theirdiscovery being a recurring data profiling task; previous research has proposed differentalgorithms to discover all inclusion dependencies within a given dataset. However; none ofthe proposed algorithms is designed to scale out; ie; none can be distributed across multiplenodes in a computer cluster to increase the performance. So on large datasets with manyinclusion dependencies; these algorithms can take days to complete; even on high-performance computers. We introduce SINDY; an algorithm that efficiently discovers allunary inclusion dependencies of a given relational dataset in a distributed fashion and …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,7
Black swan: augmenting statistics with event data,Johannes Lorey; Felix Naumann; Benedikt Forchhammer; Andrina Mascher; Peter Retzlaff; Armin ZamaniFarahani; Soeren Discher; Cindy Faehnrich; Stefan Lemme; Thorsten Papenbrock; Robert Christoph Peschel; Stephan Richter; Thomas Stening; Sven Viehmeier,Abstract A large number of statistical indicators (GDP; life expectancy; income; etc.) collectedover long periods of time as well as data on historical events (wars; earthquakes; elections;etc.) are published on the World Wide Web. By augmenting statistical outliers with relevanthistorical occurrences; we provide a means to observe (and predict) the influence andimpact of events. The vast amount and size of available data sets enable the detection ofrecurring connections between classes of events and statistical outliers with the help ofassociation rule mining. The results of this analysis are published at http://www.blackswanevents. org and can be explored interactively.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,7
Complement union for data integration,Jens Bleiholder; Sascha Szott; Melanie Herschel; Felix Naumann,A data integration process consists of mapping source data into a target representation(schema mapping); identifying multiple representations of the same real-word object(duplicate detection); and finally combining these representations into a single consistentrepresentation (data fusion). Clearly; as multiple representations of an object are generallynot exactly equal; during data fusion; we have to take special care in handling data conflicts.This paper focuses on the definition and implementation of complement union; an operatorthat defines a new semantics for data fusion.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,7
A Data Model and Query Language to Explore Enhanced Links and Paths in Life Science Sources.,George A Mihaila; Felix Naumann; Louiqa Raschid; Maria-Esther Vidal,ABSTRACT Links in life science sources capture important domain knowledge. However;current simple physical link implementations are not rich in either representation orsemantics. This paper proposes the e-link framework and tools to assist scientists inexploring and exploiting the knowledge that should be captured in links. 1. INTRODUCTIONAn abundance of Web-accessible bio-molecular data sources contain data about scientificentities; such as genes; sequences; proteins and citations. The sources have varyingdegrees of overlap in their content and they are richly interconnected to each other.Experiment protocols to retrieve relevant data objects (data integration queries) exploremultiple sources and traverse the links and the paths (informally concatenations of links)through these sources. While such navigational queries are critical to scientific …,WebDB,2005,7
Data Anamnesis: Admitting Raw Data into an Organization.,Sebastian Kruse; Thorsten Papenbrock; Hazar Harmouch; Felix Naumann,Abstract Today's internet offers a plethora of openly available datasets; bearing greatpotential for novel applications and research. Likewise; rich datasets slumber withinorganizations. However; all too often those datasets are available only as raw dumps andlack proper documentation or even a schema. Data anamnesis is the first step of any effort towork with such datasets: It determines fundamental properties regarding the datasets'content; structure; and quality to assess their utility and to put them to use appropriately.Detecting such properties is a key concern of the research area of data profiling; which hasdeveloped several viable instruments; such as data type recognition and foreign keydiscovery. In this article; we perform an anamnesis of the MusicBrainz dataset; an openlyavailable and complex discographic database. In particular; we employ data profiling …,IEEE Data Eng. Bull.,2016,6
Bootstrapping Wikipedia to answer ambiguous person name queries,Toni Gruetze; Gjergji Kasneci; Zhe Zuo; Felix Naumann,Some of the main ranking features of today's search engines reflect result popularity and arebased on ranking models; such as PageRank; implicit feedback aggregation; and more.While such features yield satisfactory results for a wide range of queries; they aggravate theproblem of search for ambiguous entities: Searching for a person yields satisfactory resultsonly if the person in question is represented by a high-ranked Web page and all requiredinformation are contained in this page. Otherwise; the user has to either reformulate/refinethe query or manually inspect low-ranked results to find the person in question. A possibleapproach to solve this problem is to cluster the results; so that each cluster represents one ofthe persons occurring in the answer set. However clustering search results has proven to bea difficult endeavor by itself; where the clusters are typically of moderate quality. A wealth …,Data Engineering Workshops (ICDEW); 2014 IEEE 30th International Conference on,2014,6
Detecting unique column combinations on dynamic data,Ziawasch Abedjan; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,The discovery of all unique (and non-unique) column combinations in an unknown datasetis at the core of any data profiling effort. Unique column combinations resemble candidatekeys of a relational dataset. Several research approaches have focused on their efficientdiscovery in a given; static dataset. However; none of these approaches are suitable forapplications on dynamic datasets; such as transactional databases; social networks; andscientific applications. In these cases; data profiling techniques should be able to efficientlydiscover new uniques and non-uniques (and validate old ones) after tuple inserts or deletes;without re-profiling the entire dataset. We present the first approach to efficiently discoverunique and non-unique constraints on dynamic datasets that is independent of the initialdataset size. In particular; Swan makes use of intelligently chosen indices to minimize …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,6
Automatic blocking key selection for duplicate detection based on unigram combinations,Tobias Vogel; Felix Naumann,ABSTRACT Duplicate detection is the process of identifying multiple but differentrepresentations of same real-world objects; which typically involves a large number ofcomparisons. Partitioning is a well-known technique to avoid many unnecessarycomparisons. However; partitioning keys are usually handcrafted; which is tedious and thekeys are often poorly chosen. We propose a technique to find suitable blocking keysautomatically for a dataset equipped with a gold standard. We then show how to re-usethose blocking keys for datasets from similar domains lacking a gold standard. Blocking keysare created based on unigrams; which we extend with length-hints for further improvement.Blocking key creation is accompanied with several comprehensive experiments on largeartificial and real-world datasets.,Proceedings of the International Workshop on Quality in Databases (QDB),2012,6
Instance-based ‘one-to-some’Assignment of Similarity Measures to Attributes,Tobias Vogel; Felix Naumann,Abstract Data quality is a key factor for economical success. It is usually defined as a set ofproperties of data; such as completeness; accessibility; relevance; and conciseness. Thelatter includes the absence of multiple representations for same real world objects. To avoidsuch duplicates; there is a wide range of commercial products and customized self-codedsoftware. These programs can be quite expensive both in acquisition and maintenance. Inparticular; small and medium-sized companies cannot afford these tools. Moreover; it isdifficult to set up and tune all necessary parameters in these programs. Recently; web-basedapplications for duplicate detection have emerged. However; they are not easy to integrateinto the local IT landscape and require much manual configuration effort. With DAQS (DataQuality as a Service) we present a novel approach to support duplicate detection. The …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2011,6
Dynamic tags for dynamic data web services,Mohammed AbuJarour; Felix Naumann,Abstract Service descriptions play a crucial role in Service-oriented Computing (SOC);especially with respect to service discovery and selection; service composition; and servicereplacement. Service providers have been the primary source of these descriptions; such asWSDL; WADL; WSMO. However; it has been observed that service providers tend to releasepoor service descriptions about their offered web services; because they focus on theimplementation aspects of their web services. However; efficient service discovery requiresrich service descriptions. Several approaches have been proposed to enrich servicedescriptions using human-generated or automatically-extracted annotations; such as tags.For data web services; eg; list of countries; recent news; new publications; etc.; such usefulannotations can be also generated through service invocation analysis. In this work; we …,Proceedings of the 5th International Workshop on Enhanced Web Service Technologies,2010,6
Emergent Data Quality Annotation And Visualization.,Paul Führing; Felix Naumann,Abstract: The systematic assessment; storage; and retrieval of data quality scores hasproven to be an elusive problem; often tackled only with classifications; questionnaires; andmodels. We present a concrete solution for the graphical annotation of data with qualityscores; to enable their efficient storage and retrieval; and ultimately their graphical displayon top of the actual data. Our tool; VIQTOR; enables users to assign quality scores usingsimple point and click techniques in a natural data display environment; such asspreadsheets. The particular challenges we tackle are the support of multiple usersassigning different quality scores; the flexible assignment of quality scores to any subset ofthe data (rows and columns); the assignment and storage of scores in multiple qualitycriteria; and the graphical display of those scores aggregated both across users and …,ICIQ,2007,6
Relationship-based duplicate detection,Melanie Weis; Felix Naumann,Recent work both in the relational and the XML world have shown that the efficacy andefficiency of duplicate detection is enhanced by regarding relationships between ancestorsand descendants. We present a novel comparison strategy that uses relationships butdisposes of the strict bottom-up and topdown approaches proposed for hierarchical data.Instead; pairs of objects at any level of the hierarchy are compared in an order that dependson their relationships: Objects with many dependants influence many other duplicity-decisions and thus it should be decided early if they are duplicates themselves. We applythis ordering strategy to two algorithms. RECONA allows to re-examine an object if itsinfluencing neighbors turn out to be duplicates. Here ordering reduces the number of suchre-comparisons. ADAMA is more efficient by not allowing any re-comparison. Here the …,*,2006,6
RDfind: scalable conditional inclusion dependency discovery in RDF datasets,Sebastian Kruse; Anja Jentzsch; Thorsten Papenbrock; Zoi Kaoudi; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,Abstract Inclusion dependencies (INDs) form an important integrity constraint on relationaldatabases; supporting data management tasks; such as join path discovery and queryoptimization. Conditional inclusion dependencies (CINDs); which define including andincluded data in terms of conditions; allow to transfer these capabilities to RDF data.However; CIND discovery is computationally much more complex than IND discovery andthe number of CINDs even on small RDF datasets is intractable. To cope with bothproblems; we first introduce the notion of pertinent CINDs with an adjustable relevancecriterion to filter and rank CINDs based on their extent and implications among each other.Second; we present RDFind; a distributed system to efficiently discover all pertinent CINDsin RDF data. RDFind employs a lazy pruning strategy to drastically reduce the CIND …,Proceedings of the 2016 International Conference on Management of Data,2016,5
Efficient order dependency detection,Philipp Langer; Felix Naumann,Abstract Order dependencies (ODs) describe a relationship of order between lists ofattributes in a relational table. ODs can help to understand the semantics of datasets and theapplications producing them. They have applications in the field of query optimization bysuggesting query rewrites. Also; the existence of an OD in a table can provide hints on whichintegrity constraints are valid for the domain of the data at hand. This work is the first todescribe the discovery problem for order dependencies in a principled manner bycharacterizing the search space; developing and proving pruning rules; and presenting thealgorithm Order; which finds all order dependencies in a given table. Order traverses thelattice of permutations of attributes in a level-wise bottom-up manner. In a comprehensiveevaluation; we show that it is efficient even for various large datasets.,The VLDB Journal,2016,5
Data Profiling - Tutorial,Ziawasch Abedjan; Lukasz Golab; Felix Naumann,Abstract is to understand the dataset at hand and its metadata. The process of metadatadiscovery is known as data profiling. Profiling activities range from ad-hoc approaches; suchas eye-balling random subsets of the data or formulating aggregation queries; to systematicinference of structural information and statistics of a dataset using dedicated profiling tools.In this tutorial; we highlight the importance of data profiling as part of any data-related use-case; and we discuss the area of data profiling by classifying data profiling tasks andreviewing the state-of-the-art data profiling systems and techniques. In particular; we discusshard problems in data profiling; such as algorithms for dependency discovery and profilingalgorithms for dynamic data and streams. We also pay special attention to visualizing andinterpreting the results of data profiling. We conclude with directions for future research in …,International Conference on Data Engineering (ICDE),2016,5
A serendipity model for news recommendation,Maximilian Jenders; T Lindhauer; Gjergji Kasneci; Ralf Krestel; Felix Naumann,Abstract Recommendation algorithms typically work by suggesting items that are similar tothe ones that a user likes; or items that similar users like. We propose a content-basedrecommendation technique with the focus on serendipity of news recommendations.Serendipitous recommendations have the characteristic of being unexpected yet fortunateand interesting to the user; and thus might yield higher user satisfaction. In our work; weexplore the concept of serendipity in the area of news articles and propose a generalframework that incorporates the benefits of serendipity-and similarity-basedrecommendation techniques. An evaluation against other baseline recommendation modelsis carried out in a user study.,Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz),2015,5
Reach for gold: An annealing standard to evaluate duplicate detection results,Tobias Vogel; Arvid Heise; Uwe Draisbach; Dustin Lange; Felix Naumann,Abstract Duplicates in a database are one of the prime causes of poor data quality and areat the same time among the most difficult data quality problems to alleviate. To detect andremove such duplicates; many commercial and academic products and methods have beendeveloped. The evaluation of such systems is usually in need of pre-classified results. Suchgold standards are often expensive to come by (much manual classification is necessary);not representative (too small or too synthetic); and proprietary and thus preclude repetition(company-internal data). This lament has been uttered in many papers and even morepaper reviews. The proposed annealing standard is a structured set of duplicate detectionresults; some of which are manually verified and some of which are merely validated bymany classifiers. As more and more classifiers are evaluated against the annealing …,Journal of Data and Information Quality (JDIQ),2014,5
Amending RDF entities with new facts,Ziawasch Abedjan; Felix Naumann,Abstract Linked and other Open Data poses new challenges and opportunities for the datamining community. Unfortunately; the large volume and great heterogeneity of availableopen data requires significant integration steps before it can be used in applications. Apromising technique to explore such data is the use of association rule mining. We introducetwo algorithms for enriching Rdf data. The first application is a suggestion engine that isbased on mining Rdf predicates and supports manual statement creation by suggesting newpredicates for a given entity. The second application is knowledge creation: Based onmining both predicates and objects; we are able to generate entirely new statements for agiven data set without any external resources.,European Semantic Web Conference,2014,5
Cost-aware query planning for similarity search,Dustin Lange; Felix Naumann,Abstract Similarity search aims to find all objects similar to a query object. Typically; somebase similarity measures for the different properties of the objects are defined; and light-weight similarity indexes for these measures are built. A query plan specifies which similarityindexes to use with which similarity thresholds and how to combine the results. Previouswork creates only a single; static query plan to be used by all queries. In contrast; ourapproach creates a new plan for each query. We introduce the novel problem of queryplanning for similarity search; ie; selecting for each query the plan that maximizescompleteness of the results with cost below a query-specific limit. By regarding thefrequencies of attribute values we are able to better estimate plan completeness and cost;and thus to better distribute our similarity comparisons. Evaluation on a large real-world …,Information Systems,2013,5
On Choosing Thresholds for Duplicate Detection,Uwe Draisbach; Felix Naumann,Abstract: Duplicate detection; ie; the discovery of records that refer to the same real-worldentity; is a task that usually depends on multiple input parameters by an expert. Mostnotably; an expert must specify some similarity measure and some threshold that declaresduplicity for record pairs if their similarity surpasses it. Both are typically developed in a trial-and-error based manner with a given (sample) dataset. We posit that the similarity measurelargely depends on the nature of the data and its contained errors that cause the duplicates;but that the threshold largely depends on the size of the dataset it was tested on. Inconsequence; configurations of duplicate detection runs work well on the test dataset; butperform worse if the size of the dataset changes. This weakness is due to the transitivenature of duplicity: In larger datasets transitivity can cause more records to enter a …,Proceedings of the 18th International Conference on Information Quality (ICIQ),2013,5
Schema decryption for large extract-transform-load systems,Alexander Albrecht; Felix Naumann,Abstract Extract-Transform-Load (Etl) tools are used for the creation; maintenance; andevolution of data warehouses; data marts; and operational data stores. Etl workflowspopulate those systems with data from various data sources by specifying and executing aDAG of transformations. Over time; hundreds of individual workflows evolve as new sourcesand new requirements are integrated into the system. The maintenance and evolution oflarge-scale Etl systems requires much time and manual effort. A key problem is tounderstand the meaning of unfamiliar attribute labels in source and target databases and Etltransformations. Hard-to-read attribute labels in schemata lead to frustration and time spentto develop and understand Etl workflows. We present a schema decryption technique tosupport Etl developers in understanding cryptic schemata of sources; targets; and Etl …,International Conference on Conceptual Modeling,2012,5
Eliminating NULLs with Subsumption and Complementation.,Jens Bleiholder; Melanie Herschel; Felix Naumann,Abstract In a data integration process; an important step after schema matching andduplicate detection is data fusion. It is concerned with the combination or merging of differentrepresentations of one real-world object into a single; consistent representation. In order tosolve potential data conflicts; many different conflict resolution strategies can be applied. Inparticular; some representations might contain missing values (NULL-values) where othersprovide a non-NULL-value. A common strategy to handle such NULL-values; is to replacethem with the existing values from other representations. Thus; the conciseness of therepresentation is increased without losing information. Two examples for relationaloperators that implement such a strategy are minimum union and complement union andtheir unary building blocks subsumption and complementation. In this paper; we define …,IEEE Data Eng. Bull.,2011,5
Information integration in service-oriented computing,Mohammed AbuJarour; Felix Naumann,Abstract—Information Integration has been the typical approach to data-driven applicationsin several domains; such as; enterprise applications. However; applying informationintegration techniques in Service-oriented Computing (SOC) is not straightforward; becauseof the lack of adequate information resources; such as rich service descriptions. This lack ofrich service descriptions reduces the (re) usability of web services. In this work; we proposea novel approach and platform to alleviate this problem and investigate the benefits ofinformation integration in SOC; where information about web services is gathered frommultiple sources; eg; service providers; consumers; invocations; etc.; and integrated in richuniversal service descriptions.,University Halle-Wittenberg Institute of Computer Science,2010,5
Towards granular data placement strategies for cloud platforms,Johannes Lorey; Felix Naumann,Traditional data placement strategies in the context of Information Lifecycle Management(ILM) are applicable only to on-site storage systems. In contrast to this approach; Cloudstorage provides a novel possibility to reduce or entirely eliminate capital expenditures forhardware. As a unique solution to buffer short-term resource demand peaks; Cloudinfrastructures can be combined with on-site systems to support efficient placement of data.The algorithms underlying this optimization must consider not only the workload as a whole;but rather variable-sized sub workloads to determine an optimal placement. As a means toidentify these sub workloads; we introduce a multi-dimensional granularization approach.Based on different granules of metadata information; we propose a flexible hybrid dataplacement system incorporating both on-site and Cloud resources.,Granular Computing (GrC); 2010 IEEE International Conference on,2010,5
Space and time scalability of duplicate detection in graph data,Melanie Weis; Felix Naumann,Abstract The task of duplicate detection consists in determining different representations of asame real-world object in a database; and that for every object in the database. Recentresearch has considered to use relationships among object representations to improveduplicate detection. In the general case where relationships form a graph; research hasmainly focused on duplicate detection effectiveness. Scalability has been neglected so far;even though it is crucial for large real-world duplicate detection tasks to scale up. In thisreport; we present how duplicate detection in graph data scales up to large amounts of dataand pairwise comparisons; using the support of a relational database system. To this end;we generalize the process of duplicate detection in graphs (DDG). We then define twomethods to scale algorithms for DDG in space (amount of data processed with limited …,Technical report,2007,5
Topic shifts in StackOverflow: ask it like Socrates,Toni Gruetze; Ralf Krestel; Felix Naumann,Abstract Community based question-and-answer (Q&A) sites rely on well-posed andappropriately tagged questions. However; most platforms have only limited capabilities tosupport their users in finding the right tags. In this paper; we propose a temporalrecommendation model to support users in tagging new questions and thus improve theiracceptance in the community. To underline the necessity of temporal awareness of such amodel; we first investigate the changes in tag usage and show different types of collectiveattention in StackOverflow; a community-driven Q&A website for computer programmingtopics. Furthermore; we examine the changes over time in the correlation between questionterms and topics. Our results show that temporal awareness is indeed important forrecommending tags in Q&A communities.,International Conference on Applications of Natural Language to Information Systems,2016,4
SOFA: An Extensible Logical Optimizer for UDF-heavy Dataflows,Astrid Rheinländer; Arvid Heise; Fabian Hueske; Ulf Leser; Felix Naumann,Abstract: Recent years have seen an increased interest in large-scale analytical dataflowson non-relational data. These dataflows are compiled into execution graphs scheduled onlarge compute clusters. In many novel application areas the predominant building blocks ofsuch dataflows are user-defined predicates or functions (UDFs). However; the heavy use ofUDFs is not well taken into account for dataflow optimization in current systems. SOFA is anovel and extensible optimizer for UDF-heavy dataflows. It builds on a concise set ofproperties for describing the semantics of Map/Reduce-style UDFs and a small set of rewriterules; which use these properties to find a much larger number of semantically equivalentplan rewrites than possible with traditional techniques. A salient feature of our approach isextensibility: We arrange user-defined operators and their properties into a subsumption …,arXiv preprint arXiv:1311.6335,2013,4
Similarity measures,Felix Naumann,Page 1. Similarity measures 11.6.2013 Felix Naumann Page 2. Duplicate Detection –Research Duplicate Detection Algorithm Similarity measure Identity Domain- independentIncremental/ Search Relational DWH XML Domain- dependent Filters Edit-based Rules Datatypes Evaluation Clustering / Learning Partitioning Relation -ships Precision/ Recall EfficiencyRelationship-aware Token-based 2 Felix Naumann | Data Profiling and Data Cleansing |Summer 2013 Page 3. Overview Similarity Measures 3 Similarity Measures Edit-basedToken-based Phonetic Hybrid Domain- dependent Dates Rules Soundex Kölner PhonetikSoft TF-IDF Monge-Elkan Words / n-grams Jaccard Dice Damerau- Levenshtein LevenshteinJaro Jaro-Winkler Smith- Waterman Metaphone Double Metaphone Smith- Waterman-GotohHamming Cosine Similarity Numerical attributes …,Information Systems,2013,4
Towards a diamond SOA operational model,Mohammed AbuJarour; Felix Naumann,The triangular operational model with the three roles of service-registry;-provider; and-consumer has been the traditional operational model in Service-oriented Architectures(SOA). The central component in this model; the service registry; plays a passive role. Thispassive role is due to the lack of adequate information resources. For example; servicedescriptions provided by service providers are in general not rich enough for servicediscovery and selection. More elaborate service descriptions are vital in several aspects inService-oriented Computing (SOC); such as service quality assessment; service discoveryand selection; etc. To increase the usability of web services with poor descriptions; wepropose a novel approach to extend the traditional SOA operational model by introducing aDiamond model with a new role: the Service Invocation Proxy (SIP). The main goal of this …,Service-Oriented Computing and Applications (SOCA); 2010 IEEE International Conference on,2010,4
Detecting inclusion dependencies on very many tables,Fabian Tschirschnitz; Thorsten Papenbrock; Felix Naumann,Abstract Detecting inclusion dependencies; the prerequisite of foreign keys; in relationaldata is a challenging task. Detecting them among the hundreds of thousands or evenmillions of tables on the web is daunting. Still; such inclusion dependencies can helpconnect disparate pieces of information on the Web and reveal unknown relationshipsamong tables. With the algorithm M any; we present a novel inclusion dependency detectionalgorithm; specialized for the very many—but typically small—tables found on the Web. Wemake use of Bloom filters and indexed bit-vectors to show the feasibility of our approach. Ourevaluation on two corpora of Web tables shows a superior runtime over known approachesand its usefulness to reveal hidden structures on the Web.,ACM Transactions on Database Systems (TODS),2017,3
A hybrid approach for efficient unique column combination discovery,Thorsten Papenbrock; Felix Naumann,Unique column combinations (UCCs) are groups of attributes in relational datasets thatcontain no value-entry more than once. Hence; they indicate keys and serve datamanagement tasks; such as schema normalization; data integration; and data cleansing.Because the unique column combinations of a particular dataset are usually unknown; UCCdiscovery algorithms have been proposed to find them. All previous such discoveryalgorithms are; however; inapplicable to datasets of typical real-world size; eg; datasets withmore than 50 attributes and a million records. We present the hybrid discovery algorithm HUCC; which uses the same discovery techniques as the recently proposed functionaldependency discovery algorithm H FD: A hybrid combination of fast approximationtechniques and e cient validation techniques. With it; the algorithm discovers all minimal …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,3
Data-driven Schema Normalization,Thorsten Papenbrock; Felix Naumann,ABSTRACT Ensuring Boyce-Codd Normal Form (BCNF) is the most popular way to removeredundancy and anomalies from datasets. Normalization to BCNF forces functionaldependencies (FDs) into keys and foreign keys; which eliminates duplicate values andmakes data constraints explicit. Despite being well researched in theory; converting theschema of an existing dataset into BCNF is still a complex; manual task; especially becausethe number of functional dependencies is huge and deriving keys and foreign keys is NP-hard. In this paper; we present a novel normalization algorithm called Normalize; which usesdiscovered functional dependencies to normalize relational datasets into BCNF. Normalizeruns entirely data-driven; which means that redundancy is removed only where it can beobserved; and it is (semi-) automatic; which means that a user may or may not interfere …,International Conference on Extending Database Technology (EDBT),2017,3
Approximate Discovery of Functional Dependencies for Large Datasets,Tobias Bleifuß; Susanne Bülow; Johannes Frohnhofen; Julian Risch; Georg Wiese; Sebastian Kruse; Thorsten Papenbrock; Felix Naumann,Abstract Functional dependencies (FDs) are an important prerequisite for various datamanagement tasks; such as schema normalization; query optimization; and data cleansing.However; automatic FD discovery entails an exponentially growing search and solutionspace; so that even today's fastest FD discovery algorithms are limited to small datasetsonly; due to long runtimes and high memory consumptions. To overcome this situation; wepropose an approximate discovery strategy that sacrifices possibly little result correctness inreturn for large performance improvements. In particular; we introduce AID-FD; an algorithmthat approximately discovers FDs within runtimes up to orders of magnitude faster than state-of-the-art FD discovery algorithms. We evaluate and compare our performance results with afocus on scalability in runtime and memory; and with measures for completeness …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,3
Ein datenbankkurs mit 6000 teilnehmern,Felix Naumann; Maximilian Jenders; Thorsten Papenbrock,Zusammenfassung Im Sommersemester 2013 boten wir auf openHPI; der Internet-Bildungsplattform des Hasso-Plattner-Instituts; den Kurs Datenmanagement mit SQL an.Von den über 6000 Teilnehmern erhielten nach sieben Wochen 1641 Teilnehmer einZertifikat und 2074 eine Teilnahmebestätigung. Der Kurs folgte der üblichen Struktur einerDatenbankeinführung und umfasste die Grundlagen der ER-Modellierung; des relationalenEntwurfs und der relationalen Algebra sowie eine ausführliche Einführung in SQL. DerVorlesungsinhalt wurde in kleine Videoeinheiten aufgebrochen; die jeweils mit kleinenSelbsttests abgeschlossen wurden. Begleitend zu jedem Themenblock mussten dieTeilnehmer online Hausaufgaben lösen und zum Abschluss des Kurses eine Klausurbearbeiten. Wir berichten über unsere Erfahrungen bei der Durchführung dieses ersten …,Informatik-Spektrum,2014,3
Sprint: ranking search results by paths,Christoph Böhm; Eyk Kny; Benjamin Emde; Ziawasch Abedjan; Felix Naumann,Abstract Graph-structured data abounds and has become the subject of much attention inthe past years; for instance when searching and analyzing social network structures.Measures such as the shortest path or the number of paths between two nodes are used asproxies for similarity or relevance [1]. These approaches benefit from the fact that themeasures are determined from some context node; eg;" me" in a social network. With Sprint;we apply these notions to a new domain; namely ranking web search results using the link-path-structure among pages. Sprint demonstrates the feasibility and effectiveness of< u>S</u> earching by< u> P</u> ath< u> R</u> anks on the< u> INT</u> ernet with two usecases: First; we re-rank intranet search results based on the position of the user's homepageon the graph. Second; as a live proof-of-concept we dynamically re-rank Wikipedia …,Proceedings of the 14th International Conference on Extending Database Technology,2011,3
Networked PIM Using PDMS.,Alexander Albrecht; Felix Naumann; Armin Roth,Abstract Personal information management (Pim) is a promising new type of applicationallowing not only to search a desktop; but to pose complex; structured queries against thedata on ones computer. We propose to remove the confines of Pim and make selected dataavailable to a network of peers using peer data management system (Pdms) technology.The result is an application for collaborative information management within workgroups. Toachieve this vision; several participating tools and technologies must be adapted: Pimsystems must be augmented with privacy concepts to protect non-public data; which in turnmust be interpreted by the Pdms query rewriting mechanisms. The entity resolution methodsof individual Pim systems must be extended across multiple Pim systems with possiblyheterogeneous schemata and must support ad-hoc queries. Finally; traditional Pdms are …,NetDB,2007,3
Data integration in the Life Sciences (DILS),Barbara Eckman; Ulf Leser; Felix Naumann,*,*,2006,3
Labeling and enhancing life sciences links,Stephan Heymann; Felix Naumann; Louiqa Raschid; Peter Rieger,Life sciences data sources contain data about scientific objects such as genes andsequences that are richly interconnected; ie; a gene object may have links to sequences;proteins; SNPs; citations; etc. Scientific knowledge is enhanced by exploration ofrelationships between scientific objects; requiring traversal of both links and paths(informally concatenations of links). There are significant limitations and challenges of suchexploration; because the links are inherently poor with respect to syntactic representationand semantic knowledge. The links are syntactically poor because the source,Computational Systems Bioinformatics Conference; 2004. CSB 2004. Proceedings. 2004 IEEE,2004,3
Qualitätsgesteuerte Anfragebearbeitung für Integrierte Informationssysteme,Felix Naumann,• Nutzer erwarten korrekte Ergebnisse; aber akzeptieren Datensätze; deren Werte nahe denAnfragebedingungen sind.• Nutzer erhoffen von einem Informationssystem ein vollständigesErgebnis; aber akzeptieren unvollständige Ergebnisse; zB bei eingeschränktenRessourcen.• Nutzer erwarten ein lückenloses Ergebnis; dh es sollte alle Attribute derAnfrage enthalten; und kein Attribut sollte null-Werte enthalten. Jedoch akzeptieren Nutzerlückenhafte Ergebnisse; also Ergebnisse mit fehlenden Attributwerten–eine unvollständigeAntwort mit einigen Lücken ist besser als keine Antwort. Um Nutzern die Informationen desWebs zu erschließen; werden integrierte webbasierte Informationssysteme entwickelt.Solche Systeme bieten Nutzern eine einheitliche und integrierende Schnittstelle für vieleInformationsquellen. Die jüngst entwickelte WebService-Spezifikation ist ein weiterer …,it-Information Technology,2003,3
Density Scores for Cooperative Query Answering.,Felix Naumann; Ulf Leser,Abstract Mediator-based information systems answer global queries by rewriting them into acombination of queries against physical data sources. One inherent assumption in mostsystems is that only a combination that satis es the global query completely is consideredvalid; ie; it must obtain values for each required attribute. Furthermore; most systems strivefor complete answers; ie; they try to access all relevant sources. These requirementsfrequently lead to a system behavior that entails a high potential for user frustration: In manyscenarios a partially incomplete answer is much more appreciated than no answer at all.Also; obtaining the data from all sources; which can be very costly; is often not necessary.Based on this observation; we developed a cooperative query planning method. For a givenquery; the set of data sources is selected based on density scores obeying a userde ned …,Föderierte Datenbanken,1999,3
Fast Approximate Discovery of Inclusion Dependencies,Sebastian Kruse; Thorsten Papenbrock; Christian Dullweber; Moritz Finke; Manuel Hegner; Martin Zabel; Christian Zöllner; Felix Naumann,Inclusion dependencies (INDs) are relevant to several data management tasks; such asforeign key detection and data integration; and their discovery is a core concern of dataprofiling. However; n-ary IND discovery is computationally expensive; so that existingalgorithms often perform poorly on complex datasets. To this end; we present F; the firstapproximate IND discovery algorithm. F combines probabilistic and exact data structures toapproximate the INDs in relational datasets. In fact; F guarantees to find all INDs and onlywith a low probability false positives might occur due to the approximation. This littleinaccuracy comes in favor of significantly increased performance; though. In our evaluation;we show that F scales to very large datasets and outperforms the state-of-the-art algorithmby a factor of up to six in terms of runtime without reporting any false positives. This shows …,Business; Technology; and Web (BTW),2017,2
Holistic Data Profiling: Simultaneous Discovery of Various Metadata.,Jens Ehrlich; Mandy Roick; Lukas Schulze; Jakob Zwiener; Thorsten Papenbrock; Felix Naumann,ABSTRACT Data profiling is the discipline of examining an unknown dataset for its structureand statistical information. It is a preprocessing step in a wide range of applications; such asdata integration; data cleansing; or query optimization. For this reason; many algorithmshave been proposed for the discovery of different kinds of metadata. When analyzing adataset; these profiling algorithms are often applied in sequence; but they do not supportone another; for instance; by sharing I/O cost or pruning information. We present the holisticalgorithm Muds; which jointly discovers the three most important metadata: inclusiondependencies; unique column combinations; and functional dependencies. By sharing I/Ocost and data structures across the different discovery tasks; Muds can clearly increase theefficiency of traditional sequential data profiling. The algorithm also introduces novel inter …,EDBT,2016,2
Sorted Neighborhood Methods,Felix Naumann,Page 1. Sorted Neighborhood Methods 2.7.2013 Felix Naumann Page 2. Duplicate DetectionFelix Naumann | Data Profiling and Data Cleansing | Summer 2013 2 Page 3. Number ofcomparisons: All pairs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 6 7 89 10 11 12 13 14 15 16 17 18 19 20 Felix Naumann | Data Profiling and Data Cleansing |Summer 2013 3 400 comparisons Page 4. Reflexivity of Similarity 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 18 19 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 FelixNaumann | Data Profiling and Data Cleansing | Summer 2013 4 380 comparisons Page5. Symmetry of Similarity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 6 78 9 10 11 12 13 14 15 16 17 18 19 20 Felix Naumann | Data Profiling and Data Cleansing |Summer 2013 5 190 comparisons Page 6. Blocking by ZIP …,*,2013,2
Projektseminar „Similarity Search Algorithms “,Dustin Lange; Tobias Vogel; Uwe Draisbach; Felix Naumann,Zusammenfassung Mithilfe von Verfahren aus dem Bereich Ähnlichkeitssuche können zueiner Anfrage an einen Datenbestand nicht nur exakte; sondern auch ähnliche Objektegefunden werden; z. B. Bilder mit ähnlichen Motiven wie auf dem Anfragebild. Mit aktuellenForschungsansätzen aus diesem Bereich befasste sich das Seminar „Similarity SearchAlgorithms “; welches wir in diesem Bericht vorstellen. Das Ziel des Seminars war ein breiterVergleich bekannter Indexierungsalgorithmen mit Datensätzen aus verschiedenenBereichen. Die Studenten befassten sich mit je zwei Ähnlichkeitsmaßen für Datensätze ausfünf verschiedenen Domänen und mit je einem von sechs verschiedenen Indexstrukturenzur Ähnlichkeitssuche in metrischen Räumen. In diesem Bericht evaluieren wir dieKombination der Ähnlichkeitsmaße mit den Indexstrukturen bzgl. Indexaufbau und knn …,Datenbank-Spektrum,2011,2
FUSE BY: Syntax und Semantik zur Informationsfusion in SQL.,Jens Bleiholder; Felix Naumann,Abstract: Daten und Informationen heterogener Quellen können gleiche Objekterepräsentieren und dennoch sich widersprechen oder sich ergänzen. Werden solche Datenintegriert; entstehen Datenkonflikte. Wir beschreiben eine einfache Ergänzung von SQL; dieDaten über gleiche Objekte fusioniert. Der FUSE BY Operator vereint sich ergänzendeDaten zu einem Tupel und löst gegebenenfalls Datenkonflikte. Neben der Syntax desAusdrucks beschreiben wir seine einfache und intuitive Semantik. Schließlich werdenErweiterungen diskutiert; die es erlauben den Ausdruck um Expertenwissen zu ergänzen.,GI Jahrestagung (1),2004,2
Systems and methods for data integration,*,A computer implemented method for integrating data into a target database may include:providing a plurality of source databases which each may include a relational schema anddata for integration into the target database; generating at least one complexity model basedon the relational schema and data of each source database; each complexity modelindicating at least one inconsistency between two or more of the data sources which may berequire to be resolved to integrate the data from the data sources into the target database;and generating an effort model that may include an effort value for each inconsistencyindicated by each complexity model; each effort value indicating at least one of a time periodand a financial cost to resolve the inconsistency to integrate data from the data sources intothe target database.,*,2016,1
Uniqueness; Density; and Keyness: Exploring Class Hierarchies.,Anja Jentzsch; Hannes Mühleisen; Felix Naumann,Abstract. The Web of Data contains a large number of openly-available datasets covering awide variety of topics. In order to benefit from this massive amount of open data; eg; to addvalue to an organization's internal data; such external datasets must be analyzed andunderstood already at the basic level of data types; uniqueness; constraints; value patterns;etc. For Linked Datasets and other Web data such meta information is currently quite limitedor not available at all. Data profiling techniques are needed to compute respective statisticsand meta information. Analyzing datasets along the vocabulary-defined taxonomichierarchies yields further insights; such as the data distribution at different hierarchy levels;or possible mappings betweens vocabularies or datasets. In particular; key candidates forentities are difficult to find in light of the sparsity of property values on the Web of Data. To …,COLD,2015,1
Who wants a computer to be a millionaire?,Saeedeh Momtazi; Felix Naumann,Abstract Competing against computers was one of the important challenges in the lastdecades. People like to compare their abilities with computers that are; in fact; their owninvention. TV game shows provide a good opportunity for such competitions. Severalattempts have been made to find out how sophisticated systems fare in game shows. Anexample of the task is competing in games with multiple-choice questions; such as “Whowants to be a Millionaire”. We propose an approach to this problem by using search enginesand knowledge bases to automatically select the answer. The experimental results indicatethe superiority of the proposed model over related work. Our proposed method achievedaverage winnings of $250;000 on a US question set and became a millionaire six times; outof fifty runs; which is much higher than the normal winning rate among human contestants.,Information Processing Letters,2015,1
Exploring Linked Data Graph Structures.,Anja Jentzsch; Christian Dullweber; Pierpaolo Troiano; Felix Naumann,Abstract. The true value of Linked Data becomes apparent when datasets are analyzed andunderstood already at the basic level of data types; constraints; value patterns etc. Such dataprofiling is especially challenging for Rdf data; the underlying data model on the Web ofData. In particular; graph analysis can be used to gain more insight into the data; induceschemas; or build indices. We present ProLod++; a tool for various profiling and miningtasks and in particular its recent extension GraphLod; which offers Rdf graph analysisfeatures. ProLod++ features many interactive profiling results specific for open data; such asschema discovery for user-generated attributes; association rule discovery to uncoversynonymous predicates; and key discovery along ontology hierarchies. GraphLod enhancesit with subgraph pattern mining; node degree distribution; component visualization and …,International Semantic Web Conference (Posters & Demos),2015,1
Semi-Supervised Consensus Clustering: Reducing Human Effort,Tobias Vogel; Felix Naumann,Machine-based clustering yields fuzzy results. For example; when detecting duplicates in adataset; different tools might end up with different clusterings. Eventually; a decision needsto be made; defining which records are in the same cluster; ie; are duplicates. Such adefinitive result is called a Consensus Clustering and can be created by evaluating theclustering attempts against each other and only resolving the disagreements by humanexperts. Yet; there can be different consensus clusterings; depending on the choice ofdisagreements presented to the human expert. In particular; they may require a differentnumber of manual inspections. We present a set of strategies to select the smallest set ofmanual inspections to arrive at a consensus clustering and evaluate their efficiency on a setof real-world and synthetic datasets.,International Workshop on Data Integration and Applications,2014,1
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1
Scalable peer-to-peer-based RDF management,Christoph Böhm; Daniel Hefenbrock; Felix Naumann,Abstract Handling web-scale RDF data requires sophisticated data management that scaleseasily and integrates seamlessly into existing analysis workflows. We present Hdrs--ascalable storage infrastructure that enables online-analysis of very large RDF data sets.Hdrs combines state-of-the-art data management techniques to organize triples in indexesthat are sharded and stored in a peer-to-peer system. The store is open source andintegrates well with Hadoop MapReduce or any other client application.,Proceedings of the 8th International Conference on Semantic Systems,2012,1
Kurz erklärt: Datenfusion,Jens Bleiholder; Felix Naumann,Moderne Werkzeuge zur Informationsintegration ermöglichen es auf zunehmend einfacheWeise; die Daten weltweit verteilter; heterogener Datenquellen zusammenzuführen. Dazuzählt die Überwindung technischer Heterogenität (Formate; Protokolle;...); die Überwindungstruktureller Heterogenität (Datenmodell; Schema;...) sowie die Überwindung dersemantischen Heterogenität; die sich also auf den konkreten Inhalt der Quellen bezieht. Imletzteren; oft vernachlässigten Gebiet ist die Datenfusion angesiedelt. Datenfusionbezeichnet die Zusammenführung mehrerer verschiedener Datensätze; die alle dasselbeObjekt in der realen Welt beschreiben. Ein typisches Beispiel ist ein mehrfach erfassterKunde; der mit möglicherweise leicht unterschiedlichen Daten in den zu integrierendenDatenbanken vertreten ist. In einem ersten Schritt werden Struktur und Schema der …,Datenbank-Spektrum,2011,1
Dr. Crowdsource or: How I Learned to Stop Worrying and Love Web Data,Felix Naumann,Abstract The wealth of freely available; structured information on the Web is constantlygrowing. Driving domains are public data from and about governments and administrations;scientific data; and data about media; such as articles; books and albums. In addition;general-purpose datasets; such as DB-pedia and Freebase from the linked open datacommunity; serve as a focal point for many data sets. Thus; it is possible to query or integratedata from multiple sources and create new; integrated data sets with added value.,BEWEB,2011,1
13th international workshop on the web and databases: WebDB 2010,Xin Luna Dong; Felix Naumann,The WebDB workshop has been held thirteen times so far: the first WebDB workshop wascolocated with EDBT'1998; whereas the other twelve were co-located with the annualSIGMOD/PODS conference. The WebDB workshop provides a forum where researchers;theoreticians; and practitioners can share their knowledge and opinions about problems andsolutions at the intersection of data management and the Web. WebDB has had a highimpact and has been a forum in which a number of seminal papers have been presented.Since 2002; the workshop always had a theme. In 2010 WebDB focused on Quality of WebData and on Linked Data; but papers on all aspects of the web and databases weresolicited; such as unstructured and semi-structured data management; data-extraction;-integration;-cleansing; and-mining; web applications and privacy; search and information …,ACM SIGMOD Record,2011,1
Encapsulating multi-stepped web forms as web services,Tobias Vogel; Frank Kaufer; Felix Naumann,Abstract HTML forms are the predominant interface between users and web applications.Many of these applications display a sequence of multiple forms on separate pages; forinstance to book a flight or order a DVD. We introduce a method to wrap these multi-steppedforms and offer their individual functionality as a single consolidated Web Service. This WebService in turn maps input data to the individual forms in the correct order. Suchconsolidation better enables operation of the forms by applications and provides a simplerinterface for human users. To this end we analyze the HTML code and sample userinteraction of each page and infer the internal model of the application. A particularchallenge is to map semantically same fields across multiple forms and choose meaningfullabels for them. Web Service output is parsed from the resulting HTML page. Experiments …,*,2010,1
Qualitäts-und Semantik-gesteuerte Anfragebearbeitung für Peer-basierte Datenmanagementsysteme (PDMS),Armin Roth; Felix Naumann,Integrierte Informationssysteme basieren meist auf einem globalen Schema; dessen Bildungund Wartung aufwändig ist. Praktiker bevorzugen jedoch den direkten Datenaustauschzwischen etablierten Systemen. Diese Anforderungen adressieren Peer-basierteDatenmanagementsysteme (PDMS) in dynamischer und skalierbarer Weise. Anstelle einesglobalen Schemas und Schema-Abbildungen zwischen globalem und lokalen Schematasind Peers untereinander durch Schema-Abbildungen verbunden; über die Anfragen undDaten transformiert und weitergeleitet werden. Solche Abbildungspfade führen allerdingsmeist zu einem Informationsverlust und vermindern die Qualität der Anfrageergebnisse. Dienaive Nutzung sämtlicher vorhandener Abbildungspfade ist ausserdem ineffizient. Wirschlagen für PDMS die Berücksichtigung der Informationsqualität bezüglich …,*,2004,1
Mediator-basierte; heterogene verteilte Informationssysteme,Ulf Leser; Susanne Busse; Herbert Weber; Christoph Holzheuer; Holger Last; M Christian; Felix Naumann; Tom Ritter; Leon Rosenberg,Hier wird der Begri Schemaintegration ausschlie lich im Sinne der Datenbankintegrationverwendet; dh wir betrachten hier die Integration von strukturierten Daten; wie sie durchDatenbankschemata beschrieben werden. Die Anwendung der weiter unten vorgestelltenIntegrationsmethoden auf semistrukturierte Daten ist nicht vorgesehen. Sie w urdezumindest eine zus atzliche\Umsetzungsschicht" erfordern; die semistrukturierteDatenquellen auf wohlde nierte Schemata abbildet. Inwieweit die Schemaintegration imKontext semistrukturierter Daten sinnvoll ist; bleibt fraglich. Die im folgenden betrachtetenAnforderungen an ein globales; integriertes Schema liegen im Kon ikt mit der densemistrukturierten Daten innewohnenden Vagheit.,*,1998,1
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*
Identifying Media Bias by Analyzing Reported Speech,Konstantina Lazaridou; Ralf Krestel; Felix Naumann,Media analysis can reveal interesting patterns in the way newspapers report the news andhow these patterns evolve over time. One example pattern is the quoting choices that mediamake; which could be used as bias indicators. Media slant can be expressed both with thechoice of reporting an event; eg a person's statement; but also with the words used todescribe the event. Thus; automatic discovery of systematic quoting patterns in the newscould illustrate to the readers the media'beliefs; such as political preferences. In this paper;we aim to discover political media bias by demonstrating systematic patterns of reportingspeech in two major British newspapers. To this end; we analyze news articles from 2000 to2015. By taking into account different kinds of bias; such as selection; coverage and framingbias; we show that the quoting patterns of newspapers are predictable.,Data Mining (ICDM); 2017 IEEE International Conference on,2017,*
Efficient Denial Constraint Discovery with Hydra,Tobias Bleifuß; Sebastian Kruse; Felix Naumann,ABSTRACT Denial constraints (DCs) are a generalization of many other integrity constraints(ICs) widely used in databases; such as key constraints; functional dependencies; or orderdependencies. Therefore; they can serve as a unified reasoning framework for all of theseICs and express business rules that cannot be expressed by the more restrictive IC types.The process of formulating DCs by hand is difficult; because it requires not only domainexpertise but also database knowledge; and due to DCs' inherent complexity; this process istedious and error-prone. Hence; an automatic DC discovery is highly desirable: we searchfor all valid denial constraints in a given database instance. However; due to the largesearch space; the problem of DC discovery is computationally expensive. We propose anew algorithm Hydra; which overcomes the quadratic runtime complexity in the number of …,Proceedings of the VLDB Endowment,2017,*
Enabling Change Exploration: Vision Paper,Tobias Bleifuß; Theodore Johnson; Dmitri V Kalashnikov; Felix Naumann; Vladislav Shkapenyuk; Divesh Srivastava,Abstract Data and metadata suffer many different kinds of change: values are inserted;deleted or updated; entities appear and disappear; properties are added or re-purposed;etc. Explicitly recognizing; exploring; and evaluating such change can alert to changes indata ingestion procedures; can help assess data quality; and can improve the generalunderstanding of the dataset and its behavior over time. We propose a data model-independent framework to formalize such change. Our change-cube enables explorationand discovery of such changes to reveal dataset behavior over time.,Proceedings of the ExploreDB'17,2017,*
What was Hillary Clinton doing in Katy; Texas?,Toni Gruetze; Ralf Krestel; Konstantina Lazaridou; Felix Naumann,Abstract During the last presidential election in the United States; Twitter drew a lot ofattention. This is because many leading persons and organizations; such as US presidentDonald J. Trump; showed a strong affection to this medium. In this work we neglect thepolitical contents and opinions shared on Twitter and focus on the question: Can wedetermine and track the physical location of presidential candidates based on posts in theTwittersphere?,Proceedings of the 26th International Conference on World Wide Web Companion,2017,*
Das Fachgebiet „Informationssysteme “am Hasso-Plattner-Institut,Felix Naumann; Ralf Krestel,Zusammenfassung Das Hasso-Plattner-Institut (HPI) ist ein privat finanziertes Institut an derUniversität Potsdam. Stifter ist Professor Hasso Plattner; Mitgründer undAufsichtsratsvorsitzender des Software-Konzerns SAP. Das FachgebietInformationssysteme; das von Prof. Dr. Felix Naumann geleitet wird; beschäftigt sich mit demeffizienten und effektiven Umgang mit heterogenen Daten und Texten. Gegründet wurdedas Fachgebiet 2006 und bietet derzeit 12 Doktoranden und circa 15 Masterstudenten eineForschungsumgebung.,Datenbank-Spektrum,2017,*
Cardinality Estimation: An Experimental Survey,Hazar Harmouch; Felix Naumann,ABSTRACT Data preparation and data profiling comprise many both basic and complextasks to analyze a dataset at hand and extract metadata; such as data distributions; keycandidates; and functional dependencies. Among the most important types of metadata isthe number of distinct values in a column; also known as the zeroth-frequency moment.Cardinality estimation itself has been an active research topic in the past decades due to itsmany applications. The aim of this paper is to review the literature of cardinality estimationand to present a detailed experimental study of twelve algorithms; scaling far beyond theoriginal experiments. First; we outline and classify approaches to solve the problem ofcardinality estimation–we describe their main idea; error-guarantees; advantages; anddisadvantages. Our experimental survey then compares the performance all twelve …,Proceedings of the VLDB Endowment,2017,*
Metacrate: Organize and Analyze Millions of Data Profiles,Sebastian Kruse; David Hahn; Marius Walter; Felix Naumann,ABSTRACT Databases are one of the great success stories in IT. However; they have beencontinuously increasing in complexity; hampering operation; maintenance; and upgrades.To face this complexity; sophisticated methods for schema summarization; data cleaning;information integration; and many more have been devised that usually rely on data pro les;such as data statistics; signatures; and integrity constraints. Such data pro les are o enextracted by automatic algorithms; which entails various problems: e pro les can be un lteredand huge in volume; di erent pro le types require di erent complex data structures; and thevarious pro le types are not integrated with each other. We introduce Metacrate; a system tostore; organize; and analyze data pro les of relational databases; thereby following theproven design of databases. In particular; we (i) propose a logical and a physical data …,*,2017,*
Uncovering Business Relationships: Context-sensitive Relationship Extraction for Difficult Relationship Types,Zhe Zuo; Michael Loster; Ralf Krestel; Felix Naumann,Abstract. This paper establishes a semi-supervised strategy for extracting various types ofcomplex business relationships from textual data by using only a few manually providedcompany seed pairs that exemplify the target relationship. Additionally; we offer a solutionfor determining the direction of asymmetric relationships; such as “ownership of”. Weimprove the reliability of the extraction process by using a holistic pattern identificationmethod that classifies the generated extraction patterns. Our experiments show that we canaccurately and reliably extract new entity pairs occurring in the target relationship by usingas few as five labeled seed pairs.,*,2017,*
Improving Company Recognition from Unstructured Text by using Dictionaries.,Michael Loster; Zhe Zuo; Felix Naumann; Oliver Maspfuhl; Dirk Thomas,ABSTRACT While named entity recognition is a much addressed research topic;recognizing companies in text is of particular difficulty. Company names are extremelyheterogeneous in structure; a given company can be referenced in many different ways;their names include person names; locations; acronyms; numbers; and other unusualtokens. Further; instead of using the official company name; quite different colloquial namesare frequently used by the general public. We present a machine learning (CRF) system thatreliably recognizes organizations in German texts. In particular; we construct and employvarious dictionaries; regular expressions; text context; and other techniques to improve theresults. In our experiments we achieved a precision of 91.11% and a recall of 78.82%;showing significant improvement over related work. Using our system we were able to …,EDBT,2017,*
Cluster-Based Sorted Neighborhood for Efficient Duplicate Detection,Ahmad Samiei; Felix Naumann,Duplicate detection intends to find multiple and syntactically different representations of thesame real-world entities in a dataset. The naive way of duplicate detection entails aquadratic number of pair-wise record comparisons to identify the duplicates. This number ofcomparisons might take hours even for an average sized dataset. As today's databasesgrow very fast; different candidate-selection methods; such as sorted neighborhood;blocking; canopy clustering and their variations; address this problem by shrinking thecomparison space. The volume and velocity of data-change require ever faster and moreflexible methods of duplicate detection. In particular; they need dynamic indices that can beupdated efficiently as new data arrives. We present a novel approach; which combines theidea of cluster-based methods with the well-known sorted neighborhood method. It …,Data Mining Workshops (ICDMW); 2016 IEEE 16th International Conference on,2016,*
The Information Systems Group at HPI,Felix Naumann; Ralf Krestel,Abstract The Hasso Plattner Institute (HPI) is a private computer science institute funded bythe eponymous SAP co-founder. It is affiliated with the University of Potsdam in Germanyand is dedicated to research and teaching; awarding B. Sc.; M. Sc.; and Ph. D. degrees. TheInformation Systems group was founded in 2006; currently has around ten Ph. D. studentsand about 15 masters students actively involved in our research activities. Our initial and stillongoing research focus has been the area of data cleansing and duplicate detection. Morerecently we have become active in the area of text mining to extract structured informationfrom text; and even more recently in data profiling; ie; the task of discovering variousmetadata and dependencies from a data instance.,ACM SIGMOD Record,2016,*
Combination of Rule-based and Textual Similarity Approaches to Match Financial Entities,Ahmad Samiei; Ioannis Koumarelas; Michael Loster; Felix Naumann,Record linkage is a well studied problem [1] with many years of publication history.Nevertheless; there are many challenges remaining to be addressed; such as the topicaddressed by FEIII Challenge 20161. Matching financial entities (FEs) is important for manyprivate and governmental organizations. In this paper we describe the problem of matchingsuch FEs across three datasets: FFIEC; LEI and SEC. We were able to achieve an f-measureof 93.78% in the first task; which is comparable to the maximum 97.44%; and 70.44% for thesecond task; where the maximum is 88.38%.,Proceedings of the Second International Workshop on Data Science for Macro-Modeling,2016,*
Datasets profiling tools; methods; and systems,*,A dataset profiling tool configured to identify unique and non-unique column combinations ina dataset which comprises a plurality of tuples; the tool including: an inserts handler moduleconfigured to: receive one or more new tuples for insertion into the dataset; receive one ormore minimal uniques and one or more maximal non-uniques for the dataset; identify andgroup; for each minimal unique; any tuples of the dataset and any of the one or more newtuples which contain duplicate values in the column combinations of the minimal unique; toform grouped tuples which are grouped according to the minimal unique to which the tuplesrelate; validate the grouped tuples to identify supersets of the minimal uniques for whichduplicate values were identified; to generate a new set of one or more minimal uniques andone or more maximal non-uniques; and output the new set of one or more updated …,*,2016,*
Method and system to discover dependencies in datasets,*,A method of processing data stored in a database which comprises a plurality of rows andcolumns; the method comprising identifying a plurality of sets of column combinations; eachset of column combinations comprising an identifier of at least one column allocating eachset of column combinations to one of a plurality of nodes mapping the nodes to a latticestructure in which the nodes are connected in a superset or subset relationship according tothe set of column combinations of each node selecting a current node processing the data inthe set of columns of the current node to detect if the column combination is unique or non-unique traversing the lattice to a next node which is connected to the current nodeprocessing the data in the set of columns of the next node to detect if the columncombination of the next node is unique or non-unique; and storing a record of whether …,*,2016,*
Bulk sorted access for efficient top-k retrieval,Dustin Lange; Felix Naumann,Abstract Efficient top-k retrieval of records from a database has been an active research fieldfor many years. We approach the problem from a real-world application point of view; inwhich the order of records according to some similarity function on an attribute is not unique:Many records have same values in several attributes and thus their ranking in thoseattributes is arbitrary. For instance; in large person databases many individuals have thesame first name; the same date of birth; or live in the same city. Existing algorithms; such asthe Threshold Algorithm (TA); are ill-equipped to handle such cases efficiently. We introducea variation of TA; the Bulk Sorted Access Algorithm (BSA); which retrieves larger chunks ofrecords from the sorted lists using fixed thresholds; and which focusses its efforts on recordsthat are ranked high in more than one ordering and are thus more promising candidates …,Proceedings of the 25th International Conference on Scientific and Statistical Database Management,2013,*
Systematic ETL Management - Experiences with High-Level Operators,Alexander Albrecht; Felix Naumann,*,ICIQ,2013,*
The Quality of Web Data.,Felix Naumann,■ Government data□ www. data. gov (380k data sets)□ data. gov. uk (9k)□ ec. europa.eu/eurostat■ Finance/business data■ Scientific databases□ www. uniprot. org□skyserver. sdss. org■ The Web□ HTML tables and lists> 1billion (estimated Feb. 2011)□General sources: Dbpedia (3.7 m); freebase (23m);…□ Domain-specific sources: IMDB;Gracenote; isbndb;…,ICIQ,2012,*
Foreword,Elke Rundensteiner; I Manolescu; S Amer-Yahia; F Naumann; V Markl; İsmail Arı,*,*,2012,*
Scalable Similarity Search with Dynamic Similarity Measures,Martin Köppelmann; Dustin Lange; Claudia Lehmann; Marika Marszalkowski; Felix Naumann; Peter Retzlaff; Sebastian Stange; Lea Voget,ABSTRACT Similarity search on structured data assumes some similarity measure on thedata–often a combination of individual measures per attribute. Users of a similarity searchsystem may have different requirements on the similarity measure; the individual measurescan be combined in many different ways; including a simple weighted sum of the similaritieswith varying weights; or; at the other end of the spectrum; much more complex machinelearning techniques. Previous approaches to similarity search work only with static similaritymeasures or cannot exploit dynamic similarity measures. In this paper; we present theDySim algorithm; a novel approach that answers similarity queries with query-specificconfigurations of similarity measures: For any query; users are allowed to define an arbitrary;including non-metric; overall similarity measure that is based on similarities of attribute …,*,2012,*
Extreme web data integration.,Felix Naumann,Page 1. Extreme Web Data Integration August 14; 2010 Felix Naumann Page 2.Acknowledgements ■ @IBM Almaden □ Howard Ho; Mauricio Hernandez; RajasekarKrishnamurthy; Lucian Popa; Roxana Stanoi ■ @HPI □ Christoph Böhm □ Bachelor projectstudent team ■ And elsewhere □ Antonio Sala (University of Modena) □ Chis Bizer (dbpedia)□ Open Data community Felix Naumann | Extreme Web Data Integration | NFIC 2010 2 Page3. Overview Felix Naumann | Extreme Web Data Integration | NFIC 2010 3 ■ Web Dataabounds – linked; open; and otherwise ■ Web Data stinks – dirt; grime; and some surprises ■Cleansing and Integration – of mops and brooms ■ The GovWILD experience – politicians;friends; and funds Page 4. Felix Naumann | Extreme Web Data Integration | NFIC 2010 4http://www4.wiwiss.fu-berlin.de/bizer/ Page 5. DBpedia - Extraction …,PIKM@ CIKM,2011,*
Improving Service Discovery through Enriched Service Descriptions.,Mohammed AbuJarour; Felix Naumann,Abstract: The increasing popularity of the Software-as-a-Service and Cloud Computingtrends has been among the main factors behind the increasing number of public webservices in several domains; eg; e-commerce; enterprise; education; government; etc.Moreover; the functionalities of such web services are becoming more complex due to thecomplexities of modern business needs and marketplaces. Additionally; it has beenobserved that service providers; who represent the single source of information about webservices; typically release poor service descriptions. Due to the aforementioned factors;service discovery has become one of the main challenges in Service-oriented Computing(SOC). In this demo; we show how to enrich service descriptions enabling enhanced servicediscovery. In our approach; web services are enriched with annotations (textual …,BTW,2011,*
Similarity Search Algorithms,Dustin Lange; Tobias Vogel; Uwe Draisbach; Felix Naumann,*,Datenbankspektrum,2011,*
WebDB 2010 at ACM SIGMOD 2010 13th International Workshop on the Web and Databases,Xin Luna Dong; Felix Naumann,The WebDB workshop focuses on providing a forum where researchers; theoreticians; andpractitioners can share their knowledge and opinions about problems and solutions at theintersection of data management and the Web. WebDB has high impact and has been aforum in which a number of seminal papers have been presented. The workshop chairswelcome researchers and practitioners to register for the workshop at http://webdb2010.org/registration. html. The registration fee is $100; rebates for students ($65) and ACMmembers ($80) are available. Breakfast; lunch; and coffee breaks will be provided.,SIGMOD Record,2009,*
Guest Editorial for the Special Issue on Data Quality in Databases,Felix Naumann; Louiqa Raschid,International Conference on Information Quality (ICIQ); and the annual QDB workshop striveto create opportunities to showcase research that bridge these disciplines and achieve acommon goal; namely; to provide information of appropriately high quality for a multitude ofdifferent use cases and applications. While information systems research concentrates onthe modeling of many facets and dimensions of information quality; computer science has afocus on the algorithmic and data management aspects of analyzing data with respect todiverse quality criteria and maintaining or increasing quality. This Special Issue on DataQuality in Databases highlights four of the latest computational and algorithmic resultscovering the following broad range of important data quality issues:—the use of data miningtechniques to analyze data quality;—methods to implant information quality dimensions …,Journal of Data and Information Quality (JDIQ),2009,*
Proceedings of the 3rd Ph. D. Retreat of the HPI Research School on Service-oriented Systems Engineering,Christoph Meinel; Hasso Plattner; Jürgen Döllner; Mathias Weske; Andreas Polze; Robert Hirschfeld; Felix Naumann; Holger Giese,Design and Implementation of service-oriented architectures imposes a huge number ofresearch questions from the fields of software engineering; system analysis and modeling;adaptability; and application integration. Component orientation and web services are twoapproaches for design and realization of complex web-based system. Both approachesallow for dynamic application adaptation as well as integration of enterprise application.Commonly used technologies; such as J2EE and. NET; form de facto standards for therealization of complex distributed systems. Evolution of component systems has lead to webservices and service-based architectures. This has been manifested in a multitude ofindustry standards and initiatives such as XML; WSDL UDDI; SOAP; etc. All theseachievements lead to a new and promising paradigm in IT systems engineering which …,*,2009,*
Informationsqualität: Antrittsvorlesung 2007-04-26,Felix Naumann,Sowohl in kommerziellen als auch in wissenschaftlichen Datenbanken sind Daten vonniedriger Qualität allgegenwärtig. Das kann zu erheblichen wirtschaftlichen Problemenführen"; erläutert der 35-jährige Informatik-Professor und verweist zum Beispiel aufDuplikate. Diese können entstehen; wenn in Unternehmen verschiedeneKundendatenbestände zusammengefügt werden; aber die Integration mehrere Datensätzedes gleichen Kunden hinterlässt." Solche doppelten Einträge zu finden; ist aus zweiGründen schwierig: Zum einen ist die Menge der Daten oft sehr groß; zum anderen könnensich Einträge über die gleiche Person leicht unterscheiden"; beschreibt Prof. Naumannhäufig auftretende Probleme. In seiner Antrittsvorlesung will er zwei Lösungswegevorstellen: Erstens die Definition geeigneter Ähnlichkeitsmaße und zweitens die Nutzung …,*,2008,*
Schema-und Metadatenmanagement in Peer Data Management Systemen.,Felix Naumann,Page 1. 1 Schema- und Metadatenmanagement in Peer Data Management Systemen Invitedtalk @ BTW Workshop; 6.3.2007 Felix Naumann Hasso-Plattner-Institut Fachgebiet„Informationssysteme“ Felix Naumann | Fachgebiet Informationssysteme | 6. März 2007 2Überblick ■ Peer Data Management ■ Metadaten Management für Peers □ Schema Mappings □Schema Mappings in PDMS □ Statistische Metadaten ■ Erhebung von Metadaten ■ SystemP Page 2. 2 Felix Naumann | Fachgebiet Informationssysteme | 6. März 2007 3 FöderierteDatenbanken ■ Globales Schema & direkter Zugriff auf Datenquellen □ Aufwändig inImplementierung und Wartung (Evolution) □ Skalierbarkeit und Flexibilität schlecht ■ Mediatorist single point of failure. Oracle; DB2… Web service Anwen- dung HTML Form Datei- systemFDBMS Felix Naumann | Fachgebiet Informationssysteme | 6. März 2007 4 …,BTW Workshops,2007,*
Querying Web-Accessible Life Science Sources: Which paths to choose?,Jens Bleiholder; Felix Naumann; Louiqa Raschid; Marıa Esther Vidal,1 Introduction Web-accessible life sciences sources are characterized by a complex graphof overlapping sources; and multiple alternate links between sources. A (navigational) querymay be answered by traversing multiple alternate paths between a start source and a targetsource. Each of these paths may have dissimilar benefit; eg; the cardinality of result objectsthat are reached in the target source. Paths may also have dissimilar costs of evaluation; ie;the execution cost of a query evaluation plan for a path. Finally; since the result objects ofalternate paths may overlap; the combined benefit of two paths are not independent. In thiscontext; we present two problems. The first problem is to determine the K-best paths orRelevant Paths with low cost and high benefit. The second problem is to choose a goodcombination of top-ó (possibly overlapping) paths. While the first problem regards paths …,Proceedings of VLDB Workshop on Information Integration on the Web (IIWeb-2004),2004,*
Brain-Gain: Wie verlockend sind Juniorprofessuren?,Felix Naumann,Seit weniger als 2 Monaten bin ich Juniorprofessor der Informatik an der Humboldt-Universität zu Berlin und schon mitten im Universitätsalltag eines Professors angekommen.Das hehre Ziel; nach 2 Jahren in der Wirtschaft endlich wieder hauptberuflich zu forschen;mußte ich etwas verschieben. In den nächsten Monaten werde ich vor allem Vorlesungen;Übungen und Seminare vorbereiten; im Wintersemester werde ich sie halten; um dann–endlich–im Sommer etwas Freiraum zu haben und mein Forschungsvorhabenvoranzubringen. Aber ich will am Anfang des Weges beginnen; der mich hierher geführt hat.Kurz nach dem Mauerfall und kurz vor der Wiedervereinigung war Berlin eine attraktiveStadt; besonders für einen Zwanzigjährigen. Ich entschied mich für einWirtschaftsmathematikstudium an der Technischen Universitaet–nicht zu theoretisch …,*,*,*
2010 IEEE International Conference on Granular Computing (GrC-2010),Yue Luo; Jia-li Feng; Johannes Lorey; Felix Naumann,This paper presents a novel and efficient face recognition technique based on Local BinaryPattern (LBP) with threshold for resolving traditional LBP's weakness of extracting globalfeatures. By setting a threshold to enhance the robustness to noise such as light and extractthe global features of face preferably. Combining the local features by LBP with globalfeatures as the total features of the...,*,*,*
BioFast Challenges in E) ploring Linked Life Sciences Sources Jens Bleiholder Humboldt-Universit at zu Berlin bleihoOinformatik. hu-berlin. de,Felix Naumann; Louiqa Raschid; Maria-Esther Vidal,1 Introduction An abundance of life sciences data sources contain data about scientificentities such as genes and sequences. Scientists are interested in exploring relationshipsbetween scientific objects; eg; between genes and bibliographic citations. A scientist maychoose the åæçæ source; which contains information related to human genetic diseases; asa starting point for her exploration; and wish to eventually retrieve all related citations fromthe ñòóôõö source. Starting with a keyword search on a certain disease; she can explore allpossible relationships between genes in åæçæ and citations in ñòóôõö. This corresponds tothe following query:" Retu rn all ci t at ions of ñòóôõö tha t ar e link edto an åæçæ en try tha tis re la te dto som e dis e as e or condi tion.",*,*,*
DATABASE: CREATION; MANAGEMENT AND UTILIZATION,Gottfried Vossen; Dennis Shasha; Sihem Amer-Yahia; Ricardo Baeza-Yates; Philippe Bonnet; Toon GK Calders; Felipe Carino Jr; Bettina Kemme; Flip Korn; Nick Koudas; Maurizio Lenzerini; Peri Loucopoulos; Felix Naumann; Philippe Pucheral; Ken A Ross; Dan Suciu; Jan Van den Bussche; Mathias Weske; Limsoon Wong; Xifeng Yan,*,*,*,*
Stratosphere: Informationsmanagement “above the Clouds”,Johann-Christoph Freytag; Odej Kao; Ulf Leser; Volker Markl; Felix Naumann,Zusammenfassung Dieser Artikel beschreibt die Vision und Ziele von Stratosphere; einervon der DFG geförderten Forschergruppe; in der fünf Fachgebiete an drei Universitäten inBerlin und Potsdam das Thema „Informationsmanagement in massiv-parallelen;virtualisierten Rechner-Infrastrukturen (Cloud)“untersuchen. Neben der Entwicklung einesForschungsprototypen zur fehlertoleranten; verteilten; adaptiven Anfrageverarbeitungwerden dabei auch Anwendungsfälle aus den Bereichen Lebenswissenschaften;Datenreinigung für Linked-Open-Daten und wissenschaftliches Rechnen untersucht.Stratosphere wird von der DFG von Oktober 2010 an für zunächst 3 Jahre gefördert.,*,*,*
Editorial for the first edition of Volume 5 of the ACM Journal on Data and Information Quality (JDIQ),Felix Naumann,*,JDIQ,*,*
Experiences in Building a High Quality Meta Search Engine,Julia Böttcher; Felix Naumann,*,*,*,*
Efficient Top K Query Rewriting over Web Sources with Overlap,Marэa Esther Vidal; Louiqa Raschid; Yao Wu; Jens Bleiholder; Jens Bleiholder; Felix Naumann; Felix Naumann,ABSTRACT The proliferation of Web accessible data sources has an impact on thecomplexity of tasks such as query rewriting and evaluation. With multiple sources; theproblem of choosing the correct set of sources becomes non-trivial. The problem is furthercompounded when there is overlap among the sources. Selecting a minimal query rewritingthat satisfies desirable criteria; eg; contact less sources or return more answers; whileproviding efficient query evaluation plans; becomes more difficult. In this paper; we considerthe problem of finding a minimal query rewriting that will cover the Top K answers; whereTop K is defined with respect to some given metric. This is the Top K Query Rewritingproblem and we propose a two-fold solution. First; we apply sampling based techniques toapproximate the Top K answers produced by a specific query rewriting. Next; we develop …,*,*,*
Anfragebearbeitung und Optimierung in Multidatenbanksystemen,Heiko Müller; Felix Naumann,*,*,*,*
