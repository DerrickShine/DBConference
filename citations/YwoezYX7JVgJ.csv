Nested mappings: schema mapping reloaded,Ariel Fuxman; Mauricio A Hernandez; Howard Ho; Renee J Miller; Paolo Papotti; Lucian Popa,Abstract Many problems in information integration rely on specifications; called schemamappings; that model the relationships between schemas. Schema mappings for bothrelational and nested data are well-known. In this work; we present a new formalism forschema mapping that extends these existing formalisms in two significant ways. First; ournested mappings allow for nesting and correlation of mappings. This results in a naturalprogramming paradigm that often yields more accurate specifications. In particular; we showthat nested mappings can naturally preserve correlations among data that existing mappingformalisms cannot. We also show that using nested mappings for purposes of exchangingdata from a source to a target will result in less redundancy in the target data. The secondextension to the mapping formalism is the ability to express; in a declarative way …,Proceedings of the 32nd international conference on Very large data bases,2006,151
Holistic data cleaning: Putting violations into context,Xu Chu; Ihab F Ilyas; Paolo Papotti,Data cleaning is an important problem and data quality rules are the most promising way toface it with a declarative approach. Previous work has focused on specific formalisms; suchas functional dependencies (FDs); conditional functional dependencies (CFDs); andmatching dependencies (MDs); and those have always been studied in isolation. Moreover;such techniques are usually applied in a pipeline or interleaved. In this work we tackle theproblem in a novel; unified framework. First; we let users specify quality rules using denialconstraints with ad-hoc predicates. This language subsumes existing formalisms and canexpress rules involving numerical values; with predicates such as “greater than” and “lessthan”. More importantly; we exploit the interaction of the heterogeneous constraints byencoding them in a conflict hypergraph. Such holistic view of the conflicts is the starting …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,95
The LLUNATIC data-cleaning framework,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract Data-cleaning (or data-repairing) is considered a crucial problem in many database-related tasks. It consists in making a database consistent with respect to a set of givenconstraints. In recent years; repairing methods have been proposed for several classes ofconstraints. However; these methods rely on ad hoc decisions and tend to hard-code thestrategy to repair conflicting values. As a consequence; there is currently no generalalgorithm to solve database repairing problems that involve different kinds of constraints anddifferent strategies to select preferred values. In this paper we develop a uniform frameworkto solve this problem. We propose a new semantics for repairs; and a chase-basedalgorithm to compute minimal solutions. We implemented the framework in a DBMS-basedprototype; and we report experimental results that confirm its good scalability and …,Proceedings of the VLDB Endowment,2013,83
Katara: A data cleaning system powered by knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Classical approaches to clean data have relied on using integrity constraints;statistics; or machine learning. These approaches are known to be limited in the cleaningaccuracy; which can usually be improved by consulting master data and involving experts toresolve ambiguity. The advent of knowledge bases KBs both general-purpose and withinenterprises; and crowdsourcing marketplaces are providing yet more opportunities toachieve higher accuracy at a larger scale. We propose KATARA; a knowledge base andcrowd powered data cleaning system that; given a table; a KB; and a crowd; interprets tablesemantics to align it with the KB; identifies correct and incorrect data; and generates top-kpossible repairs for incorrect data. Experiments show that KATARA can be applied tovarious datasets and KBs; and can efficiently annotate data and suggest possible repairs.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,63
++ Spicy: an Open-Source Tool for Second-Generation Schema Mapping and Data Exchange,Bruno Marnette; Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,ABSTRACT Recent results in schema-mapping and data-exchange research may beconsidered the starting point for a new generation of systems; capable of dealing with asignificantly larger class of applications. In this paper we demonstrate the first of thesesecond-generation systems; called++ SPiCY. We introduce a number of scenarios from avariety of data management tasks; such as data fusion; data cleaning; and ETL; and showhow; based on the system; schema mappings and data exchange techniques can be veryeffectively applied to these contexts. We compare++ SPiCY to the previous generations oftools; to show that this is much-needed advancement in the field.,PVLDB,2011,62
Probabilistic models to reconcile complex data from inaccurate data sources,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several techniques have been developed to extract and integrate data from websources. However; web data are inherently imprecise and uncertain. This paper addressesthe issue of characterizing the uncertainty of data extracted from a number of inaccuratesources. We develop a probabilistic model to compute a probability distribution for theextracted values; and the accuracy of the sources. Our model considers the presence ofsources that copy their contents from other sources; and manages the misleadingconsensus produced by copiers. We extend the models previously proposed in the literatureby working on several attributes at a time to better leverage all the available evidence. Wealso report the results of several experiments on both synthetic and real-life data to show theeffectiveness of the proposed approach.,International Conference on Advanced Information Systems Engineering,2010,60
Clip: a visual language for explicit schema mappings,Alessandro Raffio; Daniele Braga; Stefano Ceri; Paolo Papotti; Mauricio A Hernandez,Many data integration solutions in the market today include tools for schema mapping; tohelp users visually relate elements of different schemas. Schema elements are connectedwith lines; which are interpreted as mappings; ie high-level logical expressions capturing therelationship between source and target data-sets; these are compiled into queries andprograms that convert source-side data instances into target-side instances. This paperdescribes Clip; an XML schema mapping tool distinguished from existing tools in thatmappings explicitly specify structural transformations in addition to value couplings. Sinceclip maps hierarchical XML schemas; lines appear naturally nested. We describe thetransformation semantics associated with our" lines" and how they combine to formmappings that are more expressive than those generated by Clio; a well-known mapping …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,59
Core schema mappings,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich,Abstract Research has investigated mappings among data sources under two perspectives.On one side; there are studies of practical tools for schema mapping generation; these focuson algorithms to generate mappings based on visual specifications provided by users. Onthe other side; we have theoretical researches about data exchange. These study how togenerate a solution-ie; a target instance-given a set of mappings usually specified as tuplegenerating dependencies. However; despite the fact that the notion of a core of a dataexchange solution has been formally identified as an optimal solution; there are yet nomapping systems that support core computations. In this paper we introduce several newalgorithms that contribute to bridge the gap between the practice of mapping generation andthe theory of data exchange. We show how; given a mapping scenario; it is possible to …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,58
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,52
Scalable data exchange with functional dependencies,Bruno Marnette; Giansalvatore Mecca; Paolo Papotti,Abstract The recent literature has provided a solid theoretical foundation for the use ofschema mappings in data-exchange applications. Following this formalization; newalgorithms have been developed to generate optimal solutions for mapping scenarios in ahighly scalable way; by relying on SQL. However; these algorithms suffer from a seriousdrawback: they are not able to handle key constraints and functional dependencies on thetarget; ie; equality generating dependencies (egds). While egds play a crucial role in thegeneration of optimal solutions; handling them with first-order languages is a difficultproblem. In fact; we start from a negative result: it is not always possible to compute solutionsfor scenarios with egds using an SQL script. Then; we identify many practical cases in whichthis is possible; and develop a best-effort algorithm to do this. Experimental results show …,Proceedings of the VLDB Endowment,2010,50
Discovering denial constraints,Xu Chu; Ihab F Ilyas; Paolo Papotti,Abstract Integrity constraints (ICs) provide a valuable tool for enforcing correct applicationsemantics. However; designing ICs requires experts and time. Proposals for automaticdiscovery have been made for some formalisms; such as functional dependencies and theirextension conditional functional dependencies. Unfortunately; these dependencies cannotexpress many common business rules. For example; an American citizen cannot have lowersalary and higher tax rate than another citizen in the same state. In this paper; we tackle thechallenges of discovering dependencies in a more expressive integrity constraint language;namely Denial Constraints (DCs). DCs are expressive enough to overcome the limits ofprevious languages and; at the same time; have enough structure to allow efficient discoveryand application in several scenarios. We lay out theoretical and practical foundations for …,Proceedings of the VLDB Endowment,2013,46
Data exchange with data-metadata translations,Mauricio A Hernández; Paolo Papotti; Wang-Chiew Tan,Abstract Data exchange is the process of converting an instance of one schema into aninstance of a different schema according to a given specification. Recent data exchangesystems have largely dealt with the case where the schemas are given a priori andtransformations can only migrate data from the first schema to an instance of the secondschema. In particular; the ability to perform data-metadata translations; transformation inwhich data is converted into metadata or metadata is converted into data; is largely ignored.This paper provides a systematic study of the data exchange problem with data-metadatatranslation capabilities. We describe the problem; our solution; implementation andexperiments. Our solution is a principled and systematic extension of the existing dataexchange framework; all the way from the constructs required in the visual interface to …,Proceedings of the VLDB Endowment,2008,36
Mapping and cleaning,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,We address the challenging and open problem of bringing together two crucial activities indata integration and data quality; ie; transforming data using schema mappings; and fixingconflicts and inconsistencies using data repairing. This problem is made complex by severalfactors. First; schema mappings and data repairing have traditionally been considered asseparate activities; and research has progressed in a largely independent way in the twofields. Second; the elegant formalizations and the algorithms that have been proposed forboth tasks have had mixed fortune in scaling to large databases. In the paper; we introducea very general notion of a mapping and cleaning scenario that incorporates a wide variety offeatures; like; for example; user interventions. We develop a new semantics for thesescenarios that represents a conservative extension of previous semantics for schema …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,35
Descriptive and Prescriptive Data Cleaning,Anup K. Chalamalla; Ihab Ilyas; Mourad Ouzzani; Paolo Papotti,Abstract Data cleaning techniques usually rely on some quality rules to identify violatingtuples; and then fix these violations using some repair algorithms. Oftentimes; the rules;which are related to the business logic; can only be defined on some target report generatedby transformations over multiple data sources. This creates a situation where the violationsdetected in the report are decoupled in space and time from the actual source of errors. Inaddition; applying the repair on the report would need to be repeated whenever the datasources change. Finally; even if repairing the report is possible and affordable; this would beof little help towards identifying and analyzing the actual sources of errors for futureprevention of violations at the target. In this paper; we propose a system to address thisdecoupling. The system takes quality rules defined over the output of a transformation …,Sigmod,2014,32
Concise and expressive mappings with+ Spicy,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Marcello Buoncristiano,Abstract We introduce the+ Spicy mapping system. The system is based on a number ofnovel algorithms that contribute to increase the quality and expressiveness of mappings.+Spicy integrates the computation of core solutions in the mapping generation process in ahighly efficient way; based on a natural rewriting of the given mappings. This allows for anefficient implementation of core computations using common runtime languages like SQL orXQuery and guarantees very good performances; orders of magnitude better than those ofprevious algorithms. The rewriting algorithm can be applied both to mappings generated bythe system; or to pre-defined mappings provided as part of the input. To do this; the systemwas enriched with a set of expressive primitives; so that+ Spicy is the first mapping systemthat brings together a sophisticate and expressive mapping generation algorithm with an …,Proceedings of the VLDB Endowment,2009,31
Extraction and integration of partially overlapping web sources,Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract We present an unsupervised approach for harvesting the data exposed by a set ofstructured and partially overlapping data-intensive web sources. Our proposal comes withina formal framework tackling two problems: the data extraction problem; to generateextraction rules based on the input websites; and the data integration problem; to integratethe extracted data in a unified schema. We introduce an original algorithm; WEIR; to solvethe stated problems and formally prove its correctness. WEIR leverages the overlappingdata among sources to make better decisions both in the data extraction (by pruning rulesthat do not lead to redundant information) and in the data integration (by reflecting localproperties of a source over the mediated schema). Along the way; we characterize theamount of redundancy needed by our algorithm to produce a solution; and present …,Proceedings of the VLDB Endowment,2013,26
Repeatability and workability evaluation of SIGMOD 2011,Philippe Bonnet; Stefan Manegold; Matias Bjørling; Wei Cao; Javier Gonzalez; Joel Granados; Nancy Hall; Stratos Idreos; Milena Ivanova; Ryan Johnson; David Koop; Tim Kraska; René Müller; Dan Olteanu; Paolo Papotti; Christine Reilly; Dimitris Tsirogiannis; Cong Yu; Juliana Freire; Dennis Shasha,Abstract SIGMOD has offered; since 2008; to verify the experiments published in the papersaccepted at the conference. This year; we have been in charge of reproducing theexperiments provided by the authors (repeatability); and exploring changes to experimentparameters (workability). In this paper; we assess the SIGMOD repeatability process in termsof participation; review process and results. While the participation is stable in terms ofnumber of submissions; we find this year a sharp contrast between the high participationfrom Asian authors and the low participation from American authors. We also find that mostexperiments are distributed as Linux packages accompanied by instructions on how to setupand run the experiments. We are still far from the vision of executable papers.,ACM SIGMOD Record,2011,25
Detecting Data Errors: Where are we and what needs to be done?,Ziawasch Abedjan; Xu Chu; Dong Deng; Raul Castro Fernandez; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker; Nan Tang,Abstract Data cleaning has played a critical role in ensuring data quality for enterpriseapplications. Naturally; there has been extensive research in this area; and many datacleaning algorithms have been translated into tools to detect and to possibly repair certainclasses of errors such as outliers; duplicates; missing values; and violations of integrityconstraints. Since different types of errors may coexist in the same data set; we often need torun more than one kind of tool. In this paper; we investigate two pragmatic questions:(1) arethese tools robust enough to capture most errors in real-world data sets? and (2) what is thebest strategy to holistically run multiple tools to optimize the detection effort? To answerthese two questions; we obtained multiple data cleaning tools that utilize a variety of errordetection techniques. We also collected five real-world data sets; for which we could …,Proceedings of the VLDB Endowment,2016,22
Core schema mappings: Scalable core computations in data exchange,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich,Abstract Research has investigated mappings among data sources under two perspectives.On the one side; there are studies of practical tools for schema mapping generation; thesefocus on algorithms to generate mappings based on visual specifications provided by users.On the other side; we have theoretical researches about data exchange. These study how togenerate a solution–ie; a target instance–given a set of mappings usually specified as tuplegenerating dependencies. Since the notion of a core solution has been formally identified asan optimal solution; it is very important to efficiently support core computations in mappingsystems. In this paper; we introduce several new algorithms that contribute to bridge the gapbetween the practice of mapping generation and the theory of data exchange. We showhow; given a mapping scenario; it is possible to generate an executable script that …,Information Systems,2012,22
Heterogeneous data translation through XML conversion,Paolo Papotti; Riccardo Torlone,In this paper; we illustrate an approach to the translation of Web data betweenheterogeneous formats. This work fits into a larger project whose aim is the development ofa tool for the management of data described according to a large variety of models andformats used on the Web and the automatic translation of schemes and instances from onemodel to another. Data translations operate over XML representations of schemes andinstances and rely on a uniform description of models that we call metamodel. Themetamodel shows structural diversities and dictates the needed transformations. Complextranslations are derived automatically by combining a number of predefined basicprocedures. These procedures perform XML transformations and are implemented bymeans of XML query languages. Practical examples are provided to show the …,J. Web Eng.,2005,22
Schema exchange: Generic mappings for transforming data and metadata,Paolo Papotti; Riccardo Torlone,Abstract In this paper we present and study the problem of schema exchange; a naturalextension of the data exchange problem in which mappings are defined over classes ofsimilar schemas. To this end; we first introduce the notion of schema template; a tool for therepresentation of a set of schemas sharing the same structure. We then define the schemaexchange notion as the problem of:(i) taking a schema that matches a source template; and(ii) generating a new schema for a target template; on the basis of a mapping between thetwo templates defined by means of FO dependencies. This framework allows the definition;once for all; of generic transformations that can be applied to different schemas. A methodfor the generation of a “correct” solution of the schema exchange problem is proposed and anumber of general results are given. We also show how it is possible to generate …,Data & Knowledge Engineering,2009,20
Future locations prediction with uncertain data,Disheng Qiu; Paolo Papotti; Lorenzo Blanco,Abstract The ability to predict future movements for moving objects enables better decisionsin terms of time; cost; and impact on the environment. Unfortunately; future locationprediction is a challenging task. Existing works exploit techniques to predict a tripdestination; but they are effective only when location data are precise (eg; GPS data) andmovements are observed over long periods of time (eg; weeks). We introduce a data miningapproach based on a Hidden Markov Model (HMM) that overcomes these limits andimproves existing results in terms of precision of the prediction; for both the route (ie;trajectory) and the final destination. The model is resistant to uncertain location data; as itworks with data collected by using cell-towers to localize the users instead of GPS devices;and reaches good prediction results in shorter times (days instead of weeks in a …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2013,18
Flint: Google-basing the web,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several Web sites deliver a large number of pages; each publishing data about oneinstance of some real world entity; such as an athlete; a stock quote; a book. Even though itis easy for a human reader to recognize these instances; current search engines areunaware of them. Technologies for the Semantic Web aim at achieving this goal; however;so far they have been of little help in this respect; as semantic publishing is very limited. Wehave developed a system; called Flint; for automatically searching; collecting and indexingWeb pages that publish data representing an instance of a certain conceptual entity. Flinttakes as input a small set of labeled sample pages: it automatically infers a description of theunderlying conceptual entity and then searches the Web for other pages containing datarepresenting the same entity. Flint automatically extracts data from the collected pages …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,17
Temporal rules discovery for web data cleaning,Ziawasch Abedjan; Cuneyt G Akcora; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract Declarative rules; such as functional dependencies; are widely used for cleaningdata. Several systems take them as input for detecting errors and computing a" clean"version of the data. To support domain experts; in specifying these rules; several tools havebeen proposed to profile the data and mine rules. However; existing discovery techniqueshave traditionally ignored the time dimension. Recurrent events; such as persons reported inlocations; have a duration in which they are valid; and this duration should be part of therules or the cleaning process would simply fail. In this work; we study the rule discoveryproblem for temporal web data. Such a discovery process is challenging because of thenature of web data; extracted facts are (i) sparse over time;(ii) reported with delays; and (iii)often reported with errors over the values because of inaccurate sources or non robust …,Proceedings of the VLDB Endowment,2015,16
That's all folks!: llunatic goes open source,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract It is widely recognized that whenever different data sources need to be integratedinto a single target database errors and inconsistencies may arise; so that there is a strongneed to apply data-cleaning techniques to repair the data. Despite this need; databaseresearch has so far investigated mappings and data repairing essentially in isolation.Unfortunately; schema-mappings and data quality rules interact with each other; so thatapplying existing algorithms in a pipelined way--ie; first exchange then data; then repair theresult--does not lead to solutions even in simple settings. We present the Llunatic mappingand cleaning system; the first comprehensive proposal to handle schema mappings anddata repairing in a uniform way. Llunatic is based on the intuition that transforming andcleaning data are different facets of the same problem; unified by their declarative nature …,Proceedings of the VLDB Endowment,2014,16
Supporting the automatic construction of entity aware search engines,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several web sites deliver a large number of pages; each publishing data about oneinstance of some real world entity; such as an athlete; a stock quote; a book. Although it iseasy for a human reader to recognize these instances; current search engines are unawareof them. Technologies for the Semantic Web aim at achieving this goal; however; so far theyhave been of little help in this respect; as semantic publishing is very limited. We havedeveloped a method to automatically search on the web for pages that publish datarepresenting an instance of a certain conceptual entity. Our method takes as input a smallset of sample pages: it automatically infers a description of the underlying conceptual entityand then searches the web for other pages containing data representing the same entity. Wehave implemented our method in a system prototype; which has been used to conduct …,Proceedings of the 10th ACM workshop on Web information and data management,2008,16
Creating nested mappings with Clio,Mauricio A Hernández; Howard Ho; Lucian Popa; Ariel Fuxman; Renee J Miller; Takeshi Fukuda; Paolo Papotti,Schema mappings play a central role in many data integration and data exchangescenarios. In those applications; users need to quickly and correctly specify how datarepresented in one format is converted into a different format. Clio (L. Popa et al.; 2002) is ajoint research project between IBM and the University of Toronto studying the creation;maintenance; and use of schema mappings. There have always been two goals in our workin Clio: 1) the automatic creation of logical assertions that capture the way one or moresource schemas are mapped into a target schema; and 2) the generation of transformationqueries or programs that transform a source data instance into a target data instance.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,16
Lightning fast and space efficient inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which join relational tables on inequality conditions; are used invarious applications. While there have been a wide range of optimization methods for joinsin database systems; from algorithms such as sort-merge join and band join; to variousindices such as B+-tree; R*-tree and Bitmap; inequality joins have received little attentionand queries containing such joins are usually very slow. In this paper; we introduce fastinequality join algorithms. We put columns to be joined in sorted arrays and we usepermutation arrays to encode positions of tuples in one sorted array wrt the other sortedarray. In contrast to sort-merge join; we use space efficient bit-arrays that enableoptimizations; such as Bloom filter indices; for fast computation of the join results. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; and a …,Proceedings of the VLDB Endowment,2015,15
Dataxformer: An interactive data transformation tool,John Morcos; Ziawasch Abedjan; Ihab Francis Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract While syntactic transformations require the application of a formula on the inputvalues; such as unit conversion or date format conversions; semantic transformations; suchas" zip code to city"; require a look-up in some reference data. We recently presentedDataXFormer; a system that leverages Web tables; Web forms; and expert sourcing to covera wide range of transformations. In this demonstration; we present the user-interaction withDataXFormer and show scenarios on how it can be used to transform data and explore theeffectiveness and efficiency of several approaches for transformation discovery; leveragingabout 112 million tables and online sources.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,14
Discovery and correctness of schema mapping transformations,Angela Bonifati; Giansalvatore Mecca; Paolo Papotti; Yannis Velegrakis,Abstract Schema mapping is becoming pervasive in all data transformation; exchange; andintegration tasks. It brings to the surface the problem of differences and mismatches betweenheterogeneous formats and models; respectively; used in source and target databases to bemapped one to another. In this chapter; we start by describing the problem of schemamapping; its background; and technical implications. Then; we outline the early schemamapping systems; along with the new generation of schema mapping tools. Moving from theformer to the latter entailed a dramatic change in the performance of mapping generationalgorithms. Finally; we conclude the chapter by revisiting the query answering techniquesallowed by the mappings; and by discussing useful applications and future and currentdevelopments of schema mapping tools.,*,2011,14
Road to Freedom in Big Data Analytics.,Divy Agrawal; Sanjay Chawla; Ahmed K Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,ABSTRACT The world is fast moving towards a data-driven society where data is the mostvaluable asset. Organizations need to perform very diverse analytic tasks using various dataprocessing platforms. In doing so; they face many challenges; chiefly; platform dependence;poor interoperability; and poor performance when using multiple platforms. We presentRHEEM; our vision for big data analytics over diverse data processing platforms. RHEEMprovides a threelayer data processing and storage abstraction to achieve both platformindependence and interoperability across multiple platforms. In this paper; we discuss ourvision as well as present multiple research challenges that we need to address to achieve it.As a case in point; we present a data cleaning application built using some of the ideas ofRHEEM. We show how it achieves platform independence and the performance benefits …,EDBT,2016,13
Dataxformer: Leveraging the Web for Semantic Transformations.,Ziawasch Abedjan; John Morcos; Michael N Gubanov; Ihab F Ilyas; Michael Stonebraker; Paolo Papotti; Mourad Ouzzani,ABSTRACT Data transformation is a crucial step in data integration. While sometransformations; such as liters to gallons; can be easily performed by applying a formula or aprogram on the input values; others; such as zip code to city; require sifting through arepository containing explicit value mappings. There are already powerful systems thatprovide formulae and algorithms for transformations. However; the automated identificationof reference datasets to support value mapping remains largely unresolved. The Web ishome to millions of tables with many containing explicit value mappings. This is in additionto value mappings hidden behind Web forms. In this paper; we present DataXFormer; atransformation engine that leverages Web tables and Web forms to perform transformationtasks. In particular; we describe an inductive; filter-refine approach for identifying explicit …,CIDR,2015,13
What is the IQ of your Data Transformation System?,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,Abstract Mapping and translating data across different representations is a crucial problemin information systems. Many formalisms and tools are currently used for this purpose; to thepoint that developers typically face a difficult question:" what is the right tool for mytranslation task?" In this paper; we introduce several techniques that contribute to answerthis question. Among these; a fairly general definition of a data transformation system; a newand very efficient similarity measure to evaluate the outputs produced by such a system; anda metric to estimate user efforts. Based on these techniques; we are able to compare a widerange of systems on many translation tasks; to gain interesting insights about theireffectiveness; and; ultimately; about their" intelligence".,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,12
Redundancy-driven web data extraction and integration,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A large number of web sites publish pages containing structured information aboutrecognizable concepts; but these data are only partially used by current applications.Although such information is spread across a myriad of sources; the web scale implies arelevant redundancy. We present a domain independent system that exploits theredundancy of information to automatically extract and integrate data from the Web. Oursolution concentrates on sources that provide structured data about multiple instances fromthe same conceptual domain; eg; financial data; product information. Our proposal is basedon an original approach that exploits the mutual dependency between the data extractionand the data integration tasks. Experiments on a sample of 175;000 pages confirm thefeasibility and quality of the approach.,Procceedings of the 13th International Workshop on the Web and Databases,2010,11
Rheem: Enabling multi-platform task execution,Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawla; Ahmed Elmagarmid; Hossam Hammady; Yasser Idris; Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,Abstract Many emerging applications; from domains such as healthcare and oil & gas;require several data processing systems for complex analytics. This demo paper showcasessystem; a framework that provides multi-platform task execution for such applications. Itfeatures a three-layer data processing abstraction and a new query optimization approachfor multi-platform settings. We will demonstrate the strengths of system by using real-worldscenarios from three different applications; namely; machine learning; data cleaning; anddata fusion.,Proceedings of the 2016 International Conference on Management of Data,2016,10
RuleMiner: Data quality rules discovery,Xu Chu; Ihab F Ilyas; Paolo Papotti; Yin Ye,Integrity constraints (ICs) are valuables tools for enforcing correct application semantics.However; manually designing ICs require experts and time; hence the need for automaticdiscovery. Previous automatic ICs discovery suffer from (1) limited ICs languageexpressiveness; and (2) time-consuming manual verification of discovered ICs. Weintroduce RULEMINER; a system for discovering data quality rules that addresses thelimitations of existing solutions.,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,10
An Approach to Heterogeneous Data Translation based on XML Conversion.,Paolo Papotti; Riccardo Torlone,Abstract. In this paper; we illustrate a preliminary approach to the translation of Web databetween heterogeneous formats. This work fits into a larger project whose aim is thedevelopment of a tool for the management of data described according to a large variety offormats used on the Web and the (semi) automatic translation of schemes and instancesfrom one model to another. Data translations operate over XML representations of instancesand rely on a uniform representation of models that we call metamodel. The metamodelshows structural diversities and dictates the needed transformations. Complex translationcan be derived by combining a number of predefined basic functions performing XMLtransformations expressed in XQuery. Practical examples are provided to show theeffectiveness of the approach.,CAiSE Workshops (1),2004,10
Messing up with BART: error generation for evaluating data-cleaning algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract We study the problem of introducing errors into clean databases for the purpose ofbenchmarking data-cleaning algorithms. Our goal is to provide users with the highestpossible level of control over the error-generation process; and at the same time developsolutions that scale to large databases. We show in the paper that the error-generationproblem is surprisingly challenging; and in fact; NP-complete. To provide a scalablesolution; we develop a correct and efficient greedy algorithm that sacrifices completeness;but succeeds under very reasonable assumptions. To scale to millions of tuples; thealgorithm relies on several non-trivial optimizations; including a new symmetry property ofdata quality constraints. The trade-off between control and scalability is the main technicalcontribution of the paper.,Proceedings of the VLDB Endowment,2015,9
Estimating data integration and cleaning effort,S. Kruse; P. Papotti; F. Naumann,ABSTRACT Data cleaning and data integration have been the topic of intensive research forat least the past thirty years; resulting in a multitude of specialized methods and integratedtool suites. All of them require at least some and in most cases significant human input intheir configuration; during processing; and for evaluation. For managers (and for developersand scientists) it would be therefore of great value to be able to estimate the effort ofcleaning and integrating some given data sets and to know the pitfalls of such an integrationproject in advance. This helps deciding about an integration project using cost/benefitanalysis; budgeting a team with funds and manpower; and monitoring its progress. Further;knowledge of how well a data source fits into a given data ecosystem improves sourceselection. We present an extensible framework for the automatic effort estimation for …,EDBT,2015,9
Exploiting information redundancy to wring out structured data from the web,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A large number of web sites publish pages containing structured information aboutrecognizable concepts; but these data are only partially used by current applications.Although such information is spread across a myriad of sources; the web scale implies arelevant redundancy. We present a domain independent system that exploits theredundancy of information to automatically extract and integrate data from the Web. Oursolution concentrates on sources that provide structured data about multiple instances fromthe same conceptual domain; eg financial data; product information. Our proposal is basedon an original approach that exploits the mutual dependency between the data extractionand the data integration tasks. Experiments confirmed the quality and the feasibility of theapproach.,Proceedings of the 19th international conference on World wide web,2010,9
Clip: a tool for mapping hierarchical schemas,Alessandro Raffio; Daniele Braga; Stefano Ceri; Paolo Papotti; Mauricio A Hernández,Abstract Many data integration solutions in the market today include visual tools for schemamapping. Users connect schema elements with lines that are interpreted as high-levellogical expressions capturing the relationship between source and target data-sets. Theseexpressions are compiled into queries or programs that convert source-side data instancesinto target-side instances. In this demo we showcase Clip; an XML Schema mapping tool.Clip is distinguished from existing tools in that mappings explicitly specify structuraltransformations in addition to value correspondences. We show how Clip's users entermappings by drawing lines and how these lines are translated into XQuery.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,9
Interactive and deterministic data cleaning,Jian He; Enzo Veltri; Donatello Santoro; Guoliang Li; Giansalvatore Mecca; Paolo Papotti; Nan Tang,Abstract We present Falcon; an interactive; deterministic; and declarative data cleaningsystem; which uses SQL update queries as the language to repair data. Falcon does not relyon the existence of a set of pre-defined data quality rules. On the contrary; it encouragesusers to explore the data; identify possible problems; and make updates to fix them.Bootstrapped by one user update; Falcon guesses a set of possible sql update queries thatcan be used to repair the data. The main technical challenge addressed in this paperconsists in finding a set of sql update queries that is minimal in size and at the same timefixes the largest number of errors in the data. We formalize this problem as a search in alattice-shaped space. To guarantee that the chosen updates are semantically correct; Falconnavigates the lattice by interacting with users to gradually validate the set of sql update …,Proceedings of the 2016 International Conference on Management of Data,2016,8
Introduction to the special issue on data quality,Mourad Ouzzani; Paolo Papotti; Erhard Rahm,Abstract Poor data quality in databases; data warehouses; and information systems affectsevery application domain. Many data processing tasks; such as information integration; datasharing; information retrieval; information extraction; and knowledge discovery requirevarious forms of data preparation and consolidation with complex data processingtechniques. These tasks usually assume that the data input contains no missing;inconsistent or incorrect values. This leaves a large gap between the available “dirty” dataand the machinery to effectively process the data for the application purposes. In addition;tasks such as data integration and information extraction may themselves introduce errors inthe data.,Information Systems,2013,8
Method and sytsem for generating nested mapping specifications in a schema mapping formalism and for generating transformation queries based thereon,*,A method and system for generating nested mapping specifications and transformationqueries based thereon. Basic mappings are generated based on source and target schemasand correspondences between elements of the schemas. A directed acyclic graph (DAG) isconstructed whose edges represent ways in which each basic mapping is nestable underany of the other basic mappings. Any transitively implied edges are removed from the DAG.Root mappings of the DAG are identified. Trees of mappings are automatically extractedfrom the DAG; where each tree of mappings is rooted at a root mapping and expresses anested mapping specification. A transformation query is generated from the nested mappingspecification by generating a first query for transforming source data into flat views of thetarget and a second query for nesting flat view data according to the target format …,*,2008,8
DataXFormer: A robust transformation discovery system,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,In data integration; data curation; and other data analysis tasks; users spend a considerableamount of time converting data from one representation to another. For example US dates toEuropean dates or airport codes to city names. In a previous vision paper; we presented theinitial design of DataXFormer; a system that uses web resources to assist in transformationdiscovery. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. In this paper; we presentthe full fledged system along with several extensions. In particular; we present algorithms tofind (i) transformations that entail multiple columns of input data;(ii) indirect transformationsthat are compositions of other transformations;(iii) transformations that are not functions butrather relationships; and (iv) transformations from a knowledge base of public data. We …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,7
Benchmarking the chase,Michael Benedikt; George Konstantinidis; Giansalvatore Mecca; Boris Motik; Paolo Papotti; Donatello Santoro; Efthymia Tsamoura,Abstract The chase is a family of algorithms used in a number of data management tasks;such as data exchange; answering queries under dependencies; query reformulation withconstraints; and data cleaning. It is well established as a theoretical tool for understandingthese tasks; and in addition a number of prototype systems have been developed. Whileindividual chase-based systems and particular optimizations of the chase have beenexperimentally evaluated in the past; we provide the first comprehensive and publiclyavailable benchmark---test infrastructure and a set of test scenarios---for evaluating chaseimplementations across a wide range of assumptions about the dependencies and the data.We used our benchmark to compare chase-based systems on data exchange and queryanswering tasks with one another; as well as with systems that can solve similar tasks …,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,6
Generating concise entity matching rules,Rohit Singh; Vamsi Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration and cleaning. In manyapplications; the users need to understand why two entities are considered a match; whichreveals the need for interpretable and concise EM rules. We model EM rules in the form ofGeneral Boolean Formulas (GBFs) that allows arbitrary attribute matching combined byconjunctions (∨); disjunctions (∧); and negations.(¬) GBFs can generate more conciserules than traditional EM rules represented in disjunctive normal forms (DNFs). We useprogram synthesis; a powerful tool to automatically generate rules (or programs) thatprovably satisfy a high-level specification; to automatically synthesize EM rules in GBFformat; given only positive and negative matching examples. In this demo; attendees willexperience the following features:(1) Interpretability--they can see and measure the …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,5
KATARA: Reliable data cleaning with knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Data cleaning with guaranteed reliability is hard to achieve without accessingexternal sources; since the truth is not necessarily discoverable from the data at hand.Furthermore; even in the presence of external sources; mainly knowledge bases andhumans; effectively leveraging them still faces many challenges; such as aligningheterogeneous data sources and decomposing a complex task into simpler units that can beconsumed by humans. We present K atara; a novel end-to-end data cleaning systempowered by knowledge bases and crowdsourcing. Given a table; a kb; and a crowd; K atara(i) interprets the table semantics wrt the given kb;(ii) identifies correct and wrong data; and(iii) generates top-k possible repairs for the wrong data. Users will have the opportunity toexperience the following features of K atara:(1) Easy specification: Users can define a K …,Proceedings of the VLDB Endowment,2015,5
Automatically building probabilistic databases from the web,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A relevant number of web sites publish structured data about recognizableconcepts (such as stock quotes; movies; restau-rants; etc.). There is a great chance to createapplications that rely on a huge amount of data taken from the Web. We present anautomatic and domain independent system that performs all the steps required to benefitfrom these data: it discovers data intensive web sites containing information about an entityof interest; extracts and integrate the published data; and finally performs a probabilisticanalysis to characterize the impreciseness of the data and the accuracy of the sources. Theresults of the processing can be used to populate a probabilistic database.,Proceedings of the 20th international conference companion on World wide web,2011,5
Redundancy-Driven Web Data Extraction and Integration.,Paolo Papotti; Valter Crescenzi; Paolo Merialdo; Mirko Bronzi; Lorenzo Blanco,*,WebDB,2010,5
Web Data Reconciliation: Models and Experiences,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract An increasing number of web sites offer structured information about recognizableconcepts; relevant to many application domains; such as finance; sport; commercialproducts. However; web data is inherently imprecise and uncertain; and conflicting valuescan be provided by different web sources. Characterizing the uncertainty of web datarepresents an important issue and several models have been recently proposed in theliterature. This chapter illustrates state-of-the-art Bayesan models to evaluate the quality ofdata extracted from the Web and reports the results of an extensive application of the modelson real life web data. Experimental results show that for some applications even simpleapproaches can provide effective results; while sophisticated solutions are needed to obtaina more precise characterization of the uncertainty.,*,2012,4
Wrapper generation for overlapping web sources,Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Exploiting the huge amount of data available on the Web involves the generation ofwrappers to extract data from web pages. We argue that existing approaches for web dataextraction from data-intensive websites miss the opportunities related to the presence ofredundant information on the Web. We propose an innovative approach that aims at pushingfurther the level of automation of existing wrapper generation systems by leveraging theredundancy of data on the Web. An experimental evaluation of the proposed solution showsa relevant improvement for the precision of the extracted data; without a significant loss inthe recall.,Web Intelligence and Intelligent Agent Technology (WI-IAT); 2011 IEEE/WIC/ACM International Conference on,2011,4
Schema exchange: A template-based approach to data and metadata translation,Paolo Papotti; Riccardo Torlone,Abstract In this paper we study the problem of schema exchange; a natural extension of thedata exchange problem to an intensional level. To this end; we first introduce the notion ofschema template; a tool for the representation of a class of schemas sharing the samestructure. We then define the schema exchange notion as the problem of (i) taking a schemathat matches a source template; and (ii) generating a new schema for a target template; onthe basis of a set of dependencies defined over the two templates. This framework allowsthe definition; once for all; of generic transformations that work for several schemas. Amethod for the generation of a “correct” solution of the schema exchange problem isproposed and a number of general results are given. We also show how it is possible togenerate automatically a data exchange setting from a schema exchange solution. This …,International Conference on Conceptual Modeling,2007,4
Fast and scalable inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which is to join relations with inequality conditions; are used invarious applications. Optimizing joins has been the subject of intensive research rangingfrom efficient join algorithms such as sort-merge join; to the use of efficient indices such asB^+ B+-tree; R^* R∗-tree and Bitmap. However; inequality joins have received little attentionand queries containing such joins are notably very slow. In this paper; we introduce fastinequality join algorithms based on sorted arrays and space-efficient bit-arrays. We furtherintroduce a simple method to estimate the selectivity of inequality joins which is then used tooptimize multiple predicate queries and multi-way joins. Moreover; we study an incrementalinequality join algorithm to handle scenarios where data keeps changing. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; a distributed …,The VLDB Journal,2017,3
Big data quality-whose problem is it?,Shazia Sadiq; Paolo Papotti,The increased reliance on data driven enterprise has seen an unprecedented investment inbig data initiatives. Organizations averaged US $8 M in investments in big data-relatedinitiatives and programs in 2014; with 70% of large enterprises and 56% of small andmedium enterprises (SMEs) having already deployed; or planning to deploy; big-dataprojects [1]. As companies intensify their efforts to get value from big data; the growth in theamount of data being managed continues at an exponential rate; leaving organizations witha massive footprint of unexplored; unfamiliar datasets. On February 8th; 2015; a group ofglobal thought leaders from the database research community outlined the grandchallenges in getting value from big data [2]. The key message was the need to develop thecapacity tounderstand how the quality of data affects the quality of the insight we derive …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,3
Benchmarking Data Curation Systems.,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract Data curation includes the many tasks needed to ensure data maintains its valueover time. Given the maturity of many data curation tasks; including data transformation anddata cleaning; it is surprising that rigorous empirical evaluations of research ideas are soscarce. In this work; we argue that thorough evaluation of data curation systems imposesseveral major obstacles that need to be overcome. First; we consider the outputs generatedby a data curation system (for example; an integrated or cleaned database or a set ofconstraints produced by a schema discovery system). To compare the results of differentsystems; measures of output quality should be agreed upon by the community and; sincesuch measures can be quite complex; publicly available implementations of these measuresshould be developed; shared; and optimized. Second; we consider the inputs to the data …,IEEE Data Eng. Bull.,2016,3
Characterizing the uncertainty of web data: models and experiences,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract An increasing number of web sites offer structured information about recognizableconcepts; relevant to many application domains; such as finance; sport; commercialproducts. However; web data is inherently imprecise and uncertain; and conflicting valuescan be provided by different web sources. Characterizing the uncertainty of web datarepresents an important issue and several models have been recently proposed in theliterature. The paper illustrates state-of-the-art Bayesan models to evaluate the quality ofdata extracted from the Web and reports the results of an extensive application of the modelson real life web data. Our experimental results show that for some applications even simpleapproaches can provide effective results; while sophisticated solutions are needed to obtaina more precise characterization of the uncertainty.,Proceedings of the 2011 Joint WICOW/AIRWeb Workshop on Web Quality,2011,3
Method for generating nested mapping specifications in a schema mapping formalism,*,A method for generating nested mapping specifications and transformation queries basedthereon. Basic mappings are generated based on source and target schemas andcorrespondences between elements of the schemas. A directed acyclic graph (DAG) isconstructed whose edges represent ways in which each basic mapping is nestable underany of the other basic mappings. Any transitively implied edges are removed from the DAG.Root mappings of the DAG are identified. Trees of mappings are automatically extractedfrom the DAG; where each tree of mappings is rooted at a root mapping and expresses anested mapping specification.,*,2008,3
Mapping and Cleaning: the Llunatic Way,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract We address the problem of bringing together two crucial activities in dataintegration and data quality; ie; transforming data using schema mappings; and fixingconflicts and inconsistencies using data repairing. This problem is made complex by severalfactors. First; schema mappings and data repairing have traditionally been considered asseparate activities; and research has progressed in a largely independent way in the twofields. Second; the elegant formalizations and the algorithms that have been proposed forboth tasks have had mixed fortune in scaling to large databases. In the paper; we introducea very general notion of a mapping and cleaning scenario that incorporates a wide variety offeatures; like; for example; user interventions. We develop a new semantics for thesescenarios that represents a conservative extension of previous semantics for schema …,*,2014,2
IQ-METER-an evaluation tool for data-transformation systems,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,We call a data-transformation system any system that maps; translates and exchanges dataacross different representations. Nowadays; data architects are faced with a large variety oftransformation tasks; and there is huge number of different approaches and systems thatwere conceived to solve them. As a consequence; it is very important to be able to evaluatesuch alternative solutions; in order to pick up the right ones for the problem at hand. To dothis; we introduce IQ-Meter; the first comprehensive tool for the evaluation of data-transformation systems. IQ-Meter can be used to benchmark; test; and even learn the bestusage of data-transformation tools. It builds on a number of novel algorithms to measure thequality of outputs and the human effort required by a given system; and ultimately measures“how much intelligence” the system brings to the solution of a data-translation task.,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,2
Automatic generation of model translations,Paolo Papotti; Riccardo Torlone,Abstract The translation of information between heterogeneous representations is a longstanding issue. With the large spreading of cooperative applications fostered by the adventof the Internet the problem has gained more and more attention but there are still few andpartial solutions. In general; given an information source; different translations can bedefined for the same target model. In this work; we first identify general properties that “good”translations should fulfill. We then propose novel techniques for the automatic generation ofmodel translations. A translation is obtained by combining a set of basic transformations andthe above properties are verified locally (at the transformation level) and globally (at thetranslation level) without resorting to an exhaustive search. These techniques have beenimplemented in a tool for the management of heterogeneous data models and some …,International Conference on Advanced Information Systems Engineering,2007,2
Automatic techniques for data model translation,Paolo Papotti; Riccardo Torlone,Abstract Distributed information systems deal with different data models and transformationsbetween them are required. In this paper; we present a framework for the automatictranslation of data between heterogeneous representations. Translations operate overformal representations of schemes and data and rely on a uniform description of models thatwe call metamodel. Complex translations of schemes and instances are derivedautomatically by combining a number of predefined basic transformations; which areimplemented by means of query languages.,Proceedings of VLDB 2005 PhD Workshop,2005,2
System and method for checking data for errors,*,A system for checking data for errors; the system comprising a checking module operable tocheck tuples of data stored in a target database for errors; the tuples in the target databaseoriginating from the output of at least one query transformation module which applies aquery transformation to tuples of data from at least one data source an identification moduleoperable to identify a problematic tuple from a data source that produces an error in thetarget database; the identification module being operable to quantify the contribution of theproblematic tuple in producing the error in the target database; and a description generationmodule operable to generate a descriptive query which represents at least one of errorsidentified by the checking module in the target database which are produced by the at leastone query transformation module; and problematic tuples identified in a data source by …,*,2016,1
BARt in Action: Error Generation and Empirical Evaluations of Data-Cleaning Systems,Donatello Santoro; Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti,Abstract Repairing erroneous or conflicting data that violate a set of constraints is animportant problem in data management. Many automatic or semi-automatic data-repairingalgorithms have been proposed in the last few years; each with its own strengths andweaknesses. Bart is an open-source error-generation system conceived to support thoroughexperimental evaluations of these data-repairing systems. The demo is centered aroundthree main lessons. To start; we discuss how generating errors in data is a complex problem;with several facets. We introduce the important notions of detectability and repairability of anerror; that stand at the core of Bart. Then; we show how; by changing the features of errors; itis possible to influence quite significantly the performance of the tools. Finally; we concretelyput to work five data-repairing algorithms on dirty data of various kinds generated using …,Proceedings of the 2016 International Conference on Management of Data,2016,1
Systems and methods for data integration,*,A computer implemented method for integrating data into a target database may include:providing a plurality of source databases which each may include a relational schema anddata for integration into the target database; generating at least one complexity model basedon the relational schema and data of each source database; each complexity modelindicating at least one inconsistency between two or more of the data sources which may berequire to be resolved to integrate the data from the data sources into the target database;and generating an effort model that may include an effort value for each inconsistencyindicated by each complexity model; each effort value indicating at least one of a time periodand a financial cost to resolve the inconsistency to integrate data from the data sources intothe target database.,*,2016,1
Error Generation for Evaluating Data Cleaning Algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,ABSTRACT We address the problem of generating errors within clean databases for thepurpose of benchmarking data-cleaning and data-repairing algorithms. Our goal is toprovide users with the highest possible level of control over the error generation processand at the same time develop a solution that scales to large databases. This is challenging;because the error generation problem is NP-complete. The main technical contribution ofthis paper is to develop an efficient PTIME algorithm which sacrifices completeness in anintelligent fashion that allows us to succeed under reasonable assumptions. However;scaling to databases with millions of tuples requires additional non-trivial optimizationsincluding exploiting a symmetry property of data quality constraints.,*,2015,1
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1
A Short History of Schema Mapping Systems.,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,There are many applications that need to exchange; correlate; and integrate heterogenousdata sources. These information integration tasks have long been identified as importantproblems and unifying theoretical frameworks have been advocated by databaseresearchers [5]. To solve these problems; a fundamental requirement is that of manipulatingmappings among data sources. The application developer is typically given two schemas–one called the source schema; the other called the target schema–that can be based ondifferent models; technologies; and rules. Mappings; also called schema mappings; areexpressions that specify how an instance of the source repository should be translated intoan instance of the target repository. In order to be useful in practical applications; theyshould have an executable implementation–for example; by means of SQL queries or …,SEBD,2012,1
Schema Mapping and Data Exchange Tools: Time for the Golden Age,Giansalvatore Mecca; Paolo Mecca,Abstract In the last 10 years; schema mapping management has become an importantresearch area in data transformation; exchange; and integration systems. The reasons for itssuccess can be found in the declarative nature of its building block (thus enabling cleansemantics and easy to use design tools) paired with the efficiency and modularity in thedeployment step. In this paper we sketch a line of evolution in schema-mappings and dataexchange systems; through what we identify as three main ages. We start presenting thefoundations of schema mapping tools and the first tools aimed at translating data from asource to a target schema in the first; heroic age. We then discuss the silver age; whenschema mapping tools have grown their way into complex systems and have beentranslated into both commercial and open-source tools. Finally; we show how recent …,it-Information Technology,2012,1
Contextual Data Extraction and Instance-Based Integration,Paolo Papotti; Paolo Merialdo; Lorenzo Blanco; Valter Crescenzi,*,Proceedings of the First International Workshop on Searching and Integrating New Web Data Sources - Very Large Data Search (VLDS),2011,1
Data extraction and integration from imprecise web sources,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Blanco; L; Bronzi; M; Crescenzi; V; Merialdo; P & Papotti; P 2009; Data extraction and integrationfrom imprecise web sources (Extended abstract). in 17th Italian Symposium on Advanced DatabaseSystems; SEBD 2009. pp. 229-236; 17th Italian Symposium on Advanced DatabaseSystems; SEBD 2009; Camogli; Genova; Italy; 6/21/09 … Blanco L; Bronzi M; Crescenzi V; MerialdoP; Papotti P. Data extraction and integration from imprecise web sources (Extendedabstract). In 17th Italian Symposium on Advanced Database Systems; SEBD 2009. 2009. p.229-236 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2018 Elsevier BV.,17th Italian Symposium on Advanced Database Systems; SEBD 2009,2009,1
Schema Mappings: From Data Translation to Data Cleaning,Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Abstract Schema mapping management is an important research area in datatransformation; integration; and cleaning systems. The reasons for its success can be foundin the declarative nature of its building block (thus enabling clean semantics and easy to usedesign tools) paired with the efficiency and modularity in the deployment step. In this chapterwe cover the evolution of schema-mappings through what we identify as three main ages.We start presenting the foundations of schema mapping tools and the first tools aimed attranslating data from a source to a target schema in the first; heroic age. We then discuss thesilver age; when schema mapping tools have grown their way into complex systems andhave been translated into both commercial and open-source tools. Finally; we show howrecent results in schema-mapping are stimulating a third; golden age; with novel research …,*,2018,*
Synthesizing entity matching rules by examples,Rohit Singh; Venkata Vamsikrishna Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration. We study how to synthesizeentity matching rules from positive-negative matching examples. The core of our solution isprogram synthesis; a powerful tool to automatically generate rules (or programs) that satisfya given high-level specification; via a predefined grammar. This grammar describes aGeneral Boolean Formula (GBF) that can include arbitrary attribute matching predicatescombined by conjunctions (∧); disjunctions (∨) and negations (¬); and is expressiveenough to model EM problems; from capturing arbitrary attribute combinations to handlingmissing attribute values. The rules in the form of GBF are more concise than traditional EMrules represented in Disjunctive Normal Form (DNF). Consequently; they are moreinterpretable than decision trees and other machine learning algorithms that output deep …,Proceedings of the VLDB Endowment,2017,*
Robust Discovery of Positive and Negative Rules in Knowledge-Bases,Stefano Ortona; Vamsi Meduri; Paolo Papotti,Abstract We present RuDiK; a system for the discovery of declarative rules over knowledge-bases (KBs). RuDiK discovers rules that express positive relationships between entities;such as “if two persons have the same parent; they are siblings”; and negative rules; ie;patterns that identify contradictions in the data; such as “if two persons are married; onecannot be the child of the other”. While the former class infers new facts in the KB; the latterclass is crucial for other tasks; such as detecting erroneous triples in data cleaning; or thecreation of negative examples to bootstrap learning algorithms. The system is designed to:(i)enlarge the expressive power of the rule language to obtain complex rules and widecoverage of the facts in the KB;(ii) discover approximate rules (soft constraints) to be robustto errors and incompleteness in the KB;(iii) use disk-based algorithms; effectively …,*,2017,*
Errata for Lightning Fast and Space Efficient Inequality Joins (PVLDB 8 (13): 2074--2085),Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract This is in response to recent feedback from some readers; which requires someclarifications regarding our IEJ oin algorithm published in [1]. The feedback revolves aroundfour points:(1) a typo in our illustrating example of the join process;(2) a naming error for theindex used by our algorithm to improve the bit array scan;(3) the sort order used in ouralgorithms; and (4) a missing explanation on how duplicates are handled by our self joinalgorithm.,Proceedings of the VLDB Endowment,2017,*
Optimized inequality join method,*,The optimized inequality join method is a method for joining relational tables on inputinequality conditions. The optimized inequality join method is a relatively fast inequality joinmethod using permutation arrays to store positional information for sorted attributed values.Additionally; space efficient bit arrays are used to enable optimization; such as Bloom filterindices; thus providing faster computation of the join results. The method may be used; forexample; for joining various inequalities associated with a variety of measuredenvironmental conditions for raising an alarm when certain conditions are met.,*,2017,*
Towards User-Aware Rule Discovery,Venkata Vamsikrishna Meduri; Paolo Papotti,Abstract Rule discovery is a challenging but inevitable process in several data centricapplications. The main challenges arise from the huge search space that needs to beexplored; and from the noise in the data; which makes the mining results hardly useful.While existing state-of-the-art systems pose the users at the beginning and the end of themining process; we argue that this paradigm must be revised and new rule miningalgorithms should be developed to let the domain experts interact during the discoveryprocess. We discuss how new systems that embrace this approach overcome currentlimitations and ultimately result in shorter time and smaller user effort for rule discovery.,*,2017,*
Interactive Data Repairing: the FALCON Dive,Enzo Veltri; Donatello Santoro; Giansalvatore Mecca; Paolo Papotti; Jian He; Gouliang Li; Nan Tang,Abstract. In this paper we discuss Falcon; an interactive; deterministic; and declarative datacleaning system. Unlike traditional rule-based system; Falcon does not rely on the existenceof a set of pre-defined data quality rules; but it encourages users to explore the data; identifypossible problems; and make updates to fix them. The main technical challenge consists infinding a set of rules; expressed as sql update queries; that are semantically correct and thatfixes the largest number of errors in the data. Falcon navigates the lattice by interacting withusers to gradually checking the correctness of a set of rules. We have conducted extensiveexperiments using both real-world and synthetic datasets to show that Falcon can effectivelycommunicate with users in data repairing.,25th Italian Symposium on Advanced Database Systems; SEBD 2017,2017,*
Data quality between promises and results,Paolo Papotti,Improving the quality of data is a crucial task for business; health; and scientific data. Severaldata cleaning algorithms have been translated into tools to identify and repair data errorssuch as outlying values; duplicate records; typos; missing values; and violations of rules ingeneral [1];[2];[3];[4].,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,*
Methods for Identifying Denial Constraints,*,Computer implemented methods for identifying denial constraints are provided herein. Thedenial constraints can be used with a database schema R. A predicate space P can begenerated for an instance I in the schema R. An evidence set EviI can be generated. Theevidence set EviI can include sets of satisfied predicates in the predicate space P for eachinstance I. A minimal set of predicates can be identified for the evidence set EviI. Valid denialconstraints can be identified from the minimal set by inverting the predicates in the minimalset.,*,2016,*
A System for Big Data Analytics over Diverse Data Processing Platforms,Jorge Quiane; Divy Agrawal; Sanjay Chawla; Ahmed Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Nan Tang; Mohammed Zaki,Data analytics is at the core of any organization that wants to obtain measurable value fromits growing data assets. Data analytic tasks may range from simple to extremely complexpipelines; such as data extraction; transformation and loading; online analytical processing;graph processing; and machine learning (ML). Following the dictum “one size does not fitall”; academia and industry have embarked on a race of developing data processingplatforms for supporting all of these different tasks; eg; DBMSs and MapReduce-likesystems. Semantic completeness; high performance and scalability are key objectives ofsuch platforms. While there have been major achievements in these objectives; users arestill faced with many road-blocks. MOTIVATING EXAMPLE The first roadblock is thatapplications are tied to a single processing platform; making the migration of an …,Qatar Foundation Annual Research Conference Proceedings,2016,*
Data Science at QCRI,Divy Agrawal; Laure Berti; Hossam Hammady; Prasenjit Mitra; Mourad Ouzzani; Paolo Papotti; Jorge Quiane Ruiz; Nan Tang; Yin Ye; Si Yin; Mohamed Zaki," The Data Analytics group at QCRI has embarked on an ambitious endeavor to become apremiere world-class research group in Data Science by tackling diverse research topicsrelated to information extraction; data quality; data profiling; data integration; and datamining. We will present our ongoing projects to overcome different challenges encounteredin Big Data Curation; Big Data Fusion; and Big Data Analytics.(1) Big Data Curation: Due tocomplex processing and transformation layers; data errors proliferate rapidly and sometimesin an uncontrolled manner; thus compromising the value of information and impacting dataanalysis and decision making. While data quality problems can have crippling effects and noend-to-end off-the-shelf solutions to (semi-) automate error detection and correction existed;we built a commodity platform; NADEEF that can be easily customized and deployed to …,Qatar Foundation Annual Research Conference,2014,*
An Overview of the Llunatic System.,Floris Geerts; Giansalvatore Mecca; Paolo Papotti; Donatello Santoro,Data transformation and data cleaning are two very important research and applicationproblems. Data transformation; or data exchange [7]; relies on declarative schema mappingsto translate and integrate data coming from one or more source schemas into a differenttarget schema. Data cleaning; or data repairing [8]; uses declarative dataquality rules inorder to detect and remove errors and inconsistencies from the data. It is widely recognisedthat whenever mappings among different sources are in place; there is a strong need toclean and repair data. Despite this need; database research has so far investigated schemamappings and data repairing essentially in isolation. We present the LLUNATIC [11; 12]mapping and cleaning system; the first comprehensive proposal to handle schemamappings and data repairing in a uniform way. LLUNATIC is based on the intuition that …,SEBD,2014,*
On the Quality and Effectiveness of Data Transformation Systems,Giansalvatore Mecca; Paolo Papotti; Salvatore Raunich; Donatello Santoro,The problem of translating data among heterogeneous representations is a long standingissue in the IT industry and in database research. The first data translation systems dateback to the seventies. In these years; many different proposals have emerged to alleviate theburden of manually expressing complex transformations among different repositories; sothat today we have a very broad class of systems; ranging from schemamatching to schema-mappings and data-exchange; from data-integration to ETL; from object-relational mappingto data-fusion; and data-cleaning. These systems differ under many perspectives. There arevery procedural and very expressive systems; like those used in ETL. There are moredeclarative; but somehow less expressive schema-mapping systems. Some commercialsystems are essentially graphical user interfaces for defining XSLT queries. Others; like …,Proceedings of the 21st Italian Symposium on Database Systems (Sistemi Evoluti per Basi di Dati),2013,*
Flint: From Web Pages to Probabilistic Semantic Data,Paolo Merialdo; Paolo Papotti,Abstract A large and increasing number of web sites publish structured data aboutrecognizable concepts (such as stock quotes; movies; restaurants). The great chance tocreate applications that rely on the huge amount of data taken from these sites has beendiscussed for more than a decade now; but in practice; only a small fraction of suchinformation is currently used. The main reason is that extracting and integrating web data ofgood quality is an expensive task; which often requires human intervention. In this chapter;we present the main results of the Flint project; which aims at developing automatic anddomain-independent tools to perform all the steps required to benefit from Web data:discovering data-intensive web sites containing information about entities of interest;extracting and integrating the published data; and performing a probabilistic analysis to …,*,2012,*
Emerging Applications for Schema Mappings,Paolo Papotti,There are many different classes of applications that need to exchange; correlate; orintegrate data. In light of this; the need of a unifying theoretical framework for all theseapplications has been advocated by database researchers [4]. In particular; data exchangeand schema mappings yield a principled approach to improving data management. Dataexchange addresses the common problem of providing unified and transparent view to acollection of data stored in autonomous and heterogeneous data sources. The unified viewis achieved through a target schema; and is realized through some form of mapping from thesource repositories to a target; materialized database [10]. In contrast; in data integrationsetting the same objective is achieved through a virtualization mechanism based onquerying of the target (mediated) schema [15]. Mappings; also called schema mappings …,SEBD,2011,*
Proceedings-International Conference on Data Engineering: Preface,Melanie Herschel; Paolo Papotti; Carlo A Curino,Skip to main content Arizona State University Logo …,Unknown Journal,2011,*
Probabilistic reconciliation of records from inaccurate web sources,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Web data are inherently imprecise and uncertain. This paper addresses the issue ofcharacterizing the uncertainty of data extracted from a number of inaccurate sources. Wedevelop a probabilistic model to compute a probability distribution for the extracted values;and the accuracy of the sources. Our model considers the presence of sources that copytheir contents from other sources; and manages the misleading consensus produced bycopiers. We extend the models previously proposed in the literature by working on severalattributes at a time to better leverage all the available evidence of copying.,18th Italian Symposium on Advanced Database Systems; SEBD 2010,2010,*
Searching entities on the web by sample,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Blanco; L; Crescenzi; V; Merialdo; P & Papotti; P 2008; Searching entities on the web bysample. in SEBD 2008 - Proceedings of the 16th Italian Symposium on Advanced DatabaseSystems. pp. 406-413; 16th Italian Symposium on Advanced Database Systems; SEBD2008; Mondello; Palermo; Italy; 6/22/08 … Blanco L; Crescenzi V; Merialdo P; Papotti P. Searchingentities on the web by sample. In SEBD 2008 - Proceedings of the 16th Italian Symposium onAdvanced Database Systems. 2008. p. 406-413 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,16th Italian Symposium on Advanced Database Systems; SEBD 2008,2008,*
On the schema exchange problem,Paolo Papotti; Riccardo Torlone,Papotti; P & Torlone; R 2007; On the schema exchange problem. in SEBD 2007 - Proceedingsof the 15th Italian Symposium on Advanced Database Systems. pp. 439-446; 15th Italian Symposiumon Advanced Database Systems; SEBD 2007; Torre Canne di Fasano; BR; Italy; 6/17/07 …Papotti P; Torlone R. On the schema exchange problem. In SEBD 2007 - Proceedings of the15th Italian Symposium on Advanced Database Systems. 2007. p. 439-446 … Powered byPure; Scopus & Elsevier Fingerprint Engine™ © 2017 Elsevier BV.,15th Italian Symposium on Advanced Database Systems; SEBD 2007,2007,*
Translation of web data with chameleon,Paolo Papotti; Riccardo Torlone,Abstract In this paper; we illustrate the design; development and test of a tool; calledChameleon; for the automatic translation of information between heterogeneous Web data.Translations operate over XML representations of schemes and data and rely on a uniformdescription of models that we call metamodel. The metamodel shows structural diversitiesand dictates the needed transformations. Complex translations of schemes and instancesare derived automatically by combining a number of predefined basic procedures; which areimplemented by means of XML query languages. Practical examples of use of the tool andsome experimental results are provided to show the effectiveness of the approach.,13th Italian Symposium on Advanced Database Systems; SEBD 2005,2005,*
Reuse of Schema Mappings for Data Transformation Design,Paolo Atzeni; Luigi Bellomarini; Paolo Papotti; Riccardo Torlone,ABSTRACT The definition of data transformations between heterogeneous schemas is acritical activity of any database application. Currently; automated tools provide high levelinterfaces for the discovery of correspondences between elements of schemas; buttransformations (ie; schema mappings) need to be manually specified every time fromscratch; even if the problem at hand is similar to one that has already been addressed.Schema mappings are precisely defined over a pair of schemas and cannot directly bereused on different scenarios. We tackle this challenge by introducing a framework togeneralize schema mappings as meta-mappings: formalisms that describe transformationsbetween generic data structures called meta-schemas. Meta-mappings describe; in abstractterms; database transformations and are suitable to capture enterprise knowledge from …,*,*,*
Letter from the Special Issue Editors,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renee J Miller; Paolo Papotti; Donatello Santoro,The prevalence of large volumes and varieties of accessible data is profoundly changing theway business; government and individuals approach decision making. Organizational bigdata investment strategies regarding what data to collect; clean; integrate; and analyze aretypically driven by some notion of perceived value. However; the value of the data isinescapably tied to the underlying quality of the data. Although for big data; value and qualitymay be correlated; they are conceptually different. For example; a complete and accurate listof the books read on April 1; 2016 by the special editors of this issue may not have muchvalue to anyone else. Whereas even partially complete and somewhat noisy GPS data frompublic transport vehicles may have a high perceived value for transport engineers and urbanplanners. In spite of significant advances in storage and compute capabilities; the time to …,*,*,*
Discovering Data Transformations in Web Resources,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract. In data integration; data curation; and other data analysis tasks; users spend aconsiderable amount of time converting data from one representation to another. Forexample US dates to European dates or airport codes to city names. In practice; datascientists have to code most of the transformation tasks manually; search for the appropriatedictionaries; and involve domain experts. In a previous vision paper; we presented the initialdesign of DataXFormer; a system that uses web resources to assist in transformationdiscovery [1]. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. We demonstrated thesystem at SIGMOD 2015 and deployed an open version of the system; which helped us toincrease our initial workload from 50 to 120 transformations [3]. At the same time we …,*,*,*
Universita degli Studi di Roma Tre,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,ABSTRACT An increasing number of web sites publish pages containing structuredinformation about recognizable concepts; relevant to specific application domains-such asfinance; sport; products. Although such information is spread across a myriad of sources; theweb scale implies a relevant redundancy; which is apparent both at the intensional level(many sources share a core set of attributes); and at the extensional level (many instancesoccur in a number of sources). The paper investigates novel; domain independenttechniques that exploit the redundancy of information among these sources to automaticallyextract and integrate data from the Web. The presented techniques have been implementedin a working prototype; which has been used to conduct a number of experiments on bothsynthetic and real-life web sites. Experimental results confirm the feasibility of the …,*,*,*
