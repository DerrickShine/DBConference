Learning to map between ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Abstract Ontologies play a prominent role on the Semantic Web. They make possible thewidespread publication of machine understandable data; opening myriad opportunities forautomated information processing. However; because of the Semantic Web's distributednature; data on it will inevitably come from many different ontologies. Information processingacross ontologies is not possible without knowing the semantic mappings between theirelements. Manually finding such mappings is tedious; error-prone; and clearly not possibleat the Web scale. Hence; the development of tools to assist in the ontology mapping processis crucial to the success of the Semantic Web. We describe glue; a system that employsmachine learning techniques to find such mappings. Given two ontologies; for each conceptin one ontology glue finds the most similar concept in the other ontology. We give well …,Proceedings of the 11th international conference on World Wide Web,2002,1319
Crowdsourcing systems on the world-wide web,Anhai Doan; Raghu Ramakrishnan; Alon Y Halevy,As is typical for an emerging area; this effort has appeared under many names; including peerproduction; user-powered systems; user-generated content; collaborative systems; communitysystems; social systems; social search; social media; collective intelligence; wikinomics; crowdwisdom; smart mobs; mass collaboration; and human computation. The topic has been discussedextensively in books; popular press; and academia. 1;5;15;23;29;35 But this body of work hasconsidered mostly efforts in the physical world. 23;29;30 Some do consider crowdsourcing systemson the Web; but only certain system types 28;33 or challenges (for example; how to evaluateusers 12 ) … This survey attempts to provide a global picture of crowdsourcing systems on theWeb. We define and classify such systems; then describe a broad sample of systems. The sampleranges from relatively simple well-established systems such as reviewing books to …,Communications of the ACM,2011,1291
Reconciling schemas of disparate data sources: A machine-learning approach,AnHai Doan; Pedro Domingos; Alon Y Halevy,Abstract A data-integration system provides access to a multitude of data sources through asingle mediated schema. A key bottleneck in building such systems has been the laboriousmanual construction of semantic mappings between the source schemas and the mediatedschema. We describe LSD; a system that employs and extends current machine-learningtechniques to semi-automatically find such mappings. LSD first asks the user to provide thesemantic mappings for a small set of data sources; then uses these mappings together withthe sources to train a set of learners. Each learner exploits a different type of informationeither in the source schemas or in their data. Once the learners have been trained; LSDfinds semantic mappings for a new data source by applying the learners; then combiningtheir predictions using a meta-learner. To further improve matching accuracy; we extend …,ACM Sigmod Record,2001,996
Learning to match ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Robin Dhamankar; Pedro Domingos; Alon Halevy,Abstract. On the Semantic Web; data will inevitably come from many different ontologies;and information processing across ontologies is not possible without knowing the semanticmappings between them. Manually finding such mappings is tedious; error-prone; andclearly not possible on the Web scale. Hence the development of tools to assist in theontology mapping process is crucial to the success of the Semantic Web. We describeGLUE; a system that employs machine learning techniques to find such mappings. Giventwo ontologies; for each concept in one ontology GLUE finds the most similar concept in theother ontology. We give well-founded probabilistic definitions to several practical similaritymeasures and show that GLUE can work with all of them. Another key feature of GLUE isthat it uses multiple learning strategies; each of which exploits well a different type of …,The VLDB Journal,2003,618
Semantic integration research in the database community: A brief survey,AnHai Doan; Alon Y Halevy,Abstract Semantic integration has been a long-standing challenge for the databasecommunity. It has received steady attention over the past two decades; and has nowbecome a prominent area of database research. In this article; we first review databaseapplications that require semantic integration and discuss the difficulties underlying theintegration process. We then describe recent progress and identify open research issues.We focus in particular on schema matching; a topic that has received much attention in thedatabase community; but also discuss data matching (for example; tuple deduplication) andopen issues beyond the match discovery context (for example; reasoning with matches;match verification and repair; and reconciling inconsistent data values). For previoussurveys of database research on semantic integration; see Rahm and Bernstein (2001); …,AI magazine,2005,617
Ontology matching: A machine learning approach,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Summary This chapter studies ontology matching: the problem of finding the semanticmappings between two given ontologies. This problem lies at the heart of numerousinformation processing applications. Virtually any application that involves multipleontologies must establish semantic mappings among them; to ensure interoperability.Examples of such applications arise in myriad domains; including e-commerce; knowledgemanagement; e-learning; information extraction; bio-informatics; web services; and tourism(see Part D of this book on ontology applications). Despite its pervasiveness; today ontologymatching is still largely conducted by hand; in a labor-intensive and error-prone process.The manual matching has now become a key bottleneck in building large-scale informationmanagement systems. The advent of technologies such as the WWW; XML; and the …,*,2004,598
iMAP: discovering complex semantic matches between database schemas,Robin Dhamankar; Yoonkyong Lee; AnHai Doan; Alon Halevy; Pedro Domingos,Abstract Creating semantic matches between disparate data sources is fundamental tonumerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. Todate; however; virtually all of these works deal only with one-to-one (1-1) matches; such asaddress= location. They do not consider the important class of more complex matches; suchas address= concat (city; state) and room-pric= room-rate*(1+ tax-rate). We describe theiMAP system which semi-automatically discovers both 1-1 and complex matches. iMAPreformulates schema matching as a search in an often very large or infinite match space. Tosearch effectively; it employs a set of searchers; each discovering specific types of complexmatches. To further improve matching accuracy; iMAP exploits a variety of domain …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,531
Corpus-based schema matching,Jayant Madhavan; Philip A Bernstein; AnHai Doan; Alon Halevy,Schema matching is the problem of identifying corresponding elements in different schemas.Discovering these correspondences or matches is inherently difficult to automate. Pastsolutions have proposed a principled combination of multiple algorithms. However; thesesolutions sometimes perform rather poorly due to the lack of sufficient evidence in theschemas being matched. In this paper we show how a corpus of schemas and mappingscan be used to augment the evidence about the schemas being matched; so they can bematched better. Such a corpus typically contains multiple schemas that model similarconcepts and hence enables us to learn variations in the elements and their properties. Weexploit such a corpus in two ways. First; we increase the evidence about each element beingmatched by including evidence from similar elements in the corpus. Second; we learn …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,459
Principles of data integration,AnHai Doan; Alon Halevy; Zachary Ives,Principles of Data Integration is the first comprehensive textbook of data integration;covering theoretical principles and implementation issues as well as current challengesraised by the semantic web and cloud computing. The book offers a range of dataintegration solutions enabling you to focus on what is most relevant to the problem at hand.Readers will also learn how to build their own algorithms and implement their own dataintegration application. Written by three of the most respected experts in the field; this bookprovides an extensive introduction to the theory and concepts underlying today's dataintegration techniques; with detailed; instruction for their application using concreteexamples throughout to explain the concepts. This text is an ideal resource for databasepractitioners in industry; including data warehouse engineers; database system …,*,2012,359
An interactive clustering-based approach to integrating source query interfaces on the deep web,Wensheng Wu; Clement Yu; AnHai Doan; Weiyi Meng,Abstract An increasing number of data sources now become available on the Web; but oftentheir contents are only accessible through query interfaces. For a domain of interest; thereoften exist many such sources with varied coverage or querying capabilities. As an importantstep to the integration of these sources; we consider the integration of their query interfaces.More specifically; we focus on the crucial step of the integration: accurately matching theinterfaces. While the integration of query interfaces has received more attentions recently;current approaches are not sufficiently general:(a) they all model interfaces with flatschemas;(b) most of them only consider 1: 1 mappings of fields over the interfaces;(c) theyall perform the integration in a blackbox-like fashion and the whole process has to berestarted from scratch if anything goes wrong; and (d) they often require laborious …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,328
Learning to match the schemas of data sources: A multistrategy approach,Anhai Doan; Pedro Domingos; Alon Halevy,Abstract The problem of integrating data from multiple data sources—either on the Internetor within enterprises—has received much attention in the database and AI communities. Thefocus has been on building data integration systems that provide a uniform query interface tothe sources. A key bottleneck in building such systems has been the laborious manualconstruction of semantic mappings between the query interface and the source schemas.Examples of mappings are “element location maps to address” and “price maps to listed-price”. We propose a multistrategy learning approach to automatically find such mappings.The approach applies multiple learner modules; where each module exploits a different typeof information either in the schemas of the sources or in their data; then combines thepredictions of the modules using a meta-learner. Learner modules employ a variety of …,Machine Learning,2003,308
Learning Source Description for Data Integration.,AnHai Doan; Pedro M Domingos; Alon Y Levy,ABSTRACT To build a data-integration system; the application designer must specify amediated schema and supply the descriptions of data sources. A source descriptioncontains a source schema that describes the content of the source; and a mapping betweenthe corresponding elements of the source schema and the mediated schema. Manuallyconstructing these mappings is both labor-intensive and error-prone; and has proven to be amajor bottleneck in deploying large-scale data integration systems in practice. In this paperwe report on our initial work toward automatically learning mappings between sourceschemas and the mediated schema. Specifically; we investigate finding one-to-onemappings for the leaf elements of source schemas. We describe LSD; a system thatautomatically finds such mappings. LSD consults a set of learner modules–where each …,WebDB (Informal Proceedings),2000,237
Tuffy: Scaling up statistical inference in markov logic networks using an rdbms,Feng Niu; Christopher Ré; AnHai Doan; Jude Shavlik,Abstract Markov Logic Networks (MLNs) have emerged as a powerful framework thatcombines statistical and logical reasoning; they have been applied to many data intensiveproblems including information extraction; entity resolution; and text mining. Currentimplementations of MLNs do not scale to large real-world data sets; which is preventing theirwidespread adoption. We present Tuffy that achieves scalability via three novelcontributions:(1) a bottom-up approach to grounding that allows us to leverage the full powerof the relational optimizer;(2) a novel hybrid architecture that allows us to perform AI-stylelocal search efficiently using an RDBMS; and (3) a theoretical insight that shows when onecan (exponentially) improve the efficiency of stochastic local search. We leverage (3) to buildnovel partitioning; loading; and parallel algorithms. We show that our approach …,Proceedings of the VLDB Endowment,2011,215
Declarative information extraction using datalog with embedded extraction predicates,Warren Shen; AnHai Doan; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract In this paper we argue that developing information extraction (IE) programs usingDatalog with embedded procedural extraction predicates is a good way to proceed. First;compared to current ad-hoc composition using; eg; Perl or C++; Datalog provides a cleanerand more powerful way to compose small extraction modules into larger programs. Thus;writing IE programs this way retains and enhances the important advantages of currentapproaches: programs are easy to understand; debug; and modify. Second; once we writeIE programs in this framework; we can apply query optimization techniques to them. Thisgives programs that; when run over a variety of data sets; are more efficient than anymonolithic program because they are optimized based on the statistics of the data on whichthey are invoked. We show how optimizing such programs raises challenges specific to …,Proceedings of the 33rd international conference on Very large data bases,2007,197
Privacy-preserving data integration and sharing,Chris Clifton; Murat Kantarcioǧlu; AnHai Doan; Gunther Schadow; Jaideep Vaidya; Ahmed Elmagarmid; Dan Suciu,Abstract Integrating data from multiple sources has been a longstanding challenge in thedatabase community. Techniques such as privacy-preserving data mining promises privacy;but assume data has integration has been accomplished. Data integration methods areseriously hampered by inability to share the data to be integrated. This paper lays out aprivacy framework for data integration. Challenges for data integration in the context of thisframework are discussed; in the context of existing accomplishments in data integration.Many of these challenges are opportunities for the data mining community.,Proceedings of the 9th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery,2004,172
eTuner: tuning schema matching software using synthetic scenarios,Yoonkyong Lee; Mayssam Sayyadian; AnHai Doan; Arnon S Rosenthal,Abstract Most recent schema matching systems assemble multiple components; eachemploying a particular matching technique. The domain user mustthen tune the system:select the right component to be executed and correctly adjust their numerous “knobs”(eg;thresholds; formula coefficients). Tuning is skill and time intensive; but (as we show) withoutit the matching accuracy is significantly inferior. We describe eTuner; an approach toautomatically tune schema matching systems. Given a schema S; we match S againstsynthetic schemas; for which the ground truth mapping is known; and find a tuning thatdemonstrably improves the performance of matching S against real schemas. To efficientlysearch the huge space of tuning configurations; eTuner works sequentially; starting withtuning the lowest level components. To increase the applicability of eTuner; we develop …,The VLDB Journal—The International Journal on Very Large Data Bases,2007,170
Source-aware entity matching: a compositional approach,Warren Shen; Pedro DeRose; Long Vu; AnHai Doan; Raghu Ramakrishnan,Entity matching (aka record linkage) plays a crucial role in integrating multiple data sources;and numerous matching solutions have been developed. However; the solutions havelargely exploited only information available in the mentions and employed a single matchingtechnique. We show how to exploit information about data sources to significantly improvematching accuracy. In particular; we observe that different sources often vary substantially intheir level of semantic ambiguity; thus requiring different matching techniques. In addition; itis often beneficial to group and match mentions in related sources first; before consideringother sources. These observations lead to a large space of matching strategies; analogousto the space of query evaluation plans considered by a relational optimizer. We proposeviewing entity matching as a composition of basic steps into a" match execution plan". We …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,165
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; AnHai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Abstract In late May; 2008; a group of database researchers; architects; users and punditsmet at the Claremont Resort in Berkeley; California to discuss the state of the research fieldand its impacts on practice. This was the seventh meeting of this sort in twenty years; andwas distinguished by a broad consensus that we are at a turning point in the history of thefield; due both to an explosion of data and usage scenarios; and to major shifts in computinghardware and platforms. Given these forces; we are at a time of opportunity for researchimpact; with an unusually large potential for influential results across computing; thesciences and society. This report details that discussion; and highlights the group'sconsensus view of new focus areas; including new database engine architectures;declarative programming languages; the interplay of structured and unstructured data …,ACM Sigmod Record,2008,160
Combining keyword search and forms for ad hoc querying of databases,Eric Chu; Akanksha Baid; Xiaoyong Chai; AnHai Doan; Jeffrey Naughton,Abstract A common criticism of database systems is that they are hard to query for usersuncomfortable with a formal query language. To address this problem; form-based interfacesand keyword search have been proposed; while both have benefits; both also havelimitations. In this paper; we investigate combining the two with the hopes of creating anapproach that provides the best of both. Specifically; we propose to take as input a targetdatabase and then generate and index a set of query forms offline. At query time; a user witha question to be answered issues standard keyword search queries; but instead of returningtuples; the system returns forms relevant to the question. The user may then build astructured query with one of these forms and submit it back to the system for evaluation. Inthis paper; we address challenges that arise in form generation; keyword search over …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,148
Matching schemas in online communities: A web 2.0 approach,Robert McCann; Warren Shen; AnHai Doan,When integrating data from multiple sources; a key task that online communities often face isto match the schemas of the data sources. Today; such matching often incurs a hugeworkload that overwhelms the relatively small set of volunteer integrators. In such cases;community members may not even volunteer to be integrators; due to the high workload;and consequently no integration systems can be built. To address this problem; we proposeto enlist the multitude of users in the community to help match the schemas; in a Web 2.0fashion. We discuss the challenges of this approach and provide initial solutions. Finally; wedescribe an extensive set of experiments on both real-world and synthetic data thatdemonstrate the utility of the approach.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,145
On the provenance of non-answers to queries over extracted data,Jiansheng Huang; Ting Chen; AnHai Doan; Jeffrey F Naughton,Abstract In information extraction; uncertainty is ubiquitous. For this reason; it is useful toprovide users querying extracted data with explanations for the answers they receive.Providing the provenance for tuples in a query result partially addresses this problem; in thatprovenance can explain why a tuple is in the result of a query. However; in some casesexplaining why a tuple is not in the result may be just as helpful. In this work we focus onproviding provenance-style explanations for non-answers and develop a mechanism forproviding this new type of provenance. Our experience with an information extractionprototype suggests that our approach can provide effective provenance information that canhelp a user resolve their doubts over non-answers to a query.,Proceedings of the VLDB Endowment,2008,135
Muppet: MapReduce-style processing of fast data,Wang Lam; Lu Liu; STS Prasad; Anand Rajaraman; Zoheb Vacheri; AnHai Doan,Abstract MapReduce has emerged as a popular method to process big data. In the past fewyears; however; not just big data; but fast data has also exploded in volume and availability.Examples of such data include sensor data streams; the Twitter Firehose; and Facebookupdates. Numerous applications must process fast data. Can we provide a MapReduce-style framework so that developers can quickly write such applications and execute themover a cluster of machines; to achieve low latency and high scalability? In this paper wereport on our investigation of this question; as carried out at Kosmix and WalmartLabs. Wedescribe MapUpdate; a framework like MapReduce; but specifically developed for fast data.We describe Muppet; our implementation of MapUpdate. Throughout the description wehighlight the key challenges; argue why MapReduce is not well suited to address them …,Proceedings of the VLDB Endowment,2012,134
Efficient keyword search across heterogeneous relational databases,Mayssam Sayyadian; Hieu LeKhac; AnHai Doan; Luis Gravano,Keyword search is a familiar and potentially effective way to find information of interest thatis" locked" inside relational databases. Current work has generally assumed that answersfor a keyword query reside within a single database. Many practical settings; however;require that we combine tuples from multiple databases to obtain the desired answers. Suchdatabases are often autonomous and heterogeneous in their schemas and data. This paperdescribes Kite; a solution to the keyword-search problem over heterogeneous relationaldatabases. Kite combines schema matching and structure discovery techniques to findapproximate foreign-key joins across heterogeneous databases. Such joins are critical forproducing query results that span multiple databases and relations. Kite then exploits thejoins-discovered automatically across the databases-to enable fast and effective querying …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,132
Introduction to the special issue on semantic integration,AnHai Doan; Natalya F Noy; Alon Y Halevy,Abstract Semantic heterogeneity is one of the key challenges in integrating and sharing dataacross disparate sources; data exchange and migration; data warehousing; modelmanagement; the Semantic Web and peer-to-peer databases. Semantic heterogeneity canarise at the schema level and at the data level. At the schema level; sources can differ inrelations; attribute and tag names; data normalization; levels of detail; and the coverage of aparticular domain. The problem of reconciling schema-level heterogeneity is often referredto as schema matching or schema mapping. At the data level; we find differentrepresentations of the same real-world entities (eg; people; companies; publications; etc.).Reconciling data-level heterogeneity is referred to as data deduplication; record linkage;and entity/object matching. To exacerbate the heterogeneity challenges; schema …,ACM Sigmod Record,2004,123
Crossing the Structure Chasm.,Alon Y Halevy; Oren Etzioni; AnHai Doan; Zachary G Ives; Jayant Madhavan; Luke K McDowell; Igor Tatarinov,Online information comes in two flavors: unstructured corpora of text on the one hand; andstructured data managed by databases and knowledge bases on the other. These twodifferent kinds of data lead to very different authoring; management and search paradigms.In the first; search is based on keywords and answers are ranked according to relevance. Inthe second; search is based on queries in a formal language (eg; SQL); and all the answersreturned for the query are correct according to the underlying semantics of the system. In theu-world of unstructured data; authoring data is straightforward. In contrast; in the s-world ofstructured data; authoring data is a conceptual effort that requires technical expertise andsubstantial up front effort; the author is required to provide a comprehensive structure (ie;schema) of the domain before entering data. This paper is focused on the profound …,CIDR,2003,116
Community information management.,AnHai Doan; Raghu Ramakrishnan; Fei Chen; Pedro DeRose; Yoonkyong Lee; Robert McCann; Mayssam Sayyadian; Warren Shen,Abstract We introduce Cimple; a joint project between the University of Illinois and theUniversity of Wisconsin. Cimple aims to develop a software platform that can be rapidlydeployed and customized to manage data-rich online communities. We first describe theenvisioned working of such a software platform and our prototype; DBLife; which is acommunity portal being developed for the database research community. We then describethe technical challenges in Cimple and our solution approach. Finally; we discuss managinguncertainty and provenance; a crucial task in making our software platform practical.,IEEE Data Eng. Bull.,2006,114
Semantic integration,Natalya F Noy; AnHai Doan; Alon Y Halevy,*,AI magazine,2005,103
Learning to map between structured representations of data,Anhai Doan; Alon Y Halevy; Pedro M Domingos,This dissertation studies representation matching: the problem of creating semanticmappings between two data representations. Examples of mappings are “element locationof one representation maps to element address of the other”;“contact-phone maps to agent-phone”; and “listed-price maps to price*(1+ tax-rate)”. We begin this chapter by showing thatrepresentation matching is a fundamental step in numerous data management applications.Next; we show that the manual creation of semantic mappings is extremely labor intensiveand hence has become a key bottleneck hindering the widespread deployment of the aboveapplications (Sections 1.2-1.3). We then outline our semi-automatic solutions torepresentation matching (Sections 1.4-1.5). Finally; we list the contributions and give a roadmap to the rest of the dissertation (Section 1.6-1.7).,*,2002,102
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; Anhai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Here; we explore the conclusions of this self-assessment. It is by definition somewhatinward-focused but may be of interest to the broader computing community as both a windowinto upcoming directions in database research and a description of some of the community issuesand initiatives that surfaced. We describe the group's consensus view of new focus areas forresearch; including database engine architectures; declarative programming languages; interplayof structured data and free text; cloud data services; and mobile and virtual worlds. We also reporton discussions of the database community's growth and processes that may be of interest toother research areas facing similar challenges … Over the past 20 years; small groups of databaseresearchers have periodically gathered to assess the state of the field and propose directionsfor future research. 1;3;4;5;6;7 Reports of the meetings served to foster debate within the …,Communications of the ACM,2009,100
Building structured web community portals: A top-down; compositional; and incremental approach,Pedro DeRose; Warren Shen; Fei Chen; AnHai Doan; Raghu Ramakrishnan,Abstract Structured community portals extract and integrate information from raw Web pagesto present a unified view of entities and relationships in the community. In this paper weargue that to build such portals; a top-down; compositional; and incremental approach is agood way to proceed. Compared to current approaches that employ complex monolithictechniques; this approach is easier to develop; understand; debug; and optimize. In thisapproach; we first select a small set of important community sources. Next; we composeplans that extract and integrate data from these sources; using a set of extraction/integrationoperators. Executing these plans yields an initial structured portal. We then incrementallyexpand this portal by monitoring the evolution of current data sources; to detect and add newdata sources. We describe our initial solutions to the above steps; and a case study of …,Proceedings of the 33rd international conference on Very large data bases,2007,100
Corleone: hands-off crowdsourcing for entity matching,Chaitanya Gokhale; Sanjib Das; AnHai Doan; Jeffrey F Naughton; Narasimhan Rampalli; Jude Shavlik; Xiaojin Zhu,Abstract Recent approaches to crowdsourcing entity matching (EM) are limited in that theycrowdsource only parts of the EM workflow; requiring a developer to execute the remainingparts. Consequently; these approaches do not scale to the growing EM need at enterprisesand crowdsourcing startups; and cannot handle scenarios where ordinary users (ie; themasses) want to leverage crowdsourcing to match entities. In response; we propose thenotion of hands-off crowdsourcing (HOC)}; which crowdsources the entire workflow of a task;thus requiring no developers. We show how HOC can represent a next logical direction forcrowdsourcing research; scale up EM at enterprises and crowdsourcing startups; and openup crowdsourcing for the masses. We describe Corleone; a HOC solution for EM; whichuses the crowd in all major steps of the EM process. Finally; we discuss the implications …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,98
DBLife: A community information management platform for the database research community,Pedro DeRose; Warren Shen; Fei Chen; Yoonkyong Lee; Douglas Burdick; AnHai Doan; Raghu Ramakrishnan,Community Information Management: There are many communities on the Web. Some arebased on common interests; such as communities of movie goers; database researchers;and bioinformaticians; while others are based on a shared purpose; such as organizationintranets and online technical support groups. Community members often want to discover;monitor; and query entities and relationships in their community. For example; databaseresearchers might want to know if there is a connection between two given researchers;where a given paper has been cited in the past week; or what of interest has happened inthe last 24 hours. Answering such questions often requires retrieving raw; largelyunstructured data from multiple sources (eg; home pages; DBLP; mailing lists); then inferringand monitoring semantic information. Examples of such inference and monitoring include …,CIDR,2007,97
Constraint-based entity matching,Warren Shen; Xin Li; AnHai Doan,Abstract Entity matching is the problem of deciding if two given mentions in the data; such as“Helen Hunt” and “HM Hunt”; refer to the same real-world entity. Numerous solutions havebeen developed; but they have not considered in depth the problem of exploiting integrityconstraints that frequently exist in the domains. Examples of such constraints include “amention with age two cannot match a mention with salary 200K” and “if two paper citationsmatch; then their authors are likely to match in the same order”. In this paper we describe aprobabilistic solution to entity matching that exploits such constraints to improve matchingaccuracy. At the heart of the solution is a generative model that takes into account theconstraints during the generation process; and provides well-defined interpretations of theconstraints. We describe a novel combination of EM and relaxation labeling algorithms …,AAAI,2005,97
Mapping maintenance for data integration systems,Robert McCann; Bedoor AlShebli; Quoc Le; Hoa Nguyen; Long Vu; AnHai Doan,Abstract To answer user queries; a data integration system employs a set of semanticmappings between the mediated schema and the schemas of data sources. In dynamicenvironments sources often undergo changes that invalidate the mappings. Hence; once thesystem is deployed; the administrator must monitor it over time; to detect and repair brokenmappings. Today such continuous monitoring is extremely labor intensive; and poses a keybottleneck to the widespread deployment of data integration systems in practice. Wedescribe MAVERIC; an automatic solution to detecting broken mappings. At the heart ofMAVERIC is a set of computationally inexpensive modules called sensors; which capturesalient characteristics of data sources (eg; value distributions; HTML layout properties). Wedescribe how MAVERIC trains and deploys the sensors to detect broken mappings. Next …,Proceedings of the 31st international conference on Very large data bases,2005,93
Information extraction challenges in managing unstructured data,AnHai Doan; Jeffrey F Naughton; Raghu Ramakrishnan; Akanksha Baid; Xiaoyong Chai; Fei Chen; Ting Chen; Eric Chu; Pedro DeRose; Byron Gao; Chaitanya Gokhale; Jiansheng Huang; Warren Shen; Ba-Quy Vuong,Abstract Over the past few years; we have been trying to build an end-to-end system atWisconsin to manage unstructured data; using extraction; integration; and user interaction.This paper describes the key information extraction (IE) challenges that we have run into;and sketches our solutions. We discuss in particular developing a declarative IE language;optimizing for this language; generating IE provenance; incorporating user feedback into theIE process; developing a novel wiki-based user interface for feedback; best-effort IE; pushingIE into RDBMSs; and more. Our work suggests that IE in managing unstructured data canopen up many interesting research challenges; and that these challenges can greatlybenefit from the wealth of work on managing structured data that has been carried out by thedatabase community.,ACM SIGMOD Record,2009,88
Webiq: Learning from the web to match deep-web query interfaces,Wensheng Wu; AnHai Doan; Clement Yu,Integrating Deep Web sources requires highly accurate semantic matches between theattributes of the source query interfaces. These matches are usually established bycomparing the similarities of the attributes' labels and instances. However; attributes onquery interfaces often have no or very few data instances. The pervasive lack of instancesseriously reduces the accuracy of current matching techniques. To address this problem; wedescribe WebIQ; a solution that learns from both the Surface Web and the Deep Web toautomatically discover instances for interface attributes. WebIQ extends question answeringtechniques commonly used in the AI community for this purpose. We describe how toincorporate WebIQ into current interface matching systems. Extensive experiments over fiverealworld domains show the utility ofWebIQ. In particular; the results show that acquired …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,87
Managing information extraction: state of the art and research directions,AnHai Doan; Raghu Ramakrishnan; Shivakumar Vaithyanathan,Abstract This tutorial makes the case for developing a unified framework that managesinformation extraction from unstructured data (focusing in particular on text). We first surveyresearch on information extraction in the database; AI; NLP; IR; and Web communities inrecent years. Then we discuss why this is the right time for the database community toactively participate and address the problem of managing information extraction (includingin particular the challenges of maintaining and querying the extracted information; andaccounting for the imprecision and uncertainty inherent in the extraction process). Finally;we show how interested researchers can take the next step; by pointing to open problems;available datasets; applicable standards; and software tools. We do not assume priorknowledge of text management; NLP; extraction techniques; or machine learning.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,84
Object matching for information integration: A profiler-based approach,AnHai Doan Ying Lu Yoonkyong Lee; Jiawei Han,Abstract Object matching is a fundamental problem that arises in numerous informationintegration scenarios. Virtually all existing solutions to this problem have assumed that theobjects to be matched share the same set of attributes; and that they can be matched bycomparing the similarities of the attributes. We consider the more general problem where theobjects can also have disjoint attributes; such as matching tuples that come from relationaltables with schemas (age; name) and (name; salary); respectively. We describe PROM; asolution that also exploits the disjoint attributes to improve matching accuracy. In the aboveexample; PROM begins by matching any two given tuples based on the shared attributename. Then it applies a set of profilers; each of which contains some knowledge about whatconstitutes a typical person. The profilers examine the tuple pair to see if it can plausibly …,*,2003,84
Entity extraction; linking; classification; and tagging for social media: a wikipedia-based approach,Abhishek Gattani; Digvijay S Lamba; Nikesh Garera; Mitul Tiwari; Xiaoyong Chai; Sanjib Das; Sri Subramaniam; Anand Rajaraman; Venky Harinarayan; AnHai Doan,Abstract Many applications that process social data; such as tweets; must extract entitiesfrom tweets (eg;" Obama" and" Hawaii" in" Obama went to Hawaii"); link them to entities in aknowledge base (eg; Wikipedia); classify tweets into a set of predefined topics; and assigndescriptive tags to tweets. Few solutions exist today to solve these problems for social data;and they are limited in important ways. Further; even though several industrial systems suchas OpenCalais have been deployed to solve these problems for text data; little if any hasbeen published about them; and it is unclear if any of the systems has been tailored forsocial media. In this paper we describe in depth an end-to-end industrial system that solvesthese problems for social data. The system has been developed and used heavily in thepast three years; first at Kosmix; a startup; and later at WalmartLabs. We show how our …,Proceedings of the VLDB Endowment,2013,81
Building data integration systems via mass collaboration,Robert McCann; AnHai Doan; Vanitha Varadarajan; Alexander Kramnik,ABSTRACT Building data integration systems today is largely done by hand; in a very labor-intensive and error-prone process. In this paper we describe a conceptually new solution tothis problem: that of mass collaboration. The basic idea is to think about a data integrationsystem as having a finite set of parameters whose values must be set. To build such asystem the system administrators construct and deploy a system “shell”; then ask the usersto help the system “automatically converge” to the correct parameter values. This way theenormous burden of system development is lifted from the administrators and spread “thinly”over a multitude of users. We describe our current effort in applying this approach to theproblem of schema matching in the context of data integration. We present experiments withboth real and synthetic users that show the promise of the approach. Finally we discuss …,Intl. Workshop on the Web and Databases; USA,2003,77
The clinical potential of influencing Nrf2 signaling in degenerative and immunological disorders,Bifeng Gao; An Doan; Brooks M Hybertson,Abstract Nuclear factor (erythroid-derived 2)-like 2 (Nrf2; encoded in humans by the NFE2L2gene) is a transcription factor that regulates the gene expression of a wide variety ofcytoprotective phase II detoxification and antioxidant enzymes through a promoter sequenceknown as the antioxidant-responsive element (ARE). The ARE is a promoter element foundin many cytoprotective genes; therefore; Nrf2 plays a pivotal role in the ARE-driven cellulardefense system against environmental stresses. Agents that target the ARE/Nrf2 pathwayhave been tested in a wide variety of disorders; with at least one new Nrf2-activating drugnow approved by the US Food and Drug Administration. Examination of in vitro and in vivoexperimental results; and taking into account recent human clinical trial results; has led to anopinion that Nrf2-activating strategies–which can include drugs; foods; dietary …,Clinical pharmacology: advances and applications,2014,71
Efficient decision-theoretic planning: Techniques and empirical analysis,Peter Haddawy; AnHai Doan; Richard Goodwin,Abstract This paper discusses techniques for performing efficient decision-theoreticplanning. We give an overview of the DRIPS decision-theoretic refinement planning system;which uses abstraction to efficiently identify optimal plans. We present techniques forautomatically generating search control information; which can significantly improve theplanner's performance. We evaluate the efficiency of DRIPS both with and without thesearch control rules on a complex medical planning problem and compare its performanceto that of a branch-and-bound decision tree algorithm.,Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,1995,71
A relational approach to incrementally extracting and querying structure in unstructured data,Eric Chu; Akanksha Baid; Ting Chen; AnHai Doan; Jeffrey Naughton,Abstract There is a growing consensus that it is desirable to query over the structure implicitin unstructured documents; and that ideally this capability should be provided incrementally.However; there is no consensus about what kind of system should be used to support thiskind of incremental capability. We explore using a relational system as the basis for aworkbench for extracting and querying structure from unstructured data. As a proof ofconcept; we applied our relational approach to support structured queries over Wikipedia.We show that the data set is always available for some form of querying; and that as it isprocessed; users can pose a richer set of structured queries. We also provide examples ofhow we can incrementally evolve our understanding of the data in the context of therelational workbench.,Proceedings of the 33rd international conference on Very large data bases,2007,70
Toward scalable keyword search over relational data,Akanksha Baid; Ian Rae; Jiexing Li; AnHai Doan; Jeffrey Naughton,Abstract Keyword search (KWS) over relational databases has recently received significantattention. Many solutions and many prototypes have been developed. This task requiresaddressing many issues; including robustness; accuracy; reliability; and privacy. Anemerging issue; however; appears to be performance related: current KWS systems haveunpredictable running times. In particular; for certain queries it takes too long to produceanswers; and for others the system may even fail to return (eg; after exhausting memory). Inthis paper we argue that as today's users have been" spoiled" by the performance of Internetsearch engines; KWS systems should return whatever answers they can produce quicklyand then provide users with options for exploring any portion of the answer space notcovered by these answers. Our basic idea is to produce answers that can be generated …,Proceedings of the VLDB Endowment,2010,64
Building; maintaining; and using knowledge bases: a report from the trenches,Omkar Deshpande; Digvijay S Lamba; Michel Tourn; Sanjib Das; Sri Subramaniam; Anand Rajaraman; Venky Harinarayan; AnHai Doan,Abstract A knowledge base (KB) contains a set of concepts; instances; and relationships.Over the past decade; numerous KBs have been built; and used to power a growing array ofapplications. Despite this flurry of activities; however; surprisingly little has been publishedabout the end-to-end process of building; maintaining; and using such KBs in industry. Inthis paper we describe such a process. In particular; we describe how we build; update; andcurate a large KB at Kosmix; a Bay Area startup; and later at WalmartLabs; a developmentand research lab of Walmart. We discuss how we use this KB to power a range ofapplications; including query understanding; Deep Web search; in-context advertising; eventmonitoring in social media; product search; social gifting; and social mining. Finally; wediscuss how the KB team is organized; and the lessons learned. Our goal with this paper …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,62
Toward best-effort information extraction,Warren Shen; Pedro DeRose; Robert McCann; AnHai Doan; Raghu Ramakrishnan,Abstract Current approaches to develop information extraction (IE) programs have largelyfocused on producing precise IE results. As such; they suffer from three major limitations.First; it is often difficult to execute partially specified IE programs and obtain meaningfulresults; thereby producing a long" debug loop". Second; it often takes a long time before wecan obtain the first meaningful result (by finishing and running a precise IE program);thereby rendering these approaches impractical for time-sensitive IE applications. Finally; bytrying to write precise IE programs we may also waste a significant amount of effort; becausean approximate result--one that can be produced quickly--may already be satisfactory inmany IE settings. To address these limitations; we propose iFlex; an IE approach thatrelaxes the precise IE requirement to enable best-effort IE. In iFlex; a developer U uses a …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,60
Efficiently ordering query plans for data integration,AnHai Doan; A Hatevy,The goal of a data integration system is to provide a uniform interface to a multitude of datasources. Given a user query formulated in this interface; the system translates it into a set ofquery plans. Each plan is a query formulated over the data sources; and specifies a way toaccess sources and combine data to answer the user query. In practice; when the number ofsources is large; a data-integration system must generate and execute many query planswith significantly varying utilities. Hence; it is crucial that the system finds the best plansefficiently and executes them first; to guarantee acceptable time to and the quality of the firstanswers. We describe efficient solutions to this problem. First; we formally define theproblem of ordering query plans. Second; we identify several interesting structural propertiesof the problem and describe three ordering algorithms that exploit these properties …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,60
Efficiently incorporating user feedback into information extraction and integration programs,Xiaoyong Chai; Ba-Quy Vuong; AnHai Doan; Jeffrey F Naughton,Abstract Many applications increasingly employ information extraction and integration (IE/II)programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise.Hence such programs often make many IE/II mistakes; and thus can significantly benefit fromuser feedback. Today; however; there is no good way to automatically provide and processsuch feedback. When finding an IE/II mistake; users often must alert the developer team (eg;via email or Web form) about the mistake; and then wait for the team to manually examinethe program internals to locate and fix the mistake; a slow; error-prone; and frustratingprocess. In this paper we propose a solution for users to directly provide feedback and forIE/II programs to automatically process such feedback. In our solution a developer U useshlog; a declarative IE/II language; to write an IE/II program P. Next; U writes declarative …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,58
Efficient information extraction over evolving text data,Fei Chen; AnHai Doan; Jun Yang; Raghu Ramakrishnan,Most current information extraction (IE) approaches have considered only static text corpora;over which we typically have to apply IE only once. Many real-world text corpora howeverare dynamic. They evolve over time; and to keep extracted information up to date; we oftenmust apply IE repeatedly; to consecutive corpus snapshots. We describe Cyclex; anapproach that efficiently executes such repeated IE; by recycling previous IE efforts.Specifically; given a current corpus snapshot U; Cyclex identifies text portions of U that alsoappear in the previous corpus snapshot V. Since Cyclex has already executed IE over V; itcan now recycle the IE results of these parts; by combining these results with the results ofexecuting IE over the remaining parts of U; to produce the complete IE results for U.Realizing Cyclex raises many challenges; including modeling information extractors …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,58
Optimizing SQL queries over text databases,Alpa Jain; AnHai Doan; Luis Gravano,Text documents often embed data that is structured in nature; and we can expose thisstructured data using information extraction technology. By processing a text database withinformation extraction systems; we can materialize a variety of structured" relations;" overwhich we can then issue regular SQL queries. A key challenge to process SQL queries inthis text-based scenario is efficiency: information extraction is time-consuming; so queryprocessing strategies should minimize the number of documents that they process. Anotherkey challenge is result quality: in the traditional relational world; all correct executionstrategies for a SQL query produce the same (correct) result; in contrast; a SQL queryexecution over a text database might produce answers that are not fully accurate orcomplete; for a number of reasons. To address these challenges; we study a family of …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,57
Profile-based object matching for information integration,AnHai Doan; Ying Lu; Yoonkyong Lee; Jiawei Han,Traditional object-matching methods rely on similarities among shared attributes. Profile-based object matching builds on this approach but also correlates disjoint attributes toimprove matching accuracy. To illustrate the PROM approach; we use two relational tables:one contains information about movies; the other about movie reviews.,IEEE Intelligent Systems,2003,52
Chimera: Large-scale classification using machine learning; rules; and crowdsourcing,Chong Sun; Narasimhan Rampalli; Frank Yang; AnHai Doan,Abstract Large-scale classification is an increasingly critical Big Data problem. So far;however; very little has been published on how this is done in practice. In this paper wedescribe Chimera; our solution to classify tens of millions of products into 5000+ producttypes at WalmartLabs. We show that at this scale; many conventional assumptions regardinglearning and crowdsourcing break down; and that existing solutions cease to work. Wedescribe how Chimera employs a combination of learning; rules (created by in-houseanalysts); and crowdsourcing to achieve accurate; continuously improving; and cost-effective classification. We discuss a set of lessons learned for other similar Big Datasystems. In particular; we argue that at large scales crowdsourcing is critical; but must beused in combination with learning; rules; and in-house analysts. We also argue that using …,Proceedings of the VLDB Endowment,2014,48
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45
Olap over imprecise data with domain constraints,Doug Burdick; AnHai Doan; Raghu Ramakrishnan; Shivakumar Vaithyanathan,Abstract Several recent papers have focused on OLAP over imprecise data; where each factcan be a region; instead of a point; in a multi-dimensional space. They have provided amultiple-world semantics for such data; and developed efficient ways to answer OLAPaggregation queries over the imprecise facts. These solutions; however; assume that theimprecise facts can be interpreted independently of one another; a key assumption that isoften violated in practice. Indeed; imprecise facts in real-world applications are oftencorrelated; and such correlations can be captured as domain integrity constraints (eg;repairs with the same customer names and models took place in the same city; or a textspan can refer to a person or a city; but not both). In this paper we provide a framework foranswering OLAP aggregation queries over imprecise data in the presence of such …,Proceedings of the 33rd international conference on Very large data bases,2007,43
Geometric foundations for interval-based probabilities,Vu A Ha; AnHai Doan; Van H Vu; Peter Haddawy,Abstract The need to reason with imprecise probabilities arises in a wealth of situationsranging from pooling of knowledge from multiple experts to abstraction-based probabilisticplanning. Researchers have typically represented imprecise probabilities using intervalsand have developed a wide array of different techniques to suit their particular requirements.In this paper we provide an analysis of some of the central issues in representing andreasoning with interval probabilities. At the focus of our analysis is the probability cross-product operator and its interval generalization; the cc-operator. We perform an extensivestudy of these operators relative to manipulation of sets of probability distributions. Thisstudy provides insight into the sources of the strengths and weaknesses of variousapproaches to handling probability intervals. We demonstrate the application of our …,Annals of Mathematics and Artificial Intelligence,1998,42
Merging interface schemas on the deep web via clustering aggregation,Wensheng Wu; AnHai Doan; Clement Yu,We consider the problem of integrating a large number of interface schemas over the deepWeb; The scale of the problem and the diversity of the sources present serious challenges tothe conventional manual or rule-based approaches to schema integration. To address thesechallenges; we propose a novel formulation of schema integration as an optimizationproblem; with the objective of maximally satisfying the constraints given by individualschemas. Since the optimization problem can be shown to be NP-complete; we develop anovel approximation algorithm LMax; which builds the unified schema via recursiveapplications of clustering aggregation. We further extend LMax to handle the irregularitiesfrequently occurring among the interface schemas. Extensive evaluation on real-world datasets shows the effectiveness of our approach.,Data Mining; Fifth IEEE International Conference on,2005,39
Crowdsourcing applications and platforms: A data management perspective,A Doan; Michael J Franklin; Donald Kossmann; Tim Kraska,Over the past decade; crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcinginclude Wikipedia; Linux; Yahoo! Answers; YouTube; Mechanical Turk-based applications;and much effort is being directed toward developing many more. As is typical for anemerging area; this effort has appeared under many names; including peer production;userpowered systems; user-generated content; collaborative systems; community systems;social systems; social search; social media; collective intelligence; wikinomics; crowdwisdom; smart mobs; mass collaboration; and human computation. The topic has also beendiscussed extensively in books; popular press; and academia (eg;[17; 18; 16; 19; 13; 3; 5; 8;15; 11; 19; 10; 7]). This extensive attention; as well as the many successes of …,Proceedings of the VLDB Endowment,2011,37
Bootstrapping domain ontology for semantic web services from source web sites,Wensheng Wu; AnHai Doan; Clement Yu; Weiyi Meng,Abstract The vision of Semantic Web services promises a network of interoperable Webservices over different sources. A major challenge to the realization of this vision is the lackof automated means of acquiring domain ontologies necessary for marking up the Webservices. In this paper; we propose the DeepMiner system which learns domain ontologiesfrom the source Web sites. Given a set of sources in a domain of interest; DeepMiner firstlearns a base ontology from their query interfaces. It then grows the current ontology byprobing the sources and discovering additional concepts and instances from the data pagesretrieved from the sources. We have evaluated DeepMiner in several real-world domains.Preliminary results indicate that DeepMiner discovers concepts and instances with highaccuracy.,International Workshop on Technologies for E-Services,2005,37
The case for a structured approach to managing unstructured data,AnHai Doan; Jeff Naughton; Akanksha Baid; Xiaoyong Chai; Fei Chen; Ting Chen; Eric Chu; Pedro DeRose; Byron Gao; Chaitanya Gokhale; Jiansheng Huang; Warren Shen; Ba-Quy Vuong,Abstract: The challenge of managing unstructured data represents perhaps the largest datamanagement opportunity for our community since managing relational data. And yet we arerisking letting this opportunity go by; ceding the playing field to other players; ranging fromcommunities such as AI; KDD; IR; Web; and Semantic Web; to industrial players such asGoogle; Yahoo; and Microsoft. In this essay we explore what we can do to improve upon thissituation. Drawing on the lessons learned while managing relational data; we outline astructured approach to managing unstructured data. We conclude by discussing thepotential implications of this approach to managing other kinds of non-relational data; and tothe identify of our field.,arXiv preprint arXiv:0909.1783,2009,35
Crowds; clouds; and algorithms: Exploring the human side of big data applications,Sihem Amer-Yahia; AnHai Doan; Jon Kleinberg; Nick Koudas; Michael Franklin,The creation; collection; analysis; curation; and dissemination of data have becomeprofoundly democratized. Social networks spanning 100's of millions of users enableinstantaneous discussion; debate; and information sharing [8]. Streams of tweets; blogs;photos; and videos identify breaking events faster and in more detail than ever before [10].Global; ad hoc collaborations addressing scientific; commercial; political; and evenmathematical problems make progress where individual investigators or small groupscannot [6; 11; 12; 13; 14].,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,33
From science to engineering,Mark Guzdial,CS1 was in a series of studies by Elliot Soloway and his colleagues at Yale University. Theyregularly used the same problem; called “The Rainfall Problem”: Write a program thatrepeatedly reads in positive integers; until it reads the integer 99999. After seeing 99999; itshould print out the average. In one study; only 14% of students in Yale's CS1 could solvethis problem correctly. 9 The Rainfall Problem has been used under test conditions and as atake-home programming assignment; and is typically graded so that syntax errors don'tcount; though adding a negative value or 99999 into the total is an automatic zero. Everystudy that I've seen (the latest in 2009) that has used the Rainfall Problem has found similardismal performance; on a problem that seems amazingly simple. Mike McCracken realizedthe problem with Soloway's studies; or any similar study; could be that a single campus …,Communications of the ACM,2011,32
Building community wikipedias: A machine-human partnership approach,Pedro DeRose; Xiaoyong Chai; Byron J Gao; Warren Shen; AnHai Doan; Philip Bohannon; Xiaojin Zhu,The rapid growth of Web communities has motivated many solutions for building communitydata portals. These solutions follow roughly two approaches. The first approach (eg; Libra;Citeseer; Cimple) employs semi-automatic methods to extract and integrate data from amultitude of data sources. The second approach (eg; Wikipedia; Intellipedia) deploys aninitial portal in wiki format; then invites community members to revise and add material. Inthis paper we consider combining the above two approaches to building community portals.The new hybrid machine-human approach brings significant benefits. It can achieve broaderand deeper coverage; provide more incentives for users to contribute; and keep the portalmore up-to-date with less user effort. In a sense; it enables building" community wikipedias";backed by an underlying structured database that is continuously updated using …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,30
Abstracting probabilistic actions,Peter Haddawy; AnHai Doan,Abstract This paper discusses the problem of abstracting conditional probabilistic actions.We identify two distinct types of abstraction: intra-action abstraction and inter-actionabstraction. We define what it means for the abstraction of an action to be correct and thenderive two methods of intra-action abstraction and two methods of inter-action abstractionwhich are correct according to this criterion. We illustrate the developed techniques byapplying them to actions described with the temporal action representation used in theDRIPS decision-theoretic planner and we describe how the planner uses abstraction toreduce the complexity of planning.,*,1994,30
Join Optimization of Information Extraction Output: Quality Matters!,Alpa Jain; Panagiotis G Ipeirotis; AnHai Doan; Luis Gravano,Information extraction (IE) systems are trained to extract specific relations from textdatabases. Real-world applications often require that the output of multiple IE systems bejoined to produce the data of interest. To optimize the execution of a join of multipleextracted relations; it is not sufficient to consider only execution time. In fact; the quality of thejoin output is of critical importance: unlike in the relational world; different join executionplans can produce join results of widely different quality whenever IE systems are involved.In this paper; we develop a principled approach to understand; estimate; and incorporateoutput quality into the join optimization process over extracted relations. We argue that theoutput quality is affected by (a) the configuration of the IE systems used to processdocuments;(b) the document retrieval strategies used to retrieve documents; and (c) the …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,29
SQL queries over unstructured text databases,Alpa Jain; AnHai Doan; Luis Gravano,Text documents often embed data that is structured in nature. By processing a text databasewith information extraction systems; we can define a variety of structured" relations" overwhich we can then issue SQL queries. Processing SQL queries in this text-based scenariopresents multiple challenges. One key challenge is efficiency: information extraction is atime-consuming process; so query processing strategies should pick efficient extractionsystems whenever possible; and also minimize the number of documents that they process.Another key challenge is result quality: extraction systems might output erroneousinformation or miss information that they should capture; also; efficiency-related queryprocessing decisions (eg; to avoid processing large numbers of useless documents) maycompromise result completeness. To address these challenges; we characterize SQL …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,28
Modeling entity evolution for temporal record matching,Yueh-Hsuan Chiang; AnHai Doan; Jeffrey F Naughton,Abstract Temporal record matching recognizes that if the entities represented by the recordschange over time; approaches that use temporal information may do better than approachesthat do not. Any such temporal matching method relies at its heart on a temporal model thatcaptures information about how entities evolve. In their pioneering work; Li {\it et al.} used anefficiently computable model that simply tries to predict if an attribute is expected to changeover a given time interval. In our work; we propose and evaluate a more detailed model thatfocuses on the probability that a given attribute value reappears over time. The intuition hereis that an entity might change its attribute value in the way that is dependent on its pastvalues. In addition; our model considers sets of records (rather than simply pairs of records)to improve robustness and accuracy. Experimental results show that the resulting …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,27
Modeling and extracting deep-web query interfaces,Wensheng Wu; AnHai Doan; Clement Yu; Weiyi Meng,Abstract Interface modeling & extraction is a fundamental step in building a uniform queryinterface to a multitude of databases on the Web. Existing solutions are limited in that theyassume interfaces are flat and thus ignore the inherent structure of interfaces; which thenseriously hampers the effectiveness of interface integration. To address this limitation; in thischapter; we model an interface with a hierarchical schema (eg; an ordered-tree of attributes).We describe ExQ; a novel schema extraction system with two distinct features. First; ExQdiscovers the structure of an interface based on its visual representation via spatialclustering. Second; ExQ annotates the discovered schema with labels from the interface byimitating the human-annotation process. ExQ has been extensively evaluated with real-world query interfaces in five different domains and the results show that ExQ achieves …,*,2009,25
Toward entity retrieval over structured and text data,Mayssam Sayyadian; Azadeh Shakery; AnHai Doan; ChengXiang Zhai,ABSTRACT Many real-world applications increasingly involve both structured data and text.Hence; managing both in an efficient and integrated manner has received much attentionfrom both the IR and database communities. To date; however; little research has beendevoted to semantic issues in the integration of text and data. In this paper we introduced aproblem in this realm: entity retrieval. Given data fragments that describe various aspects ofa real-world entity; find all other data fragments as well as text documents that describe thatsame entity. As such; entity retrieval is a novel retrieval problem; which differs from bothregular text retrieval and database search in that it explicitly requires matching information atthe semantic level; matching syntactically as done in the current search engines andrelational databases would be inherently non-optimal. We define entity retrieval and …,Proceedings of the ACM SIGIR 2004 Workshop on the Integration of Information Retrieval and Databases (WIRD’04),2004,25
Decision-theoretic refinement planning in medical decision making: Management of acute deep venous thrombosis,Peter Haddawy; AnHai Doan; Charles E Kahn Jr,Decision-theoretic refinement planning is a new technique for finding optimal courses ofaction. The authors sought to determine whether this technique could identify optimalstrategies for medical diagnosis and therapy. An existing model of acute deep venousthrombosis of the lower extremities was encoded for analysis by the decision-theoreticrefinement planning system (DRIPS). The encoding represented 6;206 possible plans. TheDRIPS planner used artificial intelligence techniques to elimmate 5;150 plans (83%) fromconsideration without examining them explicitly. The DRIPS system identified the fivestrategies that minimized cost and mortality. The authors conclude that decision-theoreticplanning is useful for examining large medical-decision problems. Key words: decision-theoretic refinement planning; diagnosis; treatment; DRIPS; acute deep venous …,Medical Decision Making,1996,23
Databases and Web 2.0 panel at VLDB 2007,Sihem Amer-Yahia; Volker Markl; Alon Halevy; AnHai Doan; Gustavo Alonso; Donald Kossmann; Gerhard Weikum,Abstract Web 2.0 refers to a set of technologies that enables indviduals to create and sharecontent on the Web. The types of content that are shared on Web 2.0 are quite varied andinclude photos and videos (eg; Flickr; YouTube); encyclopedic knowledge (eg; Wikipedia);the blogosphere; social book-marking and even structured data (eg; Swivel; Many-eyes).One of the important distinguishing features of Web 2.0 is the creation of communities ofusers. Online communities such as LinkedIn; Friendster; Facebook; MySpace and Orkutattract millions of users who build networks of their contacts and utilize them for social andprofessional purposes. In a nutshell; Web 2.0 offers an architecture of participation anddemocracy that encourages users to add value to the application as they use it.,ACM SIGMOD Record,2008,22
Analyzing and revising data integration schemas to improve their matchability,Xiaoyong Chai; Mayssam Sayyadian; AnHai Doan; Arnon Rosenthal; Len Seligman,Abstract Data integration systems often provide a uniform query interface; called a mediatedschema; to a multitude of data sources. To answer user queries; such systems employ a setof semantic matches between the mediated schema and the data-source schemas. Findingsuch matches is well known to be difficult. Hence much work has focused on developingsemi-automatic techniques to efficiently find the matches. In this paper we consider thecomplementary problem of improving the mediated schema; to make finding such matcheseasier. Specifically; a mediated schema S will typically be matched with many sourceschemas. Thus; can the developer of S analyze and revise S in a way that preserves S'ssemantics; and yet makes it easier to match with in the future? In this paper we provide anaffirmative answer to the above question; and outline a promising solution direction …,Proceedings of the VLDB Endowment,2008,21
Optimizing complex extraction programs over evolving text data,Fei Chen; Byron J Gao; AnHai Doan; Jun Yang; Raghu Ramakrishnan,Abstract Most information extraction (IE) approaches have considered only static textcorpora; over which we apply IE only once. Many real-world text corpora however aredynamic. They evolve over time; and so to keep extracted information up to date we oftenmust apply IE repeatedly; to consecutive corpus snapshots. Applying IE from scratch to eachsnapshot can take a lot of time. To avoid doing this; we have recently developed Cyclex; asystem that recycles previous IE results to speed up IE over subsequent corpus snapshots.Cyclex clearly demonstrated the promise of the recycling idea. The work itself however islimited in that it considers only IE programs that contain a single IE``blackbox.''In practice;many IE programs are far more complex; containing multiple IE blackboxes connected in acompositional``workflow.''In this paper; we present Delex; a system that removes the …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,20
University,JIAN Chen; Pranav Vaidya; Jaehwan John Lee; Ima Hewa Nadungodage; Yuni Xia; Renfa Li; Qiang Wu,MURPHY; KRISTEN L.(b. 1972) Assistant Profes- 412 (2009). chanisms over well-characterized samples. TEL:(414) sor. BS; 1995; University of Wisconsin-Milwaukee;Niloofar M. Tabatabai; Mukut Sharma; Samuel S. Blumen- 229-5222 FAX:(414) 229-5036Ph. D; 2002; University of Wisconsin-Milwaukee. Chem- thal and David H. Petering;Enhanced expressions of sodium- Web: surface. chem. uwm. edu istry Education. Thecurrent focus of research is methods glucose cotransporters in the kidneys of diabetic Zuckerrats; Email: wtt@ uwm. edu of content delivery; student cognition in problem solvingDiabetes Res. Clin. Pract; 83; e27-e30 (2009). Robert Schennach; Carol Hirschmugl;Eduard Gilli and strategies and assessment in introductory college chemis- David H.Petering; Susan Krezoski and Niloofar M. Taba-Wilfred T. Tysoe; A new method for …,J. Biol. Chem,2010,19
Decision-theoretic refinement planning: Principles and application,AnHai Doan; Peter Haddawy,Abstract We present a general theory of action abstraction for reducing the complexity ofdecision-theoretic planning. We develop projection rules for abstract actions and prove ourabstraction techniques to be correct. We present a planning algorithm that uses theabstraction theory to e ciently explore the space of possible plans by eliminating suboptimalclasses of plans without explicitly examining all plans in those classes. An instance of thealgorithm has been implemented as the drips decision-theoretic re nement planning system.We apply the planner to the problem of selecting the optimal test/treat strategy for managingpatients suspected of having deep-vein thrombosis of the lower extremities. We show thatdrips signi cantly outperforms a standard branch-and-bound decision tree evaluationalgorithm on this domain.,*,1995,17
Modeling Probabilistic Actions for Practical Decision-Theoretic Planning.,AnHai Doan,*,AIPS,1996,16
Magellan: Toward building entity matching management systems,Pradap Konda; Sanjib Das; Paul Suganthan GC; AnHai Doan; Adel Ardalan; Jeffrey R Ballard; Han Li; Fatemah Panahi; Haojun Zhang; Jeff Naughton; Shishir Prasad; Ganesh Krishnan; Rohit Deep; Vijay Raghavendra,Abstract Entity matching (EM) has been a long-standing challenge in data management.Most current EM works focus only on developing matching algorithms. We argue that farmore efforts should be devoted to building EM systems. We discuss the limitations of currentEM systems; then present as a solution Magellan; a new kind of EM systems. Magellan isnovel in four important aspects.(1) It provides how-to guides that tell users what to do in eachEM scenario; step by step.(2) It provides tools to help users do these steps; the tools seek tocover the entire EM pipeline; not just matching and blocking as current EM systems do.(3)Tools are built on top of the data analysis and Big Data stacks in Python; allowing Magellanto borrow a rich set of capabilities in data cleaning; IE; visualization; learning; etc.(4)Magellan provides a powerful scripting environment to facilitate interactive …,Proceedings of the VLDB Endowment,2016,15
Tracking entities in the dynamic world: A fast algorithm for matching temporal records,Yueh-Hsuan Chiang; AnHai Doan; Jeffrey F Naughton,Abstract Identifying records referring to the same real world entity over time enableslongitudinal data analysis. However; difficulties arise from the dynamic nature of the world:the entities described by a temporal data set often evolve their states over time. While thestate of the art approach to temporal entity matching achieves high accuracy; this approachis computationally expensive and cannot handle large data sets. In this paper; we presentan approach that achieves equivalent matching accuracy but takes far less time. Our keyinsight is" static first; dynamic second." Our approach first runs an evidence-collection pass;grouping records without considering the possibility of entity evolution; as if the world were"static." Then; it merges clusters from the initial grouping by determining whether an entitymight evolve from the state described in one cluster to the state described in another …,Proceedings of the VLDB Endowment,2014,15
Sound abstraction of probabilistic actions in the constraint mass assignment framework,AnHai Doan; Peter Haddawy,Abstract This paper provides a formal and practical framework for sound abstraction ofprobabilistic actions. We start by precisely defining the concept of sound abstraction withinthe context of finite-horizon planning (where each plan is a finite sequence of actions). Nextwe show that such abstraction cannot be performed within the traditional probabilistic actionrepresentation; which models a world with a single probability distribution over the statespace. We then present the constraint mass assignment representation; which models theworld with a set of probability distributions and is a generalization of mass assignmentrepresentations. Within this framework; we present sound abstraction procedures for threetypes of action abstraction. We end the paper with discussions and related work on soundand approximate abstraction. We give pointers to papers in which we discuss other sound …,Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence,1996,14
Introduction to the special issue on managing information extraction,AnHai Doan; Luis Gravano; Raghu Ramakrishnan; Shivakumar Vaithyanathan,The field of information extraction (IE) focuses on extracting structured data; such as personnames and organizations; from unstructured text. This field has had a long history. Itattracted steady attention in the 80s and 90s; largely in the AI community. In the past decade;however; spurred on by the explosion of unstructured data on the World-Wide Web; thisattention has turned into a torrent; gathering the efforts of researchers in the AI; DB; WWW;KDD; Semantic Web and IR communities. New IE problems have been identified; new IEtechniques developed; many workshops organized; tutorials presented; companiesfounded; academic and industrial products deployed; and opensource prototypesdeveloped (eg;[5; 4; 3; 1; 2]; see [5] for the latest survey). The next few years are poised towitness even more accelerated activities in these areas. It is against this vibrant backdrop …,ACM SIGMOD Record,2008,12
Learning mappings between data schemas,Anhai Doan; Pedro Domingos; Alon Y Levy,*,Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data,2000,12
Decision-theoretic refinement planning: a new method for clinical decision analysis.,AnHai Doan; Peter Haddawy; Charles E Kahn Jr,Abstract Clinical decision analysis seeks to identify the optimal management strategy bymodelling the uncertainty and risks entailed in the diagnosis; natural history; and treatmentof a particular problem or disorder. Decision trees are the most frequently used model inclinical decision analysis; but can be tedious to construct; cumbersome to use; andcomputationally prohibitive; especially with large; complex decision problems. We present anew method for clinical decision analysis that combines the techniques of decision theoryand artificial intelligence. Our model uses a modular representation of knowledge thatsimplifies model building and enables more fully automated decision making. Moreover; themodel exploits problem structures to yield better computational efficiency. As an example weapply our techniques to the problem of management of acute deep venous thrombosis.,Proceedings of the Annual Symposium on Computer Application in Medical Care,1995,12
Why big data industrial systems need rules and what we can do about it,Paul Suganthan GC; Chong Sun; Haojun Zhang; Frank Yang; Narasimhan Rampalli; Shishir Prasad; Esteban Arcaute; Ganesh Krishnan; Rohit Deep; Vijay Raghavendra; AnHai Doan,Abstract Big Data industrial systems that address problems such as classification;information extraction; and entity matching very commonly use hand-crafted rules. Today;however; little is understood about the usage of such rules. In this paper we explore thisissue. We discuss how these systems differ from those considered in academia. Wedescribe default solutions; their limitations; and reasons for using rules. We show examplesof extensive rule usage in industry. Contrary to popular perceptions; we show that there is arich set of research challenges in rule generation; evaluation; execution; optimization; andmaintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustratethese challenges. Our main conclusions are (1) using rules (together with techniques suchas learning and crowdsourcing) is fundamental to building semantics-intensive Big Data …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,11
A Discriminative Approach to Ontology Mapping.,Michael L Wick; Khashayar Rohanimanesh; Andrew McCallum; AnHai Doan,ABSTRACT Techniques for automatically performing ontology mapping are vital for manyreal-world applications. Unfortunately; the problem is difficult because many types ofevidence must be integrated to make good alignment decisions; and these decisions are co-dependent. In this paper; we propose a conditional random field (CRF) for ontology mappingwhich combines probabilistic machine learning and dependencies among the prediction.We integrate multiple sources of evidence using clauses in first-order logic; and learncorresponding weights directly from labeled training data. Our experiments show examplesof impressive gains when tested on a commonly used mapping corpus; our methodachieves an average of 11%(absolute) improvement in F1 when compared to other systems.We also show that our CRF is capable of generalizing from one mapping domain to …,NTII,2008,10
Generating macro operators for decision-theoretic planning,AnHai Doan; Peter Haddawy,*,Working Notes of the AAAI Spring Symposium on Extending Theories of Action,1995,10
Falcon: Scaling up hands-off crowdsourced entity matching to build cloud services,Sanjib Das; Paul Suganthan GC; AnHai Doan; Jeffrey F Naughton; Ganesh Krishnan; Rohit Deep; Esteban Arcaute; Vijay Raghavendra; Youngchoon Park,Abstract Many works have applied crowdsourcing to entity matching (EM). While promising;these approaches are limited in that they often require a developer to be in the loop. Assuch; it is difficult for an organization to deploy multiple crowdsourced EM solutions;because there are simply not enough developers. To address this problem; a recent workhas proposed Corleone; a solution that crowdsources the entire EM workflow; requiring nodevelopers. While promising; Corleone is severely limited in that it does not scale to largetables. We propose Falcon; a solution that scales up the hands-off crowdsourced EMapproach of Corleone; using RDBMS-style query execution and optimization over a Hadoopcluster. Specifically; we define a set of operators and develop efficient implementations. Wetranslate a hands-off crowdsourced EM workflow into a plan consisting of these operators …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,9
Toward industrial-strength keyword search systems over relational data,Akanksha Baid; Ian Rae; AnHai Doan; Jeffrey F Naughton,Keyword search (KWS) over relational data; where the answers are multiple tuplesconnected via joins; has received significant attention in the past decade. Numeroussolutions have been proposed and many prototypes have been developed. Building on thisrapid progress and on growing user needs; recently several RDBMS and Web companiesas well as academic research groups have started to examine how to build industrial-strength keywords search systems. This task clearly requires addressing many issues;including robustness; accuracy; reliability; and privacy; among others. A major emergingissue; however; appears to be performance related: current KWS systems haveunpredictable run time. In particular; for certain queries it takes too long to produce answers;and for others the system may even fail to return (eg; after exhausting memory). In this …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,8
User-Centric Research Challenges in Community Information Management Systems.,AnHai Doan; Philip Bohannon; Raghu Ramakrishnan; Xiaoyong Chai; Pedro DeRose; Byron J Gao; Warren Shen,Abstract In Cimple; a joint project between Wisconsin and Yahoo! Research; we are buildingsystems that manage information for online communities. In this paper we discuss thefundamental roles users play in such systems; then the difficult user-centric researchchallenges raised by these roles; with respect to contributing to the system; accessing andusing it; and leveraging the social interaction of users.,IEEE Data Eng. Bull.,2007,7
An abstraction-based approach to decision-theoretic planning for partially observable metric domains,AnHai Doan,*,*,1995,7
On Debugging Non-Answers in Keyword Search Systems.,Akanksha Baid; Wentao Wu; Chong Sun; AnHai Doan; Jeffrey F Naughton,ABSTRACT Handling non-answers is desirable in information retrieval systems. Current e-commerce websites usually try to suppress the somewhat dreaded message that no resultshave been found. Possible solutions include; for example; augmenting the data withsynonyms and common misspellings based on query logs. Nonetheless; this is onlyachievable if we can know the cause of the non-answers. Under the hood; most e-commercedata sits in some structured format. Debugging non-answers in the underlying KWS-Ssystems is therefore not trivial—non-answers in a KWS-S system could be a problem of thedata (eg; absence of some keywords); the schema (eg; missing key-foreign-key joins); ordue to empty join results from one of possibly several joins in the generated SQL queries. Sofar; we are unaware of any previous work that explores how to enable developers to …,EDBT,2015,6
Weighted proximity best-joins for information retrieval,Risi Thonangi; Hao He; AnHai Doan; Haixun Wang; Jun Yang,We consider the problem of efficiently computing weighted proximity best-joins over multiplelists; with applications in information retrieval and extraction. We are given a multi-termquery; and for each query term; a list of all its matches with scores; sorted by locations. Theproblem is to find the overall best matchset; consisting of one match from each list; such thatthe combined score according to a scoring function is maximized. We study three types offunctions that consider both individual match scores and proximity of match locations inscoring a matchset. We present algorithms that exploit the properties of the scoring functionsin order to achieve time complexities linear in the size of the match lists. Experiments showthat these algorithms greatly outperform the naive algorithm based on taking the crossproduct of all match lists. Finally; we extend our algorithms for an alternative problem …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,5
Crossing the structure chasm,Oren Etzioni; Alon Halevy; Anhai Doan; Zachary G Ives; Jayant Madhaven; Luke McDowell; Igor Tatarinov,Abstract It has frequently been observed that most of the world's data lies outside databasesystems. The reason is that database systems focus on structured data; leaving theunstructured realm to others. The world of unstructured data has several very appealingproperties; such as ease of authoring; querying and data sharing. In contrast; authoring;querying and sharing structured data require significant effort; albeit with the benefit of richquery languages and exact answers. We argue that in order to broaden the use of datamanagement tools; we need a concerted effort to cross this structure chasm; by importing theattractive properties of the unstructured world into the structured one. As an initial effort inthis direction; we introduce the REVERE System; which offers several mechanisms forcrossing the structure chasm; and considers as its first application the chasm on the …,*,2003,5
Collaborative Development of Information Integration Systems.,AnHai Doan; Robert McCann; Warren Shen,Abstract Developing information integration systems today is largely done by hand; in a verylabor intensive and error prone process. In this paper we describe MOBS; a conceptuallynew solution to this problem. MOBS employs mass collaboration. It asks the users of aninformation integration system to “pay” for using the system by answering simple questions;then uses those answers to further develop the system. This way an enormous burden ofsystem development is lifted from the system builders and spread “thinly” over a multitude ofusers. We present both simulation and real-world experiments which suggest that MOBScan build systems accurately and efficiently. Finally we discuss the potential applications ofMOBS beyond the information integration context.,AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors,2005,4
Social Media Analytics: The Kosmix Story.,Xiaoyong Chai; Omkar Deshpande; Nikesh Garera; Abhishek Gattani; Wang Lam; Digvijay S Lamba; Lu Liu; Mitul Tiwari; Michel Tourn; Zoheb Vacheri; STS Prasad; Sri Subramaniam; Venky Harinarayan; Anand Rajaraman; Adel Ardalan; Sanjib Das; AnHai Doan,Kosmix was a Silicon Valley startup founded in 2005 by Anand Rajaraman and VenkyHarinarayan. Initially targeting Deep Web search; in early 2010 Kosmix shifted its main focusto social media; and built a large infrastructure to perform social media analytics; for a varietyof real-world applications. In 2011 Kosmix was acquired by Walmart and converted into@WalmartLabs; the advanced research and development arm of Walmart. The goals of theacquisition were to provide a core of technical people in the Valley and attract more; to helpimprove traditional e-commerce for Walmart; and to explore the future of e-commerce. Thisfuture looks increasingly social; mobile; and local. Accordingly;@ WalmartLabs continues todevelop the social media analytics infrastructure pioneered by Kosmix; and uses it toexplore a range of social e-commerce applications. In this paper we describe social …,IEEE Data Eng. Bull.,2013,3
Logical expressiveness of semantic web languages for bibliographic information modeling,Karen M Wickett,The Semantic Web promises powerful new functionality for bibliographic databases bycombining bibliographic informa-tion with knowledge about the world. However; the kinds ofrepresentation that are possible in Semantic Web languages are not widely understood inthe LIS community. In partic-ular; the W3C Semantic Web ontology languages RDFS andOWL are not su ciently expressive to deliver the full range of inferences anticipated forbibliographic applications; and will require formalized rules from other speci cations.,*,2009,3
MetaSRA: normalized human sample-specific metadata for the Sequence Read Archive,Matthew N Bernstein; AnHai Doan; Colin N Dewey,Abstract Motivation: The NCBI's Sequence Read Archive (SRA) promises great biologicalinsight if one could analyze the data in the aggregate; however; the data remain largelyunderutilized; in part; due to the poor structure of the metadata associated with each sample.The rules governing submissions to the SRA do not dictate a standardized set of terms thatshould be used to describe the biological samples from which the sequencing data arederived. As a result; the metadata include many synonyms; spelling variants and referencesto outside sources of information. Furthermore; manual annotation of the data remainsintractable due to the large number of samples in the archive. For these reasons; it has beendifficult to perform large-scale analyses that study the relationships between biomolecularprocesses and phenotype across diverse diseases; tissues and cell types present in the …,Bioinformatics,2017,2
Social media; data integration; and human computation,Anhai Doan,Abstract Social media has emerged as a major frontier on the World-Wide Web; withapplications ranging from helping teenagers track Justin Bieber to e-commerce to fosteringrevolutions. In this talk I will discuss our work in this area; as carried out at Wisconsin;Kosmix; and@ WalmartLabs. I describe how we integrate data from" traditional" Websources to build a global taxonomy; greatly expand it with social-media data; then leverage itto build consumer-facing applications. Example applications include building topic pages;detecting Twitter events; and monitoring these events. I discuss the critical role of dataintegration and human computation in processing social media. Finally; I discuss how all ofthese can help the emerging area of social commerce; and why Walmart recently acquiredKosmix to make inroads into this new and exciting area.,Proceedings of the Ninth International Workshop on Information Integration on the Web,2012,2
User manual of tuffy 0.3,AnHai Doan; Feng Niu; Christopher Ré; Jude Shavlik; Ce Zhang,Markov logic networks (MLNs)[7; 1] have emerged as a powerful framework that combinesstatistical and logical reasoning; they have been applied to many data intensive problemsincluding information extraction; entity resolution; text mining; and natural languageprocessing. Based on principled data management techniques; Tuffy is an MLN engine thatachieves scalability and orders of magnitude speedup compared to prior artimplementations. It is written in Java and relies on PostgreSQL. For a brief introduction toMLNs and the technical details of Tuffy; please see our VLDB 2011 paper [4].,*,2011,2
Entity retrieval over structured data,Hui Fang; Rishi R Sinha; Wensheng Wu; AnHai Doan; ChengXiang Zhai,Entity retrieval is the problem of finding information about a given real-world entity (eg;director Peter Jackson) from one or a set of data sources. This problem is fundamental innumerous data management settings; but has received little attention. We define the generalentity retrieval problem; then discuss the limitations of current information systems (egrelational databases; search engines) in solving it. Next; we focus on the specific problem ofentity retrieval over structured data (as opposed to text or Web pages). We show that it isinherently more general and difficult than the actively-studied problem of entity matching (ierecord linkage). We then develop the ENRICH system; which significantly extends entitymatching solutions to perform entity retrieval. In particular; ENRICH employs clusteringtechniques to obtain a global picture on how many entities are" out there" and which data …,*,2005,2
Semantic integration workshop at the second international semantic web conference (ISWC-2003),AnHai Doan; Alon Y Halevy; Natalya F Noy,*,AI Magazine,2004,2
Mining for Information Discovery on the Web: Overview and Illustrative Research,Hwanjo Yu; AnHai Doan; Jiawei Han,Abstract The Web has become a fertile ground for numerous research activities in mining. Inthis chapter; we discuss research on finding targeted information on the Web. First; webriefly survey the research area. We focus in particular on two key issues:(a) mining toimpose structures over Web data; by building taxonomies and portals for example; to aid inWeb navigation; and (b) mining to build information processing systems; such as searchengines; question answering systems; and data integration systems. Next; we describe tworecent Web mining projects that illustrate the use of mining techniques to address the abovetwo key issues. We conclude by briefly discussing novel research opportunities in the areaof mining for information discovery on the Web.,*,2004,2
Semantic Integration,AnHai Doan; Alon Halevy; Natasha Noy,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: semantics & computational sc... Search; Register; Log in …,*,2003,2
Research on statistical relational learning at the university of washington,Pedro Domingos; Yeuhi Abe; Corin Anderson; AnHai Doan; Dieter Fox; Alon Halevy; Geoff Hulten; Henry Kautz; Tessa Lau; Lin Liao; Jayant Madhavan; D Patterson Mausam; Matthew Richardson; Sumit Sanghai; Daniel Weld; Steve Wolfman,Abstract This paper presents an overview of the research on learning statistical models fromrelational data being carried out at the University of Washington. Our work falls into five maindirections: learning models of social networks; learning models of sequential relationalprocesses; scaling up statistical relational learning to massive data sources; learning forknowledge integration; and learning programs in procedural languages. We describe someof the common themes and research issues arising from this work.,Proceedings of the IJCAI-2003 Workshop on Learning Statistical Models from Relational Data,2003,2
Data Integration: A “Killer App” for Multistrategy Learning,AnHai Doan; Pedro Domingos; A Levy,Abstract To build a data-integration system; the application designer must specify amediated schema and supply the descriptions of data sources. A source descriptioncontains a source schema that describes the content of the source; and a mapping betweenthe corresponding elements of the source schema and the mediated schema. Manuallyconstructing these mappings is both laborintensive and error-prone; and has proven to be amajor bottleneck in deploying large-scale data integration systems in practice. In this paperwe report on our initial work toward automatically learning mappings between sourceschemas and the mediated schema. Specifically; we investigate finding one-to-onemappings for the leaf elements of source schemas. We describe LSD; a system thatautomatically finds such mappings. LSD consults a set of learner modules; where each …,*,2000,2
Abstraction for Decision-Theoretic Planning,AnHai Doan; Peter Haddawy,Abstract We present a general theory of action abstraction for reducing the complexity ofdecision-theoretic planning. We discuss how to abstract sets of actions; action branches;and sequences of actions. We develop projection rules for abstract actions and prove ourabstraction techniques correct with respect to projection. In developing the theory weintroduce the notion of generalized mass assignments.,*,1995,2
PANEL Crowds; Clouds; and Algorithms: Exploring the Human Side of “Big Data” Applications,Anhai Doan,Abstract The creation; collection; analysis; curation; and dissemination of data have becomeprofoundly democratized. Social networks spanning 100's of millions of users enableinstantaneous discussion; debate; and information sharing [8]. Streams of tweets; blogs;photos; and videos identify breaking events faster and in more detail than ever before [10].Global; ad hoc collaborations addressing scientific; commercial; political; and evenmathematical problems make progress where individual investigators or small groupscannot [6; 11; 12; 13; 14]. This sea change is the result of a confluence of informationtechnology advances in areas such as intensively networked systems; cloud computing;social computing; and pervasive devices. The connectivity of billions of device-enabledpeople to massive cloud-computing infrastructure has created a new dynamic that is,*,*,2
Towards Interactive Debugging of Rule-based Entity Matching.,Fatemah Panahi; Wentao Wu; AnHai Doan; Jeffrey F Naughton,ABSTRACT Entity Matching (EM) identifies pairs of records referring to the same real-worldentity. In practice; this is often accomplished by employing analysts to iteratively design andmaintain sets of matching rules. An important task for such analysts is a “debugging” cycle inwhich they make a modification to the matching rules; apply the modified rules to a labeledsubset of the data; inspect the result; and then perhaps make another change. Our goal is tomake this process interactive by minimizing the time required to apply the modified rules. Wefocus on a common setting in which the matching function is a set of rules where each rule isin conjunctive normal form (CNF). We propose the use of “early exit” and “dynamicmemoing” to avoid unnecessary and redundant computations. These techniques create anew optimization problem; and accordingly we develop a cost model and study the …,EDBT,2017,1
CloudMatcher: A Cloud/Crowd Service for Entity Matching,Yash Govind; Erik Paulson; Mukilan Ashok; Paul Suganthan GC; Ali Hitawala; AnHai Doan; Youngchoon Park; Peggy L Peissig; Eric LaRose; Jonathan C Badger,ABSTRACT Entity matching (EM) finds disparate data instances that refer to the same real-world entity. EM is critical in health informatics; and will become even more so in the age ofBig Data and data science. Many EM systems have been developed. In this paper; we firstdiscuss why it is still very difficult for domain scientists to use such EM systems. We thendescribe CloudMatcher; a cloud/crowd service for EM that we have been building.CloudMatcher aims to be a fast; easy-to-use; scalable; and highly available EM service onthe Web. We motivate CloudMatcher then describe its design and implementation. Next; wedescribe its deployment in the past six months; providing a detailed analysis of itsperformance over four representative datasets. Finally; we discuss lessons learned.,*,2016,1
MetaSRA: normalized sample-specific metadata for the Sequence Read Archive,Matthew N Bernstein; AnHai Doan; Colin N Dewey,Motivation: The NCBI9s Sequence Read Archive (SRA) promises great biological insight ifone could analyze the data in the aggregate; however; the data remain largely underutilized;in part; due to the poor structure of the metadata associated with each sample. The rulesgoverning submissions to the SRA do not dictate a standardized set of terms that should beused to describe the biological samples from which the sequencing data are derived. As aresult; the metadata include many synonyms; spelling variants; and references to outsidesources of information. Furthermore; manual annotation of the data remains intractable dueto the large number of samples in the archive. For these reasons; it has been difficult toperform large-scale analyses that study the relationships between biomolecular processesand phenotype across diverse diseases; tissues; and cell types present in the SRA …,bioRxiv,2016,1
Developing; tuning; and using schema matching systems,Yoonkyong Lee,This dissertation studies the schema matching problem that finds semantic correspondences(called matches) between disparate data sources. Examples of semantic matches include“location= address” and “name= concat (first-name; last-name).” Schema matching is one ofthe key challenges for many data sharing and exchange applications. Prime examples ofsuch applications arise in numerous contexts; including data warehousing; scientificcollaboration; e-commerce; bioinformatics; and data integration on the World Wide Web.Despite significant progress; many challenges remain. These include discovering complexmatches; a prevalent problem in practice; tuning a matching system; and deploying amatching system effectively in an application. In this dissertation; we develop solutions forthe three challenges mentioned above. First; we develop a system that discovers both …,*,2010,1
Best-effort data integration,A Doan,This position statement makes the case that; since “exact” data integration is AI complete; weshould seriously consider “best-effort” data integration. I will start by briefly summarizing thedevelopment of the field of data integration. Next; I discuss best-effort data integration as oneof the next logical research directions; then sketch a simple observation that can beleveraged to examine the topic systematically and to understand current work. Finally; Idescribe current research on the topics at the University of Wisconsin-Madison; and list openquestions that we are considering.,NSF Workshiop on Data Integration,2006,1
Collective data integration for virtual organizations,Robert McCann; Warren Shen; A Doan,*,SIGMOD-04 Workshop on Databases in Virtual Organizations (DIVO-04),2004,1
Learning Complex Semantic Mappings between Structured Representations,AnHai Doan; Pedro Domingos; Alon Y Halevy,Abstract Establishing semantic mappings between multiple representations is fundamentalto interoperability efforts in data integration; the Semantic Web; knowledge sharing; andinformation agents. Manually creating such mappings is extremely tedious and error-prone.Hence many recent works have focused on developing techniques to automate the mappingprocess. However; these works deal only with one-to-one (1-1) mappings; the simplest typeof mapping. They do not consider the important class of more complex mappings. Wedescribe the COMAP system which semi-automatically discovers both 1-1 and morecomplex mappings. The key distinguishing feature of COMAP is that it incorporates multipletypes of knowledge; search; and learning techniques in every important stage of themapping process. This feature helps maximize mapping accuracy; and makes the sytem …,Unpublished Manuscript,2002,1
E ciently Ordering Query Plans for Data Integration,AnHai Doan; Alon Levy,Abstract We describe Streamer; the query-reformulation component of a data integrationsystem. Given a utility measure and a user query; Streamer uses abstraction-based renement planning and exploits information on plan independence to produce; in decreasingorder of utility; a set of plans that access data sources to obtain answers to the query. Wethen focus on plan coverage as an important utility measure. We show how to use statisticinformation about the domain and data sources to estimate plan coverage; and how toincorporate the plancoverage framework into Streamer. In doing so; we provide the rstmethod for e ectively integrating the use of quantitative information into the query optimizerof a data-integration system. We present preliminary experimental results suggesting thatStreamer runs an order of magnitude faster than brute-force plan-ordering methods …,Proceedings of IEEE International Conference on Data Engineering,2002,1
Learning to map between ontologies,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alan Halevy,*,World Wide Web Conference,*,1
Toward a System Building Agenda for Data Integration,AnHai Doan; Adel Ardalan; Jeffrey R Ballard; Sanjib Das; Yash Govind; Pradap Konda; Han Li; Erik Paulson; Haojun Zhang,Abstract: In this paper we argue that the data management community should devote farmore effort to building data integration (DI) systems; in order to truly advance the field.Toward this goal; we make three contributions. First; we draw on our recent industrialexperience to discuss the limitations of current DI systems. Second; we propose an agendato build a new kind of DI systems to address these limitations. These systems guide usersthrough the DI workflow; step by step. They provide tools to address the" pain points" of thesteps; and tools are built on top of the Python data science and Big Data ecosystem(PyData). We discuss how to foster an ecosystem of such tools within PyData; then use it tobuild DI systems for collaborative/cloud/crowd/lay user settings. Finally; we discuss ongoingwork at Wisconsin; which suggests that these DI systems are highly promising and …,arXiv preprint arXiv:1710.00027,2017,*
Human-in-the-Loop Challenges for Entity Matching: A Midterm Report,AnHai Doan; Adel Ardalan; Jeffrey Ballard; Sanjib Das; Yash Govind; Pradap Konda; Han Li; Sidharth Mudgal; Erik Paulson; GC Suganthan; Haojun Zhang,Abstract Entity matching (EM) has been a long-standing challenge in data management. Inthe past few years we have started two major projects on EM (Magellan andCorleone/Falcon). These projects have raised many human-in-the-loop (HIL) challenges. Inthis paper we discuss these challenges. In particular; we show how these challenges forcedus to revise our solution architecture; from a typical RDBMS-style architecture to a veryhuman-centric one; in which human users are first-class objects driving the EM process;using tools at pain-point places. We discuss how such solution architectures can be viewedas combining" tools in the loop" with" human in the loop". Finally; we discuss lessonslearned which can potentially be applied to other problem settings. We also hope that moreresearchers will investigate EM; as it can be a rich" playground" for HIL research.,Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,2017,*
What is Our Agenda for Data Science?,AnHai Doan,Data science (DS) has emerged as a major interdisciplinary field; and has attracted muchattention. Our field however has been slow in reacting to this development. In this abstract Iargue that DS is here to stay and will become even more important; and that we have a lot tocontribute; but if we do not ramp up our efforts; we risk becoming increasingly irrelevant. Inparticular; I believe the time has come for us to perhaps develop a data science agenda thatbuilds on our strengths; attracts a broad participation from our community; and helps usshape this emerging field. The set of topics that this agenda can discuss includes (but is notlimited to) the following:,CIDR,2017,*
Systems and methods for event stream processing,*,Disclosed are systems and methods for processing events in an event stream using a map-update application. The events may be embodied as a key-attribute pair. An event isprocessed by one or more instances implementing either a map or an update function. Amap function receives an input event from the event stream and publishes one or moreevents to the event stream. An update function receives an event and updates acorresponding slate and publishes zero or more events. Systems and methods are alsodisclosed herein for implementing a map-update application in a multithreaded architectureand for handling overloading of a particular thread or node. Systems and methods forproviding access to slates updated according to update operations are also disclosed.,*,2015,*
Systems and methods for event stream processing,*,Disclosed are systems and methods for processing events in an event stream using a map-update application. The events may be embodied as a key-attribute pair. An event isprocessed by one or more instances implementing either a map or an update function. Amap function receives an input event from the event stream and publishes one or moreevents to the event stream. An update function receives an event and updates acorresponding slate and publishes zero or more events. Systems and methods are alsodisclosed herein for implementing a map-update application in a multithreaded architectureand for handling overloading of a particular thread or node. Systems and methods forproviding access to slates updated according to update operations are also disclosed.,*,2015,*
Development and Demonstration of a New Testing Capability for Simulating Multiple Fuel Assembly Impact During a Seismic Event,W Zhao; YC Lee; H Kunishi; Z Karoutas; A Doan,abstract As an important part in both product development and analytical model validation;Westinghouse continues to develop prototype testing capabilities. Towards betterunderstanding fuel assembly's structural performance during a seismic event; our recenteffort was to develop a new testing capability for simulating multiple fuel assembly impactthat may occur during a seismic event. The test system was developed by adding apendulum system to our existing fuel assembly mechanical test stand. Followingqualification of the testing system; a prototype fuel assembly was tested using selectedpendulum parameters. Fuel assembly displacement; impact loads at various contactlocations; and the spacer grid deformation as a function of impact forces were obtained. Thetest results provided new insights and understanding of the fuel assembly's structural …,18th International Conference on Nuclear Engineering,2010,*
Building Structured Web Community Portals Via Extraction; Integration; and Mass Collaboration,An-Hai Doan,Abstract The World-Wide Web hosts numerous communities; each focusing on a particulartopic. As such communities proliferate; so do efforts to build community portals. Most currentportals are organized according to topic taxonomies. Recently; however; there has been agrowing effort to build structured data portals (eg; IMDB; Citeseer) that present a unified viewof entities and relationships in the community. Such portals can prove extremely valuable ina wide range of domains. But how can we build them efficiently? In this talk; I will present anew research vision that addresses this question. The goal is to develop a system that asmall team (or ideally just one person) can quickly deploy to build an initial (but alreadyuseful) structured portal; then leverage the entire community in a mass collaboration fashionto improve and expand this portal. As such; the research agenda requires combining and …,Pacific Rim International Conference on Artificial Intelligence,2008,*
Understanding; Estimating; and Incorporating Output Quality Into Join Algorithms For Information Extraction,Alpa Jain; Panagiotis G Ipeirotis; Luis Gravano; Anhai Doan,Information extraction (IE) systems are trained to extract specific relations from textdatabases. Real-world applications often require that the output of multiple IE systems bejoined to produce the data of interest. To optimize the execution of a join of multipleextracted relations; it is not sufficient to consider only execution time. In fact; the quality of thejoin output is of critical importance: unlike in the relational world; different join executionplans can produce join results of widely different quality whenever IE systems are involved.In this paper; we develop a principled approach to understand; estimate; and incorporateoutput quality into the join optimization process over extracted relations. We argue that theoutput quality is affected by (a) the configuration of the IE systems used to process thedocuments;(b) the document retrieval strategies used to retrieve documents; and (c) the …,*,2008,*
Tom Crecelius 1480 Carlo A. Curino 761; 882 Emiran Curtmola 1408; 1448 D Harish D. 1124; 1325,Florian Daniel; David DeWitt; Amol Deshpande; AnHai Doan; Marcus Fontoura; Juliana Freire; Venkatesh Ganti; Hong Gao; Hector Garcia-Molina; Minos Garofalakis; Charles Garrod; Tingjian Ge; Lise Getoor; Phillip Gibbons; Lukasz Golab; Wojciech Golab; Yihong Gong; Albert Greenberg; Maxim Grinev; Peter Haas; Wook-Shin Han; Michael Hay; Monika Henzinger; Mauricio Hernandez; Mark Hill; Howard Ho; Allison Holloway; Mingsheng Hong; Chien-Yi Hou; Yanli Hu; Kien Hua; Jiansheng Huang; Ihab Francis Ilyas; Zachary G Ives; Marie Jacob; HV Jagadish; Magesh Jayapandian; David Jensen; Haifeng Jiang; Cheqing Jin; Ryan Johnson; Theodore Johnson; Vanja Josifovski,*,*,2008,*
UNIVERSITY OF,Raghu Ramakrishnan,Abstract Increasingly; Web data is displayed in pages generated according to a template(eg; product listings at amazon. com; faculty directories; class schedules). This trend makesstructured querying of such Web data a valuable capability for a growing number ofapplications; including many ad hoc; exploratory; and short-lived tasks. Unfortunately;current methods for answering such queries require writing complex Perl scripts or creatingcustomized wrappers and storing the extracted data in a DBMS; which is often overkill forthese types of on-the-fly tasks. In this paper we propose SL/C; a solution to this problem.Given a set of Web pages generated according to some templates; SLIC allows the user toquickly pose SQL queries; obtain initial results; and then iterate with the system to getincreasingly better results. At each step; SL/C asks relatively simple questions to solicit …,*,2006,*
UNIVERSITY OF,Robert McCann; Pedro DeRose; AnHai Doan; Raghu Ramakrishnan,Abstract Increasingly; Web data is displayed in pages generated according to a template(eg; product listings at amazon. com; faculty directories; class schedules). This trend makesstructured querying of such Web data a valuable capability for a growing number ofapplications; including many ad hoc; exploratory; and short-lived tasks. Unfortunately;current methods for answering such queries require writing complex Perl scripts or creatingcustomized wrappers and storing the extracted data in a DBMS; which is often overkill forthese types ofon-the-fly taslcs. In this paper we propose SLIC; a solution to this problem.Given a set of Web pages generated according to some templates; SLIC allows the user toquickly pose SQL queries; obtain initial results; and then iterate with the system to getincreasingly better results. At each step; SLIC asks relatively simple questions to solicit …,*,2006,*
MEDIATE: Learning to Match Entity Mentions across Text and Databases,AnHai Doan; Xin Li; Dan Roth,Many real-world applications increasingly involve both structured data and text. A given real-world entity is often referred to in different ways; such as``Helen Hunt''; and``Mrs. HE Hunt'';both within and across the structured data and the text. Due to this {\em semanticheterogeneity}; it remains extremely difficult to glue together information about real-worldentities from the available data sources and effectively utilize both types of information. Thispaper describes the\mediate\system which automatically matches entity mentions {\emwithin\/} and {\em across\/} both text and databases. The system can handle multiple types ofentities (eg; people; movies; locations); is easily extensible to new entity types; and operateswith no need for annotated training data. Given a relational database and a set of textdocuments;\mediate\learns from the data a {\em generative model\/} that provides a …,*,2006,*
Proceedings of the Eight International Workshop on the Web and Databases,A Doan; Frank NEVEN; R McCann; Geert Jan BEX,ABSTRACT Recently; there has been increased interest in the retrieval and integration ofhidden-Web data with a view to leverage high-quality information available in onlinedatabases. Although previous works have addressed many aspects of the actual integration;including matching form schemata and automatically filling out forms; the problem of locatingrelevant data sources has been largely overlooked. Given the dynamic nature of the Web;where data sources are constantly changing; it is crucial to automatically discover theseresources. However; considering the number of documents on the Web (Google alreadyindexes over 8 billion documents); automatically finding tens; hundreds or even thousandsof forms that are relevant to the integration task is really like looking for a few needles in ahaystack. Besides; since the vocabulary and structure of forms for a given domain are …,*,2005,*
Workshop on the Integration of Information Retrieval and Databases (WIRD’04) 29,Vojkan Mihajlovic; Djoerd Hiemstra; Henk Ernst Blok; Peter MG Apers; Mayssam Sayyadian; Azadeh Shakery; AnHai Doan; ChengXiang Zhai,This workshop is the third in the series of XML and Information Retrieval workshops thatwere held in at SIGIR'2000 (Athens; Greece; see SIGIR Forum Fall 2000 issue at http://www.acm. org/sigir/forum/S2000/XML_report. pdf) and SIGIR'2002 (Tampere; Finland; see SIGIRForum Fall 2002 issue at http://www. acm. org/sigir/forum/F2002/maarek. pdf). Itcomplements the INEX (Initiative for the Evaluation of XML Retrieval) meetings that havebeen organized for the last two years; by providing researchers a useful forum for discussing(before implementing) and evaluating their models at INEX in the second half of the year.We pursue the exchange of ideas between researchers who are now active in this IR sub-field and attract new interested researchers; on issues related to the application of IRmethods to XML data for querying; retrieval; navigating; etc. We have gone a long way …,*,2004,*
The Semantic Integration Workshop at ISWC 2003,A Doan; A Halevy; N Noy,*,SIGMOD RECORD,2004,*
iMAP,Robin Dhamankar; Yoonkyong Lee; AnHai Doan; Alon Halevy; Pedro Domingos,Abstract Creating semantic matches between disparate data sources is fundamental tonumerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. Todate; however; virtually all of these works deal only with one-to-one (1-1) matches; such asaddress= location. They do not consider the important class of more complex matches; suchas address= concat (city; state) and room-price= room-rate*(1+ tax-rate). We describe theiMAP system which semi-automatically discovers both 1-1 and complex matches. iMAPreformulates schema matching as a search in an often very large or infinite match space. Tosearch effectively; it employs a set of searchers; each discovering specific types of complexmatches. To further improve matching accuracy; iMAP exploits a variety of domain …,Proceedings of the ACM SIGMOD International Conference on Management of Data; SIGMOD 2004,2004,*
Semantic Integration Workshop (SI-2003),AnHai Doan; Alon Halevy; Natasha Noy,In numerous distributed environments; including today's World-Wide Web; organizationalintranets; and the emerging Semantic Web; the applications will inevitably use theinformation described by multiple ontologies and schemas. Interoperability amongapplications depends critically on the ability to map between them. Today; matchingbetween ontologies and schemas is still largely done by hand; in a laborintensive and error-prone process. As a consequence; semantic integration issues have now become a keybottleneck in the deployment of a wide variety of information management applications. Thehigh cost of this bottleneck has motivated numerous research activities on methods fordescribing mappings; manipulating them; and generating them semi-automatically. Thisresearch has spanned several communities (Databases; AI; WWW); but unfortunately …,*,2003,*
TOIS reviewers,Gianni Amati; Vasilis Hatzivassiloglou; Mandar Mitra; Alberto Apostolico; David Hawking; Vibhu Mittal; Nick Belkin; David Hendry; Alistair Moffat; Kwong Bor Ng; Jon Herlocker; Shlomo Moran; Chris Buckley; Djoerd Hiemstra; Amar Mukherjee; Jaime Carbonell; Teruo Higashino; Gheorgie Muresan; Raman Chandrasekar Alexander Hinneburg; Smaranda Muresan; Kaushik Chakrabarti Thomas Hofmann; Ramesh Nallapati; Michael Chau; Eduard Hovy; Jay Ponte; Francine Chen; Adele Howe; Allison Powell; John Cherbini; Zan Huang; Sunil Prabhakar; Jan Chomicki; David Hull; John Prager; Wingyan Chung; Panagiotis Ipeirotis; Dragomir Radev; Charlie Clark; David Jensen; Philip Resnik; Michael Collins; George Karypis; Berthier Ribeiro-Neto; Colleen Cool; Vikrant Kobla; Mirek Riedewald; Nick Craswell; Juergen Koennemann; Steve Robertson; Brian Davison; KL Kwok; Mark Roantree; Mark Derthick; Victor Lavrenko; Monica Rbogati; AnHai Doan; Dawn Lawrie; Motaz El Saban; Judith Donath; Lillian Lee; Shin’ichi Satoh; Daryl D’Souza; Chienting Lin; Jacques Savoy; Mark Dunlop; Chin-Yew Lin; Stan Sclaroff; David Eichman; Thomas DC Little; Eric Selberg; Christos Faloutsos; Bob Losee; Vaughan Shanks; Norbert Fuhr; Chris LuVogt; Amit Singhal; Richard Furuta; Sofus Macskassy; Ian Soberoff; Helena Galhardas; Stephane Marchand-Maillet John Smith; Mark Ginsburg; Sharad Mehrotra; Peter Smith; Eric Glover; Weiyi Meng; Robert Spence,TOIS is dependent on its editorial board to identify and manage reviewers and on thosereviewers to write critical assessments of submitted papers. The review process is crucial tothe quality of any journal and the editors and authors find TOIS reviews to be rigorous;comprehensive; and insightful. It is clear that reviewers are vigilant in insuring that only thebest work is published in TOIS. Over the past two years; the following scholars haveprovided reviews for TOIS. Associate Editors are not listed here; even though theysometimes serve as reviewers for TOIS papers managed by other Associate Editors beyondtheir responsibilities to synthesize all the reviews for papers they manage. The TOISreadership owes a debt of gratitude to these dedicated scholars.,ACM Transactions on Information Systems,2003,*
Proceedings of the ACM SIGMOD International Conference on Management of Data: ACM SIGMOD 2003; June 9-12; 2003; San Diego; California,International Conference on Management of Data; Special Interest Group on Management of Data Association for Computing Machinery,*,*,2003,*
Database research at the University of Illinois at Urbana-Champaign,Marianne Winslett; K Chang; A Doan; Jiawei Han; ChengXiang Zhai; Yuanyuan Zhou,The Department of Computer Science at the University of Illinois at Urbana-Champaign(UIUC) has identified the area of information systems; broadly construed; as one of threecore areas for the department's future directions. In tandem with a mandate from the UIUCCollege of Engineering for the department to double its number of tenure-track faculty; thisfocus on information systems has resulted in a significant expansion of the number of facultyin the department in the database area over the past few years. Our roster currently includesMarianne Winslett; who joined the department in 1987; Kevin Chang and Jiawei Han; whojoined us in 2000 and 2001; respectively; and AnHai Doan; Chengxiang Zhai; andYuanyuan Zhou; who joined the department in 2002. In the near future; we plan to round outthe information systems group with additional hires of senior faculty.,ACM SIGMOD Record,2002,*
USER MANUAL DRIPS Decision Theoretic Planning System Version 3.0,AnHai Doan; Peter Haddawy,*,*,1996,*
D ecision-Theoretic R efinem ent Planning in M edical,Peter Haddawy; AnHai Doan; Charles E Kahn Jr,*,*,1996,*
DRIPS Decision Theoretic Planning System Version 4.1 USER MANUAL,AnHai Doan; Peter Haddawy; Richard Goodwin,*,*,1995,*
bigNN: an open-source big data toolkit focused on biomedical sentence classification,Ahmad P Tafti; Ehsun Behravesh; Mehdi Assefi; Eric LaRose; Jonathan Badger; John Mayer; AnHai Doan; David Page; Peggy Peissig,Abstract—Every single day; a massive amount of text data is generated by different medicaldata sources; such as scientific literature; medical web pages; health-related social media;clinical notes; and drug reviews. Processing this wealth of data is indeed a daunting task;and it forces us to adopt smart and scalable computational strategies; including machineintelligence; big data analytics; and distributed architecture. In this contribution; we designedand developed an open-source big data neural network toolkit; namely bigNN which tacklesthe problem of large-scale biomedical text classification in an efficient fashion; facilitatingfast prototyping and reproducible text analytics researches. bigNN scales up a word2vec-based neural network model over Apache Spark 2.10 and Hadoop Distributed File System(HDFS) 2.7. 3; allowing for more efficient big data sentence classification. The toolkit …,*,*,*
CAREER: Evolving and Self-Managing Data Integration Systems,AnHai Doan,Data integration has been a long standing challenge for the database community. Indeed;all six “white papers” on future research directions that our community has published (in1989-2003) acknowledged the growing need for integrating data from multiple sources [15;136; 137; 135; 16; 3]. This need has now become critical in numerous contexts; includingintegrating data on the Web and at enterprises; building e-commerce market places; andanalyzing data for scientific research [3]. Consequently; much research has been conductedon data integration; and many integration architectures have been proposed [56; 134; 122;138; 139; 5; 22; 141; 61; 71](see [55] for detailed surveys). A well-known and importantarchitecture is that of virtual data integration systems; which provide a uniform queryinterface over a multitude of data sources [65; 98; 141; 82; 92; 63; 87; 66; 86]. Over the …,*,*,*
Learning From Multiple Users to Improve Accuracy of Data Integration Tasks,Robert McCann; Alexander Kramnik; Warren Shen; Vanitha Varadarajan; Olu Sobulo; AnHai Doan,• Tools have limited accuracy high ownership cost – a key bottleneck to widespread deploymentof DI systems • We proposed the MOBS solution – make tools learn from multitude of users toimprove accuracy – ask questions that are easy for humans; hard for machines • Experimentsshowed 9-60% accuracy gain; 29- 88% workload reduction; and often overall benefits • Benefitsof MOBS – speed up integration process – build systems where not previously possible – freebuilder to further improve the system See WebDB-03; TechReport-05 at http://anhai.cs.uiuc.edu/home/projects/mobs.html … Learning From Multiple Users to Improve Accuracy of Data IntegrationTasks Robert McCann; Alexander Kramnik; Warren Shen; Vanitha Varadarajan; Olu Sobulo;AnHai Doan ¢¡¤£¦¥¨§ © £ "!$#%#%£%¡¨ &£' ) (0 ¢©2134¡53 … How to Solicit User AnswersHow to Modify Data Integration Tools … Find books written by Isaac Asimov & priced …,*,*,*
Data Engineering,X Chai; O Deshpande; N Garera; A Gattani; W Lam; DS Lamba; L Liu; M Tiwari; M Tourn; Z Vacheri; STS Prasad; S Subramaniam; V Harinarayan; A Rajaraman; A Ardalan; S Das; P Suganthan; AH Doan; Kenneth M Anderson; Aaron Schram; Ali Alzabarah; Leysia Palen; James Caverlee; Zhiyuan Cheng; Daniel Z Sui; Krishna Y Kamath; Evangelos E Papalexakis; U Kang; Christos Faloutsos; Nicholas D Sidiropoulosx; Abhay Harpale; Xintian Yang; Yiye Ruan; Srinivasan Parthasarathy; Amol Ghoting,Maintaining the vitality of the database community within the IEEE Computer Society isimportant for the long term health of the database field. In his role as TCDE Chair; Kyu-Young Whang has initiated a number of new activities designed for this purpose. Each isdescribed below. Awards: The TCDE now supports a number of awards for outstandingwork; both of a technical nature and for professional contributions. Amr El Abadi has led thiseffort. For technical work; there is the Computer Science; Engineering and Education ImpactAward. For young members of our community; there is the TCDE Early Career Award. Andfinally; there is the TCDE Service Award for contributions to the community. These awardsare described in detail at http://tab. computer. org/tcde/tcdeawards. html.,*,*,*
Adel Ardalan,A Ardalan; W Cai; Q Wan; N Garera; A Doan; X Chai; O Deshpande; A Gattani; W Lam; DS Lamba; L Liu; M Tiwari; M Tourn; Z Vacheri; S Prasad; S Subramaniam; V Harinarayan,• Hybrid machine-human clustering for attribute value normalization• Highly scalable eventextraction in the Twittersphere from legacy tweet stores• Slot filling for TAC/MR-KBP usinglogistic regressors on large-scale data Computational systems biology; mathematicalmodeling and bioinformatics• Empirical topology for analyzing the dynamics in complexsystems• Protein identification/quantification from (tandem) mass spectrometry data Artificialintelligence; machine learning and numerical optimization,*,*,*
Building Data Integration Systems via Mass Collaboration,AnHai Doan,Page 1. AnHai Doan Dept. of Computer Science Univ. of Illinois; Urbana-Champaign Joint workwith Robert McCann; Vanitha Varadarajan; & Alexander Kramnik WebDB 2003 Building DataIntegration Systems via Mass Collaboration Page 2. 2 Architecture of Data Integration Systemmediated schema powell.com amazon.com source schema 2 bn.com source schema 3 sourceschema 1 Find books written by Isaac Asimov & priced under $15 wrapper wrapper wrapperPage 3. 3 Current State of Affairs ● Vibrant research & industrial landscape ● Research – datedback to the 70-80s; accelerated in recent years – focused on – conceptual & algorithmic aspects –building specialized systems ● Industry – more than 50 startups in 2001 Despite much R&Dactivities; however … Page 4. 4 ... DI Systems still Incur Very High Cost of Ownership! ● Mostsystems are still deployed manually by system admins …,*,*,*
Event Extraction in The Twittersphere,Adel Ardalan; Qian Wan; Nikesh Garera; AnHai Doan; Jignesh Patel,Twitter is an online microblogging platform which allows its users to post messages of lengthup to 140 characters. Users share their opinions; and promote and discuss current events.An event in the Twittersphere happens when many users tweet about a subject (possibly; areal-world event) at a particular time. Table 1 shows some sample events and theirattributes. An emerging event is an event such that the number of Twitter users tweetingabout it rises to a significant number; over a certain period of time. That is; an event is anemerging one at a particular time if (1) a considerable number of people are discussing it (itis hot) and (2) there are considerably more people talking about the event than before (it isemerging).,*,*,*
Research Program Committee,Yanif Ahmad; Aris Anagnostopoulos; Walid Aref; Ismail Ari; Shivnath Babu; Zohra Bellahsene; II Elisa Bertino; Claudio Bettini; Michael Bohlen; Paolo Boldi; Francesco Bonchi; Peter Boncz; CWI Angela Bonifati; Vinayak Borkar; Christof Bornhoevd; SAP Randal Burns; Andrea Cali; Selcuk Candan; Barbara Carminati; Deepayan Chakrabarti; Chee Yong Chan; Shimin Chen; Pittsburgh Su Chen; Yi Chen; Reynold Cheng; Sarah Cohen-Boulakia; LRI Orsay; Gao Cong; Mariano Consens; Isabel Cruz; Bin Cui; Colazzo Dario; Gautam Das; Anish Das Sarma; Khuzaima Daudjee; Antonios Deligiannakis; Stefan Dessloch; Anhai Doan; Eduard Dragut,Yanif Ahmad; Johns Hopkins University Aris Anagnostopoulos; Sapienza University of RomeWalid Aref; Purdue University Ismail Ari; Ozyegin University Soeren Auer; Leipzig School of MediaShivnath Babu; Duke University Roger Barga; Microsoft Zohra Bellahsene; University of MontpellierII Elisa Bertino; Purdue University Claudio Bettini; University of Milan Michael Bohlen; Universityof Zurich Paolo Boldi; University of Milan Francesco Bonchi; Yahoo! Research Peter Boncz; CWIAngela Bonifati; ICAR-CNR; Italy Vinayak Borkar; University of California; Irvine ChristofBornhoevd; SAP Randal Burns; Johns Hopkins University Andrea Cali; University of Oxford SelcukCandan; Arizona State University Barbara Carminati; University of Insubria; Italy DeepayanChakrabarti; Yahoo! Research Chee Yong Chan; National University of Singapore BadrishChandramouli; Microsoft Gang Chen; Zhejing University; China Shimin Chen; Intel Labs …,*,*,*
Abiteboul; S. 41 Aggarwal; CC 261;593 Agrawal; D 93; 274;496;639 Agrawal; S. 5,M Akinde; S AI-Khalifa; G Alonso; M Areal; WG Aref; V Atluri; I Atmosukarto; D Baker; R Barga; K Barker; B Benatallah; G Bhalotia; HE Blok; M Bohlen; A Bonifati; D Braga; S Bressan; N Bruno; F Buccafurri; A Campi; F Casati; AC Catlin; S Ceri; S Chakrabarti; NH Chan; S Chaudhuri; B Chen; CM Chen; J Chen; MS Chen; F Chiu; J Cho; HD Chon; L Cohen; B Cooper; R Cordova; G Cormode; G Das; S Davey; U Daya; S Decker; A Descour; A Deshpande; J Desmarais; DJ DeWitt; A Doan; M Dumas; J Dunn; MG Elfeky; CJ El1mann; AK Elmagarmid; R Elmasri; C Fa1outsos; J Fan; A Faradjian; P Felber; J Feng; S Flesca; I Foudos; J Freire; AW Fu; F Furfaro; A Gal; H Garcia-Molina; M Garofalakis; J Gehrke; D Georgakopoulos; M Gertz; A Goel,333 369 264 331 129 567; 685 ; 673 ; 490 ; 266 ; 271 263 29 393 490 212;276 716 498 492271 498 431 176 490 605 29 141 278;463 265; 663 335 488 262 ; 494 ; 266 ; 166 129 309 309333 494 275 673 309 141;567;605 309 262 453 706 329 … 267 269 176 583 269 617 268; 268 543 269;327 155;331;335 155 697 555 507 279 333 267 583 41;369 490 271 29 117 331685 333 212;498 685 685 ; 245 605 333 270 431 529 345;697 271 ;498 273 583 272 297 ;485685 274 275 333 265;663 327 … Ounopulos; D PUO; J. ..; Gtirel; A. Haas; L. ãas; p .J.Haclgtimti; H. I … ¥alevy; A. ãmmad; M. ¥aritsa; JR ¥ellerstein; J .M … Lee; D. Lee; MLLehner; W Leung; CK-S Ling; T. W ' … Ling; Y. Liu; B Liu; J Lomet; D. Low; WL Lu; H. ; Lu; JXLuo; G. Madden; S. Madhyastha; T. Maier; D. Major; G. Mani; M. Mannila; H Marian; A.,*,*,*
Richard Johnson Teresa Johnson Roy Ju Hong-Seok Kim,Vladimir Kiriansky; Brad Calder; Dong-Yuan Chen; Ben Cheng; Bruce Childers; Trishul Chilimbi; Jong-Deok Choi; Andrea Cilio; Robert Cohn; Jeff Collard; Dan Connors; Marie Conte; Tom Conte; Henk Corporaal; Bob Davidson; Alex Dean; Saumya Debray; Jim Dehnert; Amer Diwan; AnHai Doan; Alban Douillet; Evelyn Duesterwald; Carole Dulong; Kathleen Durant; Kemal Ebcioglu; Susan Eggers; Thomas Kistler; Jens Knoop; Philip Koopman; Chandra Krintz; Rakesh Krishnaiyer; Dattatraya Kulkarni; Jim Larus; Meng Lee; Allen Leung; Chien-Wei Li; Josep Llosa; Scott Mahlke; Scott McFarling; Nathaniel McIntosh; Michey Mehta; Erik Meijer; Gunnar Mein; Sam Midkiff; Markus Mock; Eliot Moss; Todd Mowry; Stefan Muszala; Robert Muth; CJ Newburn; Erik Nystrom; Mikhail Smelyanskiy; Mike Smith; Mary Lou Soffa; Ravikrishnan Sree; Larry Sullivan; Peter Sweeney; Partha Tirumalai; Omri Traub; Spyridon Triantafyllis; Dean Tullsen; Sain-Zee Ueng; Manish Vachharajani; Neil Vachharajani; Xavier Vera; Florian Waas; Zhenlin Wang; Mario Wolczko; Greg Wright; Le-Chun Wu; Peng Wu; Qiang Wu; Jun Xu; Craig Zilles; Ben Zorn,*,*,*,*
Decision-theoretic Refinement Medical Decision Making,PETER HADDAWY; ANHAI DOAN; CHARLES E KAHN Jr,Decision-theoretic refinement planning is a new technique for finding optimal courses ofaction. The authors sought to determine whether this technique could identify optimalstrategies for medical diagnosis and therapy. An existing model of acute deep venousthrombosis of the lower extremities was encoded for analysis by the decision-theoreticrefinement planning system (DRIPS). The encoding represented 6;206 possible plans. TheDPIPS planner used artificial intelligence techniques to eliminate 5;150 plans (63%) fromconsideration without examining them explicitly. The DRIPS system identified the fivestrategies that minimized cost and mortality. The authors conclude that decisiontheoreticplanning is useful for examining large medical-decision problems. Key words: decision-theoretic refinement planning; diagnosis; treatment; DRIPS; acute deep venous,*,*,*
Workshop Officers,Laura Haas; Zachary Ives; Mukesh Mohania; Manish Bhide; Divy Agrawal; Phil Bernstein; Kevin Chang; Yi Chen; Alin Deutsch; AnHai Doan; Alon Halevy; Mizuho Iwaihara; Masaru Kitsuregawa; Craig Knoblock; Sergey Melnik; Ullas Nambiar; Felix Naumann; Evaggelia Pitoura; Prasan Roy; Michael Schrefl; Kohichi Takeda; Wang-Chiew Tan; Millist Vincent; Ji-Rong Wen,*,*,*,*
A Conditional Probabilistic Machine Learning Framework for Multiple Ontology Alignment,Khashayar Rohanimanesh; Michael Wick; Andrew McCallum; An-Hai Doan,*,*,*,*
Learning to Live with Semantic Heterogeneity,AnHai Doan,*,*,*,*
PicNet: Augmenting Semantic Resources with Pictorial Representations/1 Andy Borman; Rada Mihalcea; and Paul Tarau Echo Chamber: A Game for Eliciting a Coll...,Timothy Chklovski; Yolanda Gil; Dan Cosley; AnHai Doan; Robert McCann; Warren Shen; Chuck P Lam; David G Stork; Roberto Navigli; Natalya F Noy; Ramanathan Guha; Mark A Musen; Phil Oertel; Eyal Amir; Michel Simard; Elliott Macklovitch; Marcin Skowron; Kenji Araki; Lucy Vanderwende; Luis von Ahn; Laura Dabbish; Michael Witbrock; Cynthia Matuszek; Antoine Brusseau; Robert Kahlert; C Bruce Fraser; Douglas Lenat; Bettina Berendt,Page 1. Contents PicNet: Augmenting Semantic Resources with Pictorial Representations / 1Andy Borman; Rada Mihalcea; and Paul Tarau Echo Chamber: A Game for Eliciting a ColloquialParaphrase Corpus / 8 Chris Brockett and William B. Dolan 1001 Paraphrases: IncentingResponsible Contributions in Collecting Paraphrases from Volunteers / 16 Timothy ChklovskiTowards Managing Knowledge Collection from Volunteer Contributors / 21 Timothy Chklovskiand Yolanda Gil Mining Social Theory to Build Member-Maintained Communities / 28 Dan CosleyCollaborative Development of Information Integration Systems / 34 AnHai Doan; Robert McCann;and Warren Shen Toward Optimal Labeling Strategy under Multiple Unreliable Labelers / 42Chuck P. Lam and David G. Stork Supporting Large-Scale Knowledge Acquisition with StructuralSemantic Interconnections / 48 Roberto Navigli …,*,*,*
Program Committees,Carlo Batini; IRISA Laure Berti; France Tiziana Catarci; Ahmed K Elmagarmid; Suzanne Embury; Alvaro Fernandes; Helena Galhardas; Michael Gertz; Raghav Kaushik; Chen Li; Andrea Maurino; Felix Naumann; Nilesh Dalvi; Alex Dekhtyar; Maarten Fokkinga; Ander de Keijzer; Maurice van Keulen; Thomas Lukasiewicz; Sunil Prabhakar; Martin Theobald; Guy De Tr&; Jef Wijsen; Hainaut Vladimir Zadorozhny; AnHai Doan,*,*,*,*
Information Integration using Entity-Relations across Bioinformatics Sources,Bruce Schatz; Gene Robinson; Dan Roth; AnHai Doan; ChengXiang Zhai,A major focus of modern biology is functional analysis: the study of how a biological entity isfunctionally related to other biological entities or processes. For example; does gene Xencode chemical Y in organism Z? Does gene X regulate behavior B? What other biologicalprocesses Is X involved in? Which other genes regulate the same behavior in otherorganisms? The functional analysis of genes using genomes is perhaps the key researchissue in post-genome biology [46]. To perform successful functional analysis; biologists mustintegrate data from multiple sources; to enable the examination of multiple pieces ofsuggestive evidence in the various data. Unfortunately today such data integration forfunctional analysis is still extremely difficult. It is carried out largely by hand; in a very laborintensive and error prone process; as the next section describes. As the growth of …,gene,*,*
