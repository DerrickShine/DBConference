Apache hadoop yarn: Yet another resource negotiator,Vinod Kumar Vavilapalli; Arun C Murthy; Chris Douglas; Sharad Agarwal; Mahadev Konar; Robert Evans; Thomas Graves; Jason Lowe; Hitesh Shah; Siddharth Seth; Bikas Saha; Carlo Curino; Owen O'Malley; Sanjay Radia; Benjamin Reed; Eric Baldeschwieler,Abstract The initial design of Apache Hadoop [1] was tightly focused on running massive;MapReduce jobs to process a web crawl. For increasingly diverse companies; Hadoop hasbecome the data and computational agorá---the de facto place where data andcomputational resources are shared and accessed. This broad adoption and ubiquitoususage has stretched the initial design well beyond its intended target; exposing two keyshortcomings: 1) tight coupling of a specific programming model with the resourcemanagement infrastructure; forcing developers to abuse the MapReduce programmingmodel; and 2) centralized handling of jobs' control flow; which resulted in endless scalabilityconcerns for the scheduler. In this paper; we summarize the design; development; andcurrent state of deployment of the next generation of Hadoop's compute platform: YARN …,Proceedings of the 4th annual Symposium on Cloud Computing,2013,979
Schism: a workload-driven approach to database replication and partitioning,Carlo Curino; Evan Jones; Yang Zhang; Sam Madden,Abstract We present Schism; a novel workload-aware approach for database partitioningand replication designed to improve scalability of shared-nothing distributed databases.Because distributed transactions are expensive in OLTP settings (a fact we demonstratethrough a series of experiments); our partitioner attempts to minimize the number ofdistributed transactions; while producing balanced partitions. Schism consists of two phases:i) a workload-driven; graph-based replication/partitioning phase and ii) an explanation andvalidation phase. The first phase creates a graph with a node per tuple (or group of tuples)and edges between nodes accessed by the same transaction; and then uses a graphpartitioner to split the graph into k balanced partitions that minimize the number of cross-partition transactions. The second phase exploits machine learning techniques to find a …,Proceedings of the VLDB Endowment,2010,430
A data-oriented survey of context models,Cristiana Bolchini; Carlo A Curino; Elisa Quintarelli; Fabio A Schreiber; Letizia Tanca,Abstract Context-aware systems are pervading everyday life; therefore context modeling isbecoming a relevant issue and an expanding research field. This survey has the goal toprovide a comprehensive evaluation framework; allowing application designers to comparecontext models with respect to a given target application; in particular we stress the analysisof those features which are relevant for the problem of data tailoring. The contribution of thispaper is twofold: a general analysis framework for context models and an up-to-datecomparison of the most interesting; data-oriented approaches available in the literature.,ACM Sigmod Record,2007,376
Relational cloud: A database-as-a-service for the cloud,Carlo Curino; Evan PC Jones; Raluca Ada Popa; Nirmesh Malviya; Eugene Wu; Sam Madden; Hari Balakrishnan; Nickolai Zeldovich,This paper introduces a new transactional “database-as-a-service”(DBaaS) calledRelational Cloud. A DBaaS promises to move much of the operational burden ofprovisioning; configuration; scaling; performance tuning; backup; privacy; and access controlfrom the database users to the service operator; offering lower overall costs to users. EarlyDBaaS efforts include Amazon RDS and Microsoft SQL Azure; which are promising in termsof establishing the market need for such a service; but which do not address three importantchallenges: efficient multi-tenancy; elastic scalability; and database privacy. We argue thatthese three challenges must be overcome before outsourcing database software andmanagement becomes attractive to many users; and cost-effective for service providers. Thekey technical features of Relational Cloud include:(1) a workload-aware approach to multi …,*,2011,347
Skew-aware automatic database partitioning in shared-nothing; parallel OLTP systems,Andrew Pavlo; Carlo Curino; Stanley Zdonik,Abstract The advent of affordable; shared-nothing computing systems portends a new classof parallel database management systems (DBMS) for on-line transaction processing(OLTP) applications that scale without sacrificing ACID guarantees [7; 9]. The performanceof these DBMSs is predicated on the existence of an optimal database design that is tailoredfor the unique characteristics of OLTP workloads. Deriving such designs for modern DBMSsis difficult; especially for enterprise-class OLTP systems; since they impose extra challenges:the use of stored procedures; the need for load balancing in the presence of time-varyingskew; complex schemas; and deployments with larger number of partitions. To this purpose;we present a novel approach to automatically partitioning databases for enterprise-classOLTP systems that significantly extends the state of the art by:(1) minimizing the number …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,191
Tinylime: Bridging mobile and sensor networks through middleware,Carlo Curino; Matteo Giani; Marco Giorgetta; Alessandro Giusti; Amy L Murphy; Gian Pietro Picco,In the rapidly developing field of sensor networks; bridging the gap between the applicationsand the hardware presents a major challenge. Although middleware is one solution; it mustbe specialized to the qualities of sensor networks; especially energy consumption. The workpresented here provides two contributions: a new operational setting for sensor networksand a middleware for easing software development in this setting. The operational settingwe target removes the usual assumption of a central collection point for sensor data. Insteadthe sensors are sparsely distributed in an environment; not necessarily able to communicateamong themselves; and a set of clients move through space accessing the data of sensorsnearby; yielding a system which naturally provides context relevant information to clientapplications. We further assume the clients are wirelessly networked and share locally …,Pervasive Computing and Communications; 2005. PerCom 2005. Third IEEE International Conference on,2005,188
Graceful database schema evolution: the prism workbench,Carlo A Curino; Hyun J Moon; Carlo Zaniolo,Abstract Supporting graceful schema evolution represents an unsolved problem fortraditional information systems that is further exacerbated in web information systems; suchas Wikipedia and public scientific databases: in these projects based on multipartycooperation the frequency of database schema changes has increased while tolerance fordowntimes has nearly disappeared. As of today; schema evolution remains an error-proneand time-consuming undertaking; because the DB Administrator (DBA) lacks the methodsand tools needed to manage and automate this endeavor by (i) predicting and evaluatingthe effects of the proposed schema changes;(ii) rewriting queries and applications tooperate on the new schema; and (iii) migrating the database. Our PRISM system takes a bigfirst step toward addressing this pressing need by providing:(i) a language of Schema …,Proceedings of the VLDB Endowment,2008,171
Workload-aware database monitoring and consolidation,Carlo Curino; Evan PC Jones; Samuel Madden; Hari Balakrishnan,Abstract In most enterprises; databases are deployed on dedicated database servers. Often;these servers are underutilized much of the time. For example; in traces from almost 200production servers from different organizations; we see an average CPU utilization of lessthan 4%. This unused capacity can be potentially harnessed to consolidate multipledatabases on fewer machines; reducing hardware and operational costs. Virtual machine(VM) technology is one popular way to approach this problem. However; as we demonstratein this paper; VMs fail to adequately support database consolidation; because databasesplace a unique and challenging set of demands on hardware resources; which are not well-suited to the assumptions made by VM-based consolidation. Instead; our system fordatabase consolidation; named Kairos; uses novel techniques to measure the hardware …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,162
Oltp-bench: An extensible testbed for benchmarking relational databases,Djellel Eddine Difallah; Andrew Pavlo; Carlo Curino; Philippe Cudre-Mauroux,Abstract Benchmarking is an essential aspect of any database management system (DBMS)effort. Despite several recent advancements; such as pre-configured cloud database imagesand database-as-a-service (DBaaS) offerings; the deployment of a comprehensive testingplatform with a diverse set of datasets and workloads is still far from being trivial. In manycases; researchers and developers are limited to a small number of workloads to evaluatethe performance characteristics of their work. This is due to the lack of a universalbenchmarking infrastructure; and to the difficulty of gaining access to real data andworkloads. This results in lots of unnecessary engineering efforts and makes theperformance evaluation results difficult to compare. To remedy these problems; we presentOLTP-Bench; an extensible" batteries included" DBMS benchmarking testbed. The key …,Proceedings of the VLDB Endowment,2013,115
Schema evolution in wikipedia: toward a web information system benchmark,Carlo A Curino; Letizia Tanca; Hyun J Moon; Carlo Zaniolo,Abstract Evolving the database that is at the core of an Information System represents adifficult maintenance problem that has only been studied in the framework of traditionalinformation systems. However; the problem is likely to be even more severe in webinformation systems; where open-source software is often developed through thecontributions and collaboration of many groups and individuals. Therefore; in this paper; wepresent an indepth analysis of the evolution history of the Wikipedia database and itsschema; Wikipedia is the best-known example of a large family of web information systemsbuilt using the open-source software MediaWiki. Our study is based on:(i) a set of SchemaModification Operators that provide a simple conceptual representation for complex schemachanges; and (ii) simple software tools to automate the analysis. This framework allowed …,In International Conference on Enterprise Information Systems (ICEIS,2008,98
Mobile data collection in sensor networks: The TinyLime middleware,Carlo Curino; Matteo Giani; Marco Giorgetta; Alessandro Giusti; Amy L Murphy; Gian Pietro Picco,Abstract In this paper we describe Tiny Lime; a novel middleware for wireless sensornetworks that departs from the traditional setting where sensor data is collected by a centralmonitoring station; and enables instead multiple mobile monitoring stations to access thesensors in their proximity and share the collected data through wireless links. Thisintrinsically context-aware setting is demanded by applications where the sensors aresparse and possibly isolated; and where on-site; location-dependent data collection isrequired. An extension of the Lime middleware for mobile ad hoc networks; Tiny Lime makessensor data available through a tuple space interface; providing the illusion of sharedmemory between applications and sensors. Data aggregation capabilities and a power-savvy architecture complete the middleware features. The paper presents the model and …,Pervasive and Mobile Computing,2005,78
Managing and querying transaction-time databases under schema evolution,Hyun J Moon; Carlo A Curino; Alin Deutsch; Chien-Yi Hou; Carlo Zaniolo,Abstract The old problem of managing the history of database information is now made moreurgent and complex by fast spreading web information systems; such as Wikipedia. OurPRIMA system addresses this difficult problem by introducing two key pieces of newtechnology. The first is a method for publishing the history of a relational database in XML;whereby the evolution of the schema and its underlying database are given a unifiedrepresentation. This temporally grouped representation makes it easy to formulatesophisticated historical queries on any given schema version using standard XQuery. Thesecond key piece of technology is that schema evolution is transparent to the user: shewrites queries against the current schema while retrieving the data from one or moreschema versions. The system then performs the labor-intensive and error-prone task of …,Proceedings of the VLDB Endowment,2008,76
Apache tez: A unifying framework for modeling and building data processing applications,Bikas Saha; Hitesh Shah; Siddharth Seth; Gopal Vijayaraghavan; Arun Murthy; Carlo Curino,Abstract The broad success of Hadoop has led to a fast-evolving and diverse ecosystem ofapplication engines that are building upon the YARN resource management layer. The open-source implementation of MapReduce is being slowly replaced by a collection of enginesdedicated to specific verticals. This has led to growing fragmentation and repeated effortswith each new vertical engine re-implementing fundamental features (eg fault-tolerance;security; stragglers mitigation; etc.) from scratch. In this paper; we introduce Apache Tez; anopen-source framework designed to build data-flow driven processing runtimes. Tezprovides a scaffolding and library components that can be used to quickly build scalable andefficient data-flow centric engines. Central to our design is fostering component re-use;without hindering customizability of the performance-critical data plane. This is in fact the …,Proceedings of the 2015 ACM SIGMOD international conference on Management of Data,2015,72
And what can context do for data?,Cristiana Bolchini; CA Curino; Giorgio Orsi; Elisa Quintarelli; Rosalba Rossato; Fabio A Schreiber; Letizia Tanca,a Enterprise Resource Planning. november 2009| vol. 52| no. 11| communications of theacm 137 contributed articles posed methodological approach. In the context design phase;all the possible scenarios (contexts) of the considered application are identified; then; thedesigner must determine context-aware views; as the portions of the entire dataset whichare relevant for each particular actor in each particular context. We propose the definition ofa context-guided methodology to support the designer in identifying; for a given applicationscenario; the contexts and the correspondingly interesting subsets of data. Our methodologyis composed of three basic elements: a context model; capturing all the aspects–the so-called dimensions–that allows the implicit representation of the possible applicationcontexts; a strategy for identifying; for each dimension; independently of the others; a …,Communications of the ACM,2009,67
Global Analytics in the Face of Bandwidth and Regulatory Constraints.,Ashish Vulimiri; Carlo Curino; Philip Brighten Godfrey; Thomas Jungblut; Jitu Padhye; George Varghese,Abstract Global-scale organizations produce large volumes of data across geographicallydistributed data centers. Querying and analyzing such data as a whole introduces newresearch issues at the intersection of networks and databases. Today systems that computeSQL analytics over geographically distributed data operate by pulling all data to a centrallocation. This is problematic at large data scales due to expensive transoceanic links; andmay be rendered impossible by emerging regulatory constraints. The new problem of Wide-Area Big Data (WABD) consists in orchestrating query execution across data centers tominimize bandwidth while respecting regulatory constaints. WABD combines classical queryplanning with novel network-centric mechanisms designed for a wide-area setting such aspseudo-distributed execution; joint query optimization; and deltas on cached subquery …,NSDI,2015,63
Context integration for mobile data tailoring,Cristiana Bolchini; Carlo Curino; Fabio A Schreiber; Letizia Tanca,Independent; heterogeneous; distributed; sometimes transient and mobile data sourcesproduce an enormous amount of information that should be semantically integrated andfiltered; or; as we say; tailored; based on the user's interests and context. Since both the userand the data sources can be mobile; and the communication might be unreliable; cachingthe information on the user device may become really useful. Therefore new challengeshave to be faced such as: data filtering in a context-aware fashion; integration of not-known-in-advance data sources; automatic extraction of the semantics. We propose a novel systemnamed Context-ADDICT (Context-Aware Data Design; Integration; Customization andTailoring) able to deal with the described scenario. The system we are designing aims attailoring the available information to the needs of the current user in the current context; in …,Mobile Data Management; 2006. MDM 2006. 7th International Conference on,2006,62
DBSeer: Resource and Performance Prediction for Building a Next Generation Database Cloud.,Barzan Mozafari; Carlo Curino; Samuel Madden,ABSTRACT Cloud computing is characterized by shared infrastructure and a decouplingbetween its operators and tenants. These two characteristics impose new challenges todatabases applications hosted in the cloud; namely:(i) how to price database services;(ii)how to isolate database tenants; and (iii) how to optimize database performance on thisshared infrastructure. We argue that today's solutions; based on virtual-machines; do notproperly address these challenges. We hint at new research directions to tackle theseproblems and argue that these three challenges share a common need for accuratepredictive models of performance and resource utilization. We present our approach; calledDBSeer; with our initial results on predictive models for the important class of OLTP/Webworkloads and show how they can be used to address these challenges.,CIDR,2013,60
Automating the database schema evolution process,Carlo Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,Abstract Supporting database schema evolution represents a long-standing challenge ofpractical and theoretical importance for modern information systems. In this paper; wedescribe techniques and systems for automating the critical tasks of migrating the databaseand rewriting the legacy applications. In addition to labor saving; the benefits delivered bythese advances are many and include reliable prediction of outcome; minimization ofdowntime; system-produced documentation; and support for archiving; historical queries;and provenance. The PRISM/PRISM++ system delivers these benefits; by solving the difficultproblem of automating the migration of databases and the rewriting of queries and updates.In this paper; we present the PRISM/PRISM++ system and the novel technology that made itpossible. In particular; we focus on the difficult and previously unsolved problem of …,The VLDB Journal,2013,58
Reservation-based scheduling: If you're late don't blame us!,Carlo Curino; Djellel E Difallah; Chris Douglas; Subru Krishnan; Raghu Ramakrishnan; Sriram Rao,Abstract The continuous shift towards data-driven approaches to business; and a growingattention to improving return on investments (ROI) for cluster infrastructures is generatingnew challenges for big-data frameworks. Systems originally designed for big batch jobs nowhandle an increasingly complex mix of computations. Moreover; they are expected toguarantee stringent SLAs for production jobs and minimize latency for best-effort jobs. In thispaper; we introduce reservation-based scheduling; a new approach to this problem. Wedevelop our solution around four key contributions: 1) we propose a reservation definitionlanguage (RDL) that allows users to declaratively reserve access to cluster resources; 2) weformalize planning of current and future cluster resources as a Mixed-Integer LinearProgramming (MILP) problem; and propose scalable heuristics; 3) we adaptively …,Proceedings of the ACM Symposium on Cloud Computing,2014,57
Context information for knowledge reshaping,Cristiana Bolchini; Carlo Aldo Curino; Elisa Quintarelli; Fabio A Schreiber; Letizia Tanca,More and more often; we face the necessity of extracting appropriately reshaped knowledgefrom an integrated representation of the information space. Be such a global representationa central database; a global view of several ones or an ontological representation of aninformation domain; we face the need to define personalised views for the knowledgestakeholders: single users; companies or applications. We propose exploiting theinformation usage context within a methodology for context-aware data design; where thenotion of context is formally defined together with its role within the process of view buildingby information tailoring. This paper presents our context model; called the context dimensiontree; which plays a fundamental role in tailoring the information space according to userinformation needs.,International Journal of Web Engineering and Technology,2009,56
Performance and resource modeling in highly-concurrent OLTP workloads,Barzan Mozafari; Carlo Curino; Alekh Jindal; Samuel Madden,Abstract Database administrators of Online Transaction Processing (OLTP) systemsconstantly face difficult questions. For example;" What is the maximum throughput I cansustain with my current hardware?";" How much disk I/O will my system perform if therequests per second double?"; or" What will happen if the ratio of transactions in my systemchanges?". Resource prediction and performance analysis are both vital and difficult in thissetting. Here the challenge is due to high degrees of concurrency; competition for resources;and complex interactions between transactions; all of which non-linearly impactperformance. Although difficult; such analysis is a key component in enabling databaseadministrators to understand which queries are eating up the resources; and how theirsystem would scale under load. In this paper; we introduce our framework; called DBSeer …,Proceedings of the 2013 acm sigmod international conference on management of data,2013,55
Wanalytics: Geo-distributed analytics for a data intensive world,Ashish Vulimiri; Carlo Curino; Philip Brighten Godfrey; Thomas Jungblut; Konstantinos Karanasos; Jitendra Padhye; George Varghese,Abstract Many large organizations collect massive volumes of data each day in ageographically distributed fashion; at data centers around the globe. Despite theirgeographically diverse origin the data must be processed and analyzed as a whole toextract insight. We call the problem of supporting large-scale geo-distributed analytics Wide-Area Big Data (WABD). To the best of our knowledge; WABD is currently addressed bycopying all the data to a central data center where the analytics are run. This approachconsumes expensive cross-data center bandwidth and is incompatible with data sovereigntyrestrictions that are starting to take shape. We instead propose WANalytics; a system thatsolves the WABD problem by orchestrating distributed query execution and adjusting datareplication across data centers in order to minimize bandwidth usage; while respecting …,Proceedings of the 2015 ACM SIGMOD international conference on management of data,2015,54
Relational cloud: The case for a database service,Carlo Curino; Evan Jones; Yang Zhang; Eugene Wu; Samuel Madden,ABSTRACT In this paper; we make the case for “databases as a service”(DaaS); with twotarget scenarios in mind:(i) consolidation of data management functionality for largeorganizations and (ii) outsourcing data management to a cloud-based service provider forsmall/medium organizations. We analyze the many challenges to be faced; and discuss thedesign of a database service we are building; called Relational Cloud. The system has beendesigned from scratch and combines many recent advances and novel solutions. Theprototype we present exploits multiple dedicated storage engines; provides high-availabilityvia transparent replication; supports automatic workload partitioning and live data migration;and provides serializable distributed transactions. While the system is still under activedevelopment; we are able to present promising initial results that showcase the key …,New England Database Summit,2010,50
Update rewriting and integrity constraint maintenance in a schema evolution support system: PRISM++,Carlo A Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,Abstract Supporting legacy applications when the database schema evolves represents along-standing challenge of practical and theoretical importance. Recent work has producedalgorithms and systems that automate the process of data migration and query adaptation;however; the problems of evolving integrity constraints and supporting legacy updates underschema and integrity constraints evolution are significantly more difficult and have thus farremained unsolved. In this paper; we address this issue by introducing a formal evolutionmodel for the database schema structure and its integrity constraints; and use it to deriveupdate mapping techniques akin to the rewriting techniques used for queries. Thus; we (i)propose a new set of Integrity Constraints Modification Operators (ICMOs);(ii) characterizethe impact on integrity constraints of structural schema changes;(iii) devise …,Proceedings of the VLDB Endowment,2010,46
Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters.,Konstantinos Karanasos; Sriram Rao; Carlo Curino; Chris Douglas; Kishore Chaliparambil; Giovanni Matteo Fumarola; Solom Heddaya; Raghu Ramakrishnan; Sarvesh Sakalanaga,Abstract Datacenter-scale computing for analytics workloads is increasingly common. Highoperational costs force heterogeneous applications to share cluster resources for achievingeconomy of scale. Scheduling such large and diverse workloads is inherently hard; andexisting approaches tackle this in two alternative ways: 1) centralized solutions offer strict;secure enforcement of scheduling invariants (eg; fairness; capacity) for heterogeneousapplications; 2) distributed solutions offer scalable; efficient scheduling for homogeneousapplications. We argue that these solutions are complementary; and advocate a blendedapproach. Concretely; we propose,USENIX Annual Technical Conference,2015,43
Lookup tables: Fine-grained partitioning for distributed databases,Aubrey L Tatarowicz; Carlo Curino; Evan PC Jones; Sam Madden,The standard way to get linear scaling in a distributed OLTP DBMS is to horizontally partitiondata across several nodes. Ideally; this partitioning will result in each query being executedat just one node; to avoid the overheads of distributed transactions and allow nodes to beadded without increasing the amount of required coordination. For some applications;simple strategies; such as hashing on primary key; provide this property. Unfortunately; formany applications; including social networking and order-fulfillment; many-to-manyrelationships cause simple strategies to result in a large fraction of distributed queries.Instead; what is needed is a fine-grained partitioning; where related individual tuples (eg;cliques of friends) are co-located together in the same partition. Maintaining such a fine-grained partitioning requires the database to store a large amount of metadata about …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,43
Mobius: unified messaging and data serving for mobile apps,Byung-Gon Chun; Carlo Curino; Russell Sears; Alexander Shraer; Samuel Madden; Raghu Ramakrishnan,Abstract Mobile application development is challenging for several reasons: intermittent andlimited network connectivity; tight power constraints; server-side scalability concerns; and anumber of fault-tolerance issues. Developers handcraft complex solutions that include client-side caching; conflict resolution; disconnection tolerance; and backend database sharding.To simplify mobile app development; we present Mobius; a system that addresses themessaging and data management challenges of mobile application development. Mobiusintroduces MUD (Messaging Unified with Data). MUD presents the programming abstractionof a logical table of data that spans devices and clouds. Applications using Mobius canasynchronously read from/write to MUD tables; and also receive notifications when tableschange via continuous queries on the tables. The system combines dynamic client-side …,Proceedings of the 10th international conference on Mobile systems; applications; and services,2012,39
The prism workwench: Database schema evolution without tears,Carlo A Curino; Hyun J Moon; MyungWon Ham; Carlo Zaniolo,Information Systems are subject to a perpetual evolution; which is particularly pressing inWeb Information Systems; due to their distributed and often collaborative nature. Suchcontinuous adaptation process; comes with a very high cost; because of the intrinsiccomplexity of the task and the serious rami¿ cations of such changes upon database-centricInformation System softwares. Therefore; there is a need to automate and simplify theschema evolution process and to ensure predictability and logical independence uponschema changes. Current relational technology makes it easy to change the databasecontent or to revise the underlaying storage and indexes but does little to support logicalschema evolution which nowadays remains poorly supported by commercial tools. ThePRISM system demonstrates a major new advance toward automating schema evolution …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,38
Automating database schema evolution in information system upgrades,Carlo Curino; Hyun J Moon; Carlo Zaniolo,Abstract The complexity; cost; and down-time currently created by the database schemaevolution process is the source of incessant problems in the life of information systems and amajor stumbling block that prevent graceful upgrades. Furthermore; our studies shows thatthe serious problems encountered by traditional information systems are now furtherexacerbated in web information systems and cooperative scientific databases where thefrequency of schema changes has increased while tolerance for downtimes has nearlydisappeared. The PRISM project seeks to develop the methods and tools that turn this error-prone and time-consuming process into one that is controllable; predictable and avoidsdown-time. Toward this goal; we have assembled a large testbed of schema evolutionhistories; and developed a language of Schema Modification Operators (SMO) to express …,Proceedings of the 2nd International Workshop on Hot Topics in Software Upgrades,2009,36
X-som: A flexible ontology mapper,Carlo Curino; Giorgio Orsi; Letizia Tanca,System interoperability is a well known issue; especially for heterogeneous informationsystems; where ontology-based representations may support automatic and user-transparent integration. In this paper we present X-SOM: an ontology mapping andintegration tool. The contribution of our tool is a modular and extensible architecture thatautomatically combines several matching techniques by means of a neural network;performing also ontology debugging to avoid inconsistencies. Besides describing the toolcomponents; we discuss the prototype implementation; which has been tested against theOAEI 2006 benchmark with promising results.,Database and Expert Systems Applications; 2007. DEXA'07. 18th International Workshop on,2007,29
Morpheus: Towards Automated SLOs for Enterprise Clusters.,Sangeetha Abdu Jyothi; Carlo Curino; Ishai Menache; Shravan Matthur Narayanamurthy; Alexey Tumanov; Jonathan Yaniv; Ruslan Mavlyutov; Inigo Goiri; Subru Krishnan; Janardhan Kulkarni; Sriram Rao,Abstract Modern resource management frameworks for largescale analytics leaveunresolved the problematic tension between high cluster utilization and job's performancepredictability—respectively coveted by operators and users. We address this in Morpheus; anew system that: 1) codifies implicit user expectations as explicit Service Level Objectives(SLOs); inferred from historical data; 2) enforces SLOs using novel scheduling techniquesthat isolate jobs from sharing-induced performance variability; and 3) mitigates inherentperformance variance (eg; due to failures) by means of dynamic reprovisioning of jobs. Wevalidate these ideas against production traces from a 50k node cluster; and show thatMorpheus can lower the number of deadline violations by 5× to 13×; while retaining cluster-utilization; and lowering cluster footprint by 14% to 28%. We demonstrate the scalability …,OSDI,2016,27
Reef: Retainable evaluator execution framework,Byung-Gon Chun; Tyson Condie; Carlo Curino; Chris Douglas; Sergiy Matusevych; Brandon Myers; Shravan Narayanamurthy; Raghu Ramakrishnan; Sriram Rao; Josh Rosen; Russell Sears; Markus Weimer,Abstract In this demo proposal; we describe REEF; a framework that makes it easy toimplement scalable; fault-tolerant runtime environments for a range of computationalmodels. We will demonstrate diverse workloads; including extract-transform-loadMapReduce jobs; iterative machine learning algorithms; and ad-hoc declarative queryprocessing. At its core; REEF builds atop YARN (Apache Hadoop 2's resource manager) toprovide retainable hardware resources with lifetimes that are decoupled from those ofcomputational tasks. This allows us to build persistent (cross-job) caches and cluster-wideservices; but; more importantly; supports high-performance iterative graph processing andmachine learning algorithms. Unlike existing systems; REEF aims for composability of jobsacross computational models; providing significant performance and usability gains; even …,Proceedings of the VLDB Endowment,2013,27
Benchmarking oltp/web databases in the cloud: The oltp-bench framework,Carlo A Curino; Djellel E Difallah; Andrew Pavlo; Philippe Cudre-Mauroux,Abstract Benchmarking is a key activity in building and tuning data management systems;but the lack of reference workloads and a common platform makes it a time consuming andpainful task. The need for such a tool is heightened with the advent of cloud computing--withits pay-per-use cost models; shared multi-tenant infrastructures; and lack of control onsystem configuration. Benchmarking is the only avenue for users to validate the quality ofservice they receive and to optimize their deployments for performance and resourceutilization. In this talk; we present our experience in building several adhoc benchmarkinginfrastructures for various research projects targeting several OLTP DBMSs; ranging fromtraditional relational databases; main-memory distributed systems; and cloud-basedscalable architectures. We also discuss our struggle to build meaningful micro …,Proceedings of the fourth international workshop on Cloud data management,2012,25
Improving search and navigation by combining ontologies and social tags,Silvia Bindelli; Claudio Criscione; Carlo A Curino; Mauro L Drago; Davide Eynard; Giorgio Orsi,Abstract The Semantic Web has the ambitious goal of enabling complex autonomousapplications to reason on a machine-processable version of the World Wide Web. This;however; would require a coordinated effort not easily achievable in practice. On the otherhand; spontaneous communities; based on social tagging; recently achieved noticeableconsensus and diffusion. The goal of the TagOnto system is to bridge between these tworealities by automatically mapping (social) tags to more structured domain ontologies; thus;providing assistive; navigational features typical of the Semantic Web. These novelsearching and navigational capabilities are complementary to more traditional searchengine functionalities. The system; and its intuitive AJAX interface; are released anddemonstrated on-line.,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2008,25
Scalable architecture and query optimization fortransaction-time DBs with evolving schemas,Hyun Jin Moon; Carlo A Curino; Carlo Zaniolo,Abstract The problem of archiving and querying the history of a database is made morecomplex by the fact that; along with the database content; the database schema also evolveswith time. Indeed; archival quality can only be guaranteed by storing past database contentsusing the schema versions under which they were originally created. This causes majorusability and scalability problems in preservation; retrieval and querying of databases withintense evolution histories; ie; hundreds of schema versions. This scenario is common inweb information systems and scientific databases that frequently accumulate that manyversions in just a few years. Our system; Archival Information Management System (AIMS);solves this usability issue by letting users write queries against a chosen schema versionand then performing for the users the rewriting and execution of queries on all …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,24
Managing the history of metadata in support for db archiving and schema evolution,Carlo A Curino; Hyun J Moon; Carlo Zaniolo,Abstract Modern information systems; and web information systems in particular; are facedwith frequent database schema changes; which generate the necessity to manage them andpreserve the schema evolution history. In this paper; we describe the Panta Rhei Frameworkdesigned to provide powerful tools that:(i) facilitate schema evolution and guide theDatabase Administrator in planning and evaluating changes;(ii) support automatic rewritingof legacy queries against the current schema version;(iii) enable efficient archiving of thehistories of data and metadata; and (iv) support complex temporal queries over suchhistories. We then introduce the Historical Metadata Manager (HMM); a tool designed tofacilitate the process of documenting and querying the schema evolution itself. We use theschema history of the Wikipedia database as a telling example of the many uses and …,International Conference on Conceptual Modeling,2008,24
How clean is your sandbox?,James F Terwilliger; Anthony Cleve; Carlo A Curino,Abstract Bidirectional transformations (bx) constitute an emerging mechanism formaintaining the consistency of interdependent sources of information in software systems.Researchers from many different communities have recently investigated the use of bx tosolve a large variety of problems; including relational view update; schema evolution; dataexchange; database migration; and model co-evolution; just to name a few. Each communityleveraged and extended different theoretical frameworks and tailored their use for specificsub-problems. Unfortunately; the question of how these approaches actually relate to anddiffer from each other remains unanswered. This question should be addressed to reducereplicated efforts among and even within communities; enabling more effective collaborationand fostering cross-fertilization. To effectively move forward; a systematization of these …,International Conference on Theory and Practice of Model Transformations,2012,23
Discussion of BigBench: a proposed industry standard performance benchmark for big data,Chaitanya Baru; Milind Bhandarkar; Carlo Curino; Manuel Danisch; Michael Frank; Bhaskar Gowda; Hans-Arno Jacobsen; Huang Jie; Dileep Kumar; Raghunath Nambiar; Meikel Poess; Francois Raab; Tilmann Rabl; Nishkam Ravi; Kai Sachs; Saptak Sen; Lan Yi; Choonhan Youn,Abstract Enterprises perceive a huge opportunity in mining information that can be found inbig data. New storage systems and processing paradigms are allowing for ever larger datasets to be collected and analyzed. The high demand for data analytics and rapiddevelopment in technologies has led to a sizable ecosystem of big data processing systems.However; the lack of established; standardized benchmarks makes it difficult for users tochoose the appropriate systems that suit their requirements. To address this problem; wehave developed the BigBench benchmark specification. BigBench is the first end-to-end bigdata analytics benchmark suite. In this paper; we present the BigBench benchmark andanalyze the workload from technical as well as business point of view. We characterize thequeries in the workload along different dimensions; according to their functional …,Technology Conference on Performance Evaluation and Benchmarking,2014,20
Accessing and documenting relational databases through OWL ontologies,Carlo Curino; Giorgio Orsi; Emanuele Panigati; Letizia Tanca,Abstract Relational databases have been designed to store high volumes of data and toprovide an efficient query interface. Ontologies are geared towards capturing domainknowledge; annotations; and to offer high-level; machine-processable views of data andmetadata. The complementary strengths and weaknesses of these data models motivate theresearch effort we present in this paper. The goal of this work is to bridge the relational andontological worlds; in order to leverage the efficiency and scalability of relationaltechnologies and the high level view of data and metadata proper of ontologies. The systemwe designed and developed achieves:(i) automatic ontology extraction from relational datasources and (ii) automatic query translation from SPARQL to SQL. Among the others; wefocus on two main applications of this novel technology:(i) ontological publishing of …,International Conference on Flexible Query Answering Systems,2009,20
Woo: A scalable and multi-tenant platform for continuous knowledge base synthesis,Kedar Bellare; Carlo Curino; Ashwin Machanavajihala; Peter Mika; Mandar Rahurkar; Aamod Sane,Abstract Search; exploration and social experience on the Web has recently undergonetremendous changes with search engines; web portals and social networks offering adifferent perspective on information discovery and consumption. This new perspective isaimed at capturing user intents; and providing richer and highly connected experiences. Thenew battleground revolves around technologies for the ingestion; disambiguation andenrichment of entities from a variety of structured and unstructured data sources-we refer tothis process as knowledge base synthesis. This paper presents the design; implementationand production deployment of the Web Of Objects (WOO) system; a Hadoop-based platformtackling such challenges. WOO has been designed and implemented to enable variousproducts in Yahoo! to synthesize knowledge bases (KBs) of entities relevant to their …,Proceedings of the VLDB Endowment,2013,19
PRIMA: archiving and querying historical data with evolving schemas,Hyun J Moon; Carlo A Curino; Myungwon Ham; Carlo Zaniolo,Abstract Schema evolution poses serious challenges in historical data management.Traditionally; historical data have been archived either by (i) migrating them into the currentschema version that is well-understood by users but compromising archival quality; or (ii) bymaintaining them under the original schema version in which the data was originallycreated; leading to perfect archival quality; but forcing users to formulate queries againstcomplex histories of evolving schemas. In the PRIMA system; we achieve the best of bothapproaches; by (i) archiving historical data under the schema version under which they wereoriginally created; and (ii) letting users express temporal queries using the current schemaversion. Thus; in PRIMA; the system rewrites the queries to the (potentially many) pertinentversions of the evolving schema. Moreover; the system o ers automatic documentation of …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,19
Ontology-based information tailoring,Carlo Curino; Elisa Quintarelli; Letizia Tanca,Current applications are often forced to filter the richness of datasources in order to reducethe information noise the user is subject to. We consider this aspect as a critical issue ofapplications; to be factorized at the data management level. The Context-ADDICT system;leveraging on ontology-based context and domain models; is able to personalize the data tobe made available to the user by" context-aware tailoring". In this paper we present a formalapproach to the definition of the relationship between context (represented by anappropriate context model) and application domain (modeled by a domain ontology). Oncesuch relationship has been defined; we are able to work out the boundary of the portion ofthe domain relevant to a user in a certain context. We also sketch the implementation of avisual tool supporting the application designer in this modeling task,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,18
X− SOM Results for OAEI 2007,Carlo Curino; Giorgio Orsi; Letizia Tanca,Abstract: This paper summarizes the results of the X-SOM tool in the OAEI 2007 campaign. X-SOM is an extensible ontology mapper that combines various matching algorithms bymeans of a feed-forward neural network. X-SOM exploits logical reasoning and localheuristics to improve the quality of mappings while guaranteeing their consistency.,Proc. of the 2nd Intl Workshop on Ontology Matching (OAEI− ISWC/ASWC Workshops),2007,16
CADD: a tool for context modeling and data tailoring,Cristiana Bolchini; CA Curino; Giorgio Orsi; Elisa Quintarelli; Fabio A Schreiber; Letizia Tanca,Nowadays user mobility requires that both content and services be appropriatelypersonalized; in order for the (mobile) user to be always-and anywhere-equipped with theadequate share of data. Thus; the knowledge about the user; the adopted device and theenvironment; altogether called context; has to be taken into account in order to minimize theamount of information imported on mobile devices. The Context-ADDICT (Context-AwareData Design; Integration; Customization and Tailoring) project aims at the definition of acomplete framework which; starting from a methodology for the early design phases;supports mobile users through the dynamic hooking and integration of new; availableinformation sources; so that an appropriate context-based portion of data; called data chunk;is delivered to their mobile devices. Data tailoring is needed because of two main …,Mobile Data Management; 2007 International Conference on,2007,15
Towards geo-distributed machine learning,Ignacio Cano; Markus Weimer; Dhruv Mahajan; Carlo Curino; Giovanni Matteo Fumarola,Abstract: Latency to end-users and regulatory requirements push large companies to builddata centers all around the world. The resulting data is" born" geographically distributed. Onthe other hand; many machine learning applications require a global view of such data inorder to achieve the best results. These types of applications form a new class of learningproblems; which we call Geo-Distributed Machine Learning (GDML). Such applicationsneed to cope with: 1) scarce and expensive cross-data center bandwidth; and 2) growingprivacy concerns that are pushing for stricter data sovereignty regulations. Current solutionsto learning from geo-distributed data sources revolve around the idea of first centralizing thedata in one data center; and then training locally. As machine learning algorithms arecommunication-intensive; the cost of centralizing the data is thought to be offset by the …,arXiv preprint arXiv:1603.09035,2016,13
Reef: Retainable evaluator execution framework,Markus Weimer; Yingda Chen; Byung-Gon Chun; Tyson Condie; Carlo Curino; Chris Douglas; Yunseong Lee; Tony Majestro; Dahlia Malkhi; Sergiy Matusevych; Brandon Myers; Shravan Narayanamurthy; Raghu Ramakrishnan; Sriram Rao; Russel Sears; Beysim Sezgin; Julia Wang,Abstract Resource Managers like Apache YARN have emerged as a critical layer in thecloud computing system stack; but the developer abstractions for leasing cluster resourcesand instantiating application logic are very low-level. This flexibility comes at a high cost interms of developer effort; as each application must repeatedly tackle the same challenges(eg; fault-tolerance; task scheduling and coordination) and re-implement commonmechanisms (eg; caching; bulk-data transfers). This paper presents REEF; a developmentframework that provides a control-plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF providesmechanisms that facilitate resource re-use for data caching; and state managementabstractions that greatly ease the development of elastic data processing work-flows on …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,13
X− SOM: Ontology Mapping and Inconsistency Resolution,Carlo Curino; Giorgio Orsi; Letizia Tanca,Abstract: Data integration is an old but still open issue in the database research area; whereSemantic Web technologies; such as ontologies; may be of great help. Aim of the Context-ADDICT project is to provide support for the integration and context-aware reshaping of datacoming from heterogeneous data sources. Within this framework; we use ontologyextraction; alignment and tailoring to ﬁnd and solve conﬂicts due to data sourceheterogeneity. In this paper we present X-SOM: an ontology mapping tool developed withinthe Context-ADDICT project. The contribution of this high precision mapping tool istwofold:(i) a modular and extensible architecture that automatically combines severalmatching techniques by means of a neural network; and (ii) a subsystem for the (semi)-automatic resolution of semantic inconsistencies. Besides describing the tool components …,Proc. of 4th European Semantic Web Conference (ESWC),2007,11
Context-addict,C Bolchini; C Curino; E Quintarelli; FA Schreiber; L Tanca,*,Technical Report 2006.044,2006,11
Towards user centric schema mapping platform,Guilian Wang; Rami Rifaieh; Joseph Goguen; Vitaliy Zavesov; Arcot Rajasekar; Mark Miller,Abstract Schema mapping is a fundamental problem in many important databaseapplications. The process of mapping discovery is complex and error prone; it cannot befully automated; so user input is necessary. This paper presents SCIA; a system that assistsusers in creating executable mappings between a source schema and a target schema byautomating simple matches and finding critical points where user input is both necessaryand maximally useful. SCIA finds critical points by using path contexts together with acombination of multiple matching algorithms. SCIA also helps users; when a critical pointoccurs; by asking them specific questions with sufficient context and making suggestions foradding semantic information to data transformation; such as join and grouping conditions.SCIA handles complex mappings involving n-to-m matches with semantic functions and …,International Workshop on Semantic Data and Service Integration; SDSI 2007,2007,9
Emergent Semantics and Cooperation in Multi-Knowledge Environments: the ESTEEM Architecture,C Bolchini; E Quintarelli; R Rossato; F Schreiber; L Tanca,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,VLDB Int. Workshop on Semantic data and Service Integration (SDSI'07),2007,8
PoLiDBMS: Design and Prototype Implementation of a DBMS for Portable Devices.,Cristiana Bolchini; Carlo Curino; Marco Giorgetta; Alessandro Giusti; Antonio Miele; Fabio A Schreiber; Letizia Tanca,Abstract. Very Small DataBases (VSDB) is a methodology and a complete framework fordatabase design and management in a complex environment where databases aredistributed over different systems; from high-end servers to reduced-power portable devices.Within this framework the architecture of PoLiDBMS; a Portable Light DatabaseManagement System has been designed to be hosted on such portable devices; in order toefficiently manage the data stored in Flash EEPROM memory. A flexible and modularsolution has been adopted with the aim of allowing the development of a system able to becustomized in its features; depending on the needed functionality and the availableprocessing power. The first prototype implementation provides all the elementaryfunctionalities of a DBMS; supporting a reduced set of the SQL language that can be of …,SEBD,2004,7
PerfOrator: eloquent performance models for Resource Optimization,Kaushik Rajan; Dharmesh Kakadia; Carlo Curino; Subru Krishnan,Abstract Query Optimization focuses on finding the best query execution plan; given fixedhardware resources. In BigData settings; both pay-as-you-go clouds and on-prem sharedclusters; a complementary challenge emerges: Resource Optimization: find the besthardware resources; given an execution plan. In this world; provisioning is almostinstantaneous and time-varying resources can be acquired on a per-query basis. Thisallows us to optimize allocations for completion time; resource usage; dollar cost; etc. Theseoptimizations have a huge impact on performance and cost; and pivot around a corechallenge: faithful resource-to-performance models for arbitrary BigData queries. This task ischallenging for users and tools alike due to lack of good statistics (high-velocity;unstructured data); frequent use of UDFs; impact on performance of different hardware …,Proceedings of the Seventh ACM Symposium on Cloud Computing,2016,6
Information systems integration and evolution: Ontologies at rescue,Carlo A Curino; Letizia Tanca; Carlo Zaniolo,Abstract The life of a modern Information System is often characterized by (i) a push towardintegration with other systems; and (ii) the evolution of its data management core inresponse to continuously changing application requirements. Most of the current proposalsdealing with these issues from a database perspective rely on the formal notions of mappingand query rewriting. This paper presents the research agenda of ADAM (Advanced DataAnd Metadata Manager); by harvesting the recent theoretical advances in this area into aunified framework; ADAM seeks to deliver practical solutions to the problems of automaticschema mapping and assisted schema evolution. The evolution of an Information System(IS) reflects the changes occurring in the application reality that the IS is modelling: thus;ADAM exploits ontologies to capture such changes and provide traceability and …,International Workshop on Semantic Technologies in System Maintenance (STSM),2008,6
Mining officially unrecognized side effects of drugs by combining web search and machine learning,Carlo A Curino; Yuanyuan Jia; Bruce Lambert; Patricia M West; Clement Yu,Abstract We consider the problem of finding officially unrecognized side effects of drugs. Bysubmitting queries to the Web involving a given drug name; it is possible to retrieve pagesconcerning the drug. However; many retrieved pages are irrelevant and some relevantpages are not retrieved. More relevant pages can be obtained by adding the activeingredient of the drug to the query. In order to eliminate irrelevant pages; we propose amachine learning process to filter out the undesirable pages. The process is shownexperimentally to be very effective. Since obtaining training data for the machine learningprocess can be time consuming and expensive; we provide an automatic method togenerate the training data. The method is also shown to be very accurate. The side effects ofthree drugs which are not recognized by FDA are validated by an expert. We believe that …,Proceedings of the 14th ACM international conference on Information and knowledge management,2005,6
Towards database virtualization for database as a service,Aaron J Elmore; Carlo Curino; Divyakant Agrawal; Amr El Abbadi,Abstract Advances in operating system and storage-level virtualization technologies haveenabled the effective consolidation of heterogeneous applications in a shared cloudinfrastructure. Novel research challenges arising from this new shared environment includeload balancing; workload estimation; resource isolation; machine replication; live migration;and an emergent need of automation to handle large scale operations with minimal manualintervention. Given that databases are at the core of most applications that are deployed inthe cloud; database management systems (DBMSs) represent a very important technologycomponent that needs to be virtualized in order to realize the benefits of virtualization fromautonomic management of data-intensive applications in large scale data-centers. The goalof this tutorial is to survey the techniques used in providing elasticity in virtual machine …,Proceedings of the VLDB Endowment,2013,5
A survey of context models and a preliminary methodology for context driven data view definition,C Bolchini; C Curino; E Quintarelli; FA Schreiber; L Tanca,The peer to peer scenario of the ESTEEM project introduces another interesting motivationbehind the desire to define customized data views of a semantic community information;tailored on the peer's interests. We propose to exploit the information usage context within amethodology for context-aware data design; where the notion of context must be formallydefined; together with its role within the process of information tailoring in the P2P setting.This report presents our context model; called Context Dimension Tree; which plays afundamental role in tailoring the information space according to the user's informationneeds; as well as an analysis of relevant features of context models; later used forclassification and comparison.,*,2006,5
ERA: a framework for economic resource allocation for the cloud,Moshe Babaioff; Yishay Mansour; Noam Nisan; Gali Noti; Carlo Curino; Nar Ganapathy; Ishai Menache; Omer Reingold; Moshe Tennenholtz; Erez Timnat,Abstract Cloud computing has reached significant maturity from a systems perspective; butcurrently deployed solutions rely on rather basic economics mechanisms that yieldsuboptimal allocation of the costly hardware resources. In this paper we present EconomicResource Allocation (ERA); a complete framework for scheduling and pricing cloudresources; aimed at increasing the efficiency of cloud resources usage by allocatingresources according to economic principles. The ERA architecture carefully abstracts theunderlying cloud infrastructure; enabling the development of scheduling and pricingalgorithms independently of the concrete lower-level cloud infrastructure and independentlyof its concerns. Specifically; ERA is designed as a flexible layer that can sit on top of anycloud system and interfaces with both the cloud resource manager and with the users …,Proceedings of the 26th International Conference on World Wide Web Companion,2017,4
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,David Roman,Resolving the problem of y2k compliance is a serious issue for the distributed enterprise. Asorganizations rely on distributed desktops for decision making and productivity; the risksassociated with noncompliant desktops are receiving increased attention from the media;industry analysts; government officials; and corporate leaders. Although most organizationshave been aggressively correcting the Y2K problem on their central mainframe applications;many are only beginning to address the significant risks posed by errant desktop software;hardware; and firmware. Since these distributed assets are critical to corporate productivity;organizations are vulnerable to significant risks if any of the distributed informationtechnology assets cannot properly process four-digit dates.Consider; for example;investment bankers who issue their first bond trades of the new millennium using …,Communications of the ACM,2000,4
Blind men and an elephant coalescing open-source; academic; and industrial perspectives on BigData,Chris Douglas; Carlo Curino,This tutorial is organized in two parts. In the first half; we will present an overview ofapplications and services in the BigData ecosystem. We will use known distributed databaseand systems literature as landmarks to orient the attendees in this fast-evolving space.Throughout; we will contrast models of resource management; performance; and theconstraints that shape the architectures of prominent systems. We will also discuss the roleof academia and industry in the development of open-source infrastructure; with anemphasis on open problems and strategies for collaboration. We assume only basicfamiliarity with distributed systems. In the second half; we will delve into Apache HadoopYARN. YARN (Yet Another Resource Negotiator) transformed Hadoop from a MapReduceengine to a general-purpose cluster scheduler. Since its introduction; it has been …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,3
No bits left behind,Eugene Wu; Carlo Curino; Samuel Madden,One of the key tenets of database system design is making efficient use of storage andmemory resources. However; existing database system implementations are actuallyextremely wasteful of such resources; for example; most systems leave a great deal of emptyspace in tuples; index pages; and data pages; and spend many CPU cycles reading coldrecords from disk that are never used. In this paper; we identify a number of such sources ofwaste; and present a series of techniques that limit this waste (eg; forcing better memorylocality for hot data and using empty space in index pages to cache popular tuples) withoutsubstantially complicating interfaces or system design. We show that these techniqueseffectively reduce memory requirements for real scenarios from the Wikipedia database (byup to 17.8×) while increasing query performance (by up to 8×).,*,2011,3
The shining embedded system design methodology based on self dynamic reconfigurable architectures,CA Curino; Luca Fossati; Vincenzo Rana; Francesco Redaelli; Marco D Santambrogio; Donatella Sciuto,Complex design; targeting System-on-Chip based on reconfigurable architectures; still lacksa generalized methodology allowing both the automatic derivation of a complete systemsolution able to fit into the final device; and mixed hardware-software solutions; exploitingpartial reconfiguration capabilities. The Shining methodology organizes the inputspecification of a complex System-on-Chip design into three different components:hardware; reconfigurable hardware and software; each handled by dedicated sub-flows. Acommunication model guarantees reliable and seamless interfacing of the variouscomponents. The developed system; stand-alone or OS-based; is architecture-independent.The Shining flow reduces the time for system development; easing the design of complexhardware/software reconfigurable applications.,Design Automation Conference; 2008. ASPDAC 2008. Asia and South Pacific,2008,3
Dependency-Driven Analytics: A Compass for Uncharted Data Oceans.,Ruslan Mavlyutov; Carlo Curino; Boris Asipov; Philippe Cudre-Mauroux,ABSTRACT In this paper; we predict the rise of Dependency-Driven Analytics (DDA); a newclass of data analytics designed to cope with growing volumes of unstructured data. DDAdrastically reduces the cognitive burden of data analysis by systematically leveraging acompact dependency graph derived from the raw data. The computational cost associatedwith the analysis is also reduced substantially; as the graph acts as an index for commonlyaccessed data items. We built a system supporting DDA using off-the-shelf Big Data andgraph DB technologies; and deployed it in production at Microsoft to support the analysis ofthe exhaust of our Big Data infrastructure producing petabytes of system logs daily. Thedependency graph in this setting captures lineage information among jobs and files and isused to guide the analysis of telemetry data. We qualitatively discuss the improvement …,CIDR,2017,2
Relational cloud: The case for a database service,Eugene Wu; Samuel Madden; Yang Zhang; Evan Jones; Carlo Curino,In this paper; we make the case for â databases as a serviceâ (DaaS); with two targetscenarios in mind:(i) consolidation of data management functionality for large organizationsand (ii) outsourcing data management to a cloud-based service provider for small/mediumorganizations. We analyze the many challenges to be faced; and discuss the design of adatabase service we are building; called Relational Cloud. The system has been designedfrom scratch and combines many recent advances and novel solutions. The prototype wepresent exploits multiple dedicated storage engines; provides high-availability viatransparent replication; supports automatic workload partitioning and live data migration;and provides serializable distributed transactions. While the system is still under activedevelopment; we are able to present promising initial results that showcase the key …,*,2010,2
Research meets Education: DRESD; a virtuous circle,Carlo A Curino; Marco D Santambrogio; Donatella Sciuto,ABSTRACT Research and Education have been often perceived as a dichotomy. It has oftenbeen hard to couple them in a productive and virtuous cycle. With this paper we would like todiscuss our attempt in this direction; briefly presenting the approach and the positive resultsobtained. The key idea is involving students; by means of projects and theses; in theresearch activities connected to the courses in their curricula. We monitored the activitiesand the productivity of the DRESD project; in the period 2004-2007; where students havebeen involved in the research activities of the group; and report the first encouraging results.,European Workshop on Microeletronics Education (EWME 2008),2008,1
Schema Evolution In Wikipedia,Carlo A Curino; Hyun J Moon; Letizia Tanca; Carlo Zaniolo,Abstract: Evolving the database that is at the core of an Information System represents adifficult maintenance problem that has only been studied in the framework of traditionalinformation systems. However; the problem is likely to be even more severe in webinformation systems; where open-source software is often developed through thecontributions and collaboration of many groups and individuals. Therefore; in this paper; wepresent an indepth analysis of the evolution history of the Wikipedia database and itsschema; Wikipedia is the best-known example of a large family of web information systemsbuilt using the open-source software MediaWiki. Our study is based on:(i) a set of SchemaModification Operators that provide a simple conceptual representation for complex schemachanges; and (ii) simple software tools to automate the analysis. This framework allowed …,*,2008,1
The ESTEEM Architecture for Emergent Semantics and Cooperation in MultiKnowledge Environments,C Aiello; R Baldoni; D Bianchini; C Bolchini; S Bonomi; S Castano; T Catarci; CA Curino; V De Antonellis; A Ferrara; M Melchiori; D Milano; S Montanelli; G Orsi; A Poggi; L Querzoni; E Quintarelli; R Rossato; D Salvi; M Scannapieco; FA Schreiber; L Tanca; S Tucci Pergiovanni,In the present global society; information has to be exchangeable in open and dynamicenvironments; where interacting peers do not necessarily share a common understanding ofthe world at hand; and do not have a complete picture of the context where the interactionoccurs. In this deliverable; we present the Esteem approach and the related peerarchitecture for emergent semantics in dynamic and multi-knowledge environments. InEsteem; semantic communities are built around declared interests in the form of manifestoontologies; and their autonomous nature is preserved by allowing a shared semantics tonaturally emerge from peer interactions.,Relatório Técnico. MIUR PRIN Esteem Project. Università Degli Studi Di Milano; Italy,2007,1
Context− aware views for mobile users,Cristiana Bolchini; Carlo Curino; Giorgio Orsi; Elisa Quintarelli; Rosalba Rossato; Fabio A Schreiber; Letizia Tanca,Abstract: Independent; heterogeneous; distributed; sometimes transient and mobile datasources produce an enormous amount of information that should be semantically integratedand ﬁltered; or; as we say; tailored; based on the users' interests and context. We propose toexploit knowledge about the user; the adopted device; and the environment-altogethercalled context-to the end of information tailoring. This paper presents the Context DimensionTree; a context model which is the basis for solving the information tailoring problem; alongwith its role in the framework of the Context-ADDICT architecture.,Proc. of the 10th DELOS Thematic Workshop on Personalized Access ‚Profile Management ‚and Context Awareness in Digital Libraries (PersDL− UM Workshops),2007,1
Context integration for mobile data design,Carlo Aldo Curino,*,*,2006,1
Apache REEF: Retainable Evaluator Execution Framework,Byung-Gon Chun; Tyson Condie; Yingda Chen; Brian Cho; Andrew Chung; Carlo Curino; Chris Douglas; Matteo Interlandi; Beomyeol Jeon; Joo Seong Jeong; Gyewon Lee; Yunseong Lee; Tony Majestro; Dahlia Malkhi; Sergiy Matusevych; Brandon Myers; Mariia Mykhailova; Shravan Narayanamurthy; Joseph Noor; Raghu Ramakrishnan; Sriram Rao; Russell Sears; Beysim Sezgin; Taegeon Um; Julia Wang; Markus Weimer; Youngseok Yang,Abstract Resource Managers like YARN and Mesos have emerged as a critical layer in thecloud computing system stack; but the developer abstractions for leasing cluster resourcesand instantiating application logic are very low level. This flexibility comes at a high cost interms of developer effort; as each application must repeatedly tackle the same challenges(eg; fault tolerance; task scheduling and coordination) and reimplement commonmechanisms (eg; caching; bulk-data transfers). This article presents REEF; a developmentframework that provides a control plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF providesmechanisms that facilitate resource reuse for data caching and state managementabstractions that greatly ease the development of elastic data processing pipelines on …,ACM Transactions on Computer Systems (TOCS),2017,*
Seamless cluster servicing,*,Embodiments are directed to progressively migrating source computer nodes where thesource computer nodes perform a computer-implemented service. In one embodiment; acomputer system determines that execution of the performed service is to be migrated fromthe source computer nodes to target computer nodes. The computer system groups thesource computer nodes into multiple source subgroups; where each source subgroupincludes at least one source computer node. The computer system then schedules creationof target subgroups of target nodes. These target subgroups include at least one sourcecomputer node and; themselves; correspond to a source subgroup. The computer systemactivates a first target subgroup corresponding to a first source subgroup; and deactivatesthe first source subgroup. In this manner; the first target subgroup replaces the first source …,*,2017,*
Geo-scale analytics with bandwidth and regulatory constraints,*,Various technologies described herein pertain to controlling geo-scale analytics withbandwidth and regulatory constraints. An analytical query (eg; a recurrent analytical query; anon-recurrent analytical query; etc.) to be executed over distributed data in data partitionsstored in a plurality of data centers can be received. Moreover; a query execution plan forthe analytical query can be generated; where the query execution plan includes tasks.Further; replication strategies for the data partitions can be determined. A replication strategyfor a particular data partition can specify one or more data centers to which the particulardata partition is to be replicated if the particular data partition is to be replicated. The tasks ofthe query execution plan for the analytical query can further be scheduled to the datacenters based on the replication strategies for the data partitions. The analytical query …,*,2016,*
BenchPress: Dynamic Workload Control in the OLTP-Bench Testbed,Dana Van Aken; Djellel E Difallah; Andrew Pavlo; Carlo Curino; Philippe Cudré-Mauroux,Abstract Benchmarking is an essential activity when choosing database products; tuningsystems; and understanding the trade-offs of the underlying engines. But the workloadsavailable for this effort are often restrictive and non-representative of the ever changingrequirements of the modern database applications. We recently introduced OLTP-Bench; anextensible testbed for benchmarking relational databases that is bundled with 15 workloads.The key features that set this framework apart is its ability to tightly control the request rateand dynamically change the transaction mixture. This allows an administrator to composecomplex execution targets that recreate real system loads; and opens the doors to newresearch directions involving tuning for special execution patterns and multi-tenancy. In thisdemonstration; we highlight OLTP-Bench's important features through the BenchPress …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,*
Smart Monitor System For Automatic Anomaly Detection@ Baidu,Xianping Qu,Abstract: Billions of requests are supported by hundreds of thousands of servers in Baidu.So many servers and modules bring a huge challenge to engineers for anomaly detection.When an anomaly occurs; various alarms and incidents are sent to engineers. It is verydifficult to find the root cause based on large non-organized monitoring data and alarms.Thus; we tried to build a smarter monitoring system named BIMS (Baidu IntelligentMonitoring System) to help engineers to analyze the problems and give the most possiblereasons for important anomaly such as revenue loss.,*,2015,*
Introduction to Database Systems,Carlo A Curino,*,*,2013,*
DBSeer: Resource and Performance Prediction for Building a Next Generation Database Cloud.,Carlo Curino,*,*,2013,*
Towards database virtualization for database as a service,Carlo Curino,*,Proceedings of the VLDB Endowment,2013,*
Performance and resource modeling in highly-concurrent oltp workloads,Carlo Curino,*,*,2013,*
Supporting database schema evolution represents a long-standing challenge of practical and theoretical importance for modern information systems. In this paper; w...,Wolfgang Lehner; Sunita Sarawagi; Carlo Curino; Hyun Jin Moon; Alin Deutsch; Carlo Zaniolo,In this paper; we present a technique for building a high-availability (HA) databasemanagement system (DBMS). The proposed technique can be applied to any DBMS withlittle or no customization; and with reasonable performance overhead. Our approach isbased on Remus; a commodity HA solution implemented in the virtualization layer; that usesasynchronous virtual machine state replication to provide...,The VLDB Journal,2013,*
Mobius: unified messaging and data serving for mobile apps,Carlo Curino; Rusty Sears,*,*,2012,*
Lookup tables: Fine-grained partitioning for distributed databases,Carlo Curino,*,*,2012,*
No bits left behind,Carlo Curino,*,*,2011,*
Data Integration: A New Teenage,CA Curino; G Orsi; L Tanca,The Present: 2000-2008 • Mediated Schema - DIS [2] • Reverse Engineering • Semantic Annotationand publishing • P2P - Data Exchange [1] • Full P2P • Publish & Subscribe • Data Rings andData Spaces [3; 4] … Limitations • Data sources known in advance • Static; hand-mademappings … DATA SOURCE 1 (RDBMS) DATA SOURCE 2 (XML) DATA SOURCE 3(WWW) … Challenges • Internet of Things • Autonomic data-sources • Data and Users Mobility• Embedded systems Databases • Uncertainty and Lineage Management • Multimodality • Ambientand body intelligence • Information noise … Problems • Past problems are not completelysolved. • Missing or expensive infrastructures. • Sound and complete data integration in openworld is unpractical. • 85% of interesting data is unstructured … Context-ADDICT (joint workwith C. Bolchini; E. Quintarelli and FA Schreiber) … Features • Context-aware …,*,2008,*
Emergent semantics and cooperation in multi-knowledge environments: the ESTEEM architecture,Carola Aiello; R Baldoni; D Bianchini; C Bolchini; S Bonomi; S Castano; T Catarci; CA Curino; V De Antonellis; A Ferrara; M Melchiori; D Milano; S Montanelli; G Orsi; A Poggi; L Querzoni; E Quintarelli; R Rossato; D Salvi; M Scannapieco; FA Schreiber; L Tanca; S Tucci Pergiovanni,In the present global society; information has to be exchangeable in open and dynamicenvironments; where interacting peers do not necessarily share a common understanding ofthe world at hand; and do not have a complete picture of the context where the interactionoccurs. In this paper; we present the Esteem approach and the related peer architecture foremergent semantics in dynamic and multi-knowledge environments. In Esteem; semanticcommunities are built around declared interests in the form of manifesto ontologies; andtheir autonomous nature is preserved by allowing a shared semantics to naturally emergefrom peer interactions. Copyright 2007 VLDB Endowment.,International Workshop on Semantic Data and Service Integration,2007,*
A comparative study and a formal description of the data synchronization protocol,C Bolchini; C Curino; E Quintarelli; R Rossato; FA Schreiber; L Tanca,The aim of this document is the specification of an interaction protocol between a peer andthe other peers in the community; in order to synchronize local data according to the peer'scontext; gathering information available at the other peers' sites. The scenario is that of anexisting semantic community; sharing data and services among peers; with the support of acontext-aware methodology for identifying the interesting portion of data; the aim is that ofenabling the peers to make queries that are as precise as possible in order to limitcommunication costs; to reduce the amount of data to be stored; and discarding possiblynon-interesting information. Furthermore; considering the transient character of P2Pcommunities; peers are also interested in locally caching information that may be useful atlater times; when no data exchange will be possible; in such a situation; the capability is …,*,2006,*
Context integration for mobile data tailoring,Carlo Curino,*,*,2006,*
PoLiDBMS: Design and Prototype Implementation of a DBMS for Portable Devices.,Carlo Curino,*,*,2004,*
Improving Search and Navigation by Combining Ontologies and Social Tags,Carlo A Curino; Davide Eynard; Giorgio Orsi,Abstract. The Semantic Web has the ambitious goal of enabling complex autonomousapplications to reason on a machine-processable version of the World Wide Web. This;however; would require a coordinated effort not easily achievable in practice. On the otherhand; spontaneous communities; based on social tagging; recently achieved noticeableconsensus and diffusion. The goal of the TagOnto system is to bridge between these tworealities by automatically mapping (social) tags to more structured domain ontologies; thus;providing assistive; navigational features typical of the Semantic Web. These novelsearching and navigational capabilities are complementary to more traditional searchengine functionalities. The system; and its intuitive AJAX interface; are released anddemonstrated on-line.,*,*,*
Performance Modeling and Optimization of Distributed Queries,Dharmesh Kakadia; Kaushik Rajan; Kapil Vaswani; Subramaniam Krishnan; Carlo Curino,*,*,*,*
Report on the Second International Workshop on Data Management in the Cloud (DMC 2013),Ashraf Aboulnaga; Carlo Curino,The Second International Workshop on Data Management in the Cloud took place on April8; 2013 in Brisbane; Australia; on the day before ICDE. The DMC workshop aims to bringresearchers and practitioners in cloud computing and data management systems together todiscuss the research issues at the intersection of these two areas; and also to draw moreattention from the larger data management and systems research communities to this newand highly promising field. The DMC Workshops are sponsored by the IEEE TCDEWorkgroup on Cloud Computing. The DMC 2013 program began with a keynotepresentation by Amr El Abbadi; Professor at the University of California; Santa Barbara. Thiswas followed by six technical papers presented in two sessions. The workshop concludedwith a panel discussion on research challenges in data management for the cloud.,*,*,*
CS 553 1 st project: A JAVA RMI and Socket implementation of the Chandy-Lamport distributed snapshot algorithm.,Carlo Aldo Curino,*,*,*,*
Delta Debugging,Carlo Curino; Alessandro Giusti,ABSTRACT Il debugginge una delle attivita piu onerose nel processo di sviluppo delsoftware; in particolare il compito piu arduo ed imprevedibile in termini di tempoe quello diisolare la sorgente del problema. Il Delta Debugginge una tecnica innovativa; sistematica edautomatica; che fornisce una solida base teorica per affrontare proprio questo compito; leapplicazioni sperimentate finora riguardano l'input di programmi; il codice; lo stato e loscheduling: ciascuna ha all'attivo importanti successi su scenari reali. Benche l'interventodel programmatore sia comunque necessario; questa nuova tecnica permette in molti casiuna sensibile riduzione dello sforzo dedicato al Debugging.,*,*,*
