A foundation for representing and querying moving objects,Ralf Hartmut Güting; Michael H Böhlen; Martin Erwig; Christian S Jensen; Nikos A Lorentzos; Markus Schneider; Michalis Vazirgiannis,Abstract Spatio-temporal databases deal with geometries changing over time. The goal ofour work is to provide a DBMS data model and query language capable of handling suchtime-dependent geometries; including those changing continuously that describe movingobjects. Two fundamental abstractions are moving point and moving region; describingobjects for which only the time-dependent position; or position and extent; respectively; areof interest. We propose to present such time-dependent geometries as attribute data typeswith suitable operations; that is; to provide an abstract data type extension to a DBMS datamodel and query language. This paper presents a design of such a system of abstract datatypes. It turns out that besides the main types of interest; moving point and moving region; arelatively large number of auxiliary data types are needed. For example; one needs a line …,ACM Transactions on Database Systems (TODS),2000,955
The consensus glossary of temporal database concepts—February 1998 version,Christian S Jensen; Curtis E Dyreson; Michael Böhlen; James Clifford; Ramez Elmasri; Shashi K Gadia; Fabio Grandi; Pat Hayes; Sushil Jajodia; Wolfgang Käfer; Nick Kline; Nikos Lorentzos; Yannis Mitsopoulos; Angelo Montanari; Daniel Nonen; Elisa Peressi; Barbara Pernici; John F Roddick; Nandlal L Sarda; Maria Rita Scalas; Arie Segev; Richard T Snodgrass; Mike D Soo; Abdullah Tansel; Paolo Tiberio; Gio Wiederhold,Abstract This document 1 contains definitions of a wide range of concepts specific to andwidely used within temporal databases. In addition to providing definitions; the documentalso includes explanations of concepts as well as discussions of the adopted names. Theconsensus effort that lead to this glossary was initiated in Early 1992. Earlier versionsappeared in SIGMOD Record in September 1992 and March 1994. The present glossarysubsumes all the previous documents. The glossary meets the need for creating a higherdegree of consensus on the definition and naming of temporal database concepts. Two setsof criteria are included. First; all included concepts were required to satisfy four relevancecriteria; and; second; the naming of the concepts was resolved using a set of evaluationcriteria. The concepts are grouped into three categories: concepts of general database …,*,1998,482
Coalescing in temporal databases,Michael H Böhlen; Richard T Snodgrass; Michael D Soo,Abstract Coalescing is a unary operator applicable to temporal databases; it is similar toduplicate elimination in conventional databases. Tuples in a temporal relation that agree onthe explicit attribute values and that have adjacent or overlapping time periods arecandidates for coalescing. Uncoalesced relations can arise in many ways; eg; via aprojection or union operator; or by not enforcing coalescing on update or insertion. In thispaper we show how semantically superfluous coalescing can be eliminated. We then turn toefficiently performing coalescing. We provide a variety of iterative and non-iterativeapproaches; via SQL and embedded SQL; that require no changes to the DBMS;demonstrating that coalescing can be formulated in SQL-89. Detailed performance studiesshow that all such approaches are quite expensive. We propose a spectrum of coalescing …,*,1996,162
Querying ATSQL databases with temporal logic,Jan Chomicki; David Toman; Michael H Böhlen,Abstract We establish a correspondence between temporal logic and a subset of ATSQL; atemporal extension of SQL-92. In addition; we provide an effective translation from temporallogic to ATSQL that enables a user to write high-level queries which are then evaluatedagainst a space-efficient representation of the database. A reverse translation; also providedin this paper; characterizes the expressive power of a syntactically defined subset of ATSQLqueries.,ACM Transactions on Database Systems (TODS),2001,124
Spatio-temporal models and languages: An approach based on data types,Ralf Hartmut Güting; Michael H Böhlen; Martin Erwig; Christian S Jensen; Nikos Lorentzos; Enrico Nardelli; Markus Schneider; Jose RR Viqueira,Abstract In this chapter we develop DBMS data models and query languages to deal withgeometries changing over time. In contrast to most of the earlier work on this subject; thesemodels and languages are capable of handling continuously changing geometries; ormoving objects. We focus on two basic abstractions called moving point and moving region.A moving point can represent an entity for which only the position in space is relevant. Amoving region captures moving as well as growing or shrinking regions. Examples formoving points are people; polar bears; cars; trains; or air planes; examples for movingregions are hurricanes; forest fires; or oil spills in the sea.,*,2003,122
Point-versus interval-based temporal data models,Michael H Bohlen; Renato Busatto; Christian S Jensen,The association of timestamps with various data items such as tuples or attribute values isfundamental to the management of time varying information. Using intervals in timestamps;as do most data models; leaves a data model with a variety of choices for giving a meaningto timestamps. Specifically; some such data models claim to be point based while other datamodels claim to be interval based. The meaning chosen for timestamps is important it has apervasive effect on most aspects of a data model; including database design; a variety ofquery language properties; and query processing techniques; eg; the availability of queryoptimization opportunities. The paper precisely defines the notions of point based andinterval based temporal data models; thus providing a new formal basis for characterizingtemporal data models and obtaining new insights into the properties of their query …,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,115
Transitioning temporal support in TSQL2 to SQL3,Richard T Snodgrass; Michael H Böhlen; Christian S Jensen; Andreas Steiner,Abstract This document summarizes the proposals before the SQL3 committees to allow theaddition of tables with valid-time and transactiontime support into SQL/Temporal; andexplains how to use these facilities to migrate smoothly from a conventional relationalsystem to one encompassing temporal support. Initially; important requirements to atdiscussed. The proposal then describes the language additions necessary emporal systemthat may facilitate such a transition are motivated and to add valid-time support to SQL3while fulfilling these requirements. The constructs of the language are divided into fourlevels; with each level adding increased temporal functionality to its predecessor. Aprototype system implementing these constructs on top of a conventional DBMS is publiclyavailable.,*,1998,97
Approximate matching of hierarchical data using pq-grams,Nikolaus Augsten; Michael Böhlen; Johann Gamper,Abstract When integrating data from autonomous sources; exact matches of data items thatrepresent the same real world object often fail due to a lack of common keys. Yet in manycases structural information is available and can be used to match such data. As a runningexample we use residential address information. Addresses are hierarchical structures andare present in many databases. Often they are the best; if not only; relationship betweenautonomous data sources. Typically the matching has to be approximate since therepresentations in the sources differ. We propose pq-grams to approximately matchhierarchical information from autonomous sources. We define the pq-gram distancebetween ordered labeled trees as an effective and efficient approximation of the well-knowntree edit distance. We analyze the properties of the pq-gram distance and compare it with …,Proceedings of the 31st international conference on Very large data bases,2005,96
Temporal statement modifiers,Michael H Böhlen; Christian S Jensen; Richard Thomas Snodgrass,Abstract A wide range of database applications manage time-varying data. Many temporalquery languages have been proposed; each one the result of many carefully made yetsubtly interacting design decisions. In this article we advocate a different approach toarticulating a set of requirements; or desiderata; that directly imply the syntactic structure andcore semantics of a temporal extension of an (arbitrary) nontemporal query language. Thesedesiderata facilitate transitioning applications from a nontemporal query language and datamodel; which has received only scant attention thus far. The paper then introduces thenotion of statement modifiers that provide a means of systematically adding temporalsupport to an existing query language. Statement modifiers apply to all query languagestatements; for example; queries; cursor definitions; integrity constraints; assertions; views …,ACM Transactions on Database Systems (TODS),2000,90
Temporal database system implementations,Michael H Böhlen,Abstract Although research on temporal database systems has been active for about 20years; implementations have not appeared until recently. This is one reason why currentcommercial database systems provide only limited temporal functionality. This papersummarizes extant state of the art of temporal database implementations. Rather than beingvery specific about each system we have attempted to provide an indication of thefunctionality together with pointers to additional information. It is hoped that this leads tomore efforts pushing the implementation of temporal database systems in the near future.,ACM Sigmod Record,1995,84
Visual data mining: theory; techniques and tools for visual analytics,Simeon Simoff; Michael H Böhlen; Arturas Mazeika,Visual Data Mining—Opening the Black Box Knowledge discovery holds the promise ofinsight into large; otherwise opaque datasets. Thenatureofwhatmakesaruleinterestingtoau…1 widely but most agree that it is a subjective quality based on the practical u-fulness of theinformation. Being subjective; the user needs to provide feedback to the system and; as isthe case for all systems; the sooner the feedback is given the quicker it can in? uence thebehavior of the system. There have been some impressive research activities over the pastfew years but the question to be asked is why is visual data mining only now being-vestigated commercially? Certainly; there have been arguments for visual data 2 mining fora number of years–Ankerst and others argued in 2002 that current (autonomous andopaque) analysis techniques are ine? cient; as they fail to-rectly embed the user in …,*,2008,83
Temporal data model and query language concepts. Encyclopedia of Information Systems; 4,MH Böhlen; CS Jensen,*,*,2003,72
Efficient OLAP query processing in distributed data warehouses,Michael O Akinde; Michael H Böhlen; Theodore Johnson; Laks VS Lakshmanan; Divesh Srivastava,Abstract The success of Internet applications has led to an explosive growth in the demandfor bandwidth from Internet Service Providers. Managing an Internet protocol networkrequires collecting and analyzing network data; such as flow-level traffic statistics. Suchanalyses can typically be expressed as OLAP queries; eg; correlated aggregate queries anddata cubes. Current day OLAP tools for this task assume the availability of the data in acentralized data warehouse. However; the inherently distributed nature of data collectionand the huge amount of data extracted at each collection point make it impractical to gatherall data at a centralized site. One solution is to maintain a distributed data warehouse;consisting of local data warehouses at each collection point and a coordinator site; with mostof the processing being performed at the local sites. In this paper; we consider the …,Information Systems,2003,66
Capturing and querying multiple aspects of semistructured data,Curtis E Dyreson; Michael H Böhlen; Christian S Jensen,Abstract Motivated to a large extent by the substantial and growing prominence of the World-Wide Web and the potential benefits that may be obtained by applying database conceptsand techniques to web data management; new data models and query languages haveemerged that contend with the semistructured nature of web data. These models organizedata in graphs. The nodes in a graph denote objects or values; and each edge is labeledwith a single word or phrase. Nodes are described by the labels of the paths that lead tothem; and these descriptions serve as the basis for querying. This paper proposes anextensible framework for capturing and querying properties in a semistructured data model.The paper considers temporal properties of data; prices associated with data access; qualityratings associated with the data; and access restrictions on the data. In this way; it …,VLDB,1999,64
The pq-gram distance between ordered labeled trees,Nikolaus Augsten; Michael Böhlen; Johann Gamper,Abstract When integrating data from autonomous sources; exact matches of data items thatrepresent the same real-world object often fail due to a lack of common keys. Yet in manycases structural information is available and can be used to match such data. Typically thematching must be approximate since the representations in the sources differ. We proposepq-grams to approximately match hierarchical data from autonomous sources and define thepq-gram distance between ordered labeled trees as an effective and efficient approximationof the fanout weighted tree edit distance. We prove that the pq-gram distance is a lowerbound of the fanout weighted tree edit distance and give a normalization of the pq-gramdistance for which the triangle inequality holds. Experiments on synthetic and real-worlddata (residential addresses and XML) confirm the scalability of our approach and show …,ACM Transactions on Database Systems (TODS),2010,63
Adding valid time to SQL/temporal,Richard T Snodgrass; Michael H Böhlen; Christian S Jensen; Andreas Steiner,This change proposal introduces additions to SQL/Temporal to add valid-time support toSQL3. We outline a four-level approach for the integration of time. We motivate and discusseach level in turn; and we define the syntactic extensions that correspond to each level. Wewill see that the extensions are fairly minimal. Each level is described via a quick tourconsisting of a set of examples. These examples have been tested in a prototype which ispublicly available [14]. The proposed language constructs ensure temporal upwardcompatibility; sequenced valid semantics; and non-sequenced semantics; importantproperties that will be discussed in detail in Section 5.,ANSI X3H2-96-501r2; ISO/IEC JTC,1996,57
Notions of upward compatibility of temporal query languages,John Bair; Michael H.  Boehlen; Christian S.  Jensen; Richard T.  Snodgrass,Abstract Migrating applications from conventional to temporal database managementtechnology has received scant mention in the research literature. This paper formally definesthree increasingly restrictive notions of upward compatibility which capture properties of atemporal SQL with respect to conventional SQL that; when satisfied; provide for a smoothmigration of legacy applications to a temporal system. The notions of upward compatibilitydictate the semantics of conventional SQL statements and constrain the semantics ofextensions to these statements. The paper evaluates the seven extant temporal extensionsto SQL; all of which are shown to complicate migration through design decisions that violateone or more of these notions. We then outline how SQL–92 can be systematically extendedto become a temporal query language that satisfies all three notions.,Wirtschaftsinformatik,1997,55
Spatio-temporal database support for legacy applications,Michael Böhlen; Christian S Jensen; Bjørn Skjellaug,ABSTRACT In areas such as finance; marketing; and property and resource management;many database applications manage spatio-temporal data. These applications typically runon top of a relational DBMS and manage spatio-temporal data either using the DBMS; whichprovides little support; or employ the services of a proprietary system that co-exists with theDBMS; but is separate from and not integrated with the DBMS. This wealth of applicationsmay benefit substantially from built-in; integrated spatio-temporal DBMS support. Providing afoundation for such support is an important and substantial challenge. This paper initiallydefines technical requirements to a spatio-temporal DBMS aimed at protecting businessinvestments in the existing legacy applications and at reusing personnel expertise. Theserequirements provide a foundation for making it economically feasible to migrate legacy …,Proceedings of the 1998 ACM symposium on Applied Computing,1998,53
Querying TSQL2 databases with temporal logic,Michael H Böhlen; Jan Chomicki; Richard T Snodgrass; David Toman,Abstract We establish an exact correspondence between temporal logic and a subset ofTSQL2; a consensus temporal extension of SQL-92. The translation from temporal logic toTSQL2 developed here enables a user to write high-level queries which can be evaluatedagainst a space-efficient representation of the database. The reverse translation; alsoprovided; makes it possible to characterize the expressive power of TSQL2. We demonstratethat temporal logic is equal in expressive power to a syntactically defined subset of TSQL2.,International Conference on Extending Database Technology,1996,47
Temporal alignment,Anton Dignös; Michael H Böhlen; Johann Gamper,Abstract In order to process interval timestamped data; the sequenced semantics has beenproposed. This paper presents a relational algebra solution that provides native support forthe three properties of the sequenced semantics: snapshot reducibility; extended snapshotreducibility; and change preservation. We introduce two temporal primitives; temporal splitterand temporal aligner; and define rules that use these primitives to reduce the operators of atemporal algebra to their nontemporal counterparts. Our solution supports the threeproperties of the sequenced semantics through interval adjustment and timestamppropagation. We have implemented the temporal primitives and reduction rules in the kernelof PostgreSQL to get native database support for processing interval timestamped data. Thesupport is comprehensive and includes outer joins; antijoins; and aggregations with …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,45
Multi-dimensional aggregation for temporal data,Michael Böhlen; Johann Gamper; Christian S Jensen,Abstract Business Intelligence solutions; encompassing technologies such as multi-dimensional data modeling and aggregate query processing; are being applied increasinglyto non-traditional data. This paper extends multi-dimensional aggregation to apply to datawith associated interval values that capture when the data hold. In temporal databases;intervals typically capture the states of reality that the data apply to; or capture when the dataare; or were; part of the current database state. This paper proposes a new aggregationoperator that addresses several challenges posed by interval data. First; the intervals to beassociated with the result tuples may not be known in advance; but depend on the actualdata. Such unknown intervals are accommodated by allowing result groups that arespecified only partially. Second; the operator contends with the case where an interval …,International Conference on Extending Database Technology,2006,44
Seamless integration of time into SQL,Michael H Böhlen; Christian S Jensen,*,*,1996,44
Approximate joins for data-centric XML,Nikolaus Augsten; Michael Bohlen; Curtis Dyreson; Johann Gamper,In data integration applications; a join matches elements that are common to two datasources. Often; however; elements are represented slightly different in each source; so anapproximate join must be used. For XML data; most approximate join strategies are basedon some ordered tree matching technique. But in data-centric XML the order is irrelevant:two elements should match even if their subelement order varies. In this paper we give asolution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams ina three-step process: sorting the unordered tree; extending the sorted tree with dummynodes; and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,41
Evaluating the Completeness of TSQL2,Michael H Böhlen; Christian S Jensen; Richard T Snodgrass,Abstract The question of what is a well-designed temporal data model and query languageis a difficult; but also an important one. The consensus temporal query language TSQL2attempts to take advantage of the accumulated knowledge gained from designing andstudying many of the earlier models and languages. In this sense; TSQL2 represents aconstructive answer to this question. Others have provided analytical answers by developingcriteria; formulated as completeness properties; for what is a good model and language.This paper applies important existing completeness notions to TSQL2 in order to evaluatethe design of TSQL2. It is shown that TSQL2 satisfies only a subset of these completenessnotions.,*,1995,41
Estimating the selectivity of approximate string queries,Arturas Mazeika; Michael H Böhlen; Nick Koudas; Divesh Srivastava,Abstract Approximate queries on string data are important due to the prevalence of suchdata in databases and various conventions and errors in string data. We present the VSolestimator; a novel technique for estimating the selectivity of approximate string queries. TheVSol estimator is based on inverse strings and makes the performance of the selectivityestimator independent of the number of strings. To get inverse strings we decompose alldatabase strings into overlapping substrings of length q (q-grams) and then associate eachq-gram with its inverse string: the IDs of all strings that contain the q-gram. We usesignatures to compress inverse strings; and clustering to group similar signatures. We studyour technique analytically and experimentally. The space complexity of our estimator onlydepends on the number of neighborhoods in the database and the desired estimation …,ACM Transactions on Database Systems (TODS),2007,38
Efficient OLAP query processing in distributed data warehouses,Michael O Akinde; Michael H Böhlen; Theodore Johnson; Laks VS Lakshmanan; Divesh Srivastava,Abstract The success of Internet applications has led to an explosive growth in the demandfor bandwidth from ISPs. Managing an IP network requires collecting and analyzing networkdata; such as flowlevel traffc statistics. Such analyses can typically be expressed as OLAPqueries; eg; correlated aggregate queries and data cubes. Current day OLAP tools for thistask assume the availability of the data in a centralized data warehouse. However; theinherently distributed nature of data collection and the huge amount of data extracted ateach collection point make it impractical to gather all data at a centralized site. One solutionis to maintain a distributed data warehouse; consisting of local data warehouses at eachcollection point and a coordinator site; with most of the processing being performed at thelocal sites. In this paper; we consider the problem of efficient evaluation of OLAP queries …,International Conference on Extending Database Technology,2002,37
Visual data mining: An introduction and overview,Simeon J Simoff; Michael H Böhlen; Arturas Mazeika,Abstract In our everyday life we interact with various information media; which present uswith facts and opinions; supported with some evidence; based; usually; on condensedinformation extracted from data. It is common to communicate such condensed informationin a visual form–a static or animated; preferably interactive; visualisation. For example; whenwe watch familiar weather programs on the TV; landscapes with cloud; rain and sun iconsand numbers next to them quickly allow us to build a picture about the predicted weatherpattern in a region. Playing sequences of such visualisations will easily communicate thedynamics of the weather pattern; based on the large amount of data collected by manythousands of climate sensors and monitors scattered across the globe and on weathersatellites. These pictures are fine when one watches the weather on Friday to plan what …,*,2008,36
Layered Temporal DBMS's—Concepts and Techniques,Kristian Torp; Christian S Jensen; Michael Böhlen,Abstract A wide range of database applications manage time-varying data; and it is well-known that querying and correctly updating time-varying data is difficult and error-pronewhen using standard SQL. Temporal extensions of SQL offer substantial benefits over SQLwhen managing time-varying data. The topic of this paper is the effective implementation oftemporally extended SQL's. Traditionally; it has been assumed that a temporal DBMS mustbe built from scratch; utilizing new technologies for storage; indexing; query optimization;concurrency control; and recovery. In contrast; this paper explores the concepts andtechniques involved in implementing a temporally enhanced SQL while maximally reusingthe facilities of an existing SQL implementation. The topics covered span the choice of anadequate timestamp domain that includes the time variable" NOW;" a comparison of …,*,1997,36
TASM: Top-k approximate subtree matching,Nikolaus Augsten; Denilson Barbosa; Michael Böhlen; Themis Palpanas,We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k bestmatches of a small query tree; eg; a DBLP article with 15 nodes; in a large document tree;eg; DBLP with 26M nodes; using the canonical tree edit distance as a similarity measurebetween subtrees. Evaluating the tree edit distance for large XML trees is difficult: the bestknown algorithms have cubic runtime and quadratic space complexity; and; thus; do notscale. Our solution is TASM-postorder; a memory-efficient and scalable TASM algorithm. Weprove an upper-bound for the maximum subtree size for which the tree edit distance needsto be evaluated. The upper bound depends on the query and is independent of thedocument size and structure. A core problem is to efficiently prune subtrees that are abovethis size threshold. We develop an algorithm based on the prefix ring buffer that allows us …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,34
Architectures and implementations of spatio-temporal database management systems,Martin Breunig; Can Türker; Michael H Böhlen; Stefan Dieker; Ralf Hartmut Güting; Christian S Jensen; Lukas Relly; Philippe Rigaux; Hans-Jörg Schek; Michel Scholl,Abstract This chapter is devoted to architectural and implementation aspects ofspatiotemporal database management systems. It starts with a general introduction intoarchitectures and commercial approaches to extending databases by spatiotemporalfeatures. Thereafter; the prototype systems Concert; Secondo; Dedale; Tiger; andGeoToolKit are presented.,*,2003,34
Managing temporal knowledge in deductive databases,Michael Böhlen,Abstract Many applications have to deal with a large amount of data which not onlyrepresent the perceived state of the real world at present; but also past and/or future states.This type of time-varying data arises for example in banking and insurance applications; inhealth care; in planning and scheduling problems; and in ticket reservation systems. Theseapplications are not served adequately by today's database systems. In particular; deletionsand updates in such systems have destructive semantics. This means that previousdatabase contents (repre¬ senting previous perceived states of the real world) cannot beaccessed anymore. Moreover; the formulation of queries which access different databasestates is often tedious and hence error prone without special support for the temporaldimension. Temporal database systems have been developed to provide this kind of …,*,1994,32
Adding transaction time to sql/temporal,Richard T Snodgrass; Michael H Böhlen; Christian S Jensen; Andreas Steiner,Transaction time identifies when data was asserted in the database. If transaction time issupported; the states of the database at all previous points of time are retained and updatesare append-only. Unlike valid time; transaction time cannot be entirely simulated with tableswith explicit timestamp columns. The reason is that tables with transaction-time support areappend-only: they grow monotonically. While the query functionality can be simulated ontable with no temporal support; in the same way that valid-time query functionality can betranslated into queries on table with no temporal support; there is no way to restrict the userto modifications that ensure the table is appendonly. While one can revoke permission touse DELETE; it is still possible for the user to corrupt the transaction timestamp via databaseupdates and insertions. This means that the user can never be sure that what the table …,ISO-ANSI SQL/Temporal Change Proposal; ANSI X3H2-96-152r ISO/IEC JTC1/SC21/WG3 DBL,1996,30
Adding valid time to SQL/Temporal. ANSI X3H2-96-501r2; ISO,Richard T Snodgrass; M Böhlen; C Jensen; N Kline,*,*,1996,30
Adding transaction time to SQL/temporal. Change proposal; ANSI X3H2-96-502r2; ISO,Richard T Snodgrass; Michael H Boehlen; Christian S Jensen; Andreas Steiner,*,*,1996,28
Efficient computation of subqueries in complex OLAP,Michael O Akinde; Michael H Bohlen,Expressing complex OLAP queries involving nested expressions using normal group-by;aggregation; and joins can be extremely difficult. We propose a technique that translatesnested query expressions into an algebra extended with a complex OLAP operator. TheGMDJ is an operator with a simple and easy to optimize implementation that is particularlyuseful for OLAP computations because the size of intermediate results is bound by the sizeof the base-value argument relation. We show that all SQL subqueries can be expressed inthe algebra using GMDJs. This not only makes it easy to integrate subqueries into any queryengine that supports GMDJs; but also gives access to a broad range of OLAP optimizationstrategies for evaluating subqueries. We discuss the coalescing of GMDJs and thecompletion of tuples; two GMDJ optimizations that are particularly relevant to subquery …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,25
Spatio-Temporal Database Management: International Workshop STDBM'99 Edinburgh; Scotland; September 10-11; 1999 Proceedings,Michael H Böhlen; Christian S Jensen,The 1999 Workshop on Spatio-Temporal Database Management; held in Ed-burgh;Scotland; September 10-11; 1999; brought together leading researchersanddevelopersintheareaofspatio-temporaldatabasestodiscussthestate--the-art in spatio-temporal research and applications and to understand the new challenges and futureresearch directions in this emerging and rapidly advancing area. The workshop served as aforum for disseminating research and expe-ence in spatio-temporal databases and formaximizing interchange of knowledge among researchers from the established spatial andtemporal database com-nities. Theexchangeofresearchideasandresultsnot… academicarena; but also bene? ts the user and commercial communities.Theseproceedingscontainthe researchpapersselectedforpresentationatthe workshop …,*,1999,23
Similarity joins in relational database systems,Nikolaus Augsten; Michael H Böhlen,Abstract Download Free Sample State-of-the-art database systems manage and process avariety of complex objects; including strings and trees. For such objects equalitycomparisons are often not meaningful and must be replaced by similarity comparisons. Thisbook describes the concepts and techniques to incorporate similarity into database systems.We start out by discussing the properties of strings and trees; and identify the edit distanceas the de facto standard for comparing complex objects. Since the edit distance iscomputationally expensive; token-based distances have been introduced to speed up editdistance computations. The basic idea is to decompose complex objects into sets of tokensthat can be compared efficiently. Token-based distances are used to compute anapproximation of the edit distance and prune expensive edit distance calculations. A key …,Synthesis Lectures on Data Management,2013,22
How would you like to aggregate your temporal data?,Michael H Bohlen; Johann Gamper; Christian S Jensen,Real-world data management applications generally manage temporal data; ie; theymanage multiple states of time-varying data. Many contributions have been made by theresearch community for how to better model; store; and query temporal data. In particular;several dozen temporal data models and query languages have been proposed. Motivatedin part by the emergence of non-traditional data management applications and theincreasing proliferation of temporal data; this paper puts focus on the aggregation oftemporal data. In particular; it provides a general framework of temporal aggregationconcepts; and it discusses the abilities of five approaches to the design of temporal querylanguages with respect to temporal aggregation. Rather than providing focused; polishedresults; the paper's aim; is to explore the inherent support for temporal aggregation in an …,Temporal Representation and Reasoning; 2006. TIME 2006. Thirteenth International Symposium on,2006,22
Minimizing detail data in data warehouses,Michael O Akinde; Ole Guttorm Jensen; Michael H Böhlen,Abstract Data warehouses collect and maintain large amounts of data from severaldistributed and heterogeneous data sources. Because of security reasons; operationalrequirements; and technical feasibility it is often impossible for data warehouses to accessthe data sources directly. Instead data warehouses have to replicate legacy information asdetail data in order to be able to maintain their summary data. In this paper we investigatehow to minimize the amount of detail data stored in a data warehouse. More specifically; weidentify the minimal amount of data that has to be replicated in order to maintain; eitherincrementally or by recomputation; summary data defined in terms of generalized project-select-join (GPSJ) views. We show how to minimize the number of tuples and attributes inthe current detail tables and even aggregate them where possible. The amount of data to …,International Conference on Extending Database Technology,1998,22
Overlap interval partition join,Anton Dignös; Michael H Böhlen; Johann Gamper,Abstract Each tuple in a valid-time relation includes an interval attribute T that represents thetuple's valid time. The overlap join between two valid-time relations determines all pairs oftuples with overlapping intervals. Although overlap joins are common; existing partitioningand indexing schemes are inefficient if the data includes long-lived tuples or if intervalsintersect partition boundaries. We propose Overlap Interval Partitioning (OIP); a newpartitioning approach for data with an interval. OIP divides the time range of a relation into kbase granules and defines overlapping partitions for sequences of contiguous granules. OIPis the first partitioning method for interval data that gives a constant clustering guarantee: thedifference in duration between the interval of a tuple and the interval of its partition isindependent of the duration of the tuple's interval. We offer a detailed analysis of the …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,21
Valid time integrity constraints,Michael H Böhlen,Abstract This paper investigates temporal integrity constraints in valid time databases; iedatabases that capture the time-varying nature of the part of reality being modeled. We firstprovide a taxonomy of integrity constraints in (temporal) databases in order to establish acommon terminology. The taxonomy identifies two classes of valid time integrity constraints:intrastate and interstate integrity constraints. Intrastate integrity constraints result fromgeneralizing nontemporal integrity constraints. They guarantee the consistency of everysnapshot of a valid time database. Interstate integrity constraints relate and constraindifferent valid time snapshots and; therefore; they are unique to the temporal dimension.ChronoLog; a query language based on first order predicate logic; can express both types ofintegrity constraints. Furthermore; timerestricted integrity constraints may be expressed in …,Univ of Arizona; Dept of Computer Science; TR,1994,21
Generalized MD-joins: Evaluation and reduction to SQL,Michael O Akinde; Michael H Böhlen,Abstract On-line analytical processing (OLAP) has become an increasingly importantconcern for telecommunications operators. Telecommunications systems generate hugeamounts of data that would be beneficial to analyze using OLAP technology. Recently;Chatziantoniou et al.[4] presented the MD-join; a relational operator that provides a cleanseparation between group definitions and aggregate computations; and allows to succinctlyexpress OLAP queries. In this paper; we define generalized MD-joins; describe animplementation of the GMD-join query engine on top of a commercial DBMS; and present areduction of GMD-joins to SQL. We present a practical new optimization of GMD-joinsallowing the restriction of base-values; and discuss the optimization of GMDjoins withrestrictions and coalescing of GMD-joins. We show how GMD-join optimizations can be …,International Workshop on Databases in Telecommunications,2001,20
Parsimonious temporal aggregation,Juozas Gordevičius; Johann Gamper; Michael Böhlen,Abstract Temporal aggregation is an important operation in temporal databases; anddifferent variants thereof have been proposed. In this paper; we introduce a novel temporalaggregation operator; termed parsimonious temporal aggregation (PTA); that overcomesmajor limitations of existing approaches. PTA takes the result of instant temporalaggregation (ITA) of size n; which might be up to twice as large as the argument relation;and merges similar tuples until a given error (ϵ) or size (c) bound is reached. The newoperator is data-adaptive and allows the user to control the trade-off between the result sizeand the error introduced by merging. For the precise evaluation of PTA queries; we proposetwo dynamic programming–based algorithms for size-and error-bounded queries;respectively; with a worst-case complexity that is quadratic in n. We present two …,The VLDB Journal,2012,19
E-government: towards electronic democracy,Michael Böhlen; Johann Gamper; Wolfgang Polasek; M Wimmer,*,International conference; TCGOV,2005,19
On the completeness of temporal database query languages,Michael Böhlen; Robert Marti,Abstract In this paper; we introduce a new definition of completeness for temporal querylanguages based on the relational model. Our definition relies on the following threenotions: relational completeness of non-temporal queries as defined by Codd; thepreservation of temporal irreducibility of temporal (valid time) relations; be they stored orreturned as results of temporal queries; and the notion of temporal equivalence betweentemporal and non-temporal queries. Particularly important is the notion of temporalirreducibility which requires that the valid time intervals of two tuples with the same datavalues must not touch or overlap; since unreduced relations generate incorrect answers tocertain types of temporal queries. Finally; we introduce the query language ChronoLogwhich is a temporally complete extension of Datalog.,*,1994,19
Scalable computation of isochrones with network expiration,Johann Gamper; Michael Böhlen; Markus Innerebner,Abstract An isochrone in a spatial network is the possibly disconnected set of all locationsfrom where a query point is reachable within a given time span and by a given arrival time.In this paper we propose an efficient and scalable evaluation algorithm; termed (MINEX); forthe computation of isochrones in multimodal spatial networks with different transportationmodes. The space complexity of MINEX is independent of the network size and its runtime isdetermined by the incremental loading of the relevant network portions. We show thatMINEX is optimal in the sense that only those network portions are loaded that eventuallywill be part of the isochrone. To keep the memory requirements low; we eagerly expire theisochrone and only keep in memory the minimal set of expanded vertices that is necessaryto avoid cyclic expansions. The concept of expired vertices reduces MINEX's memory …,International Conference on Scientific and Statistical Database Management,2012,18
Sequenced event set pattern matching,Bruno Cadonna; Johann Gamper; Michael H Böhlen,Abstract Event pattern matching is a query technique where a sequence of input events ismatched against a complex pattern that specifies constraints on extent; order; values; andquantification of matching events. The increasing importance of such query techniques isunderpinned by a significant amount of research work; the availability of commercialproducts; and by a recent proposal to extend SQL for event pattern matching. The proposedSQL extension includes an operator PERMUTE; which allows to express patterns that matchany permutation of a set of events. No implementation of this operator is known to theauthors. In this paper; we study the sequenced event set pattern matching problem; which isthe problem of matching a sequence of input events against a complex pattern that specifiesa sequence of sets of events rather than a sequence of single events. Similar to the …,Proceedings of the 14th International Conference on Extending Database Technology,2011,18
Cleansing Databases of Misspelled Proper Nouns.,Arturas Mazeika; Michael H Böhlen,Abstract The paper presents a data cleansing technique for string databases. We proposeand evaluate an algorithm that identifies a group of strings that consists of (multiple)occurrences of a correctly spelled string plus nearby misspelled strings. All strings in a groupare replaced by the most frequent string of this group. Our method targets proper noundatabases; including names and addresses; which are not handled by dictionaries. At thetechnical level we give an efficient solution for computing the center of a group of strings anddetermine the border of the group. We use inverse strings together with sampling toefficiently identify and cleanse a database. The experimental evaluation shows that forproper nouns the center calculation and border detection algorithms are robust and evenvery small sample sizes yield good results.,CleanDB,2006,18
Evaluating and Enhancing the Completeness of TSQL2,Michael H Böhlen; Christian S Jensen; Richard T Snodgrass,Abstract The question of what is a well-designed temporal data model and query languageis a difficult; but also an important one. The consensus temporal query language TSQL2attempts to take advantage of the accumulated knowledge gained from designing andstudying many of the earlier models and languages. In this sense; TSQL2 represents aconstructive answer to this question. Others have provided analytical answers by developingcriteria; formulated as completeness properties; for what is a good model and language.This paper applies important existing completeness notions to TSQL2 in order to evaluatethe design of TSQL2. It is shown that TSQL2 satisfies only a subset of these completenessnotions. In response to this; a minimally modified version of TSQL2; termed Applied TSQL2;is proposed; this new language satisfies the notions of temporal semi-completeness and …,*,1995,16
Defining isochrones in multimodal spatial networks,Johann Gamper; Michael Böhlen; Willi Cometti; Markus Innerebner,Abstract An isochrone in a spatial network is the minimal; possibly disconnected subgraphthat covers all locations from where a query point is reachable within a given time span andby a given arrival time. In this paper we formally define isochrones for multimodal spatialnetworks with different transportation modes that can be discrete or continuous in;respectively; space and time. For the computation of isochrones we propose the multimodalincremental network expansion (MINE) algorithm; which is independent of the actualnetwork size and depends only on the size of the isochrone. An empirical study using real-world data confirms the analytical results.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,15
Efficient top-k approximate subtree matching in small memory,Nikolaus Augsten; Denilson Barbosa; Michael Bohlen; Themis Palpanas,We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k bestmatches of a small query tree within a large document tree using the canonical tree editdistance as a similarity measure between subtrees. Evaluating the tree edit distance forlarge XML trees is difficult: the best known algorithms have cubic runtime and quadraticspace complexity; and; thus; do not scale. Our solution is TASM-postorder; a memory-efficient and scalable TASM algorithm. We prove an upper bound for the maximum subtreesize for which the tree edit distance needs to be evaluated. The upper bound depends onthe query and is independent of the document size and structure. A core problem is toefficiently prune subtrees that are above this size threshold. We develop an algorithm basedon the prefix ring buffer that allows us to prune all subtrees above the threshold in a …,IEEE Transactions on Knowledge and Data Engineering,2011,15
3D visual data mining—goals and experiences,Michael Böhlen; Linas Bukauskas; Poul Svante Eriksen; Steffen Lilholt Lauritzen; Artūras Mažeika; Peter Musaeus; Peer Mylov,Abstract The visual exploration of large databases raises a number of unresolved inferenceproblems and calls for new interaction patterns between multiple disciplines—both at theconceptual and technical level. We present an approach that is based on the interaction offour disciplines: database systems; statistical analyses; perceptual and cognitivepsychology; and scientific visualization. At the conceptual level we offer perceptual andcognitive insights to guide the information visualization process. We then choose clustersurfaces to exemplify the data mining process; to discuss the tasks involved; and to work outthe interaction patterns.,Computational statistics & data analysis,2003,15
Measuring structural similarity of semistructured data based on information-theoretic approaches,Sven Helmer; Nikolaus Augsten; Michael Böhlen,Abstract We propose and experimentally evaluate different approaches for measuring thestructural similarity of semistructured documents based on information-theoretic concepts.Common to all approaches is a two-step procedure: first; we extract and linearize thestructural information from documents; and then; we use similarity measures that are basedon; respectively; Kolmogorov complexity and Shannon entropy to determine the distancebetween the documents. Compared to other approaches; we are able to achieve a linear run-time complexity and demonstrate in an experimental evaluation that the results of ourtechnique in terms of clustering quality are on a par with or even better than those of other;slower approaches.,The VLDB Journal—The International Journal on Very Large Data Bases,2012,13
The temporal deductive database system chronolog,MH Bohlen,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Ph. D. thesis; Department Informatik; ETH Ziirich,1994,13
An incrementally maintainable index for approximate lookups in hierarchical data,Nikolaus Augsten; Michael Böhlen; Johann Gamper,Abstract Several recent papers argue for approximate lookups in hierarchical data andpropose index structures that support approximate searches in large sets of hierarchicaldata. These index structures must be updated if the underlying data changes. Since theperformance of a full index reconstruction is prohibitive; the index must be updatedincrementally. We propose a persistent and incrementally maintainable index forapproximate lookups in hierarchical data. The index is based on small tree patterns; calledpq-grams. It supports efficient updates in response to structure and value changes inhierarchical data and is based on the log of tree edit operations. We prove the correctness ofthe incremental maintenance for sequences of edit operations. Our algorithms identify asmall set of pq-grams that must be updated to maintain the index. The experimental …,Proceedings of the 32nd international conference on Very large data bases,2006,12
Query time scaling of attribute values in interval timestamped databases,Anton Dignös; Michael Böhlen; Johann Gamper,In valid-time databases with interval timestamping each tuple is associated with a timeinterval over which the recorded fact is true in the modeled reality. The adjustment of theseintervals is an essential part of processing interval timestamped data. Some attribute valuesremain valid if the associated interval changes; whereas others have to be scaled along withthe time interval. For example; attributes that record total (cumulative) quantities over time;such as project budgets; total sales or total costs; often must be scaled if the timestamp isadjusted. The goal of this demo is to show how to support the scaling of attribute values inSQL at query time.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,11
Rebom: Recovery of blocks of missing values in time series,Mourad Khayati; Michael H Böhlen,Abstract The recovery of blocks of missing values in regular time series has been addressedby model-based techniques. Such techniques are not suitable to recover blocks of missingvalues in irregular time series and restore peaks and valley. We propose REBOM (REcoveryof BlOcks of Missing values): a new technique that reconstructs shapes; amplitudes andwidth of missing peaks and valleys in irregular time series. REBOM successfully reconstructspeaks and valleys by iteratively considering the time series itself and its correlation tomultiple other time series. We provide an iterative algorithm to recover blocks of missingvalues and analytically investigate its monotonicity and termination. Our experiments withsynthetic and real world hydrological data confirm that for the recovery of blocks of missingvalues in irregular time series REBOM is more accurate than existing methods.,Proceedings of the 18th International Conference on Management of Data,2012,11
Windowed pq-grams for approximate joins of data-centric XML,Nikolaus Augsten; Michael Böhlen; Curtis Dyreson; Johann Gamper,Abstract In data integration applications; a join matches elements that are common to twodata sources. Since elements are represented slightly different in each source; anapproximate join must be used to do the matching. For XML data; most existing approximatejoin strategies are based on some ordered tree matching technique; such as the tree editdistance. In data-centric XML; however; the sibling order is irrelevant; and two elementsshould match even if their subelement order varies. Thus; approximate joins for data-centricXML must leverage unordered tree matching techniques. This is computationally hard sincethe algorithms cannot rely on a predefined sibling order. In this paper; we give a solution forapproximate joins based on unordered tree matching. The core of our solution arewindowed pq-grams which are small subtrees of a specific shape. We develop an …,The VLDB Journal,2012,11
A split operator for now-relative bitemporal databases,Mikkel Agesen; Michael H Bohlen; Lasse O Poulsen; Kristian Torp,The timestamps of now-relative bitemporal databases are modeled as growing; shrinking orrectangular regions. The shape of these regions makes it a challenge to design bitemporaloperators that (a) are consistent with the point-based interpretation of a temporaldatabase;(b) preserve the identity of the argument timestamps;(c) ensure locality and (d)perform efficiently. We identify the bitemporal split operator as the basic primitive toimplement a wide range of advanced now-relative bitemporal operations. The bitemporalsplit operator splits each tuple of a bitemporal argument relation; such that equality andstandard nontemporal algorithms can be used to implement the bitemporal counterparts withthe aforementioned properties. Both a native database algorithm and an SQLimplementation are provided. Our performance results show that the bitemporal split …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,11
Sequenced semantics,Michael H Böhlen; Christian S Jensen,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,10
Temporal aggregation,Johann Gamper; Michael Böhlen; Christian S Jensen,There are two kinds of tables: frequency tables that display the count of respondents at thecrossing of the categorical attributes (in N) and magnitude tables that display information ona numerical attribute at the crossing of the categorical attributes (in R). For example; givensome census microdata containing attributes “Job” and “Town;” one can generate afrequency table displaying the count of respondents doing each job type in each town. If thecensus microdata also contain the “Salary;” attribute; one can generate a magnitude tabledisplaying the average salary for each job type in each town. The number n of cells in atable is normally much less than the number r of respondent records in a microdata file.However; tables must satisfy several linear constraints: marginal row and column totals.Additionally; a set of tables is called linked if they share some of the crossed categorical …,*,2009,10
Multi-dimensional histograms with tight bounds for the error,Linas Baltrunas; Arturas Mazeika; Michael Bohlen,Histograms are being used as non-parametric selectivity estimators for one-dimensionaldata. For high-dimensional data it is common to either compute one-dimensional histogramsfor each attribute or to compute a multi-dimensional equi-width histogram for a set ofattributes. This either yields small low-quality or large high-quality histograms. In this paperwe introduce HIRED (high-dimensional histograms with dimensionality reduction): smallhigh-quality histograms for multi-dimensional data. HIRED histograms are adaptive; andthey are based on the shape error and directional splits. The shape error permits a precisecontrol of the estimation error of the histogram and; together with directional splits; yields amemory complexity that does not depend on the number of uniform attributes in the dataset.We provide extensive experimental results with synthetic and real world datasets. The …,Database Engineering and Applications Symposium; 2006. IDEAS'06. 10th International,2006,10
Reducing the integration of public administration databases to approximate tree matching,Nikolaus Augsten; Michael Böhlen; Johann Gamper,Abstract The integration of data from distributed sources within and among different publicauthorities is an important aspect on the agenda of e-government. In this paper we present aspecific data integration scenario at the Municipality of Bozen-Bolzano; where data fromdifferent sources have to be joined via addresses of citizens. We analyze in detail theproblems which arise in this scenario. We show that addresses can be represented in a treestructure; and hence; the problem of matching addresses can be reduced to approximatetree matching.,International Conference on Electronic Government,2004,10
METAXPath,Curtis E Dyreson; Michael H Böhlen; Christian S Jensen,Abstract This paper presents the METAXPath data model and query language. METAXPathextends XPath with support for XML metadata. XPath is a specification language forlocations in an XML document. It serves as the basis for XML query languages like XSLTand the XML Query Algebra. The METAXPath data model is a nested XPath tree. Each levelof metadata induces a new level of nesting. The data model separates metadata and datainto different dataspaces; supports meta-metadata; and enables sharing of metadatacommon to a group of nodes without duplication. The METAXPath query language has alevel shift operator to shift a query from a data,International Conference on Dublin Core and Metadata Applications,2001,10
Density surfaces for immersive explorative data analyses,Arturas Mazeika; MH Böhlen; Peer Mylov,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389346482 …,Density Surfaces for Immersive Explorative Data Analyses,2001,10
Constructing GPSJ View Graphs.,Michael O Akinde; Michael H Böhlen,Abstract A data warehouse collects and maintains integrated information fromheterogeneous data sources for OLAP and decision support. An important task in datawarehouse design is the selection of views to materialize; in order to minimize the responsetime and maintenance cost of generalized project-select-join (GPSJ) queries. We discusshow to construct GPSJ view graphs. GPSJ view graphs are directed acyclic graphs; used tocompactly encode and represent different possible ways of evaluating a set of GPSJqueries. Our view graph construction algorithm; GPSJVIEWGRAPHBUILDER; incrementallyconstructs GPSJ view graphs based on a set of merge rules. We provide a set of mergingrules to construct GPSJ view graphs in the presence of duplicate sensitive and insensitiveaggregates. The merging algorithm used in GPSJVIEWGRAPHBUILDER ensures that …,DMDW,1999,10
ISOGA: a system for geographical reachability analysis,Markus Innerebner; Michael Böhlen; Johann Gamper,Abstract In this paper; we present a web-based system; termed ISOGA; that uses isochronesto perform geographical reachability analysis. An isochrone in a spatial network covers allspace points from where a query point is reachable within given time constraints. The core ofthe system builds an efficient algorithm for the computation of isochrones in multimodalspatial networks. By joining isochrones with other databases; various kinds of geospatialreachability analysis can be performed; such as how well is a city covered by public servicesor where to look for an apartment at moderate prices that is close to the working place.ISOGA adopts a service-oriented three-tier architecture and uses technologies that arecompliant with OGC standards. We describe several application scenarios in urban andextra-urban areas; which show the applicability of the tool.,International Symposium on Web and Wireless Geographical Information Systems,2013,9
Sequenced spatio-temporal aggregation in road networks,Igor Timko; Michael H Böhlen; Johann Gamper,Abstract Many applications of spatio-temporal databases require support for sequencedspatio-temporal (SST) aggregation; eg; when analyzing traffic density in a city. Conceptually;an SST aggregation produces one aggregate value for each point in time and space. Thispaper is the first to propose a method to efficiently evaluate SST aggregation queries for theCOUNT; SUM; and AVG aggregation functions. Based on a discrete time model and adiscrete; 1.5 dimensional space model that represents a road network; we generalize theconcept of (temporal) constant intervals towards constant rectangles that represent maximalrectangles in the space-time domain over which the aggregation result is constant. Wepropose a new data structure; termed SST-tree; which extends the Balanced Tree for one-dimensional temporal aggregation towards the support for two-dimensional; spatio …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,9
Temporal Databases,Michael Hanspeter Bøhlen; Christian Søndergaard Jensen,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389326375 …,Encyclopedia of Information Systems; Academic Press; Inc,2003,9
The Jungle database search engine,Michael Böhlen; Linas Bukauskas; Curtis Dyreson,Abstract Information spread in in databases cannot be found by current search engines. Adatabase search engine is capable to access and advertise database on the WWW. Jungleis a database search engine prototype developed at Aalborg University. Operating throughJDBC connections to remote databases; Jungle extracts and indexes database data andmeta-data; building a data store of database information. This information is used toevaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitivedatabase query language that helps users to search for information without knowing howthat information is structured. This paper gives an overview of AQUA and describes theimplementation of Jungle.,ACM SIGMOD Record,1999,9
Memory-efficient centroid decomposition for long time series,Mourad Khayati; Michael Bohlen; Johann Gamper,Real world applications that deal with time series data often rely on matrix decompositiontechniques; such as the Singular Value Decomposition (SVD). The Centroid Decomposition(CD) approximates the Singular Value Decomposition; but does not scale to long time seriesbecause of the quadratic space complexity of the sign vector computation. In this paper; wepropose a greedy algorithm; termed Scalable Sign Vector (SSV); to efficiently determine signvectors for CD applications with long time series; ie; where the number of rows(observations) is much larger than the number of columns (time series). The SSV algorithmstarts with a sign vector consisting of only 1s and iteratively changes the sign of the elementthat maximizes the benefit. The space complexity of the SSV algorithm is linear in the lengthof the time series. We provide proofs for the scalability; the termination and the …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,8
A web-enabled extension of a spatio-temporal DBMS,Markus Innerebner; Michael Böhlen; Igor Timko,Abstract Many database applications deal with spatio-temporal phenomena; and during thelast decade a lot of research targeted location-based services; moving objects; traffic jampreventions; meteorology; etc. In strong contrast; there exist only very few proposals for animplementation of a spatio-temporal database system let alone a web-based spatio-temporal information system. This paper describes the design and implementation of a web-based spatio-temporal information system. The system uses Secondo as spatio-temporalDBMS for handling moving objects and MapServer as an OGC-compliant rendering enginefor static spatial data. We describe the architecture of the system and compare our systemwith a standalone application. The paper investigates in detail issues that arise in thecontext of the web. First; we describe an implementation of a lightweight client that takes …,Proceedings of the 15th annual ACM international symposium on Advances in geographic information systems,2007,8
Visual data mining,J Simoff; Monique Noirhomme; Michael H Böhlen,Documents; Authors; Tables. Log in; Sign up; MetaCart; Donate. CiteSeerX logo. Documents:Advanced Search Include Citations. Authors: Advanced Search Include Citations | Disambiguate.Tables: Visual Data Mining (2001). Download From. IEEE Download from IEEE Download Links.[www-staff.it.uts.edu.au]; [www.informatik.uni-freiburg.de] Other Repositories/Bibliography. DBLP.Save to List; Add to Collection; Correct Errors; Monitor Changes. by Edited Simeon ; J. Simoff ;Monique Noirhomme ; Michael H. Böhlen. Summary; Citations; Active Bibliography; Co-citation;Clustered Documents; Version History. BibTeX. @MISC{Simeon01visualdata; author = {EditedSimeon and J. Simoff and Monique Noirhomme and Michael H. Böhlen}; title = {Visual DataMining}; year = {2001} }. Share. Facebook; Twitter; Reddit; Bibsonomy. OpenURL. Abstract.Proceedings. Keyphrases. visual data mining …,*,2001,8
Evolving relations,Ole G Jensen; Michael H Böhlen,Abstract This paper presents a framework for evolving relation schemas that is based onconditional schema changes and tuple versioning. With each tuple a recorded schema anda conceptual schema is associated. This allows for a simple and semantically clean solutionto the problem of schema mismatches that arise when the schema of a database is changedand some data no longer fits the schema. Specifically; no data needs to be migrated to thenew schema; and no special null values are required. We precisely define evolvingschemas in terms of schema segments and corresponding attribute mappings; present analgorithm to compute answers to queries over evolving schemas; and prove that the queryanswers consider the maximal set of schema segments consistent with the evolving schema.,*,2001,8
Systems and Methods for Efficient Top-k Approximate Subtree Matching,*,Systems and method for searching for approximate matches in a database of documentsrepresented by a tree structure. A fast solution to the Top-k Approximate Subtree MatchingProblem involves determining candidate subtrees which will be considered as possiblematches to a query also represented by a tree structure. Once these candidate subtrees arefound; a tree edit distance between each candidate subtree and the query tree is calculated.The results are then sorted to find those with the lowest tree edit distance.,*,2012,7
Versioned relations: support for conditional schema changes and schema versioning,Peter Sune Jørgensen; Michael Böhlen,Abstract We introduce the versioned relational data model; which allows a user to applyconditional schema changes to a populated database without breaking applicationscompiled against an existing schema; and without loss of existing data. Our model is basedon keeping a history of conditional schema changes; and converting tuples on demand to fitthe correct schema in any schema version. We provide a concrete definition of schemaversioning: The ability to specify an operator on any schema version; such that the tuples inthe result are unaffected by schema versions created after the specified schema version.Finally; we show that our model supports schema versioning.,International Conference on Database Systems for Advanced Applications,2007,7
Adaptive density estimation,Arturas Mazeika; Michael H Böhlen; Andrej Taliun,Abstract This demonstration illustrates the APDF tree: an adaptive tree that supports theeffective and effcient computation of continuous density information. The APDF treeallocates more partition points in non-linear areas of the density function and fewer points inlinear areas of the density function. This yields not only a bounded; but a tight control of theerror. The demonstration explains the core steps of the computation of the APDF tree (split;kernel additions; tree optimization; kernel additions; unsplit) and demos the implementationfor different datasets.,Proceedings of the 32nd international conference on Very large data bases,2006,7
Tendax; a collaborative database-based real-time editor system,Stefania Leone; Thomas B Hodel-Widmer; Michael Boehlen; Klaus R Dittrich,Abstract TeNDaX is a collaborative database-based real-time editor system. TeNDaX is anew approach for word-processing in which documents (ie content and structure; tables;images etc.) are stored in a database in a semi-structured way. This supports the provisionof collaborative editing and layout; undo-and redo operations; business process definitionand execution within documents; security; and awareness. During document creationprocess and use meta data is gathered automatically. This meta data can then be used forthe TeNDaX dynamic folders; data lineage; visual-and text mining and search. We presentTeNDaX as a word-processing 'LAN-Party': collaborative editing and layout; businessprocess definition and execution; local and global undo-and redo operations; all based onthe use of multiple editors and different operating systems. In a second step we …,International Conference on Extending Database Technology,2006,7
Multitemporal conditional schema evolution,Ole G Jensen; Michael H Böhlen,Abstract Schema evolution is the ability of the database to respond to changes in the realworld by allowing the schema to evolve. The multidimensional conditionally evolvingschema (MD-CES) is a conceptual model for conditional schema changes; which modify theschema of those tuples that satisfy the change condition. The MD-CES is lossless andpreserves schemas; but has an exponential space complexity. In this paper we restrictconditional schema changes to timestamp attributes. Specifically; we develop 1D-CES forschema versioning over one time dimension; and 2D-CES for schema versioning over twotime dimensions. We show that the space complexity of these new evolution models is linearor polynomial. 1D-CES and 2D-CES are compared to temporal schema versioning; and weshow that; unlike valid time versioning; they are lossless and achieve the same space …,International Conference on Conceptual Modeling,2004,7
An algebraic framework for temporal attribute characteristics,Michael Böhlen; Johann Gamper; Christian S Jensen,Most real-world database applications manage temporal data; ie; data with associated timereferences that capture a temporal aspect of the data; typically either when the data is validor when the data is known. Such applications abound in; eg; the financial; medical; andscientific domains. In contrast to this; current database management systems offer preciouslylittle built-in query language support for temporal data management. This situation persistsalthough an active temporal database research community has demonstrated thatapplication development can be simplified substantially by built-in temporal support. Thispaper's contribution is motivated by the observation that existing temporal data models andquery languages generally make the same rigid assumption about the semantics of theassociation of data and time; namely that if a subset of the time domain is associated with …,Annals of Mathematics and Artificial Intelligence,2006,6
E-Government: Towards Electronic Democracy: International Conference; TCGOV 2005; Bolzano; Italy; March 2-4; 2005; Proceedings,Michael Böhlen; Johann Gamper; Wolfgang Polasek; Maria A Wimmer,The TCGOV 2005 international conference on e-government was held at the Free Universityof Bozen-Bolzano during March 2–4; 2005. The conference was initiated by the workinggroup “Towards Electronic Democracy”(TED) of the European Science Foundation and wasjointly organized by the Free University of Bozen-Bolzano; the Municipality of Bozen-Bolzano; the TED Working Group; and the IFIP Working Group 8.5.,*,2005,6
iTopN: incremental extraction of the N most visible objects,Linas Bukauskas; Leo Mark; Edward Omiecinski; Michael H Böhlen,Abstract The visual exploration of large databases calls for a tight coupling of database andvisualization systems. Current visualization systems typically fetch all the data and organizeit in a scene tree; which is then used to render the visible data. For immersive dataexplorations; where an observer navigates in a potentially huge data space and exploresselected data regions this approach is inadequate. A scalable approach is to make thedatabase system observer-aware and exchange the data that is visible and most relevant tothe observer. In this paper we present iTopN an incremental algorithm for extracting the mostvisible objects relative to the current position of the observer. We implement iTopN andcompare it to an improved version of the R-tree that extends LRU with the caching of the toplevels of the R-tree (LW-LRU). Our experiments show that iTopN is orders of magnitude …,Proceedings of the twelfth international conference on Information and knowledge management,2003,6
Handling temporal knowledge in a deductive database system,Michael Böhlen; Robert Marti,Abstract We describe the design and implementation of a temporal deductive databasesystem which supports (1) the storage of facts annotated with a time interval to indicate theirperceived time of validity in the real world and (2) deduction rules which express temporaldependencies between (stored or derived) facts. The system features a logic-based querylanguage in which even complex temporal queries can be expressed fairly concisely. Weshow how formulas involving temporal conjunction; disjunction; and negation can betranslated into temporal extensions of the operators of relational algebra and into thedatabase language SQL. A vital step in this translation is an implicit temporal normalizationof intermediate results to avoid a redundant representation of temporal information whichcan even lead to incorrect results in certain cases.,*,1993,6
Nearest neighbour join with groups and predicates,Francesco Cafagna; Michael H Böhlen; Annelies Bracher,Abstract This paper proposes the nearest neighbor join; rx T [G; Θ] s; with similarity on T; andintegrated support for grouping attributes G and selection predicates Θ. The correspondingvaluation algorithm; roNNJ; is robust and does not suffer from redundant fetches and indexfalse hits; which are major performance bottlenecks in nearest neighbour joins that do notsupport grouping attributes and selection predicates. Our solution does not computeredundant fetches since it accesses the fact table only once; and uses the groups of theouter relation to limit the fact table to its relevant portions. We experimentally evaluate oursolution using a data warehouse that manages analyses of animal feeds; and the TPC-H.,Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP,2015,5
Efficient event pattern matching with match windows,Bruno Cadonna; Johann Gamper; Michael H Böhlen,Abstract In event pattern matching a sequence of input events is matched against a complexquery pattern that specifies constraints on extent; order; values; and quantification ofmatching events. In this paper we propose a general pattern matching strategy that consistsof a pre-processing step and a pattern matching step. Instead of eagerly matching incomingevents; the pre-processing step buffers events in a match window to apply different pruningtechniques (filtering; partitioning; and testing for necessary match conditions). In the secondstep; an event pattern matching algorithm; A; is called only for match windows that satisfy thenecessary match conditions. This two-phase strategy with a lazy call of the matchingalgorithm significantly reduces the number of events that need to be processed by A as wellas the number of calls to A. This is important since pattern matching algorithms tend to be …,Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,2012,5
Temporal coalescing,Michael Böhlen,There are two kinds of tables: frequency tables that display the count of respondents at thecrossing of the categorical attributes (in N) and magnitude tables that display information ona numerical attribute at the crossing of the categorical attributes (in R). For example; givensome census microdata containing attributes “Job” and “Town;” one can generate afrequency table displaying the count of respondents doing each job type in each town. If thecensus microdata also contain the “Salary;” attribute; one can generate a magnitude tabledisplaying the average salary for each job type in each town. The number n of cells in atable is normally much less than the number r of respondent records in a microdata file.However; tables must satisfy several linear constraints: marginal row and column totals.Additionally; a set of tables is called linked if they share some of the crossed categorical …,*,2009,5
Adding Valid Time to SQL/Temporal: ANSI Expert's Contribution,RT Snodgrass; MH Böhlen; CS Jensen; A Steiner,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389385002 …,*,1996,5
Disjoint interval partitioning,Francesco Cafagna; Michael H Böhlen,Abstract In databases with time interval attributes; query processing techniques that arebased on sort-merge or sort-aggregate deteriorate. This happens because for intervals nototal order exists and either the start or end point is used for the sorting. Doing so leads toinefficient solutions with lots of unproductive comparisons that do not produce an outputtuple. Even if just one tuple with a long interval is present in the data; the number ofunproductive comparisons of sort-merge and sort-aggregate gets quadratic. In this paper wepropose disjoint interval partitioning (DIP DIP); a technique to efficiently perform sort-basedoperators on interval data. DIP DIP divides an input relation into the minimum number ofpartitions; such that all tuples in a partition are non-overlapping. The absence of overlappingtuples guarantees efficient sort-merge computations without backtracking. With DIP DIP …,The VLDB Journal,2017,4
Sequenced spatiotemporal aggregation for coarse query granularities,Igor Timko; Michael Böhlen; Johann Gamper,Abstract Sequenced spatiotemporal aggregation (SSTA) is an important query for manyapplications of spatiotemporal databases; such as traffic analysis. Conceptually; an SSTAquery returns one aggregate value for each individual spatiotemporal granule. While thedata is typically recorded at a fine granularity; at query time a coarser granularity is common.This calls for efficient evaluation strategies that are granularity aware. In this paper; weformally define an SSTA operator that includes a data-to-query granularity conversion.Based on a discrete time model and a discrete 1.5 dimensional space model; we generalizethe concept of time constant intervals to constant rectangles; which represent maximalrectangles in the spatiotemporal domain over which an aggregation result is constant. Wepropose an efficient evaluation algorithm for SSTA queries that takes advantage of a …,The VLDB Journal—The International Journal on Very Large Data Bases,2011,4
Nonsequenced semantics,Michael H Böhlen; Christian S Jensen; Richard T Snodgrass,The simplest way to incorporate unknown values into the relational model; is to allowvariables; in addition to constants; as entries in the columns of relations. Such constructs arecalled tables; instead of relations. A table is an incomplete database; and represents a set ofcomplete databases; each obtained by substituting all variables with constants. Differentoccurrences of the same variable (marked null) are substituted with the same constant. Thesubstitution is thus a function from the variables and constants; to the constants; such thatthe function is identity on the constants. A table T then represents the set of relations;denoted rep (T); defined as {v (T): v is a valuation}. Then the certain answer to a query q on atable T; denoted sure (q; T) is the set of tuples that occur in every answer obtained byapplying the query to every database in rep (T). In other words; the certain answer to q on …,*,2009,4
Analysis and interpretation of visual hierarchical heavy hitters of binary relations,Arturas Mazeika; Michael H Böhlen; Daniel Trivellato,Abstract The emerging field of visual analytics changes the way we model; gather; andanalyze data. Current data analysis approaches suggest to gather as much data as possibleand then focus on goal and process oriented data analysis techniques. Visual analyticschanges this approach and the methodology to interpret the results becomes the key issue.This paper contributes with a method to interpret visual hierarchical heavy hitters (VHHHs).We show how to analyze data on the general level and how to examine specific areas of thedata. We identify five common patterns that build the interpretation alphabet of VHHHs. Wedemonstrate our method on three different real world datasets and show the effectiveness ofour approach.,East European Conference on Advances in Databases and Information Systems,2008,4
Current; legacy; and invalid tuples in conditionally evolving databases,Ole Guttorm Jensen; Michael Böhlen,Abstract After the schema of a relation has evolved some tuples no longer fit the currentschema. The mismatch between the schema a tuple is supposed to have and the schema atuple actually has is inherent to evolving schemas; and is the defining property of legacytuples. Handling this mismatch is at the very core of a DBMS that supports schema evolution.The paper proposes tuple versioning as a structure for evolving databases that permitsconditional schema changes and precisely keeps track of schema mismatches at the level ofindividual tuples. Together with the change history this allows the DBMS to correctly identifycurrent; legacy; and invalid tuples. We give an algorithm that classifies tuples; in time andspace proportional to the length of the change history. We show how tuple versioningsupports a flexible semantics needed to accurately answer queries over evolving …,International Conference on Advances in Information Systems,2002,4
Spatio-Temporal Database Management-International Workshop STDBM'99 Edinburgh; Scotland; September 10-11; 1999; Proceedings,Michael H Bohlen; Christian S Jensen; Michel O Scholl,*,Lecture Notes in Computer Science,1999,4
Layered Implementation of Temporal DBMSs: Concepts and Techniques,Kristian Torp; Christian S Jensen; Michael H Böhlen,*,*,1996,4
Change Proposal for SQL/Temporal: Adding Valid Time-Part A,RT Snodgrass; MH Böhlen; CS Jensen; A Steiner,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389385251 …,*,1995,4
Declarative serializable snapshot isolation,Christian Tilgner; Boris Glavic; Michael Böhlen; Carl-Christian Kanne,Abstract Snapshot isolation (SI) is a popular concurrency control protocol; but it permits non-serializable schedules that violate database integrity. The Serializable Snapshot Isolation(SSI) protocol ensures (view) serializability by preventing pivot structures in SI schedules. Inthis paper; we leverage the SSI approach and develop the Declarative SerializableSnapshot Isolation (DSSI) protocol; an SI protocol that guarantees serializable schedules.Our approach requires no analysis of application programs or changes to the underlyingDBMS. We present an implementation and prove that it ensures serializability.,East European Conference on Advances in Databases and Information Systems,2011,3
Representing public transport schedules as repeating trips,Romans Kasperovics; Michael H Böehlen; Johann Gamper,The movement in public transport networks is organized according to schedules. The real-world schedules are specified by a set of periodic rules and a number of irregularities fromthese rules. The irregularities appear as cancelled trips or additional trips on specialoccasions such as public holidays; strikes; cultural events; etc. Under such conditions; it is achallenging problem to capture real-world schedules in a concise way. This paper presentsa practical approach for modelling real-world public transport schedules. We propose a newdata structure; called repeating trip; that combines route information and the schedule at thestarting station of the route; the schedules at other stations can be inferred. We defineschedules as semi-periodic temporal repetitions; and store them as pairs of rules andexceptions. Both parts are represented in a tree structure; termed multislice; which can …,Temporal Representation and Reasoning; 2008. TIME'08. 15th International Symposium on,2008,3
The 3dvdm approach: A case study with clickstream data,Michael H Böhlen; Linas Bukauskas; Arturas Mazeika; Peer Mylov,Abstract Clickstreams are among the most popular data sources because Web serversautomatically record each action and the Web log entries promise to add up to acomprehensive description of behaviors of users. Clickstreams; however; are large andraise a number of unique challenges with respect to visual data mining. At the technical levelthe huge amount of data requires scalable solutions and limits the presentation to summaryand model data. Equally challenging is the interpretation of the data at the conceptual level.Many analysis tools are able to produce different types of statistical charts. However; thestep from statistical charts to comprehensive information about customer behavior is stilllargely unresolved. We propose a density surface based analysis of 3D data that uses state-of-the-art interaction techniques to explore the data at various granularities.,*,2008,3
Assisting human cognition in visual data mining,Simeon J Simoff; Michael H Böhlen; Arturas Mazeika,Abstract As discussed in Part 1 of the book in chapter “Form-Semantics-Function–AFramework for Designing Visualisation Models for Visual Data Mining” the development ofconsistent visualisation techniques requires systematic approach related to the tasks of thevisual data mining process. Chapter “Visual discovery of network patterns of interactionbetween attributes” presents a methodology based on viewing visual data mining as a“reflection-in-action” process. This chapter follows the same perspective and focuses on thesubjective bias that may appear in visual data mining. The work is motivated by the fact thatvisual; though very attractive; means also subjective; and non-experts are often left to utilisevisualisation methods (as an understandable alternative to the highly complex statisticalapproaches) without the ability to understand their applicability and limitations. The …,*,2008,3
Pppa: Push and pull pedigree analyzer for large and complex pedigree databases,Arturas Mazeika; Janis Petersons; Michael H Böhlen,Abstract In this paper we introduce a novel push and pull technique to analyze pedigreedata. We present the Push and Pull Pedigree Analyzer (PPPA) to organize large andcomplex pedigrees and investigate the development of genetic diseases. PPPA receives asinput a pedigree (ancestry information) of different families. For each person the pedigreecontains information about the occurrence of a specific genetic disease. We propose a newsolution to arrange and visualize the individuals of the pedigree based on the relationshipsbetween individuals and information about the disease. PPPA starts with random positionsof the individuals; and iteratively pushes apart non-relatives with opposite diseases patternsand pulls together relatives with identical disease patterns. The goal is a visualization thatgroups families with homogeneous disease patterns. We investigate our solution …,East European Conference on Advances in Databases and Information Systems,2006,3
Querying multi-granular compact representations,Romāns Kasperovičs; Michael Böhlen,Abstract A common phenomenon of time-qualified data are temporal repetitions; ie; theassociation of multiple time values with the same data. In order to deal with finite and infinitetemporal repetitions in databases we must use compact representations. There have beenmany compact representations proposed; however; not all of them are equally efficient forquery evaluation. In order to show it; we define a class of simple queries on compactrepresentations. We compare a query evaluation time on our proposed multi-granularcompact representation GSequences with a query evaluation time on single-granularcompact representation PSets; based on periodical sets. We show experimentally how theperformance of query evaluation can benefit from the compactness of a representation andfrom a special structure of GSequences.,International Conference on Database Systems for Advanced Applications,2006,3
Parameter estimation using b-trees,Albrecht Schmidt; Michael H Bohlen,This work presents a method for accelerating algorithms for computing common statisticaloperations like parameter estimation or sampling on B-tree indexed data; the work wascarried out in the context of visualisation of large scientific data sets. The underlying idea isthe following: the shape of balanced data structures like B-trees encodes and reflects datasemantics according to the balance criterion. For example; clusters in the index attribute aresomewhat likely to be present not only on the data or leaf level of the tree but shouldpropagate up into the interior levels. The paper also hints at opportunities and limitations ofthis approach for visualisation of large data sets. The advantages of the method aremanifold. Not only does it enable advanced algorithms through a performance boost forbasic operations like density estimation; but it also builds on functionality that is already …,Database Engineering and Applications Symposium; 2004. IDEAS'04. Proceedings. International,2004,3
A Triangular Reconstruction of Density Surfaces,MH Bohlen; Algimantas Juozapavicius; Eilverijus Kondratas; Arturas Mazeika; Aleksej Struk,Abstract. In this paper we introduce a new; fast; simple visualization solution of DensitySurfaces (DSes). We introduce the MSP (m-ary space partitioning) tree to partition the inputdata into slices. The organization of data allows efficient reconstruction of the triangularmesh and reduces the overall complexity of the algorithm to O (N log N). The experimentalevaluation shows the complexity of the algorithm is reasonable and the algorithm isapplicable in real time interactive settings. The solution supports visual data mining. Itimproves the visual quality and eases interpretation of DSes.,Proceedings of the 3rd International Workshop on Visual Data Mining; VDM© ICDM'2003,2003,3
Efficient OLAP Query Processing in Distributed Data Warehouses,Michael Akinde Strategy; Michael Akinde; Michael Böhlen; Laks VS Lakshmanan; Theodore Johnson; Divesh Srivastava,Abstract sets of the detail data. Skalla generates distributed query evaluation plans (for thecoordinator architecture) as a sequence of rounds; where a round consists of:(i) each localsite performing some computation and communicating the result to the coordinator; and (ii)the coordinator synchronizing the results and (possibly) communicating with the sites. Thesemantics of the subqueries generated by Skalla ensure that the amount of data that has tobe shipped between sites is independent of the size of the underlying data at the sites. Thisis particularly important in our distributed OLAP setting. We note that such a bound does notexist for the distributed processing of traditional SQL join queries [3]. The Skalla evaluationscheme allows for a wide variety of optimizations that are easily expressed in the algebraand thus readily integrated into the query optimizer. The optimization schemes included …,In: Proc. of the 8th Intl. Conf. on Extending Database Technology; Prague; Czech Republic,2002,3
Observer relative data extraction,Linas Bukauskas; Michael Böhlen,Abstract Advanced visual data mining systems that support the analysis of large databasescall for a tight coupling of database and visualization systems. For example; for immersivedata explorations; where the observer navigates within the data space; it is interesting tomake the database system aware of the observer position. This makes it possible to optimizethe exchange of data between the database and visualization system. We propose the VR-tree; a database access structure that associates each database object with a visibilityrange. The visibility range denotes the area from where an object can be seen. The VR-treesupports observer relative data extractions; ie; the retrieval of data that an observer canactually see. We discuss perfect; optimistic; and conservative visibility ranges. Optimisticvisibility ranges offer the greatest exibility but the experimental results show that their …,In Proceedings of the International Workshop on Visual Data Mining; in conjunction with ECML/PKDD2001; 2nd European Conference on Machine Learning and 5th European http://www. opensg. org/Conference on Principles and Practice of Knowledge Discovery in Data,2001,3
Adding transaction time to SQL/Temporal: Temporal change proposal. ANSI X3H2-96-152r; ISO-ANSI SQL,R Snodgrass; M Böhlen; C Jensen; A Steiner,*,*,1996,3
Category-and selection-enabled nearest neighbor joins,Francesco Cafagna; Michael H Böhlen; Annelies Bracher,Abstract This paper proposes a category-and selection-enabled nearest neighbor join (NNJ)between relation r and relation s; with similarity on T and support for category attributes Cand selection predicate θ. Our solution does not suffer from redundant fetches and indexfalse hits; which are the main performance bottlenecks of current nearest neighbor jointechniques. A category-enabled NNJ leverages the category attributes C for queryevaluation. For example; the categories of relation r can be used to limit relation s accessedat most once. Solutions that are not category-enabled must process each categoryindependently and end up fetching; either from disk or memory; the blocks of the inputrelations multiple times. A selection-enabled NNJ performs well independent of whether theDBMS optimizer pushes the selection down or evaluates it on the fly. In contrast; index …,Information Systems,2017,2
Extending the kernel of a relational dbms with comprehensive support for sequenced temporal queries,Anton Dignös; Michael H Böhlen; Johann Gamper; Christian S Jensen,Abstract Many databases contain temporal; or time-referenced; data and use intervals tocapture the temporal aspect. While SQL-based database management systems (DBMSs)are capable of supporting the management of interval data; the support they offer can beimproved considerably. A range of proposed temporal data models and query languagesoffer ample evidence to this effect. Natural queries that are very difficult to formulate in SQLare easy to formulate in these temporal query languages. The increased focus on analyticsover historical data where queries are generally more complex exacerbates the difficultiesand thus the potential benefits of a temporal query language. Commercial DBMSs haverecently started to offer limited temporal functionality in a step-by-step manner; focusing onthe representation of intervals and neglecting the implementation of the query evaluation …,ACM Transactions on Database Systems (TODS),2016,2
TemProRA: Top-k temporal-probabilistic results analysis,Katerina Papaioannou; Michael Böhlen,The study of time and probability; as two combined dimensions in database systems; hasfocused on the correct and efficient computation of the probabilities and time intervals.However; there is a lack of analytical information that allows users to understand and tunethe probability of time-varying result tuples. In this demonstration; we present TemProRA; asystem that focuses on the analysis of the top-k temporal probabilistic results of a query. Wepropose the Temporal Probabilistic Lineage Tree (TPLT); the Temporal Probabilistic BubbleChart (TPBC) and the Temporal Probabilistic Column Chart (TPCC): for each output tuplethese three tools are created to provide the user with the most important information tosystematically modify the time-varying probability of result tuples. The effectiveness andusefulness of TemProRA are demonstrated through queries performed on a dataset …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,2
Using lowly correlated time series to recover missing values in time series: a comparison between SVD and CD,Mourad Khayati; Michael H Böhlen; Philippe Cudré Mauroux,Abstract The Singular Value Decomposition (SVD) is a matrix decomposition technique thathas been successfully applied for the recovery of blocks of missing values in time series. Inorder to perform an accurate block recovery; SVD requires the use of highly correlated timeseries. However; using lowly correlated time series that exhibit shape and/or trendsimilarities could increase the recovery accuracy. Thus; the latter time series could also beexploited by including them in the recovery process. In this paper; we compare the accuracyof the Centroid Decomposition (CD) against SVD for the recovery of blocks of missing valuesusing highly and lowly correlated time series. We show that the CD technique better exploitsthe trend and shape similarity to lowly correlated time series and yields a better recoveryaccuracy. We run experiments on real world hydrological and synthetic time series to …,International Symposium on Spatial and Temporal Databases,2015,2
Missing value imputation in time series using Top-k case matching,Kevin Wellenzohn; Hannes Mitterer; Johann Gamper; Michael H Böhlen; Mourad Khayati,In this paper; we present a simple yet effective algorithm; called the Top-k Case Matchingalgorithm; for the imputation of missing values in streams of time series data that are similarto each other. The key idea of the algorithm is to look for the k situations in the historical datathat are most similar to the current situation and to derive the missing value from themeasured values at these k time points. To efficiently identify the top-k most similar historicalsituations; we adopt Fagin's Threshold Algorithm; yielding an algorithm with sub-linearruntime complexity with high probability; and linear complexity in the worst case (excludingthe initial sorting of the data; which is done only once). We provide the results of a firstexperimental evaluation using real-world meteorological data. Our algorithm achieves ahigh accuracy and is more accurate and efficient than two more complex state of the art …,*,2014,2
Smile: enabling easy and fast development of domain-specific scheduling protocols,Christian Tilgner; Boris Glavic; Michael Böhlen; Carl-Christian Kanne,Abstract Modern server systems schedule large amounts of concurrent requests constrainedby; eg; correctness criteria and service-level agreements. Since standard databasemanagement systems provide only limited consistency levels; the state of the art is todevelop schedulers imperatively which is time-consuming and error-prone. In this poster; wepresent Smile (declarative Scheduling MIddLEware); a tool for developing domain-specificscheduling protocols declaratively. Smile decreases the effort to implement and adapt suchprotocols because it abstracts from low level scheduling details allowing developers to focuson the protocol implementation. We demonstrate the advantages of our approach byimplementing a domain-specific use case protocol.,British National Conference on Databases,2011,2
Online Computation of up-to-date Summaries in the Swiss Feed Database,Samuele Zoppi; M Böhlen,Abstract This thesis develops the up-to-date summary; an automatic approach to aggregatehistories of nutrient measurements in the Swiss Feed Database. Since measurements aretaken irregularly and are sparse in the time; a simple aggregation over the entire history isnot representative of the real world state. We fight this challenge by detecting trends inhistory of measurements with a set of data fitting functions: uniform fitting function; linearregression and kernel regression. The experimental evaluation proves the scalability of ourapproach to aggregate the measurements of good and bad quality data. Further; this thesiscontributes to the development of the Feed,Bachelorarbeit am Institut für Informatik; Université de Zurich. RP RMSE Régression selon Kernel,2011,2
Correctness proof of the declarative SS2PL protocol implementation,Christian Tilgner; Boris Glavic; Michael Boehlen; Carl-Christian Kanne,In [1]; we proposed a novel approach for scheduling. The underlying idea of our declarativescheduling approach is to (1) treat sets of requests as data collections and to (2) employdatabase query processing techniques over this request data to produce high-qualityschedules in an efficient and flexible manner. Scheduling protocols are implemented asqueries that select those requests from the request data collections whose execution doesnot violate the scheduling constraints (eg correctness criteria and service-level agreements).We denote such queries implementing scheduling protocols declaratively as declarativeprotocol implementations (DPI). DPIs are much more concise; easier to understand andeasier to modify than an imperative scheduler implementation. Our approach allows for aneasy definition of scheduling protocols of different categories such as traditional …,*,2010,2
Core: nonparametric clustering of large numeric databases,Andrej Taliun; Michael H Böhlen; Arturas Mazeika,Abstract Current clustering techniques are able to identify arbitrarily shaped clusters in thepresence of noise; but depend on carefully chosen model parameters. The choice of modelparameters is difficult: it depends on the data and the clustering technique at hand; andfinding good model parameters often requires time consuming human interaction. In thispaper we propose CORE; a new nonparametric clustering technique that explicitly computesthe local maxima of the density and represents them with cores. CORE proposes anadaptive grid and gradients to define and compute the cores of clusters. The incrementallyconstructed adaptive grid and the gradients make the identification of cores robust; scalable;and independent of small density fluctuations. Our experimental studies show that COREwithout any carefully chosen model parameters produces better quality clustering than …,*,2009,2
SQL-based temporal query languages,Michael Böhlen; Johann Gamper; Christian S Jensen; Richard T Snodgrass,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,2
Temporal compatibility,Michael H Böhlen; Christian S Jensen; Richard T Snodgrass,There are two kinds of tables: frequency tables that display the count of respondents at thecrossing of the categorical attributes (in N) and magnitude tables that display information ona numerical attribute at the crossing of the categorical attributes (in R). For example; givensome census microdata containing attributes “Job” and “Town;” one can generate afrequency table displaying the count of respondents doing each job type in each town. If thecensus microdata also contain the “Salary;” attribute; one can generate a magnitude tabledisplaying the average salary for each job type in each town. The number n of cells in atable is normally much less than the number r of respondent records in a microdata file.However; tables must satisfy several linear constraints: marginal row and column totals.Additionally; a set of tables is called linked if they share some of the crossed categorical …,*,2009,2
Towards General Temporal Aggregation,Michael H Böhlen; Johann Gamper; Christian S Jensen,Abstract Most database applications manage time-referenced; or temporal; data. Temporaldata management is difficult when using conventional database technology; and manycontributions have been made for how to better model; store; and query temporal data.Temporal aggregation illustrates well the problems associated with the management oftemporal data. Indeed; temporal aggregation is complex and among the most difficult; andthus interesting; temporal functionality to support. This paper presents a general frameworkfor temporal aggregation that accommodates existing kinds of aggregation; and it identifiesopen challenges within temporal aggregation.,British National Conference on Databases,2008,2
A greedy approach towards parsimonious temporal aggregation,Juozas Gordevicius; Johann Gamper; Michael Böhlen,Temporal aggregation is a crucial operator in temporal databases and has been studied invarious flavors. In instant temporal aggregation (ITA) the aggregate value at time instant t iscomputed from the tuples that hold at t. ITA considers the distribution of the input data andworks at the smallest time granularity; but the result size depends on the input timestampsand can get twice as large as the input relation. In span temporal aggregation (STA) the userspecifies the timestamps over which the aggregates are computed and thus controls theresult size. In this paper we introduce a new temporal aggregation operator; called greedyparsimonious temporal aggregation (gPTA); which combines features from ITA and STA.The operator extends and approximates ITA by greedily merging adjacent tuples with similaraggregate values until the number of result tuples is sufficiently small; which can be …,Temporal Representation and Reasoning; 2008. TIME'08. 15th International Symposium on,2008,2
Using nested surfaces for visual detection of structures in databases,Arturas Mazeika; Michael H Böhlen; Peer Mylov,Abstract We define; compute; and evaluate nested surfaces for the purpose of visual datamining. Nested surfaces enclose the data at various density levels; and make it possible toequalize the more and less pronounced structures in the data. This facilitates the detectionof multiple structures; which is important for data mining where the less obviousrelationships are often the most interesting ones. The experimental results illustrate thatsurfaces are fairly robust with respect to the number of observations; easy to perceive; andintuitive to interpret. We give a topology-based definition of nested surfaces and establish arelationship to the density of the data. Several algorithms are given that compute surfacegrids and surface contours; respectively.,*,2008,2
Using 2D hierarchical heavy hitters to investigate binary relationships,Daniel Trivellato; Arturas Mazeika; Michael H Böhlen,Abstract This chapter presents VHHH: a visual data mining tool to compute and investigatehierarchical heavy hitters (HHHs) for two-dimensional data. VHHH computes the HHHs for atwo-dimensional categorical dataset and a given threshold; and visualizes the HHHs in thethree dimensional space. The chapter evaluates VHHH on synthetic and real world data;provides an interpretation alphabet; and identifies common visualization patterns of HHHs.,*,2008,2
Lossless conditional schema evolution,Ole G Jensen; Michael H Böhlen,Abstract Conditional schema changes change the schema of the tuples that satisfy thechange condition. When the schema of a relation changes some tuples may no longer fit thecurrent schema. Handling the mismatch between the intended schema of tuples and therecorded schema of tuples is at the core of a DBMS that supports schema evolution. Wepropose to keep track of schema mismatches at the level of individual tuples; and prove thatevolving schemas with conditional schema changes; in contrast to database systems relyingon data migration; are lossless when the schema evolves. The lossless property is aprecondition for a flexible semantics that allows to correctly answer general queries overevolving schemas. The key challenge is to handle attribute mismatches between theintended and recorded schema in a consistent way. We provide a parametric approach to …,International Conference on Conceptual Modeling,2004,2
Toward a Unifying View of Point and Interval Temporal Data Models,Michael H Böhlen,*,null,2004,2
Using nested surfaces to detect structures in databases,Arturas Mazeika; Michael Böhlen; Peer Mylov,Abstract. We define; compute; and evaluate nested surfaces for the purpose of visual datamining. Nested surfaces enclose the data at various density levels; and make it possible toequalize the more and less pronounced structures in the data. This facilitates the detectionof multiple structures; which is important for data mining where the less obviousrelationships are often the most interesting ones. The experimental results illustrate thatsurfaces are fairly robust with respect to the number of observations; easy to perceive; andintuitive to interpret. We give a topology-based definition of nested surfaces and establish arelationship to the density of the data. Several algorithms are given that compute surfacegrids and surface contours; respectively.,Proc. of the Int. Workshop on Visual Data Mining,2001,2
PANEL The State-of-the-Art in Temporal Data Management: Perspectives from the Research and Financial Applications Communities,James Clifford; Alexander Tuzhilin,Abstract In the first part of the panel; several well-known researchers from the temporaldatabase field discussed ongoing efforts to create an infrastructure to support temporal datamanagement. What infrastructure now exists; or should exist soon?,*,1995,2
Scalable centroid decomposition,Mourad Khayati; Michael Böhlen; Johann Gamper,Abstract Real world applications that deal with time series data often rely on matrixdecomposition techniques; such as the Singular Value Decomposition (SVD). The CentroidDecomposition (CD) approximates the Singular Value Decomposition; but does not scale tolong time series because of the quadratic space complexity of the sign vector computation.In this report; we propose a greedy algorithm; termed Scalable Sign Vector (SSV); toefficiently determine sign vectors for CD applications with long time series; ie; where thenumber of rows (observations) is much larger than the number of columns (time series). TheSSV algorithm starts with a sign vector consisting of only 1s and iteratively changes the signof the element that maximizes the benefit. The space complexity of the SSV algorithm islinear in the length of the time series. We provide proofs for the scalability; the termination …,*,2013,1
The address connector: noninvasive synchronization of hierarchical data sources,Nikolaus Augsten; Michael Böhlen; Johann Gamper,Abstract Different databases often store information about the same or related objects in thereal world. To enable collaboration between these databases; data items that refer to thesame object must be identified. Residential addresses are data of particular interest as theyoften provide the only link between related pieces of information in different databases.Unfortunately; residential addresses that describe the same location might vary considerablyand hence need to be synchronized. Non-matching street names and addresses stored atdifferent levels of granularity make address synchronization a challenging task. Commonapproaches assume an authoritative reference set and correct residential addressesaccording to the reference set. Often; however; no reference set is available; and correctingaddresses with different granularity is not possible. We present the address connector …,Knowledge and information systems,2013,1
Graphical analysis of spatio-temporal patterns in forage quality,A Bracher; M Böhlen; F Cafagna; A Taliun,Due to the highly structured topography in Switzerland; crop growth conditions vary withinshort distances. Differences in altitude are one of the major causes for climatic variationresulting in significant spatio-temporal effects on forage quality in terms of nutrient contentand feeding value; particularly in grassland dominated regions. It is one of the goals of theSwiss feed database to support queries that visualize and quantify the temporal and spatialinfluence on feed quality.,Proceedings of the 22nd International Grassland Congress,2013,1
Similarity of chemotherapy histories based on imputed values,Mourad Khayati; Jay Martin Anderson; Michael H Böhlen; Johann Gamper; Manfred Mitterer,The comparison of time series of multivariate data is a long-standing problem in manyapplications in the clinical domain. We propose two approaches to retrieve from a hospitaldata warehouse the k patients P 1;…; Pn with a chemotherapy history that is most similar topatient Q: the first is based on warping distance; together with an initial alignment usingimputed values. The second is based on the volume of the Kiviat tube. In implementing theEuclidean distance; we investigate the addition of null events to achieve similar cardinality;and dynamic time warping; a widely-used technique in the comparison of time series data.The investigations are based on a real world clinical database.,International Journal of Medical Engineering and Informatics,2012,1
Die Schweizerische Futtermitteldatenbank www. feedbase. ch,Monika Boltshauser; Annelies Bracher; Michael Böhlen; Francesco Cafagna; Andrej Taliun,Die Schweizerische Futtermitteldatenbank «www. feedbase. ch» der ForschungsanstaltAgroscope Liebefeld-Posieux ALP-Haras ist seit 2007 im Internet abrufbar und befindet sichzurzeit im Umbruch. Sie wird in den nächsten Jahren fachlich; inhaltlich und technischweiterentwickelt und soll zu einer führenden Informationsquelle auf dem Gebiet derNährstoffe und Nährwerte von Futtermitteln werden.,AgrAr forschung schweiz,2012,1
θ‐Constrained multi-dimensional aggregation,Michael Akinde; Michael H Böhlen; Damianos Chatziantoniou; Johann Gamper,Abstract The SQL: 2003 standard introduced window functions to enhance the analyticalprocessing capabilities of SQL. The key concept of window functions is to sort the inputrelation and to compute the aggregate results during a scan of the sorted relation. For multi-dimensional OLAP queries with aggregation groups defined by a general θ condition anappropriate ordering does not exist; though; and hence expensive join-based solutions arerequired. In this paper we introduce θ‐constrained multi-dimensional aggregation (θ‐MDA);which supports multi-dimensional OLAP queries with aggregation groups defined byinequalities. θ‐MDA is not based on an ordering of the data relation. Instead; the tuples thatshall be considered for computing an aggregate value can be determined by a general θcondition. This facilitates the formulation of complex queries; such as multi-dimensional …,Information Systems,2011,1
Evaluating exceptions on time slices,Romans Kasperovics; Michael H Böhlen; Johann Gamper,Abstract Public transport schedules contain temporal data with many regular patterns thatcan be represented compactly. Exceptions come as modifications of the initial schedule andbreak the regular patterns increasing the size of the representation. A typical strategy topreserve the compactness of schedules is to keep exceptions separately. This; however;complicates the automated processing of schedules and imposes a more complex model onapplications. In this paper we evaluate exceptions by incorporating them into the patternsthat define schedules. We employ sets of time slices; termed multislices; as a representationformalism for schedules and exceptions. The difference of multislices corresponds to theevaluation of exceptions and produces an updated schedule in terms of a multislice. Wepropose a relational model for multislices; provide an algorithm for efficient evaluating the …,International Conference on Conceptual Modeling,2009,1
Safety and Domain Independence.,Rodney W Topor,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,1
Temporal query processing,Michael Böhlen,There are two kinds of tables: frequency tables that display the count of respondents at thecrossing of the categorical attributes (in N) and magnitude tables that display information ona numerical attribute at the crossing of the categorical attributes (in R). For example; givensome census microdata containing attributes “Job” and “Town;” one can generate afrequency table displaying the count of respondents doing each job type in each town. If thecensus microdata also contain the “Salary;” attribute; one can generate a magnitude tabledisplaying the average salary for each job type in each town. The number n of cells in atable is normally much less than the number r of respondent records in a microdata file.However; tables must satisfy several linear constraints: marginal row and column totals.Additionally; a set of tables is called linked if they share some of the crossed categorical …,*,2009,1
Current semantics,Michael H Böhlen; Christian S Jensen; Richard T Snodgrass,Query processing algorithms are designed to efficiently exploit the available cache units inthe memory hierarchy. Cache-conscious algorithms typically employ knowledge ofarchitectural parameters such as cache size and latency. This knowledge can be used toensure that the algorithms have good temporal and/or spatial locality on the target platform.,*,2009,1
Visual Data Mining,David Hutchison; Josef Kittler; Jon M Kleinberg; Friedemann Mattern; John C Mitchell; Moni Naor; Oscar Nierstrasz; C Pandu Rangan; Bernhard Steffen; Madhu Sudan; Demetri Terzopoulos; Doug Tygar; Moshe Y Vardi; Gerhard Weikum; Simeon J Simoff; Michael H Böhlen; Arturas Mazeika,*,*,2008,1
Design and Implementation of a Document Database Extension.,Stefania Leone; Ela Hunt; Thomas B Hodel; Michael H Böhlen; Klaus R Dittrich,Abstract: Integration of text and documents into database management systems has beenthe subject of much research. However; most of the approaches are limited to data retrieval.Collaborative text editing; ie the ability for multiple users to work on a document instancesimultaneously; is rarely supported. Also; documents mostly consist of plain text only; andsupport very limited meta data storage or search. We address the problem by proposing anextended definition of document data type which comprises not only the text itself but alsostructural information such as layout; template and semantics; as well as document creationmeta data. We implemented a new collaborative data type Document which supportsdocument manipulation via a text editing API and extended SQL syntax (TX SQL); asdetailed in this work. We report also on the search capabilities of our document …,ADBIS Research Communications,2006,1
" Guided cognition" and" Validated cognition" approaches in visual data mining,MH Bohlen; SJ Simoff; A Mazeika,Abstract Visual data mining is a promising way of dealing with the complexity of integrateddata sets of various granularity and sizes. It looks at having an access to the entire data setin its most granular level. This paper presents a view of the visual data mining as a"reflection-in-action" technique. We have illustrated two different types of this technique;namely" guided cognition" and" validated cognition". This work is motivated by the fact thatvisual; though very attractive; means also subjective; and non-experts are often left to utilisevisualisation methods (as an understandable alternative to the highly complex statisticalapproaches) without the ability to understand their applicability and limitations.,International Conference on Human-Computer Interaction,2005,1
Approximate Tree Matching with pq-Grams,Nikolaus Augsten; Michael Bohlen; Johann Gamper,Page 1. Approximate Tree Matching with pq-Grams Nikolaus Augsten a ; Michael B ¨ohlen; JohannGamper DIS - Center for Database and Information Systems Free University of Bozen-Bolzano;Italy www.inf.unibz.it 1– Motivation . . . . . 2 2– RelatedWork . . . . . 6 3 – pq-Grams . . . . . 7 4–Properties . . . . . 11 5– Experiments . . . . . 14 6– ConclusionandFutureWork . . . . . 21 a Supportedby the Municipality of Bozen-Bolzano. Page 2. Motivation — Example Data Sources We wantto link data items in different databases that correspond to the same real world object. VLDB 2005;Trondheim Nikolaus Augsten; Michael Böhlen; Johann Gamper Page 2 Page 3. Motivation —Example Data Sources We want to link data items in different databases that correspond to thesame real world object. Land Register Registration Office LR id num entr apt owner …,*,2005,1
Query load balancing for incremental visible object extraction,Linas Bukauskas; Michael H Bohlen,Interactive visual data explorations impose rigid real-time requirements on the extraction ofvisible objects. Often these requirements are met by deploying powerful hardware thatmaintains the entire data set in huge main memory structures. In This work we propose anapproach that retrieves the visible data on demand and is based on a tight integration of thedatabase and visualization systems. We propose to incrementally adjust the observer pathby adding and dropping path points. The result is an optimal path that minimizes theinteraction with the database system and retrieves all visible objects along the path. Thevisible objects are retrieved incrementally; and it is possible to precisely control the queryload and the number of retrieved objects. The minimal distance path method issues frequentqueries and retrieves the lowest possible number of objects at each query point. The end …,Database Engineering and Applications Symposium; 2004. IDEAS'04. Proceedings. International,2004,1
Incremental observer relative data extraction,Linas Bukauskas; Michael H Böhlen,Abstract The visual exploration of large databases calls for a tight coupling of database andvisualization systems. Current visualization systems typically fetch all the data and organizeit in a scene tree that is then used to render the visible data. For immersive data explorationsin a Cave or a Panorama; where an observer is data space this approach is far from optimal.A more scalable approach is to make the observer-aware database system and to restrictthe communication between the database and visualization systems to the relevant data. Inthis paper VR-tree; an extension of the R-tree; is used to index visibility ranges of objects.We introduce a new operator for incremental Observer Relative data Extraction (iORDE). Wepropose the Volatile Access STructure (VAST); a lightweight main memory structure that iscreated on the fly and is maintained during visual data explorations. VAST complements …,British National Conference on Databases,2004,1
Toward a unifying view of point and interval temporal data models,Michael H Bohlen,The association of timestamps with various data items is fundamental to the management oftime-varying information because it has a pervasive effect on most aspects of a data model.Using intervals in timestamps; as do most data models; leaves a data model with a variety ofchoices for giving a meaning to timestamps. Specifically; some data models claim to be point-based while other data models claim to be interval-based. The precise definition of theseproperties is often confused. We argue that these properties can (and in fact should) coexist.Together they provide a powerful tool to define temporal operators that support advancedattribute semantics.,Temporal Representation and Reasoning; 2004. TIME 2004. Proceedings. 11th International Symposium on,2004,1
Spatio-Temporal Database Support for,Michael Böhlen; Christian S Jensen; Bjørn Skjellaug,Abstract In areas such as finance; marketing; and property and resource management; manydatabase applications manage spatio-temporal data. These applications typically run on topof a relational DBMS and manage spatio-temporal data either using the DBMS; whichprovides little support; or employ the services of a proprietary system that co-exists with theDBMS; but is separate from and not integrated with the DBMS. This wealth of applicationsmay benefit substantially from built-in; integrated spatio-temporal DBMS support. Providing afoundation for such support is an important and substantial challenge. This paper initiallydefines technical requirements to a spatio-temporal DBMS aimed at protecting businessinvestments in the existing legacy applications and at reusing personnel expertise. Theserequirements provide a foundation for making it economically feasible to migrate legacy …,History,1998,1
Environmental Science and Technology Department annual report 1996,A Jensen; G Gissel Nielsen; V Gundersen; OJ Nielsen; Hanne Østergård; A Aarkrog,Abstract. Selected activities of the Environmental Science and Technology Departmentduring 1990 are presented. The research in the department is predominantly experimental;and the research topics emphasized are introduced and reviewed in eight chapters: 1.Introduction; 2. The Atmospheric Environment; 3. Plant Genetics and Biology; 4. NutrientEfficiency in Plant Production; 5. Chemistry of the Geosphere; 6. Ecology and MineralCycling; 7. Other Activities; 8. Large Facilities.,RISO-REPORTS-RISO R,1996,1
Change Proposal for SQL,RT Snodgrass; MH Böhlen; CS Jensen,*,*,1995,1
A temporal extension of the deductive database system ProQuel,Michael Böhlen; Robert Marti,Abstract We describe the design and implementation of a temporal deductive databasesystem which supports (1) the storage of facts annotated with a time interval to indicate theirperceived time of validity in the real world and (2) deduction rules which express temporaldependencies between (stored or derived) facts. The system is an extension of thedeductive database system ProQuel and features a logic-based query language in whicheven complex temporal queries can be expressed fairly concisely. We show how formulasinvolving temporal conjunction; disjunction; and negation can be translated into temporalextensions of the operators of relational algebra and into the database language SQL. Avital step in this translation is an implicit temporal normalization of intermediate results toavoid a redundant representation of temporal information which can lead to incorrect …,ETH; Eidgenössische Technische Hochschule Zürich; Departement Informatik; Institut für Informationssysteme,1992,1
Special issue on DOLAP 2015: Evolving data warehousing and OLAP cubes to big data analytics,Carlos Ordonez; Carlos Garcia-Alvarado; Il-Yeol Song; Francesco Cafagna; Michael H Böhlen; Annelies Bracher; Rudra Pratap Deb Nath; Katja Hose; Torben Bach Pedersen; Oscar Romero; Amine Roukh; Ladjel Bellatreche; Selma Bouarar; Ahcene Boukorca,We welcome the reader to the special issue containing best papers from the ACMInternational Workshop on Data Warehousing and OLAP (DOLAP) 2015. We haverevamped DOLAP to be a research venue for big data analytics; expanding its scope; butmaintaining its high quality and applied focus.,*,2017,*
VISOR: Visualizing Summaries of Ordered Data,Giovanni Mahlknecht; Michael H Bohlen; Anton Dignös; Johann Gamper,Abstract In this paper; we present the VISOR tool; which helps the user to explore data andtheir summary structures by visualizing the relationships between the size k of a datasummary and the induced error. Given an ordered dataset; VISOR allows to vary the size kof a data summary and to immediately see the effect on the induced error; by visualizing theerror and its dependency on k in an &epsis;-graph and Δ-graph; respectively. The user caneasily explore different values of k and determine the best value for the summary size.VISOR allows also to compare different summarization methods; such as piecewise constantapproximation; piecewise aggregation approximation or V-optimal histograms. We showseveral demonstration scenarios; including how to determine an appropriate value for thesummary size and comparing different summarization techniques.,Proceedings of the 29th International Conference on Scientific and Statistical Database Management,2017,*
Vertiefung-Report Linear Optimization in Relational Databases,Pascal Zehnder; Georgios Garmpis; Michael Böhlen,The task for this work was to get acquaint with the linear optimization problem and the mostfamous algorithm experienced in this area; called Simplex and then implement thisalgorithm in the context of relational databases as a set of user defined functions inPostgreSQL. For the purpose of in-database analytic methods the open-source libraryMADlib was used. An optimization problem has the goal to maximize or minimize a valuecorresponding to an objective function. The linear optimization problem is a specificoptimization problem characterized by a linear objective function and constraints that areexpressed as a set of linear equalities or inequalities. Moreover; the diet optimizationproblem is a subset of the linear optimization problems. The standard form of this problem isa minimization of the objective function with constraints representing an upper bound. For …,*,2016,*
Special issue on best papers of VLDB 2013,Michael H Böhlen; Christoph Koch,VLDB is a premier annual international forum for data management; and it targets databaseresearchers; users; vendors; and application developers. Each year the conference coverscurrent issues in data management and database systems; which remain key technologicalcornerstones of emerging applications of the twenty-first century. VLDB 2013 took place atthe picturesque town of Riva del Garda in Italy. We received a total of 559 research papersubmissions; and of these 127 submissions were accepted for presentation at the VLDBconference. A committee that included Vanja Josifovski; Mohamed Mokbel; Dan Olteanu;Ken Salem; Divesh Srivastava; and Jens Teubner selected the best papers that have beensubmitted to VLDB 2013. The authors of these papers were invited to submit an extendedversion of their papers to a special issue of the VLDB Journal. The result of this process is …,The VLDB Journal,2015,*
Computing Apps Frequently Installed Together over Very Large Amounts of Mobile Devices,Martin Spielmann; Michael Böhlen,Abstract Knowing Apps frequently installed together is valuable for applications in the field ofapp recommendations and targeted advertising. In this work; a distributed algorithm basedon Apache Spark is developed to find the apps most frequently installed together on a vastamount of mobile devices. The algorithm is optimized by understanding Spark's executionmodel and the characteristics of the input data. An available input dataset of 500'000devices was examined by the distribution of its apps and app pairs. Furthermore; thealgorithm was tested extensively on different cluster configurations and with differently sizedinput data to investigate both scalability and affordability.,*,2015,*
Density Estimation Trees,Michael Böhlen; Peter Widmayer; Arijit Khan,In machine learning density estimation; an instance of unsupervised learning; is afundamental task. Compared with supervised learning; this is a hard task; because it workswithout any ground truth regarding the estimated quantity. In supervised learning; decisiontrees have been widely used [1]. However; in this report I want to focus on decision trees forunsupervised learning. A recent paper [3] introduced Density Estimation Trees; a novelmethod to build Probability Density Functions from training samples in a histogram-likefashion. Density Estimation Methods can be roughly classified into parametric (eg MaximumLikelihood) vs nonparametric (eg Parzen Windows) models. Parametric Models try to fit agiven distribution model to the training samples by tuning its parameter. To evaluate a newobservation; only the fitted distribution function needs to be used. Nonparametric models …,*,2015,*
A Robust Skip-Till-Next-Match Selection Strategy for Event Pattern Matching,Bruno Cadonna; Johann Gamper; Michael H Böhlen,Abstract In event pattern matching; various selection strategies have been proposed toimpose additional constraints on the events that participate in a match. The skip-till-next-match selection strategy is used in scenarios where some incoming events are noise andtherefore should be ignored. Skip-till-next-match is prone to blocking noise; ie; noise thatprevents the detection of matches. In this paper; we propose the robust skip-till-next-matchselection strategy; which is robust against noise and finds matches that are missed by skip-till-next-match when blocking noise occurs in the input stream. To implement the newstrategy in automaton-based pattern matching algorithms; we propose a backtrackingmechanism. Extensive experiments using real-world data and different event patternmatching algorithms show that with skip-till-next-match the number of matches not …,East European Conference on Advances in Databases and Information Systems,2014,*
Efficient Evaluation of Ad-Hoc Range Aggregates,Christian Ammendola; Michael H Böhlen; Johann Gamper,Abstract θ-MDA is a flexible and efficient operator for complex ad-hoc multi-dimensionalaggregation queries. It separates the specification of aggregation groups for whichaggregate values are computed (base table b) and the specification of aggregation tuplesfrom which aggregate values are computed. Aggregation tuples are subsets of the detailtable r and are defined by a general θ-condition. The θ-MDA requires one scan of r; duringwhich the aggregates are incrementally updated. In this paper; we propose a two-stepevaluation strategy for θ-MDA to optimize the computation of ad-hoc range aggregates byreducing them to point aggregates. The first step scans r and computes point aggregates asa partial intermediate result x̃; which can be done efficiently. The second step combines thepoint aggregates to the final aggregates. This transformation significantly reduces the …,International Conference on Data Warehousing and Knowledge Discovery,2013,*
Embedding animal density information into the Swiss Feed Database,Lukas Urech; Michael Böhlen,The goal of my task was to import given animal density data in the Swiss Feed Databaseand allocate them with geographical data. The data should be integrated in form of newtables which are not directly connected to the already existing ones in the Swiss FeedDatabase. In the end several different queries can be made about the new data and densityinformation of the animals bovines; cows and pigs can be assigned to the area of thecommunity they were taken. This opens many possibilities about queriyng the databasegeographically The results were three additional tables to the Swiss Feed Database withwhich querries could be made about animal densities belonging to several Swisscommunities. In the end of my Report there are 5 specific querries about the new data and ashort description; what the main benefit of the additional tables is. One can for example …,*,2013,*
VLDB Endowment,Michael Böhlen; Christoph Koch; Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu; Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; Dan Olteanu; Divesh Srivastava; Jens Teubner; Stefan Manegold; Peer Kröger; Stratis D Viglas,39th International Conference on Very Large Data Bases; Riva del Garda; Trento; Italy … Proceedingsof the 39th International Conference on … Very Large Data Bases; Riva del Garda; Trento; Italy… Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu;Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; DanOlteanu; Divesh Srivastava; Jens Teubner … The 39th International Conference on Very LargeData Bases; Riva del Garda; Trento; Italy … Permission to make digital or hard copies of portionsof this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyright for components of this work owned by others than VLDBEndowment must be honored. Abstracting with credit is permitted. To copy otherwise; to …,*,2013,*
The Swiss Feedstuff Data Bank,Monika Boltshauser; Annelies Bracher; Michael Boehlen; Francesco Cafagna; Andrej Taliun,*,AGRARFORSCHUNG SCHWEIZ,2012,*
La banque de données suisse des aliments pour animaux www. feedbase. ch,Monika Boltshauser; Annelies Bracher; Michael Böhlen; Francesco Cafagna; Andrej Taliun,La banque de données suisse des aliments pour animaux «www. feedbase. ch» de laStation de recherche Agroscope Liebefeld-Posieux ALP-Haras; accessible sur Internetdepuis 2007; est actuellement en plein remaniement. Au cours des prochaines années; soncontenu et sa technologie continueront d'être développés. Objectif: en faire une sourced'informations incontournable dans le domaine des nutriments et des valeurs nutritives desaliments pour animaux.,RecheRche AgRonomique SuiSSe,2012,*
2011 Index IEEE Transactions on Knowledge and Data Engineering Vol. 23,Ahmed Abbasi; Hanady Abdulsalam; Foto N Afrati; Jesus S Aguilar Ruiz; Anastasia Ailamaki; Reda Alhajj; Gustavo Alonso; Mohammed Alshalalfa; Marcelo Arenas; Nikolaus Augsten; Hujun Bao; Zhifeng Bao; Denilson Barbosa; Ilaria Bartolini; Gustavo EAPA Batista; Elisa Bertino; Henry H Bi; Paolo Biondi; M Brian Blake; Michael M Bohlen; Danushka Bollegala; Ljiljana Brankovic; Keith E Brown; Deng Cai; Ruichu Cai; David Camacho; Manuel Cebrian; Surajit Chaudhuri; Muhammad Aamir Cheema; Degang Chen; Gang Chen; Haifeng Chen; Harr Chen; Hsinchun Chen; Jianxia Chen; Ke Chen; Kuang Chen; Lei Chen; Lijiang Chen,This index covers all technical items-papers; correspondence; reviews; etc.-that appeared inthis periodical during the year; and items from previous years that were commented upon orcorrected in this year. Departments and other items may also be covered if they have beenjudged to have archival value. The Author Index contains the primary entry for each item;listed under the first author's name. The primary entry includes the coauthors' names; the titleof the paper or other item; and its location; specified by the publication abbreviation; year;month; and inclusive pagination. The Subject Index contains entries describing the itemunder all appropriate subject headings; plus the first author's name; the publicationabbreviation; month; and year; and inclusive pages. Note that the item title is found onlyunder the primary entry in the Author Index.,IEEE Transactions on Knowledge and Data Engineering,2011,*
Managing and Querying Derived Nutrient Parameters in the Swiss Feed Database,Hannes Tresch; M Böhlen; F Cafagna,Abstract The aim of the thesis has been to integrate the computation of derived nutrients intothe Swiss animal feed database. Derived nutrients are parameters calculated by a formulathat involves other nutrient parameters. The computation of these parameters replacesexpensive chemical lab analyses. In order to get a meaningful temporal distribution ofderived nutrients; an efficient method that supports time-varying regressions must be found.For that; different SQL-queries; SQL-views and PL/pg SQL functions are implemented andpresented in this document. The most efficient solution is then used for the implementation ofan extension to the actual web application; so that all the functionalities that currently existfor non-derived nutrients are to be supported for derived nutrients as well.,*,2011,*
Querying versioned software repositories,Dietrich Christopeit; Michael Böhlen; Carl-Christian Kanne; Arturas Mazeika,Abstract Large parts of today's data is stored in text documents that undergo a series ofchanges during their lifetime. For instance during the development of a software product thesource code changes frequently. Currently; managing such data relies on version controlsystems (VCSs). Extracting information from large documents and their different versions is amanual and tedious process. We present Qvestor; a system that allows to declarativelyquery documents. It leverages information about the structure of a document that is availableas a context-free grammar and allows to declaratively query document versions through agrammar annotated with relational algebra expressions. We define and illustrate theannotation of grammars with relational algebra expressions and show how to translate theannotations to easy to use SQL views.,East European Conference on Advances in Databases and Information Systems,2011,*
On the efficient construction of multislices from recurrences,Romans Kasperovics; Michael H Böhlen; Johann Gamper,Abstract Recurrences are defined as sets of time instants associated with events and theyare present in many application domains; including public transport schedules and personalcalendars. Because of their large size; recurrences are rarely stored explicitly; but some formof compact representation is used. Multislices are a compact representation that is wellsuited for storage in relational databases. A multislice is a set of time slices where each sliceemploys a hierarchy of time granularities to compactly represent multiple recurrences. In thispaper we investigate the construction of multislices from recurrences. We define thecompression ratio of a multislice; show that different construction strategies producemultislices with different compression ratios; and prove that the construction of minimalmultislices; ie; multislices with a maximal compression ratio; is an NP-hard problem. We …,International Conference on Scientific and Statistical Database Management,2010,*
Scalable Network Distance Browsing in Spatial Databases,Markus Cadonau; M Böhlen,As online map services have become popular; it is imperative for providers to deliver resultsto queries as fast as possible. Two common tasks performed on spatial networks; shortestpath and k-nearest neighbor computation are looked at. For a static network; an existing best-first k-nearest neighbor algorithm is presented and discussed. While occupying minimalstorage; the algorithm is trying to speed up finding results by using precomputed shortestpath quadtrees and estimating network distance ranges for possible nearest neighbors.,*,2010,*
Table Normalization,Tabular Data,There are two kinds of tables: frequency tables that display the count of respondents at thecrossing of the categorical attributes (in N) and magnitude tables that display information ona numerical attribute at the crossing of the categorical attributes (in R). For example; givensome census microdata containing attributes “Job” and “Town;” one can generate afrequency table displaying the count of respondents doing each job type in each town. If thecensus microdata also contain the “Salary;” attribute; one can generate a magnitude tabledisplaying the average salary for each job type in each town. The number n of cells in atable is normally much less than the number r of respondent records in a microdata file.However; tables must satisfy several linear constraints: marginal row and column totals.Additionally; a set of tables is called linked if they share some of the crossed categorical …,*,2009,*
Towards General Temporal Aggregation,Michael H Bohlen; Johann Gamper; Christian S Jensen,Abstract. Most database applications manage time-referenced; or temporal; data. Temporaldata management is difficult when using conventional database technology; and manycontributions have been made for how to better model; store; and query temporal data.Temporal aggregation illustrates well the problems associated with the management oftemporal data. Indeed; temporal aggregation is complex and among the most difficult; andthus interesting; temporal functionality to support. This paper presents a general frameworkfor temporal aggregation that accommodates existing kinds of aggregation; and it identifiesopen challenges within temporal aggregation.,Lecture Notes in Computer Science,2008,*
Towards General Temporal Aggregation: 25th British National Conference on Databases; BNCOD 25; Cardiff; UK; July 7-10; 2008. Proceedings,Michael H Boehlen; Johann Gamper; Christian Søndergaard Jensen,Abstract: Most database applications manage time-referenced; or temporal; data. Temporaldata management is difficult when using conventional database technology; and manycontributions have been made for how to better model; store; and query temporal data.Temporal aggregation illustrates well the problems associated with the management oftemporal data. Indeed; temporal aggregation is complex and among the most difficult; andthus interesting; temporal functionality to support. This paper presents a general frameworkfor temporal aggregation that accommodates existing kinds of aggregation; and it identifiesopen challenges within temporal aggregation.,British National Conference on Databases,2008,*
Versioned Relations: Support for Conditional Schema Changes and Schema Versioning,Peter Sune Jorgensen; Michael Bohlen,*,Lecture Notes in Computer Science,2007,*
The APDF Module,Arturas Mazeika; Andrej Taliun; Michael Böhlen,This manual describes the design and implementation of the 3DVDM Adaptive ProbabilityDensity Function (APDF) module; which is part of the 3DVDM system. Density information iscrucial statistical information that can be used for a number of purposes. This manualillustrates how the APDF method with the help of density surface (DS) support visual datamining. The APDF module offers tools to compute and display density surfaces for 3D data.,*,2005,*
TED Conference on e-Government (TCGOV-2005): electronic democracy: the challenge ahead; international conference jointly organized by TED Working Group of...,M Boehlen; J Gemper; W Polasek; M Wimmer,Building Simulation applications (BSA) 2015 was the second IBPSA-Italy conference onbuilding performance simulation to take place at the Free University of Bolzano; fromFebruary 4th to 6th 2015. The main topics dealt...,*,2005,*
E-Government: Towards Electronic Democracy-International Conference; TCGOV 2005. Bolzano; Italy; March 2-4; 2005. Proceedings,Michael Bohlen; Johann Gamper; Wolfgang Polasek; Maria A Wimmer,*,Lecture Notes in Computer Science,2005,*
Core Database Technology Program Committee,Anastassia Ailamaki; Gustavo Alonso; Walid Aref; Lars Arge; Brian Babcock; Mikael Berndtsson; Elisa Bertino; Claudio Bettini; Michael Boehlen; Anthony Bonner; Philippe Bonnet; Alex Buchmann; Tiziana Catarci; Surajit Chaudhuri; Peter Dadam; Amol Deshpande; Asuman Dogac; Christos Faloutsos; Elena Ferrari; Johann-Christoph Freytag; Dieter Gawlick; Johannes Gehrke; Torsten Grust; Ralf Hartmut Güting; Jayant Haritsa; Chris Jermaine; Christoph Koch; George Kollios; Mong Li Lee; Wolfgang Lindner; David Lomet; Hongjun Lu; Samuel Madden; Giansalvatore Mecca; Alberto Mendelzon; Rosa Meo; Tova Milo; Michele Missikoff; C Mohan; Mario Nascimento; Shojiro Nishio; Ed Omiecinski; Norman Paton; Torben Bach Pedersen; Calton Pu; Philippe Pucheral; Raghu Ramakrishnan; Thomas Rölleke; Ken Ross; Gunther Saake; Albrecht Schmidt; Marc Scholl; Bernhard Seeger,Committee Chair: Martin Kersten; CWI; The Netherlands … Serge Abiteboul; INRIA; France AnastassiaAilamaki; Carnegie Mellon University; USA Gustavo Alonso; ETH Zurich; Switzerland WalidAref; Purdue University; USA Lars Arge; Aarhus University; Denmark Brian Babcock; StanfordUniversity; USA Mikael Berndtsson; University of Skövde; Sweden Elisa Bertino; PurdueUniversity; USA Claudio Bettini; University of Milan; Italy Michael Boehlen; Free University ofBolzano/Bozen; Italy Peter Boncz; CWI; The Netherlands Anthony Bonner; University ofToronto; Canada Philippe Bonnet; University of Copenhagen; Denmark Alex Buchmann; Universityof Darmstadt; Germany Tiziana Catarci; University of Rome 'La Sapienza'; Italy SurajitChaudhuri; Microsoft; USA Vassilis Christophides; FORTH; Greece Peter Dadam; Universityof Ulm; Germany Amol Deshpande; University of California; Berkeley; USA Asuman …,VLDB 2005: 31st International Conference on Very Large Data Bases: Proceedings of the 31st International Conference on Very Large Data Bases; Trondheim; Norway; August 30-September 2; 2005,2005,*
Incremental Visualizer for Visible Objects,Linas Bukauskas; Michael Hanspeter Bøhlen,Abstract: This paper discusses the integration of database back-end and visualizer front-endinto a one tightly coupled system. The main aim which we achieve is to reduce the datapipeline from database to visualization by using incremental data extraction of visibleobjects in a fly-through scenarios. We also argue that passing only relevant data from thedatabase will substantially reduce the overall load of the visualization system. We proposethe system Incremental Visualizer for Visible Objects (IVVO) which considers visible objectsand enables incremental visualization along the observer movement path. IVVO is the novelsolution which allows data to be visualized and loaded on the fly from the database andwhich regards visibilities of objects. We run a set of experiments to convince that IVVO isfeasible in terms of I/O operations and CPU load. We consider the example of data which …,*,2004,*
Third International Workshop on Evolution and Change in Data Management (ECDM 2004)-Schema Evolution and Versioning in Data Management-Multitemporal C...,Ole G Jensen; Michael H Bohlen,*,Lecture Notes in Computer Science,2004,*
Parameter Estimation for Interactive Visualisation of Scientific Data,Albrecht Schmidt; Michael H Böhlen,Abstract This paper presents a method for accelerating algorithms for computing commonstatistical operations like parameter estimation or sampling on B-Tree indexed data in thecontext of visualisation of large scientific data sets. The method builds heavily on technologythat is already part of relation database management systems and requires only smallextensions. The technical goal is the following: Given a massive set of scientific data likesensor data stored in a Relational Database Management System; enable interactiveexploration and visualisation of the data by exploiting the technology that already is in placein the database back-end. The main underlying idea is the following: the shape of balanceddata structures like B-Trees encodes and reflects data semantics according to the balancecriterion. For example; clusters in the index attribute are somewhat likely to be present not …,History,2004,*
Estimating Selectivity of Approximate String Queries Using Signatures,Arturas Mazeika; N Srivastava; M Koudas; M Böhlen; Peer Mylov,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389345896 …,IEEE ICDE'05; The 21st International Conference on Data Engineering,2004,*
Efficient OLAP Query Processing,Michael O Akinde; Michael H Böhlen; Theodore Johnson; Laks VS Lakshmanan; Divesh Srivastava,Abstract. The success of Internet applications has led to an explo-sive growth in the demandfor bandwidth from ISPs. Managing an IP network requires collecting and analyzing networkdata; such as flowlevel traffic statistics. Such analyses can typically be expressed as OLAPqueries; eg; correlated aggregate queries and data cubes. Current day OLAP tools for thistask assume the availability of the data in a centralized data warehouse. However; theinherently distributed nature of data collection and the huge amount of data extracted ateach collection point make it impractical to gather all data at a centralized site. One solutionis to maintain a distributed data warehouse; consisting of local data warehouses at eachcollection point and a coordinator site; with most of the processing being performed at thelocal sites. In this paper; we consider the problem of efficient evaluation of OLAP queries …,Advances in Database Technology-EDBT 2002: 8th International Conference on Extending Database Technology; Prague; Czech Republic; March 25-27; Proceedings,2003,*
Evaluation and Reduction to SQL,Michael O Akinde; Michael H Böhlen,Abstract. On-line analytical processing (OLAP) has become an increasingly importantconcern for telecommunications operators. Telecommunications systems generate hugeamounts of data that would be beneficial to analyze using OLAP technology. Recently;Chatziantoniou et al.[4] presented the MD-join; a relational operator that provides a cleanseparation between group definitions and aggregate computations; and allows to succinctlyexpress OLAP queries. In this paper; we define generalized MD-joins; describe animplementation of the GMD-join query engine on top of a commercial DBMS; and present areduction of GMD-joins to SQL. We present a practical new optimization of GMD-joinsallowing the restriction of base-values; and discuss the optimization of GMD-joins withrestrictions and coalescing of GMD-joins. We show how GMD-join optimizations can be …,Databases in Telecommunications II: VLDB 2001 International Workshop; DBTel 2001 Rome; Italy; September 10; 2001 Proceedings,2003,*
Introduction to special issue with best papers from EDBT 2002,Christian S Jensen,*,*,2003,*
Query Language Concepts,Michael H Bohlen; Christian S Jensen,GLOSSARY bitemporal relation A relation with exactly one systemsupported valid time andexactly one systemsupported transaction time. calendar Provides a human interpretation oftime. As such; calendars ascribe meaning to temporal values where the particular meaningor interpretation is relevant to the user. In particular; calendars determine the mappingbetween human-meaningful time values and an underlying time-line. chronon; time instant;time point A (one-dimensional) chronon is a nondecomposable time interval of some fixed;minimal duration. An n-dimensional chronon is a nondecomposable region in» dimensionaltime. Important special types of chronons include validtime; transaction-time; and bitemporalchronons. This article uses time instant and time point as synonyms to chronon; and; inkeeping with the literature on which the article is based; we generally prefer the latter term …,*,2003,*
The Density Surfaces Module,Arturas Mazeika; Michael Böhlen; Peer Mylov,This manual describes the design and implementation of the 3DVDM density surface (DS)module; which is part of the 3DVDM System. Density information is crucial statisticalinformation that can be used for a number of purposes. This manual illustrates how densitysurfaces support visual data mining. The DS module offers a broad range of tools tocompute and display density surfaces for 3D data. The purpose of this manual is to describethe DS module and related software components such that you a) can install the requiredcomponents and run the DS module; b) get to know the functionality of the DS module andlearn how to use it; c) have the essential information to extend or modify the DS module; andd) have an example how density surfaces can be used for the purpose of visual data mining.Towards this end the manual is structured into three parts.,*,2002,*
Visual Data Mining: Second International Workshop,Simeon J Simoff; Monique Noirhomme-Fraiture; Michael H Böhlen,*,*,2002,*
A Shape Preserving Space Partitioning. Submitted to the 28th VLDB Conference,A Mazeika; M Böhlen; Peer Mylov,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389345869 …,A Shape Preserving Space Partitioning. Submitted to the 28th VLDB Conference,2002,*
School of EE and CS Department of Computer Science Washington State University; USA Aalborg University; Denmark cdyreson@ eecs. wsu. edu boehlen@ cs. au...,Curtis E Dyreson; Michael H Böhlen; Christian S Jensen,Abstract This paper presents the METAXPath data model and query language. METAXPathextends XPath with support for XML metadata. XPath is a specification language forlocations in an XML document. It serves as the basis for XML query languages like XSLTand the XML Query Algebra. The METAXPath data model is a nested XPath tree. Each levelof metadata induces a new level of nesting. The data model separates metadata and datainto different dataspaces; supports meta-metadata; and enables sharing of metadatacommon to a group of nodes without duplication. The METAXPath query language has alevel shift operator to shift a query from a data level to a metadata level. METAXPathmaximally reuses XPath hence the changes needed to support metadata are few.METAXPath is fully compatible with XPath.,Journal of Digital Information,2001,*
Data Managenent Technologies in Telcos,MH Böhlen; W Jonker; M Revett,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389384602 …,*,2000,*
Spatio-temporal Database Management: International Workshop STDBM'99; Edinburgh; Scotland; September 10-11; 1999: Proceedings,STDBM'99,*,*,1999,*
Copyright c 1998RichardT. Snodgrass. Allrightsreserved.,Richard T Snodgrass; Christian S Jensen; Michael H Bohlen; Renato Busatto; Curtis E Dyreson; Heidi Gregersen; Dieter Pfoser; Kristian Torp; Sudha Ram; Anindya Datta; Kwang W Nam; Mario A Nascimento; Keun H Ryu; Michael D Soo; Andreas Steiner; Vassilis Tsotras; Jef Wijsen,*,*,1998,*
served.,Christian S Jensen; Richard T Snodgrass; Michael H Bohlen; Renato Busatto; Heidi Gregersen; Kristian Torp; Anindya Datta; Sudha Ram; Curtis E Dyreson; Kwang W Nam; Keun H Ryu; Michael D Soo; Andreas Steiner; Vassilis Tsotras; Jef Wijsen,*,*,1997,*
VrijeUniversiteitBrussel. Allrightsreserved.,Jef Wijsen; Robert Meersman; Christian S Jensen; HB Michael; Heidi Gregersen; Kristian Torp; Richard T Snodgrass; Anindya Datta; Sudha Ram; Curtis E Dyreson; Kwang W Nam; Keun H Ryu; Michael D Soo; Andreas Steiner; Vassilis Tsotras,*,*,1997,*
1997 Michael H. B¤ ohlen and Richard T. Snodgrass and MichaelD. Soo. Allrightsreserved.,HB Michael; Christian S Jensen; Heidi Gregersen; Kristian Torp,*,*,1997,*
andAndreasSteiner. Allrightsreserved.,Richard T Snodgrass; Michael H Bohlen; Christian S Jensen; Andreas Steiner; Renato Busatto; Heidi Gregersen; Kristian Torp; Anindya Datta; Curtis E Dyreson; Kwang W Nam; Keun H Ryu; Michael D Soo; Vassilis Tsotras; Jef Wijsen,*,*,1997,*
Layered Implementation of Temporal DBMSs| Concepts and Techniques,Michael H Bohlen; Renato Busatto; Heidi Gregersen; Christian S Jensen; Kristian Torp; Anindya Datta; Hong Lin; Richard T Snodgrass; Curtis E Dyreson; Michael D Soo; Andreas Steiner,*,*,1997,*
Change Proposal to SQL/Temporal: Adding Valid Time to SQL/Temporal,RT Snodgrass; M Böhlen; CS Jensen; A Steiner,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389346993 …,Change Proposal to SQL/Temporal,1996,*
Change Proposal to SQL/Temporal: Adding Transaction Time to SQL/Temporal,RT Snodgrass; M Böhlen; CS Jensen; A Steiner,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389346992 …,Change Proposal to SQL/Temporal,1996,*
Database Systems,CS Jensen; L Bækgaard; MH Böhlen,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389368086 …,*,1996,*
1 ARCADIA-GCH-OSQL,Michael H Bohlen,*,SIGMOD Record,1995,*
Evaluating and Enhancing the Completeness of TSQL2,HB Michael; Christian S Jensen; Richard T Snodgrass,Abstract The question of what is a well-designed temporal data model and query languageis a di cult; but also an important one. The consensus temporal query language TSQL2attempts to take advantage of the accumulated knowledge gained from designing andstudying many of the earlier models and languages. In this sense; TSQL2 represents aconstructive answer to this question. Others have provided analytical answers by developingcriteria; formulated as completeness properties; for what is a good model and language.This paper applies important existing completeness notions to TSQL2 in order to evaluatethe design of TSQL2. It is shown that TSQL2 satis es only a subset of these completenessnotions. In response to this; a minimally modi ed version of TSQL2; termed Applied TSQL2;is proposed; this new language satis es the notions of temporal semi-completeness and …,*,1995,*
The state-of-the-art in temporal data management: Perspectives from the research and financial applications communities,J Clifford; C Jensen; RT Snodgrass; MH Bohlen,Abstract for the book: This volume contains papers presented at the International Workshopon Temporal Databases; held in Zurich; Switzerland; from 17-18 September 1995. Thepapers cover a wide range of topics from the highly theoretical through to reports on howtemporal data bases can be used to solve real problems. In addition to the technical papers;there are also summaries of two panel discussions which assess the recently-completedTSQL2 Language Design; and examine the need for additional research into thedevelopment of TSQL3. Together these papers provide a comprehensive overview of thelatest research work into the area of temporal databases. They will provide invaluablereading for researchers; postgraduate students and practitioners.,*,1995,*
Valid Time Integrity Constraints,HB Michael,Abstract This paper investigates temporal integrity constraints in valid time databases; iedatabases that capture the time-varying nature of the part of reality being modeled. We rstprovide a taxonomy of integrity constraints in (temporal) databases in order to establish acommon terminology. The taxonomy identi es two classes of valid time integrity constraints:intrastate and interstate integrity constraints. Intrastate integrity constraints result fromgeneralizing nontemporal integrity constraints. They guarantee the consistency of everysnapshot of a valid time database. Interstate integrity constraints relate and constrain dierent valid time snapshots and; therefore; they are unique to the temporal dimension.ChronoLog; a query language based on rst order predicate logic; can express both types ofintegrity constraints. Furthermore; timerestricted integrity constraints may be expressed in …,*,1994,*
Feedbase. ch: a Data Warehouse Based System for Assessing the Quality of Animal Feed,Francesco Cafagna; Michael H Böhlen; Annelies Bracher,SUMMARY In Switzerland we have developed Feedbase. ch; a system using data from theSwiss Feed Data Warehouse. Opposite from the other Feed Databases; our system providesto the user accurate temporal (eg; measurements are stored with up a daily granularity);spatial (eg; we store the coordinates of the field where the feed has been grown andsampled) and biological dimensions (eg; the stage of maturity of the feed) to increase thelevel of detail during the data analysis; since the nutritive content of the animal feeds isdependent on those dimensions. In this paper we compare different query plans forimplementing the core functionalities of our system. For example; for driving the user throughmeaningful selections; ie; selections for which data exist in the fact table; we use data-drivenmenus. An option in a menu is shown to the user only if; for that option; data exists in the …,*,*,*
Online Statistical Computation in the Feed database,Zafer Adıgüzel; M Böhlen,In this project we deal with missing data values to be calculated in a database by anappropriate choice of a statistical regression and the evaluation of a database queryperformance. The database contains values of measurements of animal feed that have beenmade by specific data. The number of measurements is the main factor in the evaluation offood quality. But since chemical analysis is long and costly; we usually tend to make asufficient number of measurements only for a small number of components. To perform agood analysis of the mixture of feed; it is essential to look at all individual components; evenwhen they are not measured. The regression method; currently just calculated outside thedatabase and for a limited time period; helps us to fill these missing data in the fooddatabase. We have to consider that correlated components can change with time …,*,*,*
Querying TSQL2 Databases with Temporal Logic,HB Michael; Jan Chomicki; Richard T Snodgrass; David Toman,Abstract. We establish an exact correspondence between temporal logic and a subset ofTSQL2; a consensus temporal extension of SQL {92. The translation from temporal logic toTSQL2 developed here enables a user to write high-level queries which can be evaluatedagainst a spacee cient representation of the database. The reverse translation; alsoprovided; makes it possible to characterize the expressive power of TSQL2. We demonstratethat temporal logic is equal in expressive power to a syntactically de ned subset of TSQL2.,*,*,*
Selecting a Comprehensive Set of Reviews,SEMINAR BY PETER WIDMAYER; ARIJIT KHAN; MICHAEL BÖHLEN,ABSTRACT: Consumer reviews describing a personal evaluation of a product play a centralrole in the decision-making process of potential future consumers. Over the years; popularInternet-based retailers and service providers have accumulated vast databases sometimescontaining hundreds of user-generated reviews for a single product. In order to be of anyuse for users interested in a specific product; mechanisms to retrieve small; relevant andcomprehensive subsets of this abundance of reviews are required. Popular approachesdisplaying reviews based on rankings like average user-ratings fall short to account forredundancy in top-rated reviews or that the top reviews all may have the same view-point onthe product.,*,*,*
Information-Theoretic Approaches for Measuring the Structural Similarity of Semistructured Documents,Sven Helmer; Nikolaus Augsten; Michael Böhlen,Abstract We propose and experimentally evaluate different approaches for measuring thestructural similarity of semistructured documents based on informationtheoretic concepts.Common to all approaches is a twostep procedure: first we extract and linearize thestructural information from documents and then we use similarity measures that are basedon; respectively; Kolmogorov complexity and Shannon entropy to determine the distancebetween the documents. Compared to other approaches; we are able to achieve a linear runtime complexity and demonstrate in an experimental evaluation that the results of ourtechnique in terms of clustering quality are on a par with or even better than those of other;slower approaches.,*,*,*
Database Technology for Long Data,Michael Böhlen,▶ Sure; big data is a powerful lens for looking at our world. Despite its limitations andrequirements; crunching big numbers can help us learn a lot about ourselves.▶ What weneed to focus on is” long data”–information with a” massive historical sweep” that holds”tremendous potential for learning about ourselves”.,*,*,*
Conditional Statistic Projection in Relational Databases,Michael H Böhlen,*,*,*,*
ICDE ‘98 Program Committee Members,Nabil Adam; Gustavo Alonso; BR Badrinath; Sujata Banerjee; Lubomir F Bit; Alexandros Biliris; Patrick Bobbie; Michael H Boehlen; Arbee LP Chen; Ming-Syan Chen; Boris Chidlosvkii; Munir Cochinwala; Robert Demolombe; Suzanne Dietrich; Klaus Dittrich; Asuman Dogac; Maggie Dunham; Curtis Dyreson; Ophir Frieder; Narain Gehani; Shahram Ghandeharizadeh; Joachim Hammer; Jiawei Han; Ralf Hartmut Gueting; Waqar Hasan; Sandra Heiler; HV Jagadish; Yahiko Kambayashi; Vijay Kumar; Alon Levy,*,*,*,*
Implementation of Temporal Functions Benchmark in PostgreSQL,Simon Jakob; Dr Michael Böhlen,Information is the currency of the modern society and thus storing and accessing it effectivelybecomes increasingly important. To reach this goal; procedural languages were introducedto make more complex computations possible than those possible with declarative SQL.Now; a great number of the data we produce and want to handle has a time dimensionwhich should be included in the database. To implement procedural functions including thetemporal domain is harder than in nontemporal; databases and is therefore a current field ofstudy. An efficient algorithm that converts nontemporal functions; that are easier toimplement into temporal is thus of big value.,*,*,*
Implementation of an Aggregating Nearest Neighbors Join Technique in the Swiss Feed Database,M Böhlen,The computation of derived nutrients values in the Swiss Feed Database is an important andchallenging task. The automatized computation of derived nutrients has to ensure the samequality as a manual computation. Since nutrients measurements are not taken regularly; anearest neighbor (s) search technique has to be used to compute the derived values for agiven timestamp; successively; in case more than one nearest neighbor exists; all thenearest neighbors have to be aggregated in order to ensure the best quality in the result.Derived nutrients values are calculated newly for each query and cannot be stored in thedatabase since new measurements are entered every day and since the user queries aresubject to temporal and geographical restriction changes. The computation of derivednutrients requires that the formulas; that are used to infer the derived nutrients values; are …,*,*,*
Research Program Committee,Yanif Ahmad; Aris Anagnostopoulos; Walid Aref; Ismail Ari; Shivnath Babu; Zohra Bellahsene; II Elisa Bertino; Claudio Bettini; Michael Bohlen; Paolo Boldi; Francesco Bonchi; Peter Boncz; CWI Angela Bonifati; Vinayak Borkar; Christof Bornhoevd; SAP Randal Burns; Andrea Cali; Selcuk Candan; Barbara Carminati; Deepayan Chakrabarti; Chee Yong Chan; Shimin Chen; Pittsburgh Su Chen; Yi Chen; Reynold Cheng; Sarah Cohen-Boulakia; LRI Orsay; Gao Cong; Mariano Consens; Isabel Cruz; Bin Cui; Colazzo Dario; Gautam Das; Anish Das Sarma; Khuzaima Daudjee; Antonios Deligiannakis; Stefan Dessloch; Anhai Doan; Eduard Dragut,Yanif Ahmad; Johns Hopkins University Aris Anagnostopoulos; Sapienza University of RomeWalid Aref; Purdue University Ismail Ari; Ozyegin University Soeren Auer; Leipzig School of MediaShivnath Babu; Duke University Roger Barga; Microsoft Zohra Bellahsene; University of MontpellierII Elisa Bertino; Purdue University Claudio Bettini; University of Milan Michael Bohlen; Universityof Zurich Paolo Boldi; University of Milan Francesco Bonchi; Yahoo! Research Peter Boncz; CWIAngela Bonifati; ICAR-CNR; Italy Vinayak Borkar; University of California; Irvine ChristofBornhoevd; SAP Randal Burns; Johns Hopkins University Andrea Cali; University of Oxford SelcukCandan; Arizona State University Barbara Carminati; University of Insubria; Italy DeepayanChakrabarti; Yahoo! Research Chee Yong Chan; National University of Singapore BadrishChandramouli; Microsoft Gang Chen; Zhejing University; China Shimin Chen; Intel Labs …,*,*,*
Statistical Comparison of Regions in the Swiss Feed Database,Martin Leimer; M Böhlen,Abstract In this bachelor thesis we develop an approach how to find similar regions and howto calculate the probability to which degree two regions are equal. This calculation is basedon nutrient measurements on feed samples in the Swiss Feed Database. We encounter thischallenge using different statistical tests; which we finally implement on the Swiss FeedDatabase. Moreover; we will focus on an optimized implementation of the developedalgorithm. The experimental evaluation reveals that the similarity probabilities of the top-ksimilar regions algorithm can be computed in reasonable time. Further; it will be shown thatthere indeed can be very similar regions.,*,*,*
ICDE ‘98 General Chair’s Message,S Yu Philip,*,*,*,*
STDBM'99: spatio-temporal database management(Edinburgh; 10-11 September 1999),Michael H Böhlen; Christian S Jensen; Michel O Scholl,*,Lecture notes in computer science,*,*
A TimeCenter Technical Report,Kristian Torp; Christian S Jensen; Michael Bohlen,*,*,*,*
Peer reviewed paper,Curtis E Dyreson; Michael H Bohlen; Christian S Jensen,*,Journal of Digital Information,*,*
Density Surfaces for Immersive Explorative Data Analyses,Art uras Mazeika; Michael Bohlen; Peer Mylov,*,*,*,*
Abiteboul; S. 41 Aggarwal; CC 261;593 Agrawal; D 93; 274;496;639 Agrawal; S. 5,M Akinde; S AI-Khalifa; G Alonso; M Areal; WG Aref; V Atluri; I Atmosukarto; D Baker; R Barga; K Barker; B Benatallah; G Bhalotia; HE Blok; M Bohlen; A Bonifati; D Braga; S Bressan; N Bruno; F Buccafurri; A Campi; F Casati; AC Catlin; S Ceri; S Chakrabarti; NH Chan; S Chaudhuri; B Chen; CM Chen; J Chen; MS Chen; F Chiu; J Cho; HD Chon; L Cohen; B Cooper; R Cordova; G Cormode; G Das; S Davey; U Daya; S Decker; A Descour; A Deshpande; J Desmarais; DJ DeWitt; A Doan; M Dumas; J Dunn; MG Elfeky; CJ El1mann; AK Elmagarmid; R Elmasri; C Fa1outsos; J Fan; A Faradjian; P Felber; J Feng; S Flesca; I Foudos; J Freire; AW Fu; F Furfaro; A Gal; H Garcia-Molina; M Garofalakis; J Gehrke; D Georgakopoulos; M Gertz; A Goel,333 369 264 331 129 567; 685 ; 673 ; 490 ; 266 ; 271 263 29 393 490 212;276 716 498 492271 498 431 176 490 605 29 141 278;463 265; 663 335 488 262 ; 494 ; 266 ; 166 129 309 309333 494 275 673 309 141;567;605 309 262 453 706 329 … 267 269 176 583 269 617 268; 268 543 269;327 155;331;335 155 697 555 507 279 333 267 583 41;369 490 271 29 117 331685 333 212;498 685 685 ; 245 605 333 270 431 529 345;697 271 ;498 273 583 272 297 ;485685 274 275 333 265;663 327 … Ounopulos; D PUO; J. ..; Gtirel; A. Haas; L. ãas; p .J.Haclgtimti; H. I … ¥alevy; A. ãmmad; M. ¥aritsa; JR ¥ellerstein; J .M … Lee; D. Lee; MLLehner; W Leung; CK-S Ling; T. W ' … Ling; Y. Liu; B Liu; J Lomet; D. Low; WL Lu; H. ; Lu; JXLuo; G. Madden; S. Madhyastha; T. Maier; D. Major; G. Mani; M. Mannila; H Marian; A.,*,*,*
Local Organisation Chairs,Estela Saquete; Rafael Muñoz; Patricio M Barco; Alessandro Artale; Claudio Bettini; Michael Böhlen; Alexander Bolotov; UK Westminster; Carlo Combi; Stéphane Demri; Michael Fisher; Keijo Heljanko; Boris Konev; Orna Kupferman; François Laroussinie; Carsten Lutz; Roque Marín; Angelo Montanari; Bernhard Nebel; Paritosh Pandya; Ian Pratt-Hartmann; Pierre Wolper; Jef Wijsen; James Worrell,Program Chairs Valentin Goranko; Johannesburg; S. Africa X. Sean Wang; Vermont; USA …Local Organisation Chairs Estela Saquete; Alicante; Spain Rafael Muñoz; Alicante; Spain PatricioM. Barco; Alicante; Spain … Program Committee Alessandro Artale; Bozen-Bolzano; Italy ClaudioBettini; Milan; Italy Michael Böhlen; Bolzano-Bozen; Italy Alexander Bolotov; Westminster; UKCindy Chen; Lowell; MA; USA Carlo Combi; Verona; Italy Stéphane Demri; Cachan; France MichaelFisher; Liverpool; UK Keijo Heljanko; Helsinki; Finland Ian Hodkinson; London; UK BorisKonev; Liverpool; UK Orna Kupferman; Jerusalem; Israel François Laroussinie; Cachan; FranceLixin Li; Statesboro; GA; USA Carsten Lutz; Dresden; Germany Roque Marín; Murcia; Spain AngeloMontanari; Udine; Italy Malek Mouhoub; Regina; SK; Canada Bernhard Nebel; Freiburg; GermanyParitosh Pandya; Mumbai; India Ian Pratt-Hartmann; Manchester; UK James Pustejovsky …,*,*,*
DATA WAREHOUSE VERSIONING,Mantas Šimkus; Evaldas Taroza; Michael Böhlen; Artūras Mažeika,Data Warehouses are usually built following “star” or “snowflake” schema. In the center thereis a fact table; and dimension tables are gathered around. In such architecture the content ofdimension tables is considered to be static; while new insertions are made only into the facttable. The motivation for such attitude is the following: changes in the dimensions causeuncertainty when answering multidimensional queries; meaning that different interpretationsto changes can be made assumed; as shown below.,*,*,*
Lossless Database Evolution,Michael Böhlen; Andrea Antonello,*,*,*,*
Guidelines for Announcing Databases to Database Search Engines,Linas Bukauskas; Michael Böhlen,Abstract A database search engine is a search engine that advertises information stored inremote databases on the WWW. During the database announcing phase; the robot of adatabase search engine grabs and indexes information stored in these remote databases.The characteristics of the announcing phase are important parameters for the design of adatabase search engine. They determine the hit rate of subsequent searches; provide therationale for balancing storage space and query answering time; and constrain thepossibilities for re-indexing remote databases. This paper analyzes databaseannouncement in detail. We empirically determine the time that is required to announce adatabase; measure the disk space usage; evaluate the relationship between false hits andvalue-level index size; and discuss how schema enrichment increases the hit rate of …,*,*,*
Preliminary Program for VDM@ PKDD2001 Workshop,Artüras Mažeika; Michael Böhlen; Peer Mylov; Henrik R Nagel; Erik Granum; Peter Musaeus; Tokihiko Niwa; Kenji Fujikawa; Kazuyoshi Tanaka; Mayumi Oyama; Colin Ho; Ben Azvine; Jinsan Yang; Byoung-Tak Zhang; Maria Francesca Costabile; Francesca A Lisi; Penny Noy; Michael Schroeder; Monique Noirhomme-Fraiture; Simeon J Simoff,*,*,*,*
Competence Center for Medical Data Warehousing and Analysis,M Böhlen; J Gamper; P Huber; M Mitterer; A Mazeika; A Moskwita; F Ricci,ABSTRACT The Competence Center for Medical Data Warehousing and Analysis (MEDAN)will be established as a four-year competence center in the Database and InformationSystems (DIS) group in the Faculty of Computer Science at the Free University of Bozen-Bolzano (FUB). The creation of this competence center has been triggered by a strategicproject at Hospital Meran (HM) with the aim of realizing a data warehouse that forms thebackbone of a data analysis platform that supports the development of value added servicesfor the health district South Tyrol. The center's mission is to conduct basic and appliedresearch in the area of data warehousing and intelligent data analysis for health care andmedical applications. The competence center is closely collaborating with internationalresearch institutions to ensure high quality research. It works with real-world data and …,*,*,*
