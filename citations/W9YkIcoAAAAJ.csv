Clustering binary data streams with K-means,Carlos Ordonez,Abstract Clustering data streams is an interesting Data Mining problem. This article presentsthree variants of the K-means algorithm to cluster binary data streams. The variants includeOn-line K-means; Scalable K-means; and Incremental K-means; a proposed variantintroduced that finds higher quality solutions in less time. Higher quality of solutions areobtained with a mean-based initialization and incremental learning. The speedup isachieved through a simplified set of sufficient statistics and operations with sparse matrices.A summary table of clusters is maintained on-line. The K-means variants are compared withrespect to quality of results and speed. The proposed algorithms can be used to monitortransactions.,Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery,2003,235
Discovering association rules based on image content,Carlos Ordonez; Edward Omiecinski,Our focus for data mining in the paper is concerned with knowledge discovery in imagedatabases. We present a data mining algorithm to find association rules in 2-dimensionalcolor images. The algorithm has four major steps: feature extraction; object identification;auxiliary image creation and object mining. Our emphasis is on data mining of imagecontent without the use of auxiliary domain knowledge. The purpose of our experiments is toexplore the feasibility of this approach. A synthetic image set containing geometric shapeswas generated to test our initial algorithm implementation. Our experimental results showthat there is promise in image mining based on content. We compare these results againstthe rules obtained from manually identifying the shapes. We analyze the reasons fordiscrepancies. We also suggest directions for future work.,Research and Technology Advances in Digital Libraries; 1999. Proceedings. IEEE Forum on,1999,203
Integrating K-means clustering with a relational DBMS using SQL,Carlos Ordonez,Integrating data mining algorithms with a relational DBMS is an important problem fordatabase programmers. We introduce three SQL implementations of the popular K-meansclustering algorithm to integrate it with a relational DBMS: 1) a straightforward translation ofK-means computations into SQL; 2) an optimized version based on improved dataorganization; efficient indexing; sufficient statistics; and rewritten queries; and 3) anincremental version that uses the optimized version as a building block with fastconvergence and automated reseeding. We experimentally show the proposed K-meansimplementations work correctly and can cluster large data sets. We identify which K-meanscomputations are more critical for performance. The optimized and incremental K-meansimplementations exhibit linear scalability. We compare K-means implementations in SQL …,IEEE transactions on Knowledge and Data engineering,2006,148
Association rule discovery with the train and test approach for heart disease prediction,Carlos Ordonez,Association rules represent a promising technique to improve heart disease prediction.Unfortunately; when association rules are applied on a medical data set; they produce anextremely large number of rules. Most of such rules are medically irrelevant and the timerequired to find them can be impractical. A more important issue is that; in general;association rules are mined on the entire data set without validation on an independentsample. To solve these limitations; we introduce an algorithm that uses search constraints toreduce the number of rules; searches for association rules on a training set; and finallyvalidates them on an independent test set. The medical significance of discovered rules isevaluated with support; confidence; and lift. Association rules are applied on a real data setcontaining medical records of patients with heart disease. In medical terms; association …,IEEE Transactions on Information Technology in Biomedicine,2006,131
Constraining and summarizing association rules in medical data,Carlos Ordonez; Norberto Ezquerra; Cesar A Santana,Abstract Association rules are a data mining technique used to discover frequent patterns ina data set. In this work; association rules are used in the medical domain; where data setsare generally high dimensional and small. The chief disadvantage about mining associationrules in a high dimensional data set is the huge number of patterns that are discovered; mostof which are irrelevant or redundant. Several constraints are proposed for filtering purposes;since our aim is to discover only significant association rules and accelerate the searchprocess. A greedy algorithm is introduced to compute rule covers in order to summarizerules having the same consequent. The significance of association rules is evaluated usingthree metrics: support; confidence and lift. Experiments focus on discovering associationrules on a real data set to predict absence or existence of heart disease. Constraints are …,Knowledge and information systems,2006,123
Mining constrained association rules to predict heart disease,Carlos Ordonez; Edward Omiecinski; Levien De Braal; Cesar A Santana; Norberto Ezquerra; Jose A Taboada; David Cooke; Elizabeth Krawczynska; Ernest V Garcia,This work describes our experiences in discovering association rules in medical data topredict heart disease. We focus on two aspects of this work: mapping medical data to atransaction format suitable for mining association rules; and identifying useful constraints.Based on these aspects we introduce an improved algorithm to discover constrainedassociation rules. We present an experimental section explaining several interestingdiscovered rules.,Data Mining; 2001. ICDM 2001; Proceedings IEEE International Conference on,2001,115
Efficient disk-based K-means clustering for relational databases,Carlos Ordonez; Edward Omiecinski,K-means is one of the most popular clustering algorithms. We introduce an efficient disk-based implementation of K-means. The proposed algorithm is designed to work inside arelational database management system. It can cluster large data sets having very highdimensionality. In general; it only requires three scans over the data set. It is optimized toperform heavy disk I/O and its memory requirements are low. Its parameters are easy to set.An extensive experimental section evaluates quality of results and performance. Theproposed algorithm is compared against the Standard K-means algorithm as well as theScalable K-means algorithm.,IEEE Transactions on Knowledge and Data Engineering,2004,104
Comparing association rules and decision trees for disease prediction,Carlos Ordonez,Abstract Association rules represent a promising technique to find hidden patterns in amedical data set. The main issue about mining association rules in a medical data set is thelarge number of rules that are discovered; most of which are irrelevant. Such number of rulesmakes search slow and interpretation by the domain expert difficult. In this work; searchconstraints are introduced to find only medically significant association rules and makesearch more efficient. In medical terms; association rules relate heart perfusionmeasurements and patient risk factors to the degree of stenosis in four specific arteries.Association rule medical significance is evaluated with the usual support and confidencemetrics; but also lift. Association rules are compared to predictive rules mined with decisiontrees; a well-known machine learning technique. Decision trees are shown to be not as …,Proceedings of the international workshop on Healthcare information and knowledge management,2006,100
Statistical model computation with UDFs,Carlos Ordonez,Statistical models are generally computed outside a DBMS due to their mathematicalcomplexity. We introduce techniques to efficiently compute fundamental statistical modelsinside a DBMS exploiting User-Defined Functions (UDFs). Specifically; we study thecomputation of linear regression; PCA; clustering; and Naive Bayes. Two summary matriceson the data set are mathematically shown to be essential for all models: the linear sum ofpoints and the quadratic sum of cross products of points. We consider two layouts for theinput data set: horizontal and vertical. We first introduce efficient SQL queries to computesummary matrices and score the data set. Based on the SQL framework; we introduce UDFsthat work in a single table scan: aggregate UDFs to compute summary matrices for allmodels and a set of primitive scalar UDFs to score data sets. Experiments compare UDFs …,IEEE Transactions on Knowledge and Data Engineering,2010,97
FREM: fast and robust EM clustering for large data sets,Carlos Ordonez; Edward Omiecinski,Abstract Clustering is a fundamental Data Mining technique. This article presents animproved EM algorithm to cluster large data sets having high dimensionality; noise and zerovariance problems. The algorithm incorporates improvements to increase the quality ofsolutions and speed. In general the algorithm can find a good clustering solution in 3 scansover the data set. Alternatively; it can be run until it converges. The algorithm has a fewparameters that are easy to set and have defaults for most cases. The proposed algorithm iscompared against the standard EM algorithm and the On-Line EM algorithm.,Proceedings of the eleventh international conference on Information and knowledge management,2002,95
Discovering Interesting Association Rules in Medical Data.,Carlos Ordonez; Cesar A Santana; Levien De Braal,ABSTRACT We are presently exploring the idea of discovering association rules in medicaldata. There are several technical aspects which make this problem challenging. In our casemedical data sets are small; but have high dimensionality. Information content is rich: thereexist numerical; categorical; time and even image attributes. Data records are generallynoisy. We explain how to map medical data to a transaction format suitable for mining rules.The combinatorial nature of association rules matches our needs; but current algorithms areunsuitable for our purpose. We thereby introduce an improved algorithm to discoverassociation rules in medical data which incorporates several important constraints. Someinteresting results obtained by our program are discussed and we explain how the programparameters were set. We believe many of the problems we come across are likely to …,ACM SIGMOD workshop on research issues in data mining and knowledge discovery,2000,92
SQLEM: Fast clustering in SQL using the EM algorithm,Carlos Ordonez; Paul Cereghini,Abstract Clustering is one of the most important tasks performed in Data Mining applications.This paper presents an efficient SQL implementation of the EM algorithm to performclustering in very large databases. Our version can effectively handle high dimensional data;a high number of clusters and more importantly; a very large number of data records. Wepresent three strategies to implement EM in SQL: horizontal; vertical and a hybrid one. Weexpect this work to be useful for data mining programmers and users who want to clusterlarge data sets inside a relational DBMS.,ACM SIGMOD Record,2000,83
Horizontal aggregations in SQL to prepare data sets for data mining analysis,Carlos Ordonez; Zhibo Chen,Preparing a data set for analysis is generally the most time consuming task in a data miningproject; requiring many complex SQL queries; joining tables; and aggregating columns.Existing SQL aggregations have limitations to prepare data sets because they return onecolumn per aggregated group. In general; a significant manual effort is required to build datasets; where a horizontal layout is required. We propose simple; yet powerful; methods togenerate SQL code to return aggregated columns in a horizontal tabular layout; returning aset of numbers instead of one number per row. This new class of functions is calledhorizontal aggregations. Horizontal aggregations build data sets with a horizontaldenormalized layout (eg; point-dimension; observation-variable; instance-feature); which isthe standard layout required by most data mining algorithms. We propose three …,IEEE transactions on knowledge and data engineering,2012,64
Bayesian classifiers programmed in SQL,Carlos Ordonez; Sasi K Pitchaimalai,The Bayesian classifier is a fundamental classification technique. In this work; we focus onprogramming Bayesian classifiers in SQL. We introduce two classifiers: naive Bayes and aclassifier based on class decomposition using K-means clustering. We consider twocomplementary tasks: model computation and scoring a data set. We study several layoutsfor tables and several indexing alternatives. We analyze how to transform equations intoefficient SQL queries and introduce several query optimizations. We conduct experimentswith real and synthetic data sets to evaluate classification accuracy; query optimizations; andscalability. Our Bayesian classifier is more accurate than naive Bayes and decision trees.Distance computation is significantly accelerated with horizontal layout for tables;denormalization; and pivoting. We also compare naive Bayes implementations in SQL …,IEEE Transactions on Knowledge and Data Engineering,2010,64
Improving heart disease prediction using constrained association rules,Carlos Ordonez,*,Seminar Presentation at University of Tokyo,2004,54
Vertical and horizontal percentage aggregations,Carlos Ordonez,Abstract Existing SQL aggregate functions present important limitations to computepercentages. This article proposes two SQL aggregate functions to compute percentagesaddressing such limitations. The first function returns one row for each percentage in verticalform like standard SQL aggregations. The second function returns each set of percentagesadding 100% on the same row in horizontal form. These novel aggregate functions are usedas a framework to introduce the concept of percentage queries and to generate efficient SQLcode. Experiments study different percentage query optimization strategies and compareevaluation time of percentage queries taking advantage of our proposed aggregationsagainst queries using available OLAP extensions. The proposed percentage aggregationsare easy to use; have wide applicability and can be efficiently evaluated.,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,53
Data set preprocessing and transformation in a database system,Carlos Ordonez,Abstract In general; there is a significant amount of data mining analysis performed outside adatabase system; which creates many data management issues. This article presents asummary of our experience and recommendations to compute data set preprocessing andtransformation inside a database system (ie data cleaning; record selection; summarization;denormalization; variable creation; coding); which is the most time-consuming task in datamining projects. This aspect is largely ignored in the literature. We present practical issues;common solutions and lessons learned when preparing and transforming data sets with theSQL language; based on experience from real-life projects. We then provide specificguidelines to translate programs written in a traditional programming language into SQLstatements. Based on successful real-life projects; we present time performance …,Intelligent Data Analysis,2011,47
Horizontal aggregations for building tabular data sets,Carlos Ordonez,Abstract In a data mining project; a significant portion of time is devoted to building a data setsuitable for analysis. In a relational database environment; building such data set usuallyrequires joining tables and aggregating columns with SQL queries. Existing SQLaggregations are limited since they return a single number per aggregated group; producingone row for each computed number. These aggregations help; but a significant effort is stillrequired to build data sets suitable for data mining purposes; where a tabular format isgenerally required. This work proposes very simple; yet powerful; extensions to SQLaggregate functions to produce aggregations in tabular form; returning a set of numbersinstead of one number per row. We call this new class of functions horizontal aggregations.Horizontal aggregations help building answer sets in tabular form (eg point-dimension …,Proceedings of the 9th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery,2004,47
Optimization of linear recursive queries in SQL,Carlos Ordonez,Recursion is a fundamental computation mechanism which has been incorporated into theSQL language. This work focuses on the optimization of linear recursive queries in SQL.Query optimization is studied with two important graph problems: computing the transitiveclosure of a graph and getting the power matrix of its adjacency matrix. We present SQLimplementations for two fundamental algorithms: seminaive and direct. Five queryoptimizations are studied: 1) storage and indexing; 2) early selection; 3) early evaluation ofnonrecursive joins; 4) pushing duplicate elimination; and 5) pushing aggregation.Experiments compare both evaluation algorithms and systematically evaluate the impact ofoptimizations with large input tables. Optimizations are evaluated on four types of graphs:binary trees; lists; cyclic graphs; and complete graphs; going from the best to worst case …,IEEE Transactions on Knowledge and Data Engineering,2010,46
Referential integrity quality metrics,Carlos Ordonez; Javier García-García,Abstract Referential integrity is an essential global constraint in a relational database; thatmaintains it in a complete and consistent state. In this work; we assume the database mayviolate referential integrity and relations may be denormalized. We propose a set of qualitymetrics; defined at four granularity levels: database; relation; attribute and value; thatmeasure referential completeness and consistency. Quality metrics are efficiently computedwith standard SQL queries; that incorporate two query optimizations: left outer joins onforeign keys and early foreign key grouping. Experiments evaluate our proposed metricsand SQL query optimizations on real and synthetic databases; showing they can help indetecting and explaining referential errors.,Decision Support Systems,2008,44
Image mining: A new approach for data mining,Carlos Ordonez; Edward Robert Omiecinski,We introduce a new focus for data mining; which is concerned with knowledge discovery inimage databases. We expect all aspects of data mining to be relevant to image mining but inthis first work we concentrate on the problem of finding associations. To that end; we presenta data mining algorithm to find association rules in 2-dimensional color images. Thealgorithm has four major steps: feature extraction; object identification; auxiliary imagecreation and object mining. Our algorithm is general in that it does not rely on any type ofdomain knowledge. A synthetic image set containing geometric shapes was generated totest our initial algorithm implementation. Our experimental results show that image mining isfeasible. We also suggest several directions for future work in this area.,*,1998,41
Programming the K-means clustering algorithm in SQL,Carlos Ordonez,Abstract Using SQL has not been considered an efficient and feasible way to implementdata mining algorithms. Although this is true for many data mining; machine learning andstatistical algorithms; this work shows it is feasible to get an efficient SQL implementation ofthe well-known K-means clustering algorithm that can work on top of a relational DBMS. Thearticle emphasizes both correctness and performance. From a correctness point of view thearticle explains how to compute Euclidean distance; nearest-cluster queries and updatingclustering results in SQL. From a performance point of view it is explained how to clusterlarge data sets defining and indexing tables to store and retrieve intermediate and finalresults; optimizing and avoiding joins; optimizing and simplifying clustering aggregations;and taking advantage of sufficient statistics. Experiments evaluate scalability with …,Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,2004,40
Building statistical models and scoring with UDFs,Carlos Ordonez,Abstract Multidimensional statistical models are generally computed outside a relationalDBMS; exporting data sets. This article explains how fundamental multidimensionalstatistical models are computed inside the DBMS in a single table scan exploiting SQL andUser-Defined Functions (UDFs). The techniques described herein are used in a commercialdata mining tool; called Teradata Warehouse Miner. Specifically; we explain howcorrelation; linear regression; PCA and clustering; are integrated into the Teradata DBMS.Two major database processing tasks are discussed: building a model and scoring a dataset based on a model. To build a model two summary matrices are shown to be commonand essential for all linear models: the linear sum of points and the quadratic sum of cross-products of points. Since such matrices are generally significantly smaller than the data …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,35
Optimizing recursive queries in SQL,Carlos Ordonez,Abstract Recursion represents an important addition to the SQL language. This work focuseson the optimization of linear recursive queries in SQL. To provide an abstract framework fordiscussion; we focus on computing the transitive closure of a graph. Three optimizations arestudied:(1) Early evaluation of row selection conditions.(2) Eliminating duplicate rows inintermediate tables.(3) Defining an enhanced index to accelerate join computation.Optimizations are evaluated on two types of graphs: binary trees and sparse graphs. Binarytrees represent an ideal graph with no cycles and a linear number of edges. Sparse graphsrepresent an average case with some cycles and a linear number of edges. In general; theproposed optimizations produce a significant reduction in the evaluation time of recursivequeries.,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,33
Method for performing clustering in very large databases,*,A method for performing cluster analysis inside a relational database management system.The method defines a plurality of tables for the storage of data points and Gaussian mixtureparameters and executes a series of SQL statements implementing an Expectation-Maximization clustering algorithm to iteratively update the Gaussian mixture parametersstored within the tables.,*,2002,32
Evaluating statistical tests on OLAP cubes to compare degree of disease,Carlos Ordonez; Zhibo Chen,Statistical tests represent an important technique used to formulate and validate hypotheseson a dataset. They are particularly useful in the medical domain; where hypotheses linkdisease with medical measurements; risk factors; and treatment. In this paper; we propose tocompute parametric statistical tests treating patient records as elements in amultidimensional cube. We introduce a technique that combines dimension lattice traversaland statistical tests to discover significant differences in the degree of disease within pairs ofpatient groups. In order to understand a cause--effect relationship; we focus on patient grouppairs differing in one dimension. We introduce several optimizations to prune the searchspace; to discover significant group pairs; and to summarize results. We presentexperiments showing important medical findings and evaluating scalability with medical …,IEEE Transactions on Information Technology in Biomedicine,2009,29
Evaluating association rules and decision trees to predict multiple target attributes,Carlos Ordonez; Kai Zhao,Abstract Association rules and decision trees represent two well-known data miningtechniques to find predictive rules. In this work; we present a detailed comparison betweenconstrained association rules and decision trees to predict multiple target attributes. Weidentify important differences between both techniques for such goal. We conduct anextensive experimental evaluation on a real medical data set to mine rules predictingdisease on multiple heart arteries. The antecedent of association rules contains medicalmeasurements and patient risk factors; whereas the consequent refers to the degree ofdisease on one artery or multiple arteries. Predictive rules found by constrained associationrule mining are more abundant and have higher reliability than predictive rules induced bydecision trees. We investigate why decision trees miss certain rules; why they tend to …,Intelligent Data Analysis,2011,27
Efficient OLAP with UDFs,Zhibo Chen; Carlos Ordonez,Abstract Since the early 1990s; On-Line Analytical Processing (OLAP) has been a wellstudied research topic that has focused on implementation outside the database; either withOLAP servers or entirely within the client computers. Our approach involves the computationand storage of OLAP cubes using User-Defined Functions (UDF) with a databasemanagement system. UDFs offer users a chance to write their own code that can then calledlike any other standard SQL function. By generating OLAP cubes within a UDF; we are ableto create the entire lattice in main memory. The UDF also allows the user to assert morecontrol over the actual generation process than when using standard OLAP functions suchas the CUBE operator. We introduce a data structure that can not only efficiently create anOLAP lattice in main memory; but also be adapted to generate association rule itemsets …,Proceeding of the ACM 11th international workshop on Data warehousing and OLAP,2008,25
Vector and matrix operations programmed with UDFs in a relational DBMS,Carlos Ordonez; Javier García-García,Abstract In general; a relational DBMS provides limited capabilities to performmultidimensional statistical analysis; which requires manipulating vectors and matrices. Inthis work; we study how to extend a DBMS with basic vector and matrix operators byprogramming User-Defined Functions (UDFs). We carefully analyze UDF features andlimitations to implement vector and matrix operations commonly used in statistics; machinelearning and data mining; paying attention to DBMS; operating system and computerarchitecture constraints. UDFs represent a C programming interface that allows the definitionof scalar and aggregate functions that can be used in SQL. UDFs have several advantagesand limitations. A UDF allows fast evaluation of arithmetic expressions; memorymanipulation; using multidimensional arrays and exploiting all C language control …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,25
A fast algorithm to cluster high dimensional basket data,Carlos Ordonez; Edward Omiecinski; Norberto Ezquerra,Clustering is a data mining problem that has received significant attention by the databasecommunity. Data set size; dimensionality and sparsity have been identified as aspects thatmake clustering more difficult. The article introduces a fast algorithm to cluster large binarydata sets where data points have high dimensionality and most of their coordinates are zero.This is the case with basket data transactions containing items; that can be represented assparse binary vectors with very high dimensionality. An experimental section showsperformance; advantages and limitations of the proposed approach.,Data Mining; 2001. ICDM 2001; Proceedings IEEE International Conference on,2001,25
Can we analyze big data inside a DBMS?,Carlos Ordonez,Abstract Relational DBMSs remain the main data management technology; despite the bigdata analytics and no-SQL waves. On the other hand; for data analytics in a broad sense;there are plenty of non-DBMS tools including statistical languages; matrix packages; genericdata mining programs and large-scale parallel systems; being the main technology for bigdata analytics. Such large-scale systems are mostly based on the Hadoop distributed filesystem and MapReduce. Thus it would seem a DBMS is not a good technology to analyzebig data; going beyond SQL queries; acting just as a reliable and fast data repository. In thissurvey; we argue that is not the case; explaining important research that has enabledanalytics on large databases inside a DBMS. However; we also argue DBMSs cannotcompete with parallel systems like MapReduce to analyze web-scale text data. Therefore …,Proceedings of the sixteenth international workshop on Data warehousing and OLAP,2013,22
Relational versus non-relational database systems for data warehousing,Carlos Ordonez; Il-Yeol Song; Carlos Garcia-Alvarado,Abstract Relational database systems have been the dominating technology to manage andanalyze large data warehouses. Moreover; the ER model; the standard in database designhas a close relationship with the relational model. Recently; there has been a surge ofalternative technologies for large scale analytic processing; most of which are not based onthe relational model. Out of these proposals; distributed file systems together withMapReduce have become strong competitors to relational database systems to analyzelarge data sets; exploiting parallel processing. Moreover; there is progress on usingMapReduce to evaluate relational queries. With that motivation in mind; this panel willcompare pros and cons of each technology for data warehousing and will identify researchissues; considering practical aspects like ease of use; programming flexibility and cost; as …,Proceedings of the ACM 13th international workshop on Data warehousing and OLAP,2010,22
Accelerating EM clustering to find high-quality solutions,Carlos Ordonez; Edward Omiecinski,Abstract Clustering is one of the most important techniques used in data mining. This articlefocuses on the EM clustering algorithm. Two fundamental aspects are studied: achievingfaster convergence and finding higher quality clustering solutions. This work introducesseveral improvements to the EM clustering algorithm; being periodic M steps during initialiterations; reseeding of low-weight clusters and splitting of high-weight clusters the mostimportant. These improvements lead to two important parameters. The first parameter is thenumber of M steps per iteration and the second one; a weight threshold to reseed low-weight clusters. Experiments show how frequently the M step must be executed and whatweight threshold values make EM reach higher quality solutions. In general; the improvedEM clustering algorithm finds higher quality solutions than the classical EM algorithm and …,Knowledge and Information Systems,2005,20
Models for association rules based on clustering and correlation,Carlos Ordonez,Abstract Association rules require models to understand their relationship to statisticalproperties of the data set. In this work; we study mathematical relationships betweenassociation rules and two fundamental techniques: clustering and correlation. Each clusterrepresents an important itemset. We show the sufficient statistics for clustering andcorrelation on binary data sets are the linear sum of points and the quadratic sum of points;respectively. We prove itemset support can be bounded and approximated from bothmodels. Support bounds and support estimation obey the set downward closure property forfast bottom-up search for frequent itemsets. Both models can be efficiently computed withsparse matrix computations. Experiments with real and synthetic data sets evaluate modelaccuracy and speed. The clustering model is accurate to estimate support; given a …,Intelligent Data Analysis,2009,18
PCA for large data sets with parallel data summarization,Carlos Ordonez; Naveen Mohanam; Carlos Garcia-Alvarado,Abstract Parallel processing is essential for large-scale analytics. Principal ComponentAnalysis (PCA) is a well known model for dimensionality reduction in statistical analysis;which requires a demanding number of I/O and CPU operations. In this paper; we study howto compute PCA in parallel. We extend a previous sequential method to a highly parallelalgorithm that can compute PCA in one pass on a large data set based on summarizationmatrices. We also study how to integrate our algorithm with a DBMS; our solution is basedon a combination of parallel data set summarization via user-defined aggregations andcalling the MKL parallel variant of the LAPACK library to solve Singular ValueDecomposition (SVD) in RAM. Our algorithm is theoretically shown to achieve linearspeedup; linear scalability on data size; quadratic time on dimensionality (but in RAM) …,Distributed and Parallel Databases,2014,17
Data mining of large myocardial perfusion SPECT (MPS) databases to improve diagnostic decision making.,CD Cooke; Carlos Ordonez; Ernest V Garcia; Edward Omiecinski; EG Krawczynska; RD Folks; CA Santana; Levien DeBraal; NF Ezquerra,*,Journal of Nuclear Medicine,1999,17
Horizontal implementation of expectation-maximization algorithm in SQL for performing clustering in very large databases,*,A method for performing cluster analysis inside a relational database management system.The method defines a plurality of tables for the storage of data points and Gaussian mixtureparameters and executes a series of SQL statements implementing an Expectation-Maximization clustering algorithm to iteratively update the Gaussian mixture parametersstored within the tables.,*,2003,16
Extending ER models to capture database transformations to build data sets for data mining,Carlos Ordonez; Sofian Maabout; David Sergio Matusevich; Wellington Cabrera,Abstract In a data mining project developed on a relational database; a significant effort isrequired to build a data set for analysis. The main reason is that; in general; the databasehas a collection of normalized tables that must be joined; aggregated and transformed inorder to build the required data set. Such scenario results in many complex SQL queries thatare written independently from each other; in a disorganized manner. Therefore; thedatabase grows with many tables and views that are not present as entities in the ER modeland similar SQL queries are written multiple times; creating problems in database evolutionand software maintenance. In this paper; we classify potential database transformations; weextend an ER diagram with entities capturing database transformations and we introduce analgorithm which automates the creation of such extended ER model. We present a case …,Data & Knowledge Engineering,2014,15
Method; data structure; and systems for customer segmentation models,*,Methods; data structures; and systems for generating customer segmentation models areprovided. Basket transactions are analyzed and classified into a first segment type; a secondsegment type; a third segment type; or a fourth segment type. A number of the baskettransaction within a number of the segment types are separately analyzed to determine subclassifications or sub segments within a particular segment type. Each basket transaction isaugmented with a segment type that identifies the segment type classification; and a numberof the basket transactions include a segment identifier that identifies the sub segment withina segment type that a basket transaction is associated with. The augmented baskettransactions represent a customer segmentation model. In one embodiment; dailytransactions are monitored by a script and used to dynamically adjust the customer …,*,2011,15
Extended aggregations for databases with referential integrity issues,Javier García-García; Carlos Ordonez,Abstract Querying inconsistent databases remains a broad and difficult problem. In this work;we study how to improve aggregations computed on databases with referential errors in thecontext of database integration; where each source database has different tables; columnswith similar content across multiple databases; but different referential integrity constraints.Thus; a query in an integrated database may involve tables and columns with referentialintegrity errors. In a data warehouse; even though the ETL processes fix referential integrityerrors; this is generally done by inserting “dummy” records into the dimension tablescorresponding to such invalid foreign keys; thereby artificially enforcing referential integrity.When two tables are joined and aggregations are computed; rows with an invalid or nullforeign key value are skipped; effectively eliminating potentially valuable information …,Data & Knowledge Engineering,2010,15
Efficient computation of PCA with SVD in SQL,Mario Navas; Carlos Ordonez,Abstract PCA is one of the most common dimensionality reduction techniques with broadapplications in data mining; statistics and signal processing. In this work we study how toleverage a DBMS computing capabilities to solve PCA. We propose a solution thatcombines a summarization of the data set with the correlation or covariance matrix and thensolve PCA with Singular Value Decomposition (SVD). Deriving the summary matrices allowanalyzing large data sets since they can be computed in a single pass. Solving SVD withoutexternal libraries proves to be a challenge to compute in SQL. We introduce two solutions:one based in SQL queries and a second one based on User-Defined Functions.Experimental evaluation shows our method can solve larger problems in less time thanexternal statistical packages.,Proceedings of the 2nd Workshop on Data Mining using Matrices and Tensors,2009,15
Vertical implementation of expectation-maximization algorithm in SQL for performing clustering in very large databases,*,A method for performing cluster analysis inside a relational database management system.The method defines a plurality of tables for the storage of data points and Gaussian mixtureparameters and executes a series of SQL statements implementing an Expectation-Maximization clustering algorithm to iteratively update the Gaussian mixture parametersstored within the tables.,*,2003,15
Database systems research on data mining,Carlos Ordonez; Javier García-García,Abstract Data mining remains an important research area in database systems. We presenta review of processing alternatives; storage mechanisms; algorithms; data structures andoptimizations that enable data mining on large data sets. We focus on the computation ofwell-known multidimensional statistical and machine learning models. We pay particularattention to SQL and MapReduce as two competing technologies for large scale processing.We conclude with a summary of solved major problems and open research issues.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,14
Validating expert system rule confidences using data mining of myocardial perfusion SPECT databases,CD Cooke; CA Santana; TI Morris; Levien DeBraal; Carlos Ordonez; Edward Omiecinski; NF Ezquerra; Ernest V Garcia,The authors' goal with this study was to use data mining techniques; applied to imaging andtextual patient databases; to validate the confidences (certainty factors) of the heuristic rulesin the authors' previously described Expert System; PERFEX/sup TM/. A relational databasecombining textual and imaging information was generated from 655 patients who hadundergone both stress/rest myocardial perfusion SPECT and coronary angiography. Initialdata mining was concentrated on heuristic rules involving myocardial perfusion defects andthe LAD vascular territory. The results show the robustness of the expert system; andfurthermore show that data mining of large databases combining textual and imaginginformation can be used to validate and potentially improve the confidence levels associatedwith heuristic rules in expert systems.,Computers in Cardiology 2000,2000,14
One-pass data mining algorithms in a DBMS with UDFs,Carlos Ordonez; Sasi K Pitchaimalai,Abstract Data mining research is extensive; but most work has proposed efficient algorithms;data structures and optimizations that work outside a DBMS; mostly on flat files. In contrast;we present a data mining system that can work on top of a relational DBMS based on acombination of SQL queries and User-Defined Functions (UDFs); debuking the commonperception that SQL is inefficient or inadequate for data mining. We show our system cananalyze large data sets significantly faster than external data mining tools. Moreover; ourUDF-based algorithms can process a data set in one pass and have linear scalability.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,11
Fast UDFs to compute sufficient statistics on large data sets exploiting caching and sampling,Carlos Ordonez; Sasi K Pitchaimalai,Abstract User-Defined Functions (UDFs) represent an extensibility mechanism provided bymost DBMSs; whose execution happens in main memory. Also; UDFs leverage the DBMSmulti-threaded capabilities and exploit the C language speed and flexibility for mathematicalcomputations. In this article; we study how to accelerate computation of sufficient statistics onlarge data sets with UDFs exploiting caching and sampling techniques. We present anaggregate UDF computing multidimensional sufficient statistics that benefit a broad array ofstatistical models: the linear sum of points and the quadratic sum of cross-products of pointdimensions. Caching can be applied when the data set fits in main memory. Otherwise;sampling is required to accelerate processing of very large data sets. Also; sampling can beapplied on data sets that can be cached; to further accelerate processing. Experiments …,Data & Knowledge Engineering,2010,11
Information retrieval from digital libraries in SQL,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Information retrieval techniques have been traditionally exploited outside ofrelational database systems; due to storage overhead; the complexity of programming theminside the database system; and their slow performance in SQL implementations. Thisproject supports the idea that searching and querying digital libraries with informationretrieval models in relational database systems can be performed with optimized SQLqueries and User-Defined Functions. In our research; we propose several techniquesdivided into two phases: storing and retrieving. The storing phase includes executingdocument pre-processing; stop-word removal and term extraction; and the retrieval phase isimplemented with three fundamental IR models: the popular Vector Space Model; the OkapiProbabilistic Model; and the Dirichlet Prior Language Model. We conduct experiments …,Proceedings of the 10th ACM workshop on Web information and data management,2008,11
The Gamma matrix to summarize dense and sparse data sets for big data analytics,Carlos Ordonez; Yiqun Zhang; Wellington Cabrera,Data summarization is an essential mechanism to accelerate analytic algorithms on largedata sets. On the other hand; array DBMSs enable scalable computation with large matrices.With that motivation in mind; we propose a parallel array operator; based on a specific formof matrix multiplication; that computes a comprehensive data summarization matrix. Byderiving equivalent equations based on the summarization matrix; statistical methods areadapted to work in two phases:(1) Parallel summarization of the data set in one pass;(2)Iteration exploiting the summarization matrix in many intermediate computations. We proveour summarization matrix captures essential statistical properties of the data set and it allowsiterative algorithms to work faster in main memory; by decreasing the number of times thedata set is scanned; and by reducing the number of CPU operations. Specifically; we …,IEEE Transactions on Knowledge and Data Engineering,2016,10
Integrating multi-platform genomic data using hierarchical Bayesian relevance vector machines,Sanvesh Srivastava; Wenyi Wang; Ganiraju Manyam; Carlos Ordonez; Veerabhadran Baladandayuthapani,Abstract Background Recent advances in genome technologies and the subsequentcollection of genomic information at various molecular resolutions hold promise toaccelerate the discovery of new therapeutic targets. A critical step in achieving these goals isto develop efficient clinical prediction models that integrate these diverse sources of high-throughput data. This step is challenging due to the presence of high-dimensionality andcomplex interactions in the data. For predicting relevant clinical outcomes; we propose aflexible statistical machine learning approach that acknowledges and models the interactionbetween platform-specific measurements through nonlinear kernel machines and borrowsinformation within and between platforms through a hierarchical Bayesian framework. Ourmodel has parameters with direct interpretations in terms of the effects of platforms and …,EURASIP Journal on Bioinformatics and Systems Biology,2013,10
Fast and dynamic OLAP exploration using UDFs,Zhibo Chen; Carlos Ordonez; Carlos Garcia-Alvarado,Abstract OLAP is a set of database exploratory techniques to efficiently retrieve multiple setsof aggregations from a large dataset. Generally; these techniques have either involved theuse of an external OLAP server or required the dataset to be exported to a specialized OLAPtool for more efficient processing. In this work; we show that OLAP techniques can beperformed within a modern DBMS without external servers or the exporting of datasets;using standard SQL queries and UDFs. The main challenge of such approach is that SQLand UDFs are not as flexible as the C language to explore the OLAP lattice and therefore itis more difficult to develop optimizations. We compare three different ways of performingOLAP exploration: plain SQL queries; a UDF implementing a lattice structure; and a UDFprogramming the star cube structure. We demonstrate how such methods can be used to …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,10
Exploration and visualization of olap cubes with statistical tests,Carlos Ordonez; Zhibo Chen,Abstract In On-Line Analytical Processing (OLAP); users explore a database cube with roll-up and drill-down operations in order to find interesting results. Most approaches rely onsimple aggregations and value comparisons in order to validate findings. In this work; wepropose to combine OLAP dimension lattice traversal and statistical tests to discoversignificant metric differences between highly similar groups. A parametric statistical testallows pair-wise comparison of neighboring cells in cuboids; providing statistical evidenceabout the validity of findings. We introduce a two-dimensional checkerboard visualization ofthe cube that allows interactive exploration to understand significant measure differencesbetween two cuboids differing in one dimension along with associated image data. Oursystem is tightly integrated into a relational DBMS; by dynamically generating SQL code …,Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery: Integrating Automated Analysis with Interactive Exploration,2009,10
Efficient distance computation using SQL queries and UDFs,Sasi K Pitchaimalai; Carlos Ordonez; Carlos Garcia-Alvarado,Distance computation is one of the most computationally intensive operations employed bymany data mining algorithms. Performing such matrix computations within a DBMS createsmany optimization challenges. We propose techniques to efficiently compute Euclideandistance using SQL queries and User-Defined Functions (UDFs). We concentrate onefficient Euclidean distance computation for the well-known K-means clustering algorithm.We present SQL query optimizations and a scalar UDF to compute Euclidean distance. Weexperimentally evaluate performance and scalability of our proposed SQL queries and UDFwith large data sets on a modern DBMS. We benchmark distance computation on twoimportant data mining techniques: clustering and classification. In general; UDFs are fasterthan SQL queries because they are executed in main memory. Data set size is the main …,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,10
Measuring referential integrity in distributed databases,Carlos Ordonez; Javier García-García; Zhibo Chen,Abstract Distributed relational databases are used by different organizations located atmultiple sites that work together on common projects. In this article; we focus on distributedrelational databases with incomplete and inconsistent content. We propose to measurereferential integrity errors in them for integration and interoperability purposes. We proposelocal and global referential integrity metrics at three levels: column; table and database. Weassume each table can be asynchronously updated at any site and new records areperiodically broadcasted to all sites. We explain several distributed query optimizationissues. Our proposal is useful in database integration; multiple database interoperability anddata quality assurance. We discuss applications of our proposal in distributed scientificdatabases.,Proceedings of the ACM first workshop on CyberInfrastructure: information management in eScience,2007,10
Comparing columnar; row and array DBMSs to process recursive queries on graphs,Carlos Ordonez; Wellington Cabrera; Achyuth Gurram,Abstract Analyzing graphs is a fundamental problem in big data analytics; for which DBMStechnology does not seem competitive. On the other hand; SQL recursive queries are afundamental mechanism to analyze graphs in a DBMS; whose processing and optimizationare significantly harder than traditional SPJ queries. Columnar DBMSs are a new fasterclass of database system; with significantly different storage and query processingmechanisms compared to row DBMSs; still the dominating technology. With that motivationin mind; we study the optimization of recursive queries on a columnar DBMS focusing on twofundamental and complementary graph problems: transitive closure and adjacency matrixmultiplication. From a query processing perspective we consider the three fundamentalrelational operators: selection; projection and join (SPJ); where projection subsumes SQL …,Information Systems,2017,9
OLAP-based query recommendation,Carlos Garcia-Alvarado; Zhibo Chen; Carlos Ordonez,Abstract Query recommendation is an invaluable tool for enabling users to speed up theirsearches. In this paper; we present algorithms for generating query suggestions; assumingno previous knowledge of the collection. We developed an online OLAP algorithm togenerate query suggestions for the users based on the frequency of the keywords in theselected documents and the correlation between the keywords in the collection. In addition;performance and scalability experiments of these algorithms are presented as proof of theirfeasibility. We also present sampling as an additional approach for improving performanceby using approximate results. We show valid recommendations as a result of combinationsgenerated using the correlations between the keywords. The online OLAP algorithm is alsocompared with the well-known Apriori algorithm and found to be faster only when simple …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,9
Keyword search across databases and documents,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Given the continuous growth of databases and the abundance of diverse files inmodern IT environments; there is a pressing need to integrate keyword search onheterogeneous information sources. A particular case in which such integration is neededoccurs when a collection of documents (eg word processing documents; spreadsheets; textfiles and so on) is derived directly from a central database; and both repositories areindependently updated. Finding hidden relationships between documents and databases isdifficult; given the loose connection between them. This problem is especially complicatedwhen database integration techniques must be extended to handle semi-structured data (iedocuments). Our research focuses on exploiting a relational database system for integratingand exploring complex interrelationships between a database and a collection of …,Proceedings of the 2nd International Workshop on Keyword Search on Structured Data,2010,9
Metadata management for federated databases,Carlos Ordonez; Zhibo Chen; Javier García-García,Abstract A federated database consists of several loosely integrated databases; where eachdatabase may contain hundreds of tables and thousands of columns; interrelated bycomplex foreign key relationships. In general; there exists a lot of semistructured dataelements outside the database represented by documents (files); created and updated bymultiple users and programs. Documents have references to multiple databases andsubsets of their tables and columns. Manually tracking which specific tables and columnsare referred to by a document; accessed by a specific program or user is a daunting task.With such a goal in mind; we present a system that builds metadata models for a federateddatabase using a relational database as a central object type and metadata repository.Metadata includes table and columns coming from logical data models corresponding to …,Proceedings of the ACM first workshop on CyberInfrastructure: information management in eScience,2007,9
Algorithms and Optimizations for Big Data Analytics: Cubes,Carlos Ordonez,Table UDF• Main difference with aggregate UDF: returns a table (instead of single value)•Also; it can take several input values• Called in the FROM clause in a SELECT• Stream: noparallel processing; external file• Computation power same as aggregate UDF• Suitable forcomplex math operations and algorithms,Tech Talks; University of Houston; USA,*,9
Translator of statistical language programs into SQL,*,System and method for translating statements and expressions within statistical languageprograms into SQL. These statements include one or more input statements and have aplurality of data rows. These statements include one or more array statements; the arraystatements having a set of original variables. The method for translating statements andexpressions within statistical language programs into SQL includes the step of defining aselect statement including a subset of the original variables. The expressions include one ormore assignment expressions; the assignment expressions including a set of originalvariables and a set of new variables affected by the assignment expressions.,*,2014,8
Interactive exploration and visualization of OLAP cubes,Carlos Ordonez; Zhibo Chen; Javier García-García,Abstract An OLAP cube is typically explored with multiple aggregations selecting differentsubsets of cube dimensions to analyze trends or to discover unexpected results.Unfortunately; such analytic process is generally manual and fails to statistically explainresults. In this work; we propose to combine dimension lattice traversal and parametricstatistical tests to identify significant metric differences between cube cells. We present a 2Dinteractive visualization of the OLAP cube based on a checkerboard that enables isolatingand interpreting significant measure differences between two similar cuboids; which differ inone dimension and have the same values on the remaining dimensions. Cube explorationand visualization is performed by automatically generated SQL queries. An experimentalevaluation with a medical data set presents statistically significant results and interactive …,Proceedings of the ACM 14th international workshop on Data Warehousing and OLAP,2011,8
Comparing SQL and MapReduce to compute Naive Bayes in a single table scan,Sasi K Pitchaimalai; Carlos Ordonez; Carlos Garcia-Alvarado,Abstract Most data mining processing is currently performed on flat files outside the DBMS.We propose novel techniques to process such data mining computations inside the DBMS.We focus on the popular Naive Bayes classification algorithm. In contrast to mostapproaches; our techniques work completely inside the DBMS; exploiting the DBMSprogrammability mechanisms wherein the user has full access to data; but is transparent tothe DBMS internals. Specifically; SQL queries and User-Defined Functions (UDFs) are usedto program the Naive Bayes algorithm. We compare these mechanisms with MapReduce; apopular alternative used for large-scale data mining. We study two phases for the classifier:building the model and scoring another data set; using the model as input. Both building andscoring phases with SQL queries involve a single table scan; whereas scoring with UDFs …,Proceedings of the second international workshop on Cloud data management,2010,8
Optimization techniques for linear recursive queries in sql,*,A system and method of evaluating an SQL recursive query having one or more base selectstatements and one or more recursive select statements. In one technique the query relatesto a base table and has a filter condition on one or more columns from a result table returnedby the query. The technique includes the steps of receiving the query to be evaluated;evaluating a base step by evaluating one or more of the base select statements; evaluatingone or more recursive steps by evaluating one or more of the recursive select statements;evaluating the filter condition prior to evaluating any of the recursive steps; and returning theresult of the query.,*,2007,8
Bayesian variable selection in linear regression in one pass for large datasets,Carlos Ordonez; Carlos Garcia-Alvarado; Veerabhadaran Baladandayuthapani,Abstract Bayesian models are generally computed with Markov Chain Monte Carlo (MCMC)methods. The main disadvantage of MCMC methods is the large number of iterations theyneed to sample the posterior distributions of model parameters; especially for large datasets.On the other hand; variable selection remains a challenging problem due to itscombinatorial search space; where Bayesian models are a promising solution. In this work;we study how to accelerate Bayesian model computation for variable selection in linearregression. We propose a fast Gibbs sampler algorithm; a widely used MCMC method thatincorporates several optimizations. We use a Zellner prior for the regression coefficients; animproper prior on variance; and a conjugate prior Gaussian distribution; which enabledataset summarization in one pass; thus exploiting an augmented set of sufficient …,ACM Transactions on Knowledge Discovery from Data (TKDD),2014,7
Query processing on cubes mapped from ontologies to dimension hierarchies,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Text columns commonly extend core information stored as atomic values in arelational database; creating a need to explore and summarize text data. OLAP cubes canprecisely accomplish such tasks. However; cubes have been overlooked as a mechanismfor capturing not only text summarizations; but also for representing and exploring thehierarchical structure of an ontology. In this paper; we focus on exploiting cubes to computemultidimensional aggregations on classified documents stored in a DBMS (keywordfrequency; document count; document class frequency and so on). We propose CUBO(CUBed Ontologies); a novel algorithm; which efficiently manipulates the hierarchy behindan ontology. Our algorithm is optimized to compute desired summarizations without havingto search all possible dimension combinations; exploiting the sparseness of the …,Proceedings of the fifteenth international workshop on Data warehousing and OLAP,2012,7
K-means clustering using structured query language (SQL) statements and sufficient statistics,*,During clustering; a data set is partitioned into disjoint groups such that points in the same groupare similar to each other according to some similarity metric. A widely used clustering techniqueis K-means clustering. Clustering can be performed on numeric data or categorical data. Numericaldata refers to data that can be assigned a metric measure; such as height; scale; volume; andso forth. Categorical data is data that has a finite number of values not represented by ameasure. Examples of categorical data include city; state; gender; and so forth … Normally;K-means clustering algorithms are relatively difficult to implement in database managementsystems. A programmer that develops code for clustering algorithms typically has to addressissues such as storage management; concurrent access; memory leaks; false alarms; securityconcerns; and so forth. Such complexity results in lengthy development times for …,*,2008,7
The Gamma operator for big data summarization on an array DBMS,Carlos Ordonez; Yiqun Zhang; Wellington Cabrera,Abstract SciDB is a parallel array DBMS that provides multidimensional arrays; a querylanguage and basic ACID properties. In this paper; we introduce a summarization matrixoperator that computes sufficient statistics in one pass and in parallel on an array DBMS.Such sufficient statistics benefit a big family of statistical and machine learning models;including PCA; linear regression and variable selection. Experimental evaluation on aparallel cluster shows our matrix operator exhibits linear time complexity and linearspeedup. Moreover; our operator is shown to be an order of magnitude faster than SciDBbuilt-in operators; two orders of magnitude faster than SQL queries on a fast column DBMSand even faster than the R package when the data set fits in RAM. We show SciDBoperators and the R package fail due to RAM limitations; whereas our operator does not …,Proceedings of the 3rd International Workshop on Big Data; Streams and Heterogeneous Source Mining: Algorithms; Systems; Programming Models and Applications,2014,6
Bayesian variable selection for linear regression in high dimensional microarray data,Wellington Cabrera; Carlos Ordonez; David Sergio Matusevich; Veerabhadran Baladandayuthapani,Abstract Variable selection is a fundamental problem in Bayesian statistics whose solutionrequires exploring a combinatorial search space. We study the solution of variable selectionwith a well-known MCMC method; which requires thousands of iterations. We presentseveral algorithmic optimizations to accelerate the MCMC method to make it work efficientlyinside a database system. Our optimizations include sufficient statistics; variablepreselection; hash tables and calling a linear algebra library. We present experiments withvery high dimensional microarray data sets to predict cancer survival time. We discussencouraging findings; identifying specific genes likely to predict the survival time for braincancer patients. We also show our DBMS-based algorithm is orders of magnitude faster thanthe R statistical package. Our work shows a DBMS is a promising platform to analyze …,Proceedings of the 7th international workshop on Data and text mining in biomedical informatics,2013,6
Fast PCA computation in a DBMS with aggregate UDFs and LAPACK,Carlos Ordonez; Naveen Mohanam; Carlos Garcia-Alvarado; Predrag T Tosic; Edgar Martinez,Abstract Efficient and scalable execution of numerical methods inside a DBMS is difficult asits architecture is not suited for intense numerical computations. We study computingPrincipal Component Analysis (PCA) on large data sets via Singular Value Decomposition(SVD). Given the difficulty to program and optimize numerical methods on an existing DBMS;we explore an alternative reusability approach: calling the well-known numerical libraryLAPACK. Thus we study several alternatives to summarize the data set with aggregate User-Defined Functions (UDFs) and how to efficiently call SVD numerical methods available inLAPACK via Stored Procedures (SPs). We propose algorithmic and system optimizations toenhance scalability and to push processing into RAM. We show it is feasible to efficientlysolve PCA by first summarizing the data set with arrays incrementally updated with …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,6
ONTOCUBE: efficient ontology extraction using OLAP cubes,Carlos Garcia-Alvarado; Zhibo Chen; Carlos Ordonez,Abstract Ontologies are knowledge conceptualizations of a particular domain and arecommonly represented with hierarchies. While final ontologies appear deceivingly simple onpaper; building ontologies represents a time-consuming task that is normally performed bynatural language processing techniques or schema matching. On the other hand; OLAPcubes are most commonly used during decision-making processes via the analysis of datasummarizations. In this paper; we present a novel approach based on using OLAP cubes forontology extraction. The resulting ontology is obtained through an analytical process of thesummarized frequencies of keywords within a corpus. The solution was implemented withina relational database system (DBMS). In our experiments; we show how all the proposeddiscrimination measures (frequency; correlation; lift) affect the resulting classes. We also …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,6
A Data mining system based on sql queries and udfs for relational databases,Carlos Ordonez; Carlos Garcia-Alvarado,Abstract Most research on data mining has proposed algorithms and optimizations that workon flat files; outside a DBMS; mainly due to the following reasons. It is easier to developefficient algorithms in a traditional programming language. The integration of data miningalgorithms into a DBMS is difficult given its relational model foundation and systemarchitecture. Moreover; SQL may be slow and cumbersome for numerical analysiscomputations. Therefore; data mining users commonly export data sets outside the DBMSfor data mining processing; which creates a performance bottleneck and eliminatesimportant data management capabilities such as query processing and security; amongothers (eg concurrency control and fault tolerance). With that motivation in mind; wedeveloped a novel system based on SQL queries and User-Defined Functions (UDFs) …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,6
On the computation of stochastic search variable selection in linear regression with UDFs,Mario Navas; Carlos Ordonez; Veerabhadran Baladandayuthapani,Computing Bayesian statistics with traditional techniques is extremely slow; specially whenlarge data has to be exported from a relational DBMS. We propose algorithms for large scaleprocessing of stochastic search variable selection (SSVS) for linear regression that can workentirely inside a DBMS. The traditional SSVS algorithm requires multiple scans of the inputdata in order to compute a regression model. Due to our optimizations; SSVS can be done ineither one scan over the input table for large number of records with sufficient statistics; orone scan per iteration for high-dimensional data. We consider storage layouts whichefficiently exploit DBMS parallel processing of aggregate functions. Experimental resultsdemonstrate correctness; convergence and performance of our algorithms. Finally; thealgorithms show good scalability for data with a very large number of records; or a very …,Data Mining (ICDM); 2010 IEEE 10th International Conference on,2010,6
Efficient algorithms based on relational queries to mine frequent graphs,Walter Garcia; Carlos Ordonez; Kai Zhao; Ping Chen,Abstract Frequent subgraph mining is an important problem in data mining with wideapplication in science. For instance; graphs can be used to represent structural relationshipsin problems related to network topology; chemical compound; protein structures; and so on.Searching for patterns from graph databases is difficult since graph-related operationsgenerally have higher time complexity than equivalent operations on frequent itemsets.From a practical standpoint; databases keep growing with lots of opportunities and need tomine graphs. Even though there is a significant body of work on graph mining; mosttechniques work outside the database system. Programming frequent graph mining in SQLis more difficult than traditional approaches because the graph must be represented as atable and algorithmic steps must be written as relational queries. In our research; we …,Proceedings of the 3rd workshop on Ph. D. students in information and knowledge management,2010,6
Comparing reliability of association rules and OLAP statistical tests,Zhibo Chen; Carlos Ordonez; Kai Zhao,Association rules is a technique that can detect patterns within the items of a dataset. Theconstrained version applies several restrictions that reduces the number of rules and alsohelps improve performance. On the other hand; OLAP statistical tests is an integration ofexploratory On-Line Analytical Processing techniques and statistical tests. It uses a differentapproach that make it more appropriate for continuous domains and is able to discover moreinformative patterns. In this article; we thoroughly compare the reliability of the resultsreturned by both techniques by analyzing the metrics; such as confidence and p-value; bywhich these techniques are implemented in relation to the results that are generated. Whilethese two techniques are different; we were able to bring both to level ground by extendingassociation rules with pairing to discover more specific patterns and extending OLAP …,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,6
Consistent aggregations in databases with referential integrity errors,Carlos Ordonez; Javier Garcıa-Garcıa; NCR Teradata,ABSTRACT A data warehouse integrates tables coming from multiple source databases;where each database has different tables; columns with similar content across databasesand different referential integrity constraints; enforced to different compliance levels. Somesource databases may have more reliable data than others; if referential integrity is morestrictly enforced or their respective logical data model is more comprehensive. Thus; a queryin an integrated database is likely to refer to tables and columns with referential integrityerrors. In this work; we improve aggregations to handle referential integrity errors on OLAPdatabases. Specifically; when two tables are joined SQL ignores those tuples with invalidforeign key values; effectively discarding potentially valuable information. We extendaggregations to return complete answer sets in the sense that no tuple is excluded. Two …,ACM IQIS Workshop,2006,6
Enerquery: energy-aware query processing,Amine Roukh; Ladjel Bellatreche; Carlos Ordonez,Abstract Energy consumption is increasingly more important in large-scale queryprocessing. This problem requires revisiting traditional query processing in actual DBMSs toidentify the potential of energy saving; and to study the trade-offs between energyconsumption and performance. In this paper; we propose EnerQuery; a tool built on top of atraditional DBMS to capitalize the efforts invested in building energy-aware query optimizers;which have the lion's share in energy consumption. Energy consumption is estimated on allquery plan steps and integrated into a mathematical linear cost model used to select thebest query plans. To increase end users' energy awareness; EnerQuery features adiagnostic GUI to visualize energy consumption per step and its savings when tuning keyparameters during query execution.,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,5
Unified Algorithm to Solve Several Graph Problems with Relational Queries.,Wellington Cabrera; Carlos Ordonez,Abstract. Several important graph algorithms can be solved as an iteration of vector-matrixmultiplication over different semirings. On this basis; we show that the Bellman-Ford (singlesource shortest paths); reachability; PageRank; and topological sort algorithms can beexpressed as relational queries; to solve analytic graph problems in relational databases. Asa main contribution; we present a general algorithm that unifies all graph algorithmsaforementioned.,AMW,2016,5
A clustering algorithm merging MCMC and EM methods using SQL queries,David Matusevich; Carlos Ordonez,Abstract CClustering is an important problem in Statistics and Machine Learning that isusually solved using Likelihood Maximization Methods; of which the Expectation-Maximization Algorithm (EM) is the most common. In this work we present an SQLimplementation of an algorithm merging Markov Chain Monte Carlo methods with the EMalgorithm to find qualitatively better solutions for the clustering problem. Even though SQL isnot optimized for complex calculations; as it is constrained to work on tables and columns; itis unparalleled in handling all aspects of storage management; security of the information;fault management; etc. Our algorithm makes use of these characteristics to produce portablesolutions that are comparable to the results obtained by other algorithms and are moreefficient since the calculations are all performed inside the DBMS. To simplify the …,Proceedings of the 3rd International Workshop on Big Data; Streams and Heterogeneous Source Mining: Algorithms; Systems; Programming Models and Applications,2014,5
Horizontal aggregations in a relational database management system,*,A database system is able to receive a query containing a horizontal aggregate function. Inresponse to the query containing the horizontal aggregate function; aggregate values areproduced in a horizontal format. In general; the query can contain both horizontal (new)aggregate functions and vertical (standard) aggregate functions.,*,2014,5
OntoDBench: interactively benchmarking ontology storage in a database,Stéphane Jean; Ladjel Bellatreche; Carlos Ordonez; Géraud Fokou; Mickaël Baron,Abstract Nowadays; all ingredients are available for developing domain ontologies. This isdue to the presence of various types of methodologies for creating domain ontologies [3].The adoption of ontologies by real life applications generates mountains of ontological datathat need techniques and tools to facilitate their storage; management and querying.,International Conference on Conceptual Modeling,2013,5
Data mining algorithms as a service in the cloud exploiting relational database systems,Carlos Ordonez; Javier García-García; Carlos Garcia-Alvarado; Wellington Cabrera; Veerabhadran Baladandayuthapani; Mohammed S Quraishi,Abstract We present a novel cloud system based on DBMS technology; where data miningalgorithms are offered as a service. A local DBMS connects to the cloud and the cloudsystem returns computed data mining models as small relational tables that are archivedand which can be easily transferred; queried and integrated with the client database. Unlikeother analytic systems; our solution is not based on MapReduce. Our system avoidsexporting large tables outside the local DBMS and thus it avoids transmitting large volumesof data to the cloud. The system offers three processing modes: local; cloud and hybrid;where a linear cost model is used to choose processing mode. In hybrid mode processing issplit between the local DBMS and the cloud DBMS. Our system has a job scheduler withFIFO; SJF and RR policies to enhance response time and get partial results early. The …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,5
Distributed protocols for multi-agent coalition formation: a negotiation perspective,Predrag T Tošić; Carlos Ordonez,Abstract We investigate collaborative multi-agent systems and how they can use simple;scalable negotiation protocols to coordinate in a fully decentralized manner. Specifically; westudy multi-agent distributed coalition formation. We summarize our past and ongoingresearch on collaborative coalition formation and describe our original distributed coalitionformation algorithm. The present paper focuses on negotiation-based view of coalitionformation in collaborative Multi-Agent Systems (MAS). While negotiation protocols havebeen extensively studied in the context of competitive; self-interested agents; we argue thatnegotiation-based approach may be potentially very useful in the context of collaborativeagents; as well–as long as those agents; due to limitations of their sensing andcommunication abilities; have different views of and preferences over the states of the …,International Conference on Active Media Technology,2012,5
Enhancing document exploration with OLAP,Zhibo Chen; Carlos Garcia-Alvarado; Carlos Ordonez,Finding relevant documents in digital libraries has been a well studied problem ininformation retrieval. It is not uncommon to see users browsing digital collections withouthaving a clear idea of the keyword search that they should perform. However; we believethat such initial query search is not totally independent from the target search. Therefore; weuse these initial document selections to further explore these documents. In the followingdemonstration; we exploit On-line Analytical Processing (OLAP) for knowledge discovery indigital collections to achieve query refinement. Such refinement is the result of applying atraditional ranking technique; based on the vector space model; selecting the top keywordsin the resulting subset of documents; and then displaying certain cuboids of the keywords.Based on these cuboids; which are ranked by their frequency; the users can select a …,Data Mining Workshops (ICDMW); 2010 IEEE International Conference on,2010,5
Method and apparatus to cluster binary data transactions,*,A database system is capable of clustering data in received transactions. Clustering isbased on sparse distance computations and/or simplified sufficient statistics. Each of thereceived transactions contain attributes or dimensions that are binary data. In someimplementations; a summary table is also output to enable convenient viewing of the resultsof clustering.,*,2008,5
A Clustering Algorithm to Discover Low and High Density Hyper-Rectangles in Subspaces of Multidimensional Data.,Carlos Ordonez; Edward Robert Omiecinski; Shamkant B Navathe; Norberto F Ezquerra,This paper presents a clustering algorithm to discover low and high density regions insubspaces of multidimensional data for Data Mining applications. High density regionsgenerally refer to typical cases; whereas low density regions indicate infrequent and thusrare cases. For typical applications there is a large number of low density regions and a fewof these are interesting. Regions are considered interesting when they have a minimum"volume" and involve some maximum number of dimensions. Our algorithm discovers highdensity regions (clusters) and low density regions (outliers; negative clusters; holes; emptyregions) at the same time. In particular; our algorithm can find empty regions; that is; regionshaving no data points. The proposed algorithm is fast and simple. There is a large variety ofapplications in medicine; marketing; astronomy; finance; etc; where interesting and …,*,1999,5
Skycube materialization using the topmost skyline or functional dependencies,Sofian Maabout; Carlos Ordonez; Patrick Kamnang Wanko; Nicolas Hanusse,Abstract Given a table T (Id; D 1;…; D d); the skycube of T is the set of skylines with respectto to all nonempty subsets (subspaces) of the set of all dimensions &lcub; D 1;…; D d &rcub;.To optimize the evaluation of any skyline query; the solutions proposed so far in theliterature either (i) precompute all of the skylines or (ii) use compression techniques so thatthe derivation of any skyline can be done with little effort. Even though solutions (i) areappealing because skyline queries have optimal execution time; they suffer from time andspace scalability because the number of skylines to be materialized is exponential withrespect to d. On the other hand; solutions (ii) are attractive in terms of memory consumption;but as we show; they also have a high time complexity. In this article; we make contributionsto both kinds of solutions. We first observe that skyline patterns are monotonic. This …,ACM Transactions on Database Systems (TODS),2016,4
Accelerating a Gibbs sampler for variable selection on genomics data with summarization and variable pre-selection combining an array DBMS and R,David Sergio Matusevich; Wellington Cabrera; Carlos Ordonez,Abstract Variable selection in high dimensional data is a challenging problem due to theexponential number of variable combinations; and Markov Chain Monte Carlo (MCMC)methods represent the state of the art to solve it. With genomics data this problem becomeseven more difficult because there are generally more dimensions (variables) than points(records) leading to slow convergence and numerically unstable solutions. On the otherhand; despite many alternative prototypes and languages; R remains a popular system tocompute machine learning models. Unfortunately; R can be particularly slow with heavymatrix computations and the high number of iterations required by MCMC methods.Moreover; making R scale to large matrices; possibly beyond RAM; requires careful systemintegration. Recently; array DBMSs have opened the possibility of manipulating matrices …,Machine Learning,2016,4
Recursive query evaluation in a column DBMS to analyze large graphs,Carlos Ordonez; Achyuth Gurram; Nirmala Rai,Abstract Graphs represent a major challenge on big data analytics; for which there are manysystems and prototypes; most of them not based on relational database managementsystems (DBMSs). Graph problems require substantially different algorithms compared toother analytical techniques (ie; cubes; statistical models; machine learning) and they areespecially important in the analysis of social networks and the Internet. On the other hand;recursive queries are a fundamental query mechanism to analyze graphs in a DBMS; butthey can be slow with large graphs. Column DBMSs are a novel kind of faster databasesystems; but with significantly different storage and retrieval mechanisms compared totraditional row DBMSs. Thus we study the pros and cons of optimizing recursive queries ona column DBMS. Specifically; we study two inter-related graph problems: transitive …,Proceedings of the 17th International Workshop on Data Warehousing and OLAP,2014,4
A query beehive algorithm for data warehouse buffer management and query scheduling,Amira Kerkad; Ladjel Bellatreche; Pascal Richard; Carlos Ordonez; Dominique Geniet,Abstract Analytical queries; like those used in data warehouses and OLAP; are generallyinterdependent. This is due to the fact that the database is usually modeled with adenormalized star schema or its variants; where most queries pass through a large centralfact table. Such interaction has been largely exploited in query optimization techniques suchas materialized views. Nevertheless; such approaches usually ignore buffer managementand assume queries have a fixed order and are known in advance. We believe suchassumptions are too strong and thus they need to be revisited and simplified. In this paper;we study the combination of two problems: buffer management and query scheduling; inboth static and dynamic scenarios. We present an NP-hardness study of the joint problem;highlighting its complexity. We then introduce a new and highly efficient algorithm …,International Journal of Data Warehousing and Mining (IJDWM),2014,4
Efficiently repairing and measuring replica consistency in distributed databases,Javier García-García; Carlos Ordonez; Predrag T Tosic,Abstract In a distributed database; maintaining large table replicas with frequentasynchronous insertions is a challenging problem that requires carefully managing atradeoff between consistency and availability. With that motivation in mind; we proposeefficient algorithms to repair and measure replica consistency. Specifically; we adapt; extendand optimize distributed set reconciliation algorithms to efficiently compute the symmetricdifference between replicated tables in a distributed relational database. Our novelalgorithms enable fast synchronization of replicas being updated with small sets of newrecords; measuring obsolence of replicas having many insertions and deciding when toupdate a replica; as each table replica is being continuously updated in an asynchronousmanner. We first present an algorithm to repair and measure distributed consistency on a …,Distributed and Parallel Databases,2013,4
Querying external source code files of programs connecting to a relational database,Carlos Garcia-Alvarado; Carlos Ordonez; Veerabhadran Baladandayuthapani,Abstract Multiple source code files reference metadata of an existing database.Consequently; any modification that needs to be done in the source code files or in theschema of a database requires asserting the impact of performing such change. Thisproblem of data and control dependencies between a database system and source codehas been tackled before by software engineering. Unfortunately; these solutions arecumbersome to implement by requiring a long execution time for obtaining the dependencyanalysis; and do not allow a flexible analysis of the resulting data. In this research; wepresent and formalize a novel approach for using keyword search algorithms to analyze theresulting references found between the source code and the database's schema. Our initialfindings show that our algorithms are efficient to perform a rapid integration phase; allow …,Proceedings of the 5th Ph. D. workshop on Information and knowledge,2012,4
Repairing OLAP queries in databases with referential integrity errors,Javier García-García; Carlos Ordonez,Abstract Many database applications and OLAP tools dynamically generate SQL queriesinvolving join operators and aggregate functions and send these queries to a databaseserver for execution. This dynamically generated SQL code normally assumes theunderlying tables and columns are clean and lacks the necessary robustness to deal withforeign keys with null and invalid or undefined values that are ubiquitous in databases withinconsistent or incomplete content. The outcome is that at query time; several issues arisemostly as inconsistencies in answer sets; difficult to detect and explain by users of OLAPtools. In this article; we present an automated query rewriting method for automaticallygenerated OLAP queries that are executed over tables with foreign key columns havingpotentially null or invalid values. Our method is applicable in queries that use join …,Proceedings of the ACM 13th international workshop on Data warehousing and OLAP,2010,4
Computing percentages in a database system,*,A database system includes a storage to store a table; and a controller to receive a querycontaining an aggregate function to calculate a percentage. In response to the aggregatefunction; the percentage is calculated. In one implementation; the controller calculates pluralpercentage values based on plural groups specified by a group-by clause of the query.,*,2010,4
OLAP with UDFs in digital libraries,Carlos Garcia-Alvarado; Zhibo Chen; Carlos Ordonez,Abstract Queries on digital libraries generally involve the retrieval of specific documents; butmost techniques lack the ability to efficiently explore these collections. The integration ofOLAP techniques with digital libraries allows users to navigate throughout these collectionson multiple levels. In order to accomplish this; we propose the creation of OLAP networks; acomplex data structure that contains summarized representations of the original collection ofmetadata to enrich traditional retrievals and allow the users to quickly explore the collection.We developed a system that enables OLAP-based exploration on the metadata of digitallibraries through the use of a combination of efficient UDFs and optimized SQL queries. Inaddition; we also incorporated visualization methods into our system to allow fast navigationand exploration.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,4
Microarray data analysis with PCA in a DBMS,Waree Rinsurongkawong; Carlos Ordonez,Abstract Microarray data sets contain expression levels of thousands of genes. Thestatistical analysis of such data sets is typically performed outside a DBMS with statisticalpackages or mathematical libraries. In this work; we focus on analyzing them inside theDBMS. This is a difficult problem because microarray data sets have high dimensionality; butsmall size. First; due to DBMS limitations on a maximum number of columns per table; thedata set has to be pivoted and transformed before analysis. More importantly; the correlationmatrix on tens of thousands of genes has millions of values. While most high dimensionaldata sets can be analyzed with the classical PCA method; small; but high dimensional; datasets can only be analyzed with Singular Value Decomposition (SVD). We adapt theHouseholder tridiagonalization and QR factorization numerical methods to solve SVD …,Proceedings of the 2nd international workshop on Data and text mining in bioinformatics,2008,4
Solar power prediction for smart community microgrid,Wellington Cabrera; Driss Benhaddou; Carlos Ordonez,Urban areas host more than 50% of the world's populations; are responsible for 75% ofenergy consumption in the world; and they emit almost 80% of global carbon dioxide. Thereis an urgent need to develop" low carbon" cities that are smart and efficient and userenewable energy to foster the growth of the green economy. Smart grids are beingdeveloped to tackle these challenges through integration of renewable and green energy aswell as energy efficiency. They are moving toward a concept of networked microgrids.Microgrids will enable the integration of distributed renewable energy such as roof top solarpanels within smart city communities. For these microgrids to operate reliably and efficiently;prediction algorithms are a significant because of the fluctuation of solar energy and itsdependence on weather. Prediction of energy is a component of microgrids energy …,Smart Computing (SMARTCOMP); 2016 IEEE International Conference on,2016,3
ONTOCUBO: cube-based ontology construction and exploration,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract One of the major challenges of big data analytics is the diverse information content;which has no pre-defined structure or classification. This is in contrast to the well-designedstructure of a database specified on an ER model. A standard mechanism for understandinginterrelationships and the structure of documents is using ontologies. With such motivation inmind; we present a system that enables data management and querying of documentsbased on ontologies by leveraging the functionality of the DBMS. In this paper; we presentONTOCUBO; a novel system based on our research for text summarization using ontologiesand automatic extraction of concepts for building ontologies using Online AnalyticalProcessing (OLAP) cubes. ONTOCUBO is a database-centric approach that excels in itsperformance; due to an SQL-based single pass summarization phase through the original …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,3
Data analysis based on manipulation of large matrices on a persistent storage medium,*,Matrices involved in a data analysis are stored in predetermined blocks; where blocks for afirst matrix contain respective rows of the first matrix; and blocks for a second matrix containrespective columns of the second matrix. Results for the data analysis are computed usingthe blocks of the first and second matrices.,*,2012,3
Integrating and querying web databases and documents,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract There exist many interrelated information sources on the Internet that can becategorized into structured (database) and semistructured (documents). A key challenge isto integrate; query and analyze such heterogeneous collections of information. In this paper;we defend the idea of building web metadata repositories using relational databases as themain source and central data management technology of structured data; enriched by thesemistructured data surrounding it. Our proposal rests on the assumption thatheterogeneous relational databases can be integrated (ie entity resolution is assumed towork well) and thus can serve as references for external data. That is; we tackle the problemof integrating information in the deep web; departing from databases. We discuss aprototype system that can integrate and query metadata and related documents; based …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,3
Parallel multithreaded processing for data set summarization on multicore CPUs,Carlos Ordonez; Mario Navas; Carlos Garcia-Alvarado,Abstract Data mining algorithms should exploit new hardware technologies to acceleratecomputations. Such goal is difficult to achieve in database management system (DBMS) dueto its complex internal subsystems and because data mining numeric computations of largedata sets are difficult to optimize. This paper explores taking advantage of existingmultithreaded capabilities of multicore CPUs as well as caching in RAM memory toefficiently compute summaries of a large data set; a fundamental data mining problem. Weintroduce parallel algorithms working on multiple threads; which overcome the rowaggregation processing bottleneck of accessing secondary storage; while maintaining lineartime complexity with respect to data set size. Our proposal is based on a combination oftable scans and parallel multithreaded processing among multiple cores in the CPU. We …,Journal of Computing Science and Engineering,2011,3
Evaluating join performance on relational database systems,Carlos Ordonez; Javier García-García,Abstract The join operator is fundamental in relational database systems. Evaluating joinqueries on large tables is challenging because records need to be efficiently matched basedon a given key. In this work; we analyze join queries in SQL with large tables in which aforeign key may be null; invalid or valid; given a referential integrity constraint. We conductan extensive join performance evaluation on three DBMSs. Specifically; we study joinqueries varying table sizes; row size and key probabilistic distribution; inserting null; invalidor valid foreign key values. We also benchmark three well-known query optimizations: viewmaterialization; secondary index and join reordering. Our experiments show certainoptimizations perform well across DBMSs; whereas other optimizations depend on theDBMS architecture.,Journal of Computing Science and Engineering,2010,3
Consistency-aware evaluation of OLAP queries in replicated data warehouses,Javier García-García; Carlos Ordonez,Abstract OLAP tools for distributed data warehouses generally assume underlying replicatedtables are up to date. Unfortunately; maintaining updated replicas is difficult due to theinherent tradeoff between consistency and availability. In this paper; we propose techniquesto evaluate OLAP queries in distributed data warehouses assuming a lazy replication model.Considering that it may be admissible to evaluate OLAP queries with slightly outdatedreplicated tables; our technique first efficiently computes the degree of obsolescence ofreplicated local tables and when such result is acceptable; given an error threshold; then thequery is evaluated locally; avoiding the transmission of large tables over the network.Otherwise; the query can be remotely evaluated less efficiently with the master copy oftables; provided they are stored at a single site. Inconsistency measurement is computed …,Proceedings of the ACM twelfth international workshop on Data warehousing and OLAP,2009,3
Regional Pattern Discovery in Geo-Referenced Datasets Using PCA,Oner Ulvi Celepcikay; Christoph F Eick; Carlos Ordonez,Abstract Existing data mining techniques mostly focus on finding global patterns and lack theability to systematically discover regional patterns. Most relationships in spatial datasets areregional; therefore there is a great need to extract regional knowledge from spatial datasets.This paper proposes a novel framework to discover interesting regions characterized by“strong regional correlation relationships” between attributes; and methods to analyzedifferences and similarities between regions. The framework employs a two-phaseapproach: it first discovers regions by employing clustering algorithms that maximize a PCA-based fitness function and then applies post processing techniques to explain underlyingregional structures and correlation patterns. Additionally; a new similarity measure thatassesses the structural similarity of regions based on correlation sets is introduced. We …,International Workshop on Machine Learning and Data Mining in Pattern Recognition,2009,3
DBDOC: querying and browsing databases and interrelated documents,Carlos Garcia-Alvarado; Carlos Ordonez; Zhibo Chen,Abstract Large collections of documents are commonly created around a database; where atypical database schema may contain hundreds of tables and thousands of columns. Wedeveloped a system based on SQL code generation and User-Defined Functions thatanalyzes document-to-metadata links by extracting a basic set of relationships at differentlevels of granularities: coarse; medium and fine. Such relationships are then stored andqueried in the DBMS; allowing the user to explore; query; and rank how columns and tablesare related to users and applications. At the same time; our system provides typicalinformation retrieval capabilities for querying medium-sized document collections ofinterrelated documents in the DBMS; with an acceptable performance.,Proceedings of the First International Workshop on Keyword Search on Structured Data,2009,3
A model for association rules based on clustering,Carlos Ordonez,Abstract Association rules and clustering are fundamental data mining techniques used fordifferent goals. We propose a unifying theory by proving association support and ruleconfidence can be bounded and estimated from clusters on binary dimensions. Threesupport metrics are introduced: lower; upper and average support. Three confidence metricsare proposed: lower; upper and average confidence. Clusters represent a simple model thatallows understanding and approximating association rules; instead of searching for them ina large transaction data set.,Proceedings of the 2005 ACM symposium on Applied computing,2005,3
Big data analytics integrating a parallel columnar DBMS and the R language,Yiqun Zhang; Carlos Ordonez; Wellington Cabrera,Most research has proposed scalable and parallel analytic algorithms that work outside aDBMS. On the other hand; R has become a very popular system to perform machinelearning analysis; but it is limited by main memory and single-threaded processing.Recently; novel columnar DBMSs have shown to provide orders of magnitude improvementin SQL query processing speed; preserving the parallel speedup of row-based parallelDBMSs. With that motivation in mind; we present COLUMNAR; a system integrating aparallel columnar DBMS and R; that can directly compute models on large data sets storedas relational tables. Our algorithms are based on a combination of SQL queries; user-defined functions (UDFs) and R calls; where SQL queries and UDFs compute data setsummaries that are sent to R to compute models in RAM. Since our hybrid algorithms …,Cluster; Cloud and Grid Computing (CCGrid); 2016 16th IEEE/ACM International Symposium on,2016,2
Clustering cubes with binary dimensions in one pass,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Finding aggregations of records with high dimensionality in large data warehousesis a crucial and costly task. These groups of similar records are the result of partitionsobtained with GROUP BYs. In this research; we focus on obtaining aggregations of groupsof similar records by turning the problem into efficient binary clustering of a fact table as arelaxation of a GROUP BY clause. We present an efficient window-based Incremental K-Means algorithm in a relational database system implemented as a user-defined function.This variant is based on the Incremental K-Means algorithm. The speed up is achievedthrough the computation of sufficient statistics; multithreading; efficient distance computationand sparse matrix operations. Finally; the performance of our algorithm is compared againstmultiple variants of the K-Means algorithm. Our experiments show that our incremental K …,Proceedings of the sixteenth international workshop on Data warehousing and OLAP,2013,2
A fast convergence clustering algorithm merging MCMC and EM methods,David Sergio Matusevich; Carlos Ordonez; Veerabhadran Baladandayuthapani,Abstract Clustering is a fundamental problem in statistics and machine learning; whosesolution is commonly computed by the Expectation-Maximization (EM) method; which finds alocally optimal solution for an objective function called log-likelihood. Since the surface ofthe log-likelihood function is non convex; a stochastic search with Markov Chain MonteCarlo (MCMC) methods can help escaping locally optimal solutions. In this article; we tackletwo fundamental conflicting goals: Finding higher quality solutions and achieving fasterconvergence. With that motivation in mind; we introduce an efficient algorithm that combineselements of the EM and MCMC methods to find clustering solutions that are qualitativelybetter than those found by the standard EM method. Moreover; our hybrid algorithm allowstuning model parameters and understanding the uncertainty in their estimation. The main …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,2
On Finding and Learning Effective Strategies for Complex Non-Zero-Sum Repeated Games,Predrag T Tosic; Philip C Dasler; Carlos Ordonez,Abstract We study complex non-zero-sum iterated two player games; more specifically;various strategies and their performances in iterated travelerâ s dilemma (ITD). We focus onthe relative performances of several types of parameterized strategies; where each suchstrategy type corresponds to a particular â philosophyâ on how to best predict opponentâsfuture behavior and/or entice the opponent to alter its behavior. We are particularlyinterested in adaptable; learning and/or evolving strategies that try to predict the futurebehavior of the other player; and hence optimize their own behavior in the long run. We alsostudy strategies that strive to minimize risk; as risk minimization has been recently suggestedto be the appropriate solution paradigm for ITD and several other complex games that haveposed difficulties to classical game theory. We share the key insights from an elaborate …,Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology-Volume 02,2012,2
Integrating and querying source code of programs working on a database,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Programs and a database's schema contain complex data and controldependencies that make modifying the schema along with multiple portions of the sourcecode difficult to change. In this paper; we address the problem of exploring and analyzingthose dependencies that exist between a program and a database's schema using keywordsearch techniques inside a database management system (DBMS). As a result; we presentQDPC; a novel system that allows the integration and flexible querying within a DBMS ofsource code and a database's schema. The integration focuses on obtaining theapproximate matches that exist between source files (classes; function and variable names)and the database's schema (table names and column names); and then storing them insummarization tables inside a DBMS. These summarization tables are then analyzed with …,Proceedings of the Third International Workshop on Keyword Search on Structured Data,2012,2
Dynamic optimization of generalized SQL queries with horizontal aggregations,Carlos Ordonez; Javier García-García; Zhibo Chen,Abstract SQL presents limitations to return aggregations as tables with a horizontal layout. Auser generally needs to write separate queries and data definition statements to combinetransposition with aggregation. With that motivation in mind; we introduce horizontalaggregations; a complementary class of aggregations to traditional (vertical) SQLaggregations. The SQL syntax extension is minimal and it significantly enhances theexpressive power and ease of use of SQL. Our proposed SQL extension blurs the boundarybetween row values and column names. We present a prototype query optimizer that canevaluate arbitrary nested queries combining filtering; joins and both classes of aggregations.Horizontal aggregations have many applications in ad-hoc querying; OLAP cube processingand data mining. We demonstrate query optimization of horizontal aggregations …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,2
Query recommendation in digital libraries using OLAP,Carlos Garcia-Alvarado; Carlos Ordonez; Zhibo Chen,Abstract Query suggestion is well-known to enhance the user's search for relevantdocuments. In this work; we propose a novel technique that emulates a human skill whensearching or exploring digital collections. In general; a user begins searching by providing anaïve query and then analyzes the retrieved documents in order to refine the query search.We decided to emulate this behavior by generating alternative queries using OLAP. Suchqueries are the result of performing multiple data summarizations on digital libraries; andthen generating cuboids depending on the correlation between the keywords of thecollection and the subset of keywords belonging to the previous search. Moreover; weintroduce techniques to efficiently obtain query suggestions inside the DBMS by exploitingUDFs and SQL queries.,Proceedings of the 2nd International Workshop on Keyword Search on Structured Data,2010,2
A Referential Integrity Browser for Distributed Databases.,Carlos Ordonez; Javier García-García; Rogelio Montero-Campos; Carlos Garcia-Alvarado,ABSTRACT We demonstrate a program that can inspect a distributed relational database onthe Internet to discover and quantify referential integrity issues for integration purposes. Theprogram computes data quality metrics for referential integrity at four granularity levels:database; table; column and value; going from a global to a detailed view; exhibiting specificevidence about referential errors. Two orthogonal data quality dimensions are considered:completeness and consistency. Each table is stored at one primary site and it can bereplicated at multiple sites; having foreign key references to tables at the same site or atdifferent sites. The user can choose alternative query evaluation strategies to efficientlycompute referential error metrics. Our proposal can be used in data integration; datawarehousing and data quality assurance.,WebDB,2009,2
Estimating and bounding aggregations in databases with referential integrity errors,Javier García-García; Carlos Ordonez,Abstract Database integration builds on tables coming from multiple databases by creating asingle view of all these data. Each database has different tables; columns with similarcontent across databases and different referential integrity constraints. Thus; a query in anintegrated database is likely to involve tables and columns with referential integrity errors. Ina data warehouse environment; even though the ETL processes take care of the referentialintegrity errors; in many scenarios this is generally done by including'dummy'records in thedimension tables used to relate to the fact tables with referential errors. When two tables arejoined; and aggregations are computed; the tuples with an undefined foreign key value areaggregated in a group marked as undefined effectively discarding potentially valuableinformation. With that motivation in mind; we extend aggregate functions computed over …,Proceedings of the ACM 11th international workshop on Data warehousing and OLAP,2008,2
Parallel Graph Algorithms with In-database Matrix-Vector Multiplication,Wellington Cabrera; Carlos Ordonez,Abstract Graph problems are significantly harder to solve with large graphs residing on diskcompared to main memory only. In this work; we study how to solve four important graphproblems: reachability from a source vertex; single source shortest path; weakly connectedcomponents; and PageRank. It is well known that the aforementioned algorithms can beexpressed as an iteration of matrix-vector multiplications under different semi-rings. Basedon this mathematical foundation; we show how to express the computation with standardrelational queries and then we study how to efficiently evaluate them in parallel in asharednothing architecture. We identify a common algorithmic pattern that unifies the fourgraph algorithms; considering a common mathematical foundation based on sparse matrix-vector multiplication. The net gain is that our SQL-based approach enables solving” big …,Distributed and Parallel Databases (DAPD),2017,1
Optimization of Percentage Cube Queries.,Yiqun Zhang; Carlos Ordonez; Javier García-García; Ladjel Bellatreche,ABSTRACT OLAP cubes are a powerful database technology to join tables and aggregatedata to discover interesting trends. However; OLAP cubes exhibit limitations to uncoverfractional relationships on a measure aggregated at multiple granularity levels. Oneprominent example is the percentage; an intuitive probablistic metric; used in practicallyevery analytic application. With such motivation in mind; we introduce the percentage cube;a generalized data cube that takes percentages as the target aggregated measure.Specifically; the percentage cube shows the fractional relationship on a measure in everycuboid between fact table rows grouped by a set of columns (detail individual groups) andtheir rolled-up aggregation by a subset of those grouping columns (total group). Weinroduce minimal query syntax and we carefully study query optimization to compute …,EDBT/ICDT Workshops,2017,1
Energy-Aware Query Processing on a Parallel Database Cluster Node,Amine Roukh; Ladjel Bellatreche; Nikos Tziritas; Carlos Ordonez,Abstract In the last few years; we have been seeing a significant increase in research aboutthe energy efficiency of hardware and software components in both centralized and parallelplatforms. In data centers; DBMSs are one of the major energy consumers; in which; a largeamount of data is queried by complex queries running daily. Having green nodes is a pre-condition to design an energy-aware parallel database cluster. Generally; the most existingDBMSs focus on high-performance during query optimization phase; while usually ignoringthe energy consumption of the queries. In this paper; we propose a methodology; supportedby a tool called EnerQuery; that makes nodes of parallel database clusters saving energywhen optimizing queries. To show its effectiveness; we implement our proposal on the top ofPostgreSQL DBMS query optimizer. A mathematical cost model based on a machine …,International Conference on Algorithms and Architectures for Parallel Processing,2016,1
Optimizing OLAP cube processing on solid state drives,Zhibo Chen; Carlos Ordonez,Abstract Hardware technology has improved to a point where a solid state drive (SSD) canread faster than a traditional hard disk drive (HDD). This unique ability to retrieve dataquickly combines perfectly with OLAP cube processing. In this paper; we study how toimprove performance of OLAP cube processing on SSDs. The main novelty of our work isthat we do not alter the internal subsystems of the DBMS. Instead; the DBMS treats the SSDas though it was a regular HDD. We propose optimizations for SQL queries to enhance theirperformance on SSDs. An experimental evaluation with the TPC-H database comparesperformance of our optimizations on SSDs and HDDs. We found that even though SSDshave slower write speeds than HDDs; their excellent read speed more than overcomes thislimitation.,Proceedings of the sixteenth international workshop on Data warehousing and OLAP,2013,1
Fast PCA and bayesian variable selection for large data sets based on SQL and UDFs,Mario Navas; Carlos Ordonez; V Baladandayuthapani,ABSTRACT Large amounts of data are stored in relational DBMSs. However; statisticalanalysis is frequently performed outside the DBMS using statistical tools; such as the well-known R package; leading to slow processing when data sets cannot fit in main memory andgoing through a file export bottleneck. In this article; we propose algorithms for large data setprocessing of principal component analysis (PCA) and stochastic search variable selection(SSVS) that can work entirely inside a DBMS; using SQL queries and User-DefinedFunctions (UDFs). Both of our algorithms consist of two main phases: a first phase tocompute sufficient statistics in one pass with SQL queries and a second one to derive themodel from such such sufficient statistics; in main memory with UDFs. PCA is efficientlysolved with SVD via UDFs in main memory after sufficient statistics are derived. On the …,Proc. ACM KDD Workshop on Large-scale Data Mining: Theory and Applications (LDMTA),2010,1
Migration of data mining preprocessing into the DBMS,Carlos Ordonez; Javier Garcıa-Garcıa; Michael J Rote,ABSTRACT Nowadays there is a significant amount of data mining work performed outsidethe DBMS. This article discusses recommendations to push data mining analysis into theDBMS paying attention to data preprocessing (ie data cleaning; summarization andtransformation); which tends to be the most time-consuming task in data mining projects. Wepresent a discussion of practical issues and common solutions when transforming andpreparing data sets with the SQL language for data mining purposes; based on experiencefrom real-life projects. We then discuss general guidelines to create variables (features) foranalysis. We introduce a simple prototype tool that translates statistical language programsinto SQL; focusing on data manipulation statements. Based on experience from successfulprojects; we present actual time performance comparisons running SQL code inside the …,Data Mining Case Studies,2009,1
Bounding and estimating association rule support from clusters on binary data,Carlos Ordonez; Kai Zhao; Zhibo Chen,The theoretical relationship between association rules and machine learning techniquesneeds to be studied in more depth. This article studies the use of clustering as a model forassociation rule mining. The clustering model is exploited to bound and estimate associationrule support and confidence. We first study the efficient computation of the clustering modelwith K-means; we show the sufficient statistics for clustering on binary data sets is the linearsum of points. We then prove itemset support can be bounded and estimated from themodel. Finally; we show support bounds fulfill the set downward closure property.Experiments study model accuracy and algorithm speed; paying particular attention to errorbehavior in support estimation. Given a sufficiently large number of clusters; the modelbecomes fairly accurate to approximate support. However; as the minimum support …,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,1
Data mining of large myocardial perfusion SPECT (MPS) databases: Validation of expert system rule confidences.,CD Cooke; CA Santana; TI Morris; L DeBraal; C Ordonez; E Omiecinski; NF Ezquerra; EV Garcia,*,Journal of Nuclear Medicine,2000,1
Mining complex databases using the EM algorithm,Carlos Ordonez,Abstract This thesis focused on developing efficient data mining algorithms based on theExpectation-Maximization (EM) algorithm. The EM algorithm is a general numericaloptimization method that solves many important statistical problems. In particular it can beused to perform clustering; and that is the aspect this work concentrated on. An algorithm tomine association rules from a collection of images segmented by EM was introduced.Experiments with synthetic images show the algorithm is reasonably accurate and fast. Thenthe problem of programming EM inside a relational DBMS was studied. Three efficient waysto implement the EM algorithm in the SQL language and important improvements wereproposed. Then this work studied the general problem of clustering large data sets with highdimensionality. A fast and robust EM clustering algorithm was proposed for that purpose …,*,2000,1
Quantum anomaly and thermodynamics of one-dimensional fermions with three-body interactions,Joaquín E Drut; Joshua R McKenney; Wilder S Daza; Chris L Lin; Carlos R Ordóñez,Abstract: We show that a system of three species of one-dimensional fermions; with anattractive three-body contact interaction; features a scale anomaly directly related to theanomaly of two-dimensional fermions with two-body forces. We show; furthermore; thatthose two cases (and their multi species generalizations) are the only non-relativisticsystems with contact interactions that display a scale anomaly. While the two-dimensionalcase is well-known and has been under study both experimentally and theoretically foryears; the one-dimensional case presented here has remained unexplored. For the latter;we calculate the impact of the anomaly on the equation of state; which appears through thegeneralization of Tan's contact for three-body forces; and determine the pressure at finitetemperature. In addition; we show that the third-order virial coefficient is proportional to …,arXiv preprint arXiv:1802.01634,2018,*
10. Corrigendum to``The Social Relation Key: A new paradigm for security’'[Information Systems 71 (2017) 68–77],Sihyun Jeong; Jaehoon Lee; Junhyun Park; Chong-kwon Kim; Yiqun Zhang; Carlos Ordonez; Javier García-García; Ladjel Bellatreche; Humberto Carrillo; Johannes De Smedt; Jochen De Weerdt; Estefanía Serral; Jan Vanthienen; Chiara Di Francescomarino; Marlon Dumas; Marco Federici; Chiara Ghidini; Luca Simonetto; Richard Connor; Lucia Vadicamo; Franco Alberto Cardillo; Fausto Rabitti; Han van der Aa; Henrik Leopold; Hajo A Reijers; Ryan R Curtin; Javier Echauz; Andrew B Gardner; Michael Borkowski; Walid Fdhila; Matteo Nardelli; Stefanie Rinderle-Ma; Stefan Schulte; Mahmood Hosseini; Alimohammad Shahri; Keith Phalp; Raian Ali; Alfredo Bolt; Massimiliano de Leoni; Wil MP van der Aalst; Imen Bizid; Nibal Nayef; Patrice Boursier; Antoine Doucet,Accepted manuscripts: articles that have been peer reviewed and accepted for publicationby the editorial board. They have not yet been copy edited and/or formatted in thepublication house style; and may not yet have full ScienceDirect functionality; eg;supplementary files may still need to be added; links to references may not resolve yet;etc.Uncorrected proofs: articles that have been copy edited and formatted; but have not beenfinalized yet. They still need to be proof-read and corrected by the author (s) and the textcould still change before final publication.,Information Systems,2018,*
The percentage cube,Yiqun Zhang; Carlos Ordonez; Javier García-García; Ladjel Bellatreche; Humberto Carrillo,Abstract OLAP cubes provide exploratory query capabilities combining joins andaggregations at multiple granularity levels. However; cubes cannot intuitively or directlyshow the relationship between measures aggregated at different grouping levels. Oneprominent example is the percentage; which is widely used in most analytical applications.Considering this limitation; we introduce percentage cube as a generalized data cube thattakes percentages as its basic measure. More precisely; a percentage cube shows thefractional relationship in every cuboid between each aggregated measure on severaldimensions and its rolled-up measure aggregated by fewer dimensions. We propose thesyntax and introduce query optimizations to materialize the percentage cube. We justify thatpercentage cubes are significantly harder to evaluate than standard data cubes because …,Information Systems,2018,*
Scalable parallel graph algorithms with matrix–vector multiplication evaluated with queries,Wellington Cabrera; Carlos Ordonez,Abstract Graph problems are significantly harder to solve with large graphs residing on diskcompared to main memory only. In this work; we study how to solve four important graphproblems: reachability from a source vertex; single source shortest path; weakly connectedcomponents; and PageRank. It is well known that the aforementioned algorithms can beexpressed as an iteration of matrix–vector multiplications under different semi-rings. Basedon this mathematical foundation; we show how to express the computation with standardrelational queries and then we study how to efficiently evaluate them in parallel in a shared-nothing architecture. We identify a common algorithmic pattern that unifies the four graphalgorithms; considering a common mathematical foundation based on sparse matrix–vectormultiplication. The net gain is that our SQL-based approach enables solving “big data” …,Distributed and Parallel Databases,2017,*
Predicting Student Success: A Naïve Bayesian Application to Community College Data,Fermin Ornelas; Carlos Ordonez,Abstract This research focuses on developing and implementing a continuous NaïveBayesian classifier for GEAR courses at Rio Salado Community College. Previousimplementation efforts of a discrete version did not predict as well; 70%; and haddeployment issues. This predictive model has higher prediction; over 90%; accuracy for bothat-risk and successful students while easing interpretation and implementation. Predictiveresults across eleven courses and cumulative gain charts show potential improvements tobe made in students' academic success by focusing on high level risk students. Researchersat other colleges might find this empirical application relevant for implementation of earlyalert systems.,Technology; Knowledge and Learning,2017,*
A Cloud System for Machine Learning Exploiting a Parallel Array DBMS,Yiqun Zhang; Carlos Ordonez; Lennart Johnsson,Computing machine learning models in the cloud remains a central problem in big dataanalytics. In this work; we introduce a cloud analytic system exploiting a parallel array DBMSbased on a classical shared-nothing architecture. Our approach combines in-DBMS datasummarization with mathematical processing in an external program. We study how tosummarize a data set in parallel assuming a large number of processing nodes and how tofurther accelerate it with GPUs. In contrast to most big data analytic systems; we do not useJava; HDFS; MapReduce or Spark: our system is programmed in C++ and C on top of atraditional Unix le system. In our system; models are ef ciently computed using a suite ofinnovative parallel matrix operators; which compute comprehensive statistical summaries ofa large input data set (matrix) in one pass; leaving the remaining mathematically complex …,Database and Expert Systems Applications (DEXA); 2017 28th International Workshop on,2017,*
A Tool for Statistical Analysis on Network Big Data,Carlos Ordonez; Theodore Johnson; Divesh Srivastava; Simon Urbanek,Due to advances in parallel file systems for big data (ie HDFS) and larger capacity hardware(multicore CPUs; large RAM) it is now feasible to manage and query network data in aparallel DBMS supporting SQL; but performing statistical analysis remains a challenge. Onthe statistics side; the R language is popular; but it presents important limitations: R is limitedby main memory; R works in a different address space from query processing; R cannotanalyze large disk-resident data sets efficiently; and R has no data managementcapabilities. Moreover; some R libraries allow R to work in parallel; but without datamanagement capabilities. Considering the challenges and limitations described above; wepresent a system that allows combining SQL queries and R functions in a seamless manner.We justify a parallel DBMS and the R runtime are two different systems that benefit from a …,Database and Expert Systems Applications (DEXA); 2017 28th International Workshop on,2017,*
Boolean Network Models of Collective Dynamics of Open and Closed Large-Scale Multi-agent Systems,Predrag T Tošić; Carlos Ordonez,Abstract This work discusses theoretical models of decentralized large-scale cyber-physicaland other types of multi-agent systems (MAS). Arguably; various types of Boolean Networksare among the simplest such models enabling rigorous mathematical and computationalanalysis of the emerging behavior of such systems and their collective dynamics. This paperinvestigates determining possible asymptotic dynamics of several classes of BooleanNetworks (BNs) such as Discrete Hopfield Networks; Sequential and SynchronousDynamical Systems; and (finite; Boolean-valued) Cellular Automata. Viewing BNs as anabstraction for a broad variety of decentralized cyber-physical; computational; biological;social and socio-technical systems; similarities and differences between open and closedsuch systems are rigorously analyzed. Specifically; this paper addresses the problem of …,International Conference on Industrial Applications of Holonic and Multi-Agent Systems,2017,*
Integrating the R Language Runtime System with a Data Stream Warehouse,Carlos Ordonez; Theodore Johnson; Simon Urbanek; Vladislav Shkapenyuk; Divesh Srivastava,Abstract Computing mathematical functions or machine learning models on data streams isdifficult: a popular approach is to use the R language. Unfortunately; R has importantlimitations: a dynamic runtime system incompatible with a DBMS; limited by available RAMand no data management capabilities. On the other hand; SQL is well established to writequeries and manage data; but it is inadequate to perform mathematical computations. Withthat motivation in mind; we present a system that enables analysis in R on a time window;where the DBMS continuously inserts new records and propagates updates to materializedviews. We explain the low-level integration enabling fast data transfer in RAM between theDBMS query process and the R runtime. Our system enables analytic calls in bothdirections:(1) R calling SQL to evaluate streaming queries; transferring output streaming …,International Conference on Database and Expert Systems Applications,2017,*
Special issue on DOLAP 2015: Evolving data warehousing and OLAP cubes to big data analytics,Carlos Ordonez; Carlos Garcia-Alvarado; Il-Yeol Song; Francesco Cafagna; Michael H Böhlen; Annelies Bracher; Rudra Pratap Deb Nath; Katja Hose; Torben Bach Pedersen; Oscar Romero; Amine Roukh; Ladjel Bellatreche; Selma Bouarar; Ahcene Boukorca,We welcome the reader to the special issue containing best papers from the ACMInternational Workshop on Data Warehousing and OLAP (DOLAP) 2015. We haverevamped DOLAP to be a research venue for big data analytics; expanding its scope; butmaintaining its high quality and applied focus.,*,2017,*
EIC Editorial,Jian Pei,AS the number of submissions to the IEEE Transactions on Knowledge and DataEngineering (TKDE) and the diversity in topics keep increasing; TKDE needs fresh bloodand strong hands. I am pleased to officially welcome the 13 associate editors who just joinedthe editorial board: Drs. Leman Akoglu; Hongrae Lee; Justin Levandoski; Xuelong Li; RosaMeo; Carlos Ordonez; Jeff Philips; Barbara Poblete; K. Selçuk Candan; Meng Wang; JirongWen; Li Xiong; and Wenjie Zhang. This group of newly appointed associate editors areestablished and active working experts in the wonderful wide spectrum of knowledge anddata engineering. Moreover; they are very committed and dedicated to serving thecommunity and handling the review processes; as testified by their rich experience. Theirbiographies and photos are provided below. At the same time; I want to sincerely thank …,IEEE Transactions on Knowledge and Data Engineering,2016,*
Managing Big Data Analytics Workflows with a Database System,Carlos Ordonez; Javier García-García,A big data analytics workflow is long and complex; with many programs; tools and scriptsinteracting together. In general; in modern organizations there is a significant amount of bigdata analytics processing performed outside a database system; which creates many issuesto manage and process big data analytics workflows. In general; data preprocessing is themost time-consuming task in a big data analytics workflow. In this work; we defend the ideaof preprocessing; computing models and scoring data sets inside a database system. Inaddition; we discuss recommendations and experiences to improve big data analyticsworkflows by pushing data preprocessing (ie data cleaning; aggregation and columntransformation) into a database system. We present a discussion of practical issues andcommon solutions when transforming and preparing data sets to improve big data …,Cluster; Cloud and Grid Computing (CCGrid); 2016 16th IEEE/ACM International Symposium on,2016,*
Time Complexity and Parallel Speedup to Compute the Gamma Summarization Matrix.,Carlos Ordonez; Yiqun Zhang,Abstract. We study the serial and parallel computation of Γ (Gamma); a comprehensive datasummarization matrix for linear Gaussian models; widely used in big data analytics.Computing Gamma can be reduced to a single matrix multiplication with the data set; wheresuch multiplication can be evaluated as a sum of vector outer products; which enablesincremental and parallel computation; essential features for scalable computation. Byexploiting Gamma; iterative algorithms are changed to work in two phases:(1) Incremental-parallel data set summarization (ie in one scan and distributive);(2) Iteration in main memoryexploiting the summarization matrix in intermediate matrix computations (ie reducingnumber of scans). Most intermediate computations on large matrices collapse tocomputations based on Gamma; a much smaller matrix. We present specialized database …,AMW,2016,*
DOLAP 2015 Workshop Summary,Carlos Garcia-Alvarado; Carlos Ordonez; Il-Yeol Song,Abstract The ACM DOLAP workshop presents research that bridges data warehousing; On-Line Analytical Processing (OLAP); and other large-scale data processing platforms. Theprogram has four interesting sessions on data warehouse design; database modeling; queryprocessing; and text processing; as well as an invited paper on Big Data Database Design.,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,*
Clustering binary cube dimensions to compute relaxed GROUP BY aggregations,Carlos Garcia-Alvarado; Carlos Ordonez,Abstract Computing cube aggregations on large data sets with high dimensionality is acrucial and costly task that normally requires multiple passes on the input table. This taskgets harder when the number of result groups increases due to a large number ofcombinations of dimension values. In this research; we focus on reducing the number ofaggregations and providing a more succinct result by deriving aggregations on top of groupswith similar records exploiting an efficient binary clustering of the fact table; which can beviewed as a relaxation of traditional OLAP cubes. We present an efficient window-basedIncremental K-Means algorithm implemented in a DBMS as a user-defined function. Asignificant speedup is achieved through sufficient statistics; multithreading; efficient distancecomputation and sparse matrix operations. Our algorithm performance is experimentally …,Information Systems,2015,*
Dépendances fonctionnelles et requêtes skyline multidimensionnelles,Nicolas Hanusse; Patrick Kamnang Wanko; Sofian Maabout; Carlos Ordonez,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,Revue des Sciences et Technologies de l'Information-Série ISI: Ingénierie des Systèmes d'Information,2015,*
Les Dépendances Fonctionnelles pour l’Optimisation des Requêtes Skyline Multi-dimensionnelle,Nicolas Hanusse; Patrick Kamnang Wanko; Sofian Maabout; Carlos Ordonez,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,BDA: Bases de Données Avancées,2014,*
Staff,Divesh Srivastava,For close to 50 years; Neil J. Sloane has been collecting and cataloguing integersequences. The result is The Online Encyclopedia of Integer Sequences (OEIS); acomprehensive; constantly updated repository of 200;000+ sequences and one of the firstlarge-scale exercises in crowd-sourcing to expand mathematical and scientificinformation.The OEIS; for years hosted on AT&T Research servers; is now under thestewardship of The OEIS Foundation; which is charged with maintaining the OEIS as a freeand open resource to the world community. Read more,Journal of Computational Methods in Science and Engineering,2013,*
Discovering frequent pattern pairs,Carlos Ordonez; Zhibo Chen,Abstract Cubes and association rules discover frequent patterns in a data set; most of whichare not significant. Thus previous research has introduced search constraints and statisticalmetrics to discover significant patterns and reduce processing time. We introduce cube pairs(comparing cube groups based on a parametric statistical test) and rule pairs (based on twosimilar association rules); which are pattern pair generalizations of cubes and associationrules; respectively. We introduce algorithmic optimizations to discover comparable patternsets. We carefully study why both techniques agree or disagree on the validity of specificpairs; considering p-value for statistical tests; as well as confidence for association rules. Inaddition; we analyze the probabilistic distribution of target attributes given confidencethresholds. We also introduce a reliability metric based on cross-validation; which …,Intelligent Data Analysis,2013,*
Numerical Linear Algebra in a Database System for Big Data Analytics,Carlos Ordonez,Linear algebra is behind most scientific analytic computations; including numerical methods;statistics; physics and data mining. On the other hand; database systems provide extensivefeatures to manage and query data; but not to analyze it. The integration of linear algebraalgorithms into a database system is difficult due to the relational model discrete setfoundation; which in consequence hinders efficient array processing. In this talk; we discussour progress towards solving two prominent linear algebra problems on large data sets:singular value decomposition and least squares regression. We present a common matrixfoundation to summarize large data sets with an outer product and exploiting suchsummarization we discuss how to efficiently solve linear algebra problems with iterativemethods. We explain optimization of data movement going from secondary storage to …,*,2012,*
Special Issue of DOLAP 2010 Information Systems,Carlos Ordonez; Il-Yeol Song,*,*,2012,*
Reducción de dimensionalidad sobre grandes cantidades de datos,Carlos Ordoñez; René Mac Kinney Romero UAMI,*,*,2011,*
DOLAP 2010 workshop summary,Carlos Ordonez; Il-Yeol Song,Abstract The ACM DOLAP workshop presents research on data warehousing and On-LineAnalytical Processing (OLAP). The program has three interesting sessions on modeling;query processing and new trends; as well as a keynote talk on OLAP query processing anda panel comparing relational and non-relational technology for data warehousing.,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,*
Using data mining techniques to improve the accuracy of interpreting myocardial perfusion SPECT studies by an expert system.,CD Cooke; CA Santana; L Debraal; C Ordonez; E Omiecinski; EG Krawczynska; NF Ezquerra; EV Garcia,*,JOURNAL OF NUCLEAR MEDICINE,2001,*
Projection Signature Indexing for Compressed Data Warehouses,Akshai Mirchandani; Helen Thomas; Anindya Datta; Krithi Ramamrithamy; Carlos Ordonez; Sham Navathe,Abstract The large size of most data warehouses (typically hundreds of gigabytes toterabytes); which results in non-trivial storage costs; makes compression techniquesattractive for warehousing environments. For the most part; page-level compression (asopposed to attribute or record level schemes) has been shown to achieve the greatestreductions in storage size for databases. A key issue with such schemes is how to quicklyaccess the data to answer queries; since individual tuples boundaries are lost. In this paperwe introduce an approach that aims to maintain the bene ts of page-level compression (ie;large reductions in storage size); while at the same time improving query performancethrough an e cient signature le indexing scheme. The approach uses an attribute-levelsignature generation method that exploits the value distribution of each attribute in a data …,*,1999,*
MathMOuse: A Mathematical MOdels WarehoUSE to handle both Theoretical and Numerical Data,Cyrille Ponchateau; Ladjel Bellatreche; Carlos Ordonez; Mickael Baron,ABSTRACT The evolution of the numeric technologies triggered an increase in the volumeof data to process; along with the technical solutions to handle those volumes. Time seriesprocessing is not an exception; since they are widely used in many fields such as finance;medicine or physics. Our studied domain concerns the experimental science; in general;and automatic control in particular; which intensively uses time series data. In such domains;the numerical data comes from the observations of a physical system; usually capturedusing sensors. The data are then processed to find a mathematical model (usually adifferential equation); which models the system dynamic behavior. Due to the volume ofavailable data and technologies to process them; the number of the available mathematicalmodels is increasing as well. Therefore; storing and organizing those models to ease …,*,*,*
Predicting Student Success An Application to Community College Data,Fermin Ornelas; Carlos Ordonez; Daniel Huston,Academic institutions today face several challenges driven by cost concerns; increasingaccountability; and diminishing resources. For instance; a recent report by the Center onBudgets and Policy Priorities (2013) assessed state financial cuts to higher education for thefiscal years 2008-2013. Among its main findings were: all states except for North Dakota andWyoming saw severe reductions in higher education funding; eleven states cut educationalfunding by more than a third; thirty six states shrank funding by more than 20%; and Arizonaand New Hampshire occupied first and second place in the list among those states;decreasing their funding to higher education by 50%. Meanwhile; graduation rates havebeen stagnant for at least twenty years; however according to a recent New York Timesreport (2013) that trend has improved in the last five years. Furthermore; there is an …,*,*,*
Big Data; Analytical Data Platforms and Data Science/Expert Articles,Carlos Ordonez,It is now folklore that row DBMSs will dominate transaction processing and columnar DBMSwill be prerred to evaluate decision support queries [1]. The main reason is that columnarDBMSs provide orders of magnitude improvement in SQL query processing speed;preserving the parallel speedup of row-based parallel DBMSs. However; it is unclear ifcolumnar DBMSs can help with more complex analytical tasks; involving mathematicalprocessing; like linear algebra; convex optimization; machine learning and multivariatestatistics [3].In the DBMS world it is fair to say row; column and arrays are currently the maintechnologies to process structured data (tables or arrays). However; column; row and arrayDBMSs have fundamentally different storage mechanisms; which lead to different queryprocessing algorithms and optimizations. Compressed storage; a standard feature in …,*,*,*
Efficient Linear Algebra Processing Inside a Database Management System,Carlos Garcia-Alvarado; Carlos Ordonez; Naveen Mohanam; Edgar Martinez-Encarnacion; Predrag T Tosic,*,*,*,*
Fast Bayesian Variable Selection Algorithms for High Dimensional Genomics Data,Wellington Cabrera; Carlos Ordonez; David Sergio Matusevich; Veerabhadran Baladandayuthapani,*,*,*,*
