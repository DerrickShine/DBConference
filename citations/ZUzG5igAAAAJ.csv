Characterizing and selecting fresh data sources,Theodoros Rekatsinas; Xin Luna Dong; Divesh Srivastava,Abstract Data integration is a challenging task due to the large numbers of autonomous datasources. This necessitates the development of techniques to reason about the benefits andcosts of acquiring and integrating data. Recently the problem of source selection (ie;identifying the subset of sources that maximizes the profit from integration) was introducedas a preprocessing step before the actual integration. The problem was studied for staticsources and used the accuracy of data fusion to quantify the integration profit. In this paper;we study the problem of source selection considering dynamic data sources whose contentchanges over time. We define a set of time-dependent metrics; including coverage;freshness and accuracy; to characterize the quality of integrated data. We show howstatistical models for the evolution of sources can be used to estimate these metrics …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,46
HawkesTopic: A Joint Model for Network Inference and Topic Modeling from Text-Based Cascades,Xinran He; Theodoros Rekatsinas; James Foulds; Lise Getoor; Yan Liu,Abstract Understanding the diffusion of information in social networks and social mediarequires modeling the text diffusion process. In this work; we develop the HawkesTopicmodel (HTM) for analyzing text-based cascades; such as “retweeting a post” or “publishing afollow-up blog post.” HTM combines Hawkes processes and topic modeling tosimultaneously reason about the information diffusion pathways and the topicscharacterizing the observed textual information. We show how to jointly infer them with amean-field variational inference algorithm and validate our approach on both synthetic andreal-world data sets; including a news media dataset for modeling information diffusion; andan ArXiv publication dataset for modeling scientific influence. The results show that HTM issignificantly more accurate than several baselines for both tasks.,*,2015,33
Finding Quality in Quantity: The Challenge of Discovering Valuable Sources for Integration.,Theodoros Rekatsinas; Xin Luna Dong; Lise Getoor; Divesh Srivastava,ABSTRACT Data is becoming a commodity of tremendous value for many domains. This isleading to a rapid increase in the number of data sources and public access data services;such as cloud-based data markets and data portals; that facilitate the collection; publishingand trading of data. Data sources typically exhibit wide variety and heterogeneity in thetypes or schemas of the data they provide; their quality; and the fees they charge foraccessing their data. Users who want to build upon such publicly available data; must (i)discover sources that are relevant to their applications;(ii) identify sources that collectivelysatisfy the quality and budget requirements of their applications; with few effective cluesabout the quality of the sources; and (iii) repeatedly invest many person-hours in assessingthe eventual usefulness of data sources. All three steps require investigating the content …,CIDR,2015,31
Multi-relational learning using weighted tensor decomposition with modular loss,Ben London; Theodoros Rekatsinas; Bert Huang; Lise Getoor,Abstract: We propose a modular framework for multi-relational learning via tensordecomposition. In our learning setting; the training data contains multiple types ofrelationships among a set of objects; which we represent by a sparse three-mode tensor.The goal is to predict the values of the missing entries. To do so; we model each relationshipas a function of a linear combination of latent factors. We learn this latent representation bycomputing a low-rank tensor decomposition; using quasi-Newton optimization of a weightedobjective function. Sparsity in the observed data is captured by the weighted objective;leading to improved accuracy when training data is limited. Exploiting sparsity also improvesefficiency; potentially up to an order of magnitude over unweighted approaches. In addition;our framework accommodates arbitrary combinations of smooth; task-specific loss …,arXiv preprint arXiv:1303.1733,2013,22
SourceSeer: Forecasting Rare Disease Outbreaks Using Multiple Data Sources,Theodoros Rekatsinas; Saurav Ghosh; Sumiko R Mekaru; Elaine O Nsoesie; John S Brownstein; Lise Getoor; Naren Ramakrishnan,Abstract Rapidly increasing volumes of news feeds from diverse data sources; such asonline newspapers; Twitter and online blogs are proving to be extremely valuable resourcesin helping anticipate; detect; and forecast outbreaks of rare diseases. This paper presentsSourceSeer; a novel algorithmic framework that combines spatio-temporal topic models withsourcebased anomaly detection techniques to effectively forecast the emergence andprogression of infectious rare diseases. SourceSeer is capable of discovering the locationfocus of each source allowing sources to be used as experts with varying degrees ofauthoritativeness. To fuse the individual source predictions into a final outbreak predictionwe employ a multiplicative weights algorithm taking into account the accuracy of eachsource. We evaluate the performance of SourceSeer using incidence data for hantavirus …,Timeline,2015,19
SPARSI: partitioning sensitive data amongst multiple adversaries,Theodoros Rekatsinas; Amol Deshpande; Ashwin Machanavajjhala,Abstract We present SPARSI; a novel theoretical framework for partitioning sensitive dataacross multiple non-colluding adversaries. Most work in privacy-aware data sharing hasconsidered disclosing summaries where the aggregate information about the data ispreserved; but sensitive user information is protected. Nonetheless; there are applications;including online advertising; cloud computing and crowdsourcing markets; where detailedand fine-grained user data must be disclosed. We consider a new data sharing paradigmand introduce the problem of privacy-aware data partitioning; where a sensitive dataset mustbe partitioned among k untrusted parties (adversaries). The goal is to maximize the utilityderived by partitioning and distributing the dataset; while minimizing the total amount ofsensitive information disclosed. The data should be distributed so that an adversary …,Proceedings of the VLDB Endowment,2013,13
Holoclean: Holistic data repairs with probabilistic inference,Theodoros Rekatsinas; Xu Chu; Ihab F Ilyas; Christopher Ré,Abstract We introduce HoloClean; a framework for holistic data repairing driven byprobabilistic inference. HoloClean unifies qualitative data repairing; which relies on integrityconstraints or external data sources; with quantitative data repairing methods; whichleverage statistical properties of the input data. Given an inconsistent dataset as input;HoloClean automatically generates a probabilistic program that performs data repairing.Inspired by recent theoretical advances in probabilistic inference; we introduce a series ofoptimizations which ensure that inference over HoloClean's probabilistic model scales toinstances with millions of tuples. We show that HoloClean finds data repairs with an averageprecision of∼ 90% and an average recall of above∼ 76% across a diverse array of datasetsexhibiting different types of errors. This yields an average F1 improvement of more than 2 …,Proceedings of the VLDB Endowment,2017,9
Local structure and determinism in probabilistic databases,Theodoros Rekatsinas; Amol Deshpande; Lise Getoor,Abstract While extensive work has been done on evaluating queries over tuple-independentprobabilistic databases; query evaluation over correlated data has received much lessattention even though the support for correlations is essential for many natural applicationsof probabilistic databases; eg; information extraction; data integration; computer vision; etc.In this paper; we develop a novel approach for efficiently evaluating probabilistic queriesover correlated databases where correlations are represented using a factor graph; a classof graphical models widely used for capturing correlations and performing statisticalinference. Our approach exploits the specific values of the factor parameters and thedeterminism in the correlations; collectively called local structure; to reduce the complexity ofquery evaluation. Our framework is based on arithmetic circuits; factorized …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,7
Fuzzy rule based neuro-dynamic programming for mobile robot skill acquisition on the basis of a nested multi-agent architecture,John N Karigiannis; Theodoros I Rekatsinas; Costas S Tzafestas,Biologically inspired architectures that mimic the organizational structure of living organismsand in general frameworks that will improve the design of intelligent robots attract significantattention from the research community. Self-organization problems; intrinsic behaviors aswell as effective learning and skill transfer processes in the context of robotic systems havebeen significantly investigated by researchers. Our work presents a new framework ofdevelopmental skill learning process by introducing a hierarchical nested multi-agentarchitecture. A neuro-dynamic learning mechanism employing function approximators in afuzzified state-space is utilized; leading to a collaborative control scheme among thedistributed agents engaged in a continuous space; which enables the multi-agent system tolearn; over a period of time; how to perform sequences of continuous actions in a …,Robotics and Biomimetics (ROBIO); 2010 IEEE International Conference on,2010,5
SLiMFast: Guaranteed results for data fusion and source reliability,Theodoros Rekatsinas; Manas Joglekar; Hector Garcia-Molina; Aditya Parameswaran; Christopher Ré,Abstract We focus on data fusion; ie; the problem of unifying conflicting data from datasources into a single representation by estimating the source accuracies. We proposeSLiMFast; a framework that expresses data fusion as a statistical learning problem overdiscriminative probabilistic models; which in many cases correspond to logistic regression.In contrast to previous approaches that use complex generative models; discriminativemodels make fewer distributional assumptions over data sources and allow us to obtainrigorous theoretical guarantees. Furthermore; we show how SLiMFast enables incorporatingdomain knowledge into data fusion; yielding accuracy improvements of up to 50% over state-of-the-art baselines. Building upon our theoretical results; we design an optimizer thatobviates the need for users to manually select an algorithm for learning SLiMFast's …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,4
Crowdgather: Entity extraction over structured domains,Theodoros Rekatsinas; Amol Deshpande; Aditya Parameswaran,Abstract: Crowdsourced entity extraction is often used to acquire data for many applications;including recommendation systems; construction of aggregated listings and directories; andknowledge base construction. Current solutions focus on entity extraction using a singlequery; eg; only using" give me another restaurant"; when assembling a list of all restaurants.Due to the cost of human labor; solutions that focus on a single query can be highlyimpractical. In this paper; we leverage the fact that entity extraction often focuses on {\emstructured domains}; ie; domains that are described by a collection of attributes; eachpotentially exhibiting hierarchical structure. Given such a domain; we enable a richer spaceof queries; eg;" give me another Moroccan restaurant in Manhattan that does takeout".Naturally; enabling a richer space of queries comes with a host of issues; especially since …,arXiv preprint arXiv:1502.06823,2015,4
StoryPivot: comparing and contrasting story evolution,Anja Gruenheid; Donald Kossmann; Theodoros Rekatsinas; Divesh Srivastava,Abstract As the world evolves around us; so does the digital coverage of it. Events of diversetypes; associated with different actors and various locations; are continuously captured bymultiple information sources such as news articles; blogs; social media etc. day by day. Inthe digital world; these events are represented through information snippets that containinformation on the involved entities; a description of the event; when the event occurred; etc.In our work; we observe that events (and their corresponding digital representations) areoften inter-connected; ie; they form stories which represent evolving relationships betweenevents over time. Take as an example the plane crash in Ukraine in July 2014 whichinvolved multiple entities such as" Ukraine";" Malaysia"; and" Russia" and multiple eventsranging from the actual crash to the incident investigation and the presentation of the …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,3
Developmental learning of cooperative robot skills: A hierarchical multi-agent architecture,John N Karigiannis; Theodoros Rekatsinas; Costas S Tzafestas,Abstract Research activities targeting new methodologies; architectures and in generalframeworks that will improve the design of intelligent robots attract significant attention fromthe research community. Self-organization problems; intrinsic behaviors as well as effectivelearning; and skill transfer processes in the context of robotic systems have beensignificantly investigated by researchers. This chapter presents a new framework ofdevelopmental skill learning process by introducing a hierarchical multi-agent architecture.More specifically; the methodology proposed is based on using reinforcement learning (RL)techniques in a fuzzified state-space; leading to a collaborative control scheme among theagents engaged in a continuous space; which enables the multi-agent system to learn; overa period of time; how to perform sequences of continuous actions in a cooperative …,*,2011,3
Sourcesight: enabling effective source selection,Theodoros Rekatsinas; Amol Deshpande; Xin Luna Dong; Lise Getoor; Divesh Srivastava,Abstract Recently there has been a rapid increase in the number of data sources and dataservices; such as cloud-based data markets and data portals; that facilitate the collection;publishing and trading of data. Data sources typically exhibit large heterogeneity in the typeand quality of data they provide. Unfortunately; when the number of data sources is large; itis difficult for users to reason about the actual usefulness of sources for their applicationsand the trade-offs between the benefits and costs of acquiring and integrating sources. Inthis demonstration we present\textsc {SourceSight}; a system that allows users tointeractively explore a large number of heterogeneous data sources; and discover valuablesets of sources for diverse integration tasks.\textsc {SourceSight}~ uses a novel multi-levelsource quality index that enables effective source selection at different granularity levels …,Proceedings of the 2016 International Conference on Management of Data,2016,2
Forecasting rare disease outbreaks with spatio-temporal topic models,Saurav Ghosh; Theodoros Rekatsinas; Sumiko R Mekaru; Elaine O Nsoesie; John S Brownstein; Lise Getoor; Naren Ramakrishnan,Abstract Rapidly increasing volumes of news; tweets; and blogs are proving to be extremelyvaluable resources in helping anticipate; detect; and forecast significant societal events. Inthis paper; we focus on the problem of forecasting rare disease outbreaks and demonstratehow spatio-temporal topic models over health-related newspaper articles can successfullybe used to forecast outbreaks. More precisely; we present a novel framework that integratestopic models with one-class SVMs; so that modeling the underlying topic evolution andforecasting its prominence can be used as a surrogate for making near-term predictions ofdisease outbreaks. We demonstrate the effectiveness of our proposed technique usingincidence data for Hantavirus in multiple countries of Latin America.,NIPS 2013 workshop on Topic Models. Citeseer,2013,2
Multi-relational weighted tensor decomposition,Ben London; Theodoros Rekatsinas; Bert Huang; Lise Getoor,There has recently been a growing interest in tensor methods within the machine learningcommunity [1; 2; 3; 4; 5; 6; 7; 9]; partially due to their natural representation of multi-relationaldata. Multi-relational data appears in applications such as social network analysis; wherelinks between individuals may be personal; familial; or professional. In this paper; weexamine a multi-relational learning scenario in which the learner is given a small trainingset; sampled from the set of all potential pairwise relationships; and aims to performtransductive inference on the remaining; unknown relationships. The target relations weconsider may be binary-; discrete ordinal-or real-valued functions of the object pairs. Tomodel this data; we propose a tensor decomposition method that is natural for multi-relational data; and produces more accurate predictions using minimal training data. Our …,NIPS Workshop on Spectral Learning,2012,2
Forecasting rare disease outbreaks from open source indicators,Theodoros Rekatsinas; Saurav Ghosh; Sumiko R Mekaru; Elaine O Nsoesie; John S Brownstein; Lise Getoor; Naren Ramakrishnan,Background Rapidly increasing volumes of news feeds from diverse data sources; such asonline newspapers; Twitter; and online blogs; are proving to be extremely valuableresources in helping to anticipate; detect; and forecast outbreaks of rare diseases. The goalof this paper is to develop techniques that can effectively forecast the emergence andprogression of rare infectious diseases by combining data from disparate data sources.Methods We introduce SourceSeer; a novel algorithmic framework that combinesspatiotemporal topic models with source-based anomaly detection techniques. SourceSeeris capable of discovering the location focus of each source; allowing sources to be used asexperts with varying degrees of authoritativeness. To fuse the individual source predictionsinto a final outbreak prediction; we employ a multiplicative weights algorithm taking into …,Statistical Analysis and Data Mining: The ASA Data Science Journal,2017,1
On Sharing Private Data with Multiple Non-Colluding Adversaries,Theodoros Rekatsinas; Amol Deshpande; Ashwin Machanavajjhala,Abstract: We present SPARSI; a theoretical framework for partitioning sensitive data acrossmultiple non-colluding adversaries. Most work in privacy-aware data sharing has considereddisclosing summaries where the aggregate information about the data is preserved; butsensitive user information is protected. Nonetheless; there are applications; including onlineadvertising; cloud computing and crowdsourcing markets; where detailed and fine-graineduser-data must be disclosed. We consider a new data sharing paradigm and introduce theproblem of privacy-aware data partitioning; where a sensitive dataset must be partitionedamong k untrusted parties (adversaries). The goal is to maximize the utility derived bypartitioning and distributing the dataset; while minimizing the amount of sensitive informationdisclosed. The data should be distributed so that an adversary; without colluding with …,arXiv preprint arXiv:1302.6556,2013,1
A Formal Framework For Probabilistic Unclean Databases,Christopher De Sa; Ihab F Ilyas; Benny Kimelfeld; Christopher Re; Theodoros Rekatsinas,Abstract: Traditional modeling of inconsistency in database theory casts all possible"repairs" equally likely. Yet; effective data cleaning needs to incorporate statistical reasoning.For example; yearly salary of\$100 k and age of 22 are more likely than\$100 k and 122 andtwo people with same address are likely to share their last name (ie; a functionaldependency tends to hold but may occasionally be violated). We propose a formalframework for unclean databases; where two types of statistical knowledge are incorporated.The first represents a belief of how intended (clean) data is generated; and the secondrepresents a belief of how the actual database is realized through the introduction of noise.Formally; a Probabilistic Unclean Database (PUD) is a triple that consists of a probabilisticdatabase that we call the" intention"; a probabilistic data transformator that we call the" …,arXiv preprint arXiv:1801.06750,2018,*
Fonduer: Knowledge Base Construction from Richly Formatted Data,Sen Wu; Luke Hsiao; Xiao Cheng; Braden Hancock; Theodoros Rekatsinas; Philip Levis; Christopher Ré,Abstract: We introduce Fonduer; a knowledge base construction (KBC) framework for richlyformatted information extraction (RFIE); where entity relations and attributes are conveyedvia structural; tabular; visual; and textual expressions. Fonduer introduces a newprogramming model for KBC built around a unified data representation that accounts forthree challenging characteristics of richly formatted data:(1) prevalent document-levelrelations;(2) multimodality; and (3) data variety. Fonduer is the first KBC system for richlyformatted data and uses a human-in-the-loop paradigm for training machine learningsystems; referred to as data programming. Data programming softens the burden oftraditional supervision by only asking users to provide lightweight functions thatprogrammatically assign (potentially noisy) labels to the input data. Fonduer's unified data …,arXiv preprint arXiv:1703.05028,2017,*
Exploiting Features for Data Source Quality Estimation.,Manas Joglekar; Theodoros Rekatsinas; Hector Garcia-Molina; Aditya Parameswaran; Christopher Ré,ABSTRACT We study the problem of estimating the quality of data sources in data fusionsettings. In contrast to existing models that rely only on conflicting observations acrosssources to infer quality (internal signals); we propose a data fusion model; called FUSE; thatcombines internal signals with external data-source features. We show both theoreticallyand empirically; that FUSE yields better quality estimates with rigorous guarantees; incontrast; models which utilize only internal signals have weaker or no guarantees. We studydifferent approaches for learning FUSE's parameters;(i) empirical risk minimization (ERM);which utilizes ground truth and relies on fast convex optimization methods; and (ii)expectation maximization (EM); which assumes no ground truth and uses slow iterativeoptimization procedures. EM is the standard approach used in most existing methods. An …,CoRR,2015,*
Quality-aware data source management,Theodoros Rekatsinas,Abstract Data is becoming a commodity of tremendous value in many domains. The ease ofcollecting and publishing data has led to an upsurge in the number of available datasources—sources that are highly heterogeneous in the domains they cover; the quality ofdata they provide; and the fees they charge for accessing their data. However; most existingdata integration approaches; for combining information from a collection of sources; focus onfacilitating integration itself but are agnostic to the actual utility or the quality of theintegration result. These approaches do not optimize for the trade-off between the utility andthe cost of integration to determine which sources are worth integrating.,*,2015,*
A Framework for Privacy-Preserving Data Publishing Across Multiple Non-Colluding Adversaries,Theodoros Rekatsinas,Abstract With the proliferation of online services there has been an increasing interest indistributing private user data to different third-party agents. The collection and storage ofuser data raises privacy considerations that often constrain user data sharing. Most work inprivacy-aware data sharing has considered disclosing privacy preserving summaries of datathat preserve the aggregate information about the data; but hide the individual userinformation. However; there are applications; including online advertising andcrowdsourcing markets; where detailed and fine-grained information must be published. Inthis paper we consider a different data sharing paradigm where we are required to distributea dataset to multiple third-parties. In particular; we address the problem of privacy-awaredata partitioning; namely; the problem of splitting a sensitive dataset among k untrusted …,*,*,*
