Incremental record linkage,Anja Gruenheid; Xin Luna Dong; Divesh Srivastava,Abstract Record linkage clusters records such that each cluster corresponds to a singledistinct real-world entity. It is a crucial step in data cleaning and data integration. In the bigdata era; the velocity of data updates is often high; quickly making previous linkage resultsobsolete. This paper presents an end-to-end framework that can incrementally andefficiently update linkage results when data updates arrive. Our algorithms not only allowmerging records in the updates with existing clusters; but also allow leveraging newevidence from the updates to fix previous linkage errors. Experimental results on three realand synthetic data sets show that our algorithms can significantly reduce linkage timewithout sacrificing linkage quality.,Proceedings of the VLDB Endowment,2014,62
Query optimization using column statistics in hive,Anja Gruenheid; Edward Omiecinski; Leo Mark,Abstract Hive is a data warehousing solution on top of the Hadoop MapReduce frameworkthat has been designed to handle large amounts of data and store them in tables like arelational database management system or a conventional data warehouse while using theparallelization and batch processing functionalities of the Hadoop MapReduce framework tospeed up the execution of queries. Data inserted into Hive is stored in the HadoopFileSystem (HDFS); which is part of the Hadoop MapReduce framework. To make the dataaccessible to the user; Hive uses a query language similar to SQL; which is called HiveQL.When a query is issued in HiveQL; it is translated by a parser into a query execution planthat is optimized and then turned into a series of map and reduce iterations. These iterationsare then executed on the data stored in the HDFS; writing the output to a file. The goal of …,Proceedings of the 15th Symposium on International Database Engineering & Applications,2011,30
Automatic generation of instantiation rules to determine quality of data migration,*,During migration of data from at least one data source to a target system; data quality isdetermined by obtaining metadata associated with the target system; automaticallygenerating instantiated rules for assessing a quality of data to be loaded from the at leastone data source into the target system; where the instantiated rules are dependent upon theobtained metadata associated with the target system; and applying a quality analysis basedupon the instantiated rules to the data to be loaded into the target system. The qualityanalysis provides an indication of a level of compliance of the data with requirements of thetarget system.,*,2012,22
Crowdsourcing Entity Resolution,Anja Gruenheid; Donald Kossmann; Ramesh Sukriti; Florian Widmer,There are several computational tasks for which the help of people is useful. One such taskis entity resolution. For this task; human experts can help to identify whether two customersare identical given their profile. Since crowdsourcing is expensive; the goal is to ask as fewquestions as possible. At the same time; high quality results can only be achieved if severalexperts are asked for their opinion and for confirmation. This paper shows how to addressthis cost/quality trade-off and how to tolerate and resolve errors from the crowd. Specifically;this paper shows how to exploit mathematical properties such as symmetry; transitivity; andanti-transitivity of the is-same-entity-as relation to improve both cost and quality. The resultsof extensive experiments provide surprising insights on how best to crowd-source for entityresolution and other classification problems.,Technical report/Department of Computer Science; ETH; Zurich,2012,13
Fault-tolerant entity resolution with the crowd,Anja Gruenheid; Besmira Nushi; Tim Kraska; Wolfgang Gatterbauer; Donald Kossmann,Abstract: In recent years; crowdsourcing is increasingly applied as a means to enhance dataquality. Although the crowd generates insightful information especially for complex problemssuch as entity resolution (ER); the output quality of crowd workers is often noisy. That is;workers may unintentionally generate false or contradicting data even for simple tasks. Thechallenge that we address in this paper is how to minimize the cost for task requesters whilemaximizing ER result quality under the assumption of unreliable input from the crowd. Forthat purpose; we first establish how to deduce a consistent ER solution from noisy workeranswers as part of the data interpretation problem. We then focus on the next-crowdsourceproblem which is to find the next task that maximizes the information gain of the ER result forthe minimal additional cost. We compare our robust data interpretation strategies to …,arXiv preprint arXiv:1512.00537,2015,11
Crowd access path optimization: Diversity matters,Besmira Nushi; Adish Singla; Anja Gruenheid; Erfan Zamanian; Andreas Krause; Donald Kossmann,Abstract Quality assurance is one the most important challenges in crowdsourcing.Assigning tasks to several workers to increase quality through redundant answers can beexpensive if asking homogeneous sources. This limitation has been overlooked by currentcrowdsourcing platforms resulting therefore in costly solutions. In order to achieve desirablecost-quality tradeoffs it is essential to apply efficient crowd access optimization techniques.Our work argues that optimization needs to be aware of diversity and correlation ofinformation within groups of individuals so that crowdsourcing redundancy can beadequately planned beforehand. Based on this intuitive idea; we introduce the Access PathModel (APM); a novel crowd model that leverages the notion of access paths as analternative way of retrieving information. APM aggregates answers ensuring high quality …,Third AAAI Conference on Human Computation and Crowdsourcing,2015,11
StoryPivot: comparing and contrasting story evolution,Anja Gruenheid; Donald Kossmann; Theodoros Rekatsinas; Divesh Srivastava,Abstract As the world evolves around us; so does the digital coverage of it. Events of diversetypes; associated with different actors and various locations; are continuously captured bymultiple information sources such as news articles; blogs; social media etc. day by day. Inthe digital world; these events are represented through information snippets that containinformation on the involved entities; a description of the event; when the event occurred; etc.In our work; we observe that events (and their corresponding digital representations) areoften inter-connected; ie; they form stories which represent evolving relationships betweenevents over time. Take as an example the plane crash in Ukraine in July 2014 whichinvolved multiple entities such as" Ukraine";" Malaysia"; and" Russia" and multiple eventsranging from the actual crash to the incident investigation and the presentation of the …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,3
Cost and Quality Trade-Offs in Crowdsourcing.,Anja Gruenheid; Donald Kossmann,ABSTRACT Algorithms for crowdsourced tasks such as entity resolution; sorting; etc. havebeen subject to a variety of research work. So far; all of this work has focused on one specificproblem respectively. In this paper; we want to focus on the bigger picture. More specifically;we want to show how it is possible to estimate the budget or the quality of an algorithm in acrowdsourcing environment where noise is introduced through incorrect answers by crowdworkers. Such estimates are complex as noise in the information set changes the behaviorof established algorithms. Using two sorting algorithms; QuickSort and BubbleSort asexamples; we will illustrate how algorithms handle noise; which measures can be taken tomake them more robust; and how these changes to the algorithms modify the budget andquality estimates of the respective algorithm. Finally; we will present an initial idea of how …,DBCrowd,2013,2
Indexing and selecting hierarchical business logic,Alessandra Loro; Anja Gruenheid; Donald Kossmann; Damien Profeta; Philippe Beaudequin,Abstract Business rule management is the task of storing and maintaining company-specificdecision rules and business logic that is queried frequently by application users. These rulescan impede efficient query processing when they require the business rule engine to resolvesemantic hierarchies. To address this problem; this work discusses hierarchical indexes thatare performance and storage-conscious. In the first part of this work; we develop a tree-based hierarchical structure that represents client-defined semantic hierarchies as well astwo variants of this structure that improve performance and main memory allocation. Thesecond part of our work focuses on selecting the top rules out of those retrieved from theindex. We formally define a priority score-based decision scheme that allows for a conflict-free rule system and efficient rule ranking. Additionally; we introduce a weight-based lazy …,Proceedings of the VLDB Endowment,2015,1
Online Event Integration with StoryPivot,Anja Gruenheid; Donald Kossmann; Divesh Srivastava,Abstract: Modern data integration systems need to process large amounts of data from avariety of data sources and with real-time integration constraints. They are not onlyemployed in enterprises for managing internal data but are also used for a variety of webservices that use techniques such as entity resolution or data cleaning in live systems. In thiswork; we discuss a new generation of data integration systems that operate on (un-)structured data in an online setting; ie; systems which process continuously modifieddatasets upon which the integration task is based. We use as an example of such a systeman online event integration system called StoryPivot. It observes events extracted from newsarticles in data sources such as the'Guardian'or the'Washington Post'which are integrated toshow users the evolution of real-world stories over time. The design decisions for …,arXiv preprint arXiv:1610.07732,2016,*
Data Integration with Dynamic Data Sources,Anja Gruenheid,Abstract Data integration is the task of collecting data from various data sources andcombining this data in a meaningful way. Traditionally; it is achieved by first extracting thedata for example by taking snapshots of the current dataset. Afterwards; the data istransformed through batch algorithms that perform tasks such as entity resolution or datacleaning to format the data according to the user's preferences. Finally; the integrated data isloaded into a new system. This three-step process is referred to as ETL process and istraditionally applied on static datasets. In this dissertation; we examine how data integrationprocesses need to be modified if data sources are dynamically changing the data theycontain. More specifically; modern data integration systems aim to provide integrated data innear realtime where changes to the underlying dataset are propagated with little delay. In …,*,2016,*
Modelling the Evolving World.,Anja Gruenheid,ABSTRACT All around us; the world is changing continuously; evolving every minute of theday. These changes are captured in event data that are partial snapshots of its evolution.The idea of story evolution is to combine these bits of information; reconstructing the realworld and its evolution through a digital model of their relationships.,CIDR,2015,*
Business Rules Retrieval and Processing,Alessandra Loro,Abstract Business Rules Engines are systems created to deliver relevant businessinformation to applications and business processes that depend on them. Given their criticalposition; efficient indexing to minimize time and memory consumption is essential.Unfortunately; the usage of standard hashing indexing techniques has proved not to be anefficient solution to the problem; both in term of performance and memory consumption. Inthis thesis we propose an alternative hierarchical indexing strategy that stems from theanalysis of domain-specific business data. We first explain how to represent the businessdata in a more efficient manner; then we suggest three different strategies to use thisrepresentation for indexing. To prove the efficacy of our technique; we have based ouranalysis on the Amadeus Business Rules Engine; and benchmarked our solution against …,*,2014,*
Incremental Record Linkage,Divesh Srivastava; Anja Gruenheid; Xin Luna Dong,BizID ID name street address city phone B1 r1 Starbucks 123 MISSION ST STE ST1 SANFRANCISCO 4155431510 B1 r2 Starbucks 123 MISSION ST SAN FRANCISCO4155431510 B1 r3 Starbucks 123 Mission St San Francisco 4155431510 B2 r4 StarbucksCoffee 340 MISSION ST SAN FRANCISCO 4155431510,*,2014,*
Cost-Efficient Querying Strategies for the Crowd,Anja Gruenheid; Besmira Nushi; Donald Kossmann,ABSTRACT To enhance data processing; crowdsourcing is a mechanism that has evolvedrecently and has been picked up by companies that refine large amounts of data through so-called crowd workers. Generally; there are two challenges to crowdsourcing: First; the crowdis an uncertain input source because a worker not necessarily knows the right answer orsimply answers wrongly. Second; crowdsourcing comes at a monetary cost. As a result;finding ways to minimize the number of times the crowd is accessed is an essential line ofwork to make crowdsourcing a realistic option in the context of big data. To address theseproblems; crowd querying strategies have been devised which determine the task order andbudget spent per worker task. Existing querying strategies focus on either improvingcertainty in the current result or optimizing the information gain by resolving the most …,*,*,*
