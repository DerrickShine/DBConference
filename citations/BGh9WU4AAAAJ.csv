Ligra: a lightweight graph processing framework for shared memory,Julian Shun; Guy E Blelloch,Abstract There has been significant recent interest in parallel frameworks for processinggraphs due to their applicability in studying social networks; the Web graph; networks inbiology; and unstructured meshes in scientific simulation. Due to the desire to process largegraphs; these systems have emphasized the ability to run on distributed memory machines.Today; however; a single multicore server can support more than a terabyte of memory;which can fit graphs with tens or even hundreds of billions of edges. Furthermore; for graphalgorithms; shared-memory multicores are generally significantly more efficient on a percore; per dollar; and per joule basis than distributed memory systems; and shared-memoryalgorithms tend to be simpler than their distributed counterparts. In this paper; we present alightweight graph processing framework that is specific for shared-memory parallel …,ACM Sigplan Notices,2013,271
Brief announcement: the problem based benchmark suite,Julian Shun; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Aapo Kyrola; Harsha Vardhan Simhadri; Kanat Tangwongsan,Abstract This announcement describes the problem based benchmark suite (PBBS). PBBSis a set of benchmarks designed for comparing parallel algorithmic approaches; parallelprogramming language styles; and machine architectures across a broad set of problems.Each benchmark is defined concretely in terms of a problem specification and a set of inputdistributions. No requirements are made in terms of algorithmic approach; programminglanguage; or machine architecture. The goal of the benchmarks is not only to compareruntimes; but also to be able to compare code and other aspects of an implementation (eg;portability; robustness; determinism; and generality). As such the code for an implementationof a benchmark is as important as its runtime; and the public PBBS repository will includeboth code and performance results.,Proceedings of the twenty-fourth annual ACM symposium on Parallelism in algorithms and architectures,2012,110
Internally deterministic parallel algorithms can be fast,Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Julian Shun,Abstract The virtues of deterministic parallelism have been argued for decades and manyforms of deterministic parallelism have been described and analyzed. Here we areconcerned with one of the strongest forms; requiring that for any input there is a uniquedependence graph representing a trace of the computation annotated with every operationand value. This has been referred to as internal determinism; and implies a sequentialsemantics---ie; considering any sequential traversal of the dependence graph is sufficient foranalyzing the correctness of the code. In addition to returning deterministic results; internaldeterminism has many advantages including ease of reasoning about the code; ease ofverifying correctness; ease of debugging; ease of defining invariants; ease of defining goodcoverage for testing; and ease of formally; informally and experimentally reasoning about …,ACM SIGPLAN Notices,2012,79
Connected spatial networks over random points and a route-length statistic,David J Aldous; Julian Shun,Abstract We review mathematically tractable models for connected networks on randompoints in the plane; emphasizing the class of proximity graphs which deserves to be betterknown to applied probabilists and statisticians. We introduce and motivate a particularstatistic R measuring shortness of routes in a network. We illustrate; via Monte Carlo in part;the trade-off between normalized network length and R in a one-parameter family ofproximity graphs. How close this family comes to the optimal trade-off over all possiblenetworks remains an intriguing open question.,Statistical Science,2010,41
Smaller and faster: Parallel processing of compressed graphs with Ligra+,Julian Shun; Laxman Dhulipala; Guy E Blelloch,We study compression techniques for parallel in-memory graph algorithms; and show thatwe can achieve reduced space usage while obtaining competitive or improved performancecompared to running the algorithms on uncompressed graphs. We integrate thecompression techniques into Ligra; a recent shared-memory graph processing system. Thissystem; which we call Ligra+; is able to represent graphs using about half of the space forthe uncompressed graphs on average. Furthermore; Ligra+ is slightly faster than Ligra onaverage on a 40-core machine with hyper-threading. Our experimental study shows thatLigra+ is able to process graphs using less memory; while performing as well as or fasterthan Ligra.,Data Compression Conference (DCC); 2015,2015,38
Greedy sequential maximal independent set and matching are parallel on average,Guy E Blelloch; Jeremy T Fineman; Julian Shun,Abstract The greedy sequential algorithm for maximal independent set (MIS) loops over thevertices in an arbitrary order adding a vertex to the resulting set if and only if no previousneighboring vertex has been added. In this loop; as in many sequential loops; each iteratewill only depend on a subset of the previous iterates (ie knowing that any one of a vertex'sprevious neighbors is in the MIS; or knowing that it has no previous neighbors; is sufficient todecide its fate one way or the other). This leads to a dependence structure among theiterates. If this structure is shallow then running the iterates in parallel while respecting thedependencies can lead to an efficient parallel implementation mimicking the sequentialalgorithm. In this paper; we show that for any graph; and for a random ordering of thevertices; the dependence length of the sequential greedy MIS algorithm is polylogarithmic …,Proceedings of the twenty-fourth annual ACM symposium on Parallelism in algorithms and architectures,2012,38
Multicore triangle computations without tuning,Julian Shun; Kanat Tangwongsan,Triangle counting and enumeration has emerged as a basic tool in large-scale networkanalysis; fueling the development of algorithms that scale to massive graphs. Most of theexisting algorithms; however; are designed for the distributed-memory setting or the external-memory setting; and cannot take full advantage of a multicore machine; whose capacity hasgrown to accommodate even the largest of real-world graphs. This paper describes thedesign and implementation of simple and fast multicore parallel algorithms for exact; as wellas approximate; triangle counting and other triangle computations that scale to billions ofnodes and edges. Our algorithms are provably cache-friendly; easy to implement in alanguage that supports dynamic parallelism; such as Cilk Plus or OpenMP; and do notrequire parameter tuning. On a 40-core machine with two-way hyper-threading; our …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,33
Reducing contention through priority updates,Julian Shun; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons,Abstract Memory contention can be a serious performance bottleneck in concurrentprograms on shared-memory multicore architectures. Having all threads write to a small setof shared locations; for example; can lead to orders of magnitude loss in performancerelative to all threads writing to distinct locations; or even relative to a single thread doing allthe writes. Shared write access; however; can be very useful in parallel algorithms;concurrent data structures; and protocols for communicating among threads. We study the"priority update" operation as a useful primitive for limiting write contention in parallel andconcurrent programs. A priority update takes as arguments a memory location; a new value;and a comparison function> p that enforces a partial order over values. The operationatomically compares the new value with the current value in the memory location; and …,Proceedings of the twenty-fifth annual ACM symposium on Parallelism in algorithms and architectures,2013,20
A simple and practical linear-work parallel algorithm for connectivity,Julian Shun; Laxman Dhulipala; Guy Blelloch,Abstract Graph connectivity is a fundamental problem in computer science with manyimportant applications. Sequentially; connectivity can be done in linear work easily usingbreadth-first search or depth-first search. There have been many parallel algorithms forconnectivity; however the simpler parallel algorithms require super-linear work; and thelinear-work polylogarithmic-depth parallel algorithms are very complicated and notamenable to implementation. In this work; we address this gap by describing a simple andpractical expected linear-work; polylogarithmic depth parallel algorithm for graphconnectivity. Our algorithm is based on a recent parallel algorithm for generating low-diameter graph decompositions by Miller et al.; which uses parallel breadth-first searches.We discuss a (modest) variant of their decomposition algorithm which preserves the …,Proceedings of the 26th ACM symposium on Parallelism in algorithms and architectures,2014,18
Parallel lightweight wavelet tree; suffix array and FM-index construction,Julian Labeit; Julian Shun; Guy E Blelloch,Abstract We present parallel lightweight algorithms to construct wavelet trees; rank andselect structures; and suffix arrays in a shared-memory setting. The work and depth of ourfirst parallel wavelet tree algorithm match those of the best existing parallel algorithm whilerequiring asymptotically less memory and our second algorithm achieves the sameasymptotic bounds for small alphabet sizes. Our experiments show that they are both fasterand more memory-efficient than existing parallel algorithms. We also present anexperimental evaluation of the parallel construction of rank and select structures; which areused in wavelet trees. Next; we design the first parallel suffix array algorithm based oninduced copying. Our induced copying requires linear work and polylogarithmic depth forconstant alphabets sizes. When combined with a parallel prefix doubling algorithm; it is …,Journal of Discrete Algorithms,2017,16
Sorting with asymmetric read and write costs,Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Julian Shun,Abstract Emerging memory technologies have a significant gap between the cost; both intime and in energy; of writing to memory versus reading from memory. In this paper wepresent models and algorithms that account for this difference; with a focus on write-efficientsorting algorithms. First; we consider the PRAM model with asymmetric write cost; and showthat sorting can be performed in O (n) writes; O (n log n) reads; and logarithmic depth(parallel time). Next; we consider a variant of the External Memory (EM) model that chargesk> 1 for writing a block of size B to the secondary memory; and present variants of three EMsorting algorithms (multi-way merge sort; sample sort; and heap sort using buffer trees) thatasymptotically reduce the number of writes over the original algorithms; and perform roughlyk block reads for every block write. Finally; we define a variant of the Ideal-Cache model …,Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures,2015,16
Parallel wavelet tree construction,Julian Shun,We present parallel algorithms for wavelet tree construction with polylogarithmic depth;improving upon the linear depth of the recent parallel algorithms by Fuentes-Sepulveda etal. We experimentally show that on a 40-core machine with two-way hyper-threading; weoutperform the existing parallel algorithms by 1.3--5.6 x and achieve up to 27x speedup overthe sequential algorithm on a variety of real-world and artificial inputs. Our algorithms showgood scalability with increasing thread count; input size and alphabet size. We also discussextensions to variants of the standard wavelet tree.,Data Compression Conference (DCC); 2015,2015,13
Practical parallel Lempel-Ziv factorization,Julian Shun; Fuyao Zhao,In the age of big data; the need for efficient data compression algorithms has grown. Awidely used data compression method is the Lempel-Ziv-77 (LZ77) method; being asubroutine in popular compression packages such as gzip and PKZIP. There has been a lotof recent effort on developing practical sequential algorithms for Lempel-Ziv factorization(equivalent to LZ77 compression); but research in practical parallel implementations hasbeen less satisfactory. In this work; we present a simple work-efficient parallel algorithm forLempel-Ziv factorization. We show theoretically that our algorithm requires linear work andruns in O (log 2 n) time (randomized) for constant alphabets and O (n ϵ) time (ϵ<; 1) forinteger alphabets. We present experimental results showing that our algorithm is efficientand achieves good speedup with respect to the best sequential implementations of …,Data Compression Conference (DCC); 2013,2013,12
Fast parallel computation of longest common prefixes,Julian Shun,Abstract Suffix arrays and the corresponding longest common prefix (LCP) array have wideapplications in bioinformatics; information retrieval and data compression. In this work; wepropose and theoretically analyze new parallel algorithms for computing the LCP arraygiven the suffix array as input. Most of our algorithms have a work and depth (parallel time)complexity related to the LCP values of the input. We also present a slight variation ofKärkkäinen and Sanders' skew algorithm that requires linear work and poly-logarithmicdepth in the worst case. We present a comprehensive experimental study of our parallelalgorithms along with existing parallel and sequential LCP algorithms. On a variety of real-world and artificial strings; we show that on a 40-core shared-memory machine our fastestalgorithm is up to 2.3 times faster than the fastest existing parallel algorithm; and up to …,Proceedings of the International Conference for High Performance Computing; Networking; Storage and Analysis,2014,11
A simple parallel cartesian tree algorithm and its application to parallel suffix tree construction,Julian Shun; Guy E Blelloch,Abstract We present a simple linear work and space; and polylogarithmic time parallelalgorithm for generating multiway Cartesian trees. We show that bottom-up traversals of themultiway Cartesian tree on the interleaved suffix array and longest common prefix array of astring can be used to answer certain string queries. By adding downward pointers in the tree(eg using a hash table); we can also generate suffix trees from suffix arrays on arbitraryalphabets in the same bounds. In conjunction with parallel suffix array algorithms; such asthe skew algorithm; this gives a rather simple linear work parallel; O (n ε) time (0< ε< 1);algorithm for generating suffix trees over an integer alphabet Σ {1;...; n}; where n is the lengthof the input string. It also gives a linear work parallel algorithm requiring O (log2 n) time withhigh probability for constant-sized alphabets. More generally; given a sorted sequence of …,ACM Transactions on Parallel Computing,2014,11
Phase-concurrent hash tables for determinism,Julian Shun; Guy E Blelloch,Abstract We present a deterministic phase-concurrent hash table in which operations of thesame type are allowed to proceed concurrently; but operations of different types are not.Phase-concurrency guarantees that all concurrent operations commute; giving adeterministic hash table state; guaranteeing that the state of the table at any quiescent pointis independent of the ordering of operations. Furthermore; by restricting our hash table to bephase-concurrent; we show that we can support operations more efficiently than previousconcurrent hash tables. Our hash table is based on linear probing; and relies on history-independence for determinism. We experimentally compare our hash table on a modern 40-core machine to the best existing concurrent hash tables that we are aware of (hopscotchhashing and chained hashing) and show that we are 1.3--4.1 times faster on random …,Proceedings of the 26th ACM symposium on Parallelism in algorithms and architectures,2014,11
Parallel algorithms for asymmetric read-write costs,Naama Ben-David; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Charles McGuffey; Julian Shun,Abstract Motivated by the significantly higher cost of writing than reading in emergingmemory technologies; we consider parallel algorithm design under such asymmetric read-write costs; with the goal of reducing the number of writes while preserving work-efficiencyand low span. We present a nested-parallel model of computation that combines (i) smallper-task stack-allocated memories with symmetric read-write costs and (ii) an unboundedheap-allocated shared memory with asymmetric read-write costs; and show how the costs inthe model map efficiently onto a more concrete machine model under a work-stealingscheduler. We use the new model to design reduced write; work-efficient; low span parallelalgorithms for a number of fundamental problems such as reduce; list contraction; treecontraction; breadth-first search; ordered filter; and planar convex hull. For the latter two …,Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,2016,10
A simple parallel cartesian tree algorithm and its application to suffix tree construction,Guy E Blelloch; Julian Shun,Abstract We present a simple linear work and space; and poly-logarithmic time parallelalgorithm for generating multiway Cartesian trees. As a special case; the algorithm can beused to generate suffix trees from suffix arrays on arbitrary alphabets in the same bounds. Inconjunction with parallel suffix array algorithms; such as the skew algorithm; this gives arather simple linear work parallel algorithm for generating suffix trees over an integeralphabet Σ⊆[1;...; n]; where n is the length of the input string. More generally; given a sortedsequences of strings and the longest common prefix lengths between adjacent elements;the algorithm will generate a pat tree (compacted trie) over the strings. We also presentexperimental results comparing the performance of the algorithm to existing sequentialimplementations and a second parallel algorithm. We present comparisons for the …,Proceedings of the Meeting on Algorithm Engineering & Expermiments,2011,10
Efficient algorithms with asymmetric read and write costs,Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Julian Shun,Abstract: In several emerging technologies for computer memory (main memory); the cost ofreading is significantly cheaper than the cost of writing. Such asymmetry in memory costsposes a fundamentally different model from the RAM for algorithm design. In this paper westudy lower and upper bounds for various problems under such asymmetric read and writecosts. We consider both the case in which all but $ O (1) $ memory has asymmetric cost; andthe case of a small cache of symmetric memory. We model both cases using the$(M;\omega) $-ARAM; in which there is a small (symmetric) memory of size $ M $ and alarge unbounded (asymmetric) memory; both random access; and where reading from thelarge memory has unit cost; but writing has cost $\omega\gg 1$. For FFT and sortingnetworks we show a lower bound cost of $\Omega (\omega n\log_ {\omega M} n) $; which …,arXiv preprint arXiv:1511.01038,2015,9
Parallel local graph clustering,Julian Shun; Farbod Roosta-Khorasani; Kimon Fountoulakis; Michael W Mahoney,Abstract Graph clustering has many important applications in computing; but due to growingsizes of graph; even traditionally fast clustering methods such as spectral partitioning can becomputationally expensive for real-world graphs of interest. Motivated partly by this; so-called local algorithms for graph clustering have received significant interest due to the factthat they can find good clusters in a graph with work proportional to the size of the clusterrather than that of the entire graph. This feature has proven to be crucial in making suchgraph clustering and many of its downstream applications efficient in practice. While localclustering algorithms are already faster than traditional algorithms that touch the entiregraph; they are sequential and there is an opportunity to make them even more efficient viaparallelization. In this paper; we show how to parallelize many of these algorithms in the …,Proceedings of the VLDB Endowment,2016,8
An evaluation of parallel eccentricity estimation algorithms on undirected real-world graphs,Julian Shun,Abstract This paper presents efficient shared-memory parallel implementations and the firstcomprehensive experimental study of graph eccentricity estimation algorithms in theliterature. The implementations include (1) a simple algorithm based on executing two-passbreadth-first searches from a sample of vertices;(2) algorithms with sub-quadratic worst-caserunning time for sparse graphs and non-trivial approximation guarantees that executebreadth-first searches from a carefully chosen set of vertices;(3) algorithms based onprobabilistic counters; and (4) a well-known 2-approximation algorithm that executes onebreadth-first search per connected component. Our experiments on large undirected real-world graphs show that the algorithm based on two-pass breadth-first searches workssurprisingly well; outperforming the other algorithms in terms of running time and/or …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,7
Sequential random permutation; list contraction and tree contraction are highly parallel,Julian Shun; Yan Gu; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons,Abstract We show that simple sequential randomized iterative algorithms for randompermutation; list contraction; and tree contraction are highly parallel. In particular; if iterationsof the algorithms are run as soon as all of their dependencies have been resolved; theresulting computations have logarithmic depth (parallel time) with high probability. Ourproofs make an interesting connection between the dependence structure of two of theproblems and random binary trees. Building upon this analysis; we describe linear-work;polylogarithmic-depth algorithms for the three problems. Although asymptotically no betterthan the many prior parallel algorithms for the given problems; their advantages include verysimple and fast implementations; and returning the same result as the sequential algorithm.Experiments on a 40-core machine show reasonably good performance relative to the …,*,2014,7
Exploiting optimization for local graph clustering,Kimon Fountoulakis; Xiang Cheng; Julian Shun; Farbod Roosta-Khorasani; Michael W Mahoney,Abstract Local graph clustering methods aim to identify well-connected clusters around agiven “seed set” of reference nodes. The main focus of prior theoretical work has been onworst-case running time properties or on implicit statistical regularization; and the focus ofprior empirical work has been to identify structure in large social and information networks.Here; we adopt an optimization perspective on local graph clustering methods. In particular;we clarify the relationship between the local spectral algorithm of [3] and a variant of a well-studied optimization objective. This insight permits us to develop a local spectral graphclustering algorithm that has improved theoretical convergence properties. We alsodemonstrate the numerical performance of this optimization-based algorithm and someheuristic variants of it.,*,2016,6
Shared-memory parallelism can be simple; fast; and scalable,Julian Shun,Parallelism is the key to achieving high performance in computing. However; writing efficientand scalable parallel programs is notoriously difficult; and often requires significantexpertise. To address this challenge; it is crucial to provide programmers with high-leveltools to enable them to develop solutions easily; and at the same time emphasize thetheoretical and practical aspects of algorithm design to allow the solutions developed to runefficiently under many different settings. This thesis addresses this challenge using a three-pronged approach consisting of the design of shared-memory programming techniques;frameworks; and algorithms for important problems in computing. The thesis providesevidence that with appropriate programming techniques; frameworks; and algorithms;shared-memory programs can be simple; fast; and scalable; both in theory and in practice …,*,2017,5
Parallelism in randomized incremental algorithms,Guy E Blelloch; Yan Gu; Julian Shun; Yihan Sun,Abstract In this paper we show that most sequential randomized incremental algorithms arein fact parallel. We consider several random incremental algorithms including algorithms forcomparison sorting and Delaunay triangulation; linear programming; closest pair; andsmallest enclosing disk in constant dimensions; as well as least-element lists and stronglyconnected components on graphs. We analyze the dependence between iterations in analgorithm; and show that the dependence structure is shallow for all of the algorithms;implying high parallelism. We identify three types of dependences found in the algorithmsstudied and present a framework for analyzing each type of algorithm. Using the frameworkgives work-efficient polylogarithmic-depth parallel algorithms for most of the problems thatwe study. Some of these algorithms are straightforward (eg; sorting and linear …,Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,2016,3
A top-down parallel semisort,Yan Gu; Julian Shun; Yihan Sun; Guy E Blelloch,Abstract Semisorting is the problem of reordering an input array of keys such that equal keysare contiguous but different keys are not necessarily in sorted order. Semisorting isimportant for collecting equal values and is widely used in practice. For example; it is thecore of the MapReduce paradigm; is a key component of the database join operation; andhas many other applications. We describe a (randomized) parallel algorithm for the problemthat is theoretically efficient (linear work and logarithmic depth); but is designed to be morepractically efficient than previous algorithms. We use ideas from the parallel integer sortingalgorithm of Rajasekaran and Reif; but instead of processing bits of a integers in a reducedrange in a bottom-up fashion; we process the hashed values of keys directly top-down. Weimplement the algorithm and experimentally show on a variety of input distributions that it …,Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures,2015,3
Beyond synchronous: new techniques for external-memory graph connectivity and minimum spanning forest,Aapo Kyrola; Julian Shun; Guy Blelloch,Abstract GraphChi [16] is a recent high-performance system for external memory (disk-based) graph computations. It uses the Parallel Sliding Windows (PSW) algorithm which isbased on the so-called Gauss-Seidel type of iterative computation; in which updates tovalues are immediately visible within the iteration. In contrast; previous external memorygraph algorithms are based on the synchronous model where computation can only observevalues from previous iterations. In this work; we study implementations of connectedcomponents and minimum spanning forest on PSW and show that they have a competitiveI/O bound of O (sort (E) log (V/M)) and also work well in practice. We also show that our MSFimplementation is competitive with a specialized algorithm proposed by Dementiev et al.[10]while being much simpler.,International Symposium on Experimental Algorithms,2014,3
Models for connected networks over random points and a route-length statistic,David J Aldous; Julian Shun,Abstract We review mathematically tractable models for connected networks on randompoints in the plane; emphasising the little-studied class of proximity graphs and introducing anew model called the Hammersley network. We introduce and motivate a particular statisticR measuring shortness of routes in a network. We show (via Monte Carlo; in part) the trade-off between normalized network length and R in a oneparameter family of proximity graphs.How close this family comes to the optimal trade-off over all possible networks remains anintriguing open question.,preparation. Draft available at http://www. stat. berkeley. edu/users/aldous/Spatial/poisson. pdf,2009,3
Julienne: A Framework for Parallel Graph Algorithms using Work-efficient Bucketing,Laxman Dhulipala; Guy Blelloch; Julian Shun,Abstract Existing graph-processing frameworks let users develop efficient implementationsfor many graph problems; but none of them support efficiently bucketing vertices; which isneeded for bucketing-based graph algorithms such as\Delta-stepping and approximate set-cover. Motivated by the lack of simple; scalable; and efficient implementations of bucketing-based algorithms; we develop the Julienne framework; which extends a recent shared-memory graph processing framework called Ligra with an interface for maintaining acollection of buckets under vertex insertions and bucket deletions. We provide a theoreticallyefficient parallel implementation of our bucketing interface and study several bucketing-based algorithms that make use of it (either bucketing by remaining degree or by distance)to improve performance: the peeling algorithm for k-core (coreness);\Delta-stepping …,Proceedings of the 29th ACM Symposium on Parallelism in Algorithms and Architectures,2017,2
Improved parallel construction of wavelet trees and rank/select structures,Julian Shun,Existing parallel algorithms for wavelet tree construction have a work complexity of O (n logσ). This paper presents parallel algorithms for the problem with improved work complexity.Our first algorithm is based on parallel integer sorting and has either O (n log log n [log σ/√log n log log n]) work and polylogarithmic depth; or O (n [log σ/√ log n]) work and sub-lineardepth. We also describe another algorithm that has O (n [log σ/√ log n]) work and O (σ+ logn) depth. We then show how to use similar ideas to construct variants of wavelet trees(arbitrary-shaped binary trees and multiary trees) as well as wavelet matrices in parallel withlower work complexity than prior algorithms. Finally; we show that the rank and selectstructures on binary sequences and multiary sequences; which are stored on wavelet treenodes; can be constructed in parallel with improved work bounds; matching those of the …,Data Compression Conference (DCC); 2017,2017,2
Efficient algorithms under asymmetric read and write costs,Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Julian Shun,Fifty years of algorithms research has focused on settings in which reads and writes (tomemory) have similar cost. But what if reads and writes to memory have significantlydifferent costs? How would that impact algorithm design? What new techniques are usefulfor trading-off doing more cheaper operations (say more reads) in order to do fewerexpensive operations (say fewer writes)? What are the fundamental limitations on such trade-offs (lower bounds)? What well-known equivalences for traditional memory fail to hold forasymmetric memories? Such questions are coming to the fore with the arrival of newmemory technologies; such as phasechange memory (PCM); spin-torque transfer magneticRAM (STT-RAM); and memristor-based resistive RAM (ReRAM); which have the propertythat the cost of reading is significantly cheaper than the cost of writing [4; 5; 7; 16; 17; 27 …,arXiv preprint arXiv:1511.01038,2015,1
Efficient implementation of a synchronous parallel push-relabel algorithm,Niklas Baumstark; Guy Blelloch; Julian Shun,Abstract Motivated by the observation that FIFO-based push-relabel algorithms are able tooutperform highest label-based variants on modern; large maximum flow problem instances;we introduce an efficient implementation of the algorithm that uses coarse-grainedparallelism to avoid the problems of existing parallel approaches. We demonstrate goodrelative and absolute speedups of our algorithm on a set of large graph instances taken fromreal-world applications. On a modern 40-core machine; our parallel implementationoutperforms existing sequential implementations by up to a factor of 12 and other parallelimplementations by factors of up to 3.,*,2015,1
Priority update as a parallel primitive,Julian Shun; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons,Abstract Memory contention can be a serious performance bottleneck in concurrentprograms on sharedmemory multicore architectures. Having all threads write to a small setof shared locations; for example; can lead to orders of magnitude loss in performancerelative to all threads writing to distinct locations; or even relative to a single thread doing allthe writes. Shared write access; however; can be very useful in parallel algorithms;concurrent data structures; and protocols for communicating among threads. In this paperwe study the “priority update” operation as a useful primitive for limiting write contention inparallel and concurrent programs. A priority update takes as arguments a memory location;a new value; and a comparison function> p that enforces a partial order over values. Theoperation atomically compares the new value with the current value in the memory …,*,2013,1
Connected networks over random points and a route-length statistic. I. Overview,David J Aldous; Julian Shun,This paper is intended as a non-technical overview of a project which is centered on the useof a particular summary statistic (R at (3) below) to measure the efficiency of a spatialnetwork in providing short routes. For concreteness; visualize a road network linking a givenset of cities. In [5] we will present data on the values of R (smaller is more efficient) andnormalized network length θ (defined at (1) below) for real-world examples; and seek tocompare the observed value of R with the theoretical optimal value of R; that is with theminimum value over all possible hypothetical networks on the given cities with the sametotal length as the real network. To get some theoretical insight into the tradeoff betweennetwork length and R it is natural to consider the case of randomly placed cities; it is this“random model” which is the focus of the present paper. Note that the random model is …,*,2008,1
Implicit Decomposition for Write-Efficient Connectivity Algorithms,Naama Ben-David; Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Charles McGuffey; Julian Shun,Abstract: The future of main memory appears to lie in the direction of new technologies thatprovide strong capacity-to-performance ratios; but have write operations that are much moreexpensive than reads in terms of latency; bandwidth; and energy. Motivated by this trend; wepropose sequential and parallel algorithms to solve graph connectivity problems usingsignificantly fewer writes than conventional algorithms. Our primary algorithmic tool is theconstruction of an $ o (n) $-sized" implicit decomposition" of a bounded-degree graph $ G $on $ n $ nodes; which combined with read-only access to $ G $ enables fast answers toconnectivity and biconnectivity queries on $ G $. The construction breaks the linear-write"barrier"; resulting in costs that are asymptotically lower than conventional algorithms whileadding only a modest cost to querying time. For general non-sparse graphs on $ m …,arXiv preprint arXiv:1710.02637,2017,*
Variational perspective on local graph clustering,Kimon Fountoulakis; Farbod Roosta-Khorasani; Julian Shun; Xiang Cheng; Michael W Mahoney,Abstract Modern graph clustering applications require the analysis of large graphs and thiscan be computationally expensive. In this regard; local spectral graph clustering methodsaim to identify well-connected clusters around a given “seed set” of reference nodes withoutaccessing the entire graph. The celebrated Approximate Personalized PageRank (APPR)algorithm in the seminal paper by Andersen et al.(in: FOCS'06 proceedings of the 47thannual IEEE symposium on foundations of computer science; pp 475–486; 2006) is onesuch method. APPR was introduced and motivated purely from an algorithmic perspective.In other words; there is no a priori notion of objective function/optimality conditions thatcharacterizes the steps taken by APPR. Here; we derive a novel variational formulationwhich makes explicit the actual optimization problem solved by APPR. In doing so; we …,Mathematical Programming,2016,*
Notes on Simple Analysis for Parallel Maximal Independent Set and Maximal Matching Algorithms,Julian Shun,For a graph G=(V; E) we use N (V) to denote the set of all neighbors of vertices in V. Amaximal independent set U⊂ V is thus one that satisfies N (U)∩ U=∅ and N (U)∪ U= V. Weuse N (v) as a shorthand for N ({v}) when v is a single vertex. We use G [U] to denote thevertex-induced subgraph of G by vertex set U; ie; G [U] contains all vertices in U along withedges of G with both endpoints in U. A simple parallel algorithm due to Luby [3] forcomputing the maximal independent set is as follows:,*,2013,*
Program SPAA 2012,Florent Becker; Adrian Kosowski; Nicolas Nisse; Ivan Rapaport; Karol Suchan; Barbara Kempkes; Peter Kling; Friedhelm Meyer Auf der Heide; Christian Ortolf; Christian Schindelhauer; Ravi Rajwar; Yujie Liu; Stephan Diestelhorst; Michael Spear; Jun Shirako; Nick Vrvilo; Eric Mercer; Vivek Sarkar; Anastasia Braginsky; Erez Petrank; Guy Blelloch; Jeremy Fineman; Phillip Gibbons; Aapo Kyrola; Julian Shun; Harsha Vardhan Simhadri; Kanat Tangwongsan; Claire Ralph; Vitus Leung; William Mclendon; Neeraj Sharma; Sandeep Sen; Grey Ballard; James Demmel; Olga Holtz; Benjamin Lipshitz; Oded Schwartz; Henry Lin; Frans Schalekamp,Noon-6:00pm: Trip to Fallingwater Meet at the Holiday Inn lobby. Extra cost for this event.7:00-9:00pm: SPAA Reception at the Pittsburgh Athletic Association This is located almost acrossthe street from the Holiday Inn Monday 25 June … 9:20 Time vs. Space Trade-offs for Rendezvousin Trees Jurek Czyzowicz; Adrian Kosowski and Andrzej Pelc. 9:45 Allowing Each Node to CommunicateOnly Once in a Distributed System: Shared Whiteboard Models Florent Becker; AdrianKosowski; Nicolas Nisse; Ivan Rapaport and Karol Suchan. 10:10 Optimal and Competitive RuntimeBounds for Continuous; Local Gathering of Mobile Robots Barbara Kempkes; Peter Kling andFriedhelm Meyer Auf der Heide. 10:35 Online Multi-Robot Exploration of Grid Graphs with RectangularObstacles Christian Ortolf and Christian Schindelhauer. 11:00-11:30 Coffee Break … In Searchof Parallel Dimensions Ravi Rajwar. 12:30-2:00 Lunch Break … 2:00 Delegation and …,*,*,*
Models and Algorithms under Asymmetric Read and Write Costs,Guy E Blelloch; Jeremy T Fineman; Phillip B Gibbons; Yan Gu; Julian Shun,Abstract—In several emerging non-volatile technologies for main memory (NVRAM) the costof reading is significantly cheaper than the cost of writing. Such asymmetry in memory costsleads to a desire for “writeefficient” algorithms that minimize the number of writes to theNVRAM. While several prior works have explored write-efficient algorithms for databases orfor the unique properties of NAND Flash; our ongoing work seeks to develop a broadertheory of algorithm design for asymmetric memories. This talk will highlight our recentprogress on models; algorithms; and lower bounds for asymmetric memories [1];[2];[3]. Weextend the classic RAM model to the asymmetric case by defining the (M; ω)-ARAM; whichconsists of a large asymmetric memory and a much smaller symmetric memory of size M;both random access; such that for the asymmetric memory; writes cost ω> 1 times more …,*,*,*
HARCHOL-BALTER; Mor CMU-CS-15-141; CMU-CS-15-141R HARKES; Jan CMU-CS-15-115 HAUPTMANN; Alex CMU-CS-15-103 HE; Wangzi CMU-CS-15-129,Wenlu HU; Lu JIANG; Kaustubh JOSHI; Archit KARANDIKAR; Steven D KLEE; Anvesh KOMURAVELLI; Danai KOUTRA; Akshay KRISHNAMURTHY; Jayant KRISHNAMURTHY; SL LAM; Namhoon LEE; Seunghak LEE; Hyeontaek LIM; Erika LINKE; Sarah M LOOS; Chris MARTENS; Brendan MEEDER; Rui MEIRELES; Filipe David Oliveira MILITÁO; Jamie MORGANSTERN; Dana MOVSHOVITZ-ATTIAS; Yair MOVSHOVITZ-ATTIAS; Stefan K MULLER; Michael K PAPAMICHAEL; Padmanabhan PILLAI; Venkata Krishna PILLUTLA; Wolfgang RICHTER; Dan RYAN; Mehdi SAMADI; Mahadev SATYANARAYANAN; Alan SCHELLER-WOLF; Evan SHAPIRO; Daniel P SIEWIOREK; Gloriana ST CLAIR; Julian SHUN; Asim SMAILAGIC; Kristina SOJAKOVA; Prashant SRIDHAR; Brandon TAYLOR; Bernardo Parente Coutinho Fernandes TONINHO; Esha UBOWEJA; Mark VELEDNITSKY; Carol WANG; Keith WEBSTER; Gabriel Leonard WEISZ; Ji Oh YOO; Samuel ZBARSKY,2015 Technical Reports by Author Computer Science Department School of Computer Science;Carnegie Mellon University. ABE; Yoshihisa CMU-CS-15-113; CMU-CS-15-115;CMU-CS-15-118; CMU-CS-15-137 CMU-CS-15-147. ACAR; Umut A. CMU-CS-15-130;CMU-CS-15-131. AMOS; Brandon CMU-CS-15-103; CMU-CS-15-113; CMU-CS-15-139.CERVESATO; Iliano CMU-CS-15-101; CMU-CS-15-117. CHEN; Zhuo CMU-CS-15-103;CMU-CS-15-113. CHUNG; Da-Yoon CMU-CS-15-125. CRUZ; Flávio Manuel FernandesCMU-CS-15-148. DEY; Anind CMU-CS-15-118. DUFF; William A. CMU-CS-15-130;CMU-CS-15-131. FANG; Yihua CMU-CS-15-122. GANZFRIED; Sam CMU-CS-15-104. GAO;Ying CMU-CS-15-139. GARDNER; Kristen CMU-CS-15-141; CMU-CS-15-141R. GEAMBASU;Raxana CMU-CS-15-137. GILBERT; Benjamin CMU-CS-15-115 …,*,*,*
2015 Theses by Author,Yoshihisa ABE; Da-Yoon CHUNG; Flávio Manuel Fernandes CRUZ; Yihua FANG; Sam GANZFRIED; Wangzi HE; Archit KARANDIKAR; Steven D KLEE; Anvesh KOMURAVELLI; Danai KOUTRA; Akshay KRISHNAMURTHY; Jayant KRISHNAMURTHY; Namhoon LEE; Seunghak LEE; Hyeontaek LIM; Sarah M LOOS; Chris MARTENS; Brendan MEEDER; Rui MEIRELES; Filipe David Oliveira MILITÁO; Jamie MORGENSTERN; Dana MOVSHOVITZ-ATTIAS; Yair MOVSHOVITZ-ATTIAS; Michael K PAPAMICHAEL; Venkata Krishna PILLUTLA; Wolfgang RICHTER; Mehdi SAMADI; Evan SHAPIRO; Julian SHUN; Prashant SRIDHAR; Bernardo Parente Coutinho Fernandes TONINHO; Esha UBOWEJA; Carol WANG; Gabriel Leonard WEISZ; Ji Oh YOO,2015 Theses by Author Computer Science Department School of Computer Science; CarnegieMellon University. Please see abstracts for all citations/formats related to each thesis. ABE;Yoshihisa CMU-CS-15-147. CHUNG; Da-Yoon CMU-CS-15-125 (MS). CRUZ; Flávio ManuelFernandes CMU-CS-15-148. FANG; Yihua CMU-CS-15-122 (MS). GANZFRIED; SamCMU-CS-15-104. HE; Wangzi CMU-CS-15-129 (MS). KARANDIKAR; Archit CMU-CS-15-142(MS). KLEE; Steven D. CMU-CS-15-114 (MS). KOMURAVELLI; Anvesh CMU-CS-15-102.KOUTRA; Danai CMU-CS-15-126. KRISHNAMURTHY; Akshay CMU-CS-15-116.KRISHNAMURTHY; Jayant CMU-CS-15-110. LEE; Namhoon CMU-CS-15-140 (MS). LEE;Seunghak CMU-CS-15-100. LIM; Hyeontaek CMU-CS-15-132. LOOS; Sarah M.CMU-CS-15-144. MARTENS; Chris CMU-CS-15-134. MEEDER; Brendan CMU-CS-15-106 …,*,*,*
Predicting Unroll Factors Using Supervised Classfication,Nan Li; Julian Shun,*,*,*,*
Exposing Instruction-Level and Task Parallelism in Loops using Supervised Classification,Nan Li; Julian Shun,*,*,*,*
Simple; Fast and Scalable Parallel Algorithms for Shared Memory,Julian Shun,Abstract To ease the transition into the multicore/manycore era; shared-memoryprogramming must be made more natural and accessible to the community. Furthermore;shared-memory algorithms need to be fast and scalable in order to quickly process largedata. In this proposed thesis we will study techniques for simplifying parallel programmingand allowing users to easily write efficient and scalable algorithms. Our work will consist of(1) designing a benchmark suite which allows for head-to-head comparisons of parallellanguages and architectures for given problems;(2) developing methods for writingdeterministic parallel programs to simplify programming and debugging;(3) developing auseful primitive for reducing memory contention on shared-memory multicore machines;(4)building a simple framework for implementing efficient large-scale shared-memory graph …,*,*,*
