A calculus for communicating systems,R MILleR,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,LNCS,1986,1411
Data Exchange: Semantics and Query Answering,Ronald Fagin; Phokion G Kolaitis; Renée J Miller; Lucian Popa,Abstract Data exchange is the problem of taking data structured under a source schema andcreating an instance of a target schema that reflects the source data as accurately aspossible. In this paper; we address foundational and algorithmic issues related to thesemantics of data exchange and to the query answering problem in the context of dataexchange. These issues arise because; given a source instance; there may be many targetinstances that satisfy the constraints of the data exchange problem. We give an algebraicspecification that selects; among all solutions to the data exchange problem; a special classof solutions that we call universal. We show that a universal solution has no more and noless data than required for data exchange and that it represents the entire space of possiblesolutions. We then identify fairly general; yet practical; conditions that guarantee the …,Database Theory—ICDT,2003,1223
Schema Mapping as Query Discovery,Renéee J Miller; Laura M Haas; Mauricio A Hernéandez,Abstract To enable modern data intensive applications including data warehousing; globalinformation systems and electronic commerce; we must solve the schema mapping problemin which a source (legacy) database is mapped into a different; but fixed; target schema.Schema mapping involves the discovery of a query or set of queries that transform thesource data into the new structure. We introduce an interactive mapping creation paradigmbased on value correspondences that show how a value of a target attribute can be createdfrom a set of values of source attributes. We describe the use of the value correspondenceframework in Clio; a prototype tool for semi-automated schema mapping; and present analgorithm for query derivation from an evolving set of value correspondences.,VLDB,2000,698
Schema Mapping as Query Discovery,Renéee J Miller; Laura M Haas; Mauricio A Hernandez,Abstract To enable modern data intensive applications including data warehousing; globalinformation systems and electronic commerce; we must solve the schema mapping problemin which a source (legacy) database is mapped into a different; but fixed; target schema.Schema mapping involves the discovery of a query or set of queries that transform thesource data into the new structure. We introduce an interactive mapping creation paradigmbased on value correspondences that show how a value of a target attribute can be createdfrom a set of values of source attributes. We describe the use of the value correspondenceframework in Clio; a prototype tool for semi-automated schema mapping; and present analgorithm for query derivation from an evolving set of value correspondences.,VLDB,2000,698
Translating web data,Lucian Popa; Yannis Velegrakis; Mauricio A Hernández; Renée J Miller; Ronald Fagin,An important issue in modern information systems and e-commerce applications is providingsupport for interoperability of independent data sources. A broad variety of data is availableon the Web in distinct heterogeneous sources; stored under different formats: databaseformats (for example; relational model); semistructured formats (for example; DTDs; SGML orXML Schema); scientific formats; etc. Integration of such data is an increasingly importantproblem. Nonetheless; the effort involved in such integration; in practice; is considerable:translation of data from one format (or schema) to another requires writing and managingcomplex data; transformation programs or queries. The chapter presents a novel frameworkfor mapping between any combination of XML and relational schemas; in which a high-level;user specified mapping is translated into semantically meaningful queries that transform …,VLDB,2002,673
The Clio project: managing heterogeneity,Renée J Miller; Mauricio A Hernández; Laura M Haas; Ling-Ling Yan; CT Howard Ho; Ronald Fagin; Lucian Popa,Abstract Clio is a system for managing and facilitating the complex tasks of heterogeneousdata transformation and integration. In Clio; we have collected together a powerful set ofdata management techniques that have proven invaluable in tackling these difficultproblems. In this paper; we present the underlying themes of our approach and present abrief case study.,SIGMOD Record,2001,419
Association rules over interval data,Renée J Miller; Yuping Yang,Abstract We consider the problem of mining association rules over interval data (that is;ordered data for which the separation between data points has meaning). We show that themeasures of what rules are most important (also called rule interest) that are used for miningnominal and ordinal data do not capture the semantics of interval data. In the presence ofinterval data; support and confidence are no longer intuitive measures of the interest of arule. We propose a new definition of interest for association rules that takes into account thesemantics of interval data. We developed an algorithm for mining association rules underthe new definition and overview our experience using the algorithm on large real-lifedatasets.,ACM SIGMOD,1997,365
Mapping data in peer-to-peer systems: Semantics and algorithmic issues,Anastasios Kementsietsidis; Marcelo Arenas; Renée J Miller,Abstract We consider the problem of mapping data in peer-to-peer data-sharing systems.Such systems often rely on the use of mapping tables listing pairs of corresponding values tosearch for data residing in different peers. In this paper; we address semantic andalgorithmic issues related to the use of mapping tables. We begin by arguing why mappingtables are appropriate for data mapping in a peer-to-peer environment. We discussalternative semantics for these tables and we present a language that allows the user tospecify mapping tables under different semantics. Then; we show that by treating mappingtables as constraints (called mapping constraints) on the exchange of information betweenpeers it is possible to reason about them. We motivate why reasoning capabilities areneeded to manage mapping tables and show the importance of inferring new mapping …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,323
Similarity search over time-series data using wavelets,Ivan Popivanov; Renee J Miller,Considers the use of wavelet transformations as a dimensionality reduction technique topermit efficient similarity searching over high-dimensional time-series data. While numeroustransformations have been proposed and studied; the only wavelet that has been shown tobe effective for this application is the Haar wavelet. In this work; we observe that a largeclass of wavelet transformations (not only orthonormal wavelets but also bi-orthonormalwavelets) can be used to support similarity searching. This class includes the most popularand most effective wavelets being used in image compression. We present a detailedperformance study of the effects of using different wavelets on the performance of similaritysearching for time-series data. We include several wavelets that outperform both the Haarwavelet and the best-known non-wavelet transformations for this application. To ensure …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,303
The use of information capacity in schema integration and translation,Renée J Miller; Yannis E Ioannidis; Raghu Ramakrishnan,Abstract In this paper; we carefully explore the assumptions behind using informationcapacity equivalence as a measure of correctness for judging transformed schemas inschema integration and translation methodologies. We present a classification of commonintegration and translation tasks based on their operational goals and derive from them therelative information capacity requirements of the original and transformed schemas. Weshow that for many tasks; information capacity equivalence of the schemas is not strictlyrequired. Based on this; we present a new definition of correctness that reflects eachundertaken task. We then examine existing methodologies and show how anomalies canarise when using those that do not meet the proposed correctness criteria.,VLDB,1993,268
Data-driven understanding and refinement of schema mappings,Ling Ling Yan; Renée J Miller; Laura M Haas; Ronald Fagin,Abstract At the heart of many data-intensive applications is the problem of quickly andaccurately transforming data into a new form. Database researchers have long advocatedthe use of declarative queries for this process. Yet tools for creating; managing andunderstanding the complex queries necessary for data transformation are still too primitive topermit widespread adoption of this approach. We present a new framework that uses dataexamples as the basis for understanding and refining declarative schema mappings. Weidentify a small set of intuitive operators for manipulating examples. These operators permita user to follow and refine an example by walking through a data source. We show that ouroperators are powerful enough both to identify a large class of schema mappings and todistinguish effectively between alternative schema mappings. These operators permit a …,ACM SIGMOD,2001,266
LIMBO: Scalable clustering of categorical data,Periklis Andritsos; Panayiotis Tsaparas; Renée J Miller; Kenneth C Sevcik,Abstract Clustering is a problem of great practical importance in numerous applications. Theproblem of clustering becomes more challenging when the data is categorical; that is; whenthere is no inherent distance measure between data values. We introduce LIMBO; ascalable hierarchical categorical clustering algorithm that builds on the InformationBottleneck (IB) framework for quantifying the relevant information preserved when clustering.As a hierarchical algorithm; LIMBO has the advantage that it can produce clusterings ofdifferent sizes in a single execution. We use the IB framework to define a distance measurefor categorical tuples and we also present a novel distance measure for categorical attributevalues. We show how the LIMBO algorithm can be used to cluster both tuples and values.LIMBO handles large data sets by producing a memory bounded summary model for the …,International Conference on Extending Database Technology,2004,247
Conquer: Efficient management of inconsistent databases,Ariel Fuxman; Elham Fazli; Renée J Miller,Abstract Although integrity constraints have long been used to maintain data consistency;there are situations in which they may not be enforced or satisfied. In this paper; we presentConQuer; a system for efficient and scalable answering of SQL queries on databases thatmay violate a set of constraints. ConQuer permits users to postulate a set of key constraintstogether with their queries. The system rewrites the queries to retrieve all (and only) data thatis consistent with respect to the constraints. The rewriting is into SQL; so the rewrittenqueries can be efficiently optimized and executed by commercial database systems. Westudy the overhead of resolving inconsistencies dynamically (at query time). In particular; wepresent a set of performance experiments that compare the efficiency of the rewritingstrategies used by ConQuer. The experiments use queries taken from the TPC-H …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,230
The hyperion project: from data integration to data coordination,Marcelo Arenas; Vasiliki Kantere; Anastasios Kementsietsidis; Iluju Kiringa; Renée J Miller; John Mylopoulos,Abstract We present an architecture and a set of challenges for peer database managementsystems. These systems team up to build a network of nodes (peers) that coordinate at runtime most of the typical DBMS tasks such as the querying; updating; and sharing of data.Such a network works in a way similar to conventional multidatabases. Conventionalmultidatabase systems are founded on key concepts such as those of a global schema;central administrative authority; data integration; global access to multiple databases;permanent participation of databases; etc. Instead; our proposal assumes total absence ofany central authority or control; no global schema; transient participation of peer databases;and constantly evolving coordination rules among databases. In this work; we describe thestatus of the Hyperion project; present our current solutions; and outline remaining …,ACM SIGMOD Record,2003,227
Clean answers over dirty databases: A probabilistic approach,Periklis Andritsos; Ariel Fuxman; Renee J Miller,The detection of duplicate tuples; corresponding to the same real-world entity; is animportant task in data integration and cleaning. While many techniques exist to identify suchtuples; the merging or elimination of duplicates can be a difficult task that relies on ad-hocand often manual solutions. We propose a complementary approach that permits declarativequery answering over duplicated data; where each duplicate is associated with a probabilityof being in the clean database. We rewrite queries over a database containing duplicates toreturn each answer with the probability that the answer is in the clean database. Ourrewritten queries are sensitive to the semantics of duplication and help a user understandwhich query answers are most likely to be present in the clean database. The semantics thatwe adopt is independent of the way the probabilities are produced; but is able to …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,224
Discovering data quality rules,Fei Chiang; Renée J Miller,Abstract Dirty data is a serious problem for businesses leading to incorrect decision making;inefficient daily operations; and ultimately wasting both time and money. Dirty data oftenarises when domain constraints and business rules; meant to preserve data consistency andaccuracy; are enforced incompletely or not at all in application code. In this work; wepropose a new data-driven tool that can be used within an organization's data qualitymanagement process to suggest possible rules; and to identify conformant and non-conformant records. Data quality rules are known to be contextual; so we focus on thediscovery of context-dependent rules. Specifically; we search for conditional functionaldependencies (CFDs); that is; functional dependencies that hold only over a portion of thedata. The output of our tool is a set of functional dependencies together with the context in …,Proceedings of the VLDB Endowment,2008,222
Schema equivalence in heterogeneous systems: bridging theory and practice,Renée J Miller; Yannis E Ioannidis; Raghu Ramakrishnan,Abstract Current theoretical work offers measures of schema equivalence based on theinformation capacity of schemas. This work is based on the existence of abstract functionssatisfying various restrictions between the sets of all instances of two schemas. Inconsidering schemas that arise in practice; however; it is not clear how to reason about theexistence of such abstract functions. Further; these notions of equivalence tend to be tooliberal in that schemas are often considered equivalent when a practitioner would considerthem to be different. As a result; practical integration methodologies have not utilized thistheoretical foundation and most of them have relied on ad-hoc approaches. We presentresults that seek to bridge this gap. First; we consider the problem of deciding informationcapacity equivalence and dominance of schemas that occur in practice; ie; those that can …,Information Systems,1994,192
Clio: Schema mapping creation and data exchange,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,Abstract The Clio project provides tools that vastly simplify information integration.Information integration requires data conversions to bring data in different representationsinto a common form. Key contributions of Clio are the definition of non-procedural schemamappings to describe the relationship between data in heterogeneous schemas; a newparadigm in which we view the mapping creation process as one of query discovery; andalgorithms for automatically generating queries for data transformation from the mappings.Clio provides algorithms to address the needs of two major information integration problems;namely; data integration and data exchange. In this chapter; we present our algorithms forboth schema mapping creation via query discovery; and for query generation for dataexchange. These algorithms can be used in pure relational; pure XML; nested relational …,*,2009,186
Clio: A semi-automatic tool for schema mapping,Mauricio A Hernández; Renée J Miller; Laura M Haas,We consider the integration requirements of modern data intensive applications includingdata warehousing; global information systems and electronic commerce. At the heart ofthese requirements lies the schema mapping problem in which a source (legacy) databasemust be mapped into a different; but xed; target schema. The goal of schema mapping is thediscovery of a query or set of queries to map source databases into the new structure. Wedemonstrate Clio; a new semi-automated tool for creating schema mappings. Clio employs amapping-by-example paradigm that relies on the use of value correspondences describinghow a value of a target attribute can be created from a set of values of source attributes. Atypical session with Clio starts with the user loading a source and a target schema into thesystem. These schemas are read from either an underlying Object-Relational database or …,ACM SIGMOD Record,2001,181
-Mapping Adaptation under Evolving Schemas,Yannis Velegrakis; Renée J Miller; Lucian Popa,This chapter identifies the problem of mapping adaptation in dynamic environments withevolving schemas. To achieve interoperability; modem information systems and e-commerce applications use mappings to translate data from one representation to another.In dynamic environments like the Web; data sources may change not only their data but alsotheir schemas; their semantics; and their query capabilities. Such changes must be reflectedin the mappings. The chapter motivates the need for an automated system to adaptmappings and describes several areas in which the solutions can be applied. This chapterpresents a novel framework and a tool; Toronto Mapping Adaptation System (ToMAS); thatautomatically maintains the consistency of the mappings as schemas evolve. The approachis unique in many ways. It considers and manages a very general class of mappings …,*,2003,157
Nested mappings: schema mapping reloaded,Ariel Fuxman; Mauricio A Hernandez; Howard Ho; Renee J Miller; Paolo Papotti; Lucian Popa,Abstract Many problems in information integration rely on specifications; called schemamappings; that model the relationships between schemas. Schema mappings for bothrelational and nested data are well-known. In this work; we present a new formalism forschema mapping that extends these existing formalisms in two significant ways. First; ournested mappings allow for nesting and correlation of mappings. This results in a naturalprogramming paradigm that often yields more accurate specifications. In particular; we showthat nested mappings can naturally preserve correlations among data that existing mappingformalisms cannot. We also show that using nested mappings for purposes of exchangingdata from a source to a target will result in less redundancy in the target data. The secondextension to the mapping formalism is the ability to express; in a declarative way …,Proceedings of the 32nd international conference on Very large data bases,2006,151
Framework for evaluating clustering algorithms in duplicate detection,Oktie Hassanzadeh; Fei Chiang; Hyun Chul Lee; Renée J Miller,Abstract The presence of duplicate records is a major data quality concern in largedatabases. To detect duplicates; entity resolution also known as duplication detection orrecord linkage is used as a part of the data cleaning process to identify records thatpotentially refer to the same real-world entity. We present the Stringer system that providesan evaluation framework for understanding what barriers remain towards the goal of trulyscalable and general purpose duplication detection algorithms. In this paper; we useStringer to evaluate the quality of the clusters (groups of potential duplicates) obtained fromseveral unconstrained clustering algorithms used in concert with approximate jointechniques. Our work is motivated by the recent significant advancements that have madeapproximate join algorithms highly scalable. Our extensive evaluation reveals that some …,Proceedings of the VLDB Endowment,2009,148
Peer data exchange,Ariel Fuxman; Phokion G Kolaitis; Renée J Miller; Wang-Chiew Tan,Abstract In this article; we introduce and study a framework; called peer data exchange; forsharing and exchanging data between peers. This framework is a special case of a full-fledged peer data management system and a generalization of data exchange between asource schema and a target schema. The motivation behind peer data exchange is to modelauthority relationships between peers; where a source peer may contribute data to a targetpeer; specified using source-to-target constraints; and a target peer may use target-to-source constraints to restrict the data it is willing to receive; but cannot modify the data of thesource peer. A fundamental algorithmic problem in this framework is that of deciding theexistence of a solution: given a source instance and a target instance for a fixed peer dataexchange setting; can the target instance be augmented in such a way that the source …,ACM Transactions on Database Systems (TODS),2006,140
Leveraging data and structure in ontology integration,Octavian Udrea; Lise Getoor; Renée J Miller,Abstract There is a great deal of research on ontology integration which makes use of richlogical constraints to reason about the structural and logical alignment of ontologies. Thereis also considerable work on matching data instances from heterogeneous schema orontologies. However; little work exploits the fact that ontologies include both data andstructure. We aim to close this gap by presenting a new algorithm (ILIADS) that tightlyintegrates both data matching and logical reasoning to achieve better matching ofontologies. We evaluate our algorithm on a set of 30 pairs of OWL Lite ontologies with theschema and data matchings found by human reviewers. We compare against two systems-the ontology matching tool FCA-merge [28] and the schema matching tool COMA++[1].ILIADS shows an average improvement of 25% in quality over FCA-merge and a 11 …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,139
Using schematically heterogeneous structures,Reée J Miller,Abstract Schematic heterogeneity arises when information that is represented as data underone schema; is represented within the schema (as metadata) in another. Schematicheterogeneity is an important class of heterogeneity that arises frequently in integratinglegacy data in federated or data warehousing applications. Traditional query languages andview mechanisms are insufficient for reconciling and translating data between schematicallyheterogeneous schemas. Higher order query languages; that permit quantification overschema labels; have been proposed to permit querying and restructuring of data betweenschematically disparate schemas. We extend this work by considering how these languagescan be used in practice. Specifically; we consider a restricted class of higher order viewsand show the power of these views in integrating legacy structures. Our results provide …,ACM SIGMOD,1998,137
First-order query rewriting for inconsistent databases,Ariel D Fuxman; Renée J Miller,Abstract We consider the problem of retrieving consistent answers over databases that mightbe inconsistent with respect to some given integrity constraints. In particular; we concentrateon sets of constraints that consist of key dependencies. Most of the existing work hasfocused on identifying intractable cases of this problem. In contrast; in this paper we give analgorithm that computes the consistent answers for a large and practical class of conjunctivequeries. Given a query q; the algorithm returns a first-order query Q (called a query rewriting)such that for every (potentially inconsistent) database I; the consistent answers for q can beobtained by evaluating Q directly on I.,International Conference on Database Theory,2005,115
First-order query rewriting for inconsistent databases,Ariel Fuxman; Renée J Miller,Abstract We consider the problem of retrieving consistent answers over databases that mightbe inconsistent with respect to a set of integrity constraints. In particular; we concentrate onsets of constraints that consist of key dependencies; and we give an algorithm that computesthe consistent answers for a large and practical class of conjunctive queries. Given a queryq; the algorithm returns a first-order query Q (called a query rewriting) such that for every(potentially inconsistent) database I; the consistent answers for q can be obtained byevaluating Q directly on I.,Journal of Computer and System Sciences,2007,105
SECRET: a model for analysis of the execution semantics of stream processing systems,Irina Botan; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Renée J Miller; Nesime Tatbul,Abstract There are many academic and commercial stream processing engines (SPEs)today; each of them with its own execution semantics. This variation may lead to seeminglyinexplicable differences in query results. In this paper; we present SECRET; a model of thebehavior of SPEs. SECRET is a descriptive model that allows users to analyze the behaviorof systems and understand the results of window-based queries for a broad range ofheterogeneous SPEs. The model is the result of extensive analysis and experimentationwith several commercial and academic engines. In the paper; we describe the types ofheterogeneity found in existing engines; and show with experiments on real systems that ourmodel can explain the key differences in windowing behavior.,Proceedings of the VLDB Endowment,2010,104
Method for schema mapping and data transformation,*,*,*,*,104
Data sharing in the hyperion peer database system,Patricia Rodríguez-Gianolli; Anastasios Kementsietsidis; Maddalena Garzetti; Iluju Kiringa; Lei Jiang; Mehedi Masud; Renée J Miller; John Mylopoulos,Abstract This demo presents Hyperion; a prototype system that supports data sharing for anetwork of independent Peer Relational Database Management Systems (PDBMSs). Thenodes of such a network are assumed to be autonomous PDBMSs that form acquaintancesat run-time; and manage mapping tables to define value correspondences among differentdatabases. They also use distributed Event-Condition-Action (ECA) rules to enable andcoordinate data sharing. Peers perform local querying and update processing; and alsopropagate queries and updates to their acquainted peers. The demo illustrates the followingkey functionalities of Hyperion:(1) the use of (data level) mapping tables to infer newmetadata as peers dynamically join the network;(2) the ability to answer queries using datain acquaintances; and (3) the ability to coordinate peers through update propagation.,Proceedings of the 31st international conference on Very large data bases,2005,102
Preserving mapping consistency under schema changes,Yannis Velegrakis; J Miller; Lucian Popa,Abstract In dynamic environments like the Web; data sources may change not only their databut also their schemas; their semantics; and their query capabilities. When a mapping is leftinconsistent by a schema change; it has to be detected and updated. We present a novelframework and a tool (ToMAS) for automatically adapting (rewriting) mappings as schemasevolve. Our approach considers not only local changes to a schema but also changes thatmay affect and transform many components of a schema. Our algorithm detects mappingsaffected by structural or constraint changes and generates all the rewritings that areconsistent with the semantics of the changed schemas. Our approach explicitly modelsmapping choices made by a user and maintains these choices; whenever possible; as theschemas and mappings evolve. When there is more than one candidate rewriting; the …,The VLDB Journal—The International Journal on Very Large Data Bases,2004,100
NanoART synthesis; characterization; uptake; release and toxicology for human monocyte–macrophage drug delivery,Ari S Nowacek; Reagan L Miller; JoEllyn McMillan; Georgette Kanmogne; Michel Kanmogne; R Lee Mosley; Zhiya Ma; Sabine Graham; Mahesh Chaubal; Jane Werling; Barrett Rabinow; Huanyu Dou; Howard E Gendelman,Background: Factors limiting the efficacy of conventional antiretroviral therapy for HIV-1infection include treatment adherence; pharmacokinetics and penetration into viralsanctuaries. These affect the rate of viral mutation and drug resistance. In attempts to bypasssuch limitations; nanoparticles containing ritonavir; indinavir and efavirenz (described asnanoART) were manufactured to assess macrophage-based drug delivery. Methods:NanoART were made by high-pressure homogenization of crystalline drug with varioussurfactants. Size; charge and shape of the nanoparticles were assessed. Monocyte-derivedmacrophage nanoART uptake; drug release; migration and cytotoxicity were determined.Drug levels were measured by reverse-phase high-performance liquid chromatography.Results: Efficient monocyte-derived macrophage cytoplasmic vesicle uptake in less than …,Nanomedicine,2009,88
Muse: Mapping understanding and design by example,Bogdan Alexe; Laura Chiticariu; Renée J Miller; Wang-Chiew Tan,A fundamental problem in information integration is that of designing the relationships;called schema mappings; between two schemas. The specification of a semantically correctschema mapping is typically a complex task. Automated tools can suggest potentialmappings; but few tools are available for helping a designer understand mappings anddesign alternative mappings. We describe Muse; a mapping design wizard that uses dataexamples to assist designers in understanding and refining a schema mapping towards thedesired specification. We present novel algorithms behind Muse and show how Musesystematically guides the designer on two important components of a mapping design: thespecification of the desired grouping semantics for sets of data and the choice amongalternative interpretations for semantically ambiguous mappings. In every component …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,86
Mapping XML and relational schemas with Clio,Lucian Popa; Mauricio A Hernandez; Yannis Velegrakis; Renée J Miller; Felix Naumann; Howard Ho,Merging and coalescing data from multiple and diverse sources into different data formatscontinues to be an important problem in modern information systems. Schema matching (theprocess of matching elements of a source schema with elements of a target schema) andschema mapping (the process of creating a query that maps between two disparateschemas) are at the heart of data integration systems. We demonstrate Clio; a semi-automatic schema mapping tool developed at the IBM Almaden Research Center. In thispaper; we showcase Clio's mapping engine which allows mapping to and from relationaland XML schemas; and takes advantage of data constraints in order to preserve dataassociations.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,79
Creating probabilistic databases from duplicated data,Oktie Hassanzadeh; Renée J Miller,Abstract A major source of uncertainty in databases is the presence of duplicate items; ie;records that refer to the same real-world entity. However; accurate deduplication is a difficulttask and imperfect data cleaning may result in loss of valuable information. A reasonablealternative approach is to keep duplicates when the correct cleaning strategy is not certain;and utilize an efficient probabilistic query-answering technique to return query results alongwith probabilities of each answer being correct. In this paper; we present a flexible modularframework for scalably creating a probabilistic database out of a dirty relation of duplicateddata and overview the challenges raised in utilizing this framework for large relations ofstring data. We study the problem of associating probabilities with duplicates that aredetected using state-of-the-art scalable approximate join methods. We argue that …,The VLDB Journal—The International Journal on Very Large Data Bases,2009,77
Information-theoretic tools for mining database structure from large data sets,Periklis Andritsos; Renée J Miller; Panayiotis Tsaparas,Abstract Data design has been characterized as a process of arriving at a design thatmaximizes the information content of each piece of data (or equivalently; one that minimizesredundancy). Information content (or redundancy) is measured with respect to a prescribedmodel for the data; a model that is often expressed as a set of constraints. In this work; weconsider the problem of doing data redesign in an environment where the prescribed modelis unknown or incomplete. Specifically; we consider the problem of finding structural clues inan instance of data; an instance which may contain errors; missing values; and duplicaterecords. We propose a set of information-theoretic tools for finding structural summaries thatare useful in characterizing the information content of the data; and ultimately useful in datadesign. We provide algorithms for creating these summaries over large; categorical data …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,77
Clustering Categorical Data based On Information Loss Minimization,Periklis Andritsos; Panayiotis Tsaparas; Renee J. Miller; Kenneth C. Sevcik,As the size of databases continues to grow; understanding their structure gets more difficult.This; together with the lack of documentation and the unavailability of the original designersof the database adds further difficulty to the job of researchers and professionals tounderstand the structure of large and complex databases. At the same time; data sourcesare distributed over several sites and their integration introduces anomalies and oftenresults in “dirty” databases; ie; databases that contain erroneous or duplicate data records.Our research focuses on the application of data mining; and in particular clusteringtechniques; to aid the process of recovering and understanding high-level views of datasets. Clustering is a problem of great practical importance that has been the focus ofsubstantial research in several domains for decades [13; 14]. As storage capacities grow …,Second Hellenic Data Management Symposium; 2003,2003,77
A semantic approach to discovering schema mapping expressions,Yuan An; Alex Borgida; Renée J Miller; John Mylopoulos,In many applications it is important to find a meaningful relationship between the schemas ofa source and target database. This relationship is expressed in terms of declarative logicalexpressions called schema mappings. The more successful previous solutions have reliedon inputs such as simple element correspondences between schemas in addition to localschema constraints such as keys and referential integrity. In this paper; we investigate theuse of an alternate source of information about schemas; namely the presumed presence ofsemantics for each table; expressed in terms of a conceptual model (CM) associated with it.Our approach first compiles each CM into a graph and represents each table's semantics asa subtree in it. We then develop algorithms for discovering subgraphs that are plausibleconnections between those concepts/nodes in the CM graph that have attributes …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,75
A framework for semantic link discovery over relational data,Oktie Hassanzadeh; Anastasios Kementsietsidis; Lipyeow Lim; Renée J Miller; Min Wang,Abstract Discovering links between different data items in a single data source or acrossdifferent data sources is a challenging problem faced by many information systems today. Inparticular; the recent Linking Open Data (LOD) community project has highlighted theparamount importance of establishing semantic links among web data sources. Currently;LOD sources provide billions of RDF triples; but only millions of links between data sources.Many of these data sources are published using tools that operate over relational datastored in a standard RDBMS. In this paper; we present a framework for discovery ofsemantic links from relational data. Our framework is based on declarative specification oflinkage requirements by a user. We illustrate the use of our framework using several linkdiscovery algorithms on a real world scenario. Our framework allows data publishers to …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,69
Mining for empty spaces in large data sets,Jeff Edmonds; Jarek Gryz; Dongming Liang; Renée J Miller,Abstract Many data mining approaches focus on the discovery of similar (and frequent) datavalues in large data sets. We present an alternative; but complementary approach in whichwe search for empty regions in the data. We consider the problem of finding all maximalempty rectangles in large; two-dimensional data sets. We introduce a novel; scalablealgorithm for finding all such rectangles. The algorithm achieves this with a single scan overa sorted data set and requires only a small bounded amount of memory. We extend thealgorithm to find all maximal empty hyper-rectangles in a multi-dimensional space. Weconsider the complexity of this search problem and present new bounds on the number ofmaximal empty hyper-rectangles. We briefly overview experimental results obtained byapplying our algorithm to real and synthetic data sets and describe one application of …,Theoretical Computer Science,2003,59
Linkedct: A linked data space for clinical trials,Oktie Hassanzadeh; Anastasios Kementsietsidis; Lipyeow Lim; Renée J Miller; Min Wang,Abstract: The Linked Clinical Trials (LinkedCT) project aims at publishing the first opensemantic web data source for clinical trials data. The database exposed by LinkedCT isgenerated by (1) transforming existing data sources of clinical trials into RDF; and (2)discovering semantic links between the records in the trials data and several other datasources. In this paper; we discuss several challenges involved in these two steps andpresent the methodology used in LinkedCT to overcome these challenges. Our approach forsemantic link discovery involves using state-of-the-art approximate string matchingtechniques combined with ontology-based semantic matching of the records; all performedin a declarative and easy-to-use framework. We present an evaluation of the performance ofour proposed techniques in several link discovery scenarios in LinkedCT.,arXiv preprint arXiv:0908.0567,2009,58
Representing and querying data transformations,Yannis Velegrakis; Renee J Miller; John Mylopoulos,Modern information systems often store data that has been transformed and integrated froma variety of sources. This integration may obscure the original source semantics of dataitems. For many tasks; it is important to be able to determine not only where data itemsoriginated; but also why they appear in the integration as they do and through whattransformation they were derived. This problem is known as data provenance. In this work;we consider data provenance at the schema and mapping level. In particular; we considerhow to answer questions such as" what schema elements in the source (s) contributed tothis value"; or" through what transformations or mappings was this value derived?" Towardsthis end; we elevate schemas and mappings to first-class citizens that are stored in arepository and are associated with the actual data values. An extended query language …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,55
A unified model for data and constraint repair,Fei Chiang; Renee J Miller,Integrity constraints play an important role in data design. However; in an operationaldatabase; they may not be enforced for many reasons. Hence; over time; data may becomeinconsistent with respect to the constraints. To manage this; several approaches haveproposed techniques to repair the data; by finding minimal or lowest cost changes to thedata that make it consistent with the constraints. Such techniques are appropriate for the oldworld where data changes; but schemas and their constraints remain fixed. In many modernapplications however; constraints may evolve over time as application or business ruleschange; as data is integrated with new data sources; or as the underlying semantics of thedata evolves. In such settings; when an inconsistency occurs; it is no longer clear if there isan error in the data (and the data should be repaired); or if the constraints have evolved …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,54
Clio: Schema Mapping Creation and Data Exchange; Conceptual Modeling: Foundations and Applications: Essays in Honor of John Mylopoulos,Ronald Fagin; Laura M Haas; Mauricio Hernández; Renée J Miller; Lucian Popa; Yannis Velegrakis,*,*,2009,51
Continuous data cleaning,Maksims Volkovs; Fei Chiang; Jaroslaw Szlichta; Renée J Miller,In declarative data cleaning; data semantics are encoded as constraints and errors arisewhen the data violates the constraints. Various forms of statistical and logical inference canbe used to reason about and repair inconsistencies (errors) in data. Recently; unifiedapproaches that repair both errors in data and errors in semantics (the constraints) havebeen proposed. However; both data-only approaches and unified approaches are by andlarge static in that they apply cleaning to a single snapshot of the data and constraints. Weintroduce a continuous data cleaning framework that can be applied to dynamic data andconstraint environments. Our approach permits both the data and its semantics to evolveand suggests repairs based on the accumulated evidence to date. Importantly; our approachuses not only the data and constraints as evidence; but also considers the past repairs …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,48
Desktop experiment management,Y Ioannidis; Miron Livny; E Haber; R Miller; O Tsatalos; J Wiener,Traditionally; the scale and scope of an experimental study was determined by the ability ofthe research team to generate data. Over the past few years; we have been experiencing anunprecedented increase in the ability of small teams of experimental scientists to generatedata. This has led to a shift in the balance between the different components of anexperimental study. Today; it is not uncommon to find a study whose scale and scope havebeen determined by the ability of the team to manage the data rather than to generate it.Whether the discipline is experimental computer science [4]; genetics; earth and spacesciences; soil sciences; or high-energy physics; scientists are faced in their daily work withan experiment and data management bottleneck. Unfortunately; an experimental scientistcan not find ready off-the-shelf management tools that offer both the functionality required …,IEEE Data Engineering Bulletin,1993,43
Managing data mappings in the hyperion project,Anastasios Kementsietsidis; Marcelo Arenas; Renée J Miller,We consider the problem of mapping data in peer-to-peer systems. Such systems rely onsimple value searches to locate data of interest. However; different peers may use differentvalues to identify or describe the same data. To accommodate this; peer-to-peer systemsoften rely on mapping tables that list pairs of corresponding values for search domains thatare used in different peers. We illustrate how such tables are used in the genomicscommunity by expert curators. We then argue why mapping tables are appropriate for datamapping in a peer-to-peer environment and motivate the problem of managing these tables.The work presented is part of the Hyperion project.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,36
Mining for empty rectangles in large data sets,Jeff Edmonds; Jarek Gryz; Dongming Liang; Renée J Miller,Abstract Many data mining approaches focus on the discovery of similar (and frequent) datavalues in large data sets. We present an alternative; but complementary approach in whichwe search for empty regions in the data. We consider the problem of finding all maximalempty rectangles in large; two-dimensional data sets. We introduce a novel; scalablealgorithm for finding all such rectangles. The algorithm achieves this with a single scan overa sorted data set and requires only a small bounded amount of memory. We also describean algorithm to find all maximal empty hyper-rectangles in a multi-dimensional space. Weconsider the complexity of this search problem and present new bounds on the number ofmaximal empty hyper-rectangles. We briefly overview experimental results obtained byapplying our algorithm to a synthetic data set.,International Conference on Database Theory,2001,35
Modeling the execution semantics of stream processing engines with SECRET,Nihal Dindar; Nesime Tatbul; Renée J Miller; Laura M Haas; Irina Botan,Abstract There are many academic and commercial stream processing engines (SPEs)today; each of them with its own execution semantics. This variation may lead to seeminglyinexplicable differences in query results. In this paper; we present SECRET; a model of thebehavior of SPEs. SECRET is a descriptive model that allows users to analyze the behaviorof systems and understand the results of window-based queries (with time-and tuple-basedwindows) for a broad range of heterogeneous SPEs. The model is the result of extensiveanalysis and experimentation with several commercial and academic engines. In the paper;we describe the types of heterogeneity found in existing engines and show with experimentson real systems that our model can explain the key differences in windowing behavior.,The VLDB Journal,2013,33
Yam: a schema matcher factory,Fabien Duchateau; Remi Coletta; Zohra Bellahsene; Renée J Miller,Abstract In this paper; we present YAM; a schema matcher factory. YAM (Yet AnotherMatcher) is not (yet) another schema matching system as it enables the generation of a lacarte schema matchers according to user requirements. These requirements include apreference for recall or precision; a training data set (schemas already matched) andprovided expert correspondences. YAM uses a knowledge base that includes a (possiblylarge) set of similarity measures and classifiers. Based on the user requirements; YAMlearns how to best apply these tools (similarity measures and classifiers) in concert toachieve the best matching quality. In our demonstration; we will let users apply YAM to buildthe best schema matcher for different user requirements.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,32
Using sql for efficient generation and querying of provenance information,Boris Glavic; Renée J Miller; Gustavo Alonso,Abstract In applications such as data warehousing or data exchange; the ability to efficientlygenerate and query provenance information is crucial to understand the origin of data. In thischapter; we review some of the main contributions of Perm; a DBMS that generates differenttypes of provenance information for complex SQL queries (including nested and correlatedsubqueries and aggregation). The two key ideas behind Perm are representing data and itsprovenance together in a single relation and relying on query rewrites to generate thisrepresentation. Through this; Perm supports fully integrated; on-demand provenancegeneration and querying using SQL. Since Perm rewrites a query requesting provenanceinto a regular SQL query and generates easily optimizable SQL code; its performancegreatly benefits from the query optimization techniques provided by the underlying DBMS.,*,2013,30
Discovering linkage points over web data,Oktie Hassanzadeh; Ken Q Pu; Soheil Hassas Yeganeh; Renée J Miller; Lucian Popa; Mauricio A Hernández; Howard Ho,Abstract A basic step in integration is the identification of linkage points; ie; finding attributesthat are shared (or related) between data sources; and that can be used to match records orentities across sources. This is usually performed using a match operator; that associatesattributes of one database to another. However; the massive growth in the amount andvariety of unstructured and semi-structured data on the Web has created new challenges forthis task. Such data sources often do not have a fixed pre-defined schema and contain largenumbers of diverse attributes. Furthermore; the end goal is not schema alignment as theseschemas may be too heterogeneous (and dynamic) to meaningfully align. Rather; the goal isto align any overlapping data shared by these sources. We will show that even attributeswith different meanings (that would not qualify as schema matches) can sometimes be …,Proceedings of the VLDB Endowment,2013,27
TRAMP: Understanding the behavior of schema mappings through provenance,Boris Glavic; Gustavo Alonso; Renée J Miller; Laura M Haas,Abstract Though partially automated; developing schema mappings remains a complex andpotentially error-prone task. In this paper; we present TRAMP (TRAnsformation MappingProvenance); an extensive suite of tools supporting the debugging and tracing of schemamappings and transformation queries. TRAMP combines and extends data provenance withtwo novel notions; transformation provenance and mapping provenance; to explain therelationship between transformed data and those transformations and mappings thatproduced that data. In addition we provide query support for transformations; data; and allforms of provenance. We formally define transformation and mapping provenance; presentan efficient implementation of both forms of provenance; and evaluate the resulting systemthrough extensive experiments.,Proceedings of the VLDB Endowment,2010,27
Towards Inconsistency Management in Data Integration Systems.,Ariel Fuxman; Renée J Miller,Inconsistency management is a fundamental task in any data integration system.Surprisingly; though; the subject has not received much attention in the data integrationcommunity. This is probably due to the fact that it is often assumed that the global schema ofa system is not given a priori; but it is derived as an output of a schema integration process[Batini et al.; 1986]. Hence; schema integration methodologies are designed to produceglobal schemas that are consistent with respect to the sources. However; the result of theschema integration process does not always reflect the semantics that the user has in mindfor the global schema. In fact; in many cases; there is no consistent schema that satisfies therequirements of the user. To illustrate this; consider a simple data integration system; inwhich the Financial and Human Resources departments of a company are integrating …,IIWeb,2003,27
Schema and data: A holistic approach to mapping; resolution and fusion in information integration,Laura M Haas; Martin Hentschel; Donald Kossmann; Renée J Miller,Abstract To integrate information; data in different formats; from dif-ferent; potentiallyoverlapping sources; must be related and transformed to meet the users' needs. Ten yearsago; Clio introduced nonprocedural schema mappings to describe the relationship betweendata in heteroge-neous schemas. This enabled powerful tools for mapping discovery andintegration code generation; greatly simplifying the integration process. However; furtherprogress is needed. We see an opportunity to raise the level of abstraction further; toencompass both data-and schema-centric integration tasks and to isolate applications fromthe details of how the integration is accomplished. Holistic information integration supports it-eration across the various integration tasks; leveraging information about both schema anddata to improve the integrated result. Integration inde-pendence allows applications to be …,International Conference on Conceptual Modeling,2009,26
Linkage query writer,Oktie Hassanzadeh; Reynold Xin; Renée J Miller; Anastasios Kementsietsidis; Lipyeow Lim; Min Wang,Abstract We present Linkage Query Writer (LinQuer); a system for generating SQL queriesfor semantic link discovery over relational data. The LinQuer framework consists of (a)LinQL; a language for specification of linkage requirements;(b) a web interface and an APIfor translating LinQL queries to standard SQL queries;(c) an interface that assists users inwriting LinQL queries. We discuss the challenges involved in the design and implementationof a declarative and easy to use framework for discovering links between different data itemsin a single data source or across different data sources. We demonstrate different steps ofthe linkage requirements specification and discovery process in several real world scenariosand show how the LinQuer system can be used to create high-quality linked data sources.,Proceedings of the VLDB Endowment,2009,26
Accuracy of Approximate String Joins Using Grams.,Oktie Hassanzadeh; Mohammad Sadoghi; Renée J Miller,ABSTRACT Approximate join is an important part of many data cleaning and integrationmethodologies. Various similarity measures have been proposed for accurate and efficientmatching of string attributes. The accuracy of the similarity measures highly depends on thecharacteristics of the data such as amount and type of the errors and length of the strings.Recently; there has been an increasing interest in using methods based on q-grams(substrings of length q) made out of the strings; mainly due to their high efficiency. In thiswork; we evaluate the accuracy of the similarity measures used in these methodologies. Wepresent an overview of several similarity measures based on q-grams. We then thoroughlycompare their accuracy on several datasets with different characteristics. Since the efficiencyof approximate joins depend on the similarity threshold they use; we study how the value …,QDB,2007,25
Tomas: A system for adapting mappings while schemas evolve,Yannis Velegrakis; Renée J Miller; Lucian Popa; John Mylopoulos,We demonstrate the Toronto Mapping Adaptation System (ToMAS); a tool for automaticallydetecting and adapting mappings that have become invalid or inconsistent due to changesin either data semantics or schemas. Due to its modular architecture and its stand-alonenature; ToMAS can easily be applied to numerous scenarios and can interoperate withmany other tools. To the best of our knowledge; no other tool can correctly maintain theconsistency of the mappings under schema changes at the level of complexity supported byToMAS.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,25
Mining for Program Structure,RENÉE J MILLER; ASHISH GUJARATHI,We present a new mining algorithm for discovering program structure built on a warehouseof program-analysis data. We make use of statement definition-use data along with variabletyping information to learn about the structure of legacy code. Using concept analysis; wedevelop an interactive knowledge discovery framework into which different types of analysisdata can be integrated. This work is the first in a series of data warehousing experiments thatis designed to tap the wealth of information hidden in existing program-analysis data.,International Journal of Software Engineering and Knowledge Engineering,1999,25
Muse: A System for Understanding and Designing Mappings,Bogdan Alexe; Laura Chiticariu; Renée J Miller; Daniel Pepper; Wang-Chiew Tan,Abstract Schema mappings are logical assertions that specify the relationships between asource and a target schema in a declarative way. The specification of such mappings is afundamental problem in information integration. Mappings can be generated by existingmapping systems (semi-) automatically from a visual specification between two schemas. Ingeneral; the well-known 80-20 rule applies for mapping generation tools. They canautomate 80% of the work; covering common cases and creating a mapping that is close tocorrect. However; ensuring complete correctness can still require intricate manual work toperfect portions of the mapping. Previous research on mapping understanding andrefinement and anecdotal evidence from mapping designers suggest that the mappingdesign process can be perfected by using data examples to explain the mapping and …,SIGMOD,2008,22
Linking Semistructured Data on the Web,Soheil Hassas Yeganeh; Oktie Hassanzadeh; Renée J Miller,ABSTRACT Many Web data sources and APIs make their data available in XML; JSON; or adomain-specific semi-structured format; with the goal of making the data easily accessibleand usable by Web application developers. Although such data formats are more machine-processable than pure text documents; managing and analyzing such data in large scale isoften nontrivial. This is mainly due to the lack of a well-defined (or understood) structure andclear semantics in such data formats; which could result in poor data quality. In the xCuratorproject; we add structure to such data with the goal of publishing it on the Web as LinkedData. We enhance the quality of such data by: extracting entities; their types; and theirrelationships to other entities; performing entity (and entity type) identification; mergingduplicate entities (and entity types); linking related entities (internally and to external …,ACM SIGMOD WebDB,2011,21
DataWeb: Customizable Database Publishing for the Web,Renée J Miller; Odysseas G. Tsatalos; John H. Williams,The DataWeb project seeks to facilitate the querying and browsing of multimedia databasesover a wide spectrum of networks and media. We developed an intelligent query facility thatbuilds on the access paradigms supported by current hypertext-style Web applications andon decision support systems. This facility generalizes the basic navigation and abstractionmechanisms of these applications to make them extensible and scalable.,IEEE Multimedia,1997,21
Combining quantitative and logical data cleaning,Nataliya Prokoshyna; Jaroslaw Szlichta; Fei Chiang; Renée J Miller; Divesh Srivastava,Abstract Quantitative data cleaning relies on the use of statistical methods to identify andrepair data quality problems while logical data cleaning tackles the same problems usingvarious forms of logical reasoning over declarative dependencies. Each of theseapproaches has its strengths: the logical approach is able to capture subtle data qualityproblems using sophisticated dependencies; while the quantitative approach excels atensuring that the repaired data has desired statistical properties. We propose a novelframework within which these two approaches can be used synergistically to combine theirrespective strengths. We instantiate our framework using (i) metric functional dependencies;a type of dependency that generalizes functional dependencies (FDs) to identifyinconsistencies in domains where only large differences in metric data are considered to …,Proceedings of the VLDB Endowment,2015,20
Schema management,Periklis Andritsos; Ronald Fagin; Ariel Fuxman; Laura M Haas; Mauricio A Hernández; Howard Ho; Anastasios Kementsietsidis; Phokion G Kolaitis; Renée J Miller; Felix Naumann; Lucian Popa; Yannis Velegrakis,Abstract Clio is a management system for heterogeneous data that couples a traditionaldatabase management engine with additional tools for managing schemas (models of data)and mappings between schemas. In this article; we provide a brief overview of Clio andplace our solutions in the context of the rich research literature on data integration andtransformation. Clio is the result of an on-going collaboration between the University ofToronto and IBM Almaden Research Center in which we are addressing both foundationaland systems issues related to heterogeneous data; schema; and integration management.,*,2002,19
Composing local-as-view mappings: closure and applications,Patricia C Arocena; Ariel Fuxman; Reneé J Miller,Abstract Schema mapping composition is a fundamental operation in schema managementand data exchange. The mapping composition problem has been extensively studied for anumber of mapping languages; most notably source-to-target tuple-generatingdependencies (st tgds). An important class of st tgds are local-as-view (LAV) tgds. This classof mappings is prevalent in practical data integration and exchange systems; and recentwork by ten Cate and Kolaitis shows that such mappings possess desirable structuralproperties. It is known that st tgds are not closed under composition. That is; given twomappings expressed with st tgds; their composition may not be definable by any set of sttgds (and; in general; may not be expressible in first-order logic). Despite their importanceand extensive use in data integration and exchange systems; the closure properties of …,Proceedings of the 13th International Conference on Database Theory,2010,18
Kanata: adaptation and evolution in data sharing systems,Periklis Andritsos; Ariel Fuxman; Anastasios Kementsietsidis; Renee J Miller; Yannis Velegrakis,1. INTRODUCTION Data sharing systems permit the transformation; integra- tion; and exchangeof data that has been designed and devel- oped independently. The often subtle and complexinterde- pendencies within data can make the creation; maintenance; and comprehension ofsuch systems quite challenging. We have available a robust arsenal of tools and mechanismsfor reconciling semantic differences in how data is represented including views; mappings; andtransformation languages. There is also a growing maturity in our knowledge of how to create[19; 20] and use these mechanisms in such tasks as query answering [13]; data exchange[9]; data integration [17]; and data sharing [15]. Given this solid foundation in the tools and modelingstructures needed for data sharing; in this project; we are investigating the problem of maintainingsuch systems. We view an integration system as a dynamic structure which must be …,ACM SIGMOD Record,2004,17
Schema Discovery.,Renee J Miller; Periklis Andritsos,Abstract Structured data is distinguished from unstructured data by the presence of aschema describing the logical structure and semantics of the data. The schema is the meansthrough which we understand and query the underlying data. The schema permits the moresophisticated structured queries that are not possible over schema-less data. Most systemsassume that the schema is predefined and is an accurate reflection of the data. Thisassumption is often not valid in networked databases that may contain data originating frommany sources and may not be valid within legacy databases where the semantics of datahave evolved over time. As a result; querying and tasks that depend on structured queries(including data integration and schema mapping) may not be effective. In this paper; weconsider the problem of discovering schemas from data. We focus on discovering …,IEEE Data Eng. Bull.,2003,17
Querying multimedia presentations,Chao-Hui Wu; Renée J Miller; Ming T Liu,Abstract We examine the querying requirements of large libraries of multimediapresentations. Given these requirements; we examine work on querying temporal andsequence data; and identify the similarities and dissimilarities between presentations andthese other forms of data. From this analysis; we propose an integrated composition andquery capability to permit the reuse of multimedia objects; presentations and presentationsegments. The query facility permits content and attribute based queries along with queriesover temporal synchronisation characteristics. The main contributions of our work are: itaddresses both determinant and indeterminant intervals; it permits querying overpresentation libraries with heterogeneous structure; it builds on work from temporal andsequence database to address the unique semantics of presentations.,Computer Communications,1998,17
(Not) yet another matcher,Fabien Duchateau; Remi Coletta; Zohra Bellahsene; Renée J Miller,Abstract Discovering correspondences between schema elements is a crucial task for dataintegration. Most schema matching tools are semi-automatic; eg an expert must tune someparameters (thresholds; weights; etc.). They mainly use several methods to combine andaggregate similarity measures. However; their quality results often decrease when onerequires to integrate a new similarity measure or when matching particular domain schemas.This paper describes YAM (Yet Another Matcher); which is a schema matcher factory.Indeed; it enables the generation of a dedicated matcher for a given schema matchingscenario; according to user inputs. Our approach is based on machine learning sinceschema matchers can be seen as classifiers. Several bunches of experiments run againstmatchers generated by YAM and traditional matching tools show how our approach is …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,16
Creating nested mappings with Clio,Mauricio A Hernández; Howard Ho; Lucian Popa; Ariel Fuxman; Renee J Miller; Takeshi Fukuda; Paolo Papotti,Schema mappings play a central role in many data integration and data exchangescenarios. In those applications; users need to quickly and correctly specify how datarepresented in one format is converted into a different format. Clio (L. Popa et al.; 2002) is ajoint research project between IBM and the University of Toronto studying the creation;maintenance; and use of schema mappings. There have always been two goals in our workin Clio: 1) the automatic creation of logical assertions that capture the way one or moresource schemas are mapped into a target schema; and 2) the generation of transformationqueries or programs that transform a source data instance into a target data instance.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,16
The iBench integration metadata generator,Patricia C Arocena; Boris Glavic; Radu Ciucanu; Renée J Miller,Abstract Given the maturity of the data integration field it is surprising that rigorous empiricalevaluations of research ideas are so scarce. We identify a major roadblock for empiricalwork-the lack of comprehensive metadata generators that can be used to createbenchmarks for different integration tasks. This makes it difficult to compare integrationsolutions; understand their generality; and understand their performance. We presentiBench; the first metadata generator that can be used to evaluate a wide-range of integrationtasks (data exchange; mapping creation; mapping composition; schema evolution; amongmany others). iBench permits control over the size and characteristics of the metadata itgenerates (schemas; constraints; and mappings). Our evaluation demonstrates that iBenchcan efficiently generate very large; complex; yet realistic scenarios with different …,Proceedings of the VLDB Endowment,2015,15
LabBook: Metadata-driven social collaborative data analysis,Eser Kandogan; Mary Roth; Peter Schwarz; Joshua Hui; Ignacio Terrizzano; Christina Christodoulakis; Renée J Miller,Open data analysis platforms are being adopted to support collaboration in science andbusiness. Studies suggest that analytic work in an enterprise occurs in a complex ecosystemof people; data; and software working in a coordinated manner. These studies also point tofriction between the elements of this ecosystem that reduces user productivity and quality ofwork. LabBook is an open; social; and collaborative data analysis platform designedexplicitly to reduce this friction and accelerate discovery. Its goal is to help users leverageeach other's knowledge and experience to find the data; tools and collaborators they need tointegrate; visualize; and analyze data. The key insight is to collect and use more metadataabout all elements of the analytic ecosystem by means of an architecture and userexperience that reduce the cost of contributing such metadata. We demonstrate how …,Big Data (Big Data); 2015 IEEE International Conference on,2015,14
Publishing bibliographic data on the Semantic Web using BibBase,Reynold S Xin; Oktie Hassanzadeh; Christian Fritz; Shirin Sohrabi; Renée J Miller,Abstract We present BibBase; a system for publishing and managing bibliographic dataavailable in BiBTeX files. BibBase uses a powerful yet light-weight approach to transformBiBTeX files into rich Linked Data as well as custom HTML code and RSS feed that canreadily be integrated within a user's website while the data can instantly be queried onlineon the system's SPARQL endpoint. In this paper; we present an overview of several featuresof our system. We outline several challenges involved in on-the-fly transformation of highlyheterogeneous BiBTeX files into high-quality Linked Data; and present our solution to thesechallenges.,Semantic Web,2013,14
Online annotation of text streams with structured entities,Ken Q Pu; Oktie Hassanzadeh; Richard Drake; Renée J Miller,Abstract We propose a framework and algorithm for annotating unbounded text streams withentities of a structured database. The algorithm allows one to correlate unstructured anddirty text streams from sources such as emails; chats and blogs; to entities stored instructured databases. In contrast to previous work on entity extraction; our emphasis is onperforming entity annotation in a completely online fashion. The algorithm continuouslyextracts important phrases and assigns to them top-k relevant entities. Our algorithm does sowith a guarantee of constant time and space complexity for each additional word in the textstream; thus infinite text streams can be annotated. Our framework allows the onlineannotation algorithm to adapt to changing stream rate by self-adjusting multiple run-timeparameters to reduce or improve the quality of annotation for fast or slow streams …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,14
Enabling distributed petascale science,Andrew Baranovski; Shishir Bharathi; John Bresnahan; Ann Chervenak; Ian Foster; Dan Fraser; Tim Freeman; Dan Gunter; Keith Jackson; Kate Keahey; Carl Kesselman; David E Konerding; Nick Leroy; Mike Link; Miron Livny; Neill Miller; Robert Miller; Gene Oleynik; Laura Pearlman; Jennifer M Schopf; Robert Schuler; Brian Tierney,Abstract Petascale science is an end-to-end endeavour; involving not only the creation ofmassive datasets at supercomputers or experimental facilities; but the subsequent analysisof that data by a user community that may be distributed across many laboratories anduniversities. The new SciDAC Center for Enabling Distributed Petascale Science (CEDPS)is developing tools to support this end-to-end process. These tools include data placementservices for the reliable; high-performance; secure; and policy-driven placement of datawithin a distributed science environment; tools and techniques for the construction;operation; and provisioning of scalable science services; and tools for the detection anddiagnosis of failures in end-to-end data placement and distributed application hostingconfigurations. In each area; we build on a strong base of existing technology and have …,Journal of Physics: Conference Series,2007,13
ConQuer: A system for efficient querying over inconsistent databases,Ariel Fuxman; Diego Fuxman; Renée J Miller,Abstract Although integrity constraints have long been used to maintain data consistency;there are situations in which they may not be enforced or satisfied. In this demo; weshowcase ConQuer; a system for efficient and scalable answering of SQL queries ondatabases that may violate a set of constraints. ConQuer permits users to postulate a set ofkey constraints together with their queries. The system rewrites the queries to retrieve all(and only) data that is consistent with respect to the constraints. The rewriting is into SQL; sothe rewritten queries can be efficiently optimized and executed by commercial databasesystems.,Proceedings of the 31st international conference on Very large data bases,2005,13
Integrating hierarchical navigation and querying: a user customizable solution,Renée J Miller; Odysseas G Tsatalos; John H Williams,*,Proceedings of ACM Workshop on Effective Abstractions in Multimedia Layout; Presentation; and Interaction,1995,13
Understanding schemas,Renée J Miller; Yannis E Ioannidis; Raghu Ramakrishnan,Before the problem of schema integration and translation can be adequately addressed; aprecise understanding of schemas is needed. The authors present an analysis of the notionof schema as used by existing integration methodologies. They show how inherentambiguities and imprecision in traditional definitions of schema can hamper thedevelopment of formal schema integration methodologies. Specifically; traditional notion ofschema contain data in the form of metadata; as well as superfluous structuring informationthat is not semantically meaningful. It is important to cleanly separate structural informationfrom data; and remove from consideration artifacts of a specific data model or designmethodology. To this end; they introduce the notion of a schema intension to capture thesemantic content of a schema.,Research Issues in Data Engineering; 1993: Interoperability in Multidatabase Systems; 1993. Proceedings RIDE-IMS'93.; Third International Workshop on,1993,13
Geographically-sensitive link analysis,Hyun Chul Lee; Haifeng Liu; Renée J Miller,Abstract Many web pages and resources are primarily relevant to certain geographiclocations. For example; in many queries web pages on restaurants; hotels; or movie theatersare mostly relevant to those users who are in geographic proximity to these locations.Moreover; as the number of queries with a local component increases; searching for webpages which are relevant to geographic locations is becoming increasingly important. Theperformance of geographically-oriented search is greatly affected by how we usegeographic information to rank web pages. In this paper; we study the issue of ranking webpages using geographically-sensitive link analysis algorithms. More precisely; we study thequestion of whether geographic information can improve search performance. We proposeseveral geographically-sensitive link analysis algorithms which exploit the geographic …,Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence,2007,12
Provenance for Data Mining,Boris Glavic; Javed Siddique; Periklis Andritsos; Renée J Miller,Abstract Data mining aims at extracting useful information from large datasets. Most datamining approaches reduce the input data to produce a smaller output summarizing themining result. While the purpose of data mining (extracting information) necessitates thisreduction in size; the loss of information it entails can be problematic. Specifically; the resultsof data mining may be more confusing than insightful; if the user is not able to understand onwhich input data they are based and how they were created. In this paper; we argue that theuser needs access to the provenance of mining results. Provenance; while extensivelystudied by the database; workflow; and distributed systems communities; has not yet beenconsidered for data mining. We analyze the differences between database; workflow; anddata mining provenance; suggest new types of provenance; and identify new usecases …,5th USENIX Workshop on the Theory and Practice of Provenance,2013,11
Reexamining Some Holy Grails of Data Provenance.,Boris Glavic; Renée J Miller,Abstract We reconsider some of the explicit and implicit properties that underlie well-established definitions of data provenance semantics. Previous work on comparingprovenance semantics has mostly focused on expressive power (does the provenancegenerated by a certain semantics subsume the provenance generated by other semantics)and on understanding whether a semantics is insensitive to query rewrite (ie; do equivalentqueries have the same provenance). In contrast; we try to investigate why certain semanticspossess specific properties (like insensitivity) and whether these properties are alwaysdesirable. We present a new property stability with respect to query language extension that;to the best of our knowledge; has not been isolated and studied on its own.,TaPP,2011,11
Stream schema: providing and exploiting static metadata for data stream processing,Peter M Fischer; Kyumars Sheykh Esmaili; Renée J Miller,Abstract Schemas; and more generally metadata specifying structural and semanticconstraints; are invaluable in data management. They facilitate conceptual design andenable checking of data consistency. They also play an important role in permitting semanticquery optimization; that is; optimization and processing strategies that are often highlyeffective; but only correct for data conforming to a given schema. While the use of metadatais well-established in relational and XML databases; the same is not true for data streams.The existing work mostly focuses on the specification of dynamic information. In this paper;we consider the specification of static metadata for streams in a model called StreamSchema. We show how Stream Schema can be used to validate the consistency of streams.By explicitly modeling stream constraints; we show that stream queries can be simplified …,Proceedings of the 13th International Conference on Extending Database Technology,2010,11
Method for generating nested mapping specifications in a schema mapping formalism,*,A method for generating nested mapping specifications and transformation queries basedthereon. Basic mappings are generated based on source and target schemas andcorrespondences between elements of the schemas. A directed acyclic graph (DAG) isconstructed whose edges represent ways in which each basic mapping is nestable underany of the other basic mappings. Any transitively implied edges are removed from the DAG.Root mappings of the DAG are identified. Trees of mappings are automatically extractedfrom the DAG; where each tree of mappings is rooted at a root mapping and expresses anested mapping specification.,*,2008,11
Messing up with BART: error generation for evaluating data-cleaning algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract We study the problem of introducing errors into clean databases for the purpose ofbenchmarking data-cleaning algorithms. Our goal is to provide users with the highestpossible level of control over the error-generation process; and at the same time developsolutions that scale to large databases. We show in the paper that the error-generationproblem is surprisingly challenging; and in fact; NP-complete. To provide a scalablesolution; we develop a correct and efficient greedy algorithm that sacrifices completeness;but succeeds under very reasonable assumptions. To scale to millions of tuples; thealgorithm relies on several non-trivial optimizations; including a new symmetry property ofdata quality constraints. The trade-off between control and scalability is the main technicalcontribution of the paper.,Proceedings of the VLDB Endowment,2015,10
Value invention in data exchange,Patricia C Arocena; Boris Glavic; Renée J Miller,Abstract The creation of values to represent incomplete information; often referred to asvalue invention; is central in data exchange. Within schema mappings; Skolem functionshave long been used for value invention as they permit a precise representation of missinginformation. Recent work on a powerful mapping language called second-order tuplegenerating dependencies (SO tgds); has drawn attention to the fact that the use of arbitrarySkolem functions can have negative computational and programmatic properties in dataexchange. In this paper; we present two techniques for understanding when the Skolemfunctions needed to represent the correct semantics of incomplete information arecomputationally well-behaved. Specifically; we consider when the Skolem functions insecond-order (SO) mappings have a first-order (FO) semantics and are therefore …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,10
Just-in-time data integration in action,Martin Hentschel; Laura Haas; Renée J Miller,Abstract Today's data integration systems must be flexible enough to support the typicaliterative and incremental process of integration; and may need to scale to hundreds of datasources. In this work we present a novel data integration system that offers great flexibilityand scalability. Our approach to data integration is unique in that it executes mapping rulesat query runtime using annotations. On top; we have built the People People Peopleapplication. It allows users to search for people; display information about people; andbrowse through a network of related people; where the data is integrated from local andremote data sources. The demo presents all features of our underlying data integrationengine through a set of motivating scenarios.,Proceedings of the VLDB Endowment,2010,10
Limbo: A scalable algorithm to cluster categorical data,Periklis Andritsos; Panayiotis Tsaparas; Renee J Miller; Kenneth C Sevcik,Abstract Clustering is a problem of great practical importance in numerous applications. Theproblem of clustering becomes more challenging when the data is categorical; that is; whenthere is no inherent distance measure between data values. In this work; we introduceLIMBO; a scalable hierarchical categorical clustering algorithm that builds on the InformationBottleneck (IB) framework for quantifying the relevant information preserved when clustering.We use the IB framework to define a distance measure for categorical tuples and we alsopresent a novel distance measure for categorical attribute values. We show how the LIMBOalgorithm can be used to cluster both tuples and attribute values. LIMBO handles large datasets by producing a summary model for the data. We propose two different versions ofLIMBO; where we either control the size or the accuracy of the model. We present an …,*,2003,10
A Collaborative Investigation of Program-Analysis-Based Testing and Maintenance,Mary Jean Harrold; Renee Miller; Adam Porter; Gregg Rothermel,Throughout their lifetimes; software systems undergo numerous changes. To make thesechanges; we have to understand these systems; identify and evaluate alternative modication strategies; implement the changes; and validate their correctness. In practice; the costof these activities is enormous 1; 2; 5; 6]. Consider the problem of validating modi edsoftware. One common way to do this is to rerun tests from existing test suites (calledregression testing). Although valuable; this is often very expensive. For instance; somecompanies must release software products for users who speak di erent languages.Typically; they release an initial version and then localize it for speci c languages. Beforereleasing a localized version; they regression test it. Since localized versions must beavailable as soon as possible; regression testing time must be reduced. Another common …,Proc. 2nd Int'l Workshop on Emp. Studies of Softw. Maint.; Bari; Italy; October 3; 1997,1997,10
Big Data Curation.,Renée J Miller,□ Big data is no more exact a notion than big hair□ Data isn't a plural noun like pebbles; it'sa mass noun like dust□ When you've got algorithms weighing hundreds of factors over ahuge data set; you can't really know why they come to a particular decision or whether itreally makes sense,COMAD,2014,9
Schema intension graphs: A formal model for the study of schema equivalence,Renée J Miller; Yannis E Ioannidis; Raghu Ramakrishnan,Abstract We develop a formal data model; the Schema Intension Graph (SIG) model; to aidin understanding the relative information capacity of schémas containing constraints. Thebasic building blocks of the SIG model are sets that may be combined by the nestedapplication of union and product constructors. The model also permits the expression ofbinary relations on sets and simple integrity constraints on these relations. We discuss themotivation used in designing the model and establish some fundamental results on themodel. We consider the problem of constraint implication in the SIG model and give a soundand complete set of implication rules for a subclass of SIG schémas; called simple SIGschémas. The general constraint implication problem is shown to be undecidable. Finally;we consider information capacity preserving translations of a subclass of relational …,*,1994,9
Active repair of data quality rules,Fei Chiang; Renée J Miller,Abstract: The use of data quality rules; which capture business rules and domain constraints;is central to most data quality processes. Poor data quality often arises when the data andthese rules (which are meant to preserve data integrity) become inconsistent. To resolveinconsistencies; organizations often implement specific; sometimes manual; sometimescomputer-aided; cleansing routines to fix the errors. This solution necessitates frequentrepetition of the data cleaning to resolve inconsistencies continually as the data evolves orgrows. It is important to recognize that modern organizations may be as dynamic as theirdata. The business rules; application domain constraints; and data semantics will evolve. Asbusiness policies change; as data is integrated with new sources; and as the underlyingdata evolves; it becomes necessary to manage; evolve; and repair both the data and the …,Proceedings of the 16th International Conference on Information Quality (ICIQ),2011,8
Authorization-transparent access control for XML under the non-Truman model,Yaron Kanza; Alberto O Mendelzon; Renée J Miller; Zheng Zhang,Abstract In authorization-transparent access control; user queries are formulated against thedatabase schema rather than against authorization views that transform and hide data. TheTruman and the Non-Truman are two approaches to authorization transparency where in aTruman model; queries that violate the access restrictions are modified transparently by thesystem to only reveal accessible data; while in a Non-Truman model; such queries arerejected. The advantage of a Non-Truman model is that the semantics of user queries is notchanged by the access-control mechanism. This work presents an access-controlmechanism for XML; under the Non-Truman model. Security policies are specified asparameterized rules formulated using XPath. The rules specify relationships betweenelements; that should be concealed from users. Hence; not only elements; but also edges …,International Conference on Extending Database Technology,2006,8
System and method for schema mappings,*,*,*,2006,8
Limbo: A linear algorithm to cluster categorical data,Periklis Andritsos; Panayiotis Tsaparas; Renщee J Miller; Kenneth C Sevcik,*,*,2003,8
Reverse engineering meets data analysis,Periklis Andritsos; Renée J Miller,We demonstrate how the data management techniques known as On-Line AnalyticalProcessing; or OLAP; can be used to enhance the sophistication and range of softwarereverse engineering tools. This is the first comprehensive examination of the similarities anddifferences in these tasks both in how OLAP techniques meet (or fail to meet) the needs ofreverse engineering and in how reverse engineering can be recast using data analysis. Topermit the seamless integration of these technologies; we extend a multidimensional datamodel to manage dynamically changing dimensions (over which data can be aggregated).We use a case study of the Apache Web server to show how our solutions permit anintegrated view of data; ranging from low level program analysis information to abstract;aggregate information. These high-level abstractions may be provided either by humans …,Program Comprehension; 2001. IWPC 2001. Proceedings. 9th International Workshop on,2001,8
Exploring XML web collections with DescribeX,Mariano P Consens; Renée J Miller; Flavio Rizzolo; Alejandro A Vaisman,Abstract As Web applications mature and evolve; the nature of the semistructured data thatdrives these applications also changes. An important trend is the need for increasedflexibility in the structure of Web documents. Hence; applications cannot rely solely onschemas to provide the complex knowledge needed to visualize; use; query and managedocuments. Even when XML Web documents are valid with regard to a schema; the actualstructure of such documents may exhibit significant variations across collections for severalreasons: the schema may be very lax (eg; RSS feeds); the schema may be large anddifferent subsets of it may be used in different documents (eg; industry standards like UBL);or open content models may allow arbitrary schemas to be mixed (eg; RSS extensions likethose used for podcasting). For these reasons; many applications that incorporate XPath …,ACM Transactions on the Web (TWEB),2010,7
A first step towards integration independence,Laura M Haas; Renée J Miller; Donald Kossmann; Martin Hentschel,Two major forms of information integration; federation and materialization; continue todominate the market; embedded in separate products; each with their strengths andweaknesses. Application developers must make difficult choices among techniques andproducts; choices that are hard to change later. We propose a new design principle;Integration Independence; for integration engines. Integration independence frees theapplication designer from deciding how to integrate data. We then describe a new; adaptiveinformation integration engine that provides the ability to index base data or to materializetransformed data; giving us a flexible platform for experimentation.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,7
LSH ensemble: Internet-scale domain search,Erkang Zhu; Fatemeh Nargesian; Ken Q Pu; Renée J Miller,Abstract We study the problem of domain search where a domain is a set of distinct valuesfrom an unspecified universe. We use Jaccard set containment score; defined as| Q∩ X|/| Q|;as the measure of relevance of a domain X to a query domain Q. Our choice of Jaccard setcontainment over Jaccard similarity as a measure of relevance makes our work particularlysuitable for searching Open Data and data on the web; as Jaccard similarity is known tohave poor performance over sets with large differences in their domain sizes. Wedemonstrate that the domains found in several real-life Open Data and web datarepositories show a power-law distribution over their domain sizes. We present a new indexstructure; Locality Sensitive Hashing (LSH) Ensemble; that solves the domain searchproblem using set containment at Internet scale. Our index structure and search algorithm …,Proceedings of the VLDB Endowment,2016,6
Data Anamnesis: Admitting Raw Data into an Organization.,Sebastian Kruse; Thorsten Papenbrock; Hazar Harmouch; Felix Naumann,Abstract Today's internet offers a plethora of openly available datasets; bearing greatpotential for novel applications and research. Likewise; rich datasets slumber withinorganizations. However; all too often those datasets are available only as raw dumps andlack proper documentation or even a schema. Data anamnesis is the first step of any effort towork with such datasets: It determines fundamental properties regarding the datasets'content; structure; and quality to assess their utility and to put them to use appropriately.Detecting such properties is a key concern of the research area of data profiling; which hasdeveloped several viable instruments; such as data type recognition and foreign keydiscovery. In this article; we perform an anamnesis of the MusicBrainz dataset; an openlyavailable and complex discographic database. In particular; we employ data profiling …,IEEE Data Eng. Bull.,2016,6
The Vivification Problem in Real-Time Business Intelligence: A Vision,Patricia C Arocena; Renée J Miller; John Mylopoulos,Abstract In the new era of Business Intelligence (BI) technology; transforming massiveamounts of data into high-quality business information is essential. To achieve this; two non-overlapping worlds need to be aligned: the Information Technology (IT) world; representedby an organization's operational data sources and the technologies that manage them (datawarehouses; schemas; queries;...); and the business world; portrayed by business plans;strategies and goals that an organization aspires to fulfill. Alignment in this context meansmapping business queries into BI queries; and interpreting the data retrieved from the BIquery in business terms. We call the creation of this interpretation the vivification problem.The main thesis of this position paper is that solutions to the vivification problem should bebased on a formal framework that explicates assumptions and the other ingredients …,International Workshop on Business Intelligence for the Real-Time Enterprise,2012,6
Enabling Real-Time Business Intelligence,Malu Castellanos; Umeshwar Dayal; Renée J Miller,In today's competitive and highly dynamic environment; analyzing data to understand howthe business is performing; and to predict outcomes and trends have become critical. Thetraditional approach to reporting is no longer adequate. Instead users now demand easy-to-use intelligent platforms and applications capable of analyzing realtime data to provideinsight and actionable information at the right time. The end goal is to support better andtimelier decision making; enabled by the availability of up-todate; high-quality information.Although there has been progress in this direction and many companies are introducingproducts toward meeting this goal; there is still a long way to go. In particular; the wholelifecycle of business intelligence requires innovative techniques and methodologies capableof dealing with the requirements imposed by these new generation BI applications. From …,Lecture Notes in Business Information Processing,2010,6
Retrospective on Clio: Schema Mapping and Data Exchange in Practice.,Renée J Miller,Abstract. Clio is a joint research project between the University of Toronto and IBM AlmadenResearch Center started in 1999 to address both foundational and systems issues related tothe management of heterogeneous data. In this talk; I will take a look back over the last eightyears of this project to review its achievements; the lessons learned; and the challenges thatremain.,Description Logics,2007,6
Automatic Curation of Clinical Trials Data in LinkedCT,Oktie Hassanzadeh; Renée J Miller,Abstract The Linked Clinical Trials (LinkedCT) project started back in 2008 with the goal ofproviding a Linked Data source of clinical trials. The source of the data is from the XML datapublished on ClinicalTrials. gov; which is an international registry of clinical studies. Sincethe initial release; the LinkedCT project has gone through some major changes to bothimprove the quality of the data and its freshness. The result is a high-quality Linked Datasource of clinical studies that is updated daily; currently containing over 195;000 trials; 4.6million entities; and 42 million triples. In this paper; we present a detailed description of thesystem along with a brief outline of technical challenges involved in curating the raw XMLdata into high-quality Linked Data. We also present usage statistics and a number ofinteresting use cases developed by external parties. We share the lessons learned in the …,International Semantic Web Conference,2015,5
An evaluation of clustering algorithms in duplicate detection,Bilal Hussain; Oktie Hassanzadeh; Fei Chiang; Hyun Chul Lee; Renée J Miller,HC Lee Content Understanding & Personalization; LinkedIn E-mail: culee@ linkedin. comgration and cleaning of large databases. In this paper; we focus on a class of duplicatedetection algorithms that rely on clustering a similarity graph. Each node in the similaritygraph represents a record and the weight of an edge connecting two nodes reflects theamount of similarity between the corresponding records. The similarity graph can beefficiently constructed using state-of-the-art similarity join techniques. For duplicatedetection; a clustering algorithm over the similarity graph is used to produce sets of recordsthat are likely to represent the same entity. In this paper; we present a framework forevaluating the effectiveness of clustering algorithms for duplicate detection. We present theresults of our extensive evaluation of a wide range of clustering algorithms. Our …,Technical Report CSRG-620; University of Toronto; Department of Computer Science,2013,5
Autodict: Automated dictionary discovery,Fei Chiang; Periklis Andritsos; Erkang Zhu; Renée J Miller,An attribute dictionary is a set of attributes together with a set of common values of eachattribute. Such dictionaries are valuable in understanding unstructured or loosely structuredtextual descriptions of entity collections; such as product catalogs. Dictionaries provide thesupervised data for learning product or entity descriptions. In this demonstration; we willpresent AutoDict; a system that analyzes input data records; and discovers high qualitydictionaries using information theoretic techniques. To the best of our knowledge; AutoDict isthe first end-to-end system for building attribute dictionaries. Our demonstration willshowcase the different information analysis and extraction features within AutoDict; andhighlight the process of generating high quality attribute dictionaries.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,5
Semantic Link Discovery over Relational Data,Oktie Hassanzadeh; Anastasios Kementsietsidis; Lipyeow Lim; Renée J Miller; Min Wang,Abstract To make semantic search a reality; we need to be able to efficiently publish largedata sets containing rich semantic structure. We have tools for translating relational andsemi-structured data into RDF; but such translation tools do not have the goal of adding orproviding the kind of semantics necessary to achieve the goals of the Semantic Web andsemantic search over the Web. In this chapter; we present LinQuer; a tool for creatingsemantic links within a data source and between data sources. We focus on link discoveryover structured (relational) data since many Semantic Web sources are the result ofpublishing relational data as RDF and since relational engines provide the scalability andflexibility we need for large scale link discovery. The LinQuer framework is based on thedeclarative specification of linkage requirements by a user. We present algorithms for …,*,2012,4
Debugging data exchange with vagabond,Boris Glavic; Jiang Du; Renée J Miller; Gustavo Alonso; Laura M Haas,In this paper; we present Vagabond; a system that uses a novel holistic approach to helpusers to understand and debug data exchange scenarios. Developing such a scenario is acomplex and labor-intensive process where errors are often only revealed in the targetinstance produced as the result of this process. This makes it very hard to debug suchscenarios; especially for non-power users. Vagabond aides a user in debugging byautomatically generating possible explanations for target instance errors identified by theuser. Schema mappings are declarative constraints that model the relationship between asource and a target schema. Data exchange systems; such as Clio [6]; ORCHESTRA [5];and many others; use schema mappings to produce an instance of the target schema basedon an instance of the source schema. Creating a mapping between two schemata is a …,Proceedings of the VLDB Endowment,2011,4
Scalable Data Integration by Mapping Data to Queries,Martin Hentschel; Donald Kossmann; Daniela Florescu; Laura Haas; Tim Kraska; Renée J Miller,The goal of a data integration system is to allow users to query diverse information sourcesthrough a schema that is familiar to them. However; there may be many different users whomay have dif-ferent preferred schemas; and the data may be stored in data sources whichuse still other schemas. To integrate data; mapping rules must be defined to map entities ofthe data sources to entities of the users' schemas. In large information systems with manydata sources which serve sophisticated applications; there can be many such mapping rulesand they can be complex. The purpose of this paper is to study the per-formance ofalternative query processing techniques for data integration systems with many complexmapping rules. A new approach; mapping data to queries (MDQ); is presented. Throughextensive performance experiments; it is shown that this approach performs well for …,Technical report/[ETH; Department of Computer Science,2009,4
Benchmarking Data Curation Systems.,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,Abstract Data curation includes the many tasks needed to ensure data maintains its valueover time. Given the maturity of many data curation tasks; including data transformation anddata cleaning; it is surprising that rigorous empirical evaluations of research ideas are soscarce. In this work; we argue that thorough evaluation of data curation systems imposesseveral major obstacles that need to be overcome. First; we consider the outputs generatedby a data curation system (for example; an integrated or cleaned database or a set ofconstraints produced by a schema discovery system). To compare the results of differentsystems; measures of output quality should be agreed upon by the community and; sincesuch measures can be quite complex; publicly available implementations of these measuresshould be developed; shared; and optimized. Second; we consider the inputs to the data …,IEEE Data Eng. Bull.,2016,3
Gain control over your integration evaluations,Patricia C Arocena; Radu Ciucanu; Boris Glavic; Renée J Miller,Abstract Integration systems are typically evaluated using a few real-world scenarios (eg;bibliographical or biological datasets) or using synthetic scenarios (eg; based on star-schemas or other patterns for schemas and constraints). Reusing such evaluations is acumbersome task because their focus is usually limited to showcasing a specific feature ofan approach. This makes it difficult to compare integration solutions; understand theirgenerality; and understand their performance for different application scenarios. Based onthis observation; we demonstrate some of the requirements for developing integrationbenchmarks. We argue that the major abstractions used for integration problems haveconverged in the last decade which enables the application of robust empirical methods tointegration problems (from schema evolution; to data exchange; to answering queries …,Proceedings of the VLDB Endowment,2015,3
HOMER: Ontology alignment visualization and analysis,Octavian Udrea; Lise Getoor; Renée J Miller,Abstract. We present HOMER; an analysis and visualization tool for ontology alignment.HOMER features a radial-graph display GUI; a complete execution trace that allows the userto override and navigate to any match decisions during at runtime and a comparison modethat displays multiple alignments in parallel. HOMER contains a builtin plugin for the ILIADSontology alignment algorithm; but other algorithms can be plugged in as well.,International Semantic Web Conference (ISWC),2007,3
A collective; probabilistic approach to schema mapping,Angelika Kimmig; Alex Memory; Lise Getoor,We propose a probabilistic approach to the problem of schema mapping. Our approach isdeclarative; scalable; and extensible. It builds upon recent results in both schema mappingand probabilistic reasoning and contributes novel techniques in both fields. We introducethe problem of mapping selection; that is; choosing the best mapping from a space ofpotential mappings; given both metadata constraints and a data example. As selection hasto reason holistically about the inputs and the dependencies between the chosen mappings;we define a new schema mapping optimization problem which captures interactionsbetween mappings. We then introduce Collective Mapping Discovery (CMD); our solution tothis problem using stateof-the-art probabilistic reasoning techniques; which allows forinconsistencies and incompleteness. Using hundreds of realistic integration scenarios …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,2
DeepSea: Progressive Workload-Aware Partitioning of Materialized Views in Scalable Data Analytics.,Jiang Du; Renée J Miller; Boris Glavic; Wei Tan,ABSTRACT Selective materialization of intermediate query results as views is an effectivemethod for improving query performance. In this paper; we extend this technique toadaptively partition views based on the access patterns of a workload. That is; we collectinformation about the selection conditions of queries at runtime and utilize this information todetermine fragment boundaries for the initial partitioning when materializing a view.Furthermore; we refine view partitions over time based on the selection conditions ofincoming queries. We present a novel cost-benefit model for partitioned views; as well as acandidate view and fragment selection approach-both of which exploit the nature ofpartitioned views by taking the correlation among view fragments into account. Furthermore;we present DeepSea; an implementation of these techniques built on top of Hive. Our …,EDBT,2017,2
Vizcurator: A visual tool for curating open data,Bahar Ghadiri Bashardoost; Christina Christodoulakis; Soheil Hassas Yeganeh; Renée J Miller; Kelly Lyons; Oktie Hassanzadeh,Abstract Vizcurator permits the exploration; understanding and curation of open RDF data;its schema; and how it has been linked to other sources. We provide visualizations thatenable one to seamlessly navigate through RDFS and RDF layers and quickly understandthe open data; how it has been mapped or linked; how it has been structured (and could berestructured); and how deeply it has been related to other open data sources. Moreimportantly; Vizcurator provides a rich set of tools for data curation. It suggests possibleimprovements to the structure of the data and enables curators to make informed decisionsabout enhancements to the exploration and exploitation of the data. Moreover; Vizcuratorfacilitates the mining of temporal resources and the definition of temporal constraints throughwhich the curator can identify conflicting facts. Finally; Vizcurator can be used to create …,Proceedings of the 24th International Conference on World Wide Web,2015,2
VoidWiz: Resolving Incompleteness Using Network Effects,Christina Christodoulakis; Christos Faloutsos; Renée J Miller,If Lisa visits Dr. Brown; and there is no record of the drug he prescribed her; can we find it?Data sources; much to analysts' dismay; are too often plagued with incompleteness; makingbusiness analytics over the data difficult. Data entries with incomplete values are ignored;making some analytic queries fail to accurately describe how an organization is performing.We introduce a principled way of performing value imputation on missing values; allowing auser to choose a correct value after viewing possible values and why they were inferred. Weachieve this by turning our data into a graph network and performing link prediction onnodes of interest using the belief propagation algorithm.,ICDE,2014,2
Cutting the Knot: Explaining the Execution Semantics of Sliding Window Queries over Data Streams,Irina Botan; Roozbeh Derakhshan; Nihal Dindar; Laura Haas; Renee Miller; Nesime Tatbul,Abstract—Despite the availability of several data stream processing engines (SPEs) today; itremains hard to develop and maintain streaming applications. Existing SPEs vary widely intheir data and query models and capabilities. A lack of standards; and the wide (andchanging) variety of application requirements; restrict portability. Users find it difficult to knowwhich system to use; and even to understand the behavior of the system they choose. Ourgoal in this paper is to propose a formalism that can be used to explain a major subset ofthese different behaviors. We first provide an in-depth analysis of the heterogeneity problemacross three well-known and commonly used stream processing systems (two commercialand one academic); focusing on the execution semantics of sliding window queries. We thenpropose a simple yet powerful formal model that captures and explains the core …,ETH Zurich; Computer Science; Tech. Rep.; June,2009,2
Schema mapping,Ariel Fuxman; Renée J Miller,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,2
Association mining without support thresholds,C Jermain; Renee J Miller,Abstract We revisit the problem of mining for associations from a register transactiondatabase. To ensure the search for associations remains tractable; solutions have typicallymade use of a user-specified (data independent) support threshold. We considerapplications for which such a support threshold is inappropriate or unavailable. We tacklethe intractable problem of finding the most interesting associations without using support toprune the search space. Instead; we propose a novel probabilistic algorithm for finding themost interesting associations without enumerating and counting an exponential number ofalternatives. We demonstrate empirically that our (approximate) solution finds; with highprobability; the most interesting associations.,*,2001,2
Approximate Query Answering in High-Dimensional Data Cubes.,Chris Jermaine; Renee J Miller,Abstract Data mining work has been successful in both compressing and modeling largedata sets with many values. However; the problem of high-dimensions has not beensufficiently addressed. In our work; we develop a new data reduction method that aims tospeed subsequent data analysis by efficiently constructing a high-dimensional; jointprobability distribution. This distribution summarizes the data by identifying subspaces thatare the most unexpected. That is; we find regions that have many more or many fewer datapoints than expected. Our proposed data reduction method is defined with the inherentcomputational complexity of high-dimensional search in mind; and lends itself to efficientapproximation. We present results illustrating the effectiveness of this method in findingaccurate approximate answers to high dimensional data cube queries.,ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery,2000,2
VIQS: Visual Interactive Exploration of Query Semantics,Christina Christodoulakis; Eser Kandogan; Ignacio G Terrizzano; Renée J Miller,Abstract Analytics platforms such as IBM's Watson Analytics TM are collecting metadataabout their use; including user queries on uploaded datasets. The analysis of this metadatamay be valuable in improving services; such as query recommendation and automatic datavisualization. However; analysis of metadata is difficult not only in terms of scale but also interms of complexity. Generalizing and exploring query patterns across users and datasets ischallenging. Abstractions are likely to help bridge differences in specifics (eg; column namesand query details); particularly in semantics. For example; a single query;" What is the trendof sales over year?" could be abstracted in many different ways (eg;" What is the trend offinancial gain over time?"). In this paper; we describe our process of creating a dataset ofquery semantics; starting from initial metadata extraction from query logs to semantic …,Proceedings of the 2017 ACM Workshop on Exploratory Search and Interactive Data Analytics,2017,1
BARt in Action: Error Generation and Empirical Evaluations of Data-Cleaning Systems,Donatello Santoro; Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti,Abstract Repairing erroneous or conflicting data that violate a set of constraints is animportant problem in data management. Many automatic or semi-automatic data-repairingalgorithms have been proposed in the last few years; each with its own strengths andweaknesses. Bart is an open-source error-generation system conceived to support thoroughexperimental evaluations of these data-repairing systems. The demo is centered aroundthree main lessons. To start; we discuss how generating errors in data is a complex problem;with several facets. We introduce the important notions of detectability and repairability of anerror; that stand at the core of Bart. Then; we show how; by changing the features of errors; itis possible to influence quite significantly the performance of the tools. Finally; we concretelyput to work five data-repairing algorithms on dirty data of various kinds generated using …,Proceedings of the 2016 International Conference on Management of Data,2016,1
Error Generation for Evaluating Data Cleaning Algorithms,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renée J Miller; Paolo Papotti; Donatello Santoro,ABSTRACT We address the problem of generating errors within clean databases for thepurpose of benchmarking data-cleaning and data-repairing algorithms. Our goal is toprovide users with the highest possible level of control over the error generation processand at the same time develop a solution that scales to large databases. This is challenging;because the error generation problem is NP-complete. The main technical contribution ofthis paper is to develop an efficient PTIME algorithm which sacrifices completeness in anintelligent fashion that allows us to succeed under reasonable assumptions. However;scaling to databases with millions of tuples requires additional non-trivial optimizationsincluding exploiting a symmetry property of data quality constraints.,*,2015,1
Automated dictionary discovery for the online marketplace,Fei Chiang; Renée J Miller,Abstract Shopping online has become a prolific activity as the number of online vendors andconsumers continue to rise each year. In 2009; almost $15 billion in goods and serviceswere ordered online by Canadians [1]. About 53% of these consumers' window shop'bydoing product research before actually making a purchase. Therefore; it is important thatonline vendors provide up-to-date and accurate product information to assist users inmaking educated decisions. In this poster; we present a tool that discovers product features;which will assist vendors and consumers to more accurately compare products in the onlinemarketplace,Proceedings of the 2012 iConference,2012,1
Information Integration: a Vision for Integration Independence and Linking Open Data.,Renée J Miller,To integrate information; data in different formats; from different; potentially overlappingsources; must be related and transformed to meet the users' needs. Ten years ago; Cliointroduced a new paradigm for creating declarative schema mappings to describe therelationship between data in heterogeneous schemas. This enabled powerful tools formapping discovery and integration code generation; greatly simplifying the integrationprocess. In this talk; I take a look at where our field was a decade ago and where it is now interms of support for data integration. I share a vision for raising the level of abstractionfurther; to better isolate applications from the details of how the integration is accomplished.Integration independence allows applications to be independent of how; when; and whereinformation integration takes place; making materialization and the timing of …,AMW,2010,1
Encore un outil de découverte de correspondances entre schémas xml,Fabien Duchateau; Remi Coletta; Zohra Bellahsene; Renee J Miller,Dans cet article; nous présentons YAM (pour Yet Another Matcher); une fabrique d'outils demise en correspondance de schémas. YAM n'est pas un autre outil de découverte decorrespondances mais plutôt un générateur d'outils de découverte de correspondances enfonction des besoins de l'utilisateur. Ces besoins incluent notamment une préference entreprécision et rappel ainsi que des données d'entrainement (un ensemble de correspodancesdonnées par un expert ou un domaine d'intérêt). YAM utilise une base de connaissancesqui contient un ensemble (potentiellement grand) de mesures de similarité et de classifieurs.D'apres les besoins de l'utilisateur; YAM apprend appliquer ces outils (mesures de similaritéet classifieurs) afin d'obtenir la meilleure qualité possible. Dans cette démonstration; lesutilisateurs appliqueront YAM avec différents besoins pour produire le meilleur outil de …,*,2009,1
Integrating schema integration frameworks; algebraically,Zinovy Diskin; Steve Easterbrook; Renée Miller,Abstract The paper presents a framework; in which the main concepts of schema and dataintegration can be specified both semantically and syntactically in an abstract data-modelindependent way. We first define what are schema matching and integration semantically; interms of sets of instances and mappings between them. We also define a schema matchingand integration syntactically; and introduce a procedure (the how) for computing theintegration of matched schema according to the syntactical definition in fairly abstract terms.The main theorem of the paper states that the result of syntactical integration satisfies thesemantic definition and; hence; does produce what we really need. We show how thisframework unifies the approach taken in at least five other schema integration proposals andfills in some important gaps in these proposals. Viewed in a broader perspective; the …,Technical Report CSRG-583; Department of Computer Science,2008,1
System and Method for Translating Data from a Source Schema to a Target Schema,*,*,*,2004,1
Using Categorical Clustering in Schema Discovery.,Periklis Andritsos; Renée J Miller,ABSTRACT Most techniques for managing relational schemas assume a given schema thatadequately models the data [1]. However; we know that in practice; the semantics of the datamay evolve over time and its schema (the data structures and constraints) is not alwaysupdated to reflect these changes [5]. Common examples include the overloading of tables tostore facts of different types (for example; an order table originally designed to store serviceorders may be used to store various product orders as a company expands the scope of itsbusiness). Similarly; the semantics (typically represented as constraints) may evolve;perhaps because new data does not share the original semantics or because the fullsemantics were not captured in the original legacy design. Our long term research goal is tofind techniques for discovering schemas that fit the data. In this work; we are taking an …,IIWeb,2003,1
Provenance management for frequent itemsets,Javed Siddique; Boris Glavic; Renée J Miller,ABSTRACT Provenance has been studied extensively for relational queries and shown tobe important in revealing the origin and creation process of data that has been produced bypotentially complex relational transformations. Provenance for the results of data miningoperators in contrast has not been considered. We argue that provenance offers the samebenefits for mining as for relational queries; eg; it allows us to track errors caused byincorrect input data. We consider the most common mining operator; frequent itemsetmining; and introduce two types of provenance (why-and i-provenance) for this operator. Weargue that the concept of why-provenance for relational queries can be adapted for frequentitemsets; but that it poses new computational challenges due to the nature of itemset miningand the size of why-provenance. We address these challenges in two ways. First; we …,*,*,1
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*
The Future of Data Integration,Renée J Miller,Abstract The value of data explodes when it is integrated. In this talk; I present someimportant innovations in data integration over the last two decades. These include dataexchange [1]; which provides a foundation for reasoning about the correctness oftransformed data; and the use of declarative mappings in integration [2]. I discuss how datamining has been used to facilitate data integration; including constraint discovery [3];mapping discovery [4]; and in schema discovery to combat database decay and facilitateintegration [5; 6]. I present some important new data integration challenges that arise in datascience. These include the use of mining for query and visualization recommendation overmassive data lakes [7] and data set search; finding datasets of interest at interactive speeds[8].,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2017,*
Interactive navigation of open data linkages,Erkang Zhu; Ken Q Pu; Fatemeh Nargesian; Renée J Miller,Abstract We developed T oronto O pen D ata S earch to support the ad hoc; interactivediscovery of connections or linkages between datasets. It can be used to efficiently navigatethrough the open data cloud. Our system consists of three parts: a user-interface provided bya Web application; a scalable backend infrastructure that supports navigational queries; anda dynamic repository of open data tables. Our system uses LSH Ensemble; an efficient indexstructure; to compute linkages (attributes in two datasets with high containment score) in realtime at Internet scale. Our application allows users to navigate along these linkages byjoining datasets. LSH Ensemble is scalable; providing millisecond response times forlinkage discovery queries even over millions of datasets. Our system offers users a highlyinteractive experience making unrelated (and unlinked) dynamic collections of datasets …,Proceedings of the VLDB Endowment,2017,*
Data Driven Discovery of Attribute Dictionaries,Fei Chiang; Periklis Andritsos; Renée J Miller,Abstract Online product search engines such as Google and Yahoo shopping; rely onhaving extensive and complete product information to return accurate and timely searchresults. Given the expanding scope of products and updates to existing products; automatedtechniques are needed to ensure the underlying product dictionaries remain current andcomplete. Product search engines receive offers from merchants describing product specificattributes and characteristics. These offers normally contain structured attribute-value pairs;and unstructured (textual) descriptions describing product characteristics and features. Forexample; a laptop offer may contain attribute-value pairs such as “model-X42” and “RAM-8GB”; and a text description of the software; accessories; battery features; warranty; etc.Updating the product dictionaries using the textual descriptions is a more challenging …,*,2016,*
LinkedCT Live: Platform for Online Curation of Clinical Trials Data.,Oktie Hassanzadeh; Renée J Miller; Fatemeh Nargesian; Erkang Zhu,Abstract. The goal of the Linked Clinical Trials (LinkedCT) project is to transform the datapublished on ClinicalTrials. gov into a highquality knowledge base published as LinkedData on the Web. In this demonstration; we present the platform we have developed for bothonline curation of clinical trials data into linked data; and for rapid Web applicationdevelopment on top of this linked data. We also show a few sample applications built usingthis platform. We have made the project open-source and invite researchers and healthcareprofessionals to develop applications that will be hosted on LinkedCT. org.,International Semantic Web Conference (Posters & Demos),2015,*
Perspectives on Business Intelligence,Raymond T Ng; Patricia C Arocena; Denilson Barbosa; Giuseppe Carenini; Luiz Gomes; Jr; Stephan Jou; Rock Anthony Leung; Evangelos Milios; Renée J Miller; John Mylopoulos; Rachel A Pottinger; Frank Tompa; Eric Yu,Abstract Download Free Sample In the 1980s; traditional Business Intelligence (BI) systemsfocused on the delivery of reports that describe the state of business activities in the past;such as for questions like" How did our sales perform during the last quarter?" A decadelater; there was a shift to more interactive content that presented how the business wasperforming at the present time; answering questions like" How are we doing right now?"Today the focus of BI users are looking into the future." Given what I did before and how I amcurrently doing this quarter; how will I do next quarter?" Furthermore; fuelled by the demandsof Big Data; BI systems are going through a time of incredible change. Predictive analytics;high volume data; unstructured data; social data; mobile; consumable analytics; and datavisualization are all examples of demands and capabilities that have become critical …,Synthesis Lectures on Data Management,2013,*
Vivification in BI,P Arocena; RJ Miller; J Mylopoulos,E' presente una richiesta di inserimento in ANCE di una nuova rivista; utilizza la funzione "Registracodice ANCE" per registrare il codice ricevuto dal servizio LoginMIUR o inviare una nuova richiestadi inserimento oppure cercare nuovamente la rivista. E' presente una richiesta di inserimentoin ANCE di una nuova serie; utilizza la funzione "Registra codice ANCE" per registrare il codicericevuto dal servizio LoginMIUR o inviare una nuova richiesta di inserimento oppure cercarenuovamente la serie … Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatoriper il sito CINECA non sono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuareuna rivista con i dati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN doveapplicabili e il titolo della rivista … Segnalazioni con codici 20201/20202:La pubblicazionenon è stata trasferita SOLO per i docenti segnalati nel messaggio a causa di problemi …,*,2013,*
Proceedings of the 2011 ACM Sigmod International Conference on Management of Data,TK Sellis; RJ Miller; A Kementsietsidis; Y Velegrakis,Welcome to an exciting week in the city of Athens for the 2011 ACM SIGMOD Conference.Athens is a metropolitan and cosmopolitan city; with so many things to do and to see. It isalso known as the birth place of Democracy; the city with the world-renown" Acropolis andParthenon"; with the famous Theater of Herodes Atticus and the" marble stadium" where thefirst modern time Olympic Games took place in 1896; home of Socrates; Plato; Pericles(Golden Age); and home of the very successful 2004 Olympic Games. And now the home ofthe 2011 ACM SIGMOD Conference! Athens is both an" ancient" and a" modern" city; inwhich visitors can walk safely and enjoy the rich---almost 5;000 year old---history it has tooffer. The city offers a lot of sightseeing; museums; shopping and nightlife. We have aprogram of several social events to complement an excellent technical program. The …,*,2011,*
SIGMOD 2011 chairs' welcome,T Sellis; RJ Miller; A Kementsietsidis; Y Velegrakis,E' presente una richiesta di inserimento in ANCE di una nuova rivista; utilizza la funzione "Registracodice ANCE" per registrare il codice ricevuto dal servizio LoginMIUR o inviare una nuova richiestadi inserimento oppure cercare nuovamente la rivista. E' presente una richiesta di inserimentoin ANCE di una nuova serie; utilizza la funzione "Registra codice ANCE" per registrare il codicericevuto dal servizio LoginMIUR o inviare una nuova richiesta di inserimento oppure cercarenuovamente la serie … Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatoriper il sito CINECA non sono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuareuna rivista con i dati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN doveapplicabili e il titolo della rivista … Segnalazioni con codici 20201/20202:La pubblicazionenon è stata trasferita SOLO per i docenti segnalati nel messaggio a causa di problemi …,*,2011,*
BibBase triplified,Oktie Hassanzadeh; Reynold S Xin; Christian Fritz; Yang Yang; Jiang Du; Minghua Zhao; Renée J Miller,Abstract We present BibBase; a system for publishing and managing bibliographic dataavailable in BibTeX files. BibBase uses a powerful yet light-weight approach to transformBibTeX files into rich triplified data as well as custom HTML and RSS code that can readilybe integrated within a user's website while the data can instantly be queried online on thesystem's SPARQL endpoint. In this short report; we present a brief overview of the features ofour system and outline a few research challenges in building such a system.,Proceedings of the 6th International Conference on Semantic Systems,2010,*
Guest editorial: special issue on metadata management,Tiziana Catarci; Renée J Miller,In this special issue on metadata management; we present a new work on creating;gathering; managing; and understanding metadata. The work in this issue highlights thereality that the lack of metadata and effective techniques for managing them is currently oneof the biggest challenges to meaningful use and sharing of the wealth (or should we sayglut) of data available today. Our first paper;“Model-Independent Schema Translation” byPaolo Atzeni; Paolo Cappellari; Riccardo Torlone; Philip A. Bernstein; and GiorgioGianforme; considers the schema translation problem—a problem as old as the datamanagement field itself. Model Independent Data and Schema Translation (MIDST) is aframework for translating schemas from one data model to another. The explicit creation ofmetadata representing the relationship between the schema and its translation is central …,The VLDB Journal,2008,*
Efficient determination of homogeneous rectangles in a binary matrix,*,Determining maximal empty rectangles in a binary matrix includes building values in astaircase data structure for each successive entry in the matrix. The values in the staircasedata structure are removed where the values correspond to maximal rectangles having thesuccessive entry in the bottom right corner of the rectangle. The values in the staircase datastructure for each successive entry being determinable from values in the staircase datastructure for a preceding entry in the matrix. The maximal empty rectangles providing a basisfor generating efficient relational join operations on defined relational tables.,*,2006,*
In memoriam Alberto Oscar Mendelzon,Renèe J Miller,Abstract Alberto Oscar Mendelzon passed away on June 16; 2005 after a two-year battlewith cancer. This tribute to Alberto and his achievements is written in recognition of his greatintellect and his generous friendship. Both have influenced and inspired many in thedatabase research community.,ACM SIGMOD Record,2005,*
Special issue: Best papers of VLDB 2004,M Tamer Özsu; Donald Kossmann; Renée J Miller,Page 1. The VLDB Journal (2005) 14(4): 355–356 DOI 10.1007/s00778-005-0179-z GUESTEDITORIAL M. Tamer Özsu · Donald Kossmann · Renée J. Miller Special issue: Best papers ofVLDB 2004 Published online: 30 October 2005 c Springer-Verlag 2005 This special issue includesfour papers from the 30th In- ternational Conference on Very Large Data Bases; held in Toronto;Canada; 31 August—3 September 3 2004. The con- ference program included 81 research papersselected from 463 submissions. Eight papers from these 81 were invited by the Program Chairsfor inclusion in this special issue. Sub- stantially expanded and enhanced versions of these paperswere subjected to a new round of reviewing and; as a result; the four that are included in this issuewere selected. The four papers cover a wide range of topics: sen- sor networks; data warehouses;data quality; and scientific databases …,The VLDB Journal,2005,*
Proceedings 2004 VLDB Conference: The 30th International Conference on Very Large Databases (VLDB),Mario A Nascimento; M. Tamer Özsu; Donald Kossmann; Renée J. Miller,*,*,2004,*
On Schema Discovery,P Andritsos; RJ Miller,E' presente una richiesta di inserimento in ANCE di una nuova rivista; utilizza la funzione "Registracodice ANCE" per registrare il codice ricevuto dal servizio LoginMIUR o inviare una nuova richiestadi inserimento oppure cercare nuovamente la rivista. E' presente una richiesta di inserimentoin ANCE di una nuova serie; utilizza la funzione "Registra codice ANCE" per registrare il codicericevuto dal servizio LoginMIUR o inviare una nuova richiesta di inserimento oppure cercarenuovamente la serie … Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatoriper il sito CINECA non sono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuareuna rivista con i dati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN doveapplicabili e il titolo della rivista … Segnalazioni con codici 20201/20202:La pubblicazionenon è stata trasferita SOLO per i docenti segnalati nel messaggio a causa di problemi …,IEEE DATA ENGINEERING BULLETIN,2003,*
Reminiscences on Influential Papers,K Ross; F Korn; R Miller; K Voruganti,This paper abstracts from the particular features of PRISMA/DB; and evaluates and analyzesthe performance trade-offs for a wide range of parallel query processing strategies. Its clearstyle of presentation; along with careful attention to previous work both in its discussion aswell as in the experiments and analysis; make this paper into a concise introductory or“refereshment” text for researchers interested in parallel query execution.,SIGMOD RECORD,2002,*
VLDB2002 Panel Biodiversity and Ecosystem Informatics Research; Technology Transfer; or Application Development?,Judith Bayard Cushing; Kathleen Bergen; Yannis Ioannidis; Jessie Kennedy; Renée J Miller,Abstract This informal paper summarizes observations of the VLDB 2002 panel onBiodiversity and Ecosystem Informatics (BDEI) and the subsequent discussion. Backgroundon BDEI is provided and conclusions and follow on activities suggested. The moderator'sand panelists' presentations are available on the VLDB 2002 web site; http://www. cs. ust.hk/vldb2002/programinfo/panels. html.,*,*,*
Letter from the Special Issue Editors,Patricia C Arocena; Boris Glavic; Giansalvatore Mecca; Renee J Miller; Paolo Papotti; Donatello Santoro,The prevalence of large volumes and varieties of accessible data is profoundly changing theway business; government and individuals approach decision making. Organizational bigdata investment strategies regarding what data to collect; clean; integrate; and analyze aretypically driven by some notion of perceived value. However; the value of the data isinescapably tied to the underlying quality of the data. Although for big data; value and qualitymay be correlated; they are conceptually different. For example; a complete and accurate listof the books read on April 1; 2016 by the special editors of this issue may not have muchvalue to anyone else. Whereas even partially complete and somewhat noisy GPS data frompublic transport vehicles may have a high perceived value for transport engineers and urbanplanners. In spite of significant advances in storage and compute capabilities; the time to …,*,*,*
Automatic Generation and Ranking of Explanations for Mapping Errors,Seokki Lee; Zhen Wang; Boris Glavic; Renée J Miller,ABSTRACT Data transformation is facilitated by the use of visual and logical specificationsof mappings between schemas. While easy to use; mappings are hard to design. Manytechniques have been proposed to help users understand and refine mappings. However;once an error in transformed data has been identified; these systems at best provide low-level dataflow style tracing or query language facilities to help a mapping developer tracethrough the massive space of possible reasons for the error. In this work; we present anapproach for systematically exploring the space of potential explanations (causes) for errorsin transformed data. Our system leverages data provenance in combination with informationabout the mapping to automatically generate possible explanations. Since the number ofpotential explanations for a set of data errors is exponential in the size of the set it is …,*,*,*
Special Issue: Database Theory 2005 Guest Editor: Dan Suciu,Dan Suciu; Dirk Leinders; Jan Van den Bussche; Wim Martens; Joachim Niehren; Wolfgang Faber; Gianluigi Greco; Nicola Leone; Ariel Fuxman; Renée J Miller; Solmaz Kolahi; Sara Cohen; Yehoshua Sagiv; J Nathan Foster; Michael B Greenwald; Christian Kirkegaard; Benjamin C Pierce; Alan Schmitt,*,*,*,*
The Vivification Problem in Real-Time Business Intelligence,Patricia C Arocena; Renée J Miller; John Mylopoulos,Abstract. In the new era of Business Intelligence (BI) technology; transforming massiveamounts of data into high-quality business information is essential. To achieve this; two non-overlapping worlds need to be aligned: the Information Technology (IT) world; representedby an organization's operational data sources and the technologies that manage them (datawarehouses; schemas; queries;...); and the business world; portrayed by business plans;strategies and goals that an organization aspires to fulfill. Alignment in this context meansmapping business queries into BI queries; and interpreting the data retrieved from the BIquery in business terms. We call the creation of this interpretation the vivification problem.The main thesis of this position paper is that solutions to the vivification problem should bebased on a formal framework that explicates assumptions and the other ingredients …,*,*,*
Mapping Generation and Data Translation of Heterogeneous,L Popa; Y Velegrakis; RJ Miller; M Hernéandez; R Fagin,*,*,*,*
VLDB Endowment Board of Trustees,Gerhard Weikum; Laura M Haas; Paolo Atzeni; Michael J Franklin; Amr El Abbadi; Gustavo Alonso; Peter MG Apers; Elisa Bertino; Peter Buneman; Johann Christoph Freytag; HV Jagadish; Christian S Jensen; Donald Kossmann; David Lomet; Renée J Miller; Shojiro Nishio; Beng Chin Ooi; Meral Ozsoyoglu; Krithi Ramamritham; Raghu Ramakrishnan; Stanley B Zdonik,The VLDB Endowment is a non-profit foundation whose objective is to promote scientific andeducational activities in the area of large-scale data; information; and knowledgemanagement. The Endowment serves as the steering committee for the VLDB conferenceseries. The Endowment also sponsors various scholarly activities. It has established aprogram that supports summer schools; tutorials; and other training activities of this kind; incountries that could otherwise not afford the expenses for such events. The Endowment isalso the main sponsor of the biennial Conference on Innovative Data Systems Research(CIDR); and it runs the VLDB Journal; one of the most successful journals in the databasearea. On various activities; the Endowment closely cooperates with ACM SIGMOD. TheVLDB Endowment has a board of 21 elected trustees; who are the legal guardians of the …,*,*,*
Message from the ICDE 2010 workshop chairs,Christian S Jensen; Renée J Miller,The ICDE conference series continues to attract an exciting range of co-located workshopson topics related to data management. The 26th International Conference on DataEngineering (ICDE 2010); held in Long Beach; CA; USA from March 1 to March 6; 2010;features the following seven co-located workshops; in addition to a special workshop withpapers authored by Ph. D. students:,*,*,*
Lossy Data Reduction Without Loss of Information,Christopher Jermaine; Renee J Miller; Room LP396,*,*,*,*
North East DB/IR Day,Renee Miller; Doug Oard; Juliana Freire,North East DB/IR Day. October 22; 2010 AT&T Shannon Labs Building 103; 180 Park AvenueFlorham Park; NJ. Organizers: Graham Cormode; AT & T Research; graham atresearch.att.com Srinivas Bangalore; AT&T Research; srini at research.att.com Sponsoredby DIMACS and AT&T Workshop Program: 10:30 - 10:55 Welcome and Introductions 10:55 -12:00 On Schema Discovery Renee Miller; University of Toronto 12:00 - 1:45 Lunch; postersession 1:45 - 2:50 Who 'Dat? Identity resolution in large email collections Doug Oard;University of Maryland 2:50 - 3:10 Break 3:10 - 4:15 Provenance-Rich Science Juliana Freire;University of Utah 4:15 Close Friday; October 22; 2010 10:30am - 4:00pm Poster Session --details TBA Previous: Participation Next: Registration Workshop Index DIMACS HomepageContacting the Center Document last modified on Ocotber 22; 2010.,*,*,*
Finding Interesting Patterns Within an Iceberg,Christopher Jermaine; Renee Miller,Abstract Recent work has considered iceberg cube queries using the proposed SQL CUBE-BY operator. This operator provides an interesting and difficult set of problems to study;because the number of subcubes that are created is exponential with respect to the numberof attributes in the CUBE-BY clause. Thus; search can be very difficult. Previous work hasconsidered the problem of searching through the space of all subcubes using the COUNTand AVERAGE functions. In this work; we consider the problem of searching for subcubeshaving a high LLR value; where LLR is a function based on the loglikelihood ratio test fromstatistics. LLR is a rigorous; robust metric of surprise and interest; and provides a naturalway to measure the interestingness of subcubes in a high-dimensional space.,*,*,*
Collective Information Integration Using Logical and Statistical Methods,Lise Getoor; Renee J Miller,Data integration systems are software systems that permit the transformation; integration;and exchange of structured data that has been designed and developed independently. Theoften subtle and complex interdependencies within data can make the creation;maintenance; and use of such systems quite challenging. We have available a robustarsenal of tools and mechanisms for reconciling semantic differences in how data isrepresented including views; mappings; and transformation languages. There is also agrowing maturity in our knowledge of how to create [MHH00; PVM+02] and use thesemechanisms in such tasks as query answering;[Hal01]; data exchange;[FKMP03]; dataintegration;[Len02]; and data sharing.[AHT03; KAM03]. Given this solid foundation in thetools and modeling structures needed for data sharing; we see several remaining …,University of Pennsylvania,*,*
