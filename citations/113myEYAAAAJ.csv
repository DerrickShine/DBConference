Generic schema matching with cupid,Jayant Madhavan; Philip A Bernstein; Erhard Rahm,Abstract Schema matching is a critical step in many applications; such as XML messagemapping; data warehouse loading; and schema integration. In this paper; we investigatealgorithms for generic schema matching; outside of any particular data model or application.We first present a taxonomy for past solutions; showing that a rich range of techniques isavailable. We then propose a new algorithm; Cupid; that discovers mappings betweenschema elements based on their names; data types; constraints; and schema structure;using a broader set of techniques than past approaches. Some of our innovations are theintegrated use of linguistic and structural matching; context-dependent matching of sharedtypes; and a bias toward leaf structure where much of the schema content resides. Afterdescribing our algorithm; we present experimental results that compare Cupid to two …,vldb,2001,1885
Learning to map between ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Abstract Ontologies play a prominent role on the Semantic Web. They make possible thewidespread publication of machine understandable data; opening myriad opportunities forautomated information processing. However; because of the Semantic Web's distributednature; data on it will inevitably come from many different ontologies. Information processingacross ontologies is not possible without knowing the semantic mappings between theirelements. Manually finding such mappings is tedious; error-prone; and clearly not possibleat the Web scale. Hence; the development of tools to assist in the ontology mapping processis crucial to the success of the Semantic Web. We describe glue; a system that employsmachine learning techniques to find such mappings. Given two ontologies; for each conceptin one ontology glue finds the most similar concept in the other ontology. We give well …,Proceedings of the 11th international conference on World Wide Web,2002,1319
Similarity search for web services,Xin Dong; Alon Halevy; Jayant Madhavan; Ema Nemes; Jun Zhang,Abstract Web services are loosely coupled software components; published; located; andinvoked across the web. The growing number of web services available within anorganization and on the Web raises a new and challenging search problem: locatingdesired web services. Traditional keyword search is insufficient in this context: the specifictypes of queries users require are not captured; the very small text fragments in web servicesare unsuitable for keyword search; and the underlying structure and semantics of the webservices are not exploited. We describe the algorithms underlying the Woogle search enginefor web services. Woogle supports similarity search for web services; such as finding similarweb-service operations and finding operations that compose with a given one. We describenovel techniques to support these types of searches; and an experimental study on a …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,896
Reference reconciliation in complex information spaces,Xin Dong; Alon Halevy; Jayant Madhavan,Abstract Reference reconciliation is the problem of identifying when different references (ie;sets of attribute values) in a dataset correspond to the same real-world entity. Most previousliterature assumed references to a single class that had a fair number of attributes (eg;research publications). We consider complex information spaces: our references belong tomultiple related classes and each reference may have very few attribute values. A primeexample of such a space is Personal Information Management; where the goal is to providea coherent view of all the information on one's desktop. Our reconciliation algorithm hasthree principal features. First; we exploit the associations between references to design newmethods for reference comparison. Second; we propagate information betweenreconciliation decisions to accumulate positive and negative evidences. Third; we …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,636
Learning to match ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Robin Dhamankar; Pedro Domingos; Alon Halevy,Abstract. On the Semantic Web; data will inevitably come from many different ontologies;and information processing across ontologies is not possible without knowing the semanticmappings between them. Manually finding such mappings is tedious; error-prone; andclearly not possible on the Web scale. Hence the development of tools to assist in theontology mapping process is crucial to the success of the Semantic Web. We describeGLUE; a system that employs machine learning techniques to find such mappings. Giventwo ontologies; for each concept in one ontology GLUE finds the most similar concept in theother ontology. We give well-founded probabilistic definitions to several practical similaritymeasures and show that GLUE can work with all of them. Another key feature of GLUE isthat it uses multiple learning strategies; each of which exploits well a different type of …,The VLDB Journal,2003,618
Ontology matching: A machine learning approach,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Summary This chapter studies ontology matching: the problem of finding the semanticmappings between two given ontologies. This problem lies at the heart of numerousinformation processing applications. Virtually any application that involves multipleontologies must establish semantic mappings among them; to ensure interoperability.Examples of such applications arise in myriad domains; including e-commerce; knowledgemanagement; e-learning; information extraction; bio-informatics; web services; and tourism(see Part D of this book on ontology applications). Despite its pervasiveness; today ontologymatching is still largely conducted by hand; in a labor-intensive and error-prone process.The manual matching has now become a key bottleneck in building large-scale informationmanagement systems. The advent of technologies such as the WWW; XML; and the …,*,2004,598
Methods and system for model matching,*,Systems and methods for automatically and generically matching models are provided; suchas may be provided in a matching application or matching component; or provided in ageneral purpose system for managing models. The methods are generic since the methodsapply to hierarchical data sets outside of any particular data model or application. Similaritycoefficients are calculated for; and mappings are discovered between; schema elementsbased on their names; data types; constraints; and schema structure; using a broad set oftechniques. Some of these techniques include the integrated use of linguistic and structuralmatching; context dependent matching of shared types; and a bias toward subtree; or leaf;structure where much of the schema content resides.,*,2004,513
Corpus-based schema matching,Jayant Madhavan; Philip A Bernstein; AnHai Doan; Alon Halevy,Schema matching is the problem of identifying corresponding elements in different schemas.Discovering these correspondences or matches is inherently difficult to automate. Pastsolutions have proposed a principled combination of multiple algorithms. However; thesesolutions sometimes perform rather poorly due to the lack of sufficient evidence in theschemas being matched. In this paper we show how a corpus of schemas and mappingscan be used to augment the evidence about the schemas being matched; so they can bematched better. Such a corpus typically contains multiple schemas that model similarconcepts and hence enables us to learn variations in the elements and their properties. Weexploit such a corpus in two ways. First; we increase the evidence about each element beingmatched by including evidence from similar elements in the corpus. Second; we learn …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,459
Google's deep web crawl,Jayant Madhavan; David Ko; Łucja Kot; Vignesh Ganapathy; Alex Rasmussen; Alon Halevy,Abstract The Deep Web; ie; content hidden behind HTML forms; has long beenacknowledged as a significant gap in search engine coverage. Since it represents a largeportion of the structured data on the Web; accessing Deep-Web content has been a long-standing challenge for the database community. This paper describes a system for surfacingDeep-Web content; ie; pre-computing submissions for each HTML form and adding theresulting HTML pages into a search engine index. The results of our surfacing have beenincorporated into the Google search engine and today drive more than a thousand queriesper second to Deep-Web content. Surfacing the Deep Web poses several challenges. First;our goal is to index the content behind many millions of HTML forms that span manylanguages and hundreds of domains. This necessitates an approach that is completely …,Proceedings of the VLDB Endowment,2008,458
Web-scale data integration: You can only afford to pay as you go,Jayant Madhavan; Shawn R Jeffery; Shirley Cohen; Xin Dong; David Ko; Cong Yu; Alon Halevy,ABSTRACT The World Wide Web is witnessing an increase in the amount of structuredcontent–vast heterogeneous collections of structured data are on the rise due to the DeepWeb; annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper; we highlight these challenges intwo scenarios–the Deep Web and Google Base. We contend that traditional data integrationtechniques are no longer valid in the face of such heterogeneity and scale. We propose anew data integration architecture; PAYGO; which is inspired by the concept of dataspacesand emphasizes pay-as-you-go data management as means for achieving web-scale dataintegration.,*,2007,429
The piazza peer data management system,Alon Y Halevy; Zachary G Ives; Jayant Madhavan; Peter Mork; Dan Suciu; Igor Tatarinov,Intuitively; data management and data integration tools are well-suited for exchanginginformation in a semantically meaningful way. Unfortunately; they suffer from two significantproblems: They typically require a comprehensive schema design before they can be usedto store or share information and they are difficult to extend because schema evolution isheavyweight and may break backward compatibility. As a result; many small-scale datasharing tasks are more easily facilitated by nondatabase-oriented tools that have littlesupport for semantics. The goal of the peer data management system (PDMS) is to addressthis need: We propose the use of a decentralized; easily extensible data managementarchitecture in which any user can contribute new data; schema information; or evenmappings between other peers' schemes. PDMSs represent a natural step beyond data …,IEEE Transactions on Knowledge and Data Engineering,2004,322
Representing and reasoning about mappings between domain models,Jayant Madhavan; Philip A Bernstein; Pedro Domingos; Alon Y Halevy,Abstract Mappings between disparate models are fundamental to any application thatrequires interoperability between heterogeneous data and applications. Generatingmappings is a laborintensive and error prone task. To build a system that helps usersgenerate mappings; we need an explicit representation of mappings. This representationneeds to have well-defined semantics to enable reasoning and comparison betweenmappings. This paper first presents a powerful framework for defining languages forspecifying mappings and their associated semantics. We examine the use of mappings andidentify the key inference problems associated with mappings. These properties can beused to determine whether a mapping is adequate in a particular context. Finally; weconsider an instance of our framework for a language representing mappings between …,AAAI/IAAI,2002,314
-Composing Mappings Among Data Sources,Jayant Madhavan; Alon Y Halevy,Semantic mappings between data sources play a key role in several data sharingarchitectures. Mappings provide the relationships between data stored in different sources;and therefore enable answering queries that require data from other nodes in a data-sharingnetwork. This chapter investigates the theoretical underpinnings of mapping composition.The problem of sharing data from multiple sources within or between enterprises hasrecently received significant attention in research and in the commercial world. This chapterstudies the problem for a rich mapping language; GLAV that combines the advantages of theknown mapping formalisms global-as-view and local-as-view. The chapter explores thateven when composing two simple GLAV mappings; the full composition may be an infiniteset of GLAV formulas. It also explores that if one restricts the set of queries to be in CQ k …,*,2003,304
The Piazza peer data management project,Igor Tatarinov; Zachary Ives; Jayant Madhavan; Alon Halevy; Dan Suciu; Nilesh Dalvi; Xin Luna Dong; Yana Kadiyska; Gerome Miklau; Peter Mork,Abstract A major problem in today's information-driven world is that sharing heterogeneous;semantically rich data is incredibly difficult. Piazza is a peer data management system thatenables sharing heterogeneous data in a distributed and scalable way. Piazza assumes theparticipants to be interested in sharing data; and willing to define pairwise mappingsbetween their schemas. Then; users formulate queries over their preferred schema; and aquery answering system expands recursively any mappings relevant to the query; retrievingdata from other peers. In this paper; we provide a brief overview of the Piazza projectincluding our work on developing mapping languages and query reformulation algorithms;assisting the users in defining mappings; indexing; and enforcing access control overshared data.,ACM Sigmod Record,2003,263
Recovering semantics of tables on the web,Petros Venetis; Alon Halevy; Jayant Madhavan; Marius Paşca; Warren Shen; Fei Wu; Gengxin Miao; Chung Wu,Abstract The Web offers a corpus of over 100 million tables [6]; but the meaning of eachtable is rarely explicit from the table itself. Header rows exist in few cases and even whenthey do; the attribute names are typically useless. We describe a system that attempts torecover the semantics of tables by enriching the table with additional annotations. Ourannotations facilitate operations such as searching for tables and finding related tables. Torecover semantics of tables; we leverage a database of class labels and relationshipsautomatically extracted from the Web. The database of classes and relationships has verywide coverage; but is also noisy. We attach a class label to a column if a sufficient number ofthe values in the column are identified with that label in the database of class labels; andanalogously for binary relationships. We describe a formal model for reasoning about …,Proceedings of the VLDB Endowment,2011,228
Generic schema matching; ten years later,Philip A Bernstein; Jayant Madhavan; Erhard Rahm,ABSTRACT In a paper published in the 2001 VLDB Conference; we proposed treatinggeneric schema matching as an independent problem. We developed a taxonomy ofexisting techniques; a new schema matching algorithm; and an approach to comparativeevaluation. Since then; the field has grown into a major research topic. We briefly summarizethe new techniques that have been developed and applications of the techniques in thecommercial world. We conclude by discussing future trends and recommendations forfurther work.,Proceedings of the VLDB Endowment,2011,215
Google fusion tables: web-centered data management and collaboration,Hector Gonzalez; Alon Y Halevy; Christian S Jensen; Anno Langen; Jayant Madhavan; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,Abstract It has long been observed that database management systems focus on traditionalbusiness applications; and that few people use a database management system outsidetheir workplace. Many have wondered what it will take to enable the use of datamanagement technology by a broader class of users and for a much wider range ofapplications. Google Fusion Tables represents an initial answer to the question of how datamanagement functionality that focused on enabling new users and applications would lookin today's computing environment. This paper characterizes such users and applicationsand highlights the resulting principles; such as seamless Web integration; emphasis onease of use; and incentives for data sharing; that underlie the design of Fusion Tables. Wedescribe key novel features; such as the support for data acquisition; collaboration …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,175
Clustering query refinements by user intent,Eldar Sadikov; Jayant Madhavan; Lu Wang; Alon Halevy,Abstract We address the problem of clustering the refinements of a user search query. Theclusters computed by our proposed algorithm can be used to improve the selection andplacement of the query suggestions proposed by a search engine; and can also serve tosummarize the different aspects of information relevant to the original user query. Ouralgorithm clusters refinements based on their likely underlying user intents by combiningdocument click and session co-occurrence information. At its core; our algorithm operates byperforming multiple random walks on a Markov graph that approximates user searchbehavior. A user study performed on top search engine queries shows that our clusters arerated better than corresponding clusters computed using approaches that use onlydocument click or only sessions co-occurrence information.,Proceedings of the 19th international conference on World wide web,2010,166
Harvesting relational tables from lists on the web,Hazem Elmeleegy; Jayant Madhavan; Alon Halevy,Abstract A large number of web pages contain data structured in the form of" lists". Manysuch lists can be further split into multi-column tables; which can then be used in moresemantically meaningful tasks. However; harvesting relational tables from such lists can bea challenging task. The lists are manually generated and hence need not have well definedtemplates--they have inconsistent delimiters (if any) and often have missing information. Wepropose a novel technique for extracting tables from lists. The technique is domain-independent and operates in a fully unsupervised manner. We first use multiple sources ofinformation to split individual lines into multiple fields; and then compare the splits acrossmultiple lines to identify and fix incorrect splits and bad alignments. In particular; we exploit acorpus of HTML tables; also extracted from the Web; to identify likely fields and good …,Proceedings of the VLDB Endowment,2009,130
Personal information management with SEMEX,Yuhan Cai; Xin Luna Dong; Alon Halevy; Jing Michelle Liu; Jayant Madhavan,Abstract The explosion of information available in digital form has made search a hotresearch topic for the Information Management Community. While most of the research onsearch is focused on the WWW; individual computer users have developed their own vastcollections of data on their desktops; and these collections are in critical need for goodsearch and query tools. The problem is exacerbated by the proliferation of varied electronicdevices (laptops; PDAs; cellphones) that are at our disposal; which often hold subsets orvariations of our data. In fact; several recent venues have noted Personal InformationManagement (PIM) as an area of growing interest to the data management community [1; 8;6],Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,129
Google fusion tables: data management; integration and collaboration in the cloud,Hector Gonzalez; Alon Halevy; Christian S Jensen; Anno Langen; Jayant Madhavan; Rebecca Shapley; Warren Shen,Abstract Google Fusion Tables is a cloud-based service for data management andintegration. Fusion Tables enables users to upload tabular data files (spreadsheets; CSV;KML); currently of up to 100MB. The system provides several ways of visualizing the data(eg; charts; maps; and timelines) and the ability to filter and aggregate the data. It supportsthe integration of data from multiple sources by performing joins across tables that maybelong to different users. Users can keep the data private; share it with a select set ofcollaborators; or make it public and thus crawlable by search engines. The discussionfeature of Fusion Tables allows collaborators to conduct detailed discussions of the data atthe level of tables and individual rows; columns; and cells. This paper describes the innerworkings of Fusion Tables; including the storage of data in the system and the tight …,Proceedings of the 1st ACM symposium on Cloud computing,2010,128
Crossing the Structure Chasm.,Alon Y Halevy; Oren Etzioni; AnHai Doan; Zachary G Ives; Jayant Madhavan; Luke K McDowell; Igor Tatarinov,Online information comes in two flavors: unstructured corpora of text on the one hand; andstructured data managed by databases and knowledge bases on the other. These twodifferent kinds of data lead to very different authoring; management and search paradigms.In the first; search is based on keywords and answers are ranked according to relevance. Inthe second; search is based on queries in a formal language (eg; SQL); and all the answersreturned for the query are correct according to the underlying semantics of the system. In theu-world of unstructured data; authoring data is straightforward. In contrast; in the s-world ofstructured data; authoring data is a conceptual effort that requires technical expertise andsubstantial up front effort; the author is required to provide a comprehensive structure (ie;schema) of the domain before entering data. This paper is focused on the profound …,CIDR,2003,116
Harnessing the deep web: Present and future,Jayant Madhavan; Loredana Afanasiev; Lyublena Antova; Alon Halevy,Abstract: Over the past few years; we have built a system that has exposed large volumes ofDeep-Web content to Google. com users. The content that our system exposes contributes tomore than 1000 search queries per-second and spans over 50 languages and hundreds ofdomains. The Deep Web has long been acknowledged to be a major source of structureddata on the web; and hence accessing Deep-Web content has long been a problem ofinterest in the data management community. In this paper; we report on where we believethe Deep Web provides value and where it does not. We contrast two very differentapproaches to exposing Deep-Web content--the surfacing approach that we used; and thevirtual integration approach that has often been pursued in the data management literature.We emphasize where the values of each of the two approaches lie and caution against …,arXiv preprint arXiv:0909.1785,2009,101
Structured data on the web,Michael J Cafarella; Alon Halevy; Jayant Madhavan,Though the web is best known as a vast repository of shared documents; it also contains a significantamount of structured data covering a complete range of topics; from product to financial;public-record; scientific; hobby-related; and government. Structured data on the Web sharesmany similarities with the kind of data traditionally managed by commercial database systemsbut also reflects some unusual characteristics of its own; for example; it is embedded in textualWeb pages and must be extracted prior to use; there is no centralized data design as there isin a traditional database; and; unlike traditional databases that focus on a single domain; it coverseverything. Existing data-management systems do not address these challenges and assumetheir data is modeled within a well-defined domain … This article discusses the nature ofWeb-embedded structured data and the challenges of managing it. To begin; we present …,Communications of the ACM,2011,92
Web-scale extraction of structured data,Michael J Cafarella; Jayant Madhavan; Alon Halevy,Abstract A long-standing goal of Web research has been to construct a unified Webknowledge base. Information extraction techniques have shown good results on Web inputs;but even most domain-independent ones are not appropriate for Web-scale operation. Inthis paper we describe three recent extraction systems that can be operated on the entireWeb (two of which come from Google Research). The TextRunner system focuses on rawnatural language text; the WebTables system focuses on HTML-embedded tables; and thedeep-web surfacing system focuses on" hidden" databases. The domain; expressiveness;and accuracy of extracted data can depend strongly on its source extractor; we describedifferences in the characteristics of data produced by the three extractors. Finally; we discussa series of unique data applications (some of which have already been prototyped) that …,ACM SIGMOD Record,2009,92
Openii: an open source information integration toolkit,Len Seligman; Peter Mork; Alon Halevy; Ken Smith; Michael J Carey; Kuang Chen; Chris Wolf; Jayant Madhavan; Akshay Kannan; Doug Burdick,Abstract OpenII (openintegration. org) is a collaborative effort to create a suite of open-source tools for information integration (II). The project is leveraging the latest developmentsin II research to create a platform on which integration tools can be built and further researchconducted. In addition to a scalable; extensible platform; OpenII includes industrial-strengthcomponents developed by MITRE; Google; UC-Irvine; and UC-Berkeley that interoperatethrough a common repository in order to solve II problems. Components of the toolkit havebeen successfully applied to several large-scale US government II challenges.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,72
Structured data meets the Web: a few observations.,Jayant Madhavan; Alon Y Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu,Abstract The World Wide Web is witnessing an increase in the amount of structured content–vast heterogeneous collections of structured data are on the rise due to the Deep Web;annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper we articulate challenges based onour experience with addressing them at Google; and offer some principles for addressingthem in a general fashion.,IEEE Data Eng. Bull.,2006,52
Data management projects at Google,Michael Cafarella; Edward Chang; Andrew Fikes; Alon Halevy; Wilson Hsieh; Alberto Lerner; Jayant Madhavan; S Muthukrishnan,Abstract This article describes some of the ongoing research projects related to structureddata management at Google today. The organization of Google encourages researchscientists to work closely with engineering teams. As a result; the research projects tend tobe motivated by real needs faced by Google's products and services; and solutions are putinto production and tested rapidly. In addition; because of the sheer scale at which Googleoperates; the engineering challenges faced by Google's services often require researchinnovations.,ACM SIGMOD Record,2008,43
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2013,37
Harvesting relational tables from lists on the web,Hazem Elmeleegy; Jayant Madhavan; Alon Halevy,Abstract A large number of web pages contain data structured in the form of" lists". Manysuch lists can be further split into multi-column tables; which can then be used in moresemantically meaningful tasks. However; harvesting relational tables from such lists can bea challenging task. The lists are manually generated and hence need not have well-definedtemplates--they have inconsistent delimiters (if any) and often have missing information. Wepropose a novel technique for extracting tables from lists. The technique is domainindependent and operates in a fully unsupervised manner. We first use multiple sources ofinformation to split individual lines into multiple fields and then; compare the splits acrossmultiple lines to identify and fix incorrect splits and bad alignments. In particular; we exploit acorpus of HTML tables; also extracted from the web; to identify likely fields and good …,The VLDB Journal—The International Journal on Very Large Data Bases,2011,34
Scalable rendering of large spatial databases,*,Aspects of the invention provide a service for data management and integration across awide range of applications. Clustered computers may be arranged in a cloud-typeconfiguration for storing and handling large amounts of user data under the control of a front-end management server. Communities of distributed users may collaborate on the dataacross multiple enterprises. Very large tabular data files are uploaded to the storagefacilities. The data files are maintained as tables; and a composite table of relatedinformation is created and maintained in response to user queries. Different ways ofvisualizing the data are provided. Depending on the amount of information that can bedisplayed; features in a spatial index may the thinned for presentation. Spatial andstructured queries are processing and results are intersected to obtain information for …,*,2015,33
Applying WebTables in Practice.,Sreeram Balakrishnan; Alon Y Halevy; Boulos Harb; Hongrae Lee; Jayant Madhavan; Afshin Rostamizadeh; Warren Shen; Kenneth Wilder; Fei Wu; Cong Yu,*,CIDR,2015,29
Efficient spatial sampling of large geographical tables,Anish Das Sarma; Hongrae Lee; Hector Gonzalez; Jayant Madhavan; Alon Halevy,Abstract Large-scale map visualization systems play an increasingly important role inpresenting geographic datasets to end users. Since these datasets can be extremely large;a map rendering system often needs to select a small fraction of the data to visualize them ina limited space. This paper addresses the fundamental challenge of thinning: determiningappropriate samples of data to be shown on specific geographical regions and zoom levels.Other than the sheer scale of the data; the thinning problem is challenging because of anumber of other reasons:(1) data can consist of complex geographical shapes;(2) renderingof data needs to satisfy certain constraints; such as data being preserved across zoom levelsand adjacent regions; and (3) after satisfying the constraints; an optimal solution needs to bechosen based on objectives such as maximality; fairness; and importance of data. This …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,29
Exploring schema repositories with schemr,Kuang Chen; Akshay Kannan; Jayant Madhavan; Alon Halevy,Abstract Schemr is a search engine for users to search for and visualize schemas in ametadata repository. Users may search by keywords and by example; using schemafragments as query terms. Schemr uses a novel search algorithm; based on a combinationof text search and schema matching techniques; coupled with a structurally-aware scoringmetric. Schemr presents search results in a GUI that allows users to explore which elementsmatch and how well they do. The GUI supports interactions; including panning; zooming;layout and drilling-in. This paper introduces Schemr as a new component of the informationintegration toolbox and discusses its benefits in several applications.,ACM SIGMOD Record,2011,29
Corpus-based knowledge representation,Alon Y Halevy; Jayant Madhavan,Abstract A corpus-based knowledge representation system consists of a large collection ofdisparate knowledge fragments or schemas; and a rich set of statistics computed over thecorpus. We argue that by collecting such a corpus and computing the appropriate statistics;corpus-based representation offers an alternative to traditional knowledge representation fora broad class of applications. The key advantage of corpusbased representation is that weavoid the laborious process of building a (often brittle) knowledge base. We describe thebasic building blocks of a corpus-based representation system and a set of applications forwhich such a paradigm is appropriate; including one application where the approach isalready showing promising results.,IJCAI,2003,29
Mining structures for semantics,Xin Dong; Jayant Madhavan; Alon Halevy,Abstract Online data is available in two avors: unstructured data that resides as free text inHTML pages; and structured data that resides in databases and knowledge bases.Unstructured data is easily accessed as human-readable text on a browser; while structureddata is hidden behind web query interfaces (web forms); web services; and customdatabase APIs. Access to this data; popularly referred to as the hidden web; entailssubmitting correctly completed web forms or writing code to access web services usingprotocols such as SOAP.,ACM SIGKDD Explorations Newsletter,2004,28
Clustering query refinements by inferred user intent,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for clustering query refinements. One method includes building arepresentation of a graph for a first query; wherein the graph has a node for the first query; anode for each of a plurality of refinements for the first query; and a node for each documentin the document sets of the refinements; and wherein the graph has edges from the firstquery node to each of the refinement nodes; edges from the first query to each document inthe respective document set of the first query; edges from each refinement to each documentin the respective document set of the refinement; and edges from each refinement to each co-occurring query of the refinement. The method further includes clustering the refinementsinto refinement clusters by partitioning the refinement nodes in the graph into proper …,*,2013,19
Data management projects at Google,Wilson Hsieh; Jayant Madhavan; Rob Pike,Abstract This session describes three data management projects at Google. BigTable is ahighly scalable system for distributed storage and querying of structured data. Sawzall is asystem for large-scale analysis of data sets that have a flat but regular structure. Finally;GoogleBase is a system for storing and searching structured data contributed by externalparties.,*,2006,19
Facilitating searches through content which is accessible through web-based forms,*,One embodiment of the present invention provides a system that facilitates crawling throughweb-based forms to gather information to facilitate subsequent searches through contentwhich is accessible though the web-based forms. During operation; the system first obtainsweb-based forms to be searched. Note that the system can obtain these web-based formsfrom a number of sources. For example; the system can crawl through web sites to identifyweb-based forms; the system can receive manually provided web-based forms; or thesystem can find web-based forms through methods other than crawling. Next; the systemcreates database entries for the identified forms. This involves obtaining and storingmetadata describing the identified forms into database entries and then storing thesedatabase entries in a form database to facilitate searches through content which is …,*,2010,18
Table search using recovered semantic information,*,Methods; systems; and apparatus; including computer programs encoded on a computerstorage medium; for searching tables using recovered semantic information. In general; oneaspect of the subject matter described in this specification can be embodied in methods thatinclude the actions of receiving a collection of tables; each table including a plurality of rows;each row including a plurality of cells; recovering semantic information associated with eachtable of the collection of tables; the recovering including determining a class associated witheach respective table according to a class-instance hierarchy including identifying a subjectcolumn of each table of the collection of tables; and labeling each table in the collection oftables with the respective class.,*,2012,16
Self-organizing data sharing communities with SAGRES,Zachary Ives; Alon Levy; Jayant Madhavan; Rachel Pottinger; Stefan Saroiu; Igor Tatarinov; Shiori Betzler; Qiong Chen; Ewa Jaslikowska; Jing Su; Wai Tak Theodora Yeung,An increasing number of devices (eg; household appliances; PDAs; cell phones) havemicroprocessors and will soon be able to exhibit sophisticated behaviors and interactionswith other devices: a home heating system will monitor its residents' alarm clocks andschedules to set the temperature optimally; a car's GPS system will use local traffic reports tooptimize its driv er'sroute based on road conditions. The Sagres project at the University ofWashington addresses the key issues of data sharing and management in the realm ofinvisible computing. In the con text of invisible computing; data exchange and computationoccur in the background in response to cues from users. Devices are added and removedfrom the net w ork on a regular basis; and they must be able to interoperate with little humanintervention. The collection of devices that exist around a particular individual or in a …,ACM SIGMOD Record,2000,13
Identifying aspects for web-search queries,Fei Wu; Jayant Madhavan; Alon Halevy,Abstract Many web-search queries serve as the beginning of an exploration of an unknownspace of information; rather than looking for a specific web page. To answer such querieseffectively; the search engine should attempt to organize the space of relevant information ina way that facilitates exploration. We describe the Aspector system that computes aspects fora given query. Each aspect is a set of search queries that together represent a distinctinformation need relevant to the original search query. To serve as an effective means toexplore the space; Aspector computes aspects that are orthogonal to each other and to havehigh combined coverage. Aspector combines two sources of information to computeaspects. We discover candidate aspects by analyzing query logs; and cluster them toeliminate redundancies. We then use a mass-collaboration knowledge base (eg …,Journal of Artificial Intelligence Research,2011,12
Big Data Storytelling Through Interactive Maps.,Jayant Madhavan; Sreeram Balakrishnan; Kathryn Brisbin; Hector Gonzalez; Nitin Gupta; Alon Y Halevy; Karen Jacqmin-Adams; Heidi Lam; Anno Langen; Hongrae Lee; Rod McChesney; Rebecca Shapley; Warren Shen,Abstract Google Fusion Tables (GFT) brings big data collaboration and visualization to data-experts who possess neither large data-processing resources nor expertise. In this paper wehighlight our support for map visualizations over large complex geospatial datasets.Interactive maps created using GFT have already been used by journalists in numerous high-profile stories.,IEEE Data Eng. Bull.,2012,11
Searching through content which is accessible through web-based forms,*,One embodiment of the present invention provides a system that facilitates searchingthrough content which is accessible though web-based forms. During operation; the systemreceives a query containing keywords. Next; the system analyzes the query to create astructured query. The system then performs a lookup based on the structured query in adatabase containing entries describing the web-based forms. Next; the system ranks formsreturned by the lookup; and uses the rankings and associated database entries to facilitate asearch through content which is accessible through the forms.,*,2011,10
Consistent thinning of large geographical data for map visualization,Anish Das Sarma; Hongrae Lee; Hector Gonzalez; Jayant Madhavan; Alon Halevy,Abstract Large-scale map visualization systems play an increasingly important role inpresenting geographic datasets to end-users. Since these datasets can be extremely large;a map rendering system often needs to select a small fraction of the data to visualize them ina limited space. This article addresses the fundamental challenge of thinning: determiningappropriate samples of data to be shown on specific geographical regions and zoom levels.Other than the sheer scale of the data; the thinning problem is challenging because of anumber of other reasons:(1) data can consist of complex geographical shapes;(2) renderingof data needs to satisfy certain constraints; such as data being preserved across zoom levelsand adjacent regions; and (3) after satisfying the constraints; an optimal solution needs to bechosen based on objectives such as maximality; fairness; and importance of data. This …,ACM Transactions on Database Systems (TODS),2013,7
Analyzing a form page for indexing,*,Among other disclosure; a computer-implemented method of analyzing a form page forindexing includes identifying a form page that is configured for use in requesting any ofmultiple target pages. The form page includes multiple input controls. The method includesidentifying at least one of the multiple input controls as being informative with regard torequesting the multiple target pages. The method includes updating an indexing recordassociated with the form page to reflect the identification.,*,2013,7
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2015,6
Recent progress towards an ecosystem of structured data on the Web,Nitin Gupta; Alon Y Halevy; Boulos Harb; Heidi Lam; Hongrae Lee; Jayant Madhavan; Fei Wu; Cong Yu,Google Fusion Tables aims to support an ecosystem of structured data on the Web byproviding a tool for managing and visualizing data on the one hand; and for searching andexploring for data on the other. This paper describes a few recent developments in ourefforts to further the ecosystem.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,6
Discovering Structure in a Corpus of Schemas.,Alon Y Halevy; Jayant Madhavan; Philip A Bernstein,Abstract This paper describes a research program that exploits a large corpus of databaseschemas; possibly with associated data and meta-data; to build tools that facilitate thecreation; querying and sharing of structured data. The key insight is that given a largecorpus; we can discover patterns concerning how designers create structures forrepresenting domains. Given these patterns; we can more easily map between disparatestructures or propose structures that are appropriate for a given domain. We describe thefirst application of our approach to the problem of semi-automatic schema matching.,IEEE Data Eng. Bull.,2003,6
Harvesting relational tables from lists on the web,*,Computer implemented methods and apparatus for extracting list information into databasetables. A number of fields are independently determined for items in list. A number ofdatabase table columns are determined from most common number of list item fields. Newfields are determined for items with more fields than database columns. Null fields areinserted into items with fewer fields than database columns. Information from items havingthe same number of fields as database columns is written to database table rows.Information from each field is written to a corresponding database table column. Streaks ofpoorly matching cells in a database table row are determined. Streak cells are merged andnew cells are determined. Null cells are inserted if number of new cells is less than numberof cells in the streak. Information from the new cells is written to the table row and columns …,*,2012,5
Determining keyword for a form page,*,Among other disclosed subject matter; a computer-implemented method of analyzing a formpage for indexing includes identifying a form page that is configured for use in requestingany of multiple target pages; the form page including at least one text input control forretrieving any of the multiple target pages. The method includes identifying at least onekeyword as being informative with regard to the text input control. The method includesupdating an indexing record associated with the form page to reflect the identified keyword.,*,2013,4
Socialising Data with Google Fusion Tables.,Hector Gonzalez; Alon Y Halevy; Anno Langen; Jayant Madhavan; Rod McChesney; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,Abstract We describe the social features of Google Fusion Tables; a cloud-based datamanagement service whose goal is to facilitate collaboration around data sets. The socialfeatures include the ability to specify attribution of data sets; a mechanism for conductingdiscussions on data (at fine granularity; such as row; column or cell); the ability to mergetables that belong to different owners; and the ability to share specific queries andvisualizations and embed them in other properties on the Web. We describe the rationale fordesigning these features and our experiences after our first year of interacting with users.,IEEE Data Eng. Bull.,2010,4
Searching for join candidates,*,Systems and techniques are provided for receiving an input column and a search keywordand providing one or more suggested columns with which to merge the input column. Acoverage score and a refinity score are calculated for potential columns based on the inputcolumn as well as a search score based on the search keyword. The one or more suggestedcolumns may be determined based on the coverage score; refinity score; and/or the searchscore. The input column and/or a potential column may be modified based on a function andthe modification may result in a plurality of modified input and/or potential columns.Coverage; refinity; and search scores may be calculated based on the modified columns.,*,2015,2
Comparing SSD-placement Strategies to scale a Database-in-the-Cloud,Yingyi Bu; Hongrae Lee; Jayant Madhavan,Abstract Flash memory solid state drives (SSDs) have increasingly been advocated andadopted as a means of speeding up and scaling up data-driven applications. However;given the layered software architecture of cloud-based services; there are a number ofoptions available for placing SSDs. In this work; we studied the trade-offs involved indifferent SSD placement strategies; their impact of response time and throughput; andultimately the potential in achieving scalability in Google Fusion Tables (GFT); a cloud-based service for data management and visualization [1].,Proceedings of the 4th annual Symposium on Cloud Computing,2013,2
When does cotraining work in real data? Knowledge and Data Engineering,J Du; C Ling; ZH Zhou,*,IEEE Transactions on,2011,2
Research on statistical relational learning at the university of washington,Pedro Domingos; Yeuhi Abe; Corin Anderson; AnHai Doan; Dieter Fox; Alon Halevy; Geoff Hulten; Henry Kautz; Tessa Lau; Lin Liao; Jayant Madhavan; D Patterson Mausam; Matthew Richardson; Sumit Sanghai; Daniel Weld; Steve Wolfman,Abstract This paper presents an overview of the research on learning statistical models fromrelational data being carried out at the University of Washington. Our work falls into five maindirections: learning models of social networks; learning models of sequential relationalprocesses; scaling up statistical relational learning to massive data sources; learning forknowledge integration; and learning programs in procedural languages. We describe someof the common themes and research issues arising from this work.,Proceedings of the IJCAI-2003 Workshop on Learning Statistical Models from Relational Data,2003,2
Determining a geographic location relevant to a web page,*,One embodiment of the present invention provides a system that facilitates searchingthrough content which is accessible though web-based forms. During operation; the systemreceives a query containing keywords. Next; the system analyzes the query to create astructured query. The system then performs a lookup based on the structured query in adatabase containing entries describing the web-based forms. Next; the system ranks formsreturned by the lookup; and uses the rankings and associated database entries to facilitate asearch through content which is accessible through the forms.,*,2013,1
Using known schemas and mappings to construct new semantic mappings,Jayant Madhavan,Abstract This dissertation studies the problem of constructing semantic mappings; ie;expressions that relate different schemas. Semantic mappings play an important role inmodern information systems. They let applications relate data in different sources and thusenable them to fruitfully leverage information residing in various forms across multiple datasources. With the proliferation of information systems that adopt distributed and oftenheterogeneous architectures; there is a need for automated support and tools to betterfacilitate the construction of semantic mappings. Our thesis is that; given a mappingconstruction task; knowledge that exists in other schemas and previously known mappingsrelated to the mapping task; can be exploited to construct the required mapping. Ourhypothesis is based on the two intuitions that mapping construction tasks are often …,*,2005,1
Learning mappings between models of data,Jayant Madhavan,Abstract Information Integration systems today deal with data originating at multipleautonomous and heterogeneous data sources. This heterogeneity means that applicationssimultaneously manipulate data available in multiple models or schemas. Such interactionsare enabled by the presence of semantic mappings between the different models. Mappingsmake it possible to reason about data and answer queries across different data models.However; determining these semantic mappings is a difficult task because of a variety ofreasons-models use different vocabularies; different design methodologies and might havedifferent design goals. Further the large sizes of models; the need to incorporate multipleand varied evidences and the ubiquitous nature of applications that require mappings;necessitate the building of tools for automatic or semi-automatic (involving some user …,*,1999,1
Learning to map between ontologies,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alan Halevy,*,World Wide Web Conference,*,1
Using SSDs to scale up Google Fusion Tables; a database-in-the-cloud,Yingyi Bu; Felix Halim; Changkyu Kim; Hongrae Lee; Jayant Madhavan,Flash memory solid state drives (SSDs) have increasingly been advocated and adopted asa means of speeding up and scaling up data-driven applications. SSDs are becoming morewidely available as an option in the cloud. However; when an application considers SSDs inthe cloud; the best option for the application may not be immediate; among a number ofchoices for placing SSDs in the layers of the cloud. Although there have been many studieson SSDs; they often concern a specific setting; and how different SSD options in the cloudcompare with each other is less well understood. In this paper; we describe how GoogleFusion Tables (GFT) used SSDs and what optimizations were implemented to scale up its in-memory processing; clearly showing opportunities and limitations of SSDs in the cloud withquantitative analyses. We first discuss various SSD placement strategies and compare …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2016,*
Harvesting relational tables from lists on the web,*,List information can be extracted into database tables. A number of fields are independentlydetermined for items in list. A number of database table columns are determined from mostcommon number of list item fields. New fields are determined for items with more fields thandatabase columns. Null fields are inserted into items with fewer fields than databasecolumns. Information from items having the same number of fields as database columns iswritten to database table rows. Information from each field is written to a correspondingdatabase table column. Streaks of poorly matching cells in a database table row aredetermined. Streak cells are merged and new cells are determined. Null cells are inserted ifnumber of new cells is less than number of cells in the streak. Information from the new cellsis written to the table row and columns that define the streak.,*,2014,*
Table Search Using Recovered Semantics,Petros Venetis; Alon Halevy; Jayant Madhavan; Marius Pasca; Warren Shen; Fei Wu; Gengxin Miao; Chung Wu,ABSTRACT We consider the problem of searching for tables in a large table corpus. TheWeb offers a corpus of 100 million tables; and smaller but sizable corpora are found withinenterprises or individual repositories (eg; data. gov). Table search is challenging becausethe semantics of the data are typically not explicit in the table itself; and signals that workwell for search over document corpora do not apply as well to table corpora. We describethe TableFinder system that partially recovers the semantics of the tables in the corpus; bymapping tables into a database of class labels that is automatically extracted from the Webitself. The database of classes has very wide coverage; but is also noisy. TableFinderidentifies a column in each table corresponding to the table's subject and identifies theclasses describing the values in that column. Query answering proceeds by considering …,*,2010,*
KNOWLEDGE AND DATA ENGINEERING,B Cui; BC Ooi; J Su; KL Tan; YK Woon; WK Ng; EP Lim; KL Tan; AY Halevy; ZG Ives; J Madhavan; P Mork; D Suciu; I Tatarinov; S Shah; K Ramamritham; P Shenoy; HT Shen; Y Shu; B Yu; E Bertino; E Ferrari; AC Squicciarini; L Xiong; L Liu; K Aberer; A Datta; M Hauswirth,CONCISE PAPERS Databases Main Memory Indexing: The Case for BD-Tree B. Cui; BCOoi; J. Su; and K.-L. Tan ................................................................................................................................ Data Mining A Support-Ordered Trie for Fast Frequency Itemset Discovery Y.-K. Woon;W.-K. Ng; and E.-P. Lim … SPECIAL SECTION ON PEER-TO-PEER-BASED DATA MANAGEMENTGuest Editors' Introduction: Special Section on Peer-to-Peer-Based Data Management BC Ooiand K.-L. Tan ...................................................................................................................................................... The Piazza Peer Data Management System AY Halevy; ZG Ives …,*,2004,*
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 The Piazza Peer Data Management System,Alon Y Halevy; Zachary G Ives; Jayant Madhavan; Peter Mork; Dan Suciu; Igor Tatarinov,Abstract Intuitively; data management and data integration tools should be well-suited forexchanging information in a semantically meaningful way. Unfortunately; they suffer fromtwo significant problems: they typically require a comprehensive schema design before theycan be used to store or share information; and they are difficult to extend because schemaevolution is heavyweight and may break backward compatibility. As a result; many small-scale data sharing tasks are more easily facilitated by nondatabase-oriented tools that havelittle support for semantics. The goal of the peer data management system (PDMS) is toaddress this need: we propose the use of a decentralized; easily extensible datamanagement architecture in which any user can contribute new data; schema information; oreven mappings between other peers' schemas. PDMSs represent a natural step beyond …,*,2004,*
Generic Schema Matching With Cupid Jayant Madhavan,Jayant Madhavan; Afshin Rostamizadeh; Warren Shen; Kenneth Wilder; Fei Wu; Philip A Bernstein; Erhard Rahm,Philip A. Bernstein; Jayant Madhavan; Erhard Rahm (Philip A. Generic Schema Matching WithCupid (Generic Schema Matching; Ten Years Later). 2010. matching; and its subtype schemamatching; are reviewed and compared. Cupid is a mapping tool that discovers mappings betweenschema elements (9) Jayant Madhavan; Philip A Bernstein; and Erhard Rahm. “Genericschema … List of computer science publications by Jayant Madhavan … Generic SchemaMatching; Ten Years Later. Generic … Jayant Madhavan; Afshin Rostamizadeh; WarrenShen; Kenneth Wilder; Fei Wu; Quality Impact of Value Matching and Scoring in Topk Entity AttributeExtraction. Managing Complex Databases in a Schema Management Framework. Applying GenericSchema Management to Bioinformatics. Jayant Madhavan; Philip A. Bernstein; ErhardRahm: Generic Schema Matching with Cupid … Generic Schema Matching With Cupid …,*,*,*
between elements of two schemas,Jayant Madhavan,Page 1. Philip A. Bernstein Microsoft Corp. Jayant Madhavan Google Erhard Rahm Univ. of Leipzig► The problem of generating correspondences between elements of two schemas ISBNchar(15) key Title varchar(100) Author varchar(50) MarkedPrice float ID char(15) key AuthorIDinteger references AuthorInfo BookTitle varchar(150) ListPrice float DiscountPrice float BooksBookInfo AuthorID integer key LastName varchar(25) FirstName varchar(25) AuthorInfo Page2. ► Element names ► Schema structure ID char(15) key AuthorID integer references AuthorInfoBookTitle varchar(150) ListPrice float DiscountPrice float ISBN char(15) key Title varchar(100)Author varchar(50) MarkedPrice float Books BookInfo AuthorID integer key LastNamevarchar(25) FirstName varchar(25) AuthorInfo ► Constraints: data type; keys; nullability ►Synonyms ◦ Code = Id = Num = No ◦ Zip = Postal [code] ◦ Node = Server …,*,*,*
Data Engineering,Amitanand Aiyer; Mikhail Bautin; Guoqiang Jerry Chen; Pritam Damania; Prakash Khemani; Kannan Muthukkaruppan; Karthik Ranganathan; Nicolas Spiegelberg; Liyin Tang; Madhuwanti Vaidya; Arvind Arasu; Surajit Chaudhuri; Zhimin Chen; Kris Ganjam; Raghav Kaushik; Vivek Narasayya; Michael J Carey Bu; Joshua Rosen; Neoklis Polyzotis; Tyson Condie; Markus Weimer; Raghu Ramakrishnan; Ken Goodhope; Joel Koshy; Jay Kreps; Neha Narkhede; Richard Park; Jun Rao; Victor Yang Ye; Sreeram Balakrishnan Madhavan; Kathryn Brisbin; Hector Gonzalez; Nitin Gupta; Alon Halevy; Karen Jacqmin-Adams; Heidi Lam; Anno Langen; Hongrae Lee; Rod McChesney; Rebecca Shapley; Warren Shen,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Schemr: a Schema Search Engine for Information Integration,Kuang Chen; Jayant Madhavan; Alon Halevy,Schemr is a schema search engine; and provides users the ability to search for andvisualize schemas stored in a metadata repository. Users may search by keywords and byexample–using schema fragments as query terms. Schemr uses a novel search algorithm;based on a combination of text search and schema matching techniques; as well as astructurally-aware scoring metric. Schemr presents search results in a GUI that allows usersto explore which elements match and how well they do. The GUI supports interactions;including panning; zooming; layout and drilling-in. We demonstrate schema search andvisualization; introduce Schemr as a new component of the information integration toolbox;and discuss its benefits in several applications.,*,*,*
Mining Structures for Semantics,Alon Halevy; Jayant Madhavan; Xin Dong,Online data is available in two flavors: unstructured data that resides as free text in HTMLpages; and structured data that resides in databases and knowledge bases. Unstructureddata is easily accessed as human-readable text on a browser; while structured data ishidden behind web query interfaces (web forms); web services; and custom database APIs.Access to this data; popularly refered to as the hidden web; entails submitting correctlycompleted web forms or writing source code using protocols such as SOAP. Unstructuredtext while being human readable; is not readily machine understandable. The need toaccurately identify semantics from natural language text makes is very hard to automaticallyprocess such data. Structured data; with an accompanying schema defining its semantics;can be automatically processed using existing rich query languages.(Luna: Suggest …,*,*,*
Data Engineering,Bin Liu; Laura Chiticariu; Vivian Chu; HV Jagadish; Frederick R Reiss; Anno Langen; Jayant Madhavan; Rod McChesney; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,The SMDB Workshop series sponsored by the IEEE TCDE Workgroup on Self-ManagingDatabase Systems brings together researchers and practitioners to exchange ideas relatedto autonomic data management systems. Previous workshops of the SMDB series focusedon core topics in self-managing databases like physical design tuning; problem diagnosisand recovery; and database integration and protection. In addition to core topics; the 2010workshop aimed to broaden the interest range by covering emerging research areas likeCloud computing; multitenant databases; large-scale storage systems; and datacenteradministration.,*,*,*
Data Engineering,Philip A Bernstein; Nishant Dani; Badriddine Khessib; Ramesh Manne; David Shutt; Jayant Madhavan; Alon Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu; Varun Bhagwan; Mike Ching; Alex Cozzi; Raj Desai; Daniel Gruhl; Kevin Haas; Linda Kato; Jeff Kusnitz; Bryan Langston; Ferdy Nagy; Linda Nguyen; Jan Pieper; Savitha Srinivasan; Anthony Stuart; Renjie Tang,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily reflect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems. There are two DataEngineering Bulletin web sites: http://www. research. microsoft. com/research/db/debull …,*,*,*
