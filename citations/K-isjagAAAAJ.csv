Single image super-resolution using Gaussian process regression,He He; Wan-Chi Siu,In this paper we address the problem of producing a high-resolution image from a single low-resolution image without any external training set. We propose a framework for bothmagnification and deblurring using only the original low-resolution image and its blurredversion. In our method; each pixel is predicted by its neighbors through the Gaussianprocess regression. We show that when using a proper covariance function; the Gaussianprocess regression can perform soft clustering of pixels based on their local structures. Wefurther demonstrate that our algorithm can extract adequate information contained in asingle low-resolution image to generate a high-resolution image with sharp edges; which iscomparable to or even superior in quality to the performance of other edge-directed andexample-based super-resolution algorithms. Experimental results also show that our …,Computer Vision and Pattern Recognition (CVPR); 2011 IEEE Conference on,2011,165
Imitation learning by coaching,He He; Jason Eisner; Hal Daume,Abstract Imitation Learning has been shown to be successful in solving many challengingreal-world problems. Some recent approaches give strong performance guarantees bytraining the policy iteratively. However; it is important to note that these guarantees dependon how well the policy we found can imitate the oracle on the training data. When there is asubstantial difference between the oracle's ability and the learner's policy space; we may failto find a policy that has low error on the training set. In such cases; we propose to use acoach that demonstrates easy-to-learn actions for the learner and gradually approaches theoracle. By a reduction of learning by demonstration to online learning; we prove thatcoaching can yield a lower regret bound than using the oracle. We apply our algorithm to anovel cost-sensitive dynamic feature selection problem; a hard decision problem that …,Advances in Neural Information Processing Systems,2012,48
Dynamic feature selection for dependency parsing,He He; Hal Daumé III; Jason Eisner,Abstract Feature computation and exhaustive search have significantly restricted the speedof graph-based dependency parsing. We propose a faster framework of dynamic featureselection; where features are added sequentially as needed; edges are pruned early; anddecisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7languages. Our dynamic parser can achieve accuracies comparable or even superior toparsers using a full set of features; while computing fewer than 30% of the feature templates.,Proceedings of the 2013 conference on empirical methods in natural language processing,2013,34
Cost-sensitive dynamic feature selection,He He; Hal Daumé III; Jason Eisner,• Select the feature that yields the maximum immediate reward r (st; at)= margin (st; at)− cost(st; at) λ= 1; cost scaled to [0; 1]; H1N1= positive order feat. marg. cost reward 1 coughing;sore throat; headache-0.20 0.00-0.10 3 temperature (101◦) 0.55 0.05 0.50 2 nasal swab test(pos.) 0.50 0.04 0.46 4 viral culture test (pos.) 0.80 0.24 0.56 6 molecular test (pos.) 0.951.00-0.05 5 blood test (pos.) 0.90 0.62 0.28,ICML Inferning Workshop,2012,26
Rare class classification by support vector machine,He He; Ali Ghodsi,The problem of classification on highly imbalanced datasets has been studied extensively inthe literature. Most classifiers show significant deterioration in performance when dealingwith skewed datasets. In this paper; we first examine the underlying reasons for SVM'sdeterioration on imbalanced datasets. We then propose two modifications for the soft marginSVM; where we change or add constraints to the optimization problem. The proposedmethods are compared with regular SVM; cost-sensitive SVM and two re-sampling methods.Our experimental results demonstrate that this constrained SVM can consistently outperformthe other associated methods.,Pattern Recognition (ICPR); 2010 20th International Conference on,2010,25
Besting the quiz master: Crowdsourcing incremental classification games,Jordan Boyd-Graber; Brianna Satinoff; He He; Hal Daumé III,Abstract Cost-sensitive classification; where the features used in machine learning taskshave a cost; has been explored as a means of balancing knowledge against the expense ofincrementally obtaining new features. We introduce a setting where humans engage inclassification with incrementally revealed features: the collegiate trivia circuit. By providingthe community with a web-based system to practice; we collected tens of thousands ofimplicit word-by-word ratings of how useful features are for eliciting correct answers.Observing humans' classification process; we improve the performance of a state-of-the artclassifier. We also use the dataset to evaluate a system to compete in the incrementalclassification task through a reduction of reinforcement learning to classification. Our systemlearns when to answer a question; performing better than baselines and most human …,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,24
Opponent modeling in deep reinforcement learning,He He; Jordan Boyd-Graber; Kevin Kwok; Hal Daumé III,Abstract Opponent modeling is necessary in multi-agent settings where secondary agentswith competing goals also adapt their strategies; yet it remains challenging becausestrategies interact with each other and change. Most previous work focuses on developingprobabilistic models or parameterized strategies for specific applications. Inspired by therecent success of deep reinforcement learning; we present neural-based models that jointlylearn a policy and the behavior of opponents. Instead of explicitly predicting the opponent'saction; we encode observation of the opponents into a deep Q-Network (DQN); however; weretain explicit modeling (if desired) using multitasking. By using a Mixture-of-Expertsarchitecture; our model automatically discovers different strategy patterns of opponentswithout extra supervision. We evaluate our models on a simulated soccer game and a …,International Conference on Machine Learning,2016,19
Don’t until the final verb wait: Reinforcement learning for simultaneous machine translation,Alvin Grissom II; He He; Jordan Boyd-Graber; John Morgan; Hal Daumé III,Abstract We introduce a reinforcement learningbased approach to simultaneous machinetranslation—producing a translation while receiving input words—between languages withdrastically different word orders: from verb-final languages (eg; German) to verb-mediallanguages (English). In traditional machine translation; a translator must “wait” for sourcematerial to appear before translation begins. We remove this bottleneck by predicting thefinal verb in advance. We use reinforcement learning to learn when to trust predictions aboutunseen; future portions of the sentence. We also introduce an evaluation metric to measureexpeditiousness and quality. We show that our new translation model outperforms batch andmonotone translation strategies.,Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP),2014,18
Learning to search in branch and bound algorithms,He He; Hal Daume III; Jason M Eisner,Abstract Branch-and-bound is a widely used method in combinatorial optimization; includingmixed integer programming; structured prediction and MAP inference. While most work hasbeen focused on developing problem-specific techniques; little is known about how tosystematically design the node searching strategy on a branch-and-bound tree. We addressthe key challenge of learning an adaptive node searching order for any class of problemsolvable by branch-and-bound. Our strategies are learned by imitation learning. We applyour algorithm to linear programming based branch-and-bound for solving mixed integerprograms (MIP). We compare our method with one of the fastest open-source solvers; SCIP;and a very efficient commercial solver; Gurobi. We demonstrate that our approach achievesbetter solutions faster on four MIP libraries.,Advances in neural information processing systems,2014,13
Learning to search for dependencies,Kai-Wei Chang; He He; Hal Daumé III; John Langford,Abstract: We demonstrate that a dependency parser can be built using a credit assignmentcompiler which removes the burden of worrying about low-level machine learning detailsfrom the parser implementation. The result is a simple parser which robustly applies to manylanguages that provides similar statistical and computational performance with best-to-datetransition-based parsing approaches; while avoiding various downsides includingrandomization; extra feature requirements; and custom learning algorithms. Subjects:Computation and Language (cs. CL); Learning (cs. LG) Cite as: arXiv: 1503.05615 [cs.CL](or arXiv: 1503.05615 v2 [cs. CL] for this version) Submission history From: Kai-WeiChang [view email][v1] Wed; 18 Mar 2015 23: 33: 17 GMT (42kb; D)[v2] Thu; 7 May 2015 22:12: 11 GMT (48kb; D),arXiv preprint arXiv:1503.05615,2015,12
Interpretese vs. translationese: The uniqueness of human strategies in simultaneous interpretation,He He; Jordan Boyd-Graber; Hal Daumé III,Abstract Computational approaches to simultaneous interpretation are stymied by how littlewe know about the tactics human interpreters use. We produce a parallel corpus oftranslated and simultaneously interpreted text and study differences between them througha computational approach. Our analysis reveals that human interpreters regularly applyseveral effective tactics to reduce translation latency; including sentence segmentation andpassivization. In addition to these unique; clever strategies; we show that limited humanmemory also causes other idiosyncratic properties of human interpretation such asgeneralization and omission of source content.,Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2016,5
Syntax-based rewriting for simultaneous machine translation,He He; Alvin Grissom II; John Morgan; Jordan Boyd-Graber; Hal Daumé III,Abstract Divergent word order between languages causes delay in simultaneous machinetranslation. We present a sentence rewriting method that generates more monotonictranslations to improve the speedaccuracy tradeoff. We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees. We applythe rules to reference translations to make their word order closer to the source languageword order. On Japanese-English translation (two languages with substantially differentstructure); incorporating the rewritten; more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline systemthat only uses gold reference translations.,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,5
Temporal supervised learning for inferring a dialog policy from example conversations,Lihong Li; He He; Jason D Williams,This paper tackles the problem of learning a dialog policy from example dialogs-forexample; from Wizard-of-Oz style dialogs; where an expert (person) plays the role of thesystem. Learning in this setting is challenging because dialog is a temporal process in whichactions affect the future course of the conversation-ie; dialog requires planning. Past worksolved this problem with either conventional supervised learning or reinforcement learning.Reinforcement learning provides a principled approach to planning; but requires moreresources than a fixed corpus of examples; such as a dialog simulator or a reward function.Conventional supervised learning; by contrast; operates directly from example dialogs butdoes not take proper account of planning. We introduce a new algorithm called TemporalSupervised Learning which learns directly from example dialogs; while also taking proper …,Spoken Language Technology Workshop (SLT); 2014 IEEE,2014,5
Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,He He; Anusha Balakrishnan; Mihail Eric; Percy Liang,Abstract: We study a symmetric collaborative dialogue setting in which two agents; each withprivate knowledge; must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. Wecollected a dataset of 11K human-human dialogues; which exhibits interesting lexical;semantic; and strategic elements. To model both structured knowledge and unstructuredlanguage; we propose a neural model with dynamic knowledge graph embeddings thatevolve as the dialogue progresses. Automatic and human evaluations show that our modelis both more effective at achieving the goal and more human-like than baseline neural andrule-based models.,Association for Computational Linguistics (ACL),2017,3
Object detection in 20 questions,Xi Stephen Chen; He He; Larry S Davis,We propose a novel general strategy for object detection. Instead of passively evaluating allobject detectors at all possible locations in an image; we develop a divide-and-conquerapproach by actively and sequentially evaluating contextual cues related to the query basedon the scene and previous evaluations-like playing a" 20 Questions" game-to decide whereto search for the object. We formulate the problem as a Markov Decision Process and learna search policy by reinforcement learning. To demonstrate the efficacy of our genericalgorithm; we apply the 20 questions approach in the recent framework of simultaneousobject detection and segmentation. Experimental results on the Pascal VOC dataset showthat our algorithm reduces about 45.3% of the object proposals and 36% of averageevaluation time while achieving better average precision compared to exhaustive search.,Applications of Computer Vision (WACV); 2016 IEEE Winter Conference on,2016,3
A credit assignment compiler for joint prediction,Kai-Wei Chang; He He; Stephane Ross; Hal Daume III; John Langford,Abstract Many machine learning applications involve jointly predicting multiple mutuallydependent output variables. Learning to search is a family of methods where the complexdecision problem is cast into a sequence of decisions via a search space. Although thesemethods have shown promise both in theory and in practice; implementing them has beenburdensomely awkward. In this paper; we show the search space can be defined by anarbitrary imperative program; turning learning to search into a credit assignment compiler.Altogether with the algorithmic improvements for the compiler; we radically reduce thecomplexity of programming and the running time. We demonstrate the feasibility of ourapproach on multiple joint prediction tasks. In all cases; we obtain accuracies as high asalternative approaches; at drastically reduced execution and programming time.,Advances in Neural Information Processing Systems,2016,3
Trust-aware optimal crowdsourcing with budget constraint,Xiangyang Liu; He He; John S Baras,Crowdsourcing has been extensively used for aggregating data from a large pool ofworkers. In a real crowdsourcing market; each answer obtained from a worker incurs cost.The cost is associated with both the level of trustworthiness of workers and the difficulty oftasks. Typically; access to expert-level (more trustworthy) workers is more expensive than toaverage crowd and completion of a challenging task is more costly than a click-awayquestion. In this paper; we address the problem of optimal assignment of heterogeneoustasks to workers of varying trust levels with budget constraint. Specifically; we design a trust-aware task allocation algorithm that takes as inputs the estimated trust of workers and pre-set budget; and outputs the optimal assignment of tasks to workers. We derive the bound oftotal error probability that relates to budget; trustworthiness of crowds; and costs of …,Communications (ICC); 2015 IEEE International Conference on,2015,3
Crowdsourcing with multi-dimensional trust,Xiangyang Liu; He He; John S Baras,We consider a typical crowdsourcing task that aggregates input from multiple workers as aproblem in information fusion. To cope with the issue of noisy and sometimes maliciousinput from workers; trust is used to model workers' expertise. In a multi-domain knowledgelearning task; however; using scalar-valued trust to model a worker's performance is notsufficient to reflect the worker's trustworthiness in each of the domains. To address this issue;we propose a probabilistic model to jointly infer multi-dimensional trust of workers; multi-domain properties of questions; and true labels of questions. Our model is very flexible andextensible to incorporate metadata associated with questions. To show that; we furtherpropose two extended models; one of which handles input tasks with real-valued featuresand the other handles tasks with text features by incorporating topic models. Finally; we …,Information Fusion (Fusion); 2015 18th International Conference on,2015,2
Active information acquisition,He He; Paul Mineiro; Nikos Karampatziakis,Abstract: We propose a general framework for sequential and dynamic acquisition of usefulinformation in order to solve a particular task. While our goal could in principle be tackled bygeneral reinforcement learning; our particular setting is constrained enough to allow moreefficient algorithms. In this paper; we work under the Learning to Search framework andshow how to formulate the goal of finding a dynamic information acquisition policy in thatframework. We apply our formulation on two tasks; sentiment analysis and imagerecognition; and show that the learned policies exhibit good statistical performance. As anemergent byproduct; the learned policies show a tendency to focus on the most prominentparts of each instance and give harder instances more attention without explicitly beingtrained to do so. Subjects: Machine Learning (stat. ML); Learning (cs. LG) Cite as: arXiv …,arXiv preprint arXiv:1602.02181,2016,1
Proceedings of the Workshop on Human-Computer Question Answering,Mohit Iyyer; He He; Jordan Boyd-Graber; Hal Daumé III,Welcome to the first Workshop on Human-Computer Question Answering (HCQA)! Questionanswering is a central task in natural language processing (NLP). Unlike other NLP tasks; italso is easy for non-experts to understand when question answering systems perform well(or fail). The goal of this workshop is to bring the community together to discuss the state ofthe art of question answering and interactively compete with top human trivia masters. Thisworkshop highlights question answering on the real-world task of quiz bowl; a trivia game inwhich competitors are asked to identify entities such as battles; novels; and scientific terms.In quiz bowl; a moderator reads a paragraph-long question to two teams; and players arepermitted to interrupt the moderator (or “buzz in”) with a guess if they feel confident. Thissetting is especially interesting because acquiring more features (clues) comes with an …,Proceedings of the Workshop on Human-Computer Question Answering,2016,*
Sequential decisions and predictions in natural language processing,He He,Abstract Natural language processing has achieved great success in a wide range ofapplications; producing both commercial language services and open-source languagetools. However; most methods take a static or batch approach; assuming that the model hasall information it needs and makes a one-time prediction. In this dissertation; we studydynamic problems where the input comes in a sequence instead of all at once; and theoutput must be produced while the input is arriving. In these problems; predictions are oftenmade based only on partial information. We see this dynamic setting in many real-time;interactive applications. These problems usually involve a trade-off between the amount ofinput received (cost) and the quality of the output prediction (accuracy). Therefore; theevaluation considers both objectives (eg; plotting a Pareto curve).,*,2016,*
Hands-on Learning to Search for Structured Prediction,Hal Daumé III; John Langford; Kai-Wei Chang; He He; Sudha Rao,Many problems in natural language processing involve building outputs that are structured.The predominant approach to structured prediction is “global models”(such as conditionalrandom fields); which have the advantage of clean underlying semantics at the cost ofcomputational burdens and extreme difficulty in implementation. An alternative strategy isthe “learning to search”(L2S) paradigm; in which the structured prediction task is cast as asequential decision making process. One can then devise training-time algorithms that learnto make near optimal collective decisions. This paradigm has been gaining increasingtraction over the past five years: most notably in dependency parsing (eg; MaltParser;ClearNLP; etc.); but also much more broadly in less “sequential” tasks like entity/relationclassification and even graph prediction problems found in social network analysis and …,Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorial Abstracts,2015,*
