DBpedia: A nucleus for a web of open data,Sören Auer; Christian Bizer; Georgi Kobilarov; Jens Lehmann; Richard Cyganiak; Zachary Ives,Abstract DBpedia is a community effort to extract structured information from Wikipedia andto make this information available on the Web. DBpedia allows you to ask sophisticatedqueries against datasets derived from Wikipedia and to link other datasets on the Web toWikipedia data. We describe the extraction of the DBpedia datasets; and how the resultinginformation is published on the Web for human-and machine-consumption. We describesome emerging applications from the DBpedia community and show how website authorscan facilitate DBpedia content within their sites. Finally; we present the current status ofinterlinking DBpedia with other open datasets on the Web and outline how DBpedia couldserve as a nucleus for an emerging Web of open data.,6th International Semantic Web Conference (ISWC),2007,2940
DBpedia-a crystallization point for the web of data,Jens Lehmann; Chris Bizer; Georgi Kobilarov; Sören Auer; Christian Becker; Richard Cyganiak; Sebastian Hellmann,*,Journal of Web Semantics,2009,1931
DBpedia–a large-scale; multilingual knowledge base extracted from Wikipedia,Jens Lehmann; Robert Isele; Max Jakob; Anja Jentzsch; Dimitris Kontokostas; Pablo N Mendes; Sebastian Hellmann; Mohamed Morsey; Patrick Van Kleef; Sören Auer; Christian Bizer,Abstract The DBpedia community project extracts structured; multilingual knowledge fromWikipedia and makes it freely available on the Web using Semantic Web and Linked Datatechnologies. The project extracts knowledge from 111 different language editions ofWikipedia. The largest DBpedia knowledge base which is extracted from the English editionof Wikipedia consists of over 400 million facts that describe 3.7 million things. The DBpediaknowledge bases that are extracted from the other 110 Wikipedia editions together consist of1.46 billion facts and describe 10 million additional things. The DBpedia project mapsWikipedia infoboxes from 27 different language editions to a single shared ontologyconsisting of 320 classes and 1;650 properties. The mappings are created via a world-widecrowd-sourcing effort and enable knowledge from the different Wikipedia editions to be …,Semantic Web,2015,968
OntoWiki–A tool for social; semantic collaboration,Sören Auer; Sebastian Dietzold; Thomas Riechert,Abstract We present OntoWiki; a tool providing support for agile; distributed knowledgeengineering scenarios. OntoWiki facilitates the visual presentation of a knowledge base asan information map; with different views on instance data. It enables intuitive authoring ofsemantic content; with an inline editing mode for editing RDF content; similar to WYSIWYGfor text documents. It fosters social collaboration aspects by keeping track of changes;allowing to comment and discuss every single part of a knowledge base; enabling to rateand measure the popularity of content and honoring the activity of users. Ontowiki enhancesthe browsing and retrieval by offering semantic enhanced search strategies. All thesetechniques are applied with the ultimate goal of decreasing the entrance barrier for projectsand domain experts to collaborate using semantic technologies. In the spirit of the Web …,International Semantic Web Conference (ISWC),2006,478
Triplify: light-weight linked data publication from relational databases,Sören Auer; Sebastian Dietzold; Jens Lehmann; Sebastian Hellmann; David Aumueller,Abstract In this paper we present Triplify-a simplistic but effective approach to publish LinkedData from relational databases. Triplify is based on mapping HTTP-URI requests ontorelational database queries. Triplify transforms the resulting relations into RDF statementsand publishes the data on the Web in various RDF serializations; in particular as LinkedData. The rationale for developing Triplify is that the largest part of information on the Web isalready stored in structured form; often as data contained in relational databases; but usuallypublished by Web applications only as HTML mixing structure; layout and content. In orderto reveal the pure structured information behind the current Web; we have implementedTriplify as a light-weight software component; which can be easily integrated into anddeployed by the numerous; widely installed Web applications. Our approach includes a …,18th International Conference on World Wide Web (WWW),2009,370
What have innsbruck and leipzig in common? extracting semantics from wiki content,Sören Auer; Jens Lehmann,Abstract Wikis are established means for the collaborative authoring; versioning andpublishing of textual articles. The Wikipedia project; for example; succeeded in creating theby far largest encyclopedia just on the basis of a wiki. Recently; several approaches havebeen proposed on how to extend wikis to allow the creation of structured and semanticallyenriched content. However; the means for creating semantically enriched structured contentare already available and are; although unconsciously; even used by Wikipedia authors. Inthis article; we present a method for revealing this structured content by extractinginformation from template instances. We suggest ways to efficiently query the vast amount ofextracted information (eg more than 8 million RDF statements for the English Wikipediaversion alone); leading to astonishing query answering possibilities (such as for the title …,Extended Semantic Web Conference (ESWC),2007,291
Limes-a time-efficient approach for large-scale link discovery on the web of data,Axel-Cyrille Ngonga Ngomo; Sören Auer,Abstract The Linked Data paradigm has evolved into a powerful enabler for the transitionfrom the documentoriented Web into the Semantic Web. While the amount of data publishedas Linked Data grows steadily and has surpassed 25 billion triples; less than 5% of thesetriples are links between knowledge bases. Link discovery frameworks provide thefunctionality necessary to discover missing links between knowledge bases. Yet; this taskrequires a significant amount of time; especially when it is carried out on large data sets.This paper presents and evaluates LIMES; a novel time-efficient approach for link discoveryin metric spaces. Our approach utilizes the mathematical characteristics of metric spacesduring the mapping process to filter out a large number of those instance pairs that do notsuffice the mapping conditions. We present the mathematical foundation and the core …,integration,2011,290
Quality assessment for linked data: A survey,Amrapali Zaveri; Anisa Rula; Andrea Maurino; Ricardo Pietrobon; Jens Lehmann; Sören Auer,Abstract The development and standardization of Semantic Web technologies has resultedin an unprecedented volume of data being published on the Web as Linked Data (LD).However; we observe widely varying data quality ranging from extensively curated datasetsto crowdsourced and extracted data of relatively low quality. In this article; we present theresults of a systematic review of approaches for assessing the quality of LD. We gatherexisting approaches and analyze them qualitatively. In particular; we unify and formalizecommonly used terminologies across papers related to data quality and provide acomprehensive list of 18 quality dimensions and 69 metrics. Additionally; we qualitativelyanalyze the 30 core approaches and 12 tools using a set of attributes. The aim of this articleis to provide researchers and data curators a comprehensive understanding of existing …,Semantic Web,2016,262
Linkedgeodata: Adding a spatial dimension to the web of data,Sören Auer; Jens Lehmann; Sebastian Hellmann,Abstract In order to employ the Web as a medium for data and information integration;comprehensive datasets and vocabularies are required as they enable the disambiguationand alignment of other data and information. Many real-life information integration andaggregation tasks are impossible without comprehensive background knowledge related tospatial features of the ways; structures and landscapes surrounding us. In this paper wecontribute to the generation of a spatial dimension for the Data Web by elaborating on howthe collaboratively collected OpenStreetMap data can be transformed and representedadhering to the RDF data model. We describe how this data can be interlinked with otherspatial data sets; how it can be made accessible for machines according to the linked dataparadigm and for humans by means of a faceted geo-data browser.,*,2009,262
LinkedGeoData: A Core for a Web of Spatial Open Data,Claus Stadler; Jens Lehmann; Konrad Höffner; Sören Auer,Abstract The Semantic Web eases data and information integration tasks by providing aninfrastructure based on RDF and ontologies. In this paper; we contribute to the developmentof a spatial Data Web by elaborating on how the collaboratively collected OpenStreetMapdata can be interactively transformed and represented adhering to the RDF data model. Thistransformation will simplify information integration and aggregation tasks that requirecomprehensive background knowledge related to spatial features such as ways; structures;and landscapes. We describe how this data is interlinked with other spatial data sets; how itcan be made accessible for machines according to the Linked Data paradigm and forhumans by means of several applications; including a faceted geo-browser. The spatialdata; vocabularies; interlinks and some of the applications are openly available in the …,Semantic Web Journal,2012,240
DBpedia SPARQL benchmark–performance assessment with real queries on real data,Mohamed Morsey; Jens Lehmann; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract Triple stores are the backbone of increasingly many Data Web applications. It isthus evident that the performance of those stores is mission critical for individual projects aswell as for data integration on the Data Web in general. Consequently; it is of centralimportance during the implementation of any of these applications to have a clear picture ofthe weaknesses and strengths of current triple store implementations. In this paper; wepropose a generic SPARQL benchmark creation procedure; which we apply to the DBpediaknowledge base. Previous approaches often compared relational and triple stores and; thus;settled on measuring performance against a relational database which had been convertedto RDF by using SQL-like queries. In contrast to those approaches; our benchmark is basedon queries that were actually issued by humans and applications against existing RDF …,International Semantic Web Conference,2011,220
Mapping XML to OWL Ontologies.,Hannes Bohring; Sören Auer,Abstract: By now; XML has reached a wide acceptance as data exchange format in E-Business. An efficient collaboration between different participants in E-Business thus; is onlypossible; when business partners agree on a common syntax and have a commonunderstanding of the basic concepts in the domain. XML covers the syntactic level; but lackssupport for efficient sharing of conceptualizations. The Web Ontology Language (OWL[Bec04]) in turn supports the representation of domain knowledge using classes; propertiesand instances for the use in a distributed environment as the World Wide Web. We presentin this paper a mapping between the data model elements of XML and OWL. We giveaccount about its implementation within a ready-to-use XSLT framework; as well as itsevaluation for common use cases.,Leipziger Informatik-Tage,2005,206
A survey of current approaches for mapping of relational databases to RDF,Satya S Sahoo; Wolfgang Halb; Sebastian Hellmann; Kingsley Idehen; Ted Thibodeau Jr; Sören Auer; Juan Sequeda; Ahmed Ezzat,Page 1. A Survey of Current Approaches for Mapping of Relational Databases to RDF W3CRDB2RDF Incubator Group January 08 2009 This version: http://www.w3.org/2005/Incubator/rdb2rdf/RDB2RDF_SurveyReport_01082009.pdf Latest version: http://www.w3.org/2005/Incubator/rdb2rdf/RDB2RDF_SurveyReport.pdf Authors: Satya S. Sahoo; Kno.e.sis Center;Wright State University Wolfgang Halb; Joanneum Research Sebastian Hellmann; Universityof Leipzig Kingsley Idehen; OpenLink Software Ted Thibodeau Jr; OpenLink Software SörenAuer; University of Leipzig Juan Sequeda; University of Texas at Austin Ahmed Ezzat;Business Intelligence Software Division; HP Copyright …,W3C RDB2RDF Incubator Group Report,2009,193
LODStats–an extensible framework for high-performance dataset analytics,Sören Auer; Jan Demter; Michael Martin; Jens Lehmann,Abstract One of the major obstacles for a wider usage of web data is the difficulty to obtain aclear picture of the available datasets. In order to reuse; link; revise or query a datasetpublished on the Web it is important to know the structure; coverage and coherence of thedata. In order to obtain such information we developed LODStats–a statement-stream-basedapproach for gathering comprehensive statistics about datasets adhering to the ResourceDescription Framework (RDF). LODStats is based on the declarative description of statisticaldataset characteristics. Its main advantages over other approaches are a smaller memoryfootprint and significantly better performance and scalability. We integrated LODStats withthe CKAN dataset metadata registry and obtained a comprehensive picture of the currentstate of a significant part of the Data Web.,International Conference on Knowledge Engineering and Knowledge Management,2012,167
Test-driven evaluation of linked data quality,Dimitris Kontokostas; Patrick Westphal; Sören Auer; Sebastian Hellmann; Jens Lehmann; Roland Cornelissen; Amrapali Zaveri,Abstract Linked Open Data (LOD) comprises an unprecedented volume of structured dataon the Web. However; these datasets are of varying quality ranging from extensively curateddatasets to crowdsourced or extracted data of often relatively low quality. We present amethodology for test-driven quality assessment of Linked Data; which is inspired by test-driven software development. We argue that vocabularies; ontologies and knowledge basesshould be accompanied by a number of test cases; which help to ensure a basic level ofquality. We present a methodology for assessing the quality of linked data resources; basedon a formalization of bad smells and data quality problems. Our formalization employsSPARQL query templates; which are instantiated into concrete quality test case queries.Based on an extensive survey; we compile a comprehensive library of data quality test …,23rd international conference on World Wide Web (WWW),2014,163
A systematic review of open government data initiatives,Judie Attard; Fabrizio Orlandi; Simon Scerri; Sören Auer,Abstract We conduct a systematic survey with the aim of assessing open government datainitiatives; that is; any attempt; by a government or otherwise; to open data that is producedby a governmental entity. We describe the open government data life-cycle and we focus ourdiscussion on publishing and consuming processes required within open government datainitiatives. We cover current approaches undertaken for such initiatives; and classify them. Anumber of evaluations found within related literature are discussed; and from them weextract challenges and issues that hinder open government initiatives from reaching their fullpotential. In a bid to overcome these challenges; we also extract guidelines for publishingdata and provide an integrated overview. This will enable stakeholders to start with a firmfoot in a new open government data initiative. We also identify the impacts on the …,Government Information Quarterly,2015,125
Integrating NLP using linked data,Sebastian Hellmann; Jens Lehmann; Sören Auer; Martin Brümmer,Abstract We are currently observing a plethora of Natural Language Processing tools andservices being made available. Each of the tools and services has its particular strengthsand weaknesses; but exploiting the strengths and synergistically combining different tools iscurrently an extremely cumbersome and time consuming task. Also; once a particular set oftools is integrated; this integration is not reusable by others. We argue that simplifying theinteroperability of different NLP tools performing similar but also complementary tasks willfacilitate the comparability of results and the creation of sophisticated NLP applications. Inthis paper; we present the NLP Interchange Format (NIF). NIF is based on a Linked Dataenabled URI scheme for identifying elements in (hyper-) texts and an ontology for describingcommon NLP terms and concepts. In contrast to more centralized solutions such as UIMA …,International semantic web conference,2013,116
Managing the life-cycle of linked data with the LOD2 stack,Sören Auer; Lorenz Bühmann; Christian Dirschl; Orri Erling; Michael Hausenblas; Robert Isele; Jens Lehmann; Michael Martin; Pablo N Mendes; Bert Van Nuffelen; Claus Stadler; Sebastian Tramp; Hugh Williams,Abstract The LOD2 Stack is an integrated distribution of aligned tools which support thewhole life cycle of Linked Data from extraction; authoring/creation via enrichment;interlinking; fusing to maintenance. The LOD2 Stack comprises new and substantiallyextended existing tools from the LOD2 project partners and third parties. The stack isdesigned to be versatile; for all functionality we define clear interfaces; which enable theplugging in of alternative third-party implementations. The architecture of the LOD2 Stack isbased on three pillars:(1) Software integration and deployment using the Debian packagingsystem.(2) Use of a central SPARQL endpoint and standardized vocabularies for knowledgebase access and integration between the different tools of the LOD2 Stack.(3) Integration ofthe LOD2 Stack user interfaces based on REST enabled Web Applications. These three …,11th International Semantic Web Conference (ISWC),2012,112
AGDISTIS-graph-based disambiguation of named entities using linked data,Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo; Michael Röder; Daniel Gerber; Sandro Athaide Coelho; Sören Auer; Andreas Both,Abstract Over the last decades; several billion Web pages have been made available on theWeb. The ongoing transition from the current Web of unstructured data to the Web of Datayet requires scalable and accurate approaches for the extraction of structured data in RDF(Resource Description Framework) from these websites. One of the key steps towardsextracting RDF from text is the disambiguation of named entities. While several approachesaim to tackle this problem; they still achieve poor accuracy. We address this drawback bypresenting AGDISTIS; a novel knowledge-base-agnostic approach for named entitydisambiguation. Our approach combines the Hypertext-Induced Topic Search (HITS)algorithm with label expansion strategies and string similarity measures. Based on thiscombination; AGDISTIS can efficiently detect the correct URIs for a given set of named …,International Semantic Web Conference,2014,107
User-driven quality evaluation of dbpedia,Amrapali Zaveri; Dimitris Kontokostas; Mohamed A Sherif; Lorenz Bühmann; Mohamed Morsey; Sören Auer; Jens Lehmann,Abstract Linked Open Data (LOD) comprises of an unprecedented volume of structureddatasets on the Web. However; these datasets are of varying quality ranging fromextensively curated datasets to crowdsourced and even extracted data of relatively lowquality. We present a methodology for assessing the quality of linked data resources; whichcomprises of a manual and a semi-automatic process. The first phase includes the detectionof common quality problems and their representation in a quality problem taxonomy. In themanual process; the second phase comprises of the evaluation of a large number ofindividual resources; according to the quality problem taxonomy via crowdsourcing. Thisprocess is accompanied by a tool wherein a user assesses an individual resource andevaluates each fact for correctness. The semi-automatic process involves the generation …,9th International Conference on Semantic Systems (SEMANTiCS),2013,94
Dbpedia and the live extraction of structured data from wikipedia,Mohamed Morsey; Jens Lehmann; Sören Auer; Claus Stadler; Sebastian Hellmann,Purpose–DBpedia extracts structured information from Wikipedia; interlinks it with otherknowledge bases and freely publishes the results on the web using Linked Data andSPARQL. However; the DBpedia release process is heavyweight and releases aresometimes based on several months old data. DBpedia-Live solves this problem byproviding a live synchronization method based on the update stream of Wikipedia. Thispaper seeks to address these issues. Design/methodology/approach–Wikipedia providesDBpedia with a continuous stream of updates; ie a stream of articles; which were recentlyupdated. DBpedia-Live processes that stream on the fly to obtain RDF data and stores theextracted data back to DBpedia. DBpedia-Live publishes the newly added/deleted triples infiles; in order to enable synchronization between the DBpedia endpoint and other …,Program,2012,92
Towards a Semantic Wiki Experience-Desktop Integration and Interactivity in WikSAR.,David Aumüller; Sören Auer,Abstract. Common Wiki systems such as MediaWiki lack semantic annotations. WikSAR(Semantic Authoring and Retrieval within a Wiki); a prototype of a semantic Wiki; offerseffortless semantic authoring. Instant gratification of users is achieved by context awaremeans of navigation; interactive graph visualisation of the emerging ontology; as well assemantic retrieval possibilities. Embedding queries into Wiki pages creates views (asdependant collections) on the information space. Desktop integration includes accessingdates (eg reminders) entered in the Wiki via local calendar applications; maintainingbookmarks; and collecting web quotes within the Wiki. Approaches to reference documentson the local file system are sketched out; as well as an enhancement of the Wiki interface tosuggest appropriate semantic annotations to the user.,Semantic Desktop Workshop,2005,92
Crowdsourcing linked data quality assessment,Maribel Acosta; Amrapali Zaveri; Elena Simperl; Dimitris Kontokostas; Sören Auer; Jens Lehmann,Abstract In this paper we look into the use of crowdsourcing as a means to handle LinkedData quality problems that are challenging to be solved automatically. We analyzed the mostcommon errors encountered in Linked Data sources and classified them according to theextent to which they are likely to be amenable to a specific form of crowdsourcing. Based onthis analysis; we implemented a quality assessment methodology for Linked Data thatleverages the wisdom of the crowds in different ways:(i) a contest targeting an expert crowdof researchers and Linked Data enthusiasts; complemented by (ii) paid microtaskspublished on Amazon Mechanical Turk. We empirically evaluated how this methodologycould efficiently spot quality issues in DBpedia. We also investigated how the contributionsof the two types of crowds could be optimally integrated into Linked Data curation …,International Semantic Web Conference,2013,91
Class expression learning for ontology engineering,Jens Lehmann; Sören Auer; Lorenz Bühmann; Sebastian Tramp,Abstract While the number of knowledge bases in the Semantic Web increases; themaintenance and creation of ontology schemata still remain a challenge. In particularcreating class expressions constitutes one of the more demanding aspects of ontologyengineering. In this article we describe how to adapt a semi-automatic method for learningOWL class expressions to the ontology engineering use case. Specifically; we describe howto extend an existing learning algorithm for the class learning problem. We perform rigorousperformance optimization of the underlying algorithms for providing instant suggestions tothe user. We also present two plugins; which use the algorithm; for the popular Protégé andOntoWiki ontology editors and provide a preliminary evaluation on real ontologies.,Web Semantics: Science; Services and Agents on the World Wide Web,2011,91
Creating knowledge out of interlinked data,Sören Auer; Jens Lehmann,Abstract Over the past 3 years; the Semantic Web activity has gained momentum with thewidespread publishing of structured data as RDF. The Linked Data paradigm has thereforeevolved from a practical research idea into a very promising candidate for addressing one ofthe biggest challenges in the area of the Semantic Web vision: the exploitation of the Web asa platform for data and information integration. To translate this initial success into a world-scale reality; a number of research challenges need to be addressed: the performance gapbetween relational and RDF data management has to be closed; coherence and quality ofdata published on the Web have to be improved; provenance and trust on the Linked DataWeb must be established and generally the entrance barrier for data publishers and usershas to be lowered. In this vision statement we discuss these challenges and argue; that …,Semantic Web,2010,86
Introduction to linked data and its lifecycle on the web,Axel-Cyrille Ngonga Ngomo; Sören Auer; Jens Lehmann; Amrapali Zaveri,Abstract With Linked Data; a very pragmatic approach towards achieving the vision of theSemantic Web has gained some traction in the last years. The term Linked Data refers to aset of best practices for publishing and interlinking structured data on the Web. While manystandards; methods and technologies developed within by the Semantic Web communityare applicable for Linked Data; there are also a number of specific characteristics of LinkedData; which have to be considered. In this article we introduce the main concepts of LinkedData. We present an overview of the Linked Data life-cycle and discuss individualapproaches as well as the state-of-the-art with regard to extraction; authoring; linking;enrichment as well as quality of Linked Data. We conclude the chapter with a discussion ofissues; limitations and further research and development challenges of Linked Data. This …,Reasoning Web International Summer School,2014,84
Introduction to linked data and its lifecycle on the web,Axel-Cyrille Ngonga Ngomo; Sören Auer; Jens Lehmann; Amrapali Zaveri,Abstract With Linked Data; a very pragmatic approach towards achieving the vision of theSemantic Web has gained some traction in the last years. The term Linked Data refers to aset of best practices for publishing and interlinking structured data on the Web. While manystandards; methods and technologies developed within by the Semantic Web communityare applicable for Linked Data; there are also a number of specific characteristics of LinkedData; which have to be considered. In this article we introduce the main concepts of LinkedData. We present an overview of the Linked Data life-cycle and discuss individualapproaches as well as the state-of-the-art with regard to extraction; authoring; linking;enrichment as well as quality of Linked Data. We conclude the chapter with a discussion ofissues; limitations and further research and development challenges of Linked Data. This …,Reasoning Web International Summer School,2014,84
Learning of OWL class descriptions on very large knowledge bases,Sebastian Hellmann; Jens Lehmann; Sören Auer,Abstract The vision of the Semantic Web is to make use of semantic representations on thelargest possible scale-the Web. Large knowledge bases such as DBpedia; OpenCyc;GovTrack; and others are emerging and are freely available as Linked Data and SPARQLendpoints. Exploring and analysing such knowledge bases is a significant hurdle forSemantic Web research and practice. As one possible direction for tackling this problem; theauthors present an approach for obtaining complex class descriptions from objects inknowledge bases by using Machine Learning techniques. They describe in detail how weleverage existing techniques to achieve scalability on large knowledge bases available asSPARQL endpoints or Linked Data. Their algorithms are made available in the open sourceDL-Learner project and we present several real-life scenarios in which they can be used …,International Journal on Semantic Web and Information Systems (IJSWIS),2009,83
Learning of OWL class descriptions on very large knowledge bases,Sebastian Hellmann; Jens Lehmann; Sören Auer,Abstract The vision of the Semantic Web is to make use of semantic representations on thelargest possible scale-the Web. Large knowledge bases such as DBpedia; OpenCyc;GovTrack; and others are emerging and are freely available as Linked Data and SPARQLendpoints. Exploring and analysing such knowledge bases is a significant hurdle forSemantic Web research and practice. As one possible direction for tackling this problem; theauthors present an approach for obtaining complex class descriptions from objects inknowledge bases by using Machine Learning techniques. They describe in detail how weleverage existing techniques to achieve scalability on large knowledge bases available asSPARQL endpoints or Linked Data. Their algorithms are made available in the open sourceDL-Learner project and we present several real-life scenarios in which they can be used …,Proceedings of the 2007 International Conference on Posters and Demonstrations-Volume 401,2008,83
Discovering Unknown Connections-the DBpedia Relationship Finder.,Jens Lehmann; Jörg Schüppel; Sören Auer,Abstract: The Relationship Finder is a tool for exploring connections between objects in aSemantic Web knowledge base. It offers a new way to get insights about elements in anontology; in particular for large amounts of instance data. For this reason; we applied theidea to the DBpedia data set; which contains an enormous amount of knowledge extractedfrom Wikipedia. We describe the workings of the Relationship Finder algorithm and presentsome interesting statistical discoveries about DBpedia and Wikipedia.,Conference on Social Semantic Web,2007,83
DBpedia–A Linked Data Hub and Data Source for Web Applications and Enterprises,Georgi Kobilarov; Chris Bizer; Sören Auer; Jens Lehmann,The DBpedia project provides Linked Data identifiers for currently 2.6 million things andserves a large knowledge base of structured information. DBpedia developed into thecentral interlinking hub for the Linking Open Data project; its URIs are used within namedentity recognition services such as OpenCalais and annotation services such as Faviki; andthe BBC started using DBpedia as their central semantic backbone. DBpedia's structureddata serves as background information in the process interlinking datasets and provides arich source of information for application developers. Beside making the DBpediaknowledge base available as linked data and RDF dumps; we offer a Lookup Service whichcan be used by applications to discover URIs for identifying concepts; and a SPARQLendpoint that can be retrieve data from the DBpedia knowledge base to be used in …,16th International Conference on World Wide Web,2009,72
Introduction to linked data and its lifecycle on the web,Sören Auer; Jens Lehmann; Axel-Cyrille Ngonga Ngomo,Abstract With Linked Data; a very pragmatic approach towards achieving the vision of theSemantic Web has recently gained much traction. The term Linked Data refers to a set ofbest practices for publishing and interlinking structured data on the Web. While manystandards; methods and technologies developed within by the Semantic Web communityare applicable for Linked Data; there are also a number of specific characteristics of LinkedData; which have to be considered. In this article we introduce the main concepts of LinkedData. We present an overview of the Linked Data lifecycle and discuss individualapproaches as well as the state-of-the-art with regard to extraction; authoring; linking;enrichment as well as evolution of Linked Data. We conclude the chapter with a discussionof issues; limitations and further research and development challenges of Linked Data.,Proceedings of the 7th international conference on Reasoning web: semantic technologies for the web of data,2011,71
Expressing business process models as OWL-S ontologies,Muhammad Ahtisham Aslam; Sören Auer; Jun Shen; Michael Herrmann,Abstract BPEL4WS is a well-established business process standard that can be used toorchestrate service-based workflows. However; the rapid growth and automation demandsof e-business and grid applications require BPEL4WS to provide enhanced semanticannotations to achieve the goal of business processes automation. Here; OWL-S (OWL forWeb Services) is designed to represent such kind of semantic information. Furthermore;there exists a similarity in the conceptual model of OWL-S and BPEL4WS that can beemployed to overcome the lack of semantics in BPEL4WS by mapping the BPEL4WSprocess model to the OWL-S suite of ontologies. The mapped OWL-S service can be used toincrease flexibility and to automate BPEL based grid scenarios even further. This isachieved by dynamic discovery; composition and invocation of OWL-S services; for …,International Conference on Business Process Management,2006,68
Formal linked data visualization model,Josep Maria Brunetti; Sören Auer; Roberto García; Jakub Klímek; Martin Nečaský,Abstract Recently; the amount of semantic data available in the Web has increaseddramatically. The potential of this vast amount of data is enormous but in most cases it isdifficult for users to explore and use this data; especially for those without experience withSemantic Web technologies. Applying information visualization techniques to the SemanticWeb helps users to easily explore large amounts of data and interact with them. In this articlewe devise a formal Linked Data Visualization Model (LDVM); which allows to dynamicallyconnect data with visualizations. We report about our implementation of the LDVMcomprising a library of generic visualizations that enable both users and data analysts to getan overview on; visualize and explore the Data Web and perform detailed analyzes onLinked Data.,Proceedings of International Conference on Information Integration and Web-based Applications & Services,2013,65
A versioning and evolution framework for RDF knowledge bases,Sören Auer; Heinrich Herre,Abstract We present an approach to support the evolution of online; distributed; reusable;and extendable ontologies based on the RDF data model. The approach works on the basisof atomic changes; basically additions or deletions of statements to or from an RDF graph.Such atomic changes are aggregated to compound changes; resulting in a hierarchy ofchanges; thus facilitating the human reviewing process on various levels of detail. Thesederived compound changes may be annotated with meta-information and classified asontology evolution patterns. The introduced ontology evolution patterns in conjunction withappropriate data migration algorithms enable the automatic migration of instance data indistributed environments.,International Andrei Ershov Memorial Conference on Perspectives of System Informatics,2006,65
RAVEN–Active Learning of Link Specifications,Axel-Cyrille Ngonga Ngomo; Jens Lehmann; Sören Auer; Konrad Höffner,Abstract. With the growth of the Linked Data Web; time-efficient approaches for computinglinks between data sources have become indispensable. Yet; in many cases; determiningthe right specification for a link discovery problem is a tedious task that must still be carriedout manually. We present RAVEN; an approach for the semi-automatic determination of linkspecifications. Our approach is based on the combination of stable solutions of matchingproblems and active learning with the time-efficient link discovery framework LIMES. RAVENaims at requiring a small number of interactions with the user to generate classifiers of highaccuracy. We focus on using RAVEN to compute and configure boolean and weightedclassifiers; which we evaluate in three experiments against link specifications createdmanually. Our evaluation shows that we can compute linking configurations that achieve …,*,2011,64
Publishing statistical data on the web,Percy E Rivera Salas; Michael Martin; Fernando Maia Da Mota; Sören Auer; Karin Breitman; Marco A Casanova,Statistical data is one of the most important sources of information; relevant for largenumbers of stakeholders in the governmental; scientific and business domains alike. In thisarticle; we overview how statistical data can be managed on the Web. With OLAP2 DataCube and CSV2 Data Cube we present two complementary approaches on how to extractand publish statistical data. We also discuss the linking; repair and the visualization ofstatistical data. As a comprehensive use case; we report on the extraction and publishing onthe Web of statistical data describing 10 years of life in Brazil.,Semantic Computing (ICSC); 2012 IEEE Sixth International Conference on,2012,62
Publishing statistical data on the web,Percy E Rivera Salas; Michael Martin; Fernando Maia Da Mota; Sören Auer; Karin Breitman; Marco Casanova,Statistical data is one of the most important sources of information; relevant for largenumbers of stakeholders in the governmental; scientific and business domains alike. In thisarticle; we overview how statistical data can be managed on the Web. With OLAP2 DataCube and CSV2 Data Cube we present two complementary approaches on how to extractand publish statistical data. We also discuss the linking; repair and the visualization ofstatistical data. As a comprehensive use case; we report on the extraction and publishing onthe Web of statistical data describing 10 years of life in Brazil.,IEEE 6th International Conference on Semantic Computing (ICSC),2012,62
Improving the performance of semantic web applications with SPARQL query caching,Michael Martin; Jörg Unbehauen; Sören Auer,Abstract The performance of triple stores is one of the major obstacles for the deployment ofsemantic technologies in many usage scenarios. In particular; Semantic Web applications;which use triple stores as persistence backends; trade performance for the advantage offlexibility with regard to information structuring. In order to get closer to the performance ofrelational database-backed Web applications; we developed an approach for improving theperformance of triple stores by caching query results and even complete application objects.The selective invalidation of cache objects; following updates of the underlying knowledgebases; is based on analysing the graph patterns of cached SPARQL queries in order toobtain information about what kind of updates will change the query result. We evaluatedour approach by extending the BSBM triple store benchmark with an update dimension …,Extended Semantic Web Conference,2010,60
DBpedia live extraction,Sebastian Hellmann; Claus Stadler; Jens Lehmann; Sören Auer,Abstract The DBpedia project extracts information from Wikipedia; interlinks it with otherknowledge bases; and makes this data available as RDF. So far the DBpedia project hassucceeded in creating one of the largest knowledge bases on the Data Web; which is usedin many applications and research prototypes. However; the heavy-weight extractionprocess has been a drawback. It requires manual effort to produce a new release and theextracted information is not up-to-date. We extended DBpedia with a live extractionframework; which is capable of processing tens of thousands of changes per day in order toconsume the constant stream of Wikipedia updates. This allows direct modifications of theknowledge base and closer interaction of users with DBpedia. We also show how theWikipedia community itself is now able to take part in the DBpedia ontology engineering …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2009,58
Access control on RDF triple stores from a semantic wiki perspective,Sebastian Dietzold; Sören Auer,Abstract. RDF triple stores are used to store and query large RDF models. Semantic Webapplications built on top of such triple stores require methods allowing high-performanceaccess control not restricted to per model directives. For the growing number of lightweight;scripted Semantic Web applications it is crucial to rely on access control methods whichmaintain a balance between expressiveness; simplicity and scalability. Starting from aSemantic Wiki application scenario we collect requirements for useful access controlmethods provided by the triple store. We derive a basic model for triple store accessaccording to these requirements and review existing approaches in the field of policymanagement with regard to the requirements. Finally; a lightweight access controlframework based on rule-controlled query filters is described.,ESWC Workshop on Scripting for the Semantic Web,2006,57
Powl–a web based platform for collaborative semantic web development,Sören Auer,Abstract: We outline Powl; an opensource; web-based semantic web development platformfor PHP. It may be used as a foundational framework for semantic web applications. Powlallows parsing; storing; querying; manipulating; versioning; serving and serializing RDF-Sand OWL knowledge bases in a collaborative web enabled environment. Detailed projectinformation; a demonstration and complete source code of Powl is available at http://powl. sf.net.,Proceedings of the Workshop Scripting for the Semantic Web,2005,57
Sina: Semantic interpretation of user queries for question answering on interlinked data,Saeedeh Shekarpour; Edgard Marx; Axel-Cyrille Ngonga Ngomo; Sören Auer,Abstract The architectural choices underlying Linked Data have led to a compendium of datasources which contain both duplicated and fragmented information on a large number ofdomains. One way to enable non-experts users to access this data compendium is toprovide keyword search frameworks that can capitalize on the inherent characteristics ofLinked Data. Developing such systems is challenging for three main reasons. First;resources across different datasets or even within the same dataset can be homonyms.Second; different datasets employ heterogeneous schemas and each one may only containa part of the answer for a certain user query. Finally; constructing a federated formal queryfrom keywords across different datasets requires exploiting links between the differentdatasets on both the schema and instance levels. We present Sina; a scalable keyword …,Web Semantics: Science; Services and Agents on the World Wide Web,2015,54
An architecture of a distributed semantic social network,Sebastian Tramp; Philipp Frischmuth; Timofey Ermilov; Saeedeh Shekarpour; Sören Auer,Abstract Online social networking has become one of the most popular services on the Web.However; current social networks are like walled gardens in which users do not have fullcontrol over their data; are bound to specific usage terms of the social network operator andsuffer from a lock-in effect due to the lack of interoperability and standards compliancebetween social networks. In this paper we propose an architecture for an open; distributedsocial network; which is built solely on Semantic Web standards and emerging bestpractices. Our architecture combines vocabularies and protocols such as WebID; FOAF;Semantic Pingback and PubSubHubbub into a coherent distributed semantic social network;which is capable to provide all crucial functionalities known from centralized social networks.We present our reference implementation; which utilizes the OntoWiki application …,Semantic Web,2014,54
An architecture of a distributed semantic social network,Sebastian Tramp; Philipp Frischmuth; Timofey Ermilov; Saeedeh Shekarpour; Sören Auer,Abstract Online social networking has become one of the most popular services on the Web.However; current social networks are like walled gardens in which users do not have fullcontrol over their data; are bound to specific usage terms of the social network operator andsuffer from a lock-in effect due to the lack of interoperability and standards compliancebetween social networks. In this paper we propose an architecture for an open; distributedsocial network; which is built solely on Semantic Web standards and emerging bestpractices. Our architecture combines vocabularies and protocols such as WebID; FOAF;Semantic Pingback and PubSubHubbub into a coherent distributed semantic social network;which is capable to provide all crucial functionalities known from centralized social networks.We present our reference implementation; which utilizes the OntoWiki application …,Semantic Web,2014,54
Developing semantic web applications with the ontowiki framework,Norman Heino; Sebastian Dietzold; Michael Martin; Sören Auer,Abstract In this paper; we introduce the OntoWiki Application Framework for developingSemantic Web applications with a strong emphasis on collaboration. After presentingOntoWiki as our main show case for the framework; we give both an architectural overviewand a detailed view on the included components. We conclude this paper with apresentation of different use cases where the framework was strongly involved.,*,2009,51
Keyword-driven sparql query generation leveraging background knowledge,Saeedeh Shekarpour; Soren Auer; Axel-Cyrille Ngonga Ngomo; Daniel Gerber; Sebastian Hellmann; Claus Stadler,Abstract The search for information on the Web of Data is becoming increasingly difficult dueto its dramatic growth. Especially novice users need to acquire both knowledge about theunderlying ontology structure and proficiency in formulating formal queries (eg SPARQLqueries) to retrieve information from Linked Data sources. So as to simplify and automate thequerying and retrieval of information from such sources; we present in this paper a novelapproach for constructing SPARQL queries based on user-supplied keywords. Ourapproach utilizes a set of predefined basic graph pattern templates for generating adequateinterpretations of user queries. This is achieved by obtaining ranked lists of candidateresource identifiers for the supplied keywords and then injecting these identifiers intosuitable positions in the graph pattern templates. The main advantages of our approach …,Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Volume 01,2011,50
Internationalization of linked data: The case of the greek dbpedia edition,Dimitris Kontokostas; Charalampos Bratsas; Sören Auer; Sebastian Hellmann; Ioannis Antoniou; George Metakides,Abstract This paper describes the deployment of the Greek DBpedia and the contribution tothe DBpedia information extraction framework with regard to internationalization (I18n) andmultilingual support. I18n filters are proposed as pluggable components in order to addressissues when extracting knowledge from non-English Wikipedia editions. We report on ourstrategy for supporting the International Resource Identifier (IRI) and introduce two newextractors to complement the I18n filters. Additionally; the paper discusses the definition ofTransparent Content Negotiation (TCN) rules for IRIs to address de-referencing and IRIserialization problems. The aim of this research is to establish best practices (complementedby software) to allow the DBpedia community to easily generate; maintain and properlyinterlink language-specific DBpedia editions. Furthermore; these best practices can be …,Web Semantics: Science; Services and Agents on the World Wide Web,2012,49
Internationalization of linked data: The case of the greek dbpedia edition,Dimitris Kontokostas; Charalampos Bratsas; Sören Auer; Sebastian Hellmann; Ioannis Antoniou; George Metakides,Abstract This paper describes the deployment of the Greek DBpedia and the contribution tothe DBpedia information extraction framework with regard to internationalization (I18n) andmultilingual support. I18n filters are proposed as pluggable components in order to addressissues when extracting knowledge from non-English Wikipedia editions. We report on ourstrategy for supporting the International Resource Identifier (IRI) and introduce two newextractors to complement the I18n filters. Additionally; the paper discusses the definition ofTransparent Content Negotiation (TCN) rules for IRIs to address de-referencing and IRIserialization problems. The aim of this research is to establish best practices (complementedby software) to allow the DBpedia community to easily generate; maintain and properlyinterlink language-specific DBpedia editions. Furthermore; these best practices can be …,Web Semantics: Science; Services and Agents on the World Wide Web,2012,49
Linked open data statistics: Collection and exploitation,Ivan Ermilov; Michael Martin; Jens Lehmann; Sören Auer,Abstract This demo presents LODStats; a web application for collection and exploration ofthe Linked Open Data statistics. LODStats consists of two parts: the core collects statisticsabout the LOD cloud and publishes it on the LODStats web portal; a front-end for explorationof dataset statistics. Statistics are published both in human-readable and machine-readableformats; thus allowing consumption of the data through web front-end by the users as well asthrough an API by services and applications. As an example for the latter we showcase howto visualize the statistical data with the CubeViz application.,International Conference on Knowledge Engineering and the Semantic Web,2013,48
Semantifying Requirements Engineering–The SoftWiki Approach,Steffen Lohmann; Philipp Heim; Sören Auer; Sebastian Dietzold; Thomas Riechert,*,4th International Conference on Semantic Technologies (I-SEMANTICS),2008,48
Question answering on interlinked data,Saeedeh Shekarpour; Axel-Cyrille Ngonga Ngomo; Sören Auer,Abstract The Data Web contains a wealth of knowledge on a large number of domains.Question answering over interlinked data sources is challenging due to two inherentcharacteristics. First; different datasets employ heterogeneous schemas and each one mayonly contain a part of the answer for a certain question. Second; constructing a federatedformal query across different datasets requires exploiting links between the differentdatasets on both the schema and instance levels. We present a question answering system;which transforms user supplied queries (ie natural language sentences or keywords) intoconjunctive SPARQL queries over a set of interlinked data sources. The contribution of thispaper is two-fold: Firstly; we introduce a novel approach for determining the most suitableresources for a user-supplied query from different datasets (disambiguation). We employ …,Proceedings of the 22nd international conference on World Wide Web,2013,47
Towards semantic based requirements engineering,Thomas Riechert; Kim Lauenroth; Jens Lehmann; Sören Auer,Abstract: Requirements Engineering is recognized as a crucial part of project and softwaredevelopment processes. This is due to the fact that the different stakeholders involved in adevelopment project have to establish common terminologies as well as goals; scenariosand requirements expressed using these terminologies. Within the Semantic Web initiativevarious standards emerged for the creation and use of terminologies; expressed in theshape of semantic networks; taxonomies and ontologies. We develop an approach forsemantic based Requirements Engineering. We present an ontology for capturingrequirements relevant information. Furthermore; we report about a tool for semantic basedRequirements Engineering and its application in a real-world development project scenariofrom the E-Government domain.,7th International Conference on Knowledge Management (I-Know),2007,47
Triplecheckmate: A tool for crowdsourcing the quality assessment of linked data,Dimitris Kontokostas; Amrapali Zaveri; Sören Auer; Jens Lehmann,Abstract Linked Open Data (LOD) comprises of an unprecedented volume of structureddatasets on the Web. However; these datasets are of varying quality ranging fromextensively curated datasets to crowdsourced and even extracted data of relatively lowquality. We present a methodology for assessing the quality of linked data resources; whichcomprises of a manual and a semi-automatic process. In this paper we focus on the manualprocess where the first phase includes the detection of common quality problems and theirrepresentation in a quality problem taxonomy. The second phase comprises of theevaluation of a large number of individual resources; according to the quality problemtaxonomy via crowdsourcing. This process is implemented by the tool TripleCheckMatewherein a user assesses an individual resource and evaluates each fact for correctness …,International Conference on Knowledge Engineering and the Semantic Web,2013,46
The rdfa content editor-from wysiwyg to wysiwym,Ali Khalili; Sören Auer; Daniel Hladky,Recently practical approaches for managing and supporting the life-cycle of semanticcontent on the Web of Data made quite some progress. However; the currently leastdeveloped aspect of the semantic content life-cycle is the user-friendly manual and semi-automatic creation of rich semantic content. In this paper we present the RDFaCE approachfor combining WYSIWYG text authoring with the creation of rich semantic annotations. Ourapproach is based on providing four different views to the content authors: a classicalWYSIWYG view; a WYSIWYM (What You See Is What You Mean) view making the semanticannotations visible; a fact view and the respective HTML/RDFa source code view. The viewsare synchronized such that changes made in one of the views automatically update theothers. They provide different means of semantic content authoring for the different …,Computer Software and Applications Conference (COMPSAC); 2012 IEEE 36th Annual,2012,43
User-driven semantic mapping of tabular data,Ivan Ermilov; Sören Auer; Claus Stadler,Abstract Governments and public administrations started recently to publish large amountsof structured data on the Web; mostly in the form of tabular data such as CSV files or Excelsheets. Various tools and projects have been launched aiming at facilitating the lifting oftabular data to reach semantically structured and linked data. However; none of these toolssupported a truly incremental; pay-as-you-go data publication and mapping strategy; whichenables effort sharing between data owners; community experts and consumers. In thisarticle; we present an approach for enabling the user-driven semantic mapping of largeamounts tabular data. We devise a simple mapping language for tabular data; which is easyto understand even for casual users; but expressive enough to cover the vast majority ofpotential tabular mappings use cases. We outline a formal approach for mapping tabular …,Proceedings of the 9th International Conference on Semantic Systems,2013,42
LESS-template-based syndication and presentation of linked data,Sören Auer; Raphael Doehring; Sebastian Dietzold,Abstract Recently; the publishing of structured; semantic information as linked data hasgained quite some momentum. For ordinary users on the Internet; however; this informationis not yet very visible and (re-) usable. With LESS we present an end-to-end approach forthe syndication and use of linked data based on the definition of templates for linked dataresources and SPARQL query results. Such syndication templates are edited; published andshared by using a collaborative Web platform. Templates for common types of entities canthen be combined with specific; linked data resources or SPARQL query results andintegrated into a wide range of applications; such as personal homepages; blogs/wikis;mobile widgets etc. In order to improve reliability and performance of linked data; LESScaches versions either for a certain time span or for the case of inaccessibility of the …,Extended Semantic Web Conference,2010,40
Weaving a social data web with semantic pingback,Sebastian Tramp; Philipp Frischmuth; Timofey Ermilov; Sören Auer,Abstract In this paper we tackle some pressing obstacles of the emerging Linked Data Web;namely the quality; timeliness and coherence of data; which are prerequisites in order toprovide direct end user benefits. We present an approach for complementing the LinkedData Web with a social dimension by extending the well-known Pingback mechanism; whichis a technological cornerstone of the blogosphere; towards a Semantic Pingback. It is basedon the advertising of an RPC service for propagating typed RDF links between Data Webresources. Semantic Pingback is downwards compatible with conventional Pingbackimplementations; thus allowing to connect and interlink resources on the Social Web withresources on the Data Web. We demonstrate its usefulness by showcasing use cases of theSemantic Pingback implementations in the semantic wiki OntoWiki and the Linked Data …,International Conference on Knowledge Engineering and Knowledge Management,2010,39
RapidOWL—An agile knowledge engineering methodology,Sören Auer; Heinrich Herre,Abstract The analysis of the application of the existing knowledge engineeringmethodologies and tools shows that they are up to now virtually not used in practice (see[13; page 16]). This stands in contrast to the often proclaimed necessity for knowledgeengineering. What can be the reason for this discrepancy? Most of the existing knowledgeengineering methodologies adopt techniques and apply process models from softwareengineering. However; in many scenarios required knowledge engineering tasks revealspecific characteristics; which an knowledge engineering methodology should be aware of.In the following; we describe briefly some specific characteristics of Knowledge Engineeringimportant for Rapid-OWL.,International Andrei Ershov Memorial Conference on Perspectives of System Informatics,2006,39
Deqa: deep web extraction for question answering,Jens Lehmann; Tim Furche; Giovanni Grasso; Axel-Cyrille Ngonga Ngomo; Christian Schallhart; Andrew Sellers; Christina Unger; Lorenz Bühmann; Daniel Gerber; Konrad Höffner; David Liu; Sören Auer,Abstract Despite decades of effort; intelligent object search remains elusive. Neither searchengine nor semantic web technologies alone have managed to provide usable systems forsimple questions such as “find me a flat with a garden and more than two bedrooms near asupermarket.” We introduce deqa; a conceptual framework that achieves this elusive goalthrough combining state-of-the-art semantic technologies with effective data extraction. Tothat end; we apply deqa; to the UK real estate domain and show that it can answer asignificant percentage of such questions correctly. deqa achieves this by mapping naturallanguage questions to Sparql patterns. These patterns are then evaluated on an RDFdatabase of current real estate offers. The offers are obtained using OXPath; a state-of-the-art data extraction system; on the major agencies in the Oxford area and linked through …,International Semantic Web Conference,2012,37
RDFauthor: employing RDFa for collaborative knowledge engineering,Sebastian Tramp; Norman Heino; Sören Auer; Philipp Frischmuth,Abstract In this paper we present RDFauthor; an approach for authoring information thatadheres to the RDF data model. RDFauthor completely hides syntax as well as RDF andontology data model difficulties from end users and allows to edit information on arbitraryRDFa-annotated web pages. RDFauthor extends RDFa with representations for provenanceand update endpoint information. RDFauthor is based on extracting RDF triples from RDFaannotations and transforming the RDFa-annotated HTML view into an editable form by usinga set of authoring widgets. As a result; every RDFa-annotated web page can be made easilywriteable; even if information originates from different sources.,Knowledge Engineering and Management by the Masses,2010,37
Semantic Web In-Use Track-DBpedia: A Nucleus for a Web of Open Data,Soren Auer; Christian Bizer; Georgi Kobilarov; Jens Lehmann; Richard Cyganiak; Zachary Ives,*,Lecture Notes in Computer Science,2007,34
Towards an open question answering architecture,Edgard Marx; Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo; Konrad Höffner; Jens Lehmann; Sören Auer,Abstract Billions of facts pertaining to a multitude of domains are now available on the Webas RDF data. However; accessing this data is still a difficult endeavour for non-expert users.In order to meliorate the access to this data; approaches imposing minimal hurdles to theirusers are required. Although many question answering systems over Linked Data havebeing proposed; retrieving the desired data is still significantly challenging. In addition;developing and evaluating question answering systems remains a very complex task. Toovercome these obstacles; we present a modular and extensible open-source questionanswering framework. We demonstrate how the framework can be used by integrating twostate-of-the-art question answering systems. As a result our evaluation shows that overallbetter results can be achieved by the use of combination rather than individual stand …,Proceedings of the 10th International Conference on Semantic Systems,2014,32
Evopat–pattern-based evolution and refactoring of RDF knowledge bases,Christoph Rieß; Norman Heino; Sebastian Tramp; Sören Auer,Abstract Facilitating the seamless evolution of RDF knowledge bases on the Semantic Webpresents still a major challenge. In this work we devise EvoPat–a pattern-based approachfor the evolution and refactoring of knowledge bases. The approach is based on thedefinition of basic evolution patterns; which are represented declaratively and can capturesimple evolution and refactoring operations on both data and schema levels. For moreadvanced and domain-specific evolution and refactorings; several simple evolution patternscan be combined into a compound one. We performed a comprehensive survey of possibleevolution patterns with a combinatorial analysis of all possible before/after combinations;resulting in an extensive catalog of usable evolution patterns. Our approach wasimplemented as an extension for the OntoWiki semantic collaboration platform and …,International Semantic Web Conference,2010,31
Linked Open Data--Creating Knowledge Out of Interlinked Data,Soren Auer; Volha Bryl; Sebastian Tramp,*,*,2014,30
Ontowiki–an authoring; publication and visualization interface for the data web,Philipp Frischmuth; Michael Martin; Sebastian Tramp; Thomas Riechert; Sören Auer,Abstract OntoWiki is a front-end application for the Semantic Data Web; which was originallydeveloped to support distributed knowledge engineering scenarios. Due to its extensibility italso serves as a development framework for knowledge intensive applications. On thesurface; OntoWiki is a generic user interface for arbitrary RDF knowledge graphs. It supportsthe navigation through RDF knowledge bases using SPARQL-generated lists; tables andtrees (eg class trees and taxonomies). All resources are automatically represented ashyperlinks and backlinks are created whenever feasible; thus enabling users to easilytraverse entire knowledge graphs. Since all collections of resources displayed in OntoWikiare generated by SPARQL queries; they can be further refined by applying additional filters.In order to explore large datasets; a comprehensive statistical data management and …,Semantic Web,2015,29
Keyword query expansion on linked data using linguistic and semantic features,Saeedeh Shekarpour; Konrad Hoffner; Jens Lehmann; Soren Auer,Effective search in structured information based on textual user input is of high importance inthousands of applications. Query expansion methods augment the original query of a userwith alternative query elements with similar meaning to increase the chance of retrievingappropriate resources. In this work; we introduce a number of new query expansion featuresbased on semantic and linguistic inferencing over Linked Open Data. We evaluate theeffectiveness of each feature individually as well as their combinations employing severalmachine learning approaches. The evaluation is carried out on a training dataset extractedfrom the QALD question answering benchmark. Furthermore; we propose an optimizedlinear combination of linguistic and lightweight semantic features in order to predict theusefulness of each expansion candidate. Our experimental study shows a considerable …,Semantic Computing (ICSC); 2013 IEEE Seventh International Conference on,2013,29
User interfaces for semantic authoring of textual content: A systematic literature review,Ali Khalili; Sören Auer,Abstract Practical approaches for managing and supporting the life-cycle of semanticcontent on the Web of Data have recently made quite some progress. In particular in thearea of the user-friendly manual and semi-automatic creation of rich semantic content wehave observed recently a large number of approaches and systems being described in theliterature. With this survey we aim to provide an overview on the rapidly emerging field ofSemantic Content Authoring (SCA). We conducted a systematic literature review comprisinga thorough analysis of 31 primary studies out of 175 initially retrieved papers addressing thesemantic authoring of textual content. We obtained a comprehensive set of quality attributesfor SCA systems together with corresponding user interface features suggested for theirrealization. The quality attributes include aspects such as usability; automation …,*,2013,28
Linked-data aware uri schemes for referencing text fragments,Sebastian Hellmann; Jens Lehmann; Sören Auer,Abstract The NLP Interchange Format (NIF) is an RDF/OWL-based format that aims toachieve interoperability between Natural Language Processing (NLP) tools; languageresources and annotations. The motivation behind NIF is to allow NLP tools to exchangeannotations about text documents in RDF. Hence; the main prerequisite is that parts of thedocuments (ie strings) are referenceable by URIs; so that they can be used as subjects inRDF statements. In this paper; we present two NIF URI schemes for different use cases andevaluate them experimentally by benchmarking the stability of both NIF URI schemes in aWeb annotation scenario. Additionally; the schemes are compared with other availableschemes used to address text with URIs. The String Ontology; which is the basis for NIF;fixes the referent (ie a string in a given text) of the URIs unambiguously for machines and …,International Conference on Knowledge Engineering and Knowledge Management,2012,27
Knowledge extraction from structured sources,Jörg Unbehauen; Sebastian Hellmann; Sören Auer; Claus Stadler,Abstract This chapter surveys knowledge extraction approaches from structured sourcessuch as relational databases; XML and CSV. A general definition of knowledge extraction isdevised that covers structured as well as unstructured sources. We summarize currentprogress on conversion of structured data to RDF and OWL. As an example; we provide aformalization and description of SparqlMap; which implements the relational database toRDF mapping language R2RML currently being standardized by the W3C.,*,2012,27
From overview to facets and pivoting for interactive exploration of semantic web data,Josep Maria Brunetti; Roberto García; Sören Auer,Abstract The proliferation of Linked Open Data on the Web has increased the amount of dataavailable for analysis and reuse. However; casual users find it difficult to explore and useSemantic Web Data due to the prevalence of specialised browsers that require complexqueries to be formed and intimate knowledge on the structure of datasets. The authorsaddress this problem in the Rhizomer tool by applying the data analysis mantra of overview;zoom and filter. These interaction patterns are implemented using information architecturecomponents users are already familiar with but that are automatically generated from dataand ontologies. This approach makes it possible to obtain an overview of the dataset beingexplored using techniques; such as navigation menus; treemaps or sitemaps; which areusually not available in text-based semantic web browsers. From there; users can …,International Journal on Semantic Web and Information Systems (IJSWIS),2013,26
Exploring the web of spatial data with facete,Claus Stadler; Michael Martin; Sören Auer,Abstract The majority of data (including data published on the Web as Linked Open Data)has a spatial dimension. However; the efficient; user friendly exploration of spatial dataremains a major challenge. We present Facete; a web-based exploration and visualizationapplication enabling the spatial-faceted browsing of data with a spatial dimension. Faceteimplements a novel spatial data exploration paradigm based on the following three keycomponents: First; a domain independent faceted filtering module; which operates directlyon SPARQL and supports nested facets. Second; an algorithm that efficiently detects spatialinformation related to those resources that satisfy the facet selection. The detected relationsare used for automatically presenting data on a map. And third; a workflow for making themap display interact with data sources that contain large amounts of geometric …,Proceedings of the 23rd International Conference on World Wide Web,2014,25
Linked SDMX data,Sarven Capadisli; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract As statistical data is inherently highly structured and comes with rich metadata (inform of code lists; data cubes etc.); it would be a missed opportunity to not tap into it from theLinked Data angle. At the time of this writing; there exists no simple way to transformstatistical data into Linked Data since the raw data comes in different shapes and forms.Given that SDMX (Statistical Data and Metadata eXchange) is arguably the most widelyused standard for statistical data exchange; a great amount of statistical data about oursocieties is yet to be discoverable and identifiable in a uniform way. In this article; wepresent the design and implementation of SDMX-ML to RDF/XML XSL transformations; aswell as the publication of OECD; BFS; FAO; ECB; and IMF datasets with that tooling.,Semantic Web,2015,24
Towards a semantic administrative shell for industry 4.0 components,Irlán Grangel-González; Lavdim Halilaj; Gökhan Coskun; Sören Auer; Diego Collarana; Michael Hoffmeister,In the engineering and manufacturing domain; there is currently an atmosphere of departureto a new era of digitized production. In different regions; initiatives in these directions areknown under different names; such as industrie du futur in France; industrial internet in theUS or Industrie 4.0 in Germany. While the vision of digitizing production and manufacturinggained much traction lately; it is still relatively unclear how this vision can actually beimplemented with concrete standards and technologies. Within the German Industry 4.0initiative; the concept of an Administrative Shell was devised to respond to theserequirements. The Administrative Shell is planned to provide a digital representation of allinformation being available about and from an object which can be a hardware system or asoftware platform. In this paper; we present an approach to develop such a digital re …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,23
Luzzu--A Framework for Linked Data Quality Assessment,Jeremy Debattista; Sören Auer; Christoph Lange,The increasing variety of Linked Data on the Web makes it challenging to determine thequality of this data; and subsequently to make this information explicit to data consumers.Despite the availability of a number of tools and frameworks to assess Linked Data Quality;the output of such tools is not suitable for machine consumption; and thus consumers canhardly compare and rank datasets in the order of fitness for use. This paper describes Luzzu;a framework for Linked Data Quality Assessment. Luzzu is based on four majorcomponents:(1) an extensible interface for defining new quality metrics;(2) an interoperable;ontology-driven back-end for representing quality metadata and quality problems that canbe reused within different semantic frameworks;(3) a scalable stream processor for datadumps and SPARQL endpoints; and (4) a customisable ranking algorithm taking into …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,23
Weaving a distributed; semantic social network for mobile users,Sebastian Tramp; Philipp Frischmuth; Natanael Arndt; Timofey Ermilov; Sören Auer,Abstract Smartphones; which contain a large number of sensors and integrated devices; arebecoming increasingly powerful and fully featured computing platforms in our pockets. Formany people they already replace the computer as their window to the Internet; to the Webas well as to social networks. Hence; the management and presentation of information aboutcontacts; social relationships and associated information is one of the main requirementsand features of today's smartphones. The problem is currently solved only for centralizedproprietary platforms (such as Google mail; contacts & calendar) as well as data-silo-likesocial networks (eg Facebook). Within the Semantic Web initiative standards and best-practices for social; Semantic Web applications such as FOAF emerged. However; there isno comprehensive strategy; how these technologies can be used efficiently in a mobile …,Extended Semantic Web Conference,2011,23
Knowledge engineering for historians on the example of the catalogus professorum lipsiensis,Thomas Riechert; Ulf Morgenstern; Sören Auer; Sebastian Tramp; Michael Martin,Abstract Although the Internet; as an ubiquitous medium for communication; publication andresearch; already significantly influenced the way historians work; the capabilities of theWeb as a direct medium for collaboration in historic research are not much explored. Wereport about the application of an adaptive; semantics-based knowledge engineeringapproach for the development of a prosopographical knowledge base on the Web-theCatalogus Professorum Lipsiensis. In order to enable historians to collect; structure andpublish prosopographical knowledge an ontology was developed and knowledgeengineering facilities based on the semantic data wiki OntoWiki were implemented. Theresulting knowledge base contains information about more than 14.000 entities and is tightlyinterlinked with the emerging Web of Data. For access and exploration by other historians …,International Semantic Web Conference,2010,23
Accessing relational data on the web with sparqlmap,Jörg Unbehauen; Claus Stadler; Sören Auer,Abstract The vast majority of the structured data of our age is stored in relational databases.In order to link and integrate this data on the Web; it is of paramount importance to makerelational data available according to the RDF data model and associated serializations. Inthis article we present SparqlMap; a SPARQL-to-SQL rewriter based on the specifications ofthe W3C R2RML working group. The rationale is to enable SPARQL querying on existingrelational databases by rewriting a SPARQL query to exactly one corresponding SQL querybased on mapping definitions expressed in R2RML. The SparqlMap process of rewriting aquery on a mapping comprises the three steps (1) mapping candidate selection;(2) querytranslation; and (3) query execution. We showcase our SparqlMap implementation andbenchmark data that demonstrates that SparqlMap outperforms the current state-of-the-art.,Joint International Semantic Technology Conference,2012,22
daQ; an Ontology for Dataset Quality Information.,Jeremy Debattista; Christoph Lange; Sören Auer,ABSTRACT Data quality is commonly defined as fitness for use. The problem of identifyingthe quality of data is faced by many data consumers. To make the task of finding goodquality datasets more efficient; we introduce the Dataset Quality Ontology (daQ). The daQ isa lightweight; extensible vocabulary for attaching the results of quality benchmarking of alinked open dataset to that dataset. We discuss the design considerations; give examples forextending daQ by custom quality metrics; and present use cases such as browsing datasetsby quality. We also discuss how tools can use the daQ to enable consumers find the rightdataset for use.,LDOW,2014,21
Databugger: a test-driven framework for debugging the web of data,Dimitris Kontokostas; Patrick Westphal; Sören Auer; Sebastian Hellmann; Jens Lehmann; Roland Cornelissen,Abstract Linked Open Data (LOD) comprises of an unprecedented volume of structured dataon the Web. However; these datasets are of varying quality ranging from extensively curateddatasets to crowd-sourced or extracted data of often relatively low quality. We presentDatabugger; a framework for test-driven quality assessment of Linked Data; which isinspired by test-driven software development. Databugger ensures a basic level of quality byaccompanying vocabularies; ontologies and knowledge bases with a number of test cases.The formalization behind the tool employs SPARQL query templates; which are instantiatedinto concrete quality test queries. The test queries can be instantiated automatically basedon a vocabulary or manually based on the data semantics. One of the main advantages ofour approach is that domain specific semantics can be encoded in the data quality test …,Proceedings of the 23rd International Conference on World Wide Web,2014,21
AGDISTIS-Agnostic Disambiguation of Named Entities Using Linked Open Data.,Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo; Michael Röder; Daniel Gerber; Sandro Athaide Coelho; Sören Auer; Andreas Both,Abstract. Over the last decades; several billion Web pages have been made available on theWeb. The ongoing transition from the current Web of unstructured data to the Data Web yetrequires scalable and accurate approaches for the extraction of structured data in RDF(Resource Description Framework) from these websites. One of the key steps towardsextracting RDF from text is the disambiguation of named entities. We address this issue bypresenting AGDISTIS; a novel knowledge-base-agnostic approach for named entitydisambiguation. Our approach combines the Hypertext-Induced Topic Search (HITS)algorithm with label expansion strategies and string similarity measures. Based on thiscombination; AGDISTIS can efficiently detect the correct URIs for a given set of namedentities within an input text.,ECAI,2014,21
Wysiwym authoring of structured content based on schema. org,Ali Khalili; Sören Auer,Abstract Structured data is picking up on the Web; particularly in the search world. Schema.org; jointly initiated by Google; Microsoft; and Yahoo! provides a hierarchical set ofvocabularies to embed metadata in HTML pages for an enhanced search and browsingexperience. RDFa-Lite; Microdata and JSON-LD as lower semantic techniques have gainedmore attention by Web users to markup Web pages and even emails based on Schema. org.However; from the user interface point of view; we still lack user-friendly tools that facilitatethe process of structured content authoring. The majority of information still is contained inand exchanged using unstructured documents; such as Web pages; text documents; imagesand videos. This can also not be expected to change; since text; images and videos are thenatural way how humans interact with information. In this paper we present RDFaCE as …,International Conference on Web Information Systems Engineering,2013,21
Linked statistical data analysis,Sarven Capadisli; Sören Auer; Reinhard Riedl,Abstract. Linked Data principles are increasingly employed to publish high-fidelity;heterogeneous statistical datasets in a distributed way. Currently; there exists no simple wayfor researchers; journalists and interested people to compare statistical data retrieved fromdifferent data stores on the Web. Given that the RDF Data Cube vocabulary is used todescribe statistical data; its use makes it possible to discover and identify statistical dataartifacts in a uniform way. In this article; the design and implementation of an application andservice is presented; which utilizes federated SPARQL queries to gather statistical data fromdistributed data stores. The R language for statistical computing is employed to performstatistical analyses and visualizations. The Shiny application and server bridges the front-end Web user interface with R on the server-side in order to compare statistical …,Semantic Web Challenge,2013,21
Ontowiki mobile–knowledge management in your pocket,Timofey Ermilov; Norman Heino; Sebastian Tramp; Sören Auer,Abstract As comparatively powerful mobile computing devices are becoming more common;mobile web applications have started gaining in popularity. In this paper we present anapproach for a mobile semantic collaboration platform based on the OntoWiki framework. Itallows users to collect instance data; refine the structure of knowledge bases and browsedata using hierarchical or faceted navigation on-the-go even without a present dataconnection. A crucial part of OntoWiki Mobile is the advanced replication and conflictresolution for RDF content. The approach for conflict resolution is based on a combination ofdistributed revision control strategies and the EvoPat method for data evolution and ontologyrefactoring. OntoWiki mobile is available as an HTML5 Web application and can be used inscenarios where semantically rich information has to be collected in field-conditions such …,Extended Semantic Web Conference,2011,20
Linked data in enterprise information integration,Philipp Frischmuth; Jakub Klímek; Sören Auer; Sebastian Tramp; Jörg Unbehauen; Kai Holzweissig; Carl-Martin Marquardt,Abstract. Data integration in large enterprises is a crucial but at the same time costly; longlasting and challenging problem. While business-critical information is often alreadygathered in integrated information systems; such as ERP; CRM and SCM systems; theintegration of these systems itself as well as the integration with the abundance of otherinformation sources is still a major challenge. In the last decade; the prevalent dataintegration approaches were primarily based on XML; Web Services and Service OrientedArchitectures (SOA). However; we become increasingly aware that these technologies arenot sufficient to ultimately solve data integration challenges in large enterprises. In thisarticle; we argue that classic SOA architectures may be well-suited for transactionprocessing; however more efficient technologies are available today that can be …,Semantic Web,2012,19
LODStats: The data web census dataset,Ivan Ermilov; Jens Lehmann; Michael Martin; Sören Auer,Abstract Over the past years; the size of the Data Web has increased significantly; whichmakes obtaining general insights into its growth and structure both more challenging andmore desirable. The lack of such insights hinders important data management tasks such asquality; privacy and coverage analysis. In this paper; we present the LODStats dataset;which provides a comprehensive picture of the current state of a significant part of the DataWeb. LODStats is based on RDF datasets from data. gov; publicdata. eu and datahub. iodata catalogs and at the time of writing lists over 9000 RDF datasets. For each RDF dataset;LODStats collects comprehensive statistics and makes these available in adhering to theLDSO vocabulary. This analysis has been regularly published and enhanced over the pastfive years at the public platform lodstats. aksw. org. We give a comprehensive overview …,International Semantic Web Conference,2016,18
Wysiwym–integrated visualization; exploration and authoring of semantically enriched un-structured content,Ali Khalili; Sören Auer,Abstract The Semantic Web and Linked Data gained traction in the last years. However; themajority of information still is contained in unstructured documents. This can also not beexpected to change; since text; images and videos are the natural way how humans interactwith information. Semantic structuring on the other hand enables the (semi-) automaticintegration; repurposing; rearrangement of information. NLP technologies and formalisms forthe integrated representation of unstructured and semantic content (such as RDFa andMicrodata) aim at bridging this semantic gap. However; in order for humans to truly benefitfrom this integration; we need ways to author; visualize and explore unstructured andsemantically enriched content in an integrated manner. In this paper; we present theWYSIWYM (What You See is What You Mean) concept; which addresses this issue and …,Semantic Web,2015,18
When to reach for the cloud: Using parallel hardware for link discovery,Axel-Cyrille Ngonga Ngomo; Lars Kolb; Norman Heino; Michael Hartung; Sören Auer; Erhard Rahm,Abstract With the ever-growing amount of RDF data available across the Web; the discoveryof links between datasets and deduplication of resources within knowledge bases havebecome tasks of crucial importance. Over the last years; several link discovery approacheshave been developed to tackle the runtime and complexity problems that are intrinsic to linkdiscovery. Yet; so far; little attention has been paid to the management of hardwareresources for the execution of link discovery tasks. This paper addresses this research gapby investigating the efficient use of hardware resources for link discovery. We implement theHR^3 approach for three different parallel processing paradigms including the use of GPUsand MapReduce platforms. We also perform a thorough performance comparison for theseimplementations. Our results show that certain tasks that appear to require cloud …,Extended Semantic Web Conference,2013,18
Usage-Centric Benchmarking of RDF Triple Stores.,Mohamed Morsey; Jens Lehmann; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract A central component in many applications is the underlying data managementlayer. In Data-Web applications; the central component of this layer is the triple store. It isthus evident that finding the most adequate store for the application to develop is of crucialimportance for individual projects as well as for data integration on the Data Web in general.In this paper; we propose a generic benchmark creation procedure for SPARQL; which weapply to the DBpedia knowledge base. In contrast to previous approaches; our benchmark isbased on queries that were actually issued by humans and applications against existingRDF data not resembling a relational schema. In addition; our approach does not only takethe query string but also the features of the queries into consideration during the benchmarkgeneration process. Our generic procedure for benchmark creation is based on query-log …,AAAI,2012,18
Managing web content using linked data principles-combining semantic structure with dynamic content syndication,Norman Heino; Sebastian Tramp; Soren Auer,Despite the success of the emerging Linked Data Web; offering content in a machine-processable way and--at the same time--as a traditional Web site is still not a trivial task. Inthis paper; we present the OntoWiki-CMS-an extension to the collaborative knowledgeengineering toolkit OntoWiki for managing semantically enriched Web content. OntoWiki-CMS is based on OntoWiki for the collaborative authoring of semantically enriched Webcontent; vocabularies and taxonomies for the semantic structuring of the Web content andthe OntoWiki Site Extension; a template and dynamic syndication system for representingthe semantically enriched content as a Web site and the dynamic integration ofsupplementary content. OntoWiki-CMS facilitates and integrates existing content-specificcontent management strategies (such as blogs; bibliographic repositories or social …,Computer Software and Applications Conference (COMPSAC); 2011 IEEE 35th Annual,2011,18
I18n of semantic web applications,Sören Auer; Matthias Weidl; Jens Lehmann; Amrapali J Zaveri; Key-Sun Choi,Abstract Recently; the use of semantic technologies has gained quite some traction. Withincreased use of these technologies; their maturation not only in terms of performance;robustness but also with regard to support of non-latin-based languages and regionaldifferences is of paramount importance. In this paper; we provide a comprehensive review ofthe current state of the internationalization (I18n) of Semantic Web technologies. Sinceresource identifiers play a crucial role for the Semantic Web; the internatinalization ofresource identifiers is of high importance. It turns out that the prevalent resourceidentification mechanism on the Semantic Web; ie URIs; are not sufficient for an efficientinternationalization of knowledge bases. Fortunately; with IRIs a standard for internationalresource identifiers is available; but its support needs much more penetration and …,International Semantic Web Conference,2010,17
context–lightweight text analytics using linked data,Ali Khalili; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract The Web democratized publishing–everybody can easily publish information on aWebsite; Blog; in social networks or microblogging systems. The more the amount ofpublished information grows; the more important are technologies for accessing; analysing;summarising and visualising information. While substantial progress has been made in thelast years in each of these areas individually; we argue; that only the intelligent combinationof approaches will make this progress truly useful and leverage further synergies betweentechniques. In this paper we develop a text analytics architecture of participation; whichallows ordinary people to use sophisticated NLP techniques for analysing and visualizingtheir content; be it a Blog; Twitter feed; Website or article collection. The architecturecomprises interfaces for information access; natural language processing and …,European Semantic Web Conference,2014,16
The Open Government Data Stakeholder Survey.,Michael Martin; Martin Kaltenböck; Helmut Nagy; Sören Auer,Abstract. This paper describes the results of the LOD2 Open Government Data StakeholderSurvey 2010 (OGD Stakeholder Survey). The objective of the survey was to involve as manyrelevant stakeholders as possible in the 27 European Union countries in an onlinequestionnaire and ask them about their needs and requirements in the area of open data aswell as for the publicdata. eu portal. The main areas of the survey have been questionsabout Open Government Data itself; questions about data; about the usage of data;questions about the requirements for a centralised data catalogue as well as questionsabout the participants themselves. The goal of the OGD Stakeholder Survey has been toreach a broad audience of the main stakeholders of open data: citizens; publicadministration; politics and industry. In the course of the survey that was open for 5 weeks …,OKCon,2011,16
Neural network-based question answering over knowledge graphs on word and character level,Denis Lukovnikov; Asja Fischer; Jens Lehmann; Sören Auer,Abstract Question Answering (QA) systems over Knowledge Graphs (KG) automaticallyanswer natural language questions using facts contained in a knowledge graph. Simplequestions; which can be answered by the extraction of a single fact; constitute a large part ofquestions asked on the web but still pose challenges to QA systems; especially when askedagainst a large knowledge resource. Existing QA systems usually rely on variouscomponents each specialised in solving different sub-tasks of the problem (such assegmentation; entity recognition; disambiguation; and relation classification etc.). In thiswork; we follow a quite different approach: We train a neural network for answering simplequestions in an end-to-end manner; leaving all decisions to the model. It learns to ranksubject-predicate pairs to enable the retrieval of relevant facts given a question. The …,Proceedings of the 26th international conference on World Wide Web,2017,15
LinkDaViz–automatic binding of linked data to visualizations,Klaudia Thellmann; Michael Galkin; Fabrizio Orlandi; Sören Auer,Abstract As the Web of Data is growing steadily; the demand for user-friendly means forexploring; analyzing and visualizing Linked Data is also increasing. The key challenge forvisualizing Linked Data consists in providing a clear overview of the data and supportingnon-technical users in finding suitable visualizations while hiding technical details of LinkedData and visualization configuration. In order to accomplish this; we propose a largelyautomatic workflow which guides users through the process of creating visualizations byautomatically categorizing and binding data to visualization parameters. The approach isbased on a heuristic analysis of the structure of the input data and a comprehensivevisualization model facilitating the automatic binding between data and visualizationparameters. The resulting assignments are ranked and presented to the user. With …,International Semantic Web Conference,2015,15
Multilingual linked data patterns,Jose Emilio Labra Gayo; Dimitris Kontokostas; Sören Auer,Abstract The increasing publication of linked data makes the vision of the semantic web aprobable reality. Although it may seem that the web of data is inherently multilingual; datausually contain labels; comments; descriptions; etc. that depend on the natural languageused. When linked data appears in a multilingual setting; it is a challenge to publish andconsume it. This paper presents a survey of patterns to publish Multilingual Linked Data andidentifies some issues that should be taken into account. As a use case; the paper describesthe patterns employed in the DBpedia Internationalization project.,Semantic Web,2015,15
Representing dataset quality metadata using multi-dimensional views,Jeremy Debattista; Christoph Lange; Sören Auer,Abstract Data quality is commonly defined as fitness for use. The problem of identifyingquality of data is faced by many data consumers. Data publishers often do not have themeans to identify quality problems in their data. To make the task for both stakeholderseasier; we have developed the Dataset Quality Ontology (daQ). daQ is a core vocabulary forrepresenting the results of quality benchmarking of a linked dataset. It represents qualitymetadata as multi-dimensional and statistical observations using the Data Cube vocabulary.Quality metadata are organised as a self-contained graph; which can; eg; be embedded intolinked open datasets. We discuss the design considerations; give examples for extendingdaQ by custom quality metrics; and present use cases such as analysing data versions;browsing datasets by quality; and link identification. We finally discuss how data cube …,Proceedings of the 10th International Conference on Semantic Systems,2014,15
Diachronic linked data: towards long-term preservation of structured interrelated information,Sören Auer; Theodore Dalamagas; Helen Parkinson; François Bancilhon; Giorgos Flouris; Dimitris Sacharidis; Peter Buneman; Dimitris Kotzinos; Yannis Stavrakas; Vassilis Christophides; George Papastefanatos; Kostas Thiveos,Abstract The Linked Data Paradigm is a promising technology for publishing; sharing; andconnecting data on the Web; which provides new perspectives for data integration andinteroperability. However; the proliferation of distributed; interconnected linked data sourceson the Web poses significant new challenges for consistently managing the vast number ofpotentially large datasets and their interdependencies. In this article we focus on the keyproblem of preserving evolving structured interlinked data. We argue that a number ofissues; which hinder applications and users; are related to the temporal aspect that isintrinsic in Linked Data. We present three use cases to motivate our approach; we discussproblems that occur; and propose a direction for a solution.,Proceedings of the First International Workshop on Open Data,2012,15
Linked Open Data--Creating Knowledge Out of Interlinked Data: Results of the LOD2 Project,Sören Auer; Volha Bryl; Sebastian Tramp,Linked Open Data (LOD) is a pragmatic approach for realizing the Semantic Web vision ofmaking the Web a global; distributed; semantics-based information system. This bookpresents an overview on the results of the research project “LOD2--Creating Knowledge outof Interlinked Data”. LOD2 is a large-scale integrating project co-funded by the EuropeanCommission within the FP7 Information and Communication Technologies Work Program.Commencing in September 2010; this 4-year project comprised leading Linked Open Dataresearch groups; companies; and service providers from across 11 European countries andSouth Korea. The aim of this project was to advance the state-of-the-art in research anddevelopment in four key areas relevant for Linked Data; namely 1. RDF data management;2. the extraction; creation; and enrichment of structured RDF data; 3. the interlinking and …,*,2014,14
SlideWiki: elicitation and sharing of corporate knowledge using presentations,Ali Khalili; Sören Auer; Darya Tarasowa; Ivan Ermilov,Abstract Presentations play a crucial role in knowledge management within organizations; inparticular to facilitate organizational learning and innovation. Much of the corporate strategy;direction and accumulated knowledge within organizations is encapsulated inpresentations. In this paper; we investigate the limitations of current presentation tools forsemi-structured knowledge representation and sharing within organizations. We addresschallenges such as collaborative creation of presentations; tracking changes within them;sharing and reusing existing presentations. Then we present SlideWiki as a crowd-sourcingplatform for the elicitation and sharing of corporate knowledge using presentations. WithSlideWiki users can author; collaborate and arrange slides in organizational presentationsby employing Web 2.0 strategies. Presentations can be organized hierarchically; so as to …,International Conference on Knowledge Engineering and Knowledge Management,2012,14
Towards a Korean DBpedia and an Approach for Complementing the Korean Wikipedia based on DBpedia.,Eun-kyung Kim; Matthias Weidl; Key-Sun Choi; Sören Auer,Abstract. In the first part of this paper we report about experiences when applying theDBpedia extraction framework to the Korean Wikipedia. We improved the extraction of non-Latin characters and extended the framework with pluggable internationalizationcomponents in order to facilitate the extraction of localized information. With theseimprovements we almost doubled the amount of extracted triples. We also will present theresults of the extraction for Korean. In the second part; we present a conceptual study aimedat understanding the impact of international resource synchronization in DBpedia. In theabsence of any information synchronization; each country would construct its own datasetsand manage it from its users. Moreover the cooperation across the various countries isadversely affected.,OKCon,2010,14
Ubiquitous semantic applications: A systematic literature review,Timofey Ermilov; Ali Khalili; Sören Auer,ABSTRACT Recently practical approaches for development of ubiquitous semanticapplications have made quite some progress. In particular in the area of the ubiquitousaccess to the semantic data the authors recently observed a large number of approaches;systems and applications being described in the literature. With this survey the authors aimto provide an overview on the rapidly emerging field of Ubiquitous Semantic Applications(UbiSA). The authors conducted a systematic literature review comprising a thoroughanalysis of 48 primary studies out of 172 initially retrieved papers. The authors obtained acomprehensive set of quality attributes for UbiSA together with corresponding applicationfeatures suggested for their realization. The quality attributes include aspects such asmobility; usability; heterogeneity; collaboration; customizability and evolvability. The …,Mobile Computing and Wireless Networks: Concepts; Methodologies; Tools; and Applications: Concepts; Methodologies; Tools; and Applications,2015,13
Introduction to LOD2,Sören Auer,Abstract In this introductory chapter we give a brief overview on the Linked Data concept; theLinked Data lifecycle as well as the LOD2 Stack–an integrated distribution of aligned toolswhich support the whole life cycle of Linked Data from extraction; authoring/creation viaenrichment; interlinking; fusing to maintenance. The stack is designed to be versatile; for allfunctionality we define clear interfaces; which enable the plugging in of alternative third-party implementations. The architecture of the LOD2 Stack is based on three pillars:(1)Software integration and deployment using the Debian packaging system.(2) Use of acentral SPARQL endpoint and standardized vocabularies for knowledge base access andintegration between the different tools of the LOD2 Stack.(3) Integration of the LOD2 Stackuser interfaces based on REST enabled Web Applications. These three pillars comprise …,*,2014,13
Nif combinator: Combining nlp tool output,Sebastian Hellmann; Jens Lehmann; Sören Auer; Marcus Nitzschke,Abstract The NLP Interchange Format (NIF) is an RDF/OWL-based format that providesinteroperability between Natural Language Processing (NLP) tools; language resources andannotations by allowing NLP tools to exchange annotations about text documents in RDF.Other than more centralized solutions such as UIMA and GATE; NIF enables the creation ofheterogeneous; distributed and loosely coupled NLP applications; which use the Web as anintegration platform. NIF wrappers have to be only created once for a particular tool and cansubsequently interoperate with a potentially large number of other tools. We present (1) thecurrently implemented NIF wrappers; which are available as free web services and (2) a GUIcalled the NIF Combinator; which allows to combine the output of the implemented NIF webservices.,International Conference on Knowledge Engineering and Knowledge Management,2012,13
Olap2datacube: An ontowiki plug-in for statistical data publishing,Percy E Rivera Salas; Michael Martin; Fernando Maia Da Mota; Sören Auer; Karin K Breitman; Marco A Casanova,Abstract Statistical data is one of the most important sources of information; relevant for largenumbers of stakeholders in the governmental; scientific and business domains alike. In thisarticle; we introduce an Ontowiki plugin that extracts and publishes statistical data in RDF.We illustrate the plugin with a comprehensive use case reporting on the extraction andpublishing on the Web of statistical data about 10 years of Brazilian government.,Proceedings of the Second International Workshop on Developing Tools as Plug-Ins,2012,13
Redd-observatory: Using the web of data for evaluating the research-disease disparity,Amrapali Zaveri; Ricardo Pietrobon; Soren Auer; Jens Lehmann; Michael Martin; Timofey Ermilov,Abstract It is widely accepted that there is a large disparity between the availability oftreatment options and the prevalence of diseases all over the world; thus placing individualsin danger. This disparity is partially caused by the restricted access to information that wouldallow health care and research policy makers to formulate more appropriate measures tomitigate it. Specifically; this shortage of information is caused by the difficulty in reliablyobtaining and integrating data regarding the disease burden and the respective researchinvestments. In response to these challenges; the Linked Data paradigm provides a simplemechanism for publishing and interlinking structured information on the Web. In conjunctionwith the ever increasing data on diseases and health care research available as LinkedData; an opportunity is created to reduce this information gap that would allow for better …,Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Volume 01,2011,13
Towards linked data internationalization-realizing the greek dbpedia,Dimitris Kontokostas; Charalampos Bratsas; Sören Auer; Sebastian Hellmann; Ioannis Antoniou; George Metakides,ABSTRACT This paper describes the realization of the Greek DBpedia as part of theDBpedia Internationalization proccess.“I18n filters” are proposed as pluggable componentsof the DBpedia Information Extraction Framework; in order to address issues concerningcovering more knowledge from non-English Wikipedia's and International ResourceIdentifier (IRI) support. Moreover; a new extractor is introduced that uses the WikipediaInterlanguage Links to connect international DB-pedia's and transitively to the LOD Cloud.Finally; the paper illustrates the first international project which provides TransparentContent Negotiation (TCN) rules for International Resource Identifier's (IRI's) for de-referencing purposes. This work could serve as a guide not only for other multilingualDBpedias; but for publishing linked data in languages based on non-Latin character sets …,*,2011,13
Use cases and requirements for mapping relational databases to rdf,Sören Auer; L Feigenbaum; D Miranker; A Fogarolli; J Sequeda,*,W3c working draft; W3C,2010,13
Luzzu—A Methodology and Framework for Linked Data Quality Assessment,Jeremy Debattista; SÖren Auer; Christoph Lange,Abstract The increasing variety of Linked Data on the Web makes it challenging to determinethe quality of this data and; subsequently; to make this information explicit to dataconsumers. Despite the availability of a number of tools and frameworks to assess LinkedData Quality; the output of such tools is not suitable for machine consumption; and thusconsumers can hardly compare and rank datasets in the order of fitness for use. This articledescribes a conceptual methodology for assessing Linked Datasets; and Luzzu; aframework for Linked Data Quality Assessment. Luzzu is based on four majorcomponents:(1) an extensible interface for defining new quality metrics;(2) an interoperable;ontology-driven back-end for representing quality metadata and quality problems that canbe re-used within different semantic frameworks;(3) scalable dataset processors for data …,Journal of Data and Information Quality (JDIQ),2016,12
Git4voc: Git-based versioning for collaborative vocabulary development,Lavdim Halilaj; Irlán Grangel-González; Gökhan Coskun; Sören Auer,Collaborative vocabulary development in the context of data integration is the process offinding consensus between the experts of the different systems and domains. Thecomplexity of this process is increased with the number of involved people; the variety of thesystems to be integrated and the dynamics of their domain. In this paper we advocate thatthe realization of a powerful version control system is the heart of the problem. Driven by thisidea and the success of Git in the context of software development; we investigate theapplicability of Git for collaborative vocabulary development. Even though vocabularydevelopment and software development have much more similarities than differences thereare still important differences. These need to be considered within the development of asuccessful versioning and collaboration system for vocabulary development. Therefore …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,12
Extending the WebID protocol with access delegation,Sebastian Tramp; Henry Story; Andrei Sambra; Philipp Frischmuth; Michael Martin; Sören Auer,The WebID protocol enables the global identi cation and authentication of agents in adistributed manner by combining asymmetric cryptography and Linked Data. In order todecide whether access should be granted or denied to a particular WebID; theauthenticating web server may need to retrieve other pro les and linked resources to workout eg if the requesting agent is member of an authorized group. If these resources arerequired to be publicly available for the server to access it; then this would be a majorprivacy limitation on a linked Social Network. In this paper we explore di fferent ways inwhich an agent can act as a user and we propose an extension to the WebID protocol whichallows for delegation of access authorization from a WebID to a third party; eg allowing aserver to be able to act on behalf of its users. This extends the range of application …,Proceedings of the Third International Workshop on Consuming Linked Data (COLD2012),2012,12
Semantic wiki representations for building an enterprise knowledge base,Sören Auer; Berit Jungmann; Frank Schönefeld,Abstract In the literature semantically enabled knowledge technologies are described as anew kind of web ([1]; XI). In the science domain many ideas and interesting tool prototypesexist. In companies; however; there is less estimation for the effort and ways for usingsemantic web technologies. Special application scenarios for business purposes andexperiences in real-life projects are necessary ([1]; 303). This paper focuses on the need forsemantic technologies from a business perspective and explains ideas of a scientificpartner. Furthermore; the collaborative research project SoftWiki is introduced.,Reasoning Web International Summer School,2007,12
Block chain technologies & the semantic web: A framework for symbiotic development,Matthew English; Sören Auer; John Domingue,Abstract. The concept of peer-to-peer applications is not new; nor is the concept ofdistributed hash tables. What emerged in 2008 with the publication of the Bitcoin whitepaper was an incentive structure that unified these two software paradigms with a set ofeconomic stimuli to motivate the creation of a dedicated computing network orders ofmagnitude more powerful than the world's fastest supercomputers. The purpose of which isthe maintenance of a massive distributed database known as the Bitcoin “block chain”. Apartfrom the digital currency it enables; block chain technology is a fascinating new computingparadigm with broad implications for the future development of the World Wide Web; and byextension; the further growth of Linked Data and the Semantic Web. This work is divided intotwo main sections; we first demonstrate how block chain technologies can contribute …,Computer Science Conference for University of Bonn Students; J. Lehmann; H. Thakkar; L. Halilaj; and R. Asmat; Eds,2016,11
Quality assessment of linked datasets using probabilistic approximation,Jeremy Debattista; Santiago Londoño; Christoph Lange; Sören Auer,Abstract With the increasing application of Linked Open Data; assessing the quality ofdatasets by computing quality metrics becomes an issue of crucial importance. For large andevolving datasets; an exact; deterministic computation of the quality metrics is too timeconsuming or expensive. We employ probabilistic techniques such as Reservoir Sampling;Bloom Filters and Clustering Coefficient estimation for implementing a broad set of dataquality metrics in an approximate but sufficiently accurate way. Our implementation isintegrated in the comprehensive data quality assessment framework Luzzu. We evaluatedits performance and accuracy on Linked Open Datasets of broad relevance.,European Semantic Web Conference,2015,11
Leveraging the crowdsourcing of lexical resources for bootstrapping a linguistic data cloud,Sebastian Hellmann; Jonas Brekle; Sören Auer,Abstract We present a declarative approach implemented in a comprehensive open-sourceframework based on DBpedia to extract lexical-semantic resources–an ontology aboutlanguage use–from Wiktionary. The data currently includes language; part of speech;senses; definitions; synonyms; translations and taxonomies (hyponyms; hyperonyms;synonyms; antonyms) for each lexical word. Main focus is on flexibility to the loose schemaand configurability towards differing language-editions of Wiktionary. This is achieved by adeclarative mediator/wrapper approach. The goal is to allow the addition of languages justby configuration without the need of programming; thus enabling the swift and resource-conserving adaption of wrappers by domain experts. The extracted data is as fine granularas the source data in Wiktionary and additionally follows the lemon model. It enables use …,Joint International Semantic Technology Conference,2012,11
The web of data: Decentralized; collaborative; interlinked and interoperable,Sören Auer; Sebastian Hellmann,Abstract Recently the publishing and integration of structured data on the Web gainedtraction with initiatives such as Linked Data; RDFa and schema. org. In this article we outlinesome fundamental principles and aspects of the emerging Web of Data. We stress theimportance of open licenses as an enabler for collaboration; sharing and reuse of structureddata on the Web. We discuss some features of the RDF data model and its suitability forintegrating structured data on the Web. Two particularly crucial aspects are performance andscalability as well as conceptual interoperability; when using the Web as a medium for dataintegration. Last but not least we outline our vision of a Web of interlinked linguisticresources; which includes the establishment of a distributed ecosystem of heterogeneousNLP tools and services by means of structural; conceptual and access interoperability …,Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC-2012),2012,11
Ontosfeeder–a versatile semantic context provider for web content authoring,Alex Klebeck; Sebastian Hellmann; Christian Ehrlich; Sören Auer,Abstract As the amount of structured information available on the Web as Linked Data hasreached a respectable size. However; the question arises; how this information can beoperationalised in order to boost productivity. A clear improvement over the keyword-baseddocument retrieval as well as the manual aggregation and compilation of facts is theprovision of contextual information in an integrated fashion. In this demo; we present theOntos Feeder–a system serving as context information provider; that can be integrated intoContent Management Systems in order to support authors by supplying additionalinformation on the fly. During the creation of text; relevant entities are highlighted andcontextually disambiguated; facts from trusted sources such as DBpedia or Freebase areshown to the author. Productivity is increased; because the author does not have to leave …,Extended Semantic Web Conference,2011,11
The Qanary Ecosystem: getting new insights by composing Question Answering pipelines,Dennis Diefenbach; Kuldeep Singh; Andreas Both; Didier Cherix; Christoph Lange; Sören Auer,Abstract The field of Question Answering (QA) is very multi-disciplinary as it requiresexpertise from a large number of areas such as natural language processing (NLP); artificialintelligence; machine learning; information retrieval; speech recognition and semantictechnologies. In the past years a large number of QA systems were proposed usingapproaches from different fields and focusing on particular tasks in the QA process.Unfortunately; most of these systems cannot be easily reused; extended; and results cannotbe easily reproduced since the systems are mostly implemented in a monolithic fashion; lackstandardized interfaces and are often not open source or available as Web services. Toaddress these issues we developed the knowledge-based Qanary methodology forchoreographing QA pipelines distributed over the Web. Qanary employs the qa …,International Conference on Web Engineering,2017,10
Linked Data Notifications: a resource-centric communication protocol,Sarven Capadisli; Amy Guy; Christoph Lange; Sören Auer; Andrei Sambra; Tim Berners-Lee,Abstract In this article we describe the Linked Data Notifications (LDN) protocol; which is aW3C Candidate Recommendation. Notifications are sent over the Web for a variety ofpurposes; for example; by social applications. The information contained within a notificationis structured arbitrarily; and typically only usable by the application which generated it in thefirst place. In the spirit of Linked Data; we propose that notifications should be reusable bymultiple authorised applications. Through separating the concepts of senders; receivers andconsumers of notifications; and leveraging Linked Data principles of shared vocabulariesand URIs; LDN provides a building block for decentralised Web applications. This permitsend users more freedom to switch between the online tools they use; as well as generatinggreater value when notifications from different sources can be used in combination. We …,European Semantic Web Conference,2017,10
VoCol: an integrated environment to support version-controlled vocabulary development,Lavdim Halilaj; Niklas Petersen; Irlán Grangel-González; Christoph Lange; Sören Auer; Gökhan Coskun; Steffen Lohmann,Abstract Vocabularies are increasingly being developed on platforms for hosting version-controlled repositories; such as GitHub. However; these platforms lack important featuresthat have proven useful in vocabulary development. We present VoCol; an integratedenvironment that supports the development of vocabularies using Version Control Systems.VoCol is based on a fundamental model of vocabulary development; consisting of the threecore activities modeling; population; and testing. We implemented VoCol using a loosecoupling of validation; querying; analytics; visualization; and documentation generationcomponents on top of a standard Git repository. All components; including the version-controlled repository; can be configured and replaced with little effort to cater for various usecases. We demonstrate the applicability of VoCol with a real-world example and report on …,European Knowledge Acquisition Workshop,2016,10
Interest-based RDF update propagation,Kemele M Endris; Sidra Faisal; Fabrizio Orlandi; Sören Auer; Simon Scerri,Abstract Many LOD datasets; such as DBpedia and LinkedGeoData; are voluminous andprocess large amounts of requests from diverse applications. Many data products andservices rely on full or partial local LOD replications to ensure faster querying andprocessing. Given the evolving nature of the original and authoritative datasets; to ensureconsistent and up-to-date replicas frequent replacements are required at a great cost. In thispaper; we introduce an approach for interest-based RDF update propagation; whichpropagates only interesting parts of updates from the source to the target dataset. Effectively;this enables remote applications to 'subscribe'to relevant datasets and consistently reflectthe necessary changes locally without the need to frequently replace the entire dataset (or arelevant subset). Our approach is based on a formal definition for graph-pattern-based …,International Semantic Web Conference,2015,10
The linked data visualization model,Josep Maria Brunetti Fernéandez; Sören Auer; Roberto Garcia,*,ISWC (Posters & Demos),2012,10
Learning of owl class expressions on very large knowledge bases and its applications,Sebastian Hellmann; Jens Lehmann; Sören Auer,ABSTRACT The vision of the Semantic Web aims to make use of semantic representationson the largest possible scale-the Web. Large knowledge bases such as DBpedia; OpenCyc;and GovTrack are emerging and freely available as Linked Data and SPARQL endpoints.Exploring and analysing such knowledge bases is a significant hurdle for Semantic Webresearch and practice. As one possible direction for tackling this problem; the authorspresent an approach for obtaining complex class expressions from objects in knowledgebases by using Machine Learning techniques. The chapter describes in detail how toleverage existing techniques to achieve scalability on large knowledge bases available asSPARQL endpoints or Linked Data. The algorithms are made available in the open sourceDL-Learner project and this chapter presents several real-life scenarios in which they can …,Interoperability Semantic Services and Web Applications: Emerging Concepts; editors; Learning of OWL Class Expressions on Very Large Knowledge Bases and its Applications,2011,10
Towards agile knowledge engineering: Methodology; concepts and applications,Sören Auer,*,*,2007,10
A Web based platform for collaborative ontology management,Sören Auer,Abstract With the Ontology Web Language there now exists a semantic web language forknowledge representation on the Web. Although there are some application programminginterfaces and tools for OWL available; a framework for parsing; storing; querying;manipulating; serving and serializing OWL knowledge bases in a collaborative web enabledenvironment for the most distributed web application platform (Linux/Apache/PHP/MySQL)was still missing. We give an overview on pOWL; a webbased semantic web developmentplatform for PHP; which may be used as a foundation framework for semantic webapplications.,Proc. International Semantic Web Conference (ISWC),2004,10
Big Data; Big Opportunities,Stefan Wrobel; Hans Voss; Joachim Köhler; Uwe Beyer; Sören Auer,Zusammenfassung Angetrieben von den technischen Innovationen in der Informatik stehenin allen Bereichen von Wirtschaft; Gesellschaft und Privatleben heute immer mehr Daten zurVerfügung; die potenziell übertragen; gespeichert und analysiert werden könnten; umdaraus nützliche Informationen als Grundlage für neue Dienste zu gewinnen. TechnischeNeuerungen wie die verteilte oder speicherresidente Verarbeitung von Daten haben dazugeführt; dass unsere Analysefähigkeiten so stark gewachsen sind; dass eine neue Klassevon Anwendungen möglich erscheint. Unter dem Schlagwort;; Big Data “scheint sich daherzurzeit eine Revolution bei der Nutzung von Daten in allen Bereichen anzukündigen. Dervorliegende Artikel versucht angesichts aktueller Studien zur Nutzung von Big Data-Ansätzen zu beleuchten; inwieweit die großen öffentlichen Erwartungen sich tatsächlich …,Informatik-Spektrum,2015,9
Cubeviz: Exploration and visualization of statistical linked data,Michael Martin; Konrad Abicht; Claus Stadler; Axel-Cyrille Ngonga Ngomo; Tommaso Soru; Sören Auer,Abstract CubeViz is a flexible exploration and visualization platform for statistical datarepresented adhering to the RDF Data Cube vocabulary. If statistical data is providedadhering to the Data Cube vocabulary; CubeViz exhibits a faceted browsing widget allowingto interactively filter observations to be visualized in charts. Based on the selected structuralpart; CubeViz offers suitable chart types and options for configuring the visualization byusers. In this demo we present the CubeViz visualization architecture and components;sketch its underlying API and the libraries used to generate the desired output. By employingadvanced introspection; analysis and visualization bootstrapping techniques CubeViz hidesthe schema complexity of the encoded data in order to support a user-friendly explorationexperience.,Proceedings of the 24th International Conference on World Wide Web,2015,9
OpenCourseWare observatory: does the quality of OpenCourseWare live up to its promise?,Sahar Vahdati; Christoph Lange; Sören Auer,Abstract A vast amount of OpenCourseWare (OCW) is meanwhile being published online tomake educational content accessible to larger audiences. The awareness of such coursesamong users and the popularity of systems providing such courses are increasing. However;from a subjective experience; OCW is frequently cursory; outdated or non-reusable. In orderto obtain a better understanding of the quality of OCW; we assess the quality in terms offitness for use. Based on three OCW use case scenarios; we define a range of dimensionsaccording to which the quality of courses can be measured. From the definition of eachdimension a comprehensive list of quality metrics is derived. In order to obtain arepresentative overview of the quality of OCW; we performed a quality assessment on a setof 100 randomly selected courses obtained from 20 different OCW repositories. Based on …,Proceedings of the Fifth International Conference on Learning Analytics And Knowledge,2015,9
Publishing and interlinking the global health observatory dataset,Amrapali Zaveri; Jens Lehmann; Sören Auer; Mofeed M Hassan; Mohamed A Sherif; Michael Martin,Abstract The improvement of public health is one of the main indicators for societal progress.Statistical data for monitoring public health is highly relevant for a number of sectors; such asresearch (eg in the life sciences or economy); policy making; health care; pharmaceuticalindustry; insurances etc. Such data is meanwhile available even on a global scale; eg in theGlobal Health Observatory (GHO) of the United Nations's World Health Organization (WHO).GHO comprises more than 50 different datasets; it covers all 198 WHO member countriesand is updated as more recent or revised data becomes available or when there arechanges to the methodology being used. However; this data is only accessible via complexspreadsheets and; therefore; queries over the 50 different datasets as well as combinationswith other datasets are very tedious and require a significant amount of manual work. By …,Semantic Web,2013,9
Creating knowledge out of interlinked data: making the web a data washing machine,Sören Auer,Abstract Over the past 4 years; the semantic web activity has gained momentum with thewidespread publishing of structured data as RDF. The Linked Data paradigm has thereforeevolved from a practical research idea into a very promising candidate for addressing one ofthe biggest challenges in the area of the Semantic Web vision: the exploitation of the Web asa platform for data and information integration. To translate this initial success into a world-scale reality; a number of research challenges need to be addressed: the performance gapbetween relational and RDF data management has to be closed; coherence and quality ofdata published on the Web have to be improved; provenance and trust on the Linked DataWeb must be established and generally the entrance barrier for data publishers and usershas to be lowered. We discuss approaches for tackling these challenges and their …,Proceedings of the International Conference on Web Intelligence; Mining and Semantics,2011,9
Methods and applications of the social Semantic Web,S Auer,*,Semantic Web and/or Web,2008,9
Integrating ontologies and relational data,Sören Auer; Zachary G Ives,Abstract In recent years; an increasing number of scientific and other domains haveattempted to standardize their terminology and provide reasoning capabilities throughontologies; in order to facilitate data exchange. This has spurred research into Web-basedlanguages; formalisms; and especially query systems based on ontologies.,Technical Reports (CIS),2007,9
The RapidOWL Methodology--Towards Agile Knowledge Engineering,Soren Auer,Agile methodologies have recently gained growing success in many economic andtechnical spheres. This is due to the fact that flexibility; in particular fast and efficientreactions to changed prerequisites; is becoming increasingly important in the informationsociety. To support adaptive; semantic collaboration between domain experts andknowledge engineers; a new; agile knowledge engineering methodology; called RapidOWLis proposed. This methodology is based on the idea of iterative refinement; annotation andstructuring of a knowledge base. A central paradigm for the RapidOWL methodology is theconcentration on smallest possible information chunks. The collaborative aspect comes intoplay; when those information chunks can be selectively added; removed; annotated withcomments or ratings. Design rationales for the RapidOWL methodology are to be light …,Enabling Technologies: Infrastructure for Collaborative Enterprises; 2006. WETICE'06. 15th IEEE International Workshops on,2006,9
Fuhsen: a platform for federated; RDF-based hybrid search,Diego Collarana; Christoph Lange; Sören Auer,Abstract The increasing amount of structured and semi-structured information available onthe Web and in distributed information systems; as well as the Web's diversification intodifferent segments such as the Social Web; the Deep Web; or the Dark Web; requires newmethods for horizontal search. FuhSen is a federated; RDF-based; hybrid search platformthat searches; integrates and summarizes information about entities from distributedheterogeneous information sources using Linked Data. As a use case; we present scenarioswhere law enforcement institutions search and integrate data spread across these differentWeb segments to identify cases of organized crime. We present the architecture andimplementation of FuhSen and explain the queries that can be addressed with this newapproach.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,8
Crowdlearn: Crowd-sourcing the creation of highly-structured e-learning content,Darya Tarasowa; Ali Khalili; Sören Auer,Abstract While nowadays there is a plethora of Learning Content Management Systems; thecollaborative; community-based creation of rich e-learning content is still not sufficiently wellsupported. Few attempts have been made to apply crowd-sourcing and wiki-approaches forthe creation of e-learning content. However; the paradigm is only applied to unstructured;textual content and cannot be used in SCORM-compliant systems. To address this issue wedeveloped the CrowdLearn concept to exploit the wisdom; creativity and productivity of thecrowd for the creation of rich; deep-semantically structured e-learning content. TheCrowdLearn concept combines the wiki style for collaborative content authoring withSCORM requirements for re-usability. Therefore; it enables splitting the learning materialinto Learning Objects (LOs) with an adjustable level of granularity. In order to realize the …,International Journal of Engineering Pedagogy (iJEP),2015,8
Luzzu Quality Metric Language--A DSL for Linked Data Quality Assessment,Jeremy Debattista; Christoph Lange; Sören Auer,Abstract: The steadily growing number of linked open datasets brought about a number ofreservations amongst data consumers with regard to the datasets' quality. Qualityassessment requires significant effort and consideration; including the definition of dataquality metrics and a process to assess datasets based on these definitions. Luzzu is aquality assessment framework for linked data that allows domain-specific metrics to beplugged in. LQML offers notations; abstractions and expressive power; focusing on therepresentation of quality metrics. It provides expressive power for defining sophisticatedquality metrics. Its integration with Luzzu enables their efficient processing and executionand thus the comprehensive assessment of extremely large datasets in a streaming way. Wealso describe a novel ontology that enables the reuse; sharing and querying of such …,arXiv preprint arXiv:1504.07758,2015,8
LinDA-visualising and exploring linked data,Klaudia Thellmann; Fabrizio Orlandi; Sören Auer,Abstract. The main goal of our work in the context of the LinDA (Linked Data Analytics)project is to offer small and medium sized enterprises (SMEs) possibilities for integrating andconsuming data by using Linked Data technologies. One of the major challenges of thisproject consists in providing user-friendly means of exploring and visualising Linked Data.To achieve this; a Semantic Web application has been created; based on state-of-the-artlinked data visualisation approaches; which allows a largely automatic matching andbinding of data to visualisations. Hence; in this demo paper we demonstrate the potential ofa visualisation framework which is capable of dealing with different data formats;serialisations and Semantic Web ontologies.,Proceedings of the Posters and Demos Track of 10th International Conference on Semantic Systems-SEMANTiCS2014; Leipzig; Germany,2014,8
Semantic similarity and correlation of linked statistical data analysis,Sarven Capadisli; A Merono; Sören Auer; Reinhard Riedl,Abstract. Statistical data is increasingly made available in the form of Linked Data on theWeb. As more and more statistical datasets become available; a fundamental question onstatistical data comparability arises: To what extent can arbitrary statistical datasets befaithfully compared? Besides a purely statistical comparability; we are interested in the rolethat semantics plays in the data to be compared. Our hypothesis is that semanticrelationships between different components of statistical datasets might have a relationshipwith their statistical correlation. Our research focuses in studying whether these statisticaland semantic relationships influence each other; by comparing the correlation of statisticaldata with their semantic similarity. The ongoing research problem is; hence; to investigatewhy machines have a difficulty in revealing meaningful correlations or establishing non …,*,2014,8
Large-scale RDF dataset slicing,Edgard Marx; Saeedeh Shekarpour; Soren Auer; Axel-Cyrille Ngonga Ngomo,In the last years an increasing number of structured data was published on the Web asLinked Open Data (LOD). Despite recent advances; consuming and using Linked OpenData within an organization is still a substantial challenge. Many of the LOD datasets arequite large and despite progress in RDF data management their loading and querying withina triple store is extremely time-consuming and resource-demanding. To overcome thisconsumption obstacle; we propose a process inspired by the classical Extract-Transform-Load (ETL) paradigm. In this article; we focus particularly on the selection and extractionsteps of this process. We devise a fragment of SPARQL dubbed SliceSPARQL; whichenables the selection of well-defined slices of datasets fulfilling typical information needs.SliceSPARQL supports graph patterns for which each connected sub graph pattern …,Semantic Computing (ICSC); 2013 IEEE Seventh International Conference on,2013,8
Kollaborative Wissensarbeit mit OntoWiki.,Sebastian Dietzold; Sören Auer; Thomas Riechert,Abstract: In diesem Papier beschreiben wir; wie Semantic Web und Web 2.0 Paradigmenund Technologien für kollaboratives Knowledge Engineering in Sozialen Netzwerkengenutzt werden können. Darauf aufbauend präsentieren wir einen Werkzeug-Prototypen;welcher diese Ideen umsetzt und agiles Knowledge Engineering in einer reinenWebumgebung ermöglicht.,GI Jahrestagung (2),2006,8
vernetzte kirche”: Building a semantic web,Sören Auer; Bart Pieterse,Abstract The only possibility for federally structured organizations and communities toenable consistent views on their meta-data and contents is to establish methods to gatherthe meta-data from the distributed peers; integrate it into a common conceptual model andfinally make it accessible to humans and software systems. We present the approach takento implement this strategy for the over 2000 affiliated organizations of the Bavarian LutheranChurch. It showcases how different Semantic Web standards; vocabularies; andmethodologies can be coherently integrated into a consistent framework delivering addedvalue to the large audience of participating peers.,Proceedings of ISWC Workshop Semantic Web Case Studies and Best Practices for eBusiness (SWCASE05),2005,8
Wikidata through the Eyes of DBpedia,Ali Ismayilov; Dimitris Kontokostas; Sören Auer; Jens Lehmann; Sebastian Hellmann,Abstract DBpediaáis one of the earliest and most prominent nodes of the Linked Open Datacloud. DBpediaáextracts and provides structured data for various crowd-maintainedinformation sources such as over 100 Wikipedia language editions as well as WikimediaCommons by employing a mature ontology and a stable and thorough Linked Datapublishing lifecycle. Wikidata; on the other hand; has recently emerged as a user curatedsource for structured information which is included in Wikipedia. In this paper; we presenthow Wikidataáis incorporated in the DBpediaáeco-system. Enriching DBpediaáwithstructured information from Wikidataáprovides added value for a number of usagescenarios. We outline those scenarios and describe the structure and conversion process ofthe DBpediaWikidataá (DBw) dataset.,Semantic Web,2018,7
Are linked datasets fit for open-domain question answering? a quality assessment,Harsh Thakkar; Kemele M Endris; Jose M Gimenez-Garcia; Jeremy Debattista; Christoph Lange; Sören Auer,Abstract The current decade is a witness to an enormous explosion of data being publishedon the Web as Linked Data to maximise its reusability. Answering questions that usersspeak or write in natural language is an increasingly popular application scenario for WebData; especially when the domain of the questions is not limited to a domain wherededicated curated datasets exist; like in medicine. The increasing use of Web Data in thisand other settings has highlighted the importance of assessing its quality. While quite somework has been done with regard to assessing the quality of Linked Data; only few effortshave been dedicated to quality assessment of linked data from the question answeringdomain's perspective. From the linked data quality metrics that have so far been welldocumented in the literature; we have identified those that are most relevant for QA. We …,Proceedings of the 6th International Conference on Web Intelligence; Mining and Semantics,2016,7
Industrial Data Space: Digital Souvereignity Over Data,Boris Otto; Sören Auer; Jan Cirullies; Jan Jürjens; Nadja Menz; Jochen Schon; Sven Wenzel,*,*,2016,7
Value Creation on Open Government Data,Judie Attard; Fabrizio Orlandi; Sören Auer,Governments are one of the largest producers and collectors of data in many differentdomains. As one major aim of open government data initiatives is the release of social andcommercial value; we here explore existing processes of value creation on governmentdata. We identify the dimensions that impact; or are impacted by value creation; anddistinguish between the different value creating roles and participating stakeholders. Wepropose the use of Linked Data as an approach to enhance the value creation process; andprovide a Value Creation Assessment Framework to analyse the resulting impact.,System Sciences (HICSS); 2016 49th Hawaii International Conference on,2016,7
Identifying web tables: Supporting a neglected type of content on the web,Mikhail Galkin; Dmitry Mouromtsev; Sören Auer,Abstract The abundance of the data in the Internet facilitates the improvement of extractionand processing tools. The trend in the open data publishing encourages the adoption ofstructured formats like CSV and RDF. However; there is still a plethora of unstructured dataon the Web which we assume contain semantics. For this reason; we propose an approachto derive semantics from web tables which are still the most popular publishing tool on theWeb. The paper also discusses methods and services of unstructured data extraction andprocessing as well as machine learning techniques to enhance such a workflow. Theeventual result is a framework to process; publish and visualize linked open data. Thesoftware enables tables extraction from various open data sources in the HTML format andan automatic export to the RDF format making the data linked. The paper also gives the …,International Conference on Knowledge Engineering and the Semantic Web,2015,7
Crowdsourced semantic annotation of scientific publications and tabular data in PDF,Jaana Takis; AQM Islam; Christoph Lange; Sören Auer,Abstract Significant amounts of knowledge in science and technology have so far not beenpublished as Linked Open Data but are contained in the text and tables of legacy PDFpublications. Making such information available as RDF would; for example; provide directaccess to claims and facilitate surveys of related work. A lot of valuable tabular informationthat till now only existed in PDF documents would also finally become machineunderstandable. Instead of studying scientific literature or engineering patents for months; itwould be possible to collect such input by simple SPARQL queries. The SemAnn approachenables collaborative annotation of text and tables in PDF documents; a format that is stillthe common denominator of publishing; thus maximising the potential user base. Theresulting annotations in RDF format are available for querying through a SPARQL …,Proceedings of the 11th International Conference on Semantic Systems,2015,7
Towards an ontology for representing strings,Sebastian Hellmann; Jens Lehmann; Sören Auer,Abstract. The NLP Interchange Format (NIF) is an RDF/OWL-based format that aims toachieve interoperability between Natural Language Processing (NLP) tools; languageresources and annotations. The motivation behind NIF is to allow NLP tools to exchangeannotations about text documents in RDF. Hence; the main prerequisite is that parts of thedocuments (ie strings) are referenceable by URIs; so that they can be used as subjects inRDF statements. The String Ontology; which is the basis for NIF; fixes the referent (ie a stringin a given text) of annotations unambiguously for machines and thus enables the creation ofheterogeneous; distributed and loosely coupled NLP applications; which use the Web as anintegration platform. We evaluate the String Ontology based on the adequacy of thecollected requirements and furthermore by benchmarking the stability of the NIF URI …,Proceedings of the EKAW,2012,7
A wiki on the semantic web,Michel Buffa; F Gandon; G Erto,The wiki concept is more than 10 years old but has attained public success only recently;thanks to Wikipedia. However; in the intranet world; several studies have shown that theusage of wikis is subject to debate. Acceptance of such open; low-structured collaborativetools is not the rule. There are different reasons for explaining such low acceptance: socialreasons (corporate culture may not be adapted) but also usability reasons (the wiki is notstructured enough; it is hard to navigate and find relevant information; the wiki markuplanguage used by most wiki engine makes people reluctant to contribute to the wiki; etc.). Inthis chapter we present SweetWiki; a new wiki engine that relies on Semantic Webtechnologies and addresses most usability problems that have been reported in Buffa andGandon (2006); Chat and Nahaboo (2006); and Powers;(2005). SweetWiki is an example …,Emerging Technologies for Semantic Web Environments: Techniques; Methods and Applications; Fraunhofer Institute for Experimental Software Engineering (IESE); Germany (July 2007),2008,7
xOperator-interconnecting the semantic web and instant messaging networks,Sebastian Dietzold; Jörg Unbehauen; Sören Auer,Abstract Instant Messaging (IM) is in addition to Web and Email the most popular service onthe Internet. With xOperator we present a strategy and implementation which deeplyintegrates Instant Messaging networks with the Semantic Web. The xOperator concept isbased on the idea of creating an overlay network of collaborative information agents on topof social IM networks. It can be queried using a controlled and easily extensible languagebased on AIML templates. Such a deep integration of semantic technologies and InstantMessaging bears a number of advantages and benefits for users when compared to theseparated use of Semantic Web technologies and IM; the most important ones being contextawareness as well as provenance and trust. We showcase how the xOperator approachnaturally facilitates contacts and calendar management as well as access to large scale …,Proceedings of the 5th European semantic web conference on The semantic web: research and applications,2008,7
Implementing SPARQL Support for Relational Databases and Possible Enhancements.,Christian Weiske; Sören Auer,Abstract: In order to make the Semantic Web real we need the infrastructure to store; queryand update information adhering to the RDF paradigm. Such infrastructure can bedeveloped from scratch or benefit from developments and experiences made in otherscience & technology realms such as within the database domain. For querying RDF datathe World Wide Web Consortium released a Working Draft for the SPARQL query language.A large portion of the Web is meanwhile driven by server-side Web applications. PHP is thescripting language most widely used for Web applications. In this paper we present our PHPimplementation of the SPARQL standard directly interacting with an underlying databasesystem. The approach is based on the rationale of pushing as much work into the RDBMSas possible in order to profit most from the query optimization techniques developed for …,CSSW,2007,7
A survey of current approaches for mapping of relational databases to RDF; 2009,Satya S Sahoo; Wolfgang Halb; Sebastian Hellmann; Kingsley Idehen; Ted Thibodeau Jr; Sören Auer; Juan Sequeda; Ahmed Ezzat,*,URL http://www. w3. org/2005/Incubator/rdb2rdf/RDB2RDF_SurveyReport_01082009. pdf,*,7
OpenResearch: collaborative management of scholarly communication metadata,Sahar Vahdati; Natanael Arndt; Sören Auer; Christoph Lange,Abstract Scholars often need to search for matching; high-profile scientific events to publishtheir research results. Information about topical focus and quality of events is not madesufficiently explicit in the existing communication channels where events are announced.Therefore; scholars have to spend a lot of time on reading and assessing calls for papers butmight still not find the right event. Additionally; events might be overlooked because of thelarge number of events announced every day. We introduce OpenResearch; a crowdsourcing platform that supports researchers in collecting; organizing; sharing anddisseminating information about scientific events in a structured way. It enables quality-related queries over a multidisciplinary collection of events according to a broad range ofcriteria such as acceptance rate; sustainability of event series; and reputation of people …,European Knowledge Acquisition Workshop,2016,6
Git4Voc: collaborative vocabulary development based on git,Lavdim Halilaj; Irlán Grangel-González; Gökhan Coskun; Steffen Lohmann; Sören Auer,Collaborative vocabulary development in the context of data integration is the process offinding consensus between experts with different backgrounds; system understanding anddomain knowledge. The complexity of this process increases with the number of peopleinvolved; the variety of the systems to be integrated and the dynamics of their domain. In thispaper; we advocate that the usage of a powerful version control system is one of the keys toaddress this problem. Driven by this idea and the success of the version control system Git inthe context of software development; we investigate the applicability of Git for collaborativevocabulary development. Even though vocabulary development and software developmenthave much more similarities than differences; there are still important challenges. Theseneed to be considered in the development of a successful versioning and collaboration …,International Journal of Semantic Computing,2016,6
Linked'Big'Data: towards a manifold increase in big data value and veracity,Jeremy Debattista; Christoph Lange; Simon Scerri; Sören Auer,The Web of Data is an increasingly rich source of information; which makes it useful for BigData analysis. However; there is no guarantee that this Web of Data will provide theconsumer with truthful and valuable information. Most research has focused on Big Data'sVolume; Velocity; and Variety dimensions. Unfortunately; Veracity and Value; often regardedas the fourth and fifth dimensions; have been largely overlooked. In this paper we discussthe potential of Linked Data methods to tackle all five V's; and particularly propose methodsfor addressing the last two dimensions. We draw parallels between Linked and Big Datamethods; and propose the application of existing methods to improve and maintain qualityand address Big Data's veracity challenge.,Big Data Computing (BDC); 2015 IEEE/ACM 2nd International Symposium on,2015,6
Enabling accessible knowledge,Sarven Capadisli; Reinhard Riedl; Sören Auer,Abstract: The purpose of this document is to enable Web researchers to discover and sharetheir knowledge using the native Web stack for maximum openness; accessibility; andflexibility. We invite the reader to consume this document from its canonical location(previously published) at http://csarven. ca/enabling-accessible-knowledge,Conference for E-Democracy and Open Governement,2015,6
Dataset retrieval,Sven R Kunze; Soren Auer,Recently; a large number of dataset repositories; catalogs and portals are emerging in thescience and government realms. Once a large number of datasets are published on suchdata portals; the question arises how to retrieve datasets satisfying an information need. Inthis paper; we present an approach for retrieving datasets according to user queries. Wedefine dataset retrieval as a specialization of information retrieval. Instead of retrievingdocuments that are relevant to a certain information need; dataset retrieval describes theprocess of returning relevant RDF datasets. As with information retrieval; the term relevancecannot be clearly defined when using traditional methods like stemming. The inherent usageof RDF in these datasets enables a better way of retrieving relevant ones. We thereforepropose an additional retrieval mechanism; which is inspired by facet search: dataset …,Semantic Computing (ICSC); 2013 IEEE Seventh International Conference on,2013,6
TowardsWeb-Scale Collaborative Knowledge Extraction,Sebastian Hellmann; Sören Auer,Abstract While the Web of Data; the Web of Documents and Natural Language Processingare well researched individual fields; approaches to combine all three are fragmented andnot yet well aligned. This chapter analyzes current efforts in collaborative knowledgeextraction to uncover connection points between the three fields. The special focus is onthree prominent RDF data sets (DBpedia; LinkedGeoData and Wiktionary2RDF); whichallow users to influence the knowledge extraction process by adding another crowd-sourcedlayer on top. The recently published NLP Interchange Format (NIF) provides a way toannotate textual resources on the Web through the assignment of URIs with fragmentidentifiers. We will show how this formalism can easily be extended to encompass newannotation layers and vocabularies.,*,2013,6
Anisa Rula; Andrea Maurino; Ricardo Pietrobon; Jens Lehmann; and Sören Auer. Quality assessment methodologies for linked open data,Amrapali Zaveri,*,Semantic Web Journal. submitted on,2012,6
NIF: An ontology-based and linked-data-aware NLP Interchange Format,Sebastian Hellmann; Jens Lehmann; Sören Auer,ABSTRACT We are currently observing a plethora of Natural Language Processing toolsand services being made available. Each of the tools and services has its particularstrengths and weaknesses; but exploiting the strengths and synergistically combiningdifferent tools is currently an extremely cumbersome and time consuming task. Also; once aparticular set of tools is integrated this integration is not reusable by others. We argue thatsimplifying the interoperability of different NLP tools performing similar but alsocomplementary tasks will facilitate the comparability of results and the creation ofsophisticated NLP applications. In addition; the synergistic combination of tools mightultimately yield a boost in precision and recall for common NLP tasks. In this paper; wepresent the NLP Interchange Format (NIF). NIF is based on a Linked Data enabled URI …,Working Draft,2012,6
The BigDataEurope platform–supporting the variety dimension of big data,Sören Auer; Simon Scerri; Aad Versteden; Erika Pauwels; Angelos Charalambidis; Stasinos Konstantopoulos; Jens Lehmann; Hajira Jabeen; Ivan Ermilov; Gezim Sejdiu; Andreas Ikonomopoulos; Spyros Andronopoulos; Mandy Vlachogiannis; Charalambos Pappas; Athanasios Davettas; Iraklis A Klampanos; Efstathios Grigoropoulos; Vangelis Karkaletsis; Victor de Boer; Ronald Siebes; Mohamed Nadjib Mami; Sergio Albani; Michele Lazzarini; Paulo Nunes; Emanuele Angiuli; Nikiforos Pittaras; George Giannakopoulos; Giorgos Argyriou; George Stamoulis; George Papadakis; Manolis Koubarakis; Pythagoras Karampiperis; Axel-Cyrille Ngonga Ngomo; Maria-Esther Vidal,Abstract The management and analysis of large-scale datasets–described with the term BigData–involves the three classic dimensions volume; velocity and variety. While the formertwo are well supported by a plethora of software components; the variety dimension is stillrather neglected. We present the BDE platform–an easy-to-deploy; easy-to-use andadaptable (cluster-based and standalone) platform for the execution of big data componentsand tools like Hadoop; Spark; Flink; Flume and Cassandra. The BDE platform was designedbased upon the requirements gathered from seven of the societal challenges put forward bythe European Commission in the Horizon 2020 programme and targeted by theBigDataEurope pilots. As a result; the BDE platform allows to perform a variety of Big Dataflow tasks like message passing; storage; analysis or publishing. To facilitate the …,International Conference on Web Engineering,2017,5
KBox—Transparently Shifting Query Execution on Knowledge Graphs to the Edge,Edgard Marx; Ciro Baron; Tommaso Soru; Sören Auer,The Semantic Web architecture choices lead to a tremendous amount of informationpublished on the Web. However; to query and build applications on top of Linked Open Datais still significantly challenging. In this work; we present Knowledge Box (KBox); anapproach for transparently shifting query execution on Knowledge Graphs to the edge. Weshow that our approach makes the consumption of Knowledge Graphs more reliable andfaster than the formerly introduced methods. In practice; KBox demands less time andresources to setup than traditional approaches; while being twice as fast as SPARQLendpoints; even when serving a single client.,Semantic Computing (ICSC); 2017 IEEE 11th International Conference on,2017,5
Fuhsen: A federated hybrid search engine for building a knowledge graph on-demand (short paper),Diego Collarana; Mikhail Galkin; Christoph Lange; Irlán Grangel-González; Maria-Esther Vidal; Sören Auer,Abstract A vast amount of information about various types of entities is spread across theWeb; eg; people or organizations on the Social Web; product offers on the Deep Web or onthe Dark Web. These data sources can comprise heterogeneous data and are equippedwith different search capabilities eg; Search API. End users such as investigators from lawenforcement institutions searching for traces and connections of organized crime have todeal with these interoperability problems not only during search time but also while mergingdata collected from different sources. We devise FuhSen; a keyword-based federatedengine that exploits the search capabilities of heterogeneous sources during queryprocessing and generates knowledge graphs on-demand applying an RDF-Moleculeintegration approach in response to keyword-based queries. The resulting knowledge …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2016,5
Towards cleaning-up open data portals: A metadata reconciliation approach,Alan Tygel; Sören Auer; Jeremy Debattista; Fabrizio Orlandi; Maria Luiza Machado Campos,This paper presents an approach for metadata reconciliation; curation and linking for OpenGovernamental Data Portals (ODPs). ODPs have been lately the standard solution forgovernments willing to put their public data available for the society. Portal managers useseveral types of metadata to organize the datasets; one of the most important ones being thetags. However; the tagging process is subject to many problems; such as synonyms;ambiguity or incoherence; among others. As our empiric analysis of ODPs shows; theseissues are currently prevalent in most ODPs and effectively hinders the reuse of Open Data.In order to address these problems; we develop and implement an approach for tagreconciliation in Open Data Portals; encompassing local actions related to individual portals;and global actions for adding a semantic metadata layer above individual portals. The …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,5
Towards Vocabulary Development by Convention.,Irlán Grangel-González; Lavdim Halilaj; Gökhan Coskun; Sören Auer,Abstract: A major bottleneck for a wider deployment and use of ontologies and knowledgeengineering techniques is the lack of established conventions along with cumbersome andinefficient support for vocabulary and ontology authoring. We argue; that the pragmaticdevelopment by convention paradigm well-accepted within software engineering; can besuccessfully applied for ontology engineering; too. However; the definition of a valid set ofconventions requires broadly-accepted best-practices. In this regard; we empiricallyanalyzed a number of popular vocabularies and ontology development efforts with respectto their use of guidelines and common practices. Based on this analysis; we identified thefollowing main aspects of common practices: documentation; internationalization; naming;structure; reuse; validation and authoring. In this paper; these aspects are presented and …,KEOD,2015,5
LODFlow: a workflow management system for linked data processing,Sandro Rautenberg; Ivan Ermilov; Edgard Marx; Sören Auer; Axel-Cyrille N Ngomo,Abstract The extraction and maintenance of Linked Data datasets is a cumbersome; time-consuming and resource-intensive activity. The cost for producing Linked Data can bereduced by a workflow management system; which describes plans to systematicallysupport the lifecycle of RDF datasets. We present the LODFlow Linked Data WorkflowManagement System; which provides an environment for planning; executing; reusing; anddocumenting Linked Data workflows. The LODFlow approach is based on a comprehensiveknowledge model for describing the workflows and a workflow execution engine supportingsystematic workflow execution; reporting; and exception handling. The environment wasevaluated in a large-scale real-world use case. As result; LODFlow supports Linked Dataengineers to systematically plan; execute and assess Linked Data production and …,Proceedings of the 11th International Conference on Semantic Systems,2015,5
Linked open data-introduction to the special theme,Irini Fundulaki; Sören Auer,*,ERCIM News,2014,5
Optimizing SPARQL-to-SQL rewriting,Jörg Unbehauen; Claus Stadler; Sören Auer,Abstract The vast majority of the structured data of our age is stored in relational databases.In order to link and integrate this data on the Web; it is of paramount importance to maprelational data to the RDF data model and make Linked Data interfaces to the dataavailable. We can distinguish two main approaches: First; the database can be transformedinto RDF row by row and the resulting knowledge base can be exposed using a triple store.Second; an RDB2RDF mapper performs SPARQL-to-SQL rewriting and thus exposes avirtual RDF graph based on the relational database. The key challenge of such a SPARQL-to-SQL rewriting is to create a SQL query which can be efficiently executed by the optimizerof the underlying relational database. In this article we discuss and evaluate the impact ofdifferent optimizations on query execution time using SparqlMap; a R2RML compliant …,Proceedings of International Conference on Information Integration and Web-based Applications & Services,2013,5
Generating SPARQL queries using templates,Saeedeh Shekarpour; Sören Auer; Axel-Cyrille Ngonga Ngomo; Daniel Gerber; Sebastian Hellmann; Claus Stadler,Abstract The search for information on the Web of Data is becoming increasingly difficult dueto its considerable growth. Especially novice users need to acquire both knowledge aboutthe underlying ontology structure and proficiency in formulating formal queries (eg SPARQLqueries) to retrieve information from Linked Data sources. So as to simplify and automate thequerying and retrieval of information from such sources; this paper presents an approach forconstructing SPARQL queries based on user-supplied keywords. Our approach utilizes a setof predefined basic graph pattern templates for generating adequate interpretations of userqueries. This is achieved by obtaining ranked lists of candidate resource identifiers for thesupplied keywords and then injecting these identifiers into suitable positions in the graphpattern templates. The main advantages of our approach are that it is completely agnostic …,Web Intelligence and Agent Systems: An International Journal,2013,5
Query segmentation and resource disambiguation leveraging background knowledge,Saeedeh Shekarpour; A-C Ngonga Ngomo; Sören Auer,Abstract. Accessing the wealth of structured data available on the Data Web is still a keychallenge for lay users. Keyword search is the most convenient way for users to accessinformation (eg; from data repositories). In this paper we introduce a novel approach fordetermining the correct resources for user-supplied keyword queries based on a hiddenMarkov model. In our approach the user-supplied query is modeled as the observed dataand the background knowledge is used for parameter estimation. Instead of learningparameter estimation from training data; we leverage the semantic relationships betweendata items for computing the parameter estimations. In order to maximize accuracy andusability; query segmentation and resource disambiguation are mutually tightly interwoven.First; an initial set of potential segmentations is obtained leveraging the underlying …,Proceedings of WoLE Workshop,2012,5
The catalogus professorum lipsiensis-semantics-based collaboration and exploration for historians,Thomas Riechert; Ulf Morgenstern; Sören Auer; Sebastian Tramp; Michael Martin,Performance is the most critical aspect towards achieving high scalability of Semantic Web reasoningapplications; and considerably limits the application areas of them. There is still a deep mismatchbetween the requirements for reasoning on a Web scale and performance of the existing reasoningengines. The performance limitation can be considerably reduced by utilizing such large-scalee-Infrastructures as LarKC - the Large Knowledge Collider - an experimental platform for massivedistributed incomplete reasoning; which offers several innovative approaches removing the scalabilitybarriers; in particularly; by enabling transparent access to HPC systems. Efficient utilization ofsuch resources is facilitated by means of parallelization being the major element for accomplishingperformance and scalability of semantic applications. Here we discuss application of some emergingparallelization strategies and show the benefits obtained by using such systems as LarKC.,Proceedings of the 2010 International Conference on Posters & Demonstrations Track-Volume 658,2010,5
An integration life cycle for semantic web services composition,Muhammad Ahtisham Aslam; Jun Shen; Soren Auer; Michael Herrmann,Business applications are more and more often developed on the basis of Web services.The aim is to provide platform independence and loose coupling between businessapplications to facilitate distributed and grid computing scenarios. However; most efforts todeploy and publish Web services are manual. Manual discovery; invocation andcomposition of Web services in a distributed computing environment significantly hamperthe automatic process of enterprise application integration. Semantic enhancements in Webservices aim at making the process of Web services discovery; invocation and compositiondynamic by exposing the machine understandable description of Web service capabilitiesand Web service requests. In this paper we compare recent dynamic Web servicecomposition approaches. We highlight some dynamic composition issues and compare …,Computer Supported Cooperative Work in Design; 2007. CSCWD 2007. 11th International Conference on,2007,5
Towards an Integrated Graph Algebra for Graph Pattern Matching with Gremlin,Harsh Thakkar; Dharmen Punjani; Sören Auer; Maria-Esther Vidal,Abstract Graph data management has revealed beneficial characteristics in terms offlexibility and scalability by differently balancing between query expressivity and schemaflexibility. This has resulted into an rapid developing new task specific graph systems; querylanguages and data models; such as property graphs; key-value; wide column; resourcedescription framework (RDF); etc. Present day graph query languages are focused towardsflexible graph pattern matching (aka sub-graph matching); where as graph computingframeworks aim towards providing fast parallel (distributed) execution of instructions. Theconsequence of this rapid growth in the variety of graph based data management systemshas resulted in a lack of standardization. Gremlin; a graph traversal language and machine;provides a common platform for supporting any graph computing system (such as an …,International Conference on Database and Expert Systems Applications,2017,4
RQUERY: Rewriting Natural Language Queries on Knowledge Graphs to Alleviate the Vocabulary Mismatch Problem.,Saeedeh Shekarpour; Edgard Marx; Sören Auer; Amit P Sheth,Abstract For non-expert users; a textual query is the most popular and simple means forcommunicating with a retrieval or question answering system. However; there is a risk ofreceiving queries which do not match with the background knowledge. Query expansionand query rewriting are solutions for this problem but they are in danger of potentiallyyielding a large number of irrelevant words; which in turn negatively influences runtime aswell as accuracy. In this paper; we propose a new method for automatic rewriting inputqueries on graph-structured RDF knowledge bases. We employ a Hidden Markov Model todetermine the most suitable derived words from linguistic resources. We introduce theconcept of triplebased co-occurrence for recognizing co-occurred words in RDF data. Thismodel was bootstrapped with three statistical distributions. Our experimental study …,AAAI,2017,4
Exploring Term Networks for Semantic Search over RDF Knowledge Graphs,Edgard Marx; Konrad Höffner; Saeedeh Shekarpour; Axel-Cyrille Ngonga Ngomo; Jens Lehmann; Sören Auer,Abstract Information retrieval approaches are considered as a key technology to empowerlay users to access the Web of Data. A large number of related approaches such asQuestion Answering and Semantic Search have been developed to address this problem.While Question Answering promises more accurate results by returning a specific answer;Semantic Search engines are designed to retrieve the best top-K ranked resources. In thiswork; we propose* path; a Semantic Search approach that explores term networks forquerying RDF knowledge graphs. The adequacy of the approach is evaluated employingbenchmark datasets against state-of-the-art Question Answering as well as SemanticSearch systems. The results show that* path achieves better F _1-score than the currentlybest performing Semantic Search system.,Research Conference on Metadata and Semantics Research,2016,4
How much? Is not enough: an analysis of open budget initiatives,Alan Freihof Tygel; Judie Attard; Fabrizio Orlandi; Maria Luiza Machado Campos; Sören Auer,Abstract A worldwide movement towards the publication of Open Government Data is takingplace; and budget data is one of the key elements pushing this trend. Its importance ismostly related to transparency; but publishing budget data; combined with other actions; canalso improve democratic participation; allow comparative analysis of governments and boostdata-driven business. However; the lack of standards and common evaluation criteria stillhinders the development of appropriate tools and the materialization of the appointedbenefits. In this paper; we present a model to analyse government initiatives to publishbudget data. We identify the main features of these initiatives with a double objective:(i) todrive a structured analysis; relating some dimensions to their possible impacts; and (ii) toderive characterization attributes to compare initiatives based on each dimension. We …,Proceedings of the 9th International Conference on Theory and Practice of Electronic Governance,2016,4
Data driven governments: creating value through open government data,Judie Attard; Fabrizio Orlandi; Sören Auer,Abstract Governments are one of the largest producers and collectors of data in manydifferent domains and one major aim of open government data initiatives is the release ofsocial and commercial value. Hence; we here explore existing processes of value creationon government data. We identify the dimensions that impact; or are impacted by valuecreation; and distinguish between the different value creating roles and participatingstakeholders. We propose the use of Linked Data as an approach to enhance the valuecreation process; and provide a Value Creation Assessment Framework to analyse theresulting impact. We also implement the assessment framework to evaluate two governmentdata portals.,*,2016,4
Crowd-sourcing (semantically) structured multilingual educational content (CoSMEC),Darya Tarasowa; Sören Auer; Ali Khalili; Jörg Unbehauen,Abstract The support of multilingual content becomes crucial for educational platforms due tothe benefits it offers. In this paper we propose a concept that allows content authors to usethe power of the crowd to create (semantically) structured multilingual educational contentout of their material. To enable the collaboration of the crowd; we expand our previouslydeveloped CrowdLearn concept and WikiApp data model to support multilingualeducational content. The expanded concept; CoSMEC; was evaluated with an exampleimplementation within the web-based educational platform SlideWiki. Based on thisexperience; we provide solutions for the most complicated technical issues we faced duringthe implementation. This paper also discusses statistics which show the flow of multilingualcontent usage.,Open Praxis,2014,4
Linked enterprise data,T Pellegrini; H Sack; S Auer,*,*,2014,4
Towards linked statistical data analysis,Sarven Capadisli; Sören Auer; Reinhard Riedl,Abstract. Linked Data principles are increasingly employed to publish high-fidelity;heterogeneous statistical datasets in a distributed way. Currently; there exists no simple wayfor researchers; journalists and interested people to compare statistical data retrieved fromdifferent data stores on the Web. Given that the RDF Data Cube vocabulary is used todescribe statistical data; its use makes it possible to discover and identify statistical dataartifacts in a uniform way. In this article; the design and implementation of an application andservice is presented; which utilizes federated SPARQL queries to gather statistical data fromdistributed data stores. The R language for statistical computing is employed to performstatistical analyses and visualizations. The Shiny application and server bridges the front-end Web user interface with R on the server-side in order to compare statistical …,Proceedings of the 1st International Workshop on Semantic Statistics; Sydney; Australia,2013,4
Crowd-Sourcing the Large-Scale Semantic Mapping of Tabular Data,Ivan Ermilov; Sören Auer; Claus Stadler,ABSTRACT Governments and public administrations started recently to publish largeamounts of structured data on the Web; mostly in the form of tabular data such as CSV filesor Excel sheets. Various tools and projects have been launched aiming at facilitating thelifting of tabular data to reach semantically structured and linked data. However; none ofthese tools supported a truly incremental; pay-as-you-go data publication and mappingstrategy; which enables effort sharing between data owners; community experts andconsumers. In this article; we present an approach for enabling the crowd-sourcing of thelarge-scale semantic mapping of tabular data. We devise a simple mapping language fortabular data; which is easy to understand even for casual users; but expressive enough tocover the vast majority of potential tabular mappings use cases. Default mappings are …,Proceeding of the ACM Web Science,2013,4
Facilitation the publication of Open Governmental Data with the LOD2 Stack,Sören Auer; Michael Martin; Phillip Frischmuth; Bastiaan Deblieck,One of the biggest challenges in the area of intelligent information management is theexploitation of the Web as a platform for data and information integration as well as forsearch and querying. Just as we publish unstructured textual information on the Web asHTML pages and search such information by using keyword-based search engines; we willsoon be able to easily publish structured information; reliably interlink this information withother data published on the Web and search the resulting data space by using expressivequerying. The Linked Data paradigm has evolved as a powerful enabler for the transition ofthe current document-oriented Web into a Web of interlinked Data and; ultimately; into theSemantic Web. The term Linked Data here refers to a set of best practices for publishing andconnecting structured data on the Web. These best practices have been adopted by an …,Share-PSI. eu Workshop,2011,4
Learning semantic web technologies with the web-based SPARQLTrainer,Daniel Gerber; Marvin Frommhold; Michael Martin; Sebastian Tramp; Sören Auer,Abstract The success of the Semantic Web in research; technology and standardizationcommunities has resulted in a large variety of different approaches; standards andtechniques. This diversity and heterogeneity often involve an increasing difficulty ofbecoming acquainted with Semantic Web technologies. In this work; we present theSPARQLTrainer approach for educating novices in semantic technologies in a playful way.With SPARQLTrainer educators can devise a SPARQL course by defining a number ofexercises either generically or for a specific domain. Learners can complete courses bystepwise answering questions of increasing complexity. These questions usually require thelearner to build a SPARQL query for querying a certain knowledge base and using certainSPARQL features. The SPARQL queries created by a learner are compared with example …,Proceedings of the 6th International Conference on Semantic Systems,2010,4
Networked Knowledge-Networked Media:-Bringing the Pieces Together,Tassilo Pellegrini; Sören Auer; Sebastian Schaffert; Klaus Tochtermann,Abstract The book title Networked Knowledge-Networked Media reflects on the convergenceof Social Media and the Semantic Web. When these developments became popular a fewyears ago it was a simple co-existence between the two; but in the meantime they haveincreasingly melted making it impossible to think of knowledge technologies without thinkingof the Semantic Web.,*,2009,4
Semantisch unterstütztes Requirements Engineering,Thomas Riechert; Kim Lauenroth; Jens Lehmann,Abstract: Requirements Engineering ist eine erfolgsentscheidende Phase von Software-Entwicklungsprojekten; welche sich besonders dadurch auszeichnen; dass vieleverschiedene Stakeholder gemeinsam Ziele; Szenarien und Anforderungen für dasgeplante System erheben. Neben der Entwicklung eines Vorgehens für ein semantischunterstütztes Requirements Engineering; wurde eine Ontologie zur Abbildunganforderungsrelevanter Information entwickelt. Diese wird zusammen mit einemWissensmodellierungs-Werkzeug; anhand eines realen Anwendungsfalls aus dem BereichE-Government; beschrieben.,Proceedings of the SABRE-07 SoftWiki Workshop; Leipzig/Germany,2007,4
Trying Not to Die Benchmarking: Orchestrating RDF and Graph Data Management Solution Benchmarks Using LITMUS,Harsh Thakkar; Yashwant Keswani; Mohnish Dubey; Jens Lehmann; Sören Auer,Abstract Knowledge graphs; usually modelled via RDF or property graphs; have gainedimportance over the past decade. In order to decide which Data Management Solution(DMS) performs best for specific query loads over a knowledge graph; it is required toperform benchmarks. Benchmarking is an extremely tedious task demanding repetitivemanual effort; therefore it is advantageous to automate the whole process. However; there iscurrently no benchmarking framework which supports benchmarking and comparing diverseDMSs for both RDF and property graph DMS. To this end; we introduce; the first workingprototype of; LITMUS which provides this functionality as well as fine-grained environmentconfiguration options; a comprehensive set of DMS and CPU-specific key performanceindicators and a quick analytical support via custom visualization (ie plots) for the …,Proceedings of the 13th International Conference on Semantic Systems,2017,3
QAestro–semantic-based composition of question answering pipelines,Kuldeep Singh; Ioanna Lytra; Maria-Esther Vidal; Dharmen Punjani; Harsh Thakkar; Christoph Lange; Sören Auer,Abstract The demand for interfaces that allow users to interact with computers in an intuitive;effective; and efficient way is increasing. Question Answering (QA) systems address thisneed by answering questions posed by humans using knowledge bases. In recent years;many QA systems and related components have been developed both by practitioners andthe research community. Since QA involves a vast number of (partially overlapping)subtasks; existing QA components can be combined in various ways to build tailored QAsystems that perform better in terms of scalability and accuracy in specific domains and usecases. However; to the best of our knowledge; no systematic way exists to formally describeand automatically compose such components. Thus; in this work; we introduce QAestro; aframework for semantically describing both QA components and developer requirements …,International Conference on Database and Expert Systems Applications,2017,3
Decentralised authoring; annotations and notifications for a read-write web with dokieli,Sarven Capadisli; Amy Guy; Ruben Verborgh; Christoph Lange; Sören Auer; Tim Berners-Lee,Abstract While the Web was designed as a decentralised environment; individual authorsstill lack the ability to conveniently author and publish documents; and to engage in socialinteractions with documents of others in a truly decentralised fashion. We present dokieli; afully decentralised; browser-based authoring and annotation platform with built-in support forsocial interactions; through which people retain ownership of and sovereignty over theirdata. The resulting “living” documents are interoperable and independent of dokieli sincethey follow standards and best practices; such as HTML+ RDFa for a fine-grained semanticstructure; Linked Data Platform for personal data storage; and Linked Data Notifications forupdates. This article describes dokieli's architecture and implementation; demonstratingadvanced document authoring and interaction without a single point of control. Such an …,International Conference on Web Engineering,2017,3
Alligator: a deductive approach for the integration of Industry 4.0 standards,Irlán Grangel-González; Diego Collarana; Lavdim Halilaj; Steffen Lohmann; Christoph Lange; María-Esther Vidal; Sören Auer,Abstract Industry 4.0 standards; such as AutomationML; are used to specify properties ofmechatronic elements in terms of views; such as electrical and mechanical views of a motorengine. These views have to be integrated in order to obtain a complete model of the artifact.Currently; the integration requires user knowledge to manually identify elements in the viewsthat refer to the same element in the integrated model. Existing approaches are not able toscale up to large models where a potentially large number of conflicts may exist across thedifferent views of an element. To overcome this limitation; we developed Alligator; adeductive rule-based system able to identify conflicts between AutomationML documents.We define a Datalog-based representation of the AutomationML input documents; and a setof rules for identifying conflicts. A deductive engine is used to resolve the conflicts; to …,European Knowledge Acquisition Workshop,2016,3
Monitoring and automating factories using semantic models,Niklas Petersen; Michael Galkin; Christoph Lange; Steffen Lohmann; Sören Auer,Abstract Keeping factories running at any time is a critical task for every manufacturingenterprise. Optimizing the flows of goods and services inside and between factories is achallenge that attracts much attention in research and business. The idea to fully describe afactory in a digital form to improve decision making is called a virtual factory. Whilepromising virtual factory frameworks have been proposed; their semantic models lack depthand suffer from limited expressiveness. We propose an enhanced semantic model of afactory; which enables views spanning from the high level of supply chains to the low level ofmachines on the shop floor. The model includes a mapping to relational productiondatabases to support federated queries on different legacy systems in use. We evaluate themodel in a production line use case; demonstrating that it can be used for typical factory …,Joint International Semantic Technology Conference,2016,3
Enterprise Knowledge Graphs: A Backbone of Linked Enterprise Data,Mikhail Galkin; Sören Auer; Simon Scerri,Semantic technologies in enterprises have recently received increasing attention from boththe research and industrial side. The concept of Linked Enterprise Data (LED) describes aframework to incorporate benefits of semantic technologies into enterprise IT environments.However; LED still remains an abstract idea lacking a point of origin; ie; station zero fromwhich it comes to existence. In this paper we argue and demonstrate that EnterpriseKnowledge Graphs (EKGs) might be considered as an embodiment of LED lifting corporateinformation management to a semantic level which ultimately allows for real artificialintelligence applications. By EKG we refer to a semantic network of concepts; properties;individuals and links representing and referencing foundational and domain knowledgerelevant for an enterprise. Although the concept of EKGs was not invented yesterday; both …,Web Intelligence (WI); 2016 IEEE/WIC/ACM International Conference on,2016,3
An RDF-based approach for implementing industry 4.0 components with administration shells,Irlán Grangel-González; Lavdim Halilaj; Sören Auer; Steffen Lohmann; Christoph Lange; Diego Collarana,Industry 4.0 is a global endeavor of automation and data exchange to create smart factoriesmaximizing production capabilities and allowing for new business models. The ReferenceArchitecture Model for Industry 4.0 (RAMI 4.0) describes the core aspects of Industry 4.0 anddefines Administration Shells as digital representations of Industry 4.0 components. In thispaper; we present an approach to model and implement Industry 4.0 components with theResource Description Framework (RDF). The approach addresses the challenges ofinteroperable communication and machine comprehension in Industry 4.0 settings usingsemantic technologies. We show how related standards and vocabularies; such as IEC62264; eCl@ ss; and the Ontology of Units of Measure (OM); can be utilized along with theRDF-based representation of the RAMI 4.0 concepts. Finally; we demonstrate the …,Emerging Technologies and Factory Automation (ETFA); 2016 IEEE 21st International Conference on,2016,3
Minimally invasive semantification of light weight service descriptions,Fathoni A Musyaffa; Lavdim Halilaj; Ronald Siebes; Fabrizio Orlandi; Sören Auer,Unification and automation of RESTful web services' documentation and descriptions iscurrently receiving increasing attention. The open-source OpenAPI Specification (formerlyknown as Swagger) has become core of this effort and has been adopted by a number ofmajor companies. It allows the description of RESTful web services using objectsrepresented in JSON or YAML file formats. As a result; the created descriptions are humanand machine-readable; but not machine-understandable. In this paper; we propose anonintrusive approach for the addition of semantic annotations (similar to RDFa and JSON-LD for HTML) to specific fields of the OpenAPI Specification. We created a lightweightvocabulary for describing RESTful web services using this specification. Furthermore; wepractically demonstrate how OpenAPI objects can be enriched with semantic descriptions …,Web Services (ICWS); 2016 IEEE International Conference on,2016,3
The Semantic Web. Latest Advances and New Domains: 13th International Conference; ESWC 2016; Heraklion; Crete; Greece; May 29--June 2; 2016; Proceedings,Harald Sack; Eva Blomqvist; Mathieu d'Aquin; Chiara Ghidini; Simone Paolo Ponzetto; Christoph Lange,The 47 revised full papers presented together with three invited talks were carefullyreviewed and selected from 204 submissions. This program was completed by ademonstration and poster session; in which researchers had the chance to present theirlatest results and advances in the form of live demos. In addition; the PhD Symposiumprogram included 10 contributions; selected out of 21 submissions. The core tracks of theresearch conference were complemented with new tracks focusing on linked data; machinelearning; mobile web; sensors and semantic streams; natural language processing andinformation retrieval; reasoning; semantic data management; big data; and scalability;services; APIs; processes and cloud computing; smart cities; urban and geospatial data; trustand privacy; and vocabularies; schemas; and ontologies.,*,2016,3
Integration strategies for enterprise knowledge graphs,Mikhail Galkin; Sören Auer; Haklae Kim; Simon Scerri,Semantic computing and enterprise Linked Data have recently gained traction inenterprises. Although the concept of Enterprise Knowledge Graphs (EKGs) has meanwhilereceived some attention; a formal conceptual framework for designing such graphs has notyet been developed. By EKG we refer to a semantic network of concepts; properties;individuals and links representing and referencing foundational and domain knowledgerelevant for an enterprise. Through the efforts reported in this paper; we aim to bridge thegap between the increasing need for EKGs and the lack of formal methods for realisingthem. We present a thorough study of the key concepts of knowledge graphs design alongwith an analysis of the advantages and disadvantages of various design decisions. Inparticular; we distinguish between two polar approaches towards data fusion; ie; the …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,3
Measuring the quality of relational-to-RDF mappings,Darya Tarasowa; Christoph Lange; Sören Auer,Abstract Mapping from relational data to Linked Data (RDB2RDF) is an essentialprerequisite for evolving the World Wide Web into the Web of Data. We propose amethodology to evaluate the quality of such mappings against a set of objective metrics. Ourmethodology; whose key principles have been approved by a survey among RDB2RDFexperts; can be applied to evaluate both automatically and manually performed mappingsregardless of their representation. The main contributions of the paper are:(1) assessing thequality requirements for mappings between relational databases and RDF; and (2)proposing methods for measuring how well a given mapping meets the requirements. Weshowcase the usage of the individual metrics with illustrative examples from a real-lifeapplication. Additionally; we provide guidelines to improve a given mapping with regard …,International Conference on Knowledge Engineering and the Semantic Web,2015,3
Balanced Scoring Method for Multiple-mark Questions.,Darya Tarasowa; Sören Auer,*,CSEDU,2013,3
Lodstats–large scale dataset analytics for linked open data,Ivan Ermilov; Jan Demter; Michael Martin; Jens Lehmann; Sören Auer,Abstract. In order to reuse; link; revise or query data made available on the Web; it isimportant to know the structure; size and coverage of it. To achieve this; we developed andevaluated LODStats–a statementstream-based approach for gathering comprehensivestatistics about data adhering to the RDF data model. LODStats is based on the declarativedescription of statistical dataset characteristics. Its main advantages over relatedapproaches are a smaller memory footprint and significantly better performance andscalability. We integrated LODStats with CKAN and obtained a comprehensive picture of thecurrent state of a significant part of the Data Web. This analysis is regularly published andenhanced over the past two years at the public platform stats. lod2. eu.,Under review in ISWC,2013,3
Keyword-driven resource disambiguation over rdf knowledge bases,Saeedeh Shekarpour; Axel-Cyrille Ngonga Ngomo; Sören Auer,Abstract Keyword search is the most popular way to access information. In this paper weintroduce a novel approach for determining the correct resources for user-supplied queriesbased on a hidden Markov model. In our approach the user-supplied query is modeled asthe observed data and the background knowledge is used for parameter estimation. Weleverage the semantic relationships between resources for computing the parameterestimations. In this approach; query segmentation and resource disambiguation aremutually tightly interwoven. First; an initial set of potential segments is obtained leveragingthe underlying knowledge base; then; the final correct set of segments is determined afterthe most likely resource mapping was computed. While linguistic analysis (eg named entity;multi-word unit recognition and POS-tagging) fail in the case of keyword-based queries …,Joint International Semantic Technology Conference,2012,3
The emerging web of linked data,Sören Auer,Abstract Over the past 4 years; the semantic web activity has gained momentum with thewidespread publishing of structured data as RDF. The Linked Data paradigm has thereforeevolved from a practical research idea into a very promising candidate for addressing one ofthe biggest challenges in the area of the Semantic Web vision: the exploitation of the Web asa platform for data and information integration. To translate this initial success into a world-scale reality; a number of research challenges need to be addressed. While manystandards; methods and technologies developed within the Semantic Web activity areapplicable for Linked Data; there are also a number of specific characteristics of LinkedData; which have to be considered. In this talk we present an overview of the Linked Datalife-cycle and discuss some promising approaches with regard to extraction; storage and …,Proceedings of the 2011 International Conference on Intelligent Semantic Web-Services and Applications,2011,3
Categorisation of Semantic Web Applications,Michael Martin; Sören Auer,Abstract—The recent success of the Semantic Web in research; technology andstandardisation communities has also resulted in a large variety of different standards;technologies and tools. This diversity and heterogeneity goes along with an increasingcomplexity in assessing; evaluating; selecting and combining different approaches for thedevelopment of Semantic Web Applications (SWA). With this work we aim at lowering theentrance barrier for the development and engineering of Semantic Web Applications bypresenting a classification of SWAs according to the dimensions semantic technology depth;information flow direction; richness of knowledge representation; semantic integration anduser involvement. This categorisation helps to establish and consolidate theconceptualisation with regard to the engineering of SWAs and facilitates the comparability …,Proceedings of the 4th International Conference on Advances in Semantic Processing (SEMAPRO2010); October,2010,3
Evaluating the disparity between active areas of biomedical research and the global burden of disease employing Linked Data and data-driven discovery,Amrapali Zaveri; Ricardo Pietrobon; Timofey Ermilov; Michael Martin; Norman Heino; Soren Auer,ABSTRACT Although biomedical research has brought substantial benefit to people all overthe world; by dramatically improving their life expectancy and the quality of life; thedistribution of this benefit is not equitable. An important contributor to this is the currentabsence of accurate; interlinked data and information that enables a precise description ofthe degree of inequality between current efforts in biomedical research and global healthcare needs. In this position paper we present an approach for evaluating this disparity;which involves converting and inter-linking relevant datasets into Linked Data; andanalyzing them to represent the disparity as a visual map. We identify different data sets;relevant for answering the research question. Since bio-medical statistical data is ofparamount importance in this data integration project; we describe a tool and …,Tuberculosis IMISE Repo,2010,3
RapidOWL: A Methodology for Enabling Social Semantic Collaboration,Sören Auer,Abstract In this chapter we give a brief overview on the recently emerging concepts of SocialSoftware and Web 2.0. Both concepts stress the adaptive; agile methodological character ofcommunication and collaboration. In order to lift the adaptive collaboration andcommunication patterns of Social Software and the Web 2.0 towards a truly semanticcollaboration; we outline an adaptive knowledge engineering methodology–RapidOWL. It isinspired by adaptive software development methodologies from software engineering andemphasises support for small enduser contributions to knowledge bases.,*,2010,3
Networked knowledge-networked media: integrating knowledge management; new media technologies and semantic systems,Tassilo Pellegrini; Sören Auer; Klaus Tochtermann; Sebastian Schaffert,Studies in Computational Intelligence; Volume 221 Editor-in-Chief Prof. Janusz Kacprzyk SystemsResearch Institute Polish Academy of Sciences ul. Newelska 6 01-447 Warsaw PolandE-mail: kacprzyk@ ibspan. waw. pl Further volumes of this series can be found on ourhomepage: springer. com Vol. 200. Dimitri Plemenos and Georgios Miaoulis Visual Complexityand Intelligent Computer Graphics Techniques Enhancements; 2009 ISBN 978-3-642-01258-7 Vol. 201. Aboul-Ella Hassanien; Ajith Abraham; Athanasios V. Vasilakos; and Witold Pedrycz(Eds.) Foundations of Computational Intelligence Volume 1; 2009 ISBN 978-3-642-01081-1Vol. 202. Aboul-Ella Hassanien; Ajith Abraham; and Francisco Herrera (Eds.) Foundations ofComputational Intelligence Volume 2; 2009 ISBN 978-3-642-01532-8 Vol. 203. AjithAbraham; Aboul-Ella Hassanien; Patrick Siarry; and Andries Engelbrecht (Eds.) …,*,2009,3
Bridging the semantic gap between business processes and semantic web services,Muhammad Ahtisham Aslam; Sören Auer; Jun Shen; Klaus-Peter Fähnrich,Bridging the semantic gap between business process models and semantic Web servicesbecomes increasingly important in order to help automating business process integration inlarge organizations. Traditional workflow languages (such as BPEL4WS) support themodeling of business processes as syntax based compositions of Web services. When suchprocesses are exported as Web services they as well expose syntactical interfaces. Thesesyntactical interfaces allow only static composition and hence limit interactions betweenbusiness partners. The obstacles of syntax based integration and composition can beaddressed by enhancing business processes with semantics. This enables us to 1) edit andmodel the compositions of Web services on the basis of matching semantics 2) providesemantically enriched descriptions of business processes. In particular; it will support the …,網際網路技術學刊,2007,3
Product Models in Service Engineering,Klaus-Peter Fähnrich; Sören Auer,*,Proceedings der 17th International Conference on Production and Research,2003,3
Scripting and Development for the Semantic Web (SFSW),Chris Bizer; Sören Auer; Gunnar AAstrand Grimnes,*,CEUR Workshop Proceedings. May.: http://CEUR-WS. org,*,3
Facilitating data-flows at a global publisher using the LOD2 stack,Christian Dirschl; Katja Eck; Jens Lehmann; Lorenz Bühmann; Sören Auer,Abstract. The publishing industry is at the verge of an era; wherein particular professionalcustomers of publishing products are not so much interested in comprehensive books andjournals; ie traditional publishing products; anymore as they now are interested in possiblystructured information pieces delivered just-in-time as a certain information need arises. Thisrequires a transformation of the publishing workflows towards the production of much richermeta-data for fine-grained and highly interlinked pieces of content. Linked Data can play acrucial role in this transition. The LOD2 Stack is an integrated distribution of aligned toolswhich support the whole lifecycle of Linked Data from extraction; authoring/creation viaenrichment; interlinking; fusing to maintenance. In this application paper; we describe a real-world usage scenario of the LOD2 stack at a global publishing company. We give an …,submitted to the Semantic Web journal,*,3
Semantic data integration for knowledge graph construction at query time,Diego Collarana; Mikhail Galkin; Ignacio Traverso-Ribón; Christoph Lange; Maria-Esther Vidal; Sören Auer,The evolution of the Web of documents into a Web of services and data has resulted in anincreased availability of data from almost any domain. For example; general domainknowledge bases such as DBpedia or Wikidata; or domain specific Web sources like theOxford Art archive; allow for accessing knowledge about a wide variety of entities includingpeople; organizations; or art paintings. However; these data sources publish data in differentways; and they may be equipped with different search capabilities; eg; SPARQL endpointsor REST services; thus requiring data integration techniques that provide a unified view ofthe published data. We devise a semantic data integration approach named FuhSen thatexploits keyword and structured search capabilities of Web data sources and generates on-demand knowledge graphs merging data collected from available Web sources …,Semantic Computing (ICSC); 2017 IEEE 11th International Conference on,2017,2
Proactive Prevention of False-Positive Conflicts in Distributed Ontology Development.,Lavdim Halilaj; Irlán Grangel-González; Maria-Esther Vidal; Steffen Lohmann; Sören Auer,Abstract: A Version Control System (VCS) is usually required for successful ontologydevelopment in distributed settings. VCSs enable the tracking and propagation of ontologychanges; as well as collecting metadata to describe changes; eg; who made a change atwhich point in time. Modern VCSs implement an optimistic approach that allows forsimultaneous changes of the same artifact and provides mechanisms for automatic as wellas manual conflict resolution. However; different ontology development tools serialize theontology artifacts in different ways. As a consequence; existing VCSs may identify a hugenumber of false-positive conflicts during the merging process; ie; conflicts that do not resultfrom ontology changes but the fact that two ontology versions are differently serialized.Following the principle of prevention is better than cure; we designed SerVCS; an …,KEOD,2016,2
A Preliminary Investigation Towards Improving Linked Data Quality Using Distance-Based Outlier Detection,Jeremy Debattista; Christoph Lange; Sören Auer,Abstract With more and more data being published on the Web as Linked Data; Web Dataquality is becoming increasingly important. While quite some work has been done withregard to quality assessment of Linked Data; only few works have addressed qualityimprovement. In this article; we present a preliminary an approach for identifying potentiallyincorrect RDF statements using distance-based outlier detection. Our method follows a threestage approach; which automates the whole process of finding potentially incorrectstatements for a certain property. Our preliminary evaluation shows that a high precision ismaintained with different settings.,Joint International Semantic Technology Conference,2016,2
Towards an Ontology-based Representation of Accessibility Profiles for Learners.,Mirette Elias; Steffen Lohmann; Sören Auer,Abstract. Web accessibility has gained significant attention over the past decades due to thewidespread use of the internet; which has urged web developers to address the needs andpreferences of a variety of users. In e-learning contexts; learner profiles can be used todescribe the needs and preferences of users and adapt the educational resourcesaccordingly. We propose the use of ontologies to represent accessibility needs andpreferences in learner profiles in order to structure the knowledge and to access theinformation for recommendations and adaptations in OpenCourseWare systems. Inparticular; we propose to use and extend the ACCESSIBLE ontology containing knowledgeabout disabilities and web accessibility standards. In this work; we extend the ACCESSIBLEontology to represent accessibility knowledge and requirements for learning contexts with …,EKM@ EKAW,2016,2
fostering accessibility of OpenCourseWare with semantic technologies–a literature review,Mirette Elias; Steffen Lohmann; Sören Auer,Abstract Accessibility has become a fundamental requirement for web applications;especially when it comes to e-learning and educational websites for OpenCourseWare.There are various types of disabilities and numerous ways of addressing them. Usingsemantic technologies to structure and represent the available concepts and taxonomiesenables sharing and reusing the knowledge in a variety of systems. This paper provides aliterature review of standards and ontologies that were developed to address accessibilityrequirements. The findings and recommendations reveal missing and future needs forbuilding accessible OpenCourseWare services.,International Conference on Knowledge Engineering and the Semantic Web,2016,2
Co-evolution of RDF Datasets,Sidra Faisal; Kemele M Endris; Saeedeh Shekarpour; Sören Auer; Maria-Esther Vidal,Abstract Linking Data initiatives have fostered the publication of large number of RDFdatasets in the Linked Open Data (LOD) cloud; as well as the development of queryprocessing infrastructures to access these data in a federated fashion. However; differentexperimental studies have shown that availability of LOD datasets cannot be alwaysensured; being RDF data replication required for envisioning reliable federated queryframeworks. Albeit enhancing data availability; RDF data replication requiressynchronization and conflict resolution when replicas and source datasets are allowed tochange data over time; ie; co-evolution management needs to be provided to ensureconsistency. In this paper; we tackle the problem of RDF data co-evolution and devise anapproach for conflict resolution during co-evolution of RDF datasets. Our proposed …,International Conference on Web Engineering,2016,2
LDOW2016: 9th Workshop on Linked Data on the Web,Sören Auer; Tom Heath; Christian Bizer; Tim Berners-Lee,Abstract The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal;Quebec; Canada on April 12; 2016 and co-located with the 25rd International World WideWeb Conference (WWW2016). The Web is developing from a medium for publishing textualdocuments into a medium for sharing structured data. This trend is fueled on the one handby the adoption of the Linked Data principles by a growing number of data providers. On theother hand; large numbers of websites have started to semantically mark up the content oftheir HTML pages and thus also contribute to the wealth of structured data available on theWeb. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and furtherresearch into the challenges of publishing; consuming; and integrating structured data fromthe Web as well as mining knowledge from the global Web of Data.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,2
SCORVoc: vocabulary-based information integration and exchange in supply networks,Niklas Petersen; Irlán Grangel-González; Gökhan Coskun; Sören Auer; Marvin Frommhold; Sebastian Tramp; Maxime Lefrançois; Antoine Zimmermann,Advanced; highly specialized economies require instant; robust and efficient informationflows within its value-added and Supply Chain networks. Especially also in the context of therecent Industry 4.0; smart manufacturing or cyber-physical systems initiatives more efficientand effective information exchange in supply networks is of paramount importance. TheSupply Chain Operation Reference (SCOR) is a cross-industry approach to lay thegroundwork for this goal by defining a conceptual model for Supply Chain relatedinformation. Semantics-based approaches could facilitate information flows in supplynetworks; and enable to analyze; monitor and optimize Supply Chains (in particular forrobustness). This paper first reviews existing formalizations of the Supply Chain Council'sSCOR standard. It then introduces the SCORVoc RDFS vocabulary which fully formalizes …,Semantic Computing (ICSC); 2016 IEEE Tenth International Conference on,2016,2
Ontology-based Skills Demand and Trend Analysis,Elisa Margareth Sibarani; Simon Scerri; Najmeh Mousavi Nejad; Afshin Sadeghi; Sören Auer,Abstract. Recognizing the importance of a qualified labor force has triggered the need toanalyzing labor demand. We present an approach for identifying and quantifying the skilldemands across industry by analysing online job adverts using an Ontology-basedInformation Extraction (OBIE) method. An ontology called SARO (Skills and RecruitmentOntology) has been developed to capture important terms and relationships to facilitate theskills analysis. In the extraction and annotation; we focus on job posting attributes and jobspecific skills (Tool; Product; Topic). We present the TOBIE system where text processing isdecoupled in two phases:(1) a customized-pipeline for extracting information whose resultsare integrated into an RDF knowledge-base; and (2) a visualization for skills co-occurrences. Using a gold standard comprising a manually-annotated corpus of job …,*,2016,2
Collaborative authoring of OpenCourseWare: the best practices and complex solution,Darya Tarasowa; Sören Auer,Abstract The lack of high-quality educational resources is a key issue on the way to successof the OpenCourseWare movement. However; the state-of-art approaches of producing suchcontent demand a lot of resources; thus limiting the percent of such courses in a total amountof content available. An important step towards decreasing costs while increasing the qualityof the educational material is applying collaborative techniques to the production process.Such collaboration affects different aspects of OpenCourseWare production; such as:content annotation; personalization; sharing and other. In the current paper we aim toinvestigate the state-of-art of the collaborative authoring of OpenCourseWare in all itsaspects; finding out the major gaps and the most promising approaches to fulfill them. Basedon the study results we developed Slidewiki-an example application for the …,*,2016,2
The WDAqua ITN: Answering Questions using Web Data,Christoph Lange; Saeedeh Shekarpour; Soren Auer,Abstract: WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded underEU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims atadvancing the state of the art by intertwining training; research and innovation efforts;centered around one service: data-driven question answering. Question answering isimmediately useful to a wide audience of end users; and we will demonstrate this in settingsincluding e-commerce; public sector information; publishing and smart cities. Questionanswering also covers web science and data science broadly; leading to transferrableresearch results and to transferrable skills of the researchers who have finished our trainingprogramme. To ensure that our research improves question answering overall; everyindividual research project connects at least two of these steps. Intersectional …,arXiv preprint arXiv:1506.04094,2015,2
This ‘paper’is a demo,Sarven Capadisli; Sören Auer; Reinhard Riedl,Abstract This 'paper'; when viewed on the Web; is the demo itself; since the interactive andsemantic features can be directly observed while reading and consuming. The demoshowcases; how scholarly communication can adapt to the audience; whether the content isread on a screen or printed on paper; listen with a screen reader; watched as a movie;shown as a presentation; or even interacted with in the document. To experience thedescribed features please open this document in your Web browser under its canonical URI:http://csarven. ca/this-paper-is-a-demo.,International Semantic Web Conference,2015,2
From Symptoms to Diseases–Creating the Missing Link,Heiner Oberkampf; Turan Gojayev; Sonja Zillner; Dietlind Zühlke; Sören Auer; Matthias Hammon,Abstract A wealth of biomedical datasets is meanwhile published as Linked Open Data.Each of these datasets has a particular focus; such as providing information on diseases orsymptoms of a certain kind. Hence; a comprehensive view can only be provided byintegrating information from various datasets. Although; links between diseases andsymptoms can be found; these links are far too sparse to enable practical applications suchas a disease-centric access to clinical reports that are annotated with symptom information.For this purpose; we build a model of disease-symptom relations. Utilizing existing ontologymappings; we propagate semantic type information for disease and symptom acrossontologies. Then entities of the same semantic type from different ontologies are clusteredand object properties between entities are mapped to cluster-level relations. The …,European Semantic Web Conference,2015,2
VoCol: An Integrated Environment to Support Collaborative Vocabulary Development with Version Control Systems,Lavdim Halilaj; Niklas Petersen; Irlán Grangel-González; Christoph Lange; Sören Auer; Gökhan Coskun; Steffen Lohmann,*,20th International Conference on Knowledge Engineering and Knowledge Management (EKAW’16); to appear,2015,2
ExConQuer Framework-Softening RDF Data to Enhance Linked Data Reuse.,Judie Attard; Fabrizio Orlandi; Sören Auer,Abstract. Efforts towards the wider adoption of Linked Open Data; including theimplementation of Linked Data principles and the consumption of Linked Data; are evidentin the existing literature and available tools. Yet; these efforts rarely cater for stakeholderswho are not familiar with RDF or SPARQL. Hence; we propose the ExConQuer Framework;which facilitates the publication and consumption of RDF in a variety of generic formats. Inthis manner; any stakeholder can export and work with RDF data in the formats they aremost accustomed with; thus lowering the entry barrier to the use of semantic technologies;and possibly enabling the exploitation of Linked Open Data to its full potential.,International Semantic Web Conference (Posters & Demos),2015,2
Towards web intelligence through the crowdsourcing of semantics,Sören Auer; Dimitris Kontokostas,Sören Auer EIS; University of Bonn and Fraunhofer IAIS Römerstrasse 164; 53117 Bonn; Germanyauer@cs.uni-bonn.de … Dimitris Kontokostas AKSW; Computer Science; University of LeipzigAugustusplatz 10-11; 04009 Leipzig; Germany kontokostas@informatik.uni-leipzig.de … ABSTRACTA key success factor for the Web as a whole was and is its participatory nature. We discuss strategiesfor engaging human-intelligence to make the Web more semantic … Categories and SubjectDescriptors H.1.2 [Information Systems]: User/Machine Systems— Human factors;Human informationprocessing … 1. TALK SUMMARY A key success factor for the Web as a whole was and is itsparticipatory nature – in a truly distributed and democratic nature everybody can publish informationon the Web and interlink it with related content. In order to make the Web more intelligent; wehave to empower people to easily create and interlink structured information and …,Proceedings of the 23rd International Conference on World Wide Web,2014,2
Linked Open Data--Creating Knowledge Out of Interlinked Data: Results of the LOD2 Project,Volha Bryl; Sören Auer; Sebastian Tramp,*,*,2014,2
Linked Data in Enterprise Integration,Sören Auer; P Frischmuth; J Klimek,CONTENTS Introduction......................................................................................................... 169Challenges in Data Integration for Large Enterprises................................... 173 Linked DataParadigm for Integrating Enterprise Data................................ 178 RuntimeComplexity.......................................................................................... 180 Preliminaries................................................................................................... 181 The HR3Algorithm........................................................................................ 183 Indexing Scheme........................................................................................ 183 Approach.................................................................................................... 184Evaluation....................................................................................................... 187 ExperimentalSetup................................................................................... 187 Results......................................................................................................... 188 Discrepancy …,Big Data Computing,2013,2
Enabling linked data access to the internet of things,Timofey Ermilov; Sören Auer,Abstract The term Internet of Things refers to the vision; that all kinds of physical objects areuniquely identifiable and have a virtual representation on the Internet. We present anapproach for equipping embedded and smart devices on the Internet of Things with a LinkedData interface. The approach is based on mapping existing structured data on the device tovocabularies and ontologies and exposing this information as dereferencable RDF directlyfrom within the device. As a result; all smart devices (eg tablets; smartphones; TVs) caneasily provide standardized structured information and become first class citizens on theData Web. A particular specific requirement when dealing with smart and embeddeddevices is resource constraints. Our evaluation shows; that the overhead introduced byequipping a device with a Linked Data interface is neglectable given modern software …,Proceedings of International Conference on Information Integration and Web-based Applications & Services,2013,2
Towards an efficient RDF dataset slicing,Edgard Marx; Tommaso Soru; Saeedeh Shekarpour; Sören Auer; Axel-Cyrille Ngonga Ngomo; Karin Breitman,Over the last years; a considerable amount of structured data has been published on theWeb as Linked Open Data (LOD). Despite recent advances; consuming and using LinkedOpen Data within an organization is still a substantial challenge. Many of the LOD datasetsare quite large and despite progress in Resource Description Framework (RDF) datamanagement their loading and querying within a triple store is extremely time-consumingand resource-demanding. To overcome this consumption obstacle; we propose a processinspired by the classical Extract-Transform-Load (ETL) paradigm. In this article; we focusparticularly on the selection and extraction steps of this process. We devise a fragment ofSPARQL Protocol and RDF Query Language (SPARQL) dubbed SliceSPARQL; whichenables the selection of well-defined slices of datasets fulfilling typical information needs …,International Journal of Semantic Computing,2013,2
Test-driven data quality evaluation for sparql endpoints,Dimitris Kontokostas; Sören Auer; Sebastian Hellmann; Jens Lehmann; Patrick Westphal; Roland Cornelissen; Amrapali Zaveri,Abstract. Linked Open Data (LOD) comprises of an unprecedented volume of structured dataon the Web. However; these datasets are of varying quality ranging from extensively curateddatasets to crowdsourced or extracted data of often relatively low quality. In this paper we;present a methodology for test-driven data quality assessment; which is inspired by test-driven software development. We argue; that knowledge bases should be accompanied bya number of test-cases; which help to ensure a basic level of quality. We present amethodology for assessing the quality of linked data resources; based on a formalization ofbad smells and data quality problems. Our formalization employs SPARQL query templates;which are instantiated into concrete quality test queries. Based on an extensive literaturereview; we compile a comprehensive library of quality test patterns. The main contribution …,Submitted to 12th International Semantic Web Conference,2013,2
Towards facilitating scientific publishing and knowledge exchange through linked data,Sören Auer; Christoph Lange; Timofey Ermilov,Scientific knowledge exchange (cf. Fig. 1) often involves structured information; such as experimentalresults; collected data; taxonomies or formulas. Data portals can be used to publish data underlyinga certain publication. However; even the actual text of scientific publications often contains structuredinformation currently hidden in prose. Examples include (a) claims and supporting evidencefor these; (b) related approaches with their advantages and disadvantages; or (c) a taxonomicalclassification of the approach described in a certain publication. Such information could easilybe expressed and represented in a structured way in RDF. Once scientific publications are increasinglyrepresented in a way that preserves the structure of information; related or similar informationfrom different publications can easily be interlinked and integrated. A survey on a certain researcharea; for example; could then possibly be generated almost automatically; by collecting …,International Conference on Theory and Practice of Digital Libraries,2013,2
The digital agenda scoreboard: A statistical anatomy of europe’s way into the information age,Michael Martin; Bert van Nuffelen; Stefano Abruzzini; Sören Auer,Abstract. Evidence-based policy is policy informed by rigorously established objectiveevidence. An important aspect of evidence-based policy is the use of scientifically rigorousstudies to identify programs and practices capable of improving policy relevant outcomes.Statistics represent a crucial means to determine whether progress is made towards policytargets. In May 2010; the European Commission adopted the Digital Agenda for Europe; astrategy to take advantage of the potential offered by the rapid progress of digitaltechnologies. The Digital Agenda contains commitments to undertake a number of specificpolicy actions intended to stimulate a circle of investment in and usage of digitaltechnologies. It identifies 13 key performance targets. In order to chart the progress of boththe announced policy actions and the key performance targets a scoreboard is published …,Semantic Web Jorunal,2012,2
LinkedGeoData,Sören Auer; Jens Lehmann; Claus Stadler,*,Agile Knowledge Engineering and Semantic Web (AKSW); University of Leipzig; accessed August,2011,2
Rdface: The rdfa content editor,Ali Khalili; Sören Auer,Recently practical approaches for managing and supporting the life-cycle of semanticcontent on the Web of Data made quite some progress. However; the currently leastdeveloped aspect of the semantic content life-cycle is the userfriendly manual and semi-automatic creation of rich semantic content. In this paper we present the RDFaCE (RDFaContent Editor) approach for combining WYSIWYG text authoring with the creation of richsemantic annotations. WYSIWYG text authoring is meanwhile ubiquitous on the Web andpart of most content creation and management workflows. It is part of Content ManagementSystems; Weblogs; Wikis; fora; product data management systems and online shops; just tomention a few. Our goal with this work is to integrate the semantic annotation directly into thecontent creation process and to make the annotation as easy and non-intrusive as …,*,2011,2
Managing Multimodal and Multilingual Semantic Content.,Michael Martin; Daniel Gerber; Norman Heino; Sören Auer; Timofey Ermilov,Abstract: With the advent and increasing popularity of Semantic Wikis and the Linked Datathe management of semantically represented knowledge became mainstream. However;certain categories of semantically enriched content; such as multimodal documents as wellas multilingual textual resources are still difficult to handle. In this paper; we present acomprehensive strategy for managing the life-cycle of both multimodal and multilingualsemantically enriched content. The strategy is based on extending a number of semanticknowledge management techniques such as authoring; versioning; evolution; access andexploration for semantically enriched multimodal and multilingual content. We showcase animplementation and user interface based on the semantic wiki paradigm and present a usecase from the e-tourism domain.,WEBIST,2011,2
Towards Bottom-Up; Stakeholder-Driven Research Funding–Open Science and Open Peer Review,Sören Auer; Holger Braun-Thürmann,Abstract: The current practices of research funding do not yet use means of communicationand collaboration of the Internet age effectively. Combined with a number of information flowbarriers associated with research funding this results in inefficiencies and intransparencies.We present a vision how an open science platform for research funding and cross-fertilization could be realized. It is based on stake-holder involvement and community self-organisation. We identify problems which have to be solved in order to realize this visionand present with Cofundos a concept and portal which already implements a simplifiedversion of the envisioned open science model for the pooling of ideas and resourcesregarding open source software development.,Informatik. uni-Leipzig. de,2011,2
Towards creating knowledge out of interlinked data,Sören Auer,Abstract. Over the past 3 years; the semantic web activity has gained momentum with thewidespread publishing of structured data as RDF. The Linked Data paradigm has thereforeevolved from a practical research idea into a very promising candidate for addressing one ofthe biggest challenges in the area of the Semantic Web vision: the exploitation of the Web asa platform for data and information integration. To translate this initial success into a world-scale reality; a number of research challenges need to be addressed: the performance gapbetween relational and RDF data management has to be closed; coherence and quality ofdata published on the Web have to be improved; provenance and trust on the Linked DataWeb must be established and generally the entrance barrier for data publishers and usershas to be lowered. In this vision statement we discuss these challenges and argue; that …,Semantic Web–Interoperability; Usability; Applicability,2010,2
DBpedia—querying wikipedia like a database and an interlinking-hub in the web of data,Chris Bizer; Sören Auer; Georgi Kobilarov; Jens Lehmann; Christian Becker; Sebastian Hellmann,*,Querying Wikipedia Like a Database (4/4/2009) FU Berlin; Universitt Leipzig,2009,2
LinkedGeoData–collaboratively created geo-information for the semantic web,Sören Auer; Jens Lehmann,Abstract. In this Semantic Web Challenge submission we present Linked-GeoData; a projectwhich transforms data collected by the OpenStreetMap project into RDF; enriches this datawith a light-weight ontology; establishes links to other entities on the Data Web and providesa browser for exploration and authoring of the rich spatial data collection. The amount ofdata contributed by LinkedGeoData to the Data Web reaches more than 3 billion RDFtriples. It contains descriptions of entities with a spatial location such as roads; buildings;shops; pubs; mailboxes. Most of these entities were previously not available on the DataWeb. For the ones available; we applied automatic interlinking techniques to DBpedia andthus other LOD datasets with a spatial dimension. In order to explore this wealth ofinformation we implemented a tool; which combines the faceted-browsing approach with …,Semantic Web Challenge; ISWC,2009,2
Semantische Mashups auf Basis Vernetzter Daten,Sören Auer; Jens Lehmann; Christian Bizer,Zusammenfassung Semantische Mashups sind Anwendungen; die vernetzte Daten ausmehreren Web-Datenquellen mittels standardisierter Datenformate undZugriffsmechanismen nutzen. Der Artikel gibt einen Überblick über die Idee und Motivationder Vernetzung von Daten. Es werden verschiedene Architekturen und Ansätze zurGenerierung von RDF-Daten aus bestehenden Web 2.0-Datenquellen; zur Vernetzung derextrahierten Daten sowie zur Veröffentlichung der Daten im Web anhand konkreterBeispiele diskutiert. Hierbei wird insbesondere auf Datenquellen; die aus sozialenInteraktionen hervorgegangen sind eingegangen. Anschließend wird ein Überblick überverschiedene; im Web frei zugängliche semantische Mashups gegeben und aufleichtgewichtige Inferenzansätze eingegangen; mittels derer sich die Funktionalität von …,Social Semantic Web,2009,2
Integrating sparql endpoints into directory services,Sebastian Dietzold; Sören Auer,We demonstrate1 the integration of RDF knowledge bases from our social; semanticcollaboration tool OntoWiki via SPARQL endpoints into directory services based on theLightweigth Directory Access Protocol (LDAP). In order to achieve this; we translate LDAPqueries into SPARQL queries and transform the incoming SPARQL results back into theLDAP data model. The transformation component is implemented as a backend for thewidely used OpenLDAP server2. LDAP based directory services are an important part in theIT infrastructure of most organisations and enterprises. They act as a central service forintegrating new applications into an IT infrastructure and can be accessed by many differenttypes of clients ranging from content management systems to personal email tools.,Proceedings,2007,2
What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content. accepted at ESWC 2007,Sören Auer; Jens Lehmann,*,*,2007,2
DBpedia: A nucleus for a web of open data. 6th Int. Semantic Web Conference; Busan,Sören Auer; Christian Bizer; Georgi Kobilarov; Jens Lehmann; R Cyganiak; Z Ives,*,Korea; Nov,2007,2
Querying Wikipedia like a Database,Chris Bizer; Sören Auer; Georgi Kobilarov; Jens Lehmann; Richard Cyganiak,● Give me all Sitcoms that are set in NYC?● All tennis players from Moscow?● All films byQuentin Tarentino?● All German musicians that were born in Berlin in the 19th century?●All soccer players with tricot number 11; playing for a club having a stadium with over 40;000seats and is born in a country with over 10 million inhabitants?,Developers track presentation. 16th International World Wide Web Conference. WWW2007,2007,2
The Social Semantic Web 2007,Sören Auer; Christian Bizer; Claudia Müller; Anna V Zhdanova,We are pleased to welcome you to the 1st Conference on Social Semantic Web (CSSW) andwish you a wonderful stay in Leipzig and an enjoyable and rewarding conferenceparticipation! The concept of Social Software was characterizes a variety of software andservices on the Web; which enable new ways of communication and social interaction forcreating large content bases from a multitude of user contributions. The Semantic Web is anextension of the current Web aiming at enhanced search and navigation facilities and atinformation integration from multiple sources.” How the different approaches of SocialSoftware and Semantic Web can be combined in a synergetic way?”-this question appearsmore and more often in current research and development. The aim of the Conference onSocial Semantic Web is to provide a podium for exploration of Social Software concepts …,Proceedings of the 1st Conference of Social Semantic Web. Bonn: Gesellschaft für Informatik,2007,2
Wiki-based knowledge engineering: second workshop on semantic Wikis,Max Völkel; Sebastian Schaffert; Elena Pasaru-Bontas; Sören Auer,ABSTRACT Wikis are collaborative environments for authoring Web content. This workshopexplores the role of semantic wikis in knowledge engineering. Semantic Wikis try to combinethe strengths of semantic (machine processable; data integration; complex queries) and Wiki(easy to use and contribute; strongly interconnected; collaboration) technologies.,Proceedings of the 2006 international symposium on Wikis,2006,2
Informatik-Kolloquium,Sören Auer,Abstract: In the Web age database technologies face the challenge to efficiently supportdata; information and knowledge integration on the Web. For example; databases have toefficiently handle information represented according to the RDF data model; deal with theimplicit semantics encoded in ontologies and be able to interlink local knowledge bases withWeb accessible remote ones. In this talk we will give an overview on research approachestackling these challenges (such as OWLDB and Triplify) and applications (such as DBpediaand Semantic Wikis) aiming at enriching the current Web of documents with a Web of LinkedData.,*,2006,2
Towards a semantic wiki experience–desktop integration and interactivity,David Aumueller; Sören Auer,*,WikSAR; Proc. of 1st Workshop on The Semantic Desktop; Galway; Ireland,2005,2
Knowledge Engineering for IT-based Services,Sören Auer; Patryk Burek; Tonio Grawe,Abstract: A formal product model contains all the information (structured and formalized) tosystematically reproduce a specific product (as economic asset). There are severalapproaches for formalizing product model information in the old economy (for exampleCAD/CAM-based product models in discrete parts manufacturing). The service sectorevolved to the most important sector in all developed economies. Knowledge plays a crucialrole for delivering many services. For complex; IT-based service products high in variants(such as insurances; IT outsourcing or public administration services) existing approachesare not suitable but formalization is desired (eg it allows easier rendering; export or trade ofsuch products). This paper elicits a possible strategy for defining formal product models forknowledge-based services using knowledge representation and semantic web …,Proceedings of the 4th International Conference on Knowledge Management (I-KNOW’04); Graz,2004,2
Connecting Crowdsourced Spatial Information to the Data Web with Sparqlify,Claus Stadler; Jörg Unbehauen; Jens Lehmann; Sören Auer,*,*,*,2
Linkedgeodata: A core for a web of spatial open data; 2011,Claus Stadler; Jens Lehmann; Konrad Höffner; Sören Auer,*,Under review; preliminary version at http://www. semantic-web-journal. net/sites/default/files/swj173_1. pdf,*,2
The RDFa content editor-from WYSIWYG to WYSIWYM (2011),Ali Khalili; Sören Auer,*,Universität Leipzig; Institut für Informatik; AKSW,*,2
A Stitch in Time Saves Nine--SPARQL querying of Property Graphs using Gremlin Traversals,Harsh Thakkar; Dharmen Punjani; Yashwant Keswani; Jens Lehmann; Sören Auer,Abstract: Knowledge graphs have become popular over the past decade and frequently relyon the Resource Description Framework (RDF) or Property Graph (PG) databases as datamodels. However; the query languages for these two data models--SPARQL for RDF andthe property graph traversal language Gremlin--are lacking interoperability. We presentGremlinator; a novel SPARQL to Gremlin translator. Gremlinator translates SPARQL queriesto Gremlin traversals for executing graph pattern matching queries over graph databases.This allows to access and query a wide variety of Graph Data Management Systems (DMSs)using the W3C standardized SPARQL and avoid the steep learning curve of a new GraphQuery Language (GQL). Gremlin is a graph computing system agnostic traversal language(covering both OLTP graph database or OLAP graph processors); making it a desirable …,arXiv preprint arXiv:1801.02911,2018,1
Semantic Zooming for Ontology Graph Visualizations,Vitalis Wiens; Steffen Lohmann; Sören Auer,Abstract Visualizations of ontologies; in particular graph visualizations in the form of node-link diagrams; are often used to support ontology development; exploration; verification; andsensemaking. With growing size and complexity of ontology graph visualizations; theirrepresented information tend to become hard to comprehend due to visual clutter andinformation overload. We present a new approach of semantic zooming for ontology graphvisualizations that abstracts and simplifies the underlying graph structure. It separates thecomprised information into three layers with discrete levels of detail. The approach isapplied to a force-directed graph layout using the VOWL notation. The mental map ispreserved by using smart expanding and ordering of elements in the layout. Navigation andsensemaking are supported by local and global exploration methods; halo visualization …,Proceedings of the Knowledge Capture Conference,2017,1
Capturing Knowledge in Semantically-typed Relational Patterns to Enhance Relation Linking,Kuldeep Singh; Isaiah Onando Mulang; Ioanna Lytra; Mohamad Yaser Jaradeh; Ahmad Sakor; Maria-Esther Vidal; Christoph Lange; Sören Auer,Abstract Transforming natural language questions into formal queries is an integral task inQuestion Answering (QA) systems. QA systems built on knowledge graphs like DBpedia;require a step after natural language processing for linking words; specifically includingnamed entities and relations; to their corresponding entities in a knowledge graph. Toachieve this task; several approaches rely on background knowledge bases containingsemantically-typed relations; eg; PATTY; for an extra disambiguation step. Two major factorsmay affect the performance of relation linking approaches whenever background knowledgebases are accessed: a) limited availability of such semantic knowledge sources; and b) lackof a systematic approach on how to maximize the benefits of the collected knowledge. Wetackle this problem and devise SIBKB; a semantic-based index able to capture …,Proceedings of the Knowledge Capture Conference,2017,1
Realizing an RDF-Based Information Model for a Manufacturing Company–A Case Study,Niklas Petersen; Lavdim Halilaj; Irlán Grangel-González; Steffen Lohmann; Christoph Lange; Sören Auer,Abstract The digitization of the industry requires information models describing assets andinformation sources of companies to enable the semantic integration and interoperableexchange of data. We report on a case study in which we realized such an informationmodel for a global manufacturing company using semantic technologies. The informationmodel is centered around machine data and describes all relevant assets; key terms andrelations in a structured way; making use of existing as well as newly developed RDFvocabularies. In addition; it comprises numerous RML mappings that link different datasources required for integrated data access and querying via SPARQL. The technicalinfrastructure and methodology used to develop and maintain the information model isbased on a Git repository and utilizes the development environment VoCol as well as the …,International Semantic Web Conference,2017,1
Towards a Knowledge Graph Representing Research Findings by Semantifying Survey Articles,Said Fathalla; Sahar Vahdati; Sören Auer; Christoph Lange,Abstract Despite significant advances in technology; the way how research is done andespecially communicated has not changed much. We have the vision that ultimatelyresearchers will work on a common knowledge base comprising comprehensivedescriptions of their research; thus making research contributions transparent andcomparable. The current approach for structuring; systematizing and comparing researchresults is via survey or review articles. In this article; we describe how surveys for researchfields can be represented in a semantic way; resulting in a knowledge graph that describesthe individual research problems; approaches; implementations and evaluations in astructured and comparable way. We present a comprehensive ontology for capturing thecontent of survey articles. We discuss possible applications and present an evaluation of …,International Conference on Theory and Practice of Digital Libraries,2017,1
Smjoin: A multi-way join operator for sparql queries,Mikhail Galkin; Kemele M Endris; Maribel Acosta; Diego Collarana; Maria-Esther Vidal; Sören Auer,Abstract Join operators are particularly important in SPARQL query engines that collect RDFdata using Web access interfaces. State-of-the-art SPARQL query engines rely on binaryjoin operators tailored for merging results from SPARQL queries over Web accessinterfaces. However; in queries with a large number of triple patterns; binary joins constitutea significant burden on the query performance. Multi-way joins that handle more than twoinputs are able to reduce the complexity of pre-processing stages and reduce the executiontime. Whereas in the relational databases field multi-way joins have already received someattention; the applicability of multi-way joins in SPARQL query processing remainsunexplored. We devise SMJoin; a multi-way non-blocking join operator tailored forindependently merging results from more than two RDF data sources. SMJoin …,Proceedings of the 13th International Conference on Semantic Systems,2017,1
SJoin: A Semantic Join Operator to Integrate Heterogeneous RDF Graphs,Mikhail Galkin; Diego Collarana; Ignacio Traverso-Ribón; Maria-Esther Vidal; Sören Auer,Abstract Semi-structured data models like the Resource Description Framework (RDF);naturally allow for modeling the same real-world entity in various ways. For example;different RDF vocabularies enable the definition of various RDF graphs representing thesame drug in Bio2RDF or Drugbank. Albeit semantically equivalent; these RDF graphs maybe syntactically different; ie; they have distinctive graph structure or entity identifiers andproperties. Existing data-driven integration approaches only consider syntactic matchingcriteria or similarity measures to solve the problem of integrating RDF graphs. However;syntactic-based approaches are unable to semantically integrate heterogeneous RDFgraphs. We devise SJoin; a semantic similarity join operator to solve the problem ofmatching semantically equivalent RDF graphs; ie; syntactically different graphs …,International Conference on Database and Expert Systems Applications,2017,1
MINTE: semantically integrating RDF graphs,Diego Collarana; Mikhail Galkin; Ignacio Traverso-Ribón; Maria-Esther Vidal; Christoph Lange; Sören Auer,Abstract The nature of the RDF data model allows for numerous descriptions of the sameentity. For example; different RDF vocabularies may be utilized to describepharmacogenomic data; and the same drug or gene is represented by different RDF graphsin DBpedia or Drug-bank. To provide a unified representation of the same real-world entity;RDF graphs need to be semantically integrated. Semantic integration requires themanagement of knowledge encoded in RDF vocabularies to determine the relatedness ofdifferent RDF representations of the same entity; eg; axiomatic definition of vocabularyproperties or resource equivalences. We devise MINTE; an integration technique that relieson both: knowledge stated in RDF vocabularies and semantic similarity measures to mergesemantically equivalent RDF graphs; ie; graphs corresponding to the same real-world …,Proceedings of the 7th International Conference on Web Intelligence; Mining and Semantics,2017,1
MateTee: a semantic similarity metric based on translation embeddings for knowledge graphs,Camilo Morales; Diego Collarana; Maria-Esther Vidal; Sören Auer,Abstract Large Knowledge Graphs (KGs); eg; DBpedia or Wikidata; are created with the goalof providing structure to unstructured or semi-structured data. Having these special datasetsconstantly evolving; the challenge is to utilize them in a meaningful; accurate; and efficientway. Further; exploiting semantics encoded in KGs; eg; class and property hierarchies;provides the basis for addressing this challenge and producing a more accurate analysis ofKG data. Thus; we focus on the problem of determining relatedness among entities in KGs;which corresponds to a fundamental building block for any semantic data integration task.We devise MateTee; a semantic similarity measure that combines the gradient descentoptimization method with semantics encoded in ontologies; to precisely compute values ofsimilarity between entities in KGs. We empirically study the accuracy of MateTee with …,International Conference on Web Engineering,2017,1
LDOW2017: 10th Workshop on Linked Data on the Web,Jens Lehmann; Sören Auer; Sarven Capadisli; Krzysztof Janowicz; Christian Bizer; Tom Heath; Aidan Hogan; Tim Berners-Lee,ABSTRACT The 10th Linked Data on the Web workshop (LDOW2017) 1 was held in Perth;Western Australia on April 3rd; 2017; colocated with the 26th International World Wide WebConference (WWW2017). In its 10th anniversary edition; the LDOW workshop aims tostimulate discussion and further research into the challenges of publishing; consuming; andintegrating structured data on the Web as well as mining knowledge from said data.,Proceedings of the 26th International Conference on World Wide Web Companion,2017,1
Exploiting the Value of Data through Data Value Networks,Judie Attard; Fabrizio Orlandi; Sören Auer,Abstract Open data is increasingly permeating into all dimensions of our society and hasbecome an indispensable commodity that serves as a basis for many products and services.Governments are generating a huge amount of data spanning different dimensions. Thisdataification shows the paramount need to identify the means and methods in which thevalue of data and knowledge can be exploited. While not restricted to the governmentdomain; this dataification is certainly relevant in a government context; particularly due to thelarge volume of data generated by public institutions. In this paper we identify the variousactivities and roles within a data value chain; and hence proceed to provide our owndefinition of a Data Value Network. We specifically cater for non-tangible data products andcharacterise three dimensions that play a vital role within the Data Value Network. We …,Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance,2017,1
Torpedo: Improving the state-of-the-art rdf dataset slicing,Edgard Marx; Saeedeh Shekarpour; Tommaso Soru; Adrian MP Braşoveanu; Muhammad Saleem; Ciro Baron; Albert Weichselbraun; Jens Lehmann; Axel-Cyrille Ngonga Ngomo; Sören Auer,Over the last years; the amount of data published as Linked Data on the Web has grownenormously. In spite of the high availability of Linked Data; organizations still encounter anaccessibility challenge while consuming it. This is mostly due to the large size of some of thedatasets published as Linked Data. The core observation behind this work is that a subset ofthese datasets suffices to address the needs of most organizations. In this paper; weintroduce Torpedo; an approach for efficiently selecting and extracting relevant subsets fromRDF datasets. In particular; Torpedo adds optimization techniques to reduce seekoperations costs as well as the support of multi-join graph patterns and SPARQL FILTERsthat enable to perform a more granular data selection. We compare the performance of ourapproach with existing solutions on nine different queries against four datasets. Our …,Semantic Computing (ICSC); 2017 IEEE 11th International Conference on,2017,1
Data Value Networks: Enabling a New Data Ecosystem,Judie Attard; Fabrizio Orlandi; Sören Auer,With the increasing permeation of data into all dimensions of our information society; data isprogressively becoming the basis for many products and services. It is hence becomingmore and more vital to identify the means and methods how to exploit the value of this data.In this paper we provide our definition of the Data Value Network; where we specificallycater for non-tangible data products. We also propose a Demand and Supply DistributionModel with the aim of providing insight on how an entity can participate in the global datamarket by producing a data product; as well as a concrete implementation through theDemand and Supply as a Service. Through our contributions we project our vision ofgenerating a new Economic Data Ecosystem that has the Web of Data as its core.,Web Intelligence (WI); 2016 IEEE/WIC/ACM International Conference on,2016,1
Linked Data in Business,Witold Abramowicz; Sören Auer; Tom Heath,The linked data principles 1 were proposed 10 years ago and since then have received everincreasing attention from researchers; developers; companies; and governments as ameans of data distribution and integration that is consistent with the architecture of the WorldWide Web. 2 This last decade has seen an explosion in the availability of data; driven by arange of factors such as open data initiatives worldwide; the increasing use of sensors tocreate a socalled internet of things; and by continued interest in the concept of big data. Inparallel to these trends; the broader Semantic Web vision has also evolved. While theSemantic Web stack was originally seen as rather monolithic and; at times; inaccessible todevelopers or different technology ecosystems; the RDF data model now appears a moretruly lingua franca of data integration; bridging different knowledge representation …,*,2016,1
Linked data workflow project ontology: uma ontologia de domínio para publicação e preservação de dados conectados,Sandro Rautenberg; Edgard Marx; Ivan Ermilov; Sören Auer,Resumo No domínio da Web Semântica; a criação; a produção e a manutenção de basesde dados conectados não são atividades triviais. Esforços e recursos consideráveis sãoconsumidos durante a execução de workflowspara tais fins. É desejável que estes esforçossejam planejados; baseando-se em processos bem estabelecidos e conduzidos de formasistemática ao longo do tempo; a fim de garantir a preservação de dados na Web de Dados.Neste contexto; este artigo apresenta a Linked Data Workflow Project Ontology; umaontologia para modelar os aspectos de planejamento e de execução para a produção e amanutenção de bases de dados conectados na Web de Dados. Caracterizando-se comouma pesquisa aplicada; metodologicamente; o desenvolvimento dessaontologiafundamentou-se em um conjunto de melhores práticas da Engenharia de …,XVII Encontro Nacional de Pesquisa em Ciência da Informação,2016,1
Eulaide: Interpretation of end-user license agreements using ontology-based information extraction,Najmeh Mousavi Nejad; Simon Scerri; Sören Auer; Elisa M Sibarani,Abstract Ignoring End-User License Agreements (EULAs) for online services due to theirlength and complexity is a risk undertaken by the majority of online and mobile serviceusers. This paper presents an Ontology-Based Information Extraction (OBIE) method forEULA term and phrase extraction to facilitate a better understanding by humans. Anontology capturing important terms and relationships has been developed and used toguide the OBIE process. Through a feedback cycle we have improved its domain-specificcoverage by identifying additional concepts. In the detection and extraction; we focus onthree key rights and conditions: permission; prohibition and duty. We present the EULAidesystem; which comprises a custom information extraction pipeline and a number of customextraction rules tailored for EULA processing. To evaluate our approach; we created and …,Proceedings of the 12th International Conference on Semantic Systems,2016,1
Towards semantification of big data technology,Mohamed Nadjib Mami; Simon Scerri; Sören Auer; Maria-Esther Vidal,Abstract Much attention has been devoted to support the volume and velocity dimensions ofBig Data. As a result; a plethora of technology components supporting various datastructures (eg; key-value; graph; relational); modalities (eg; stream; log; real-time) andcomputing paradigms (eg; in-memory; cluster/cloud) are meanwhile available. However;systematic support for managing the variety of data; the third dimension in the classical BigData definition; is still missing. In this article; we present SeBiDA; an approach for managinghybrid Big Data. SeBiDA supports the Semantification of Big Data using the RDF datamodel; ie; non-semantic Big Data is semantically enriched by using RDF vocabularies. Weempirically evaluate the performance of SeBiDA for two dimensions of Big Data; ie; volumeand variety; the Berlin Benchmark is used in the study. The results suggest that even in …,International Conference on Big Data Analytics and Knowledge Discovery,2016,1
LITMUS: An open extensible framework for benchmarking RDF data management solutions,Harsh Thakkar; Mohnish Dubey; Gezim Sejdiu; Axel-Cyrille Ngonga Ngomo; Jeremy Debattista; Christoph Lange; Jens Lehmann; Sören Auer; Maria-Esther Vidal,Abstract: Developments in the context of Open; Big; and Linked Data have led to anenormous growth of structured data on the Web. To keep up with the pace of efficientconsumption and management of the data at this rate; many data Management solutionshave been developed for specific tasks and applications. We present LITMUS; a frameworkfor benchmarking data management solutions. LITMUS goes beyond classical storagebenchmarking frameworks by allowing for analysing the performance of frameworks acrossquery languages. In this position paper we present the conceptual architecture of LITMUS aswell as the considerations that led to this architecture.,arXiv preprint arXiv:1608.02800,2016,1
Towards federated; semantics-based supply chain analytics,Niklas Petersen; Christoph Lange; Sören Auer; Marvin Frommhold; Sebastian Tramp,Abstract Supply Chain Management aims at optimizing the flow of goods and services fromthe producer to the consumer. Closely interconnected enterprises that align their production;logistics and procurement with one another thus enjoy a competitive advantage in themarket. To achieve a close alignment; an instant; robust and efficient information flow alongthe supply chain between and within enterprises is required. However; less efficient humancommunication is often used instead of automatic systems because of the great diversity ofenterprise systems and models. This paper describes an approach and its implementationSCM Intelligence App; which enables the configuration of individual supply chains togetherwith the execution of industry accepted performance metrics. Based on machine-processable supply chain data model (the SCORVoc RDF vocabulary implementing the …,International Conference on Business Information Systems,2016,1
An Empirical; Quantitative Analysis of the Differences between Sarcasm and Irony,Jennifer Ling; Roman Klinger,Abstract A variety of classification approaches for the detection of ironic or sarcasticmessages has been proposed in the last decade to improve sentiment classification.However; despite the availability of psychologically and linguistically motivated theoriesregarding the difference between irony and sarcasm; these typically do not carry over to ause in predictive models; one reason might be that these concepts are often considered verysimilar. In this paper; we contribute an empirical analysis of Tweets and how authors labelthem as irony or sarcasm. We use this distantly labeled corpus to estimate a model todistinguish between39 both classes of figurative language with the aim to; ultimately;improve the semantically correct interpretation of opinionated statements. Our modelseparates irony from sarcasm with 79% accuracy on a balanced set. This result suggests …,International Semantic Web Conference,2016,1
WDAqua–answering questions using web data,Ioanna Lytra; Maria-Esther Vidal; Christoph Lange; Sören Auer; Elena Demidova,WDAqua; a Marie Skłodowska Curie Innovative Training Network (ITN) running fromJanuary 2015 to December 2018; involves six academic partners4; and employs 15 PhDstudents (early stage researchers; ESRs) in total. The main motivation of this project is thatsharing; connecting; analyzing; and understanding data on the Web can provide betterservices to citizens; communities; and the industry. A vehicle to achieve this is data-drivenquestion answering (QA); having the key objective of delivering precise and comprehensiveanswers to natural language questions primarily by making better use of data. Powerful QAtools promise to improve access to the large amount of information available on the Web; oreven private data collections; and can be immediately useful to a wide audience of endusers in their private and professional life. Data-driven QA comprises four simplified steps …,EU Project Networking,2016,1
A semi-automatic approach for detecting dataset references in social science texts,Behnam Ghavimi; Philipp Mayr; Christoph Lange; Sahar Vahdati; Sören Auer,Abstract Today; full-texts of scientific articles are often stored in different locations than theused datasets. Dataset registries aim at a closer integration by making datasets citable butauthors typically refer to datasets using inconsistent abbreviations and heterogeneousmetadata (eg title; publication year). It is thus hard to reproduce research results; to accessdatasets for further analysis; and to determine the impact of a dataset. Manually detectingreferences to datasets in scientific articles is time-consuming and requires expert knowledgein the underlying research domain. We propose and evaluate a semi-automatic three-stepapproach for finding explicit references to datasets in social sciences articles. We first extractpre-defined special features from dataset titles in the da| ra registry; then detect references todatasets using the extracted features; and finally match the references found with …,Information Services & Use,2016,1
Semantic Clustering of Website Based on Its Hypertext Structure,Vladimir Salin; Maria Slastihina; Ivan Ermilov; René Speck; Sören Auer; Sergey Papshev,Abstract The volume of unstructured information presented on the Internet is constantlyincreasing; together with the total amount of websites and their contents. To process this vastamount of information it is important to distinguish different clusters of related webpages.Such clusters are used; for example; for knowledge extraction; named entity recognition; andrecommendation algorithms. A variety of applications (such as semantic analysis systems;crawlers and search engines) utilizes semantic clustering algorithms to recognizethematically connected webpages. The majority of them relies on text analysis of the webdocuments content; and this leads to certain limitations; such as long processing time; needof representative text content; or vagueness of natural language. In this article; we present aframework for unsupervised domain and language independent semantic clustering of …,International Conference on Knowledge Engineering and the Semantic Web,2015,1
Streaming Transformation of XML to RDF using XPath-based Mappings,Jyun-Yao Huang; Christoph Lange; Sören Auer,Abstract The Extensible Markup Language (XML) has become a widely adopted datainterchange format. With the rise of Linked Data published using the Resource DescriptionFramework (RDF); a number of tools for transforming XML to RDF have been developed.Specifying XML→ RDF mappings for these tools often requires skills in programminglanguages such as XSLT or XQuery. Moreover; these tools are rarely able to deal with largeXML inputs. We introduce an XML to RDF transformation approach; which is based onmappings comprising RDF triple templates that employ simple XPath expressions. Thanks tothe restricted XPath expressions; which can be evaluated against a stream of XML data; ourimplementation can handle extremely large input XML files. To process the XML inputefficiently; we employ XML filtering techniques and a strategy for selecting relevant XML …,Proceedings of the 11th International Conference on Semantic Systems,2015,1
Distributed linked data business communication networks: the LUCID endpoint,Sebastian Tramp; Ruben Navarro Piris; Timofey Ermilov; Niklas Petersen; Marvin Frommhold; Sören Auer,Abstract With the LUCID Endpoint; we demonstrate how companies can utilize Linked Datatechnology to provide major data items for their business partners in a timely manner;machine readable and with open and extensible schemata. The main idea is to provide aLinked Data infrastructure which enables all partners to fetch; as well as to clone and tosynchronize datasets from other partners over the network. This concept allows for buildingof networks of business partners much like as social network but in a distributed manner. Itfurthermore provides a technical infrastructure for business communication acts such assupply chain communication or master data management.,International Semantic Web Conference,2015,1
Semantic Interpretation Of User Queries For Question Answering On Interlinked Data,Saeedeh Shekarpour,Abstract The Web of Data contains a wealth of knowledge belonging to a large number ofdomains. Retrieving data from such precious interlinked knowledge bases is an issue. Bytaking the structure of data into account; it is expected that upcoming generation of searchengines is approaching to question answering systems; which directly answer userquestions. But developing a question answering over these interlinked data sources is stillchallenging because of two inherent characteristics: First; different datasets employheterogeneous schemas and each one may only contain a part of the answer for a certainquestion. Second; constructing a federated formal query across different datasets requiresexploiting links between these datasets on both the schema and instance levels. In thisrespect; several challenges such as resource disambiguation; vocabulary mismatch …,*,2015,1
Crowdsourced Semantic Annotation of Scientific Publications,Jaana Takis; Sören Auer,Abstract This work presents the architecture of a semantic annotation tool for scientificpublications and implements a prototype. This work also advances the state of the art in thesemantic publishing field by enabling semantic tagging of scholarly papers as acrowdsourced effort of interested parties. Earlier semantic publishing research has notsufficiently addressed the semantic annotation of papers published already in PDF format.The architecture presented in this work supports the semantic annotation of PDF documentswith any available annotation vocabulary and with resources from the linked open datasetDBpedia derived from Wikipedia. It also includes the implementation of a functionality torecommend similar papers. A novel approach here is the ability to take into account thestructural context of annotations with a special focus on the discourse elements of …,Signature,2014,1
Linked Enterprise Data: Management und Bewirtschaftung vernetzter Unternehmensdaten mit Semantic Web Technologien,Tassilo Pellegrini; Harald Sack; Sören Auer,Die Herausgeber und Autoren führen praxisnah in neue Methoden und Technologien des(Meta-) Daten-Managements für die Vernetzung und Integration verteilter; heterogenerDatenbestände ein. Dabei werden die neuen Technologien und Methoden von bereitsetablierten Ansätzen deutlich abgegrenzt; ihre Potenziale und auch ihre Grenzen klarbenannt. Vor allem Semantic-Web-Technologien; deren betrieblicher Einsatz anhandanschaulicher Fallstudien erläutert wird; spielen eine zentrale Rolle.,*,2014,1
Datenintegration im Unternehmen mit Linked Enterprise Data,Sören Auer; Rene Pietzsch; Jörg Unbehauen,Zusammenfassung Datenintegration ist in großen Unternehmen nach wie vor eine zentraleHerausforderung und wird es auch auf absehbare Zeit bleiben. Ein erfolgversprechenderAnsatz ist die Verwendung des Linked Data Paradigmas für die Integration vonUnternehmensdaten. Ebenso wie inzwischen ein Web der Daten das Dokumenten-zentrierte Web ergänzt; können Daten-Intranets die existierenden Intranet-und SOA-Landschaften in großen Unternehmen erweitern und flexibilisieren. Ein weiterer Vorteil desLinked Data Paradigmas ist die Möglichkeit der Nutzung von Daten aus der inzwischen aufüber 50 Mrd. Fakten angewachsenen Linked Open Data (LOD) Cloud. Im Ergebnis kann einunternehmensinternes Daten-Intranet dazu beitragen die Brücke zwischen strukturiertemDatenmanagement (in ERP; CRM; SCM Systemen) sowie semi-und unstrukturierten …,*,2014,1
Publishing and Interlinking the USPTO Patent Data,Amrapali Zaveri; Mofeed M Hassan; Tariq Yousef; Simon Chill; Sören Auer; Jens Lehmann,Abstract. Patents are widely used to protect intellectual property and a measure ofinnovation output. Each year; the USPTO grants over 150; 000 patents to individuals andcompanies all over the world. For instance; there were more than 300; 000 patent grantsissued in the US in 2013. However; accessing; searching and analyzing those patents isoften still cumbersome and inefficient. To overcome those problems; Google indexes patentsand converts them to XML files using OCR techniques. In this article; we take this idea onestep further and provide semantically rich; machine-understandable patents in RDF format.This data can be integrated with other data sources in order to further simplify use casessuch as trend analysis; structured patent search and exploration and societal progressmeasurements. We describe the conversion; publishing; interlinking process along with …,Semantic Web Journal; http://www. semantic-web-journal. net/content/publishing-andinterlinking-uspto-patent-data,2014,1
Linked open data,Irini Fundulaki; Sören Auer,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: Mostrare; Search; Register; Log in …,ECRIM News; Special Theme,2014,1
Internationalization of linked data,Dimitris Kontokostas; Charalampos Bratsas; Sören Auer; Sebastian Hellmann; Ioannis Antoniou; George Metakides,Abstract This paper describes the deployment of the Greek DBpedia and the contribution tothe DBpedia information extraction framework with regard to internationalization (I18n) andmultilingual support. I18n filters are proposed as pluggable components in order to addressissues when extracting knowledge from non-English Wikipedia editions. We report about ourstrategy to support International Resource Identifier (IRI) and introduce two new extractors tocomplement the I18n filters. Additionally; the paper discusses the definition of TransparentContent Negotiation (TCN) rules for IRIs to address de-referencing and IRI serializationproblems. The aim of this research is to establish best practices (complemented by software)to allow the DBpedia community to easily generate; maintain and properly interlinklanguage-specific DBpedia editions. Furthermore; these best practices can be applied for …,Web Semantics: Science; Services and Agents on the World Wide Web,2012,1
CrowdLearn: Collaborative engineering of (semi-) structured learning objects,Darya Tarasowa; Ali Khalili; Sören Auer,Abstract A special activity; that became possible nowadays; is the collaborative e-learningand collaborative engineering of learning materials. Such an opportunity is available ineducational systems; based on the wiki paradigm. However; Ward Cunningham's wikiparadigm was mainly only applied to unstructured; textual content thus limiting the contentstructuring; repurposing and reuse. Some steps in this direction are made by Semantic Webtechnologies and their combination with the wiki paradigm. In many potential usagescenarios; however; the (semi-) structured educational content (eg presentations;questionnaires; diagrams; etc.) should be managed and the collaboration of large usercommunities around such content should be effectively facilitated. We present theCrowdLearn concept for collaborative engineering of the semantically structured learning …,Proc. 3rd Russian Conference on Knowledge Engineering and Semantic Web; Saint-Petersburg; Russia,2012,1
Web Engineering: 11th International Conference; ICWE 2011; Paphos; Cyprus; June 20-24; 2011; Proceedings,Sören Auer; Oscar Diaz; George A Papadopoulos,This book constitutes the refereed proceedings of the 11th International Conference on WebEngineering; held in Paphos; Cyprus; in June 2011. The 22 revised full papers and 15revised poster papers presented together with 2 invited lectures were carefully reviewed andselected from 90 submissions for inclusion in the book. The papers topics cover a broadrange of areas; namely; the Semantic Web; Web Services; Mashups; Web 2.0; Web quality;Web development; etc.,*,2011,1
Raven: Towards zero-configuration link discovery,A-C Ngonga Ngomo; Jens Lehmann; Sören Auer; Konrad Höffner,Abstract. With the growth of the Linked Data Web; time-efficient approaches for computinglinks between data sources have become indispensable. Yet; in many cases; determiningthe right specification for a link discovery problem is a tedious task that must still be carriedout manually. In this article we present RAVEN; an approach for the semiautomaticdetermination of link specifications. Our approach is based on the combination of stablesolutions of matching problems and active learning leveraging the time-efficient linkdiscovery framework LIMES. RAVEN is designed to require a small number of interactionswith the user in order to generate classifiers of high accuracy. We focus with RAVEN on thecomputation and configuration of Boolean and weighted classifiers; which we evaluate inthree experiments against link specifications created manually. Our evaluation shows that …,Proceedings of OM@ ISWC,2011,1
DBpedia internationalization-a graphical tool for I18n infobox-to-ontology mappings,Charalampos Bratsas; Lazaros Ioannidis1 Dimitris Kontokostas; Sören Auer; Christian Bizer; Sebastian Hellmann; Ioannis Antoniou,Abstract. During the past two decades; the use of the Web has spread across multiplecountries and cultures. While the Semantic Web is already served in many languages; weare still facing challenges concerning its internationalization. The DBpedia project; acommunity effort to extract structured information from Wikipedia; is already supportingmultiple languages. This paper presents a graphical tool for creating internationalizedmappings for DBpedia.,10th International Semantic Web Conference (ISWC2011). URL: http://iswc2011. semanticweb. org/fileadmin/iswc/Papers/PostersDemos/iswc11pd _submission_36. pdf; ανακτήθηκε,2011,1
Eine schnittstelle für arztpraxisdaten mittels einer ontologie auf basis von HL7 version 3,Jan Kunze; Thomas Riechert; Sören Auer,Abstract: Eine besondere Eigenschaft von XML-Schnittstellen im medizinischen Umfeld istderen häufige Änderung und Schema-Anpassung durch neue Gegebenheiten. Es ist daherwünschenswert eine solche Schnittstelle möglichst dynamisch zu gestalten; um auf neueAnforderungen schnell reagieren zu können. Die gewünschte Flexibilität ist mit XML;welches feste Kommunikationsregeln definiert; nur schwer zu erreichen. Eine möglicheLösung für flexiblere Schnittstellen im Gesundheitswesen stellen die Technologien desSemantic Web dar. Ausgehend von einer aktuellen Schnittstelle; basierend auf HL7 Version3 zur Archivierung von Daten einer Arztpraxis; wurde untersucht inwieweit diese XML-Schnittstelle nach OWL respektive RDF/RDFS konvertiert werden kann. Eine manuellegewonnene Repräsentation dieser Schnittstelle wird vorgestellt und die mögliche …,Tagungsband XML-Tage,2006,1
Wissensarbeit mit OntoWiki,S Dietzold; S Auer; T Riechert,Abstract In diesem Papier präsentieren wir das browserbasierte Werkzeug OntoWiki;welches kolloboratives und agiles Wissensmanagement nach Art eines Wikis unterstützt.Die vom OntoWiki verwalteten Wissensbasen lassen sich sowohl in einer Vielzahl vonSichten erkunden und betrachten; als auch mit mehreren intiutiven Methoden bearbeiten.So ist OntoWiki nicht nur ein generischer Browser für Semantic Web Wissenbasen aufGrundlage des Resource Description Frameworks (RDF;[LS99]); sondern auch einkolloboratives Werkzeug welches die Zusammenarbeit mit ein Vielzahl von Interaktions-undKommunikationsmöglichkeiten unterstützt. Onto-Wiki wird durch das Bundesministerium fürBildung und Forschung (BMBF; Fördernummer xxxxxx) gefördert und kann als OpenSourceSoftware bezogen werden.,*,2006,1
Semantic Web Content Management,Norman Beck; Sören Auer,Kurzfassung Im Rahmen der Semantic Web Initiative des W3C sind in jüngsterVergangenheit die RDF-basierten Standards RDFS und OWL verabschiedet worden; die alsWissensrepräsentationssprachen gemäß dem Ontologie-Paradigma im Web fungieren.Aufbauend auf diesen Technologien wird in dieser Arbeit gezeigt; wie im Zusammenspielmit einem Ontologie-Editor die Grundlage für ein wissensbasiertes Web ContentManagement geschaffen werden kann. Dabei finden die üblichen Paradigmen des WebContent Managements Berücksichtigung und Vorteile der Modellierung von Wissensbasengegenüber einem relationalen oder auch XML basierten Inhaltsmodell werden genutzt.Dieser Ansatz leistet damit einen Beitrag; Web Content Management vom reinenInformationsmanagement zum Wissensmanagement weiterzuentwickeln.,*,2004,1
Collaborative Authoring of Open-CourseWare: a state-of-art analysis,Darya Tarasowa; Sören Auer,*,*,*,1
ExConQuer: Lowering barriers to RDF and Linked Data re-use,Judie Attard; Fabrizio Orlandi; Sören Auer,Abstract A major obstacle to the wider use of semantic technology is the perceivedcomplexity of RDF data by stakeholders who are not familiar with the Linked Data paradigm;or are otherwise unaware of a dataset's underlying schema. In order to help overcome thisbarrier; we propose the ExConQuer Framework (Explore; Convert; and Query Framework)as a set of tools that preserve the semantic richness of the data model while catering forsimplified and workable views of the data. Through the available tools users are able toexplore and query linked open datasets without requiring any knowledge of SPARQL or thedatasets' underlying schema. Moreover; executed queries are persisted so that they can beeasily explored and re-used; and even edited. Therefore; with this framework we attempt totarget the evident niche in existing tools that are intended to be used by non-experts to …,Semantic Web,*,1
Studies in Computational Intelligence; Volume 174,Janusz Kacprzyk,This volume is a collection of selected papers that were presented at the internationalconference Model-Based Reasoning in Science and Technology. Abduction; Logic; andComputational Discovery (MBR09 BRAZIL); held at the University of Campinas; Campinas;Brazil in December 2009.,*,*,1
Real-life SOA Experiences and an Approach Towards Semantic SOA,Muhammad Ahtisham Aslam; Michael Herrmann; Sören Auer; Richard Golden,Abstract. Service Oriented Architecture promises the reuse of services. We recognized in anearly stage of our SOA project; the gap between technical and semantical reuse of services.Thus; an agile Service Oriented Architecture (SOA) needs to be supported by ontologies. Inthis paper we introduce our real-life project and some already available results. We presenta new 4-tier architecture to support Web services integration in semantic service orientedparadigm. The proposed 4-tier architecture is result of differentiation between architecturalcomponents (services) and those components that interact with services (orchestration).There is a further need for applying ontologies in order to describe services. Our research isbased on a real-life project that measures the maturity of SOA and proves the need ofsemantics.,Proceedings of 4th International Workshop on SOA and Web Services in conjunction with ACM SIGPLAN International Conference on Object-Oriented Programming; Systems; Languages; and Applications (OOPSLA 2006),*,1
Killing Two Birds with One Stone--Querying Property Graphs using SPARQL via GREMLINATOR,Harsh Thakkar; Dharmen Punjani; Jens Lehmann; Sören Auer,Abstract: Knowledge graphs have become popular over the past decade and frequently relyon the Resource Description Framework (RDF) or Property Graph (PG) databases as datamodels. However; the query languages for these two data models--SPARQL for RDF andthe PG traversal language Gremlin--are lacking interoperability. We present Gremlinator; thefirst translator from SPARQL--the W3C standardized language for RDF--and Gremlin--apopular property graph traversal language. Gremlinator translates SPARQL queries toGremlin path traversals for executing graph pattern matching queries over graph databases.This allows a user; who is well versed in SPARQL; to access and query a wide variety ofGraph Data Management Systems (DMSs) avoiding the steep learning curve for adapting toa new Graph Query Language (GQL). Gremlin is a graph computing system-agnostic …,arXiv preprint arXiv:1801.09556,2018,*
Supporting Scientometric Studies with Linked Open Data,Sandro Rautenberg; Edgard Marx; Antonio Costa Gomes Filho; Sören Auer,RESUMO Na Cientometria; mensurar indicadores é uma tarefa complexa devido aosdesafios em coletar; organizar e relacionar dados; sobretudo; na web; onde os dados sãodistribuídos em várias fontes e formatos incompatíveis. Esses problemas podem serresolvidos com o emprego de tecnologias e metodologias baseadas nos princípios LinkedOpen Data. Tais princípios são fundamentados num conjunto de melhores práticas de WebSemântica e Dados Abertos para organizar; publicar e conectar dados na web. Por meiodestes; os dados são acessados e consumidos sem restrições; em diversas aplicações. Nopresente trabalho; relata-se a experiência na disponibilização do histórico do índice Qualisconforme o Linked Open Data. Pressupõe-se que tal empreendimento é importante nasatividades de coleta de dados primários em pesquisas bibliométricas/cientométricas …,Perspectivas em Ciência da Informação,2017,*
Ontology-Based Representation of Learner Profiles for Accessible OpenCourseWare Systems,Mirette Elias; Steffen Lohmann; Sören Auer,Abstract The development of accessible web applications has gained significant attentionover the past couple of years due to the widespread use of the Internet and the equality lawsenforced by governments. Particularly in e-learning contexts; web accessibility plays animportant role; as e-learning often requires to be inclusive; addressing all types of learners;including those with disabilities. However; there is still no comprehensive formalrepresentation of learners with disabilities and their particular accessibility needs in e-learning contexts. We propose the use of ontologies to represent accessibility needs andpreferences of learners in order to structure the knowledge and to access the information forrecommendations and adaptations in e-learning contexts. In particular; we reused theconcepts of the ACCESSIBLE ontology and extended them with concepts defined by the …,International Conference on Knowledge Engineering and the Semantic Web,2017,*
Integration of Scholarly Communication Metadata Using Knowledge Graphs,Sören Auer,Abstract. Important questions about the scientific community; eg; what authors are theexperts in a certain field; or are actively engaged in international collaborations; can beanswered using publicly available datasets. However; data required to answer suchquestions is often scattered over multiple isolated datasets. Recently; the Knowledge Graph(KG) concept has been identified as a means for interweaving heterogeneous datasets andenhancing answer completeness and soundness. We present a pipeline for creating highquality knowledge graphs that comprise data collected from multiple isolated structureddatasets. As proof of concept; we illustrate the different steps in the construction of aknowledge graph in the domain of scholarly communication metadata (SCMKG).Particularly; we demonstrate the benefits of exploiting semantic web technology to …,Research and Advanced Technology for Digital Libraries: 21st International Conference on Theory and Practice of Digital Libraries; TPDL 2017; Thessaloniki; Greece; September 18-21; 2017; Proceedings,2017,*
Towards a Multi-way Similarity Join Operator,Mikhail Galkin; Maria-Esther Vidal; Sören Auer,Abstract Increasing volumes of data consumed and managed by enterprises demandeffective and efficient data integration approaches. Additionally; the amount and variety ofdata sources impose further challenges for query engines. However; the majority of existingquery engines rely on binary join-based query planners and execution methods withcomplexity that depends on the number of involved data sources. Moreover; traditionalbinary join operators are not able to distinguish between similar and different tuples; treatingevery incoming tuple as an independent object. Thus; if tuples are represented differentlybut refer to the same real-world entity; they are still considered as non-related objects. Wepropose MSimJoin; an approach towards a multi-way similarity join operator. MSimJoinaccepts more than two inputs and is able to identify duplicates that correspond to similar …,Advances in Databases and Information Systems,2017,*
Integration of Scholarly Communication Metadata Using Knowledge Graphs,Afshin Sadeghi; Christoph Lange; Maria-Esther Vidal; Sören Auer,Abstract Important questions about the scientific community; eg; what authors are the expertsin a certain field; or are actively engaged in international collaborations; can be answeredusing publicly available datasets. However; data required to answer such questions is oftenscattered over multiple isolated datasets. Recently; the Knowledge Graph (KG) concept hasbeen identified as a means for interweaving heterogeneous datasets and enhancing answercompleteness and soundness. We present a pipeline for creating high quality knowledgegraphs that comprise data collected from multiple isolated structured datasets. As proof ofconcept; we illustrate the different steps in the construction of a knowledge graph in thedomain of scholarly communication metadata (SCM-KG). Particularly; we demonstrate thebenefits of exploiting semantic web technology to reconcile data about authors; papers …,International Conference on Theory and Practice of Digital Libraries,2017,*
Analysing Scholarly Communication Metadata of Computer Science Events,Said Fathalla; Sahar Vahdati; Christoph Lange; Sören Auer,Abstract Over the past 30 years we have observed the impact of the ubiquitous availability ofthe Internet; email; and web-based services on scholarly communication. The preparation ofmanuscripts as well as the organisation of conferences; from submission to peer review topublication; have become considerably easier and efficient. A key question now is whatwere the measurable effects on scholarly communication in computer science? Of particularinterest are the following questions: Did the number of submissions to conferencesincrease? How did the selection processes change? Is there a proliferation of publications?We shed light on some of these questions by analysing comprehensive scholarlycommunication metadata from a large number of computer science conferences of the last30 years. Our transferable analysis methodology is based on descriptive statistics …,International Conference on Theory and Practice of Digital Libraries,2017,*
Ontology-guided job market demand analysis: A cross-sectional study for the data science field,Elisa Margareth Sibarani; Simon Scerri; Camilo Morales; Sören Auer; Diego Collarana,Abstract The rapid changes in the job market; including a continuous year-on-year increasein new skills in sectors like information technology; has resulted in new challenges for jobseekers and educators alike. The former feel less informed about which skills they shouldacquire to raise their competitiveness; whereas the latter are inadequately prepared to offercourses that meet the expectations by fast-evolving sectors like data science. In this paper;we describe efforts to obtain job demand data and employ a information extraction methodguided by a purposely-designed vocabulary to identify skills requested by the job vacancies.The Ontology-based Information Extraction (OBIE) method employed relies on the Skills andRecruitment Ontology (SARO); which we developed to represent job postings in the contextof skills and competencies needed to fill a job role. Skill demand by employers is then …,Proceedings of the 13th International Conference on Semantic Systems,2017,*
Semantic Similarity based Clustering of License Excerpts for Improved End-User Interpretation,Najmeh Mousavi Nejad; Simon Scerri; Sören Auer,Abstract With the omnipresent availability and use of cloud services; software tools; Webportals or services; legal contracts in the form of End-User License Agreements (EULA)regulating their use are of paramount importance. Often the textual documents describingthese regulations comprise many pages and can not be reasonably assumed to be read andunderstood by humans. In this work; we describe a method for extracting and clusteringrelevant parts of such documents; including permissions; obligations; and prohibitions. Theclustering is based on semantic similarity employing a distributional semantics approach onlarge word embeddings database. An evaluation shows that it can significantly improvehuman comprehension and that improved feature-based clustering has a potential to furtherreduce the time required for EULA digestion. Our implementation is available as a web …,Proceedings of the 13th International Conference on Semantic Systems,2017,*
MULDER: Querying the Linked Data Web by Bridging RDF Molecule Templates,Kemele M Endris; Mikhail Galkin; Ioanna Lytra; Mohamed Nadjib Mami; Maria-Esther Vidal; Sören Auer,Abstract The increasing number of RDF data sources that allow for querying Linked Data viaWeb services form the basis for federated SPARQL query processing. Federated SPARQLquery engines provide a unified view of a federation of RDF data sources; and rely onsource descriptions for selecting the data sources over which unified queries will beexecuted. Albeit efficient; existing federated SPARQL query engines usually ignore themeaning of data accessible from a data source; and describe sources only in terms of thevocabularies utilized in the data source. Lack of source description may conduce to theerroneous selection of data sources for a query; thus affecting the performance of queryprocessing over the federation. We tackle the problem of federated SPARQL queryprocessing and devise MULDER; a query engine for federations of RDF data sources …,International Conference on Database and Expert Systems Applications,2017,*
Large-scale storage and query processing for semantic sensor data,Farah Karim; Mohamed Nadjib Mami; Maria-Esther Vidal; Sören Auer,Abstract Nowadays; there is a rapid increase in the number of sensor data produced by awide variety of devices and sensors. Collections of sensor data can be semanticallydescribed using ontologies; eg; the Semantic Sensor Network (SSN) ontology. Albeitsemantically enriched; the volume of semantic sensor data is considerably larger than rawsensor data. Moreover; some measurement values can be observed several times; and alarge number of repeated facts can be generated. We devise a compact or factorizedrepresentation of semantic sensor data; where repeated values are represented only once.To scale up to large datasets; tabular representation is utilized to store and managefactorized semantic sensor data using Big data technologies. We empirically study theeffectiveness of the proposed factorized representation of semantic sensor data; and the …,Proceedings of the 7th International Conference on Web Intelligence; Mining and Semantics,2017,*
The Qanary Ecosystem: Getting New Insights by Composing Question Answering Pipelines,Christoph Lange; Sören Auer,Abstract. The field of Question Answering (QA) is very multidisciplinary as it requiresexpertise from a large number of areas such as natural language processing (NLP); artificialintelligence; machine learning; information retrieval; speech recognition and semantictechnologies. In the past years a large number of QA systems were proposed usingapproaches from different fields and focusing on particular tasks in the QA process.Unfortunately; most of these systems cannot be easily reused; extended; and results cannotbe easily reproduced since the systems are mostly implemented in a monolithic fashion; lackstandardized interfaces and are often not open source or available as Web services. Toaddress these issues we developed the knowledge-based Qanary methodology forchoreographing QA pipelines distributed over the Web. Qanary employs the qa …,Web Engineering: 17th International Conference; ICWE 2017; Rome; Italy; June 5-8; 2017; Proceedings,2017,*
Towards a Knowledge Graph based Speech Interface,Ashwini Jaya Kumar; Sören Auer; Christoph Schmidt,Abstract: Applications which use human speech as an input require a speech interface withhigh recognition accuracy. The words or phrases in the recognised text are annotated with amachine-understandable meaning and linked to knowledge graphs for further processing bythe target application. These semantic annotations of recognised words can be representedas a subject-predicate-object triples which collectively form a graph often referred to as aknowledge graph. This type of knowledge representation facilitates to use speech interfaceswith any spoken input application; since the information is represented in logical; semanticform; retrieving and storing can be followed using any web standard query languages. In thiswork; we develop a methodology for linking speech input to knowledge graphs and studythe impact of recognition errors in the overall process. We show that for a corpus with …,arXiv preprint arXiv:1705.09222,2017,*
Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech Recognition,Ashwini Jaya Kumar; Camilo Morales; Maria-Esther Vidal; Christoph Schmidt; Sören Auer,Abstract: With the evolution of neural network based methods; automatic speech recognition(ASR) field has been advanced to a level where building an application with speechinterface is a reality. In spite of these advances; building a real-time speech recogniser facesseveral problems such as low recognition accuracy; domain constraint; and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving theacoustic model; language model; decoder and by rescoring the N-best list at the output ofthe decoder. We are considering the N-best list rescoring approach to improve therecognition accuracy. Most of the methods in the literature use the grammatical; lexical;syntactic and semantic connection between the words in a recognised sentence as a featureto rescore. In this paper; we have tried to see the semantic relatedness between the …,arXiv preprint arXiv:1705.08018,2017,*
A RADAR for information reconciliation in Question Answering systems over Linked Data1,Serena Villata; Elena Cabrio; Alessio Palmero Aprosio; Christina Unger; Axel-Cyrille Ngonga Ngomo; Philipp Cimiano; Sören Auer; George Paliouras,*,Open Journal Of Semantic Web,2017,*
Ontology-guided Job Market Demand Analysis,Elisa Margareth Sibarani; Simon Scerri; Camilo Morales; Sören Auer; Diego Collarana,In the past twenty years; the use of the Web has increased dramatically and as aconsequence; job advertisements are now mainly published electronically online [14]. Inaddition; due to the digitization of society and economy; the job markets are consistently andprofoundly changing. New job pro les are emerging and corresponding skills are in high-demand; while formerly important skills become irrelevant. As a result; it is of paramountimportance to give job seekers and education providers a timely and comprehensiveoverview of the current situation on the job market in terms of required skills; competences;and technologies. e aim of this study is to identify the most needed technical skills that areimportant for a data scientist's work by analyzing job advertisements. We performed a cross-sectional analysis which focuses on a snapshot of the demand of the current job market. e …,*,2017,*
Big Data Europe.,Hajira Jabeen; Phil Archer; Simon Scerri; Aad Versteden; Ivan Ermilov; Giannis Mouchakis; Jens Lehmann; Soeren Auer,ABSTRACT e BigDataEurope (BDE) project is developing exactly the kind of computinginfrastructure that European stakeholders need when handling large volumes of data in avariety of formats; the results are open-source and their use is completely free. Coordinatedby Fraunhofer IAIS; BDE is working directly with partners that represent the seven SocietalChallenges identi ed by the European Commission (Health; Food; Energy; Transport;Climate; Social Sciences and Security). For each community; a pilot that makes use of BDE stechnology stack to address the Big Data needs identi ed by these challenges is well underway.,EDBT/ICDT Workshops,2017,*
Social Semantic Web,Jürgen Ziegler; Steffen Lohmann; Sören Auer,Jump to ContentJump to Main Navigation Languages. Languages: Deutsch. User Account:Log in; Register; Help; Take a Tour; Sign up for a free trial; Subscribe. OWV Logo. Search Close.Advanced SearchHelp. My Content (1) Recently viewed (1). Social Semantic Web : ... MySearches (0). (0) My CartAdded To Cart. Check Out. Menu: Subjects: Architecture and Design;Arts; Asian and Pacific Studies; Business and Economics; Chemistry; Classical and AncientNear Eastern Studies; Computer Sciences; Cultural Studies; Engineering; General Interest;Geosciences; History; Industrial Chemistry; Jewish Studies; Law; Library and Information Science;Book Studies; Life Sciences; Linguistics and Semiotics; Literary Studies; Materials Sciences;Mathematics; Medicine; Music; Pharmacy; Philosophy; …,Issues,2017,*
Factorization Techniques for Longitudinal Linked Data (Short Paper),Farah Karim; Maria-Esther Vidal; Sören Auer,Abstract Longitudinal linked data are RDF descriptions of observations from relatedsampling frames or sensors at multiple points in time; eg; patient medical records or climatesensor data. Observations are expressed as measurements whose values can be repeatedseveral times in a sampling frame; resulting in a considerable increase in data volume. Wedevise a factorized compact representation of longitudinal linked data to reduce repetition ofsame measurements; and propose algorithms to generate collections of factorizedlongitudinal linked data that can be managed by existing RDF triple stores. We empiricallystudy the effectiveness of the proposed factorized representation on linked observation data.We show that the total data volume can be reduced by more than 30% on average withoutloss of information; as well as improve compression ratio of state-of-the-art compression …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2016,*
Interview with Frank van Harmelen on “Linked Data and Business Information Systems”,Sören Auer,Frank van Harmelen (1960) is a professor in Knowledge Representation and Reasoning inthe Computer Science department at the Vrije Universiteit Amsterdam. Since 2000; he hasplayed a leading role in the development of the Semantic Web. He was co-PI on the firstEuropean Semantic Web project (OnToKnowledge; 1999); which laid the foundations for theWeb Ontology Language OWL. He co-authored the Semantic Web Primer; the first academictextbook of the field and now in its third edition. He was one of the architects of Sesame; anRDF storage and retrieval engine; which is in wide academic and industrial use with over200;000 downloads. This work received the 10-year impact award at the 11th InternationalSemantic Web Conference in 2012. In recent years; he pioneered the development of largescale reasoning engines. He was scientific director of the 10 m euro EU-funded Large …,Business & Information Systems Engineering,2016,*
Disintermediation of Inter-Blockchain Transactions,S Matthew English; Fabrizio Orlandi; Sören Auer,Abstract: Different versions of peer-to-peer electronic cash exist as data represented byseparate blockchains. Payments between such systems cannot be sent directly from oneparty to another without going through a financial institution. Bitcoin provided part of thesolution but its utility is limited to intra-blockchain transactions. The benefits are lost if atrusted third party is required to execute inter-blockchain transactions. We propose asolution to the inter-blockchain transaction problem using the same fundamental principlesof Bitcoin. The protocol is described by the Uberledger framework; a hierarchical meta-blockchain layer that encapsulates information regarding the fidelity of peer-to-peertransaction facilitators. Subjects: Cryptography and Security (cs. CR) Cite as: arXiv:1609.02598 [cs. CR](or arXiv: 1609.02598 v1 [cs. CR] for this version) Submission history …,arXiv preprint arXiv:1609.02598,2016,*
The Semantic Web: Eswc 2016 Satellite Events; Heraklion; Crete; Greece; May 29-June 2; 2016; Revised Selected Papers,Sören Auer,*,*,2016,*
Distributed Linked Data Business Communication Networks: The LUCID Endpoint,Niklas Petersen; Marvin Frommhold; Sören Auer,Abstract. With the LUCID Endpoint; we demonstrate how companies can utilize Linked Datatechnology to provide major data items for their business partners in a timely manner;machine readable and with open and extensible schemata. The main idea is to provide aLinked Data infrastructure which enables all partners to fetch; as well as to clone and tosynchronize datasets from other partners over the network. This concept allows for buildingof networks of business partners much like as social network but in a distributed manner. Itfurthermore provides a technical infrastructure for business communication acts such assupply chain communication or master data management.,The Semantic Web: ESWC 2015 Satellite Events: ESWC 2015 Satellite Events; Portorož; Slovenia; May 31–June 4; 2015; Revised Selected Papers,2015,*
LDOW 2013: The 8th Workshop on Linked Data on the Web,Sören Auer; Tim Berners-Lee; Christian Bizer; Tom Heath,Abstract This paper presents a brief summary of the eight workshop on Linked Data on theWeb. The LDOW 2013 workshop is held in conjunction with the World Wide Web conference2013. The focus is on data publishing; integration and consumption using RDF and othersemantic representation formalisms and technologies.,Proceedings of the 24th International Conference on World Wide Web,2015,*
2014 Reviewers List,Ahmed Abbasi; Osman Abul; Evrim Acar; Palakorn Achananuparp; Sibel Adali; Nitin Agarwal; Rodrigo Agerri; Ankit Agrawal; Divyakant Agrawal; Manoj Agrawal; Chowdhury Ahmed; Mueen Ahmed; Luca Aiello; Mohammad Akbari; Reza Akbarinia; Oguz Akbilgic; Leman Akoglu; Zaher Al Aghbari; Luai Al-Shalabi; Barron-Cedeno Alberto; Nanopoulos Alexandros; Neda Alipanah; Mohammed Altaf; Toshiyuki Amagasa; Giuseppe Amato; Yuan An; Aris Anagnostopoulos; David Anastasiu; Kai Ang; François Anton; Kemafor Anyanwu; Arvind Arasu; Marcelo Arenas; Nikos Armenatzoglou; Ismailcem Arpinar; Anastasios Arvanitis; Mahdi Asadpour; Ira Assent; Juan Carlos Augusto; G Ausiello; Hakan Aydin,We thank the following reviewers for the time and energy they have given to TKDE … AhmedAbbasi Osman Abul Evrim Acar Palakorn Achananuparp Sibel Adali Nitin Agarwal Rodrigo AgerriAnkit Agrawal Divyakant Agrawal Manoj Agrawal Chowdhury Ahmed Mueen Ahmed Luca AielloMohammad Akbari Reza Akbarinia Oguz Akbilgic Leman Akoglu Zaher Al Aghbari Luai Al-ShalabiBarron-Cede˜no Alberto Nanopoulos Alexandros Neda Alipanah Mohammed Altaf ToshiyukiAmagasa Giuseppe Amato Yuan An Aris Anagnostopoulos David Anastasiu Kai Ang FrançoisAnton Kemafor Anyanwu Arvind Arasu Marcelo Arenas Nikos Armenatzoglou Ismailcem ArpinarAnastasios Arvanitis Mahdi Asadpour Ira Assent Juan Carlos Augusto G. Ausiello Hakan AydinB … Prabhakaran B. Sumeet Bajaj Petko Bakalov Spiridon Bakiras José Balcázar Tim BaldwinForrest Bao … Jie Bao Zhifeng Bao Nathalie Baracaldo Elena Baralis Nicola Barbieri …,IEEE Transactions on Knowledge and Data Engineering,2015,*
BISE-Call for Papers Issue 5/2016,Witold Abramowicz; Sören Auer; Tom Heath,There is a common misunderstanding concerning enterprise data–linked not necessarilymeans open. Internal company data linked to open data can still be private. This wayenterprises gain additional value by extension; enhancements and verification of own dataagainst external sources. Still the biggest challenge is quality. Current linked open datadatasets; including the flagship DBpedia; are only partially suited for commercialapplications. Building business around them requires advanced methods for maintenanceand quality assurance; eg; data service levels. In this special focus issue on “Linked Data inBusiness” we would like to focus on research that studies the exploitation of linked data ineconomics and management. Enterprises can integrate data and discover new insightsmore easily and this can lead to the emergence of new products and services. They will …,Business & Information Systems Engineering,2014,*
SINA: semantic interpretation of user queries for question answering on interlinked data by Saeedeh Shekarpour with Prateek Jain as coordinator,Saeedeh Shekarpour; Sören Auer,Abstract The Data Web contains a wealth of knowledge on a large number of domains.Question answering over interlinked data sources is challenging due to two inherentcharacteristics. First; different datasets employ heterogeneous schemas and each one mayonly contain a part of the answer for a certain question. Second; constructing a federatedformal query across different datasets requires exploiting links between the differentdatasets on both the schema and instance levels. In this dissertation; we present a questionanswering system; which transforms user supplied queries (ie either natural languagesentences or keywords) into conjunctive SPARQL queries over a set of interlinked datasources. The contribution of this work is as follows: 1. A novel approach for determining themost suitable resources for a user-supplied query from different datasets (disambiguation …,ACM SIGWEB Newsletter,2014,*
LDOW 2014: Proceedings of the Workshop on Linked Data on the Web co-located with the 23rd International World Wide Web Conference (WWW 2014); Seoul; Kor...,Christian Bizer; Tom Heath; Sören Auer; Tim Berners-Lee,*,CEUR Workshop Proceedings,2014,*
Linking Data and Knowledge in Enterprises; Research and Society.,Christoph Lange; Sören Auer,Abstract. The Linked Data paradigm has emerged as a powerful enabler for data andknowledge interlinking and exchange using standardised Web technologies. In this article;we discuss our vision how the Linked Data paradigm can be employed to evolve theintranets of large organisations–be it enterprises; research organisations or governmentaland public administrations–into networks of internal data and knowledge. In particular forlarge enterprises data integration is still a key challenge. The Linked Data paradigm seemsa promising approach for integrating enterprise data. Like the Web of Data; which nowcomplements the original document-centred Web; data intranets may help to enhance andflexibilise the intranets and service-oriented architectures that exist in large organisations.Furthermore; using Linked Data gives enterprises access to 50+ billion facts from the …,DB&IS,2014,*
Linked Data Kuratierung und Visualisierung mit semantischen Daten Wikis,Philipp Frischmuth; Michael Martin; Sebastian Tramp; Sören Auer,Zusammenfassung Die visuelle Aufbereitung von Linked Data sowohl für Zwecke derProzessverarbeitung als auch zur Konsumierung durch Endanwender ist ein wichtigesDesignelement in der unternehmerischen Aneignung von Semantic Web Technologien. DerBeitrag stellt aktuelle Ansätze in der Linked Data-Visualisierung vor; diskutiert derenBedeutung im Enterprise Information Management und zeigt Beispiele; wie durch BestPractices unternehmerische Nutzungsszenarien von Linked Data unterstützt werdenkönnen.,*,2014,*
conTEXT-A Mashup Platform for Lightweight Text Analytics.,Ali Khalili; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract. Social media technologies such as Weblogs; Microblogging; Wikis and SocialNetworks have become one of the most important parts of our daily life as they enable us tocommunicate and share stories with a lot of people. The more the amount of publishedinformation grows; the more important are solutions for accessing; analyzing; summarizingand visualizing information. While substantial progress has been made in the last years ineach of these areas individually; we argue; that only the intelligent combination ofapproaches will make this progress truly useful and leverage further synergies betweentechniques. conTEXT aims to provide a user-friendly and lightweight Mashup platformenabling endusers to use sophisticated NLP techniques for analyzing and visualizing theircontent. it provides a flexible text analytics architecture of participation by innovative …,AIMashup@ ESWC,2014,*
Extraktion; Mapping und Verlinkung von Daten im Web,Sören Auer; Jens Lehmann; Axel-Cyrille Ngonga Ngomo; Claus Stadler; Jörg Unbehauen,Zusammenfassung In diesem Artikel geben wir einen Überblick über verschiedeneHerausforderungen des Managements von Linked Data im Web. Mit der DBpediaWissensextraktion aus Wikipedia; dem skalierbaren Linking von Wissensbasen und demMapping relationaler Daten nach RDF stellen wir drei Ansätze vor; die zentrale Phasen desLebenszyklus von Daten im Web ausmachen.,Datenbank-Spektrum,2013,*
Crowd-sourced Open Courseware Authoring with SlideWiki. org,Sˆren Auer; Ali Khalili; Darya Tarasowa,Abstract While many Learning Content Management Systems are available; thecollaborative; community-based creation of rich e-learning content is still not sufficiently wellsupported. Few attempts have been made to apply crowd-sourcing and wiki-approaches forthe creation of e-learning content. In this article; we showcase SlideWiki--an OpenCourseware Authoring platform supporting the crowdsourced creation of richly structuredlearning content.,International Journal of Emerging Technologies in Learning (iJET),2013,*
LDOW2013 Linked Data on the Web: Proceedings of the WWW2013 Workshop on Linked Data on the Web; Rio de Janeiro; Brazil; 14 May; 2013,Christian Bizer; Tom Heath; Tim Berners-Lee; Michael Hausenblas; Sören Auer,*,CEUR workshop proceedings,2013,*
From overview to facets and pivoting for interactive exploration of semantic web data,Josep Maria Brunetti Fernández; Roberto García González; Sören Auer,Abstract The proliferation of Linked Open Data on the Web has increased the amount of dataavailable for analysis and reuse. However; casual users find it difficult to explore and useSemantic Web Data due to the prevalence of specialised browsers that require complexqueries to be formed and intimate knowledge on the structure of datasets. The authorsaddress this problem in the Rhizomer tool by applying the data analysis mantra of overview;zoom and filter. These interaction patterns are implemented using information architecturecomponents users are already familiar with but that are automatically generated from dataand ontologies. This approach makes it possible to obtain an overview of the dataset beingexplored using techniques; such as navigation menus; treemaps or sitemaps; which areusually not available in text-based semantic web browsers. From there; users can …,International Journal on Semantic Web and Information Systems; 2013; vol. 9; núm. 1; p. 1-20,2013,*
RDFaCE-Lite: a WYSIWYM editor for user-friendly semantic text authoring,Ali Khalili; Sören Auer,Abstract Recently practical approaches for managing and supporting the life-cycle ofsemantic content on the Web of Data made quite some progress. However; the currentlyleast developed aspect of the semantic content life-cycle is the user-friendly manual andsemi-automatic creation of rich semantic content.,Extended Semantic Web Conference,2012,*
From WYSIWYG to WYSIWYM–Content and Value Enrichment with Semantic Metadata,Daniel Hladky; Victor Klintsov; Ali Khalili; Sören Auer,Многие компании стремятся повысить свою ценность; предлагая традиционный веб-поиск и новые приложения. Учитывая рост популярности Semantic Web и Linked OpenData; в этой статье представлен метод создания обогащенных семантическиеаннотаций; использующий RDFaCE подход. Подход основывается на предоставленииразличных взглядов на контент; таких как классический WYSIWYG взгляд и WYSIWYM(What You See Is What You Mean) взгляд; позволяющий сделать семантическиеаннотации наглядными.,*,2012,*
The hype; the hope and the LOD2: Sören Auer engaged in the next generation LOD,Sören Auer,The paneuropean Project LOD2 (Linking Open Data) is one of the biggest projects dealingwith linked data. Scientists; programmers and software architects in various europeancountries are working on the next generation of linked open data. In a series of interviewsThomas Thruner of the Semantic Web Company (SWC) is presenting people working onand with LOD2. As a start SWC talked to Sören Auer; head of the LOD2 project.ThomasThurner: Over the recent years the LOD movement gained tremendous momentum. As oneof the key players in this area how do you perceive this development? Hype or hope?,*,2011,*
In this paper we motivate why it is crucial to associate linguistic information with ontologies and why more expressive models; beyond the label systems implemente...,Eran Toch; Iris Reinhartz-Berger; Dov Dori; Jens Lehmann; Sören Auer; Lorenz Bühmann; Sebastian Tramp,Resolving semantic heterogeneity across distinct data sources remains a highly relevantproblem in the GIS domain requiring innovative solutions. Our approach; called GSim;semantically aligns tables from respective GIS databases by first choosing attributes forcomparison. We then examine their instances and calculate a similarity value between themcalled entropy-based distribution (EBD) 1...,Web Semantics: Science; Services and Agents on the World Wide Web,2011,*
LIMES–A Time-Efficient Approach for Large-Scale Link Discovery on the Web of Data,Axel Cyrille; Ngonga Ngomo; Soeren Auer,*,Proceedings of IJCAI,2011,*
LOD2 Deliverable 1.2: State of the Art Analysis,Tassilo Pellegrini; Peter Boncz; Jens Lehmann; Robert Isele; Gabriela Vulcu; Lorenz Buhmann; Sören Auer; Sebastian Tramp; Sebastian Hellmann,Abstract: This report documents the state of the art in the central technology areas of theLOD2 project. It identifies challenges and action items that have to be handled throughoutthe course of the project to reach the objectives defined in the Description of Work. Thesetasks and challenges; as well as their interdependencies are illustrated by a roadmap thatspans the project runtime and gives an insight into the various expected outcomes. Theinformation in this document reflects only the author's views and the European Community isnot liable for any use that may be made of the information contained therein. The informationin this document is provided “as is” without guarantee or warranty of any kind; express orimplied; including but not limited to the fitness of the information for a particular purpose. Theuser thereof uses the information at his/her sole risk and liability.,Month,2010,*
Proceedings of the 5th Open Knowledge Conference; OKCon 2010; London; UK; April 24; 2010,S Auer; J Gray; C Müller-Birn; R Pollock; SW Gray,UCL Discovery is UCL's open access repository; showcasing and providingaccess to UCL research outputs from all UCL disciplines.,OKCon,2010,*
Workshop Web Science (WSW 2010)-Social Computing Applications-(Vorwort).,Sören Auer; Claudia Müller-Birn; Steffen Staab,*,GI Jahrestagung (2),2010,*
Studentenkonferenz Informatik Leipzig (SKIL 2010).,Thomas Riechert; Sören Auer,In Anbetracht stagnierender oder sogar zurückgehender Studierendenzahlen ist eine umfassendeNachwuchsförderung in der Informatik von besonderer Bedeutung. Studentenkonferenzen sindeine Möglichkeit die Identifikation für das Studienfach Informatik und die Begeisterung fürIT-Themen allgemein bei Studenten zu wecken. Bei einer Studentenkonferenz reichen Studierendekurze Artikel über Studien-; Abschlussarbeiten oder in der Freizeit absolvierte Informatik-relevanteProjekte ein. Andere Studierende; Doktoranden; wissenschaftliche Mitarbeiter und Professorenbewerten und diskutieren die eingereichten Arbeiten. Interessante und gut ausgearbeitete Einreichungenwerden zur Präsentation auf der Konferenz angenommen und die besten Arbeiten mit Preisenprämiert. Eine Studentenkonferenz unterscheidet sich damit kaum von einer anderen wissenschaftlichenKonferenz. Die Themenvielfalt kann allerdings durch die Breite der vertretenen Themen …,GI Jahrestagung (2),2010,*
Networked Knowledge-Networked Media,Sören Auer; Klaus Tochtermann; Sebastian Schaffert,*,*,2009,*
Realisierung von Sozialen Netzwerken im Semantic Web mit OntoWikiOntoWiki a Social Semantic Web Wiki-Node,Sebastian Dietzold; Sören Auer,Zusammenfassung In diesem Beitrag zeigen wir; wie auf Basis einfach zuimplementierender Kommunikations-Technologien; wie Linked Data und Pingback; einsoziales Semantisches Web aufgebaut werden kann. Wir erläutern dies an wichtigenAktivitätsmustern für Social Networks und stellen unsere Implementierung vor; welche in dasSemantische Daten-Wiki OntoWiki integriert ist. Abstract In this paper; we show howSemantic Web and Social Web technologies can be tied together to be the foundation of adecentralized Social Semantic Web. We illustrate this approach by analyzing different well-known social activity patterns; which can be found in today′ s social networking sites.Furthermore; we present our implementation of a Social Semantic Web client; which isbased on the semantic data wiki OntoWiki.,i-com Zeitschrift für interaktive und kooperative Medien,2009,*
Proceedings of the International Workshop on Interacting with Multimedia Content,Sören Auer; Sebastian Dietzold; Steffen Lohmann; Jürgen Ziegler,Media sharing and social networking websites have attracted many millions of usersresulting in vast collections of user generated content. The contents are typically poorlystructured and spread over several platforms; each supporting specific media types. With theincreasing growth and diversity of these websites; new ways to access and manage thecontents are required–both within and across Web platforms. For these reasons; theconvergence of the Social and the Semantic Web is of great potential for the future evolutionof the Web. The goal of the International Workshop on Interacting with Multimedia Content inthe Social Semantic Web (IMC-SSW'08) is to provide a forum for researchers andpractitioners from the Semantic Web; Human-computer Interaction; and multimediacommunities to discuss these topics and share experiences from a multidisciplinary …,*,2008,*
Vorwort der Organisatoren des Workshops “Nutzerinteraktion im Social Semantic Web “,Jürgen Ziegler; Steffen Lohmann; Sören Auer,Ziegler; J.; Lohmann; S. & Auer; S.; (2008). Vorwort der Organisatoren des Workshops “Nutzerinteraktionim Social Semantic Web“. In: Lucke; U.; Kindsmüller; MC; Fischer; S.; Herczeg; M. &Seehusen; S. (Hrsg.); Workshop Proceedings der Tagungen Mensch & Computer 2008; DeLFI2008 und Cognitive Design 2008. Berlin: Logos Verlag. (S. 155-156) … Um Dateien anzeigenzu können; müssen Sie eingeloggt sein … Gesellschaft für Informatik eV (GI); FachbereichMensch-Computer-Interaktion (FB MCI); Kontakt: digilib@imis.uni-luebeck.de Diese Digital Librarybasiert auf DSpace … Um diese Seite benutzen zu können; benötigen Sie Javascript. Bitteaktivieren Sie JavaScript in Ihren Browsereinstellungen; oder geben Sie ggf. die Seite mit NoScriptfrei.,Workshop Proceedings der Tagungen Mensch & Computer 2008; DeLFI 2008 und Cognitive Design 2008,2008,*
Expressing business process models as OWL-S ontologies,Jun Shen; Michael Herrmann; Muhammad A Aslam; Soren Auer,*,*,2006,*
Schriftenreihe,Thomas Laufer,ПОИСК. Найти. Расширенный поиск …,*,2005,*
Liebe Leserinnen und Leser,Ihr Fridolin Singler; Frist für Legionellenprüfung,wie Sie bereits der Dezember-Ausgabe entnehmen konnten; ist Herr Friedrich Plettenbergzum Jahresende 2012 in den wohlverdienten Ruhestand getreten. Zum 1. Januar 2013wurde ich vom Aufsichtsrat zum geschäftsführenden Vorstand gewählt und habe damit dieNachfolge von Herrn Plettenberg angetreten; eine Aufgabe; die nur mit der Unterstützungder Mitarbeiterinnen und Mitarbeiter zu bewerkstelligen ist. Ich bin mir sicher; dass wir unsauch künftig als eingespieltes Team den kommenden Anforderungen stellen werden. In dennächsten Jahren stehen zahlreiche gesetzliche Änderungen und Vorgaben an.Stellvertretend hierfür nenne ich den Einbau von Wärmemengenzählern sowie dieDichtigkeitsprüfung der Abwasserleitungen. Auch die mittlerweile in die Jahre gekommenenInstallationsleitungen müssen teilweise erneuert werden. Bei dieser Gelegenheit werden …,*,2005,*
Thanks to International Psychogeriatrics Reviewers,S Auer; R Austria Baldwin; G Byrne; J Australia Byrne; E Cocco; P Darzins; T Australia Dening; A Homma; T Japan Hope; MA McColl; P Merrick; L Onega; R Ponds; N Germany Purandare; A Shah; R Sweet,Papers submitted to International Psychogeriatrics undergo peer review; with eachmanuscript read by two or more reviewers who have expertise specific to the paper's topic.Their advice is often highly valuable to authors working to strengthen papers throughrevision; and their comments and recommendations are central in determining what ispublished in International Psychogeriatrics.,International Psychogeriatrics,2002,*
Leipzig: literarische Spaziergänge,Werner Marx; Margit Emmrich,*,*,2001,*
Device-Independent Visual Ontology Modeling,Vitalis Wiens; Steffen Lohmann; Sören Auer,Abstract. The development of ontologies typically involves ontology engineers and domainexperts with different backgrounds in semantic technologies and ontology modeling.Domain experts; who provide the conceptualization of the knowledge domain; often lackmodeling skills and find it hard to follow logical notations in OWL representation.Visualizations of ontologies; in particular graph visualizations in the form of node-linkdiagrams; are commonly used to support ontology modeling and related tasks. In order tomore directly involve domain experts in ontology modeling; approaches that are immediatelyavailable; easy to use; and independent of the device and interaction context are needed.We present a device-independent approach for visual ontology modeling that reduces theentrance barrier to engage in ontology modeling. The device-independence is achieved …,*,*,*
SemSur: A Core Ontology for the Semantic Representation of Research Findings,Said Fathalla; Sören Auer; Christoph Lange,Abstract. The way how research is communicated using text publications has not changedmuch over the past decades. We have the vision that ultimately researchers will work on acommon structured knowledge base comprising comprehensive semantic and machine-comprehensible descriptions of their research; thus making research contributionstransparent and comparable. The current approach for structuring; systematizing andcomparing research results is based on survey or review articles. We present the SemSurontology for semantically capturing the information commonly found in survey and reviewarticles. SemSur allows to represent individual research problems; approaches;implementations and evaluations in a structured and comparable way. We discuss possibleapplications and present an evaluation of our approach with the retrospective; exemplary …,*,*,*
‘Teach me to fish’Querying Semantic Data Lakes,Mohamed Nadjib Mami; Hajira Jabeen; Sören Auer,Abstract. We have recently made a huge leap in terms of data formats; data modalities; andstorage capabilities. Dozens of data storage techniques have been created as a result.Today; we are able to store clusterwide data; and to choose a storage technique that suitsour application needs; rather than the opposite. If different data stores are interlinked andintegrated; this data can generate valuable knowledge and insights. In this article; wepresent an approach that uses semantic technologies to query heterogeneous Big Datastored in a Data Lake in a unified manner. Our approach is based on equipping original datastored in the Data Lake with mappings and adding transformations to the SPARQL querysyntax to make heterogeneous data joinable across the Data Lake. We devise animplementation; named Sparkall; that uses Apache Spark as the underlying query engine …,*,*,*
Towards Linked Data Internationalization-Realizing the Greek DBpedia,Charalampos Bratsas; Sören Auer; Sebastian Hellmann; Ioannis Antoniou,ABSTRACT This paper describes the realization of the Greek DBpedia as part of theDBpedia Internationalization proccess.“I18n filters” are proposed as pluggable componentsof the DBpedia Information Extraction Framework; in order to address issues concerningcovering more knowledge from non-English Wikipedia's and International ResourceIdentifier (IRI) support. Moreover; a new extractor is introduced that uses the WikipediaInterlanguage Links to connect international DB-pedia's and transitively to the LOD Cloud.Finally; the paper illustrates the first international project which provides TransparentContent Negotiation (TCN) rules for International Resource Identifier's (IRI's) for de-referencing purposes. This work could serve as a guide not only for other multilingualDBpedias; but for publishing linked data in languages based on non-Latin character sets …,*,*,*
The Potential of Linked Data in Business,Sören Auer; Witold Abramowicz; Tom Heath,*,*,*,*
Evaluating the Quality of the LOD Cloud: An Empirical Investigation,Jeremy Debattista; Christoph Lange; Sören Auer; Dominic Cortis,Abstract. The increasing adoption of the Linked Data principles brought with it anunprecedented dimension to the Web; transforming the traditional Web of Documents to avibrant information ecosystem; also known as the Web of Data. This transformation;however; does not come without any pain points. Similar to the Web of Documents; the Webof Data is heterogenous in terms of the various domains it covers. The diversity of the Web ofData is also reflected in its quality. Data quality impacts the fitness for use of the data for theapplication at hand; and choosing the right dataset is often a challenge for data consumers.In this quantitative empirical survey; we analyse 130 datasets (≈ 3.7 billion quads);extracted from the latest Linked Open Data Cloud using 27 Linked Data quality metrics; andprovide insights into the current quality conformance. Furthermore; we publish the quality …,*,*,*
A smart distance power electronic measurement using smartphone applications,Raad Farhood Chisab; Alex Van den Bossche UGent; A Schrauwen; K Van Stiphout; J Demeulemeester; Wouter Devulder; CM Comrie; K Temst; A Vantomme; Lachlan Hyde; Eugene Kats; Chenyang Xue; Hongyan Xu; Bart F Vermeulen; Jackson Wu; Johan Swerts; Sebastien Couet; Iuliana P Radu; Guido Groeseneken; Johanna K Jochum; Margriet Van Bael; Kristiaan Temst,Ghent University Ghent University Academic Bibliography. Add publications; |; Statistics; |;Marked list 0; |; Saved searches 0. Advanced. Home; |; Publications; |; People; |;Organizations; |; Projects. 6051 – 6100 of 246234. show: 50; |; sort: date added (newto old). News feed; Embed this list; Save this search; Mark all; Export. Cancel: 5; |; 10; |;15; |; 20; |; 50; |; 100; |; 250. Cancel year (new to old) …,Sort,*,*
DemoEffTE: A Demonstrator of Dependency-aware Evaluation of Test Cases over Ontology,Lavdim Halilaj; Irlán Grangel-González; Maria-Esther Vidal; Steffen Lohmann; Sören Auer,Abstract. Traditional approaches; which follow a test-driven development technique; allow aset of test cases to be exhaustively evaluated ensuring that each modification of an ontologydoes not violate predefined requirements. However; the time required for the evaluation oftest cases is high and usually represents a bottleneck in an ontology development process.The EffTE framework tackles this problem; it relies on a graph-based model of thedependencies between test cases to support users during an ontology developmentprocess. Traversing the dependency graph is realized using breadth-first search along witha mechanism that tracks tabu test cases; ie; test cases that will be ignored for furtherevaluation due to faulty parent test cases. As a result; the number of test cases that areevaluated is minimized; thus reducing the time required for validating an ontology after …,*,*,*
Exploring Term Networks for Semantic Search over Large RDF Knowledge Graphs,Edgard Marx; Saeedeh Shekarpour; Konrad Höffner; Axel-Cyrille Ngonga Ngomo; Jens Lehmann; Sören Auer,Abstract. Information retrieval approaches are currently regarded as a key technology toempower lay users to access the Web of Data. To assist such need; a large number ofapproaches such as Question Answering and Semantic Search have been developed.While Question Answering promises accurate results by returning a specific answer;Semantic Search engines are designed to retrieve the top-K resources on a given scoringfunction. In this work; we focus on the latter paradigm. We aim to address one of the majordrawbacks of current implementations; ie; the accuracy. We propose* P; a Semantic Searchapproach that explores term networks to answer keyword queries on large RDF knowledgegraphs. The proposed method is based on a novel graph disambiguation model. Theadequacy of the approach is demonstrated on the QALD benchmark data set against …,*,*,*
Research Track Papers,Natalia Avdeeva; Galina Artemova; Kirill Boyarsky; Natalia Gusarova; Natalia Dobrenko; Eugeny Kanevsky; Claudia Bretschneider; Heiner Oberkampf; Sonja Zillner; Efthymios Chondrogiannis; Vassiliki Andronikou; Efstathios Karanastasis; Theodora Varvarigou; Mikhail Galkin; Dmitry Mouromtsev; Sören Auer; Vladislav A Grozin; Natalia F Gusarova; Natalia V Dobrenko; Ali Hasnain; Qaiser Mehmood; Syeda Sana e Zainab; Stefan Decker; MR Kogalovsky; SI Parinov; Maxim Kolchin; Nikolay Klimov; Alexey Andreev; Ivan Shilin; Daniil Garayzuev; Danil Zakoldaev; Igor Lashkov; Alexander Smirnov; Alexey Kashevnik; Vladimir Parfenov; Martin Ledvinka; Petr Křemen,Subtopic Segmentation of Scientific Texts: Parameter Optimisation … NataliaAvdeeva; Galina Artemova; Kirill Boyarsky; Natalia Gusarova; Natalia Dobrenko; and EugenyKanevsky … UIMA2LOD: Integrating UIMA Text Annotations into the Linked Open DataCloud … Claudia Bretschneider; HeinerOberkampf; and Sonja Zillner … An Advanced Query and Result Rewriting Mechanism for InformationRetrieval Purposes from RDF Datasources … EfthymiosChondrogiannis; Vassiliki Andronikou; Efstathios Karanastasis; and Theodora Varvarigou …Identifying Web Tables: Supporting a Neglected Type of Content on the Web . . . . . . . . . . . . . .… Mikhail Galkin; Dmitry Mouromtsev; and Sören Auer …Feature Selection for Language Independent Text Forum Summarization . . . .,*,*,*
The LITMUS Test: Benchmarking RDF and Graph Data Management Systems,Yashwant Keswani; Harsh Thakkar; Mohnish Dubey; Jens Lehmann; Sören Auer,Abstract. In this paper we demonstrate the working of proposed LITMUS benchmark suite.Benchmarking is an extremely tedious task demanding repetitive manual effort; therefore it isadvantageous to automate the whole process. LITMUS; is an automated benchmarkingframework which supports benchmarking and comparing diverse DMSs for both RDF andproperty graph DMS. It also provides custom visualization (ie plots) support for thebenchmarked DMSs against a wide range of CPU and memory specific evaluationparameters.,*,*,*
Efficient Processing of Semantically Represented Sensor Data,Farah Karim; Maria-Esther Vidal; Sören Auer,Abstract: Large collections of sensor data are semantically described using ontologies; eg;the Semantic Sensor Network (SSN) ontology. Semantic sensor data are RDF descriptionsof sensor observations from related sampling frames or sensors at multiple points in time;eg; climate sensor data. Sensor values can be repeated in a sampling frame; eg; a particulartemperature value can be repeated several times; resulting in a considerable increase indata volume. We devise a factorized compact representation of semantic sensor data usinglinked data technologies to reduce repetition of same sensor values; and proposealgorithms to generate collections of factorized semantic sensor data that can be managedby existing RDF triple stores. We empirically study the effectiveness of the proposedfactorized representation of semantic sensor data. We show that the size of semantic …,*,*,*
FaRBIE: A Faceted Reactive Browsing Interface for Multi RDF Knowledge Graph Exploration,Luis Fuenmayor; Diego Collarana; Steffen Lohmman; Sören Auer,Abstract Linked Data brings numerous RDF Knowledge Graphs to the Web; and as a result;the Linking Open Data (LOD) cloud comprises several independent–but linked–graphdatasets. Faceted Browsing is a popular type of User Interface (UI) to explore the knowledgein these RDF graphs. Due to the distributed and linked nature of RDF graphs; exploringmore than one graph at a time is a common interaction scenario in this context. However;most state-of-the-art user interfaces allow users to browse only one RDF graph at a time.Additionally; exploring multiple RDF graphs adds a degree of uncertainty to the userinterface; such as connectivity problems and longer query response times as well asvariations in the size and semantic complexity of the retrieved data. In this paper; we presenta faceted browsing approach named FaRBIE; which enables users to explore multiple …,*,*,*
communication protocol,Sarven Capadisli; Amy Guy; Christoph Lange; Sören Auer,Identifier: http://csarven. ca/linked-data-notifications Abstract. In this article we describe theLinked Data Notifications (LDN) protocol; which is a W3C Candidate Recommendation.Notifications are sent over the Web for a variety of purposes; for example; by socialapplications. The information contained within a notification is structured arbitrarily; andtypically only usable by the application which generated it in the first place. In the spirit ofLinked Data; we propose that notifications should be reusable by multiple authorisedapplications. Through separating the concepts of senders; receivers and consumers ofnotifications; and leveraging Linked Data principles of shared vocabularies and URIs; LDNprovides a building block for decentralised Web applications. This permits end users morefreedom to switch between the online tools they use; as well as generating greater value …,*,*,*
Distributed Vocabulary Development with Version Control Systems,Lavdim Halilaj; Steffen Lohmann; Christian Mader; Sören Auer,Abstract​. Vocabularies are increasingly being developed on platforms for hosting version-controlled repositories; such as GitHub. However; these platforms lack important featuresthat have proven useful in vocabulary development. We present VoCol; an integratedenvironment that supports the development of vocabularies using​ Version ControlSystems​. VoCol is based on a fundamental model of vocabulary development; consistingof the three core activities modeling; population; and testing. It uses a loose coupling ofvalidation; querying; analytics; visualization; and documentation generation components ontop of a standard Git repository. All components; including the version-controlled repository;can be configured and replaced with little effort to cater for various use cases.,*,*,*
The Industry 4.0 Standards Landscape from a Semantic Integration Perspective,Irlán Grangel-González; Paul Baptista; Lavdim Halilaj; Steffen Lohmann; Maria-Esther Vidal; Christian Mader; Sören Auer,Abstract—Interoperability among actors; sensors; and heterogeneous systems is a crucialfactor for realizing the Industry 4.0 vision; ie; the creation of Smart Factories by enablingintelligent human-to-machine and machine-to-machine cooperation. In order to empowerinteroperability in Smart Factories; standards and reference architectures have beenproposed. Standards allow for the description of components; systems; and processes; aswell as interactions among them. Reference architectures classify; align; and integrateindustrial standards according to their purposes and features; industrial communities inEurope; the United States; and Asia have proposed their own reference architectures.However; interoperability among analogous standards in these reference architectures ishampered due to different granularity representation of similar processes or production …,*,*,*
Factorization Techniques for Longitudinal Linked Data,Farah Karim; Maria-Esther Vidal; Sören Auer,Abstract. Large collections of longitudinal linked data are publicly available as part of theLinking Open Data (LOD) cloud. Longitudinal linked data are RDF descriptions ofobservations from related sampling frames or sensors at multiple points in time; eg; patientmedical records or climate sensor data. Observations are expressed as measurementswhose values can be repeated several times in a sampling frame; resulting in aconsiderable increase in data volume. We devise a factorized compact representation oflongitudinal linked data to reduce repetition of same measurements; and propose algorithmsto generate collections of factorized longitudinal linked data that can be managed byexisting RDF triple stores. We empirically study the effectiveness of the proposed factorizedrepresentation on linked observation data. We show that the total data volume can be …,*,*,*
Enterprise Knowledge Graphs: A Semantic Approach for Knowledge Management in the Next Generation of Enterprise Information Systems,Mikhail Galkin; Sören Auer; María-Esther Vidal; Simon Scerri,Abstract: In enterprises; Semantic Web technologies have recently received increasingattention from both the research and industrial side. The concept of Linked Enterprise Data(LED) describes a framework to incorporate benefits of Semantic Web technologies intoenterprise IT environments. However; LED still remains an abstract idea lacking a point oforigin; ie; station zero from which it comes to existence. We devise Enterprise KnowledgeGraphs (EKGs) as a formal model to represent and manage corporate information at asemantic level. EKGs are presented and formally defined; as well as positioned in EnterpriseInformation Systems (EISs) architectures. Furthermore; according to the main features ofEKGs; existing EISs are analyzed and compared using a new unified assessmentframework. We conduct an evaluation study; where cluster analysis allows for identifying …,*,*,*
Stephane Grumbach; INRIA; France,Raza Abidi; Soren Auer; Marian Babik; Nik Bessis; Sami Bhiri; Peter Brezany; Mario Cannataro; Nguyen Ngoc Chan; Xue Chen; Gao Chao; Jingde Cheng; Day David; Bruno Defude; Yaokai Feng; Khaled Gaaloul; Zhiqiang Gao; Aditya Ghose; ZhiGuo Gong; Mohamed Graiet,Raza Abidi; Dalhousie University; Canada Soren Auer; Leipzig University; Australia MarianBabik; Center for Information Technologies; Slovakia Nik Bessis; Derby University; United KingdomSami Bhiri; National University of Ireland; Galway; Ireland Peter Brezany; University ofVienna; Austria Mario Cannataro; University “Magna Græcia” of Catanzaro; Italy Nguyen NgocChan; TELECOM SudParis; France Xue Chen; Shanghai University; China Gao Chao; SouthwestUniversity; China Jingde Cheng; Saitama University; Japan Day David; Sheffield HallamUniversity; United Kingdom Bruno Defude; Institut Telecom; Telecom SudParis; France YaokaiFeng; Kyushu University; Japan Khaled Gaaloul; CRP Henri Tudor; Luxembourg ZhiqiangGao; Southeast University; China Aditya Ghose; University of Wollongong; Australia ZhiGuoGong; University of Macau; China Mohamed Graiet; ISMIM-Monastir; Tunisia Stephane …,*,*,*
Industrial Data Space: Semantic integration of Enterprise Data with VoCol,Lavdim Halilaj; Niklas Petersen; Irlán Grangel-González; Christoph Lange; Steffen Lohmann; Christian Mader; Sören Auer,Nowadays; business processes are being digitized across all industries [5]. Just-in-timemanufacturing and mass customization generate vast amounts of data at a faster pace thanever. Specialization and outsourcing multiply the number of actors involved in businessexchanges. Data management is adapting to these trends: data quality is assuredproactively and data is increasingly considered a strategic asset.,*,*,*
pOWL–Features and Usage Overview,Sören Auer; Norman Beck,The broad application of ontologies as shared terminological knowledge representations isone of the main strategies of the semantic web paradigm. With OWL (Web OntologyLanguage [5]) there exists now a W3C standard for defining web enabled ontologies whichfits in the semantic layering of web languages. Although there are some OWL ontologymanagement solutions available; most of them are complicated to deploy or handle; do notsupport strategies for collaborative; distributed development of ontologies; are not OpenSource or are not available for the most distributed web technologies. Since PHP ([2]) is byfar the most distributed web development technology (as regularly confirmed by Netcraft [3]);the semantic web paradigm will probably only be successful in a broad perspective if thereare applications and tools available tightly interacting with this language. The goal of this …,*,*,*
Towards a Smart Data Repository for the SDIL,Andreas Poxrucker; Christian Mader; Peter Hevesi; Lavdim Halilaj; Steffen Lohmann; Paul Lukowicz; Sören Auer,We present our work towards a smart; community and crowd driven data repository in thecontext of the SDI-X project. The goal is to extend the Smart Data Innovation Lab (SDIL)towards a platform offering an extensive collection of Big Data datasets from various areasand sources of industrial and academic research as well as state-of-the-art hardware andsoftware processing capabilities. We describe our concept for the planned repository as wellas the intended benefits for the SDIL platform. Additionally; we explain strategies and toolsfor data curation ensuring a successful operation in the future,Liebe Kollegen; Forschungspartner und Smart Data Innovatoren,*,*
LDWPO–A Lightweight Ontology for Linked Data Management,Sandro Rautenberg; Ivan Ermilov; Edgard Marx; Sören Auer,Abstract. Managing the lifecycle of RDF datasets is a cumbersome activity. Substantialefforts are spent on reproducing datasets over time. But; these efforts can be reduced by adata management workflow framework. We present the Linked Data Workflow Projectontology as the knowledge model for such a workflow framework. The ontology is centeredon the Plan; Method; and Execution classes; facilitating the description of: i) themethodological process that guides the lifecycle of RDF datasets; ii) the complete plan of theRDF dataset production workflow; and iii) the executions of workflow. As a result; ourapproach enables the reproducibility and repeatability of Linked Data processing steps overtime.,*,*,*
FuhSen: A Federated Hybrid Search Engine for building a knowledge graph on-demand,Diego Collarana; Mikhail Galkin; Christoph Lange; Irlán Grangel-González; Maria-Esther Vidal; Sören Auer,Abstract. A vast amount of information about various types of entities is spread acrossseveral parts of the Web; eg; people or organizations on the Social Web; product offers onthe Deep Web or on the Dark Web. These data sources can comprise heterogeneous dataand are equipped with different search capabilities; eg; the Google+ API can return theprofile of a user; while the Twitter API also allows for finding the trends of a place. End userssuch as investigators from law enforcement institutions searching for traces and connectionsof organized crime have to deal with these interoperability problems not only during searchtime but also while merging data collected from different sources. We devise FuhSen; a,*,*,*
Integrating and Benchmarking Entity Linking for Question Answering with Qanary,Dennis Diefenbach; Kuldeep Singh; Andreas Both; Didier Cherix; Christoph Lange; Sören Auer,*,*,*,*
Factorizing Longitudinal Linked Data,Farah Karim; Maria-Esther Vidal; Sören Auer,Abstract. Large collections of longitudinal linked data are publicly available as part of theLinking Open Data (LOD) cloud. Longitudinal linked data are RDF descriptions ofobservations from related sampling frames or sensors at multiple points in time; eg; patientmedical records or climate sensor data. Observations are expressed as measurementswhose values can be repeated several times in a sampling frame; resulting in aconsiderable increase in data volume. In this paper; we devise a factorized compactrepresentation of longitudinal linked data to reduce repetition of same measurements; andpropose algorithms to generate collections of factorized longitudinal linked data. Weempirically study the effectiveness of the proposed factorized representation on linkedobservation data; and we show that the total data volume can be reduced by more than …,*,*,*
VoCol: An Integrated Environment to Support Vocabulary Development with Version Control Systems,Lavdim Halilaj; Niklas Petersen; Irlán Grangel-González; Christoph Lange; Sören Auer; Gökhan Coskun; Steffen Lohmann,*,*,*,*
FuhSen: A Federated Hybrid Search Engine,Diego Collarana; Christoph Lange; Sören Auer; Maria-Esther Vidal; Irlán Grangel-González,Abstract A vast amount of information about various types of entities is spread across severalparts of the Web; eg; persons or organizations in the Social Web; in the Web of Documents;or in the Dark Web. End users; for example; law enforcement institutions searching for tracesof the organized crime; require federated search engines that integrate distributed andheterogeneous information. This paper presents FuhSen; a keyword-based federatedsearch engine that integrates and summarizes information about entities from existing Webs.The RDF-based; federated hybrid search engine FuhSen implements a multi-layeredarchitectural pattern allowing for its adaptability to different domains or use cases. FuhSeninteractively queries and retrieves data from a diverse set of Web APIs; to collect up-to-datedata. Linked Data wrappers around these Web APIs enable ondemand; semantic …,*,*,*
Are LOD Datasets Well Represented? A Data Representation Quality Survey.⋆,Jeremy Debattista; Christoph Lange; Sören Auer,Abstract The widespread and rapid adoption of Linked Data principles gave rise to the Webof Data as we know it. Although data is being continuously added or removed; to or from thisglobal space; there is no indication about its quality. Data quality; defined as “fitness for use”;can affect the prospects of the application that uses this data. In this empirical study; weanalyse; quantify; and understand the quality of how well data is represented on the Web.We use Luzzu [8]; a Linked Data quality assessment framework; to assess 130 datasets(more than 3.5 billion quads) over seven quality metrics from the representational categoryas classified by Zaveri et al.[24]. Following the assessment; we publish the quality metadatafor each of the assessed datasets as dereferenceable Linked Data resources that can belinked.,*,*,*
Dritte Studentenkonferenz Informatik Leipzig 2012,Johannes Schmidt; Thomas Riechert; Sören Auer,Leipziger Informatik Verbund “(LIV) als Zusammenschluss und Interessenverbundverschiedener Informatik-Einrichtungen im Jahr 2003 begründeten Reihe liegt darin; zeitnahund umfassend über abgeschlossene oder laufende wissenschaftliche Arbeiten sowie überneu entstehende Forschungsfelder zu berichten. Die Reihe stellt die innovativeThemenvielfalt in den Herausgeberbänden neben die hohe wissenschaftlicheDurchdringung in Habilitationen und Dissertationen. Zudem ergänzt sieforschungsrelevante Bereiche mit praxisorientierten technischen Beiträgen undDokumentationen.,*,*,*
Improving Linked Data Quality using Outlier Detection,Jeremy Debattista; Christoph Lange; Sören Auer,ABSTRACT With more and more data being published on the Web as Linked Data; WebData quality is increasingly important. While quite some work has been done with regard toquality assessment of Linked Data; only few works have addressed quality improvement. Inthis article; we present an approach for identifying potentially incorrect RDF statements. Theapproach is based on distancebased k-nearest neighbour (k-NN) style clustering ofstatements with the same property and identifying outliers from these clusters. Our methodfollows a three stage approach; which automates the whole process of finding potentiallyincorrect statements for a certain property. In the initial stage; RDF statements are added to areservoir sampler based on Vitter's rejection-acceptance technique. The mapping stagegroups data objects in various cells. Finally; the colouring stage identifies the cells that …,*,*,*
A Survey of Current Approaches for Mapping of Relational Databases to RDF. w3org (2009),Satya S Sahoo; Wolfgang Halb; Sebastian Hellmann; Kingsley Idehen; Ted Thibodeau Jr; Sören Auer; Juan Sequeda; Ahmed Ezzat,*,*,*,*
Linked Enterprise Data,A Sören,VI Vorwort der Herausgeber mantic Web Technologien; sowohl als Produktions-als auch alsDistributionsinfrastruktur für umfassende Datensammlungen; eine zentrale Rolle. Denndurch Semantic Web Technologien werden Daten zu Netzgütern und erlauben neueFormen der Datenhaltung und Bewirtschaftung. Hierbei stellt sich die Frage; inwieweit dieföderalen; selbstorganisierenden und kollaborativen Mechanismen des Webs in denkontrollierten Umgebungen einer Organisation sinnvoll zum Einsatz gebracht werdenkönnen; um neue Ressourcen aus bestehenden Informationsbeständen zu generieren undum von der Fülle an verfügbaren; qualitativ hochwertigen (offenen) Datenquellen im Web zuprofitieren. Dazu diskutieren die einzelnen Beiträge technologische und methodischeAspekte des semantisch gestützten Datenmanagements und zeigen mittels Fallstudien …,*,*,*
Keyword Extraction for Webpage Clusters,Vladimir Salin; Maria Slastihina; Ivan Ermilov; René Speck; Sören Auer; Alexander Sytnik,Abstract. The volume of unstructured information presented on the Internet is constantlyincreasing; together with the total amount of websites and their contents. To process this vastamount of information it is important to distinguish different clusters of related webpages.Such clusters are used; for example; for template induction; keyword extraction; andrecommendation algorithms. A variety of applications (such as semantic analysis systems;crawlers and search engines) utilize clustering algorithms to recognize thematicallyconnected webpages. The clustering is performed based on sets of webpage features; forinstance; hyperlinks between webpages; search logs or DOM tree structures. In this articlewe present an approach for keyword extraction; which utilizes two different clusteringalgorithms. The first algorithm is based on the analysis of the textual information; the …,*,*,*
Facilitating Data-Flows at a Global Publisher using the Linked Data Stack,Christian Dirschl; Katja Eck; Jens Lehmann; Lorenz Bühmann; Sören Auer; Bert Van Nuffelen,Abstract. The publishing industry is at the verge of an era; wherein particular professionalcustomers of publishing products are not so much interested in comprehensive books andjournals; ie traditional publishing products; anymore as they now are interested in possiblystructured information pieces delivered just-in-time as a certain information need arises. Thisrequires a transformation of the publishing workflows towards the production of much richermeta-data for fine-grained and highly interlinked pieces of content. Linked Data can play acrucial role in this transition. The Linked Data Stack is an integrated distribution of alignedtools which support the whole lifecycle of Linked Data from extraction; authoring/creation viaenrichment; interlinking; fusing to maintenance. In this application paper; we describe a real-world usage scenario of the Linked Data Stack at a global publishing company. We give …,*,*,*
SCORVoc: a Vocabulary based on the Supply Chain Operation Reference Model,Niklas Petersen; Irlán Grangel-González; Sören Auer; Gökhan Coskun; Marvin Frommhold; Sebastian Tramp,*,*,*,*
RQUERY: Rewriting Text Queries to Alleviate the Vocabulary Mismatch Problem on RDF Knowledge Bases,Saeedeh Shekarpour; Sören Auer,*,*,*,*
WebS 2010 Program Committee/Reviewers,Christina Feilmayr; Wolfram Wöß; José Francisco Aldana Montes; Kerstin Altmanninger; Sören Auer; Elena Baralis; ISEC Jorge Bernardino; Walter Binder; Brian Blake; Barbara Carminati; DISI Barbara Catania; Cláudio De Souza Baptista; John Debenham; Steven A Demurjian; Ying Ding; Nickolas JG Falkner; Ling Feng; Alfio Ferrara; Elena Ferrari; Abdelkader Hameurlain; Carmem Satie Hara; Bernhard Haslhofer; Stijn Heymans; Hiroyuki Kawano; Ralf Klischewski; Ismael Navas Delgado; Dimitris Plexousakis; Isidoro Ramos; Bernhard Schandl; Ulrich Schiel; Elena Simperl; Bala Srinivasan,Chairs Christina Feilmayr; Johannes Kepler University of Linz; Austria Wolfram Wöß; JohannesKepler University of Linz; Austria … Program Committee Members José Francisco AldanaMontes; Universidad de Málaga; Spain Kerstin Altmanninger; Johannes Kepler UniversityLinz; Austria Sören Auer; University of Leipzig; Germany Elena Baralis; Politecnico di Torino;Italy Jorge Bernardino; ISEC; Polytechnic Institute of Coimbra; Portugal Walter Binder; Universityof Lugano; Switzerland Brian Blake; Georgetown University; Washington; DC; USA AlessioBosca; CELI srl; Turino; Italy Paul Buhler; Modus21; LLC; Charleston; South Carolina; USA BarbaraCarminati; University of Insubria; Italy Barbara Catania; DISI; University of Genoa; Italy SunilChoenni; Rotterdam University & Ministry of Justice; The Hague; The Netherlands BrianDavis; Digital Enterprise Research Institute (DERI); National University of Galway; Ireland …,*,*,*
SEMAPRO 2009,Petre Dini; James Hendler; Josef Noll; René Witte; Filip Zavoral; Dumitru Roman; Umberto Straccia; Massimo Paolucci; Peter Yeh; Sofia J Athenikos; Nima Dokoohaki; Arun Kumar; Meena Nagarajan,Advisory Chairs Petre Dini; Cisco Systems Inc.; USA / Concordia University; Canada JamesHendler; Rensselaer Polytechnic Institute - Troy; USA Josef Noll; ConnectedLife@UNIK /UiO- Kjeller; Norway … Industry-Research Chairs Massimo Paolucci; DOCOMO CommunicationsLaboratories Europe GmbH - Munich; Germany Peter Yeh; Accenture Technology Labs; USA… Publicity Chairs Sofia J. Athenikos; Drexel University; USA Nima Dokoohaki; Royal Instituteof Technology (KTH)- Kista; Sweden Arun Kumar; IBM India Research Laboratory-NewDelhi; India Meena Nagarajan; Wright State University; USA … Technical Program CommitteeChairs René Witte; Concordia University; Canada Filip Zavoral; Charles University inPrague; Czech Republic Dumitru Roman; University of Innsbruck; Austria Umberto Straccia;ISTI - CNR; Italy … Technical Program Committee Harith Alani; University of …,*,*,*
Johannes Heinecke; France Telecom,Sören Auer; Agnese Augello; Ramazan Aygun; Kathy Baker; Lamberto Ballan; Roberto Basili; Stephen Beale; Ivan Bedini; Marco Bertini; Michael Bloodgood; David Bracewell; Thorsten Brants; Volha Bryl; Nicoletta Calzolari; Kasturi Chatterjee; Chao Chen; Jason Corso; Alfredo Cuzzocrea; Claudia D’Amato; Ernesto D’Avanzo; Thierry Declerck; Zhongli Ding; Massimo Esposito; Alex Chengyu Fang; Nicola Fanizzi; David Farwell; Luigi Gallo; Marsal Gavalda; Jose Manuel Gomez-Perez; Thomas Gottron; William I Grosky; Simon Handley; Sanda Harabagiu; Choochart Haruechaiyasak; Takako Hashimoto,Sören Auer; University of Leipzig; Germany Agnese Augello; ICAR-CNR Ramazan Aygun; Universityof Alabama; Huntsville; USA Kathy Baker; US Government; USA Lamberto Ballan; Universityof Florence; Italy Roberto Basili; University of Roma Tor Vergata; Italy Stephen Beale; Universityof Maryland; Baltimore County; USA Ivan Bedini; Bell Labs Marco Bertini; Universita' degli Studidi Firenze; Italy Michael Bloodgood; University of Maryland; USA David Bracewell; LanguageComputer Corporation Thorsten Brants; Google Inc.; USA Volha Bryl; Fondazione Bruno KesslerNicoletta Calzolari; Istituto di Linguistica Computazionale del CNR; Italy Kasturi Chatterjee; TechnoratiMediaInc.; USA Chao Chen; Capital One Bank; USA Matthew Cooper; FXPAL; USA Jason Corso; SUNYat Buffalo; USA Alfredo Cuzzocrea; University of Calabria; Italy Claudia D'Amato; University ofBari; Italy Ernesto D'Avanzo; Università degli Studi di Salerno; Italy Thierry Declerck …,*,*,*
Large-scale RDF Dataset Slicing,Edgard Marx Saeedeh Shekarpour; Sören Auer; Axel-Cyrille Ngonga Ngomo,Abstract—In the last years an increasing number of structured data was published on theWeb as Linked Open Data (LOD). Despite recent advances; consuming and using LinkedOpen Data within an organization is still a substantial challenge. Many of the LOD datasetsare quite large and despite progress in RDF data management their loading and queryingwithin a triple store is extremely time-consuming and resource-demanding. To overcome thisconsumption obstacle; we propose a process inspired by the classical Extract-Transform-Load (ETL) paradigm. In this article; we focus particularly on the selection and extractionsteps of this process. We devise a fragment of SPARQL dubbed SliceSPARQL; whichenables the selection of well-defined slices of datasets fulfilling typical information needs.SliceSPARQL supports graph patterns for which each connected subgraph pattern …,*,*,*
Studentenkonferenz Informatik Leipzig 2011 Leipzig; Deutschland; 2. Dezember 2011 Tagungsband,Sören Auer; Thomas Riechert; Johannes Schmidt,Konferenzen zu Themen der Informatik gibt es viele–doch die SKIL ist in vielerlei Hinsichtanders. Das wohl wichtigste Unterscheidungsmerkmal ist; dass für diese Konferenzausschließlich Studierende zur Einreichung eines Betrags aufgerufen sind. Das Studium derInformatik vermittelt ua Fähigkeiten zur schnellen Erfassung von technischen Inhalten; zurAbstraktion von Konzepten; zur Wiederverwendung und Ubertragung vonLösungskonzepten in neuen Kontexten sowie zur Präsentation und Diskussion vonwissenschaftlichen Inhalten vor einem großen Publikum. Die Studierenden; die für die SKILeinen Beitrag eingereicht haben; können nachweisen; dass sie diese Qualitäten besitzen.Es ist schwer; einen guten wissenschaftlichen Artikel zu schreiben. Dazu ist einstrukturiertes Vorgehen und Disziplin von Nöten. Zunächst muss ein Thema oder ein …,*,*,*
ICSC 2013,Sören Auer; Agnese Augello; Ramazan Aygun; Kathy Baker; Lamberto Ballan; Roberto Basili; Ivan Bedini; Marco Bertini; Michael Bloodgood; David Bracewell; Volha Bryl; Nicoletta Calzolari; Yu Cao; Kasturi Chatterjee; Chao Chen; Jason Corso; Claudia D’Amato; Stamatia Dasiopoulou; Ernesto D'Avanzo; Thierry Declerck; Alexiei Dingli; Massimo Esposito; Alex Chengyu Fang; Nicola Fanizzi; Luigi Gallo; Jose Manuel Gomez-Perez; Thomas Gottron; William I Grosky; Rodrigo Guido; Sanda Harabagiu,Sören Auer; University of Leipzig; Germany Agnese Augello; ICAR-CNR Ramazan Aygun; Universityof Alabama; Huntsville; USA Kathy Baker; US Government; USA Lamberto Ballan; Universityof Florence; Italy Roberto Basili; Univ. of Roma Tor Vergata; Italy Ivan Bedini; Bell Labs MarcoBertini; Universita' degli Studi di Firenze; Italy Michael Bloodgood; University of Maryland; USADavid Bracewell; Language Computer Corporation Volha Bryl; Fondazione Bruno Kessler NicolettaCalzolari; Istituto di Linguistica Computazionale del CNR; Italy Yu Cao; The University of Tennesseeat Chattanooga; USA Kasturi Chatterjee; TechnoratiMedia Inc.; USA Chao Chen; Capital OneBank; USA Matthew Cooper; FXPAL; USA Jason Corso; SUNY at Buffalo; USA ClaudiaD'Amato; University of Bari; Italy Stamatia Dasiopoulou; Informatics and Telematics Institute;Greece Ernesto D'Avanzo; Università degli Studi di Salerno; Italy Thierry Declerck; DFKI …,*,*,*
ICSC 2013,Sven R Kunze; Sören Auer,Dataset Retrieval ..........................................................................................................................................1 Sven R. Kunze and Sören Auer … Structural Parse Tree Features for Text Representation..............................................................................9 Sean Massung; Chengxiang Zhai; and Julia Hockenmaier… A Framework for Composition and Reuse on the Linked Open Data .........................................................17 Cristiano E. Ribeiro and Adriana S. Vivacqua … Emoticon Recommendation forJapanese Computer-Mediated Communication .......................................25 Yuki Urabe; RzepkaRafal; and Kenji Araki … Semantic Entity Search Diversification .......................................................................................................32 Tuukka Ruotsalo and Matias Frosterus … Interlinking Unstructuredand Structured Knowledge in an Integrated Framework ..................................................................................................................................................40 Francesco Corcoglioniti; Marco …,*,*,*
LOD2 Deliverable D5. 1.1: Initial release faceted spatial-semantic browsing component,Claus Stadler; Jens Lehmann; Konrad Höffner; Sören Auer,Abstract: This prototype deliverable consists of a software release of JavaScript widgets forfaceted spatial-semantic browsing and an accompanying deliverable report. The software isopen source and can be downloaded at https://github. com/AKSW/SpatialSemanticBrowsingWidgets. The widgets are being used in the LinkedGeoDataproject. This deliverable describes these widgets in more detail.,*,*,*
LOD2 Deliverable D3. 1.1: Report on Knowledge Extraction from Structured Sources,Sebastian Hellmann; Jörg Unbehauen; Amrapali Zaveri; Jens Lehmann; Sören Auer; Sebastian Tramp; Hugh Williams; Orri Erling; Ted Thibodeau,Abstract: This report contains a survey of Knowledge Extraction from structured sources suchas relational databases; XML and CSV. As the existing literature was either too specializedor incomplete; a general definition of Knowledge Extraction was created that coversstructured as well as unstructured sources (Chapter 2). A summary of the current progresson conversion of relational databases to RDF is given (Chapter 3); followed by a detaileddescription of an examplary tool (Chapter 4); which shall enable the reader to gain an in-depth familiarity with the topic. Based on the definition of Knowledge Extraction and existingsurveys on knowledge extraction from relational databases; classification criteria weredeveloped and refined in a Knowledge Extraction Tool Survey Schema OWL ontology(Chapter 5) Finally; almost 30 existing tools (implementations available) were collected …,*,*,*
reviewer thanks,Aynur Abdurazik; Pekka Abrahamsson; Alain Abran; Steve Adolph; Waseem Ahmed; Ban Al-Ani; Jonathan Aldrich; Sergej Alekseev; Ian Alexander; Tristan Allwood; Thomas Alspaugh; Scott Ambler; Pierre America; Jennitta Andrea; Nicolas Anquetil; Sven Apel; Raman Aravamudhan; Erik Arisholm; Ove Armbrust; Iñigo Artundo; Mike Ashworth; Soren Auer; Amitava Banerjee; Elisa Baniassad; Victor Basili; Benoit Baudry; Leandro Becker; Sarah Beecham; Martin Beer; Andrew Begel; Rachel Bellamy; Paolo Bellavista; Salima Benbernou; Brian Berenbach; Ian Berry; Claude Besner; Danilo Beuche; Jean Bezivin; Robert Biddle; Gavin Bierman; Christian Bird; Andreas Birk; Alan Blackwell; Bob Blainey; Peter Bloodsworth; Jorgen Boegh; Christian Bogner; Marko Boskovic; Gilad Bracha; Chris Branton; Chris Brealey; Marian Bubak; Frank Budinsky; Luigi Buglione; Andrew Burton; Collin Carbno; David Card,*,*,*,*
WI-IAT Workshops 2008,Jason J Jung; Hak Lae Kim; Krzysztof Juszczyszyn; Ngoc Thanh Nguyen; David Peterson; Hong Gee Kim; Cecile Bothorel; Subhasish Dasgupta; Fred Freitas; Daniela Godoy; Jennifer Golback; Adam Jatowt; Dariusz Krol; Peter Mika; Heiko Stoermer; Iwan Tabakow; Anna V Zhdanova; Soeren Auer; Jane Hunter; Alan Ruttenberg; Hideaki Takeda,Organizers Jason J. Jung; Yeungnam University; Korea Tudor Groza; DERI; National Universityof Ireland; Galway; Ireland Hak Lae Kim; DERI; National University of Ireland; Galway; Ireland… Krzysztof Juszczyszyn; Wroclaw University of Technology; Poland Ngoc Thanh Nguyen; WroclawUniversity of Technology; Poland Jason J. Jung; Yeungnam University; Korea Tudor Groza;DERI; National University of Ireland; Galway; Ireland Paolo Torroni; DEIS; Universita diBologna; Italy Siegfried Handschuh; DERI; National University of Ireland; Galway; Ireland DavidPeterson; BoaB Interactive; Australia Hak Lae Kim; DERI; National University of Ireland;Galway; Ireland John Breslin; DERI; National University of Ireland; Galway; Ireland Hong GeeKim; Bio-Medical Knowledge Engineering Lab; Seoul National University; South Korea … ProgramCommittee Cecile Bothorel; Frace Telecom R&D; France Longbing Cao; UTS; Australia …,*,*,*
Towards an Open-Governmental Data Web,Ivan Ermilov; Claus Stadler; Michael Martin; Sören Auer,Abstract. Up to the present day much effort has been made to publish government data onthe Web. However; such data has been published in different formats. For any particularsource and use (eg exploration; visualization; integration) of such information specificapplications have to be written. This limits the overall usability of the information providedand makes it difficult to access information resources. These limitations can be overridden; ifthe information will be provided using a homogeneous data and access model complyingwith the Linked Data principles. In this paper we showcase how raw Open Government Data(OGD) from heterogeneous sources can be processed; converted; published and used onthe Web of Linked Data. In particular we demonstrate our experience in processing of OGDon two use cases: the Digital Agenda Scoreboard and the Financial Transparency …,*,*,*
COMPSAC 2009,Juha-Markus Aalto; Alia Abdelmoty; Alex Abramovich; Khalil Abuosba; Rafael Accorsi; Ademar Aguiar; Sheikh Iqbal Ahamed; Gail-Joon Ahn; Marco Aiello; Sabah Al-Fedaghi; Christo Angelov; Grigoris Antoniou; Mikio Aoyama; Ezendu Ariwa; Tughrul Arslan; Colin Atkinson; Sören Auer; Paris Avgeriou; Alberto Avritzer; Mehmet Emin Aydin; Costin Badica; Doo-Hwan Bae; Susmit Bagchi; Arun Bahulkar; Xiaoying Bai; Emanuel Baker; Janaka Balasooriya; Sergey N Baranov; Luciano Baresi; Samik Basu; Zeki Bayram; Doina Bein; Paolo Bellavista; Fevzi Belli; Fausto Bernardini; Carlo Bertolli; Ricardo Bettati; Ernst Biersack; Filip Blagojevic; Carlo Blundo; Lianne Bodenstaff; Taisuke Boku; Victor Bos; Jan Bosch; Tibor Bosse; Mariusz Bozga; Chiara Brahin; Francois Bronsard; Robert Bruckner; Roberto Bruni; Christof Budnik; John Buford; Michele Bugliesi; Suren Byna; Kursat Cagiltay; Kai-Yuan Cai; Jan Camenisch; Joao Cangussu; Dave Card; Barbara Carmianti; Bertrand du Castel; Darek Ceglarek; Duygu Celik; Christophe Cerin; Sungduk Cha; WK Chan; Melissa Chase; Xiang Chen; TY Chen; YC Chen; Shu-Ching Chen; Weifeng Chen; Yong Chen; Jian-Jia Chen; Xiaochun Cheng; Albert Cheng; Chi-Hung Chi,Juha-Markus Aalto Alia Abdelmoty Alex Abramovich Khalil Abuosba Rafael Accorsi AdemarAguiar Sheikh Iqbal Ahamed Gail-Joon Ahn Marco Aiello Sabah Al-Fedaghi Christo AngelovGrigoris Antoniou Mikio Aoyama Ezendu Ariwa Tughrul Arslan Colin Atkinson Sören Auer ParisAvgeriou Alberto Avritzer Mehmet Emin Aydin Costin Badica Doo-Hwan Bae Susmit Bagchi ArunBahulkar Xiaoying Bai Emanuel Baker Janaka Balasooriya Sergey N. Baranov Luciano BaresiSamik Basu Zeki Bayram Doina Bein Paolo Bellavista Fevzi Belli Fausto Bernardini Carlo BertolliRicardo Bettati Ernst Biersack Filip Blagojevic Carlo Blundo … Lianne Bodenstaff Taisuke BokuVictor Bos Jan Bosch Tibor Bosse Mariusz Bozga Chiara Brahin Francois Bronsard Robert BrucknerRoberto Bruni Christof Budnik John Buford Michele Bugliesi Suren Byna Kursat CagiltayKai-Yuan Cai Jan Camenisch Joao Cangussu Dave Card Barbara Carmianti Bertrand du …,*,*,*
Crowd-sourcing the Evaluation of Linked Data Quality using the Example of DBpedia,Amrapali Zaveri; Dimitris Kontokostas; Mohamed A Sherif; Mohamed Morsey; Lorenz Bühmann; Sören Auer; Jens Lehmann,Abstract. Linked Open Data (LOD) comprises of an unprecedented volume of structured databeing available on the Web. However; these datasets are of very varying quality rangingfrom extensively curated datasets to crowd-sourced and even extracted data of relatively lowquality. We present a methodology for crowd-sourcing the quality assessment of linked dataresources. The first step of the methodology comprises the detection of common qualityproblems and their representation in a quality problem taxonomy. In a second phase; theevaluation of a large number of individual resources according to the quality problemtaxonomy is crowd-sourced. Our methodology is accompanied by a tool supportingparticularly the second phase wherein a user assesses an individual resource andevaluates each fact for correctness. We report about the application of our methodology …,*,*,*
ICSC 2012 Program Committee,Sören Auer; Agnese Augello; Andrew Bagdanov; Lamberto Ballan; Roberto Basili; Ivan Bedini; Marco Bertini; Michael Bloodgood; David Bracewell; Volha Bryl; Nicoletta Calzolari; Antonio Chella; Wei-Bang Chen; Matthew Cooper; Jason Corso; Paulo CG Costa; Alfredo Cuzzocrea; Claudia D’Amato; Ernesto D’Avanzo; Stamatia Dasiopoulou; Thierry Declerck; Mona Talat Diab; Zhongli Ding; Massimo Esposito; Nicholas Evans; Alex Chengyu Fang; Nicola Fanizzi; David Farwell; Marjorie Freedman; Gerald Friedland; Salvatore Gaglio; Stefania Galizia; Luigi Gallo; Antonio Gentile; Salvatore Giammarresi; Jose Manuel Gomez-Perez; Thomas Gottron; William Grosky; Yuanbo Guo; Brian Harrington; Takak Hashimoto; Johannes Heinecke; Anne Hunt; Chengjia Huo; Eero Hyvönen; M José Ibáñez; Nancy Ide; Domenica FioredistellaIezzi; Hasan Jamil; Cliff Joslyn; Artem Katasonov,Sören Auer; Universität Leipzig Agnese Augello; ICAR-CNR Andrew Bagdanov; FAO of the UnitedNations Lamberto Ballan; University of Florence Roberto Basili; University of Roma “TorVergata” Ivan Bedini; Bell Labs Marco Bertini; Università degli Studi di Firenze MichaelBloodgood; University of Maryland David Bracewell; Language Computer Corporation VolhaBryl; Fondazione Bruno Kessler Nicoletta Calzolari; Istituto di Linguistica Computazionale–CNRAntonio Chella; Università di Palermo Wei-Bang Chen; University of Alabama at BirminghamMatthew Cooper; FX Palo Alto Lab; Inc. Jason Corso; SUNY at Buffalo Paulo CG Costa; C4I Centerof Excellence at George Mason University Alfredo Cuzzocrea; ICAR-CNR and University of CalabriaClaudia D'Amato; University of Bari Ernesto D'Avanzo; Università degli Studi di Salerno StamatiaDasiopoulou; Informatics and Telematics Institute Thierry Declerck; DFKI GmbH Mona …,*,*,*
Leveraging the Crowdsourcing of Lexical Resources for Bootstrapping a Linguistic Data Cloud,Jonas Brekle; Sebastian Hellmann; Sören Auer,*,*,*,*
WikiApp–Engineering of Domain-specific Wiki Applications,Darya Tarasowa; Ali Khalili; Ivan Ermilov; Sören Auer,Abstract. Since its inception in the early 2000s; wiki technology became a ubiquitous pillarfor enabling large-scale collaboration. However; the wiki paradigm was mainly applied tounstructured; textual content thus limiting the content structuring; repurposing and reuse.More recently with the appearance of semantic wiki's the wiki concept was also applied andextended towards semantic content with adverse effects on scalability. Often; however;(semi-) structured content should be managed and the collaboration of potentially very large usercommunities around such content should be effectively facilitated. In this paper we presentan approach for applying the wiki paradigm to semi-structured content. We present a datamodel and a model-driven generation approach to implement it. Structured content objectsare versioned thus mimicking sophisticated versioning control. We implement and …,*,*,*
Workshop WebS 2011,Christina Feilmayr; Wolfram Wöß; José Francisco Aldana Montes; Kerstin Altmanninger; Mario Arrigoni Neri; Sören Auer; Elena Baralis; ISEC Jorge Bernardino; Sourav Saha Bhowmick; Walter Binder; Radek Burget; Sunil Choenni; Valeria De Antonellis; Cláudio De Souza Baptista; John Debenham; Steven A Demurjian; Ian J Dickinson; Ying Ding; Nickolas JG Falkner; Bernadette Farias Lóscio; Ling Feng; Alfio Ferrara; Elena Ferrari; Stephan Grimm; Abdelkader Hameurlain; Carmem Satie Hara; Bernhard Haslhofer; Eva Maria Hauth; Stijn Heymans; Hiroyuki Kawano; Ralf Klischewski; In-Young Ko; Ora Lassila; Michele Melchiori; Michele Missikoff; Ismael Navas Delgado; Dimitris Plexousakis; Detlef Plump; Niko Popitsch,Program Committee Co-chairs Christina Feilmayr; Johannes Kepler University Linz; Austria WolframWöß; Johannes Kepler University Linz; Austria … Program Committee José Francisco AldanaMontes; Universidad de Málaga; Spain Kerstin Altmanninger; Johannes Kepler UniversityLinz; Austria Mario Arrigoni Neri; Università degli Studi di Bergamo; Italy Sören Auer; Universityof Leipzig; Germany Elena Baralis; Politecnico di Torino; Italy Jorge Bernardino; ISEC; PolytechnicInstitute of Coimbra; Portugal Sourav Saha Bhowmick; Nanyang Technological University; SingaporeWalter Binder; University of Lugano; Switzerland Radek Burget; Faculty of InformationTechnology; Brno University of Technology; Czech Republic Barbara Catania; DISI; Universityof Genoa; Italy Sunil Choenni; Rotterdam University & Ministry of Justice; The Hague; The NetherlandsValeria De Antonellis; Dipartimento di Elettronica per l'Automazione; Brescia; Italy …,*,*,*
Workshop Web Science (WSW 2010)–Social Computing Applications–,Sören Auer; Claudia Müller-Birn; Steffen Staab,[BLHH+06a];[Shn07].[It][...] aims to map how decentralized information structures can serve[...] scientific; representational and communicational requirements; and to produce designsand design principles governing such structures.[BLHH+06b] While technologies such asSemantic Web; Web Services; and Cloud Computing are germane to the broad proliferationof Web technologies; we also need to understand the human side of the Web; in order toretain its usefulness and benefit to people. This is at the center of attention of Web Scienceand includes; in addition to the aforementioned technological approaches; research relatedto online communities; information diffusion; Web governance; global network structuresbeyond the individual communities on the Web; and incentive and monetization systems[HSH+08]. Because the Web itself is socially embedded; a particular focus of this first GI …,Gesellschaft für Informatik eV (GI) publishes this series in order to make available to a broad public recent findings in informatics (ie computer science and informa-tion systems); to document conferences that are organized in co-operation with GI and to publish the annual GI Award dissertation.,*,*
Creating Knowledge out of Interlinked Data: The Integrated LOD2 Tool Stack,Bert Van Nuffelen; Sebastian Tramp; Sören Auer,*,*,*,*
Creating Knowledge out of Interlinked Data: The Integrated LOD2 Tool Stack,Sebastian Tramp; Bert Van Nuffelen; Philipp Frischmuth; Sören Auer,Abstract. This joint tutorial of the consortium of the European IP project LOD2-CreatingKnowledge out of Interlinked Data will give an overview on the area of creating; managingand using Linked Data sources. As a prerequisite to the main tutorial part; we give anoverview of the life cycle of Linked Data usage and its challenges as well as the LOD2software stack; which is an integrated distribution of aligned tools which support the life-cycle of Linked Data from extraction; authoring/creation over enrichment; interlinking; fusingto visualization and maintenance. Based on this more theoretic explanations; we give adetailed insight into the usage of LOD2 stack both with practical and non-practical parts.More specifically; we present tools and usage scenarios for the following Linked Data lifecycle tasks: extraction (Triplify/D2R); storage and querying (Openlink Virtuoso); authoring …,*,*,*
WikiApp–Engineering of Domain-specific Wiki Applications,Sören Auer; Ali Khalili; Darya Tarasowa; Ivan Ermilov; Timofey Ermilov,ABSTRACT Since its inception in the early 2000s; Wiki technology became a ubiquitouspillar for enabling large-scale collaboration. However; the Wiki paradigm was mainly appliedto unstructured; textual content thus limiting the content structuring; repurposing and reuse.More recently with the appearance of Semantic Wiki's the Wiki concept was also applied andextended towards semantic content with adverse effects on scalability. Often; however;(semi-) structured content should be managed and the collaboration of potentially very large usercommunities around such content should be effectively facilitated. In this paper we present amodeldriven approach for applying the Wiki paradigm to semistructured content and for theengineering of domain-specific wiki applications. The approach is based on the definition ofcontent types; objects and their relationships. Based on a single reflexive relation content …,*,*,*
Web Service Composition to Facilitate Grid and Distributed Computing: Current Approaches and Future Framework,Muhammad Ahtisham Aslam; Sören Auer; Jun Shen; Michael Herrmann,*,*,*,*
Strategies for Modelling and Representing Knowledge about Variant Rich Service Products,Sören Auer; Romy Pfretzschner,Abstract A formal product model contains all information (structured and formalized) tosystematically reproduce a specific product (as economic asset). There exist severalapproaches for formalizing product model information in old economy (as for example indiscrete parts manufacturing). The service sector until now evolved to the most importantsector in all developed economies. Especially knowledge plays a more and more crucialrole for delivering many services. For complex; IT based service products high in variants (aseg insurances; IT outsourcing or public administration services) existing approaches are notsuitable but formalization is desired (since eg it allows easier export or trade of suchproducts). The paper elicits a possible strategy for defining formal product models forknowledge-based services.,Content-und Wissensmanagement,*,*
Towards Zero-Configuration Link Discovery,Axel-Cyrille Ngonga; Sören Auer; Jens Lehmann,ABSTRACT The amount of data published as Linked Data grows steadily. Yet; less than 5%of the triples in the Linked Data Web are links between knowledge bases. Link discoveryframeworks for the Web of Data suffer from two main drawbacks: First; they usually require asignificant amount of time to discover links between data sources. Second; determining theright specification for a link discovery problem remains a tedious task that must be carriedout manually. We present an approach for the semi-automatic determination of the optimalconfiguration for a given link discovery task. Our approach is based on the combination ofactive learning with the timeefficient link discovery framework LIMES. In this paper; wepresent how LIMES can be combined with active learning to enable the semi-automaticdiscovery of an optimal configuration for link discovery. We also present a first evaluation …,*,*,*
Semantikextraktion aus Wikipedia,Jörg Schüppel; Sören Auer; Jens Lehmann,Gegenwärtig erfreut sich das World Wide Web einer nie dagewesenen Beliebtheit. Nie gabes soviele Webseiten und soviele Benutzer; die Informationen dort veröffentlichen oderrecherchieren. Mit der wachsenden Größe des World Wide Web steigt natürlich dieSchwierigkeit für die Benutzer genau die Daten zu finden; die sie auch suchen. Aus diesemGrund rückt die Idee des Semantic Web (vgl. Kapitel 2.1) immer weiter in den Vordergrund.Die Daten und Informationen im Web sollen künftig auch von Maschinen interpretiertwerden können. Das Problem dabei ist; dass ein Computer oft keine strukturiertenInformationen über den Inhalt eines Dokumentes besitzt und somit dem Benutzer nicht ratenkann; eine bestimmte Seite für ein bestimmtes Suchkriterium aufzusuchen. Die einzigeMöglichkeit sind bisher Volltextsuchmethoden; die jedoch häufig nicht die gewünschten …,*,*,*
Towards Semantic Business Processes: Concepts; Methodology; and Implementation,Muhammad Ahtisham Aslam; Sören Auer; Klaus-Peter Fähnrich,*,*,*,*
xOperator-An extensible Semantic Agent for Instant Messaging Networks,Jörg Unbehauen; Michael Martin; Sebastian Hellmann; Sebastian Dietzold; Sören Auer,Abstract. Instant Messaging is in addition to Web and Email the most popular service on theInternet. With xOperator we demonstrate the implementation of a strategy which deeplyintegrates Instant Messaging networks with the Semantic Web. The xOperator concept isbased on the idea of creating an overlay network of collaborative information agents on topof social IM networks. It can be queried using a controlled and easily extensible languagebased on AIML templates. Such a deep integration of semantic technologies and InstantMessaging bears a number of advantages and benefits for users when compared to theseparated use of Semantic Web technologies and IM; the most important ones being contextawareness as well as provenance and trust. Our demonstration showcases how thexOperator approach naturally facilitates enterprise and personal information management …,*,*,*
Improving the Performance of Semantic Web Applications with SPARQL Query Result Caching,Michael Martin; Jörg Unbehauen; Sören Auer,Abstract. The performance of triple stores is one of the major obstacles for the deployment ofsemantic technologies in many usage scenarios. In particular Semantic Web applications;which use triple stores as persistence backends; trade performance in for the advantage offlexibility with regard to information structuring. In order to get closer to the performance ofrelational database-backed Web applications; we developed an approach for improving theperformance of triple stores by caching query results and even complete application objects.The selective invalidation of cache objects; following updates of the underlying knowledgebases; is based on analysing the graph patterns of cached SPARQL queries in order toobtain information which updates will change the query result. We evaluated our approachby extending the BSBM triple store benchmark with an update dimension as well as in …,*,*,*
Scalable Reasoning and Querying for the Semantic Web,Sören Auer; Zachary Ives,ABSTRACT Ontologies; in the OWL language; form the basis of the Semantic Web: theydefine concepts and relationships; and are thus the fundamental model for encodinginformation. As the Semantic Web has been more widely adopted; extremely largeontologies have begun to emerge; particularly in the life sciences. Reasoning about suchontologies requires scalability beyond that of current Description Logic-based reasoningtools. We argue that in many practical settings; the solution to the scalability problem is toexploit relational database systems. Relational queries (views); like Description Logics; arebased on a subset of first-order logic. Many OWL class definitions can be cleanly convertedinto relational view definitions; which; when supplemented with an external operation thatrecomputes multiple views until fixpoint is achieved; can perform inference. We …,*,*,*
Ontology Evolution Patterns Based on Hierarchical Versioning,Sören Auer; Heinrich Herre,Abstract We present an approach to support the evolution of online; distributed; reusable;and extendable ontologies based on the RDF data model. The approach works on the basisof atomic changes; basically additions or deletions of statements to or from an RDF graph.Such atomic changes are aggregated to more complex changes; resulting in a hierarchy ofchanges; thus facilitating the human reviewing process on various levels of detail. Thesederived compound changes may be annotated with meta-information and classified asontology evolution patterns. The introduced ontology evolution patterns in conjunction withappropriate data migration algorithms enable the automatic migration of instance data indistributed environments.,*,*,*
