CrowdDB: answering queries with crowdsourcing,Michael J Franklin; Donald Kossmann; Tim Kraska; Sukriti Ramesh; Reynold Xin,Abstract Some queries cannot be answered by machines only. Processing such queriesrequires human input for providing information that is missing from the database; forperforming computationally difficult functions; and for matching; ranking; or aggregatingresults based on fuzzy criteria. CrowdDB uses human input via crowdsourcing to processqueries that neither database systems nor search engines can adequately answer. It usesSQL both as a language for posing complex queries and as a way to model data. WhileCrowdDB leverages many aspects of traditional database systems; there are also importantdifferences. Conceptually; a major change is that the traditional closed-world assumption forquery processing does not hold for human input. From an implementation perspective;human-oriented query operators are needed to solicit; integrate and cleanse …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,567
Crowder: Crowdsourcing entity resolution,Jiannan Wang; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract Entity resolution is central to data integration and data cleaning. Algorithmicapproaches have been improving in quality; but remain far from perfect. Crowdsourcingplatforms offer a more accurate but expensive (and slow) way to bring human insight into theprocess. Previous work has proposed batching verification tasks for presentation to humanworkers but even with batching; a human-only approach is infeasible for data sets of evenmoderate size; due to the large numbers of matches to be tested. Instead; we propose ahybrid human-machine approach in which machines are used to do an initial; coarse passover all the data; and people are used to verify only the most likely matching pairs. We showthat for such a hybrid system; generating the minimum number of verification tasks of a givensize is NP-Hard; but we develop a novel two-tiered heuristic approach for creating …,Proceedings of the VLDB Endowment,2012,333
Building a database on S3,Matthias Brantner; Daniela Florescu; David Graf; Donald Kossmann; Tim Kraska,Abstract There has been a great deal of hype about Amazon's simple storage service (S3).S3 provides infinite scalability and high availability at low cost. Currently; S3 is used mostlyto store multi-media documents (videos; photos; audio) which are shared by a community ofpeople and rarely updated. The purpose of this paper is to demonstrate the opportunitiesand limitations of using S3 as a storage system for general-purpose database applicationswhich involve small objects and frequent updates. Read; write; and commit protocols arepresented. Furthermore; the cost ($); performance; and consistency properties of such astorage system are studied.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,331
An evaluation of alternative architectures for transaction processing in the cloud,Donald Kossmann; Tim Kraska; Simon Loesing,Abstract Cloud computing promises a number of advantages for the deployment of data-intensive applications. One important promise is reduced cost with a pay-as-you-gobusiness model. Another promise is (virtually) unlimited throughput by adding servers if theworkload increases. This paper lists alternative architectures to effect cloud computing fordatabase applications and reports on the results of a comprehensive evaluation of existingcommercial cloud services that have adopted these architectures. The focus of this work ison transaction processing (ie; read and update workloads); rather than analytics or OLAPworkloads; which have recently gained a great deal of attention. The results are surprising inseveral ways. Most importantly; it seems that all major vendors have adopted a differentarchitecture for their cloud services. As a result; the cost and performance of the services …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,275
Consistency rationing in the cloud: pay only when it matters,Tim Kraska; Martin Hentschel; Gustavo Alonso; Donald Kossmann,Abstract Cloud storage solutions promise high scalability and low cost. Existing solutions;however; differ in the degree of consistency they provide. Our experience using suchsystems indicates that there is a non-trivial trade-off between cost; consistency andavailability. High consistency implies high cost per transaction and; in some situations;reduced availability. Low consistency is cheaper but it might result in higher operational costbecause of; eg; overselling of products in a Web shop. In this paper; we present a newtransaction paradigm; that not only allows designers to define the consistency guaranteeson the data instead at the transaction level; but also allows to automatically switchconsistency guarantees at runtime. We present a number of techniques that let the systemdynamically adapt the consistency level by monitoring the data and/or gathering temporal …,Proceedings of the VLDB Endowment,2009,257
MLbase: A Distributed Machine-learning System.,Tim Kraska; Ameet Talwalkar; John C Duchi; Rean Griffith; Michael J Franklin; Michael I Jordan,ABSTRACT Machine learning (ML) and statistical techniques are key to transforming bigdata into actionable knowledge. In spite of the modern primacy of data; the complexity ofexisting ML algorithms is often overwhelming—many users do not understand the trade-offsand challenges of parameterizing and choosing between different learning techniques.Furthermore; existing scalable systems that support machine learning are typically notaccessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work; we present our vision for MLbase; a novel system harnessingthe power of machine learning for both end-users and ML researchers. MLbase provides (1)a simple declarative way to specify ML tasks;(2) a novel optimizer to select and dynamicallyadapt the choice of learning algorithm;(3) a set of high-level operators to enable ML …,Cidr,2013,206
How is the weather tomorrow?: towards a benchmark for the cloud,Carsten Binnig; Donald Kossmann; Tim Kraska; Simon Loesing,Abstract Traditionally; the goal of benchmarking a software system is to evaluate itsperformance under a particular workload for a fixed configuration. The most prominentexamples for evaluating transactional database systems as well as other components on top(such as a application-servers or web-servers) are the various TPC benchmarks. In thispaper we argue that traditional benchmarks (like the TPC benchmarks) are not sufficient foranalyzing the novel cloud services. Moreover; we present some initial ideas how such a newbenchmark should look like that fits better to the characteristics of cloud computing (eg;scalability; pay-per-use and fault-tolerance). The main challenge of such a new benchmarkis to make the reported results comparable because different providers offer differentservices with different capabilities and guarantees.,Proceedings of the Second International Workshop on Testing Database Systems,2009,180
MDCC: Multi-data center consistency,Tim Kraska; Gene Pang; Michael J Franklin; Samuel Madden; Alan Fekete,Abstract Replicating data across multiple data centers allows using data closer to the client;reducing latency for applications; and increases the availability in the event of a data centerfailure. MDCC (Multi-Data Center Consistency) is an optimistic commit protocol for geo-replicated transactions; that does not require a master or static partitioning; and is stronglyconsistent at a cost similar to eventually consistent protocols. MDCC takes advantage ofGeneralized Paxos for transaction processing and exploits commutative updates with valueconstraints in a quorum-based system. Our experiments show that MDCC outperformsexisting synchronous transactional replication protocols; such as Megastore; by requiringonly a single message round-trip in the normal operational case independent of the master-location and by scaling linearly with the number of machines as long as transaction …,Proceedings of the 8th ACM European Conference on Computer Systems,2013,169
Leveraging transitive relations for crowdsourced joins,Jiannan Wang; Guoliang Li; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract The development of crowdsourced query processing systems has recently attracteda significant attention in the database community. A variety of crowdsourced queries havebeen investigated. In this paper; we focus on the crowdsourced join query which aims toutilize humans to find all pairs of matching objects from two collections. As a human-onlysolution is expensive; we adopt a hybrid human-machine approach which first usesmachines to generate a candidate set of matching pairs; and then asks humans to label thepairs in the candidate set as either matching or non-matching. Given the candidate pairs;existing approaches will publish all pairs for verification to a crowdsourcing platform.However; they neglect the fact that the pairs satisfy transitive relations. As an example; if o 1matches with o 2; and o 2 matches with o 3; then we can deduce that o 1 matches with o 3 …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,131
MLI: An API for distributed machine learning,Evan R Sparks; Ameet Talwalkar; Virginia Smith; Jey Kottalam; Xinghao Pan; Joseph Gonzalez; Michael J Franklin; Michael I Jordan; Tim Kraska,MLI is an Application Programming Interface designed to address the challenges of buildingMachine Learning algorithms in a distributed setting based on data-centric computing. Itsprimary goal is to simplify the development of high-performance; scalable; distributedalgorithms. Our initial results show that; relative to existing systems; this interface can beused to build distributed implementations of a wide variety of common Machine Learningalgorithms with minimal complexity and highly competitive performance and scalability.,Data Mining (ICDM); 2013 IEEE 13th International Conference on,2013,122
Crowdsourced enumeration queries,Beth Trushkowsky; Tim Kraska; Michael J Franklin; Purnamrita Sarkar,Hybrid human/computer database systems promise to greatly expand the usefulness ofquery processing by incorporating the crowd for data gathering and other tasks. Suchsystems raise many implementation questions. Perhaps the most fundamental question isthat the closed world assumption underlying relational query semantics does not hold insuch systems. As a consequence the meaning of even simple queries can be called intoquestion. Furthermore; query progress monitoring becomes difficult due to non-uniformitiesin the arrival of crowdsourced data and peculiarities of how people work in crowdsourcingsystems. To address these issues; we develop statistical tools that enable users andsystems developers to reason about query completeness. These tools can also help drivequery execution and crowdsourcing strategies. We evaluate our techniques using …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,95
Extending XQuery with window functions,Irina Botan; Donald Kossmann; Peter M Fischer; Tim Kraska; Dana Florescu; Rokas Tamosevicius,Abstract This paper presents two extensions for XQuery. The first extension allows thedefinition and processing of different kinds of windows over an input sequence; ie; tumbling;sliding; and landmark windows. The second extension extends the XQuery data model(XDM) to support infinite sequences. This extension makes it possible to use XQuery as alanguage for continuous queries. Both extensions have been integrated into a Java-basedopen source XQuery engine. This paper gives details of this implementation and presentsthe results of running the Linear Road benchmark on the extended XQuery engine.,Proceedings of the 33rd international conference on Very large data bases,2007,94
A sample-and-clean framework for fast and accurate query processing on dirty data,Jiannan Wang; Sanjay Krishnan; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo,Abstract In emerging Big Data scenarios; obtaining timely; high-quality answers toaggregate queries is difficult due to the challenges of processing and cleaning large; dirtydata sets. To increase the speed of query processing; there has been a resurgence ofinterest in sampling-based approximate query processing (SAQP). In its usual formulation;however; SAQP does not address data cleaning at all; and in fact; exacerbates answerquality problems by introducing sampling error. In this paper; we explore an intriguingopportunity. That is; we explore the use of sampling to actually improve answer quality. Weintroduce the Sample-and-Clean framework; which applies data cleaning to a relativelysmall subset of the data and uses the results of the cleaning process to lessen the impact ofdirty data on aggregate query answers. We derive confidence intervals as a function of …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,62
Finding the needle in the big data systems haystack,Tim Kraska,With the increasing importance of big data; many new systems have been developed to"solve" the big data challenge. At the same time; famous database researchers argue thatthere is nothing new about these systems and that they're actually a step backward. Thisarticle sheds some light on this discussion.,IEEE Internet Computing,2013,61
PIQL: Success-tolerant query processing in the cloud,Michael Armbrust; Kristal Curtis; Tim Kraska; Armando Fox; Michael J Franklin; David A Patterson,Abstract Newly-released web applications often succumb to a" Success Disaster;" whereoverloaded database machines and resulting high response times destroy a previouslygood user experience. Unfortunately; the data independence provided by a traditionalrelational database system; while useful for agile development; only exacerbates theproblem by hiding potentially expensive queries under simple declarative expressions. As aresult; developers of these applications are increasingly abandoning relational databases infavor of imperative code written against distributed key/value stores; losing the manybenefits of data independence in the process. Instead; we propose PIQL; a declarativelanguage that also provides scale independence by calculating an upper bound on thenumber of key/value store operations that will be performed for any query. Coupled with a …,Proceedings of the VLDB Endowment,2011,58
A demonstration of the bigdawg polystore system,Aaron Elmore; Jennie Duggan; Mike Stonebraker; Magdalena Balazinska; Ugur Cetintemel; Vijay Gadepally; Jeffrey Heer; Bill Howe; Jeremy Kepner; Tim Kraska; Samuel Madden; David Maier; Timothy Mattson; Stavros Papadopoulos; Jeff Parkhurst; Nesime Tatbul; Manasi Vartak; Stan Zdonik,Abstract This paper presents BigDAWG; a reference implementation of a new architecturefor" Big Data" applications. Such applications not only call for large-scale analytics; but alsofor real-time streaming support; smaller analytics at interactive speeds; data visualization;and cross-storage-system queries. Guided by the principle that" one size does not fit all"; webuild on top of a variety of storage engines; each designed for a specialized use case. Toillustrate the promise of this approach; we demonstrate its effectiveness on a hospitalapplication using data from an intensive care unit (ICU). This complex application serves theneeds of doctors and researchers and provides real-time support for streams of patient data.It showcases novel approaches for querying across multiple storage engines; datavisualization; and scalable real-time analytics.,Proceedings of the VLDB Endowment,2015,55
Automating model search for large scale machine learning,Evan R Sparks; Ameet Talwalkar; Daniel Haas; Michael J Franklin; Michael I Jordan; Tim Kraska,Abstract The proliferation of massive datasets combined with the development ofsophisticated analytical techniques has enabled a wide variety of novel applications such asimproved product recommendations; automatic image tagging; and improved speech-driveninterfaces. A major obstacle to supporting these predictive applications is the challengingand expensive process of identifying and training an appropriate predictive model. Recentefforts aiming to automate this process have focused on single node implementations andhave assumed that model training itself is a black box; limiting their usefulness forapplications driven by large-scale datasets. In this work; we build upon these recent effortsand propose an architecture for automatic machine learning at scale comprised of a cost-based cluster resource allocation estimator; advanced hyper-parameter tuning …,Proceedings of the Sixth ACM Symposium on Cloud Computing,2015,48
Crowddb: Query processing with the vldb crowd,Amber Feng; Michael Franklin; Donald Kossmann; Tim Kraska; Samuel R Madden; Sukriti Ramesh; Andrew Wang; Reynold Xin,Databases often give incorrect answers when data are missing or semantic understandingof the data is required. Processing such queries requires human input for providing themissing information; for performing computationally difficult functions; and for matching;ranking; or aggregating results based on fuzzy criteria. In this demo we present CrowdDB; ahybrid database system that automatically uses crowdsourcing to integrate human input forprocessing queries that a normal database system cannot answer. CrowdDB uses SQL bothas a language to ask complex queries and as a way to model data stored electronically andprovided by human input. Furthermore; queries are automatically compiled and optimized.Special operators provide user interfaces in order to integrate and cleanse human input.Currently CrowdDB supports two crowdsourcing platforms: Amazon Mechanical Turk and …,*,2011,48
Stormy: an elastic and highly available streaming service in the cloud,Simon Loesing; Martin Hentschel; Tim Kraska; Donald Kossmann,Abstract In recent years; new highly scalable storage systems have significantly contributedto the success of Cloud Computing. Systems like Dynamo or Bigtable have underpinnedtheir ability to handle tremendous amounts of data and scale to a very large number ofnodes. Although these systems are designed the store data; the fundamental architecturalproperties and the techniques used (eg; request routing; replication and load balancing) canalso be applied to data streaming systems. In this paper; we present Stormy; a distributedstream processing service for continuous data processing. Stormy is based on proventechniques from existing Cloud storage systems that are adapted to efficiently executestreaming workloads. The primary design focus lies in providing a scalable; elastic; and fault-tolerant framework for continuous data processing; while at the same time optimizing …,Proceedings of the 2012 Joint EDBT/ICDT Workshops,2012,47
Cloudy: A modular cloud storage system,Donald Kossmann; Tim Kraska; Simon Loesing; Stephan Merkli; Raman Mittal; Flavio Pfaffhauser,Abstract This demonstration presents Cloudy; a modular cloud storage system. Cloudyprovides a highly flexible architecture for distributed data storage and is designed to operatewith multiple workloads. Based on a generic data model; Cloudy can be customized to meetapplication requirements. The goal of this demonstration is to show the ability of Cloudy toefficiently process different query languages; and to automatically adapt to varying loadscenarios.,Proceedings of the VLDB Endowment,2010,46
S-Store: a streaming NewSQL system for big velocity applications,Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; John Meehan; Andrew Pavlo; Michael Stonebraker; Erik Sutherland; Nesime Tatbul; Kristin Tufte; Hao Wang; Stanley Zdonik,Abstract First-generation streaming systems did not pay much attention to state managementvia ACID transactions (eg;[3; 4]). S-Store is a data management system that combines OLTPtransactions with stream processing. To create S-Store; we begin with H-Store; a main-memory transaction processing engine; and add primitives to support streaming. Thisincludes triggers and transaction workflows to implement push-based processing; windowsto provide a way to bound the computation; and tables with hidden state to implementscoping for proper isolation. This demo explores the benefits of this approach by showinghow a naïve implementation of our benchmarks using only H-Store can yield incorrectresults. We also show that by exploiting push-based semantics and our implementation oftriggers; we can achieve significant improvement in transaction throughput. We demo two …,Proceedings of the VLDB Endowment,2014,44
Data management in the cloud: promises; state-of-the-art; and open questions,Donald Kossmann; Tim Kraska,Abstract Cloud Computing has the potential to significantly change the IT world. It promisesdramatic reductions in cost and time-to-market. This paper gives an introduction to cloudcomputing; thereby stating how cloud computing tries to fulfill these promises. Furthermore;this paper studies the state-of-the-art of cloud computing platforms and answers the questionof how well the current generation of systems meets these promises. Based on that analysis;this paper states several fundamental questions that need to be addressed when designinga cloud computing platform. The focus of the paper is on database workloads; morespecifically; on online transaction processing workloads (OLTP) in public clouds.,Datenbank-Spektrum,2010,44
An architecture for compiling udf-centric workflows,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Carsten Binnig; Ugur Cetintemel; Stan Zdonik,Abstract Data analytics has recently grown to include increasingly sophisticated techniques;such as machine learning and advanced statistics. Users frequently express these complexanalytics tasks as workflows of user-defined functions (UDFs) that specify each algorithmicstep. However; given typical hardware configurations and dataset sizes; the core challengeof complex analytics is no longer sheer data volume but rather the computation itself; andthe next generation of analytics frameworks must focus on optimizing for this computationbottleneck. While query compilation has gained widespread popularity as a way to tackle thecomputation bottleneck for traditional SQL workloads; relatively little work addresses UDF-centric workflows in the domain of complex analytics. In this paper; we describe a novelarchitecture for automatically compiling workflows of UDFs. We also propose several …,Proceedings of the VLDB Endowment,2015,43
XQuery reloaded,Roger Bamford; Vinayak Borkar; Matthias Brantner; Peter M Fischer; Daniela Florescu; David Graf; Donald Kossmann; Tim Kraska; Dan Muresan; Sorin Nasoi; Markos Zacharioudakis,Abstract This paper describes a number of XQuery-related projects. Its goal is to show thatXQuery is a useful tool for many different application scenarios. In particular; this paper triesto correct a common myth that XQuery is merely a query language and that SQL is the betterquery language. Instead; XQuery is a full-fledged programming language for Webapplications and services. Furthermore; this paper tries to correct a second myth that XQueryis slow. This paper gives an overview of the state-of-the-art in XQuery implementation andoptimization techniques and discusses one particular open-source XQuery processor;Zorba; in more detail. Among others; this paper presents an XQuery Benchmark Servicewhich helps practitioners and XQuery processor vendors to find performance problems in anXQuery processor.,Proceedings of the VLDB Endowment,2009,42
S-Store: streaming meets transaction processing,John Meehan; Nesime Tatbul; Stan Zdonik; Cansu Aslantas; Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; Andrew Pavlo; Michael Stonebraker; Kristin Tufte; Hao Wang,Abstract Stream processing addresses the needs of real-time applications. Transactionprocessing addresses the coordination and safety of short atomic computations. Heretofore;these two modes of operation existed in separate; stove-piped systems. In this work; weattempt to fuse the two computational paradigms in a single system called S-Store. In thisway; S-Store can simultaneously accommodate OLTP and streaming applications. Wepresent a simple transaction model for streams that integrates seamlessly with a traditionalOLTP system; and provides both ACID and stream-oriented guarantees. We chose to build S-Store as an extension of H-Store-an open-source; in-memory; distributed OLTP databasesystem. By implementing S-Store in this way; we can make use of the transaction processingfacilities that H-Store already provides; and we can concentrate on the additional features …,Proceedings of the VLDB Endowment,2015,38
RTP: robust tenant placement for elastic in-memory database clusters,Jan Schaffner; Tim Januschowski; Megan Kercher; Tim Kraska; Hasso Plattner; Michael J Franklin; Dean Jacobs,Abstract In the cloud services industry; a key issue for cloud operators is to minimizeoperational costs. In this paper; we consider algorithms that elastically contract and expanda cluster of in-memory databases depending on tenants' behavior over time whilemaintaining response time guarantees. We evaluate our tenant placement algorithms usingtraces obtained from one of SAP's production on-demand applications. Our experimentsreveal that our approach lowers operating costs for the database cluster of this applicationby a factor of 2.2 to 10; measured in Amazon EC2 hourly rates; in comparison to the state ofthe art. In addition; we carefully study the trade-off between cost savings obtained bycontinuously migrating tenants and the robustness of servers towards load spikes andfailures.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,38
Crowdsourcing applications and platforms: A data management perspective,A Doan; Michael J Franklin; Donald Kossmann; Tim Kraska,Over the past decade; crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcinginclude Wikipedia; Linux; Yahoo! Answers; YouTube; Mechanical Turk-based applications;and much effort is being directed toward developing many more. As is typical for anemerging area; this effort has appeared under many names; including peer production;userpowered systems; user-generated content; collaborative systems; community systems;social systems; social search; social media; collective intelligence; wikinomics; crowdwisdom; smart mobs; mass collaboration; and human computation. The topic has also beendiscussed extensively in books; popular press; and academia (eg;[17; 18; 16; 19; 13; 3; 5; 8;15; 11; 19; 10; 7]). This extensive attention; as well as the many successes of …,Proceedings of the VLDB Endowment,2011,38
CrowdQ: Crowdsourced Query Understanding.,Gianluca Demartini; Beth Trushkowsky; Tim Kraska; Michael J Franklin,ABSTRACT Work in hybrid human-machine query processing has thus far focused on thedata: gathering; cleaning; and sorting. In this paper; we address a missed opportunity to usecrowdsourcing to understand the query itself. We propose a novel hybrid human-machineapproach that leverages the crowd to gain knowledge of query structure and entityrelationships. The proposed system exploits a combination of query log mining; naturallanguage processing (NLP); and crowdsourcing to generate query templates that can beused to answer whole classes of different questions rather than focusing on just a specificquestion and answer.,CIDR,2013,34
The end of slow networks: It's time for a redesign,Carsten Binnig; Andrew Crotty; Alex Galakatos; Tim Kraska; Erfan Zamanian,Abstract The next generation of high-performance networks with remote direct memoryaccess (RDMA) capabilities requires a fundamental rethinking of the design of distributed in-memory DBMSs. These systems are commonly built under the assumption that the networkis the primary bottleneck and should be avoided at all costs; but this assumption no longerholds. For instance; with InfiniBand FDR 4×; the bandwidth available to transfer data acrossthe network is in the same ballpark as the bandwidth of one memory channel. Moreover;RDMA transfer latencies continue to rapidly improve as well. In this paper; we first argue thattraditional distributed DBMS architectures cannot take full advantage of high-performancenetworks and suggest a new architecture to address this problem. Then; we discuss initialresults from a prototype implementation of our proposed architecture for OLTP and OLAP …,Proceedings of the VLDB Endowment,2016,29
Interactive data exploration using semantic windows,Alexander Kalinin; Ugur Cetintemel; Stan Zdonik,Abstract We present a new interactive data exploration approach; called Semantic Windows(SW); in which users query for multidimensional" windows" of interest via standard DBMS-style queries enhanced with exploration constructs. Users can specify SWs using (i) shape-based properties; eg;" identify all 3-by-3 windows"; as well as (ii) content-based properties;eg;" identify all windows in which the average brightness of stars exceeds 0.8". This SWapproach enables the interactive processing of a host of useful exploratory queries that aredifficult to express and optimize using standard DBMS techniques. SW uses a sampling-guided; data-driven search strategy to explore the underlying data set and quickly identifywindows of interest. To facilitate human-in-the-loop style interactive processing; SW isoptimized to produce online results during query execution. To control the tension …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,27
XQuery in the Browser,Ghislain Fourny; Markus Pilman; Daniela Florescu; Donald Kossmann; Tim Kraska; Darin McBeath,Abstract Since the invention of the Web; the browser has become more and more powerful.By now; it is a programming and execution environment in itself. The predominant languageto program applications in the browser today is JavaScript. With browsers becoming morepowerful; JavaScript has been extended and new layers have been added (eg; DOM-Support and XPath). Today; JavaScript is very successful and applications and GUI featuresimplemented in the browser have become increasingly complex. The purpose of this paperis to improve the programmability of Web browsers by enabling the execution of XQueryprograms in the browser. Although it has the potential to ideally replace JavaScript; it ispossible to run it in addition to JavaScript for more flexibility. Furthermore; it allows instantcode migration from the server to the client and vice-versa. This enables a significant …,Proceedings of the 18th international conference on World wide web,2009,27
Tupleware:" Big" Data; Big Analytics; Small Clusters.,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Ugur Çetintemel; Stanley B Zdonik,ABSTRACT There is a fundamental discrepancy between the targeted and actual users ofcurrent analytics frameworks. Most systems are designed for the challenges of the Googlesand Facebooks of the world—processing petabytes of data distributed across large clouddeployments consisting of thousands of cheap commodity machines. Yet; the vast majority ofusers analyze relatively small datasets of up to several terabytes in size; perform primarilycompute-intensive operations; and operate clusters ranging from only a few to a few dozennodes. Targeting these users fundamentally changes the way we should build analyticssystems. This paper describes our vision for the design of TUPLEWARE; a new systemspecifically aimed at complex analytics on small clusters. TUPLEWARE's architecture bringstogether ideas from the database and compiler communities to create a powerful end-to …,CIDR,2015,26
Generalized scale independence through incremental precomputation,Michael Armbrust; Eric Liang; Tim Kraska; Armando Fox; Michael J Franklin; David A Patterson,Abstract Developers of rapidly growing applications must be able to anticipate potentialscalability problems before they cause performance issues in production environments. Anew type of data independence; called scale independence; seeks to address thischallenge by guaranteeing a bounded amount of work is required to execute all queries inan application; independent of the size of the underlying data. While optimization strategieshave been developed to provide these guarantees for the class of queries that are scale-independent when executed using simple indexes; there are important queries for whichsuch techniques are insufficient. Executing these more complex queries scale-independently requires precomputation using incrementally-maintained materialized views.However; since this precomputation effectively shifts some of the query processing …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,25
Repeatability and workability evaluation of SIGMOD 2011,Philippe Bonnet; Stefan Manegold; Matias Bjørling; Wei Cao; Javier Gonzalez; Joel Granados; Nancy Hall; Stratos Idreos; Milena Ivanova; Ryan Johnson; David Koop; Tim Kraska; René Müller; Dan Olteanu; Paolo Papotti; Christine Reilly; Dimitris Tsirogiannis; Cong Yu; Juliana Freire; Dennis Shasha,Abstract SIGMOD has offered; since 2008; to verify the experiments published in the papersaccepted at the conference. This year; we have been in charge of reproducing theexperiments provided by the authors (repeatability); and exploring changes to experimentparameters (workability). In this paper; we assess the SIGMOD repeatability process in termsof participation; review process and results. While the participation is stable in terms ofnumber of submissions; we find this year a sharp contrast between the high participationfrom Asian authors and the low participation from American authors. We also find that mostexperiments are distributed as Linux packages accompanied by instructions on how to setupand run the experiments. We are still far from the vision of executable papers.,ACM SIGMOD Record,2011,25
Vizdom: Interactive analytics through pen and touch,Andrew Crotty; Alex Galakatos; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Machine learning (ML) and advanced statistics are important tools for drawinginsights from large datasets. However; these techniques often require human intervention tosteer computation towards meaningful results. In this demo; we present V izdom; a newsystem for interactive analytics through pen and touch. V izdom's frontend allows users tovisually compose complex workflows of ML and statistics operators on an interactivewhiteboard; and the back-end leverages recent advances in workflow compilationtechniques to run these computations at interactive speeds. Additionally; we are exploringapproximation techniques for quickly visualizing partial results that incrementally refine overtime. This demo will show V izdom's capabilities by allowing users to interactively buildcomplex analytics workflows using real-world datasets.,Proceedings of the VLDB Endowment,2015,21
Stale view cleaning: Getting fresh answers from stale materialized views,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska,Abstract Materialized views (MVs); stored pre-computed results; are widely used to facilitatefast queries on large datasets. When new records arrive at a high rate; it is infeasible tocontinuously update (maintain) MVs and a common solution is to defer maintenance bybatching updates together. Between batches the MVs become increasingly stale withincorrect; missing; and superfluous rows leading to increasingly inaccurate query results.We propose Stale View Cleaning (SVC) which addresses this problem from a data cleaningperspective. In SVC; we efficiently clean a sample of rows from a stale MV; and use theclean sample to estimate aggregate query results. While approximate; the estimated queryresults reflect the most recent data. As sampling can be sensitive to long-tailed distributions;we further explore an outlier indexing technique to give increased accuracy when the …,Proceedings of the VLDB Endowment,2015,17
Getting it all from the crowd,Beth Trushkowsky; Tim Kraska; Michael J Franklin; Purnamrita Sarkar,Abstract: Hybrid human/computer systems promise to greatly expand the usefulness ofquery processing by incorporating the crowd for data gathering and other tasks. Suchsystems raise many database system implementation questions. Perhaps most fundamentalis that the closed world assumption underlying relational query semantics does not hold insuch systems. As a consequence the meaning of even simple queries can be called intoquestion. Furthermore query progress monitoring becomes difficult due to non-uniformitiesin the arrival of crowdsourced data and peculiarities of how people work in crowdsourcingsystems. To address these issues; we develop statistical tools that enable users andsystems developers to reason about tradeoffs between time/cost and completeness. Thesetools can also help drive query execution and crowdsourcing strategies. We evaluate our …,arXiv preprint arXiv:1202.2335,2012,14
How progressive visualizations affect exploratory analysis,Emanuel Zgraggen; Alex Galakatos; Andrew Crotty; Jean-Daniel Fekete; Tim Kraska,The stated goal for visual data exploration is to operate at a rate that matches the pace ofhuman data analysts; but the ever increasing amount of data has led to a fundamentalproblem: datasets are often too large to process within interactive time frames. Progressiveanalytics and visualizations have been proposed as potential solutions to this issue. Byprocessing data incrementally in small chunks; progressive systems provide approximatequery answers at interactive speeds that are then refined over time with increasingprecision. We study how progressive visualizations affect users in exploratory settings in anexperiment where we capture user behavior and knowledge discovery through interactionlogs and think-aloud protocols. Our experiment includes three visualization conditions anddifferent simulated dataset sizes. The visualization conditions are:(1) blocking; where …,IEEE transactions on visualization and computer graphics,2017,13
Fault-tolerant entity resolution with the crowd,Anja Gruenheid; Besmira Nushi; Tim Kraska; Wolfgang Gatterbauer; Donald Kossmann,Abstract: In recent years; crowdsourcing is increasingly applied as a means to enhance dataquality. Although the crowd generates insightful information especially for complex problemssuch as entity resolution (ER); the output quality of crowd workers is often noisy. That is;workers may unintentionally generate false or contradicting data even for simple tasks. Thechallenge that we address in this paper is how to minimize the cost for task requesters whilemaximizing ER result quality under the assumption of unreliable input from the crowd. Forthat purpose; we first establish how to deduce a consistent ER solution from noisy workeranswers as part of the data interpretation problem. We then focus on the next-crowdsourceproblem which is to find the next task that maximizes the information gain of the ER result forthe minimal additional cost. We compare our robust data interpretation strategies to …,arXiv preprint arXiv:1512.00537,2015,11
SampleClean: Fast and Reliable Analytics on Dirty Data.,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu,Abstract An important obstacle to accurate data analytics is dirty data in the form of missing;duplicate; incorrect; or inconsistent values. In the SampleClean project; we have developeda new suite of techniques to estimate the results of queries when only a sample of data canbe cleaned. Some forms of data corruption; such as duplication; can affect samplingprobabilities; and thus; new techniques have to be designed to ensure correctness of theapproximate query results. We first describe our initial project on computing statisticallybounded estimates of sum; count; and avg queries from samples of cleaned data. Wesubsequently explored how the same techniques could apply to other problems in databaseresearch; namely; materialized view maintenance. To avoid expensive incrementalmaintenance; we maintain only a sample of rows in a view; and then leverage …,IEEE Data Eng. Bull.,2015,11
Fine-grained and concurrent access to a virtualized disk in a distributed system,*,A method for updating a block on disk is provided. The method involves one or more clientscommitting log records to queues corresponding to blocks. The method further involvescheckpointing; which includes obtaining a flushing lock on the queue by a client; receivinglog records from the queue; applying the log records to a block on disk; and deleting the logrecords from the queue. A block on disk may be updated by first applying the log recordsfrom a queue to a locally cached version of the block corresponding to the queue. Thelocally cached version may then be stored on the disk replacing the original block on disk orbeing stored as a new version of the original block.,*,2014,11
Tupleware: Distributed Machine Learning on Small Clusters.,Andrew Crotty; Alex Galakatos; Tim Kraska,Abstract There is a fundamental discrepancy between the targeted and actual users ofcurrent analytics frameworks. Most systems are designed for the challenges of the Googlesand Facebooks of the world—petabytes of data distributed across large cloud deploymentsconsisting of thousands of cheap commodity machines. Yet; the vast majority of usersoperate clusters ranging from a few to a few dozen nodes; analyze relatively small datasetsof up to several terabytes in size; and perform primarily compute-intensive operations.Targeting these users fundamentally changes the way we should build analytics systems.This paper describes our vision for the design of Tupleware; a new system specificallyaimed at performing complex analytics (eg; distributed machine learning) on small clusters.Tupleware's architecture brings together ideas from the database and compiler …,IEEE Data Eng. Bull.,2014,11
The new database architectures,Tim Kraska; Beth Trushkowsky,Page 1. Big Data Editor: Tim Kraska • kraskat@cs.brown.edu 72 Published by the IEEE ComputerSociety 1089-7801/13/$31.00 © 2013 IEEE IEEE INTERNET COMPUTING Every big data systemfaces one fundamen- tal challenge: scalability. The common assumption that even the mostobscure data might contain some value; and the result- ing obsession to store as much as possible;make predicting data volumes essentially impossible. Even worse; for interactive (Web)applications; sudden spikes in usage can create short-term resource demands; which go waybeyond a sys- tem's envisioned capacity. Cloud computing lets system administrators adjustresources in an on-demand fashion. How- ever; as application developers began to lever- agethe cloud; many felt that standard databases were too inflexible to adapt to the cloud-scale setting …,IEEE internet computing,2013,11
Mlbase: A distributed machine learning wrapper,Ameet Talwalkar; Tim Kraska; Rean Griffith; John Duchi; Joseph Gonzalez; Denny Britz; Xinghao Pan; Virginia Smith; Evan Sparks; Andre Wibisono; Michael J Franklin; Michael I Jordan,Abstract Machine learning (ML) and statistical techniques are key to transforming big datainto actionable knowledge. In spite of the modern primacy of data; the complexity of existingML algorithms is often overwhelming—many users do not understand the trade-offs andchallenges of parameterizing and choosing between different learning techniques.Furthermore; existing scalable systems that support machine learning are typically notaccessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work; we present our vision for MLbase; a novel system harnessingthe power of machine learning for both end-users and ML researchers. MLbase provides (1)a simple declarative way to specify ML tasks;(2) a novel optimizer to select and dynamicallyadapt the choice of learning algorithm;(3) a set of high-level operators to enable ML …,NIPS Big Learning Workshop,2012,11
Building database applications in the cloud,Tim Kraska,Abstract Cloud computing has become one of the fastest growing fields in computer science.It promises infinite scalability and high availability at low cost. To achieve this goal; cloudsolutions are based on commodity hardware; are highly distributed and designed to be fault-tolerant against network and hardware failures. Although sometimes considered as a hype;the main success of cloud computing is not technology-driven but economical. Cloudcomputing allows companies to outsource the IT infrastructure and thus; to profit from ashorter time-to-market; the economies of scale and the leverage effect of outsourcing.Although the advantages of using cloud computing for building web-based databaseapplications are compelling; so far cloud infrastructure is also subject to certain limitations.By today; there exists no consensus on cloud services; thus different providers offer …,*,2010,11
Cost-based fault-tolerance for parallel data processing,Abdallah Salama; Carsten Binnig; Tim Kraska; Erfan Zamanian,Abstract In order to deal with mid-query failures in parallel data engines (PDEs); differentfault-tolerance schemes are implemented today:(1) fault-tolerance in parallel databases istypically implemented in a coarse-grained manner by restarting a query completely when amid-query failure occurs; and (2) modern MapReduce-style PDEs implement a fine-grainedfault-tolerance scheme; which either materializes intermediate results or implements alineage model to recover from mid-query failures. However; neither of these schemes canefficiently handle mixed workloads with both short running interactive queries as well aslong running batch queries nor do these schemes efficiently support a wide range ofdifferent cluster setups which vary in cluster size and other parameters such as the meantime between failures. In this paper; we present a novel cost-based fault-tolerance …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,10
Estimating the impact of unknown unknowns on aggregate query results,Yeounoh Chung; Michael Lind Mortensen; Carsten Binnig; Tim Kraska,Abstract It is common practice for data scientists to acquire and integrate disparate datasources to achieve higher quality results. But even with a perfectly cleaned and merged dataset; two fundamental questions remain:(1) is the integrated data set complete and (2) what isthe impact of any unknown (ie; unobserved) data on query results? In this work; we developand analyze techniques to estimate the impact of the unknown data (aka; unknownunknowns) on simple aggregate queries. The key idea is that the overlap between differentdata sources enables us to estimate the number and values of the missing data items. Ourmain techniques are parameter-free and do not assume prior knowledge about thedistribution. Through a series of experiments; we show that estimating the impact ofunknown unknowns is invaluable to better assess the results of aggregate queries over …,Proceedings of the 2016 International Conference on Management of Data,2016,9
Planet: making progress with commit processing in unpredictable environments,Gene Pang; Tim Kraska; Michael J Franklin; Alan Fekete,Abstract Latency unpredictability in a database system can come from many factors; such asload spikes in the workload; inter-query interactions from consolidation; or communicationcosts in cloud computing or geo-replication. High variance and high latency environmentsmake developing interactive applications difficult; because transactions may take too long tocomplete; or fail unexpectedly. We propose Predictive Latency-Aware NEtworkedTransactions (PLANET); a new transaction programming model and underlying systemsupport to address this issue. The model exposes the internal progress of the transaction;provides opportunities for application callbacks; and incorporates commit likelihoodprediction to enable good user experience even in the presence of significant transactiondelays. The mechanisms underlying PLANET can be used for admission control; thus …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,9
The case for interactive data exploration accelerators (ideas),Andrew Crotty; Alex Galakatos; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Enabling interactive visualization over new datasets at" human speed" is key todemocratizing data science and maximizing human productivity. In this work; we first arguewhy existing analytics infrastructures do not support interactive data exploration and thenoutline the challenges and opportunities of building a system specifically designed forinteractive data exploration. Finally; we present an Interactive Data Exploration Accelerator(IDEA); a new type of system for interactive data exploration that is specifically designed tointegrate with existing data management landscapes and allow users to explore their datainstantly without expensive data preparation costs.,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,8
Making the case for query-by-voice with echoquery,Gabriel Lyons; Vinh Tran; Carsten Binnig; Ugur Cetintemel; Tim Kraska,Abstract Recent advances in automatic speech recognition and natural languageprocessing have led to a new generation of robust voice-based interfaces. Yet; there is verylittle work on using voice-based interfaces to query database systems. In fact; one mighteven wonder who in her right mind would want to query a database system using voicecommands! With this demonstration; we make the case for querying database systems usinga voice-based interface; a new querying and interaction paradigm we call Query-by-Voice(QbV). We will demonstrate the practicality and utility of QbV for relational DBMSs using ausing a proof-of-concept system called EchoQuery. To achieve a smooth and intuitiveinteraction; the query interface of EchoQuery is inspired by casual human-to-humanconversations. Our demo will show that voice-based interfaces present an intuitive means …,Proceedings of the 2016 International Conference on Management of Data,2016,8
CASTLE: crowd-assisted system for text labeling and extraction,Sean Louis Goldberg; Daisy Zhe Wang; Tim Kraska,Abstract The amount of text data has been growing exponentially and with it the demand forimproved information extraction (IE) efforts to analyze and query such data. While automaticIE systems have proven useful in controlled experiments; in practice the gap betweenmachine learning extraction and human extraction is still quite large. In this paper; wepropose a system that uses crowdsourcing techniques to help close this gap. One of thefundamental issues inherent in using a large-scale human workforce is deciding the optimalquestions to pose to the crowd. We demonstrate novel solutions using mutual informationand token clustering techniques in the domain of bibliographic citation extraction. Ourexperiments show promising results in using crowd assistance as a cost-effective way toclose up the” last mile” between extraction systems and a human annotator.,First AAAI Conference on Human Computation and Crowdsourcing,2013,8
Controlling false discoveries during interactive data exploration,Zheguang Zhao; Lorenzo De Stefani; Emanuel Zgraggen; Carsten Binnig; Eli Upfal; Tim Kraska,Abstract Recent tools for interactive data exploration significantly increase the chance thatusers make false discoveries. They allow users to (visually) examine many hypotheses andmake inference with simple interactions; and thus incur the issue commonly known instatistics as the" multiple hypothesis testing error." In this work; we propose a solution tointegrate the control of multiple hypothesis testing into interactive data exploration systems.A key insight is that existing methods for controlling the false discovery rate (such as FDR)are not directly applicable to interactive data exploration. We therefore discuss a set of newcontrol procedures that are better suited for this task and integrate them in our system;QUDE. Via extensive experiments on both real-world and synthetic data sets wedemonstrate how QUDE can help experts and novice users alike to efficiently control …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,7
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Christopher Ré; Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan,Abstract Machine learning seems to be eating the world with a new breed of high-value data-driven applications in image analysis; search; voice recognition; mobile; and officeproductivity products. To paraphrase Mike Stonebraker; machine learning is no longer azero-billion-dollar business. As the home of high-value; data-driven applications for overfour decades; a natural question for database researchers to ask is: what role should thedatabase community play in these new data-driven machine-learning-based applications?,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,7
Should we all be teaching intro to data science instead of intro to databases?,Bill Howe; Michael J Franklin; Juliana Freire; James Frew; Tim Kraska; Raghu Ramakrishnan,Abstract The Database Community has a unique perspective on the challenges andsolutions of long-term management of data and the value of data as a resource. In currentcomputer science curricula; however; these insights are typically locked up in the context ofthe traditional Intro to Databases class that was developed years (or in some cases;decades) before the modern concept of Data Science arose and embedded in thediscussion of legacy data management systems. We consider how to bring these conceptsfront and center into the emerging wave of Data Science courses; degree programs andeven departments.,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,7
The Multi-Tenant Data Placement Problem,Jan Schaffner; Dean Jacobs; Tim Kraska; Hasso Plattner,Abstract—With the advent of the Software-as-a-Service (SaaS) deployment model;managing operational costs becomes more and more important for providers of hostedsoftware. The cost for hosting and providing a service is directly proportional to theoperational margin that can be achieved when running a SaaS business. Possible avenuesfor reducing operational costs are consolidation (ie co-locating multiple customers onto thesame server) and automation of cluster management (ie migration of customers betweenservers; automatic replication for performance or high availability). In this paper; we proposethe formalization for the problem of assigning “tenants”(ie the customers) to servers of an on-demand database cluster. We will pose this problem in the form of an optimization problem;omitting database specifics and thus presenting the problem in an abstract fashion; using …,DBKDA: The Fourth International Conference on Advances in Databases; Knowledge; and Data Applications,2012,7
PathBank: Web-based querying and visualziation of an integrated biological pathway database,Joshua Wing Kei Ho; Tristan Manwaring; Seok-Hee Hong; Uwe Roehm; David Cho Yau Fung; Kai Xu; Tim Kraska; David Hart,PathBank is a Web-based query and visualization system for biological pathways using anintegrated pathway database. To address the needs for biologists to visualize and analyzebiological pathways; PathBank is designed to be user-friendly; flexible and extensible. It is;to the best of our knowledge; the first Web-based system that allows biological pathways tobe visualized in three dimensions. PathBank demonstrates the ability to automaticallygenerate and layout biological pathways in response to Web-based database query aboutproteins; genes; gene ontology and small molecules. Using a novel OWL-to-relationaldatabase schema generation approach; it can automatically integrate biological data fromdifferent sources that support the BioPAX exchange format (eg KEGG; Bio-Cyc). Thesystems Web interface allows both simple keyword and complex query-based searches …,Computer Graphics; Imaging and Visualisation; 2006 International Conference on,2006,7
Genea: Schema-Aware Mapping of Ontologies into Relational Databases.,Tim Kraska; Uwe Röhm,Abstract Ontologies have become an important mechanism for describing and exchangingdata in the semantic web; as well as in application areas such as bioinformatics or healthcare. This paper addresses the problem of how to efficiently store and query ontologyinstance data using an RDBMS. Our approach automatically creates a schema-awarerelational representation of the ontology using generic mapping rules. Part of the ontologyreasoning; such as on subsumption relationships; is done during schema-generation andalso at load-time of the instance data; so that query processing becomes faster. Weimplemented our approach in an OWL-mapping tool; called Genea; that can automaticallycreate a compact relational schema and load the instance data from a given ontology writtenin OWL. We report on results from a quantitative and qualitative evaluation of our …,COMAD,2006,7
Vistrees: fast indexes for interactive data exploration,Muhammad El-Hindi; Zheguang Zhao; Carsten Binnig; Tim Kraska,Abstract Visualizations are arguably the most important tool to explore; understand andconvey facts about data. As part of interactive data exploration; visualizations might be usedto quickly skim through the data and look for patterns. Unfortunately; database systems arenot designed to efficiently support these workloads. As a result; visualizations often take verylong to produce; creating a significant barrier to interactive data analysis. In this paper; wefocus on the interactive computation of histograms for data exploration. To address thisissue; we present a novel multi-dimensional index structure called VisTree. As a keycontribution; this paper presents several techniques to better align the design of multi-dimensional indexes with the needs of visualization tools for data exploration. Ourexperiments show that the VisTree achieves a speed increase of up to three orders of …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,6
The Case for Learned Index Structures,Tim Kraska; Alex Beutel; Ed H Chi; Jeffrey Dean; Neoklis Polyzotis,Abstract: Indexes are models: a B-Tree-Index can be seen as a model to map a key to theposition of a record within a sorted array; a Hash-Index as a model to map a key to a positionof a record within an unsorted array; and a BitMap-Index as a model to indicate if a datarecord exists or not. In this exploratory research paper; we start from this premise and positthat all existing index structures can be replaced with other types of models; including deep-learning models; which we term learned indexes. The key idea is that a model can learn thesort order or structure of lookup keys and use this signal to effectively predict the position orexistence of records. We theoretically analyze under which conditions learned indexesoutperform traditional index structures and describe the main challenges in designinglearned index structures. Our initial results show; that by using neural nets we are able to …,arXiv preprint arXiv:1712.01208,2017,5
The end of a myth: distributed transactions can scale,Erfan Zamanian; Carsten Binnig; Tim Harris; Tim Kraska,Abstract The common wisdom is that distributed transactions do not scale. But what ifdistributed transactions could be made scalable using the next generation of networks and aredesign of distributed databases? There would no longer be a need for developers to worryabout co-partitioning schemes to achieve decent performance. Application developmentwould become easier as data placement would no longer determine how scalable anapplication is. Hardware provisioning would be simplified as the system administrator canexpect a linear scale-out when adding more machines rather than some complex sub-linearfunction; which is highly application specific. In this paper; we present the design of ournovel scalable database system NAM-DB and show that distributed transactions with thevery common Snapshot Isolation guarantee can indeed scale using the next generation …,Proceedings of the VLDB Endowment,2017,5
Privateclean: Data cleaning and differential privacy,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska,Abstract Recent advances in differential privacy make it possible to guarantee user privacywhile preserving the main characteristics of the data. However; most differential privacymechanisms assume that the underlying dataset is clean. This paper explores the linkbetween data cleaning and differential privacy in a framework we call PrivateClean.PrivateClean includes a technique for creating private datasets of numerical and discrete-valued attributes; a formalism for privacy-preserving data cleaning; and techniques foranswering sum; count; and avg queries after cleaning. We show:(1) how the degree ofprivacy affects subsequent aggregate query accuracy;(2) how privacy potentially amplifiescertain types of errors in a dataset; and (3) how this analysis can be used to tune the degreeof privacy. The key insight is to maintain a bipartite graph relating dirty values to clean …,Proceedings of the 2016 International Conference on Management of Data,2016,5
Answering enumeration queries with the crowd,Beth Trushkowsky; Tim Kraska; Michael J Franklin; Purnamrita Sarkar,摘要人力/计算机混合型数据库系统有望通过融合众人之力来大大拓展查询处理的实用性.这样的系统提出了许多实施问题. 或许; 最基本的问题是作为关系查询语义的基础的封闭世界假定并不能囊括此类系统. 这造成即使简单查询的意义也会产生疑问. 此外;由于众包数据到达的不均匀性; 以及人力在众包系统中工作方式的特性; 使得查询进度监控变得困难. 为解决这些问题; 我们开发了一些统计学工具; 让用户和系统开发人员能够推导出查询完整度. 这些工具也能帮助推动查询执行与众包策略. 我们在一个流行众包平台上进行实验;以此评估我们的技术.,Communications of the ACM,2015,4
SpotADAPT: Spot-Aware (re-) Deployment of Analytical Processing Tasks on Amazon EC2,Dalia Kaulakienė; Christian Thomsen; Torben Bach Pedersen; Ugur Çetintemel; Tim Kraska,Abstract Having constantly increasing amounts of data; the analysis of it is often entrusted fora MapReduce framework. The execution of an analytical workload can be cheapened byadopting cloud computing resources; and in particular by using spot instances (cheap;fluctuating price instances) offered by Amazon Web Services (AWS). The users aiming forthe spot market are presented with many instance types placed in multiple datacenters in theworld; and thus it is difficult to choose the optimal deployment. In this paper; we propose theframework SpotADAPT (Spot-Aware (re-) Deployment of Analytical Processing Tasks) whichis designed to help users by first; estimating the workload execution time on different AWSinstance types; and; second; proposing the deployment (ie; specific availability zone;instance type; pricing model) aligned with user-provided optimization goals (fastest or …,Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP,2015,4
Query processing in the cloud,Raman Mittal; Tim Kraska; Donald Kossmann,Abstract Companies with huge amounts of data have already gone past what couldreasonably fit on a single machine. Relational databases (RDBMS) seem to have alimitation on handling such big data volumes and modern workloads. This has led to theintroduction of various cloud storage systems like Amazon Dynamo; Yahoo PNUTS; GoogleBig Table; Cassandra; Microsoft Azure etc. All of these systems store data in various formatsranging from a simple key-value pair to a specific data model. Similarly; they providedifferent query interfaces to access the stored data. However; SQL is still the standard ofdata processing and none of the modern cloud storage service interface is rich enough toprovide the users with SQL like query capabilities of aggregation and joins. This thesisintroduces a cloud storage system; Cloudy2 which apart from offering a highly scalable …,*,2010,4
Scalable Data Integration by Mapping Data to Queries,Martin Hentschel; Donald Kossmann; Daniela Florescu; Laura Haas; Tim Kraska; Renée J Miller,The goal of a data integration system is to allow users to query diverse information sourcesthrough a schema that is familiar to them. However; there may be many different users whomay have dif-ferent preferred schemas; and the data may be stored in data sources whichuse still other schemas. To integrate data; mapping rules must be defined to map entities ofthe data sources to entities of the users' schemas. In large information systems with manydata sources which serve sophisticated applications; there can be many such mapping rulesand they can be complex. The purpose of this paper is to study the per-formance ofalternative query processing techniques for data integration systems with many complexmapping rules. A new approach; mapping data to queries (MDQ); is presented. Throughextensive performance experiments; it is shown that this approach performs well for …,Technical report/[ETH; Department of Computer Science,2009,4
Crowdsourcing enumeration queries: Estimators and interfaces,Beth Trushkowsky; Tim Kraska; Michael J Franklin; Purnamrita Sarkar; Venketaram Ramachandran,Hybrid human/computer database systems promise to greatly expand the usefulness ofquery processing by incorporating the crowd for data gathering and other tasks. Suchsystems raise many implementation questions. Perhaps the most fundamental issue is thatthe closed world assumption underlying relational query semantics does not hold in suchsystems. As a consequence; the meaning of even simple queries can be called intoquestion. Furthermore; query progress monitoring becomes difficult due to non-uniformitiesin the arrival of crowd-sourced data and peculiarities of how people work in crowd-sourcingsystems. To address these issues; we develop statistical tools that enable users andsystems developers to reason about query completeness. These tools can also help drivequery execution and crowd-sourcing strategies. We evaluate our techniques using …,IEEE Transactions on Knowledge and Data Engineering,2015,3
Putting analytics on the spot: or how to lower the cost for analytics,Elkhan Dadashov; Ugur Cetintemel; Tim Kraska,In contrast to the prevalent cloud services that use fixed-rate pricing models; Amazon's SpotInstance Market uses bidding-driven; dynamic pricing; allowing for remarkable savings inoverall computing costs when used judiciously. The authors describe the salientopportunities and challenges of the spot market for data-intensive analytics and devise costreduction strategies for popular classes of analytics systems operating on it.,IEEE Internet Computing,2014,3
Streaming in the cloud,Stephan Merkli,Abstract Streaming applications are becoming more and more a commodity. They are usedto monitor servers; for fault detection; for message filtering and many other scenarios.However; many scenarios just require one or two continuous queries for which the burden ofinstalling and maintaining a full-blown stream system is often an overkill. The goal of thisthesis is to build a new streaming system; Stormy; which offers streaming capabilities as aservice. Ideally; such a system would automatically adapt the required resources to the load;is fault-tolerant and always consistent. Current streaming systems; however; do not evenmeet the first requirement as they are designed for a fixed machine setup. In this thesis; weexplore how to transfer well-established cloud-techniques to build a new kind of streamingsystem; which provides exactly those properties. Therefore; Stormy is co-developed with …,*,2010,3
Windows for XQuery–Use Cases,Peter M Fischer; Donald Kossmann; Tim Kraska; Rokas Tamosevicius,This document describes the use cases which have driven the XQuery for Windowsextension [1]. The use cases cover topics from different domains like RFID entry controls upto really complex financial cases such as detecting arbitrage possibilities in a financialstream. Although the sample inputs of these use cases are finite; they are all extensible toinfinite data streams. To show the compatibility with other extensions we have includedadditional examples which use GroupBy [2]; XQuery Update [3] and XQueryP [4]. XQueryP isa small extension which enables XQuery expressions to exchange state information throughvariables. This extension makes it easier to develop applications in XQuery without relyingon a host programming language.,*,2006,3
An exploration of crowdsourcing citation screening for systematic reviews,Michael L Mortensen; Gaelen P Adam; Thomas A Trikalinos; Tim Kraska; Byron C Wallace,Abstract Systematic reviews are increasingly used to inform health care decisions; but areexpensive to produce. We explore the use of crowdsourcing (distributing tasks to untrainedworkers via the web) to reduce the cost of screening citations. We used Amazon MechanicalTurk as our platform and 4 previously conducted systematic reviews as examples. For eachcitation; workers answered 4 or 5 questions that were equivalent to the eligibility criteria. Weaggregated responses from multiple workers into an overall decision to include or excludethe citation using 1 of 9 algorithms and compared the performance of these algorithms to thecorresponding decisions of trained experts. The most inclusive algorithm (designating acitation as relevant if any worker did) identified 95% to 99% of the citations that wereultimately included in the reviews while excluding 68% to 82% of irrelevant citations …,Research synthesis methods,2017,2
Revisiting reuse in main memory database systems,Kayhan Dursun; Carsten Binnig; Ugur Cetintemel; Tim Kraska,Abstract Reusing intermediates in databases to speed-up analytical query processing wasstudied in prior work. Existing solutions require intermediate results of individual operators tobe materialized using materialization operators. However; inserting such materializationoperations into a query plan not only incurs additional execution costs but also ofteneliminates important cache-and register-locality opportunities; resulting in even higherperformance penalties. This paper studies a novel reuse model for intermediates; whichcaches internal physical data structures materialized during query processing (due topipeline breakers) and externalizes them so that they become reusable for upcomingoperations. We focus on hash tables; the most commonly used internal data structure inmain memory databases to perform join and aggregation operations. As queries arrive …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,2
Rethinking Distributed Query Execution on High-Speed Networks.,Abdallah Salama; Carsten Binnig; Tim Kraska; Ansgar Scherp; Tobias Ziegler,Abstract In modern high-speed RDMA-capable networks; the bandwidth to transfer dataacross machines is getting close to the bandwidth of the local memory bus. Recent work hasstarted to investigate how to redesign individual distributed query operators to best leverageRDMA. However; all these novel RDMA-based query operators are still designed for aclassical shared-nothing architecture that relies on a shuffle-based execution model toredistribute the data. In this paper; we revisit query execution for distributed databasesystems on fast networks in a more holistic manner by reconsidering all aspects from theoverall database architecture; over the partitioning scheme to the execution model. Ourexperiments show that in the best case our prototype database system called I-Store; whichis designed for fast networks from scratch; provides 3× speed-up over a shuffle-based …,IEEE Data Eng. Bull.,2017,2
Towards a Benchmark for Interactive Data Exploration.,Philipp Eichmann; Emanuel Zgraggen; Zheguang Zhao; Carsten Binnig; Tim Kraska,Abstract Existing benchmarks for analytical database systems such as TPC-DS and TPC-Hare designed for static reporting scenarios. The main metric of these benchmarks is theperformance of running different SQL queries over a predefined database. In this paper; weargue that such benchmarks are not suitable for evaluating modern interactive dataexploration (IDE) systems; which allow data scientists of varying skill levels to manipulate;analyze; and explore large data sets; as well as to build models and apply machine learningat interactive speeds. While query performance is still important for data exploration; webelieve that a much better metric would reflect the number and complexity of insights usersgain in a given amount of time. This paper discusses challenges of creating such a metricand presents ideas towards a new benchmark that simulates typical user behavior and …,IEEE Data Eng. Bull.,2016,2
The bigdawg architecture and reference implementation,Jennie Duggan; Aaron Elmore; Tim Kraska; Sam Madden; Tim Mattson; Michael Stonebraker,ABSTRACT This paper presents the reference implementation of a new architecture forfuture “Big Data” applications. Such applications require “big analytics” as one might expect;but they also require real-time streaming support; real-time analytics; data visualization; andcross-storage queries. We are guided by the principle “one size does not fit all”[7]; and webuild on top of three storage engines; each designed for specialized use cases. In addition;we demonstrate novel support for querying across multiple storage engines as well aspioneering solutions to data visualization. In the remainder of this short paper; we describethe first of three BigDawg reference implementations; Bulldog. In the next two years weexpect to follow with Pitbull and Rottweiler releases.,New England Database Day,2015,2
A framework for adaptive crowd query processing,Beth Trushkowsky; Tim Kraska; Michael J Franklin,Abstract Search engines can yield poor results for information retrieval tasks when theycannot interpret query predicates. Such predicates are better left for humans to evaluate. Wepropose an adaptive processing framework for deciding (a) which parts of a query should beprocessed by machines and (b) the order the crowd should process the remaining parts;optimizing for result quality and processing cost. We describe an algorithm and experimentalresults for the first framework component.,First AAAI Conference on Human Computation and Crowdsourcing,2013,2
Benchmarking web-application architectures in the cloud,Simon Loesing; Donald Kossmann; Tim Kraska,Abstract Cloud computing has introduced a shift of applications and services from local datacenters to remote internet servers. Providing high scalability; fault-tolerance and a pay-per-use cost model; the cloud offers attractive services which cover the entire range of dataprocessing and data storage. While clouds become more and more popular; there arecurrently only few performance evaluations to verify how well cloud services performcompared to traditional architectures. Moreover; with the increasing number of cloudproviders and services; it has become a challenge to compare all offerings. To address thisissue; we have distinguished the most common architectures for applications for the cloudand developed a benchmark taking the specific properties of the cloud into account. Weimplemented this benchmark on services of different cloud providers and carried out an …,*,2009,2
A-Tree: A Bounded Approximate Index Structure,Alex Galakatos; Michael Markovitch; Carsten Binnig; Rodrigo Fonseca; Tim Kraska,Abstract: Index structures are one of the most important tools that DBAs leverage in order toimprove the performance of analytics and transactional workloads. However; with theexplosion of data that is constantly being generated in a wide variety of domains includingautonomous vehicles; Internet of Things (IoT) devices; and E-commerce sites; buildingseveral indexes can often become prohibitive and consume valuable system resources. Infact; a recent study has shown that indexes created as part of the TPC-C benchmark canaccount for 55% of the total memory available in a state-of-the-art in-memory DBMS. Thisoverhead consumes valuable and expensive main memory; and limits the amount of spacethat a database has available to store new data or process existing data. In this paper; wepresent a novel approximate index structure called A-Tree. At the core of our index is a …,arXiv preprint arXiv:1801.10207,2018,1
Revisiting reuse for approximate query processing,Alex Galakatos; Andrew Crotty; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Visual data exploration tools allow users to quickly gather insights from newdatasets. As dataset sizes continue to increase; though; new techniques will be necessary tomaintain the interactivity guarantees that these tools require. Approximate query processing(AQP) attempts to tackle this problem and allows systems to return query results at" humanspeed." However; existing AQP techniques start to break down when confronted with ad hocqueries that target the tails of the distribution. We therefore present an AQP formulation thatcan provide low-error approximate results at interactive speeds; even for queries over raresubpopulations. In particular; our formulation treats query results as random variables inorder to leverage the ample opportunities for result reuse inherent in interactive dataexploration. As part of our approach; we apply a variety of optimization techniques that …,Proceedings of the VLDB Endowment,2017,1
A data quality metric (DQM): how to estimate the number of undetected errors in data sets,Yeounoh Chung; Sanjay Krishnan; Tim Kraska,Abstract Data cleaning; whether manual or algorithmic; is rarely perfect leaving a datasetwith an unknown number of false positives and false negatives after cleaning. In manyscenarios; quantifying the number of remaining errors is challenging because our dataintegrity rules themselves may be incomplete; or the available gold-standard datasets maybe too small to extrapolate. As the use of inherently fallible crowds becomes more prevalentin data cleaning problems; it is important to have estimators to quantify the extent of sucherrors. We propose novel species estimators to estimate the number of distinct remainingerrors in a dataset after it has been cleaned by a set of crowd workers--essentially;quantifying the utility of hiring additional workers to clean the dataset. This problem requiresnew estimators that are robust to false positives and false negatives; and we empirically …,Proceedings of the VLDB Endowment,2017,1
IncMap: A Journey towards Ontology-based Data Integration,Christoph Pinkel; Carsten Binnig; Ernesto Jimenez-Ruiz; Evgeny Kharlamov; Andriy Nikolov; Andreas Schwarte; Christian Heupel; Tim Kraska,Ontology-based data integration (OBDI) allows users to federate over heterogeneous datasources using a semantic rich conceptual data model. An important challenge in ODBI is thecuration of mappings between the data sources and the global ontology. In the last years;we have built IncMap; a system to semi-automatically create mappings between relationaldata sources and a global ontology. IncMap has since been put into practice; both foracademic and in industrial applications. Based on the experience of the last years; we haveextended the original version of IncMap in several dimensions to enhance the mappingquality:(1) IncMap can detect and leverage semantic-rich patterns in the relational datasources such as inheritance for the mapping creation.(2) IncMap is able to leveragereasoning rules in the ontology to overcome structural differences from the relational data …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,1
Spotlytics: How to Use Cloud Market Places for Analytics?,Tim Kraska; Elkhan Dadashov; Carsten Binnig,In contrast to fixed-priced cloud computing services; Amazon's Spot market uses a demand-driven pricing model for renting out virtual machine instances. This allows for remarkablesavings when used intelligently. However; a peculiarity of Amazon's Spot market is; thatmachines can suddenly be taken away from the user if the price on the market increases.This can be considered as a distinct form of a machine failure. In this paper; we first analyzeAmazon's current spot market rules and based on the results develop a general marketmodel. This model is valid for Amazon's current Spot service but also many potentialvariations of it; as well as other cloud computing markets. Using the developed marketmodel; we then make recommendations on how to deploy analytical systems with thefollowing three fault-tolerance/recovery strategies: re-execution as used by traditional …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,1
Acquiring Object Experiences at Scale,John Oberlin; Maria Meier; Tim Kraska; Stefanie Tellex,Abstract—The aim of this project is to improve the performance of automatic object detection;tracking and manipulation by using robots to collect a corpus of perceptual data for onemillion real-world objects. Robots lack the ability to robustly identify; localize; andmanipulate the objects in our daily lives. The field of object detection and recognition isdriven by annotated corpora [21; 8; 15; 26] which researchers use to train and test models[13; 10]. These corpora consist of photos taken by a human photographer and may containmany examples of objects; but typically only a single view of each individual object. Existingcorpora of object instances contain many fewer examples (on the order of hundreds) and noexperience interacting with the object [14; 23]. Our proposed approach; in contrast; uses anindustrial robot (Baxter) to automatically collect a database of object models; including …,AAAI-RSS Special Workshop on the 50th Anniversary of Shakey: The Role of AI to Harmonize Robots and Humans; blue Sky Award,2015,1
SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks,Linnan Wang; Jinmian Ye; Yiyang Zhao; Wei Wu; Ang Li; Shuaiwen Leon Song; Zenglin Xu; Tim Kraska,Abstract: Going deeper and wider in neural architectures improves the accuracy; while thelimited GPU DRAM places an undesired restriction on the network design domain. DeepLearning (DL) practitioners either need change to less desired network architectures; ornontrivially dissect a network across multiGPUs. These distract DL practitioners fromconcentrating on their original machine learning tasks. We present SuperNeurons: adynamic GPU memory scheduling runtime to enable the network training far beyond theGPU DRAM capacity. SuperNeurons features 3 memory optimizations;\textit {LivenessAnalysis};\textit {Unified Tensor Pool}; and\textit {Cost-Aware Recomputation}; all togetherthey effectively reduce the network-wide peak memory usage down to the maximal memoryusage among layers. We also address the performance issues in those memory saving …,arXiv preprint arXiv:1801.04380,2018,*
Slice Finder: Automated Data Slicing for Model Interpretability,Yeounoh Chung; Tim Kraska; Steven Euijong Whang; Neoklis Polyzotis,ABSTRACT As machine learning (ML) systems become democratized; helping users easilydebug their models becomes increasingly important. Yet current data tools are still primitivewhen it comes to helping users trace model performance problems all the way to the data.We focus on the particular problem of slicing data to identify subsets of the training datawhere the model performs poorly. Unlike general techniques (eg; clustering) that can findarbitrary slices; our goal is to find interpretable slices (which are easier to take actioncompared to arbitrary subsets) that are problematic and large. We propose Slice Finder;which is an interactive framework for identifying such slices using statistical techniques. Theslices can be used for applications like diagnosing model fairness and fraud detectionwhere describing slices that are interpretable to humans is necessary.,*,2018,*
What you see is not what you get!: Detecting Simpson's Paradoxes during Data Exploration,Yue Guo; Carsten Binnig; Tim Kraska,Abstract Visual data exploration tools; such as Vizdom or Tableau; significantly simplify dataexploration for domain experts and; more importantly; novice users. These tools allow todiscover complex correlations and to test hypotheses and differences between variouspopulations in an entirely visual manner with just a few clicks; unfortunately; often ignoringeven the most basic statistical rules. For example; there are many statistical pitfalls that auser can" tap" into when exploring data sets. As a result of this experience; we started tobuild QUDE [1]; the first system to Quantifying the Uncertainty in Data Exploration; which ispart of Brown's Interactive Data Exploration Stack (called IDES). The goal of QUDE is toautomatically warn and; if possible; protect users from common mistakes during the dataexploration process. In this paper; we focus on a different type of error; the Simpson's …,Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,2017,*
Approximate Query Processing for Interactive Data Science,Tim Kraska,Abstract shift in the algorithms and tools used to analyze data towards more interactivesystems with highly collaborative and visual interfaces. Ideally; a data scientist and a domainexpert should be able to make discoveries together by directly manipulating; analyzing; andvisualizing data on the spot; for example; using an interactive whiteboard like the recentlyreleased Microsoft Surface Hub. While such an interactive pattern would democratize datascience and make it more accessible to a wider range of users; it also requires a rethinkingof the full analytical stack. Most importantly; it necessitates the next generation ofapproximate query processing (AQP) techniques to guarantee (visual) results at interactivespeeds during the data exploration process. The first generation of AQP focused on onlineaggregation for simple OLAP queries; a small subset of the functionality needed for data …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Safe Visual Data Exploration,Zheguang Zhao; Emanuel Zgraggen; Lorenzo De Stefani; Carsten Binnig; Eli Upfal; Tim Kraska,Abstract Exploring data via visualization has become a popular way to understand complexdata. Features or patterns in visualization can be perceived as relevant insights by users;even though they may actually arise from random noise. Moreover; interactive dataexploration and visualization recommendation tools can examine a large number ofobservations; and therefore result in further increasing chance of spurious insights. Thuswithout proper statistical control; the risk of false discovery renders visual data explorationunsafe and makes users susceptible to questionable inference. To address these problems;we present QUDE; a visual data exploration system that interacts with users to formulatehypotheses based on visualizations and provides interactive control of false discoveries.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Data Science Education: We're Missing the Boat; Again,Bill Howe; Michael Franklin; Laura Haas; Tim Kraska; Jeffrey Ullman,In the first wave of data science education programs; data engineering topics (systems;scalable algorithms; data management; integration) tended to be de-emphasized in favor ofmachine learning and statistical modeling. The anecdotal evidence suggests this was amistake: data scientists report spending most of their time grappling with data far upstream ofmodeling activities. A second wave of data science education is emerging; one withincreased emphasis on practical issues in ethics; legal compliance; scientific reproducibility;data quality; and algorithmic bias. The data engineering community has a second chance toinfluence these programs beyond just providing a set of tools. In this panel; we'll discuss therole of data engineering in data science education programs; and how best to capitalize onemerging opportunities in this space.,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*
Letter from the Special Issue Editor.,Tim Kraska,Globalization has significantly altered the way in which the world operates. More than everbefore are businesses required to operate at a global scale with customers expecting fastand seamless access to applications independent of where they are located. Similar; alsothe availability requirements for applications have changed over the last years. Whereas adecade ago; it was still sufficient to be able to sustain a single machine failure; nowadaysmission-critical applications are expected to tolerate an entire data-center outage. Thus; it isno longer sufficient to think about data management at the scale of a few servers in somebasement. Rather; we need to think about data management at the global-scale; from how isdata replicated across data centers; over how bandwidth and latency of slow wide-areareplication can be avoided or hidden to not negatively impact the application latency; up …,IEEE Data Eng. Bull.,2017,*
A Data Quality Metric (DQM),Yeounoh Chung; Sanjay Krishnan; Tim Kraska,ABSTRACT Dirty data in the form of duplicate entities; inconsistencies; and missing values;can significantly affect data analysis. In almost all data processing pipelines; some form ofdata cleaning is required for reliable analysis. As the use of crowds becomes more prevalentin data cleaning problems; it is important to reason about the impact of crowd's inherentnondeterminism on the ultimate accuracy of any subsequent analytics. At any given time;there may be a large number of unknown errors in a data set missed by the crowd workers.To this end; this paper proposes techniques to address this issues with a metric thatestimates the number of remaining errors in a data set after crowd-sourced data cleaning.This problem is similar to species estimation problems; and we propose several novelestimators that are robust to false positives and compatible with prioritization. We show …,How to Estimate the Number of Undetected Errors in Data Sets. PVLDB,2017,*
Revisiting Reuse in Main Memory Database Systems,Kayhan Dursun Carsten Binnig Ugur Cetintemel; Tim Kraska,ABSTRACT Reusing intermediates in databases to speed-up analytical query processinghas been studied in the past. Existing solutions typically require intermediate results ofindividual operators to be materialized into temporary tables to be considered for reuse insubsequent queries. However; these approaches are fundamentally ill-suited for use inmodern main memory databases. The reason is that modern main memory DBMSs aretypically limited by the bandwidth of the memory bus; thus query execution is heavilyoptimized to keep tuples in the CPU caches and registers. To that end; adding additionalmaterialization operations into a query plan not only add additional traffic to the memory busbut more importantly prevent the important cache-and registerlocality opportunities resultingin high performance penalties. In this paper we study a novel reuse model for …,arXiv preprint arXiv:1608.05678,2016,*
Dark Data: Are we solving the right problems?,Michael Cafarella; Ihab F Ilyas; Marcel Kornacker; Tim Kraska; Christopher Ré,With the increasing urge of the enterprises to ingest as much data as they can in what'scommonly referred to as “Data Lakes”; the new environment presents serious challenges totraditional ETL models and to building analytic layers on top of well-understood globalschema. With the recent development of multiple technologies to support this “load-first”paradigm; even traditional enterprises have fairly large HDFS-based data lakes now. Theyhave even had them long enough that their first generation IT projects delivered on some;but not all; of the promise of integrating their enterprise's data assets. In short; we movedfrom no data to Dark data. Dark data is what enterprises might have in their possession;without the ability to access it or with limited awareness of what this data represents. Inparticular; business-critical information might still remain out of reach. This panel is about …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
CrowdFilter: Semi-Automated Multi-Criteria Filtering in the Open World,Michael Lind Mortensen; Byron C Wallace; Tim Kraska; Ira Assent,Abstract: Multi-criteria filtering of mixed open/closed-world data is a time-consuming task;requiring significant manual effort when latent open-world attributes are present. In this workwe introduce a novel open-world filtering framework CrowdFilter; enabling automatic UIgeneration and label elicitation for complex multi-criteria search problems throughcrowdsourcing. The CrowdFilter system is capable of supporting both criteria-level labelsand n-gram rationales; capturing the human decision making process behind each filteringchoice. Using the data provided through CrowdFilter we also introduce a novel multi-criteriaactive learning method; capable of incorporating labels and n-gram rationales per inclusioncriteria; and thus capable of determining both clear includes/excludes; as well as complexborderline cases. By incorporating the active learning approach into the elicitation …,22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2016,*
Transaction Processing over High-speed Networks,Erfan Zamanian; Tim Kraska; Maurice Herlihy; Stan Zdonik,Abstract By avoiding high cost of disk I/O; memory-resident OLTP databases reduce theruntime of typical singlesited transactions to a fraction of a millisecond. With disk gone fromthe picture; network has become the next bottleneck. In fact; the traditional wisdom is thatnetwork is the new disk; and distributed transactions must be avoided as much as possible;through techniques such as partitioning. However; recent technological trends toward low-latency; high-bandwidth networks which are capable of minimizing communicationoverhead through Remote Direct Memory Access (RDMA) are changing our view on thefundamental assumption that network is significantly the bottleneck. For example; the latencyin InfiniBand FDR/EDR is less than 1µs; two orders of magnitude faster than the traditional1Gb Ethernet; and is only one order of magnitude slower than local memory access (0.1 …,*,2015,*
Smart Monitor System For Automatic Anomaly Detection@ Baidu,Xianping Qu,Abstract: Billions of requests are supported by hundreds of thousands of servers in Baidu.So many servers and modules bring a huge challenge to engineers for anomaly detection.When an anomaly occurs; various alarms and incidents are sent to engineers. It is verydifficult to find the root cause based on large non-organized monitoring data and alarms.Thus; we tried to build a smarter monitoring system named BIMS (Baidu IntelligentMonitoring System) to help engineers to analyze the problems and give the most possiblereasons for important anomaly such as revenue loss.,*,2015,*
The Expected Optimal Labeling Order Problem for Crowdsourced Joins and Entity Resolution,Jiannan Wang; Guoliang Li; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract: In the SIGMOD 2013 conference; we published a paper extending our earlier workon crowdsourced entity resolution to improve crowdsourced join processing by exploitingtransitive relationships [Wang et al. 2013]. The VLDB 2014 conference has a paper thatfollows up on our previous work [Vesdapunt et al.; 2014]; which points out and corrects amistake we made in our SIGMOD paper. Specifically; in Section 4.2 of our SIGMOD paper;we defined the" Expected Optimal Labeling Order"(EOLO) problem; and proposed analgorithm for solving it. We incorrectly claimed that our algorithm is optimal. In their paper;Vesdapunt et al. show that the problem is actually NP-Hard; and based on that observation;propose a new algorithm to solve it. In this note; we would like to put the Vesdapunt et al.results in context; something we believe that their paper does not adequately do.,arXiv preprint arXiv:1409.7472,2014,*
Distributed Machine Learning,Alex Galakatos; Andrew Crotty; Tim Kraska,Distributed machine learning refers to multi-node machine learning algorithms and systemsthat are designed to improve performance; increase accuracy; and scale to larger input datasizes. Increasing the input data size for many algorithms can significantly reduce thelearning error and can often be more effective than using more complex methods [8].Distributed machine learning allows companies; researchers; and individuals to makeinformed decisions and draw meaningful conclusions from large amounts of data. Manysystems exist for performing machine learning tasks in a distributed environment. Thesesystems fall into three primary categories: database; general; and purpose-built systems.Each type of system has distinct advantages and disadvantages; but all are used in practicedepending upon individual use cases; performance requirements; input data sizes; and …,*,2014,*
Methuselah: Intelligent Data Aging,Nick Lanham; Tim Kraska; Michael Franklin,ABSTRACT Recent years has seen an ever widening gulf develop between access times fordata stored in memory versus data on disk. Concurrently; growth in main memory sizes hasled to large gains in the popularity of database systems that keep their working sets primarilyin memory. These systems make the assumption that either all data in always in memory; orthat access to disk; managed by a standard buffer pool; will suffice. However; with data sizesgrowing steadily and more quickly than available main memory; it is clear that all in-memorysystems will need some way to move data to a cold backing store. This paper proposes anew online; statistics based; batchoriented technique to allow an RDBMS to leverage coldstorage to increase data capacity without overly impacting query performance. Our solutioncouples well with semantic knowledge about an application; making it easy to take …,*,2014,*
Workload Management for Main Memory Databases in Data Clouds,Jan Schaffner; Alexander Zeier; Hasso Plattner; Dr- Helmert-Str; Tim Kraska; Michael J Franklin; Michael I Jordan; David A Patterson; Dean Jacobs,Abstract In this report we give a formal description for the problem of assigning tenants toservers of an ondemand system; which is one of the key problems in our Future SOC Labproject. We will pose this problem as an optimization problem and omit database specificsand pose the problem in an abstract fashion; using the metaphor of assigning tokens tobaskets (ie; tenants to servers). We present a first set of greedy algorithms and describe howwe used the computing resources of the Future SOC Lab in the process of experimentation.We outline the next steps for our project.,Proceedings of the Fall 2010 Future SOC Lab Day,2011,*
Towards Interactive Curation & Automatic Tuning of ML Pipelines,Carsten Binnig; Benedetto Buratti1 Yeounoh Chung1 Cyrus Cousins; Dylan Ebert; Tim Kraska; Zeyuan Shang1 Isabella Tromba3 Eli Upfal; Linnan Wang1 Robert Zeleznik; Emanuel Zgraggen,ABSTRACT Democratizing Data Science requires a fundamental rethinking of the way dataanalytics and model discovery is done. Available tools for analyzing massive data sets andcurating machine learning models are limited in a number of fundamental ways. First;existing tools require well-trained data scientists to select the appropriate techniques to buildmodels and to evaluate their outcomes. Second; existing tools require heavy datapreparation steps and are often too slow to give interactive feedback to domain experts inthe model building process; severely limiting the possible interactions. Third; current tools donot provide adequate analysis of statistical risk factors in the model development. In thiswork; we present the first iteration of QuIC-M (pronounced quick-m); an interactive humanin-the-loop data exploration and model building suite. The goal is to enable domain experts …,*,*,*
Learned Index Structures,Tim Kraska; Alex Beutel; H Chi; Jeffrey Dean; Neoklis Polyzotis,ABSTRACT Indexes are models: a BTree-Index can be seen as a model to map a key to theposition of a record within a sorted array; a Hash-Index as a model to map a key to a positionof a record within an unsorted array; and a BitMap-Index as a model to indicate if a datarecord exists or not. In this talk; we start from this premise and posit that all existing indexstructures can be replaced with other types of models; including deeplearning models;which we term learned indexes. The key idea is that a model can learn the sort order orstructure of lookup keys and use this signal to effectively predict the position or existence ofrecords. Our initial results show; that by using simple neural nets we are able to outperformcache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude inmemory over several real-world data sets. More importantly though; we believe that the …,*,*,*
Investigating the Effect of the Multiple Comparisons Problem in Visual Analysis,Emanuel Zgraggen; Zheguang Zhao; Robert Zeleznik; Tim Kraska,ABSTRACT The goal of a visualization system is to facilitate data-driven insight discovery.But what if the insights are spurious? Features or patterns in visualizations can be perceivedas relevant insights; even though they may arise from noise. We often comparevisualizations to a mental image of what we are interested in: a particular trend; distributionor an unusual pattern. As more visualizations are examined and more comparisons aremade; the probability of discovering spurious insights increases. This problem is well-knownin Statistics as the multiple comparisons problem (MCP) but overlooked in visual analysis.We present a way to evaluate MCP in visualization tools by measuring the accuracy of userreported insights on synthetic datasets with known ground truth labels. In our experiment;over 60% of user insights were false. We show how a confirmatory analysis approach that …,*,*,*
Screen Shot 2017-03-13 at 5.15. 39 PM,Sam Madden; Jane Greenberg; Carsten Binnig; Tim Kraska; Danny Weitzner; Sam Grabus,Summary A part of the NSF Big Data regional innovation hub program; the Northeast hub; isaddressing key data sharing challenges by:• Creating a licensing model for data thatfacilitates sharing data that is not necessarily open or free between different organizations;•Developing a prototype data sharing software platform; ShareDB; which will enforces theterms and restrictions of the developed licenses; and• Developing and integrating relevantmetadata that will accompany the datasets shared under the different licenses; making themeasily searchable and interpretable.,*,*,*
A Machine Learning Approach to Databases Indexes,Alex Beutel; Tim Kraska; H Chi; Jeffrey Dean; Neoklis Polyzotis,Abstract Databases rely on indexing data structures to efficiently perform many of their coreoperations. In order to look up all records in a particular range of keys; databases use aBTree-Index. In order to look up the record for a single key; databases use a Hash-Index. Inorder to check if a key exists; databases use a BitMap-Index (a bloom filter). These datastructures have been studied and improved for decades; carefully tuned to best utilize eachCPU cycle and cache available. However; they do not focus on leveraging the distribution ofdata they are indexing. In this paper; we demonstrate that these critical data structures aremerely models; and can be replaced with more flexible; faster; and smaller machine learnedneural networks. Further; we demonstrate how machine learned indexes can be combinedwith classic data structures to provide the guarantees expected of database indexes. Our …,*,*,*
Controlling False Discoveries During Interactive Data Exploration,Zheguang Zhao Lorenzo De Stefani Emanuel; Zgraggen Carsten Binnig; Eli Upfal; Tim Kraska,ABSTRACT Recent tools for interactive data exploration significantly increase the chancethat users make false discoveries. The crux is that these tools implicitly allow the user to testa large body of different hypotheses with just a few clicks thus incurring in the issuecommonly known in statistics as the “multiple hypothesis testing error”. In this paper; wepropose solutions to integrate multiple hypothesis testing control into interactive dataexploration tools. A key insight is that existing methods for controlling the false discovery rate(such as FDR) are not directly applicable for interactive data exploration. We thereforediscuss a set of new control procedures that are better suited and integrated them in oursystem called AWARE. By means of extensive experiments using both real-world andsynthetic data sets we demonstrate how AWARE can help experts and novice users alike …,*,*,*
Data Engineering,Abdallah Salama; Carsten Binnig; Tim Kraska; Ansgar Scherp; Tobias Ziegler; Animesh Trivedi Stuedi; Jonas Pfefferle; Radu Stoica; Bernard Metzler; Nikolas Ioannou; Ioannis Koltsidas; Xiaoyi Lu; Dipti Shankar; Dhabaleswar K DK Panda; Chinmay Kulkarni; Aniraj Kesavan; Robert Ricci; Ryan Stutsman,The Technical Committee on Data Engineering held an election last fall for chair of the TC.The voting deadline was December 22 of last year. The candidates were Xiaofang Zhou andErich Neuhold. My thanks both candidates for being willing to run. Being chair of theTechnical Committee is largely invisible; but it is an important responsibility for the successof the data engineering community. The winner; with 69% of the vote is the current chair;Xiaofang Zhou; who has now won his second term. Congratulations to Xiaofang for hiselectoral victory. Xiaofang knows what the job entails; is experienced in doing it; and does itwell. I very much appreciate Xiaofang's efforts and his continued involvement; both at theTCDE and at the Computer Society more widely.,*,*,*
Data Engineering,Aditya Parameswarany; Akash Das Sarma; Vipul Venkataramani; Olga Papaemmanouil; Yanlei Diao; Kyriaki Dimitriadou; Liping Peng; Philipp Eichmann; Emanuel Zgraggen; Zheguang Zhao; Carsten Binnig; Tim Kraska; Michael R Anderson; Dolan Antenucci; Michael Cafarella,Abstract As one of the successful forms of using Wisdom of Crowd; crowdsourcing; has beenwidely used for many human intrinsic tasks; such as image labeling; natural languageunderstanding; market predication and opinion mining. Meanwhile; with advances inpervasive technology; mobile devices; such as mobile phones; tablets; and PDA; havebecome extremely popular. These mobile devices can work as sensors to collect varioustypes of data; such as pictures; videos; audios and texts. Therefore; in crowdsourcing; arequester can unitize power of mobile devices and their location information to ask for datarelated a specific location; subsequently; the mobile users who would like to perform the taskwill travel to the target location and collect the data (videos; audios; or pictures); which isthen sent to the requester. This type of crowdsourcing is called spatial crowdsourcing …,*,*,*
Towards Sustainable Insights,Carsten Binnig Lorenzo De Stefani; Tim Kraska; Eli Upfal; Emanuel Zgraggen Zheguang Zhao,ABSTRACT Have you ever been in a sauna? If yes; according to our recent surveyconducted on Amazon Mechanical Turk; people who go to saunas are more likely to knowthat Mike Stonebraker is not a character in “The Simpsons”. While this result clearly makesno sense; recently proposed tools to automatically suggest visualizations; correlations; orperform visual data exploration; significantly increase the chance that a user makes a falsediscovery like this one. In this paper; we first show how current tools mislead users toconsider random fluctuations as significant discoveries. We then describe our vision andearly results for QUDE; a new system for automatically controlling the various risk factorsduring the data exploration process.,*,*,*
CrowdDB: Answering Queries using Crowdsourcing,Michael J Franklin; Donald Kossmann; Tim Kraska; Sukriti Ramesh; Reynold Xin,Workers develop relationship with requesters and skills for certain types of HITs. Notuncommon to find workers doing only image classification. Hesitant to do tasks fromrequesters who don't provide well-defined tasks/pay appropriately. CrowdDB design to takelonger-term view on task and worker community development.,*,*,*
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan; Christopher Ré,Machine learning seems to be eating the world with a new breed of high-value data-drivenapplications in image analysis; search; voice recognition; mobile; and office productivityproducts. To paraphrase Mike Stonebraker; machine learning is no longer a zero-billion-dollar business. As the home of high-value; data-driven applications for over four decades; anatural question for database researchers to ask is: what role should the databasecommunity play in these new datadriven machine-learning-based applications? The last fewyears have seen increasing crossover between database research and machine learning.But is this crossover a wise choice for database research? What are the opportunities andthe costs of this approach to industry; to the future of database research; and to academics?Do database researchers have something to contribute to this trend? These two areas …,*,*,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Beth Trushkowsky; Tim Kraska; Michael J Franklin; Purnamrita Sarkar,Hybrid human/computer database systems promise to greatly expand the usefulness ofquery processing by incorporating the crowd. Such systems raise many implementationquestions. Perhaps the most fundamental issue is that the closed-world assumptionunderlying relational query semantics does not hold in such systems. As a consequence themeaning of even simple queries can be called into question. Furthermore; query progressmonitoring becomes difficult due to nonuniformities in the arrival of crowdsourced data andpeculiarities of how people work in crowdsourcing systems. To address these issues; wedevelop statistical tools that enable users and systems developers to reason about querycompleteness. These tools can also help drive query execution and crowdsourcingstrategies. We evaluate our techniques using experiments on a popular crowdsourcing …,Communications of the ACM,*,*
Data Engineering,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu; Yachao Lu; Saravanan Thirumuruganathan; Nan Zhang; Gautam Das,Abstract There has been much research on various aspects of Approximate QueryProcessing (AQP); such as different sampling strategies; error estimation mechanisms; andvarious types of data synopses. However; many subtle challenges arise when building anactual AQP engine that can be deployed and used by real world applications. Thesesubtleties are often ignored (or at least not elaborated) by the theoretical literature andacademic prototypes alike. For the first time to the best of our knowledge; in this article; wefocus on these subtle challenges that one must address when designing an AQP system.Our intention for this article is to serve as a handbook listing critical design choices thatdatabase practitioners must be aware of when building or using an AQP system; not toprescribe a specific solution to each challenge.,*,*,*
I-Store: Data Management for Fast Networks,Carsten Binnig; Ugur Cetintemel; Tim Kraska; Stan Zdonik; Erfan Zamanian; Andrew Crotty,Motivation: Existing distributed data management systems typically assume that the networkis a major bottleneck [10]. Consequently; avoiding remote data transfers is an importantdesign aspect of existing systems. In extreme cases; this has lead to system designs; whichexplicitly do not support certain distributed operations (eg; BigTable only supports joins if theinner table contains less than 8 MB of data). A common design principle; however; is tominimize remote data transfer by the two following techniques: First; existing systems try tofind an optimal partitioning scheme to co-partition data in order to avoid remote datatransfers for operations such as joins or to avoid distributed transactions. Second; locality-aware scheduling strategies aim to increase data-locality by shipping computation to thenodes where the data is stored. However; these techniques still result in major limitations …,*,*,*
