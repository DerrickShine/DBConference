The object-oriented database system manifesto,Malcolm Atkinson; David DeWitt; David Maier; Francois Bancilhon; Klaus Dittrich; Stanley Zdonik,Download full text in PDF Opens in a new window. Article suggestions will be shown in a dialogon return to ScienceDirect … Please enable JavaScript to use all the features on this page.… This paper attempts to define an object-oriented database system. It describes the main featuresand characteristics that a system must have to qualify as an object-oriented databasesystem … Mandatory; the ones the system must satisfy in order to be termed an object-orienteddatabase system. These are complex objects; object identity; encapsulation; types orclasses; inheritance; overriding combined with late binding; extensibility; computationalcompleteness; persistence; secondary storage management; concurrency; recovery and an adhoc query facility … Optional; the ones that can be added to make the system better; but whichare not mandatory. These are multiple inheritance; type checking and inferencing …,*,1990,1738
Aurora: a new model and architecture for data stream management,Daniel J Abadi; Don Carney; Ugur Çetintemel; Mitch Cherniack; Christian Convey; Sangdon Lee; Michael Stonebraker; Nesime Tatbul; Stan Zdonik,Abstract. This paper describes the basic processing model and architecture of Aurora; a newsystem to manage data streams for monitoring applications. Monitoring applications differsubstantially from conventional business data processing. The fact that a software systemmust process and react to continual inputs from many sources (eg; sensors) rather than fromhuman operators requires one to rethink the fundamental architecture of a DBMS for thisapplication area. In this paper; we present Aurora; a new DBMS currently under constructionat Brandeis University; Brown University; and MIT We first provide an overview of the basicAurora model and architecture and then describe in detail a stream-oriented set ofoperators.,the VLDB Journal,2003,1722
The Design of the Borealis Stream Processing Engine.,Daniel J Abadi; Yanif Ahmad; Magdalena Balazinska; Ugur Cetintemel; Mitch Cherniack; Jeong-Hyon Hwang; Wolfgang Lindner; Anurag Maskey; Alex Rasin; Esther Ryvkina; Nesime Tatbul; Ying Xing; Stanley B Zdonik,Abstract Borealis is a second-generation distributed stream processing engine that is beingdeveloped at Brandeis University; Brown University; and MIT. Borealis inherits core streamprocessing functionality from Aurora [14] and distribution functionality from Medusa [51].Borealis modifies and extends both systems in non-trivial and critical ways to provideadvanced capabilities that are commonly required by newly-emerging stream processingapplications. In this paper; we outline the basic design and functionality of Borealis. Throughsample real-world applications; we motivate the need for dynamically revising query resultsand modifying query specifications. We then describe how Borealis addresses thesechallenges through an innovative set of features; including revision records; time travel; andcontrol lines. Finally; we present a highly flexible and scalable QoS-based optimization …,Cidr,2005,1512
Broadcast disks: data management for asymmetric communication environments,Swarup Acharya; Rafael Alonso; Michael Franklin; Stanley Zdonik,Abstract This paper proposes the use of repetitive broadcast as a way of augmenting thememory hierarchy of clients in an asymmetric communication environment. We describe anew technique called “Broadcast Disks” for structuring the broadcast in a way that providesimproved performance for non-uniformly accessed data. The Broadcast Disk superimposesmultiple disks spinning at different speeds on a single broadcast channel—in effect creatingan arbitrarily fine-grained memory hierarchy. In addition to proposing and defining themechanism; a main result of this work is that exploiting the potential of the broadcaststructure requires a re-evaluation of basic cache management policies. We examine several“pure” cache management policies and develop and measure implementableapproximations to these policies. These results and others are presented in a set of …,*,1995,1336
C-store: a column-oriented DBMS,Mike Stonebraker; Daniel J Abadi; Adam Batkin; Xuedong Chen; Mitch Cherniack; Miguel Ferreira; Edmond Lau; Amerson Lin; Sam Madden; Elizabeth O'Neil; Pat O'Neil; Alex Rasin; Nga Tran; Stan Zdonik,Abstract This paper presents the design of a read-optimized relational DBMS that contrastssharply with most current systems; which are write-optimized. Among the many differencesin its design are: storage of data by column rather than by row; careful coding and packing ofobjects into storage including main memory during query processing; storing an overlappingcollection of column-oriented projections; rather than the current fare of tables and indexes;a non-traditional implementation of transactions which includes high availability andsnapshot isolation for read-only transactions; and the extensive use of bitmap indexes tocomplement B-tree structures. We present preliminary performance data on a subset of TPC-H and show that the system we are building; C-Store; is substantially faster than popularcommercial products. Hence; the architecture looks very encouraging.,Proceedings of the 31st international conference on Very large data bases,2005,1250
Monitoring streams—a new class of data management applications,Don Carney; Uğur Çetintemel; Mitch Cherniack; Christian Convey; Sangdon Lee; Greg Seidman; Nesime Tatbul; Stan Zdonik; Michael Stonebraker,Monitoring applications are those where streams of information; triggers; real-timerequirements; and imprecise data are prevalent. Traditional DBMSs are based on the HADPmodel; and thus cannot provide adequate support for such applications. Traditional DBMSshave been oriented toward business data processing; and consequently are designed toaddress the needs of these applications. First; they have assumed that the DBMS is apassive repository storing a large collection of data elements; and that humans initiatequeries and transactions on this repository. Second; they have assumed that the currentstate of the data is the only thing that is important. Hence; current values of data elementsare easy to obtain; while previous values can only be found torturously by decoding theDBMS log. The third assumption is that triggers and alerters are second-class citizens …,*,2002,1148
-Load Shedding in a Data Stream Manager,Nesime Tatbul; Uğur Çetintemel; Stan Zdonik; Mitch Cherniack; Michael Stonebraker,This chapter discusses load shedding in a data stream manager. A Data Stream Manageraccepts push-based inputs from a set of data sources; processes these inputs with respect toa set of standing queries; and produces outputs based on Quality-of-Service (QoS)specifications. When input rates exceed system capacity; the system will becomeoverloaded and latency will deteriorate. Under these conditions; the system will shed load;thus degrading the answer; to improve the observed latency of the results. The chapterexamines a technique for dynamically inserting and removing drop operators into queryplans as required by the current load. It examines two types of drops: the first drops a fractionof the tuples in a randomized fashion; and the second drops tuples based on the importanceof their content. The chapter addresses the problems of determining when load shedding …,*,2003,689
Scalable Distributed Stream Processing.,Mitch Cherniack; Hari Balakrishnan; Magdalena Balazinska; Donald Carney; Ugur Cetintemel; Ying Xing; Stanley B Zdonik,Abstract Stream processing fits a large class of new applications for which conventionalDBMSs fall short. Because many stream-oriented systems are inherently geographicallydistributed and because distribution offers scalable load management and higheravailability; future stream processing systems will operate in a distributed fashion. They willrun across the Internet on computers typically owned by multiple cooperating administrativedomains. This paper describes the architectural challenges facing the design of large-scaledistributed stream processing systems; and discusses novel approaches for addressing loadmanagement; high availability; and federated operation issues. We describe two streamprocessing systems; Aurora* and Medusa; which are being designed to explorecomplementary solutions to these challenges. This paper discusses the architectural …,CIDR,2003,616
Balancing push and pull for data broadcast,Swarup Acharya; Michael Franklin; Stanley Zdonik,Abstract The increasing ability to interconnect computers through internet-working; wirelessnetworks; high-bandwidth satellite; and cable networks has spawned a new class ofinformation-centered applications based on data dissemination. These applications employbroadcast to deliver data to very large client populations. We have proposed the BroadcastDisks paradigm [Zdon94; Acha95b] for organizing the contents of a data broadcast programand for managing client resources in response to such a program. Our previous work onBroadcast Disks focused exclusively on the “push-based” approach; where data is sent outon the broadcast channel according to a periodic schedule; in anticipation of client requests.In this paper; we study how to augment the push-only model with a “pull-based” approach ofusing a backchannel to allow clients to send explicit requests for data to the server. We …,ACM SIGMOD Record,1997,581
The 8 requirements of real-time stream processing,Michael Stonebraker; Uǧur Çetintemel; Stan Zdonik,Abstract Applications that require real-time processing of high-volume data steams arepushing the limits of traditional data processing infrastructures. These stream-basedapplications include market feed processing and electronic trading on Wall Street; networkand infrastructure monitoring; fraud detection; and command and control in militaryenvironments. Furthermore; as the" sea change" caused by cheap micro-sensor technologytakes hold; we expect to see everything of material significance on the planet get" sensor-tagged" and report its state or location in real time. This sensorization of the real world willlead to a" green field" of novel monitoring and control applications with high-volume and low-latency processing requirements. Recently; several technologies have emerged---includingoff-the-shelf stream processing engines---specifically to address the challenges of …,ACM Sigmod Record,2005,550
Readings in object-oriented database systems,Stanley Benjamin Zdonik; David Maier,Fundamentals of objet-oriented databases; Object-oriented fundamentals; Semantic datamodels and persistent languages; Object-oriented database systems; Implementation;Transaction processing; Special features; Relational extensions and extensible databases;Interfaces; Applications.,*,1990,521
H-store: a high-performance; distributed main memory transaction processing system,Robert Kallman; Hideaki Kimura; Jonathan Natkins; Andrew Pavlo; Alexander Rasin; Stanley Zdonik; Evan PC Jones; Samuel Madden; Michael Stonebraker; Yang Zhang; John Hugg; Daniel J Abadi,Abstract Our previous work has shown that architectural and application shifts have resultedin modern OLTP databases increasingly falling short of optimal performance [10]. Inparticular; the availability of multiple-cores; the abundance of main memory; the lack of userstalls; and the dominant use of stored procedures are factors that portend a clean-slateredesign of RDBMSs. This previous work showed that such a redesign has the potential tooutperform legacy OLTP databases by a significant factor. These results; however; wereobtained using a bare-bones prototype that was developed just to demonstrate the potentialof such a system. We have since set out to design a more complete execution platform; andto implement some of the ideas presented in the original paper. Our demonstrationpresented here provides insight on the development of a distributed main memory OLTP …,Proceedings of the VLDB Endowment,2008,439
Inheritance as an incremental modification mechanism or what like is and isn’t like,Peter Wegner; Stanley B Zdonik,Abstract Incremental modification is a fundamental mechanism not only in software systems;but also in physical and mathematical systems. Inheritance owes its importance in largemeasure to its flexibility as a discrete incremental modification mechanism. Four increasinglypermissive properties of incremental modification realizable by inheritance are examined:behavior compatibility; signature compatibility. name compatibility; and cancellation.Inheritance for entities with finite sets of attributes is defined and characterized asincremental modification with deferred binding of self-reference. Types defined aspredicates for type checking are contrasted with classes defined as templates for objectgeneration. Mathematical; operational; and conceptual models of inheritance are thenexamined in detail; leading to a discussion of algebraic models of behavioral …,European Conference on Object-Oriented Programming,1988,395
Dissemination-based data delivery using broadcast disks,Swarup Acharya; Michael Franklin; Stanley Zdonik,Mobile computers and wireless networks are emerging technologies which promise to makeubiquitous computing a reality. One challenge that must be met in order to truly realize thispotential is that of providing mobile clients with ubiquitous access to data. One way (andperhaps the only way) to address these challenges is to provide stationary server machineswith a relatively high-bandwidth channel over which to broadcast data to a client populationin anticipation of the need for that data by the clients. Such a system can be said to beasymmetric due to the disparity in the transmission capacities of clients and servers. Wehave proposed a mechanism called broadcast disks to provide database access in thisenvironment as well as in other asymmetric systems such as cable and direct broadcastsatellite television networks and information distribution services. The broadcast disk …,IEEE Personal Communications,1995,376
The management of changing types in an object-oriented database,Andrea H Skarra; Stanley B Zdonik,Abstract We examine the problem of type evolution in an object-oriented databaseenvironment. Type definitions are persistent objects in the database and as such may bemodified and shared. The effects of changing a type extend to objects of the type and toprograms that use objects of the type. We propose a solution to the problem through anextension of the semantic data model. A change in the interface defined by a type may resultin errors when programs use new or old objects of the type. Through the use of anabstraction of the type over time; timestamping and error handling mechanisms providesupport for the type designer in creating compatible versions of the type. The mechanismsare incorporated into the behavior defined by the type and are inherited via the type-lattice.,ACM Sigplan Notices,1986,319
-Operator Scheduling in a Data Stream Manager,Don Carney; Uğur Çetintemel; Alex Rasin; Stan Zdonik; Mitch Cherniack; Mike Stonebraker,This chapter reveals that many stream-based applications have sophisticated dataprocessing requirements and real-time performance expectations that need to be met underhigh-volume; time-varying data streams. To address these challenges; this chapter proposesa novel operator scheduling approaches that specify: which operators to schedule; in whichorder to schedule the operators; and how many tuples to process at each execution step.This chapter studies the approaches in the context of the Aurora data stream manager. Itargues that a fine-grained scheduling approach in combination with various schedulingtechniques can significantly improve system efficiency by reducing various systemoverheads. It also discusses application-aware extensions that make scheduling decisionsaccording to per-application quality of service (QoS) specifications. The chapter presents …,*,2003,318
“Data in your face”: push technology in perspective,Michael Franklin; Stan Zdonik,Push technology stems from a vel-y simple idea. Rather than requiring USCIS to explicitlyrequest (ie;“pull”) the information that they need; data can be sent to users without havingthem specifically ask for it. The advantages of push are straightforward. The traditional pullapproach requires that users know a priori where and when to look for data or that theyspend an inordinate amount of time polling known sites for updates and/or hunting on thenetwork for relevant sites. Push relieves the user of these burdens. The problems of pushare also fairly obvious. Push transfers control from the users to the data providers; raising thepotential that users receive irrelevant data while not receiving the information they need.These potential problems can arise due to issues ranging from poor prediction of userinterests to outright abuse of the mechanism; such as “spamming”. The “in-your-face” …,ACM SIGMOD Record,1998,307
High-availability algorithms for distributed stream processing,J-H Hwang; Magdalena Balazinska; Alex Rasin; Ugur Cetintemel; Michael Stonebraker; Stan Zdonik,Stream-processing systems are designed to support an emerging class of applications thatrequire sophisticated and timely processing of high-volume data streams; often originating indistributed environments. Unlike traditional data-processing applications that require preciserecovery for correctness; many stream-processing applications can tolerate and benefit fromweaker recovery guarantees. In this paper; we study various recovery guarantees andpertinent recovery techniques that can meet the correctness and performance requirementsof stream-processing applications. We discuss the design and algorithmic challengesassociated with the proposed recovery techniques and describe how each can providedifferent guarantees with proper combinations of redundant processing; checkpointing; andremote logging. Using analysis and simulations; we quantify the cost of our recovery …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,303
Fido: A cache that learns to fetch,Mark Palmer; Stanley B Zdonik,A major performance factor for current OODB systems is the cost of fetching objects fromsecondary storage into a local cache as needed. In workstation-server architectures; thiscost is com-pounded as data crosses several I/O boundaries on its path from the server'ssecondary storage to an application's memory. Given the high relative cost of performing 1/0;an ability to anticipate data movement across communication links and prefetch objectspresents a potent means of improving performance. Data caching in a workstation-serverarchitecture improves performance; as described by Cattell et al [RKC87]. Chang andKat:[CK89] found that cache management policy has the largest effect on response time;followed by clustering; a topic of much interest [Ct-82];[S84];[I-IK90]. Clustering creates fordata a locality of reference like that present in code; the property exploited by virtual …,*,1991,281
Overview of SciDB: large scale array storage; processing and analysis,Paul G Brown,Abstract SciDB [4; 3] is a new open-source data management system intended primarily foruse in application domains that involve very large (petabyte) scale array data; for example;scientific applications such as astronomy; remote sensing and climate modeling; bio-scienceinformation management; risk management systems in financial applications; and theanalysis of web log data. In this talk we will describe our set of motivating examples and usethem to explain the features of SciDB. We then briefly give an overview of the project'inflight'; explaining our novel storage manager; array data model; query language; andextensibility frameworks.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,280
Approximate queries and representations for large data sequences,Hagit Shatkay; Stanley B Zdonik,Many new database application domains such as experimental sciences and medicine arecharacterized by large sequences as their main form of data. Using approximaterepresentation can significantly reduce the required storage and search space. A goodchoice of representation; can support a broad new class of approximate queries; needed inthere domains. These queries are concerned with application dependent features of thedata as opposed to the actual sampled points. We introduce a new notion of generalizedapproximate queries and a general divide and conquer approach that supports them. Thisapproach uses families of real-valued functions as an approximate representation. Wepresent an algorithm for realizing our technique; and the results of applying it to medicalcardiology data.,Data Engineering; 1996. Proceedings of the Twelfth International Conference on,1996,268
A query algebra for object-oriented databases,Gail M Shaw; Stanley B Zdonik,An algebra that synthesizes relational query concepts with object-oriented databases isdefined. The algebra fully supports abstract data types and object identity; while providingassociative access to objects. The operations take an abstract view of objects and accesstyped collections of objects through the public interface defined for the type. The algebrasupports access to relationships implied by the structure of the objects; as well as thedefinition and creation of new relationships between objects. The operations create newobjects with unique identities and can use object identity in the manipulation of objects. Thesupport for object identity leads to new definitions for equality of results and operators thatcan manipulate the identities of objects.,Data Engineering; 1990. Proceedings. Sixth International Conference on,1990,257
A shared; segmented memory system for an object-oriented database,Mark F Hornick; Stanley B Zdonik,Abstract This paper describes the basic data model of an object-oriented database and thebasic architecture of the system implementing it. In particular; a secondary storagesegmentation scheme and a transaction-processing scheme are discussed. Thesegmentation scheme allows for arbitrary clustering of objects; including duplicates. Thetransaction scheme allows for many different sharing protocols ranging from those thatenforce serializability to those that are nonserializable and require communication with theserver only on demand. The interaction of these two features is described such that segment-level transfer and object-level locking is achieved.,ACM Transactions on Information Systems (TOIS),1987,251
Disseminating updates on broadcast disks,Swarup Acharya; Michael J Franklin; Stanley Zdonik,Abstract Lately there has been increasing interest in the use of data dissemination as ameans for delivering data from servers to clients in both wired and wireless environments.Using data dissemination; the transfer of data is initiated by servers; resulting in a reversal ofthe traditional relationship between clients and servers. In previous papers; we haveproposed Broadcast Disks as a model for structuring the repetitive transmission of data in abroadcast medium. Broadcast Disks are intended for use in environments where; for eitherphysical or application-dependent reasons; there is asymmetry in the communicationcapacity between clients and servers. Examples of such environments include wirelessnetworks with mobile clients; cable and direct satellite broadcast; and information dispersalapplications. Our initial studies of Broadcast Disks focused on the performance of the …,VLDB,1996,249
Dynamic load distribution in the borealis stream processor,Ying Xing; Stan Zdonik; J-H Hwang,Distributed and parallel computing environments are becoming cheap and commonplace.The availability of large numbers of CPU's makes it possible to process more data at higherspeeds. Stream-processing systems are also becoming more important; as broad classes ofapplications require results in real-time. Since load can vary in unpredictable ways;exploiting the abundant processor cycles requires effective dynamic load distributiontechniques. Although load distribution has been extensively studied for the traditional pull-based systems; it has not yet been fully studied in the context of push-based continuousquery processing. In this paper; we present a correlation based load distribution algorithmthat aims at avoiding overload and minimizing end-to-end latency by minimizing loadvariance and maximizing load correlation. While finding the optimal solution for such a …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,241
Prefetching from a broadcast disk,Swarup Acharya; Michael Franklin; Stanley Zdonik,Broadcast disks have been proposed as a means to efficiently deliver data to clients in"asymmetric" environments where the available bandwidth from the server to the clientsgreatly exceeds the bandwidth in the opposite direction. A previous study investigated theuse of cost based caching to improve performance when clients access the broadcast in ademand driven manner (S. Acharya et al.; 1995). Such demand driven access however;does not fully exploit the dissemination based nature of the broadcast; which is particularlyconducive to client prefetching. With a broadcast disk; pages continually flow past the clientsso that in contrast to traditional environments; prefetching can be performed without placingadditional load on shared resources. We argue for the use of a simple prefetch heuristiccalled PT and show that PT balances the cache residency time of a data item with its …,Data Engineering; 1996. Proceedings of the Twelfth International Conference on,1996,235
Aurora: a data stream management system,Daniel Abadi; Donald Carney; Ugur Cetintemel; Mitch Cherniack; Christian Convey; C Erwin; Eduardo Galvez; M Hatoun; Anurag Maskey; Alex Rasin; A Singer; Michael Stonebraker; Nesime Tatbul; Ying Xing; Rongguo Yan; S Zdonik,Streams are continuous data feeds generated by such sources as sensors; satellites; andstock feeds. Monitoring applications track data from numerous streams; filtering them forsigns of abnormal activity; and processing them for purposes of filtering; aggregation;reduction; and correlation. Aurora [1; 2; 3] is a general-purpose data stream manager that isbeing designed and implemented (at Brandeis University; Brown University; and MIT) toefficiently support a variety of real-time monitoring applications.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,222
The Lowell database research self-assessment,Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk; David Maier; Jeff Naughton; Hans Schek; Timos Sellis; Avi Silberschatz; Mike Stonebraker; Rick Snodgrass; Jeff Ullman; Gerhard Weikum; Jennifer Widom; Stan Zdonik,A group of senior database researchers gathers every few years to assess the state of databaseresearch and to point out problem areas that deserve additional focus. This article summarizesthe discussion and conclusions of the sixth such meeting in Lowell; MA; in May 2003. It followsa number of earlier reports with similar goals; including [1; 2; 5-7] … Continuing thistradition; 25 senior database researchers representing a broad cross section of the field in termsof research interests; affiliations; and geography gathered in Lowell for two days of intensivediscussion on where the database field is and where it should be going … Several importantobservations came out of this meeting. Information management continues to be a critical componentof most complex software systems. We recommend that database researchers increase theirfocus on the integration of text; data; code; and streams; fusion of information from …,Communications of the ACM,2005,203
Towards a streaming SQL standard,Namit Jain; Shailendra Mishra; Anand Srinivasan; Johannes Gehrke; Jennifer Widom; Hari Balakrishnan; Uǧur Çetintemel; Mitch Cherniack; Richard Tibbetts; Stan Zdonik,Abstract This paper describes a unification of two different SQL extensions for streams andits associated semantics. We use the data models from Oracle and StreamBase as ourexamples. Oracle uses a time-based execution model while StreamBase uses a tuple-basedexecution model. Time-based execution provides a way to model simultaneity while tuple-based execution provides a way to react to primitive events as soon as they are seen by thesystem. The result is a new model that gives the user control over the granularity at whichone can express simultaneity. Of course; it is possible to ignore simultaneity altogether. Theproposed model captures ordering and simultaneity through partial orders on batches oftuples. The batching and the ordering are encapsulated in and can be modified by means ofa powerful new operator that we call SPREAD. This paper describes the semantics of …,Proceedings of the VLDB Endowment,2008,200
Requirements for Science Data Bases and SciDB.,Michael Stonebraker; Jacek Becla; David J DeWitt; Kian-Tat Lim; David Maier; Oliver Ratzesberger; Stanley B Zdonik,*,CIDR,2009,197
Intermedia: A case study of the differences between relational and object-oriented database systems,Karen E Smith; Stanley B Zdonik,Abstract This paper compares two approaches to meeting the data handling requirements ofIntermedia; a hypermedia system developed at the Institute for Research in Information andScholarship at Brown University. Intermedia; though written using an object-orientedprogramming language; relies on a traditional relational database management system fordata storage and retrieval. We examine the ramifications of replacing the relational databasewith an object-oriented database. We begin by describing the important characteristics eachdatabase system. We then describe Intermedia and give an overview of its architecture andits data handling requirements. We explain why and how we used a relational databasemanagement system and the problems that we encountered with this system. We thenpresent the design of an object-oriented database schema for Intermedia and compare …,ACM SIGPLAN Notices,1987,193
Retrospective on aurora,Hari Balakrishnan; Magdalena Balazinska; Don Carney; Uğur Çetintemel; Mitch Cherniack; Christian Convey; Eddie Galvez; Jon Salz; Michael Stonebraker; Nesime Tatbul; Richard Tibbetts; Stan Zdonik,Abstract. This experience paper summarizes the key lessons we learned throughout thedesign and implementation of the Aurora stream-processing engine. For the past 2 years;we have built five stream-based applications using Aurora. We first describe in detail theseapplications and their implementation in Aurora. We then reflect on the design of Aurorabased on this experience. Finally; we discuss our initial ideas on a follow-on project; calledBorealis; whose goal is to eliminate the limitations of Aurora as well as to address new keychallenges and applications in the stream-processing domain.,The VLDB Journal,2004,192
Skew-aware automatic database partitioning in shared-nothing; parallel OLTP systems,Andrew Pavlo; Carlo Curino; Stanley Zdonik,Abstract The advent of affordable; shared-nothing computing systems portends a new classof parallel database management systems (DBMS) for on-line transaction processing(OLTP) applications that scale without sacrificing ACID guarantees [7; 9]. The performanceof these DBMSs is predicated on the existence of an optimal database design that is tailoredfor the unique characteristics of OLTP workloads. Deriving such designs for modern DBMSsis difficult; especially for enterprise-class OLTP systems; since they impose extra challenges:the use of stored procedures; the need for load balancing in the presence of time-varyingskew; complex schemas; and deployments with larger number of partitions. To this purpose;we present a novel approach to automatically partitioning databases for enterprise-classOLTP systems that significantly extends the state of the art by:(1) minimizing the number …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,191
Object-oriented fundamentals,Stanley B Zdonik; David Maier,*,Readings in object-oriented database systems,1989,191
Type evolution in an object-oriented databases,Andrea H Skarra,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Research directions in object-oriented programming,1987,176
Object views: Extending the vision,Sandra Heiler; Stanley Zdonik,A mechanism for producing views in an object-oriented system is presented. The results areanalogous to database views in traditional database systems; except that the object viewspresented hide or expose methods as well as data. The mechanism is based on the objectmodel's built-in facilities for defining data and procedural abstractions and for constructingnew types and objects. It uses the type system and the query language of the model tosupport arbitrary transformations of the underlying representations in defining databaseviews; without introducing any new mechanism specifically to support views. Careful use ofthe query language allows one to define updatable views. Also indicated are howabstraction and view mapping capabilities can be used to support federation ofheterogeneous software and databases.,Data Engineering; 1990. Proceedings. Sixth International Conference on,1990,175
ISIS: interface for a semantic information system,Kenneth J Goldman; Sally A Goldman; Paris C Kanellakis; Stanley B Zdonik,Abstract ISIS IS an experimental system for graphically manlpulatmg a database The system1s based on a simply specified high-level semantic data model It demonstrates thecapablbtles of a workstation environment by mtegratmg three aspects of databaseprogramming m one graphical setting Namely; it permits database constructlon andmodification; it allows browsing at the schema and data levels; and provides a graphicalquery language In all of these activities it maintains uniform graphlcal representations andconsistent user mteractlon techniques,ACM SIGMOD Record,1985,159
Knowledge-based query processing,Michael Hammer; Stanley B Zdonik,Abstract Contemporary database query processing systems base their actions principallyon" syntactic" considerations; and seek only the most efficacious way of answering a queryas originally formulated. An alternative approach seeks to use knowledge of the semanticsof the database's application to transform the original query into an alternative form; possiblyquite different in its expression; but which is both equivalent to the original (in terms of theset of records from the database that it qualifies) and more efficient to process; given theexisting file structures and access methods. The architecture of a system supporting suchknowledge-based" semantic" transformations has been developed. It addresses such issuesas the kinds of knowledge that should be included in the knowledge base and how it shouldbe expressed; the kinds of transformations that can exploit this knowledge to improve …,Proceedings of the sixth international conference on Very Large Data Bases-Volume 6,1980,156
A framework for scalable dissemination-based systems,Michael Franklin; Stanley Zdonik,Abstract The dramatic improvements in global interconnectivity due to intranets; extranets;and the Internet has led to an explosion in the number and variety of new data-intensiveapplications. Along with the proliferation of these new applications have come increasedproblems of scale. This is demonstrated by frequent delays and service; disruptions whenaccessing networked data sources. Recently; push-based techniques have been proposedas a solution to scalability problems for distributed applications. This paper argues that pushindeed has its place; but that it is just one aspect of a much larger design space fordistributed information systems. We propose the notion of a Dissemination-BasedInformation System (DBIS) which integrates a variety of data delivery mechanisms andinformation broker hierarchies. We discuss the properties of such systems and provide …,ACM SIGPLAN Notices,1997,154
A demonstration of SciDB: a science-oriented DBMS,Philippe Cudré-Mauroux; Hideaki Kimura; K-T Lim; Jennie Rogers; Roman Simakov; Emad Soroush; Pavel Velikhov; Daniel L Wang; Magdalena Balazinska; Jacek Becla; D DeWitt; Bobbi Heath; David Maier; Samuel Madden; J Patel; Michael Stonebraker; S Zdonik,Abstract In CIDR 2009; we presented a collection of requirements for SciDB; a DBMS thatwould meet the needs of scientific users. These included a nested-array data model; science-specific operations such as regrid; and support for uncertainty; lineage; and named versions.In this paper; we present an overview of SciDB's key features and outline a demonstration ofthe first version of SciDB on data and operations from one of our lighthouse users; the LargeSynoptic Survey Telescope (LSST).,Proceedings of the VLDB Endowment,2009,153
Issues in the design of object-oriented database programming languages,Toby Bloom; Stanley B Zdonik,Abstract We see a trend toward extending object-oriented languages in the direction ofdatabases; and; at the same time; toward extending database systems with object-orientedideas. On the surface; these two activities seem to be moving in a consistent direction.However; at a deeper level; we see difficulties that may inhibit their ending up at the samepoint. We feel that many of these difficulties are a result of the underlying assumptions thatare inherent in the fields of programming language and database systems research. Many ofthese assumptions are historical and contribute to a set of cultural biases that often preventthe two communities from interacting as effectively as possible. The purpose of this paper isto try to uncover some of the cultural presuppositions that have inhibited development of afully integrated database programming language. We have identified database and …,ACM Sigplan Notices,1987,151
Window-aware load shedding for aggregation queries over data streams,Nesime Tatbul; Stan Zdonik,Abstract Data stream management systems may be subject to higher input rates than theirresources can handle. When overloaded; the system must shed load in order to maintainlow-latency query results. In this paper; we describe a load shedding technique for queriesconsisting of one or more aggregate operators with sliding windows. We introduce a newtype of drop operator; called a" Window Drop". This operator is aware of the windowproperties (ie; window size and window slide) of its downstream aggregate operators in thequery plan. Accordingly; it logically divides the input stream into windows andprobabilistically decides which windows to drop. This decision is further encoded into tuplesby marking the ones that are disallowed from starting new windows. Unlike earlierapproaches; our approach preserves integrity of windows throughout a query plan; and …,Proceedings of the 32nd international conference on Very large data bases,2006,150
An object-oriented query algebra,Gail M Shaw; Stanley B Zdonik,Abstract We define a query algebra for object-oriented databases that fully supports abstractdata types and object identity while providing associative access to objects; including a joincapability that respects the discipline of data abstraction. The structure of the algebra andthe abstract access to objects offer opportunities for query optimization. The algebraicoperations take an abstract view of objects and access typed collections of objects onlythrough the public interface defined for the type. The algebra supports access torelationships implied by the structure of the objects; as well as the definition and creation ofnew relationships between objects. We introduce two notions of object equality to supportthe creation of new objects by the algebraic operations.,Proceedings of the second international workshop on Database programming languages,1990,149
Object management system concepts,Stanley B Zdonik,Abstract An office database should be a “total” information resource in that it should becapable of storing data of many arbitrary types. Users of such a system should be able tostore conveniently their documents and graphics objects in the same logical storage spaceas their more traditional records-oriented data. This paper presents a data model that can beused to describe more effectively the objects that occur naturally in the office environment.This model exploits some of the richer semantics of office objects such as the containment ofone object within another (eg; reports contain chapters) and the version histories of objectsand their constituent parts. This model forms the basis for our experimental objectmanagement system which is used to support the creation of new office applicationprograms.,ACM SIGOA Newsletter,1984,149
Staying fit: Efficient load shedding techniques for distributed stream processing,Nesime Tatbul; Uǧur Çetintemel; Stan Zdonik,Abstract In distributed stream processing environments; large numbers of continuousqueries are distributed onto multiple servers. When one or more of these servers becomeoverloaded due to bursty data arrival; excessive load needs to be shed in order to preservelow latency for the query results. Because of the load dependencies among the servers; loadshedding decisions on these servers must be well-coordinated to achieve end-to-endcontrol on the output quality. In this paper; we model the distributed load shedding problemas a linear optimization problem; for which we propose two alternative solution approaches:a solver-based centralized approach; and a distributed approach based on metadataaggregation and propagation; whose centralized implementation is also available. Both ofour solutions are based on generating a series of load shedding plans in advance; to be …,Proceedings of the 33rd international conference on Very large data bases,2007,142
Profile-driven cache management,Mitch Cherniack; Eduardo F Galvez; Michael J Franklin; Stan Zdonik,Modern distributed information systems cope with disconnection and limited bandwidth byusing caches. In communication-constrained situations; traditional demand-drivenapproaches are inadequate. Instead; caches must be preloaded in order to mitigate theabsence of connectivity or the paucity of bandwidth. We propose to use application-levelknowledge expressed as profiles to manage the contents of caches. We propose a simple;but rich profile language that permits high-level expression of a user's data needs for thepurpose of expressing desirable contents of a cache. We consider techniques forprefetching a cache on the basis of profiles expressed in our framework; both for basic andpreemptive prefetching; the latter referring to the case where staging a cache can beinterrupted at any point without prior warning. We examine the effectiveness of three …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,138
A cooperative; self-configuring high-availability solution for stream processing,Jeong-Hyon Hwang; Ying Xing; Ugur Cetintemel; Stan Zdonik,We present a collaborative; self-configuring high availability (HA) approach for streamprocessing that enables low-latency failure recovery while incurring small run-timeoverhead. Our approach relies on a novel fine-grained checkpointing model that allowsquery fragments at each server to be backed up at multiple other servers and recoveredcollectively (in parallel) when there is a failure. In this paper; we first address the problem ofdetermining the appropriate query fragments at each server. We then discuss; for eachfragment; which server to use as its backup as well as the proper checkpoint schedule. Wealso introduce and analyze operator-specific delta-checkpointing techniques to reduce theoverall HA cost. Finally; we quantify the benefits of our approach using results from ourprototype implementation and a detailed simulator.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,121
One size fits all? Part 2: Benchmarking results,Michael Stonebraker; Chuck Bear; Uğur Çetintemel; Mitch Cherniack; Tingjian Ge; Nabil Hachem; Stavros Harizopoulos; John Lifter; Jennie Rogers; Stan Zdonik,ABSTRACT Two years ago; some of us wrote a paper predicting the demise of “One SizeFits All (OSFA)”[Sto05a]. In that paper; we examined the stream processing and datawarehouse markets and gave reasons for a substantial performance advantage tospecialized architectures in both markets. Herein; we make three additional contributions.First; we present reasons why the same performance advantage is enjoyed by specializedimplementations in the text processing market. Second; the major contribution of the paper isto show “apples to apples” performance numbers between commercial implementations ofspecialized architectures and relational DBMSs in both stream processing and datawarehouses. Finally; we also show comparison numbers between an academic prototype ofa specialized architecture for scientific and intelligence applications; a relational DBMS …,Proc. CIDR,2007,120
Answering aggregation queries in a secure system model,Tingjian Ge; Stan Zdonik,Abstract As more sensitive data is captured in electronic form; security becomes more andmore important. Data encryption is the main technique for achieving security. While in thepast enterprises were hesitant to implement database encryption because of the very highcost; complexity; and performance degradation; they now have to face the ever-growing riskof data theft as well as emerging legislative requirements. Data encryption can be done atmultiple tiers within the enterprise. Different choices on where to encrypt the data offerdifferent security features that protect against different attacks. One class of attack that needsto be taken seriously is the compromise of the database server; its software or administrator.A secure way to address this threat is for a DBMS to directly process queries on theciphertext; without decryption. We conduct a comprehensive study on answering SUM …,Proceedings of the 33rd international conference on Very large data bases,2007,119
A cooperative transaction model for design databases,Marian H Nodine; Sridhar Ramaswamy; Stanley B Zdonik,Serializability has long been held as the definitive notion of correctness of transactionexecution in databases. This is because transactions that run in most “traditional” databasessuch as those for banking and airline reservation systems satisfy certain criteria implicitly. Ifwe assume that all transactions are carefully designed to preserve the consistency of thedata that they access; and that transactions have no need to interact during their execution;then the notion of serializability serves as a good correctness criterion for transactionexecution. Serializability makes it easier to reason about programs because they appear toexecute in isolation. Serializable transactions are expected to be short; and thereforeunlikely to be disrupted by a failure in the underlying computer system. Thus; since thetransactions run independently of one another; the cost of aborting a particular …,*,1992,114
Providing resiliency to load variations in distributed stream processing,Ying Xing; Jeong-Hyon Hwang; Uǧur Çetintemel; Stan Zdonik,Abstract Scalability in stream processing systems can be achieved by using a cluster ofcomputing devices. The processing burden can; thus; be distributed among the nodes bypartitioning the query graph. The specific operator placement plan can have a huge impacton performance. Previous work has focused on how to move query operators dynamically inreaction to load changes in order to keep the load balanced. Operator movement is tooexpensive to alleviate short-term bursts; moreover; some systems do not support the abilityto move operators dynamically. In this paper; we develop algorithms for selecting anoperator placement plan that is resilient to changes in load. In other words; we assume thatoperators cannot move; therefore; we try to place them in such a way that the resultingsystem will be able to withstand the largest set of input rate combinations. We call this a …,Proceedings of the 32nd international conference on Very large data bases,2006,106
Expressing user profiles for data recharging,Mitch Cherniack; Michael J Franklin; Stan Zdonik,Mobile devices need two basic renewable resources-power and data. Power recharging iseasy; data recharging is a much more problematic activity. It requires complex interactionbetween a user and a collection of data sources. We provide an automatic data rechargingcapability based on user profiles written in an expressive profile language. A profileidentifies relevant information and orders it by its usefulness. We discuss the issues involvedin designing a profile language for data recharging.,IEEE Personal Communications,2001,106
Data Curation at Scale: The Data Tamer System.,Michael Stonebraker; Daniel Bruckner; Ihab F Ilyas; George Beskales; Mitch Cherniack; Stanley B Zdonik; Alexander Pagan; Shan Xu,ABSTRACT Data curation is the act of discovering a data source (s) of interest; cleaning andtransforming the new data; semantically integrating it with other local data sources; anddeduplicating the resulting composite. There has been much research on the variouscomponents of curation (especially data integration and deduplication). However; there hasbeen little work on collecting all of the curation components into an integrated end-to-endsystem. In addition; most of the previous work will not scale to the sizes of problems that weare finding in the field. For example; one web aggregator requires the curation of 80;000URLs and a second biotech company has the problem of curating 8000 spreadsheets. Atthis scale; data curation cannot be a manual (human) effort; but must entail machinelearning approaches with a human assist only when necessary.,CIDR,2013,105
Strategic directions in database systems—breaking out of the box,Avi Silberschatz; Stan Zdonik,The field of database systems research and development has been enormously successfulover its 30-year history. It has led to a $10 billion industry with an installed base that touchesvirtually every major company in the world. It would be unthinkable to manage the largevolume of valuable information that keeps corporations running without support fromcommercial database management systems (DBMSs). Today; the field of database researchis largely defined by its previous successes; and much current research is aimed atincreasing the functionality and performance of DBMSs. A DBMS is a very complex systemincorporating a rich set of technologies. These technologies have been assembled in a waythat is ideally suited for solving problems of large-scale data management in the corporatesetting. However; a DBMS; like any large tool; places some requirements on the …,ACM Computing Surveys (CSUR),1996,105
Dissemination-based information systems,Michael Franklin; Stan Zdonik,Abstract This paper examines the construction of distributed information systems thatincorporate the ability to push data out to clients (ie; dissemination) in addition to havingclients pull data from servers. One key issue that distinguishes such dissemination-basedinformation systems (DBIS) from more traditional ones is communications asymmetry.Asymmetry arises in many new applications due to both physical characteristics such asnetwork bandwidths; as well as to workload characteristics. We outline a number of differentdata delivery mechanisms; and then focus on one particular dissemination mechanism thatwe have developed; called Broadcast Disks. Finally; we discuss issues that arise in thedesign of a DBIS architecture that can provide many different modes of data delivery.,Data Engineering,1996,103
Top-k queries on uncertain data: on score distribution and typical answers,Tingjian Ge; Stan Zdonik; Samuel Madden,Abstract Uncertain data arises in a number of domains; including data integration andsensor networks. Top-k queries that rank results according to some user-defined score arean important tool for exploring large uncertain data sets. As several recent papers haveobserved; the semantics of top-k queries on uncertain data can be ambiguous due totradeoffs between reporting high-scoring tuples and tuples with a high probability of being inthe resulting data set. In this paper; we demonstrate the need to present the scoredistribution of top-k vectors to allow the user to choose between results along this score-probability dimensions. One option would be to display the complete distribution of allpotential top-k tuple vectors; but this set is too large to compute. Instead; we propose toprovide a number of typical vectors that effectively sample this distribution. We propose …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,100
Are “Disks in the Air” Just Pie in the Sky?,Stanley Zdonik; Michael Franklin; Rafael Alonso; Swarup Acharya,Mobile computers and wireless networks are emerging technologies which will soon makeubiquitous computing a reality. In the wireless environment; mobile clients may often bedisconnected from stationary server machines or may have only a low-bandwidth channelfor sending messages to servers. This environment raises new challenges for the support ofdatabase applications for three reasons: 1) the limited storage capacities of mobilemachines; 2) the inability to accurately predict the future data needs of many data-intensiveapplications; and 3) the need to provide clients with new or updated data values in order toensure consistent data access. One way (perhaps the only way) to address thesechallenges is to provide the stationary server machines with a relatively high bandwidthchannel over which to broadcast portions of the database. Wireless networks are but one …,Mobile Computing Systems and Applications; 1994. WMCSA 1994. First Workshop on,1994,98
Version management in an object-oriented database,Stanley B Zdonik,Abstract We describe a database system that includes a built-in version control mechanismthat can be used in the definition of any new object types. This database system is object-oriented in the sense that it supports data abstraction; object types; and inheritance. Weshow how this version control mechanism can be used to manage change in the definition ofa system. In particular; we show how versions of type defining objects serve to maintainconsistent behavior of objects as the system evolves over time. We also show how thesystem can use other information such as the component-of relationship to propagatechanges to the appropriate places. The version mechanism is also used to controlconsistency during the process of design. The notions of consistency surface; design step;and slice are introduced. The version mechanism also gives us a way of potentially …,*,1986,98
Concurrency control and object-oriented databases,Andrea H Skarra; Stanley B Zdonik,*,Object-Oriented Concepts; Databases; and Applications,1989,92
Distributed operation in the borealis stream processing engine,Yanif Ahmad; Bradley Berg; Uǧur Cetintemel; Mark Humphrey; Jeong-Hyon Hwang; Anjali Jhingran; Anurag Maskey; Olga Papaemmanouil; Alexander Rasin; Nesime Tatbul; Wenjuan Xing; Ying Xing; Stan Zdonik,Abstract Borealis is a distributed stream processing engine that is being developed atBrandeis University; Brown University; and MIT. Borealis inherits core stream processingfunctionality from Aurora and inter-node communication functionality from Medusa. Wepropose to demonstrate some of the key aspects of distributed operation in Borealis; using amulti-player network game as the underlying application. The demonstration will illustratethe dynamic resource management; query optimization and high availability mechanismsemployed by Borealis; using visual performance-monitoring tools as well as the gamingexperience.,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,90
An object server for an object-oriented database system,Andrea H Skarra; Stanley B Zdonik; Stephen P Reiss,Abstract This paper summarizes the interface; implementation; and use of a server processthat is used as a backend by an object-oriented database system. This server is responsiblefor managing objects on secondary storage; managing transactions; and implementing asimple form of trigger. We sketch the interface of this system and point out some of the moreinteresting implementation issues that were encountered in building it. Client processescommunicate asynchronously with the server by message sending. The system is designedto be as efficient as possible since one of its clients is the GARDEN system; an object-oriented programming environment. GARDEN views both static and dynamic programpieces as objects. Our back-end server provides persistent and sharable storage forGARDEN. The paper includes an extended example of how GARDEN makes use of this …,Proceedings on the 1986 international workshop on Object-oriented database systems,1986,90
Learning-based query performance modeling and prediction,Mert Akdere; Ugur Çetintemel; Matteo Riondato; Eli Upfal; Stanley B Zdonik,Accurate query performance prediction (QPP) is central to effective resource management;query optimization and query scheduling. Analytical cost models; used in current generationof query optimizers; have been successful in comparing the costs of alternative query plans;but they are poor predictors of execution latency. As a more promising approach to QPP; thispaper studies the practicality and utility of sophisticated learning-based models; which haverecently been applied to a variety of predictive tasks with great success; in both static (ie;fixed) and dynamic query workloads. We propose and evaluate predictive modelingtechniques that learn query execution behavior at different granularities; ranging fromcoarse-grained plan-level models to fine-grained operator-level models. We demonstratethat these two extremes offer a tradeoff between high accuracy for static workload queries …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,87
An efficient scheme for dynamic data replication,Swarup Acharya; Stanley B Zdonik,Abstract This paper presents an efficient scheme for dynamic replication of data indistributed environments. The aim of the scheme is to increase system performance byintelligent data placement so as to optimize the message traffic in the network. Research inthe recent past has comparatively focussed very little on using replication for increasingperformance but has instead been directed more at improving system availability throughreplication. However; with the advent of mobile or nomadic computing; research inreplication needs to change direction--the underlying assumption of high speed networks nolonger hold true. Wireless networks not only have lower bandwidth but are also veryexpensive to use. In such an environment; it is imperative that data be distributedintelligently to achieve a good system performance in terms of message costs and …,*,1993,87
The AQUA data model and algebra,Theodore W Leung; Bharathi Subramanian; Scott L Vandenberg; Gail Mitchell; Bennet Vance; Stanley B Zdonik,Abstract This paper describes a new object-oriented model and query algebra that will beused as an input language for the query optimizers that are being built as a part of the EREQproject. The model adopts a uniform view of objects and values and separates syntactic;semantic; and implementation concerns. The algebra addresses issues of type-definedequality and duplicate elimination as well as extensions to bulk types other than sets.,*,1994,84
Fast; secure encryption for indexing in a column-oriented DBMS,Tingjian Ge; Stan Zdonik,Networked information systems require strong security guarantees because of the newthreats that they face. Various forms of encryption have been proposed to deal with thisproblem. In a database system; there are often two contradictory goals: security of theencryption and fast performance of queries. There have been a number of proposals ofdatabase encryption schemes to facilitate queries on encrypted columns. Order-preservingencryption techniques are well-suited for databases since they support a simple; andefficient way to build indices. However; as we will show; they are insecure understraightforward attack scenarios. We propose a new light-weight database encryptionscheme (called FCE) for column stores in data warehouses with trusted servers. The lowdecryption overhead of FCE makes comparisons of ciphertexts and hence indexing …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,82
Cooperative Transaction Hierarchies: A Transaction Model to Support Design Applications.,Marian H Nodine; Stanley B Zdonik,Abstract Traditional atomic and nested transactions are not; always well-suited tocooperative applications. Cooperative applications place requirements on the databasewhich may conflict with the serializability requirement. We define a new transactionframework; called a cooperative transaction hierarchy; which allows us to relax therequirement for atomic; serializable transactions. Each internal node (transaction group) inthe transaction hierarchy can enforce its own constraints on how objects can be sharedamong its children (members). Patterns specify the constraints imposed on an operationhistory for it; to be correct. At a given node in the hiera. rchy; we use a type of augmentedfinite state automaton called an operation machine to enforce correctness. We provideintentions to manage the propagation of object copies and their associated privileges …,VLDB,1990,82
Language and methodology for object-oriented database environments,Stanley B Zdonik; Peter Wegner,Abstract This paper describes an object-oriented database language being implemented atBrown for use on workstations; and demonstrates its use in defining an object-orientedprogramming environment. The database language is illustrated by specifications of theUNIX file system; Ada packages; program structure; and multiple views of program modules.Each example illustrates a different feature of object-oriented programming methodology.Collectively the examples serve both as an introduction to our database language and as atutorial for object-oriented system programming.,*,1988,81
Research in data broadcast and dissemination,Demet Aksoy; Mehmet Altinel; Rahul Bose; Ugur Cetintemel; Michael Franklin; Jane Wang; Stan Zdonik,Abstract The proliferation of the Internet and intranets; the development of wireless andsatellite networks; and the availability of asymmetric; high-bandwidth links to the home; havefueled the development of a wide range of new “dissemination-based” applications. Theseapplications involve the timely distribution of data to a large set of consumers; and includestock and sports tickers; traffic information systems; electronic personalized newspapers;and entertainment delivery. Dissemination-oriented applications have special characteristicsthat render traditional client-server data management approaches ineffective. These include:tremendous scale. a high-degree of overlap in user data needs. asymmetric data flow fromsources to consumers.,*,1999,80
Transaction groups: A model for controlling cooperative transactions,Mary F Fernandez; Stanley B Zdonik,Abstract Many interactive applications that are being developed for high-performanceworkstations involve several users in a collaborative activity. Modern database systemssynchronize activities on shared data by ensuring that the results are serializable.Serializability is often too limiting a correctness criterion for cooperative work. Instead; weneed a way of specifying correctness in an application-dependent way; such that thedatabase system can monitor the evolving schedules and determine when an activity wouldviolate an assumption being made by another concurrent activity. The storage managermust be capable of supporting interactions with many cooperative activities each of which iscomposed of multiple agents. This paper discusses how a distributed object server cansupport heterogenous sharing protocols without interference. We define active entities …,*,1990,77
Anti-caching: A new approach to database management system architecture,Justin DeBrabant; Andrew Pavlo; Stephen Tu; Michael Stonebraker; Stan Zdonik,Abstract The traditional wisdom for building disk-based relational database managementsystems (DBMS) is to organize data in heavily-encoded blocks stored on disk; with a mainmemory block cache. In order to improve performance given high disk latency; thesesystems use a multi-threaded architecture with dynamic record-level locking that allowsmultiple transactions to access the database at the same time. Previous research has shownthat this results in substantial overhead for on-line transaction processing (OLTP)applications [15]. The next generation DBMSs seek to overcome these limitations witharchitecture based on main memory resident data. To overcome the restriction that all data fitin main memory; we propose a new technique; called anti-caching; where cold data ismoved to disk in a transactionally-safe manner as the database grows in size. Because …,Proceedings of the VLDB Endowment,2013,75
Object-oriented queries: equivalence and optimization,Gail M Shaw; Stanley B Zdonik,We are interested in efficiently accessing data in an object-oriented database. We havedeveloped a query algebra which fully supports object identity and abstract data types; andhave identified a variety of algebraic query transformations. The equivalence of two queriesis complicated by the presence of object identity. In this paper we define a hierarchy ofnotions of equivalence for queries; and present examples of equivalent querytransformations for each level of the hierarchy.Deductive and Object-Oriented Databases W.Kim; J.-M. Nicolas; and S. Nishio (Editors)© Elsevier Science Publishers BV (North-Holland);1990 281 OBJECT-ORIENTED QUERIES: EQUIVALENCE AND OPTIMIZATION* Gail M.Shaw and Stanley B. Zdonik Department of Computer Science Brown University Providence;RI 02912 We are interested in efficiently accessing data in an object-oriented database …,*,1990,75
Data staging for on-demand broadcast,Demet Aksoy; Michael J Franklin; Stan Zdonik,Abstract The increasing deployment of broadband services that are inherently broadcast-capable has made wide-area data broadcast an attractive data delivery alternative for largeclient populations. There has been significant work towards developing on-line broadcastscheduling algorithms for systems where all data items are readily available in the server'smain memory. These studies ignore the data management issues that arise when the data tobe broadcast must be obtained from secondary storage or remote locations. In this paper;we propose three complementary solutions to such data staging concerns: opportunisticscheduling; server caching; and prefetching. These techniques exploit hints provided by thescheduling algorithm. A detailed performance evaluation using an IP Multicastbased testbedshows that these data staging techniques can dramatically enhance the performance of a …,VLDB,2001,74
Object-Oriented Type Evolution.,Stanley B Zdonik,*,DBPL,1987,70
Clovers: The dynamic behavior of types and instances,Lynn Andrea Stein; Stanley B Zdonik,Abstract Clovers are a new mechanism for object-oriented languages that relax theconstraints of the conventional type/instance distinction. Clovers provide a new deﬁnition ofobject-hood; in which a single object may consist of multiple overlapping representations;sharing aspects of both behavior and identity. We show how clovers can be used toimplement multiple views; changes to the type of an object; and expanded type notions suchas minimal template. We argue that clovers provide a useful uniﬁcation of the type/instancerelaxations that have been presented in the literature; such as versioning; prototypes; andboolean classes.,*,1989,68
Rule languages and internal algebras for rule-based optimizers,Mitch Cherniack; Stanley B Zdonik,Abstract Rule-based optimizers and optimizer generators use rules to specify querytransformations. Rules act directly on query representations; which typically are based onquery algebras. But most algebras complicate rule formulation; and rules over thesealgebras must often resort to calling to externally defined bodies of code. Code makes rulesdifficult to formulate; prove correct and reason about; and therefore compromises theeffectiveness of rule-based systems. In this paper we present KOLA: a combinator-basedalgebra designed to simplify rule formulation. KOLA is not a user language; and KOLA'svariable-free queries are difficult for humans to read. But KOLA is an effective internalalgebra because its combinator-style makes queries manipulable and structurally revealing.As a result; rules over KOLA queries are easily expressed without the need for …,ACM SIGMOD Record,1996,67
The bigdawg polystore system,Jennie Duggan; Aaron J Elmore; Michael Stonebraker; Magda Balazinska; Bill Howe; Jeremy Kepner; Sam Madden; David Maier; Tim Mattson; Stan Zdonik,Abstract This paper presents a new view of federated databases to address the growingneed for managing information that spans multiple data models. This trend is fueled by theproliferation of storage engines and query languages based on the observation that “no onesize fits all”. To address this shift; we propose a polystore architecture; it is designed to unifyquerying over multiple data models. We consider the challenges and opportunitiesassociated with polystores. Open questions in this space revolve around query optimizationand the assignment of objects to storage engines. We introduce our approach to thesetopics and discuss our prototype in the context of the Intel Science and Technology Centerfor Big Data,ACM Sigmod Record,2015,63
Revision processing in a stream processing engine: A high-level design,Esther Ryvkina; Anurag S Maskey; Mitch Cherniack; Stan Zdonik,Data stream processing systems have become ubiquitous in academic [1; 2; 5; 6] andcommercial [11] sectors; with application areas that include financial services; network trafficanalysis; battlefield monitoring and traffic control [3]. The append-only model of streamsimplies that input data is immutable and therefore always correct. But in practice; streamingdata sources often contend with noise (eg; embedded sensors) or data entry errors (eg;financial data feeds) resulting in erroneous inputs and therefore; erroneous query results.Many data stream sources (eg; commercial ticker feeds) issue" revision tuples"(revisions)that amend previously issued tuples (eg erroneous share prices). Ideally; any streamprocessing engine should process revision inputs by generating revision outputs that correctprevious query results. We know of no stream processing system that presently has this …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,62
Database systems-breaking out of the box,Abraham Silberschatz; Stan Zdonik,The eld of database systems research and development has been enormously successfulover its 30 year history. It has led to a $10 billion industry with an installed base that touchesvirtually every major company in the world. It would be unthinkable to manage the largevolume of valuable information that keeps corporations running without support fromcommercial database management systems (DBMS). Today; the eld of database research islargely de ned by its previous successes; and much current research is aimed at increasingthe functionality and performance of DBMS's. A DBMS is a very complex systemincorporating a rich set of technologies. These technologies have been assembled in a waythat is ideally suited for solving problems of large-scale data management in the corporatesetting. However; a DBMS; as does any large tool; places some requirements on the …,SIGMOD Record,1997,59
DBIS-Toolkit: Adaptable middleware for large scale data delivery,Mehmet Altinel; Demet Aksoy; Thomas Baby; Michael Franklin; William Shapiro; Stan Zdonik,The proliferation of the Internet and intranets; advances in wireless and satellite networks;and the availability of asymmetric; high-bandwidth links to the home; have fueled thedevelopment of a wide range of new “dissemination-based” applications. These applicationsinvolve the timely distribution of data to a large set of consumers; and include stock andsports tickers; traffic information systems; electronic personalized newspapers; andentertainment delivery. Dissemination-oriented applications have special characteristics thatrender traditional client-server data management approaches ineffective. These include:tremendous scale; significant overlap in user data needs; and asymmetric data flow fromsources to consumers. The mismatch between the data access characteristics of theseapplications and the technology used to implement them on the WWW results in …,ACM SIGMOD Record,1999,56
Broadcast disks: dissemination-based data management for asymmetric communication environments,Swarup Acharya; Stanley Zdonik,Abstract The increasing ability to interconnect computers through internetworking; wirelessnetworks; high-bandwidth satellite; and cable networks has spawned a new class ofinformation-centered applications based on data dissemination. These applications; whichoften serve huge client populations; employ broadcast to efficiently deliver data to theclients. In data dissemination; the data transfer is initiated by the server; inverting thetraditional relationship between the client and the server. This thesis proposes a novel “multi-disk” framework for data dissemination called Broadcast Disks. The Broadcast Disksapproach significantly improves upon the previous work in dissemination-based systemsand raises a number of fundamentally new research challenges. In this thesis; we firstmotivate why the rise of asymmetric environments (ie; networks which have a significantly …,*,1998,56
A demonstration of the bigdawg polystore system,Aaron Elmore; Jennie Duggan; Mike Stonebraker; Magdalena Balazinska; Ugur Cetintemel; Vijay Gadepally; Jeffrey Heer; Bill Howe; Jeremy Kepner; Tim Kraska; Samuel Madden; David Maier; Timothy Mattson; Stavros Papadopoulos; Jeff Parkhurst; Nesime Tatbul; Manasi Vartak; Stan Zdonik,Abstract This paper presents BigDAWG; a reference implementation of a new architecturefor" Big Data" applications. Such applications not only call for large-scale analytics; but alsofor real-time streaming support; smaller analytics at interactive speeds; data visualization;and cross-storage-system queries. Guided by the principle that" one size does not fit all"; webuild on top of a variety of storage engines; each designed for a specialized use case. Toillustrate the promise of this approach; we demonstrate its effectiveness on a hospitalapplication using data from an intensive care unit (ICU). This complex application serves theneeds of doctors and researchers and provides real-time support for streams of patient data.It showcases novel approaches for querying across multiple storage engines; datavisualization; and scalable real-time analytics.,Proceedings of the VLDB Endowment,2015,55
Query Steering for Interactive Data Exploration.,Ugur Cetintemel; Mitch Cherniack; Justin DeBrabant; Yanlei Diao; Kyriaki Dimitriadou; Alexander Kalinin; Olga Papaemmanouil; Stanley B Zdonik,ABSTRACT Traditional DBSMs are suited for applications in which the structure; meaningand contents of the database; as well as the questions to be asked are already wellunderstood. There is; however; a class of applications that we will collectively refer to asInteractive Data Exploration (IDE) applications; in which this is not the case. IDE is a keyingredient of a diverse set of discovery-oriented applications we are dealing with; includingones from scientific computing; financial analysis; evidence-based medicine; and genomics.The need for effective IDE will only increase as data are being collected at anunprecedented rate. IDE is fundamentally a multi-step; non-linear process with impreciseend-goals. For example; data-driven scientific discovery through IDE often requires non-expert users to iteratively interact with the system to make sense of and to identify …,CIDR,2013,55
Fast and highly-available stream processing over wide area networks,Jeong-Hyon Hwang; Ugur Cetintemel; Stan Zdonik,We present a replication-based approach that realizes both fast and highly-available streamprocessing over wide area networks. In our approach; multiple operator replicas sendoutputs to each downstream replica so that it can use whichever data arrives first. To furtherexpedite the data flow; replicas run independently; possibly processing data in differentorders. Despite this complication; our approach always delivers what non-replicatedprocessing would produce without failures. We call this guarantee replication transparency.In this paper; we first discuss semantic issues for replication transparency and extend stream-processing primitives accordingly. Next; we develop an algorithm that manages replicas atgeographically dispersed servers. This algorithm strives to achieve the best latencyguarantee; relative to the cost of replication. Finally; we substantiate the utility of our work …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,55
Control of an extensible query optimizer: A planning-based approach,Gail Mitchell; Umeshwar Dayal; Stanley B Zdonik,Abstract III this paper we address the problem of controlling the execution of a queryoptimizer. We describe a control for the optimization process that is based on planning. Thecontroller described here is a goal-directed planner that intermingles planning with theexecution of query transformations; and uses execution results to direct further planning ofoptimizer processing. We describe this control in the context of the Epoq extensiblearchitecture. Epoq iz an approach to extensible query optimization that integratesspecialized rewrite strategies through its extensible control mechanism. This paper de-scribes our planning-based approach to extensible control and illustrates it with a simpleexample.,VLDB,1993,54
The AQUA approach to querying lists and trees in object-oriented databases,Bharathi Subramanian; Theodore W Leung; Scott L Vandenberg; Stanley B Zdonik,Relational database systems and most object-oriented database systems provide supportfor queries. Usually these queries represent retrievals over sets or multisets. Many newapplications for databases; such as multimedia systems and digital libraries; need supportfor queries on complex bulk types such as lists and trees. In this paper we describe an object-oriented query algebra called AQUA (= A Query Algebra) for lists and trees. The operators inthe algebra preserve the ordering between the elements of a list or tree; even when theresult list or tree contains an arbitrary set of nodes from the original tree. We also presentpredicate languages for lists and trees which allow order-sensitive queries because theyuse pattern matching to examine groups of list or tree nodes rather than individual nodes.The ability to decompose predicate patterns enables optimizations that make use of …,Data Engineering; 1995. Proceedings of the Eleventh International Conference on,1995,50
On predictive modeling for optimizing transaction execution in parallel OLTP systems,Andrew Pavlo; Evan PC Jones; Stanley Zdonik,Abstract A new emerging class of parallel database management systems (DBMS) isdesigned to take advantage of the partitionable workloads of on-line transaction processing(OLTP) applications [23; 20]. Transactions in these systems are optimized to execute tocompletion on a single node in a shared-nothing cluster without needing to coordinate withother nodes or use expensive concurrency control measures [18]. But some OLTPapplications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. These distributed transactions access data not stored within theirlocal partitions and subsequently require more heavy-weight concurrency control protocols.Further difficulties arise when the transaction's execution properties; such as the number ofpartitions it may need to access or whether it will abort; are not known beforehand. The …,Proceedings of the VLDB Endowment,2011,47
CORADD: Correlation aware database designer for materialized views and indexes,Hideaki Kimura; George Huo; Alexander Rasin; Samuel Madden; Stanley B Zdonik,Abstract We describe an automatic database design tool that exploits correlations betweenattributes when recommending materialized views (MVs) and indexes. Although there is asubstantial body of related work exploring how to select an appropriate set of MVs andindexes for a given workload; none of this work has explored the effect of correlatedattributes (eg; attributes encoding related geographic information) on designs. Our toolidentifies a set of MVs and secondary indexes such that correlations between the clusteredattributes of the MVs and the secondary indexes are enhanced; which can dramaticallyimprove query performance. It uses a form of Integer Linear Programming (ILP) called ILPFeedback to pick the best set of MVs and indexes for given database size constraints. Wecompare our tool with a state-of-the-art commercial database designer on two workloads …,Proceedings of the VLDB Endowment,2010,46
S-Store: a streaming NewSQL system for big velocity applications,Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; John Meehan; Andrew Pavlo; Michael Stonebraker; Erik Sutherland; Nesime Tatbul; Kristin Tufte; Hao Wang; Stanley Zdonik,Abstract First-generation streaming systems did not pay much attention to state managementvia ACID transactions (eg;[3; 4]). S-Store is a data management system that combines OLTPtransactions with stream processing. To create S-Store; we begin with H-Store; a main-memory transaction processing engine; and add primitives to support streaming. Thisincludes triggers and transaction workflows to implement push-based processing; windowsto provide a way to bound the computation; and tables with hidden state to implementscoping for proper isolation. This demo explores the benefits of this approach by showinghow a naïve implementation of our benchmarks using only H-Store can yield incorrectresults. We also show that by exploiting push-based semantics and our implementation oftriggers; we can achieve significant improvement in transaction throughput. We demo two …,Proceedings of the VLDB Endowment,2014,44
Changing the rules: Transformations for rule-based optimizers,Mitch Cherniack; Stan Zdonik,Abstract Rule-based optimizers are extensible because they consist of modifiable sets ofrules. For modification to be straightforward; rules must be easily reasoned about (ie;understood and verified). At the same time; rules must be expressive and efficient (to fire) forrule-based optimizers to be practical. Production-style rules (as in [15]) are expressed withcode and are hard to reason about. Pure rewrite rules (as in [1]) lack code; but cannotatomically express complex transformations (eg; normalizations). Some systems allow rulesto be grouped; but sacrifice efficiency by providing limited control over their firing. Therefore;none of these approaches succeeds in making rules expressive; efficient andunderstandable. We propose a language (COKO) for expressing an alternative form of inputto a rule-based optimizer. A COKO transformation consists of a set of declarative (KOLA) …,ACM SIGMOD Record,1998,43
Confidence-based data management for personal area sensor networks,Nesime Tatbul; Mark Buller; Reed Hoyt; Steve Mullen; Stan Zdonik,Abstract The military is working on embedding sensors in a" smart uniform" that will monitorkey biological parameters to determine the physiological status of a soldier. The soldier'sstatus can only be determined accurately by combining the readings from several sensorsusing sophisticated physiological models. Unfortunately; the physical environment and thelow-bandwidth; push-based personal-area network (PAN) introduce uncertainty in the inputsto the models. Thus the model must produce a confidence level as well as a physiologicalstatus value. This paper explores how confidence levels can be used to influence datamanagement decisions. In particular; we look at power-efficient ways to keep the confidenceabove a given threshold. We also contrast push-based broadcast schedules with otherschedules that are made possible by two-way communication.,Proceeedings of the 1st international workshop on Data management for sensor networks: in conjunction with VLDB 2004,2004,41
Cooperative transaction hierarchies: Transaction support for design applications,Marian H Nodine; Stanley B Zdonik,Abstract Traditional atomic and nested transactions are not always well-suited tocooperative applications; such as design applications. Cooperative applications placerequirements on the database that may conflict with the serializability requirement. Theyrequire transactions to be long; possibly nested; and able to interact with each other in astructured way. We define a transaction framework; called a cooperative transactionhierarchy; that allows us to relax the requirement for atomic; serializable transactions tobetter support cooperative applications. In cooperative transaction hierarchies; we allow thecorrectness specification for groups of designers to be tailored to the needs of theapplication. We use patterns and conflicts to specify the constraints imposed on a group'shistory for it to be correct. We also provide some primitives to smooth the operation of the …,The VLDB Journal—The International Journal on Very Large Data Bases,1992,41
Incremental database systems: databases from the ground up,Stanley B Zdonik,Abstract This paper discusses a new approach to database management systems that isbetter suited to a wide class of new applications such as scientific; hypermedia; and financialapplications. These applications are characterized by their need to store large amounts ofraw; unstructured data. Our premise is that; in these situations; database systems need away to store data without imposing a schema; and a way to provide a schema incrementallyas we process the data. This requires that the raw data be mapped in complex ways to anevolving schema.,ACM SIGMOD Record,1993,40
S-Store: streaming meets transaction processing,John Meehan; Nesime Tatbul; Stan Zdonik; Cansu Aslantas; Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; Andrew Pavlo; Michael Stonebraker; Kristin Tufte; Hao Wang,Abstract Stream processing addresses the needs of real-time applications. Transactionprocessing addresses the coordination and safety of short atomic computations. Heretofore;these two modes of operation existed in separate; stove-piped systems. In this work; weattempt to fuse the two computational paradigms in a single system called S-Store. In thisway; S-Store can simultaneously accommodate OLTP and streaming applications. Wepresent a simple transaction model for streams that integrates seamlessly with a traditionalOLTP system; and provides both ACID and stream-oriented guarantees. We chose to build S-Store as an extension of H-Store-an open-source; in-memory; distributed OLTP databasesystem. By implementing S-Store in this way; we can make use of the transaction processingfacilities that H-Store already provides; and we can concentrate on the additional features …,Proceedings of the VLDB Endowment,2015,38
A database approach to languages; libraries and environments,Stanley B Zdonik; Peter Wegner,*,*,1985,37
A flexible framework for transaction management in engineering environments,Sandra Heiler; Sara Haradhvala; Stanley Zdonik; Barbara Blaustein; Arnon Rosenthal,*,Database transaction models for advanced applications,1992,36
Scalable application-aware data freshening,Donald Carney; Sangdon Lee; Stan Zdonik,Distributed databases and other networked information systems use copies or mirrors toreduce latency and to increase availability. Copies need to be refreshed. In a looselycoupled system; the copy sites are typically responsible for synchronizing their own copies.This involves polling and can be quite expensive if not done in a disciplined way. Weexplore the topic of how to determine a refresh schedule given knowledge of the updatefrequencies and limited bandwidth. The emphasis here is on how to use additionalinformation about the aggregate interest of the user community in each of the copies in orderto maximize the perceived freshness of the copies. We develop a model and an optimalsolution for small cases; presents several heuristic algorithms that work for large cases; thenexplores the impact of object size on the refresh schedule. It also presents experimental …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,35
Dealing with overload in distributed stream processing systems,Nesime Tatbul; Stan Zdonik,Overload management has been an important problem for large-scale dynamic systems. Inthis paper; we study this problem in the context of our Borealis distributed stream processingsystem. We show that server nodes must coordinate in their load shedding decisions toachieve global control on output quality. We describe a distributed load shedding approachwhich provides this coordination by upstream metadata aggregation and propagation.Metadata enables an upstream node to make fast local load shedding decisions which willinfluence its descendant nodes in the best possible way.,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,34
Load shedding on data streams,Nesime Tatbul; Ugur Çetintemel; Stan Zdonik; Mitch Cherniack; Michael Stonebraker,Page 1. Load Shedding on Data Streams Nesime Tatbul; Uğur Çetintemel; Stan Zdonik @ BrownUniversity Mitch Cherniack @ Brandeis University Michael Stonebraker @ MIT Page 2. HandlingOverload with Load Shedding ∎ real-time data pushed from financial data feeds; sensors; andalike ∎ high and unpredictable data rates ∎ resource overload => growing queues and late results ∎solution: “load shedding” ∎ eliminate excess load by dropping data Page 3. Drop k % RandomDrop Filter P(value) Semantic Drop Load Shedding by Inserting Drops QoS QoS ☠ ☠ σ π U σ πσ two types of drops: Page 4. Quality of Service ∎ Value-based QoS ∎ Loss-tolerance QoS utility %delivery 100 50 0 1.0 0.7 utility values 0 80 120 200 1.0 0.4 ▪ Latency-based QoS is handled byscheduler. Page 5. Problem Statement ∎ N: query network ∎ I : set of input streams ∎ C: processingcapacity when Load(N(I)) > C; transform N to N' …,Proceedings of the Workshop on Management and Processing of Data Streams (MPDS 03); San Diego; CA; USA,2003,34
A prolegomenon on OLTP database systems for non-volatile memory,Justin DeBrabant; Joy Arulraj; Andrew Pavlo; Michael Stonebraker; Stan Zdonik; Subramanya Dulloor,ABSTRACT The design of a database management system's (DBMS) architecture ispredicated on the target storage hierarchy. Traditional diskoriented systems use a two-levelhierarchy; with fast volatile memory used for caching; and slower; durable device used forprimary storage. As such; these systems use a buffer pool and complex concurrency controlschemes to mask disk latencies. Compare this to main memory DBMSs that assume all datacan reside in DRAM; and thus do not need these components. But emerging non-volatilememory (NVM) technologies require us to rethink this dichotomy. Such memory devices areslightly slower than DRAM; but all writes are persistent; even after power loss. We exploretwo possible use cases of NVM for on-line transaction processing (OLTP) DBMSs. The first iswhere NVM completely replaces DRAM and the other is where NVM and DRAM coexist …,ADMS@ VLDB,2014,33
An architecture for compiling udf-centric workflows,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Carsten Binnig; Ugur Cetintemel; Stan Zdonik,Abstract Data analytics has recently grown to include increasingly sophisticated techniques;such as machine learning and advanced statistics. Users frequently express these complexanalytics tasks as workflows of user-defined functions (UDFs) that specify each algorithmicstep. However; given typical hardware configurations and dataset sizes; the core challengeof complex analytics is no longer sheer data volume but rather the computation itself; andthe next generation of analytics frameworks must focus on optimizing for this computationbottleneck. While query compilation has gained widespread popularity as a way to tackle thecomputation bottleneck for traditional SQL workloads; relatively little work addresses UDF-centric workflows in the domain of complex analytics. In this paper; we describe a novelarchitecture for automatically compiling workflows of UDFs. We also propose several …,Proceedings of the VLDB Endowment,2015,32
A skip-list approach for efficiently processing forecasting queries,Tingjian Ge; Stan Zdonik,Abstract Time series data is common in many settings including scientific and financialapplications. In these applications; the amount of data is often very large. We seek to supportprediction queries over time series data. Prediction relies on model building which can betoo expensive to be practical if it is based on a large number of data points. We propose touse statistical tests of hypotheses to choose a proper subset of data points to use for a givenprediction query interval. This involves two steps: choosing a proper history length andchoosing the number of data points to use within this history. Further; we use an I/Oconscious skip list data structure to provide samples of the original data set. Based on thestatistics collected for a query workload; which we model as a probability mass function(PMF) over query intervals; we devise a randomized algorithm that selects a set of pre …,Proceedings of the VLDB Endowment,2008,32
The Case for Predictive Database Systems: Opportunities and Challenges.,Mert Akdere; Ugur Cetintemel; Matteo Riondato; Eli Upfal; Stanley B Zdonik,ABSTRACT This paper argues that next generation database management systems shouldincorporate a predictive model management component to effectively support both inward-facing applications; such as self management; and user-facing applications such as data-driven predictive analytics. We draw an analogy between model management and datamanagement functionality and discuss how model management can leverage profiling;physical design and query optimization techniques; as well as the pertinent challenges. Wethen describe the early design and architecture of Longview; a predictive DBMS prototypethat we are building at Brown; along with a case study of how models can be used to predictquery execution performance.,CIDR,2011,31
Fast and reliable stream processing over wide area networks,Jeong-Hyon Hwang; Ugur Cetintemel; Stan Zdonik,We present a replication-based approach that enables both fast and reliable streamprocessing over wide area networks. Our approach replicates stream processing operatorsin a manner where operator replicas compete with each other to make the earliest impact.Therefore; any processing downstream from such replicas can proceed by relying on thefastest replica without being held back by slow or failed ones. Furthermore; our approachallows replicas to produce output in different orders so as to avoid the cost of forcing anidentical execution across replicas; without sacrificing correctness. We first considersemantic issues for correct replicated stream processing and; based on a formal foundation;extend common stream-processing primitives. Next; we discuss strategies for deployingreplicas. Finally; we present preliminary remits obtained from experiments on Planet-Lab …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,31
Object management systems for design environments,Stanley Zdonik,ABSTRACT Object management systems can be distinguished from their more conventionalcounterparts (ie; DBMS's) by their ability to deal with arbitrary object types in an environmentthat is constantly changing. They are particularly suited for environments such as officeinformation systems; electrical CAD; and programming environments. All of theseenvironments deal with the process of design.,Quarterly Bulletin of IEEE on Database Engineering,1985,30
The end of slow networks: It's time for a redesign,Carsten Binnig; Andrew Crotty; Alex Galakatos; Tim Kraska; Erfan Zamanian,Abstract The next generation of high-performance networks with remote direct memoryaccess (RDMA) capabilities requires a fundamental rethinking of the design of distributed in-memory DBMSs. These systems are commonly built under the assumption that the networkis the primary bottleneck and should be avoided at all costs; but this assumption no longerholds. For instance; with InfiniBand FDR 4×; the bandwidth available to transfer data acrossthe network is in the same ballpark as the bandwidth of one memory channel. Moreover;RDMA transfer latencies continue to rapidly improve as well. In this paper; we first argue thattraditional distributed DBMS architectures cannot take full advantage of high-performancenetworks and suggest a new architecture to address this problem. Then; we discuss initialresults from a prototype implementation of our proposed architecture for OLTP and OLAP …,Proceedings of the VLDB Endowment,2016,29
Health informatics in the cloud,Mark L Braunstein,Despite its high cost; the US healthcare system produces relatively short life spans; and iswasteful; inefficient and has serious safety and quality issues. While other industries havesurmounted similar challenges by transforming themselves through information technology;healthcare lags behind. Major reasons are that our approaches to care delivery andfinancial incentives were designed for a bygone era. Beyond that the technology offered topractitioners has often been overly expensive; poorly designed; overly proprietary; hard toimplement and difficult to use. Spurred by a unique; one-time Federal stimulus and the newmobile; wireless and cloud technologies now available; this landscape is rapidly changing.To succeed going forward practitioners; and those interested in entering the field; need tounderstand the new driving forces and have a basic understanding of contemporary …,*,2012,28
Interactive data exploration using semantic windows,Alexander Kalinin; Ugur Cetintemel; Stan Zdonik,Abstract We present a new interactive data exploration approach; called Semantic Windows(SW); in which users query for multidimensional" windows" of interest via standard DBMS-style queries enhanced with exploration constructs. Users can specify SWs using (i) shape-based properties; eg;" identify all 3-by-3 windows"; as well as (ii) content-based properties;eg;" identify all windows in which the average brightness of stars exceeds 0.8". This SWapproach enables the interactive processing of a host of useful exploratory queries that aredifficult to express and optimize using standard DBMS techniques. SW uses a sampling-guided; data-driven search strategy to explore the underlying data set and quickly identifywindows of interest. To facilitate human-in-the-loop style interactive processing; SW isoptimized to produce online results during query execution. To control the tension …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,27
A visual interface for a database with version management,Jay W Davison; Stanley B Zdonik,Abstract This paper describes a graphical interface to an experimental database systemwhich incorporates a built-in version control mechanism that maintains a history of thedatabase development and changes. The system is an extension of ISIS [6]; Interface for aSemantic Information System; a workstation-based; graphical database programming tooldeveloped at Brown University. ISIS supports a graphical interface to a modified subset ofthe Semantic Data Model (SDM)[7]. The ISIS extension introduces a transaction mechanismthat interacts with the version control facilities. A series of version control support tools havebeen added to ISIS to provide a notion of history to user-created databases. The user canform new versions of three types of ISIS objects: a class definition object (a type); the set ofinstances of a class (the content); and an entity. A version-viewing mechanism is provided …,ACM Transactions on Information Systems (TOIS),1986,27
Tupleware:" Big" Data; Big Analytics; Small Clusters.,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Ugur Çetintemel; Stanley B Zdonik,ABSTRACT There is a fundamental discrepancy between the targeted and actual users ofcurrent analytics frameworks. Most systems are designed for the challenges of the Googlesand Facebooks of the world—processing petabytes of data distributed across large clouddeployments consisting of thousands of cheap commodity machines. Yet; the vast majority ofusers analyze relatively small datasets of up to several terabytes in size; perform primarilycompute-intensive operations; and operate clusters ranging from only a few to a few dozennodes. Targeting these users fundamentally changes the way we should build analyticssystems. This paper describes our vision for the design of TUPLEWARE; a new systemspecifically aimed at complex analytics on small clusters. TUPLEWARE's architecture bringstogether ideas from the database and compiler communities to create a powerful end-to …,CIDR,2015,26
Automatic Vertical-Database Design,*,An automatic physical-layout designer for a database-management system determines thedatabase's physical layout from a set of training queries; the database's logical design; anda parameter k that indicates how many storage nodes can be lost without losing access toany of the data. The designer lays the database out as a column store such that the storedcolumns constitute redundant projections on the system's different storage nodes. Itrepeatedly identifies a projection; whose addition to the design will result in the greatestperformance improvement for the training queries. In doing so; it takes into account thedifferent compression formats to which the different projections lend themselves. When aprojection has been identified as one to be added; it is added on one node; and kprojections having the same columns are added to other nodes. The designer continues …,*,2008,26
A comparison of stream-oriented high-availability algorithms,Jeong-Hyon Hwang; Magdalena Balazinska; Alexander Rasin; Ugur Cetintemel; Michael Stonebraker; Stan Zdonik,Abstract Recently; significant efforts have focused on developing novel data-processingsystems to support a new class of applications that commonly require sophisticated andtimely processing of high-volume data streams. Early work in stream processing hasprimarily focused on streamoriented languages and resource-constrained; one-pass query-processing. High availability; an increasingly important goal for virtually all data processingsystems; is yet to be addressed. In this paper; we first describe how the standardhighavailability approaches used in data management systems can be applied to distributedstream processing. We then propose a novel stream-oriented approach that exploits theunique data-flow nature of streaming systems. Using analysis and a detailed simulationstudy; we characterize the performance of each approach and demonstrate that the …,*,2003,26
Bridging the archipelago between row-stores and column-stores for hybrid workloads,Joy Arulraj; Andrew Pavlo; Prashanth Menon,Abstract Data-intensive applications seek to obtain trill insights in real-time by analyzing acombination of historical data sets alongside recently collected data. This means that tosupport such hybrid workloads; database management systems (DBMSs) need to handleboth fast ACID transactions and complex analytical queries on the same database. But thecurrent trend is to use specialized systems that are optimized for only one of theseworkloads; and thus require an organization to maintain separate copies of the database.This adds additional cost to deploying a database application in terms of both storage andadministration overhead. To overcome this barrier; we present a hybrid DBMS architecturethat efficiently supports varied workloads on the same database. Our approach differs fromprevious methods in that we use a single execution engine that is oblivious to the storage …,Proceedings of the 2016 International Conference on Management of Data,2016,25
Correlation maps: a compressed access method for exploiting soft functional dependencies,Hideaki Kimura; George Huo; Alexander Rasin; Samuel Madden; Stanley B Zdonik,Abstract In relational query processing; there are generally two choices for access pathswhen performing a predicate lookup for which no clustered index is available. One option isto use an unclustered index. Another is to perform a complete sequential scan of the table.Many analytical workloads do not benefit from the availability of unclustered indexes; thecost of random disk I/O becomes prohibitive for all but the most selective queries. It has beenobserved that a secondary index on an unclustered attribute can perform well under certainconditions if the unclustered attribute is correlated with a clustered index attribute [4]. Theclustered index will co-locate values and the correlation will localize access through theunclustered attribute to a subset of the pages. In this paper; we show that in a realapplication (SDSS) and widely used benchmark (TPC-H); there exist many cases of …,Proceedings of the VLDB Endowment,2009,25
Ordered types in the aqua data model,Bharathi Subramanian; Theodore W Leung; Stanley B Zdonik; Scott L Vandenberg,Abstract We present a query algebra that supports ordering among the data elements. Orderis defined as a relationship between various data elements of an instance. This relationshipcan be a total or partial order among the elements or among equivalence classes whereeach equivalence class consists of one or more elements. In terms of data structures;ordered types can be viewed as graphs; trees; or lists. Lately there has been a lot of interestin bulk types like lists; trees; and graphs that are not supported by traditional data modelsand query algebras. This interest is fueled by the fact that much of the data in the scientificdomain is inherently ordered. Therefore; scientific applications that involve genomesequences; satellite data; scientific data; etc. require database support for ordered datastructures like lists; trees; and graphs. In this paper; we discuss an extension to the AQUA …,*,1994,25
Handling uncertain data in array database systems,Tingjian Ge; Stan Zdonik,Scientific and intelligence applications have special data handling needs. In these settings;data does not fit the standard model of short coded records that had dominated the datamanagement area for three decades. Array database systems have a specializedarchitecture to address this problem. Since the data is typically an approximation of reality; itis important to be able to handle imprecision and uncertainty in an efficient and provablyaccurate way. We propose a discrete approach for value distributions and adopt a standardmetric (ie; variation distance) in probability theory to measure the quality of a resultdistribution. We then propose a novel algorithm that has a provable upper bound on thevariation distance between its result distribution and the" ideal" one. Complementary to that;we advocate the usage of a" statistical mode" suitable for the results of many queries and …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,24
An architecture for query processing in persistent object stores,Gail Mitchell; Stanley Benjamin Zdonik; Umeshwar Dayal,Query optimizers for persistent object systems should be extensible to react to user-suppliedabstract types. Current architectures support only a single; non-extensible technique forcontrolling the optimization process. The authors propose an alternative to the currentextensible architectures that will support multiple optimizer control strategies and theaddition of new control strategies. The optimizer consists of a collection of optimizationregions; each of which can transform queries according to a particular control strategy; set oftransformations and cost model. A global optimizer control coordinates the movement of aquery between these regions. This architecture provides extensibility in the optimizer'srepertoire of control strategies through the addition of new regions. The authors describetheir approach and demonstrate its utility by following the optimizer as it works on an …,System Sciences; 1992. Proceedings of the Twenty-Fifth Hawaii International Conference on,1992,24
Ss-db: A standard science dbms benchmark,Philippe Cudre-Mauroux; Hideaki Kimura; Kian-Tat Lim; Jennie Rogers; Samuel Madden; Michael Stonebraker; Stanley B Zdonik; Paul G Brown,*,Under submission,2010,21
Specifying Flexible Tasks in a Multi-database.,Marian H Nodine; Noela Nakos; Stanley B Zdonik,Abstract Interactions are open nested exible transactions used to de ne long-lived tasks thataccess multiple heterogeneous databases. Interactions operate on a multidatabase wherethe local databases are accessed via procedures called steps. The steps in a local databaseencapsulate its information and allow it to be accessed uniformly. In this paper; we focus onissues of exible task speci cation in a multidatabase. We de ne a language; TaSL; thatallows a multidatabase user to de ne an Interaction to the multidatabase in terms of the stepsprovided by the local databases. TaSL allows an Interaction de ner to specify sequences ofsteps that should be executed atomically (as classic transactions) in the multidatabase. Theuser also can specify constraints that must be maintained on the data for the Interaction itselfto remain consistent. If these constraints are violated; committed transactions may need to …,CoopIS,1994,21
Searchlight: Enabling integrated search and exploration over large multidimensional data,Alexander Kalinin; Ugur Cetintemel; Stan Zdonik,Abstract We present a new system; called Searchlight; that uniquely integrates constraintsolving and data management techniques. It allows Constraint Programming (CP)machinery to run efficiently inside a DBMS without the need to extract; transform and movethe data. This marriage concurrently offers the rich expressiveness and efficiency ofconstraint-based search and optimization provided by modern CP solvers; and the ability ofDBMSs to store and query data at scale; resulting in an enriched functionality that caneffectively support both data-and search-intensive applications. As such; Searchlight is thefirst system to support generic search; exploration and mining over large multi-dimensionaldata collections; going beyond point algorithms designed for point search and mining tasks.,Proceedings of the VLDB Endowment,2015,20
Object-Oriented Query Optimization: What's the Problem?,Gail Mitchell; Stanley Benjamin Zdonik; Umeshwar Dayal,Abstract An object-oriented database model can support features such as abstract datatypes; methods; encapsulation; subtyping (or inheritance); complex structures; and objectidentity. The processing of queries in such a model also entails support for these features.Query optimization will require new techniques for supporting the object-oriented features.Although many of the problems that must be solved by an object-oriented query optimizerare similar to problems solved by relational and extensible optimizers; there are also manyproblems that are unique to the object-oriented model. In this paper we explore some of theproblems that are encountered when trying to optimize queries in an object-orienteddatabase. We present these problems in the context of the modelling constructs whichgenerate the problem. We also survey current approaches to solving some of these …,*,1991,20
Synchronization and Recovery in Cooperative Transactions.,Marian H Nodine; Andrea H Skarra; Stanley B Zdonik,Abstract Traditional atomic transactions do not work well in databases used by designapplications. A typical design task is divided into parallel subtasks that are interdependent.We relax the constraint of atomicity in these situations to allow cooperative transactions towork together in groups to accomplish design tasks in the database. These groups may bearbitrarily nested to form a cooperative transaction hierarchy. We examine the notions ofcorrectness; synchronization; and recovery within a single group in the cooperativetransaction hierarchy. Each cooperative transaction in a group maintains only partialconsistency in the database; and shares data with other transactions in the group in a waythat may violate the atomicity of each. We define a programmable mechanism to allow adatabase administrator to specify and enforce the correctness of the cooperative …,POS,1990,20
Maintaining consistency in a database with changing types,Stanley B Zdonik,Abstract In this work; we address the problem of maintaining consistency between a set ofpersistent objects and a set of type definitions that can change. We cast this work in thecontext of an object-oriented database system. The solution involves the use of a versioncontrol mechanism and a set of error handlers associated with the versions of a type. Wedescribe the structure of this error handling mechanism and demonstrate how it can be usedto address this problem.,ACM SIGPLAN Notices,1986,20
Load management and high availability in the borealis distributed stream processing engine,Nesime Tatbul; Yanif Ahmad; Uğur Çetintemel; Jeong-Hyon Hwang; Ying Xing; Stan Zdonik,Abstract Borealis is a distributed stream processing engine that has been developed atBrandeis University; Brown University; and MIT. It extends the first generation of data streamprocessing systems with advanced capabilities such as distributed operation; scalability withtime-varying load; high availability against failures; and dynamic data and querymodifications. In this paper; we focus on aspects that are related to load management andhigh availability in Borealis. We describe our algorithms for balanced and resilient loaddistribution; scalable distributed load shedding; and cooperative and self-configuring highavailability. We also present experimental results from our prototype implementationshowing the effectiveness of these algorithms.,*,2008,18
ENCORE: An object-oriented approach to database modelling and querying,Stanley B Zdonik; Gail Mitchell,*,Data Engineering,1991,18
Why properties are objects or some refinements of “is-a”,Stanley B Zdonik,Abstract This paper contains several examples that illustrate some problems with the is-arelationship. as defined by many objectoriented programming languages. These problemsrelate to two distinct areas: 1. the confusion between the inheritance of behavior and theinheritance of representation and 2. the lack of any requirement for semantic relationshipsbetween a named operation on a type and a replacement operation with the same name ona subtype. We indicate how these problems can be improved by making some usefuldistinctions. We then show how these distinctions can be built into the system easily bytreating properties as first-class objects and using the basic specialization techniques of thelanguage to express the differences.,Proceedings of 1986 ACM Fall joint computer conference,1986,18
Network Awareness in Internet-Scale Stream Processing.,Yanif Ahmad; Ugur Cetintemel; John Jannotti; Alexander Zgolinski; Stanley B Zdonik,Abstract Efficient query processing across a wide-area network requires network awareness;ie; tracking and leveraging knowledge of network characteristics when making optimizationdecisions. This paper summarizes our work on network-aware query processing techniquesfor widely-distributed; large-scale stream-processing applications. We first discuss theoperator placement problem (ie; deciding where to execute the operators of a query plan)and present results; based on a prototype deployment on the PlanetLab network testbed;that quantify the benefits of network awareness. We then present a summary of our presentfocus on the operator distribution problem; which involves parallelizing the evaluation of asingle operator in a networked setting.,IEEE Data Eng. Bull.,2005,17
FUGUE: a model for engineering information systems and other baroque applications,Sandra Heiler; Stanley B Zdonik,Abstract This paper describes the FUGUE 1 data model; an extension of DAPLEX that wasdeveloped to support engineering information systems. It begins by discussing theapplication requirements that must be met by such a model. They include the accuraterepresentation of engineering data and processes; mechanisms to integrate heterogeneoushardware and software components and to tailor each instance of an EIS to localrequirements and policies; and a type system that is powerful enough to support productionsystems; yet flexible enought to support design environments. The primitive notions of objectand function are described. Then; a type system that combines the discipline of conventionaltypes and instances with the flexibility of prototypes is presented. Finally; we show variousways of creating new objects and deriving new functions.,*,1988,17
Can objects change type? Can type objects change,Stanley B Zdonik,*,Proceedings Workshop on Database Programming Languages; Altair; France,1987,17
Upi: A primary index for uncertain databases,Hideaki Kimura; Samuel Madden; Stanley B Zdonik,Abstract Uncertain data management has received growing attention from industry andacademia. Many efforts have been made to optimize uncertain databases; including thedevelopment of special index data structures. However; none of these efforts have exploredprimary (clustered) indexes for uncertain databases; despite the fact that clustering has thepotential to offer substantial speedups for non-selective analytic queries on large uncertaindatabases. In this paper; we propose a new index called a UPI (Uncertain Primary Index)that clusters heap files according to uncertain attributes with both discrete and continuousuncertainty distributions. Because uncertain attributes may have several possible values; aUPI on an uncertain attribute duplicates tuple data once for each possible value. To preventthe size of the UPI from becoming unmanageable; its size is kept small by placing low …,Proceedings of the VLDB Endowment,2010,16
Borealis-r: a replication-transparent stream processing system for wide-area monitoring applications,Jeong-Hyon Hwang; Sanghoon Cha; Uǧur Cetintemel; Stan Zdonik,ABSTRACT Borealis-R is a replication-based system for both fast and highly-availableprocessing of data streams over wide-area networks. In Borealis-R; multiple operatorreplicas send outputs to downstream replicas; allowing each replica to use whichever dataarrives first. To further reduce latency; replicas run without coordination; possibly processingdata in different orders. Despite this flexibility; Borealis-R guarantees that applicationsalways receive the same results as in the non-replicated; failure-free case. In addition;Borealis-R deploys replicas at select network locations to effectively improve performanceas well as availability. We demonstrate the strengths of Borealis-R using a live wide-areamonitoring application. We show that Borealis-R outperforms previous solutions in terms oflatency and that it uses system resources efficiently by carefully deploying and discarding …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,16
Heterogeneous information systems: Understanding integration,Sandra Heiler; Michael Siegel; Stanley Zdonik,Research in heterogeneous database systems has provided the basis for the developmentof integration tools or frameworks. These tools are used to build systems that provideinformation from multiple disparate systems or data sources. As many as thirty commercialframeworks exist for integrating heterogeneous database systems and numerous integratedsystems are being developed for organizations with multiple information sourcerequirements. Unfortunately; among integration framework and system developers there islittle agreement on the definition of integration or on how to describe the frameworks andwhat they do. The authors present a definition of integration and define parameters fordescribing; specifying; and comparing integration frameworks and integrated systems. Theparameters are based on the integration space where the dimensions define the hiding or …,Interoperability in Multidatabase Systems; 1991. IMS'91. Proceedings.; First International Workshop on,1991,16
Query optimization in object-oriented databases,Stanley B Zdonik,The use of data abstraction in object-oriented databases places a burden on the ability ofthe system to perform query optimization. A framework for query specification andoptimization is discussed that is applicable to object-oriented database systems that take astrict view of data abstraction. Techniques that preserve much of the optimization potential ofrelational languages by limiting the query language are examined. Techniques are given forquery optimization that involve type-specific rewrite rules.,System Sciences; 1989. Vol. II: Software Track; Proceedings of the Twenty-Second Annual Hawaii International Conference on,1989,16
Object management system concepts: Supporting integrated office workstation applications,Stanley Benjamin Zdonik,Abstract The capabilities of a system for storing and retrieving office style objects aredescribed in this work. Traditional file systems provide facilities for the storage and retrievalof objects that are created in user programs; but the semantics of these objects are notavailable to the file system. Database management systems provide a means of describingthe semantics of objects using a single basic paradigm; the record. This model is inadequatefor describing the richer semantics of office objects. An object management systemcombines the advantages of both a file system and a database management system in that itcan store arbitrarily defined programming language objects and at the same time maintain ahigh-level description of their meaning. This work presents a high-level model of data thatcan be used to describe office objects more effectively than data processing oriented …,*,1983,16
The VC-dimension of SQL queries and selectivity estimation through sampling,Matteo Riondato; Mert Akdere; Uǧur Çetintemel; Stanley B Zdonik; Eli Upfal,Abstract We develop a novel method; based on the statistical concept of VC-dimension; forevaluating the selectivity (output cardinality) of SQL queries–a crucial step in optimizing theexecution of large scale database and data-mining operations. The major theoreticalcontribution of this work; which is of independent interest; is an explicit bound on the VC-dimension of a range space defined by all possible outcomes of a collection (class) ofqueries. We prove that the VC-dimension is a function of the maximum number of Booleanoperations in the selection predicate; and of the maximum number of select and joinoperations in any individual query in the collection; but it is neither a function of the numberof queries in the collection nor of the size of the database. We develop a method based onthis result: given a class of queries; it constructs a concise random sample of a database …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2011,15
Aredisks in the air'just pie in the sky,Stanley Zdonik; Rafael Alonso,Abstract Mobile computers and wireless networks are emerg-ing technologies which willsoon make ubiquitous com-puting a reality. In the wireless environment; mo-bile clients mayoften be disconnected from stationary server machines or inay have only a low-bandwidthchannel for sending messages to servers. This en-vironment raises new challenges for thesupport of daiabase applications for three reasons: 1) the limited storage capacities ofmobile machines; 2) the inabil-ity to accurately predict the future data needs of many data-intensive applications; and 3) ihe need to pro-vide clients with new or updated data valuesin order to ensure consistent data access. One way (perhaps the only way) to address thesechallenges is to provide the stationary server machines with a relatively high-bandswidthchannel over which to broadcast portions of the database. Wireless networks are but one …,In Proceedings of the IEEE Workshop on Mobile Computing Systems and Applications,1994,15
Implementing Persistent Object Bases: Principles and Practice,Alan Dearle; Gail M Shaw; Stanley Benjamin Zdonik,*,*,1991,15
Data abstraction and query optimization,Stanley B Zdonik,Abstract The use of data abstraction in object-oriented databases places a burden on theability of the system to perform query optimization. This paper discusses a framework forquery specification and optimization that is applicable to object-oriented database systemsthat take a strict view of data abstraction. It examines techniques that preserve much of theoptimization potential of relational languages by limiting the query language. It furtherexamines techniques for query optimization that involve type-specific rewrite rules.,International Workshop on Object-Oriented Database Systems,1988,15
Index hint for on-demand broadcasting,Sangdon Lee; Donard P Carney; Stan Zdonik,We describe an index hint mechanism for on-demand data broadcast environments; whichenables mobile clients to spend less power consumption by reducing their tuning times. Wepropose approaches to estimate the data broadcast times for a set of data items so that theindex mechanism can be applied to on-demand data broadcasts. We also propose how toorganize and apply index hints using the estimated broadcast times of data items. Oursimulation results show that the proposed index hint mechanism effectively reduces thetuning times of mobile clients; thereby reducing their power consumption.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,14
The aurora and medusa projects,Stan Zdonik Sbz; Stan Zdonik; Michael Stonebraker; Mitch Cherniack; Ugur C Etintemel; Magdalena Balazinska; Hari Balakrishnan,Abstract This document summarizes the research conducted in two interrelated projects. TheAurora project being implemented at Brown and Brandeis under the direction of U gur Cetintemel; Mitch Cherniack; Michael Stonebraker and Stan Zdonik strives to build a single-site high performance stream processing engine. It has an innovative collection of operators;workflow orientation; and strives to maximize quality of service for connecting applications. Afurther goal of Aurora is to extend this engine to a distributed environment in which multiplemachines closely co-operate in achieving high quality of service.,IEEE Data Engineering Bulletin,2003,14
The Aurora and Borealis Stream Processing Engines,Uğur Çetintemel; Daniel Abadi; Yanif Ahmad; Hari Balakrishnan; Magdalena Balazinska; Mitch Cherniack; Jeong-Hyon Hwang; Samuel Madden; Anurag Maskey; Alexander Rasin; Esther Ryvkina; Mike Stonebraker; Nesime Tatbul; Ying Xing; Stan Zdonik,Abstract Over the last several years; a great deal of progress has been made in the area ofstream-processing engines (SPEs). Three basic tenets distinguish SPEs from current dataprocessing engines. First; they must support primitives for streaming applications. UnlikeOnline Transaction Processing (OLTP); which processes messages in isolation; streamingapplications entail time series operations on streams of messages. Second; streamingapplications entail a real-time component. If one is content to see an answer later; then onecan store incoming messages in a data warehouse and run a historical query on thewarehouse to find information of interest. This tactic does not work if the answer must beconstructed in real time. The need for real-time answers also dictates a fundamentallydifferent storage architecture. DBMSs universally store and index data records before …,*,2016,13
What Makes Object-Oriented Database Management Systems Different?,Stanley B Zdonik,Abstract In this paper; we briefly introduce the main features of an object-oriented databasemanagement system (OODBMS). We describe the typical architecture of OODBMSs andpoint out the differences between this architecture and that of a more conventional system(eg; relational database). We discuss some of the issues involved in implementing this styleof database system including the movement of data between client and server in adistributed setting. We also mention some of the unique aspects of querying an object-oriented database.,*,1994,13
A demonstration of SciDB: A science-oriented DBMS,M Balazinska; J Becla; D Heath; D Maier; M Stonebraker; S Zdonik,ABSTRACT In CIDR 2009; we presented a collection of requirements for SciDB; a DBMSthat would meet the needs of scientific users. These included a nested-array data model;sciencespecific operations such as regrid; and support for uncertainty; lineage; and namedversions. In this paper; we present an overview of SciDB's key features and outline ademonstration of the first version of SciDB on data and operations from one of our lighthouseusers; the Large Synoptic Survey Telescope (LSST).,Cell,2009,12
Profile driven data management,M Cherniak; E Galvez; D Brooks; M Franklin; S Zdonik,Abstract To support the popular vision of a semantic web; data management must beintroduced as part of the web's infrastructure. Traditional database systems follow the DBA-based model for data management—a DBA consults with clients about their intended usesof the database and sets data management policies (schema design; indexes; clusteringetc.) accordingly. This model does not transfer well to the web where there are too manyclients to consider in determining policy; and autonomous data sources prevent externallyset data management policies from being imposed. Profile-driven data management is aframework for determining and implementing data management policies upon the analysisof user data requirements specified in profiles. Profiledriven data management systems actas middleware intermediaries between users and the web; automatically offering data …,28th International Conference on Very Large Databases,2002,12
Cooperative transaction hierarchies,Marian H Nodine; Mary F Fernandez; Andrea H Skarra; Stanley B Zdonik,Abstract In this paper; we examine some of the requirements designers place on databases.Based on these requirements; we deﬁne a new transaction framework called a coup-erativetransaction hierarchy. This framework allows us to relax the criterion that transactions beatomic; while restricting the eifect that this has on synchronization and recovery.,Brown University; Providence; RI,1990,12
The Object-Oriented Database System Manifesto,F Bancilhon; David DeWitt; Klaus Dittrich; David Maier; Stanley Zdonik,Abstract This paper attempts to define an object-oriented database system. It describes themain features and characteristics that a system must have to qualify as an object-orienteddatabase system. We have separated these characteristics into three groups: o Mandatory;the ones the system must satisfy in order to be termed an object-oriented database system.These are complex objects; object identity; encapsulation; types or classes; inheritance;overriding combined with late binding; extensibility; computational completeness;persistence; secondary storage management; concurrency; recovery and an ad hoc queryfacility. o Optional; the ones that can be added to make the system better; but which are notmandatory. These are multiple inheritance; type checking and inferencing; distribution;design transactions and versions. o Open; the points where the designer can make a …,1st International Conference on Deductive and Object-Oriented Databases; Kyoto,1989,12
Tupleware: Redefining modern analytics,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Ugur Cetintemel; Stan Zdonik,Abstract: There is a fundamental discrepancy between the targeted and actual users ofcurrent analytics frameworks. Most systems are designed for the data and infrastructure ofthe Googles and Facebooks of the world---petabytes of data distributed across large clouddeployments consisting of thousands of cheap commodity machines. Yet; the vast majority ofusers operate clusters ranging from a few to a few dozen nodes; analyze relatively smalldatasets of up to a few terabytes; and perform primarily compute-intensive operations.Targeting these users fundamentally changes the way we should build analytics systems.This paper describes the design of Tupleware; a new system specifically aimed at thechallenges faced by the typical user. Tupleware's architecture brings together ideas from thedatabase; compiler; and programming languages communities to create a powerful end …,arXiv preprint arXiv:1406.6667,2014,11
Inferring function semantics to optimize queries,Mitch Cherniack; Stanley B Zdonik,Abstract The goal of the COKO-KOLA project [10; 91 is to express rules of rule-basedoptimizers in a manner permitting verification with a theorem prover. In [IO]; we consideredquery transformations that were too general to be expressed with rewrite rules. In this paper;we consider the complementary issue of expressing query transformations that are toospecific for rewrite rules. Such transformations require rewrite rules to be supplemented withsemantic conditions to guard rule firing. This work considers the expression of suchtransformations using conditional rewrite rules; and the expression of inference rules toguide the optimizer in deciding if semantic conditions hold. This work differs from existingwork in semantic query optimization in that semantic transformations in our framework areverifiable with a theorem prover. Further; our use of inference rules to guide semantic …,VLDB,1998,11
Optimization of object-oriented queries: Problems and approaches,Gail Mitchell; Stanley B Zdonik; Umeshwar Dayal,Abstract An object-oriented data model can support features such as abstract data types;methods; encapsulation; subtyping (or inheritance); complex structures; and object identity.The processing of queries in such a model must incorporate support for these features.Query optimization will require new techniques for supporting the object-oriented features.Although some of the problems that must be solved by an object-oriented query optimizerare similar to problems solved by relational and extensible optimizers; there are also manyproblems that are unique to the object-oriented data model. In this paper we explore some ofthe problems that are encountered when trying to optimize queries in an object-orienteddatabase management system. We present each problem in the context of the object-oriented modelling constructs generating the problem. We also survey current …,*,1994,11
An automatic physical design tool for clustered column-stores,Alexander Rasin; Stan Zdonik,Abstract Good database design is typically a very difficult and costly process. As databasesystems get more complex and as the amount of data under management grows; the stakesincrease accordingly. Past research produced a number of design tools capable ofautomatically selecting secondary indexes and materialized views for a known workload.However; a significant bulk of research on automated database design has been done in thecontext of row-store DBMSes. While this work has produced effective design tools; newspecialized database architectures demand a rethinking of automated design algorithms. Inthis paper; we present results for an automatic design tool that is aimed at column-orientedDBMSes on OLAP workloads. In particular; we have chosen a commercial column storeDBMS that supports data sorting. In this setting; the key problem is selecting proper sort …,Proceedings of the 16th International Conference on Extending Database Technology,2013,10
The impact of transaction management on object-oriented multidatabase views,Marian H Nodine; Stanley B Zdonik,*,Object-oriented multidatabase systems,1995,10
ObServer: An object server for an object-oriented database system,Andrea H Skarra; Stanley B Zdonik; Steven P Reiss,Abstract ObServer is a server process that manages persistent objects in an object-orienteddatabase system. The system is implemented under UNIX 1 4.3 BSD on a network ofworkstations with the server and its data residing on a single node. Clients communicateasynchronously with ObServer from possibly remote and dissimilar machines by messagessent according to interprocess communication (IPC) protocols. ObServer managessecondary storage; transactions; and concurrent access to objects in a multiuserenvironment in which transactions are interactive and probably long. The implementationemphasizes efficiency in the transfer of objects through the potential bottlenecks of IPC andfile access. ObServer needs minimal semantic knowledge of the objects it manages; andthus can support application programs built on arbitrary type systems. The paper …,*,1991,10
CherniackM,Camey D AbadiD; U Cetintemel,*,Convey C; Lee S; Stonebraker M; Tatbul N; Zdonik S. Aurora Anewmodeland architecturefordatastreammanagement. TheVLDBJoumal,2003,9
Algorithms and data structures,Fr Ank Dehne; Roberto Solis-Oba,Algorithms and data structures in bioinformatics -- Algorithms and data structures incombinatorics-- Algorithms and data structures in computational geometry -- Algorithms and data structuresin databases -- Algorithms and data structures in graphics -- Parallel and distributedcomputing … XIV; 538 p. 132 illus.;[electronic resource] :;online resource … Items in HannanDLare protected by copyright; with all rights reserved; unless otherwise indicated.,*,1989,9
Physio-Med Web: Real time monitoring of Physiological Strain Index (PSI) of soldiers during an urban training operation,Reed W Hoyt; Mark Buller; Stan Zdonik; Chris Kearns; Beau Freund,Abstract: This field study of simulated urban combat explored the array of issues associatedwith the collection and use of real-time physiologic data. Six male soldiers (age= 22+/-4 yMEAN+/-SD; ht= 172+/-5 cm wt= 69.3+/-1.6 kg;% body fat= 13.9+/-6.9 load carried= 19.0+/-2.9 kg) were monitored in real-time during a simulated attack on the McKenna MilitaryOperations in Urban Terrain (MOUT) facility at the Dismounted Battlespace Battle Lab; Ft.Benning; Georgia. Physiological strain index (PSI); derived from heart rate (HR) and coretemperature (Tcore); was used to monitor thermal/work strain (Moran et al.; Am. J. Physiol.275: R129-R134; 1998). Meteorologic conditions: air temp= 21 to 24 deg C; relativehumidity= 55-65%; solar radiation= 150-430 W/sq m (estimated WBGT= 19 to 23 deg C).Tcore was measured by ingested radio telemetry pill; and HR was measured …,*,2002,8
Automating compensation in a multidatabase,Marian H Nodine; Stanley B Zdonik,Abstract Compensation is the process by which a committed transaction in a database isundone by running the semantic inverse of that transaction on the database. Compensationhas been proposed as a technique for undoing committed work in various situations wherestrict atomicity cannot be maintained [GS87; MR91]. In this paper; we discuss compensationin longrunning multidatabase transactions. We define the step approach to integrating localdatabase schemas into a mulitdatabase. In the step approach; each local database isencapsulated by a set of procedures (steps). Steps can be grouped into atomic globaltransactions. Each step also has an associated compensating step; which is called if thecompensating transaction is run. We examine two areas of multidatabase transactionmanagement where compensation is required. The first is implementing compensation as …,HICSS (2),1994,8
Data Ingestion for the Connected World.,John Meehan; Cansu Aslantas; Stan Zdonik; Nesime Tatbul; Jiang Du,ABSTRACT In this paper; we argue that in many “Big Data” applications; getting data into thesystem correctly and at scale via traditional ETL (Extract; Transform; and Load) processes isa fundamental roadblock to being able to perform timely analytics or make real-timedecisions. The best way to address this problem is to build a new architecture for ETL whichtakes advantage of the push-based nature of a stream processing system. We discuss therequirements for a streaming ETL engine and describe a generic architecture which satisfiesthose requirements. We also describe our implementation of streaming ETL using a scalablemessaging system (Apache Kafka); a transactional stream processing system (S-Store); anda distributed polystore (Intel's BigDAWG); as well as propose a new time-series databaseoptimized to handle ingestion internally.,CIDR,2017,7
Integrating real-time and batch processing in a polystore,John Meehan; Stan Zdonik; Shaobo Tian; Yulong Tian; Nesime Tatbul; Adam Dziedzic; Aaron Elmore,This paper describes a stream processing engine called S-Store and its role in theBigDAWG polystore. Fundamentally; S-Store acts as a frontend processor that accepts inputfrom multiple sources; and massages it into a form that has eliminated errors (data cleaning)and translates that input into a form that can be efficiently ingested into BigDAWG. S-Storealso acts as an intelligent router that sends input tuples to the appropriate components ofBigDAWG. All updates to S-Store's shared memory are done in a transactionally consistent(ACID) way; thereby eliminating new errors caused by non-synchronized reads and writes.The ability to migrate data from component to component of BigDAWG is crucial. We havedescribed a migrator from S-Store to Postgres that we have implemented as a first proof ofconcept. We report some interesting results using this migrator that impact the evaluation …,High Performance Extreme Computing Conference (HPEC); 2016 IEEE,2016,7
Larger-than-memory data management on modern storage hardware for in-memory OLTP database systems,Lin Ma; Joy Arulraj; Sam Zhao; Andrew Pavlo; Subramanya R Dulloor; Michael J Giardino; Jeff Parkhurst; Jason L Gardner; Kshitij Doshi; Stanley Zdonik,Abstract In-memory database management systems (DBMSs) outperform disk-orientedsystems for on-line transaction processing (OLTP) workloads. But this improvedperformance is only achievable when the database is smaller than the amount of physicalmemory available in the system. To overcome this limitation; some in-memory DBMSs canmove cold data out of volatile DRAM to secondary storage. Such data appears as if it residesin memory with the rest of the database even though it does not. Although there have beenseveral implementations proposed for this type of cold data storage; there has not been athorough evaluation of the design decisions in implementing this technique; such as policiesfor when to evict tuples and how to bring them back when they are needed. These choicesare further complicated by the varying performance characteristics of different storage …,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,7
Database Programming Languages (DBPL-5),Mitch Cherniack; Stanley B Zdonik; Marian H Nodine,Abstract The AQUA [16] query algebra allows user-defined equivalence relations asarguments to query operators that generalize standard set operations. These predicatesdetermine what objects are included in the query result; and the duplicates that must beremoved. While an expressive enhancement; the use of arbitrary equivalence relations todecide set membership can result in sets with counterintuitive behavior; and therefore canmake queries return unreasonable results. In this paper; we show that equality predicatesassume two roles with respect to sets. Distinguishers differentiate between set members andimplicitly give meaning to standard set properties such as set equality. Constructorsdetermine which object from input sets contribute to the query result. The requirements ofdistinguishers and constructors differ. AQUA's set operators are problematic because they …,*,1995,7
Predictive Caching,Mark L Palmer; Stanley Benjamin Zdonik,Abstract Prefetching data objects or program pages in advance of their иве ie a powerfulmeans of improving performance which has proved difficult to realize. Current OODBsystems that maintain object caches use policies for fetching and replacing referencedobjects approximating those used for virtual memory demand paging. These policies usuallyassume no knowledge of the future. Object cache managers prefetch objects by usingexplicit representations of data structure; or employ demand fetching combined with dataclustering to effect prefetching. The latter two methods can be awkward to implement andineffective in systems where encapsulation is present; or where the usage patterns servicedare incompatible. Recent work at Brown [MP89] has introduced the notion of a black box; theEstimating Prophet; that assimilates access patterns of individual users over time and …,*,1990,7
Models of inheritance,Peter Wegner; Stanley B Zdonik,*,Proceedings of the second international workshop on Database programming languages,1989,7
An object management system for office applications,Stanley B Zdonik,Abstract The field of data processing has traditionally been concerned with data intensiveapplications. That is; the complexity of the data in these applications dominates thecomplexity of the processes. A program that prints a report listing the outstanding accountsreceivable is relatively simple compared to the structure of the general accounting data ofthe firm.,*,1985,7
Window-aware Load Shedding for Data Streams,Nesime Tatbul; Stan Zdonik,Abstract Data stream management systems may be subject to higher input rates than theirresources can handle. In this case; results get delayed and Quality of Service (QoS) atsystem outputs may fall below acceptable levels. Load shedding addresses this problem byallowing data loss in exchange for reduced latency. Drop operators are placed at carefullychosen points in a query plan; in order to relieve overload with minimal loss in answerquality. In this paper; we describe a load shedding technique for queries consisting of one ormore aggregate operators with sliding windows. We introduce a sophisticated drop operator;called a “Windowed Drop”. This operator is aware of window properties (ie; window size andwindow slide) of downstream aggregate operators in the query plan. Accordingly; it logicallypartitions the stream into windows and probabilistically decides which windows to drop …,*,2004,6
Object-oriented data model,Stanley B Zdonik,*,Advances in database programming languages,1990,6
ObServer; a Storage System for Object-oriented Applications,Mary F Fernandez; Stanley Benjamin Zdonik; Alan N Ewald,Abstract ObServer is a storage system designed to support object-oriented applications suchae computer aided design systems; interactive programming environments and object-oriented databases. For these systems to be used effectively; the underlying storage servermust support interactive transactions; provide shared access to objects and performefficiently in a distributed workstation environment. In this paper; we present the design andimplementation of ObServer; consider its limitations and discuss areas of current and futureresearch. We also provide performance statistics obtained from three types of exampledatabases.,*,1990,6
OPAQUE: protecting path privacy in directions search,Ken CK Lee; Wang-Chien Lee; Hong Va Leong; Baihua Zheng,Directions search returns the shortest path from a source to a destination on a road network.However; the search interests of users may be exposed to the service providers; thus raisingprivacy concerns. For instance; a path query that finds a path from a resident address to aclinic may lead to a deduction about" who is related to what disease". To protect user privacyfrom accessing directions search services; we introduce the OPAQUE system; whichconsists of two major components:(1) an obfuscator that formulates obfuscated path queriesby mixing true and fake sources/destinations; and (2) an obfuscated path query processorinstalled in the server for obfuscated path query processing. OPAQUE reduces the likelihoodof path queries being revealed and allows retrieval of requested paths. We propose twotypes of obfuscated path queries; namely; independently obfuscated path query and …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,5
Design issues for second generation stream processing engines,Daniel J Abadi; Yanif Ahmad; Magdalena Balazinska; Ugur Cetintemel; Mitch Cherniack; Jeong-Hyon Hwang; Wolfgang Lindner; A Maskey; N Tatbul; Y Xing; S Zdonik,Abstract Borealis is a second-generation distributed stream processing engine that is beingdeveloped at Brandeis University; Brown University; and MIT. Borealis inherits core streamprocessing functionality from Aurora [13] and distribution functionality from Medusa [49].Borealis modifies and extends both systems in non-trivial and critical ways to provideadvanced capabilities that are commonly required by newly-emerging stream processingapplications. In this paper; we outline the basic design and functionality of Borealis. Throughsample real-world applications; we motivate the need for dynamically revising query resultsand modifying query specifications. We then describe how Borealis addresses thesechallenges through an innovative set of features; including revision records; time travel; andcontrol lines. Finally; we present a highly flexible and scalable QoS-based optimization …,Proc. of the Conference for Innovative Database Research (CIDR); Asilomar; CA,2005,5
Directions in object-oriented databases,Stanley Zdonik,Directions in Object-Oriented Databases Stanley Zdonik Dept of Computer Science Brown University… 1. Different Views of Objects There is strong historical precedent for the object-oriented viewin both the programming language and the databw research communities. The programminglanguage world has contributed abstraction-based languages and polymorphic typesystems; and the database world has pro- duced semantic data models and dataindependence. The distinctions between these two areas are becoming less clear; and the currentwork on object-oriented databases is an example of research that is conciously directed at eliminatingthis division. However; remnants of these distinctions still remain. For example; in many programminglanguages; automatic checking of correctness is done at compile-time when identifiers are; ingeneral; bound only to types. This leads to languages that are statically checkable. The …,Computer Software and Applications Conference; 1989. COMPSAC 89.; Proceedings of the 13th Annual International,1989,5
U. etintemel; M,D Abadi; D Carney,*,Cherniack; C. Convey; S. Lee; M. Stonebraker; N. Tatbul; and S. Zdonik,*,5
Handling Shared; Mutable State in Stream Processing with Correctness Guarantees.,Nesime Tatbul; Stan Zdonik; John Meehan; Cansu Aslantas; Michael Stonebraker; Kristin Tufte; Chris Giossi; Hong Quach,Abstract S-Store is a next-generation stream processing system that is being developed atBrown; Intel; MIT; and Portland State University. It is designed to achieve very highthroughput; while maintaining a number of correctness guarantees required to handleshared; mutable state in streaming applications. This paper explores these correctnesscriteria and describes how S-Store achieves them; including a new model of streamprocessing that provides support for ACID transactions.,IEEE Data Eng. Bull.,2015,4
Monte carlo query processing of uncertain multidimensional array data,Tingjian Ge; David Grabiner; Stan Zdonik,Array database systems are architected for scientific and engineering applications. In theseapplications; the value of a cell is often imprecise and uncertain. There are at least tworeasons that a Monte Carlo query processing algorithm is usually required for suchuncertain data. Firstly; a probabilistic graphical model must often be used to modelcorrelation; which requires a Monte Carlo inference algorithm for the operations in ourdatabase. Secondly; mathematical operators required by science and engineering domainsare much more complex than those of SQL. State-of-the-art query processing uses MonteCarlo approximation. We give an example of using Markov Random Fields combined withan array's chunking or tiling mechanism to model correlated data. We then proposesolutions for two of the most challenging problems in this framework; namely the …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,4
A*-tree: A structure for storage and modeling of uncertain multidimensional arrays,Tingjian Ge; Stan Zdonik,Abstract Multidimensional array database systems are suited for scientific and engineeringapplications. Data in these applications is often uncertain and imprecise due to errors in theinstruments and observations; etc. There are often correlations exhibited in the distribution ofvalues among the cells of an array. Typically; the correlation is stronger for cells that areclose to each other and weaker for cells that are far away. We devise a novel data structure;called the A*-tree (multidimensional Array tree); demonstrating that by taking advantage ofthe predictable and structured correlations of multidimensional data; we can have a moreefficient way of modeling and answering queries on large-scale array data. An A*-tree is aunified model for storage and inference. The graphical model that is assumed in an A*-treeis essentially a Bayesian Network. We analyze and experimentally verify the accuracy of …,Proceedings of the VLDB Endowment,2010,4
Light-weight; runtime verification of query sources,Tingjian Ge; Stan Zdonik,Modern database systems increasingly make use of networked storage. This storage can bein the form of SAN's or in the form of shared-nothing nodes in a cluster. One type of attack ondatabases is arbitrary modification of data in a database through the file system; bypassingdatabase access control. Additionally; for many applications; ensuring strict and definiteauthenticity of query source and results is required or highly desirable. In this paper; wepropose a lightweight approach for verifying the minimum information that a database serverneeds from the storage system to execute a query. The verification is definite and produceshigh confidence results because of its online manner (ie; the information is verified rightbefore it is used). It is lightweight in three ways:(1) We use the Merkle hash tree datastructure and fast cryptographic hash functions to ensure the verification itself is fast and …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,4
Requirements for science data bases and scidb,Kian-Tat Lim; SLAC David Maier; Oliver Ratzesberger; Stan Zdonik,Abstract: For the past year; we have been assembling requirements from a collection ofscientific data base users from astronomy; particle physics; fusion; remote sensing;oceanography; and biology. The intent has been to specify a common set of requirementsfor a new science data base system; which we call SciDB. In addition; we have discoveredthat very complex business analytics share most of the same requirements as “big science”.We have also constructed a partnership of companies to fund the development of SciDB;including eBay; the Large Synoptic Survey Telescope (LSST); Microsoft; the Stanford LinearAccelerator Center (SLAC) and Vertica. Lastly; we have identified two “lighthousecustomers”(LSST and eBay) who will run the initial system; once it is constructed.,Conference on Innovative Data Systems Research,2009,4
Streaming for dummies,Stan Zdonik; Peter Sibley; Alexander Rasin; Victoria Sweetser; Philip Montgomery; Jenine Turner; John Wicks; Alexander Zgolinski; Derek Snyder; Mark Humphrey; Charles Williamson,Despite over thirty years of continuing development and a rising prominence in the publiceye; today's databases continue to be outpaced by the human appetite for new ways toprocess information. Although databases continue to serve as a reliable platform fortransaction-based operations; traditional relational database management systems havecome up short against a new class of data management problems. This paper provides anoverview of these problems; examines why traditional relational systems are inadequate todeal with them; and identifies a new class of data processing currently known as DataStream Management Systems (DSMS) designed to handle them. We will provide anoverview of the current status of the field; as well as focus on some prototypical systems.,*,2004,4
Load Shedding in a Data Stream Manager,Mitch Cherniack; Michael Stonebraker; Nesime Tatbul; Uğur Çetintemel; Stan Zdonik,Abstract A Data Stream Manager accepts push-based inputs from a set of data sources;processes these inputs with respect to a set of standing queries; and produces outputsbased on Quality-of-Service (QoS) specifications. When input rates exceed system capacity;the system will become overloaded and latency will deteriorate. Under these conditions; thesys-tem will shed load; thus degrading the answer; in order to improve the observed latencyof the results. This paper examines a technique for dynamically inserting and removing dropoperators into query plans as required by the current load. We examine two types of drops:the first drops a fraction of the tuples in a randomized fashion; and the second drops tuplesbased on the importance of their content. We address the problems of determining whenload shedding is needed; where in the query plan to insert drops; and how much of the …,Proceedings 2003 VLDB Conference: 29th International Conference on Very Large Databases (VLDB),2003,4
Reducing execution overhead in a data stream manager,Don Carney; Uğur Çetintemel; Alex Rasin; Stan Zdonik; Mitch Cherniack; Mike Stonebraker,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda):Applications that deal with potentially unbounded; continuous streams of dataare becoming increasingly popular due to a confluence of advances in real-time;wide-area data dissemination technologies and the.,In ACM Workshop on Management and Processing of Data Streams,2003,4
Extensibility and Asynchrony in the Brown-Object Storage System.,David E Langworthy; Stanley B Zdonik,Abstract This paper discusses the general design of an object sotrage system called BOSS.The BOSS system combines optimistic concurrency control and asynchronous cachecoherency algorithms with the ability to extend the underlying storage types. This systemallows a client to interact with more than one server through a two-phase commit protocol. Ituses a unique log management technique to provide e cient stable storage. The paperdescribes the main components of our system. They include a transaction manager; a namemanager; and a set of storage class modules. We discuss the interface to each of thesecomponents along with the algorithms and protocols that bind them together in a runningsystem. The paper concludes with a set of tests that were performed on the system. Thesetests show that our use of optimistic concurrency control in a distributed environment …,*,1996,4
Storage Class Extensibility in the Brown Object Storage System,David E Langworthy; Stanley B Zdonik,Abstract The Brown Object Storage System (BOSS) provides extensible support forpersistence in a distributed multi-client; multi-server environment. BOSS is built on a looselycoupled; asynchronous computation model. One of the artifacts of asynchrony is increasedextensiblity. This paper presents the storage class; the means by which the BOSS clientinterface is extended. The BOSS prototype is completed and operated in a network ofSparclOs running SunOS 4.1. Currently the untuned prototype operates at 6 shorttransactions (20 reads; 4 writes) per second.,*,1995,4
On the use of domain-specific knowledge in the processing of data base queries,Stanley B Zdonik,*,*,1980,4
MISSION: A low-cost data management system for the biomedical community,David Fram; Paul Castleman; Frederick Webb; Walter Bilofsky; Stanley Zdonik,Abstract A new minicomputer system; called MISSION; for the visualization and analysis ofbiomedical data is described. Patterned after the much larger; NIH-supported PROPHETsystem; MISSION provides an integrated set of capabilities in the areas of tabular datamanagement; graphing; and statistical computation. The user communicates with the systemvia simple English-like commands. MISSION currently exists as a demonstration prototype;and is undergoing continued further development.,Proceedings of the Annual Symposium on Computer Application in Medical Care,1977,4
Optimizing index deployment order for evolving OLAP,Hideaki Kimura; Carleton Coffrin; Alexander Rasin; Stanley B Zdonik,Abstract Many database applications deploy hundreds or thousands of indexes to speed upquery execution. Despite a plethora of prior work on index selection; designing anddeploying indexes remains a difficult task for database administrators. First; real-worldbusinesses often require online index deployment; and the traditional off-line approach toindex selection ignores intermediate workload performance during index deployment.Second; recent work on on-line index selection does not address effects of complexinteractions that manifest during index deployment. In this paper; we propose a newapproach that incorporates transitional design performance into the overall problem ofphysical database design. We call our approach Incremental Database Design. As the firststep in this direction; we study the problem of ordering index deployment. The benefits of …,Proceedings of the 15th International Conference on Extending Database Technology,2012,3
Stream-Oriented Query Languages and Operators,Mitch Cherniack; Stan Zdonik,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,3
Workshop report,Vijay Kumar; Stan Zdonik,Executive Summary There is a general agreement among researchers and developers thatsupport for mobility has become one of the key requirements of today's informationmanagement infrastructure. Mobility improves productivity by using working time moreefficiently. First-generation mobile architectures treat mobile devices like static client-serversystems in which clients issue standard transactions [1] to servers. More responsive futureapplications will require different levels of interaction and service depending on the user'scontext. Context captures those aspects of the user environment that affect the observablebehavior of the system. For example; if a shopper's assistant running on a PDA; which is fullyconnected via wireless channels; alerts a user of nearby bargains on interesting items; thenthe context would include the user's position and perhaps a list of things that the user has …,Proc. NSF Workshop Context-Aware Mobile and Sensor Information Management,2002,3
Optimization of object-oriented query languages: Problems and approaches,G Mitchell; SB Zdonik; U Dayal,*,NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES,1994,3
ISIS: interface for a semantic information system,KJ Goldman; SA Goldman; PC Kanellakis; SB Zdonik,*,Readings in object-oriented database systems,1989,3
Monitoring Streams,Don Carney; Cherniack M CU; C Convey; S Lee; G Seidman; M Stonebraker; N Tatbul; S Zdonik,*,Proc. 28th VLDB,2002,2
The Brown Object Storage System,Stanley B Zdonik; DE Langworthy,*,DARPA Software Technology Conference,1992,2
A Model for Controlling Cooperative Transactions,Mary F Fernandez; Stanley B Zdonik,ABSTRACT Many interactive applications that are being developed for high-performanceworkstations involve several users in a collaborative activity. Modern database systemssynchronize activities on shared data by ensuring that the results are serializable.Serializability is often too limiting a correctness criterion for cooperative work. Instead; weneed a way of specifying correctness in an application-dependent way; such that thedatabase system can monitor the evolving schedules and determine when an activity wouldviolate an assumption being made by another concurrent activity. The storage managermust be capable of supporting interactions with many cooperative activities each of which iscomposed of multiple agents. This paper discusses how a distributed object server cansupport heterogenous sharing protocols without interference. We define active entities …,Persistent Object Systems: Proceedings of the Third International Workshop 10–13 January 1989; Newcastle; Australia,2012,1
Tastes Great; Less Filling: Low-Impact OLAP MapReduce Queries on High-Performance OLTP Systems,Xin Jia; Andrew Pavlo; Stan Zdonik,ABSTRACT The previous decade saw the rise of separate; dedicated databasemanagement systems (DBMS) for online transaction processing (OLTP) and onlineanalytical processing (OLAP) workloads [3]. The former are focused on executing short-lived; small-footprint transactions with high throughput and strong consistency guarantees.OLAP DBMSs typically target longer running and more complex queries that examine thedatabase after it is offloaded from the front-end OLTP DBMS. For many; the latencyoverhead of transferring data between these two systems; as well as their administrativecosts; is too onerous. A burgeoning alternative is to use a hybrid approach where an OLTPsystem is able execute OLAP-style queries alongside the transactional workload [1]. Thisprovides users the ability to execute business intelligence and other analytical queries in …,Tiny Transactions on Computer Science,2012,1
ASAP: A System for Efficient Storage and Processing of Arrays,Ugur Cetintemel; Esther Ryvkina; Mitch Cherniack; Michael Stonebraker; Alex Rasin; Stan Zdonik,*,*,2007,1
No false positives: Window-aware load shedding for data streams,Nesime Tatbul; Stan Zdonik,Abstract Data stream management systems may be subject to higher input rates than theirresources can handle. In this case; results get delayed and Quality of Service (QoS) atsystem outputs may fall below acceptable levels. Load shedding addresses this problem byallowing data loss in exchange for reduced latency. In this paper; we describe a loadshedding technique for queries consisting of one or more aggregate operators with slidingwindows. We introduce a sophisticated drop operator; called a “Window Drop”. This operatoris aware of window properties (ie; window size and window slide) of downstream aggregateoperators in the query plan. Accordingly; it logically divides the stream into windows andprobabilistically decides which windows to drop. This decision is further encoded into tuplesby marking the ones that are disallowed from starting new windows. Unlike earlier …,*,2005,1
Monitoring and Mining Data Streams,Stan B Zdonik,Abstract: The work of the first year of this effort was to support the implementation of theprototype for the Aurora stream processing engine and to investigate applications for thistechnology. An initial version of Aurora was completed; and we worked with WilliamVanderschallie on an Army application that involved water supply threat-level detection. Therest of this report discusses these two activities. Descriptors:* DATA PROCESSING;* MININGENGINEERING;* STREAMS; DETECTION; THREATS; ARMY PERSONNEL; LEVEL(QUANTITY); ENGINES; AURORAE; WATER SUPPLIES.,*,2003,1
Evolving database systems: a persistent view,GNC Kirby; R Morrison; RCH Connor; SB Zdonik,Orthogonal persistence ensures that information will exist for as long as it is useful; for whichit must have the ability to evolve with the growing needs of the application systems that useit. This may involve evolution of the data; meta-data; programs and applications; as well asthe users' perception of what the information models. The need for evolution has been wellrecognised in the traditional (data processing) database community and the cost of failing toevolve can be gauged by the resources being invested in interfacing with legacy systems.Zdonik has identified new classes of application; such as scientific; financial andhypermedia; that require new approaches to evolution. These applications are characterisedby their need to store large amounts of data whose structure must evolve as it is discoveredby the applications that use it. This requires that the data be mapped dynamically to an …,*,1997,1
Prefetching from a Broadcast Disk,Swarup Acharya Michael Franklin Stanley Zdonik,*,Proceedings of the 12th International Conference on Data Engineering,1996,1
An E cient Scheme for Dynamic Data Replication,Swarup Acharya; Stanley B Zdonik; E An,Abstract This paper presents an e cient scheme for dynamic replication of data in distributedenvironments. The aim of the scheme is to increase system performance by intelligent dataplacement so as to optimize the message tra c in the network. Research in the recent pasthas comparatively focussed very little on using replication for increasing performance buthas instead been directed more at improving system availability through replication.However; with the advent of mobile or nomadic computing; research in replication needs tochange direction {the underlying assumption of high speed networks no longer hold true.Wireless networks not only have lower bandwidth but are also very expensive to use. Insuch an environment; it is imperative that data be distributed intelligently to achieve a goodsystem performance in terms of message costs and turnaround time. Besides; with …,*,1993,1
Supporting Reactive Planning Tesks on an Evolving Multidatabase,Marian H Nodine; Stanley B Zdonik,Abstract In this paper we discuss our open nested multidatabase transaction model formultidatabases called Interactions. Interactions sit above a layer that provides atomic;serializable global multidatabase transactions. This model provides additional support forplanning applications; which are long-running applications that may need to respond tochanges in the underlying evolving multidatabase. We begin by describing Interactions andthe multidatabase architecture that they run on. We then discuss synchronization ofInteractions. We describe the nature of events; which the applications can use to monitorchanges in the information in the multidatabase. When relevant events occur; we de ne howthey are detected and how to respond to them. This response usually involves semanticallyundoing part of at least one Interaction; and then potentially re-executing the Interaction …,*,1992,1
Transaction Control For Cooperative Applications,Sridhar Ramaswamy; Stanley Benjamin Zdonik,Abstract We analyze existing models of design transaction control systems. We use thefollowing criteria: ﬂexibility in specifying concurrency control; computational complexity ofchecking consistency; computational complexity of concurrency control; support forabstraction; and support for recovery. Based on this analysis; we propose a new way ofdoing concurrency control and recovery. We then discuss the advantages and limitations ofthis approach and outline open problems in this area.,*,1991,1
Relational extensions and extensible database systems,SB Zdonik; D Maier,*,Readings in object-oriented database systems,1989,1
Precision and Recall for Range-Based Anomaly Detection,Tae Jun Lee; Justin Gottschlich; Nesime Tatbul; Eric Metcalf; Stan Zdonik,Abstract: Classical anomaly detection is principally concerned with point-based anomalies;anomalies that occur at a single data point. In this paper; we present a new mathematicalmodel to express range-based anomalies; anomalies that occur over a range (or period) oftime. Subjects: Artificial Intelligence (cs. AI) Cite as: arXiv: 1801.03175 [cs. AI](or arXiv:1801.03175 v1 [cs. AI] for this version) Submission history From: Justin Gottschlich [viewemail][v1] Tue; 9 Jan 2018 23: 01: 07 GMT (52kb),arXiv preprint arXiv:1801.03175,2018,*
Greenhouse: A Zero-Positive Machine Learning System for Time-Series Anomaly Detection,Tae Jun Lee; Justin Gottschlich; Nesime Tatbul; Eric Metcalf; Stan Zdonik,Abstract: This short paper describes our ongoing research on Greenhouse-a zero-positivemachine learning system for time-series anomaly detection. Subjects: Artificial Intelligence(cs. AI) Cite as: arXiv: 1801.03168 [cs. AI](or arXiv: 1801.03168 v1 [cs. AI] for this version)Submission history From: Justin Gottschlich [view email][v1] Tue; 9 Jan 2018 22: 44: 21 GMT(255kb; D),arXiv preprint arXiv:1801.03168,2018,*
Towards Dynamic Data Placement for Polystore Ingestion,Jiang Du; John Meehan; Nesime Tatbul; Stan Zdonik,Abstract Integrating low-latency data streaming into data warehouse architectures hasbecome an important enhancement to support modern data warehousing applications. Inthese architectures; heterogeneous workloads with data ingestion and analytical queriesmust be executed with strict performance guarantees. Furthermore; the data warehouse mayconsists of multiple different types of storage engines (aka; polystores or multi-stores). Aparamount problem is data placement; different workload scenarios call for different dataplacement designs. Moreover; workload conditions change frequently. In this paper; weprovide evidence that a dynamic; workload-driven approach is needed for data placement inpolystores with low-latency data ingestion support. We study the problem based on thecharacteristics of the TPC-DI benchmark in the context of an abbreviated polystore that …,Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics,2017,*
Interactive Search and Exploration of Waveform Data with Searchlight,Alexander Kalinin; Ugur Cetintemel; Stan Zdonik,Abstract Searchlight enables search and exploration of large; multi-dimensional data setsinteractively. It allows users to explore by specifying rich constraints for the" objects" they areinterested in identifying. Constraints can express a variety of properties; including a shape ofthe object (eg; a waveform interval of length 10-100ms); its aggregate properties (eg; theaverage amplitude of the signal over the interval is greater than 10); and similarity to anotherobject (eg; the distance between the interval's waveform and the query waveform is lessthan 5). Searchlight allows users to specify an arbitrary number of such constraints; withmixing different types of constraints in the same query. Searchlight enhances the queryexecution engine of an array DBMS (currently SciDB) with the ability to performsophisticated search using the power of Constraint Programming (CP). This allows an …,Proceedings of the 2016 International Conference on Management of Data,2016,*
Data curation system with version control for workflow states and provenance,*,A data curation system that includes various methods to enable efficient reuse of human andmachine effort. To reuse effort; various facilities are presented that model; save; and allowthe querying of provenance and state information of a curation workflow and allow forincremental; stateful transitions of the data and the metadata.,*,2016,*
Transaction Processing over High-speed Networks,Erfan Zamanian; Tim Kraska; Maurice Herlihy; Stan Zdonik,Abstract By avoiding high cost of disk I/O; memory-resident OLTP databases reduce theruntime of typical singlesited transactions to a fraction of a millisecond. With disk gone fromthe picture; network has become the next bottleneck. In fact; the traditional wisdom is thatnetwork is the new disk; and distributed transactions must be avoided as much as possible;through techniques such as partitioning. However; recent technological trends toward low-latency; high-bandwidth networks which are capable of minimizing communicationoverhead through Remote Direct Memory Access (RDMA) are changing our view on thefundamental assumption that network is significantly the bottleneck. For example; the latencyin InfiniBand FDR/EDR is less than 1µs; two orders of magnitude faster than the traditional1Gb Ethernet; and is only one order of magnitude slower than local memory access (0.1 …,*,2015,*
a Model for Engineering Information Systems and Other Baroque Applications,Stanley B Zdonik,Abstract This paper describes the FUGUE1 data model; an extension of DAPLEX that wasdeveloped to support engineering information systems. It begins by discussing theapplication requirements that must be met by such a model. They include the accuraterepresentation of engineering data and processes; mechanisms to integrate heterogeneoushardware and software components and to tailor each instance of an EIS to localrequirements and policies; and a type system that is powerful enough to support productionsystems; yet flexible enought to support design environments. The primitive notions of objectand function are described. Then; a type system that combines the discipline of conventionaltypes and instances with the flexibility of prototypes is presented. Finally; we show variousways of creating new objects and deriving new functions.,Proceedings of the Third International Conference on Data and Knowledge Bases: Improving Usability and Responsiveness,2014,*
AN OBJECT,STANLEY B ZDONIK,The field of data processing has traditionally been concerned with data intensiveapplications. That is; the complexity of the data in these applications dominates thecomplexity of the processes. A program that prints a report listing the outstanding accountsreceivable is relatively simple compared to the structure of the general accounting data ofthe firm. Database management systems have been successful in providing an environmentfor controlling the complexity of data processing applications. Programming time is reducedin an environment in which there exists a uniform view of data expressed in a language thatpresents a high-level semantic interface.,Languages for Automation,2013,*
Storage Class Extensibility,David E Langworthy; Stanley B Zdonik,Abstract The Brown Object Storage System (BOSS) provides extensible sup-port forpersistence in a distributed multi-client; multi-server environment. BOSS is built on a looselycoupled; asynchronous computation model. One of the artifacts of asynchrony is increasedextensiblity. This paper presents the storage class; the means by which the BOSS clientinterface is extended.,Persistent Object Systems: Proceedings of the Sixth International Workshop on Persistent Object Systems; Tarascon; Provence; France; 5–9 September 1994,2012,*
UNCORRECTED,Stan Zdonik; Peng Ning; Shashi Shekhar; Jonathan Katz; Xindong Wu; Lakhmi C Jain; David Padua; Xuemin Shen; Borko Furht; VS Subrahmanian; Martial Hebert; Katsushi Ikeuchi; Bruno Siciliano,*,*,2012,*
Optimizing Index Deployment Order for Evolving OLAP (Extended Version),Hideaki Kimura; Carleton Coffrin; Alexander Rasin; Stanley B Zdonik,Abstract: Query workloads and database schemas in OLAP applications are becomingincreasingly complex. Moreover; the queries and the schemas have to continually\textit{evolve} to address business requirements. During such repetitive transitions; the\textit{order} of index deployment has to be considered while designing the physical schemassuch as indexes and MVs. An effective index deployment ordering can produce (1) a promptquery runtime improvement and (2) a reduced total deployment time. Both of these areessential qualities of design tools for quickly evolving databases; but optimizing the problemis challenging because of complex index interactions and a factorial number of possiblesolutions.,arXiv preprint arXiv:1107.3606,2011,*
Revolutionize database performance,Stan Zdonik,Abstract This article presents an overview of the evolution of relational databasemanagement systems (RDBMSs) and how the gap between data warehouse requirementsand data warehouse database capabilities has come about. I will also explain sevendatabase innovations that will revolutionize data warehousing.,Information Management,2008,*
Swarup Acharya”; Rafael Alonso,Michael Franklin; Stanley Zdonik,*,Mobile Computing,2007,*
Stephen Tu,Wenting Zheng; Stephen Tu; Eddie Kohler; Barbara Liskov; Justin DeBrabant; Andrew Pavlo; Michael Stonebraker; Stan Zdonik; Samuel Madden; M Frans Kaashoek; Nickolai Zeldovich; Haiping Zhao; Iain Proctor; Minghui Yang; Xin Qi; Mark Williams; Guilherme Ottoni; Charlie Gao; Andrew Paroski; Scott MacVicar; Jason Evans; Michael Armbrust; Nick Lanham; Armando Fox; Michael Franklin; David Patterson; Michael J Franklin; David A Patterson; Beth Trushkowsky; Jesse Trutna,Page 1. Stephen Tu http://www.cs.berkeley.edu/~stephentu Education University of California;Berkeley Aug. 2014 - Present Ph.D. student in Electrical Engineering and Computer Science(EECS) Advised by Professor Ben Recht Massachusetts Institute of Technology Sept. 2011 -May 2014 SM in Electrical Engineering and Computer Science (EECS) Advised by ProfessorSamuel Madden Thesis: Fast Transactions for Multicore In-Memory Databases GPA:5.0/5.0 University of California; Berkeley Aug. 2006 - Dec. 2010 BA in Computer Scienceand BS in Mechanical Engineering GPA: 3.97/4.0 Publications Fast Databases with FastDurability and Recovery through Multicore Parallelism. Wenting Zheng; Stephen Tu; EddieKohler; and Barbara Liskov OSDI 2014. Anti-Caching: A New Approach to Swapping inMain Memory OLTP Database Systems …,University of California; Berkeley,2006,*
Stephen Tu,Justin DeBrabant; Andrew Pavlo; Stephen Tu; Michael Stonebraker; Stan Zdonik; Wenting Zheng; Eddie Kohler; Barbara Liskov; Samuel Madden; M Frans Kaashoek; Nickolai Zeldovich; Haiping Zhao; Iain Proctor; Minghui Yang; Xin Qi; Mark Williams; Guilherme Ottoni; Charlie Gao; Andrew Paroski; Scott MacVicar; Jason Evans; Michael Armbrust; Nick Lanham; Armando Fox; Michael Franklin; David Patterson; Michael J Franklin; David A Patterson; Beth Trushkowsky; Jesse Trutna,Ph.D. student in Electrical Engineering and Computer Science (EECS) Advised by ProfessorSamuel Madden Master's thesis: Fast Transactions for Multicore In-Memory Databases GPA:5.0/5.0 … BA in Computer Science and BS in Mechanical Engineering GPA: 3.97/4.0 … PublicationsAnti-Caching: A New Approach to Swapping in Main Memory OLTP Database Systems. JustinDeBrabant; Andrew Pavlo; Stephen Tu; Michael Stonebraker; Stan Zdonik To appear in VLDB2014 … Speedy Transactions in Multicore In-Memory Databases. Stephen Tu; WentingZheng; Eddie Kohler; Barbara Liskov; and Samuel Madden SOSP 2013 … Processing AnalyticalQueries over Encrypted Data. Stephen Tu; M. Frans Kaashoek; Samuel Madden; and NickolaiZeldovich VLDB 2013 … The HipHop Compiler for PHP. Haiping Zhao; Iain Proctor; MinghuiYang; Xin Qi; Mark Williams; Guilherme Ottoni; Charlie Gao; Andrew Paroski; Scott …,University of California; Berkeley,2006,*
Bachelor of Science; MIT; Electrical Engineering (Computer Science); June 1970 Bachelor of Science; MIT; Industrial Management; June 1970 MS; Electrical Engine...,Stanley B Zdonik,1968-1972 QEI; Inc.; Senior Systems Programmer 1972-1976 Bolt Beranek and Newman;Inc.; Senior Systems Analyst 1976-1983 MIT; Laboratory for Computer Science; DatabaseSystems/Office Automation Group 1979-1981 Arthur D. Little; Inc.; Database Systems Group1983-1989 Assistant Professor of Computer Science; Brown University 1990-1991 ResearchAffiliate; MIT Laboratory for Computer Science 1989-1995 Associate Professor of ComputerScience; Brown University 1995- present Professor of Computer Science; Brown University2003- present Chief Architect; StreamBase Systems; Inc … • Office of Naval Research - YoungInvestigator Award (1986) … 1988 - 1994 Board of Technical Advisors; Object Design; Inc.;Burlington; MA … 1994 - 1999 Advisory Board for Institute for Medical Computing; Brown University… 2004 – present Member of the Board of Trustees of the VLDB (Very Large Databases) …,*,2005,*
Message from the general chairs and program chairs,Betty Salzberg; Mike Stonebraker; Meral Ozsoyoglu; Stan Zdonik,Welcome to the 20th International Conference on Data Engineering (ICDE 2004). This year'sconference is being held in Boston Massachusetts; a center for technology andbiotechnology and the home of many world-renowned universities. ICDE 2004 continues thetradition of being a premier forum for the presentation of both theoretical and practicalresearch in database management systems and data-centric applications. We hope you willfind the program both stimulating and informative. The success of any large conference suchas this one depends on the hard work and cooperation of many people. The localarrangements committee; George Kollios; Betty O'Neil; Arnon Rosenthal and DonghuiZhang; selected the hotel; chose the Aquarium for the banquet; and helped with numerousdetails to make the conference run smoothly. The publicity chair; Dina Goldin; worked …,Proceedings-20th International Conference on Data Engineering-ICDE 2004,2004,*
The Lowell Database Research Self Assessment,Stan Zdonik; Jennifer Widom; Gerhard Weikum; Jeff Ullman; Rick Snodgrass; Mike Stonebraker; Avi Silberschatz; Timos Sellis; Hans Schek; Jeff Naughton; David Maier; Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk,Abstract A group of senior database researchers gathers every few years to assess the stateof database research and to point out problem areas that deserve additional focus. Thisreport summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6;2003 in Lowell; Mass. It observes that information management continues to be a criticalcomponent of most complex software systems. It recommends that database researchersincrease focus on: integration of text; data; code; and streams; fusion of information fromheterogeneous data sources; reasoning about uncertain data; unsupervised data mining forinteresting correlations; information privacy; and self-adaptation and repair.,*,2003,*
Broadcast Objects for Effective Data Dissemination in BADD,Stanley B Zdonik,Abstract: Data dissemination systems are difficult to design properly. Since interest in thistopic is relatively new; there is little excerption with methods for making performancedecisions. this project has been focused on providing tools for understanding the impact ofdesign decisions in dissemination-based data delivery. We have developed two broadclasses of tools. The first is a set of simulation-based tools that allowed us to studyfundamental algorithms. We have applied these simulators to the problem of informationchannelization. We have built a test harness for controlling the deployment of multipleexperiments and the collection of results into a convenient graphical form. We see this amajor step toward a commander's workbench; a tool to help commanders make resourceallocation decisions. The second class of tool is a tool kit that allows us to quickly …,*,2000,*
Proceedings of the Twenty-fifth International Conference on Very Large Databases: VLDB'99; Edinburgh; 07-10.09. 1999,Stan Zdonik,*,*,1999,*
Proceedings of the 25th International Conference on Very Large Databases,M Atkinson; ME Orlowska; P Valduriez; S Zdonik; M Brodie,skip nav …,VLDB'99,1999,*
Dissemination-based information systems: Your data may be where you least expect it,Stanley B Zdonik,Abstract. This talk examines a new architecture for distributed information systems that isbased on the idea that asymmetry in communication channels will require a new approachto data delivery. Most current information systems require processes to pull (ie; request) datafrom external information sources. We advocate the use of broadcast or push as a way todeliver data in asymmetric situations. A dissemination-based information system is one thatcan balance the use of pull and push to suit the current parameters of the network and therequirements of the applications. This talk will discuss a particular form of dissemination thatuses the communication channels as a storage medium. We will describe a new proposalcalled broadcast disks which pushes data from storage servers to clients in a cyclic fashionin advance of any request. By appropriately choosing the frequency of broadcast for each …,British National Conference on Databases,1996,*
The Design and Implementation of a High-Performance Storage Server.,Stanley B Zdonik,Abstract: This project investigated the design and implementation of storage servers toaccount for costs imposed by current network constraints. The activity had two main focuses.The first was on the design of a storage server that was based on the use of asynchronousprotocols and techniques where ever possible to cut down on the message traffic associatedwith synchrony. The second was on the use of push-based data broadcast to alleviate theproblems of high network asymmetry. In the first part of our study; our objective was toexplore the use of asynchronous protocols in the construction of a distributed object storagemanager. We have shown that we can achieve better performance than a system that useshighly synchronized techniques such as standard locking. We have also shown that such anapproach can facilitate extensibility in the sense that we can decouple basic system …,*,1996,*
Master Project Report Optimizing Predicates in Object-Oriented Queries,Yung-Ming Chien; Stanley B Zdonik,In my project; I am focusing on optimizing predicates in the EPOQ object-oriented queryoptimizer. Since often more than one query plan could satisfy the same query; I developed strategiesto find more efficient query plans for evaluating predicates. Right now; this project consists ofthe EPOQ regions: "normalize"; "select"; "apply"; "andnot"; and "CNF'(also known as ConjunctiveNormal Form). Under the "CNF' region; there are two other regions: "Simplify AND" and "SimplifyOR". I developed the "normalize"; "CNF'; "Simplify AND"; and "Simplify OR" regions. Section 3is an overview of these regions … In [1]; Mitchell introduced an extensible architecture for aquery optimizer in an object-oriented database. It modularizes different query optimization strategiesinto sepa rate optimization regions; and uses an overall control strategy to determine which regionsare applicable to a specific query; and in what order … The EPOQ query optimizer …,*,1996,*
ACM SIGPLAN OOPS Messenger Volume 6 Issue 3,Stanley B Zdonick,*,*,1995,*
The Aqua Data Model And Algebra Theodore W. LeungP Gail Mitchell2 Bharathi Subramanian3,Bennet Vance; Scott L Vandenberg; Stanley B Zdonik,Abstract This paper describes a new object-oriented model and query algebra that will beused as an input language for the query optimizers that are being built as a part of the EREQproject. The model adopts a uniform view of objects and values and separates syntactic;semantic; and implementation concerns. The algebra addresses issues of type-definedequality and duplicate elimination as well as extensions to bulk types other than sets.,*,1993,*
THE BOXWOOD PRESS PACIFIC GROVE; CA; USA,Fred J Maryanski; Serge Abiteboul; Peter Buneman; Steven A Demurjian; Hans-Detlef Gerhardt; Peter Gray; Roger King; Michel Ldonard; John Mylopoulos; FJ Radermacher; Ron Sacks-Davis; Joachim W Schmidt; David Stemple; Kyu-Young Whang; Michel Adiba; Walter A Burkhard; Hector Garcia-Molina; Nathan Goodman; Kuan-Tsae Huang; Masaru Kitsuregawa; Dennis McLeod; Antoni Oliv; K Ramamohanarao; Peter Scheuerman; Kenneth Sevcik; Stanley YW Su; Hartmut Wedekind; Arie Shoshani; Thomas Wu; Hans-J Schek; Antonio Albano; George Copeland; Georges Gardarin; Georg Gottlo; Tadao Ichikawa; Tosiyasu L Kunii; Robert A Meersman; Alain Pirotte; Andreas Reuter; Gunter Schlageter; Arne Soivberg; Yannis Vassiliou; Stanley Zdonik; Michael Rys,Submission of a paper to The VLDBJournalis understood to imply that it is not beingconsidered for publication elsewhere and that the author's permission to publish his/herarticle (s) in this journal implies the exclusive authorization of the publisher to deal with allissues concerning the copyright therein.,The International Journal on Very Large Data Bases,1993,*
Fred J. Maryanski Storrs; CT; USA,Serge Abiteboul; Peter Buneman; Hector Garcia-Molina; Nathan Goodman; Kuan-Tsae Huang; Masaru Kitsuregawa; Dennis McLeod; Antoni Oliv; FJ Radermacher; Ron Sacks-Davis; Joachim W Schmidt; Stanley YW Su; Hartmut Wedekind; Michel Adiba; Walter A Burkhard; Georges Gardarin; Georg Gottlo; Tadao Ichikawa; Tosiyasu L Kunii; Robert A Meersman; M Tamer Ozsu; K Ramamohanarao; Peter Scheuerman; Kenneth Sevcik; Yannis Vassiliou; Stanley Zdonik; Arie Shoshani; Thomas Wu,Users of an index have specific topics in mind and wish to find cues to subject matterdiscussed in the volume. These cues (words and phrases) should refer to well-definedstructures; actions; or concepts of the subject matter--the subject-matter organization ortaxonomy. Because the study of very large databases is a relatively new academic pursuit;the terminology is evolving and sometimes has inconsistent or undefined referents. TheEditors of The VLDB Journal are attempting to develop a useful; consistent vocabulary forthe articles published in the journal.,The International Journal on Very Large Data Bases,1993,*
Ordered Types Data Model,Bharathi Subramanian; Stanley B Zdonik; Theodore W Leung,*,Proceedings,1993,*
The AQUA Data Model and Algebra,Bharathi Subramanian; Theodore W Leung; Gail Mitchell; Bennet Vance; Scott L Vandenberg; Stanley B Zdonik,*,Proceedings,1993,*
A Cooperative Transaction,Stanley B Zdonik,*,Database Transaction Models for Advanced Applications,1992,*
Letter from the editor on non-standard transaction models,Stanley B Zdonik,*,Office Knowledge Engineering,1991,*
Synchronization and Recovery in Cooperative Transactions^,Marian H Nodine Andrea H Skarra; Stanley B Zdonik,*,Implementing persistent object bases: principles and practice,1991,*
SIGNATURE NAME COMPATIBLE COMPATIBLE,Peter Wegner; Stanley B Zdonik,*,Proceedings,1990,*
Interfaces,SB Zdonik; D Maier,*,Readings in object-oriented database systems,1989,*
Transaction processing,Stanley B Zdonik; David Maier,Google; Inc. (search). SIGN IN SIGN UP. Transaction processing. Authors: Stanley B. Zdonik;,Readings in object-oriented database systems,1989,*
Semantic data models and persistent languages,Stanley B Zdonik; David Maier,*,Readings in object-oriented database systems,1989,*
PANEL ON TRANSACTIONS IN OBJECT-ORIENTED SYSTEMS-DISCUSSION,SB ZDONIK; T BLOOM; D STEMPLE; J WING,*,SIGPLAN NOTICES,1989,*
REPORT ON THE OBJECT-ORIENTED DATABASE WORKSHOP-SEMANTIC ASPECTS,MJ CARUSO; SB ZDONIK,*,SIGPLAN NOTICES,1988,*
Why Like Isn't Like Is-a (or as You Like It),Peter Wegner; Stanley B Zdonik,*,*,1987,*
Panel on OODB,Stan Zdonik,Stan addressed his own questions by stating that OODBs add to traditional programminglanguages such concepts as persistence; sharing/transactions; and efficient access to largeamounts of data; and to traditional databases concepts such as abstraction; extensibletyping; and procedures. He mentioned; in response to his second question that some of thebarriers to the achievement of appropriate performance are query optimization; graphwalking; and distribution. At this point the panel members took turns presenting theirpositions. It would have been interesting to see each panel member address the questionsdirectly. I'm sure a lively debate would have followed. But each panelist had prepared a tenminute talk on aspects of their work that they believed relevant to the panel discussion. Infairness to the panelists; I think Stan sprung these questions on them at the last minute …,ACM SIGPLAN Notices,1987,*
Workshop on OODB semantics,Michael Caruso; Stan Zdonik,A limited attendance; half-day workshop was held as part of OOPSLA'87 to discuss thesemantic issues appropriate to the design (and implementation) of Object-Oriented DataBase Management Systems. The integration of computational completeness; persistence;and sharing in an Object-Oriented DBMS raises a number of fundamental conceptualissues. As publicized; the goals of the workshop were to address the questions,ACM SIGPLAN Notices,1987,*
ACM SIGOIS Bulletin Volume 7 Issue 2-3,Carl Hewitt; Stanley Zdonik,Google; Inc. (search) …,*,1986,*
ACM Transactions on Information Systems (TOIS) Volume 4 Issue 3,Carl Hewitt; Stanley B Zdonik; Robert B Allen,*,*,1986,*
SPECIAL ISSUE-RESEARCH CONTRIBUTIONS-SELECTED PAPERS FROM THE 1986 CONFERENCE ON OFFICE INFORMATION-SYSTEMS; BROWN-UNIVER...,C HEWITT; SB ZDONIK,*,*,1986,*
Approaches to change in an object-oriented database,Stanley Zdonik,*,ACM SIGPLAN Notices,1986,*
IBM SOLIDDB,Katriina Vakkila; Stan Zdonik,2. HA Controller (HAC) is the automatic redundancy management program for IBM solidDBHSB. HAC detects failures; performs failovers; and restarts servers when necessary. HACalso has an API that enables HA Managers to connect to it. 3. HA Manager is a GUI-basedtool that shows the status of HotStandby servers and the state of HA Controllers. The HAManager also Includes basic functionality for managing the HAC. This tool is used in thedemonstration to simulate a failure on the primary server and make a switch to thesecondary server.,*,*,*
1994 First Workshop on Mobile Computing Systems and Applications (WMCSA),S Zdonik; M Franklin; R Alonso; S Acharya; A Herzberg; H Krawczyk; G Tsudik,Mobile computers and wireless networks are emerging technologies which will soon makeubiquitous computing a reality. In the wireless environment; mobile clients may often bedisconnected from stationary server machines or may have only a low-bandwidth channelfor sending messages to servers. This environment raises new challenges for the support ofdatabase applications for three reasons: 1) the...,*,*,*
TOC http://portal. acm. org/toc. cfm? id= 800023&type= proce...,Chairmen Clarence A Ellis; James Bair; Alain Wegmann; Stanley B Zdonik; Matts Ahlsén; Anders Björnerstedt; Stef an Britts; Christer Hultén; Lars Söderlund,*,*,*,*
Data Engineering,Michael Barry Ramasamy; Andrew Jorgensen; Christopher Kellogg; Neng Lu; Bill Graham; Jingwei Wu; Paris Carbone; Stephan Ewen; Seif Haridi; Asterios Katsifodimos; Volker Markl; Kostas Tzoumas; Michael J Franklin; Sailesh Krishnamurthy; Amit Bhat; Madhu Kumar; Robert Lerche; Kim Macpherson; Badrish Chandramouli; Jonathan Goldstein; Mike Barnett; James F Terwilliger; Yuanzhen Ji Heinze; Lars Roediger; Valerio Pappalardo; Andreas Meister; Zbigniew Jerzak; Christof Fetzer; Elke A Rundensteiner; Olga Poppe; Chuan Lei; Medhabi Ray; Lei Cao; Yingmei Qi; Mo Liu; Di Wang; Stan Zdonik; John Meehan; Cansu Aslantas; Michael Stonebraker; Kristin Tufte; Chris Giossi; Hong Quach; Michael Grossniklaus; Marc H Scholl; Andreas Weiler,This December; 2015 issue of the Bulletin is; as some of you may notice; being published inJuly of 2016; after the March and June; 2016 issues have been published. Put simply; theissue is late; and the March and June issues were published in their correct time slots. Theformatting of the issue; and the surrounding editorial material; eg the inside front cover andcopyright notice; are set to the December; 2015 timeframe. Indeed; the only mention of thisinverted ording of issues is in this paragraph. Things do not always go as planned. However;I am delighted that the current issue is being published; and I have high confidence that youwill enjoy reading about next-generation stream processing; the topic of the issue.,*,*,*
conduit! condu t!,Stan Zdonik,local mainframe where a local installation of a database engine resided. Now andincreasingly in the future; the information is “out there;” somewhere in the network cloud;residing in anything from a supercomputing cluster to a handheld computer. This increasedaccess to geographically distributed data has been accompanied by unbounded growth inInternet traffic. The popularity of multimedia applications; the large number of businessestaking to the Web; and burgeoning Internet commerce are all contributing to the informationglut on the Internet. Microchips and hard drives just can't keep up. As hard drives get larger—growing by some estimates to 200 GB in 2003—the data they store becomes harder toaccess. Simply put; the increase in hard-drive capacity is outpacing improvements in themechanisms responsible for positioning the read-head. As microchips currently double in …,*,*,*
Inheritance as an Incremental Modiﬁcation Mechanism or What Like 13 and Isn’t Like,Peter Wegner; Stanley B Zdonik,Abstract: Incremental modiﬁcation is a fundamental mechanism not only in software systems;but'also in physical and mathematical systems. Inheritance owes its importance in largemeasure to its ﬂexibility as a discrete incremental modiﬁcation mechanism. Four increasinglypermissive properties of incremental modiﬁcation realizable by inheritance are examined:behavior compatibility; signature compatibility; name compatibility. and cancellation.Inheritance for entities with finite sets of attributes is deﬁned and characterized asincremental modiﬁcation with deferred binding of self-reference. Types defined aspredicates for type checking are contrasted with classes deﬁned as templates for objectgeneration. Mathematical; operational; and conceptual models of inheritance are thenexamined in detail; leading to a discussion of algebraic models of behavioral …,*,*,*
VLDB Endowment Board of Trustees,Gerhard Weikum; Laura M Haas; Paolo Atzeni; Michael J Franklin; Amr El Abbadi; Gustavo Alonso; Peter MG Apers; Elisa Bertino; Peter Buneman; Johann Christoph Freytag; HV Jagadish; Christian S Jensen; Donald Kossmann; David Lomet; Renée J Miller; Shojiro Nishio; Beng Chin Ooi; Meral Ozsoyoglu; Krithi Ramamritham; Raghu Ramakrishnan; Stanley B Zdonik,The VLDB Endowment is a non-profit foundation whose objective is to promote scientific andeducational activities in the area of large-scale data; information; and knowledgemanagement. The Endowment serves as the steering committee for the VLDB conferenceseries. The Endowment also sponsors various scholarly activities. It has established aprogram that supports summer schools; tutorials; and other training activities of this kind; incountries that could otherwise not afford the expenses for such events. The Endowment isalso the main sponsor of the biennial Conference on Innovative Data Systems Research(CIDR); and it runs the VLDB Journal; one of the most successful journals in the databasearea. On various activities; the Endowment closely cooperates with ACM SIGMOD. TheVLDB Endowment has a board of 21 elected trustees; who are the legal guardians of the …,*,*,*
FAR EAST,William Armstrong; Christos Faloutsos; Vassos Hadzilacos; HV Jagadish; Ravi Krishnamurthy; David Lomet; Dennis McLeod; Shamkant Navathe; Ekow Otoo; Raghu Ramakrishnan; Betty Salzberg; Jacob Slonim; Irving Traiger; Carlo Zaniolo; Serge Abiteboul; Peter Apers; Horst Biller; Peter Dadam; Robert Demolombe; Frank Eliassen; Georg Gottlob; Peter Lockmann; Robert Meersman; Andreas Reuter; Hans-Joerg Schek; Nicolas Spyratos; Yannis Vasiliou; Bruce Croft; Hector Garcia-Molina; Paula Hawthorn; Randy Katz; Bruce Lindsay; Wo-Shun Luk; Albert Mendelzon; Jack Orenptein; Z Meral Ozsoyoglu; Arnon Rosenthal; Timos Sellis; Toby Teorey; Grant Weddell; Stanley Zdonik; Michele Adiba; Janis Bubenko; Klaus Dittrich; Norbert Fuhr; Theo Haerder; Heikki Mannila; Antoni Olive; Yehoshua Sagiv; Gunter Schlageter; Bernd Walter; Umeshwar Dayal; Goetz Graefe; Yannis Ioannidis; Roger King; Guy Lohman; Ian McLeod; Tim Merrett; Sylvia Osborn; M Tamer Ozsu; Nick Roussopoulos; Kenneth Sevcik; Frank Tompa; Clement Yu; Antonio Albano; Francois Bancilhon; Stefano Ceri; Hartmut Ehrig; Georges Gardarin; Witold Litwin; Rainer Manthey; Peter Pistor; Felix Saltor; Amilcar Sernadas; Henry Tirri; Roberto Zicari,William Armstrong (Canada) Christos Faloutsos (USA) Vassos Hadzilacos (Canada) HV Jagadish(USA) Ravi Krishnamurthy (USA) David Lomet (USA) Dennis McLeod (USA) Shamkant Navathe(USA) Ekow Otoo (Canada) Raghu Ramakrishnan (USA) Betty Salzberg (USA) Jacob Slonim(Canada) Irving Traiger (USA) Carlo Zaniolo (USA) … Serge Abiteboul (France) Peter Apers(Netherlands) Horst Biller (Germany) Peter Dadam (Germany) Robert Demolombe (France) FrankEliassen (Norway) Georg Gottlob (Austria) Peter Lockmann (Germany) Robert Meersman(Netherlands) Andreas Reuter (Germany) Hans-Joerg Schek (Switzerland) Nicolas Spyratos(France) Yannis Vasiliou (Greece) … Ching-Chen Chang (Taiwan; China) Yahiko Kambayashi(Japan) Sukho Lee (Korea) Leszek Maciaszek (Australia) Maria Orlowska (Australia) RonSacks-Davis (Australis) KP Tan (Singapore) KY Whang (Korea),*,*,*
UNITED STATES PATENT AND TRADEMARK OFFICE _ BEFORE THE PATENT TRIAL AND APPEAL BOARD _ Ex parte SHILPA LAWANDE; ALEXANDER RASIN,OMER TRAJMAN; STANLEY B ZDONIK,*,Appeal,*,*
Krablin; GL Burroughs Corporation; PO Box 203; Paoli; PA 19310; USA.,D MacQueen; DCI Matthews; Com Exchange Street; T Merrett; R Morrison; KY16 Fife; RS Nikhil; D Pedreschi; Italia Corso; P Wegner; RI02912 Providence; W Weihl; SB Zdonik,List of Authors Ait-Kaci; H. Microelectronics and Computer Technology Corporation; 9430 ResearchBoulevard; Austin; TX78759; USA. Albano; A. University of Pisa; Instituto di Science delInformazione; Corso; Italia 40; 1-56100; Pisa; Italy. Atkinson; MP Department of ComputingScience; University of Glasgow; 14 Lilybank Gardens; Glasgow; 012 8QQ; Scotland.Borgida; A. Department of Computer Science; Rutgers University; New Brunswick; NewJersey; USA. Buneman; P. Department of Computer Information Science; The Moore Schoolof Electrical Engineering; University of Pennsylvania; Philadelphia 19104-3897; USA.Cardelli; L. AT&T Bell Laboratories; Room2C 322; Murray Hill; NJ07974; USA. Cockshott; WPMEMEX; 21 Landsdowne Crescent; Edinburgh; Scotland. Fairbairn; J. Computer Laboratory;University of Cambridge; Corn Exchange Street; Cambridge; CB2 3QG; England. Ghelli …,Topics in Information Systems,*,*
Object-Oriented Database Programming Languages,Toby Bloom; Stanley B Zdonik,*,*,*,*
The AQUA Approach to Querying Lists and Trees in Object-Oriented Databases* Bharathi Subramanian Brown University t,Stanley B Zdonik; Brown Universityt,*,*,*,*
I-Store: Data Management for Fast Networks,Carsten Binnig; Ugur Cetintemel; Tim Kraska; Stan Zdonik; Erfan Zamanian; Andrew Crotty,Motivation: Existing distributed data management systems typically assume that the networkis a major bottleneck [10]. Consequently; avoiding remote data transfers is an importantdesign aspect of existing systems. In extreme cases; this has lead to system designs; whichexplicitly do not support certain distributed operations (eg; BigTable only supports joins if theinner table contains less than 8 MB of data). A common design principle; however; is tominimize remote data transfer by the two following techniques: First; existing systems try tofind an optimal partitioning scheme to co-partition data in order to avoid remote datatransfers for operations such as joins or to avoid distributed transactions. Second; locality-aware scheduling strategies aim to increase data-locality by shipping computation to thenodes where the data is stored. However; these techniques still result in major limitations …,*,*,*
SpringerBriefs in Computer Science,Peng Ning; Lakhmi C Jain; David Padua; Shashi Shekhar; Stan Zdonik; Borko Furht; Xindong Wu; Jonathan Katz; Xuemin Shen; VS Subrahmanian,Mobile Social Networks are networks in which mobile social users physically interact witheach other and further reach network service; even in the absence of network infrastructureor end-to-end connectivity [69][86]. MSNets can be viewed as a kind of socially-awareDelay/Disruption Tolerant Networks (DTNs). Thanks to the popularization of smartphones(eg; iPhone; Nokia N95; and Blackberry); MSNets have begun to attract more attention to bedeployed in a number of critical areas; including large-scale disaster recovery; battlefields;vehicular ad hoc networks; and wide-area sensor networks. However; the intermittent anduncertain network connectivity makes data dissemination in MSNets a challenging problem.Broadcasting is the operation of sending data from a source user to all other users in thenetwork; which is frequently used in many applications of mobile ad hoc networks …,*,*,*
NEWS SECTION,G Bracchi; S Christodoulakis; Bruce Croft; Eli Gerson; Irene Greif; Ben Konsynski; Yoshifumi Masunaga; Norm Meyrowitz; Alain Michard; Juzar Motiwalla; John Mylopoulos; Bill Newman; Margi Olson NYU; Faust Rabitti; Jeff Rulifson; Lucy Suchman; Dennis Tsichritzis; CJ van Rijsbergen; Andrew Whinston; Stan Zdonik; Dave Choy; Fred Lochovsky; Skip Ellis MCC; Sig Treu; Alex Verrijn-Stuart; Robert B Allen,*,*,*,*
Mehmet Altinel,William Shapiro; Stan Zdonik,*,*,*,*
Stream-lined Data Management,Stan Zdonik,Why all the sudden interest in data streams? In our view; a stream is simply an appendonlycollection of tuples that is ordered by some increasing key value (often time). The notion ofappend-only relations is not that startling. Is this just the latest trendy topic or is theresomething more fundamental going on? Data streams are interesting primarily because ofthe ways in which they arise and the ways in which they are used. Unlike other recent “hottopics” in which novel structure in the data (eg; XML; design data; human genome) drove theinterest; the excitement here lies in the unusual processing requirements that streams andstream-based applications bring to the table. The focus is not on new data models; but ratheron new system architectures; non-traditional optimizations; and algorithms.,*,*,*
Daniel A. Ford; IBM Almaden,Michael Franklin; Ophir Frieder; Norbert Fuhr; Dimitrios Georgakopoulos; Carol Goble; Theo Haerder; Willem Jonker; Yahiko Kambayashi; Jessie Kennedy; Martin Kersten; Hiroyuki Kitagawa; Masaru Kitsuregawa; Matthias Klusch; Mong Li Lee; Qing Li; Xiaoming Li; Ling Liu; Mengchi Liu; Frederick H Lochovsky; Hongjun Lu; Heiko Schuldt; Timos Sellis; Ming-Chien Shan; Timothy K Shih; Il-Yeol Song; Kian Lee Tan; Katsumi Tanaka; David Toman; Frank W Tompa; Shan Wang; Gerhard Weikum; Kam-Fai Wong; Masatoshi Yoshikawa; S Yu Philip; Osmar Zaiane; Carlo Zaniolo; Stan Zdonik; Aoying Zhou; Lizhu Zhou; Shuigeng Zhou,*,*,*,*
ÈÖÓ¬ Ð ¹ Ö Ú Ò Ø Å Ò Ñ ÒØ,Mitch Cherniack; Michael J Franklin; Stan Zdonik,Abstract To support the popular vision of a semantic web; data management must beintroduced as part of the web's infrastructure. Traditional database systems follow the DBA-based model for data management—a DBA consults with clients about their intended usesof the database and sets data management policies (schema design; indexes; clusteringetc.) accordingly. This model does not transfer well to the web where there are too manyclients to consider in determining policy; and autonomous data sources prevent externallyset data management policies from being imposed. Profile-driven data management is aframework for determining and implementing data management policies upon the analysisof user data requirements specified in profiles. Profiledriven data management systems actas middleware intermediaries between users and the web; automatically offering data …,*,*,*
Data Engineering,Stan Zdonik; Michael Stonebraker; Mitch Cherniack; Magdalena Balazinska; Hari Balakrishnan; Sailesh Krishnamurthy; Sirish Chandrasekaran; Owen Cooper; Amol Deshpande; Michael J Franklin; Joseph M Hellerstein; Wei Hong; Samuel R Madden; Fred Reiss; Mehul A Shah,Bulletin of the Technical Committee on Data Engineering March 2003 Vol. 26 No. 1 IEEE ComputerSociety Letters Letter from the Editor-in-Chief...................................................... David Lomet 1 Letterfrom the Special Issue Editor................................................ Johannes Gehrke 2 Special Issue onData Stream Processing The Aurora and Medusa Projects … EditorialBoard Editor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 RedmondWA 98052-6399 lomet@ microsoft. com Associate Editors Umeshwar Dayal Hewlett-PackardLaboratories 1501 Page Mill Road; MS 1142 Palo Alto; CA 94304 Johannes Gehrke Departmentof Computer Science Cornell University Ithaca; NY 14853 Christian S. Jensen Department ofComputer Science Aalborg University Fredrik Bajers Vej 7E DK-9220 Aalborg Øst; DenmarkRenée J. Miller Dept. of Computer Science University of Toronto 6 King's College Rd …,Urbana,*,*
SciDB Model and Operators; Rev. 5,David Maier; Stan Zdonik; Mike Stonebraker,*,*,*,*
1994 Workshop on Mobile Computing Systems and Applications,A Demers; K Petersen; M Spreitzer; D Terry; M Theimer; B Welch; A Mukherjee; DP Siewiorek; S Zdonik; M Franklin; R Alonso; S Acharya; J Saldanha; DL Cohn; JA Cobb; CC Edmondson-Yurkanan; MG Gouda; MR Ebling; LB Mummert; DC Steere; L Huston; P Honeyman; GH Kuenning; M Ahamad; FJ Torres-Rojas; R Kordale; J Singh; S Smith; R Gruber; F Kaashoek; B Liskov; L Shrira; N Mirghafori; A Fontaine,A. Demers; K. Petersen; M. Spreitzer; D. Terry; M. Theimer; and B. Welch Mobility: A Mediumfor Computation; Communication; and Control … A. Mukherjee and DP Siewiorek Are "Disksin the Air" Just Pie in the Sky … S. Zdonik; M. Franklin; R. Alonso; and S. Acharya A HybridModel for Mobile File Systems … J. Saldanha and DL Cohn Universal Mobile Addressing inthe Internet … JA Cobb; CC Edmondson-Yurkanan; and MG Gouda … Chair: Peter HoneymanOvercoming the Network Bottleneck in Mobile Computing … MR Ebling; LB Mummert; and DCSteere * Peephole Log Optimization L. Huston and P. Honeyman The Design of the SEER PredictiveCaching System … GH Kuenning . Detecting Mutual Consistency of Shared Objects … M.Ahamad; FJ Torres-Rojas; R. Kordale; J. Singh; and S. Smith Disconnected Operation in theThor Object-Oriented Database System … R. Gruber; F. Kaashoek; B. Liskov; and L …,*,*,*
