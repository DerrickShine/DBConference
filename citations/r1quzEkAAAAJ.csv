Open information extraction from the web.,Michele Banko; Michael J Cafarella; Stephen Soderland; Matthew Broadhead; Oren Etzioni,Abstract Traditionally; Information Extraction (IE) has focused on satisfying precise; narrow;pre-specified requests from small homogeneous corpora (eg; extract the location and time ofseminars from a set of announcements). Shifting to a new domain requires the user to namethe target relations and to manually create new extraction rules or hand-tag new trainingexamples. This manual labor scales linearly with the number of target relations. This paperintroduces Open IE (OIE); a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring anyhuman input. The paper also introduces TEXTRUNNER; a fully implemented; highlyscalable OIE system where the tuples are assigned a probability and indexed to supportefficient extraction and exploration via user queries. We report on experiments over a …,IJCAI,2007,1426
Unsupervised named-entity extraction from the web: An experimental study,Oren Etzioni; Michael Cafarella; Doug Downey; Ana-Maria Popescu; Tal Shaked; Stephen Soderland; Daniel S Weld; Alexander Yates,Abstract The KnowItAll system aims to automate the tedious process of extracting largecollections of facts (eg; names of scientists or politicians) from the Web in an unsupervised;domain-independent; and scalable manner. The paper presents an overview of KnowItAll'snovel architecture and design principles; emphasizing its distinctive ability to extractinformation without any hand-labeled training examples. In its first major run; KnowItAllextracted over 50;000 class instances; but suggested a challenge: How can we improveKnowItAll's recall and extraction rate without sacrificing precision? This paper presents threedistinct ways to address this challenge and evaluates their performance. Pattern Learninglearns domain-specific extraction rules; which enable additional extractions. SubclassExtraction automatically identifies sub-classes in order to boost recall (eg;“chemist” and “ …,Artificial intelligence,2005,1150
Web-scale information extraction in knowitall:(preliminary results),Oren Etzioni; Michael Cafarella; Doug Downey; Stanley Kok; Ana-Maria Popescu; Tal Shaked; Stephen Soderland; Daniel S Weld; Alexander Yates,Abstract Manually querying search engines in order to accumulate a large bodyof factualinformation is a tedious; error-prone process of piecemealsearch. Search engines retrieveand rank potentially relevantdocuments for human perusal; but do not extract facts;assessconfidence; or fuse information from multiple documents. This paperintroducesKnowItAll; a system that aims to automate the tedious process ofextracting large collectionsof facts from the web in an autonomous; domain-independent; and scalable manner. Thepaper describes preliminary experiments in which an instance of KnowItAll; running for fourdays on a single machine; was able to automatically extract 54;753 facts. KnowItAllassociates a probability with each fact enabling it to trade off precision and recall. The paperanalyzes KnowItAll's architecture and reports on lessons learned for the design of large …,Proceedings of the 13th international conference on World Wide Web,2004,910
Webtables: exploring the power of tables on the web,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang,Abstract The World-Wide Web consists of a huge number of unstructured documents; but italso contains structured data in the form of HTML tables. We extracted 14.1 billion HTMLtables from Google's general-purpose web crawl; and used statistical classificationtechniques to find the estimated 154M that contain high-quality relational data. Becauseeach relational table has its own" schema" of labeled and typed columns; each such tablecan be considered a small structured database. The resulting corpus of databases is largerthan any other corpus we are aware of; by at least five orders of magnitude.,Proceedings of the VLDB Endowment,2008,522
Machine reading; proceedings of the 21st national conference on Artificial intelligence,Oren Etzioni; Michele Banko; Michael J Cafarella,*,*,2006,318
Textrunner: open information extraction on the web,Alexander Yates; Michael Cafarella; Michele Banko; Oren Etzioni; Matthew Broadhead; Stephen Soderland,Abstract Traditional information extraction systems have focused on satisfying precise;narrow; pre-specified requests from small; homogeneous corpora. In contrast; theTextRunner system demonstrates a new kind of information extraction; called OpenInformation Extraction (OIE); in which the system makes a single; data-driven pass over theentire corpus and extracts a large set of relational tuples; without requiring any humaninput.(Banko et al.; 2007) TextRunner is a fully-implemented; highly scalable example ofOIE. TextRunner's extractions are indexed; allowing a fast query mechanism.,Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,2007,237
Hadoop: a framework for running applications on large clusters built of commodity hardware,Andrzej Bialecki,データインテンシブコンピューティングの省電力化に向けた GPU ノードの活用 (2010年並列/分散/協調処理に関する 『金沢』 サマー・ワークショップ SWoPP2010),http://lucene. apache. org/hadoop,2005,213
Automatic optimization for MapReduce programs,Eaman Jahani; Michael J Cafarella; Christopher Ré,Abstract The MapReduce distributed programming framework has become popular; despiteevidence that current implementations are inefficient; requiring far more hardware than atraditional relational databases to complete similar tasks. MapReduce jobs are amenable tomany traditional database query optimizations (B+ Trees for selections; column-store-styletechniques for projections; etc); but existing systems do not apply them; substantiallybecause free-form user code obscures the true data operation being performed. Forexample; a selection in SQL is easily detected; but a selection in a MapReduce program isembedded in Java code along with lots of other program logic. We could ask theprogrammer to provide explicit hints about the program's data semantics; but one ofMapReduce's attractions is precisely that it does not ask the user for such information …,Proceedings of the VLDB Endowment,2011,174
Methods for domain-independent information extraction from the web: An experimental comparison,Oren Etzioni; Michael Cafarella; Doug Downey; Ana-Maria Popescu; Tal Shaked; Stephen Soderland; Daniel S Weld; Alexander Yates,*,AAAI,2004,158
KnowItNow: Fast; scalable information extraction from the web,Michael J Cafarella; Doug Downey; Stephen Soderland; Oren Etzioni,Abstract Numerous NLP applications rely on search-engine queries; both to extractinformation from and to compute statistics over the Web corpus. But search engines oftenlimit the number of available queries. As a result; query-intensive NLP applications such asInformation Extraction (IE) distribute their query load over several days; making IE a slow;offline process. This paper introduces a novel architecture for IE that obviates queries tocommercial search engines. The architecture is embodied in a system called KnowItNowthat performs high-precision IE in minutes instead of days. We compare KnowItNowexperimentally with the previously-published KnowItAll system; and quantify the tradeoffbetween recall and speed. KnowItNow's extraction rate is two to three orders of magnitudehigher than KnowItAll's.,Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,2005,154
Data integration for the relational web,Michael J Cafarella; Alon Halevy; Nodira Khoussainova,Abstract The Web contains a vast amount of structured information such as HTML tables;HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However; integrating data from this relational webraises several challenges that are not addressed by current data integration systems ormash-up tools. First; the structured data is usually not published cleanly and must beextracted (say; from an HTML list) before it can be used. Second; due to the vastness of thecorpus; a user can never know all of the potentially-relevant databases ahead of time (muchless write a wrapper or mapping for each one); the source databases must be discoveredduring the integration process. Third; some of the important information regarding the data isonly present in its enclosing web page and needs to be extracted appropriately. This …,Proceedings of the VLDB Endowment,2009,146
Machine Reading.,Oren Etzioni; Michele Banko; Michael J Cafarella,*,AAAI,2006,141
Uncovering the Relational Web.,Michael J Cafarella; Alon Y Halevy; Yang Zhang; Daisy Zhe Wang; Eugene Wu,ABSTRACT The World-Wide Web consists of a huge number of unstructured hypertextdocuments; but it also contains structured data in the form of HTML tables. Many of thesetables contain both relational-style data and a small “schema” of labeled and typed columns;making each such table a small structured database. The WebTables project is an effort toextract and make use of the huge number of these structured tables on the Web. A cleancollection of relational-style tables could be useful for improving web search; schemadesign; and many other applications. This paper describes the first stage of the WebTablesproject. First; we give an in-depth study of the Web's HTML table corpus. For example; weextracted 14.1 billion HTML tables from a several-billion-page portion of Google'sgeneralpurpose web crawl; and estimate that 154 million of these tables contain high …,WebDB,2008,133
A search engine for natural language applications,Michael J Cafarella; Oren Etzioni,Abstract Many modern natural language-processing applications utilize search engines tolocate large numbers of Web documents or to compute statistics over the Web corpus. YetWeb search engines are designed and optimized for simple human queries---they are notwell suited to support such applications. As a result; these applications are forced to issuemillions of successive queries resulting in unnecessary search engine load and in slowapplications with limited scalability. In response; this paper introduces the Bindings Engine(BE); which supports queries containing typed variables and string-processing functions. Forexample; in response to the query" powerful‹ noun›" BE will return all the nouns in its indexthat immediately follow the word" powerful"; sorted by frequency. In response to the query"Cities such as ProperNoun (Head (‹ NounPhrase›))"; BE will return a list of proper nouns …,Proceedings of the 14th international conference on World Wide Web,2005,133
Structured querying of Web text,Michael J Cafarella; Christopher Re; Dan Suciu; Oren Etzioni; Michele Banko,ABSTRACT The Web contains a huge amount of text that is currently beyond the reach ofstructured access tools. This unstructured data often contains a substantial amount of implicitstructure; much of which can be captured using information extraction (IE) algorithms. Bycombining an IE system with an appropriate data model and query language; we couldenable structured access to all of the Web's unstructured data. We propose a general-purpose query system called the extraction database; or ExDB; which supports SQL-likestructured queries over Web text. We also describe the technical challenges involved;motivated in part by our experiences with an early 90M-page prototype.,3rd Biennial Conference on Innovative Data Systems Research (CIDR); Asilomar; California; USA,2007,116
Building nutch: Open source search,Mike Cafarella; Doug Cutting,Abstract Search engines are as critical to Internet use as any other part of the networkinfrastructure; but they differ from other components in two important ways. First; theirinternal workings are secret; unlike; say; the workings of the DNS (domain name system).Second; they hold political and cultural power; as users increasingly rely on them tonavigate online content.,Queue,2004,109
Ontology-driven information extraction with ontosyphon,Luke K McDowell; Michael Cafarella,Abstract The Semantic Web's need for machine understandable content has led researchersto attempt to automatically acquire such content from a number of sources; including theweb. To date; such research has focused on “document-driven” systems that individuallyprocess a small set of documents; annotating each with respect to a given ontology. Thispaper introduces OntoSyphon; an alternative that strives to more fully leverage existingontological content while scaling to extract comparatively shallow content from millions ofdocuments. OntoSyphon operates in an “ontology-driven” manner: taking any ontology asinput; OntoSyphon uses the ontology to specify web searches that identify possible semanticinstances; relations; and taxonomic information. Redundancy in the web; together withinformation from the ontology; is then used to automatically verify these candidate …,International Semantic Web Conference,2006,102
Theoretical limits of hydrogen storage in metal–organic frameworks: Opportunities and trade-offs,Jacob Goldsmith; Antek G Wong-Foy; Michael J Cafarella; Donald J Siegel,Because of their high surface areas; crystallinity; and tunable properties; metal–organicframeworks (MOFs) have attracted intense interest as next-generation materials for gascapture and storage. While much effort has been devoted to the discovery of new MOFs; avast catalog of existing MOFs resides within the Cambridge Structural Database (CSD);many of whose gas uptake properties have not been assessed. Here we employ datamining and automated structure analysis to identify;“cleanup;” and rapidly predict thehydrogen storage properties of these compounds. Approximately 20 000 candidatecompounds were generated from the CSD using an algorithm that removes solvent/guestmolecules. These compounds were then characterized with respect to their surface area andporosity. Employing the empirical relationship between excess H2 uptake and surface …,Chemistry of Materials,2013,93
Structured data on the web,Michael J Cafarella; Alon Halevy; Jayant Madhavan,Though the web is best known as a vast repository of shared documents; it also contains a significantamount of structured data covering a complete range of topics; from product to financial;public-record; scientific; hobby-related; and government. Structured data on the Web sharesmany similarities with the kind of data traditionally managed by commercial database systemsbut also reflects some unusual characteristics of its own; for example; it is embedded in textualWeb pages and must be extracted prior to use; there is no centralized data design as there isin a traditional database; and; unlike traditional databases that focus on a single domain; it coverseverything. Existing data-management systems do not address these challenges and assumetheir data is modeled within a well-defined domain … This article discusses the nature ofWeb-embedded structured data and the challenges of managing it. To begin; we present …,Communications of the ACM,2011,92
Web-scale extraction of structured data,Michael J Cafarella; Jayant Madhavan; Alon Halevy,Abstract A long-standing goal of Web research has been to construct a unified Webknowledge base. Information extraction techniques have shown good results on Web inputs;but even most domain-independent ones are not appropriate for Web-scale operation. Inthis paper we describe three recent extraction systems that can be operated on the entireWeb (two of which come from Google Research). The TextRunner system focuses on rawnatural language text; the WebTables system focuses on HTML-embedded tables; and thedeep-web surfacing system focuses on" hidden" databases. The domain; expressiveness;and accuracy of extracted data can depend strongly on its source extractor; we describedifferences in the characteristics of data produced by the three extractors. Finally; we discussa series of unique data applications (some of which have already been prototyped) that …,ACM SIGMOD Record,2009,92
Brainwash: A Data System for Feature Engineering.,Michael R Anderson; Dolan Antenucci; Victor Bittorf; Matthew Burgess; Michael J Cafarella; Arun Kumar; Feng Niu; Yongjoo Park; Christopher Ré; Ce Zhang,ABSTRACT A new generation of data processing systems; including web search; Google'sKnowledge Graph; IBM's Watson; and several different recommendation systems; combinerich databases with software driven by machine learning. The spectacular successes ofthese trained systems have been among the most notable in all of computing and havegenerated excitement in health care; finance; energy; and general business. But buildingthem can be challenging; even for computer scientists with PhD-level training. If thesesystems are to have a truly broad impact; building them must become easier. We exploreone crucial pain point in the construction of trained systems: feature engineering. Given thesheer size of modern datasets; feature developers must (1) write code with few effectiveclues about how their code will interact with the data and (2) repeatedly endure long …,CIDR,2013,67
Using social media to measure labor market flows,Dolan Antenucci; Michael Cafarella; Margaret Levenstein; Christopher Ré; Matthew D Shapiro,ABSTRACT Social media enable promising new approaches to measuring economic activityand analyzing economic behavior at high frequency and in real time using informationindependent from standard survey and administrative sources. This paper uses data fromTwitter to create indexes of job loss; job search; and job posting. Signals are derived bycounting job-related phrases in Tweets such as “lost my job.” The social media indexes areconstructed from the principal components of these signals. The University of MichiganSocial Media Job Loss Index tracks initial claims for unemployment insurance at mediumand high frequencies and predicts 15 to 20 percent of the variance of the prediction error ofthe consensus forecast for initial claims. The social media indexes provide real-timeindicators of events such as Hurricane Sandy and the 2013 government shutdown …,*,2014,63
Sample-driven schema mapping,Li Qian; Michael J Cafarella; HV Jagadish,Abstract End-users increasingly find the need to perform light-weight; customized schemamapping. State-of-the-art tools provide powerful functions to generate schema mappings;but they usually require an in-depth understanding of the semantics of multiple schemas andtheir correspondences; and are thus not suitable for users who are technicallyunsophisticated or when a large number of mappings must be performed. We propose asystem for sample-driven schema mapping. It automatically constructs schema mappings; inreal time; from user-input sample target instances. Because the user does not have toprovide any explicit attribute-level match information; she is isolated from the possiblycomplex structure and semantics of both the source schemas and the mappings. In addition;the user never has to master any operations specific to schema mappings: she simply …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,58
Extracting and Querying a Comprehensive Web Database.,Michael J Cafarella,ABSTRACT Recent research in domain-independent information extraction holds thepromise of an automatically-constructed structured database derived from the Web. A querysystem based on this database would offer the same breadth as a Web search engine; butwith much more sophisticated query tools than are common today. Unfortunately; thesedomain-independent Web extractors are usually not modelindependent; eg; an extractor thatonly finds binary relations from text will be blind to relational data found in tables. Because atopic area often has a data model that is a natural fit (eg; population statistics are usually intables; while biographical facts about Einstein are embedded in text); even a high-qualitydomain-independent extractor will miss a substantial amount of data. Our omnivore systemattempts to build a comprehensive Web database by running multiple domain …,CIDR,2009,55
Manimal: relational optimization for data-intensive programs,Michael J Cafarella; Christopher Ré,Abstract The MapReduce distributed programming framework is very popular; but currentlylacks the optimization techniques that have been standard with relational database systemsfor many years. This paper proposes Manimal; which uses static code analysis to detectMapReduce program semantics and thereby enable wholly-automatic optimization ofMapReduce programs. For example; a programmer's map function that emits data onlywhen an if... statement holds true is essentially encoding a selection condition; codeanalysis can detect and characterize these conditions. If Manimal has an appropriate indexavailable; it can then alter MapReduce execution to use it. Manimal can address manydifferent optimization opportunities; including projections; structure-aware data compression;and others. However; this paper illustrates the system by focusing on one: efficient …,Procceedings of the 13th International Workshop on the Web and Databases,2010,50
Relational web search,Michael J Cafarella; Michele Banko; Oren Etzioni,ABSTRACT Facts are naturally organized in terms of entities; classes; and their relationshipsas in an entity-relationship diagram or a semantic network. Search engines have eschewedsuch structures because; in the past; their creation and processing have not been practicalat Web scale. This paper introduces the extraction graph; a textual approximation to an entity-relationship graph; which is automatically extracted from Web pages. The extraction graph isan intermediate representation that is more informative than a mere page-hyperlink graphbut far easier to construct than a semantic network. The paper also introduces TextRunner; asearch engine that utilizes this representation to answer complex relational queries that aredifficult to answer using today's search engines or Web Information Extraction (IE) systems.The paper compares TextRunner to a state-of-the-art IE system on list searches; and finds …,WWW Conference,2006,49
Automatic web spreadsheet data extraction,Zhe Chen; Michael Cafarella,Abstract Spreadsheets contain a huge amount of high-value data but do not observe astandard data model and thus are difficult to integrate. A large number of data integrationtools exist; but they generally can only work on relational data. Existing systems forextracting relational data from spreadsheets are too labor intensive to support ad-hocintegration tasks; in which the correct extraction target is only learned during the course ofuser interaction. This paper introduces a system that automatically extracts relational datafrom spreadsheets; thereby enabling relational spreadsheet integration. The resultingintegrated relational data can be queried directly or can be translated into RDF triples. Whencompared to standard techniques for spreadsheet data extraction on a set of 100 randomWeb spreadsheets; the system reduces the amount of human labor by 72% to 92%. In …,Proceedings of the 3rd International Workshop on Semantic Search over the Web,2013,45
Ontology-driven; unsupervised instance population,Luke K McDowell; Michael Cafarella,Abstract The Semantic Web's need for machine understandable content has led researchersto attempt to automatically acquire such content from a number of sources; including theweb. To date; such research has focused on “document-driven” systems that individuallyprocess a small set of documents; annotating each with respect to a given ontology. Thisarticle introduces OntoSyphon; an alternative that strives to more fully leverage existingontological content while scaling to extract comparatively shallow content from millions ofdocuments. OntoSyphon operates in an “ontology-driven” manner: taking any ontology asinput; OntoSyphon uses the ontology to specify web searches that identify possible semanticinstances; relations; and taxonomic information. Redundancy in the web; together withinformation from the ontology; is then used to automatically verify these candidate …,Web Semantics: Science; Services and Agents on the World Wide Web,2008,43
Data management projects at Google,Michael Cafarella; Edward Chang; Andrew Fikes; Alon Halevy; Wilson Hsieh; Alberto Lerner; Jayant Madhavan; S Muthukrishnan,Abstract This article describes some of the ongoing research projects related to structureddata management at Google today. The organization of Google encourages researchscientists to work closely with engineering teams. As a result; the research projects tend tobe motivated by real needs faced by Google's products and services; and solutions are putinto production and tested rapidly. In addition; because of the sheer scale at which Googleoperates; the engineering challenges faced by Google's services often require researchinnovations.,ACM SIGMOD Record,2008,43
Voice browser call control: CCXML version 1.0,RJ Auburn; M Cafarella; D Jackson; J Peck; P Sharma; S Shanmughan; C Stohs; Y Zhang,*,W3C Working Draft,2005,39
Navigating Extracted Data with Schema Discovery.,Michael J Cafarella; Dan Suciu; Oren Etzioni,ABSTRACT Open Information Extraction (OIE) is a recently-introduced type of informationextraction that extracts small individual pieces of data from input text without anydomainspecific guidance such as special training data or extraction rules. For example; anOIE system might discover the triple Frenzy; year; 1972 from a set of documents aboutmovies. Because OIE is domain-independent; it promises to help users when they have acorpus of structured data; but that structure is unknown; such as when browsing a noveldomain or formulating a query. We can describe the structure to the user by displaying arelational schema that fits the extracted data. Unfortunately; the extractions do not carry fullschema information: we have extracted values; but not the correct relations; their rows; ortheir columns. In response we propose TGen; an algorithm for schema discovery; which …,WebDB,2007,35
Open information extraction from the Web,*,To implement open information extraction; a new extraction paradigm has been developedin which a system makes a single data-driven pass over a corpus of text; extracting a largeset of relational tuples without requiring any human input. Using training data; a Self-Supervised Learner employs a parser and heuristics to determine criteria that will be usedby an extraction classifier (or other ranking model) for evaluating the trustworthiness ofcandidate tuples that have been extracted from the corpus of text; by applying heuristics tothe corpus of text. The classifier retains tuples with a sufficiently high probability of beingtrustworthy. A redundancy-based assessor assigns a probability to each retained tuple toindicate a likelihood that the retained tuple is an actual instance of a relationship between aplurality of objects comprising the retained tuple. The retained tuples comprise an …,*,2011,31
Integrating spreadsheet data via accurate and low-effort extraction,Zhe Chen; Michael Cafarella,Abstract Spreadsheets contain valuable data on many topics. However; spreadsheets aredifficult to integrate with other data sources. Converting spreadsheet data to the relationalmodel would allow data analysts to use relational integration tools. We propose a two-phasesemiautomatic system that extracts accurate relational metadata while minimizing user effort.Based on an undirected graphical model; our system enables downstream spreadsheetintegration applications. First; the automatic extractor uses hints from spreadsheets'graphical style and recovered metadata to extract the spreadsheet data as accurately aspossible. Second; the interactive repair identifies similar regions in distinct spreadsheetsscattered across large spreadsheet corpora; allowing a user's single manual repair to beamortized over many possible extraction errors. Our experiments show that a human can …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,29
Senbazuru: a prototype spreadsheet database management system,Zhe Chen; Michael Cafarella; Jun Chen; Daniel Prevo; Junfeng Zhuang,Abstract Spreadsheets have become a critical data management tool; but they lack explicitrelational metadata; making it difficult to join or integrate data across multiple spreadsheets.Because spreadsheet data are widely available on a huge range of topics; a tool that allowseasy spreadsheet integration would be hugely beneficial for a variety of users. Wedemonstrate that Senbazuru; a prototype spreadsheet database management system(SSDBMS); is able to extract relational information from spreadsheets. By doing so; it opensup opportunities for integration among spreadsheets and with other relational sources.Senbazuru allows users to search for relevant spreadsheets in a large corpus;probabilistically constructs a relational version of the data; and offers several relationaloperations over the resulting extracted data (including joins to other spreadsheet data) …,Proceedings of the VLDB Endowment,2013,25
Visualization-aware sampling for very large databases,Yongjoo Park; Michael Cafarella; Barzan Mozafari,Interactive visualizations are crucial in ad hoc data exploration and analysis. However; withthe growing number of massive datasets; generating visualizations in interactive timescalesis increasingly challenging. One approach for improving the speed of the visualization tool isvia data reduction in order to reduce the computational overhead; but at a potential cost invisualization accuracy. Common data reduction techniques; such as uniform and stratifiedsampling; do not exploit the fact that the sampled tuples will be transformed into avisualization for human consumption. We propose a visualization-aware sampling (VAS)that guarantees high quality visualizations with a small subset of the entire dataset. Wevalidate our method when applied to scatter and map plots for three common visualizationgoals: regression; density estimation; and clustering. The key to our sampling method's …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,20
Leveraging Noisy Lists for Social Feed Ranking.,Matthew Burgess; Alessandra Mazzia; Eytan Adar; Michael J Cafarella,Abstract Active users of social networks are subjected to extreme information overload; asthey tend to follow hundreds (or even thousands of other users). Aggregated social feeds onsites like Twitter are insufficient; showing superfluous content and not allowing users toseparate their topics of interest or place a priority on the content being pushed to them bytheir “friends.” The major social network platforms have begun to implement various featuresto help users organize their feeds; but these solutions require significant human effort tofunction properly. In practice; the burden is so high that most users do not adopt thesefeatures. We propose a system that seeks to help users find more relevant content on theirfeeds; but does not require explicit user input. Our system; BUTTER-WORTH; automaticallygenerates a set of “rankers” by identifying sub-communities of the user's social network …,ICWSM,2013,19
Open information extraction from the web,*,To implement open information extraction; a new extraction paradigm has been developedin which a system makes a single data-driven pass over a corpus of text; extracting a largeset of relational tuples without requiring any human input. Using training data; a Self-Supervised Learner employs a parser and heuristics to determine criteria that will be usedby an extraction classifier (or other ranking model) for evaluating the trustworthiness ofcandidate tuples that have been extracted from the corpus of text; by applying heuristics tothe corpus of text. The classifier retains tuples with a sufficiently high probability of beingtrustworthy. A redundancy-based assessor assigns a probability to each retained tuple toindicate a likelihood that the retained tuple is an actual instance of a relationship between aplurality of objects comprising the retained tuple. The retained tuples comprise an …,*,2015,18
Using web corpus statistics for program analysis,Chun-Hung Hsiao; Michael Cafarella; Satish Narayanasamy,Abstract Several program analysis tools-such as plagiarism detection and bug finding-relyon knowing a piece of code's relative semantic importance. For example; a plagiarismdetector should not bother reporting two programs that have an identical simple loop countertest; but should report programs that share more distinctive code. Traditional programanalysis techniques (eg; finding data and control dependencies) are useful; but do not sayhow surprising or common a line of code is. Natural language processing researchers haveencountered a similar problem and addressed it using an n-gram model of text frequency;derived from statistics computed over text corpora. We propose and compute an n-grammodel for programming languages; computed over a corpus of 2.8 million JavaScriptprograms we downloaded from the Web. In contrast to previous techniques; we describe …,ACM SIGPLAN Notices,2014,16
Structured Queries Over Web Text.,Michael J Cafarella; Oren Etzioni; Dan Suciu,Abstract The Web contains a vast amount of text that can only be queried using simplekeywords-in; documentsout search queries. But Web text often contains structured elements;such as hotel location and price pairs embedded in a set of hotel reviews. Queries thatprocess these structural text elements would be much more powerful than our currentdocument-centric queries. Of course; text does not contain metadata or a schema; making itunclear what a structured text query means precisely. In this paper we describe threepossible models for structured queries over text; each of which implies different querysemantics and user interaction.,IEEE Data Eng. Bull.,2006,16
Ringtail: Feature Selection For Easier Nowcasting.,Dolan Antenucci; Michael J Cafarella; Margaret Levenstein; Christopher Ré; Matthew Shapiro,ABSTRACT In recent years; social media “nowcasting”—the use of online user activity topredict various ongoing real-world social phenomena—has become a popular researchtopic; yet; this popularity has not led to widespread actual practice. We believe a majorobstacle to widespread adoption is the feature selection problem. Typical nowcastingsystems require the user to choose a set of relevant social media objects; which is difficult;time-consuming; and can imply a statistical background that users may not have. Wepropose Ringtail; which helps the user choose relevant social media signals. It takes asingle user input string (eg; unemployment) and yields a number of relevant signals the usercan use to build a nowcasting model. We evaluate Ringtail on six different topics using acorpus of almost 6 billion tweets; showing that features chosen by Ringtail in a wholly …,WebDB,2013,14
Long-tail vocabulary dictionary extraction from the web,Zhe Chen; Michael Cafarella; HV Jagadish,Abstract A dictionary---a set of instances belonging to the same conceptual class---is centralto information extraction and is a useful primitive for many applications; including query loganalysis and document categorization. Considerable work has focused on generatingaccurate dictionaries given a few example seeds; but methods to date cannot obtain long-tail (rare) items with high accuracy and recall. In this paper; we develop a novel method toconstruct high-quality dictionaries; especially for long-tail vocabularies; using just a few user-provided seeds for each topic. Our algorithm obtains long-tail (ie; rare) items by building andexecuting high-quality webpage-specific extractors. We use webpage-specific structural andtextual information to build more accurate per-page extractors in order to detect the long-tailitems from a single webpage. These webpage-specific extractors are obtained via a co …,Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,2016,13
Diagramflyer: A search engine for data-driven diagrams,Zhe Chen; Michael Cafarella; Eytan Adar,Abstract A large amount of data is available only through data-driven diagrams such as barcharts and scatterplots. These diagrams are stylized mixtures of graphics and text and arethe result of complicated data-centric production pipelines. Unfortunately; neither text norimage search engines exploit these diagram-specific properties; making it difficult for usersto find relevant diagrams in a large corpus. In response; we propose DiagramFlyer; a searchengine for finding data-driven diagrams on the web. By recovering the semantic roles ofdiagram components (eg; axes; labels; etc.); we provide faceted indexing and retrieval forvarious statistical diagrams. A unique feature of DiagramFlyer is that it is able to" expand"queries to include not only exactly matching diagrams; but also diagrams that are likely to berelated in terms of their production pipelines. We demonstrate the resulting search system …,Proceedings of the 24th International Conference on World Wide Web,2015,13
Link-prediction enhanced consensus clustering for complex networks,Matthew Burgess; Eytan Adar; Michael Cafarella,Many real networks that are collected or inferred from data are incomplete due to missingedges. Missing edges can be inherent to the dataset (Facebook friend links will never becomplete) or the result of sampling (one may only have access to a portion of the data). Theconsequence is that downstream analyses that “consume” the network will often yield lessaccurate results than if the edges were complete. Community detection algorithms; inparticular; often suffer when critical intra-community edges are missing. We propose a novelconsensus clustering algorithm to enhance community detection on incomplete networks.Our framework utilizes existing community detection algorithms that process networksimputed by our link prediction based sampling algorithm and merges their multiple partitionsinto a final consensus output. On average our method boosts performance of existing …,PloS one,2016,11
Neighbor-sensitive hashing,Yongjoo Park; Michael Cafarella; Barzan Mozafari,Abstract Approximate kNN (k-nearest neighbor) techniques using binary hash functions areamong the most commonly used approaches for overcoming the prohibitive cost ofperforming exact kNN queries. However; the success of these techniques largely dependson their hash functions' ability to distinguish kNN items; that is; the kNN items retrievedbased on data items' hashcodes; should include as many true kNN items as possible. Awidely-adopted principle for this process is to ensure that similar items are assigned to thesame hashcode so that the items with the hashcodes similar to a query's hashcode are likelyto be true neighbors. In this work; we abandon this heavily-utilized principle and pursue theopposite direction for generating more effective hash functions for kNN tasks. That is; we aimto increase the distance between similar items in the hashcode space; instead of …,Proceedings of the VLDB Endowment,2015,11
Methods for domain-independent information extraction from the web,Oren Etzioni; Michael Cafarella; Doug Downey; Ana Maria Popescu; Tal Shaked; Stephen Soderland; Daniel S Weld; Alexander Yates,Abstract Our KNOWITALL system aims to automate the tedious process of extracting largecollections of facts (eg; names of scientists or politicians) from the Web in an autonomous;domain-independent; and scalable manner. In its first major run; KNOWITALL extracted over50;000 facts with high precision; but suggested a challenge: How can we improveKNOWITALL's recall and extraction rate without sacrificing precision? This paper presentsthree distinct ways to address this challenge and evaluates their performance. RuleLearning learns domain-specific extraction rules. Subclass Extraction automaticallyidentifies sub-classes in order to boost recall. List Extraction locates lists of class instances;learns a" wrapper" for each list; and extracts elements of each list. Since each methodbootstraps from KNOWITALL's domain-independent methods; no hand-labeled training …,Proceedings-Nineteenth National Conference on Artificial Intelligence (AAAI-2004): Sixteenth Innovative Applications of Artificial Intelligence Conference (IAAI-2004),2004,11
HARE: Hardware accelerator for regular expressions,Vaibhav Gogte; Aasheesh Kolli; Michael J Cafarella; Loris D'Antoni; Thomas F Wenisch,Rapidly processing text data is critical for many technical and business applications.Traditional software-based tools for processing large text corpora use memory bandwidthinefficiently due to software overheads and thus fall far short of peak scan rates possible onmodern memory systems. Prior hardware designs generally target I/O rather than memorybandwidth. In this paper; we present HARE; a hardware accelerator for matching regularexpressions against large in-memory logs. HARE comprises a stall-free hardware pipelinethat scans input data at a fixed rate; examining multiple characters from a single input streamin parallel in a single accelerator clock cycle. We describe a 1GHz 32-character-wide HAREdesign targeting ASIC implementation that processes data at 32 GB/s-matching modernmemory bandwidths. This ASIC design outperforms software solutions by as much as two …,Microarchitecture (MICRO); 2016 49th Annual IEEE/ACM International Symposium on,2016,9
Minimizing remote accesses in mapreduce clusters,Prateek Tandon; Michael J Cafarella; Thomas F Wenisch,MapReduce; in particular Hadoop; is a popular framework for the distributed processing oflarge datasets on clusters of relatively inexpensive servers. Although Hadoop clusters arehighly scalable and ensure data availability in the face of server failures; their efficiency ispoor. We study data placement as a potential source of inefficiency. Despite networkingimprovements that have narrowed the performance gap between map tasks that accesslocal or remote data; we find that nodes servicing remote HDFS requests see significantslowdowns of collocated map tasks due to interference effects; whereas nodes making theserequests do not experience proportionate slowdowns. To reduce remote accesses; and thusavoid their destructive performance interference; we investigate an intelligent dataplacement policy we call'partitioned data placement'. We find that; in an unconstrained …,Parallel and Distributed Processing Symposium Workshops & PhD Forum (IPDPSW); 2013 IEEE 27th International,2013,9
Searching for statistical diagrams,Shirley Zhe Chen; Michael J Cafarella; Eytan Adar,Statistical; or data-driven; diagrams are an important method for communicating complexinformation. For many technical documents; the diagrams may be readers' only access to theraw data underlying the documents' conclusions. Unfortunately; finding diagrams online isvery difficult using current search systems. Standard text-based search will only retrieve thediagrams' enclosing documents. Web image search engines may retrieve some diagrams;but they generally work by examining textual content that surrounds images; thus missingout on many important signals of diagram content (Bhatia et al.; 2010; Carberry et al.; 2006).Even the text that is present in diagrams has meaning that is hugely dependent on theirgeometric positioning within the diagram's frame; a number in the caption means somethingquite different from the same number in the x-axis scale (Bertin; 1983).,Frontiers of Engineering; National Academy of Engineering,2011,9
Extracting and managing structured web data,Michael John Cafarella,Abstract The Web contains a large amount of structured data embedded in natural languagetext; two-dimensional tables; and other forms. This “Structured Web” of data is vast; messy;and diverse; it also promises new and compelling applications. Unfortunately; existing toolssuch as search engines and relational databases ignore Structured Web data entirely. Thisdissertation identifies four design criteria for a successful Structured Web managementsystem. Such systems are:(1) Extraction-Focused—They obtain structured data wherever itcan be found.(2) Domain-Independent—They are not tied to one particular topic area.(3)Domain-Scalable—They can effectively manage many domains simultaneously.(4)Computationally-Efficient—They can handle the Web's enormous size.,*,2009,9
Foofah: Transforming data by example,Zhongjun Jin; Michael R Anderson; Michael Cafarella; HV Jagadish,Abstract Data transformation is a critical first step in modern data analysis: before anyanalysis can be done; data from a variety of sources must be wrangled into a uniform formatthat is amenable to the intended analysis and analytical software package. This datatransformation task is tedious; time-consuming; and often requires programming skillsbeyond the expertise of data analysts. In this paper; we develop a technique to synthesizedata transformation programs by example; reducing this burden by allowing the analyst todescribe the transformation with a small input-output example pair; without being concernedwith the transformation steps required to get there. We implemented our technique in asystem; FOOFAH; that efficiently searches the space of possible data transformationoperations to generate a program that will perform the desired transformation. We …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,8
Extracting databases from dark data with deepdive,Ce Zhang; Jaeho Shin; Christopher Ré; Michael Cafarella; Feng Niu,Abstract DeepDive is a system for extracting relational databases from dark data: the massof text; tables; and images that are widely collected and stored but which cannot beexploited by standard relational tools. If the information in dark data---scientific papers; Webclassified ads; customer service notes; and so on---were instead in a relational database; itwould give analysts access to a massive and highly-valuable new set of" big data" to exploit.DeepDive is distinctive when compared to previous information extraction systems in itsability to obtain very high precision and recall at reasonable engineering cost; in a numberof applications; we have used DeepDive to create databases with accuracy that meets thatof human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance; materials science; genomics; paleontologists; law …,Proceedings of the 2016 International Conference on Management of Data,2016,8
Database learning: Toward a database that becomes smarter every time,Yongjoo Park; Ahmad Shahab Tajik; Michael Cafarella; Barzan Mozafari,Abstract In today's databases; previous query answers rarely benefit answering futurequeries. For the first time; to the best of our knowledge; we change this paradigm in anapproximate query processing (AQP) context. We make the following observation: theanswer to each query reveals some degree of knowledge about the answer to another querybecause their answers stem from the same underlying distribution that has produced theentire dataset. Exploiting and refining this knowledge should allow us to answer queriesmore analytically; rather than by reading enormous amounts of raw data. Also; processingmore queries should continuously enhance our knowledge of the underlying distribution;and hence lead to increasingly faster response times for future queries. We call this novelidea---learning from past query answers---Database Learning. We exploit the principle of …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,7
Input selection for fast feature engineering,Michael R Anderson; Michael Cafarella,The application of machine learning to large datasets has become a vital component ofmany important and sophisticated software systems built today. Such trained systems areoften based on supervised learning tasks that require features; signals extracted from thedata that distill complicated raw data objects into a small number of salient values. A trainedsystem's success depends substantially on the quality of its features. Unfortunately; featureengineering-the process of writing code that takes raw data objects as input and outputsfeature vectors suitable for a machine learning algorithm-is a tedious; time-consumingexperience. Because “big data” inputs are so diverse; feature engineering is often a trial-and-error process requiring many small; iterative code changes. Because the inputs are so large;each code change can involve a time-consuming data processing task (over each page …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,7
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Christopher Ré; Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan,Abstract Machine learning seems to be eating the world with a new breed of high-value data-driven applications in image analysis; search; voice recognition; mobile; and officeproductivity products. To paraphrase Mike Stonebraker; machine learning is no longer azero-billion-dollar business. As the home of high-value; data-driven applications for overfour decades; a natural question for database researchers to ask is: what role should thedatabase community play in these new data-driven machine-learning-based applications?,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,7
An integrated development environment for faster feature engineering,Michael R Anderson; Michael Cafarella; Yixing Jiang; Guan Wang; Bochun Zhang,Abstract The application of machine learning to large datasets has become a corecomponent of many important and exciting software systems being built today. The extremevalue in these trained systems is tempered; however; by the difficulty of constructing them.As shown by the experience of Google; Netflix; IBM; and many others; a critical problem inbuilding trained systems is that of feature engineering. High-quality machine learningfeatures are crucial for the system's performance but are difficult and time-consuming forengineers to develop. Data-centric developer tools that improve the productivity of featureengineers will thus likely have a large impact on an important area of work. We have built ademonstration integrated development environment for feature engineers. It accelerates oneparticular step in the feature engineering development cycle: evaluating the effectiveness …,Proceedings of the VLDB Endowment,2014,7
Web data management,Michael J Cafarella; Alon Y Halevy,Abstract Web Data Management (or WDM) refers to a body of work concerned withleveraging the large collections of structured data that can be extracted from the Web. Overthe past few years; several research and commercial efforts have explored these collectionsof data with the goal of improving Web search and developing mechanisms for surfacingdifferent kinds of search answers. This work has leveraged (1) collections of structured datasuch as HTML tables; lists and forms;(2) recent ontologies and knowledge bases created bycrowd-sourcing; such as Wikipedia and its derivatives; DBPedia; YAGO and Freebase; and(3) the collection of text documents from the Web; from which facts could be extracted in adomain-independent fashion. The promise of this line of work is based on the observationthat new kinds of results can be obtained by leveraging a huge collection of …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,7
About hadoop,Doug Cutting; M Cafarella,*,*,2005,7
Mindtagger: a demonstration of data labeling in knowledge base construction,Jaeho Shin; Christopher Ré; Michael Cafarella,Abstract End-to-end knowledge base construction systems using statistical inference areenabling more people to automatically extract high-quality domain-specific information fromunstructured data. As a result of deploying DeepDive framework across several domains; wefound new challenges in debugging and improving such end-to-end systems to constructhigh-quality knowledge bases. DeepDive has an iterative development cycle in which usersimprove the data. To help our users; we needed to develop principles for analyzing thesystem's error as well as provide tooling for inspecting and labeling various data products ofthe system. We created guidelines for error analysis modeled after our colleagues' bestpractices; in which data labeling plays a critical role in every step of the analysis. To enablemore productive and systematic data labeling; we created Mindtagger; a versatile tool that …,Proceedings of the VLDB Endowment,2015,6
Ringtail: a generalized nowcasting system,Dolan Antenucci; Erdong Li; Shaobo Liu; Bochun Zhang; Michael J Cafarella; Christopher Ré,Abstract Social media nowcasting--using online user activity to describe real-worldphenomena--is an active area of research to supplement more traditional and costly datacollection methods such as phone surveys. Given the potential impact of such research; wewould expect general-purpose nowcasting systems to quickly become a standard toolamong noncomputer scientists; yet it has largely remained a research topic. We believe amajor obstacle to widespread adoption is the nowcasting feature selection problem. Typicalnowcasting systems require the user to choose a handful of social media objects from a poolof billions of potential candidates; which can be a time-consuming and error-prone process.We have built RINGTAIL; a nowcasting system that helps the user by automaticallysuggesting high-quality signals. We demonstrate that RINGTALL can make nowcasting …,Proceedings of the VLDB Endowment,2013,6
DeepDive: declarative knowledge base construction,Ce Zhang; Christopher Ré; Michael Cafarella; Christopher De Sa; Alex Ratner; Jaeho Shin; Feiran Wang; Sen Wu,Abstract The dark data extraction or knowledge base construction (KBC) problem is topopulate a relational database with information from unstructured data sources; such asemails; webpages; and PDFs. KBC is a long-standing problem in industry and research thatencompasses problems of data extraction; cleaning; and integration. We describeDeepDive; a system that combines database and machine learning ideas to help to developKBC systems. The key idea in DeepDive is to frame traditional extract-transform-load (ETL)style data management problems as a single large statistical inference task that isdeclaratively defined by the user. DeepDive leverages the effectiveness and efficiency ofstatistical inference and machine learning for difficult extraction tasks; whereas not requiringusers to directly write any probabilistic inference algorithms. Instead; domain experts …,Communications of the ACM,2017,5
Hawk: Hardware support for unstructured log processing,Prateek Tandon; Faissal M Sleiman; Michael J Cafarella; Thomas F Wenisch,Rapidly processing high-velocity text data is critical for many technical and businessapplications. Widely used software solutions for processing these large text corpora targetdisk-resident data and rely on pre-computed indexes and large clusters to achieve highperformance. However; greater capacity and falling costs are enabling a shift to RAM-resident data sets. The enormous bandwidth of RAM can facilitate scan operations that arecompetitive with pre-computed indexes for interactive; ad-hoc queries. However; softwareapproaches for processing these large text corpora fall far short of saturating availablebandwidth and meeting peak scan rates possible on modern memory systems. In this paper;we present HAWK; a hardware accelerator for ad hoc queries against large in-memory logs.HAWK comprises a stall-free hardware pipeline that scans input data at a constant rate …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,4
A declarative query processing system for nowcasting,Dolan Antenucci; Michael R Anderson; Michael Cafarella,Abstract Nowcasting is the practice of using social media data to quantify ongoing real-worldphenomena. It has been used by researchers to measure flu activity; unemploymentbehavior; and more. However; the typical nowcasting workflow requires either slow andtedious manual searching of relevant social media messages or automated statisticalapproaches that are prone to spurious and low-quality results. In this paper; we propose amethod for declaratively specifying a nowcasting model; this method involves processing auser query over a very large social media database; which can take hours. Due to thehuman-in-the-loop nature of constructing nowcasting models; slow runtimes place anextreme burden on the user. Thus we also propose a novel set of query optimizationtechniques; which allow users to quickly construct nowcasting models over very large …,Proceedings of the VLDB Endowment,2016,3
A query system for social media signals,Dolan Antenucci; Michael R Anderson; Penghua Zhao; Michael Cafarella,Social media nowcasting; the process of estimating real-world phenomena from socialmedia data; has grown in popularity over the last several years as an alternative totraditional data collection methods like phone surveys. Unfortunately; current nowcastingmethods depend on pre-existing; traditionally collected survey data as an aid to sift throughthe huge number of signals that can be derived from social media. This dependenceseverely limits the applicability of current nowcasting techniques. If we could remove thisneed for conventional data; social media signals could describe a much wider range oftarget phenomena. We have built a nowcasting querying system that estimates real-worldphenomena without requiring any conventional data; relying instead upon an interactiveexploration with users. Specifically; our system exploits a user-provided multi-part query …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,3
DBExplorer: Exploratory Search in Databases.,Manish Singh; Michael J Cafarella; HV Jagadish,ABSTRACT A traditional relational database can evaluate complex queries but requiresusers to precisely express their information need. But users often do not know whatinformation is available in a database; and hence cannot correctly express their informationneed. Traditional databases do not provide convenient means for users to gain familiaritywith the data. In this paper; we study the problem of exploratory search; which a user maywish to perform to get an understanding of the data set. We note that users often have somedecisions already made; so what they need is not an overall database summary; but rather asummary “in context” of the relevant portion of the database. Towards this end; we devise anovel data summarization technique called the Conditional Attribute Dependency (CAD)View; which shows the conditional dependencies between attribute values conditioned …,EDBT,2016,3
Spreadsheet Property Detection With Rule-assisted Active Learning,Zhe Chen; Sasha Dadiomov; Richard Wesley; Gang Xiao; Daniel Cory; Michael Cafarella; Jock Mackinlay,Abstract Spreadsheets are a critical and widely-used data management tool. Convertingspreadsheet data into relational tables would bring benefits to a number of fields; includingpublic policy; public health; and economics. Research to date has focused on designingdomain-specific languages to describe transformation processes or automatically convertinga specific type of spreadsheets. To handle a larger variety of spreadsheets; we have toidentify various spreadsheet properties; which correspond to a series of transformationprograms that contribute towards a general framework that converts spreadsheets torelational tables. In this paper; we focus on the problem of spreadsheet property detection.We propose a hybrid approach of building a variety of spreadsheet property detectors toreduce the amount of required human labeling effort. Our approach integrates an active …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,2
A semiautomatic approach for accurate and low-effort spreadsheet data extraction,Zhe Chen; Michael Cafarella,ABSTRACT Spreadsheets contain valuable data on many topics; but they are difficult tointegrate with other sources. Converting spreadsheet data to the relational model wouldallow relational integration tools to be used; but using manual methods to do this requireslarge amounts of work for each integration candidate. Automatic data extraction would beuseful but it is very challenging: spreadsheet designs generally requires human knowledgeto understand the metadata being described. Even if it is possible to obtain this metadatainformation automatically; a single mistake can yield an output relation with a huge numberof incorrect tuples. We propose a two-phase semiautomatic system that extracts accuraterelational metadata while minimizing user effort. Based on conditional random fields (CRFs);our system enables downstream spreadsheet integration applications. First; the automatic …,Ann Arbor,2014,2
Foofah: a programming-by-example system for synthesizing data transformation programs,Zhongjun Jin; Michael R Anderson; Michael Cafarella; HV Jagadish,Abstract Advancements in new data analysis and visualization technologies have resulted inwide applicability of data-driven decision making. However; raw data from various sourcesmust be wrangled into a suitable form before they are processed by the downstream datatools. People traditionally write data transformation programs to automate this process; andsuch work is cumbersome and tedious. We built a system called FOOFAH for helping theuser easily synthesize a desired data transformation program. Our system minimizes theuser's effort by only asking for a small illustrative example comprised of the raw input dataand the target transformed output; FOOFAH then synthesizes a program that can perform thedesired data transformation. This demonstration showcases how the user can applyFOOFAH to real-world data transformation tasks.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
DQBarge: Improving Data-Quality Tradeoffs in Large-Scale Internet Services.,Michael Chow; Kaushik Veeraraghavan; Michael J Cafarella; Jason Flinn,Abstract Modern Internet services often involve hundreds of distinct software componentscooperating to handle a single user request. Each component must balance the competinggoals of minimizing service response time and maximizing the quality of the serviceprovided. This leads to low-level components making data-quality tradeoffs; which we defineto be explicit decisions to return lowerfidelity data in order to improve response time orminimize resource usage. We first perform a comprehensive study of low-level data-qualitytradeoffs at Facebook. We find that such tradeoffs are widespread. We also find that existingdata-quality tradeoffs are often suboptimal because the low-level components making thetradeoffs lack global knowledge that could enable better decisions. Finally; we find that mosttradeoffs are reactive; rather than proactive; and so waste resources and fail to mitigate …,OSDI,2016,1
Detecting at least one predetermined pattern in stream of symbols,*,An apparatus comprises pattern matching circuitry for detecting instances of at least onepredetermined pattern of symbols within a subject stream of symbols. Encoding circuitry isprovided for generating an encoded stream of symbols from an input stream of symbols;where the encoding circuitry maps a number of consecutive repetitions of a same pattern ofone or more symbols detected within the input stream to a single instance of a symbol of theencoded stream and a corresponding repetition indicator indicative of the number ofconsecutive repetitions. Control circuitry controls the pattern matching circuitry to process theencoded stream of symbols generated by the encoding circuitry as the subject stream.,*,2016,1
Runtime Support for Human-in-the-Loop Feature Engineering System.,Michael R Anderson; Dolan Antenucci; Michael J Cafarella,Abstract A machine learning system is only as good as its features; the representativeproperties of a phenomenon that are used as input to a machine learning algorithm.Developing features; or feature engineering; can be a lengthy; human-in-the-loop process;requiring many time-consuming cycles of writing feature code; extracting features from rawdata; and model training. However; many opportunities exist to improve this workflow; suchas assisting feature code development and speeding up feature extraction. In this paper; wediscuss two projects that take different approaches to accelerate feature engineering;allowing the engineer to spend more time doing what humans do best: applying domaininsight to engineer high-impact features. ZOMBIE is a general-purpose system that reducesthe amount of time needed to extract features. RACCOONDB is a system that helps users …,IEEE Data Eng. Bull.,2016,1
Reducing MapReduce Abstraction Costs for Text-centric Applications,Chun-Hung Hsiao; Michael Cafarella; Satish Narayanasamy,The MapReduce framework has become widely popular for programming large clusters;even though MapReduce jobs may use underlying resources relatively inefficiently. Therehas been substantial research in improving MapReduce performance for applications thatwere inspired by relational database queries; but almost none for text-centric applications;including inverted index construction; processing large log files; and so on. We identify twosimple optimizations to improve MapReduce performance on text-centric tasks: frequency-buffering and spill-matcher. The former approach improves buffer efficiency for intermediatemap outputs by identifying frequent keys; effectively shrinking the amount of work that theshuffle phase must perform. Spill-matcher is a runtime controller that improvesparallelization of MapReduce framework background tasks. Together; our two …,Parallel Processing (ICPP); 2014 43rd International Conference on,2014,1
How best to build web-scale data managers?,Philip A Bernstein; Daniel J Abadi; Michael J Cafarella; Joseph M Hellerstein; Donald Kossmann; Samuel Madden,1. PANEL OVERVIEW Many of the largest database-driven web sites use custom web- scaledata managers (WDMs). On the surface; these WDMs are being applied to problems that arewell-suited for relational database systems. Some examples are the following: • Map-Reduce[5]; Hadoop [7]; and Dryad [9] are used to process queries on large data sets using sequentialscan and aggregation. Hive [8] is a data warehouse built on Hadoop. • Google's Bigtable [3] isused to store a replicated table of rows of semi-structured data. • Amazon's Dynamo [6] is usedto store partitioned; replicated databases of key-value pairs. Cassandra [2] is similar. • Objectcaching systems are used instead of a persistent store; such as memcached [10]; Oracle'sCoherence; and Microsoft's Velocity project … These WDMs have challenging requirementsthat are not met by current relational database products. They need to scale out to …,Proceedings of the VLDB Endowment,2009,1
BE: a search engine for NLP research,Michael J Cafarella; Oren Etzioni,Abstract Many modern natural language-processing applications utilize search engines tolocate large numbers of Web documents or to compute statistics over the Web corpus. YetWeb search engines are designed and optimized for simple human queries---they are notwell suited to support such applications. As a result; these applications are forced to issuemillions of successive queries resulting in unnecessary search engine load and in slowapplications with limited scalability.,Proceedings of the 2nd International Workshop on Web as Corpus,2006,1
Link-Prediction Enhanced Consensus Clustering for Complex Networks (Open Access),Matthew Burgess; Eytan Adar; Michael Cafarella,Abstract: Many real networks that are collected or inferred from data are incomplete due tomissing edges. Missing edges can be inherent to the dataset (Facebook friend links willnever be complete) or the result of sampling (one may only have access to a portion of thedata). The consequence is that downstream analyses that consume the network will oftenyield less accurate results than if the edges were complete. Community detection algorithms;in particular; often suffer when critical intra-community edges are missing. We propose anovel consensus clustering algorithm to enhance community detection on incompletenetworks. Our framework utilizes existing community detection algorithms that processnetworks imputed by our link prediction based sampling algorithm and merges their multiplepartitions into a final consensus output. On average our method boosts performance of …,*,2016,*
Dark Data: Are we solving the right problems?,Michael Cafarella; Ihab F Ilyas; Marcel Kornacker; Tim Kraska; Christopher Ré,With the increasing urge of the enterprises to ingest as much data as they can in what'scommonly referred to as “Data Lakes”; the new environment presents serious challenges totraditional ETL models and to building analytic layers on top of well-understood globalschema. With the recent development of multiple technologies to support this “load-first”paradigm; even traditional enterprises have fairly large HDFS-based data lakes now. Theyhave even had them long enough that their first generation IT projects delivered on some;but not all; of the promise of integrating their enterprise's data assets. In short; we movedfrom no data to Dark data. Dark data is what enterprises might have in their possession;without the ability to access it or with limited awareness of what this data represents. Inparticular; business-critical information might still remain out of reach. This panel is about …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
Querying input data,*,A hardware accelerator 2 for performing queries into; for example; an indexed text log files isformed of plurality of hardware execution units (text engines) 4; each executing a partialquery program upon the same full set of input data. These partial query programs mayswitch between different query algorithms on up to a per-character basis. The sequence ofdata when loaded into a buffer memory 16 for querying may be searched for delimiters asthe data is loaded. The hardware execution units may support a number match programinstruction which serves to identify a numeric variable; and to determine a value of thatnumeric variable located at a variable position within a sequence of characters beingqueried.,*,2016,*
Querying input data,*,A hardware accelerator 2 for performing queries into; for example; an indexed text log files isformed of plurality of hardware execution units (text engines) 4; each executing a partialquery program upon the same full set of input data. These partial query programs mayswitch between different query algorithms on up to a per-character basis. The sequence ofdata when loaded into a buffer memory 16 for querying may be searched for delimiters asthe data is loaded. The hardware execution units may support a number match programinstruction which serves to identify a numeric variable; and to determine a value of thatnumeric variable located at a variable position within a sequence of characters beingqueried.,*,2016,*
Synthesizing Data Programs,Michael Cafarella,1. ABSTRACT At least two important tasks in modern data management exist outsidetraditional database models and query languages: data transformation and featureprograms. Data transformation is the informal preprocessing code that transforms raw datainto a dataset that is appropriate for import into a relational database for deeper analysis.Feature programs transform raw data into a compact piece of training data that is suitable foruse in a statistical training procedure. These two tasks are driven by two applications thatare intellectually exciting; economically important; and which rightfully garner substantialattention in the database community: data analytics and machine learning. Unfortunately; todate; both data transformation and feature programs have largely existed in an ad hocnetherworld of Python programs and shell scripts. Consider a stock trader engaged in an …,Ann Arbor,2015,*
Using Web Corpus Statistics for Program Analysis,Satish Narayanasamy Chun-Hung Hsiao; Michael Cafarella,*,OOPSLA,2014,*
An Integrated Development Environment for Faster Feature Engineering,and Bochun Zhang Michael R. Anderson; Michael Cafarella; Yixing Jiang; Guan Wang,*,VLDB Demo,2014,*
Open information extraction,*,A system for identifying relational tuples is provided. The system extracts a relation phrasefrom a sentence by identifying a verb in the sentence and then identifying a relation phraseof the sentence as a phrase in the sentence starting with the identified verb that satisfiesboth a syntactic constraint and a lexical constraint. The system also identifies arguments fora relation phrase. To extract the arguments; the system applies a left-argument-left-boundclassifier; a left-argument-right-bound classifier; and a right-argument-right-bound classifierto identify a left argument and right argument for the relation phrase such that the leftargument; the relation phrase; and the right argument form a relational tuple.,*,2014,*
KNOWITNOW,Michael J Cafarella; Doug Downey; Stephen Soderland; Oren Etzioni,Abstract Numerous NLP applications rely on search-engine queries; both to extractinformation from and to compute statistics over the Web corpus. But search engines oftenlimit the number of available queries. As a result; query-intensive NLP applications such asInformation Extraction (IE) distribute their query load over several days; making IE a slow;offline process. This paper introduces a novel architecture for IE that obviates queries tocommercial search engines. The architecture is embodied in a system called KNOWITNOWthat performs high-precision IE in minutes instead of days. We compare KNOWITNOWexperimentally with the previouslypublished KNOWITALL system; and quantify the tradeoffbetween recall and speed. KNOWITNOW's extraction rate is two to three orders ofmagnitude higher than KNOWITALL's.© 2005 Association for Computational Linguistics.,Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing; HLT/EMNLP 2005; Co-located with the 2005 Document Understanding Conference; DUC and the 9th International Workshop on Parsing Technologies; IWPT,2005,*
Statistical Learning of ISP Peering Policies,Michael Cafarella; Daniel Lowd,Abstract Spring et al's research on path inflation uses a large number of traceroute probes todeduce facts about ISP policies [3]. They are able to learn an ISP's network topology; itsindividual router policies; and the overall ISP peering strategy. However; the authors useonly very basic heuristics to make deductions from the data. Also; collecting the necessarytraceroutes is a substantial burden. Finally; a relationship between two ISPs is characterizedonly as being in one of several categories. We use a number of different machine learningtechniques to make similar deductions. We believe these techniques are well-grounded instatistics. They enable us to describe a peering relationship with much greater precisionthan the commonly-considered ISP policies.,*,2004,*
Predicate Optimization for a Visual Analytics Database,Michael R Anderson; Michael Cafarella; Thomas F Wenisch; German Ros,ABSTRACT Querying the content of images; video; and other non-textual data sourcesrequires expensive content extraction methods. Modern extraction techniques are based ondeep convolutional neural networks (CNNs) and can classify objects within images withastounding accuracy. Unfortunately; these methods are slow; needing several millisecondsper image using modern GPUs. The cost of content-based queries over a huge video corpusis prohibitive. A promising approach to reduce the runtime cost of queries of visual content isto use a hierarchical model; such as a cascade; where simple cases are handled by aninexpensive classifier. Prior work has sought to design cascades that optimize thecomputational cost of inference by; for example; using smaller CNNs. However; we observethat there are critical factors besides the inference time that dramatically impact the …,*,*,*
Data Engineering,Aditya Parameswarany; Akash Das Sarma; Vipul Venkataramani; Olga Papaemmanouil; Yanlei Diao; Kyriaki Dimitriadou; Liping Peng; Philipp Eichmann; Emanuel Zgraggen; Zheguang Zhao; Carsten Binnig; Tim Kraska; Michael R Anderson; Dolan Antenucci; Michael Cafarella,Abstract As one of the successful forms of using Wisdom of Crowd; crowdsourcing; has beenwidely used for many human intrinsic tasks; such as image labeling; natural languageunderstanding; market predication and opinion mining. Meanwhile; with advances inpervasive technology; mobile devices; such as mobile phones; tablets; and PDA; havebecome extremely popular. These mobile devices can work as sensors to collect varioustypes of data; such as pictures; videos; audios and texts. Therefore; in crowdsourcing; arequester can unitize power of mobile devices and their location information to ask for datarelated a specific location; subsequently; the mobile users who would like to perform the taskwill travel to the target location and collect the data (videos; audios; or pictures); which isthen sent to the requester. This type of crowdsourcing is called spatial crowdsourcing …,*,*,*
2013 IEEE International Symposium on Parallel & Distributed Processing; Workshops and Phd Forum (IPDPSW),Vincent Boyer; Didier El Baz,In the last decade; Graphics Processing Units (GPUs) have gained an increasing popularityas accelerators for High Performance Computing (HPC) applications. Recent GPUs are notonly powerful graphics engines but also highly threaded parallel computing processors thatcan achieve sustainable speedup as compared with CPUs. In this context; researchers try toexploit the capability of this architecture...,*,*,*
Faster Feature Engineering by Approximate Evaluation,Michael R Anderson; Michael Cafarella,ABSTRACT The application of machine learning to large datasets has become a vitalcomponent of many important and sophisticated software systems built today. Such trainedsystems are often based on supervised learning tasks that require features; or extractedsignals that distill complicated raw data objects into a small number of salient values. Atrained system's success depends on the quality of its features. Unfortunately; featureengineering—writing code that turns raw data objects into feature vectors suitable for amachine learning algorithm—is tedious and time-consuming. Because “big data” inputs areso diverse; feature engineering is a trialand-error process with many small; iterative codechanges. Because the inputs are so large; each code change can involve time-consumingdata processing (over each page in a Web crawl; for example). We introduce Zombie; a …,*,*,*
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan; Christopher Ré,Machine learning seems to be eating the world with a new breed of high-value data-drivenapplications in image analysis; search; voice recognition; mobile; and office productivityproducts. To paraphrase Mike Stonebraker; machine learning is no longer a zero-billion-dollar business. As the home of high-value; data-driven applications for over four decades; anatural question for database researchers to ask is: what role should the databasecommunity play in these new datadriven machine-learning-based applications? The last fewyears have seen increasing crossover between database research and machine learning.But is this crossover a wise choice for database research? What are the opportunities andthe costs of this approach to industry; to the future of database research; and to academics?Do database researchers have something to contribute to this trend? These two areas …,*,*,*
Web Data,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang WebTables,Since the previous edition of this collection; the World Wide Web has unequivocally laid anylingering questions regarding its longevity and global impact to rest. Several multi-Billion-user services including Google and Facebook have become central to modern life in the firstworld; while Internet-and Web-related technology has permeated both business andpersonal interactions. The Web is undoubtedly here to stayat least for the foreseeable future.Web data systems bring a new set of challenges; including high scale; data heterogeneity;and a complex and evolving set of user interaction modes. Classical relational databasesystem designs did not have the Web workload in mind; and are not the technology ofchoice in this context. Rather; Web data management requires a melange of techniquesspanning information retrieval; database internals; data integration; and distributed …,*,*,*
Research Philosophy,Michael J Cafarella,*,small,*,*
Ullas Nambiar (IBM India Research Lab; New Delhi; India); Cochair Zaiqing Nie (Microsoft Research Asia; Beijing; PR China); Cochair,Alon Halevy; Kevin Chen-Chuan Chang; Subbarao Kambhampati; Avigdor Gal; Andrew McCallum; Biplav Srivastava; Bing Liu; Craig Knoblock; Chen Li; Felix Naumann; Ganesh Ramakrishnan; Gautam Das; Hasan Davulcu; Ji-Rong Wen; Kamal Karlapalem; Louiqa Raschid; Michael Cafarella; Misha Bilenko; Mong Li Lee; Nicholas Kushmerick; Robert Grossman; Steven Minton; Thomas Y Lee; Vanja Josifovski; Weiyi Meng; William Cohen,Alon Halevy (Google Inc. Mountain View; California; USA) Kevin Chen-Chuan Chang (Universityof Illinois at Urbana-Champaign; Illinois; USA) Subbarao Kambhampati (Arizona StateUniversity; Tempe; Arizona; USA) … Avigdor Gal (Technion – Israel Institute of Technology)Andrew McCallum (University of Massachusetts Amherst; USA) Biplav Srivastava (IBM IndiaResearch Lab) Bing Liu (University of Illinois at Chicago; USA) Craig Knoblock (University ofSouthern California; USA) Chen Li (University of California; Irvine; USA) Felix Naumann (HassoPlattner Institut; Postdam; Germany) Ganesh Ramakrishnan (IBM India Research Lab) GautamDas (University of Texas; Arlington; USA) Hasan Davulcu (Arizona State University; USA) Ji-RongWen (Microsoft Research Asia) Kamal Karlapalem (IIIT – Hyderabad; India) Louiqa Raschid(University of Maryland College Park; USA) Michael Cafarella (University of Washington …,*,*,*
How Best to Build Web-Scale Data Managers? A Panel Discussion,Philip A Bernstein; Daniel J Abadi; Michael J Cafarella; Joseph M Hellerstein; Donald Kossmann,Many of the largest database-driven web sites use custom webscale data managers(WDMs). On the surface; these WDMs are being applied to problems that are well-suited forrelational database systems. Some examples are the following:• Map-Reduce [5]; Hadoop[7]; and Dryad [9] are used to process queries on large data sets using sequential scan andaggregation. Hive [8] is a data warehouse built on Hadoop.• Google's Bigtable [3] is used tostore a replicated table of rows of semi-structured data.• Amazon's Dynamo [6] is used tostore partitioned; replicated databases of key-value pairs. Cassandra [2] is similar.• Objectcaching systems are used instead of a persistent store; such as memcached [10]; Oracle'sCoherence; and Microsoft's Velocity project. These WDMs have challenging requirementsthat are not met by current relational database products. They need to scale out to …,*,*,*
