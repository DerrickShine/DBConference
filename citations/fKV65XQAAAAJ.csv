Crowder: Crowdsourcing entity resolution,Jiannan Wang; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract Entity resolution is central to data integration and data cleaning. Algorithmicapproaches have been improving in quality; but remain far from perfect. Crowdsourcingplatforms offer a more accurate but expensive (and slow) way to bring human insight into theprocess. Previous work has proposed batching verification tasks for presentation to humanworkers but even with batching; a human-only approach is infeasible for data sets of evenmoderate size; due to the large numbers of matches to be tested. Instead; we propose ahybrid human-machine approach in which machines are used to do an initial; coarse passover all the data; and people are used to verify only the most likely matching pairs. We showthat for such a hybrid system; generating the minimum number of verification tasks of a givensize is NP-Hard; but we develop a novel two-tiered heuristic approach for creating …,Proceedings of the VLDB Endowment,2012,333
Pass-join: A partition-based method for similarity joins,Guoliang Li; Dong Deng; Jiannan Wang; Jianhua Feng,Abstract As an essential operation in data cleaning; the similarity join has attractedconsiderable attention from the database community. In this paper; we study string similarityjoins with edit-distance constraints; which find similar string pairs from two large sets ofstrings whose edit distance is within a given threshold. Existing algorithms are efficient eitherfor short strings or for long strings; and there is no algorithm that can efficiently andadaptively support both short strings and long strings. To address this problem; we proposea partition-based method called Pass-Join. Pass-Join partitions a string into a set ofsegments and creates inverted indices for the segments. Then for each string; Pass-Joinselects some of its substrings and uses the selected substrings to find candidate pairs usingthe inverted indices. We devise efficient techniques to select the substrings and prove that …,Proceedings of the VLDB Endowment,2011,140
Trie-join: Efficient trie-based string similarity joins with edit-distance constraints,Jiannan Wang; Jianhua Feng; Guoliang Li,Abstract A string similarity join finds similar pairs between two collections of strings. It is anessential operation in many applications; such as data integration and cleaning; and hasattracted significant attention recently. In this paper; we study string similarity joins with edit-distance constraints. Existing methods usually employ a filter-and-refine framework andhave the following disadvantages:(1) They are inefficient for the data sets with short strings(the average string length is no larger than 30);(2) They involve large indexes;(3) They areexpensive to support dynamic update of data sets. To address these problems; we proposea novel framework called trie-join; which can generate results efficiently with small indexes.We use a trie structure to index the strings and utilize the trie structure to efficiently find thesimilar string pairs based on subtrie pruning. We devise efficient trie-join algorithms and …,Proceedings of the VLDB Endowment,2010,140
Leveraging transitive relations for crowdsourced joins,Jiannan Wang; Guoliang Li; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract The development of crowdsourced query processing systems has recently attracteda significant attention in the database community. A variety of crowdsourced queries havebeen investigated. In this paper; we focus on the crowdsourced join query which aims toutilize humans to find all pairs of matching objects from two collections. As a human-onlysolution is expensive; we adopt a hybrid human-machine approach which first usesmachines to generate a candidate set of matching pairs; and then asks humans to label thepairs in the candidate set as either matching or non-matching. Given the candidate pairs;existing approaches will publish all pairs for verification to a crowdsourcing platform.However; they neglect the fact that the pairs satisfy transitive relations. As an example; if o 1matches with o 2; and o 2 matches with o 3; then we can deduce that o 1 matches with o 3 …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,131
Can we beat the prefix filtering?: an adaptive framework for similarity join and search,Jiannan Wang; Guoliang Li; Jianhua Feng,Abstract As two important operations in data cleaning; similarity join and similarity searchhave attracted much attention recently. Existing methods to support similarity join usuallyadopt a prefix-filtering-based framework. They select a prefix of each object and pruneobject pairs whose prefixes have no overlap. We have an observation that prefix lengthshave significant effect on the performance. Different prefix lengths lead to significantlydifferent performance; and prefix filtering does not always achieve high performance. Toaddress this problem; in this paper we propose an adaptive framework to support similarityjoin. We propose a cost model to judiciously select an appropriate prefix for each object. Toefficiently select prefixes; we devise effective indexes. We extend our method to supportsimilarity search. Experimental results show that our framework beats the prefix-filtering …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,129
Fast-join: An efficient method for fuzzy token matching based string similarity join,Jiannan Wang; Guoliang Li; Jianhua Fe,String similarity join that finds similar string pairs between two string sets is an essentialoperation in many applications; and has attracted significant attention recently in thedatabase community. A significant challenge in similarity join is to implement an effectivefuzzy match operation to find all similar string pairs which may not match exactly. In thispaper; we propose a new similarity metrics; called “fuzzy token matching based similarity”;which extends token-based similarity functions (eg; Jaccard similarity and Cosine similarity)by allowing fuzzy match between two tokens. We study the problem of similarity join usingthis new similarity metrics and present a signature-based method to address this problem.We propose new signature schemes and develop effective pruning techniques to improvethe performance. Experimental results show that our approach achieves high efficiency …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,94
Massjoin: A mapreduce-based method for scalable string similarity joins,Dong Deng; Guoliang Li; Shuang Hao; Jiannan Wang; Jianhua Feng,String similarity join is an essential operation in data integration. The era of big data calls forscalable algorithms to support large-scale string similarity joins. In this paper; we studyscalable string similarity joins using MapReduce. We propose a MapReduce-basedframework; called MASSJOIN; which supports both set-based similarity functions andcharacter-based similarity functions. We extend the existing partition-based signaturescheme to support set-based similarity functions. We utilize the signatures to generate key-value pairs. To reduce the transmission cost; we merge key-value pairs to significantlyreduce the number of key-value pairs; from cubic to linear complexity; while not sacrificingthe pruning power. To improve the performance; we incorporate “light-weight” filter units intothe key-value pairs which can be utilized to prune large number of dissimilar pairs without …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,68
Entity matching: How similar is similar,Jiannan Wang; Guoliang Li; Jeffrey Xu Yu; Jianhua Feng,Abstract Entity matching that finds records referring to the same entity is an importantoperation in data cleaning and integration. Existing studies usually use a given similarityfunction to quantify the similarity of records; and focus on devising index structures andalgorithms for efficient entity matching. However it is a big challenge to define" how similar issimilar" for real applications; since it is rather hard to automatically select appropriatesimilarity functions. In this paper we attempt to address this problem. As there are a largenumber of similarity functions; and even worse thresholds may have infinite values; it israther expensive to find appropriate similarity functions and thresholds. Fortunately; we havean observation that different similarity functions and thresholds have redundancy; and wehave an opportunity to prune inappropriate similarity functions. To this end; we propose …,Proceedings of the VLDB Endowment,2011,64
A Sample-and-Clean Framework for Fast and Accurate Query Processing on Dirty Data,Jiannan Wang; Sanjay Krishnan; Michael J Franklin; Ken Goldberg; Tova Milo; Tim Kraska,Abstract In emerging Big Data scenarios; obtaining timely; high-quality answers toaggregate queries is difficult due to the challenges of processing and cleaning large; dirtydata sets. To increase the speed of query processing; there has been a resurgence ofinterest in sampling-based approximate query processing (SAQP). In its usual formulation;however; SAQP does not address data cleaning at all; and in fact; exacerbates answerquality problems by introducing sampling error. In this paper; we explore an intriguingopportunity. That is; we explore the use of sampling to actually improve answer quality. Weintroduce the Sample-and-Clean framework; which applies data cleaning to a relativelysmall subset of the data and uses the results of the cleaning process to lessen the impact ofdirty data on aggregate query answers. We derive confidence intervals as a function of …,SIGMOD,2014,62
QASCA: A quality-aware task assignment system for crowdsourcing applications,Yudian Zheng; Jiannan Wang; Guoliang Li; Reynold Cheng; Jianhua Feng,Abstract A crowdsourcing system; such as the Amazon Mechanical Turk (AMT); provides aplatform for a large number of questions to be answered by Internet workers. Such systemshave been shown to be useful to solve problems that are difficult for computers; includingentity resolution; sentiment analysis; and image recognition. In this paper; we investigate theonline task assignment problem: Given a pool of n questions; which of the k questionsshould be assigned to a worker? A poor assignment may not only waste time and money;but may also hurt the quality of a crowdsourcing application that depends on the workers'answers. We propose to consider quality measures (also known as evaluation metrics) thatare relevant to an application during the task assignment process. Particularly; we explorehow Accuracy and F-score; two widely-used evaluation metrics for crowdsourcing …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,57
Crowdsourced data management: A survey,Guoliang Li; Jiannan Wang; Yudian Zheng; Michael J Franklin,Any important data management and analytics tasks cannot be completely addressed byautomated processes. These tasks; such as entity resolution; sentiment analysis; and imagerecognition can be enhanced through the use of human cognitive ability. Crowdsouringplatforms are an effective way to harness the capabilities of people (ie; the crowd) to applyhuman computation for such tasks. Thus; crowdsourced data management has become anarea of increasing interest in research and industry. We identify three important problems incrowdsourced data management.(1) Quality Control: Workers may return noisy or incorrectresults so effective techniques are required to achieve high quality;(2) Cost Control: Thecrowd is not free; and cost control aims to reduce the monetary cost;(3) Latency Control: Thehuman workers can be slow; particularly compared to automated computing time scales …,IEEE Transactions on Knowledge and Data Engineering,2016,55
Trie-join: a trie-based method for efficient string similarity joins,Jianhua Feng; Jiannan Wang; Guoliang Li,Abstract A string similarity join finds similar pairs between two collections of strings. Manyapplications; eg; data integration and cleaning; can significantly benefit from an efficientstring-similarity-join algorithm. In this paper; we study string similarity joins with edit-distanceconstraints. Existing methods usually employ a filter-and-refine framework and suffer fromthe following limitations:(1) They are inefficient for the data sets with short strings (theaverage string length is not larger than 30);(2) They involve large indexes;(3) They areexpensive to support dynamic update of data sets. To address these problems; we proposea novel method called trie-join; which can generate results efficiently with small indexes. Weuse a trie structure to index the strings and utilize the trie structure to efficiently find similarstring pairs based on subtrie pruning. We devise efficient trie-join algorithms and pruning …,The VLDB Journal—The International Journal on Very Large Data Bases,2012,50
Towards dependable data repairing with fixing rules,Jiannan Wang; Nan Tang,ABSTRACT One of the main challenges that data cleaning systems face is to automaticallyidentify and repair data errors in a dependable manner. Though data dependencies (akaintegrity constraints) have been widely studied to capture errors in data; automated anddependable data repairing on these errors has remained a notoriously hard problem. In thiswork; we introduce an automated approach for dependably repairing data errors; based ona novel class of fixing rules. A fixing rule contains an evidence pattern; a set of negativepatterns; and a fact value. The heart of fixing rules is deterministic: given a tuple; theevidence pattern and the negative patterns of a fixing rule are combined to precisely capturewhich attribute is wrong; and the fact indicates how to correct this error. We study severalfundamental problems associated with fixing rules; and establish their complexity. We …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,49
Method for efficiently supporting interactive; fuzzy search on structured data,*,A method to support efficient; interactive; and fuzzy search on text data includes aninteractive; fuzzy search on structured data used in applications such as query relaxation;autocomplete; and spell checking; where inconsistencies and errors exist in user queries aswell as data. It utilizes techniques to efficiently and interactively answer fuzzy queries onstructured data to allow users to efficiently search for information interactively; and they canfind records and documents even if these records and documents are slightly different fromthe user keywords.,*,2011,41
Learning accurate kinematic control of cable-driven surgical robots using data cleaning and gaussian process regression,Jeffrey Mahler; Sanjay Krishnan; Michael Laskey; Siddarth Sen; Adithyavairavan Murali; Ben Kehoe; Sachin Patil; Jiannan Wang; Mike Franklin; Pieter Abbeel; Ken Goldberg,Precise control of industrial automation systems with non-linear kinematics due to jointelasticity; variation in cable tensioning; or backlash is challenging; especially in systems thatcan only be controlled through an interface with an imprecise internal kinematic model.Cable-driven Robotic Surgical Assistants (RSAs) are one example of such an automationsystem; as they are designed for master-slave teleoperation. We consider the problem oflearning a function to modify commands to the inaccurate control interface such thatexecuting the modified command on the system results in a desired state. To achieve this;we must learn a mapping that accounts for the non-linearities in the kinematic chain that arenot accounted for by the system's internal model. Gaussian Process Regression (GPR) is adata-driven technique that can estimate this non-linear correction in a task-specific region …,Automation Science and Engineering (CASE); 2014 IEEE International Conference on,2014,37
Supporting efficient top-k queries in type-ahead search,Guoliang Li; Jiannan Wang; Chen Li; Jianhua Feng,Abstract Type-ahead search can on-the-fly find answers as a user types in a keyword query.A main challenge in this search paradigm is the high-efficiency requirement that queriesmust be answered within milliseconds. In this paper we study how to answer top-k queries inthis paradigm; ie; as a user types in a query letter by letter; we want to efficiently find the kbest answers. Instead of inventing completely new algorithms from scratch; we studychallenges when adopting existing top-k algorithms in the literature that heavily rely on twobasic list-access methods: random access and sorted access. We present two algorithms tosupport random access efficiently. We develop novel techniques to support efficient sortedaccess using list pruning and materialization. We extend our techniques to support fuzzytype-ahead search which allows minor errors between query keywords and answers. We …,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,31
Clamshell: Speeding up crowds for low-latency data labeling,Daniel Haas; Jiannan Wang; Eugene Wu; Michael J Franklin,Abstract Data labeling is a necessary but often slow process that impedes the developmentof interactive systems for modern data analysis. Despite rising demand for manual datalabeling; there is a surprising lack of work addressing its high and unpredictable latency. Inthis paper; we introduce CLAMShell; a system that speeds up crowds in order to achieveconsistently low-latency data labeling. We offer a taxonomy of the sources of labelinglatency and study several large crowd-sourced labeling deployments to understand theirempirical latency profiles. Driven by these insights; we comprehensively tackle each sourceof latency; both by developing novel techniques such as straggler mitigation and poolmaintenance and by optimizing existing methods such as crowd retainer pools and activelearning. We evaluate CLAMShell in simulation and on live workers on Amazon's …,Proceedings of the VLDB Endowment,2015,27
Interactive and fuzzy search: a dynamic way to explore MEDLINE,Jiannan Wang; Inci Cetindil; Shengyue Ji; Chen Li; Xiaohui Xie; Guoliang Li; Jianhua Feng,Abstract Motivation: The MEDLINE database; consisting of over 19 million publicationrecords; is the primary source of information for biomedicine and health questions. Althoughthe database itself has been growing rapidly; the search paradigm of MEDLINE hasremained largely unchanged. Results: Here; we propose a new system for exploring theentire MEDLINE collection; represented by two unique features:(i) interactive: providinginstant feedback to users' query letter by letter; and (ii) fuzzy: allowing approximate search.We develop novel index structures and search algorithms to make such a search modelpossible. We also develop incremental-update techniques to keep the data up to date.Availability: Interactive and fuzzy searching algorithms for exploring MEDLINE areimplemented in a system called iPubMed; freely accessible over the web at http …,Bioinformatics,2010,26
Wisteria: Nurturing scalable data cleaning infrastructure,Daniel Haas; Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Eugene Wu,Abstract Analysts report spending upwards of 80% of their time on problems in datacleaning. The data cleaning process is inherently iterative; with evolving cleaning workflowsthat start with basic exploratory data analysis on small samples of dirty data; then refineanalysis with more sophisticated/expensive cleaning operators (eg; crowdsourcing); andfinally apply the insights to a full dataset. While an analyst often knows at a logical level whatoperations need to be done; they often have to manage a large search space of physicaloperators and parameters. We present Wisteria; a system designed to support the iterativedevelopment and optimization of data cleaning workflows; especially ones that utilize thecrowd. Wisteria separates logical operations from physical implementations; and driven byanalyst feedback; suggests optimizations and/or replacements to the analyst's choice of …,Proceedings of the VLDB Endowment,2015,24
Efficient parallel partition-based algorithms for similarity search and join with edit distance constraints,Yu Jiang; Dong Deng; Jiannan Wang; Guoliang Li; Jianhua Feng,Abstract The quantity of data in real-world applications is growing significantly while the dataquality is still a big problem. Similarity search and similarity join are two important operationsto address the poor data quality problem. Although many similarity search and joinalgorithms have been proposed; they did not utilize the abilities of modern hardware withmulti-core processors. It calls for new parallel algorithms to enable multi-core processors tomeet the high performance requirement of similarity search and join on big data. To this end;in this paper we propose parallel algorithms to support efficient similarity search and joinwith edit-distance constraints. We adopt the partition-based framework and extend it tosupport parallel similarity search and join on multi-core processors. We also develop twonovel pruning techniques. We have implemented our algorithms and the experimental …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,24
Extending string similarity join to tolerant fuzzy token matching,Jiannan Wang; Guoliang Li; Jianhua Feng,Abstract String similarity join that finds similar string pairs between two string sets is anessential operation in many applications and has attracted significant attention recently inthe database community. A significant challenge in similarity join is to implement an effectivefuzzy match operation to find all similar string pairs which may not match exactly. In thisarticle; we propose a new similarity function; called fuzzy-token-matching-based similaritywhich extends token-based similarity functions (eg; jaccard similarity and cosine similarity)by allowing fuzzy match between two tokens. We study the problem of similarity join usingthis new similarity function and present a signature-based method to address this problem.We propose new signature schemes and develop effective pruning techniques to improvethe performance. We also extend our techniques to support weighted tokens …,ACM Transactions on Database Systems (TODS),2014,21
DBease: Making databases user-friendly and easily accessible,Guoliang Li; Ju Fan; Hao Wu; Jiannan Wang; Jianhua Feng,ABSTRACT Structured query language (SQL) is a classical way to access relationaldatabases. Although SQL is powerful to query relational databases; it is rather hard forinexperienced users to pose SQL queries; as they are required to be familiar with SQLsyntax and have a thorough understanding of the underlying schema. To provide analternative search paradigm;,Proc. of the Fifth CIDR Conf,2011,21
ActiveClean: interactive data cleaning for statistical modeling,Sanjay Krishnan; Jiannan Wang; Eugene Wu; Michael J Franklin; Ken Goldberg,Abstract Analysts often clean dirty data iteratively--cleaning some data; executing theanalysis; and then cleaning more data based on the results. We explore the iterativecleaning process in the context of statistical model training; which is an increasingly popularform of data analytics. We propose ActiveClean; which allows for progressive and iterativecleaning in statistical modeling problems while preserving convergence guarantees.ActiveClean supports an important class of models called convex loss models (eg; linearregression and SVMs); and prioritizes cleaning those records likely to affect the results. Weevaluate ActiveClean on five real-world datasets UCI Adult; UCI EEG; MNIST; IMDB; andDollars For Docs with both real and synthetic errors. The results show that our proposedoptimizations can improve model accuracy by up-to 2.5 x for the same amount of data …,Proceedings of the VLDB Endowment,2016,17
Data cleaning: Overview and emerging challenges,Xu Chu; Ihab F Ilyas; Sanjay Krishnan; Jiannan Wang,Abstract Detecting and repairing dirty data is one of the perennial challenges in dataanalytics; and failure to do so can result in inaccurate analytics and unreliable decisions.Over the past few years; there has been a surge of interest from both industry and academiaon data cleaning problems including new abstractions; interfaces; approaches for scalability;and statistical techniques. To better understand the new advances in the field; we will firstpresent a taxonomy of the data cleaning literature in which we highlight the recent interest intechniques that use constraints; rules; or patterns to detect errors; which we call qualitativedata cleaning. We will describe the state-of-the-art techniques and also highlight theirlimitations with a series of illustrative examples. While traditionally such approaches aredistinct from quantitative approaches such as outlier detection; we also discuss recent …,Proceedings of the 2016 International Conference on Management of Data,2016,17
Stale view cleaning: Getting fresh answers from stale materialized views,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska,Abstract Materialized views (MVs); stored pre-computed results; are widely used to facilitatefast queries on large datasets. When new records arrive at a high rate; it is infeasible tocontinuously update (maintain) MVs and a common solution is to defer maintenance bybatching updates together. Between batches the MVs become increasingly stale withincorrect; missing; and superfluous rows leading to increasingly inaccurate query results.We propose Stale View Cleaning (SVC) which addresses this problem from a data cleaningperspective. In SVC; we efficiently clean a sample of rows from a stale MV; and use theclean sample to estimate aggregate query results. While approximate; the estimated queryresults reflect the most recent data. As sampling can be sensitive to long-tailed distributions;we further explore an outlier indexing technique to give increased accuracy when the …,Proceedings of the VLDB Endowment,2015,17
SampleClean: Fast and Reliable Analytics on Dirty Data.,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu,Abstract An important obstacle to accurate data analytics is dirty data in the form of missing;duplicate; incorrect; or inconsistent values. In the SampleClean project; we have developeda new suite of techniques to estimate the results of queries when only a sample of data canbe cleaned. Some forms of data corruption; such as duplication; can affect samplingprobabilities; and thus; new techniques have to be designed to ensure correctness of theapproximate query results. We first describe our initial project on computing statisticallybounded estimates of sum; count; and avg queries from samples of cleaned data. Wesubsequently explored how the same techniques could apply to other problems in databaseresearch; namely; materialized view maintenance. To avoid expensive incrementalmaintenance; we maintain only a sample of rows in a view; and then leverage …,IEEE Data Eng. Bull.,2015,11
Efficient fuzzy type-ahead search in tastier,Guoliang Li; Shengyue Ji; Chen Li; Jiannan Wang; Jianhua Feng,TASTIER is a research project on the new information-access paradigm called type-aheadsearch; in which systems find answers to a keyword query on-the-fly as users type in thequery. In this paper we study how to support fuzzy type-ahead search in TASTIER.Supporting fuzzy search is important when users have limited knowledge about the exactrepresentation of the entities they are looking for; such as people records in an onlinedirectory. We have developed and deployed several such systems; some of which havebeen used by many people on a daily basis. The systems received overwhelmingly positivefeedbacks from users due to their friendly interfaces with the fuzzy-search feature. Wedescribe the design and implementation of the systems; and demonstrate several suchsystems. We show that our efficient techniques can indeed allow this search paradigm to …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,11
Activeclean: Interactive data cleaning while learning convex loss models,Sanjay Krishnan; Jiannan Wang; Eugene Wu; Michael J Franklin; Ken Goldberg,Abstract: Data cleaning is often an important step to ensure that predictive models; such asregression and classification; are not affected by systematic errors such as inconsistent; out-of-date; or outlier data. Identifying dirty data is often a manual and iterative process; and canbe challenging on large datasets. However; many data cleaning workflows can introducesubtle biases into the training processes due to violation of independence assumptions. Wepropose ActiveClean; a progressive cleaning approach where the model is updatedincrementally instead of re-training and can guarantee accuracy on partially cleaned data.ActiveClean supports a popular class of models called convex loss models (eg; linearregression and SVMs). ActiveClean also leverages the structure of a user's model toprioritize cleaning those records likely to affect the results. We evaluate ActiveClean on …,arXiv preprint arXiv:1601.03797,2016,9
Crowdsourced data management: Overview and challenges,Guoliang Li; Yudian Zheng; Ju Fan; Jiannan Wang; Reynold Cheng,Abstract Many important data management and analytics tasks cannot be completelyaddressed by automated processes. Crowdsourcing is an effective way to harness humancognitive abilities to process these computer-hard tasks; such as entity resolution; sentimentanalysis; and image recognition. Crowdsourced data management has been extensivelystudied in research and industry recently. In this tutorial; we will survey and synthesize awide spectrum of existing studies on crowdsourced data management. We first give anoverview of crowdsourcing; and then summarize the fundamental techniques; includingquality control; cost control; and latency control; which must be considered in crowdsourceddata management. Next we review crowdsourced operators; including selection; collection;join; top-k; sort; categorize; aggregation; skyline; planning; schema matching; mining and …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,5
Finding gangs in war from signed networks,Lingyang Chu; Zhefeng Wang; Jian Pei; Jiannan Wang; Zijin Zhao; Enhong Chen,Abstract Given a signed network where edges are weighted in real number; and positiveweights indicate cohesion between vertices and negative weights indicate opposition; weare interested in finding k-Oppositive Cohesive Groups (k-OCG). Each k-OCG is a group of ksubgraphs such that (1) the edges within each subgraph are dense and cohesive; and (2)the edges crossing different subgraphs are dense and oppositive. Finding k-OCGs ischallenging since the subgraphs are often small; there are multiple k-OCGs in a largesigned network; and many existing dense subgraph extraction methods cannot handleedges of two signs. We model k-OCG finding task as a quadratic optimization problem.However; the classical Proximal Gradient method is very costly since it has to use the entireadjacency matrix; which is huge on large networks. Thus; we develop FOCG; an …,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2016,5
Privateclean: Data cleaning and differential privacy,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska,Abstract Recent advances in differential privacy make it possible to guarantee user privacywhile preserving the main characteristics of the data. However; most differential privacymechanisms assume that the underlying dataset is clean. This paper explores the linkbetween data cleaning and differential privacy in a framework we call PrivateClean.PrivateClean includes a technique for creating private datasets of numerical and discrete-valued attributes; a formalism for privacy-preserving data cleaning; and techniques foranswering sum; count; and avg queries after cleaning. We show:(1) how the degree ofprivacy affects subsequent aggregate query accuracy;(2) how privacy potentially amplifiescertain types of errors in a dataset; and (3) how this analysis can be used to tune the degreeof privacy. The key insight is to maintain a bipartite graph relating dirty values to clean …,Proceedings of the 2016 International Conference on Management of Data,2016,5
Automatic URL completion and prediction using fuzzy type-ahead search,Jiannan Wang; Guoliang Li; Jianhua Feng; Chen Li,Abstract Type-ahead search is a new information-access paradigm; in which systems canfind answers to keyword queries" on-the-fly" as a user types in a query. It improvestraditional autocomplete search by allowing query keywords to appear at different places inan answer. In this paper we study the problem of automatic URL completion and predictionusing fuzzy type-ahead search. That is; we interactively find relevant URLs that containwords matching query keywords; even approximately; as the user types in a query.Supporting fuzzy search is very important when the user has limited knowledge about URLs.We describe the design and implementation of our method; and report the experimentalresults on Firefox.,Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,2009,5
Activeclean: An interactive data cleaning framework for modern machine learning,Sanjay Krishnan; Michael J Franklin; Ken Goldberg; Jiannan Wang; Eugene Wu,Abstract Databases can be corrupted with various errors such as missing; incorrect; orinconsistent values. Increasingly; modern data analysis pipelines involve Machine Learning;and the effects of dirty data can be difficult to debug. Dirty data is often sparse; and naivesampling solutions are not suited for high-dimensional models. We propose ActiveClean; aprogressive framework for training Machine Learning models with data cleaning. Ourframework updates a model iteratively as the analyst cleans small batches of data; andincludes numerous optimizations such as importance weighting and dirty data detection. Wedesigned a visual interface to wrap around this framework and demonstrate ActiveClean fora video classification problem and a topic modeling problem.,Proceedings of the 2016 International Conference on Management of Data,2016,4
Skipping-oriented partitioning for columnar layouts,Liwen Sun; Michael J Franklin; Jiannan Wang; Eugene Wu,Abstract As data volumes continue to grow; modern database systems increasingly rely ondata skipping mechanisms to improve performance by avoiding access to irrelevant data.Recent work [39] proposed a fine-grained partitioning scheme that was shown to improvethe opportunities for data skipping in row-oriented systems. Modern analytics and big datasystems increasingly adopt columnar storage schemes; and in such systems; a row-basedapproach misses important opportunities for further improving data skipping. The flexibility ofcolumn-oriented organizations; however; comes with the additional cost of tuplereconstruction. In this paper; we develop Generalized Skipping-Oriented Partitioning(GSOP); a novel hybrid data skipping framework that takes into account these row-basedand column-based tradeoffs. In contrast to previous column-oriented physical design …,Proceedings of the VLDB Endowment,2016,2
Schemaless Join for Result Set Preferences,Chuancong Gao; Jian Pei; Jiannan Wang; Yi Chang,In many applications; such as data integration and big data analytics; one has to integratedata from multiple sources without detailed and accurate schema information. The state ofthe art focuses on matching attributes among sources based on the information derived fromthe data in those sources. However; a best join result according to a method's own pre-determined criteria may not fit a user's best interest. In this paper; we tackle the challengefrom a novel angle and investigate how to join schemaless tables to meet a user preferencethe best. We identify a set of essential preferences that are useful in various scenarios; suchas minimizing the number of tuples in outer join results and maximizing the entropy of thejoining key's distribution. We also develop a systematic method to compute the best joinpredicate optimizing an objective function representing a user preference. We conduct …,Information Reuse and Integration (IRI); 2017 IEEE International Conference on,2017,1
Preference-driven similarity join,Chuancong Gao; Jiannan Wang; Jian Pei; Rui Li; Yi Chang,Abstract: Similarity join; which can find similar objects (eg; products; names; addresses)across different sources; is powerful in dealing with variety in big data; especially web data.Threshold-driven similarity join; which has been extensively studied in the past; assumesthat a user is able to specify a similarity threshold; and then focuses on how to efficientlyreturn the object pairs whose similarities pass the threshold. We argue that the assumptionabout a well set similarity threshold may not be valid for two reasons. The optimal thresholdsfor different similarity join tasks may vary a lot. Moreover; the end-to-end time spent onsimilarity join is likely to be dominated by a back-and-forth threshold-tuning process. Inresponse; we propose preference-driven similarity join. The key idea is to provide severalresult-set preferences; rather than a range of thresholds; for a user to choose from …,arXiv preprint arXiv:1706.04266,2017,1
SmartCrawl: Deep Web Crawling Driven By Data Enrichment,Pei Wang; Ryan Shea; Jiannan Wang; Eugene Wu,ABSTRACT Entity resolution is defined as finding different records that refer to the same real-world entity. In this paper; we study deep entity resolution (DeepER) which aims to find pairsof records that describe the same entity between a local database and a hidden database.The local database can be accessed freely but the hidden database can only be accessedby a keyword-search query interface. To the best of our knowledge; we are the first to studythis problem. We first show that straightforward solutions are inefficient because they fail toexploit the ideas of query sharing and local-database-aware crawling. In response; wepropose SMARTCRAWL; a novel framework to overcome the limitations. Given a budget of bqueries; SMARTCRAWL first constructs a query pool based on the local database and theniteratively issues b queries to the hidden database such that the union of the query results …,*,*,1
Dependable Data Repairing with Fixing Rules,Jiannan Wang; Nan Tang,Abstract One of the main challenges that data-cleaning systems face is to automaticallyidentify and repair data errors in a dependable manner. Though data dependencies (alsoknown as integrity constraints) have been widely studied to capture errors in data;automated and dependable data repairing on these errors has remained a notoriouslydifficult problem. In this work; we introduce an automated approach for dependably repairingdata errors; based on a novel class of fixing rules. A fixing rule contains an evidence pattern;a set of negative patterns; and a fact value. The heart of fixing rules is deterministic: given atuple; the evidence pattern and the negative patterns of a fixing rule are combined toprecisely capture which attribute is wrong; and the fact indicates how to correct this error. Westudy several fundamental problems associated with fixing rules and establish their …,Journal of Data and Information Quality (JDIQ),2017,*
The Expected Optimal Labeling Order Problem for Crowdsourced Joins and Entity Resolution,Jiannan Wang; Guoliang Li; Tim Kraska; Michael J Franklin; Jianhua Feng,Abstract: In the SIGMOD 2013 conference; we published a paper extending our earlier workon crowdsourced entity resolution to improve crowdsourced join processing by exploitingtransitive relationships [Wang et al. 2013]. The VLDB 2014 conference has a paper thatfollows up on our previous work [Vesdapunt et al.; 2014]; which points out and corrects amistake we made in our SIGMOD paper. Specifically; in Section 4.2 of our SIGMOD paper;we defined the" Expected Optimal Labeling Order"(EOLO) problem; and proposed analgorithm for solving it. We incorrectly claimed that our algorithm is optimal. In their paper;Vesdapunt et al. show that the problem is actually NP-Hard; and based on that observation;propose a new algorithm to solve it. In this note; we would like to put the Vesdapunt et al.results in context; something we believe that their paper does not adequately do.,arXiv preprint arXiv:1409.7472,2014,*
Cleaning Crowdsourced Labels Using Oracles For Supervised Learning,Mohamad Dolatshah; Mathew Teoh; Jiannan Wang; Jian Pei,ABSTRACT Nowadays; crowdsourcing is being widely used to collect training data forsupervised learning. However; crowdsourced labels are often noisy; and there is aperformance gap between learning with noisy labels and learning with true labels. In thispaper; we consider how to apply oracle-based label cleaning to reduce the gap. Wepropose TARS; a label-cleaning advisor that can provide two pieces of valuable advice fordata scientists when they need to train or/and test a model using noisy labels. Firstly; in themodel testing stage; given a test dataset with noisy labels; and a classification model; TARScan use the test data to estimate how well the model will perform wrt true labels. Secondly; inthe model training stage; given a training dataset with noisy labels; and a supervised-learning algorithm; TARS can determine which label should be sent to an oracle to clean …,*,*,*
Deeper: A Data Enrichment System Powered by Deep Web,Pei Wang; Yongjun He; Ryan Shea; Jiannan Wang; Eugene Wu,ABSTRACT Data scientists often spend more than 80% of their time on data preparation.Data enrichment; the act of extending a local database with new attributes from external datasources; is among the most time-consuming tasks. Existing data enrichment works areresource intensive: data-intensive by relying on web tables or knowledge bases; monetarily-intensive by purchasing entire datasets; or timeintensive by fully crawling a web-based datasource. In this work; we explore a more targeted alternative that uses resources (in terms ofweb API calls) proportional to the size of the local database of interest. We build Deeper; adata enrichment system powered by deep web. The goal of Deeper is to help data scientiststo link a local database to a hidden database so that they can easily enrich the localdatabase with the attributes from the hidden database. We find that a challenging …,*,*,*
AQP++: Connecting Approximate Query Processing With Aggregate Precomputation for Interactive Analytics,Jinglin Peng; Dongxiang Zhang; Jiannan Wang; Jian Pei,ABSTRACT Interactive analytics requires database systems to be able to answeraggregation queries within interactive response times. As the amount of data is continuouslygrowing at an unprecedented rate; this is becoming increasingly challenging. In the past; thedatabase community has proposed two separate ideas; sampling-based approximate queryprocessing (AQP) and aggregate precomputation (AggPre) such as data cubes; to addressthis challenge. In this paper; we argue for the need to connect these two separate ideas forinteractive analytics. We propose AQP++; a novel framework to enable the connection. Theframework can leverage both a sample as well as a precomputed aggregate to answer userqueries. We discuss the advantages of having such a unified framework and identify newchallenges to fulfill this vision. We conduct an in-depth study of these challenges for …,*,*,*
Data Engineering,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu; Yachao Lu; Saravanan Thirumuruganathan; Nan Zhang; Gautam Das,Abstract There has been much research on various aspects of Approximate QueryProcessing (AQP); such as different sampling strategies; error estimation mechanisms; andvarious types of data synopses. However; many subtle challenges arise when building anactual AQP engine that can be deployed and used by real world applications. Thesesubtleties are often ignored (or at least not elaborated) by the theoretical literature andacademic prototypes alike. For the first time to the best of our knowledge; in this article; wefocus on these subtle challenges that one must address when designing an AQP system.Our intention for this article is to serve as a handbook listing critical design choices thatdatabase practitioners must be aware of when building or using an AQP system; not toprescribe a specific solution to each challenge.,*,*,*
Improving Data Management Applications Using Microtask Platforms,Jiannan Wang; Reynold S Xin,*,*,*,*
