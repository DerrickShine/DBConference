Holistic twig joins: optimal XML pattern matching,Nicolas Bruno; Nick Koudas; Divesh Srivastava,Abstract XML employs a tree-structured data model; and; naturally; XML queries specifypatterns of selection predicates on multiple elements related by a tree structure. Finding alloccurrences of such a twig pattern in an XML database is a core operation for XML queryprocessing. Prior work has typically decomposed the twig pattern into binary structural(parent-child and ancestor-descendant) relationships; and twig matching is achieved by:(i)using structural join algorithms to match the binary relationships against the XML database;and (ii) stitching together these basic matches. A limitation of this approach for matching twigpatterns is that intermediate result sizes can get large; even when the input and output sizesare more manageable. In this paper; we propose a novel holistic twig join algorithm;TwigStack; for matching an XML query twig pattern. Our technique uses a chain of linked …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,1306
Structural joins: A primitive for efficient XML query pattern matching,Shurug Al-Khalifa; HV Jagadish; Nick Koudas; Jignesh M Patel; Divesh Srivastava; Yuqing Wu,XML queries typically specify patterns of selection predicates on multiple elements that havesome specified tree structured relationships. The primitive tree structured relationships areparent-child and ancestor-descendant; and finding all occurrences of these relationships inan XML database is a core operation for XML query processing. We develop two families ofstructural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithmsare a natural extension of traditional merge joins and the multi-predicate merge joins; whilethe stack-tree algorithms have no counterpart in traditional relational join processing. Wepresent experimental results on a range of data and queries using the TIMBER native XMLquery engine built on top of SHORE. We show that while; in some cases; tree-mergealgorithms can have performance comparable to stack-tree algorithms; in many cases …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,1206
Answering queries using views,Alon Y Levy; Alberto O Mendelzon; Yehoshua Sagiv,Abstract We consider the problem of computing answers to queries by using materializedviews. Aside from its potential in optimizing query evaluation; the problem also arises inapplications such as Global Information Systems; Mobile Computing and maintainingphysical data independence. We consider the problem of finding a rewriting of a query thatuses the materialized views; the problem of finding minimal rewritings; and finding completerewritings(ie; rewritings that use only the views). We show that all the possible rewritings canbe obtained by considering cent ainment mappings from the views to the query; and that theproblems we consider are NP-complete when both the query and the views are conjunctiveand don't involve builtin comparison predicates. We show that the problem has twoindependent sources of complexity(the number of possible containment mappings; and …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,921
Semantic data caching and replacement,Shaul Dar; Michael J Franklin; Bjorn T Jonsson; Divesh Srivastava; Michael Tan,Abstract We propose a semantic model for client-side caching and replacement in a client-server database system and compare this approach to page caching and tuple cachingstrategies. Our caching model is based on; and derives its advantages from; three key ideas.First; the client maintains a semantic description of the data in its cache; which allows for acompact specification; as a remainder query; of the tuples needed to answer a query that arenot available in the cache. Second; usage information for replacement policies is maintainedin an adaptive fashion for semantic regions; which are associated with collections of tuples.This avoids the high overheads of tuple caching and; unlike page caching; is insensitive tobad clustering. Third; maintaining a semantic description of cached data enables the use ofsophisticated value functions that incorporate semantic notions of locality; not just LRU or …,Proceedings of the international conference on Very Large Data Bases,1996,717
Semantic data caching and replacement,Michael J Franklin; Bjorn T Jonsson; Divesh Srivastava; Michael Tan Shaul Dar,*,Proceedings of the 22th International Conference on Very Large Data Bases,1996,717
Approximate string joins in a database (almost) for free,Luis Gravano; Panagiotis G Ipeirotis; Hosagrahar Visvesvaraya Jagadish; Nick Koudas; Shanmugauelayut Muthukrishnan; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data especially formore complex queries involving joins. This is due; for example; to the prevalence oftypographical errors in data; and multiple conventions for recording attributes such as nameand address. Commercial databases do not support approximate string joins directly; and itis a challenge to implement this functionality efficiently with user-defined functions (UDFs). Inthis paper; we develop a technique for building approximate string join capabilities on top ofcommercial databases by exploiting facilities already available in them. At the core; ourtechnique relies on matching short substrings of length а; called а-grams; and taking intoaccount both positions of individual matches and the total number of such matches. Our …,Proceedings of the international conference on very large data bases,2001,634
Objectrank: Authority-based keyword search in databases,Andrey Balmin; Vagelis Hristidis; Yannis Papakonstantinou,Abstract The ObjectRank system applies authority-based ranking to keyword search indatabases modeled as labeled graphs. Conceptually; authority originates at the nodes(objects) containing the keywords and flows to objects according to their semanticconnections. Each node is ranked according to its authority with respect to the particularkeywords. One can adjust the weight of global importance; the weight of each keyword of thequery; the importance of a result actually containing the keywords versus being referencedby nodes containing them; and the volume of authority flow via each type of semanticconnection. Novel performance challenges and opportunities are addressed. First; schemasimpose constraints on the graph; which are exploited for performance purposes. Second; inorder to address the issue of authority ranking with respect to the given keywords (as …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,607
Yu.; C.: Timber: a Native XML Database,HV Jagadish; Shurug Al-Khalifa; Adriane Chapman; LV Lakshmanan; Andrew Nierman; Stelios Paparizos; J Patel; Divesh Srivastava; N Wiwatwattana; Y Wu,*,VLDB Journal,2002,570
The information manifold,Thomas Kirk; Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,Abstract We describe the Information Manifold (IM); a system for browsing and querying ofmultiple networked information sources. As a first contribution; the system demonstrates theviability of knowledge representation technology for retrieval and organization of informationfrom disparate (structured and unstructured) information sources. Such an organizationallows the user to pose high-level queries that use data from multiple information sources.As a second contribution; we describe novel query processing algorithms used to combineinformation from multiple sources. In particular; our algorithms are guaranteed to find exactlythe set of information sources relevant to a query; and to completely exploit knowledgeabout local closed world information (Etzioni et al. 1994).,Proceedings of the AAAI 1995 Spring Symp. on Information Gathering from Heterogeneous; Distributed Enviroments,1995,554
On computing correlated aggregates over continual data streams,Johannes Gehrke; Flip Korn; Divesh Srivastava,Abstract In many applications from telephone fraud detection to network management; dataarrives in a stream; and there is a need to maintain a variety of statistical summaryinformation about a large number of customers in an online fashion. At present; suchapplications maintain basic aggregates such as running extrema values (MIN; MAX);averages; standard deviations; etc.; that can be computed over data streams with limitedspace in a straightforward way. However; many applications require knowledge of morecomplex aggregates relating different attributes; so-called correlated aggregates. As anexample; one might be interested in computing the percentage of international phone callsthat are longer than the average duration of a domestic phone call. Exact computation of thisaggregate requires multiple passes over the data stream; which is infeasible. We propose …,ACM SIGMOD Record,2001,387
Answering queries with aggregation using views,Divesh Srivastava; Shaul Dar; HV Jagadish; A Levy,Abstract We present novel algorithms for the problem of using materialized views to computeanswers to SQL queries with grouping and aggregation; in the presence of multiset tables. Inaddition to its obvious potential in query optimization; this problem is important in manyapplications; such as data warehousing; very large transaction recording systems; globalinformation systems and mobile computing; where access to local or cached materializedviews may be cheaper than access to the underlying database. Our contributions are thefollowing: First; we show that in the case where the query has grouping and aggregation butthe views do not; a view is usable in answering a query only if there is an isomorphismbetween the view and a portion of the query. Second; when the views also have groupingand aggregation we identify conditions under which the aggregation information present …,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,1996,381
Aggregate query answering on anonymized tables,Qing Zhang; Nick Koudas; Divesh Srivastava; Ting Yu,Privacy is a serious concern when microdata need to be released for ad hoc analyses. Theprivacy goals of existing privacy protection approaches (eg; k-anonymity and l-diversity) aresuitable only for categorical sensitive attributes. Since applying them directly to numericalsensitive attributes (eg; salary) may result in undesirable information leakage; we proposeprivacy goals to better capture the need of privacy protection for numerical sensitiveattributes. Complementing the desire for privacy is the need to support ad hoc aggregateanalyses over microdata. Existing generalization-based anonymization approaches cannotanswer aggregate queries with reasonable accuracy. We present a general framework ofpermutation-based anonymization to support accurate answering of aggregate queries andshow that; for the same grouping; permutation-based techniques can always answer …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,371
TAX: A tree algebra for XML,H Jagadish; Laks Lakshmanan; Divesh Srivastava; Keith Thompson,Abstract Querying XML has been the subject of much recent investigation. A formal bulkalgebra is essential for applying database-style optimization to XML queries. We developsuch an algebra; called TAX (Tree Algebra for XML); for manipulating XML data; modeled asforests of labeled ordered trees. Motivated both by aesthetic considerations of intuitiveness;and by efficient computability and amenability to optimization; we develop TAX as a naturalextension of relational algebra; with a small set of operators. TAX is complete for relationalalgebra extended with aggregation; and can express most queries expressible in popularXML query languages. It forms the basis for the Timber XML database system currentlyunder development by us.,Database Programming Languages,2002,358
Data model and query evaluation in global information systems,Alon Y Levy; Divesh Srivastava; Thomas Kirk,Abstract Global information systems involve a large number of information sourcesdistributed over computer networks. The variety of information sources and disparity ofinterfaces makes the task of easily locating and efficiently accessing information over thenetwork very cumbersome. We describe an architecture for global information systems thatis especially tailored to address the challenges raised in such an environment; anddistinguish our architecture from architectures of multidatabase and distributed databasesystems. Our architecture is based on presenting a conceptually unified view of theinformation space to a user; specifying rich descriptions of the contents of the informationsources; and using these descriptions for optimizing queries posed in the unified view. Thecontributions of this paper include:(1) we identify aspects of site descriptions that are …,Journal of Intelligent Information Systems,1995,341
Fast computation of sparse datacubes,Kenneth A Ross; Divesh Srivastava,Abstract Datacube queries compute aggregates over database relations at a variety ofgranularities; and they constitute an important class of decision support queries. Real-worlddata is frequently sparse; and hence e ciently computing datacubes over large sparserelations is important. We show that current techniques for computing datacubes over sparserelations do not scale well with the number of CUBE BY attributes; especially when therelation is much larger than main memory. We propose a novel algorithm for the fastcomputation of datacubes over sparse relations; and demonstrate the e ciency of ouralgorithm using synthetic; benchmark and real-world data sets. When the relation ts inmemory; our technique performs multiple in-memory sorts; and does not incur any I/Obeyond the input of the relation and the output of the datacube itself. When the relation …,Proceedings of the international conference on very large data bases,1997,324
Text joins in an RDBMS for web data integration,Luis Gravano; Panagiotis G Ipeirotis; Nick Koudas; Divesh Srivastava,Abstract The integration of data produced and collected across autonomous; heterogeneousweb services is an increasingly important and challenging problem. Due to the lack of globalidentifiers; the same entity (eg; a product) might have different textual representations acrossdatabases. Textual data is also often noisy because of transcription errors; incompleteinformation; and lack of standard formats. A fundamental task during data integration ismatching of strings that refer to the same entity. In this paper; we adopt the widely used andestablished cosine similarity metric from the information retrieval field in order to identifypotential string matches across web sources. We then use this similarity metric tocharacterize this key aspect of data integration as a join between relations on textualattributes; where the similarity of matches exceeds a specified threshold. Computing an …,Proceedings of the 12th international conference on World Wide Web,2003,319
Minimization of tree pattern queries,Sihem Amer-Yahia; SungRan Cho; Laks VS Lakshmanan; Divesh Srivastava,Abstract Tree patterns forms a natural basis to query tree-structured data such as XML andLDAP. Since the efficiency of tree pattern matching against a tree-structured databasedepends on the size of the pattern; it is essential to identify and eliminate redundant nodesin the pattern and do so as quickly as possible. In this paper; we study tree patternminimization both in the absence and in the presence of integrity constraints (ICs) on theunderlying tree-structured database. When no ICs are considered; we call the process ofminimizing a tree pattern; constraint-independent minimization. We develop a polynomialtime algorithm called CIM for this purpose. CIM's efficiency stems from two key properties:(i)a node cannot be redundant unless its children are; and (ii) the order of elimination ofredundant nodes is immaterial. When ICs are considered for minimization; we refer to it …,ACM SIGMOD Record,2001,317
Integrating conflicting data: the role of source dependence,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values and differentsources can often provide conflicting values. To present quality data to users; it is critical thatdata integration systems can resolve conflicts and discover true values. Typically; we expecta true value to be provided by more sources than any particular false one; so we can takethe value provided by the majority of the sources as the truth. Unfortunately; a false valuecan be spread through copying and that makes truth discovery extremely tricky. In this paper;we consider how to find true values from conflicting information when there are a largenumber of sources; among which some may copy from others. We present a novel …,Proceedings of the VLDB Endowment,2009,297
Materialized view maintenance and integrity constraint checking: Trading space for time,Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We investigate the problem of incremental maintenance of an SQL view in the faceof database updates; and show that it is possible to reduce the total time cost of viewmaintenance by materializing (and maintaining) additional views. We formulate the problemof determining the optimal set of additional views to materialize as an optimization problemover the space of possible view sets (which includes the empty set). The optimizationproblem is harder than query optimization since it has to deal with multiple view sets;updates of multiple relations; and multiple ways of maintaining each view set for eachupdated relation. We develop a memoing solution for the problem; the solution can beimplemented using the expression DAG representation used in rule-based optimizers suchas Volcano. We demonstrate that global optimization cannot; in general; be achieved by …,ACM SIGMOD Record,1996,290
Record linkage: similarity measures and algorithms,Nick Koudas; Sunita Sarawagi; Divesh Srivastava,Abstract This tutorial provides a comprehensive and cohesive overview of the key researchresults in the area of record linkage methodologies and algorithms for identifyingapproximate duplicate records; and available tools for this purpose. It encompassestechniques introduced in several communities including databases; information retrieval;statistics and machine learning. It aims to identify similarities and differences across thetechniques as well as their merits and limitations.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,286
CORAL: Control; relations and logic,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract CORAL is a modular declarative query language/programming language thatsupports general Horn clauses with complex terms; set-grouping; aggregation; negation;and relations with tuples that contain (universally quanti ed) variables. Support for persistentrelations is provided by using the EXODUS storage manager. A unique feature of CORAL isthat it provides a wide range of evaluation strategies and allows users to| optionally| tailorexecution of a program through high-level annotations. A CORAL program is organized as acollection of modules; and this structure is used as the basis for expressing control choices.CORAL has an interface to C++; and uses the class structure of C++ to provide extensibility.Finally; CORAL supports a command sublanguage; in which statements are evaluated in auser-speci ed order. The statements can be queries; updates; production-system style …,Proceedings of the International Conference on Very Large Data Bases,1992,253
Tree pattern relaxation,Sihem Amer-Yahia; SungRan Cho; Divesh Srivastava,Abstract Tree patterns are fundamental to querying tree-structured data like XML. Becauseof the heterogeneity of XML data; it is often more appropriate to permit approximate querymatching and return ranked answers; in the spirit of Information Retrieval; than to return onlyexact answers. In this paper; we study the problem of approximate XML query matching;based on tree pattern relaxations; and devise efficient algorithms to evaluate relaxed treepatterns. We consider weighted tree patterns; where exact and relaxed weights; associatedwith nodes and edges of the tree pattern; are used to compute the scores of query answers.We are interested in the problem of finding answers whose scores are at least as large as agiven threshold. We design data pruning algorithms where intermediate query results arefiltered dynamically during the evaluation process. We develop an optimization that …,Advances in Database Technology—EDBT 2002,2002,249
Keyword proximity search in XML trees,Vagelis Hristidis; Nick Koudas; Yannis Papakonstantinou; Divesh Srivastava,Recent works have shown the benefits of keyword proximity search in querying XMLdocuments in addition to text documents. For example; given query keywords overShakespeare's plays in XML; the user might be interested in knowing how the keywordscooccur. In this paper; we focus on XML trees and define XML keyword; proximity queries toreturn the (possibly heterogeneous) set of minimum connecting trees (MCTs) of the matchesto the individual keywords in the query. We consider efficiently executing keyword proximityqueries on labeled trees (XML) in various settings: 1) when the XML database has beenpreprocessed and 2) when no indices are available on the XML database. We perform adetailed experimental evaluation to study the benefits of our approach and show that ouralgorithms considerably outperform prior algorithms and other applicable approaches.,IEEE Transactions on Knowledge and Data Engineering,2006,241
Big data integration,Xin Luna Dong; Divesh Srivastava,The Big Data era is upon us: data is being generated; collected and analyzed at anunprecedented scale; and data-driven decision making is sweeping through all aspects ofsociety. Since the value of data explodes when it can be linked and fused with other data;addressing the big data integration (BDI) challenge is critical to realizing the promise of BigData. BDI differs from traditional data integration in many dimensions:(i) the number of datasources; even for a single domain; has grown to be in the tens of thousands;(ii) many of thedata sources are very dynamic; as a huge amount of newly collected data are continuouslymade available;(iii) the data sources are extremely heterogeneous in their structure; withconsiderable variety even for substantially similar entities; and (iv) the data sources are ofwidely differing qualities; with significant differences in the coverage; accuracy and …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,237
Class-based graph anonymization for social network data,Graham Cormode; Divesh Srivastava; Smriti Bhagat; Balachander Krishnamurthy,*,Proceedings of the VLDB Endowment,2009,218
Approximate XML joins,Sudipto Guha; HV Jagadish; Nick Koudas; Divesh Srivastava; Ting Yu,Abstract XML is widely recognized as the data interchange standard for tomorrow; becauseof its ability to represent data from a wide variety sources. Hence; XML is likely to be theformat through which data from multiple sources is integrated. In this paper we study theproblem of integrating XML data sources through correlations realized as join operations. Achallenging aspect of this operation is the XML document structure. Two documents mightconvey approximately or exactly the same information but may be quite different in structure.Consequently approximate match in structure; in addition to; content has to be folded in thejoin operation. We quantify approximate match in structure and content using well definednotions of distance. For structure; we propose computationally inexpensive lower and upperbounds for the tree edit distance metric between two trees. We then show how the tree …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,189
Truth discovery and copying detection in a dynamic world,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Modern information management applications often require integrating data from avariety of data sources; some of which may copy or buy data from other sources. When thesedata sources model a dynamically changing world (eg; people's contact informationchanges over time; restaurants open and go out of business); sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different; possibly dependent; sources; it ischallenging for data integration systems to provide the true values. Straightforward ways toresolve such inconsistencies (eg; voting) may lead to noisy results; often with detrimentalconsequences. In this paper; we study the problem of finding true values and determiningthe copying relationship between sources; when the update history of the sources is …,Proceedings of the VLDB Endowment,2009,186
Structure and content scoring for XML,Sihem Amer-Yahia; Nick Koudas; Amélie Marian; Divesh Srivastava; David Toman,Abstract XML repositories are usually queried both on structure and content. Due tostructural heterogeneity of XML; queries are often interpreted approximately and theiranswers are returned ranked by scores. Computing answer scores in XML is an active areaof research that oscillates between pure content scoring such as the well-known tf* idf andtaking structure into account. However; none of the existing proposals fully accounts forstructure and combines it with content to score query answers. We propose novel XMLscoring methods that are inspired by tf* idf and that account for both structure and contentwhile considering query relaxations. Twig scoring; accounts for the most structure andcontent and is thus used as our reference method. Path scoring is an approximation thatloosens correlations between query nodes hence reducing the amount of time required to …,Proceedings of the 31st international conference on Very large data bases,2005,178
Anonymizing bipartite graph data using safe groupings,Graham Cormode; Divesh Srivastava; Ting Yu; Qing Zhang,Abstract Private data often comes in the form of associations between entities; such ascustomers and products bought from a pharmacy; which are naturally represented in theform of a large; sparse bipartite graph. As with tabular data; it is desirable to be able topublish anonymized versions of such data; to allow others to perform ad hoc analysis ofaggregate graph properties. However; existing tabular anonymization techniques do notgive useful or meaningful results when applied to graphs: small changes or masking of theedge structure can radically change aggregate graph properties. We introduce a new familyof anonymizations; for bipartite graph data; called (k; l)-groupings. These groupingspreserve the underlying graph structure perfectly; and instead anonymize the mapping fromentities to nodes of the graph. We identify a class of" safe"(k; l)-groupings that have …,Proceedings of the VLDB Endowment,2008,176
Truth Finding on the Deep Web: Is the Problem Solved?,Xian Li; Xin Luna Dong; Kenneth B Lyons; Weiyi Meng; Divesh Srivastava,Abstract The amount of useful information available on the Web has been growing at adramatic pace in recent years and people rely more and more on the Web to fulfill theirinformation needs. In this paper; we study truthfulness of Deep Web data in two domainswhere we believed data are fairly clean and data quality is important to people's lives: Stockand Flight. To our surprise; we observed a large amount of inconsistency on data fromdifferent sources and also some sources with quite low accuracy. We further applied onthese two data sets state-of-the-art data fusion methods that aim at resolving conflicts andfinding the truth; analyzed their strengths and limitations; and suggested promising researchdirections. We wish our study can increase awareness of the seriousness of conflicting dataon the Web and in turn inspire more research in our community to tackle this problem.,*,2013,174
Differentially private spatial decompositions,Graham Cormode; Cecilia Procopiuc; Divesh Srivastava; Entong Shen; Ting Yu,Differential privacy has recently emerged as the de facto standard for private data release.This makes it possible to provide strong theoretical guarantees on the privacy and utility ofreleased data. While it is well-understood how to release data based on counts and simplefunctions under this guarantee; it remains to provide general purpose techniques to releasedata that is useful for a variety of queries. In this paper; we focus on spatial data such aslocations and more generally any multi-dimensional data that can be indexed by a treestructure. Directly applying existing differential privacy methods to this type of data simplygenerates noise. We propose instead the class of" private spatial decompositions'': theseadapt standard spatial indexing methods such as quad trees and kd-trees to provide aprivate description of the data distribution. Equipping such structures with differential …,Data engineering (ICDE); 2012 IEEE 28th international conference on,2012,171
-Finding Hierarchical Heavy Hitters in Data Streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Aggregation along hierarchies is a critical summary technique in a large variety of onlineapplications including decision support (OLAP); network management (IP clustering anddenial-of-service attack monitoring); text (on prefixes of strings occurring in the text) and XMLsummarization (on prefixes of root-to-leaf paths in the XML data tree). In these applications;the data is inherently hierarchical and one needs to maintain aggregates at different levelsof the hierarchy over time in a dynamic fashion. It formalizes the problem of finding heavyhitters in massive data streams that considers their hierarchical structure. Such hierarchicalheavy hitters (HHHs) present a “cartogram” summary of the data stream distribution. Thischapter presents comprehensive solutions to the problem of estimating HHHs on datastreams. The resulting summary gives a topological “cartogram” of the hierarchical data. It …,*,2003,167
Counting twig matches in a tree,Zhiyuan Chen; HV Jagadish; Flip Korn; Nick Koudas; S Muthukrishnan; Raymond Ng; Divesh Srivastava,Describes efficient algorithms for accurately estimating the number of matches of a smallnode-labeled tree; ie a twig; in a large node-labeled tree; using a summary data structure.This problem is of interest for queries on XML and other hierarchical data; to provide queryfeedback and for cost-based query optimization. Our summary data structure scalablyrepresents approximate frequency information about twiglets (ie small twigs) in the data tree.Given a twig query; the number of matches is estimated by creating a set of query twiglets;and combining two complementary approaches: set hashing; used to estimate the number ofmatches of each query twiglet; and maximal overlap; used to combine the query twigletestimates into an estimate for the twig query. We propose several estimation algorithms thatapply these approaches on query twiglets formed using variations on different twiglet …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,166
Apparatus and methods for retrieving information by modifying query plan based on description of information sources,*,Techniques for optimizing queries in a system in which executing the query requiresretrieval of information from a number of different data bases which are accessible via anetwork. In the techniques; a query results in a query plan which includes subplans forquerying the data bases which contain the required information. When a subplan isexecuted in one of the data bases; the data base returns not only the information whichresults from the execution of the subplan; but also source and constraint information aboutthe data in the data base. The source and constraint information is then used to optimize thequery plan by pruning redundant subplans. An embodiment is disclosed in which queriesare made to a domain model implemented using a knowledge base system. The domainmodel includes a world view of the data; a set of descriptions of the data bases; and a set …,*,1997,164
Using q-grams in a DBMS for Approximate String Processing,Luis Gravano; Panagiotis G.  Ipeirotis; Hosagrahar Visvesvaraya Jagadish; Nick Koudas; Shanmugauelayut Muthukrishnan; Lauri Pietarinen; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data. This is due; forexample; to the prevalence of typographical errors in data; and multiple conventions forrecording attributes such as name and address. Commercial databases do not supportapproximate string queries directly; and it is a challenge to implement this functionalityefficiently with user-defined functions (UDFs). In this paper; we develop a technique forbuilding approximate string processing capabilities on top of commercial databases byexploiting facilities already available in them. At the core; our technique relies on generatingshort substrings of length Õ; called Õ-grams; and processing them using standard methodsavailable in the DBMS. The proposed technique enables various approximate string …,IEEE Data Engineering Bulletin,2001,158
Tree pattern query minimization,Sihem Amer-Yahia; SungRan Cho; Laks VS Lakshmanan; Divesh Srivastava,Abstract. Tree patterns form a natural basis to query tree-structured data such as XML andLDAP. To improve the efficiency of tree pattern matching; it is essential to quickly identify andeliminate redundant nodes in the pattern. In this paper; we study tree pattern minimizationboth in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database. In the absence of ICs; we develop a polynomial-time queryminimization algorithm called CIM; whose efficiency stems from two key properties:(i) a nodecannot be redundant unless its children are; and (ii) the order of elimination of redundantnodes is immaterial. When ICs are considered for minimization; we develop a technique forquery minimization based on three fundamental operations: augmentation (an adaptation ofthe well-known chase procedure); minimization (based on homomorphism techniques) …,The VLDB Journal,2002,136
Optimizing the secure evaluation of twig queries,SungRan Cho; Laks VS Lakshmanan; Sihem Amer-Yahia; Divesh Srivastava,This chapter focuses on the problem of secure evaluation of eXtensible Markup Language(XML) twig queries; for the simple; but useful; multilevel security model. The rapidemergence of XML as a standard for data exchange over the Web has led to considerableinterest in the problem of securing XML documents. In this context; query evaluation enginesneed to ensure that user queries only use and return XML data that the user is allowed toaccess. These added access control checks can considerably increase query evaluationtime. Companies are using the Web as the main means of information dissemination;sparking interest in models; and efficient mechanisms for controlled access to informationcontent over the Web. In this respect; securing XML documents is an important step;because XML is rapidly emerging as the standard for data representation and exchange …,*,2002,135
On generating near-optimal tableaux for conditional functional dependencies,Lukasz Golab; Howard Karloff; Flip Korn; Divesh Srivastava; Bei Yu,Abstract Conditional functional dependencies (CFDs) have recently been proposed as auseful integrity constraint to summarize data semantics and identify data inconsistencies. ACFD augments a functional dependency (FD) with a pattern tableau that defines the context(ie; the subset of tuples) in which the underlying FD holds. While many aspects of CFDshave been studied; including static analysis and detecting and repairing violations; therehas not been prior work on generating pattern tableaux; which is critical to realize the fullpotential of CFDs. This paper is the first to formally characterize a" good" pattern tableau;based on naturally desirable properties of support; confidence and parsimony. We show thatthe problem of generating an optimal tableau for a given FD is NP-complete but can beapproximated in polynomial time via a greedy algorithm. For large data sets; we propose …,Proceedings of the VLDB Endowment,2008,131
Efficient processing of top-k queries in uncertain databases with x-relations,Ke Yi; Feifei Li; George Kollios; Divesh Srivastava,This work introduces novel polynomial algorithms for processing top-k queries in uncertaindatabases under the generally adopted model of x-relations. An x-relation consists of anumber of x-tuples; and each x-tuple randomly instantiates into one tuple from one or morealternatives. Our results significantly improve the best known algorithms for top-k queryprocessing in uncertain databases; in terms of both runtime and memory usage. In the single-alternative case; the new algorithms are 2 to 3 orders of magnitude faster than the previousalgorithms. In the multialternative case; we introduce the first-known polynomial algorithms;while the current best algorithms have exponential complexity in both time and space. Ouralgorithms run in near linear or low polynomial time and cover both types of top-k queries inuncertain databases. We provide both the theoretical analysis and an extensive …,IEEE transactions on knowledge and data engineering,2008,128
Navigation-vs. index-based XML multi-query processing,Nicolas Bruno; Luis Gravano; Nick Koudas; Divesh Srivastava,XML path queries form the basis of complex filtering of XML data. Most current XML pathquery processing techniques can be divided in two groups. Navigation-based algorithmscompute results by analyzing an input document one tag at a time. In contrast; index-basedalgorithms take advantage of precomputed numbering schemes over the input XMLdocument. We introduce a new index-based technique; index-filter; to answer multiple XMLpath queries. Index-filter uses indexes built over the document tags to avoid processinglarge portions of the input document that are guaranteed not to be part of any match. Weanalyze index-filter and compare it against Y-filter; a state-of-the-art navigation-basedtechnique. We show that both techniques have their advantages; and we discuss thescenarios under which each technique is superior to the other one. In particular; we show …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,122
Diamond in the rough: Finding hierarchical heavy hitters in multi-dimensional data,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items archived in data warehouses or those that arrive online as streamstypically have attributes which take values from multiple hierarchies (eg; time andgeographic location; source and destination IP addresses). Providing an aggregate view ofsuch data is important to summarize; visualize; and analyze. We develop the aggregate viewbased on certain hierarchically organized sets of large-valued regions (" heavy hitters").Such Hierarchical Heavy Hitters (HHHs) were previously introduced as a crucialaggregation technique in one dimension. In order to analyze the wider range of datawarehousing applications and realistic IP data streams; we generalize this problem tomultiple dimensions. We identify and study two variants of HHHs for multi-dimensional data;namely the" overlap" and" split" cases; depending on how an aggregate computed for a …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,119
What can hierarchies do for data warehouses?,Hosagrahar V Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Abstract Data in a warehouse typically has multiple dimensions of interest; such as location;time; and product. It is well-recognized that these dimensions have hierarchies defined onthem; such as" store-city-state-region" for location. The standard way to model such data iswith a star/snowflake schema. However; current approaches do not give a first-class statusto dimensions. Consequently; a substantial class of interesting queries involving dimensionhierarchies and their interaction with the fact tables are quite verbose to write; hard to read;and difficult to optimize. We propose the SQL (%) model and a natural extension to the SQLquery language; that gives a first-class status to dimensions; and we pin down its semantics.Our model permits structural and schematic heterogeneity in dimension hierarchies;situations often arising in practice that cannot be modeled satisfactorily using the star …,VLDB,1999,119
Bed-tree: an all-purpose index structure for string similarity search based on edit distance,Zhenjie Zhang; Marios Hadjieleftheriou; Beng Chin Ooi; Divesh Srivastava,Abstract Strings are ubiquitous in computer systems and hence string processing hasattracted extensive research effort from computer scientists in diverse areas. One of the mostimportant problems in string processing is to efficiently evaluate the similarity between twostrings based on a specified similarity measure. String similarity search is a fundamentalproblem in information retrieval; database cleaning; biological sequence analysis; andmore. While a large number of dissimilarity measures on strings have been proposed; editdistance is the most popular choice in a wide spectrum of applications. Existing indexingtechniques for similarity search queries based on edit distance; eg; approximate selectionand join queries; rely mostly on n-gram signatures coupled with inverted list structures.These techniques are tailored for specific query types only; and their performance …,Proceedings of the 2010 international conference on Management of data,2010,117
Efficient processing of top-k queries in uncertain databases,Ke Yi; Feifei Li; George Kollios; Divesh Srivastava,This work introduces novel polynomial-time algorithms for processing top-k queries inuncertain databases; under the generally adopted model of x-relations. An x-relationconsists of a number of x-tuples; and each x-tuple randomly instantiates into one tuple fromone or more alternatives. Our results significantly improve the best known algorithms for top-k query processing in uncertain databases; in terms of both running time and memoryusage. Focusing on the single-alternative case; the new algorithms are orders of magnitudefaster.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,116
Merging the results of approximate match operations,Sudipto Guha; Nick Koudas; Amit Marathe; Divesh Srivastava,Abstract Data Cleaning is an important process that has been at the center of researchinterest in recent years. An important end goal of effective data cleaning is to identify therelational tuple or tuples that are" most related" to a given query tuple. Various techniqueshave been proposed in the literature for efficiently identifying approximate matches to aquery string against a single attribute of a relation. In addition to constructing a ranking (ie;ordering) of these matches; the techniques often associate; with each match; scores thatquantify the extent of the match. Since multiple attributes could exist in the query tuple;issuing approximate match operations for each of them separately will effectively create anumber of ranked lists of the relation tuples. Merging these lists to identify a final ranking andscoring; and returning the top-K tuples; is a challenging task. In this paper; we adapt the …,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,2004,116
Adaptive processing of top-k queries in XML,Amelie Marian; Sihem Amer-Yahia; Nick Koudas; Divesh Srivastava,The ability to compute top-k matches to XML queries is gaining importance due to theincreasing number of large XML repositories. The efficiency of top-k query evaluation relieson using scores to prune irrelevant answers as early as possible in the evaluation process.In this context; evaluating the same query plan for all answers might be too rigid because; atany time in the evaluation; answers have gone through the same number and sequence ofoperations; which limits the speed at which scores grow. Therefore; adaptive queryprocessing that permits different plans for different partial matches and maximizes the bestscores is more appropriate. In this paper; we propose an architecture and adaptivealgorithms for efficiently computing top-k matches to XML queries. Our techniques can beused to evaluate both exact and approximate matches where approximation is defined by …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,115
Flexible string matching against large databases in practice,Nick Koudas; Amit Marathe; Divesh Srivastava,Abstract Data Cleaning is an important process that has been at the center of researchinterest in recent years. Poor data quality is the result of a variety of reasons; including dataentry errors and multiple conventions for recording database fields; and has a significantimpact on a variety of business issues. Hence; there is a pressing need for technologies thatenable flexible (fuzzy) matching of string information in a database. Cosine similarity with tf-idf is a well-established metric for comparing text; and recent proposals have adapted thissimilarity measure for flexibly matching a query string with values in a single attribute of arelation. In deploying tf-idf based flexible string matching against real AT&T databases; weobserved that this technique needed to be enhanced in many ways. First; along thefunctionality dimension; where there was a need to flexibly match along multiple string …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,114
On query result diversification,Marcos R Vieira; Humberto L Razente; Maria CN Barioni; Marios Hadjieleftheriou; Divesh Srivastava; Caetano Traina; Vassilis J Tsotras,In this paper we describe a general framework for evaluation and optimization of methodsfor diversifying query results. In these methods; an initial ranking candidate set produced bya query is used to construct a result set; where elements are ranked with respect torelevance and diversity features; ie; the retrieved elements should be as relevant aspossible to the query; and; at the same time; the result set should be as diverse as possible.While addressing relevance is relatively simple and has been heavily studied; diversity is aharder problem to solve. One major contribution of this paper is that; using the aboveframework; we adapt; implement and evaluate several existing methods for diversifyingquery results. We also propose two new approaches; namely the Greedy with MarginalContribution (GMC) and the Greedy Randomized with Neighborhood Expansion (GNE) …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,113
Fast indexes and algorithms for set similarity selection queries,Marios Hadjieleftheriou; Amit Chandel; Nick Koudas; Divesh Srivastava,Data collections often have inconsistencies that arise due to a variety of reasons; and it isdesirable to be able to identify and resolve them efficiently. Set similarity queries arecommonly used in data cleaning for matching similar data. In this work we concentrate onset similarity selection queries: Given a query set; retrieve all sets in a collection withsimilarity greater than some threshold. Various set similarity measures have been proposedin the past for data cleaning purposes. In this work we concentrate on weighted similarityfunctions like TF/IDF; and introduce variants that are well suited for set similarity selections ina relational database context. These variants have special semantic properties that can beexploited to design very efficient index structures and algorithms for answering queriesefficiently. We present modifications of existing technologies to work for set similarity …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,112
Reverse nearest neighbor aggregates over data streams,Flip Korn; S Muthukrishnan; Divesh Srivastava,This chapter introduces and investigates the problem of reverse nearest neighboraggregates (RNNAs) computation over a data stream of client arrivals and departures; with astatic set of available servers. RNNAs are of natural interest in decision support systems forapplications that compute proximity; based on geographical distance or vector spacesimilarity; between “servers” and “clients.” Applications for RNNAs range from the classical(such as facility location) to the emerging (such as fixed wireless telephony access andsensor-based traffic monitoring). Increasingly; and especially in emerging applications;server and client data arrives in streams; and decision support tools need to be able tocompute answers to queries in an online fashion. Reverse nearest neighbor (RNN) querieshave been studied for finite; stored data sets and are of interest for decision support …,*,2002,112
Global detection of complex copying relationships between sources,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract Web technologies have enabled data sharing between sources but also simplifiedcopying (and often publishing without proper attribution). The copying relationships can becomplex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source; and some transitively copy from another. Understanding suchcopying relationships is desirable both for business purposes and for improving many keycomponents in data integration; such as resolving conflicts across various sources;reconciling distinct references to the same real-world entity; and efficiently answeringqueries over multiple sources. Recent works have studied how to detect copying between apair of sources; but the techniques can fall short in the presence of complex copyingrelationships. In this paper we describe techniques that discover global copying …,Proceedings of the VLDB Endowment,2010,109
Less is more: Selecting sources wisely for integration,Xin Luna Dong; Barna Saha; Divesh Srivastava,Abstract We are often thrilled by the abundance of information surrounding us and wish tointegrate data from as many sources as possible. However; understanding; analyzing; andusing these data are often hard. Too much data can introduce a huge integration cost; suchas expenses for purchasing data and resources for integration and cleaning. Furthermore;including low-quality data can even deteriorate the quality of integration results instead ofbringing the desired quality gain. Thus;" the more the better" does not always hold for dataintegration and often" less is more". In this paper; we study how to select a subset of sourcesbefore integration such that we can balance the quality of integrated data and integrationcost. Inspired by the Marginalism principle in economic theory; we wish to integrate a newsource only if its marginal gain; often a function of improved integration quality; is higher …,*,2013,108
Multiple aggregations over data streams,Rui Zhang; Nick Koudas; Beng Chin Ooi; Divesh Srivastava,Abstract Monitoring aggregates on IP traffic data streams is a compelling application for datastream management systems. The need for exploratory IP traffic data analysis naturallyleads to posing related aggregation queries on data streams; that differ only in the choice ofgrouping attributes. In this paper; we address this problem of efficiently computing multipleaggregations over high speed data streams; based on a two-level LFTA/HFTA DSMSarchitecture; inspired by Gigascope. Our first contribution is the insight that in such ascenario; additionally computing and maintaining fine-granularity aggregation queries(phantoms) at the LFTA has the benefit of supporting shared computation. Our secondcontribution is an investigation into the problem of identifying beneficial LFTA configurationsof phantoms and user-queries. We formulate this problem as a cost optimization problem …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,108
The CORAL deductive system,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan; Praveen Seshadri,Abstract CORAL is a deductive system that supports a rich declarative language; and aninterface to C++; which allows for a combination of declarative and imperative programming.A CORAL declarative program can be organized as a collection of interacting modules.CORAL supports a wide range of evaluation strategies; and automatically chooses anefficient strategy for each module in the program. Users can guide query optimization byselecting from a wide range of control choices. The CORAL system provides imperativeconstructs to update; insert; and delete facts. Users can program in a combination ofdeclarative CORAL and C++ extended with CORAL primitives. A high degree of extensibilityis provided by allowing C++ programmers to use the class structure of C++ to enhance theCORAL implementation. CORAL provides support for main-memory data and; using the …,The VLDB Journal—The International Journal on Very Large Data Bases,1994,108
Ranked join indices,Panayiotis Tsaparas; Themistoklis Palpanas; Yannis Kotidis; Nick Koudas; Divesh Srivastava,A plethora of data sources contain data entities that could be ordered according to a varietyof attributes associated with the entities. Such orderings result effectively in a ranking of theentities according to the values in the attribute domain. Commonly; users correlate suchsources for query processing purposes through join operations. In query processing; it isdesirable to incorporate user preferences towards specific attributes or their values. A way toincorporate such preferences is by utilizing scoring functions that combine user preferencesand attribute values and return a numerical score for each tuple in the join result. Then; atarget query; which we refer to as top-k join query; seeks to identify the k tuples in the joinresult with the highest scores. We propose a novel technique; which we refer to as rankedjoin index; to efficiently answer top-k join queries for arbitrary; user specified; preferences …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,107
Method of clustering electronic documents in response to a search query,*,A method of presenting clusters of documents in response to a search query where thedocuments within a cluster are determined to be related to one another. This relationship isassessed by comparing documents which match one or more terms in the query todetermine the extent to which the documents have commonality with respect to termsappearing infrequently in the collection of documents. As a consequence; the cluster ofdocuments represents a response or query result that is split across multiple documents. In afurther variation the cluster can be constituted by a structured document and an unstructureddocument.,*,2000,103
Interactive data exploration apparatus and methods,*,A data exploration tool which has a graphical user interface that employs directed graphs toprovide histories of the data exploration operations. Nodes in the directed graphs representoperations on data; the edges represent relationships between the operations. One type ofthe directed graphs is the derivation graph; in which the root of the graph is a noderepresenting a data set and an edge leading from a first node to a second node indicatesthat the operation represented by the second node is performed on the result of theoperation represented by the first node. Operations include query; segmentation;aggregation; and data view operations. A user may edit the derivation graph and may selecta node for execution. When that is done; all of the operations represented by the nodesbetween the root node and the selected node are performed as indicated in the graph …,*,1999,103
Pushing constraint selections,Divesh Srivastava; Raghu Ramakrishnan,Abstract Bottom-up evaluation of a program-query pair in a constraint query language oftencomputes only ground facts. Constraints do not contribute to answers; but are used only toprune derivations. The Magic Templates evaluation cannot utilize all the constraintinformation present in such program-query pairs while computing only ground facts. Ingeneral; constraint facts are computed; making the resulting evaluation more expensive. Wedescribe an optimization that propagates constraints occuring in the program and the query;such that the rewritten program fully utilizes the constraint information present in the originalprogram. Only constraint-relevant facts are computed; and if the evaluation of the originalprogram computed only ground facts; so does the evaluation of the rewritten program. Ourprocedure can be combined with the Magic Templates transformation to propagate query …,The Journal of Logic Programming,1993,102
X^ 3: A cube operator for xml olap,Nuwee Wiwatwattana; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,With increasing amounts of data being exchanged and even generated or stored in XML; anatural question is how to perform OLAP on XML data; which can be structurallyheterogeneous (eg; parse trees) and/or marked-up text documents. A core operator forOLAP is the data cube. While the relational cube can be extended in a straightforward wayto XML; we argue such an extension would not address the specific issues posed by XML.While in a relational warehouse; facts are flat records and dimensions may have hierarchies;in an XML warehouse; both facts and dimensions may be hierarchical. Second; XML isflexible:(a) an element may have missing or repeated subelements;(b) different instances ofthe same element type may have different structure. We identify the challenges introducedby these features of XML for cube definition and computation. We propose a definition for …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,99
Distributed evalulation of directory queries using a topology cache,*,A technique for performing query evaluation on distributed directories utilizes the creation ofa “topology cache” defining the hierarchical relationship between the various directoryservers (ie; identifying “subordinate” and “superior” knowledge references associated witheach directory server in the system). The created topology cache is then stored at eachdirectory server; and forwarded to a client upon submitting a query to the system. Using thetopology cache information at the client; a distributed query evaluation plan can bedeveloped for use with complex queries; such as hierarchical and aggregate queries.,*,2005,99
Holistic UDAFs at streaming speeds,Graham Cormode; Theodore Johnson; Flip Korn; Shan Muthukrishnan; Oliver Spatscheck; Divesh Srivastava,Abstract Many algorithms have been proposed to approximate holistic aggregates; such asquantiles and heavy hitters; over data streams. However; little work has been done toexplore what techniques are required to incorporate these algorithms in a data stream queryprocessor; and to make them useful in practice. In this paper; we study the performanceimplications of using user-defined aggregate functions (UDAFs) to incorporate selection-based and sketch-based algorithms for holistic aggregates into a data stream managementsystem's query processing architecture. We identify key performance bottlenecks andtradeoffs; and propose novel techniques to make these holistic UDAFs fast and space-efficient for use in high-speed data stream applications. We evaluate performance usinggenerated and actual IP packet data; focusing on approximating quantiles and heavy …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,99
Method for providing more informative results in response to a search of electronic documents,*,A method provides a more informative result to a user in connection with the search fordocuments in a database. In particular; the method provides augmented addresses; in theInternet environment augmented universal resource locators; which include an indication ofa document attribute which may be of interest to the user. Such attributes may include anindication of the language of the document (eg; English or Japanese) or the popularity of thedocument.,*,2000,99
Implementation of the CORAL deductive database system,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan; Praveen Seshadri,Abstract CORAL is a deductive database system that supports a rich declarative language;provides a wide range of evaluation methods; and allows a combination of declarative andimperative programming. The data can be persistent on disk or can reside in main-memory.We describe the architecture and implementation of CORAL. There were two importantgoals in the design of the CORAL architecture:(1) to integrate the different evaluationstrategies in a reasonable fashion; and (2) to allow users to influence the optimizationtechniques used so as to exploit the full power of the CORAL implementation. A CORALdeclarative program can be organized as a collection of interacting modules and thismodular structure is the key to satisfying both these goals. The high level module interfaceallows modules with different evaluation techniques to interact in a transparent fashion …,International Conference on Management of Data: Proceedings of the 1993 ACM SIGMOD international conference on Management of data,1993,97
Method for effective indexing of partially dynamic documents,*,A method more efficiently indexes dynamic documents. The method adjusts the frequencywith which dynamic documents are retrieved taking into account the extent to which thedocument varies between its most recent retrievals. Furthermore; the method selectsportions of the document to be indexed based on the substance of the differences betweenrecently retrieved copies.,*,1999,96
Cost-based optimization for magic: Algebra and implementation,Praveen Seshadri; Joseph M Hellerstein; Hamid Pirahesh; TY Leung; Raghu Ramakrishnan; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query; whichdiffer greatly in execution performance. We propose cost-based techniques for selecting anefficient variant from the many choices. Our first contribution is a practical scheme thatmodels magic sets rewriting as a special join method that can be added to any cost-basedquery optimizer. We derive cost formulas that allow an optimizer to choose the best variant ofthe rewriting and to decide whether it is beneficial. The order of complexity of theoptimization process is preserved by limiting the search space in a reasonable manner. Wehave implemented this technique in IBM's DB2 C/S V2 database system. Our performancemeasurements demonstrate that the cost-based magic optimization technique performs …,ACM SIGMOD Record,1996,94
Privbayes: Private data release via bayesian networks,Jun Zhang; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Xiaokui Xiao,Abstract Privacy-preserving data publishing is an important problem that has been the focusof extensive study. The state-of-the-art solution for this problem is differential privacy; whichoffers a strong degree of privacy protection without making restrictive assumptions about theadversary. Existing techniques using differential privacy; however; cannot effectively handlethe publication of high-dimensional data. In particular; when the input dataset contains alarge number of attributes; existing methods require injecting a prohibitive amount of noisecompared to the signal in the data; which renders the published data next to useless. Toaddress the deficiency of the existing methods; this paper presents P riv B ayes; adifferentially private method for releasing high-dimensional data. Given a dataset D; P riv Bayes first constructs a Bayesian network N; which (i) provides a succinct model of the …,ACM Transactions on Database Systems (TODS),2017,92
Data stream query processing,Nick Koudas; Divesh Srivastava,Gigascope: GSQL Queries select tb; srcIP; sum (len) from IPv4 where protocol= 6 group bytime/60 as tb; srcIP having count (*)> 5 select S. tstmp; S. srcIP; S. destIP; S. srcPort; S.destPort (A. tstmp–S. tstmp) as rtt from tcp_syn S; tcp_syn_ack A where S. srcIP= A. destIPand S. destIP= A. srcIP and S. srcPort= A. destPort and S. destPort= A. srcPort and S. tb= A.tb,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING,2005,91
Optimization of queries using relational algebraic theta-semijoin operator,*,A collection of equivalence rules involving the multiset version of the relational algebraictheta-semijoin operator is used to generate relational algebraic expressions equivalent to acomputer programming language query. These expressions may be employed as a searchspace which is utilized by; for example; optimizing software or software that determines theequivalency of queries. Cost formulas for the multiset version of the theta-semijoin operatormay be used in computing cost estimates for the generated expressions. Based on thesecomputed cost estimates; the least costly implementation of a complex query is determined.Thus; queries are cost-based optimized on both a local and global basis by use of therelational algebraic theta-semijoin operator.,*,2000,90
Dense subgraph maintenance under streaming edge weight updates for real-time story identification,Albert Angel; Nikos Sarkas; Nick Koudas; Divesh Srivastava,Abstract Recent years have witnessed an unprecedented proliferation of social media.People around the globe author; every day; millions of blog posts; micro-blog posts; socialnetwork status updates; etc. This rich stream of information can be used to identify; on anongoing basis; emerging stories; and events that capture popular attention. Stories can beidentified via groups of tightly-coupled real-world entities; namely the people; locations;products; etc.; that are involved in the story. The sheer scale; and rapid evolution of the datainvolved necessitate highly efficient techniques for identifying important stories at every pointof time. The main challenge in real-time story identification is the maintenance of densesubgraphs (corresponding to groups of tightly-coupled entities) under streaming edgeweight updates (resulting from a stream of user-generated content). This is the first work …,Proceedings of the VLDB Endowment,2012,89
Grouping in XML,Stelios Paparizos; Shurug Al-Khalifa; H Jagadish; Laks Lakshmanan; Andrew Nierman; Divesh Srivastava; Yuqing Wu,Abstract XML permits repeated and missing sub-elements; and missing attributes. Wediscuss the consequent implications on grouping; both with respect to specification and withrespect to implementation. The techniques described here have been implemented in theTIMBER native XML database system being developed at the University of Michigan.,XML-Based Data Management and Multimedia Engineering—EDBT 2002 Workshops,2002,89
Coral++: Adding object-orientation to a logic database language,Divesh Srivastava; Raghu Ramakrishnan; Praveen Seshadri; S Sudarshan,*,Proceedings of the International Conference on Very Large Data Bases,1993,88
Linking temporal records,Pei Li; X Dong; Andrea Maurino; Divesh Srivastava,ABSTRACT Many data sets contain temporal records over a long period of time; each recordis associated with a time stamp and describes some aspects of a realworld entity at thatparticular time (eg; author information in DBLP). In such cases; we often wish to identifyrecords that describe the same entity over time and so be able to enable interestinglongitudinal data analysis. However; existing record linkage techniques ignore the temporalinformation and can fall short for temporal data. This paper studies linking temporal records.First; we apply time decay to capture the effect of elapsed time on entity value evolution.Second; instead of comparing each pair of records locally; we propose clustering methodsthat consider time order of the records and make global decisions. Experimental resultsshow that our algorithms significantly outperform traditional linkage methods on various …,Proceedings of the VLDB Endowment,2011,87
The threshold join algorithm for top-k queries in distributed sensor networks,Demetrios Zeinalipour-Yazti; Zografoula Vagena; Dimitrios Gunopulos; Vana Kalogeraki; V Tsotras; Michail Vlachos; Nick Koudas; Divesh Srivastava,Abstract In this paper we present the Threshold Join Algorithm (TJA); which is an efficientTOP-k query processing algorithm for distributed sensor networks. The objective of a top-kquery is to find the k highest ranked answers to a user defined similarity function. Theevaluation of such a query in a sensor network environment is associated with the transfer ofdata over an extremely expensive communication medium. TJA uses a non-uniformthreshold on the queried attribute in order to minimize the number of tuples that have to betransferred towards the querying node. Additionally; TJA resolves queries in the networkrather than in a centralized fashion; which minimizes even more the consumption ofbandwidth and delay. Our preliminary experimental results; using our trace driven simulator;show that TJA is both practical and efficient.,Proceedings of the 2nd international workshop on Data management for sensor networks,2005,87
Colorful XML: one hierarchy isn't enough,HV Jagadish; Laks VS Lakshmanan; Monica Scannapieco; Divesh Srivastava; Nuwee Wiwatwattana,Abstract XML has a tree-structured data model; which is used to uniformly representstructured as well as semi-structured data; and also enable concise query specification inXQuery; via the use of its XPath (twig) patterns. This in turn can leverage the recentlydeveloped technology of structural join algorithms to evaluate the query efficiently. In thispaper; we identify a fundamental tension in XML data modeling:(i) data represented as deeptrees (which can make effective use of twig patterns) are often un-normalized; leading toupdate anomalies; while (ii) normalized data tends to be shallow; resulting in heavy use ofexpensive value-based joins in queries. Our solution to this data modeling problem is anovel multi-colored trees (MCT) logical data model; which is an evolutionary extension of theXML data model; and permits trees with multi-colored nodes to signify their participation …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,86
Compressed accessibility map: Efficient access control for XML,Ting Yu; Laks VS Lakshmanan; Divesh Srivastava; HV Jagadish,This chapter proposes a space-and time-efficient solution to the access control problem foreXtensible Markup Language (XML) data. The solution is based on a novel notion of acompressed accessibility map (CAM); which compactly identifies the XML data items towhich a user has access; by exploiting structural locality of accessibility in tree-structureddata. Transacting business over the Internet using XML is becoming more and more of areality as one moves towards a world of Internet-worked data; with applications requiringaccess to data on the Internet. In this setting; there is a need for much more sophisticatedtypes of access control than is permitted by simple firewalls. XML is widely regarded as apromising means for data representation integration; and exchange. As companies transactbusiness over the Internet; the sensitive nature of the information mandates that access …,*,2002,86
Subsumption and indexing in constraint query languages with linear arithmetic constraints,Divesh Srivastava,Abstract Bottom-up evaluation of a program-query pair in a constraint query language (CQL)starts with the facts in the database and repeatedly applies the rules of the program; initerations; to compute new facts; until we have reached a fixpoint. Checking if a fixpoint hasbeen reached amounts to checking if any “new” facts were computed in an iteration. Such acheck also enhances efficiency in that subsumed facts can be discarded; and not be used tomake any further derivations in subsequent iterations; if we use Semi-naive evaluation. Weshow that the problem of subsumption in CQLs with linear arithmetic constraints is co-NPcomplete; and present a deterministic algorithm; based on the divide and conquer strategy;for this problem. We also identify polynomial-time sufficient conditions for subsumption andnon-subsumption in CQLs with linear arithmetic constraints. We adapt indexing strategies …,Annals of Mathematics and Artificial Intelligence,1993,84
Intensional associations between data and metadata,Divesh Srivastava; Yannis Velegrakis,Abstract There is a growing need to associate a variety of metadata with the underlying data;but a simple; elegant approach to uniformly model and query both the data and themetadata has been elusive. In this paper; we argue that (1) the relational model augmentedwith queries as data values is a natural way to uniformly model data; arbitrary metadata andtheir associations; and (2) relational queries with a join mechanism augmented to permitmatching of query result relations; instead of only atomic values; is an elegant way touniformly query across data and metadata. We describe the architecture of a system wehave prototyped for this purpose; demonstrate the generality of our approach and evaluatethe performance of the system; in comparison with previous proposals for metadatamanagement.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,83
MEAD: support for Real‐Time Fault‐Tolerant CORBA,Priya Narasimhan; TA Dumitraş; Aaron M Paulos; Soila M Pertet; Carlos F Reverte; Joseph G Slember; Deepti Srivastava,Abstract The OMG's Real-Time CORBA (RT-CORBA) and Fault-Tolerant CORBA (FT-CORBA) specifications make it possible for today's CORBA implementations to exhibit eitherreal-time or fault tolerance in isolation. While real-time requires a priori knowledge of thesystem's temporal operation; fault tolerance necessarily deals with faults that occurunexpectedly; and with possibly unpredictable fault recovery times. The MEAD (Middlewarefor Embedded Adaptive Dependability) system attempts to identify and to reconcile theconflicts between real-time and fault tolerance; in a resource-aware manner; for distributedCORBA applications. MEAD supports transparent yet tunable fault tolerance in real-time;proactive dependability; resource-aware system adaptation to crash; communication andtiming faults with bounded fault detection and fault recovery. Copyright© 2005 John Wiley …,Concurrency and Computation: Practice and Experience,2005,83
Method for effective indexing of partially dynamic documents,*,A method more efficiently indexes dynamic documents. The method adjusts the frequencywith which dynamic documents are retrieved taking into account the extent to which thedocument varies between its most recent retrievals. Furthermore; the method selectsportions of the document to be indexed based on the substance of the differences betweenrecently retrieved copies.,*,2003,83
Data quality: The other face of big data,Barna Saha; Divesh Srivastava,In our Big Data era; data is being generated; collected and analyzed at an unprecedentedscale; and data-driven decision making is sweeping through all aspects of society. Recentstudies have shown that poor quality data is prevalent in large databases and on the Web.Since poor quality data can have serious consequences on the results of data analyses; theimportance of veracity; the fourthV'of big data is increasingly being recognized. In thistutorial; we highlight the substantial challenges that the first threeV's; volume; velocity andvariety; bring to dealing with veracity in big data. Due to the sheer volume and velocity ofdata; one needs to understand and (possibly) repair erroneous data in a scalable and timelymanner. With the variety of data; often from a diversity of sources; data quality rules cannotbe specified a priori; one needs to let the “data to speak for itself” in order to discover the …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,82
Querying network directories,HV Jagadish; Laks VS Lakshmanan; Tova Milo; Divesh Srivastava; Dimitra Vista,Abstract Heirarchically structured directories have recently proliferated with the growth of theInternet; and are being used to store not only address books and contact information forpeople; but also personal profiles; network resource information; and network and servicepolicies. These systems provide a means for managing scale and heterogeneity; whileallowing for conceptual unity and autonomy across multiple directory servers in the network;in a way for superior to what conventional relational or object-oriented databases offer. Yet;in deployed systems today; much of the data is modeled in an ad hoc manner; and many ofthe more sophisticated “queries” involve navigational access. In this paper; we develop thecore of a formal data model for network directories; and propose a sequence of efficientlycomputable query languages with increasing expressive power. The directory data model …,ACM SIGMOD Record,1999,80
Summarizing relational databases,Xiaoyan Yang; Cecilia M Procopiuc; Divesh Srivastava,Abstract Complex databases are challenging to explore and query by users unfamiliar withtheir schemas. Enterprise databases often have hundreds of inter-linked tables; so evenwhen extensive documentation is available; new users must spend a considerable amountof time understanding the schema before they can retrieve any information from thedatabase. The problem is aggravated if the documentation is missing or outdated; whichmay happen with legacy databases. In this paper we identify limitations of previousapproaches to address this vexing problem; and propose a principled approach tosummarizing the contents of a relational database; so that a user can determine at a glancethe type of information it contains; and the main tables in which that information resides. Ourapproach has three components: First; we define the importance of each table in the …,Proceedings of the VLDB Endowment,2009,79
On effective multi-dimensional indexing for strings,HV Jagadish; Nick Koudas; Divesh Srivastava,Abstract As databases have expanded in scope from storing purely business data to includeXML documents; product catalogs; e-mail messages; and directory data; it has becomeincreasingly important to search databases based on wild-card string matching: prefixmatching; for example; is more common (and useful) than exact matching; for such data. Inmany cases; matches need to be on multiple attributes/dimensions; with correlationsbetween the dimensions. Traditional multi-dimensional index structures; designed with(fixed length) numeric data in mind; are not suitable for matching unbounded length stringdata. In this paper; we describe a general technique for adapting a multi-dimensional indexstructure for wild-card indexing of unbounded length string data. The key ideas are (a) acarefully developed mapping function from strings to rational numbers;(b) representing an …,ACM SIGMOD Record,2000,79
Apparatus and methods for retrieving information,*,A query translator translates a query between a graphical user interface and a knowledgerepresentation system. The knowledge representation system reformulates the query andgenerates an access plan to access data requested by the query. The access plan utilizesseveral different protocols to access the query information located in dissimilar databasesdistributed throughout a network. The knowledge representation system generates theaccess plan by first processing the query through a world view which defines the informationin conceptual terms that a human being would understand and then processes the querythrough a system/network view which redefines the query into network and database accessinformation so that the data requested by the query can be located. Placing the world viewand the system network view in the knowledge representation system enables real-time …,*,1997,79
Anonymized data: generation; models; usage,Graham Cormode; Divesh Srivastava,Abstract Data anonymization techniques have been the subject of intense investigation inrecent years; for many kinds of structured data; including tabular; graph and item set data.They enable publication of detailed information; which permits ad hoc queries and analyses;while guaranteeing the privacy of sensitive information in the data against a variety ofattacks. In this tutorial; we aim to present a unified framework of data anonymizationtechniques; viewed through the lens of uncertainty. Essentially; anonymized data describesa set of possible worlds; one of which corresponds to the original data. We show thatanonymization approaches such as suppression; generalization; perturbation andpermutation generate different working models of uncertain data; some of which have beenwell studied; while others open new directions for research. We demonstrate that the …,Proceedings of the 35th SIGMOD international conference on Management of data,2009,76
Magic sets and bottom-up evaluation of well-founded models,David B Kemp; Peter J Stuckey; Divesh Srivastava,Abstract We present a bottom-up operational procedure for computing well-founded modelsof allowed DATALOG programs with negation. This procedure provides a practical methodof handling programs that involve unstrati ed negation in a manner that may be mixed withother evaluation approaches; such as semi-naive evaluation. We also de ne classes ofprograms and sips for which the magic sets transformation preserves well-founded modelswith respect to the query. The class of programs and sips we consider strictly subsume thosealready considered in the literature; and include strati ed programs (with any choice of sips);modularly strati ed programs (with left-to-right sips) and programs with three-valued well-founded models (with well-founded sips). For these programs and sips; our procedure forcomputing well-founded models is applicable to the magic programs; thus allowing …,Proceedings of the 1991 Int. Symposium on Logic Programming,1991,76
Optimal histograms for hierarchical range queries,Nick Koudas; S Muthukrishnan; Divesh Srivastava,Now there is tremendous interest in data warehousing and OLAP applications. OLAPapplications typically view data as having multiple logical dimensions eg; product; locationwith natural hierarchies defined on each dimension; and analyze the behavior of variousmeasure attributes eg; sales; volume in terms of the dimensions. OLAP queries typicallyinvolve hierarchical selections on some of the dimensions eg; product is classified under thejeans product category; or location is in the north-east region; often aggregating measureattributes see; eg; 6. Cost-based query optimization of such OLAP queries needs goodestimates of the selectivity of hierarchical selections. Histograms capture attribute valuedistribution statistics in a space-e cient fashion. They have been designed to work well fornumeric attribute value domains; and have long been used to support cost-based query …,Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2000,75
Fusing data with correlations,Ravali Pochampally; Anish Das Sarma; Xin Luna Dong; Alexandra Meliou; Divesh Srivastava,Abstract Many applications rely on Web data and extraction systems to accomplishknowledge-driven tasks. Web information is not curated; so many sources provideinaccurate; or conflicting information. Moreover; extraction systems introduce additionalnoise to the data. We wish to automatically distinguish correct data and erroneous data forcreating a cleaner set of integrated data. Previous work has shown that a naive votingstrategy that trusts data provided by the majority or at least a certain number of sources maynot work well in the presence of copying between the sources. However; correlationbetween sources can be much broader than copying: sources may provide data fromcomplementary domains (negative correlation); extractors may focus on different types ofinformation (negative correlation); and extractors may apply common rules in extraction …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,74
Routing XML queries,Nick Koudas; Michael Rabinovich; Divesh Srivastava; Ting Yu,In file-sharing P2P networks; a fundamental problem is that of identifying databases that arerelevant to user queries. This problem is referred to as the location problem in P2P literature.We propose a scalable solution to the location problem in a data-sharing P2P network;consisting of a network of XML database nodes and XML router nodes; and make thefollowing contributions. We develop the internal organization and routing protocols for theXML router nodes; to enable scalable XPath query and update processing; under the openand the agreement cooperation models between nodes. Since router nodes tend to bememory constrained; we facilitate a space/performance tradeoff by permitting aggregatedrouting states; and developing algorithms for generating and using such aggregatedinformation. We experimentally demonstrate the scalability of our approach; and the …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,74
Substring selectivity estimation,HV Jagadish; Raymond T Ng; Divesh Srivastava,Abstract With the explosion of the Internet; LDAP directories and XML; there is an evergreater need to evaluate queries involving (sub) string matching. Effective queryoptimization in this context requires good selectivity estimates. In this paper; we use prunedcount-suffix trees as the basic framework for substring selectivity estimation. We present anovel technique to obtain a good estimate for a given substring matching query; called MO(for Maximal Overlap); that estimates the selectivity of a query based on all maximalsubstrings of the query in the pruned count-suffix tree. We show that MO is provably betterthan the (independence-based) substring selectivity estimation technique proposed byKrishnan et al.[6]; called KVI; under the natural assumption that strings exhibit the so-called“short memory” property. We complement our analysis with an experiment; using a real …,Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,1999,74
Rule ordering in bottom-up fixpoint evaluation of logic programs,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract Logic programs can be evaluated bottom-up by repeatedly applying all rules; in“iterations”; until the fixpoint is reached. However; it is often desirable—and in some cases;eg programs with stratified negation; even necessary to guarantee the semantics-—to applythe rules in some order. A desirable property of a ﬁxpoint evaluation algorithm is that it doesnot repeat inferences. We say that such an algorithm has the non-repetition property; andthe “semi-naive” algorithms in the literature have this property. However; these algorithms donot address the issue of how to apply rules in a speciﬁed order while retaining the non-repetition property. We present two algorithms that address this issue. One of them (GSN) iscapable of dealing with a wide range of rule orderings but with a little more overhead thanthe usual semi-naive algorithm (which we call BSN). The other (PSN) handles a smaller …,*,1991,74
Systems and methods for distributing video on demand,*,A method of receiving content includes joining an in-progress multicast stream to receive afirst portion of a content. The method further includes sending a request to a peer for a catch-up portion of the content; the request including a deadline for delivery of the content; andreceiving the catch-up portion of the content from the peer prior to the deadline.,*,2014,73
Group linkage,Byung-Won On; Nick Koudas; Dongwon Lee; Divesh Srivastava,Poor quality data is prevalent in databases due to a variety of reasons; includingtranscription errors; lack of standards for recording database fields; etc. To be able to queryand integrate such data; considerable recent work has focused on the record linkageproblem; ie; determine if two entities represented as relational records are approximately thesame. Often entities are represented as groups of relational records; rather than individualrelational records; eg; households in a census survey consist of a group of persons. We referto the problem of determining if two entities represented as groups are approximately thesame as group linkage. Intuitively; two groups can be linked to each other if (i) there is highenough similarity between" matching" pairs of individual records that constitute the twogroups; and (ii) there is a large fraction of such matching record pairs. In this paper; we …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,73
Declarative message addressing,*,A messaging system; and method of operation thereof; which supports combinations ofdirectory and mailing list addressing mechanisms. Intended message recipients arespecified as declarative addresses; which may include combinations of directory andmailing list information. The messaging system includes a messaging server and an addressresolution module. The messaging server receives a message from a sender system andtransmits the message to the recipient system. The address resolution module; which iscoupled to the messaging server; receives a declarative address associated with themessage; resolves the declarative address into at least one messaging address andtransmits the at least one messaging address to the messaging server. In one embodiment;a database system may be coupled to the address resolution module to allow address …,*,2001,73
Cost-based maintenance of materialized views,*,A method of incrementally maintaining a first materialized view of data in a database; bymeans of an additional materialized view; first determines whether a cost in time ofincrementally maintaining the first materialized view with the additional materialized view isless than the cost of incrementally maintaining the first materialized view without theadditional materialized view. The method creates the additional materialized view only if thecost in time is less therewith. Determining whether the cost of employing an additionalmaterialized view is less includes using an expression directed acyclic graph thatcorresponds to the first materialized view. Another method of determining whether the cost isless includes pruning an expression directed acyclic graph to produce a single expressiontree; and using the single expression tree to determine whether the cost is less. Both the …,*,2000,73
Method and system for using materialized views to evaluate queries involving aggregation,*,The present invention is a method and system for using materialized views to computeanswers to SQL queries with grouping and aggregation. A query is evaluated a using amaterialized view. The materialized view is semantically analyzed to determine whether thematerialized view is usable in evaluating an input query. The semantic analysis includesdetermining that the materialized view does not project out any columns needed to evaluatethe input query and determining that the view does not discard any tuple that satisfies acondition enforced in the input query. If the view is usable; the input query is rewritten toproduce an output query that is multi-set equivalent to the input query and that specifies oneor more occurrences of the materialized view as a source of information to be returned bythe output query. The output query is then evaluated. The semantic analysis and rewriting …,*,1999,72
Forward decay: A practical time decay model for streaming systems,Graham Cormode; Vladislav Shkapenyuk; Divesh Srivastava; Bojian Xu,Temporal data analysis in data warehouses and datastreaming systems often uses timedecay to reduce the importance of older tuples; without eliminating their influence; on theresults of the analysis. While exponential time decay is commonly used in practice; otherdecay functions (eg polynomial decay) are not; even though they have been identified asuseful. We argue that this is because the usual definitions of time decay are" backwards": thedecayed weight of a tuple is based on its age; measured backward from the current time.Since this age is constantly changing; such decay is too complex and unwieldy for scalableimplementation. In this paper; we propose a new class of" forward" decay functions basedon measuring forward from a fixed point in time. We show that this model captures the morepractical models already known; such as exponential decay and landmark windows; but …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,71
Controlling the search in bottom-up evaluation,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract Bottom-up evaluation of queries on deductive databases has many advantagesover an evaluation scheme such as Prolog. It is sound and complete with respect to thedeclarative semantics of least Herbrand models for positive Horn clause programs. Inparticular; it is able to avoid in nite loops by detecting repeated (possibly cyclic) subgoals.Further; in many database applications; it is more e cient than Prolog due to its set-orientedness. However; the completely set-oriented; breadth-rst search strategy of bottomupevaluation has certain disadvantages. For example; to evaluate several classes of programswith negation (or aggregation); it is necessary to order the inferences; in essence; we mustevaluate all answers to a negative subgoal before making an inference that depends uponthe negative subgoal. A completely breadth-rst search strategy (14]) would have to …,Proceedings of the Joint International Conference and Symposium on Logic Programming,1992,70
Database analysis using clusters,*,A method for mapping relationships in a database results in a cluster graph. Arepresentative sample of records in each of a plurality of tables in the database is analyzedfor nearest neighbor join edges instantiated by the record. Records with correspondingnearest neighbor join edges are grouped into clusters. Cluster pairs which share a joinrelationship between two tables are identified. A weighting may be applied to cluster pairsbased on the number of records for the cluster pair. Meaningful cluster pairs above aweighted threshold may be ordered according to table and displayed as a cluster graph.Analyses of the cluster graph may reveal important characteristics of the database.,*,2012,68
Sequential dependencies,Lukasz Golab; Howard Karloff; Flip Korn; Avishek Saha; Divesh Srivastava,Abstract We study sequential dependencies that express the semantics of data with ordereddomains and help identify quality problems with such data. Given an interval g; we write X→g Y to denote that the difference between the Y-attribute values of any two consecutiverecords; when sorted on X; must be in g. For example; time→(0;∞) sequence_numberindicates that sequence numbers are strictly increasing over time; whereassequence_number→[4; 5] time means that the time" gaps" between consecutive sequencenumbers are between 4 and 5. Sequential dependencies express relationships betweenordered attributes; and identify missing (gaps too large); extraneous (gaps too small) and out-of-order data. To make sequential dependencies applicable to real-world data; we relaxtheir requirements and allow them to hold approximately (with some exceptions) and …,Proceedings of the VLDB Endowment,2009,68
Space-and time-efficient deterministic algorithms for biased quantiles over data streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Skew is prevalent in data streams; and should be taken into account by algorithmsthat analyze the data. The problem of finding" biased quantiles"—that is; approximatequantiles which must be more accurate for more extreme values—is a framework forsummarizing such skewed data on data streams. We present the first deterministicalgorithms for answering biased quantiles queries accurately with small—sublinear in theinput size—space and time bounds in one pass. The space bound is near-optimal; and theamortized update cost is close to constant; making it practical for handling high speednetwork data streams. We not only demonstrate theoretical properties of the algorithm; butalso show it uses less space than existing methods in many practical settings; and is fast tomaintain.,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,68
Method for improving the results of a search in a structured database,*,A method enhances the presentation of search results from a structured database. Inaccordance with the method; a search query including two or more attribute/value pairs ispresented to a system. The system then identifies a plurality of records which each minimallymatch the search query. Each document or record in the plurality of identified records isassigned a weight based on at least two factors: the extent to which the record matches theentire search query; and the relative frequency with which the attribute/value pair thatmatches the given record matches the records of the remainder of the structured database.The plurality of records that minimally match the search query are then identified to therequester in ranked order based on the assigned weights.,*,2000,68
Method of calculating tuples for data cubes,*,A method and apparatus of calculating data cubes is shown in which a data set is partitionedinto memory sized data fragments and cuboid tuples are calculated from the data fragments.A search lattice of the data cube is used as a basis for ordering calculations of lowerdimensional cuboids in the data cube. Identification of a minimum number of paths throughthe lattice that is sufficient to traverse all nodes in the lattice is achieved by iterativelyduplicating twice all paths in a lower dimensional space; distributing a new attribute to thefirst duplicate; moving end points from paths of the second duplicate to a corresponding pathin the first duplicate and merging the first and second duplicates.,*,1999,68
Bottom-up evaluation and query optimization of well-founded models,David B Kemp; Divesh Srivastava; Peter J Stuckey,Abstract We present a bottom-up operational procedure for computing well-founded modelsof allowed programs with negation. This procedure provides a practical method of handlingprograms that involve unstratified negation in a manner that may be mixed with otherevaluation approaches; such as semi-naive evaluation and various programtransformations. We define classes of programs and sideways information of passingstrategies (sips) for which the magic sets transformation preserves well-founded models withrespect to the query. The classes of programs and sips we consider strictly subsume thosealready considered in the literature; and include stratified programs (with any choice of sips);left-to-right modularly stratified programs (with left-to-right sips) and arbitrary programs (withwell-founded sips). For these programs and sips; our procedure for computing well …,Theoretical computer science,1995,68
Effecting constraint magic rewriting on a query with the multiset version of the relational algebric theta-semijoin operator,*,An equivalence rule having the multiset version of the relational algebraic theta-semijoinoperator is used in effectuating Constraint Magic rewriting on a computer programminglanguage query having non-equality; as well as equality; predicates. In particular; the ruleeffectuates Constraint Magic rewriting for a single join. When applied repeatedly on asequence of joins; Constraint Magic rewriting is effectuated for a single block query. The rulemay be used to generate relational algebraic expressions to optimize or determine theequivalency of queries. Cost estimates for alternative ways of evaluating a complex querycan be efficiently computed. Based on these computed cost estimates; the least costlyimplementation of a complex query is determined.,*,2000,67
Anonymizing bipartite graph data using safe groupings,Graham Cormode; Divesh Srivastava; Ting Yu; Qing Zhang,Abstract Private data often come in the form of associations between entities; such ascustomers and products bought from a pharmacy; which are naturally represented in theform of a large; sparse bipartite graph. As with tabular data; it is desirable to be able topublish anonymized versions of such data; to allow others to perform ad hoc analysis ofaggregate graph properties. However; existing tabular anonymization techniques do notgive useful or meaningful results when applied to graphs: small changes or masking of theedge structure can radically change aggregate graph properties. We introduce a new familyof anonymizations for bipartite graph data; called (k; ℓ)-groupings. These groupingspreserve the underlying graph structure perfectly; and instead anonymize the mapping fromentities to nodes of the graph. We identify a class of “safe”(k; ℓ)-groupings that have …,The VLDB Journal,2010,66
CPM: Adaptive video-on-demand with cooperative peer assists and multicast,Vijay Gopalakrishnan; Bobby Bhattacharjee; KK Ramakrishnan; Rittwik Jana; Divesh Srivastava,We present CPM; a unified approach that exploits server multicast; assisted by peerdownloads; to provide efficient video-on-demand (VoD) in a service provider environment.We describe our architecture and show how CPM is designed to dynamically adapt to awide range of situations including highly different peer-upload bandwidths; contentpopularity; user request arrival patterns; video library size; and subscriber population. Wedemonstrate the effectiveness of CPM using simulations (based on an actualimplementation codebase) across the range of situations described above and show thatCPM does significantly better than traditional unicast; different forms of multicast; as well aspeer-to-peer schemes. Along with synthetic parameters; we augment our experiments usingdata from a deployed VoD service to evaluate the performance of CPM.,INFOCOM 2009; IEEE,2009,66
Efficient OLAP query processing in distributed data warehouses,Michael O Akinde; Michael H Böhlen; Theodore Johnson; Laks VS Lakshmanan; Divesh Srivastava,Abstract The success of Internet applications has led to an explosive growth in the demandfor bandwidth from Internet Service Providers. Managing an Internet protocol networkrequires collecting and analyzing network data; such as flow-level traffic statistics. Suchanalyses can typically be expressed as OLAP queries; eg; correlated aggregate queries anddata cubes. Current day OLAP tools for this task assume the availability of the data in acentralized data warehouse. However; the inherently distributed nature of data collectionand the huge amount of data extracted at each collection point make it impractical to gatherall data at a centralized site. One solution is to maintain a distributed data warehouse;consisting of local data warehouses at each collection point and a coordinator site; with mostof the processing being performed at the local sites. In this paper; we consider the …,Information Systems,2003,66
A compressed accessibility map for XML,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,Abstract XML is the undisputed standard for data representation and exchange. Ascompanies transact business over the Internet; letting authorized customers directly access;and even modify; XML data offers many advantages in terms of cost; accuracy; andtimeliness. Given the complex business relationships between companies; and the sensitivenature of information; access must be provided selectively; using sophisticated accesscontrol specifications. Using the specification directly to determine if a user has access to anXML data item can be extremely inefficient. The alternative of fully materializing; for eachdata item; the users authorized to access it can be space-inefficient. In this article; weintroduce a compressed accessibility map (CAM) as a space-and time-efficient solution tothe access control problem for XML data. A CAM compactly identifies the XML data items …,ACM Transactions on Database Systems (TODS),2004,65
Text joins for data cleansing and integration in an rdbms,Luis Gravano; Panagiotis G Ipeirotis; Nick Koudas; Divesh Srivastava,An organization's data records are often noisy because of transcription errors; incompleteinformation; lack of standard formats for textual data or combinations thereof. A fundamentaltask in a data cleaning system is matching textual attributes that refer to the same entity (eg;organization name or address). This matching is effectively performed via the cosinesimilarity metric from the information retrieval field. For robustness and scalability; these" textjoins" are best done inside an RDBMS; which is where the data is likely to reside.Unfortunately; computing an exact answer to a text join can be expensive. We propose anapproximate; sampling-based text join execution strategy that can be robustly executed in astandard; unmodified RDBMS.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,65
Sailing the information ocean with awareness of currents: Discovery and application of source dependence,Laure Berti-Equille; Anish Das Sarma; Amelie Marian; Divesh Srivastava,Abstract: The Web has enabled the availability of a huge amount of useful information; buthas also eased the ability to spread false information and rumors across multiple sources;making it hard to distinguish between what is true and what is not. Recent examples includethe premature Steve Jobs obituary; the second bankruptcy of United airlines; the creation ofBlack Holes by the operation of the Large Hadron Collider; etc. Since it is important to permitthe expression of dissenting and conflicting opinions; it would be a fallacy to try to ensurethat the Web provides only consistent information. However; to help in separating the wheatfrom the chaff; it is essential to be able to determine dependence between sources. Giventhe huge number of data sources and the vast volume of conflicting data available on theWeb; doing so in a scalable manner is extremely challenging and has not been …,arXiv preprint arXiv:0909.1776,2009,64
Incremental record linkage,Anja Gruenheid; Xin Luna Dong; Divesh Srivastava,Abstract Record linkage clusters records such that each cluster corresponds to a singledistinct real-world entity. It is a crucial step in data cleaning and data integration. In the bigdata era; the velocity of data updates is often high; quickly making previous linkage resultsobsolete. This paper presents an end-to-end framework that can incrementally andefficiently update linkage results when data updates arrive. Our algorithms not only allowmerging records in the updates with existing clusters; but also allow leveraging newevidence from the updates to fix previous linkage errors. Experimental results on three realand synthetic data sets show that our algorithms can significantly reduce linkage timewithout sacrificing linkage quality.,Proceedings of the VLDB Endowment,2014,61
Metric functional dependencies,Nick Koudas; Avishek Saha; Divesh Srivastava; Suresh Venkatasubramanian,When merging data from various sources; it is often the case that small variations in dataformat and interpretation cause traditional functional dependencies (FDs) to be violated;without there being an intrinsic violation of semantics. Examples include differing addressformats; or different reported latitude/longitudes for a given address. In this paper; we definemetric functional dependencies; which strictly generalize traditional FDs by allowing smalldifferences (controlled by a metric) in values of the consequent attribute of an FD. Wepresent efficient algorithms for the verification problem: determining whether a given metricFD holds for a given relation. We experimentally demonstrate the validity and efficiency ofour approach on various data sets that lie in multidimensional spaces.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,61
Minimizing minimality and maximizing utility: Analyzing method-based attacks on anonymized data,Graham Cormode; Divesh Srivastava; Ninghui Li; Tiancheng Li,Abstract The principle of anonymization for data sharing has become a very popularparadigm for the preservation of privacy of the data subjects. Since the introduction of k-anonymity; dozens of methods and enhanced privacy definitions have been proposed.However; over-eager attempts to minimize the information lost by the anonymizationpotentially allow private information to be inferred. Proof-of-concept of this" minimality attack"has been demonstrated for a variety of algorithms and definitions [16]. In this paper; weprovide a comprehensive analysis and study of this attack; and demonstrate that with care itseffect can be almost entirely countered. The attack allows an adversary to increase his(probabilistic) belief in certain facts about individuals over the data. We show that (a) a largeclass of algorithms are not affected by this attack;(b) for a class of algorithms that have a" …,Proceedings of the VLDB Endowment,2010,60
Complex aggregation at multiple granularities,Kenneth Ross; Divesh Srivastava; Damianos Chatziantoniou,Abstract Datacube queries compute simple aggregates at multiple granularities. In thispaper we examine the more general and useful problem of computing a complex subqueryinvolving multiple dependent aggregates at multiple granularities. We call such queries“multi-feature cubes.” An example is “Broken down by all combinations of month andcustomer; find the fraction of the total sales in 1996 of a particular item due to supplierssupplying within 10% of the minimum price (within the group); showing all subtotals acrosseach dimension.” We classify multi-feature cubes based on the extent to which finegranularity results can be used to compute coarse granularity results; this classificationincludes distributive; algebraic and holistic multi-feature cubes. We provide syntacticsufficient conditions to determine when a multi-feature cube is either distributive or …,Advances in Database Technology—EDBT'98,1998,60
On multi-column foreign key discovery,Meihui Zhang; Marios Hadjieleftheriou; Beng Chin Ooi; Cecilia M Procopiuc; Divesh Srivastava,Abstract A foreign/primary key relationship between relational tables is one of the mostimportant constraints in a database. From a data analysis perspective; discovering foreignkeys is a crucial step in understanding and working with the data. Nevertheless; more oftenthan not; foreign key constraints are not specified in the data; for various reasons; eg; someassociations are not known to designers but are inherent in the data; while others becomeinvalid due to data inconsistencies. This work proposes a robust algorithm for discoveringsingle-column and multi-column foreign keys. Previous work concentrated mostly ondiscovering single-column foreign keys using a variety of rules; like inclusion dependencies;column names; and minimum/maximum values. We first propose a general rule; termedRandomness; that subsumes a variety of other rules. We then develop efficient …,Proceedings of the VLDB Endowment,2010,59
Benchmarking declarative approximate selection predicates,Amit Chandel; Oktie Hassanzadeh; Nick Koudas; Mohammad Sadoghi; Divesh Srivastava,Abstract Declarative data quality has been an active research topic. The fundamentalprinciple behind a declarative approach to data quality is the use of declarative statementsto realize data quality primitives on top of any relational data source. A primary advantage ofsuch an approach is the ease of use and integration with existing applications. Over the lastfew years several similarity predicates have been proposed for common quality primitives(approximate selections; joins; etc) and have been fully expressed using declarative SQLstatements. In this paper we propose new similarity predicates along with their declarativerealization; based on notions of probabilistic information retrieval. In particular we show howlanguage models and hidden Markov models can be utilized as similarity predicates for dataquality and present their full declarative instantiation. We also show how other scoring …,International Conference on Management of Data: Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,57
Differentially private summaries for sparse data,Graham Cormode; Cecilia Procopiuc; Divesh Srivastava; Thanh TL Tran,Abstract Differential privacy is fast becoming the method of choice for releasing data understrong privacy guarantees. A standard mechanism is to add noise to the counts incontingency tables derived from the dataset. However; when the dataset is sparse in itsunderlying domain; this vastly increases the size of the published data; to the point ofmaking the mechanism infeasible. We propose a general framework to overcome thisproblem. Our approach releases a compact summary of the noisy data with the same privacyguarantee and with similar utility. Our main result is an efficient method for computing thesummary directly from the input data; without materializing the vast noisy data. Weinstantiate this general framework for several summarization methods. Our experimentsshow that this is a highly practical solution: The summaries are up to 1000 times smaller …,Proceedings of the 15th International Conference on Database Theory,2012,56
Index structures for matching XML twigs using relational query processors,Zhiyuan Chen; Johannes Gehrke; Flip Korn; Nick Koudas; Jayavel Shanmugasundaram; Divesh Srivastava,Abstract Various index structures have been proposed to speed up the evaluation of XMLpath expressions. However; existing XML path indices suffer from at least one of threelimitations: they focus only on indexing the structure (relying on a separate index for nodecontent); they are useful only for simple path expressions such as root-to-leaf paths; or theycannot be tightly integrated with a relational query processor. Moreover; there is no unifiedframework to compare these index structures. In this paper; we present a framework defininga family of index structures that includes most existing XML path indices. We also proposetwo novel index structures in this family; with different space–time tradeoffs; that are effectivefor the evaluation of XML branching path expressions (ie; twigs) with value conditions. Wealso show how this family of index structures can be implemented using the access …,Data & Knowledge Engineering,2007,56
Towards efficient information gathering agents,Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,*,Working Notes of the AAAI Spring Symposium on Software Agents,1994,56
Finding hierarchical heavy hitters in streaming data,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items that arrive online as streams typically have attributes which take valuesfrom one or more hierarchies (time and geographic location; source and destination IPaddresses; etc.). Providing an aggregate view of such data is important for summarization;visualization; and analysis. We develop an aggregate view based on certain organized setsof large-valued regions (“heavy hitters”) corresponding to hierarchically discountedfrequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We firstconsider computing (approximate) HHHs over a data stream drawn from a singlehierarchical attribute. We formalize the problem and give deterministic algorithms to findthem in a single pass over the input. In order to analyze a wider range of realistic datastreams (eg; from IP traffic-monitoring applications); we generalize this problem to …,ACM Transactions on Knowledge Discovery from Data (TKDD),2008,55
Foundations of aggregation constraints,Kenneth A Ross; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; that is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We formally study the fundamental problem of determining if aconjunction of aggregation constraints is satisfiable; and show that; for many classes ofaggregation constraints; the problem is undecidable. We describe a complete and minimalaxiomatization of aggregation constraints; for the SQL aggregate functions min; max; sum;count and average; over a non-empty; finite multiset on several domains. Thisaxiomatization helps identify classes of aggregation constraints for which the satisfiabilitycheck is efficient. We present a polynomial-time algorithm that directly checks for satisfiabilityof a conjunction of aggregation range constraints over a single multiset; this is a …,Theoretical Computer Science,1998,52
Constraint objects,Divesh Srivastava; Raghu Ramakrishnan; Peter Revesz,Abstract We describe the Constraint Object Data Model (CODM); which enhances an object-based data model with existential constraints to naturally represent partially specifiedinformation. We present the Constraint Object Query Language (COQL); a declarative; rule-based; language that can be used to infer relationships about and monotonically refineinformation represented in the CODM. COQL has a model-theoretic and a fixpoint semanticsbased on the notions of constraint entailment and “proofs in all possible worlds”. We alsoprovide a novel polynomial-time algorithm for quantifier elimination for set-order constraints;a restricted class of set constraints that uses ε and⊑.,Principles and Practice of Constraint Programming,1994,52
Privacy in dynamic social networks,Smriti Bhagat; Graham Cormode; Balachander Krishnamurthy; Divesh Srivastava,Abstract Anonymization of social networks before they are published or shared has becomean important research question. Recent work on anonymizing social networks has looked atprivacy preserving techniques for publishing a single instance of the network. However;social networks evolve and a single instance is inadequate for analyzing the evolution of thesocial network or for performing any longitudinal data analysis. We study the problem ofrepeatedly publishing social network data as the network evolves; while preserving privacyof users. Publishing multiple instances of the same network independently has privacy risks;since stitching the information together may allow an adversary to identify users in thenetworks. We propose methods to anonymize a dynamic network such that the privacy ofusers is preserved when new nodes and edges are added to the published network …,Proceedings of the 19th international conference on World wide web,2010,51
Hashed samples: selectivity estimators for set similarity selection queries,Marios Hadjieleftheriou; Xiaohui Yu; Nick Koudas; Divesh Srivastava,Abstract We study selectivity estimation techniques for set similarity queries. A wide varietyof similarity measures for sets have been proposed in the past. In this work we concentrateon the class of weighted similarity measures (eg; TF/IDF and BM25 cosine similarity andvariants) and design selectivity estimators based on a priori constructed samples. First; westudy the pitfalls associated with straightforward applications of random sampling; and arguethat care needs to be taken in how the samples are constructed; uniform random samplingyields very low accuracy; while query sensitive realtime sampling is more expensive thanexact solutions (both in CPU and I/O cost). We show how to build robust samples a priori;based on existing synopses for distinct value estimation. We prove the accuracy of ourtechnique theoretically; and verify its performance experimentally. Our algorithm is orders …,Proceedings of the VLDB Endowment,2008,51
XTreeNet: Scalable overlay networks for XML content dissemination and querying (synopsis),William Fenner; Michael Rabinovich; KK Ramakrishnan; Divesh Srivastava; Yin Zhang,XML is becoming a ubiquitous format for information exchange on the Internet. To alleviatethe problems of" whom to ask" and" whom to tell" when connecting XML informationproducers with consumers over the network; content-based querying and dissemination ofinformation have been investigated in the literature. Our XTreeNet project unifies thepublish/subscribe and query/response models with a single common XML aware overlaynetwork for XML-based information producers and consumers. This integrated frameworklends itself to a variety of applications.,Web Content Caching and Distribution; 2005. WCW 2005. 10th International Workshop on,2005,50
Interaction of query evaluation and buffer management for information retrieval,Björn T Jónsson; Michael J Franklin; Divesh Srivastava,Abstract The proliferation of the World Wide Web has brought information retrieval (IR)techniques to the forefront of search technology. To the average computer user;“searching”now means using IR-based systems for finding information on the WWW or in otherdocument collections. IR query evaluation methods and workloads differ significantly fromthose found in database systems. In this paper; we focus on three such differences. First;due to the inherent fuzziness of the natural language used in IR queries and documents; anadditional degree of flexibility is permitted in evaluating queries. Second; IR queryevaluation algorithms tend to have access patterns that cause problems for traditional bufferreplacement policies. Third; IR search is often an iterative process; in which a query isrepeatedly refined and resubmitted by the user. Based on these differences; we develop …,ACM SIGMOD Record,1998,50
Online data fusion,Xuan Liu; Xin Luna Dong; Beng Chin Ooi; Divesh Srivastava,ABSTRACT The Web contains a significant volume of structured data in various domains;but a lot of data are dirty and erroneous; and they can be propagated through copying. Whiledata integration techniques allow querying structured data on the Web; they take the unionof the answers retrieved from different sources and can thus return conflicting information.Data fusion techniques; on the other hand; aim to find the true values; but are designed foroffline data aggregation and can take a long time. This paper proposes SOLARIS; the firstonline data fusion system. It starts with returning answers from the first probed source; andrefreshes the answers as it probes more sources and applies fusion techniques on theretrieved data. For each returned answer; it shows the likelihood that the answer is correct;and stops retrieving data for it after gaining enough confidence that data from the …,Proceedings of the VLDB Endowment,2011,49
Methods; computer programs and apparatus for caching directory queries,*,A framework for answering Lightweight Directory Access Protocol (LDAP) queries frompreviously cached queries includes a proxy server configured to receive client directoryqueries. The proxy server maintains a cache of data (entries) and semantic informationassociated with a query. In response to a query received from client; the proxy invokes aquery containment procedure which uses the semantics of the incoming and stored queriesto determine whether the query can be answered from cached queries. The proxy answersqueries from the local cache when possible; and for other queries it sends the request to adirectory server. The semantics of the new query and the resulting data (entries) are addedto the cache. The method and apparatus can be used for positive conjunctive queries foranswering equality; range and substring queries.,*,2006,49
Efficient bottom-up evaluation of logic programs,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract In recent years; much work has been directed towards evaluating logic programsand queries on deductive databases by using an iterative bottom-up fixpoint computation.The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation inseveral situations. They are sound and complete for positive Horn clause programs; are well-suited to applications with large volumes of data (facts); and can support a variety ofextensions to the standard logic programming paradigm. We present the basics of databasequery evaluation and logic programming evaluation; and then discuss bottom-up fixpointevaluation. We discuss an approach based upon using a program transformation (“MagicTemplates”) to restrict search; followed by fixpoint computation using a technique (“Semi-naive evaluation”) that avoids repeated inferences. The program transformation …,Computer Systems and Software Engineering: State-Of-The-Art,1992,49
Method for matching XML twigs using index structures and relational query processors,*,A framework defining a family of index structures useful in evaluating XML path expressions(ie; twigs) in XML database is disclosed. Within this framework; two particular indexstructures with different space-time tradeoffs are presented that prove effective for theevaluation of twigs with value conditions. These index structures can be realized usingaccess methods of an underlying relational database system. Experimental results show thatthe indices disclosed achieve significant improvement in performance for evaluating twigqueries as compared with previously proposed XML path indices.,*,2006,48
Idea: Interactive data exploration and analysis,Peter G Selfridge; Divesh Srivastava; Lynn O Wilson,Abstract The analysis of business data is often an ill-defined task characterized by largeamounts of noisy data. Because of this; business data analysis must combine two kinds ofintertwined tasks: exploration and analysis. Exploration is the process of finding theappropriate subset of data to analyze; and analysis is the process of measuring the data toprovide the business answer. While there are many tools available both for exploration andfor analysis; a single tool or set of tools may not provide full support for these intertwinedtasks. We report here on a project that set out to understand a specific business dataanalysis problem and build an environment to support it. The results of this understandingare; first of all; a detailed list of requirements of this task; second; a set of capabilities thatmeet these requirements; and third; an implemented client-server solution that addresses …,ACM SIGMOD Record,1996,48
Method for using region-sets to focus searches in hierarchical structures,*,A method improves a search in a hierarchical structure by focusing the search to selectedregions within the structure. The method defines one or more region-sets and uses theregion-set (s) as either a filter for the results of a key-word search or an integrated part of asearch engine to increase the efficiency of the search engine. The method also provides fordynamic creation of new region-sets from existing region-sets using a prescribed set ofoperators.,*,2000,47
Characterizing and selecting fresh data sources,Theodoros Rekatsinas; Xin Luna Dong; Divesh Srivastava,Abstract Data integration is a challenging task due to the large numbers of autonomous datasources. This necessitates the development of techniques to reason about the benefits andcosts of acquiring and integrating data. Recently the problem of source selection (ie;identifying the subset of sources that maximizes the profit from integration) was introducedas a preprocessing step before the actual integration. The problem was studied for staticsources and used the accuracy of data fusion to quantify the integration profit. In this paper;we study the problem of source selection considering dynamic data sources whose contentchanges over time. We define a set of time-dependent metrics; including coverage;freshness and accuracy; to characterize the quality of integrated data. We show howstatistical models for the evolution of sources can be used to estimate these metrics …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,46
Record linkage with uniqueness constraints and erroneous values,Songtao Guo; Xin Luna Dong; Divesh Srivastava; Remi Zajac,Abstract Many data-management applications require integrating data from a variety ofsources; where different sources may refer to the same real-world entity in different waysand some may even provide erroneous data. An important task in this process is torecognize and merge the various references that refer to the same entity. In practice; someattributes satisfy a uniqueness constraint---each real-world entity (or most entities) has aunique value for the attribute (eg; business contact phone; address; and email). Traditionaltechniques tackle this case by first linking records that are likely to refer to the same real-world entity; and then fusing the linked records and resolving conflicts if any. Such methodscan fall short for three reasons: first; erroneous values from sources may prevent correctlinking; second; the real world may contain exceptions to the uniqueness constraints and …,Proceedings of the VLDB Endowment,2010,46
Performance and overhead of semantic cache management,Björn Þór Jónsson; María Arinbjarnar; Bjarnsteinn Þórsson; Michael J Franklin; Divesh Srivastava,Abstract The emergence of query-based online data services and e-commerce applicationshas prompted much recent research on data caching. This article studies semantic caching;a caching architecture for such applications; that caches the results of selection queries. Theprimary contribution of this article is to revisit the performance and overhead of semanticcaching using a modern database server and modern hardware. Initially; the performancestudy focuses on simple workloads and demonstrates several benefits of semantic caching;including low overhead; insensitivity to the physical layout of the database; reduced networktraffic; and the ability to answer some queries without contacting the server. With moderatelycomplex workloads; careful coding of remainder queries is required to maintain efficientquery processing at the server. Using very complex workloads; we demonstrate that …,ACM Transactions on Internet Technology (TOIT),2006,46
Snakes and sandwiches: Optimal clustering strategies for a data warehouse,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Abstract Physical layout of data is a crucial determinant of performance in a data warehouse.The optimal clustering of data on disk; for minimizing expected I/O; depends on the queryworkload. In practice; we often have a reasonable sense of the likelihood of different classesof queries; eg; 40% of the queries concern calls made from some specific telephone numberin some month. In this paper; we address the problem of finding an optimal clustering ofrecords of a fact table on disk; given an expected workload in the form of a probabilitydistribution over query classes. Attributes in a data warehouse fact table typically havehierarchies defined on them (by means of auxiliary dimension tables). The product of thedimensional hierarchy levels forms a lattice and leads to a natural notion of query classes.Optimal clustering in this context is a combinatorially explosive problem with a huge …,ACM SIGMOD Record,1999,46
Using LDAP directory caches,Sophie Cluet; Olga Kapitskaia; Divesh Srivastava,LDAP (Lightweight Directory Access Protocol) directories have recently proliferated with the growthof the Internet; and are being used in a wide variety of network-based applications to store datasuch as personal profiles; address books; and network and service policies. These systems providea means for managing heterogeneity in a way far superior to what conventional relational orobject-oriented databases can offer. To achieve fast performance for declarative queryanswering; it is desirable to use client caching based on semantic information (instead of individualdirectory entries). We formally consider the problem of reusing cached LDAP directory entriesfor answering declarative LDAP queries. A semantic LDAP directory cache contains directoryentries; which are semantically described by a set of query templates. We show that; for conjunctivequeries and LDAP directory caches with positive templates; the complexity of cache …,Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,1999,46
Explaining program execution in deductive systems,Tarun Arora; Raghu Ramakrishnan; William Roth; Praveen Seshadri; Divesh Srivastava,Abstract Programs in deductive database and programming systems have a naturalmeaning that is based upon their mathematical reading as logical rules. High-level'explanations' of a program evaluation/execution can be constructed to provide addedfunctionality:(1) To debug a program by following a chain of deductions leading to anunexpected (and possibly incorrect) conclusion;(2) To follow the derivation of certain correctconclusions to determine why and how they are reached;(3) To identify consequences of a(typically; incorrect or unexpected) fact. This functionality can be utilized either to performpost-mortem analysis of a session; or to interactively develop programs by running queriesand viewing their deductions simultaneously.'Explanations' of programs are especiallyimportant in the context of deductive databases for three reasons:(1) These programs …,Deductive and Object-Oriented Databases,1993,46
Method for using query templates in directory caches,*,The present invention discloses the use of generalized queries; referred to as querytemplates; obtained by generalizing individual user queries; as the semantic basis for lowoverhead; high benefit directory caches for handling declarative queries. Cachingeffectiveness can be improved by maintaining a set of generalizations of queries andadmitting such generalizations into the cache when their estimated benefits are sufficientlyhigh. In a preferred embodiment of the invention; the admission of query templates into thecache can be done in what is referred to by the inventors as a “revolutionary” fashion—followed by stable periods where cache admission and replacement can be doneincrementally in an evolutionary fashion. The present invention can lead to considerablyhigher hit rates and lower server-side execution and communication costs than …,*,2005,45
-A System for Keyword Proximity Search on XML Databases,Andrey Balmin; Yannis Papakonstantinou; Vagelis Hristidis; Tianqiu Wang; Divesh Srivastava; Nick Koudas,This chapter discusses keyword proximity search on XML database. Keyword proximitysearch is a user-friendly information discovery technique that has been extensively studiedfor text documents. In extending this technique to structured databases; recent works providekeyword proximity search on labeled graphs. A keyword proximity search does not requirethe user to know the structure of the graph; the role of the objects containing the keywords;or the type of the connections between the objects. The user simply submits a list ofkeywords and the system returns the sub-graphs that connect the objects containing thekeywords. XML and its labeled graph/tree abstractions are becoming the data model ofchoice for representing semistructured; self-describing data; and keyword proximity searchis well-suited to XML documents as well.A System for Keyword Proximity Search on XML …,*,2003,45
Event-based messaging,*,A messaging system that handles messages of any kind; and a method of operation thereof;in which advanced messaging services can be implemented for multiple users; acrossmultiple mail clients; in a more flexible and less limited fashion than previous messagingsystems. The messaging system includes a plurality of messaging entities together having astate. An event supplier detects a change in the state of the messaging entities andgenerates an event announcement. An event manager receives the event announcementand generates an event notification. An event consumer receives the event notification andexamines or manipulates at least one messaging entity.,*,2002,45
Chasing constrained tuple-generating dependencies,Michael J Maher; Divesh Srivastava,Abstract We investigate the implication problem for constrained tuple-generatingdependencies(CTGDS); the extension of tuple- and equality-generating dependencies thatpermits expression of semantic relations(constraints) on variables. The implication problemis central to identifying redundant integrity constraints; checking integrity constraints onconstraint databases; detecting independence of queries and updates; and optimizingqueries. We provide two chase procedures for the implication problem. The first is cautious;generating tuples and constraints only when justified; whereas the second is speculative;generating tuples and constraints that have attached conditions about when they exist/hold.The taut ious chase is more efficient; in some sense; but less powerful in demonstrating thata CTGD is implied. We demonstrate that; for constraint domains with Independence of …,Proceedings of the fifteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1996,45
Conductimetric immunosensor based on poly (3; 4-ethylenedioxythiophene),Mandakini Kanungo; Divesh N Srivastava; Anil Kumar; AQ Contractor,From the above discussion it is clear that; to date; there has been no report where change inthe conformation of conjugated polymer; as a consequence of antibody∶antigen binding; hasbeen used for the design of immunosensors. Swager and coworkers have suggested that thisapproach may not succeed because there may not be a significant conformational change inthe conjugated polymer upon antigen binding to the surface entrapped antibody. 6 However;in the present work; we have developed a conductimetric reagentless immunosensor using thebiospecific binding pair of goat antirabbit IgG and rabbit IgG. Another feature of the present workis the easy fabrication of the immunosensor by physical entrapment of the antibody or the antigenduring the polymerization. The concept of the present immunosensor is based on the changein the conformation of the polymer due to the formation of antigen–antibody adduct. This …,Chemical Communications,2002,44
Incremental maintenance of length normalized indexes for approximate string matching,Marios Hadjieleftheriou; Nick Koudas; Divesh Srivastava,Abstract Approximate string matching is a problem that has received a lot of attentionrecently. Existing work on information retrieval has concentrated on a variety of similaritymeasures TF/IDF; BM25; HMM; etc.) specifically tailored for document retrieval purposes. Asnew applications that depend on retrieving short strings are becoming popular (eg; localsearch engines like YellowPages. com; Yahoo! Local; and Google Maps) new indexingmethods are needed; tailored for short strings. For that purpose; a number of indexingtechniques and related algorithms have been proposed based on length normalizedsimilarity measures. A common denominator of indexes for length normalized measures isthat maintaining the underlying structures in the presence of incremental updates isinefficient; mainly due to data dependent; precomputed weights associated with each …,Proceedings of the 35th SIGMOD international conference on Management of data,2009,43
TIMBER: A native system for querying XML,Stelios Paparizos; Shurug Al-Khalifa; Adriane Chapman; HV Jagadish; Laks VS Lakshmanan; Andrew Nierman; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu,Abstract XML has become ubiquitous; and XML data has to be managed in databases. Thecurrent industry standard is to map XML data into relational tables and store this informationin a relational database. Such mappings create both expressive power problems andperformance problems. In the T IMBER [7] project we are exploring the issues involved instoring XML in native format. We believe that the key intellectual contribution of this system isa comprehensive set-at-a-time query processing ability in a native XML store; with all thestandard components of relational query processing; including algebraic rewriting and acost-based optimizer.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,43
Estimating the confidence of conditional functional dependencies,Graham Cormode; Lukasz Golab; Korn Flip; Andrew McGregor; Divesh Srivastava; Xi Zhang,Abstract Conditional functional dependencies (CFDs) have recently been proposed asextensions of classical functional dependencies that apply to a certain subset of the relation;as specified by a pattern tableau. Calculating the support and confidence of a CFD (ie; thesize of the applicable subset and the extent to which it satisfies the CFD) gives valuableinformation about data semantics and data quality. While computing the support is easier;computing the confidence exactly is expensive if the relation is large; and estimating it from arandom sample of the relation is unreliable unless the sample is large. We study how toefficiently estimate the confidence of a CFD with a small number of passes (one or two) overthe input using small space. Our solutions are based on a variety of sampling and sketchingtechniques; and apply when the pattern tableau is known in advance; and also the harder …,Proceedings of the 35th SIGMOD international conference on Management of data,2009,42
Method of clustering electronic documents in response to a search query,*,A method of presenting clusters of documents in response to a search query where thedocuments within a cluster are determined to be related to one another. This relationship isassessed by comparing documents which match one or more terms in the query todetermine the extent to which the documents have commonality with respect to termsappearing infrequently in the collection of documents. As a consequence; the cluster ofdocuments represents a response or query result that is split across multiple documents. In afurther variation the cluster can be constituted by a structured document and an unstructureddocument.,*,2002,42
Prediction promotes privacy in dynamic social networks,Smriti Bhagat; Graham Cormode; Balachander Krishnamurthy; Divesh Srivastava,Abstract Recent work on anonymizing online social networks (OSNs) has looked at privacypreserving techniques for publishing a single instance of the network. However; OSNsevolve and a single instance is inadequate for analyzing their evolution or performinglongitudinal data analysis. We study the problem of repeatedly publishing OSN data as thenetwork evolves while preserving privacy of users. Publishing multiple instancesindependently has privacy risks; since stitching the information together may allow anadversary to identify users. We provide methods to anonymize a dynamic network when newnodes and edges are added to the published network. These methods use link predictionalgorithms to model the evolution. Using this predicted graph to perform group-basedanonymization; the loss in privacy caused by new edges can be eliminated almost …,Proceedings of the 3rd conference on Online social networks,2010,41
Fast algorithms for hierarchical range histogram construction,Sudipto Guha; Nick Koudas; Divesh Srivastava,Abstract Data Warehousing and OLAP applications typically view data an having multiplelogical dimensions (eg; product; location) with natural hierarchies defined on eachdimension. OLAP queries usually involve hierarchical selections on some of the dimensions;and often aggregate measure attributes (eg; sales; volume). Accurately estimating thedistribution of measure attributes; under hierarchical selections; is important in a variety ofscenarios; including approximate query evaluation and cost-based optimization of queries.In this paper; we propose fast (near linear time) algorithms for the problem of approximatingthe distribution of measure attributes with hierarchies defined on them; using histograms.Our algorithms are based on dynamic programming and a novel notion of sparse intervalsthat we introduce; and are the first practical algorithms for this problem. They effectively …,Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2002,41
Differentially private publication of sparse data,Graham Cormode; Magda Procopiuc; Divesh Srivastava; Thanh TL Tran,Abstract: The problem of privately releasing data is to provide a version of a dataset withoutrevealing sensitive information about the individuals who contribute to the data. The modelof differential privacy allows such private release while providing strong guarantees on theoutput. A basic mechanism achieves differential privacy by adding noise to the frequencycounts in the contingency tables (or; a subset of the count data cube) derived from thedataset. However; when the dataset is sparse in its underlying space; as is the case for mostmulti-attribute relations; then the effect of adding noise is to vastly increase the size of thepublished data: it implicitly creates a huge number of dummy data points to mask the truedata; making it almost impossible to work with. We present techniques to overcome thisroadblock and allow efficient private release of sparse data; while maintaining the …,arXiv preprint arXiv:1103.0825,2011,39
-Efficient Approximation of Optimization Queries Under Parametric Aggregation Constraints,Sudipto Guha; Dimitrios Gunopoulos; Michail Vlachos; Nick Koudas; Divesh Srivastava,This chapter introduces a new class of queries; optimization under parametric aggregationconstraint (OPAC) queries. Such queries aim to identify sets of database tuples constitutingsolutions to a large class of optimization problems involving the database tuples. Thechapter introduces algorithms that pre-process relations and construct indexes to efficientlyprovide answers to such queries. This chapter analytically quantified the accuracyguarantees of the construction and a thorough evaluation highlighting the impact of variousparameters on the performance of the schemes is presented. The constraints and theobjective function are specified in terms of aggregate functions of relational attributes; andthe parameter values identify the constants used in the aggregation constraints. Resultsindicate that the methodology is effective and can be deployed easily; utilizing index …,*,2003,39
Sender-paid electronic messaging,*,The present invention is a messaging system; and method of operation thereof; whichprovides message recipients with control over the delivery of message and charges the costof a message to the sender of the message. A message is received at a messaging serverfrom a sender system; the message including an indication of a recipient system. Anotification message is transmitted to the recipient system; allowing the message recipient todetermine whether they desire the message to be delivered. If so; an activation message isreceived from the recipient system and the message is transmitted to the recipient system. Acharge for the message is assessed to the sender of the message. The message is stored inthe messaging server until the activation message is received. At least a portion of theassessed charge may be credited or debited to the recipient of the message. The …,*,2000,39
E cient incremental evaluation of queries with aggregation,Raghu Ramakrishnan; Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We present a technique for e ciently evaluating queries on programs withmonotonic aggregation; a class of programs de ned by Ross and Sagiv. Our techniqueconsists of the following components: incremental computation of aggregate functions;incremental xpoint evaluation of monotonic programs and Magic Sets transformation ofmonotonic programs. We also present a formalization of the notion of incrementalcomputation of aggregate functions on a multiset; and upper and lower bounds forincremental computation of a variety of aggregate functions. We describe a proof-theoreticreformulation of the monotonic semantics in terms of computations; following the approachof Beeri et al.; this reformulation greatly simpli es the task of proving the correctness of ouroptimizations.,Proceedings of the International Logic Programming Symposium,1994,39
Systems and methods for distributing video on demand,*,A method of providing content comprises making the content available on a central server;and surveying a plurality of peers for a portion of the content. The portion of the content fromone of the peers is obtained when the portion of the content is available from the one of thepeers; and obtained from the central server when the portion of the content is not availablefrom the plurality of peers.,*,2013,38
Estimating the selectivity of approximate string queries,Arturas Mazeika; Michael H Böhlen; Nick Koudas; Divesh Srivastava,Abstract Approximate queries on string data are important due to the prevalence of suchdata in databases and various conventions and errors in string data. We present the VSolestimator; a novel technique for estimating the selectivity of approximate string queries. TheVSol estimator is based on inverse strings and makes the performance of the selectivityestimator independent of the number of strings. To get inverse strings we decompose alldatabase strings into overlapping substrings of length q (q-grams) and then associate eachq-gram with its inverse string: the IDs of all strings that contain the q-gram. We usesignatures to compress inverse strings; and clustering to group similar signatures. We studyour technique analytically and experimentally. The space complexity of our estimator onlydepends on the number of neighborhoods in the database and the desired estimation …,ACM Transactions on Database Systems (TODS),2007,38
Two-dimensional substring indexing,Paolo Ferragina; Nick Koudas; S Muthukrishnan; Divesh Srivastava,Abstract As databases have expanded in scope to storing string data (XML documents;product catalogs); it has become increasingly important to search databases based onmatching substrings; often on multiple; correlated dimensions. While string B-trees are I/Ooptimal in one dimension; no index structure with non-trivial query bounds is known for two-dimensional substring indexing. In this paper; we present a technique for two-dimensionalsubstring indexing based on a reduction to the geometric problem of identifying commoncolors in two ranges containing colored points. We develop an I/O efficient algorithm forsolving the common colors problem; and use it to obtain an I/O efficient (poly-logarithmicquery time) algorithm for the two-dimensional substring indexing problem. Our techniquesresult in a family of secondary memory index structures that trade space for time; with no …,Journal of Computer and System Sciences,2003,38
Extending the well-founded and valid semantics for aggregation,S Sudarshan; Divesh Srivastava; Raghu Ramakrishnan; Catriel Beeri,Abstract We present a very general technique for de ning semantics for programs that useaggregation. We use the technique to extend the well-founded semantics and the validsemantics; both of which were designed to provide semantics for programs with negation; tohandle programs that contain possibly recursive use of aggregation. The generalization isbased on a simple but powerful idea of aggregation on three-valued multisets. The use ofthree-valued multisets makes our extended well-founded semantics; which we callaggregate-well-founded semantics; more intuitive than the extension of well-foundedmodels by Van Gelder Van92]. Our semantics and Van Gelder's semantics agree on manyprograms; and on others our semantics provides results that Van Gelder says are intuitiveand desirable; but that his semantics does not provide. The extended valid semantics …,Proc. International Logic Programming Symposium,1993,38
Efficient OLAP query processing in distributed data warehouses,Michael Akinde; Michael Böhlen; Theodore Johnson; Laks Lakshmanan; Divesh Srivastava,Abstract The success of Internet applications has led to an explosive growth in the demandfor bandwidth from ISPs. Managing an IP network requires collecting and analyzing networkdata; such as flowlevel traffc statistics. Such analyses can typically be expressed as OLAPqueries; eg; correlated aggregate queries and data cubes. Current day OLAP tools for thistask assume the availability of the data in a centralized data warehouse. However; theinherently distributed nature of data collection and the huge amount of data extracted ateach collection point make it impractical to gather all data at a centralized site. One solutionis to maintain a distributed data warehouse; consisting of local data warehouses at eachcollection point and a coordinator site; with most of the processing being performed at thelocal sites. In this paper; we consider the problem of efficient evaluation of OLAP queries …,Advances in Database Technology—EDBT 2002,2002,37
Foundations of aggregation constraints,Kenneth Ross; Divesh Srivastava; Peter Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; which is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We study the fundamental problem of checking if a conjunction ofaggregation constraints is solvable; and present undecidability results for many differentclasses of aggregation constraints. We describe a complete and minimal axiomatization ofthe class of aggregation constraints over finite multisets of reals; which permits a naturalreduction from the class of aggregation constraints to the class of mixed integer/real; non-linear arithmetic constraints. We then present a polynomial-time algorithm that directlychecks for solvability of a useful class of aggregation constraints; where the reduction-basedapproach does not lead to efficient checks for solvability.,Principles and Practice of Constraint Programming,1994,37
Reverse engineering complex join queries,Meihui Zhang; Hazem Elmeleegy; Cecilia M Procopiuc; Divesh Srivastava,Abstract We study the following problem: Given a database D with schema G and an outputtable Out; compute a join query Q that generates OUT from D. A simpler variant allows Q toreturn a superset of Out. This problem has numerous applications; both by itself; and as abuilding block for other problems. Related prior work imposes conditions on the structure ofQ which are not always consistent with the application; but simplify computation. We discussseveral natural SQL queries that do not satisfy these conditions and cannot be discoveredby prior work. In this paper; we propose an efficient algorithm that discovers queries witharbitrary join graphs. A crucial insight is that any graph can be characterized by thecombination of a simple structure; called a star; and a series of merge steps over the star.The merge steps define a lattice over graphs derived from the same star. This allows us to …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,36
Small synopses for group-by query verification on outsourced data streams,Ke Yi; Feifei Li; Graham Cormode; Marios Hadjieleftheriou; George Kollios; Divesh Srivastava,Abstract Due to the overwhelming flow of information in many data stream applications; dataoutsourcing is a natural and effective paradigm for individual businesses to address theissue of scale. In the standard data outsourcing model; the data owner outsources streamingdata to one or more third-party servers; which answer queries posed by a potentially largenumber of clients on the data owner's behalf. Data outsourcing intrinsically raises issues oftrust; making outsourced query assurance on data streams a problem with importantpractical implications. Existing solutions proposed in this model all build upon cryptographicprimitives such as signatures and collision-resistant hash functions; which only work forcertain types of queries; for example; simple selection/aggregation queries. In this article; weconsider another common type of queries; namely;“GROUP BY; SUM” queries; which …,ACM Transactions on Database Systems (TODS),2009,36
Approximate joins: Concepts and techniques,Nick Koudas; Divesh Srivastava,Abstract The quality of the data residing in information repositories and databases getsdegraded due to a multitude of reasons. Such reasons include typing mistakes duringinsertion (eg; character transpositions); lack of standards for recording database fields (eg;addresses); and various errors introduced by poor database design (eg; missing integrityconstraints). Data of poor quality can result in significant impediments to popular businesspractices: sending products or bills to incorrect addresses; inability to locate customerrecords during service calls; inability to correlate customers across multiple services; etc.,Proceedings of the 31st international conference on Very large data bases,2005,36
Text joins for data cleansing and integration in a relational database management system,*,An organization's data records are often noisy: because of transcription errors; incompleteinformation; and lack of standard formats for textual data. A fundamental task during datacleansing and integration is matching strings—perhaps across multiple relations—that referto the same entity (eg; organization name or address). Furthermore; it is desirable to performthis matching within an RDBMS; which is where the data is likely to reside. In this paper; Weadapt the widely used and established cosine similarity metric from the information retrievalfield to the relational database context in order to identify potential string matches acrossrelations. We then use this similarity metric to characterize this key aspect of data cleansingand integration as a join between relations on textual attributes; where the similarity ofmatches exceeds a specified threshold. Computing an exact answer to the text join can …,*,2005,36
Multi-dimensional substring selectivity estimation,HV Jagadish; Olga Kapitskaia; Raymond T Ng; Divesh Srivastava,Abstract With the explosion of the Internet; LDAP directories and XML; there is an evergreater need to evaluate queries involving (sub) string matching. In many cases; matchesneed to be on multiple attributes/dimensions; with correlations between the dimensions.Effective query optimization in this context requires good selectivity estimates. In this paper;we use multi-dimensional countsuffix trees as the basic framework for substring selectivityestimation. Given the enormous size ofthese trees for large databases; we develop a spaceand time efficient probabilistic algorithm to construct multi-dimensional pruned count-suffixtrees directly. We then present two techniques to obtain good estimates for a given multi-dimensional substring matching query; using a pruned countsuffix tree. The first one; calledGNO (for Greedy Non-Overlap); generalizes the greedy parsing suggested by Krishnan et …,Proceedings of the International Conference on Very Large Data Bases,1999,36
Discovery of complex glitch patterns: A novel approach to quantitative data cleaning,Laure Berti-Equille; Tamraparni Dasu; Divesh Srivastava,Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques todetect; quantify; and correct data quality problems (or glitches). Current QDC approachesfocus on addressing each category of data glitch individually. However; in real-world data;different types of data glitches co-occur in complex patterns. These patterns and interactionsbetween glitches offer valuable clues for developing effective domain-specific quantitativecleaning strategies. In this paper; we address the shortcomings of the extant QDC methodsby proposing a novel framework; the DEC (Detect-Explore-Clean) framework. It is acomprehensive approach for the definition; detection and cleaning of complex; multi-typedata glitches. We exploit the distributions and interactions of different types of glitches todevelop data-driven cleaning strategies that may offer significant advantages over blind …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,35
Recommending join queries via query log analysis,Xiaoyan Yang; Cecilia M Procopiuc; Divesh Srivastava,Complex ad hoc join queries over enterprise databases are commonly used by businessdata analysts to understand and analyze a variety of enterprise-wide processes. However;effectively formulating such queries is a challenging task for human users; especially overdatabases that have large; heterogeneous schemas. In this paper; we propose a novelapproach to automatically create join query recommendations based on input-outputspecifications (ie; input tables on which selection conditions are imposed; and output tableswhose attribute values must be in the result of the query). The recommended join querygraph includes (i)" intermediate''tables; and (ii) join conditions that connect the input andoutput tables via the intermediate tables. Our method is based on analyzing an existingquery log over the enterprise database. Borrowing from program slicing techniques …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,35
Validating multi-column schema matchings by type,Bing Tian Dai; Nick Koudas; Divesh Srivastava; Anthony KH Tung; Suresh Venkatasubramanian,Validation of multi-column schema matchings is essential for successful databaseintegration. This task is especially difficult when the databases to be integrated contain littleoverlapping data; as is often the case in practice (eg; customer bases of differentcompanies). Based on the intuition that values present in different columns related by aschema matching will have similar" semantic type"; and that this can be captured usingdistributions over values (" statistical types"); we develop a method for validating 1-1 andcompositional schema matchings. Our technique is based on three key technical ideas. First;we propose a generic measure for comparing two columns matched by a schema matching;based on a notion of information-theoretic discrepancy that generalizes the standardgeometric discrepancy; this provides the basis for 1: 1 matching. Second; we present an …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,35
Efficient approximation of correlated sums on data streams,Rohit Ananthakrishna; Abhinandan Das; Johannes Gehrke; Flip Korn; S Muthukrishnan; Divesh Srivastava,In many applications such as IP network management; data arrives in streams and queriesover those streams need to be processed online using limited storage. Correlated-sum (CS)aggregates are a natural class of queries formed by composing basic aggregates on (x; y)pairs and are of the form SUM {g (y): x/spl les/f (AGG (x))}; where AGG (x) can be any basicaggregate and f (); g () are user-specified functions. CS-aggregates cannot be computedexactly in one pass through a data stream using limited storage; hence; we study theproblem of computing approximate CS-aggregates. We guarantee a priori error boundswhen AGG (x) can be computed in limited space (eg; MIN; MAX; AVG); using two variants ofGreenwald and Khanna's summary structure for the approximate computation of quantiles.Using real data sets; we experimentally demonstrate that an adaptation of the quantile …,IEEE Transactions on Knowledge and Data Engineering,2003,35
Weighted set-based string similarity,Marios Hadjieleftheriou; Divesh Srivastava,Abstract Consider a universe of tokens; each of which is associated with a weight; and adatabase consisting of strings that can be represented as subsets of these tokens. Given aquery string; also represented as a set of tokens; a weighted string similarity query identifiesall strings in the database whose similarity to the query is larger than a user specifiedthreshold. Weighted string similarity queries are useful in applications like data cleaning andintegration for finding approximate matches in the presence of typographical mistakes;multiple formatting conventions; data transformation errors; etc. We show that this problemhas semantic properties that can be exploited to design index structures that support veryefficient algorithms for query answering.,IEEE Data Eng. Bull,2010,34
Query restricted bottom-up evaluation of normal logic programs,David B Kemp; Peter J Stuckey; Divesh Srivastava,Abstract Several program transformations| magic sets; envelopes; NRSU transformationsand context transformations; among others| have been proposed for e ciently computing theanswers to a query while taking advantage of the query constants. These transformationsuse sideways information passing strategies (sips) to restrict bottom-up evaluation to factspotentially relevant to the query. It is of interest to extend these transformations to all logicprograms with negation; and identify classes of programs and sips for which thesetransformations preserve well-founded models with respect to the query. In a previous paperwe identi ed classes of programs and sips for which the magic sets transformation preserveswell-founded models wrt the query. We continue this line of research to othertransformations that use sips. We identify classes of programs and sips for which the …,Proceedings of the Joint International Conference and Symposium on Logic Programming,1992,34
Untitled,*,*,*,*,34
Efficient and effective explanation of change in hierarchical summaries,Deepak Agarwal; Dhiman Barman; Dimitrios Gunopulos; Neal E Young; Flip Korn; Divesh Srivastava,Abstract Dimension attributes in data warehouses are typically hierarchical (eg; geographiclocations in sales data; URLs in Web traffic logs). OLAP tools are used to summarize themeasure attributes (eg; total sales) along a dimension hierarchy; and to characterizechanges (eg; trends and anomalies) in a hierarchical summary over time. When thenumberof changes identified is large (eg; total sales in many stores differed from their expectedvalues); a parsimonious explanation of the most significant changes is desirable. In thispaper; we propose a natural model of parsimonious explanation; as a composition of nodeweights along the root-to-leaf paths in a dimension hierarchy; which permits changes to beaggregated with maximal generalization along the dimension hierarchy. We formalize thismodel of explaining changes in hierarchical summaries and investigate the problem of …,Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,2007,33
Effective computation of biased quantiles over data streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Skew is prevalent in many data sources such as IP traffic streams. To continually summarizethe distribution of such data; a high-biased set of quantiles (eg; 50th; 90th and 99thpercentiles) with finer error guarantees at higher ranks (eg; errors of 5; 1 and 0.1 percent;respectively) is more useful than uniformly distributed quantiles (eg; 25th; 50th and 75thpercentiles) with uniform error guarantees. In this paper; we address the following twoproblems. First; can we compute quantiles with finer error guarantees for the higher ranks ofthe data distribution effectively using less space and computation time than computing allquantiles uniformly at the finest error? Second; if specific quantiles and their error boundsare requested a priori; can the necessary space usage and computation time be reduced?We answer both questions in the affirmative by formalizing them as the" high-biased" and …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,33
Conductometric mercury [II] sensor based on polyaniline–cryptand-222 hybrid,C Muthukumar; Sagar D Kesarkar; Divesh N Srivastava,Abstract We report a sensor for mercuric ion based on polyaniline modified with cryptand-222. The transduction process is deprotonation and reprotonation of polyaniline followingthe protonation and deprotonation; respectively; of cryptand-222 on binding with Hg 2+ ion.The sensor response has been investigated under various conditions; like the amount ofreceptor; pH; and the electrochemical state of the polymer. The device has sensitivesegments in the range of 10− 12–10− 8 M in various conditions; which is an important regionas per World Health Organization guideline for drinking water. The interference on thesensor response with other metal ions has been investigated.,Journal of electroanalytical chemistry,2007,32
Method and apparatus for packet analysis in a network,*,A method and system for monitoring traffic in a data communication network and forextracting useful statistics and information is disclosed. In accordance with an embodimentof the invention; a network interface card has a run-time system and one or more processingblocks executing on the network interface. The run-time system module feeds informationderived from a network packet to the processing modules which process the information andgenerate output such as condensed statistics about the packets traveling through thenetwork.,*,2007,32
One-dimensional and multi-dimensional substring selectivity estimation,HV Jagadish; Olga Kapitskaia; Raymond T Ng; Divesh Srivastava,Abstract With the increasing importance of XML; LDAP directories; and text-basedinformation sources on the Internet; there is an ever-greater need to evaluate queriesinvolving (sub) string matching. In many cases; matches need to be on multipleattributes/dimensions; with correlations between the multiple dimensions. Effective queryoptimization in this context requires good selectivity estimates. In this paper; we use prunedcount-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. Forthe 1-D problem; we present a novel technique called MO (Maximal Overlap). We thendevelop and analyze two 1-D estimation algorithms; MOC and MOLC; based on MO and aconstraint-based characterization of all possible completions of a given PST. For the kDproblem; we first generalize PSTs to multiple dimensions and develop a space-and time …,The VLDB Journal,2000,32
Finding Quality in Quantity: The Challenge of Discovering Valuable Sources for Integration.,Theodoros Rekatsinas; Xin Luna Dong; Lise Getoor; Divesh Srivastava,ABSTRACT Data is becoming a commodity of tremendous value for many domains. This isleading to a rapid increase in the number of data sources and public access data services;such as cloud-based data markets and data portals; that facilitate the collection; publishingand trading of data. Data sources typically exhibit wide variety and heterogeneity in thetypes or schemas of the data they provide; their quality; and the fees they charge foraccessing their data. Users who want to build upon such publicly available data; must (i)discover sources that are relevant to their applications;(ii) identify sources that collectivelysatisfy the quality and budget requirements of their applications; with few effective cluesabout the quality of the sources; and (iii) repeatedly invest many person-hours in assessingthe eventual usefulness of data sources. All three steps require investigating the content …,CIDR,2015,31
DPT: differentially private trajectory synthesis using hierarchical reference systems,Xi He; Graham Cormode; Ashwin Machanavajjhala; Cecilia M Procopiuc; Divesh Srivastava,Abstract GPS-enabled devices are now ubiquitous; from airplanes and cars to smartphonesand wearable technology. This has resulted in a wealth of data about the movements ofindividuals and populations; which can be analyzed for useful information to aid in city andtraffic planning; disaster preparedness and so on. However; the places that people go candisclose extremely sensitive information about them; and thus their use needs to be filteredthrough privacy preserving mechanisms. This turns out to be a highly challenging task: rawtrajectories are highly detailed; and typically no pair is alike. Previous attempts fail either toprovide adequate privacy protection; or to remain sufficiently faithful to the original behavior.This paper presents DPT; a system to synthesize mobility data based on raw GPStrajectories of individuals while ensuring strong privacy protection in the form of ε …,Proceedings of the VLDB Endowment,2015,30
Accurate and efficient private release of datacubes and contingency tables,Grigory Yaroslavtsev; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava,A central problem in releasing aggregate information about sensitive data is to do soaccurately while providing a privacy guarantee on the output. Recent work focuses on theclass of linear queries; which include basic counting queries; data cubes; and contingencytables. The goal is to maximize the utility of their output; while giving a rigorous privacyguarantee. Most results follow a common template: pick a “strategy” set of linear queries toapply to the data; then use the noisy answers to these queries to reconstruct the queries ofinterest. This entails either picking a strategy set that is hoped to be good for the queries; orperforming a costly search over the space of all possible strategies. In this paper; wepropose a new approach that balances accuracy and efficiency: we show how to improvethe accuracy of a given query set by answering some strategy queries more accurately …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,30
Approximate string processing,Marios Hadjieleftheriou; Divesh Srivastava,Abstract One of the most important primitive data types in modern data processing is text.Text data are known to have a variety of inconsistencies (eg; spelling mistakes andrepresentational variations). For that reason; there exists a large body of literature related toapproximate processing of text. This monograph focuses specifically on the problem ofapproximate string matching; where; given a set of strings S and a query string υ; the goal isto find all strings s ε S that have a user specified degree of similarity to υ. Set S could be; forexample; a corpus of documents; a set of web pages; or an attribute of a relational table. Thesimilarity between strings is always defined with respect to a similarity function that is chosenbased on the characteristics of the data and application at hand. This work presents a surveyof indexing techniques and algorithms specifically designed for approximate string …,*,2011,30
DB&IR integration: report on the Dagstuhl seminar,Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,Abstract This paper is based on a five-day workshop on" Ranked XML Querying" that tookplace in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people fromthree different research communities: database systems (DB); information retrieval (IR); andWeb. The seminar title was interpreted in an IR-style" andish" sense (it covered also subsetsof {Ranking; XML; Querying}; with larger sets being favored) rather than the DB-style strictlyconjunctive manner. So in essence; the seminar really addressed the integration of DB andIR technologies with Web 2.0 being an important target area.,ACM SIGIR Forum,2008,30
DB&IR Integration: Report on the Dagstuhl Seminar" Ranked XML Querying",Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,*,SIGMOD record,2008,30
Method and apparatus for providing anonymization of data,*,A method and apparatus for providing an anonymization of data are disclosed. For example;the method receives a communications graph that encodes a plurality of types of interactionsbetween two or more entities. The method partitions the two or more entities into a pluralityof classes; and applies a type of anonymization to the communications graph.,*,2013,29
Updates through views: A new hope,Yannis Kotidis; Divesh Srivastava; Yannis Velegrakis,Database views are extensively used to represent unmaterialized tables. Applications rarelydistinguish between a materialized base table and a virtual view; thus; they may issueupdate requests on the views. Since views are virtual; update requests on them need to betranslated to updates on the base tables. Existing literature has shown the difficulty oftranslating view updates in a side-effect free manner. To address this problem; we propose anovel approach for separating the data instance into a logical and a physical level. Thisseparation allows us to achieve side-effect free translations of any kind of update on theview. Furthermore; deletes on a view can be translated without affecting the base tables. Wedescribe the implementation of the framework and present our experimental results,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,29
Messaging system with application-defined states,*,A messaging system in which a core messaging infrastructure stores and managesmessaging attributes; but applications external to the core infrastructure define and modifymost attributes. Attribute types may be easily defined or modified; the manner in whichattribute values are obtained may be easily defined or modified; and the entity types to whichattributes are assigned may be easily defined or modified. The messaging system includes aplurality of messaging entities; such as messages; folders; and users; a plurality of attributesassociated with the messaging entities; and a plurality of applications. Each application isoperable to examine and modify at least some of the messaging entities and attributes. Anapplication selection device is operable to examine at least some of the messaging entitiesand at least some of the attributes and to select an application to be invoked; from among …,*,2002,29
CORAL: A database programming language,Raghu Ramakrishnan; Per Bothner; Divesh Srivastava; S Sudarshan,CORAL1 is a database programming language being developed at University of Wisconsin-Madison. We present an outline of the language design; which is currently far from complete.However; this outline should give the reader a feel for the language and the goals of theproject.,Proceedings of the NACLP90 Workshop on Deductive Databases,1990,28
Data auditor: Exploring data quality and semantics using pattern tableaux,Lukasz Golab; Howard Karloff; Flip Korn; Divesh Srivastava,Abstract We present Data Auditor; a tool for exploring data quality and data semantics. Givena rule or an integrity constraint and a target relation; Data Auditor computes pattern tableaux;which concisely summarize subsets of the relation that (mostly) satisfy or (mostly) fail theconstraint. This paper describes 1) the architecture and user interface of Data Auditor; 2) thesupported constraints for testing data consistency and completeness; 3) the heuristics usedby Data Auditor to" tune" a given constraint or its associated parameters for better fit with thedata; and 4) several demonstration scenarios. using real data sets.,Proceedings of the VLDB Endowment,2010,27
What's on the Grapevine?,Albert Angel; Nick Koudas; Nikos Sarkas; Divesh Srivastava,Abstract User generated content and social media (in the form of blogs; wikis; online video;microblogs; etc) are proliferating online. Grapevine conducts large scale data analysis onthe social media collective; distilling and extracting information in real time. It aims to trackentities and stories of interest in millions of blog posts; thousands of tweets; news items; etc.;daily. Grapevine facilitates the interactive exploration of content; allowing users to discoverinteresting or surprising stories; optionally narrowed down on a specific demographic ofinterest (eg" What are Torontonians talking about on blogs?";" What are popular storiesacross news sources in Canada?";" What are financiers in Texas blogging about today?").Stories of interest can be explored in a variety of ways; such as modifying their scope;obtaining related content (blog posts; news; etc); and examining their temporal evolution.,Proceedings of the 35th SIGMOD international conference on Management of data,2009,27
Integrating XML data sources using approximate joins,Sudipto Guha; HV Jagadish; Nick Koudas; Divesh Srivastava; Ting Yu,Abstract XML is widely recognized as the data interchange standard of tomorrow because ofits ability to represent data from a variety of sources. Hence; XML is likely to be the formatthrough which data from multiple sources is integrated. In this article; we study the problemof integrating XML data sources through correlations realized as join operations. Achallenging aspect of this operation is the XML document structure. Two documents mightconvey approximately or exactly the same information but may be quite different in structure.Consequently; an approximate match in structure; in addition to content; has to be foldedinto the join operation. We quantify an approximate match in structure and content for pairsof XML documents using well defined notions of distance. We show how notions of distancethat have metric properties can be incorporated in a framework for joins between XML …,ACM Transactions on Database Systems (TODS),2006,27
Private release of graph statistics using ladder functions,Jun Zhang; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Xiaokui Xiao,Abstract Protecting the privacy of individuals in graph structured data while making accurateversions of the data available is one of the most challenging problems in data privacy. Mostefforts to date to perform this data release end up mired in complexity; overwhelm the signalwith noise; and are not effective for use in practice. In this paper; we introduce a new methodwhich guarantees differential privacy. It specifies a probability distribution over possibleoutputs that is carefully defined to maximize the utility for the given input; while still providingthe required privacy level. The distribution is designed to form a'ladder'; so that each outputachieves the highest'rung'(maximum probability) compared to less preferable outputs. Weshow how our ladder framework can be applied to problems of counting the number ofoccurrences of subgraphs; a vital objective in graph analysis; and give algorithms whose …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,26
Empirical privacy and empirical utility of anonymized data,Graham Cormode; Cecilia M Procopiuc; Entong Shen; Divesh Srivastava; Ting Yu,Procedures to anonymize data sets are vital for companies; government agencies and otherbodies to meet their obligations to share data without compromising the privacy of theindividuals contributing to it. Despite much work on this topic; the area has not yet reachedstability. Early models (k-anonymity and ℓ-diversity) are now thought to offer insufficientprivacy. Noise-based methods like differential privacy are seen as providing strongerprivacy; but less utility. However; across all methods sensitive information of someindividuals can often be inferred with relatively high accuracy. In this paper; we reverse theidea of aprivacy attack;'by incorporating it into a measure of privacy. Hence; we advocate thenotion of empirical privacy; based on the posterior beliefs of an adversary; and their ability todraw inferences about sensitive values in the data. This is not a new model; but rather a …,Data Engineering Workshops (ICDEW); 2013 IEEE 29th International Conference on,2013,26
Randomized synopses for query assurance on data streams,Ke Yi; Feifei Li; Marios Hadjieleftheriou; George Kollios; Divesh Srivastava,The overwhelming flow of information in many data stream applications forces manycompanies to outsource to a third-party the deployment of a Data Stream ManagementSystem (DSMS) for performing desired computations. Remote computations intrinsicallyraise issues of trust; making query execution assurance on data streams a problem withpractical implications. Consider a client observing the same data stream as a remote server(eg; network traffic); that registers a continuous query on the server's DSMS; and receivesanswers upon request. The client needs to verify the integrity of the results using significantlyfewer resources than evaluating the query locally. Towards that goal; we propose aprobabilistic algorithm for selection and aggregate/group-by queries; that uses constantspace irrespective of the result-set size; has low update cost; and arbitrarily small …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,26
Method of performing approximate substring indexing,*,Approximate substring indexing is accomplished by decomposing each string in a databaseinto overlapping “positional q-grams”; sequences of a predetermined length q; andcontaining information regarding the “position” of each q-gram within the string (ie; 1st q-gram; 4th q-gram; etc.). An index is then formed of the tuples of the positional q-gram data(such as; for example; a B-tree index or a hash index). Each query applied to the database issimilarly parsed into a plurality of positional q-grams (of the same length); and a candidateset of matches is found. Position-directed filtering is used to remove the candidates whichhave the q-grams in the wrong order and/or too far apart to form a “verified” output ofmatching candidates. If errors are permitted (defined in terms of an edit distance betweeneach candidate and the query); an edit distance calculation can then be performed to …,*,2006,26
A mapping schema and interface for XML stores,Sihem Amer-Yahia; Divesh Srivastava,Abstract Most XML storage efforts have focused on mapping documents to relationaldatabases. Mapping choices range from storing documents verbatim to shreddingdocuments into relations in various ways. These choices are usually hard-coded into eachstorage system which makes sharing loading and querying utilities and exchanginginformation between different XML storage systems hard. To address these issues; wedesigned MXM and IMXM; a mapping schema and an interface API to define and query XML-to-relational mappings. A mapping is expressed as an instance of MXM. MXM is declarative;concise and captures most existing XML-to-relational mappings. Mappings can beexpressed for documents for which no schema information is provided or documents thatconform to either a DTD or an XML Schema. IMXM is an interface that allows querying of …,Proceedings of the 4th international workshop on Web information and data management,2002,26
Evolution and revolutions in LDAP directory caches,Olga Kapitskaia; Raymond Ng; Divesh Srivastava,Abstract LDAP directories have recently proliferated with the growth of the Internet; and arebeing used in a wide variety of network-based applications. In this paper; we propose theuse of generalized queries; referred to as query templates; obtained by generalizingindividual user queries; as the semantic basis for low overhead; high benefit LDAP directorycaches for handling declarative queries. We present efficient incremental algorithms that;given a sequence of user queries; maintain a set of potentially beneficial candidate querytemplates; and select a sub-set of these candidates for admission into the directory cache. Anovel feature of our algorithms is their ability to deal with overlapping query templates.Finally; we demonstrate the advantages of template caches over query caches; with anexperimental study based on real data and a prototype implementation of the LDAP …,Advances in Database Technology—EDBT 2000,2000,26
Adaptive processing of top-k queries in nested-structure arbitrary markup language such as XML,*,A method of adaptively evaluating a top-k query involves (1204) forming a servers havingrespective server queues storing candidate answers; processing (1322) the candidateanswers; and (1232) providing a top-k set as a query evaluation. Processing includes (1402)adaptively choosing a winning server to whose queue a current candidate answer should besent;(1404) sending the current candidate answer to the winning server's queue;(1334)adaptively choosing a next candidate answer to process from the winning server'squeue;(1336) computing a join between the current candidate answer and next candidateanswers at the winning server; so as to produce a new current candidate answer; and(1338) updating the top-k set with the new current candidate answer only if a score of thenew current candidate answer exceeds a score of a top-k answer in a top-k set. A method …,*,2013,25
Enabling content dissemination using efficient and scalable multicast,Tae Won Cho; Michael Rabinovich; KK Ramakrishnan; Divesh Srivastava; Yin Zhang,Multicast is an approach that uses network and server resources efficiently to distributeinformation to groups. As networks evolve to become information-centric; users willincreasingly demand publish-subscribe based access to fine-grained information; andmulticast will need to evolve to (i) manage an increasing number of groups; with a distinctgroup for each piece of distributable content;(ii) support persistent group membership; asgroup activity can vary over time; with intense activity at some times; and infrequent (but stillcritical) activity at others. These requirements raise scalability challenges that are not met bytoday's multicast techniques. In this paper; we propose the MAD (multicast with adaptivedual-state) architecture to provide efficient multicast service at massive scale. MAD canscalably support a vast number of multicast groups; with varying activity over time; based …,INFOCOM 2009; IEEE,2009,25
-Phrase Matching in XML,Sihem Amer-Yahia; Mary Fernández; Divesh Srivastava; Yu Xu,Phrase matching is a common information retrieval (IR) technique to search text and identifyrelevant documents in a document collection. Phrase matching in XML presents newchallenges as text may be interleaved with arbitrary markup; thwarting search techniquesthat require strict contiguity or close proximity of keywords. This chapter presents atechnique for phrase matching in XML that permits dynamic specification of both the phraseto be matched and the markup to be ignored. The chapter develops an effective algorithm forthe technique that utilizes inverted indices on phrase words and XML tags. It describesexperimental results comparing the algorithm to an indexed-nested loop algorithm thatillustrates algorithm's efficiency.Phrase Matching in XML Sihem Amer-Yahia; MaryFernandez; Divesh Srivastava AT&T Labs-Research {sihem; raft; divesh}@ research. att …,*,2003,25
Computer systems; methods and computer program products for data anonymization for aggregate query answering,*,Computer program products are provided for anonymizing a database that includes tuples.A respective tuple includes at least one quasi-identifier and sensitive attributes associatedwith the quasi-identifier. These computer program products include computer readableprogram code that is configured to (k; e)-anonymize the tuples over a number k of differentvalues in a range e of values; while preserving coupling at least two of the sensitiveattributes to one another in the sets of attributes that are anonymized to provide a (k; e)-anonymized database. Related computer systems and methods are also provided.,*,2012,24
Solomon: Seeking the truth via copying detection,Xin Luna Dong; Laure Berti-Equille; Yifan Hu; Divesh Srivastava,Abstract We live in the Information Era; with access to a huge amount of information from avariety of data sources. However; data sources are of different qualities; often providingconflicting; out-of-date and incomplete data. Data sources can also easily copy; reformat andmodify data from other sources; propagating erroneous data. These issues make theidentification of high quality information and sources non-trivial. We demonstrate theSolomon system; whose core is a module that detects copying between sources. Wedemonstrate that we can effectively detect copying relationship between data sources;leverage the results in truth discovery; and provide a user-friendly interface to facilitate usersin identifying sources that best suit their information needs.,Proceedings of the VLDB Endowment,2010,24
Dexter: large-scale discovery and extraction of product specifications on the web,Disheng Qiu; Luciano Barbosa; Xin Luna Dong; Yanyan Shen; Divesh Srivastava,Abstract The web is a rich resource of structured data. There has been an increasing interestin using web structured data for many applications such as data integration; web search andquestion answering. In this paper; we present Dexter; a system to find product sites on theweb; and detect and extract product specifications from them. Since product specificationsexist in multiple product sites; our focused crawler relies on search queries and backlinks todiscover product sites. To perform the detection; and handle the high diversity ofspecifications in terms of content; size and format; our system uses supervised learning toclassify HTML fragments (eg; tables and lists) present in web pages as specifications or not.To perform large-scale extraction of the attribute-value pairs from the HTML fragmentsidentified by the specification detector; D exter adopts two lightweight strategies: a …,Proceedings of the VLDB Endowment,2015,23
Processing top-k join queries,Minji Wu; Laure Berti-Equille; Amélie Marian; Cecilia M Procopiuc; Divesh Srivastava,Abstract We consider the problem of efficiently finding the top-k answers for join queries overweb-accessible databases. Classical algorithms for finding top-k answers use branch-and-bound techniques to avoid computing scores of all candidates in identifying the top-kanswers. To be able to apply such techniques; it is critical to efficiently compute (lower andupper) bounds and expected scores of candidate answers in an incremental fashion duringthe evaluation. In this paper; we describe novel techniques for these problems. The firstcontribution of this paper is a method to efficiently compute bounds for the score of a queryresult when tuples in tables from the" FROM" clause are discovered incrementally; througheither sorted or random access. Our second contribution is an algorithm that; given a set ofpartially evaluated candidate answers; determines a good order in which to access the …,Proceedings of the VLDB Endowment,2010,23
I4E: interactive investigation of iterative information extraction,Anish Das Sarma; Alpa Jain; Divesh Srivastava,Abstract Information extraction systems are increasingly being used to mine structuredinformation from unstructured text documents. A commonly used unsupervised technique isto build iterative information extraction (IIE) systems that learn task-specific rules; calledpatterns; to generate the desired tuples. Oftentimes; output from an information extractionsystem may contain unexpected results which may be due to an incorrect pattern; incorrecttuple; or both. In such scenarios; users and developers of the extraction system could greatlybenefit from an investigation tool that can quickly help them reason about and repair theoutput. In this paper; we develop an approach for interactive post-extraction investigation forIIE systems. We formalize three important phases of this investigation; namely; explain theIIE result; diagnose the influential and problematic components; and repair the output …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,23
Distributed evaluation of network directory queries,Sihem Amer-Yahia; Divesh Srivastava; Dan Suciu,We describe novel efficient techniques for the distributed evaluation of hierarchicalaggregate selection queries over LDAP directory data; distributed across multipleautonomous directory servers. Such queries are useful for emerging applications like thedirectory enabled networks initiative. Our techniques follow the LDAP approach ofdistributed query evaluation by referrals; where each relevant server computes answerslocally; and the LDAP client coordinates between directory servers. We make a conceptualseparation between the identification of relevant servers and the distributed computation ofanswers. We focus on the challenging task of generating an efficient plan for evaluatinghierarchical aggregate selection queries; which involves correlating directory entries acrossmultiple servers. The key features of our plan are: 1) the network traffic consists of query …,IEEE Transactions on Knowledge and Data Engineering,2004,23
Focusing search in hierarchical structures with directory sets,Guy Jacobson; Balachander Krishnamurthy; Divesh Srivastava; Dan Suciu,Focusing Search in Hierarchical Structures with Directory Sets Guy Jacobson AT&T Labs-Researchguy@research .att.com Divesh Srivastava AT&T Labs-Research divesh@research.att.com BalachanderKrishnamurthy AT&T Labs-Research bala@research.att.com Dan Suciu AT&T Labs-Researchsuciu@research.att.com Abstract Keyword-based searches on the World Wide Web are oftenof limited use; because they return too many uninteresting matches. We propose here a novelmechanism that permits the user to specify directory sets to restrict the space of doc- umentssearched; and; at the same time; increase the speed of the search. We view the Web as asingle; huge hierarchy; reflected in the structure of the URLs. We refer to each sub- tree in thishierarchy as a directory; and group semantical^ related documents from multiple directories intoa directory set. Starting from a collection of pre-defined directory sets; a user can …,Proceedings of the seventh international conference on Information and knowledge management,1998,23
The valid model semantics for logic programs,Catriel Beeri; Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract We present the valid model semantics; a new approach to providing semantics forlogic programs with negation; set-terms and grouping. The valid model semantics is a three-valued semantics; and is defined in terms of a 'normal form'computation. The valid modelsemantics also gives meaning to the generation and use of non-ground facts (ie; facts withvariables) in a computation. The formulation of the semantics in terms of a normal formcomputation offers important insight not only into the valid model semantics; but also intoother semantics proposed earlier. We show that the valid model semantics extends the well-founded semantics in a natural manner; and has several advantages over it. The well-founded semantics can also be undertood using a variant of the normal form computationsthat we use; the normal form computations used for valid semantics seem more natural …,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,23
On repairing structural problems in semi-structured data,Flip Korn; Barna Saha; Divesh Srivastava; Shanshan Ying,Abstract Semi-structured data such as XML are popular for data interchange and storage.However; many XML documents have improper nesting where open-and close-tags areunmatched. Since some semi-structured data (eg; Latex) have a flexible grammar and sincemany XML documents lack an accompanying DTD or XSD; we focus on computing asyntactic repair via the edit distance. To solve this problem; we propose a dynamicprogramming algorithm which takes cubic time. While this algorithm is not scalable; well-formed substrings of the data can be pruned to enable faster computation. Unfortunately;there are still cases where the dynamic program could be very expensive; hence; we givebranch-and-bound algorithms based on various combinations of two heuristics; calledMinCost and MaxBenefit; that trade off between accuracy and efficiency. Finally; we …,Proceedings of the VLDB Endowment,2013,22
Efficient and Effective Analysis of Data Quality using Pattern Tableaux.,Lukasz Golab; Flip Korn; Divesh Srivastava,Abstract Data Auditor is a system for analyzing data quality via exploring data semantics.Given a user-supplied constraint; such as a functional dependency or an inclusiondependency; the system computes pattern tableaux; which are concise summaries ofsubsets of the data that satisfy (or fail) the constraint. The engine of Data Auditor is anefficient algorithm for finding these patterns; which defers expensive computation onpatterns until needed during search; thereby pruning wasted effort. We demonstrate theutility of our approach on a variety of data as well as the performance gain from employingthis algorithm.,IEEE Data Eng. Bull.,2011,22
Automatic discovery of attributes in relational databases,Meihui Zhang; Marios Hadjieleftheriou; Beng Chin Ooi; Cecilia M Procopiuc; Divesh Srivastava,Abstract In this work we design algorithms for clustering relational columns into attributes; ie;for identifying strong relationships between columns based on the common properties andcharacteristics of the values they contain. For example; identifying whether a certain set ofcolumns refers to telephone numbers versus social security numbers; or names ofcustomers versus names of nations. Traditional relational database schema languages usevery limited primitive data types and simple foreign key constraints to express relationshipsbetween columns. Object oriented schema languages allow the definition of custom datatypes; still; certain relationships between columns might be unknown at design time or theymight appear only in a particular database instance. Nevertheless; these relationships arean invaluable tool for schema matching; and generally for better understanding and …,Proceedings of the 2011 international conference on Management of data,2011,22
Summary graphs for relational database schemas,Xiaoyan Yang; Cecilia M Procopiuc; Divesh Srivastava,Abstract Increasingly complex databases need ever more sophisticated tools to help usersunderstand their schemas and interact with the data. Existing tools fall short of eitherproviding the “big picture;” or of presenting useful connectivity information. In this paper wedefine summary graphs; a novel approach for summarizing schemas. Given a set of user-specified query tables; the summary graph automatically computes the most relevant tablesand joins for that query set. The output preserves the most informative join paths betweenthe query tables; while meeting size constraints. In the process; we define a novelinformation-theoretic measure over join edges. Unlike most subgraph extraction work; weallow meta edges (ie; edges in the transitive closure) to help reduce output complexity. Weprove that the problem is NP-Hard; and solve it as an integer program. Our extensive …,*,2011,22
Incremental Maintenance of Inverted Indexes for Approximate String Matching,*,In embodiments of the disclosed technology; indexes; such as inverted indexes; are updatedonly as necessary to guarantee answer precision within predefined thresholds which aredetermined with little cost in comparison to the updates of the indexes themselves. With thepresent technology; a batch of daily updates can be processed in a matter of minutes; ratherthan a few hours for rebuilding an index; and a query may be answered with assurances thatthe results are accurate or within a threshold of accuracy.,*,2009,22
Architecting and implementing versatile dependability,Tudor Dumitraş; Deepti Srivastava; Priya Narasimhan,Abstract Distributed applications must often consider and select the appropriate trade-offsamong three important aspects–fault-tolerance; performance and resources. We introduce anovel concept; called versatile dependability; that provides a framework for analyzing andreasoning about these trade-offs in dependable software architectures. We present thearchitecture of a middleware framework that implements versatile dependability by providingthe appropriate” knobs” to tune and re-calibrate the trade-offs. Our framework can adjust theproperties and the behavior of the system at development-time; at deployment-time; andthroughout the application's life-cycle. This renders the versatile dependability approachuseful both to applications that require static fault-tolerance configurations supporting theloss/addition of resources and changing workloads; as well as to applications that evolve …,*,2005,22
Method for effective indexing of partially dynamic documents,*,A method more efficiently indexes dynamic documents. The method adjusts the frequencywith which dynamic documents are retrieved taking into account the extent to which thedocument varies between its most recent retrievals. Furthermore; the method selectsportions of the document to be indexed based on the substance of the differences betweenrecently retrieved copies.,*,2002,22
Method and system for pattern matching having holistic twig joins,*,As is known in the art; the extensible Markup Language (XML) employs a tree-structured modelfor representing data. Queries in XML query languages typically specify patterns of selectionpredicates on multiple elements that have some specified tree structured relationships. Forexample; the XQuery expression: book[title='XML' ]\\author[fn='jane' AND ln='doe' ] matches authorelements that (i) have a child subelement “fn” with content “jane”; (ii) have a child subelement“ln” with content “doe”; and (iii) are descendants of book elements that have a child title subelementwith content XML. This expression can be represented as a node-labeled twig (or small tree)pattern with elements and string values as node labels … Finding all occurrences of a twig patternin a database is a core operation in XML query processing; both in relational implementationsof XML databases; and in native XML databases. Known processing techniques typically …,*,2007,21
Combining quantitative and logical data cleaning,Nataliya Prokoshyna; Jaroslaw Szlichta; Fei Chiang; Renée J Miller; Divesh Srivastava,Abstract Quantitative data cleaning relies on the use of statistical methods to identify andrepair data quality problems while logical data cleaning tackles the same problems usingvarious forms of logical reasoning over declarative dependencies. Each of theseapproaches has its strengths: the logical approach is able to capture subtle data qualityproblems using sophisticated dependencies; while the quantitative approach excels atensuring that the repaired data has desired statistical properties. We propose a novelframework within which these two approaches can be used synergistically to combine theirrespective strengths. We instantiate our framework using (i) metric functional dependencies;a type of dependency that generalizes functional dependencies (FDs) to identifyinconsistencies in domains where only large differences in metric data are considered to …,Proceedings of the VLDB Endowment,2015,20
Big data integration,Divesh Srivastava,Abstract The Big Data era is upon us: data is being generated; collected and analyzed at anunprecedented scale; and data-driven decision making is sweeping through all aspects ofsociety. Since the value of data explodes when it can be linked and fused with other data;addressing the big data integration (BDI) challenge is critical to realizing the promise of BigData. BDI differs from traditional data integration in many dimensions:(i) the number of datasources; even for a single domain; has grown to be in the tens of thousands;(ii) many of thedata sources are very dynamic; as a huge amount of newly collected data are continuouslymade available;(iii) the data sources are extremely heterogeneous in their structure; withconsiderable variety even for substantially similar entities; and (iv) the data sources are ofwidely differing qualities; with significant differences in the coverage; accuracy and …,Proceedings of the 19th International Conference on Management of Data,2013,20
Optimization techniques for reactive network monitoring,Ahmet Bulut; Nick Koudas; Anand Meka; Ambuj K Singh; Divesh Srivastava,We develop a framework for minimizing the communication overhead of monitoring globalsystem parameters in IP networks and sensor networks. A global system predicate is definedas a conjunction of the local properties of different network elements. A typical example is toidentify the time windows when the outbound traffic from each network element exceeds apredefined threshold. Our main idea is to optimize the scheduling of local event reportingacross network elements for a given network traffic load and local event frequencies. Thesystem architecture consists of N distributed network elements coordinated by a centralmonitoring station. Each network element monitors a set of local properties and the centralstation is responsible for identifying the status of global parameters registered in the system.We design an optimal algorithm; the partition and rank (PAR) scheme; when the local …,IEEE Transactions on Knowledge and Data Engineering,2009,20
Optimizing away joins on data streams,Lukasz Golab; Theodore Johnson; Nick Koudas; Divesh Srivastava; David Toman,Abstract Monitoring aggregates on network traffic streams is a compelling application of datastream management systems. Often; streaming aggregation queries involve joining multipleinputs (eg; client requests and server responses) using temporal join conditions (eg; within 5seconds); followed by computation of aggregates (eg; COUNT) over temporal windows (eg;every 5 minutes). These types of queries help identify malfunctioning servers (missingresponses); malicious clients (bursts of requests during a denial-of-service attack); orimproperly configured protocols (short timeout intervals causing many retransmissions).However; while such query expression is natural; its evaluation over massive data streamsis inefficient. In this paper; we develop rewriting techniques for streaming aggregationqueries that join multiple inputs. Our techniques identify conditions under which …,Proceedings of the 2nd international workshop on Scalable stream processing system,2008,20
Flexible list management in a directory,HV Jagadish; Mark A Jones; Divesh Srivastava; Dimitra Vista,Abstract Lists of entities must often be speciﬁed in many real-world applications such ascustomer lists; electronic distribution lists and access control lists. These lists are typicallyspeciﬁed through explicit enumeration; frequently aided by recursive expansion. In thispaper; we discuss the declara-tive speciﬁcation and extraction of members of such lists asqueries over a directory that maintains information both about individuals and about lists;and identify key features that the directory must support to manage lists in a ﬂexible manner.X. 500 is the industry standard for modeling infor-mation about individuals in a directory; andLDAP is the proposed standard for accessing directory information. We have designed andbuilt a system to represent and manage lists in the X. 500 information model; and developedeffi-ciently evaluable extensions to the LDAP query language for the location and …,Proceedings of the seventh international conference on Information and knowledge management,1998,20
Data fusion: resolving conflicts from multiple sources,Xin Luna Dong; Laure Berti-Equille; Divesh Srivastava,Abstract Many data management applications; such as setting up Web portals; managingenterprise data; managing community data; and sharing scientific data; require integratingdata from multiple sources. Each of these sources provides a set of values; and differentsources can often provide conflicting values. To present quality data to users; it is critical toresolve conflicts and discover values that reflect the real world; this task is called data fusion.Typically; we expect a true value to be provided by more sources than any particular falseone; so we can take the value provided by the largest number of sources as the truth.Unfortunately; a false value can be spread through copying and that makes truth discoveryextremely tricky. In this chapter; we consider how to find true values from conflictinginformation when there are a large number of sources; among which some may copy from …,*,2013,19
Streaming multiple aggregations using phantoms,Rui Zhang; Nick Koudas; Beng Chin Ooi; Divesh Srivastava; Pu Zhou,Abstract Data streams characterize the high speed and large volume input of a new class ofapplications such as network monitoring; web content analysis and sensor networks. Amongthese applications; network monitoring may be the most compelling one--the backbone of alarge internet service provider can generate 1 petabyte of data per day. For many networkmonitoring tasks such as traffic analysis and statistics collection; aggregation is a primitiveoperation. Various analytical and statistical needs naturally lead to related aggregatequeries. In this article; we address the problem of efficiently computing multiple aggregationsover high-speed data streams based on the two-level query processing architecture of GS; areal data stream management system deployed in AT & T. We discern that additionallycomputing and maintaining fine-granularity aggregations (called phantoms) has the …,The VLDB Journal,2010,19
Information theory for data management,Divesh Srivastava; Suresh Venkatasubramanian,Abstract We explore the use of information theory as a tool to express and quantify notions ofinformation content and information transfer for representing and analyzing data; usingexamples from database design; data integration and data anonymization. We also examinethe computational challenges associated with information-theoretic primitives; indicatinghow they might be computed efficiently.,Proceedings of the 2010 international conference on Management of data,2010,19
Interpretable and informative explanations of outcomes,Kareem El Gebaly; Parag Agrawal; Lukasz Golab; Flip Korn; Divesh Srivastava,Abstract In this paper; we solve the following data summarization problem: given a multi-dimensional data set augmented with a binary attribute; how can we construct aninterpretable and informative summary of the factors affecting the binary attribute in terms ofthe combinations of values of the dimension attributes? We refer to such summaries asexplanation tables. We show the hardness of constructing optimally-informative explanationtables from data; and we propose effective and efficient heuristics. The proposed heuristicsare based on sampling and include optimizations related to computing the informationcontent of a summary from a sample of the data. Using real data sets; we demonstrate theadvantages of explanation tables compared to related approaches that can be adapted tosolve our problem; and we show significant performance benefits of our optimizations.,Proceedings of the VLDB Endowment,2014,18
Method and apparatus for providing anonymization of data,*,A method and apparatus for providing an anonymization of data are disclosed. For example;the method receives a request for anonymizing; wherein the request comprises a bipartitegraph for a plurality of associations or a table that encodes the plurality of associations forthe bipartite graph. The method places each node in the bipartite graph in a safe group andprovides an anonymized graph that encodes the plurality of associations of the bipartitegraph; if a safe group for all nodes of the bipartite graph is found.,*,2013,18
Exploring a few good tuples from text databases,Alpa Jain; Divesh Srivastava,Information extraction from text databases is a useful paradigm to populate relational tablesand unlock the considerable value hidden in plain-text documents. However; informationextraction can be expensive; due to various complex text processing steps necessary inuncovering the hidden data. There are a large number of text databases available; and notevery text database is necessarily relevant to every relation. Hence; it is important to be ableto quickly explore the utility of running an extractor for a specific relation over a given textdatabase before carrying out the expensive extraction task. In this paper; we present a novelexploration methodology of {\em finding a few good tuples} for a relation that can beextracted from a database which allows for judging the relevance of the database for therelation. Specifically; we propose the notion of a good (k; $\ell $) query as one that can …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,18
Method of pattern searching,*,Structural join mechanisms provide efficient query pattern matching.In one embodiment; tree-merge mechanisms are provided. In anotherembodiment; stack-tree mechanisms are provided.,*,2008,18
METHODS AND SYSTEMS TO STORE STATE USED TO FORWARD MULTICAST TRAFFIC,*,*,*,2008,18
THE CORAL USER MANUAL A Tutorial Introduction to CORAL,Raghu Ramakrishnan; Praveen Seshadri; Divesh Srivastava; S Sudarshan,CORAL1 is a database programming language based on Horn clause logic developed atthe University of Wisconsin {Madison. Source code for CORAL (written in C++) is availableby anonymous ftp over the internet. The CORAL project was initiated in 1988-89; under thename Conlog; and a preliminary report was presented at a workshop in the NACLP 89conference. Preliminary versions of the system have been used by a few groups; but this isthe rst widely available release. We would welcome any feedback on the system.Comments; bug reports and questions should be mailed to coral@ cs. wisc. edu. We wouldlike to acknowledge the contributions of the following people to the CORAL system. PerBothner; who was largely responsible for the initial implementation of CORAL that served asthe basis for subsequent development; was a major early contributor. Joseph Albert …,Computer Science Department; University of Wisconsin-Madison; available via anonymous ftp from ftp. cs. wisc. edu in the directory coral doc,1993,18
Identifying the extent of completeness of query answers over partially complete databases,Simon Razniewski; Flip Korn; Werner Nutt; Divesh Srivastava,Abstract In many applications including loosely coupled cloud databases; collaborativeediting and network monitoring; data from multiple sources is regularly used for queryanswering. For reasons such as system failures; insufficient author knowledge or networkissues; data may be temporarily unavailable or generally nonexistent. Hence; not all dataneeded for query answering may be available. In this paper; we propose a natural class ofcompleteness patterns; expressed by selections on database tables; to specify completeparts of database tables. We then show how to adapt the operators of relational algebra sothat they manipulate these completeness patterns to compute completeness patternspertaining to query answers. Our proposed algebra is computationally sound and completewith respect to the information that the patterns provide. We show that stronger …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,17
Scaling up copy detection,Xian Li; Xin Luna Dong; Kenneth B Lyons; Weiyi Meng; Divesh Srivastava,Recent research shows that copying is prevalent for Deep-Web data and consideringcopying can significantly improve truth finding from conflicting values. However; existingcopy detection techniques do not scale for large sizes and numbers of data sources; so truthfinding can be slowed down by one to two orders of magnitude compared with thecorresponding techniques that do not consider copying. In this paper; we study how toimprove scalability of copy detection on structured data. Our algorithm builds an invertedindex for each shared value and processes the index entries in decreasing order of howmuch the shared value can contribute to the conclusion of copying. We show how we usethe index to prune the data items we consider for each pair of sources; and to incrementallyrefine our results in iterative copy detection. We also apply a sampling strategy with which …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,17
Multicast with adaptive dual-state,*,A method and system are described to multicast with an adaptive dual state. The systemreceives multicast traffic over a membership tree including a first plurality of nodesconnected in a first topology destined for a plurality of multicast members of a first multicastgroup. Next; the system determines a rate of multicast traffic that exceeds a predeterminedthreshold based on the receiving the multicast traffic. Next; the system generates adissemination tree including a second plurality of nodes connected in a second topology toreduce a number of hops to communicate the multicast traffic to the plurality of multicastmembers of the first multicast group. Finally; the system forwards the multicast traffic to theplurality of multicast members of the first multicast group over the dissemination tree.,*,2011,17
Enabling real time data analysis,Divesh Srivastava; Lukasz Golab; Rick Greer; Theodore Johnson; Joseph Seidel; Vladislav Shkapenyuk; Oliver Spatscheck; Jennifer Yates,Abstract Network-based services have become a ubiquitous part of our lives; to the pointwhere individuals and businesses have often come to critically rely on them. Building andmaintaining such reliable; high performance network and service infrastructures requires theability to rapidly investigate and resolve complex service and performance impacting issues.To achieve this; it is important to collect; correlate and analyze massive amounts of data froma diverse collection of data sources in real time. We have designed and implemented avariety of data systems at AT&T Labs-Research to build highly scalable databases thatsupport real time data collection; correlation and analysis; including (a) the Daytona datamanagement system;(b) the DataDepot data warehousing system;(c) the GS tool datastream management system; and (d) the Bistro data feed manager. Together; these data …,Proceedings of the VLDB Endowment,2010,17
Streams; security and scalability,Theodore Johnson; S Muthukrishnan; Oliver Spatscheck; Divesh Srivastava,Abstract Network-based attacks; such as DDoS attacks and worms; are threatening thecontinued utility of the Internet. As the variety and the sophistication of attacks grow; earlydetection of potential attacks will become crucial in mitigating their impact. We argue that theGigascope data stream management system has both the functionality and the performanceto serve as the foundation for the next generation of network intrusion detection systems.,Data and Applications Security XIX,2005,17
Method and apparatus for substring selectivity estimation,*,A method for estimating string-occurrence probability in a database comprises receiving afirst probability of occurrence for each maximal substring from a plurality of substrings; eachmaximal substring in the plurality of substrings belonging to the string; obtaining an overallprobability of occurrence; receiving a probability of occurrence for a maximal overlap of eachmaximal substring in the plurality of maximal substrings; obtaining a normalization factor;and dividing the overall probability of occurrence by the normalization factor to obtain theestimate.,*,2002,17
Answering SQL queries using materialized views,Divesh Srivastava; Shaul Dar; HV Jagadish; Alon Y Levy,*,Proc. of VLDB. Bombay; India,1996,17
Online entity resolution using an oracle,Donatella Firmani; Barna Saha; Divesh Srivastava,Abstract Entity resolution (ER) is the task of identifying all records in a database that refer tothe same underlying entity. This is an expensive task; and can take a significant amount ofmoney and time; the end-user may want to take decisions during the process; rather thanwaiting for the task to be completed. We formalize an online version of the entity resolutiontask; and use an oracle which correctly labels matching and non-matching pairs throughqueries. In this setting; we design algorithms that seek to maximize progressive recall; anddevelop a novel analysis framework for prior proposals on entity resolution with an oracle;beyond their worst case guarantees. Finally; we provide both theoretical and experimentalanalysis of the proposed algorithms.,Proceedings of the VLDB Endowment,2016,16
Method of performing approximate substring indexing,*,Approximate substring indexing is accomplished by decomposing each string in a databaseinto overlapping “positional q-grams”; sequences of a predetermined length q; andcontaining information regarding the “position” of each q-gram within the string (ie; 1st q-gram; 4th q-gram; etc.). An index is then formed of the tuples of the positional q-gram data(such as; for example; a B-tree index or a hash index). Each query applied to the database issimilarly parsed into a plurality of positional q-grams (of the same length); and a candidateset of matches is found. Position-directed filtering is used to remove the candidates whichhave the q-grams in the wrong order and/or too far apart to form a “verified” output ofmatching candidates. If errors are permitted (defined in terms of an edit distance betweeneach candidate and the query); an edit distance calculation can then be performed to …,*,2008,16
Rapid identification of column heterogeneity,Bing Tian Dai; Nick Koudas; Beng Chin Ooi; Divesh Srivastava; Suresh Venkatasubramanian,Data quality is a serious concern in every data management application; and a variety ofquality measures have been proposed; eg; accuracy; freshness and completeness; tocapture common sources of data quality degradation. We identify and focus attention on anovel measure; column heterogeneity; that seeks to quantify the data quality problems thatcan arise when merging data from different sources. We identify desiderata that a columnheterogeneity measure should intuitively satisfy; and describe our technique to quantifydatabase column heterogeneity based on using a novel combination of cluster entropy andsoft clustering. Finally; we present detailed experimental results; using diverse data sets ofdifferent types; to demonstrate that our approach provides a robust mechanism foridentifying and quantifying database column heterogeneity.,Data Mining; 2006. ICDM'06. Sixth International Conference on,2006,16
Iterative multi-tier management information modeling,J-P Martin-Flatin; Divesh Srivastava; Andrea Westerinen,The management information models currently used in the Internet have several limitations.Some of them contain errors; are missing important features; or are difficult to understand.Second; standards bodies keep reinventing the wheel; which confuses the terminology(hence customers) and wastes precious time. Third; finding a good balance between tooabstract; and overly detailed models is a tough challenge; rarely achieved in practice. Last;the learning curve of existing data models is too steep. We propose to alleviate theseproblems by adopting a new process for designing and standardizing managementinformation models. It is inspired by two techniques form software engineering: the iterativeand incremental software development process; which addresses the shortcomings of thewaterfall process usually adhered to by the IETF and DMTF; and multi-tier models; which …,IEEE Communications Magazine,2003,16
Hierarchical or relational? A case for a modern hierarchical data model,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Much of the data we deal with every day is organized hierarchically: file systems; libraryclassification schemes and yellow page categories are salient examples. Business data too;benefits from a hierarchical organization; and indeed the hierarchical data model was quiteprevalent thirty years ago. Due to the recently increased importance of X. 500/LDAPdirectories; which are hierarchical; and the prevalence of aggregation hierarchies indatacubes; there is now renewed interest in the hierarchical organization of data. Wedevelop a framework for a modern hierarchical data model; substantially improved from theoriginal version by taking advantage of the lessons learned in the relational databasecontext. We argue that this new hierarchical data model has many benefits with respect tothe ubiquitous flat relational data model. We argue also that this model is well-suited for …,Knowledge and Data Engineering Exchange; 1999.(KDEX'99) Proceedings. 1999 Workshop on,1999,16
Verification of outsourced data streams,*,Embodiments disclosed herein are directed to verifying query results of an untrusted server.A data owner outsources a data stream to the untrusted server; which is configured torespond to a query from a client with the query result; which is returned to the client. Thedata owner can maintain a vector associated with query results returned by the server andcan generate a verification synopsis using the vector and a seed. The verification synopsisincludes a polynomial; where coefficients of the polynomial are determined based on theseed. The data owner outputs the verification synopsis and the seed to a client forverification of the query results.,*,2012,15
Phrase matching in documents having nested-structure arbitrary (document-specific) markup,*,A method of searching a document having nested-structure document-specific markup (suchas Extensible Markup Language (XML)) involves 112 receiving a query that designates atleast (A) a phrase to be matched in a phrase matching process; and (B) a selectivedesignation of at least a tag or annotation that is to be ignored during the phrase matchingprocess. The method further involves 114 deriving query-specific indices based on query-independent indices that were created specific to each document; and 116 carrying out thephrase matching process using the query-specific indices on the document having thenested-structure document-specific markup.,*,2008,15
Architectural support for mode-driven fault tolerance in distributed applications,Deepti Srivastava; Priya Narasimhan,Abstract Many distributed applications exhibit different types of system behaviors; or modes;during the course of their operation. Each such mode may have different functional and non-functional requirements (such as fault tolerance; availability; and security). A static softwarefault-tolerance solution can not cater to the needs of every mode; and also does not utilizesystem resources intelligently. A flexible architecture is required to provide dependabilitythat can be tailored for such applications. We propose a novel mode-driven fault-toleranceapproach that includes:(i) a generic framework to extend the specification of modes with fault-tolerance requirements; and (ii) a software architecture that uses this description to providethe appropriate fault tolerance for each mode at runtime. We also present a case study usinga distributed multi-modal CORBA application to demonstrate the effectiveness of our …,ACM SIGSOFT Software Engineering Notes,2005,15
The data warehouse of newsgroups,Himanshu Gupta; Divesh Srivastava,Abstract Electronic newsgroups are one of the primary means for the dissemination;exchange and sharing of information. We argue that the current newsgroup model isunsatisfactory; especially when posted articles are relevant to multiple newsgroups. Wedemonstrate that considerable additional flexibility can be achieved by managingnewsgroups in a data warehouse; where each article is a tuple of attribute-value pairs; andeach newsgroup is a view on the set of all posted articles. Supporting this paradigm for alarge set of newsgroups makes it imperative to efficiently support a very large number ofviews: this is the key difference between newsgroup data warehouses and conventionaldata warehouses. We identify two complementary problems concerning the design of such anewsgroup data warehouse. An important design decision that the system needs to make …,Database Theory—ICDT’99,1999,15
Compact explanation of data fusion decisions,Xin Luna Dong; Divesh Srivastava,Abstract Despite the abundance of useful information on the Web; different Web sourcesoften provide conflicting data; some being out-of-date; inaccurate; or erroneous. Data fusionaims at resolving conflicts and finding the truth. Advanced fusion techniques apply iterativeMAP (Maximum A Posteriori) analysis that reasons about trustworthiness of sources andcopying relationships between them. Providing explanations for such decisions is importantfor a better understanding; but can be extremely challenging because of the complexity ofthe analysis during decision making. This paper proposes two types of explanations for data-fusion results: snapshot explanations take the provided data and any other decision inferredfrom the data as evidence and provide a high-level understanding of a fusion decision;comprehensive explanations take only the data as evidence and provide an in-depth …,Proceedings of the 22nd international conference on World Wide Web,2013,14
Chronos: Facilitating history discovery by linking temporal records,Pei Li; Christina Tziviskou; Haidong Wang; Xin Luna Dong; Xiaoguang Liu; Andrea Maurino; Divesh Srivastava,Abstract Many data sets contain temporal records over a long period of time; each record isassociated with a time stamp and describes some aspects of a real-world entity at thatparticular time. From such data; users often wish to search for entities in a particular periodand understand the history of one entity or all entities in the data set. A major challenge forenabling such search and exploration is to identify records that describe the same real-worldentity over a long period of time; however; linking temporal records is hard given that thevalues that describe an entity can evolve over time (eg.; a person can move from oneaffiliation to another). We demonstrate the Chronos system which offers users the useful toolfor finding real-world entities over time and understanding history of entities in thebibliography domain. The core of Chronos is a temporal record-linkage algorithm; which …,Proceedings of the VLDB Endowment,2012,14
Methods and apparatus to determine statistical dominance point descriptors for multidimensional data,*,Methods and apparatus to determine statistical dominance point descriptors formultidimensional data are disclosed. An example method disclosed herein comprisesdetermining a first joint dominance value for a first data point in a multidimensional data set;data points in the multidimensional data set comprising multidimensional values; eachdimension corresponding to a different measurement of a physical event; the first jointdominance value corresponding to a number of data points in the multidimensional data setdominated by the first data point in every dimension; determining a first skewness value forthe first data point; the first skewness value corresponding to a size of a first dimension of thefirst data point relative to a combined size of all dimensions of the first data point; andcombining the first joint dominance and first skewness values to determine a first …,*,2012,14
System and Method for Identifying Hierarchical Heavy Hitters in Multi-Dimensional Data,*,A method including receiving a plurality of elements of a data stream; storing a multi-dimensional data structure in a memory; said multi-dimensional data structure storing theplurality of elements as a hierarchy of nodes; each node having a frequency countcorresponding to the number of elements stored therein; comparing the frequency count ofeach node to a threshold value based on a total number of the elements stored in the nodesand identifying each node for which the frequency count is at least as great as the thresholdvalue as a hierarchical heavy hitter (HHH) node and propagating the frequency count ofeach non-HHH nodes to its corresponding parent nodes.,*,2009,14
Distribution based microdata anonymization,Nick Koudas; Divesh Srivastava; Ting Yu; Qing Zhang,Abstract Before sharing to support ad hoc aggregate analyses; microdata often need to beanonymized to protect the privacy of individuals. A variety of privacy models have beenproposed for microdata anonymization. Many of these models (eg; t-closeness) essentiallyrequire that; after anonymization; groups of sensitive attribute values follow specifieddistributions. To support such models; in this paper we study the problem of transforming agroup of sensitive attribute values to follow a certain target distribution with minimal datadistortion. Specifically; we develop and evaluate a novel methodology that combines the useof sensitive attribute permutation and generalization with the addition of fake sensitiveattribute values to achieve this transformation. We identify metrics related to accuracy ofaggregate query answers over the transformed data; and develop efficient anonymization …,Proceedings of the VLDB Endowment,2009,14
Microtubular conductometric biosensor for ethanol detection,AK Ajay; Divesh N Srivastava,Abstract A conductometric sensor using microtubules of polyaniline as transducer cumimmobilization matrix is reported; capable of detecting ethanol in liquid phase. Enzyme ADH(alcohol dehydrogenase) and its coenzyme NAD+ have been used to improve the selectivityof the sensor. The sensor concept is based on the protonation of the polyaniline by thehydrogen ion produced in the enzyme-catalyzed reaction; leading to changes in theelectrical conductance of the polyaniline. The sensor works well on the physiological pH;can detect ethanol as low as 0.02%(v/v)(0.092 M) and has a linear trend at par healthcareguidelines. The sensor responses were measured in various permutation and combinationof enzyme and coenzyme concentrations and site of immobilization. The sensor showsminor interference with other functional groups and alcohols. The possible causes for …,Biosensors and Bioelectronics,2007,14
Dense subgraph maintenance under streaming edge weight updates for real-time story identification,Albert Angel; Nick Koudas; Nikos Sarkas; Divesh Srivastava; Michael Svendsen; Srikanta Tirthapura,Abstract Recent years have witnessed an unprecedented proliferation of social media.People around the globe author; everyday; millions of blog posts; social network statusupdates; etc. This rich stream of information can be used to identify; on an ongoing basis;emerging stories; and events that capture popular attention. Stories can be identified viagroups of tightly coupled real-world entities; namely the people; locations; products; etc; thatare involved in the story. The sheer scale and rapid evolution of the data involvednecessitate highly efficient techniques for identifying important stories at every point of time.The main challenge in real-time story identification is the maintenance of dense subgraphs(corresponding to groups of tightly coupled entities) under streaming edge weight updates(resulting from a stream of user-generated content). This is the first work to study the …,The VLDB Journal,2014,13
Systems and associated computer program products that disguise partitioned data structures using transformations having targeted distributions,*,A data structure that includes at least one partition containing non-confidential quasi-identifier microdata and at least one other partition containing confidential microdata isformed. The partitioned confidential microdata is disguised by transforming the confidentialmicrodata to conform to a target distribution. The disguised confidential microdata and thequasi-identifier microdata are combined to generate a disguised data structure. Thedisguised data structure is used to carry out statistical analysis and to respond to a statisticalquery is directed to the use of confidential microdata. In this manner; the privacy of theconfidential microdata is preserved.,*,2012,13
Efficient identification of coupled entities in document collections,Nikos Sarkas; Albert Angel; Nick Koudas; Divesh Srivastava,The relentless pace at which textual data are generated on-line necessitates novelparadigms for their understanding and exploration. To this end; we introduce a methodologyfor discovering strong entity associations in all the slices (meta-data value restrictions) of adocument collection. Since related documents mention approximately the same group ofcore entities (people; locations; etc.); the groups of coupled entities discovered can be usedto expose themes in the document collection. We devise and evaluate algorithms capable ofaddressing two flavors of our core problem: algorithm THR-ENT for computing all sufficientlystrong entity associations and algorithm TOP-ENT for computing the top-k strongest entityassociations; for each slice of the document collection.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,13
Routing XML queries,*,A vast amount of information currently accessible over the Web; and in corporate networks;is stored in a variety of databases; and is being exported as XML data. However; queryingthis totality of information in a declarative and timely fashion is problematic because this setof databases is dynamic; and a common schema is difficult to maintain. The presentinvention provides a solution to the problem of issuing declarative; ad hoc XPath queriesagainst such a dynamic collection of XML databases; and receiving timely answers. There isproposed a decentralized architectures; under the open and the agreement cooperationmodels between a set of sites; for processing queries and updates to XML data. Each siteconsists of XML data nodes.(which export their data as XML; and also pose queries) andone XML router node (which manages the query and update interactions between sites) …,*,2010,13
Method for using query templates in directory caches,*,The present invention discloses the use of generalized queries; referred to as querytemplates; obtained by generalizing individual user queries; as the semantic basis for lowoverhead; high benefit directory caches for handling declarative queries. Cachingeffectiveness can be improved by maintaining a set of generalizations of queries andadmitting such generalizations into the cache when their estimated benefits are sufficientlyhigh. In a preferred embodiment of the invention; the admission of query templates into thecache can be done in what is referred to by the inventors as a “revolutionary” fashion—followed by stable periods where cache admission and replacement can be doneincrementally in an evolutionary fashion. The present invention can lead to considerablyhigher hit rates and lower server-side execution and communication costs than …,*,2009,13
Index-based approximate XML joins,Sudipto Guha; Nick Koudas; Divesh Srivastava; Ting Yu,XML data integration tools are facing a variety of challenges for their efficient and effectiveoperation. Among these is the requirement to handle a variety of inconsistencies or mistakespresent in the data sets. We study the problem of integrating XML data sources throughindex assisted join operations; using notions of approximate match in the structure andcontent of XML documents as the join predicate. We show how a well known and widelydeployed index structure; namely the R-tree; can be adopted to improve the performance ofsuch operations. We propose novel search and join algorithms for R-trees adopted to indexXML document collections. We also propose novel optimization objectives for R-treeconstruction; making R-trees better suited for this application.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,13
Finding the K highest-ranked answers in a distributed network,Demetrios Zeinalipour-Yazti; Zografoula Vagena; Vana Kalogeraki; Dimitrios Gunopulos; Vassilis J Tsotras; Michail Vlachos; Nick Koudas; Divesh Srivastava,Abstract In this paper; we present an algorithm for finding the k highest-ranked (or Top-k)answers in a distributed network. A Top-K query returns the subset of most relevant answers;in place of all answers; for two reasons:(i) to minimize the cost metric that is associated withthe retrieval of all answers; and (ii) to improve the recall and the precision of the answer-set;such that the user is not overwhelmed with irrelevant results. Our study focuses on multi-hopdistributed networks in which the data is accessible by traversing a network of nodes. Such asetting captures very well the computation framework of emerging Sensor Networks; Peer-to-Peer Networks and Vehicular Networks. We present the Threshold Join Algorithm (TJA); anefficient algorithm that utilizes a non-uniform threshold on the queried attribute in order tominimize the transfer of data when a query is executed. Additionally; TJA resolves queries …,Computer Networks,2009,12
XTreeNet: democratic community search,Emiran Curtmola; Alin Deutsch; Dionysios Logothetis; KK Ramakrishnan; Divesh Srivastava; Kenneth Yocum,Abstract We describe XTreeNet; a distributed query dissemination engine which facilitatesdemocratization of publishing and efficient data search among members of onlinecommunities with powerful full-text queries. This demonstration shows XTreeNet in fullaction. XTreeNet serves as a proof of concept for democratic community search byproposing a distributed novel infrastructure in which data resides only with the publishersowning it. Expressive user queries are disseminated to publishers. Given the virtual natureof the global data collection (eg; the union of all local data published in the community) ourinfrastructure efficiently locates the publishers that contain matching documents with aspecified query; processes the complex full-text query at the publisher and returns allrelevant documents to querier.,Proc. 34th Very Large Data Bases Conf,2008,12
Circumventing data quality problems using multiple join paths,Yannis Kotidis; Amélie Marian; Divesh Srivastava,ABSTRACT We propose the Multiple Join Path (MJP) framework for obtaining high qualityinformation by linking fields across multiple databases; when the underlying databases havepoor quality data; which are characterized by violations of integrity constraints like keys andfunctional dependencies within and across databases. MJP associates quality scores withcandidate answers by first scoring individual data paths between a pair of field values takinginto account data quality with respect to specified integrity constraints; and thenagglomerating scores across multiple data paths that serve as corroborating evidences for acandidate answer. We address the problem of finding the top-few (highest quality) answersin the MJP framework using novel techniques; and demonstrate the utility of our techniquesusing real data and our Virtual Integration Prototype testbed.,CleanDB Workshop,2006,12
On bounding-schemas for LDAP directories,Sihem Amer-Yahia; H Jagadish; Laks Lakshmanan; Divesh Srivastava,Abstract As our world gets more networked; ever increasing amounts of information arebeing stored in LDAP directories. While LDAP directories have considerable flexibility in themodeling and retrieval of information for network applications; the notion of schema theyprovide for enabling consistent and coherent representation of directory information is ratherweak. In this paper; we propose an expressive notion of bounding-schemas for LDAPdirectories; and illustrate their practical utility. Bounding-schemas are based on lower boundand upper bound specifications for the content and structure of an LDAP directory. Given abounding-schema specification; we present algorithms to efficiently determine:(i) if an LDAPdirectory is legal wrt the bounding-schema; and (ii) if directory insertions and deletionspreserve legality. Finally; we show that the notion of bounding-schemas has wider …,Advances in Database Technology—EDBT 2000,2000,12
Polymer Composites of Charge-Transfer Materials-I Phenothiazine-Iodine (2: 3) in Poly (vinylchloride),DN Srivastava; RA Singh,Polymer composites of phenothiazine-iodine (2: 3 molar ratio) charge-transi'ei complex(CTC) with poly (vinyl-chloride) have been prepared in difieren! weight r. it: os andcharacterized by the spectral; thermal and electrical methods. The mechanical and structural(optical microphotographs and XRD) properties have been studied and compared with theproperties of the charge-transfer complexes. Hlectrieal properties (both dc and ac) havebeen measured. The polymer composites exhibit semiconducting behaviour having thethermal activation energies in the range of 0.2 eV lo O. XeV. The currentvoltagecharacteristics; frequency dependence of conductivity of these composites has beendetermined. Very low percolation threshold (4% wt. CTC) has been found indicating thatthese polymer composites could be better alternative to pure chargetransfer complexes …,MOLECULAR MATERIALS,1999,12
Indexing evolving events from tweet streams,Hongyun Cai; Zi Huang; Divesh Srivastava; Qing Zhang,Tweet streams provide a variety of real-life and real-time information on social events thatdynamically change over time. Although social event detection has been actively studied;how to efficiently monitor evolving events from continuous tweet streams remains open andchallenging. One common approach for event detection from text streams is to use single-pass incremental clustering. However; this approach does not track the evolution of events;nor does it address the issue of efficient monitoring in the presence of a large number ofevents. In this paper; we capture the dynamics of events using four event operations (create;absorb; split; and merge); which can be effectively used to monitor evolving events.Moreover; we propose a novel event indexing structure; called Multi-layer Inverted List (MIL);to manage dynamic event databases for the acceleration of large-scale event search and …,IEEE Transactions on Knowledge and Data Engineering,2015,11
Conditional heavy hitters: detecting interesting correlations in data streams,Katsiaryna Mirylenka; Graham Cormode; Themis Palpanas; Divesh Srivastava,Abstract The notion of heavy hitters—items that make up a large fraction of the population—has been successfully used in a variety of applications across sensor and RFID monitoring;network data analysis; event mining; and more. Yet this notion often fails to capture thesemantics we desire when we observe data in the form of correlated pairs. Here; we areinterested in items that are conditionally frequent: when a particular item is frequent withinthe context of its parent item. In this work; we introduce and formalize the notion ofconditional heavy hitters to identify such items; with applications in network monitoring andMarkov chain modeling. We explore the relationship between conditional heavy hitters andother related notions in the literature; and show analytically and experimentally theusefulness of our approach. We introduce several algorithm variations that allow us to …,The VLDB Journal,2015,11
A dataset search engine for the research document corpus,Meiyu Lu; Srinivas Bangalore; Graham Cormode; Marios Hadjieleftheriou; Divesh Srivastava,A key step in validating a proposed idea or system is to evaluate over a suitable dataset.However; to this date there have been no useful tools for researchers to understand whichdatasets have been used for what purpose; or in what prior work. Instead; they have tomanually browse through papers to find the suitable datasets and their corresponding URLs;which is laborious and inefficient. To better aid the dataset discovery process; and provide abetter understanding of how and where datasets have been used; we propose a frameworkto effectively identify datasets within the scientific corpus. The key technical challenges areidentification of datasets; and discovery of the association between a dataset and the URLswhere they can be accessed. Based on this; we have built a user friendly web-based searchinterface for users to conveniently explore the dataset-paper relationships; and find …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,11
System and method for generating statistical descriptors for a data stream,*,Described is a system and method for receiving a data stream of multi-dimensional items;collecting a sample of the data stream having a predetermined number of items and dividingthe sample into a plurality of subsamples; each subsample corresponding to a singledimension of each of the predetermined number of items. A query is then executed on aparticular item in at least two of the subsamples to generate data for the correspondingsubsample. This data is combined into a single value.,*,2010,11
Type-based categorization of relational attributes,Babak Ahmadi; Marios Hadjieleftheriou; Thomas Seidl; Divesh Srivastava; Suresh Venkatasubramanian,Abstract In this work we concentrate on categorization of relational attributes based on theirdata type. Assuming that attribute type/characteristics are unknown or unidentifiable; weanalyze and compare a variety of type-based signatures for classifying the attributes basedon the semantic type of the data contained therein (eg; router identifiers; social securitynumbers; email addresses). The signatures can subsequently be used for other applicationsas well; like clustering and index optimization/compression. This application is useful incases where very large data collections that are generated in a distributed; ungovernedfashion end up having unknown; incomplete; inconsistent or very complex schemata andschema level meta-data. We concentrate on heuristically generating type-based attributesignatures based on both local and global computation approaches. We show …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,11
Method and apparatus for ranked join indices,*,A method and apparatus for ranked join indices includes a solution providing performanceguarantees for top-k join queries over two relations; when preprocessing to construct aranked join index for a specific join condition is permitted. The concepts of ranking joinindices presented herein are also applicable in the case of a single relation. In this case; theconcepts herein provide a solution to the top-k selection problem with monotone linearfunctions; having guaranteed worst case search performance for the case of two rankedattributes and arbitrary preference vectors.,*,2007,11
METHOD AND APPARATUS FOR USING TAG TOPOLOGY,*,*,*,2006,11
Column heterogeneity as a measure of data quality,Bing Tian Dai; Nick Koudas; Beng Chin Ooi; Divesh Srivastava; Suresh Venkatasubramanian,ABSTRACT Data quality is a serious concern in every data management application; and avariety of quality measures have been proposed; including accuracy; freshness andcompleteness; to capture the common sources of data quality degradation. We identify andfocus attention on a novel measure; column heterogeneity; that seeks to quantify the dataquality problems that can arise when merging data from different sources. We identifydesiderata that a column heterogeneity measure should intuitively satisfy; and discuss apromising direction of research to quantify database column heterogeneity based on using anovel combination of cluster entropy and soft clustering. Finally; we present a fewpreliminary experimental results; using diverse data sets of semantically different types; todemonstrate that this approach appears to provide a robust mechanism for identifying …,CleanDB Workshop,2006,11
Spider: flexible matching in databases,Nick Koudas; Amit Marathe; Divesh Srivastava,Abstract We present a prototype system; SPIDER; developed at AT&T Labs-Research; whichsupports flexible string attribute value matching in large databases. We discuss the designprinciples on which SPIDER is based; describe the basic techniques encompassed by thetool and provide a description of the demo.,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,11
Generating conditional functional dependencies,*,Techniques are disclosed for generating conditional functional dependency (CFD) patterntableaux having the desirable properties of support; confidence and parsimony. Thesetechniques include both a greedy algorithm for generating a tableau and; for large data sets;an “on-demand” algorithm that outperforms the basic greedy algorithm in running time by anorder of magnitude. In addition; a range tableau; as a generalization of a pattern tableau;can achieve even more parsimony.,*,2014,10
Online ordering of overlapping data sources,Mariam Salloum; Xin Luna Dong; Divesh Srivastava; Vassilis J Tsotras,Abstract Data integration systems offer a uniform interface for querying a large number ofautonomous and heterogeneous data sources. Ideally; answers are returned as sources arequeried and the answer list is updated as more answers arrive. Choosing a good ordering inwhich the sources are queried is critical for increasing the rate at which answers arereturned. However; this problem is challenging since we often do not have complete orprecise statistics of the sources; such as their coverage and overlap. It is further exacerbatedin the Big Data era; which is witnessing two trends in Deep-Web data: first; obtaining a fullcoverage of data in a particular domain often requires extracting data from thousands ofsources; second; there is often a big variation in overlap between different data sources. Inthis paper we present OASIS; an Online query Answering System for overlappIng …,Proceedings of the VLDB Endowment,2013,10
Dependency between sources in truth discovery,*,A method and system for truth discovery may implement a methodology that accounts foraccuracy of sources and dependency between sources. The methodology may be based onBayesian probability calculus for determining which data object values published by sourcesare likely to be true. The method may be recursive with respect to dependency; accuracy;and actual truth discovery for a plurality of sources.,*,2012,10
Large-scale copy detection,Xin Luna Dong; Divesh Srivastava,Abstract The Web has enabled the availability of a vast amount of useful information inrecent years. However; the web technologies that have enabled sources to share theirinformation have also made it easy for sources to copy from each other and often publishwithout proper attribution. Understanding the copying relationships between sources hasmany benefits; including helping data providers protect their own rights; improving variousaspects of data integration; and facilitating in-depth analysis of information flow. Theimportance of copy detection has led to a substantial amount of research in many disciplinesof Computer Science; based on the type of information considered; such as text; images;videos; software code; and structured data. This tutorial explores the similarities anddifferences between the techniques proposed for copy detection across the different types …,Proceedings of the 2011 international conference on Management of data,2011,10
Divdb: A system for diversifying query results,Marcos R Vieira; Humberto L Razente; Maria CN Barioni; Marios Hadjieleftheriou; Divesh Srivastava; Caetano Traina Jr; Vassilis J Tsotras,*,Proceedings of the VLDB Endowment,2011,10
Method and apparatus for optimizing queries under parametric aggregation constraints,*,In today's rapidly changing business landscape; corporations increasingly rely on databasesto help organize; manage and monitor every aspect of their business. Databases are deployedat the core of important business operations; including Customer Relationship Management;Supply Chain Management; and Decision Support Systems. The increasing complexity of theways in which businesses use databases creates an ongoing demand for sophisticated querycapabilities … Novel types of queries seek to enhance the way information is utilized; whileensuring that they can be easily realized in a relational database environment without the needfor significant modifications to the underlying relational engine. However; as the size of databasescontinues to grow; coupled with the desire by users to formulate complex queries; traditionalmethods of responding to queries require a tremendous amount of computational cycles …,*,2010,10
System and method for identifying hierarchical heavy hitters in a multidimensional environment,*,A method including receiving a plurality of elements of a data stream; storing a multi-dimensional data structure in a memory; said multi-dimensional data structure storing theplurality of elements as a hierarchy of nodes; each node having a frequency countcorresponding to the number of elements stored therein; comparing the frequency count ofeach node to a threshold value based on a total number of the elements stored in the nodesand identifying each node for which the frequency count is at least as great as the thresholdvalue as a hierarchical heavy hitter (HHH) node and propagating the frequency count ofeach non-HHH nodes to its corresponding parent nodes.,*,2009,10
Updates through views,*,A method and system are disclosed that allow database views and base tables to be treatedidentically with respect to queries; insertions; deletions and updates. The method andsystem include separating the data instance of a view into a logical data instance and aphysical data instance. The physical data instance is extended to include identifiers on datavalues that are used to query insert; delete and update information in base tables. Themanner in which users and applications interface with the view remains unchanged sincethose interactions occur at the logical level.,*,2008,10
Parsimonious Explanations of Change in Hierarchical Data,Dhiman Barman; Flip Korn; Divesh Srivastava; Dimitris Gunopulos; Neal Young; Deepak Agarwal,Dimension attributes in data warehouses are typically hierarchical; and a variety of OLAPapplications (such as point-of-sales analysis and decision support) call for summarizing themeasure attributes in fact tables along the hierarchies of these attributes. For example; thetotal sales at different stores can be summarized hierarchically by geographic location (eg;state/city/zip_code/store); by time (eg; year/month/day/hour); or by product category (eg;clothing/outerwear/jackets/brand). Existing OLAP tools help to summarize and navigate thedata at different levels of aggregation (eg; jackets sold in each state during December 2006)via drill-down and roll-up operators. OLAP tools are also used to characterize changes inthese hierarchical summaries over time (eg; the sales in December 2006 compared to salesin December 2005 over different locations) to detect anomalies and characterize trends …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,10
Method and apparatus for identifying hierarchical heavy hitters in a data stream,*,A method; apparatus; and computer readable medium for processing a data stream isdescribed. In one example; a set of elements of a data stream are received. The set ofelements are stored in a memory as a hierarchy of nodes. Each of the nodes includesfrequency data associated with either an element in the set of elements or a prefix of anelement in the set of elements. A set of hierarchical heavy hitters is then identified among thenodes in the hierarchy. The frequency data of each of the hierarchical heavy hitter nodes;after discounting any portion thereof attributed to a descendent hierarchical heavy hitternode in said set of hierarchical heavy hitter nodes; being greater than or equal to a fractionof the number of elements in the set of elements.,*,2005,10
Efficient handling of positional predicates within XML query processing,Zografoula Vagena; Nick Koudas; Divesh Srivastava; Vassilis Tsotras,Abstract The inherent order within the XML document-centric data model is typicallyexposed through positional predicates defined over the XPath navigation axes. Althoughprocessing algorithms for each axis have already been proposed; the incorporation ofpositional predicates in them has received very little attention. In this paper; we presenttechniques that leverage the power of existing; state of the art methods; to efficiently supportpositional predicates as well. Our preliminary experimental comparisons with alternativeapproaches reveal the performance benefits of the proposed techniques.,Database and XML Technologies,2005,10
PIX: exact and approximate phrase matching in XML,Sihem Amer-Yahia; Mary Fernández; Divesh Srivastava; Yu Xu,XML permits the interleaving of text with structural and semantic markup in documents.Markup is added to a document by tagging a portion of the text or by augmenting the textwith annotations. The example below is inspired from the XML documents published by theLibrary Of Congress (www. loc. gov). It describes legislative bills where bill sponsors aremarked up using the tag< sponsor>; and an annotation< footnote> is added to demarcateparenthetical remarks in the text.< bill bill-stage='Introduction'>< congress> 110thCONGRESS</congress>< legis-num> HR 133</legis-num>< action-desc>< sponsor> Mr.English</sponsor>< footnote> For himself and Mr. Coyne</footnote> introduced thisbill.</action-desc></bill> In the absence of markup; phrase matching is a common techniqueto search text and identify relevant documents. Phrase matching typically requires that …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,10
Approximate matching in XML,Sihem Amer-Yahia; Nick Koudas; Divesh Srivastava,Page 1. Approximate Matching in XML Sihem Amer-Yahia; Nick Koudas; Divesh Srivastava AT&TLabs–Research http://www.research.att.com/˜ sihem; koudas; divesh¡ / Approximate Matchingin XML ICDE 2003 Page 2. Outline of Seminar ¢ Motivation; language proposals: Divesh Srivastava¢ Matching query twig to data tree: Sihem Amer-Yahia ¢ Matching data tree to data tree: NickKoudas Approximate Matching in XML ICDE 2003 Page 3. What Makes XML Appealing? ¢Represent structured; semi-structured; unstructured data £ traditional databases: structure-rich £marked-up documents: text-rich ¢ Represent homogeneous; heterogeneous structure-rich data £repetition: chapter ¤ section¥ ;... optionality: book ¤ cdrom¦ ;... alternation: book ¤ (editors § authors); .. . recursion: section ¤ section¨ ;... Approximate Matching in XML ICDE 2003 Page 4. XML Example:Data Trees "MK" Editor "SA" Address Country …,Proceedings of the International Conference on Data Engineering,2003,10
Logical and physical support for heterogeneous data,Sihem Amer-Yahia; Mary Fernandez; Rick Greer; Divesh Srivastava,Abstract Heterogeneity arises naturally in virtually all real-world data. This paper presentsevolutionary extensions to a relational database system for supporting three classes of dataheterogeneity: variational; structural and annotational heterogeneities. We define theseclasses and show the impact of these new features on data storage; data-accessmechanisms; and the data-description language. Since XML is an important source ofheterogeneity; we describe how the system automatically utilizes these new features whenstoring XML documents.,Proceedings of the eleventh international conference on Information and knowledge management,2002,10
TIMBER: A native XML database,Stelios Paparizos; Shurug Al-Khalifa; Y Wu; N Wiwatwattana; HV Jagadish; Andrew Nierman; C Yu; LVS Lakshmanan; D Srivastava; A Chapman; Jignesh M Patel,This paper describes the overall design and architecture of the Timber XML databasesystem currently being implemented at the University of Michigan. The system is based upona bulk algebra for manipulating trees; and natively stores XML. New access methods havebeen developed to evaluate queries in the XML context; and new cost estimation and queryoptimization techniques have also been developed. We present performance numbers tosupport some of our design decisions. We believe that the key intellectual contribution of thissystem is a comprehensive set-at-a-time query processing ability in a native XML store; withall the standard components of relational query processing; including algebraic rewritingand a cost-based optimizer.,*,2002,10
The CORAL deductive database system,Raghu Ramakrishnan; William G Roth; Praveen Seshadri; Divesh Srivastava; S Sudarshan,CORAL[4; 5] is a deductive database system that supports a powerful declarative querylanguage. The language supports general Horn clauselogic programs; extended with SQL-style groupiug; set-generation; aud negation. Programs can be organized intoindependently optimiied modules; and users can provide optimization hints in the form ofhigh-level annotations. The system supports a wide variety of optimizw tion techniques.There is art interface to C++ that enables programs to be written in a combination ofimperative and declarative styles; C++ code can be called from declarative programs; andvice versa. A notable feature of the CORAL system is that it is extensible. In particular; newdata types can be defined; and new relation and index implementations can be added. Aninterface to the EXODUS storage manager [2] provides support for disk-resident data …,ACM SIGMOD Record,1993,10
The elephant in the room: getting value from Big Data,Serge Abiteboul; Luna Dong; Oren Etzioni; Divesh Srivastava; Gerhard Weikum; Julia Stoyanovich; Fabian M Suchanek,Big Data; and its 4 Vs–volume; velocity; variety; and veracity–have been at the forefront ofsocietal; scientific and engineering discourse. Arguably the most important 5th V; value; isnot talked about as much. How can we make sure that our data is not just big; but alsovaluable? WebDB 2015 1 has as its theme “Freshness; Correctness; Quality of Informationand Knowledge on the Web”. The workshop attracted 31 submissions; of which the best 9were selected for presentation at the workshop; and for publication in the proceedings. Toset the stage; we have interviewed several prominent members of the data managementcommunity; soliciting their opinions on how we can ensure that data is not just available inquantity; but also in quality. In this interview Serge Abiteboul; Oren Etzioni; DiveshSrivastava with Luna Dong; and Gerhard Weikum shared with us their motivation for …,Proceedings of the 18th international workshop on web and databases,2015,9
Fine-grained controversy detection in Wikipedia,Siarhei Bykau; Flip Korn; Divesh Srivastava; Yannis Velegrakis,The advent of Web 2.0 gave birth to a new kind of application where content is generatedthrough the collaborative contribution of many different users. This form of contentgeneration is believed to generate data of higher quality since the “wisdom of the crowds”makes its way into the data. However; a number of specific data quality issues appear withinsuch collaboratively generated data. Apart from normal updates; there are cases ofintentional harmful changes known as vandalism as well as naturally occurringdisagreements on topics which don't have an agreed upon viewpoint; known ascontroversies. While much work has focused on identifying vandalism; there has been littleprior work on detecting controversies; especially at a fine granularity. Knowing aboutcontroversies when processing user-generated content is essential to understand the …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,9
Join paths across multiple databases,*,Methods; systems and computer instructions on computer readable media are disclosed foroptimizing a query; including a first join path; a second join path; and an optimizer; toefficiently provide high quality information from large; multiple databases. The methods andsystems include evaluating a schema graph identifying the join paths between a field X anda field Y; and a value X= x; to identify the top-few values of Y= y that are reachable from aspecified X= x value when using the join paths. Each data path that instantiates the schemajoin paths can be scored and evaluated as to the quality of the data with respect to specifiedintegrity constraints to alleviate data quality problems. Agglomerative scoring methodologiescan be implemented to compute high quality information in the form of a top-few answers toa specified problem as requested by the query.,*,2009,9
A native system for quering XML,Stelios Paparizos; Shurug Al-Khalifa; Adriane Chapman; HV Jagadish; Laks VS Lakshmanan; Andrew Nierman; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu Timber,*,Proc. SIGMOD Conf,2003,9
Revisiting the hierarchical data model,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Much of the data we deal with every day is organized hierarchically: file systems; libraryclassification schemes and yellow page categories are salient examples. Business data too;benefits from a hierarchical organization; and indeed the hierarchical data model was quiteprevalent thirty years ago. Due to the recently increased importance of X. 500/LDAPdirectories; which are hierarchical; and the prevalence of aggregation hierarchies indatacubes; there is now renewed interest in the hierarchical organization of data. In thispaper; we develop a framework for a modern hierarchical data model; substantiallyimproved from the original version by taking advantage of the lessons learned in therelational database context. We argue that this new hierarchical data model has manybenefits with respect to the ubiquitous flat relational data model.,IEICE TRANSACTIONS on Information and Systems,1999,9
Discovering conservation rules,Lukasz Golab; Howard Karloff; Flip Korn; Barna Saha; Divesh Srivastava,Many applications process data in which there exists a “conservation law” between relatedquantities. For example; in traffic monitoring; every incoming event; such as a packet'sentering a router or a car's entering an intersection; should ideally have an immediateoutgoing counterpart. We propose a new class of constraints-Conservation Rules-thatexpress the semantics and characterize the data quality of such applications. We giveconfidence metrics that quantify how strongly a conservation rule holds and presentapproximation algorithms (with error guarantees) for the problem of discovering a concisesummary of subsets of the data that satisfy a given conservation rule. Using real data; wedemonstrate the utility of conservation rules and we show order-of-magnitude performanceimprovements of our discovery algorithms over naive approaches.,IEEE Transactions on Knowledge and Data Engineering,2014,8
Forward decay temporal data analysis,*,A disclosed method for implementing time decay in the analysis of streaming data objects isbased on the age; referred to herein as the forward age; of a data object measured from alandmark time in the past to a time associated with the occurrence of the data object; eg; anobject's timestamp. A forward time decay function is parameterized on the forward age.Because a data object's forward age does not depend on the current time; a value of theforward time decay function is determined just once for each data object. A scaling factor orweight associated with a data object may be weighted according to its decay function value.Forward time decay functions are beneficial in determining decayed aggregates; includingdecayed counts; sums; and averages; decayed minimums and maximums; and for drawingdecay-influenced samples.,*,2013,8
Finding interesting correlations with conditional heavy hitters,Katsiaryna Mirylenka; Themis Palpanas; Graham Cormode; Divesh Srivastava,The notion of heavy hitters-items that make up a large fraction of the population-has beensuccessfully used in a variety of applications across sensor and RFID monitoring; networkdata analysis; event mining; and more. Yet this notion often fails to capture the semantics wedesire when we observe data in the form of correlated pairs. Here; we are interested in itemsthat are conditionally frequent: when a particular item is frequent within the context of itsparent item. In this work; we introduce and formalize the notion of Conditional Heavy Hittersto identify such items; with applications in network monitoring; and Markov chain modeling.We introduce several streaming algorithms that allow us to find conditional heavy hittersefficiently; and provide analytical results. Different algorithms are successful for differentinput characteristics. We perform experimental evaluations to demonstrate the efficacy of …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,8
Linking temporal records,Pei Li; Xin Luna Dong; Andrea Maurino; Divesh Srivastava,Abstract Many data sets contain temporal records which span a long period of time; eachrecord is associated with a time stamp and describes some aspects of a real-world entity at aparticular time (eg; author information in DBLP). In such cases; we often wish to identifyrecords that describe the same entity over time and so be able to perform interestinglongitudinal data analysis. However; existing record linkage techniques ignore temporalinformation and fall short for temporal data. This article studies linking temporal records.First; we apply time decay to capture the effect of elapsed time on entity value evolution.Second; instead of comparing each pair of records locally; we propose clustering methodsthat consider the time order of the records and make global decisions. Experimental resultsshow that our algorithms significantly outperform traditional linkage methods on various …,Frontiers of Computer Science,2012,8
Method and system for performing queries on data streams,*,A method and system for performing a data stream query. A data stream query requiring ajoin operation on multiple data streams is approximated without performing the joinoperation. It is determined whether conditions of the query are proper to accuratelyapproximate the join operation; and if the conditions are proper the join operation isapproximated. The join operation is approximated by independently aggregating values ofthe data streams and comparing the independently aggregated values.,*,2011,8
Method and system for pattern matching having holistic twig joins,*,As is known in the art; the eXtensible Markup Language (XML) employs a tree-structured modelfor representing data. Queries in XML query languages typically specify patterns of selectionpredicates on multiple elements that have some specified tree structured relationships. Forexample; the XQuery expression: book[title='XML///author[fn=jane' AND In='doe] matches authorelements that (i) have a child subelement “fn” with content lane”; (ii) have a child subelement“In” with content “doe”; and (iii) are descendants of book elements that have a child title subelementwith content XML. This expression can be represented as a node-labeled twig (or small tree)pattern with elements and string values as node labels … Finding all occurrences of a twig patternin a database is a core operation in XML query processing; both in relational implementationsof XML databases; and in native XML databases. Known processing techniques typically …,*,2008,8
Answering order-based queries over xml data,Zografoula Vagena; Nick Koudas; Divesh Srivastava; Vassilis J Tsotras,Abstract Order-based queries over XML data include XPath navigation axes such asfollowing-sibling and following. In this paper; we present holistic algorithms that evaluatesuch order-based queries. An experimental comparison with previous approaches showsthe performance benefits of our algorithms.,Special interest tracks and posters of the 14th international conference on World Wide Web,2005,8
Architecting and implementing versatile dependability; Architecting Dependable Systems III,Tudor Dumitraş; Deepti Srivastava; Priya Narasimhan,*,*,2005,8
Space optimization in the bottom-up evaluation of logic programs,S Sudarshan; Divesh Srivastava; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of a logic program; all generated facts are usuallyassumed to be stored until the end of the evaluation. Considerable gains can be achievedby instead discarding facts that are no longer required: the space needed to evaluate theprogram is reduced; 1/0 costs may be reduced; and the costs of maintaining and accessingindices; eliminating duplicates etc. are reduced. Thus; discarding facts early could achievetime as well as space improvements. Given an evaluation method that is sound; completeand does not repeat derivation steps; we consider how facts can be discarded during theevaluation without compromising these properties. Our first contribution is to show that sucha space optimization technique has three distinct components. Informally; we must make allderivations that we can with each fact; detect all duplicate derivations of facts and try to …,ACM SIGMOD Record,1991,8
Systems and methods for distributing video on demand,*,A method of receiving content includes joining an in-progress multicast stream to receive afirst portion of a content. The method further includes sending a request to a peer for a catch-up portion of the content; the request including a deadline for delivery of the content; andreceiving the catch-up portion of the content from the peer prior to the deadline.,*,2016,7
Generating minimality-attack-resistant data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for generating data and data sets that are resistant to minimality attacks. Data setshaving a number of tuples are received; and the tuples are ordered according to an aspectof the tuples. The tuples can be split into groups of tuples; and each of the groups may beanalyzed to determine if the group complies with a privacy requirement. Groups that satisfythe privacy requirement may be output as new data sets that are resistant to minimalityattacks.,*,2014,7
Methods and apparatus to anonymize a dataset of spatial data,*,Methods and apparatus are disclosed to anonymize a dataset of spatial data. An examplemethod includes generating a spatial indexing structure with spatial data; establishing aheight value associated with the spatial indexing structure to generate a plurality of treenodes; each of the plurality of tree nodes associated with spatial data counts; calculating alocalized noise budget value for respective ones of the tree nodes based on the height valueand an overall noise budget; and anonymizing the plurality of tree nodes with aanonymization process; the anonymization process using the localized noise budget valuefor respective ones of the tree nodes.,*,2014,7
Aggregate query answering on possibilistic data with cardinality constraints,Graham Cormode; Divesh Srivastava; Entong Shen; Ting Yu,Uncertainties in data can arise for a number of reasons: when data is incomplete; containsconflicting information or has been deliberately perturbed or coarsened to remove sensitivedetails. An important case which arises in many real applications is when the data describesa set of possibilities; but with cardinality constraints. These constraints represent correlationsbetween tuples encoding; eg that at most two possible records are correct; or that there is an(unknown) one-to-one mapping between a set of tuples and attribute values. Although therehas been much effort to handle uncertain data; current systems are not equipped to handlesuch correlations; beyond simple mutual exclusion and co-existence constraints. Vitally; theyhave little support for efficiently handling aggregate queries on such data. In this paper; weaim to address some of these deficiencies; by introducing LICM (Linear Integer Constraint …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,7
System and method for generating statistical descriptors for a data stream,*,Described is a system and method for receiving a data stream of multi-dimensional items;collecting a sample of the data stream having a predetermined number of items and dividingthe sample into a plurality of subsamples; each subsample corresponding to a singledimension of each of the predetermined number of items. A query is then executed on aparticular item in at least two of the subsamples to generate data for the correspondingsubsample. This data is combined into a single value.,*,2012,7
We challenge you to certify your updates,Su Chen; Xin Luna Dong; Laks VS Lakshmanan; Divesh Srivastava,Abstract Correctness of data residing in a database is vital. While integrity constraintenforcement can often ensure data consistency; it is inadequate to protect against updatesthat involve careless; unintentional errors; eg; whether a specified update to an employee'srecord was for the intended employee. We propose a novel approach that is complementaryto existing integrity enforcement techniques; to guard against such erroneous updates. Ourapproach is based on (a) updaters providing an update certificate with each databaseupdate; and (b) the database system verifying the correctness of the update certificateprovided before performing the update. We formalize a certificate as a (challenge; response)pair; and characterize good certificates as those that are easy for updaters to provide and;when correct; give the system enough confidence that the update was indeed intended …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,7
Set similarity selection queries at interactive speeds,*,The similarity between a query set comprising query set tokens and a database setcomprising database set tokens is determined by a similarity score. The database setsbelong to a data collection set; which contains all database sets from which information maybe retrieved. If the similarity score is greater than or equal to a user-defined threshold; thedatabase set has information relevant to the query set. The similarity score is calculated withan inverse document frequency method (IDF) similarity measure independent of termfrequency. The document frequency is based at least in part on the number of database setsin the data collection set and the number of database sets which contain at least one queryset token. The length of the query set and the length of the database set are normalized.,*,2011,7
System and method for managing data streams,*,A system for a data stream management system includes a filter transport aggregate for ahigh speed input data stream with a plurality of packets each packet comprising attributes.The system includes an evaluation system to evaluate the high speed input data stream andpartitions the packets into groups the attributes and a table; wherein the table stores theattributes of each packets using a hash function. A phantom query is used to definepartitioned groups of packets using attributes other than those used to group the packets forsolving user queries without performing the user queries on the high speed input datastream.,*,2009,7
Method and systems for content access and distribution,*,Distribution of content between publishers and consumers is accomplished using an overlaynetwork that may make use of XML language to facilitate content identification. The overlaynetwork includes a plurality of routers that may be in communication with each other and thepublishers and consumers on the Internet. Content and queries are identified by contentdescriptors that are routed from the originator to a nearest router in the overlay network. Thenearest router; for each unique content descriptor; generates a hash identification of thecontent descriptor which is used by remaining routers in the overlay network to provide theappropriate functions with respect to the content descriptor. In particular; this allows allrouters in the overlay network except the nearest router to properly route content withoutprocessing every content descriptor.,*,2009,7
Efficient table anonymization for aggregate query answering,Cecilia M Procopiuc; Divesh Srivastava,Privacy protection is a major concern when microdata is released for ad hoc analyses.Anonymization schemes have to guarantee privacy goals; as well as preserve sufficientinformation to support reasonably accurate answers to ad hoc queries. In this paper; wefocus on the case when the sensitive attributes are numerical (eg; salary) for which $(k; e) $-anonymity was shown to be an appropriate privacy goal. We develop efficient algorithms fortwo optimization criteria for $(k; e) $-anonymity schemes; significantly improving on previousresults. We evaluate our methods on a large real dataset; and show that they are scalableand accurate.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,7
RouteBazaar: An Economic Framework for Flexible Routing,Holly Esquivel; Chitra Muthukrishnan; Feng Niu; Shuchi Chawla; Aditya Akella,ABSTRACT The Internet's routing protocol provides users a single endto-end route that isnot guaranteed to be available or to meet user requirements. Our paper addresses thisrigidity using an economically grounded approach that appeals both to users and to serviceproviders. We propose a framework called RouteBazaar in which service providersannounce links connecting different parts of the Internet; along with dynamically changingprices associated with using the links. All topology information is exposed to end-users.Users simply use routes that best match their cost-performance requirements. The appeal tousers is that they can always obtain a route of their choice as long as they are willing to payfor it. The appeal to providers is that our framework offers them means for monetary gains aslong as they are willing to offer greater visibility into their topology and greater flexibility in …,Technical Report TR1654; Department of Computer Sciences,2009,7
Method and system for pattern matching having holistic twig joins,*,Book[tittle='XML']//author[fn='jane' AND in='doe'] Matches author elements that (i) have a childsubelement “fn” with content “jane”; (ii) have a child subelement “In” with content “doe”; and(iii) are descendants of book elements that have a child title subelement with content XML. Thisexpression can be represented as a node-labeled twig (or small tree) pattern with elements andstring values as node labels … Finding all occurrences of a twig pattern in a database is a coreoperation in XML query processing; both in relational implementations of XML databases; andin native XML databases. Known processing techniques typically decompose the twig patterninto a set of binary (parent-child and ancestor-descendant) relationships between pairs ofnodes; eg; the parent-child relationships (book; title) and (author; fn); and the ancestor-descendantrelationship (book; author). The query twig pattern can then be matched by (i) matching …,*,2009,7
Apparatus and method for merging results of approximate matching operations,*,A device and a method are provided. Approximate match operations are performed for eachof a group of attributes for each of a group of tuples with respect to a query to create arespective ranking for each of the group of attributes. The rankings of the group of attributesare combined to provide a ranking score for each of the group of tuples. Data representing aranking score of each of the group of tuples is generated according to a position of arespective ranking of each one of the group of tuples for a first k positions of the ranking. K oftop ranked ones of the group of tuples are identified based at least in part on the generateddata; wherein a number of the group of tuples is n and k< n.,*,2008,7
Method and apparatus for optimizing queries under parametric aggregation constraints,*,In today's rapidly changing business landscape; corporations increasingly rely on databasesto help organize; manage and monitor every aspect of their business. Databases are deployedat the core of important business operations; including Customer Relationship Management;Supply Chain Management; and Decision Support Systems. The increasing complexity of theways in which businesses use databases creates an ongoing demand for sophisticated querycapabilities … Novel types of queries seek to enhance the way information is utilized; whileensuring that they can be easily realized in a relational database environment without the needfor significant modifications to the underlying relational engine. However; as the size of databasescontinues to grow; coupled with the desire by users to formulate complex queries; traditionalmethods of responding to queries require a tremendous amount of computational cycles …,*,2008,7
MMS: Using Queries As Data Values for Metadata Management,Divesh Srivastava; Yannis Velegrakis,We demonstrate MMS; a system for storing and managing a variety of metadata in a simple;elegant and uniform way. The system is based on two observations. First; that the relationalmodel augmented with queries as data values is a natural way to uniformly model data;arbitrary metadata and their association. Second; that relational queries with a joinmechanism augmented to permit matching of query result relations; instead of only atomicvalues; is an elegant way to uniformly query across data and metadata.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,7
Propagating updates in SPIDER,Nick Koudas; Amit Marathe; Divesh Srivastava,SPIDER; developed at AT&T Labs-Research; is a system that efficiently supports flexiblestring matching against attribute values in large databases; and is extensively used in AT&T.The scoring methodology is based on tf. idf weighting and cosine similarity; and SPIDERmaintains indexes containing string tokens and their weights; for fast matching at query time.Given the" global" nature of the weights maintained in the indexes; even a few updates tothe underlying database tables would necessitate a (near-complete recomputation of theindexes; which can be prohibitively expensive. In this paper; we explore novel techniques toconsiderably reduce the cost of propagating updates in SPIDER; without a significantdegradation of answer accuracy or query performance. We present experimental evidenceusing real data sets to demonstrate the practical benefits of our techniques.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,7
Making designer schemas with colors,Nuwee Wiwatwattana; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,XML schema design has two opposing goals: elimination of update anomalies requires thatthe schema be as normalized as possible; yet higher query performance and simpler queryexpression are often obtained through the use of schemas that permit redundancy. In thispaper; we show that the recently proposed MCT data model; which extends XML by addingcolors; can be used to address this dichotomy effectively. Specifically; we formalize theintuition of anomaly avoidance in MCT using notions of node normal and edge normalforms; and the goal of efficient query processing using notions of association recoverabilityand direct recoverability. We develop algorithms for transforming design specifications givenas ER diagrams into MCT schemas that are in a node or edge normal form and satisfyassociation or direct recoverability. Experimental results using a wide variety of ER …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,7
Counting relaxed twig matches in a tree,Dongwon Lee; Divesh Srivastava,Abstract We consider the problem of accurately estimating the number of approximate XMLanswers for a given query; and propose an efficient method that (1) accurately computesselectivity estimates for each relaxed XML query; using a natural generalization of thecorrelated subpath tree (CST) summary structure; and (2) carefully combines theseestimates by analyzing the nature of overlap between the different relaxed twig queries.,Database Systems for Advanced Applications,2004,7
Molecularly dispersed polymer composites of charge-transfer materials: morphology and physical properties,DN Srivastava; RA Singh,Abstract The dispersed type of composites of two conducting charge-transfer materials{phenothiazine–iodine (in 2: 3 molar ratio) and benzidine–iodine (in 1: 1 molar ratio)} ininsulating polymer; polystyrene have been prepared and characterized by FT-IR; XRD andfor their microstructure; electrical properties (dc as well ac) and mechanical properties. Acorrelation between the morphology and physical properties has been made on the basis ofdata obtained. The method leads to an alternative route for the preparation of processableconducting polymers with improved mechanical properties in which the conductivities couldbe controlled by the loading of insulating polymers with conducting charge-transfermaterials.,Synthetic metals,2000,7
Space optimization in deductive databases,Divesh Srivastava; S Sudarshan; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of logic programs and recursively defined views ondatabases; all generated facts are usually assumed to be stored until the end of theevaluation. Discarding facts during the evaluation; however; can considerably improve theefficiency of the evaluation: the space needed to evaluate the program; the I/O costs; thecosts of maintaining and accessing indices; and the cost of eliminating duplicates may all bereduced. Given an evaluation method that is sound; complete; and does not repeatderivation steps; we consider how facts can be discarded during the evaluation withoutcompromising these properties. We show that every such space optimization method hascertain components; the first to ensure soundness and completeness; the second to avoidredundancy (ie; repetition of derivations); and the third to reduce “fact lifetimes”(ie; the …,ACM Transactions on Database Systems (TODS),1995,7
The CORAL User Manual,Raghu Ramakrishnan; Praveen Seshadri; Divesh Srivastava; S Sudarshan,*,University of Wisconsin; Madison,1993,7
Size-constrained weighted set cover,Lukasz Golab; Flip Korn; Feng Li; Barna Saha; Divesh Srivastava,In this paper; we introduce a natural generalization of Weighted Set Cover and MaximumCoverage; called Size-Constrained Weighted Set Cover. The input is a collection of nelements; a collection of weighted sets over the elements; a size constraint k; and aminimum coverage fraction ŝ; the output is a sub-collection of up to k sets whose unioncontains at least ŝn elements and whose sum of weights is minimal. We prove thehardness of approximation of this problem; and we present efficient approximationalgorithms with provable quality guarantees that are the best possible. In many applications;the elements are data records; and the set collection to choose from is derived fromcombinations (patterns) of attribute values. We provide optimization techniques for thisspecial case. Finally; we experimentally demonstrate the effectiveness and efficiency of …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,6
Anonymization of data over multiple temporal releases,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for anonymizing data over multiple temporal releases. Data is received; and nodesand connections in the data are identified. The data also is analyzed to identify predictedconnections. The nodes; the connections; and the predicted connections are analyzed todetermine how to group the nodes in the data. The data is published; and the grouping ofthe nodes is extended to subsequent temporal releases of the data; the nodes of which aregrouped in accordance with the grouping used with the data.,*,2014,6
Join paths across multiple databases,*,Methods; systems and computer instructions on computer readable media are disclosed foroptimizing a query; including a first join path; a second join path; and an optimizer; toefficiently provide high quality information from large; multiple databases. The methods andsystems include evaluating a schema graph identifying the join paths between a field X anda field Y; and a value X= x; to identify the top-few values of Y= y that are reachable from aspecified X= x value when using the join paths. Each data path that instantiates the schemajoin paths can be scored and evaluated as to the quality of the data with respect to specifiedintegrity constraints to alleviate data quality problems. Agglomerative scoring methodologiescan be implemented to compute high quality information in the form of a top-few answers toa specified problem as requested by the query.,*,2012,6
Method of pattern searching,*,The present application is a continuation of US patent application Ser. No. 10/748;832 filedDec. 30; 2003; now US Pat. No. 7;451;144 now pending; which claims the benefit of US ProvisionalPatent Application No. 60/450;222; filed on Feb. 25; 2003; where each of the above cited applicationsis incorporated herein by reference … The government may have certain rights in the inventionpursuant to a National Science Foundation grant under Grant Numbers IIS-9986030 andIIS-0208852 … The present invention relates generally to processing queries in a computersystem and; more particularly; to processing computer queries using pattern matching … Asis known in the art; the eXtensible Markup Language (XML) employs a tree-structured modelfor representing data. Queries in XML query languages typically specify patterns of selectionpredicates on multiple elements that have some specified tree structured relationships …,*,2012,6
Bistro data feed management system,Vladislav Shkapenyuk; Theodore Johnson; Divesh Srivastava,Abstract Data feed management is a critical component of many data intensive applicationsthat depend on reliable data delivery to support real-time data collection; correlation andanalysis. Data is typically collected from a wide variety of sources and organizations; using arange of mechanisms-some data are streamed in real time; while other data are obtained atregular intervals or collected in an ad hoc fashion. Individual applications are forced to makeseparate arrangements with feed providers; learn the structure of incoming files; monitordata quality; and trigger any processing necessary. The Bistro data feed manager; designedand implemented at AT&T Labs-Research; simplifies and automates this complex task ofdata feed management: efficiently handling incoming raw files; identifying data feeds anddistributing them to remote subscribers. Bistro supports a flexible specification language …,Proceedings of the 2011 international conference on Management of data,2011,6
Meta-data indexing for XPath location steps,*,Techniques are disclosed that efficiently support the querying of meta-data in XMLdocuments. The techniques include efficiently identifying XML elements along each locationstep in an XPath query that satisfy range constraints on ordered meta-data. The techniquesinclude generating an inheritance meta-data index in which actual meta-data levels areassociated only with elements for which a value is explicitly specified and associating non-leaf nodes of the index structure with inherited meta-data levels and inheritance sourcenodes. The techniques may be used with navigation-based and join-based XPathevaluation strategies.,*,2010,6
Schema Extraction,Divesh Srivastava; Chin Ooi; Cecilia M Procopiuc; Xiaoyan Yang; Meihui Zhang,–The column names of foreign/primary keys should be similar–A foreign key should havesignificant cardinality–A foreign key should have good coverage of the primary key–Theprimary key should have only a small percentage of values outside the range of the foreignkey,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,6
Load-balanced query dissemination in privacy-aware online communities,Emiran Curtmola; Alin Deutsch; KK Ramakrishnan; Divesh Srivastava,Abstract We propose a novel privacy-preserving distributed infrastructure in which dataresides only with the publishers owning it. The infrastructure disseminates user queries topublishers; who answer them at their own discretion. The infrastructure enforces a publisherk-anonymity guarantee; which prevents leakage of information about which publishers arecapable of answering a certain query. Given the virtual nature of the global data collection;we study the challenging problem of efficiently locating publishers in the community thatcontain data items matching a specified query. We propose a distributed index structure;UQDT; that is organized as a union of Query Dissemination Trees (QDTs); and realized onan overlay (ie; logical) network infrastructure. Each QDT has data publishers as its leafnodes; and overlay network nodes as its internal nodes; each internal node routes …,Proceedings of the 2010 international conference on Management of data,2010,6
Using queries to associate metadata with data,Divesh Srivastava; Yannis Velegrakis,As relational databases proliferate and become increasingly complex; both in their internalstructure and in their interactions with other databases and applications; there is a growingneed to associate a variety of metadata with the underlying data. Even though the need hasbeen apparent; a simple; elegant approach to uniformly model and query both data andmetadata has been elusive. In this paper; we argue that the relational model augmentedwith queries as data values is a natural way to uniformly model data; arbitrary metadata andtheir association.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,6
Meta-Data indexing for XPath location steps,SungRan Cho; Nick Koudas; Divesh Srivastava,Abstract XML is the de facto standard for data representation and exchange over the Web.Given the diversity of the information available in XML; it is very useful to annotate XML datawith a wide variety of meta-data; such as quality and sensitivity. When querying such XMLdata; say using XPath; it is important to efficiently identify the data that meet specifiedconstraints on the meta-data. For example; different users may be satisfied with differentlevels of quality guarantees; or may only have access to different parts of the XML databased on specified security policies. In this paper; we address the problem of efficientlyidentifying the XML elements along a location step in an XPath query; that satisfy meta-datarange constraints; when the meta-data levels are specifically drawn from an ordered domain(eg; accuracy in [0; 1]; recency using timestamps; multi-level security; etc.). More …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,6
PIX: a system for phrase matching in XML documents,Sihem Amer-Yahia; MF Fernández; D Srivastava; Y Xu,*,*,2003,6
Souvenir purchase patterns of domestic tourists: Case study of Takayama city; Japan,M Nomura,*,Menomonie; Wisconsin: University of Wisconsin-Stout,2002,6
Constraint Databases and Applications Second International Workshop on Constraint Database Systems; CDB'97 Delphi; Greece; January 11–12; 1997 CP'96 Work...,Volker Gaede; Alexander Brodsky; Oliver Günther; Divesh Srivastava; Victor Vianu; Mark Wallace,The 18 revised full papers presented were carefully reviewed and selected for inclusion inthe volume; also included are the extended abstract of an invited contribution and twosurvey papers. The papers are organized in sections on languages; expressiveness ofspatial languages; systems; temporal applications; new applications; indexing; constraintprogramming; and optimization.,Conference proceedings CDB,1997,6
Effective and complete discovery of order dependencies via set-based axiomatization,Jaroslaw Szlichta; Parke Godfrey; Lukasz Golab; Mehdi Kargar; Divesh Srivastava,Abstract Integrity constraints (ICs) are useful for query optimization and for expressing andenforcing application semantics. However; formulating constraints manually requiresdomain expertise; is prone to human errors; and may be excessively time consuming;especially on large datasets. Hence; proposals for automatic discovery have been made forsome classes of ICs; such as functional dependencies (FDs); and recently; orderdependencies (ODs). ODs properly subsume FDs; as they can additionally expressbusiness rules involving order; eg; an employee never has a higher salary while payinglower taxes than another employee. We present a new OD discovery algorithm enabled by anovel polynomial mapping to a canonical form of ODs; and a sound and complete set ofaxioms (inference rules) for canonical ODs. Our algorithm has exponential worst-case …,Proceedings of the VLDB Endowment,2017,5
Empirical glitch explanations,Tamraparni Dasu; Ji Meng Loh; Divesh Srivastava,Abstract Data glitches are unusual observations that do not conform to data qualityexpectations; be they logical; semantic or statistical. By applying data integrity constraints;potentially large sections of data could be flagged as being noncompliant. Ignoring orrepairing significant sections of the data could fundamentally bias the results andconclusions drawn from analyses. In the context of Big Data where large numbers andvolumes of feeds from disparate sources are integrated; it is likely that significant portions ofseemingly noncompliant data are actually legitimate usable data. In this paper; we introducethe notion of Empirical Glitch Explanations-concise; multi-dimensional descriptions ofsubsets of potentially dirty data-and propose a scalable method for empirically generatingsuch explanatory characterizations. The explanations could serve two valuable functions …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,5
Methods and systems to store and forward multicast traffic,*,Methods and systems are described to store and forward multicast traffic. The methodincludes receiving a request to add a first node to a membership tree including a firstplurality of nodes associated with a multicast group; identifying a second node in the firstplurality of nodes; communicating a node identifier that identifies the first node over anetwork to the second node where the node identifier is stored at the second node to addthe first node to the membership tree where the node identifier is stored in the membershiptree to enable the second node to forward multicast traffic to the first node; and where thefirst and second nodes are separated from each other by at least one other node of the firstplurality of node. The first node can be associated with a multicast member that has beenadded to the multicast group. Other embodiments are disclosed.,*,2014,5
Method and apparatus for propagating updates in databases,*,A method and apparatus for propagating updates in databases are disclosed. For example;the present method uses “blocking” and/or “thresholding” to delay update propagationand/or to limit the propagation of updates to an optimal stage. For example; the presentmethod receives at least one database update and extracts at least one token from the atleast one database update. The method then determines whether a threshold forpropagating the at least one database update for the at least one token is reached. Themethod then propagates the at least one database update for updating an index structure ofa database pertaining to the at least one token whose threshold has been reached.,*,2014,5
Accurate and efficient private release of datacubes and contingency tables,Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Grigory Yaroslavtsev,Abstract: A central problem in releasing aggregate information about sensitive data is to doso accurately while providing a privacy guarantee on the output. Recent work focuses on theclass of linear queries; which include basic counting queries; data cubes; and contingencytables. The goal is to maximize the utility of their output; while giving a rigorous privacyguarantee. Most results follow a common template: pick a" strategy" set of linear queries toapply to the data; then use the noisy answers to these queries to reconstruct the queries ofinterest. This entails either picking a strategy set that is hoped to be good for the queries; orperforming a costly search over the space of all possible strategies. In this paper; wepropose a new approach that balances accuracy and efficiency: we show how to improvethe accuracy of a given query set by answering some strategy queries more accurately …,arXiv preprint arXiv:1207.6096,2012,5
Method and apparatus for associating metadata with data,*,Method and apparatus for associating at least one query expression to an original databasetable is described. In one example; a metadata table is added to a database; wherein atleast one portion of the metadata table comprises the at least one query expression.Afterwards; the at least one query expression is associated to at least one value from at leastone tuple belonging to a data table of the database.,*,2012,5
System and method for providing structure and content scoring for XML,*,A system; method and computer readable medium are disclosed. The method embodimentrelates to a method of computing score of candidate answers to a database query. Themethod comprises receiving a database query; assigning a first score to a match to thequery; the first score being associated with a relative importance of an individual keyword ina collection of documents based on all structural and content predicates in the query;assigning a second score to the match; the second score being associated with a relativeimportance of a keyword in an individual document and using the assigned first score andsecond score to compute an answer score for the query.,*,2011,5
Method and apparatus for optimizing queries under parametric aggregation constraints,*,This application is a continuation of US patent application Ser. No. 10/828;839 filed Apr. 21;2004; now US Pat. No. 7;668;801 which is currently allowed and claims the benefit of US provisionalpatent application Ser. No. 60/464;256; filed Apr. 21; 2003; where all of the above cited applicationsare herein incorporated in their entity by reference … Embodiments of the present inventiongenerally relate to a method for optimizing queries. More specifically; the present invention disclosesan efficient method for providing answers to queries under parametric aggregationconstraints … In today's rapidly changing business landscape; corporations increasingly relyon databases to help organize; manage and monitor every aspect of their business. Databasesare deployed at the core of important business operations; including Customer RelationshipManagement; Supply Chain Management; and Decision Support Systems. The …,*,2011,5
Psalm: Cardinality estimation in the presence of fine-grained access controls,Huaxin Zhang; Ihab F Ilyas; Kenneth Salem,In database systems that support fine-grained access controls; each user has access rightsthat determine which tuples are accessible and which are inaccessible. Queries areanswered as if the inaccessible tuples are not present in the database. Thus; users withdifferent access rights may get different answers to a given query. To process queriesefficiently in the presence of fine-grained access controls; the database system needsaccurate estimates of the number of tuples that are both accessible according to the accessrights of the submitting user and relevant according to the selection predicates in the query.In this paper; we present PSALM; a sampling-based cardinality estimation technique for usein the presence of fine-grained access controls. Our technique exploits the fact that accessrights are relatively static and are common to all queries that are evaluated on behalf of a …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,5
Method and apparatus for finding biased quantiles in data streams,*,A method and apparatus for computing biased or targeted quantiles are disclosed. Forexample; the present invention reads a plurality of items from a data stream and inserts eachof the plurality of items that was read from the data stream into a data structure. Periodically;the data structure is compressed to reduce the number of stored items in the data structure.In turn; the compressed data structure can be used to output a biased or targeted quantile.,*,2006,5
LockX: a system for efficiently querying secure XML,SungRan Cho; Sihem Amer-Yahia; Laks VS Lakshmanan; Divesh Srivastava,Much of the work on XML access control to date has studied models for the specification ofXML access control policies; focusing on issues such as granularity of access and conflictresolution. However; there has been little work on enforcement of access control policies forqueries. A naive two-step solution to secure query evaluation is to first compute queryresults; and then use access control policies to filter the results. Consider the XML databaseof an online-seller; which has information on books and customers. Assume that a specificuser is allowed access to books and not to customer information. If only query results arefiltered for accessibility; the XPath query:,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,5
Challenges for global information systems,Alon Y Levy; Avi Silberschatz; Divesh Srivastava; Maria Zemankova,Currently; the Internet provides access to a very large number and wide variety ofinformation sources (eg; textual databases; sites containing technical reports; directorylistings); and systems to access these sources (eg; World Wide Web; Gopher; WAIS). Thechallenge is to provide easy; efficient; robust and secure access to this information and otherkinds (eg; relational and object oriented databases). This aim of this panel is to explorewhether there are any new technical problems; relevant to the Database field; that need tobe solved in order to realize such global information systems. In particular; we debatewhether existing techniques from database systems (eg; multidatabases and distributeddatabases) can be applied or straigtitforwardly extended to global information systems.Furthermore; we attempt to establish realistic goals for database technologies in global …,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,1994,5
An overview of coral,Raghu Ramakrishnan; Praveen Seshadri; Divesh Srivastava; S Sudarshan,*,Manuscript (full version of RSS92b]; which appeared in VLDB92),1993,5
Knowledge curation and knowledge fusion: challenges; models and applications,Xin Luna Dong; Divesh Srivastava,Abstract Large-scale knowledge repositories are becoming increasingly important as afoundation for enabling a wide variety of complex applications. In turn; building high-qualityknowledge repositories critically depends on the technologies of knowledge curation andknowledge fusion; which share many similar goals with data integration; while facing evenmore challenges in extracting knowledge from both structured and unstructured data; acrossa large variety of domains; and in multiple languages. Our tutorial highlights the similaritiesand differences between knowledge management and data integration; and has two goals.First; we introduce the Database community to the techniques proposed for the problems ofentity linkage and relation extraction by the Knowledge Management; Natural LanguageProcessing; and Machine Learning communities. Second; we give a detailed survey of …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,4
Data management challenges and opportunities in cloud computing,Kyuseok Shim; Sang Kyun Cha; Lei Chen; Wook-Shin Han; Divesh Srivastava; Katsumi Tanaka; Hwanjo Yu; Xiaofang Zhou,Abstract Analyzing large data is a challenging problem today; as there is an increasing trendof applications being expected to deal with vast amounts of data that usually do not fit in themain memory of a single machine. For such data-intensive applications; database researchcommunity has started to investigate cloud computing as a cost effective option to buildscalable parallel data management systems which are capable of serving petabytes of datafor millions of users. The goal of this panel is to initiate an open discussion within thecommunity on data management challenges and opportunities in cloud computing. Potentialtopics to be discussed in the panel include: MapReduce framework; shared-nothingarchitecture; parallel query processing; security; analytical data management; transactionaldata management and fault tolerance.,Proceedings of the 17th international conference on Database Systems for Advanced Applications-Volume Part II,2012,4
Method and apparatus for packet analysis in a network,*,A method and system for extracting useful statistics and information and removing aprocessing module based on the information to enhance a run-time system on a networkinterface card is disclosed. The run-time system module feeds information derived from anetwork packet to processing modules which process the information and generate outputsuch as condensed statistics about the packets traveling through the network. The run-timesystem can be enhanced to included facilities for removing processing modules withoutreplacing the run-time system module.,*,2012,4
CPM-adaptive VoD with cooperative peer assist and multicast,KK Ramakrishnan; Vijay Gopalakrishnan; Rittwik Jana; Divesh Srivastava; Bobby Bhattacharjee,att_abstract={{We present CPM; a unified approach that exploits server multicast; assistedby peer downloads; to provide efficient video-on-demand (VoD) in a service providerenvironment. We describe our architecture and show how CPM is designed to dynamicallyadapt to a wide range of situations including highly different peer-upload bandwidths;content popularity; user request arrival patterns (including flash-crowds); video library size;and subscriber population. We demonstrate the effectiveness of CPM using simulations(based on the an actual implementation codebase) across the range of situations describedabove and show that CPM does significantly better than traditional unicast; different forms ofmulticast; as well as peer-to-peer schemes. Along with synthetic parameters; we augmentour experiments using data from a deployed VoD service to evaluate the performance of …,Keynote Speech at IEEE LANMAN,2008,4
Summarizing two-dimensional data with skyline-based statistical descriptors,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Much real data consists of more than one dimension; such as financial transactions(eg; price× volume) and IP network flows (eg; duration× numBytes); and capturerelationships between the variables. For a single dimension; quantiles are intuitive androbust descriptors. Processing and analyzing such data; particularly in data warehouse ordata streaming settings; requires similarly robust and informative statistical descriptors thatgo beyond one-dimension. Applying quantile methods to summarize a multidimensionaldistribution along only singleton attributes ignores the rich dependence amongst thevariables. In this paper; we present new skyline-based statistical descriptors for capturingthe distributions over pairs of dimensions. They generalize the notion of quantiles in theindividual dimensions; and also incorporate properties of the joint distribution. We …,Scientific and Statistical Database Management,2008,4
Fast identification of relational constraint violations,Amit Chandel; Nick Koudas; Ken Q Pu; Divesh Srivastava,Logical constraints;(eg;phone numbers in Toronto can have prefixes 416; 647; 905 only');are ubiquitous in relational databases. Traditional integrity constraints; such as functionaldependencies; are examples of such logical constraints as well. However; under frequentdatabase updates; schema evolution and transformations; they can be easily violated. As aresult; tables become inconsistent and data quality is degraded. In this paper we study theproblem of validating collections of user defined constraints on a number of relational tables.Our primary goal is to quickly identify which tables violate such constraints. Logicalconstraints are potentially complex logical formuli; and we demonstrate that they cannot beefficiently evaluated by SQL queries. In order to enable fast identification of constraintviolations; we propose to build and maintain specialized logical indices on the relational …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,4
Reasoning about approximate match query results,Sudipto Guha; Nick Koudas; Divesh Srivastava; Xiaohui Yu,Join techniques deploying approximate match predicates are fundamental data cleaningoperations. A variety of predicates have been utilized to quantify approximate match in suchoperations and some have been embedded in a declarative data cleaning framework.These techniques return pairs of tuples from both relations; tagged with a score; signifyingthe degree of similarity between the tuples in the pair according to the specific approximatematch predicate. In this paper; we consider the problem of estimating various parameters onthe output of declarative approximate join algorithms for planning purposes. Such algorithmsare highly time consuming; so precise knowledge of the result size as well as its scoredistribution is a pressing concern. This knowledge aids decisions as to which operations aremore promising for identifying highly similar tuples; which is a key operation for data …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,4
Index Structures for Matching XML Twigs using Relational Query Processor,Zhuyan Chan; Johannes Gehrke; Flip Korn; Nick Koudas; Jayavel Shanmugasundram; Divesh Srivastava,*,Proceeding of 21 st International Conference on Data Engineering Workshop ICDEW; Tokyo-Japan,2005,4
Directories: Managing Data for Networked Applications,Divesh Srivastava,*,ICDE; Los Alamos; California; USA,2000,4
Possibility of fractal growth in polymer composites of charge-transfer materials,DN Srivastava; RA Singh,Abstract The possibility of fractal growth in polymer composites of charge-transfer materialsby analysing the optical image has been reported. Charge-transfer complexes usually growin reticulate (net-like) or in a channel form in nonconducting polymers. We report; for the firsttime; an unusual growth in polymer composite of charge-transfer materials (phenothiazine—TCNQ) in poly (vinyl acetate). The surface coverage is found to be about 70%. The doublelogarithmic plot of N (R); number of boxes needed to cover the object completely; and R;reduction factor of each box to the initial object; has been plotted and the fractal dimensionshave been found to be 1.8 and 1.4 in two different regions.,Synthetic metals,1998,4
Michael Tan. Data Caching and Replacement,Shaul Dar; Michael J Franklin; Björn Þór Jónsson; Divesh Srivastava,*,VLDB 1996,1996,4
StoryPivot: comparing and contrasting story evolution,Anja Gruenheid; Donald Kossmann; Theodoros Rekatsinas; Divesh Srivastava,Abstract As the world evolves around us; so does the digital coverage of it. Events of diversetypes; associated with different actors and various locations; are continuously captured bymultiple information sources such as news articles; blogs; social media etc. day by day. Inthe digital world; these events are represented through information snippets that containinformation on the involved entities; a description of the event; when the event occurred; etc.In our work; we observe that events (and their corresponding digital representations) areoften inter-connected; ie; they form stories which represent evolving relationships betweenevents over time. Take as an example the plane crash in Ukraine in July 2014 whichinvolved multiple entities such as" Ukraine";" Malaysia"; and" Russia" and multiple eventsranging from the actual crash to the incident investigation and the presentation of the …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,3
Processing data using sequential dependencies,*,The specification describes data processes for analyzing large data steams for targetanomalies.“Sequential dependencies”(SDs) are chosen for ordered data and present aframework for discovering which subsets of the data obey a given sequential dependency.Given an interval G; an SD on attributes X and Y; written as X→ GY; denotes that thedistance between the Y-values of any two consecutive records; when sorted on X; are withinG. SDs may be extended to Conditional Sequential Dependencies (CSDs); consisting of anunderlying SD plus a representation of the subsets of the data that satisfy the SD. Theconditional approximate sequential dependencies may be expressed as pattern tableaux;ie; compact representations of the subsets of the data that satisfy the underlyingdependency.,*,2014,3
Selectivity estimation of set similarity selection queries,*,The invention relates to a system and/or methodology for selectivity estimation of setsimilarity queries. More specifically; the invention relates to a selectivity estimation techniqueemploying hashed sampling. The invention providing for samples constructed a priori thatcan efficiently and quickly provide accurate estimates for arbitrary queries; and can beupdated efficiently as well.,*,2012,3
Detecting clones; copying and reuse on the web,Xin Luna Dong; Divesh Srivastava,The Web has enabled the availability of a vast amount of useful information in recent years.However; the web technologies that have enabled sources to share their information havealso made it easy for sources to copy from each other and often publish without properattribution. Understanding the copying relationships between sources has many benefits;including helping data providers protect their own rights; improving various aspects of dataintegration; and facilitating in-depth analysis of information flow. The importance of copydetection has led to a substantial amount of research in many disciplines of ComputerScience; based on the type of information considered; such as text; images; videos; softwarecode; and structured data. This seminar explores the similarities and differences betweenthe techniques proposed for copy detection across the different types of information. We …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,3
Optimizing tree pattern queries over secure XML databases,Hui Wang; Divesh Srivastava; Laks Lakshmanan; SungRan Cho; Sihem Amer-Yahia,Abstract The rapid emergence of XML as a standard for data representation and exchangeover the Web has sparked considerable interest in models and efficient mechanisms forcontrolled access; especially using queries; to information represented in XML (see;eg;[3];[5]–[7];[11]–[13];[31]).,Secure Data Management in Decentralized Systems,2007,3
XML publishing: Look at siblings too!,Sihem Amer-Yahia; Yannis Kotidis; Divesh Srivastava,In order to publish a nested XML document from flat relational data; multiple SQL queriesare often needed. The efficiency of publishing relies on how fast these queries can beevaluated and their results shipped to the client. We illustrate novel optimization techniquesthat enable computation sharing between queries that construct sibling elements in the XMLtree. Such queries typically share large common join expressions that can be exploitedthrough appropriate rewritings. These rewritings are fundamental to XML publishing andprovide considerable performance benefits without having to modify the relational engine.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,3
Efficient OLAP Query Processing in Distributed Data Warehouses,Michael Akinde Strategy; Michael Akinde; Michael Böhlen; Laks VS Lakshmanan; Theodore Johnson; Divesh Srivastava,Abstract sets of the detail data. Skalla generates distributed query evaluation plans (for thecoordinator architecture) as a sequence of rounds; where a round consists of:(i) each localsite performing some computation and communicating the result to the coordinator; and (ii)the coordinator synchronizing the results and (possibly) communicating with the sites. Thesemantics of the subqueries generated by Skalla ensure that the amount of data that has tobe shipped between sites is independent of the size of the underlying data at the sites. Thisis particularly important in our distributed OLAP setting. We note that such a bound does notexist for the distributed processing of traditional SQL join queries [3]. The Skalla evaluationscheme allows for a wide variety of optimizations that are easily expressed in the algebraand thus readily integrated into the query optimizer. The optimization schemes included …,In: Proc. of the 8th Intl. Conf. on Extending Database Technology; Prague; Czech Republic,2002,3
Minimization of Tree Pattern Queries In: Timos Sellis (Ed.): Proceedings of the 2001 ACM SIGMOD international conference on Management of data,Sihem Amer-Yahia; SungRan Cho; Laks VS Lakshmanan; Divesh Srivastava,*,SIGMOD Conference,2001,3
Semantic data caching and replacement,Bjorn Jonsson; Divesh Srivastava; S Dar,Page 1. SEMANTIC DATA CACHING AND REPLACEMENT Βασίλης Φωτόπουλος Paper By:Shaul Dar; Michael J. Franklin; Bjorn Jonsson; Divesh Srivastava; Michael Tan Presented by:Appeared: VLDB conference 1996 Page 2. Agenda 1. Data-Shipping Architectures & Caching2. Architectures for Cache managements i. Page Caching (page servers) ii. Tuple Caching (objectservers) 3. Semantic Caching Model i. Example ii. Semantic Regions iii. Replacement IssuesPage 3. Caching σε Data-Shipping Architecture □ Data-Shipping αρχιτεκτονική είναι ένα client-server DB σύστημα στο οποίο μεγάλο μέρος της ερώτησης εκτελείται στην πλευρά του client. □Copies από data μεταφέρονται από τον server προς τον client για να μπορεί ο client να εκτελέσειτο query. (fault-driven client-server interaction). □ Για να μειώσουμε το response time (interactionswith server) η cache management strategy που θα …,Proc of the 22nd Intl Conf on VLDB. Bombay; India,1996,3
Marginal Release Under Local Differential Privacy,Tejas Kulkarni; Graham Cormode; Divesh Srivastava,Abstract: Many analysis and machine learning tasks require the availability of marginalstatistics on multidimensional datasets while providing strong privacy guarantees for thedata subjects. Applications for these statistics range from finding correlations in the data tofitting sophisticated prediction models. In this paper; we provide a set of algorithms formaterializing marginal statistics under the strong model of local differential privacy. Weprove the first tight theoretical bounds on the accuracy of marginals compiled under eachapproach; perform empirical evaluation to confirm these bounds; and evaluate them fortasks such as modeling and correlation testing. Our results show that releasing informationbased on (local) Fourier transformations of the input is preferable to alternatives baseddirectly on (local) marginals. Subjects: Databases (cs. DB) Cite as: arXiv: 1711.02952 [cs …,arXiv preprint arXiv:1711.02952,2017,2
Sourcesight: enabling effective source selection,Theodoros Rekatsinas; Amol Deshpande; Xin Luna Dong; Lise Getoor; Divesh Srivastava,Abstract Recently there has been a rapid increase in the number of data sources and dataservices; such as cloud-based data markets and data portals; that facilitate the collection;publishing and trading of data. Data sources typically exhibit large heterogeneity in the typeand quality of data they provide. Unfortunately; when the number of data sources is large; itis difficult for users to reason about the actual usefulness of sources for their applicationsand the trade-offs between the benefits and costs of acquiring and integrating sources. Inthis demonstration we present\textsc {SourceSight}; a system that allows users tointeractively explore a large number of heterogeneous data sources; and discover valuablesets of sources for diverse integration tasks.\textsc {SourceSight}~ uses a novel multi-levelsource quality index that enables effective source selection at different granularity levels …,Proceedings of the 2016 International Conference on Management of Data,2016,2
Efficient publication of sparse data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for publishing data. A data summary summarizing the data can be generated andpublished according to several publishing schemes. In some embodiments; non-zero entriesare selected and modified and zero entries are sampled according to one or moredistribution functions. The sampled and modified values are added to a data summary; or asample of the sampled and modified values are added to the data summary. The datasummary is published; released; used; or otherwise output. In other embodiments; priorityvalues are assigned to each value associated with the data; and a number of entries with thehighest values are selected and added to the data summary.,*,2016,2
Some sufficient conditions for Poisson distribution series associated with conic regions,Divesh Srivastava; Saurabh Porwal,Page 1. International Journal of Advanced Technology in Engineering and Sciencewww.ijates.com Volume No 03; Special Issue No. 01; March 2015 ISSN (online): 2348 – 7550229 | P age SOME SUFFICIENT CONDITIONS FOR POISSON DISTRIBUTION SERIESASSOCIATED WITH CONIC REGIONS Divesh Srivastava 1 ; Saurabh Porwal 2 1Departmentof Mathematics; Integral University; Lucknow; (India) .2Department of Mathematics; UIET; CSJMUniversity; Kanpur; (India) ABSTRACT The purpose of the present paper is to investigate somesufficient conditions for the convolution operator I(m)f(z) belonging to the classes k−UCV (α);k−Sp(α); S∗ (λ) and C(λ). Keywords: Convex Functions; Harmonic Functions; Poisson DistributionSeries; Starlike Function; Univalent Functions. 2010 Mathematics Subject Classification: 30C45.I. INTRODUCTION Let A denote the class of functions f(z) of the form (1.1) …,International Journal of Advanced Technology in Engineering and Science,2015,2
Meta-data indexing for XPath location steps,*,In accordance with a method of encoding meta-data associated with tree-structured data; afirst set of elements of a plurality of elements in the tree-structured is associated explicitlywith explicit meta-data levels; and a second set of elements of the plurality of elements isassociated by inheritance with explicit meta-data levels of closest ancestor elements of thefirst set of elements. The plurality of elements is packed into a plurality of leaf nodes of anindex structure. The plurality of leaf nodes is merged into a plurality of non-leaf nodes until aroot non-leaf node is generated. The plurality of non-leaf nodes of the index structure isassociated with indicators representing ranges of the explicit meta-data levels in the packedfirst set of elements; such that explicit meta-data level ranges of descendant non-leaf nodesare subsets of explicit meta-data level ranges of ancestor non-leaf nodes.,*,2011,2
Selection and ordering of candidate documents for effective query answering in XML databases,Mariam Salloum; Vassilis J Tsotras; Divesh Srivastava; Luna Dong,*,Fifth International Workshop on Ranking in Databases,2011,2
Discovering pattern tableaux for data quality analysis: a case study,Lukasz Golab; Flip Korn; Divesh Srivastava,ABSTRACT In this paper; we present a case study that illustrates the utility of pattern tableaudiscovery for data quality analysis. Given a usersupplied integrity constraint; such as aboolean predicate expected to be satisfied by every tuple; a functional dependency; or aninclusion dependency; a pattern tableau is a concise summary of subsets of the data thatsatisfy or fail the constraint. We describe Data Auditor—our system for automatic tableaudiscovery from data—and we give real-life examples of characterizing data quality in anetwork monitoring database used by a large Internet Service Provider.,Proceedings of the International Workshop on Quality in Databases (QDB),2011,2
Analyzing data quality using data auditor,Divesh Srivastava,Abstract Monitoring databases maintain configuration and measurement tables aboutcomputer systems; such as networks and computing clusters; and serve important businessfunctions; such as troubleshooting customer problems; analyzing equipment failures;planning system upgrades; etc. These databases are prone to many data quality issues:configuration tables may be incorrect due to data entry errors; while measurement tablesmay be affected by incorrect; missing; duplicate and delayed polls. We describe DataAuditor; a system for analyzing data quality and exploring data semantics of monitoringdatabases. Given a user-supplied constraint; such as a boolean predicate expected to besatisfied by every tuple; a functional dependency; or an inclusion dependency; Data Auditorcomputes" pattern tableaux"; which are concise summaries of subsets of the data that …,International Conference on Web-Age Information Management,2010,2
Database exploration using join paths,Cecilia M Procopiuc; Divesh Srivastava,Complex database schemas are challenging to explore and query; due to the exponentiallymany sequences of join edges in the schema graph; not all of which result in valid joinpaths. The problem becomes even more difficult when tables exhibit structuralheterogeneity; ie; different join paths are meaningful for different subsets of tuples in thesame table. In this paper; we propose effective ways to identify meaningful join paths incomplex schemas; and to compute the probability with which different destination tables arereached via join paths from a given source table.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,2
XML 情報検索における構造問合せを利用した部分文書スコアリング,波多野賢治， アメルヤヒアシーハム， スリバスタバディベッシュ,抄録 本稿では; XML 情報検索における構造問合せを利用した; 検索質問で指定された索引語の重みを計算する方法を提案する. 従来の XML 情報検索では; 検索対象となる XML文書の内容とその構造を利用して; 索引語の重みづけを行っているが; 本提案はそれだけにとどまらず; さらに検索質問として入力されるキーワードとそのキーワードが含まれている部分文書の構造を利用して索引語の重みを計算する方法を提案し; 検索精度の向上を図るものである.評価実験において; 提案手法の採用が検索システムの検索精度向上に役立つことを確認した.,電子情報通信学会技術研究報告. DC; ディペンダブルコンピューティング,2007,2
Randomized synopses for query verification on data streams,K Yi; F Li; Marios Hadjieleftheriou; D Srivastava; G Kollios,ABSTRACT Due to the overwhelming flow of information in many data stream applications;many companies may not be willing to acquire the necessary resources for deploying a DataStream Management System (DSMS); choosing; alternatively; to outsource the data streamand the desired computations to a third-party. But data outsourcing and remotecomputations intrinsically raise issues of trust; making outsourced query verification on datastreams a problem with important practical implications. Consider a setting where acontinuous “GROUP BY; SUM” query is processed using a remote; untrusted server. A clientwith limited processing capabilities observing exactly the same stream as the server;registers the query on the server's DSMS and receives results upon request. The clientwants to verify the integrity of the results using significantly fewer resources than …,AT&T Labs Inc.; Tech. Rep,2007,2
The bellman data quality browser,Divesh Srivastava,Data quality is a serious concern in complex industrial-scale databases; which often havethousands of tables and tens of thousands of columns. Commonly encountered problemsinclude missing data (null values); duplicates and default values in columns supposed totreated as keys; data inconsistencies (violation of functional dependencies); and poor qualityjoin paths (lack of referential integrity). Compounding the data quality problems areincomplete and out-of-date metadata about the database and the processes used topopulate the database. These problems make the task of analyzing data particularlychallenging. To effectively address such problems; we have built the Bellman data qualitybrowser at AT&T. Bellman profiles the database and computes concise statistical summariesof the contents of the database; to identify approximate keys; frequent values of a field …,CleanDB Workshop,2006,2
MIX: a meta-data indexing system for XML,SungRan Cho; Nick Koudas; Divesh Srivastava,Abstract We present a system for efficient meta-data indexed querying of XML documents.Given the diversity of the information available in XML; it is very useful to annotate XML datawith a wide variety of meta-data; such as quality and security assessments. We address themeta-data indexing problem of efficiently identifying the XML elements along a location stepin an XPath query; that satisfy meta-data range constraints. Our system; named MIX;incorporates query processing on all XPath axes suitably enhanced with meta-data featuresoffering not only query answering but also dynamic maintenance of meta-data levels forXML documents.,Proceedings of the 31st international conference on Very large data bases,2005,2
XML & Data Streams,Nicolas Bruno; Luis Gravano; Nick Koudas; Divesh Srivastava,Abstract XQuery path queries form the basis of complex matching and processing of XMLdata. Most current XML query processing techniques can be divided in two groups.Navigation-based algorithms compute results by analyzing an input stream of documentsone tag at a time. In contrast; index-based algorithms take advantage of (precomputed orcomputed-on-demand) numbering schemes over each input XML document in the stream. Inthis chapter; we present an index-based technique; Index-Filter; to answer multiple pathqueries. Index-Filter uses indexes built over the document tags to avoid processing largeportions of an input document that are guaranteed not to be part of any match. We analyzeIndex-Filter; compare it against Y-Filter; a state-of-the-art navigation-based technique; andpresent the advantages of each technique.,Stream Data Management,2005,2
Teaching relational optimizers about XML processing,Sihem Amer-Yahia; Yannis Kotidis; Divesh Srivastava,Abstract Due to their numerous benefits; relational systems play a major role in storing XMLdocuments. XML also benefits relational systems by providing a means to publish legacyrelational data. Consequently; a large volume of XML data is stored in and produced fromrelations. However; relational systems are not well-tuned to produce XML data efficiently.This is mainly due to the flat nature of relational data as opposed to the tree structure of XMLdocuments. In this paper; we argue that relational query optimizers need to incorporate newoptimization techniques that are better suited for XML. In particular; we explore newoptimization techniques that enable computation sharing between queries that constructsibling elements in the XML tree. Such queries often have large common join expressionsthat can be shared through appropriate rewritings. We show experimentally that these …,Database and XML Technologies,2004,2
Flexible string matching against large databases in practice,Nick Koudas Amit; Amit Marathe; Divesh Srivastava,Abstract Data Cleaning is an important process that has been at the center of researchinterest in recent years. Poor data quality is the result of a variety of reasons; including dataentry errors and multiple conventions for recording database fields; and has a significantimpact on a variety of business issues. Hence; there is a pressing need for technologies thatenable flexible (fuzzy) matching of string information in a database. Cosine similarity with tf-idf is a well-established metric for comparing text; and recent proposals have adapted thissimilarity measure for flexibly matching a query string with values in a single attribute of arelation.,In VLDB,2004,2
Ting Yu. Indexbased approximate XML joins,Sudipto Guha; Nick Koudas; Divesh Srivastava,*,Proceedings of the International Conference on Data Engineering (ICDE),2003,2
Preparation and characterization of polymer composites based on charge-transfer complex phenothiazine-iodine (2: 3) in poly (vinylacetate),RA Singh,Abstract: Phcnothiazine-iodine (in 2: 3 molar ratio) charge-transfer complex has been usedas a filler in poly (vinylacetate) to preparc the composite by the solution evaporation method.These composites were then characterized for their IR. XRD; microstructure; meehanicaland electrical properties. The current–voltage characteristics; frequency and temperaturedependence of their electrical conductivity have been studied. The lR data shows thesuperimposition of the peaks of composite and pure components; which confirms that thesematerials arc just a physical mixture of their constituents. A graduał increase of lensilestrength have been observed on increasing the polyner contcnt. The increasing trend ofconductivity as a function of temperature confirmed the semicondueting nature of thesecomposites.,Indian J. Phys. 74A,2000,2
Polymer Composites of Charge-Transfer Materials-II: Benzidine-Iodine (1: 1) in Poly (Vinylchloride),DN Srivastava; RA Singh,Benzidine-iodine (1: 1) charge-transfer complex has been used as a filler for preparing thepolymer composite in poly (vinylchloride). The composite is characterized by spectral;thermal; mechanical; structural and electrical methods. The properties of the compositeswere compared with the properties of the pure charge-transfer complex. The polymercomposites exhibit high semiconducting behaviour with electrical conductivities in the rangeof 10-7 to 10loS/cm; and thermal activation energies in the range of 0.4 eV to 0.7 cV. Verylow percolation threshold (5wt%) has been found. The frequency dependence of ACconductivity of these composites showed contribution from grain; grain boundary andelectrode as a function of charge-transfer complex (CTC) content. At low CTC content; all thethree contributions were present but at high CTC content; only one contribution (grain and …,MOLECULAR MATERIALS,2000,2
Evaluating Answer Quality/E ciency Tradeoff s,Ugur Cetintemel; Michael J Franklin; C Lee Giles; Divesh Srivastava,Abstract For many emerging applications and environments; information systems designersand implementers must consider the tradeo s between e ciency and the quality of queryanswers. This exibility; while allowing numerous opportunities for optimization; complicatesthe development of various system components and their performance evaluation. In thisshort paper; we rst outline the issues that arise in evaluating answer quality/e ciency tradeos. We then describe how we address these issues in two ongoing projects that adopt notionsof answer quality from the eld of Information Retrieval.,Proceedings of the 5th KRDB Workshop,1998,2
Generating advanced query interfaces,Dongwon Lee; Divesh Srivastava; Dimitra Vista,*,Computer networks and ISDN systems,1998,2
Grink: To grok and link,Guy Jacobson; Balachander Krishnamurthy; Divesh Srivastava,*,*,1996,2
HV Jagadish,Divesh Srivastava; Dimitra Vista,*,Levy. Answ eringqueriesw ith aggregation usingview s. InP roc. VLDB Conf,1996,2
The IM global information system: Data model and query evaluation,Alon Y Levy; Divesh Srivastava; Thomas Kirk,*,AT&T Bell Laboratories Technical Report. Submitted for publication,1994,2
The Save Module facility in CORAL,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,*,*,1993,2
Query restricted bottom-up evaluation of well-founded models,David B Kemp; Peter J Stuckey; Divesh Srivastava,*,Proceedings of the 1992 Joint Conference and Symposium on Logic Programming; Washington DC,1992,2
TAX: A Tree Algebra for XML; 2001,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Keith Thompson,*,Manuscript available from http://www. eecs. umich. edu/db/timber/tax_full. ps,*,2
Scalable Informative Rule Mining,Guoyao Feng; Lukasz Golab; Divesh Srivastava,We present SIRUM: a system for Scalable Informative RUle Mining from multi-dimensionaldata. Informative rules have recently been studied in several contexts; including datasummarization; data cube exploration and data quality. The objective is to produce a smallset of rules (patterns) over the values of the dimension attributes that provide the mostinformation about the distribution of a numeric measure attribute. Within SIRUM; we proposeseveral optimizations for tall; wide and distributed datasets. We implemented SIRUM inSpark and observed significant performance and scalability improvements on real datasetsdue to our optimizations. As a result; SIRUM is able to generate informative rules on muchwider and taller datasets than using distributed implementations of the previous state of theart.,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,1
Incremental maintenance of inverted indexes for approximate string matching,*,In embodiments of the disclosed technology; indexes; such as inverted indexes; are updatedonly as necessary to guarantee answer precision within predefined thresholds which aredetermined with little cost in comparison to the updates of the indexes themselves. With thepresent technology; a batch of daily updates can be processed in a matter of minutes; ratherthan a few hours for rebuilding an index; and a query may be answered with assurances thatthe results are accurate or within a threshold of accuracy.,*,2016,1
Estimating quantiles from the union of historical and streaming data,Sneha Aman Singh; Divesh Srivastava; Srikanta Tirthapura,Abstract Modern enterprises generate huge amounts of streaming data; for example; micro-blog feeds; financial data; network monitoring and industrial application monitoring. WhileData Stream Management Systems have proven successful in providing support for real-time alerting; many applications; such as network monitoring for intrusion detection and real-time bidding; require complex analytics over historical and real-time data over the datastreams. We present a new method to process one of the most fundamental analyticalprimitives; quantile queries; on the union of historical and streaming data. Our methodcombines an index on historical data with a memory-efficient sketch on streaming data toanswer quantile queries with accuracy-resource tradeoffs that are significantly better thancurrent solutions that are based solely on disk-resident indexes or solely on streaming …,Proceedings of the VLDB Endowment,2016,1
User-powered recommendation system,*,Recommendation systems are widely used in Internet applications. In currentrecommendation systems; users only play a passive role and have limited control over therecommendation generation process. As a result; there is often considerable mismatchbetween the recommendations made by these systems and the actual user interests; whichare fine-grained and constantly evolving. With a user-powered distributed recommendationarchitecture; individual users can flexibly define fine-grained communities of interest in adeclarative fashion and obtain recommendations accurately tailored to their interests byaggregating opinions of users in such communities. By combining a progressive samplingtechnique with data perturbation methods; the recommendation system is both scalable andprivacy-preserving.,*,2016,1
Data Quality for Temporal Streams.,Tamraparni Dasu; Rong Duan; Divesh Srivastava,Abstract Temporal data pose unique data quality challenges due to the presence ofautocorrelations; trends; seasonality; and gaps in the data. Data streams are a special caseof temporal data where velocity; volume and variety present additional layers of complexityin measuring the veracity of the data. In this paper; we discuss a general; widely applicableframework for data quality measurement of streams in a dynamic environment that takes intoaccount the evolving nature of streams. We classify data quality anomalies using four typesof constraints; identify violations that could be potential data glitches; and use statisticaldistortion as a metric for measuring data quality in a near real-time fashion. We illustrate ourframework using commercially available streams of NYSE stock prices consisting ofaggregates of prices and trading volumes collected every minute over a one year period …,IEEE Data Eng. Bull.,2016,1
Conservation dependencies,*,Given a set of data for which a conservation law is an appropriate characterization;“hold”and/or “fail” tableaux are provided for the underlying conservation law; thereby providing aconservation dependency whereby portions of the data for which the law approximatelyholds or fails can be discovered and summarized in a semantically meaningful way.,*,2015,1
FIT to monitor feed quality,Tamraparni Dasu; Vladislav Shkapenyuk; Divesh Srivastava; Deborah F Swayne,Abstract While there has been significant focus on collecting and managing data feeds; it isonly now that attention is turning to their quality. In this paper; we propose a principledapproach to online data quality monitoring in a dynamic feed environment. Our goal is toalert quickly when feed behavior deviates from expectations. We make contributions in twodistinct directions. First; we propose novel enhancements to permit a publish-subscribeapproach to incorporate data quality modules into the DFMS architecture. Second; wepropose novel temporal extensions to standard statistical techniques to adapt them to onlinefeed monitoring for outlier detection and alert generation at multiple scales along threedimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity;multiple lengths of data history for varying the speed at which models adapt to change; …,Proceedings of the VLDB Endowment,2015,1
Robust group linkage,Pei Li; Xin Luna Dong; Songtao Guo; Andrea Maurino; Divesh Srivastava,Abstract We study the problem of group linkage: linking records that refer to multiple entitiesin the same group. Applications for group linkage include finding businesses in the samechain; finding social network users from the same organization; and so on. Group linkagefaces new challenges compared to traditional entity resolution. First; although differentmembers in the same group can share some similar global values of an attribute; theyrepresent different entities so can also have distinct local values for the same or differentattributes; requiring a high tolerance for value diversity. Second; we need to be able todistinguish local values from erroneous values. We present a robust two-stage algorithm: thefirst stage identifies pivots--maximal sets of records that are very likely to belong to the samegroup; while being robust to possible erroneous values; the second stage collects strong …,Proceedings of the 24th International Conference on World Wide Web,2015,1
Web Information Systems Engineering--WISE 2013: 14th International Conference; Nanjing; China; October 13-15; 2013; Proceedings,Xuemin Lin; Yannis Manolopoulos; Divesh Srivastava; Guangyan Huang,This book constitutes the proceedings of the 14th International Conference on WebInformation Systems Engineering; WISE 2013; held in Nanjing; China; in October 2013. The48 full papers; 29 short papers; and 10 demo and 5 challenge papers; presented in the two-volume proceedings LNCS 8180 and 8181; were carefully reviewed and selected from 198submissions. They are organized in topical sections named: Web mining; Webrecommendation; Web services; data engineering and database; semi-structured data andmodeling; Web data integration and hidden Web; challenge; social Web; informationextraction and multilingual management; networks; graphs and Web-based businessprocesses; event processing; Web monitoring and management; and innovative techniquesand creations.,*,2013,1
Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,Ken Ross; Divesh Srivastava; Dimitris Papadias; Stavros Papadopoulos,It is our great pleasure to welcome you to the 2013 ACM SIGMOD Conference onManagement of Data; SIGMOD'13. This year the conference is being held in New York City;at the Millennium Broadway Hotel in the Times Square theater district. New York Cityprovides a wide range of attractions; including world-class restaurants; theater; museums;and monuments; with a distinctive architecture and skyline. Many of the larger technicalsessions will take place in the Hudson Theater; a real Broadway-style theater; which willgive a local flavor to the conference. SIGMOD 2013 hosts an exciting technical program; withtwo keynote talks that reflect New York City's status as a major financial center;" Big Data inCapital Markets" by Alex Nazaruk and Michael Rauchman (Middle Lake Partners LLC); and"Managing Database Technology at Enterprise Scale" by Paul Yaron (JP Morgan Chase); …,Proceedings of the ACM SIGMOD International Conference on Management of Data,2013,1
System and method for distributed content transformation,*,A distributed transformation network provides delivery of content from a content publisher toa content recipient. Content from the content publisher is received at an entry node of thedistributed transformation network and transmitted to a transformation node in the distributedtransformation network. The content is transformed according to publisher; recipient ornetwork administrator specifications and transmitting to delivery nodes which deliver thetransformed content to the content recipient. The published content may be in an XML-basedformat and transformed into an XML-related format or any other structured language formatas desired in the provided specification.,*,2012,1
Bistro Data Feed Management System,Theodore Johnson; Vladislav Shkapenyuk; Divesh Srivastava,ABSTRACT Data feed management is a critical component of many data intensiveapplications that depend on reliable data delivery to support real-time data collection;correlation and analysis. Data is typically collected from a wide variety of sources andorganizations; using a range of mechanisms-some data are streamed in real time; whileother data are obtained at regular intervals or collected in an ad hoc fashion. Individualapplications are forced to make separate arrangements with feed providers; learn thestructure of incoming files; monitor data quality; and trigger any processing necessary. TheBistro data feed manager; designed and implemented at AT&T Labs-Research; simplifiesand automates this complex task of data feed management: efficiently handling incomingraw files; identifying data feeds and distributing them to remote subscribers. Bistro …,*,2011,1
Data auditor: Analyzing data quality using pattern tableaux,Divesh Srivastava,Abstract Monitoring databases maintain configuration and measurement tables aboutcomputer systems; such as networks and computing clusters; and serve important businessfunctions; such as troubleshooting customer problems; analyzing equipment failures;planning system upgrades; etc. These databases are prone to many data quality issues:configuration tables may be incorrect due to data entry errors; while measurement tablesmay be affected by incorrect; missing; duplicate and delayed polls. We describe DataAuditor; a tool for analyzing data quality and exploring data semantics of monitoringdatabases. Given a user-supplied constraint; such as a boolean predicate expected to besatisfied by every tuple; a functional dependency; or an inclusion dependency; Data Auditorcomputes" pattern tableaux"; which are concise summaries of subsets of the data that …,International Conference on Conceptual Modeling,2009,1
Adaptive reactive network monitoring,Ahmet Bulut; Ambuj K Singh; Nick Koudas; Divesh Srivastava,Abstract We develop a framework to minimize the communication overhead of monitoringglobal system parameters; defined as function of local properties of different networkelements; in IP networks. For example; identifying when the total amount of interface outtraffic from an organization's sub-network exceeds some threshold permits activation of back-up lines. Our main idea is to optimize the scheduling of local event reporting across networkelements for a given monitoring query workload; local event frequencies; and network trafficload. Our system architecture consists of N distributed network elements and a centralmonitoring station. Each network element monitors a set of local properties; and the centralstation is responsible for identifying the status of global parameters registered in the system.We demonstrate that our monitoring algorithm adapts well to changing network conditions …,submitted for publication,2005,1
Approximate string joins,Divesh Srivastava,∎ Substantial amounts of data in existing RDBMSs are strings ∎ There is a need to correlatedata stored in different tables ∎ Applications: data cleaning; data integration ∎ Example: Findcommon customers across different services … ∎ Typing mistakes; abbreviations; different conventions∎ Standard equality joins do not “forgive such mistakes” … … Comp. Sci. Dept. Hatronic Inc.Euroaft Corp. KIA International John Paul McDougal Divesh Srivastava Service B … KIADept. of Comp. Sci. John P. McDougal Divesh Shrivastava Euroaft Hatronic Corp. EurodraftCorp … ∎ Match entries with typing mistakes ∎ Divesh Srivastava vs. Divesh Shrivastava …∎ Match entries with abbreviations ∎ Euroaft Corporation vs. Euroaft Corp … ∎ Match entrieswith different conventions ∎ Comp. Sci. Dept. vs. Dept. of Comp. Sci … String Edit Distance:Number of single character insertions; deletions; and modifications to transform one …,15th International Conference on Scientific and Statistical Database Management (SSDBM’03),2003,1
Reminiscences on influential papers,Kenneth A Ross,First; together with Agrawal; Imielinski; and Swami's SIGMOD'93 paper:" Mining AssociationRules between Sets of Items in Large Databases;" it identifies a new and important task indata mining: association rule mining; ie; finding frequent patterns or itemsets (sets of items)that occur frequently together in large databases. This has proven truly useful for frequentpattern or association mining; dependency or correlation analysis; etc.; with manyapplications. Some following studies have shown that it is also usefifl for associatiombasedclassification; sequential or structured pattern analysis; constraint-based mining; clusteranalysis; semantic data compression; data cube computation; and so on. Identification of acrucial research problem itself makes the paper distinct from many others. Second; itdiscovers a nice and elegant property in association mining; namely Apriori; which states …,ACM SIGMOD Record,2001,1
Answering queries using views,Alon Y Levy; Alberto O Mendelzon; Yehoshua Sagiv; Divesh Srivastava,*,Materialized views,1999,1
Selecting and Maintaining Materialized Views for Message Management,Himanshu Gupta; Divesh Srivastava,Electronic messaging has become one of the primary means for the dissemination;exchange and sharing of information. This is facilitated; especially within an organization; bythe use of shared folders; which are supported by current electronic messaging systems. Wedemonstrate that considerable additional flexibility can be achieved by modeling themessaging system as a data warehouse; where each message is a tuple of attribute-valuepairs; and each folder is a view on the set of all messages in the messaging system; bothuser mailboxes and current-day shared folders can be treated as specialized views.Supporting this paradigm in emerging messaging systems; which support thousands ofusers; makes it imperative to efficiently support a very large number of folders; each definedas a selection view: this is the key difference with conventional data warehouses. We …,*,1998,1
A visual language for interactive data exploration and analysis,Peter Selfridge; Divesh Srivastava,The analysis of large amounts of data to extract generalizations; exceptions; trends; andhidden relationships is a common activity in the business and scientific communities. Whilesome kinds of" knowledge" can be extracted automatically with preselected algorithms ordata mining techniques; others require an experienced human; often an expert in analysis;the business or scientific context; or both. We have found that such humans combineexploration; the search for a relevant subset or view of the data; with analysis; statistical orother techniques for measurement. We designed and implemented a visual language; IDEA;to assist the data analyst in these two intertwined tasks. The language is a convenientrepresentation for data analysis and provides environmental support for keeping track ofsequences of operations; reuse of the data analysis itself; and enforced semantics …,Visual Languages; 1996. Proceedings.; IEEE Symposium on,1996,1
Hermod: A Distributed Infrastructure for Electronic Messaging,A Biliris; R Gruber; G Hjalmtysson; HV Jagadish; MA Jones; MF McGroary; E Panagos; M Rabinovich; AW Robinson; S Spear; D Srivastava; D Vista,Abstract Recent trends in the use of messaging for marketing; electronic commerce;information exchange; and entertainment have resulted in a vast increase in the number ofmessages being sent and received. The large volume of messages and the many purposesfor which messages are exchanged requires considerably richer facilities for managingmessages and creating messagingenabled services. We describe an extensible frameworkfor developing applications and services for uni ed messaging called Hermod. The view of amessaging system as a distributed active database is central to this framework. The core ofthe system comprises multiple heterogeneous data stores: for depositing and retrievingmessage contents; for managing user folders; for representing and managing thesemistructured nature of messages; and for maintaining information about users and user …,Submitted for publication,*,1
Enabling Content Dissemination Using Efficient and Scalable Multicast (extended version),T Cho; Michael Rabinovich; KK Ramakrishnan; Divesh Srivastava; Yin Zhang,Abstract—Multicast is an approach that uses network and server resources efficiently todistribute information to groups. As networks evolve to become information-centric; users willincreasingly demand publish-subscribe based access to fine-grained information; andmulticast will need to evolve to (i) manage an increasing number of groups; with a distinctgroup for each piece of distributable content;(ii) support persistent group membership; asgroup activity can vary over time; with intense activity at some times; and infrequent (but stillcritical) activity at others. These requirements raise scalability challenges that are not met bytoday's multicast techniques. In this paper; we propose the MAD (Multicast with AdaptiveDual-state) architecture to provide efficient multicast service at massive scale. MAD canscalably support a vast number of multicast groups; with varying activity over time; based …,*,*,1
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*
Variance-Optimal Offline and Streaming Stratified Random Sampling,Trong Duc Nguyen; Ming-Hung Shih; Divesh Srivastava; Srikanta Tirthapura; Bojian Xu,Abstract: Stratified random sampling (SRS) is a fundamental sampling technique thatprovides accurate estimates for aggregate queries using a small size sample; and has beenused widely for approximate query processing. A key question in SRS is how to partition atarget sample size among different strata. While {\em Neyman allocation} provides a solutionthat minimizes the variance of an estimate using this sample; it works under the assumptionthat each stratum is abundant; ie has a large number of data points to choose from. Thisassumption may not hold in general: one or more strata may be bounded; and may notcontain a large number of data points; even though the total data size may be large. We firstpresent\voila; an offline method for allocating sample sizes to strata in a variance-optimalmanner; even for the case when one or more strata may be bounded. We next consider …,arXiv preprint arXiv:1801.09039,2018,*
Methods and systems for content access and distribution,*,Abstract A method for disseminating content over an overlay network having a plurality ofrouters in communication with providers and consumers of content is disclosed. A router inthe overlay network receives a content descriptor corresponding to a query for content froma node; and generates a subscriber interface list based on the query for the content. Thesubscriber interface list is transmitted to a plurality of subsequent routers in the overlaynetwork; wherein a content identified by the subsequent routers that correspond to the querywill be routed to the router based on the subscriber interface list. The router receives first andsecond documents corresponding to the query for the content from the node. If the seconddocument is the duplicate; the router deletes the second document and transmits the firstdocument to the node or if not the router transmits both the first and second documents to …,*,2017,*
METHOD AND APPARATUS FOR ENRICHING METADATA VIA A NETWORK,*,Abstract: A method and apparatus for enriching metadata are disclosed. For example; themethod implemented via a processor monitors metadata associated with a first webpage ofa plurality of webpages; the first webpage having been determined to be similar to a secondwebpage of the plurality of webpages; detects a change to the metadata associated with thefirst webpage; determines whether the change to the metadata associated with the firstwebpage invokes an update to a metadata associated with the second webpage; andprocesses the update of the metadata associated with the second webpage when thechange invokes the update to the metadata associated with the second webpage.,*,2017,*
Mind the Gaps (and Bumps): Statistical Smoothing of Spatiotemporal Streams,Philip E Brown; Tamraparni Dasu; Yaron Kanza; Divesh Srivastava,Abstract Spatiotemporal streams are prone to data quality issues such as missing;duplicated and delayed data when data generating sensors malfunction; data transmissionsexperience problems; or when data is stored or processed improperly. However; manyimportant real-time applications rely on the continuous availability of stream values; eg; tomonitor traffic flow; resource usage; weather phenomena; etc. Other non real-timeapplications that support continuous or offline historical analytics also require high qualitydata to avoid producing misleading output such as false positives; erroneous conclusionsand decisions. In this paper; we propose the use of nonparametric (data-driven; distributionfree) statistical methods to provide an uninterrupted stream of high-quality spatiotemporaldata to real-time applications even when the raw stream suffers data quality issues. Our …,Proceedings of the 8th ACM SIGSPATIAL Workshop on GeoStreaming,2017,*
Composing Differential Privacy and Secure Computation: A case study on scaling private record linkage,Xi He; Ashwin Machanavajjhala; Cheryl Flynn; Divesh Srivastava,Abstract Private record linkage (PRL) is the problem of identifying pairs of records that aresimilar as per an input matching rule from databases held by two parties that do not trust oneanother. We identify three key desiderata that a PRL solution must ensure:(1) perfectprecision and high recall of matching pairs;(2) a proof of end-to-end privacy; and (3)communication and computational costs that scale subquadratically in the number of inputrecords. We show that all of the existing solutions for PRL? including secure 2-partycomputation (S2PC); and their variants that use non-private or differentially private (DP)blocking to ensure subquadratic cost--violate at least one of the three desiderata. Inparticular; S2PC techniques guarantee end-to-end privacy but have either low recall orquadratic cost. In contrast; no end-to-end privacy guarantee has been formalized for …,Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,2017,*
Constrained Differential Privacy for Count Data,Graham Cormode; Tejas Kulkarni; Divesh Srivastava,Abstract: Concern about how to aggregate sensitive user data without compromisingindividual privacy is a major barrier to greater availability of data. The model of differentialprivacy has emerged as an accepted model to release sensitive information while giving astatistical guarantee for privacy. Many different algorithms are possible to address differenttarget functions. We focus on the core problem of count queries; and seek to designmechanisms to release data associated with a group of n individuals. Prior work has focusedon designing mechanisms by raw optimization of a loss function; without regard to theconsequences on the results. This can leads to mechanisms with undesirable properties;such as never reporting some outputs (gaps); and overreporting others (spikes). We tamethese pathological behaviors by introducing a set of desirable properties that …,arXiv preprint arXiv:1710.00608,2017,*
A Tool for Statistical Analysis on Network Big Data,Carlos Ordonez; Theodore Johnson; Divesh Srivastava; Simon Urbanek,Due to advances in parallel file systems for big data (ie HDFS) and larger capacity hardware(multicore CPUs; large RAM) it is now feasible to manage and query network data in aparallel DBMS supporting SQL; but performing statistical analysis remains a challenge. Onthe statistics side; the R language is popular; but it presents important limitations: R is limitedby main memory; R works in a different address space from query processing; R cannotanalyze large disk-resident data sets efficiently; and R has no data managementcapabilities. Moreover; some R libraries allow R to work in parallel; but without datamanagement capabilities. Considering the challenges and limitations described above; wepresent a system that allows combining SQL queries and R functions in a seamless manner.We justify a parallel DBMS and the R runtime are two different systems that benefit from a …,Database and Expert Systems Applications (DEXA); 2017 28th International Workshop on,2017,*
Integrating the R Language Runtime System with a Data Stream Warehouse,Carlos Ordonez; Theodore Johnson; Simon Urbanek; Vladislav Shkapenyuk; Divesh Srivastava,Abstract Computing mathematical functions or machine learning models on data streams isdifficult: a popular approach is to use the R language. Unfortunately; R has importantlimitations: a dynamic runtime system incompatible with a DBMS; limited by available RAMand no data management capabilities. On the other hand; SQL is well established to writequeries and manage data; but it is inadequate to perform mathematical computations. Withthat motivation in mind; we present a system that enables analysis in R on a time window;where the DBMS continuously inserts new records and propagates updates to materializedviews. We explain the low-level integration enabling fast data transfer in RAM between theDBMS query process and the R runtime. Our system enables analytic calls in bothdirections:(1) R calling SQL to evaluate streaming queries; transferring output streaming …,International Conference on Database and Expert Systems Applications,2017,*
Reverse engineering aggregation queries,Wei Chit Tan; Meihui Zhang; Hazem Elmeleegy; Divesh Srivastava,Abstract Query reverse engineering seeks to re-generate the SQL query that produced agiven query output table from a given database. In this paper; we solve this problem forOLAP queries with group-by and aggregation. We develop a novel three-phase algorithmnamed REGAL 1 for this problem. First; based on a lattice graph structure; we identify a setof group-by candidates for the desired query. Second; we apply a set of aggregationconstraints that are derived from the properties of aggregate operators at both the table-leveland the group-level to discover candidate combinations of group-by columns andaggregations that are consistent with the given query output table. Finally; we find a multi-dimensional filter; ie; a conjunction of selection predicates over the base table attributes; thatis needed to generate the exact query output table. We conduct an extensive …,Proceedings of the VLDB Endowment,2017,*
Enabling Change Exploration: Vision Paper,Tobias Bleifuß; Theodore Johnson; Dmitri V Kalashnikov; Felix Naumann; Vladislav Shkapenyuk; Divesh Srivastava,Abstract Data and metadata suffer many different kinds of change: values are inserted;deleted or updated; entities appear and disappear; properties are added or re-purposed;etc. Explicitly recognizing; exploring; and evaluating such change can alert to changes indata ingestion procedures; can help assess data quality; and can improve the generalunderstanding of the dataset and its behavior over time. We propose a data model-independent framework to formalize such change. Our change-cube enables explorationand discovery of such changes to reveal dataset behavior over time.,Proceedings of the ExploreDB'17,2017,*
Repairing Noisy Graphs,Divesh Srivastava,Abstract Graphs are a flexible way to represent data in a variety of applications; with nodesrepresenting domain-specific entities (eg; records in record linkage; products and types inan ontology) and edges capturing a variety of relationships between these entities (eg; anequivalence relationship between records in record linkage; a type-subtype relationshipbetween types in an ontology). Often; the edges in this graph are noisy; in that some edgesare missing (ie; real-world relationships that do not have corresponding edges in the graph)and some edges are spurious (ie; edges in the graph that do not have corresponding real-world relationships). Directly analyzing noisy graphs can lead to undesirable outcomes;making it important to repair noisy graphs. In this talk; we describe an approach that takesadvantage of properties of real-world relationships and their estimated probabilities to ask …,Proceedings of the 2nd International Workshop on Network Data Analytics,2017,*
Tell me more using Ladders in Wikipedia,Siarhei Bykau; Jihwan Lee; Divesh Srivastava; Yannis Velegrakis,Abstract We focus on the problem of" tell me more" information related to a given fact inWikipedia. We use the novel notion of role to link information in an infobox with differentplaces in the text of the same Wikipedia page (space) as well as information across differentrevisions of the page (time). In this way; it is possible to link together pieces of informationthat may not represent the same real world entity; yet have served in the same role. Toachieve this; we introduce a novel structure called ladder that allows such spatial andtemporal linking and we show how to effectively and efficiently construct such structures fromWikipedia data.,Proceedings of the 20th International Workshop on the Web and Databases,2017,*
Incremental maintenance of inverted indexes for approximate string matching,*,In embodiments of the disclosed technology; indexes; such as inverted indexes; are updatedonly as necessary to guarantee answer precision within predefined thresholds which aredetermined with little cost in comparison to the updates of the indexes themselves. With thepresent technology; a batch of daily updates can be processed in a matter of minutes; ratherthan a few hours for rebuilding an index; and a query may be answered with assurances thatthe results are accurate or within a threshold of accuracy.,*,2017,*
System For Continuous Monitoring Of Data Quality In A Dynamic Feed Environment,*,A system for providing continuous monitoring of data quality in a dynamic feed environmentis disclosed. In particular; the system utilizes a feed inspection tool to detect anomalies indata gathering detected from feed metadata and anomalies in data measurement detectedbased on file contents. In order to do so; the feed inspection tool may aggregate; for aplurality of aggregation intervals; data feeds and associated metadata feeds. Once the datafeeds and metadata feeds are aggregated; the feed inspection tool may generate; for abaseline model feed; baseline statistical models by utilizing historical data of the aggregatedfeeds in sliding windows of different lengths. The feed inspection tool may then identify; for aplurality of monitoring time delays; data outliers by comparing the aggregated feeds with thebaseline model feed. A data quality feed based on the data outliers identified may then be …,*,2017,*
Scaling Private Record Linkage using Output Constrained Differential Privacy,Xi He; Ashwin Machanavajjhala; Cheryl Flynn; Divesh Srivastava,Abstract: Many scenarios require computing the join of databases held by two or moreparties that do not trust one another. Private record linkage is a cryptographic tool that allowssuch a join to be computed without leaking any information about records that do notparticipate in the join output. However; such strong security comes with a cost: except forexact equi-joins; these techniques have a high computational cost. While blocking has beenused to successfully scale non-private record linkage trading off efficiency with outputaccuracy; we show that prior techniques that use blocking; even based on differentiallyprivate algorithms; result in leakage of non-matching records. In this paper; we seekmethods for speeding up private record linkage with strong provable privacy guarantees. Wepropose a novel privacy model; called output constrained differential privacy; that shares …,arXiv preprint arXiv:1702.00535,2017,*
Proceedings of the VLDB Endowment Volume 10 Issue 6,Divesh Srivastava,*,*,2017,*
Harmonic starlikeness and convexity of integral operators generated by Poisson distribution series,Saurabh Porwal; Divesh Srivastava,Abstract. The purpose of the present paper is to establish connections between varioussubclasses of harmonic univalent functions by applying certain integral operator involvingthe Poisson distribution series. To be more precise; we investigate such connections withharmonic starlike and harmonic convex mappings in the plane.,Mathematica Moravica,2017,*
Online Event Integration with StoryPivot,Anja Gruenheid; Donald Kossmann; Divesh Srivastava,Abstract: Modern data integration systems need to process large amounts of data from avariety of data sources and with real-time integration constraints. They are not onlyemployed in enterprises for managing internal data but are also used for a variety of webservices that use techniques such as entity resolution or data cleaning in live systems. In thiswork; we discuss a new generation of data integration systems that operate on (un-)structured data in an online setting; ie; systems which process continuously modifieddatasets upon which the integration task is based. We use as an example of such a systeman online event integration system called StoryPivot. It observes events extracted from newsarticles in data sources such as the'Guardian'or the'Washington Post'which are integrated toshow users the evolution of real-world stories over time. The design decisions for …,arXiv preprint arXiv:1610.07732,2016,*
Online data fusion,*,An online data fusion system receives a query; probes a first source for an answer to thequery; returns the answer from the first source; refreshes the answer while probing anadditional source; and applies fusion techniques on data associated with an answer that isretrieved from the additional source. For each retrieved answer; the online data fusionsystem computes the probability that the answer is correct and stops retrieving data for theanswer after gaining enough confidence that data retrieved from the unprocessed sourcesare unlikely to change the answer. The online data fusion system returns correct answersand terminates probing additional sources in an expeditious manner without sacrificing thequality of the answers.,*,2016,*
User-Powered Recommendation System,*,Recommendation systems are widely used in Internet applications. In currentrecommendation systems; users only play a passive role and have limited control over therecommendation generation process. As a result; there is often considerable mismatchbetween the recommendations made by these systems and the actual user interests; whichare fine-grained and constantly evolving. With a user-powered distributed recommendationarchitecture; individual users can flexibly define fine-grained communities of interest in adeclarative fashion and obtain recommendations accurately tailored to their interests byaggregating opinions of users in such communities. By combining a progressive samplingtechnique with data perturbation methods; the recommendation system is both scalable andprivacy-preserving.,*,2016,*
Managing opt-in and opt-out for private data access,*,Concepts and technologies disclosed herein are for managing opt-in and opt-out for privatedata access. According to one aspect disclosed herein; a mobile device can receive arequest to obtain private data associated with a user of the mobile device and; in responseto the request; determine whether an application program associated with the request ispermitted to access the private data based upon a rule. The mobile device; in response todetermining that the application program is permitted to access the private data based uponthe rule; can instruct the application program to proceed to obtain the private data. Themobile device; in response to determining that the application program is not permitted toaccess the private data based upon the rule; can instruct the application program to avoidobtaining the private data.,*,2016,*
TreeScope: finding structural anomalies in semi-structured data,Shanshan Ying; Flip Korn; Barna Saha; Divesh Srivastava,Abstract Semi-structured data are prevalent on the web; with formats such as XML andJSON soaring in popularity due to their generality; flexibility and easy customization.However; these very same features make semi-structured data prone to a range of dataquality errors; from errors in content to errors in structure. While the former has been wellstudied; little attention has been paid to structural errors. In this demonstration; we present Tree S cope; which analyzes semi-structured data sets with the goal of automaticallyidentifying structural anomalies from the data. Our techniques learn robust structural modelsthat have high support; to identify potential errors in the structure. Identified structuralanomalies are then concisely summarized to provide plausible explanations of the potentialerrors. The goal of this demonstration is to enable an interactive exploration of the …,Proceedings of the VLDB Endowment,2015,*
Managing opt-in and opt-out for private data access,*,Concepts and technologies disclosed herein are for managing opt-in and opt-out for privatedata access. According to one aspect disclosed herein; a mobile device can receive arequest to obtain private data associated with a user of the mobile device and; in responseto the request; determine whether an application program associated with the request ispermitted to access the private data based upon a rule. The mobile device; in response todetermining that the application program is permitted to access the private data based uponthe rule; can instruct the application program to proceed to obtain the private data. Themobile device; in response to determining that the application program is not permitted toaccess the private data based upon the rule; can instruct the application program to avoidobtaining the private data.,*,2015,*
On Axiomatization and Inference Complexity over a Hierarchy of Functional Dependencies,Jaroslaw Szlichta; Lukasz Golab; Divesh Srivastava,Abstract. Functional dependencies (FDs) have recently been extended for data qualitypurposes with various notions of similarity instead of strict equality. We study theseextensions in this paper. We begin by constructing a hierarchy of dependencies; showingwhich dependencies generalize others. We then focus on an extension of FDs that we callAntecedent Metric Functional Dependencies (AMFDs). An AMFD asserts that if two tupleshave similar but not necessarily equal values of the antecedent attributes; then theirconsequent values must be equal. We present a sound and complete axiomatization as wellas an inference algorithm for AMFDs. We compare the axiomatization of AMFDs to those ofthe other dependencies; and we show that while the complexity of inference for some FDextensions is quadratic or even co-NP complete; the inference problem for AMFDs …,Alberto Mendelzon International Workshop on Foundations of Data Management,2015,*
Method and apparatus for using tag topology,*,A method and apparatus for using tag topology for enhancing search capabilities; eg;searching over the web; are disclosed. For example; the present method receives a userquery contain a search term from a user. The method then generates a search resultcontaining at least one entity; wherein the at least one entity is found based on a plurality ofuser provided tags that is associated with the at least one entity.,*,2014,*
Processing data using sequential dependencies,*,Methods and apparatus for processing data using sequential dependencies are disclosedherein. An example method includes modifying a first number of values in a sequence of adata set to generate a modified sequence such that each difference between eachsuccessive pair of values is within a threshold. A satisfiability metric is determined for themodified sequence based on a relationship between a number of modifications to the valuesin the sequence and a size of the sequence.,*,2014,*
Data feed management,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for data feed management. Data feeds can be received at one or more landingdirectories associated with a feed management system. The feed management system canbe configured to map the data feeds in the landing directories to one or more subscribers;and the data feeds can be moved to one or more staging directories associated with thesubscribers. The data feeds can be delivered to the one or more subscribers from thestaging directories. Receipts indicating delivery of the data feeds can be stored by the feedmanagement system. The feed management system can be configured to manage datafeeds; landing directories; and staging directories.,*,2014,*
Incremental Record Linkage,Divesh Srivastava; Anja Gruenheid; Xin Luna Dong,BizID ID name street address city phone B1 r1 Starbucks 123 MISSION ST STE ST1 SANFRANCISCO 4155431510 B1 r2 Starbucks 123 MISSION ST SAN FRANCISCO4155431510 B1 r3 Starbucks 123 Mission St San Francisco 4155431510 B2 r4 StarbucksCoffee 340 MISSION ST SAN FRANCISCO 4155431510,*,2014,*
UMicS: from anonymized data to usable microdata,Graham Cormode; Entong Shen; Xi Gong; Ting Yu; Cecilia M Procopiuc; Divesh Srivastava,Abstract There is currently a tug-of-war going on surrounding data releases. On one side;there are many strong reasons pulling to release data to other parties: business factors;freedom of information rules; and scientific sharing agreements. On the other side; concernsabout individual privacy pull back; and seek to limit releases. Privacy technologies such asdifferential privacy have been proposed to resolve this deadlock; and there has been muchstudy of how to perform private data release of data in various forms. The focus of suchworks has been largely on the data owner: what process should they apply to ensure thatthe released data preserves privacy whilst still capturing the input data distributionaccurately. Almost no attention has been paid to the needs of the data user; who wants tomake use of the released data within their existing suite of tools and data. The difficulty of …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,*
Staff,Divesh Srivastava,For close to 50 years; Neil J. Sloane has been collecting and cataloguing integersequences. The result is The Online Encyclopedia of Integer Sequences (OEIS); acomprehensive; constantly updated repository of 200;000+ sequences and one of the firstlarge-scale exercises in crowd-sourcing to expand mathematical and scientificinformation.The OEIS; for years hosted on AT&T Research servers; is now under thestewardship of The OEIS Foundation; which is charged with maintaining the OEIS as a freeand open resource to the world community. Read more,Journal of Computational Methods in Science and Engineering,2013,*
Method and apparatus for exploring and selecting data sources,*,A system and method for choosing data sources for use in a data repository first chooses aninitial selection of data sources based on keywords. An exploration tool is provided toorganize the sources according to content and other attributes. The tool is used to pre-selectdata sources. The sources to include in the data repository are then selected based on amarginalism economic theory that considers both costs and quality of data.,*,2013,*
VLDB Endowment,Michael Böhlen; Christoph Koch; Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu; Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; Dan Olteanu; Divesh Srivastava; Jens Teubner; Stefan Manegold; Peer Kröger; Stratis D Viglas,39th International Conference on Very Large Data Bases; Riva del Garda; Trento; Italy … Proceedingsof the 39th International Conference on … Very Large Data Bases; Riva del Garda; Trento; Italy… Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu;Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; DanOlteanu; Divesh Srivastava; Jens Teubner … The 39th International Conference on Very LargeData Bases; Riva del Garda; Trento; Italy … Permission to make digital or hard copies of portionsof this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyright for components of this work owned by others than VLDBEndowment must be honored. Abstracting with credit is permitted. To copy otherwise; to …,*,2013,*
Madison WI 53706; USA,RAGHU RAMAKRISHNAN; DIVESH SRIVASTAVA; S SUDARSHAN,Abstract. In recent years; much work has been directed towards evaluating logic programsand queries on deductive databases by using an iterative bottom-up fixpoint computation.The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation inseveral situations. They are sound and complete for positive Horn clause programs; are well-suited to applications with large volumes of data (facts); and can support a variety ofextensions to the standard logic programming paradigm. We present the basics of databasequery evaluation and logic programming evaluation; and then discuss bottom-up fixpointevaluation. We discuss an approach based upon using a program transformation (“MagicTemplates”) to restrict search; followed by fixpoint computation using a technique (“Semi-naive evaluation”) that avoids repeated inferences. The program transformation …,Computer Systems and Software Engineering: State-of-the-art,2012,*
System and method for distributed content transformation,*,A distributed transformation network provides delivery of content from a content publisher toa content recipient. Content from the content publisher is received at an entry node of thedistributed transformation network and transmitted to a transformation node in the distributedtransformation network. The content is transformed according to publisher; recipient ornetwork administrator specifications and transmitting to delivery nodes which deliver thetransformed content to the content recipient. The published content may be in an XML-basedformat and transformed into an XML-related format or any other structured language formatas desired in the provided specification.,*,2012,*
Method and apparatus for rapid identification of column heterogeneity,*,A method and apparatus for rapid identification of column heterogeneity in databases aredisclosed. For example; the method receives data associated with a column in a database.The method computes a cluster entropy for the data as a measure of data heterogeneity andthen determines whether said data is heterogeneous in accordance with the cluster entropy.,*,2012,*
Special issue: best papers of APWeb 2010,Wook-Shin Han; Divesh Srivastava; Ge Yu,The first paper;“A Space and Time Efficient Algorithm for SimRank Computation;” by WeirenYu; Wenjie Zhang; Qing Zhang; Xuemin Lin; and Jiajin Le proposes novel optimizationtechniques to accelerate SimRank Computation. In addition; the authors develop areordering scheme to achieve both faster convergence rate and I/O efficiency. They alsoperform extensive experiments using many data sets to demonstrate the efficiency of theproposed techniques. The second paper;“Quick Identification of Near-Duplicate VideoSequences with Cut Signature;” by Qing Xie; Zi Huang; Heng Tao Shen; Xiaofang Zhou; andChaoyi Pang is about fast identification of duplicate video sequences. The authors considerthe issue of how to efficiently and effectively detect near-duplicate video sequences overcontinuous video streams; and propose a novel compact signature called CutSig to …,World Wide Web,2012,*
Detecting clones; copying and reuse on the web (DASFAA 2012 tutorial),Xin Luna Dong; Divesh Srivastava,Abstract The Web has enabled the availability of a vast amount of useful information inrecent years. However; the Web technologies that have enabled sources to share theirinformation have also made it easy for sources to copy from each other and often publishwithout proper attribution. Understanding the copying relationships between sources hasmany benefits; including helping data providers protect their own rights; improving variousaspects of data integration; and facilitating in-depth analysis of information flow.,Proceedings of the 17th international conference on Database Systems for Advanced Applications-Volume Part II,2012,*
Method and system for pattern matching having holistic twig joins,*,As is known in the art; the eXtensible Markup Language (XML) employs a tree-structured modelfor representing data. Queries in XML query languages typically specify patterns of selectionpredicates on multiple elements that have some specified tree structured relationships. Forexample; the XQuery expression: book[title='XML///author[fn=jane' AND In='doe] matches authorelements that (i) have a child subelement “fn” with content lane”; (ii) have a child subelement“In” with content “doe”; and (iii) are descendants of book elements that have a child title subelementwith content XML. This expression can be represented as a node-labeled twig (or small tree)pattern with elements and string values as node labels … Finding all occurrences of a twig patternin a database is a core operation in XML query processing; both in relational implementationsof XML databases; and in native XML databases. Known processing techniques typically …,*,2012,*
Securing database content,*,A method for securing content in a database includes identifying a challenge columnassociated with a database column referenced in an update query. A challenge value for thechallenge column may be received and resolved for a match with a corresponding valuestored in the challenge column. In case of a match; the update query may be certified forexecution on the database; otherwise; the update query may be prevented from executing.Challenge columns may be determined by an analysis of the database on the basis ofdiscriminating power; description complexity; and/or diversity.,*,2012,*
Proceedings of the 2012 joint EDBT/ICDT workshops,Divesh Srivastava; İsmail Arı,*,*,2012,*
In Search of Truth (on the Deep Web).,Divesh Srivastava,Abstract. The Deep Web has enabled the availability of a huge amount of useful informationand people have come to rely on it to fulfill their information needs in a variety of domains.We present a recent study on the accuracy of data and the quality of Deep Web sources intwo domains where quality is important to people's lives: Stock and Flight. We observe that;even in these domains; the quality of the data is less than ideal; with sources providingconflicting; out-of-date and incomplete data. Sources also copy; reformat and modify datafrom other sources; making it di cult to discover the truth. We describe techniques proposedin the literature to solve these problems; evaluate their strengths on our data; and identifydirections for future work in this area.,AMW,2012,*
Divesh Srivastava Speaks Out on the importance of looking at real data; abstracting problems and more,Divesh Srivastava,*,*,2011,*
Foundations and Trends® in Databases,Marios Hadjieleftheriou; Divesh Srivastava,*,Foundations and Trends® in Databases,2011,*
Advances in Web Technologies and Applications-Proceedings of the 12th Asia-Pacific Web Conference; APWeb 2010: Message from the general chairs and progra...,Bonghee Hong; Lizhu Zhou; Xiaofang Zhou; Wook-Shin Han; Divesh Srivastava; Ge Yu,skip nav …,Advances in Web Technologies and Applications-Proceedings of the 12th Asia-Pacific Web Conference; APWeb 2010,2010,*
Method and Apparatus for Ranked Join Indices,P Tsaparas; T Palpanas; Y Kotidis; N Koudas; D Srivastava,A method and apparatus for ranked join indices includes a solution providing performanceguarantees for top-k join queries over two relations; when preprocessing to construct aranked join index for a specific join condition is permitted. The concepts of ranking joinindices presented herein are also applicable in the case of a single relation. In this case; theconcepts herein provide a solution to the top-k selection problem with monotone linearfunctions; having guaranteed worst case search performance for the case of two rankedattributes and arbitrary preference vectors.,*,2010,*
Multiple aggregations over data streams,*,A system for a data stream management system includes a filter transport aggregate for ahigh speed input data stream with a plurality of packets each packet comprising attributes.The system includes an evaluation system to evaluate the high speed input data stream andpartitions the packets into groups the attributes and a table; wherein the table stores theattributes of each packets using a hash function. A phantom query is used to definepartitioned groups of packets using attributes other than those used to group the packets forsolving user queries without performing the user queries on the high speed input datastream.,*,2009,*
Special issue: best papers of VLDB 2007,Minos Garofalakis; Johannes Gehrke; Divesh Srivastava,This special issue of the VLDB Journal is dedicated to the best papers from the 33rdInternational Conference on Very Large Data Bases; which took place on 23–28 September2007 at the University of Vienna in Austria. The conference received 668 submissionsoverall. The Core Database Technology Track received 263 submissions out of which 46(17.5%) were accepted; the Infrastructure for Information Systems Track received 275submissions out of which 45 (16.4%) were accepted; the Industrial; Applications; andExperience Track received 56 submissions out of which 17 (30.4%) were accepted; and theDemonstrations Track received 74 submissions out of which 29 (39.2%) were accepted.,The VLDB Journal,2009,*
Weighted set similarity: queries and updates,Divesh Srivastava,Consider a universe of items; each of which is associated with a weight; and a databaseconsisting of subsets of these items. Given a query set; a weighted set similarity queryidentifies either (i) all sets in the database whose normalized similarity to the query set isabove a pre-specified threshold; or (ii) the sets in the database with the k highest similarityvalues to the query set. Weighted set similarity queries are useful in applications like datacleaning and integration for finding approximate matches in the presence of typographicalmistakes; multiple formatting conventions; transformation errors; etc. We show that thisproblem has semantic properties that can be exploited to design index structures thatsupport efficient algorithms for answering queries; these algorithms can achieve arbitrarilystronger pruning than the family of Threshold Algorithms. We describe how these index …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,*
XML Indexing,Xin Luna Dong; Divesh Srivastava,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,*
Censorship-resistant Publishing,Emiran Curtmola; Alin Deutsch; KK Ramakrishnan; Divesh Srivastava,We propose a novel privacy-preserving distributed infrastructure in which data resides onlywith the publishers owning it. The infrastructure disseminates user queries to publishers;who answer them at their own discretion. The infrastructure enforces a publisher k-anonymity guarantee; which prevents leakage of information about which publishers arecapable of answering a certain query. Given the virtual nature of the global data collection;we study the challenging problem of efficiently locating publishers in the community thatcontain data items matching a specified query. We propose a distributed index structure;UQDT; that is organized as a union of Query Dissemination Trees (QDTs); and realized onan overlay (ie; logical) network infrastructure. Each QDT has data publishers as its leafnodes; and overlay network nodes as its internal nodes; each internal node routes …,*,2008,*
Uniform Management Of Data And Metadata,D Srivastava; Y Velegrakis,E' presente una richiesta di inserimento in ANCE di una nuova rivista; utilizza la funzione "Registracodice ANCE" per registrare il codice ricevuto dal servizio LoginMIUR o inviare una nuova richiestadi inserimento oppure cercare nuovamente la rivista. E' presente una richiesta di inserimentoin ANCE di una nuova serie; utilizza la funzione "Registra codice ANCE" per registrare il codicericevuto dal servizio LoginMIUR o inviare una nuova richiesta di inserimento oppure cercarenuovamente la serie … Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatoriper il sito CINECA non sono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuareuna rivista con i dati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN doveapplicabili e il titolo della rivista … Segnalazioni con codici 20201/20202:La pubblicazionenon è stata trasferita SOLO per i docenti segnalati nel messaggio a causa di problemi …,HDMS¿ 08,2008,*
Sound ranking algorithms for XML search in PF/Tijah,Djoerd Hiemstra; Stefan Klinger; Henning Rode; Jan Flokstra; Peter Apers,Abstract We argue that ranking algorithms for XML should reflect the actual combinedcontent and structure constraints of queries; while at the same time producing equalrankings for queries that are semantically equal. Ranking algorithms that produce differentrankings for queries that are semantically equal are easily detected by tests on largedatabases: We call such algorithms {em not sound}. We report the behaviour of differentapproaches to ranking content-and-structure que-ries on pairs of queries for which weexpect equal ranking results from the query semantics. We show that most of theseapproaches are not sound. Of the remaining approaches; only 3 adhere to the W3C XQueryFull-Text standard.,Dagstuhl Seminar Proceedings,2008,*
08111 Abstracts Collection--Ranked XML Querying,Sihem Amer-Yahia; Divesh Srivastava; Gerhard Weikum,Abstract From 09.03. to 14.03. 08; the Dagstuhl Seminar 08111``Ranked XML Querying''washeld in the International Conference and Research Center (IBFI); Schloss Dagstuhl. Duringthe seminar; several participants presented their current research; and ongoing work andopen problems were discussed. Abstracts of the presentations given during the seminar aswell as abstracts of seminar results and ideas are put together in this paper. The first sectiondescribes the seminar topics and goals in general. Links to extended abstracts or full papersare provided; if available.,Dagstuhl Seminar Proceedings,2008,*
Personalizing XML Full Text Search in PIMENTO,Irini Fundulaki; Sihem Amer-Yahia; Lakshmanan Laks,Abstract In PIMENTO we advocate a novel approach to XML search that leverages userinformation to return more relevant query answers. This approach is based on formalizing{em user profiles} in terms of {em scoping rules} which are used to rewrite an input query;and of {em ordering rules} which are combined with query scoring to customize the rankingof query answers to specific users.,Dagstuhl Seminar Proceedings,2008,*
Other Workshop Reports-DB&IR Integration: Report on the Dagstuhl Seminar" Ranked XML Querying",Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,*,SIGIR Forum,2008,*
08111 Report--Ranked XML Querying,Sihem Amer-Yahia; Djoerd Hiemstra; Thomas Roelleke; Divesh Srivastava; Gerhard Weikum,Abstract This paper is based on a five-day workshop on" Ranked XML Querying" that tookplace in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people fromthree different research communities: database systems (DB); information retrieval (IR); andWeb. The seminar title was interpreted in an IR-style" andish" sense (it covered also subsetsof {Ranking; XML; Querying}; with larger sets being favored) rather than the DB-style strictlyconjunctive manner. So in essence; the seminar really addressed the integration of DB andIR technologies with Web 2.0 being an important target area.,Dagstuhl Seminar Proceedings,2008,*
33rd International Conference on,Christoph Koch; Johannes Gehrke; Minos Garofalakis; Divesh Srivastava; Anand Deshpande; Dana Florescu; Chee-Yong Chan; Venkatesh Ganti; Carl-Christian Kanne; Wolfgang Klas; Erich J Neuhold,Editors: Christoph Koch; Saarland University; Germany Johannes Gehrke; CornellUniversity; USA Minos Garofalakis; Yahoo! Research and UC Berkeley; USA DiveshSrivastava; AT&T Labs Research; USA Karl Aberer; EPFL; Switzerland Anand Deshpande; PersistentSystems; India Dana Florescu; Oracle; USA Chee-Yong Chan; National University ofSingapore; Singapore Venkatesh Ganti; Microsoft Research; USA Carl-Christian Kanne; Universityof Mannheim; Germany Wolfgang Klas; University of Vienna; Austria Erich J. Neuhold; ResearchStudio Digital Memory Engineering; Austria,*,2007,*
Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB); University of Vienna; Austria; September 23-27; 2007,Christoph Koch; Johannes Gehrke; Minos N Garofalakis; Divesh Srivastava; Karl Aberer; Anand Deshpande; Chee Yong Chan; Venkatesh Ganti; Carl-Christian Kanne; Wolfgang Klas; Erich J Neuhold,*,*,2007,*
Finding hierarchical heavy hitters...,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items that arrive online as streams typically have attributes which take valuesfrom one or more hierarchies (time and geographic location; source and destination IPaddresses; etc.). Providing an aggregate view of such data is important for summarization;visualization; and analysis. We develop an aggregate view based on certain organized setsof large-valued regions (“heavy hitters”) corresponding to hierarchically discountedfrequency counts. We formally define the notion of Hierarchical Heavy Hitters (HHHs). Wefirst consider computing (approximate) HHHs over a data stream drawn from a singlehierarchical attribute. We formalize the problem and give deterministic algorithms to findthem in a single pass over the input. In order to analyze a wider range of realistic datastreams (eg; from IP traffic monitoring applications); we generalize this problem to …,*,2007,*
Using SPIDER: an experience report,Nick Koudas; Amit Marathe; Divesh Srivastava,Abstract At AT&T Labs-Research; we have been developing a prototype system calledSPIDER to efficiently support flexible string matching of attribute values in large databases.SPIDER has been used in AT&T; both as a key component of an operational portal formatching customer names and addresses; and for a variety of ad hoc data quality analyses.In this talk; we report on experiences with SPIDER.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,*
Achieving the Much-desired Side-effect-free View Update,Y Kotidis; D Srivastava; Y Velegrakis,E' presente una richiesta di inserimento in ANCE di una nuova rivista; utilizza la funzione "Registracodice ANCE" per registrare il codice ricevuto dal servizio LoginMIUR o inviare una nuova richiestadi inserimento oppure cercare nuovamente la rivista. E' presente una richiesta di inserimentoin ANCE di una nuova serie; utilizza la funzione "Registra codice ANCE" per registrare il codicericevuto dal servizio LoginMIUR o inviare una nuova richiesta di inserimento oppure cercarenuovamente la serie … Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatoriper il sito CINECA non sono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuareuna rivista con i dati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN doveapplicabili e il titolo della rivista … Segnalazioni con codici 20201/20202:La pubblicazionenon è stata trasferita SOLO per i docenti segnalati nel messaggio a causa di problemi …,Hellenic Symposium on Data Management (HDMS ‘06),2006,*
ISOLATION OF ZINC RESISTANT BACTERIAL ISOLATE; IT'S ACCUMULATION AND INTERACTION WITH CELLULAR PROTEINS,K Kalia; JS Patel; DN Joshi; D Srivastava,*,Journal of Cell and Tissue Research,2006,*
In-Situ Micron Dimension Electrochemical Sensor for Detection of Fluoride Ion,DN Srivastava; P Shyam,Fluoride is widely added to drinking water systems to prevent tooth decay. Some toothpastealso contains fluoride for the same purpose. According to WHO as well as Indian standardsthe optimum fluoride levels of around 1 ppm is safe. Consumption of fluoridated waterbetween levels of 0.7 to 1.2 ppm has beneficial effects on the tooth; in that it causes lessdental caries. But level above 1.5 ppm causes dental fluorosis and it has hazardous effectson nervous; muscular; gastric; urinary and hormonal systems. On the other hand hydrogenfluoride and fluorine are very often used in various industrial processes. Both these gasesare also very toxic. So; there is a great demand for on-line sensors for these gases. Fluoridealong with their other halogen family members is usually detected by the colour change ofthe solution on addition of specific reagents [1]. But for on-line quantitative detection of …,*,2004,*
Industrial and Applications Sessions,Andreas Behm; Serge Rielau; Richard Swagerman; Conor Cunningham; Cesar Galindo-Legaria; Goetz Graefe; Walid Rjaibi; Paul Bird; Nagender Bandi; Chengyu Sun; Divyakant Agrawal; Amr El Abbadi; Sang K Cha; Changbin Song; Meikel Poess; John M Stephens Jr; Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan; Sougata Mukherjea; Bhuvan Bamba; Nick Koudas; Amit Marathe; Divesh Srivastava; Benoit Dageville; Dinesh Das; Karl Dias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin; Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; Arun Marathe; Vivek Narasayya; Manoj Syamala; Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W Warner; Vikas Arora; Susan Kotsovolos; Shankar Pal; Istvan Cseri; Oliver Seeliger; Gideon Schaller; Leo Giakoumakis; Vasili Zolotov; Ashraf Aboulnaga; Peter Haas; Mokhtar Kandil; Sam Lightstone; Guy Lohman; Volker Markl; Ivan Popivanov; Vijayshankar Raman; Marcus Fontoura; Eugene Shekita; Jason Zien; Sridhar Rajagopalan; Andreas Neumann; Bishwaranjan Bhattacharjee; Christof Bornhövd; Tao Lin; Stephan Haller; Joachim Schaper; Managing RDFI Data; Sudarshan S Chawathe; Venkat Krishnamurthyy; Sridhar Ramachandran; David Campbell; Toby Bloom; Ted Sharpe; Rakesh Nagarajan; Mushtaq Ahmed; Aditya Phatak; Raymie Stata; Patrick Hunt; William O'Connell; Ramesh Bhashyam; Roger MacNicol; Blaine French,PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS ConorCunningham; Cesar Galindo-Legaria; Goetz Graefe (Microsoft Corp.) … P*TIME: Highly ScalableOLTP DBMS for Managing Update-Intensive Stream Workload Sang K. Cha (Transact InMemory; Inc); Changbin Song (Seoul National Univ.) … Supporting Ontology-based SemanticMatching in RDBMS Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan(Oracle Corp.) … Automatic SQL Tuning in Oracle 10g Benoit Dageville; Dinesh Das; KarlDias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin (Oracle Corp.) … Database TuningAdvisor for Microsoft SQL Server 2005 Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; ArunMarathe; Vivek Narasayya; Manoj Syamala (Microsoft Corp.) … Query Rewrite for XML in OracleXML DB Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W. Warner; Vikas …,Proceedings of the Thirtieth International Conference on Very Large Data Bases: Toronto; Canada; August 31-September 3; 2004,2004,*
Optical packet access protocols for WDM networks [Book Review],A Jajszczyk,In recent years; two parallcl developments could be observed in telecommunications: first;the tremendous success of the Internet; and second; the snccess of cellular networks. TheWire-less Mobile Internel is about data transmission in cellular networks; and trends andsolutions in merging IP and cellular networks in a single global mobile network: The'authordefines a wireless mobile Internet as an integrated network that makes it possible to'useInternet and services known from cellular networks in different types of terminals and acommon network infrastructure. The book is divided into 13 chapters grouped into threeparts that present different levels of detail. The first part;“The Wireless Internet;” includes fourchapters and introduces us to problems related to wireless Internet. Chapter 1 presentsfundamental trends toward realization of wireless Internet supported by several statistics …,IEEE Communications Magazine,2003,*
Efficient OLAP Query Processing,Michael O Akinde; Michael H Böhlen; Theodore Johnson; Laks VS Lakshmanan; Divesh Srivastava,Abstract. The success of Internet applications has led to an explo-sive growth in the demandfor bandwidth from ISPs. Managing an IP network requires collecting and analyzing networkdata; such as flowlevel traffic statistics. Such analyses can typically be expressed as OLAPqueries; eg; correlated aggregate queries and data cubes. Current day OLAP tools for thistask assume the availability of the data in a centralized data warehouse. However; theinherently distributed nature of data collection and the huge amount of data extracted ateach collection point make it impractical to gather all data at a centralized site. One solutionis to maintain a distributed data warehouse; consisting of local data warehouses at eachcollection point and a coordinator site; with most of the processing being performed at thelocal sites. In this paper; we consider the problem of efficient evaluation of OLAP queries …,Advances in Database Technology-EDBT 2002: 8th International Conference on Extending Database Technology; Prague; Czech Republic; March 25-27; Proceedings,2003,*
Panel: querying networked databases,Nick Koudas; Divesh Srivastava,A large number of useful databases are currently accessible over the Web and withincorporate networks. In addition to being frequently updated; this collection of databasestends to be highly dynamic: new databases appear often; and databases (just like Websites) also disappear. In this environment; the goal of providing flexible; timely anddeclarative query access over all these databases remains elusive.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,*
Special Issue on PODS 2001,Ronald Fagin; Amnon Lotem; Moni Naor; Kim S Larsen; Dimitris Achlioptas; Noga Alon; Tova Milo; Frank Neven; Dan Suciu; Victor Vianu; Nilesh N Dalvi; Sumit K Sanghai; Prasan Roy; S Sudarshan; Paolo Ferragina; Nick Koudas; S Muthukrishnan; Divesh Srivastava; Georg Gottlob; Nicola Leone; Francesco Scarcello; Anand Rajaraman; Jeffrey D Ullman,*,*,2003,*
PIX: a system for phrase matching in XML documents: a demonstration,Sihem Amer-Yahia; Mary Fernández; Divesh Srivastava; Yu Xu,Abstract We present a system that enables flexible and efficient phrase matching in XMLdocuments. Since XML allows structured and unstructured information to be interleaved;phrase matching in XML raises new challenges. Our system; named PIX; permits phrasematching in XML documents that contain “mixed content”. A key feature of PIX is that userscan specify which element and content to ignore when matching a phrase. PIX uses invertedindices and an efficient evaluation algorithm to compute the set of matches and returnsanswers where phrases; ignored tags and content are highlighted. In addition; queryanswers are sorted using a ranking function. PIX is implemented as an extension of GALAX;a full-fledged XQuery engine. The functionality of PIX is fully integrated into XQuery andpermits a natural combination of XPath-based structure matching with phrase matching.,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING,2003,*
Introduction to special issue with best papers from EDBT 2002,Christian S Jensen,*,*,2003,*
Achlioptas; Dimitris; 671 Agarwal; Pankaj K.; 207 Alon; Noga; 688 Arge; Lars; 207 B,Ste Grumbach; Stefan Haar; Alon Halevy; Monika R Henzinger; Jon Kleinberg; Hirotada Kobayashi; Flip Korn; Nick Koudas; Kim S Larsen; Stefano Leonardi; Nicola Leone; Leonid Libkin; Philip M Long; Amnon Lotem; James F Lynch; Fre Magniez; Jean Mairesse; Michele Malgeri; Giuseppe Mangioni; Renata Mansini; Keiji Matsumoto; Pierre McKenzie; Todd Millstein; Tova Milo; S Muthukrishnan; Moni Naor; Frank Neven; Anand Rajaraman; M Reynolds; Roy Prasan; Sumit K Sanghai; Miklos Santha; Martin Sauerhoff; Francesco Scarcello; Luc Segoufin; Maria Grazia Speranza; Marc Spielmann; Divesh Srivastava; Dan Suciu; S Sudarshan; Simone Tini; Leonardo Tininini; Jeffrey D Ullman; Victor Vianu,*,Journal of Computer and System Sciences,2003,*
Reminiscences on Influential Papers,Christos Faloutsos; A Levy; P O'Neil; E Simon; D Srivastava; V Vianu; G Weikum,I continue to invite unsolicited contributions to this column. (I haven't received any so far; butthe previous issue has only been out a month or so at the time of writing.) See http://www.acm.org/sigmod/record/author.html for submission guidelines … Christos Faloutsos; Carnegie MellonUniversity; christos@cs.cmu.edu … [Manfred Schroeder. Fractals; Chaos; Power Laws: Minutesfrom an Infinite Paradise. WH Freeman and Company; 1991.] … "What was the single most influentialwork for your research?" There is a handful of truly in- fluential papers: the R-tree; RAID; the AssociationRules; each started a revolution. However; I find myself citing repeatedly this masterpiecebook. Beyond George Kingsley Zipf and his famous 'law'; and beyond the milestone book byMandelbrot on fractals; Schroeder's book explains how self-similarity and power laws appearin countless phenomena; it shows how to measure the fractal dimensions; and; it gives a …,SIGMOD RECORD,2000,*
Readings in Object-Oriented Database Systems Readings in Object-Oriented Database Systems; 1990,Hiroyuki KITAGAWA; Yoshiharu ISHIKAWA; HV JAGADISH; Laks VS LAKSHMANAN; Divesh SRIVASTAVA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,IEICE transactions on information and systems,1997,*
Semantic Data Caching and Replacement,Divesh Srivastava,*,Proceedings of the... International Conference on Very Large Data Bases,1996,*
Linear Clustering of Objects with Multiple,HV Jagadish; Tian Zhang; Miron Livny; HV Jagadish; Divesh Srivastava,*,Information Processing and Management,1995,*
Proceedings of the Post-ILPS'94 Workshop on Constraints and Databases: Nov. 17; 1994; Ithaca; NY; USA,Peter Revesz; Divesh Srivastava; Peter Stuckey,*,*,1994,*
Semantics and Optimization of Constraint Queries in Databases,Raghu Ramakrishnan; Divesh Srivastava,Semantics and Optimization of Constraint Queries in Databases Raghu Ramakrishnan* Universityof Wisconsin; Madison raghu@ cs. wisc. edu Divesh Srivastava AT&T Bell Laboratoriesdivesh@ research. att. com 1 Introduction Our research focuses on the use of constraints to representand query information in a database. We have worked on two main issues; semantics and queryoptimization. 2 Optimization of Constraint Queries Recent work (eg;[1]) seeks to increase theexpressive power of database query languages by integrating con- straint paradigms withlogic-based database query languages; such languages are referred to as constraint query languages(CQLs). In [4]; we consider the problem of optimizing a CQL program-query pair hP; Qi byprop- agating constraints occurring in P and Q. More precisely; the problem is to find a set ofconstraints for each predicate in the program such that: Adding the corresponding set of …,IEEE Data Eng. Bull.,1994,*
Representing and querying complex information in the coral deductive database system,Divesh Srivastava,Abstract The inadequacy of relational databases for new classes of applications has led toconsiderable research directed at enhancing the modeling capability of the database andthe expressive power of the query language. The development of deductive databases wasaimed at providing a declarative; potentially complete query language based on Datalog.However; Datalog lacks features such as aggregation and negation; does not support themanipulation of numeric values; and inherits the relational data model with its limitedmodeling power. Our goal in this thesis is to resolve these limitations of Datalog and todemonstrate that powerful and practical database query languages based on Datalog canbe designed and e ciently implemented. We present a Magic-sets based bottom-upevaluation technique; Ordered Search; that can be used to e ciently evaluate programs …,*,1993,*
Representing the querying complex information in the coral deductive database system(Ph. D. Thesis),DIVESH SRIVASTAVA,*,*,1993,*
Zhiyuan Chen,S Muthukrishnan; Raymond Ng; Divesh Srivastava,*,*,1993,*
R plasmid transfer of antibiotic resistance in a wastewater treatment plant,Patrick Alan Mach,*,*,1980,*
Geotagging IP Packets for Location-Aware So ware-Defined Networking in the Presence of Virtual Network Functions,Tamraparni Dasu; Yaron Kanza; Divesh Srivastava,ABSTRACT A substantial portion of the global telecommunication is based on the InternetProtocol (IP); where IP packets are routed from a source host to a destination host via acommunication network. While there is some loose connection between IP addresses andgeospatial locations; associating packets to geographic coordinates merely according to IPaddresses is hard; and o en infeasible in realtime; given the rapidity and prodigious volumeof packet tra c via routers and switches. is obstructs using geospatial information about theorigin; destination; or route of IP packets or ows. In this paper we introduce a vision ofadding geotags to IP packets; to enhance the capabilities of communication networks and oflocation-based services. We explain how the augmentation can be done exibly and eectively using two new networking technologies:(1) so ware de ned networking (SDN)—a …,*,*,*
Efficient Processing of ${\rm Top}\hbox {-} k $ Queries in Uncertain Databases with x-Relations,Ke Yi; Feifei Li; George Kollios; Divesh Srivastava,*,*,*,*
Compact Access Control Labeling for Efficient Secure XML Query Evaluation; Huaxin Zhang Ning Zhang Kenneth Salem Donghui Zhuo................................... 21-30 W...,Zhiyuan Chen; Johannes Gehrke; Flip Korn; Nick Koudas; Jayavel Shanmugasundaram; Divesh Srivastava; Guoren Wang Chuan Yang Rui Zhou,Signature-based Filtering Techniques for Structural Joins of XML Data; Huan Huo Guoren WangChuan Yang Rui Zhou .................................................................11-20 … Compact Access ControlLabeling for Efficient Secure XML Query Evaluation; Huaxin Zhang Ning Zhang Kenneth SalemDonghui Zhuo...................................21-30 … WebVigiL: MONITORING MULTIPLE WEB PAGESAND Presentation OF XML Pages; Sharavan Chamakura; Alpa Sachde; Sharma Chakravarthyand Akshaya Arora .............................................................................................................................31-40 … XML Query Optimization: .................................................................................... 41-76 Optimizationof Nested XQuery Expressions with Orderby Clauses; Song Wang; Elke A. Rundensteiner andMurali Mani.......................................................................41-50 … VAMANA- A Scalable Cost-DrivenXPath Engine; Venkatesh Raghavan; Kurt Deschler; Elke A. Rundensteiner …,*,*,*
ICDE 2016 committees,Boris Novikov; Eljas Soisalon-Soininen; Mei Hsu; Alfons Kemper; Timos Sellis; C Mohan; Stefan Manegold; Xuemin Lin; Divesh Srivastava; Giovanna Guerrini; Georgia Koutrika; Umeshwar Dayal; North America; Malu Castellanos; Sami El-Mahgary; Antoni Wolski; Anna Yarygina,General Chairs Boris Novikov (Saint Petersburg University; Russia) Eljas Soisalon-Soininen(Aalto University School of Science; Finland) … Program Committee Chairs for Research PapersMei Hsu (HP Labs; USA) Alfons Kemper (Technische Universität München; Germany) TimosSellis (Swinburne University of Technology; Australia) … Program Committee Chair for Industrialand Applications Papers C. Mohan (IBM Almaden Research Center; USA) … Program CommitteeChair for Demonstrations Stefan Manegold (CWI; the Netherlands) … TKDE Posters Chair XueminLin (University of New South Wales; Australia) … Panel Chair Divesh Srivastava (AT&TLabs-Research; USA) … Workshop and Tutorial Chairs Giovanna Guerrini (University ofGenova; Italy) Georgia Koutrika (HP Labs; USA) … IEEE CS Liaison Umeshwar Dayal (HitachiAmerica Ltd; USA) … Sponsorship Chair North America: Malu Castellanos (HP Labs; USA),*,*,*
String...,Luis Gravano; Panagiotis G Ipeirotis; HV Jagadish; Nick Koudas; S Muthukrishnan; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data especially formore complex queries involving joins. This is due; for example; to the prevalence oftypographical errors in data; and multiple conventions for recording attributes such as nameand address. Commercial databases do not support approximate string joins directly; and itis a challenge to implement this functionality efficiently with user-defined functions (UDFs). Inthis paper; we develop a technique for building approximate string join capabilities on top ofcommercial databases by exploiting facilities already available in them. At the core; ourtechnique relies on matching short substrings of length; called-grams; and taking intoaccount both positions of individual matches and the total number of such matches. Our …,*,*,*
Workshop Organization,Alberto HF Laender; Juliana Freire; Dan Suciu; Mirella M Moro; Vanessa Braganholo; Clodoveu Davis Jr; Marcos André Gonçalves; Francesco Bonchi; Angela Bonifati; Andrea Calì; Sara Cohen; Isabel Cruz; Wolfgang Gatterbauer; Boris Glavic; Claudio Gutierrez; Solmaz Kolahi; Dongwon Lee; Domenico Lembo; Marta Mattoso; Regina Motz; Frank Neven; Rachel Pottinger; Vibhor Rastogi; Altigran S da Silva; Cristina Sirangelo; Divesh Srivastava; Julia Stoyanovich; David Toman; Alejandro Vaisman; Stijn Vansummeren; Ke Yi; Daniel Oliveira,The Alberto Mendelzon International Workshop on Foundations of Data Management (AMW2012) held in Ouro Preto; Brazil; on June 27-30; 2012; is the sixth workshop of a serieswhich started in 2006; as part on an initiative of the Latin American community ofresearchers in data management to honor the memory of our friend; colleague and mentorAlberto Mendelzon. The AMW series has been a venue for high-quality research onfoundational aspects of data management and it has helped foster and solidify the researchin this area throughout Latin America. This event; as the previous ones; has encouraged theparticipation of Latin American graduate students and includes activities specially designedfor them. In addition; with sponsorship from the VLDB Endowment; travel grants have beenprovided for students to attend the event. The proceedings of the workshop consist of 14 …,*,*,*
Flexible String Matching Against Large Databases in Practice,Divesh Srivastava,Base (tid; sva;...) insert into BaseIDF (token; idf) BaseSize (size) select T. token; LOG (S.size)-LOG (COUNT (T. tid)) BaseTF (tid; token; tf) from BaseTF T; BaseSize S group by T.token insert into BaseLength (tid; len) insert into BaseWeights (tid; token; weight) select T.tid; SQRT (SUM (I. idf∗ I. idf∗ T. tf∗ T. tf)) select T. tid; T. token; T. tf∗ I. idf/L. len from BaseTFT; BaseIDF I from BaseTF T; BaseIDF I; BaseLength L where T. token= I. token where T.token= I. token and T. tid= L. tid group by T. tid,*,*,*
Data Auditor: Analyzing Data Quality Using Pattern Tableaux,Lukasz Golab; Howard Karloff; Flip Korn; Divesh Srivastava,Example: Config Table; FD tid interface router date ip type 1 interface1 routerB 09/11/0910.30. 15.10 0 2 interface1 routerB 09/11/09 10.30. 15.10 0 3 interface1 routerB 09/11/0910.30. 15.10 5 4 interface2 routerB 09/11/09 10.30. 15.25 0 5 interface2 routerB 09/11/0910.30. 15.25 0 6 interface3 routerB 11/11/09 10.30. 15.30 4 7 interface3 routerB 11/11/0910.30. 15.40 4,*,*,*
2007 IEEE 23rd International Conference on Data Engineering,Wei-Shinn Ku; Roger Zimmermann; Haixun Wang,We describe DSPHERE a decentralized system for crawling; indexing; searching andranking of documents in the World Wide Web. Unlike most of the existing searchtechnologies that depend heavily on a page-centric view of the Web; we advocate a source-centric view of the Web and propose a decentralized architecture for crawling; indexing andsearching the Web in a distributed source-specific fashion...,*,*,*
Linking Business Listings to Identify Chains,Pei Li; Xin Luna Dong; Songtao Guo; Andrea Maurino; Divesh Srivastava,ABSTRACT Many local search engines; such as Google Maps; Yahoo! Local; Yellowpages;can benefit from the knowledge of business chains to improve the search experiences.However; business chains are rarely specified explicitly in business-listing data; but need tobe inferred from the listings. While records that belong to the same chain typically sharesome similar values such as names and URL domains; they can also vary a lot; providingdifferent addresses; different local phone numbers; etc. Traditional record-linage techniques;which often require high similarity between records for linking them; can fall short in thiscontext. This paper studies identifying business listings that belong to the same chain. Oursolution contains two stages: the first stage identifies cores containing business listings thatare very likely to belong to the same chain; the second stage collects strong evidence …,*,*,*
Data Engineering,Jens Bleiholder; Melanie Herschel; Felix Naumann; Lukasz Golab; Flip Korn; Divesh Srivastava; Arvind Arasu; Surajit Chaudhuri; Zhimin Chen; Kris Ganjam; Raghav Kaushik; Vivek Narasayya; Doug Burdick; Mauricio Hernandez; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana R Stanoi; Shivakumar Vaithyanathan; Sanjiv Das,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
TAX: A Tree Algebra for XML НV Jagadish University of Michigan jag@ eecs. umich. edu,Divesh Srivastava; Keith Thompson,Abstract Querying XML has been the subject of much recent investigation. A formal bulkalgebra is essential for applying database-style optimization to XML queries. We developsuch an algebra; called TAX (Tree Algebra for XML); for manipulating XML data; modeled asforests of labeled ordered trees. Motivated both by aesthetic considerations of intuitiveness;and by eÌcient computability and amenability to optimization; we develop TAX as a naturalextension of relational algebra; with a small set of operators. TAX is complete for relationalalgebra extended with aggregation; and can express most queries expressible in popularXML query languages. It forms the basis for the Timber XML database system currentlyunder development by us.,*,*,*
Compressed Accessibility Map: E cient Access Control for XML,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Ting Yu,*,*,*,*
Program Vice-Chairs,Elisa Bertino; Divesh Srivastava; Surajit Chaudhuri; Jiawei Han; Bernhard Mitschang; David Lomet; Keith Jeffery; Raghu Ramakrishnan; Alberto Mendelzon; Hongjun Lu; Amr El Abbadi; Yannis Ioannidis; Karl Aberer; Charu C Aggarwal; Divy Agrawal; Demet Aksoy; Sihem Amer-Yahia; Paolo Atzeni; Daniel Barbara; Roger Barga; Alfonso Cardenas; Barbara Catania; Sang K Cha; Soumen Chakrabarti; Sharma Chakravarthy; Edward Chang; Kevin Chang; Qiming Chen; David W Cheung; Stavros Christodoulakis; Lois Delcambre; Stefan Dessloch; Max Egenhofer; Ahmed Elmagarmid; Martin Ester; Georgios Evangelidis; Mary Fernandez; Daniela Florescu; Juliana Freire; Christoph Freytag,*,*,*,*
The CORAL Deductive Database System,Raghu Ramakrishnany; William G Roth; Praveen Seshadri; Divesh Srivastava; S Sudarshanz,CORAL 4; 5] is a deductive database system that supports a powerful declarative querylanguage. The language supports general Horn clause logic programs; extended with SQL-style grouping; set-generation; and negation. Programs can be organized intoindependently optimized modules; and users can provide optimization hints in the form ofhigh-level annotations. The system supports a wide variety of optimization techniques. Thereis an interface to C++ that enables programs to be written in a combination of imperative anddeclarative styles; C++ code can be called from declarative programs; and vice versa. Anotable feature of the CORAL system is that it is extensible. In particular; new data types canbe de ned; and new relation and index implementations can be added. An interface to theEXODUS storage manager 2] using a client-server architecture provides support for disk …,*,*,*
Compressed Accessibility Map: E cient Access Control for Hierarchical Data,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,*,*,*,*
Luis Gravano Panagiotis G. Ipeirotis HV Jagadish Columbia University Columbia University University of Michigan,Nick Koudas; S Muthukrishnan; Lauri Pietarinen; Divesh Srivastava,*,*,*,*
Flexible String Matching Against Large Databases in Practice (Paper# 709),Nick Koudas; Amit Marathe; Divesh Srivastava,*,*,*,*
Reverse Nearest Neighbor Aggregates,Flip Korn; S Muthukrishnan; Divesh Srivastava,*,*,*,*
The Data arehouse of $ ewsgroups,Himanshu GuptaT; Divesh Srivastava,*,*,*,*
Shurug Al-Khalifa,Jignesh M Patel; Divesh Srivastava; Yuqing Wu,*,*,*,*
Metadata= Data+ Queries,Yannis Velegrakis; Divesh Srivastava,*,*,*,*
XTreeNet: A Scalable Unified Overlay Network for XML Content Access and Distribution,Michael Rabinovich; KK Ramakrishnan; Divesh Srivastava; Yin Zhang,*,*,*,*
A Visual Language for Interactive Data Exploration,Peter G Selfridge; Divesh Srivastava,Analyzing vast amounts of data to extract\knowledge" is now a business imperative.Typically; attention is focused on learning and data mining algorithms (see; eg; AIS93;PS91]) that provide the core capability of generalizing from large numbers of speci c facts touseful high-level rules. However; real-world data analysis tasks can be extremely complex;and focusing attention on autonomous approaches (eg; rule induction) tends tounderemphasize the key role played by the human data analyst in all current day dataanalysis BA94]. Current day data analysts use a variety of statistical and other tools foranalyzing business data. However; the task of the data analyst very often is complicated bythe fact that she does not know what subset or view of the data is relevant to the task athand. Determining this necessitates a prior step of data exploration; but the task of data …,*,*,*
ACM SIGMOD Conference; June 1996; pp. 447-458,Kenneth A Ross; Divesh Srivastava; S Sudarshan,*,*,*,*
Optimizing Away Joins on Data Streams,Nick Koudas; Divesh Srivastava; David Toman,Abstract. Monitoring aggregates on IP traffic data streams is a compelling application fordata stream management systems. Often; such streaming aggregation queries involvejoining multiple streams (eg; streams of SYN and ACK packets) using temporal joinconditions (eg; within 5 seconds); followed by computation of aggregates (eg; COUNT) overtemporal tumbling windows (eg; every 5 minutes). While such a query expression is natural;its evaluation over high speed IP traffic data streams is infeasible in practice. In this paper;we develop rewriting techniques for streaming aggregation queries that identify conditionsunder which such joins can be optimized away; while providing error bounds for results ofthe rewritten queries. The basis of the optimization is a powerful but decidable theory inwhich constraints over data streams can be formulated. The result error bounds are …,*,*,*
Approximate Substring Indexing,HV Jagadish; Nick Koudas; S Muthukrishnan; Divesh Srivastava,*,*,*,*
Structural Joins: A Primitive for Efficient XML Query Pattern Matching Shurug Al-Khalifa Univ of Michigan shurug@ eecs. umich. edu,Jignesh M Patel; Divesh Srivastava; Yuqing Wu,Abstract XML queries typically specify patterns of selection predicates on multiple elementsthat have some specified tree structured relationships. The primitive tree structuredrelationships are parent-child and ancestor-descendant; and finding all occurrences of thesestructural relationships in an XML database is a core operation for XML query processing. Inthis paper; we develop two families of structural join algorithms for this task: tree-merge andstack-tree. The tree-merge algorithms are a natural extension of traditional merge joins andthe recently proposed multi-predicate merge joins; while the stack-tree algorithms have nocounterpart in traditional relationaljoin processing. We present experimental results on arange of data and queries using (i) the TimbER native XML query engine built on top ofSHOR E; and (ii) a commercial relational database system. In all cases; our structural join …,*,*,*
Constraint databases and applications(Delphi; January 11-12; 1997; Cambridge MA; August 19; 1996),V Gaede; A Brodsky; O Günther; D Srivastava; V Vianu; M Wallace,*,Lecture notes in computer science,*,*
Alon Y. Levy,Divesh Srivastava,*,*,*,*
Com pressed Accessibility M ap,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,*,*,*,*
Reasoning with Aggregation Constraints in Views,Shaul Dar; HV Jagadish; Alon Y Levy; Divesh Srivastava,Abstract We investigate the problem of using materialized views to compute answers to SQLqueries with grouping and aggregation; in the presence of multiset tables. This problem isimportant in many applications; such as data warehousing; mobile computing; globalinformation systems; and maintaining physical data independence; where access to local orcached materialized views may be cheaper than access to the underlying database. Inaddition; this problem has obvious potential in optimizing query evaluation. The problem isformally stated as nding a rewriting of an SQL query Q where the materialized views occur inthe FROM clause; and the rewritten query is multiset-equivalent to Q. First; we study the casewhere the query has grouping and aggregation but the views do not; and show that usabilityof a view in evaluating a query essentially requires an isomorphism between the view …,*,*,*
Multicast Architecture with Adaptive Dual-state,Tae Won Cho; Michael Rabinovich; KK Ramakrishnan; Divesh Srivastava; Yin Zhang,ABSTRACT Multicast is an approach that uses network and server resources efficiently todistribute information to groups. As networks evolve to become information-centric; users willincreasingly demand publish-subscribe based access to finegrained information; andmulticast will need to evolve to (i) manage an increasing number of groups; with a distinctgroup for each piece of distributable content;(ii) support persistent group membership; asgroup activity can vary over time; with intense activity at some times; and infrequent (but stillcritical) activity at others. These requirements raise scalability challenges that are not met bytoday's multicast techniques. In this paper; we propose the MAD (Multicast with AdaptiveDual-state) architecture that can scalably support a vast number of multicast groups; withvarying activity over time; based on two key novel ideas:(i) decouple group membership …,*,*,*
Content Distribution,W Fenner; M Rabinovich; K Ramakrishnan; D Srivastava; Y Zhang,XTreeNet: Scalable Overlay Networks for XML Content Dissemination and Querying........................... 41 W. Fenner; M. Rabinovich; K. Ramakrishnan; D. Srivastava; and Y. Zhang A Two-levelSemantic Caching Scheme for Super-peer Networks............................................................ 47 P.Garbacki; D. Epema; and M. van Steen DOMProxy: Enabling Dynamic-content Front-end WebCaching … Flexible Media Reflection for Collaborative StreamingScenarios ............................................................ 71 V. Kahmann; J. Brandt; and L. Wolf A Novel TwoTiered Proxy Caching Scheme for Video on Demand Applications ................................... 77 A.Nayyeri; M. Hashemi; and N. Yazdani Understanding the Impact of Diverse Streaming Workloadson End-user Quality of Service................... 83 M. Spasojevic; N. Bhatti; S. Roy; and L. Kontothanassis… Predictability of Web-server Traffic Congestion …,*,*,*
SWAE'08 Program Committee,Sonia Bergamaschi; Francesco Guerra; Yannis Velegrakis; Francesco Bellomi; Omar Boucelma; Paolo Bouquet; Christoph Bussler; Jorge Cardoso; Oscar Corcho; Matteo Cristani; Isabel Cruz; Alfio Ferrara; Mohand-Said Hacid; Mustafa Jarrar; Alain Leger; Andrea Maurino; Lyndon Nixon; Aris M Ouksel; Lucian Popa; Christoph Quix; Peter Spyns; Divesh Srivastava; Armando Stellato; Zografoula Vagena,Chairs Sonia Bergamaschi; University of Modena and Reggio Emilia Francesco Guerra; Universityof Modena and Reggio Emilia Yannis Velegrakis; University of Trento … Francesco Bellomi;Università di Verona; Italy Omar Boucelma; Université Aix-Marseille; France Paolo Bouquet;Università di Trento; Italia Christoph Bussler; Merced Systems; USA Jorge Cardoso; Universityof Madeira; Portugal Oscar Corcho; University of Manchester; UK Matteo Cristani; Universityof Verona ; Italy Isabel Cruz; University of Illinois at Chicago; USA Alfio Ferrara; Università degliStudi di Milano; Italy Mohand-Said Hacid; Universite Claude Bernard Lyon 1; France MustafaJarrar; University of Cyprus; Cyprus Alain Leger; France Telecom R&D; France AndreaMaurino; Università degli Studi di Milano Bicocca; Italy Lyndon Nixon; University of Berlin; GermanyAris M. Ouksel; University of Illinois at Chicago; USA Lucian Popa; IBM Almaden …,*,*,*
Linear Clustering of Objects with Multiple Atributes.</title,HV Jagadish; Tian Zhang; Raghu Ramakrishnan; Miron Livny; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Keith Thompson,*,*,*,*
Towards E cient Information Gathering Agents,Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,Abstract Information gathering agents are required in many software agent applications toanswer queries; posed by other agents; using a variety of available information sources. Weformally consider the problem of designing information gathering agents; and make twoimportant contributions. First; we examine the key issue of integrating knowledge fromexternal sites into our knowledge base; and present an expressive language for thispurpose. A noteworthy feature of our language is its ability to capture the knowledge thatsome external sites have complete information of a certain kind; using rich semanticconstraints. Given a query on the knowledge base; it is important for the agent to rstdetermine the set of external sites that contain information relevant to answering the query;and then access those sites. Our second contribution is to show that; given a query and …,*,*,*
Guy Jacobson,Divesh Srivastava; Dan Suciu,*,*,*,*
Bulletin of the Technical Committee on,Gonzalo Navarro; Ricardo Baeza-Yates; Erkki Sutinen; Jorma Tarhio; Panagiotis G Ipeirotis; HV Jagadish; Nick Koudas; S Muthukrishnan; Lauri Pietarinen; Divesh Srivastava; Sriram Raghavan; Héctor Garcıa-Molina,Bulletin of the Technical Committee on Ø Ò Ò Ö Ò December 2001 Vol. 24 No. 4 IEEE ComputerSociety Letters Letter from the Editor-in-Chief...................................................... David Lomet 1 NewTCDE Chair for 2002-2003........................ Paul Larson; Masaru Kitsuregawa; Betty Salzberg 2Letter from the Special Issue Editor.................................................. Luis Gravano 2 Special Issueon Text and Databases DB2 Optimization in Support of Full Text … Editorial BoardEditor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 Redmond WA98052-6399 lomet@ microsoft. com Associate Editors Luis Gravano Computer Science DepartmentColumbia University 1214 Amsterdam Avenue New York; NY 10027 Alon Halevy University ofWashington Computer Science and Engineering Dept. Sieg Hall; Room 310 Seattle; WA 98195Sunita Sarawagi School of Information Technology Indian Institute of Technology …,*,*,*
Stelios Paparizos1; Shurug Al-Khalifa1; HV Jagadish1; Laks Lakshmanan2,Andrew Nierman; Divesh Srivastava; Yuqing Wu,*,*,*,*
Data Engineering,Raghu Ramakrishnan; Divesh Srivastava; Gustavo Alonso; Amr El Abbadi; Sudarshan S Chawathe; Hector Garcia-Molina; Jennifer Widom,Bulletin of the Technical Committee on Data Engineering Date of Issue Vol. N No. m IEEE ComputerSociety Letters Special Issue on Database Constraint Management Letter from the Special IssueEditor............................................ Jennifer Widom 2 Constraint Management in Chimera.............. Stefano Ceri; Piero Fraternali; Stefano Paraboschi 4 Integrity Control in Advanced DatabaseSystems.. Paul WPJ Grefen; Rolf A. de By; Peter MG Apers 9 Semantics and Optimization of ConstraintQueries in Databases …,Urbana,*,*
