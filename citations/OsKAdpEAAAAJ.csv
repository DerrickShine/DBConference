Robust sentiment detection on twitter from biased and noisy data,Luciano Barbosa; Junlan Feng,Abstract In this paper; we propose an approach to automatically detect sentiments on Twittermessages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages. Moreover; we leverage sources ofnoisy labels as our training data. These noisy labels were provided by a few sentimentdetection websites over twitter data. In our experiments; we show that since our features areable to capture a more abstract representation of tweets; our solution is more effective thanprevious ones and also more robust regarding biased and noisy data; which is the kind ofdata provided by these sources.,Proceedings of the 23rd International Conference on Computational Linguistics: Posters,2010,776
An adaptive crawler for locating hidden-web entry points,Luciano Barbosa; Juliana Freire,Abstract In this paper we describe new adaptive crawling strategies to efficiently locate theentry points to hidden-Web sources. The fact that hidden-Web sources are very sparselydistributedmakes the problem of locating them especially challenging. We deal with thisproblem by using the contents ofpages to focus the crawl on a topic; by prioritizingpromisinglinks within the topic; and by also following links that may not lead to immediatebenefit. We propose a new frameworkwhereby crawlers automatically learn patterns ofpromisinglinks and adapt their focus as the crawl progresses; thus greatly reducing theamount of required manual setup andtuning. Our experiments over real Web pages in arepresentativeset of domains indicate that online learning leadsto significant gains inharvest rates' the adaptive crawlers retrieve up to three times as many forms as crawlers …,Proceedings of the 16th international conference on World Wide Web,2007,209
Siphoning hidden-web data through keyword-based interfaces,Luciano Barbosa; Juliana Freire,*,*,2004,204
Searching for hidden-web databases,Luciano Barbosa; Juliana Freire,ABSTRACT Recently; there has been increased interest in the retrieval and integration ofhidden-Web data with a view to leverage high-quality information available in onlinedatabases. Although previous works have addressed many aspects of the actual integration;including matching form schemata and automatically filling out forms; the problem of locatingrelevant data sources has been largely overlooked. Given the dynamic nature of the Web;where data sources are constantly changing; it is crucial to automatically discover theseresources. However; considering the number of documents on the Web (Google alreadyindexes over 8 billion documents); automatically finding tens; hundreds or even thousandsof forms that are relevant to the integration task is really like looking for a few needles in ahaystack. Besides; since the vocabulary and structure of forms for a given domain are …,Proceedings of WebDB,2005,173
Combining classifiers to identify online databases,Luciano Barbosa; Juliana Freire,Abstract We address the problem of identifying the domain of onlinedatabases. Moreprecisely; given a set F of Web forms automaticallygathered by a focused crawler and anonline databasedomain D; our goal is to select from F only the formsthat are entry points todatabases in D. Having a set ofWebforms that serve as entry points to similar onlinedatabasesis a requirement for many applications and techniques thataim to extract andintegrate hidden-Web information; suchas meta-searchers; online database directories;hidden-Webcrawlers; and form-schema matching and merging. We propose a new strategythat automatically and accuratelyclassifies online databases based on features that canbeeasily extracted from Web forms. By judiciously partitioningthe space of form features; thisstrategy allows theuse of simpler classifiers that can be constructed using …,Proceedings of the 16th international conference on World Wide Web,2007,109
An exploratory study of information retrieval techniques in domain analysis,Vander Alves; Christa Schwanninger; Luciano Barbosa; Awais Rashid; Peter Sawyer; Paul Rayson; Christoph Pohl; Andreas Rummler,Domain analysis involves not only looking at standard requirements documents (eg; usecase specifications) but also at customer information packs; market analyses; etc. Lookingacross all these documents and deriving; in a practical and scalable way; a feature modelthat is comprised of coherent abstractions is a fundamental and non-trivial challenge. Weconduct an exploratory study to investigate the suitability of Information Retrieval (IR)techniques for scalable identification of commonalities and variabilities in requirementspecifications for software product lines. Accordingly; based on observations derived fromindustrial experience and on state-of-the-art research and practice; we also propose aninitial framework; leveraging IR to systematically abstract requirements from existingspecifications of a given domain into a feature model. We evaluate this framework …,Software Product Line Conference; 2008. SPLC'08. 12th International,2008,96
Organizing hidden-web databases by clustering visible web documents,Luciano Barbosa; Juliana Freire; Altigran Silva,In this paper we address the problem of organizing hidden-Web databases. Given aheterogeneous set of Web forms that serve as entry points to hidden-Web databases; ourgoal is to cluster the forms according to the database domains to which they belong. Wepropose a new clustering approach that models Web forms as a set of hyperlinked objectsand considers visible information in the form context-both within and in the neighborhood offorms-as the basis for similarity comparison. Since the clustering is performed over featuresthat can be automatically extracted; the process is scalable. In addition; because it uses arich set of metadata; our approach is able to handle a wide range of forms; including content-rich forms that contain multiple attributes; as well as simple keyword-based searchinterfaces. An experimental evaluation over real Web data shows that our strategy …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,78
System and method for building diverse language models,*,Disclosed herein are systems; methods; and non-transitory computer-readable storagemedia for collecting web data in order to create diverse language models. A systemconfigured to practice the method first crawls; such as via a crawler operating on acomputing device; a set of documents in a network of interconnected devices according to avisitation policy; wherein the visitation policy is configured to focus on novelty regions for acurrent language model built from previous crawling cycles by crawling documents whosevocabulary considered likely to fill gaps in the current language model. A language modelfrom a previous cycle can be used to guide the creation of a language model in the followingcycle. The novelty regions can include documents with high perplexity values over thecurrent language model.,*,2017,33
Learning Hybrid Representations to Retrieve Semantically Equivalent Questions,Cıcero dos Santos; Luciano Barbosa; Dasha Bogdanova; Bianca Zadrozny,Abstract Retrieving similar questions in online Q&A community sites is a difficult taskbecause different users may formulate the same question in a variety of ways; using differentvocabulary and structure. In this work; we propose a new neural network architecture toperform the task of semantically equivalent question retrieval. The proposed architecture;which we call BOW-CNN; combines a bag-ofwords (BOW) representation with a distributedvector representation created by a convolutional neural network (CNN). We performexperiments using data collected from two Stack Exchange communities. Our experimentalresults evidence that:(1) BOW-CNN is more effective than BOW based information retrievalmethods such as TFIDF;(2) BOW-CNN is more robust than the pure CNN for long texts.,ACL (Short Papers),2015,32
Method and system for clustering identified forms,*,A method is provided for organizing a plurality of documents that include forms. An initial setof clusters is defined for the plurality of documents. The initial set of clusters is reclusteredbased on similarity values calculated in multiple feature spaces. For example; a first featurespace may be associated with a content of a document while a second feature space maybe associated with a content of a form associated with the document. Each cluster has anassociated centroid vector in each feature space that is used to represent the cluster. Thesimilarity between the document and each cluster is calculated in both feature spaces. Eachdocument is assigned to the cluster whose centroid is most similar. The cluster centroidsmay be recalculated and the process repeated until the cluster assignments become stable.,*,2011,30
Structured open urban data: understanding the landscape,Luciano Barbosa; Kien Pham; Claudio Silva; Marcos R Vieira; Juliana Freire,Abstract A growing number of cities are now making urban data freely available to thepublic. Besides promoting transparency; these data can have a transformative effect insocial science research as well as in how citizens participate in governance. Theseinitiatives; however; are fairly recent and the landscape of open urban data is not wellknown. In this study; we try to shed some light on this through a detailed study of over 9;000open data sets from 20 cities in North America. We start by presenting general statisticsabout the content; size; nature; and popularity of the different data sets; and then examine inmore detail structured data sets that contain tabular data. Since a key benefit of having alarge number of data sets available is the ability to fuse information; we investigateopportunities for data integration. We also study data quality issues and time-related …,*,2014,26
Dexter: large-scale discovery and extraction of product specifications on the web,Disheng Qiu; Luciano Barbosa; Xin Luna Dong; Yanyan Shen; Divesh Srivastava,Abstract The web is a rich resource of structured data. There has been an increasing interestin using web structured data for many applications such as data integration; web search andquestion answering. In this paper; we present Dexter; a system to find product sites on theweb; and detect and extract product specifications from them. Since product specificationsexist in multiple product sites; our focused crawler relies on search queries and backlinks todiscover product sites. To perform the detection; and handle the high diversity ofspecifications in terms of content; size and format; our system uses supervised learning toclassify HTML fragments (eg; tables and lists) present in web pages as specifications or not.To perform large-scale extraction of the attribute-value pairs from the HTML fragmentsidentified by the specification detector; D exter adopts two lightweight strategies: a …,Proceedings of the VLDB Endowment,2015,23
Detecting semantically equivalent questions in online user forums,Dasha Bogdanova; Cicero dos Santos; Luciano Barbosa; Bianca Zadrozny,Abstract Two questions asking the same thing could be too different in terms of vocabularyand syntactic structure; which makes identifying their semantic equivalence challenging.This study aims to detect semantically equivalent questions in online user forums. Weperform an extensive number of experiments using data from two different Stack Exchangeforums. We compare standard machine learning methods such as Support Vector Machines(SVM) with a convolutional neural network (CNN). The proposed CNN generates distributedvector representations for pairs of questions and scores them using a similarity metric. Weevaluate in-domain word embeddings versus the ones trained with Wikipedia; estimate theimpact of the training set size; and evaluate some aspects of domain adaptation. Ourexperimental results show that the convolutional neural network with in-domain word …,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,2015,21
Siphon++: a hidden-webcrawler for keyword-based interfaces,Karane Vieira; Luciano Barbosa; Juliana Freire; Altigran Silva,Abstract The hidden Web consists of data that is generally hidden behind form interfaces;and as such; it is out of reach for traditional search engines. With the goal of leveraging thehigh-quality information in this largely unexplored portion of the Web; in this paper; wepropose a new strategy for automatically retrieving data hidden behind keyword-based forminterfaces. Unlike previous approaches to this problem; our strategy adapts the querygeneration and selection by detecting features of the index. We describe an extensiveexperimental evaluation which shows that: our strategy is able to derive appropriate queriesto obtain high coverage while; at the same time; avoiding the retrieval of redundant data;and it obtains higher coverage and is more efficient approaches that use a fixed strategy forquery generation.,Proceeding of the 17th ACM conference on Information and knowledge management,2008,21
Creating and exploring web form repositories,Luciano Barbosa; Hoa Nguyen; Thanh Nguyen; Ramesh Pinnamaneni; Juliana Freire,Abstract We present DeepPeep (http://www. deeppeep. org); a new system for discovering;organizing and analyzing Web forms. DeepPeep allows users to explore the entry points tohidden-Web sites whose contents are out of reach for traditional search engines. Besidesdemonstrating important features of DeepPeep and describing the infrastructure we used tobuild the system; we will show how this infrastructure can be used to create form collectionsand form search engines for different domains. We also present the analysis component ofDeepPeep which allows users to explore and visualize information in form repositories;helping them not only to better search and understand forms in different domains; but also torefine the form gathering process.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,19
Method and system for quantifying the quality of search results based on cohesion,*,A method and system for quantifying the quality of search results from a search enginebased on cohesion. The method and system include modeling a set of search engine searchresults as a cluster and measuring the cohesion of the cluster. In an embodiment; thecohesion of the cluster is the average similarity between the cluster elements to a centroidvector. The centroid vector is the average of the weights of the vectors of the cluster. Thesimilarity between the centroid vector and the cluster's elements is the cosine similaritymeasure. Each document in the set of search results is represented by a vector where eachcell of the vector represents a stemmed word. Each cell has a cell value which is thefrequency of the corresponding stemmed word in a document multiplied by a weight thattakes into account the location of the stemmed word within the document.,*,2010,19
Looking at both the present and the past to efficiently update replicas of web content,Luciano Barbosa; Ana Carolina Salgado; Francisco De Carvalho; Jacques Robin; Juliana Freire,Abstract Since Web sites are autonomous and independently updated; applications thatkeep replicas of Web data; such as Web warehouses and search engines; must periodicallypoll the sites and check for changes. Since this is a resource-intensive task; in order to keepthe copies up-to-date; it is important to devise efficient update schedules that adapt to thechange rate of the pages and avoid visiting pages not modified since the last visit. In thispaper; we propose a new approach that learns to predict the change behavior of Web pagesbased both on the static features and change history of pages; and refreshes the copiesaccordingly. Experiments using real-world data show that our technique leads to substantialperformance improvements compared to previously proposed approaches.,Proceedings of the 7th annual ACM international workshop on Web information and data management,2005,17
Semantic Traffic Diagnosis with STAR-CITY: Architecture and Lessons Learned from Deployment in Dublin; Bologna; Miami and Rio,Freddy Lecue; Robert Tucker; Simone Tallevi-Diotallevi; Rahul Nair; Yiannis Gkoufas; Giuseppe Liguori; Mauro Borioni; Alexandre Rademaker; Luciano Barbosa,Abstract IBM STAR-CITY is a system supporting S emantic road T raffic A na-lytics and Reasoning for CITY. The system has ben designed (i) to provide insight on historical and real-time traffic conditions; and (ii) to support efficient urban planning by integrating (human andmachine-based) sensor data using variety of formats; velocities and volumes. Initiallydeployed and experimented in Dublin City (Ireland); the system and its architecture havebeen strongly limited by its flexibility and scalability to other cities. This paper describes itslimitations and presents the “any-city” architecture of STAR-CITY together with its semanticconfiguration for flexible and scalable deployment in any city. This paper also stronglyfocuses on lessons learnt from its deployment and experimentation in Dublin (Ireland);Bologna (Italy); Miami (USA) and Rio (Brazil).,International Semantic Web Conference,2014,16
For a few dollars less: Identifying review pages sans human labels,Luciano Barbosa; Ravi Kumar; Bo Pang; Andrew Tomkins,Abstract We address the problem of large-scale automatic detection of online reviewswithout using any human labels. We propose an efficient method that combines two basicideas: Building a classifier from a large number of noisy examples and using the structure ofthe website to enhance the performance of this classifier. Experiments suggest that ourmethod is competitive against supervised learning methods that mandate expensive humaneffort.,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,2009,13
Automatically constructing a directory of molecular biology databases,Luciano Barbosa; Sumit Tandon; Juliana Freire,Abstract There has been an explosion in the volume of biology-related information that isavailable in online databases. But finding the right information can be challenging. Not onlyis this information spread over multiple sources; but often; it is hidden behind form interfacesof online databases. There are several ongoing efforts that aim to simplify the process offinding; integrating and exploring these data. However; existing approaches are notscalable; and require substantial manual input. Notable examples include the NCBIdatabases and the NAR database compilation. As an important step towards a scalablesolution to this problem; we describe a new infrastructure that automates; to a large extent;the process of locating and organizing online databases. We show how this infrastructurecan be used to automate the construction and maintenance of a Molecular Biology …,Data Integration in the Life Sciences,2007,11
Method and apparatus for detecting a sentiment of short messages,*,A method; computer readable medium and apparatus for detecting a sentiment for a shortmessage are disclosed. For example; the method receives the short message; and obtainsan abstraction of the short message. The method then determines the sentiment of the shortmessage based upon the abstraction.,*,2017,9
System and method for locating bilingual web sites,*,Disclosed herein are systems; methods; and non-transitory computer-readable storagemedia for bootstrapping a language translation system. A system configured to practice themethod performs a bidirectional web crawl to identify a bilingual website. The systemanalyzes data on the bilingual website to make a classification decision about whether theroot of the bilingual website is an entry point for the bilingual website. The bilingual site cancontain pairs of parallel pages. Each pair can include a first website in a first language and asecond website in a second language; and a first portion of the first web page correspondsto a second portion of the second web page. Then the system analyzes the first and secondweb pages to identify corresponding information pairs in the first and second languages;and extracts the corresponding information pairs from the first and second web pages for …,*,2016,8
A scalable approach to building a parallel corpus from the Web,Vivek Kumar Rangarajan Sridhar; Luciano Barbosa; Srinivas Bangalore,Abstract Parallel text acquisition from the Web is an attractive way for augmenting statisticalmodels (eg; machine translation; crosslingual document retrieval) with domainrepresentative data. The basis for obtaining such data is a collection of pairs of bilingualWeb sites or pages. In this work; we propose a crawling strategy that locates bilingual Websites by constraining the visitation policy of the crawler to the graph neighborhood ofbilingual sites on the Web. Subsequently; we use a novel recursive mining technique thatrecursively extracts text and links from the collection of bilingual Web sites obtained from thecrawling. Our method does not suffer from the computationally prohibitive combinatorialmatching typically used in previous work that uses document retrieval techniques to match acollection of bilingual webpages. We demonstrate the efficacy of our approach in the …,Twelfth Annual Conference of the International Speech Communication Association,2011,8
Bus Travel Time Predictions Using Additive Models,Matthias Kormaksson; Luciano Barbosa; Marcos Vieira; Bianca Zadrozny,Many factors can affect the predictability of public bus services such as traffic; weather; dayof week; and hour of day. However; the exact nature of such relationships between traveltimes and predictor variables is; in most situations; not known. In this paper we develop aframework that allows for flexible modeling of bus travel times through the use of AdditiveModels. The proposed class of models provides a principled statistical framework that ishighly flexible in terms of model building. The experimental results demonstrate uniformlysuperior performance of our best model as compared to previous prediction methods whenapplied to a very large GPS data set obtained from buses operating in the city of Rio deJaneiro.,ICDM,2014,7
Vistradas: Visual Analytics for Urban Trajectory Data,Luciano Barbosa; Matthıas Kormáksson; Marcos R Vieira; Rafael L Tavares; Bianca Zadrozny,Abstract. In the past few years a growing number of cities have started monitoring theposition of public transportation vehicles using GPS devices. Most of these trajectory dataare released in raw format and usually have issues; such as measurement errors. Providinginsights from these valuable (and noisy) data is a major challenge in larger cities. In thispaper we present a system; called Vistradas; for visual analytics of urban trajectory data.Vistradas allows users to analyze use cases related to trajectories of public buses such as:analysis of bus uniformity; verification of bus route; and the impact of events in bus traffic.Our proposed Vistradas system helps user to get insights into various aspects of publictransportation.,GeoInfo,2014,6
Harvesting parallel text in multiple languages with limited supervision,Luciano Barbosa; Vivek Kumar Rangarajan Sridhar; Mahsa Yarmohammadi; Srinivas Bangalore,Abstract The Web is an ever increasing; dynamically changing; multilingual repository oftext. There have been several approaches to harvest this repository for bootstrapping;supplementing and adapting data needed for training models in speech and languageapplications. In this paper; we present semi-supervised and unsupervised approaches toharvesting multilingual text that rely on a key observation of link collocation. We demonstratethe effectiveness of our approach in the context of statistical machine translation byharvesting parallel texts and training translation models in 20 different languages.Furthermore; by exploiting the DOM trees of parallel webpages; we extend our harvestingtechnique to create parallel data for resource limited languages in an unsupervised manner.We also present some interesting observations concerning the socio-economic factors …,Proceedings of COLING 2012,2012,6
Voistv: Voice-enabled social tv,Bernard Renger; Junlan Feng; Ovidiu Dan; Harry Chang; Luciano Barbosa,Abstract Until recently; the TV viewing experience has not been a very social activitycompared to activities on the World Wide Web. In this work; we will present a Voice-enabledSocial TV system (VoiSTV) which allows users to interact; follow and monitor the onlinesocial media messages related to a TV show while watching it. Users can create; send; andreply to messages using spoken language. VoiSTV also provides metadata informationabout TV shows such as trends; hot topics; popularity as well as aggregated sentiment ofshow-related messages; all of which are valuable for TV program search andrecommendation.,Proceedings of the 20th international conference companion on World wide web,2011,6
Crawling back and forth: Using back and out links to locate bilingual sites,Luciano Barbosa; Srinivas Bangalore; Vivek Kumar Sridhar Rangarajan,Abstract This paper presents a novel crawling strategy to locate bilingual sites. It does so byfocusing on the Web graph neighborhood of these sites and exploring the patterns of thelinks in this region to guide its visitation policy. A sub-task in the problem of bilingual sitediscovery is the job of detecting bilingual sites; ie; given a Web site; verify whether it isbilingual or not. We perform this task by combining supervised learning and languageidentification. Experimental results demonstrate that our crawler outperforms previouscrawling approaches and produces a high-quality collection of bilingual sites; which weevaluate in the context of machine translation in the tourism and hospitality domain. Theparallel text obtained using our novel crawling strategy results in a relative improvement of22% in BLEU score (English-to-Spanish) over an out-ofdomain seed translation model …,Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP),2011,6
USapiens: A System for Urban Trajectory Data Analytics,Marcos Vieira; Luciano Barbosa; Matthias Kormaksson; Bianca Zadrozny,In the past few years a growing number of cities have started monitoring the position ofpublic transportation vehicles using GPS devices. In this paper; we focus on a particularlyimportant urban dataset: GPS bus data. Buses are valuable sensors and informationassociated with buses can provide unprecedented insight into many different aspects ofcity's life; from human behavior to mobility patterns. But analyzing these large urban datasetspresents many challenges. Urban datasets are complex; containing location and temporalcomponents in addition that they are commonly released in their raw format. Furthermore;urban datasets may have noisy and missing data; locations gathered in a low sampling rateand not mapped to the underlying road network; among other issues which makes it difficultfor citizens; administrators and developers to get insights. In this paper; we present a …,MDM,2015,4
SpeechForms: From web to speech and back,Luciano Barbosa; Diamantino Caseiro; Giuseppe Di Fabbrizio; Amanda Stent,Abstract This paper describes SpeechForms; a system that uses novel techniques toautomatically identify form element semantics and form element content; and to semi-automatically generate language models that allow users to fill out each web form elementby voice. Preliminary experimental results show that simple per-element language modelsare faster and may be more accurate than statistical n-gram language models trained onlarge amounts of web text data.,Twelfth Annual Conference of the International Speech Communication Association,2011,4
Blue Man Group no ASSIN: Usando Representações Distribuídas para Similaridade Semântica e Inferência Textual,Luciano Barbosa; Paulo Cavalin; Victor Guimaraes; Matthias Kormaksson,Page 1. Proposta recebida em Setembro 2016 e aceite para publicaç˜ao em Novembro 2016.Blue Man Group no ASSIN: Usando Representaç˜oes Distribu´ıdas para Similaridade Semânticae Inferência Textual Blue Man Group at ASSIN: Using Distributed Representations for SemanticSimilarity and Entailment Recognition Luciano Barbosa IBM Research lucianoa@br.ibm.comPaulo Cavalin IBM Research pcavalin@br.ibm.com Victor Guimar˜aes IBM Researchvictorl@br.ibm.com Matthias Kormaksson IBM Research matkorm@br.ibm.com Resumo Nesteartigo apresentamos a metodologia e os re- sultados obtidos pela equipe Blue Man Group; nacompetiç˜ao de Avaliaç˜ao de Similaridade Semântica e Inferência Textual do PROPOR …,Linguamática,2016,3
Bridging Vocabularies to Link Tweets and News,Tuan-Anh Hoang-Vu; Aline Bessa; Luciano Barbosa; Juliana Freire,ABSTRACT Social media has become a popular platform for publishing; sharing andconsuming news. However; it is not a replacement for traditional sources of news—they arecomplementary. While news sites provide in-depth and comprehensive coverage of eventsand topics; social media postings include comments; opinions and rumors about factspublicized on the news. Social media can thus serve as a useful sensor for how popular astory (or topic) is; for how long; and people's sentiments about it. To use social media as asensor; we first need to associate postings to news stories and topics. But doing so ischallenging since postings are short and the vocabularies used in postings and in news canbe very different. In this paper; we take a first step towards addressing this problem. Wepropose a framework that uses news as a proxy to build a topic model and associates …,WebDB,2014,3
Using latent-structure to detect objects on the web,Luciano Barbosa; Juliana Freire,Abstract An important requirement for emerging applications which aim to locate andintegrate content distributed over the Web is to identify pages that are relevant for a givendomain or task. In this paper; we address the problem of identifying pages that containobjects with a latent structure; ie; the structure is implicitly represented in the page. Wepropose an algorithm which; given a set of instances of an object type; derives rules byautomatically extracting statistically significant patterns present inside the objects. Theserules can then be used to detect the presence of these objects in new; unseen pages. Ourapproach has several advantages when compared against learning-based text classifiers.Because it relies only on positive examples; constructing accurate object detectors is simplerthan constructing learning classifiers; which require both positive and negative examples …,Procceedings of the 13th International Workshop on the Web and Databases,2010,3
Blue Man Group at ASSIN: Using Distributed Representations for Semantic Similarity and Entailment Recognition,Luciano Barbosa; Paulo Cavalin; Victor Guimaraes; Matthias Kormaksson,Abstract In this paper; we present the methodology and the results obtained by our team;dubbed Blue Man Group; in the ASSIN (from the Portuguese Avaliaçao de SimilaridadeSemântica e Inferência Textual) competition; held at PROPOR 20161. Our team's strategyconsisted of evaluating methods based on semantic word vectors; following two distinctdirections: 1) to make use of low-dimensional; compact; feature sets; and 2) deep learning-based strategies dealing with highdimensional feature vectors. Evaluation resultsdemonstrated that the first strategy was more promising; so that the results from the secondstrategy have been discarded. As a result; by considering the best run of each of the sixteams; we have been able to achieve the best accuracy and F1 values in entailmentrecognition; in the Brazilian Portuguese set; and the best F1 score overall. In the semantic …,Linguamatica,2016,2
Focusing on Novelty: A Crawling Strategy to Build Diverse Language Models,Luciano Barbosa; Srinivas Bangalore,Abstract Word prediction performed by language models has an important role in manytasks as eg word sense disambiguation; speech recognition; hand-writing recognition; queryspelling and query segmentation. Recent research has exploited the textual content of theWeb to create language models. In this paper; we propose a new focused crawling strategyto collect Web pages that focuses on novelty in order to create diverse language models. Ineach crawling cycle; the crawler tries to ll the gaps present in the current language modelbuilt from previous cycles; by avoiding visiting pages whose vocabulary is already wellrepresented in the model. It relies on an information theoretic measure to identify these gapsand then learns link patterns to pages in these regions in order to guide its visitation policy.To handle constantly evolving domains; a key feature of our crawler approach is its ability …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,2
Siphoning Hidden-Web Data through Keyword-Based Interfaces: Retrospective,Luciano Barbosa; Juliana Freire,In this paper; we proposed the first; fully-automatic approach to crawling the Hidden Web throughkeyword-based interfaces. Our crawler uses an algorithm for automatically deriving a series ofkeyword-based queries whose goal is to obtain high coverage while minimizing the costs. Inother words; our goal is to retrieve as much of the hidden contents as possible while minimizingthe number of required queries. The intuition behind our algorithm is that; by obtaining samplesof the hidden contents in a online database or document collection; we are able to discover keywordsthat have high frequency. Then; by using these high-frequency keywords we are able to constructqueries that return a large number of answers … Since our paper was published in the Proceedingsof the Brazilian Database Sypomsium in 2004; it has been cited sixty six times1. Otherhidden-Web crawlers were later proposed which make use of our algorithm [Madhavan …,Journal of Information and Data Management,2010,2
Automatically constructing collections of online database directories,Luciano Barbosa; Juliana Freire,Due the the explosion in the number of online databases; there has been increased interestin leveraging the highquality information present in these databases [6; 1; 8]. However;finding the right databases can be very challenging. For example; if a biologist needs tolocate databases related to molecular biology and searches on Google for the keywords“molecular biology database” over 27 million documents are returned. Among these; she willfind pages that contain databases; but the results also include a very large number of pagesfrom journals; scientific articles; etc. Recognizing the need for better mechanisms to locateonline databases; people have started to create online database collections such as theMolecular Biology Database Collection [4]; which lists databases of value to biologists. Thiscollection; however; has been manually created and is manually maintained by the …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,2
Systems and methods for social media data mining,*,*,*,*,2
Extracting records and posts from forum pages with limited supervision,Luciano Barbosa; Guilherme Ferreira,Abstract Internet forums are rich sources of human-generated content. Many applications;such as opinion mining and question answering; can greatly benefit from mining andexploring such useful content. An important step towards making user content from forumsmore easily accessible is to extract it from forum pages. We propose REPEX (REcord andPost EXtractor); a two-step solution that uses limited supervision to achieve this goal. Givena forum page; REPEX first extracts data records that contain human-generated content andthen; from these records; extracts their user content. The record extraction assumes that (1) arecord is composed of an automatic-generated part; which we call record template; and ahuman-generated part; and (2) the structure of record templates are usually consistentacross records. Based on those; the record extractor initially locates the subtree that …,International Conference on Web Information Systems Engineering,2015,1
Paramopama: a Brazilian-Portuguese Corpus for Named Entity Recognition,C. Junior; H. Macedo; T. Bispo; F. Oliveira; N. Silva; L. Barbosa,Abstract—Named Entity Recognition (NER) is one of the most important Natural LanguageProcessing (NLP) tasks. NER concerns the ability to automatically identify words or pieces oftext that refers to some desired target entity; such as name of places; people; organizationsand time units or event dates. NER is central to Information Extraction (IE) applications.Despite that; there are few NE-tagged corpora for Brazilian Portuguese (PtBR). This paperproposes a new tagged Brazilian Portuguese corpus for Named Entity Recognition; whichwe call Paramopama. We evaluate the quality of Paramopama corpus by measuringPrecision; Recall and F-measure of an NER classifier trained from this corpus. In ourexperiments; an NER classifier built from Panorama corpus has achieved better F-measureresults compared to NER classifiers built from other well-known PtBR NE-tagged corpora.,12th National Meeting on Artificial and Computational Intelligence (ENIAC),2015,1
CA2JU: an Assistive Tool for Children with Cerebral Palsy,F. Santos; C. Junior; H. Macedo; M. Chella; R. Givigi; L. Barbosa.,Abstract This paper presents CA2JU; a hardware/software tool that aims to help individualswith severe speech or language problems in their communication in order to promote theirsocial and digital inclusion. CA2JU is composed of two applications: CA2JU Accelerated;which makes typing faster by suggesting potential words to the user; and CA2JU Illustrated;which automatically converts a sentence of words into a sequence of pictographic symbols;allowing a user familiar with the symbols to verify whether the written sentence is correct. Wehave implemented; evaluated in a controlled scenario; and deployed CA2JU in a realenvironment with children with cerebral palsy. In the controlled settings; the results confirmCA2JU Accelerated speed up typing by reducing the number of clicks made by users; andCA2JU Illustrated obtained high accuracy by suggesting the correct pictograms from …,MEDINFO,2015,1
SYSTEM AND METHOD FOR BUILDING DIVERSE LANGUAGE MODELS,*,Abstract: Disclosed herein are systems; methods; and non-transitory computer-readablestorage media for collecting web data in order to create diverse language models. A systemconfigured to practice the method first crawls; such as via a crawler operating on acomputing device; a set of documents in a network of interconnected devices according to avisitation policy; wherein the visitation policy is configured to focus on novelty regions for acurrent language model built from previous crawling cycles by crawling documents whosevocabulary considered likely to fill gaps in the current language model. A language modelfrom a previous cycle can be used to guide the creation of a language model in the followingcycle. The novelty regions can include documents with high perplexity values over thecurrent language model.,*,2017,*
Methodology and Results for the Competition on Semantic Similarity Evaluation and Entailment Recognition for PROPOR 2016,Luciano Barbosa; Paulo R Cavalin; Victor Guimaraes; Matthias Kormaksson,Abstract: In this paper; we present the methodology and the results obtained by our teams;dubbed Blue Man Group; in the ASSIN (from the Portuguese {\it Avalia\c {c}\~ ao deSimilaridade Sem\^ antica e Infer\^ encia Textual}) competition; held at PROPOR2016\footnote {International Conference on the Computational Processing of the PortugueseLanguage-this http URL}. Our team's strategy consisted of evaluating methods based onsemantic word vectors; following two distinct directions: 1) to make use of low-dimensional;compact; feature sets; and 2) deep learning-based strategies dealing with high-dimensionalfeature vectors. Evaluation results demonstrated that the first strategy was more promising;so that the results from the second strategy have been discarded. As a result; by consideringthe best run of each of the six teams; we have been able to achieve the best accuracy and …,arXiv preprint arXiv:1709.08694,2017,*
System and method for application of materials through coordination with automated data collection vehicles,*,An agricultural material application management system includes an automated agriculturaldata collection vehicle including a location sensor. The automated agricultural datacollection vehicle includes a receiver that receives sensor data including crop information; amemory that stores the plurality of locations and the sensor data; and a processor thatgenerates a mapping correlating the crop information with the plurality of locations in theagricultural area; and generates an agricultural material application recommendation foreach of the plurality of locations based on the mapping. The agricultural material applicationmanagement system includes an agricultural vehicle including an interface unit thatinterfaces with the automated agricultural data collection vehicle and receives theagricultural material application recommendations from the agricultural data collection …,*,2017,*
Harvesting Forum Pages from Seed Sites,Luciano Barbosa,Abstract Web forums are rich sources of conversational content. Many applications; such asopinion mining and question answering; can greatly benefit from mining and exploring suchuseful content. A key step towards making this content more easily available is to collectconversational pages on forum sites–so-called thread pages. In this paper; we propose atwo-step crawling solution for the problem of collecting thread pages in large scale. First;since thread pages are located within forum sites; we propose an inter-site crawler thatlocates forum sites on the Web. To do that; the inter-site crawler focuses on the Web graphneighbourhood of forum sites; and explores the content patterns of the links in this region toguide its visitation policy. Next; to collect thread pages within the discovered forum sites; wepropose an intra-site crawler that finds thread pages by learning the context of links that …,International Conference on Web Engineering,2017,*
Machine learning and training a computer-implemented neural network to retrieve semantically equivalent questions using hybrid in-memory representations,*,Determining semantically equivalent text or questions using hybrid representations basedon neural network learning. Weighted bag-of-words and convolutional neural networks(CNN) based distributed vector representations of questions or text may be generated tocompute the semantic similarity between questions or text. Weighted bag-of-words and CNNbased distributed vector representations may be jointly used to compute the semanticsimilarity. A pair-wise ranking loss function trains neural network. In one embodiment; theparameters of the system are trained by minimizing a pair-wise ranking loss function over atraining set using stochastic gradient descent (SGD).,*,2017,*
Pooling Hybrid Representations for Web Structured Data Annotation,Luciano Barbosa; Breno W Carvalho; Bianca Zadrozny,Abstract: Automatically identifying data types of web structured data is a key step in theprocess of web data integration. Web structured data is usually associated with entities orobjects in a particular domain. In this paper; we aim to map attributes of an entity in a givendomain to pre-specified classes of attributes in the same domain based on their values. Toperform this task; we propose a hybrid deep learning network that relies on the format of theattributes' values. It does so without any pre-processing or using pre-defined hand-craftedfeatures. The hybrid network combines sequence-based neural networks; namelyconvolutional neural networks (CNN) and recurrent neural networks (RNN); to learn thesequence structure of attributes' values. The CNN captures short-distance dependencies inthese sequences through a sliding window approach; and the RNN captures long …,arXiv preprint arXiv:1610.00493,2016,*
Method and apparatus for associating micro-blogs with media programs,*,A system that incorporates teachings of the present disclosure may operate; for example;obtaining a number of blogs including an initial set of annotated blogs and unannotatedblogs. The initial set of annotated blogs are annotated as being either relevant to a selectedmedia program or not relevant to the selected media program. A set of features isdetermined associating the selected media program with the unannotated blogs and atrained classifier is generated based on the set of features. The trained classifier is appliedto the blogs to identify a subset of blogs relevant to the selected media program. An analysisis performed on the selected blogs to determine a trend related to the selected mediaprogram and a graphical user interface is presented that concurrently presents the selectedblogs; the trend; and the selected media program. Other embodiments are disclosed.,*,2016,*
Finding seeds to bootstrap focused crawlers,Karane Vieira; Luciano Barbosa; Altigran Soares Da Silva; Juliana Freire; Edleno Moura,Abstract Focused crawlers are effective tools for applications requiring a high number ofpages belonging to a specific topic. Several strategies for implementing these crawlers havebeen proposed in the literature; which aim to improve crawling efficiency by increasing thenumber of relevant pages retrieved while avoiding non-relevant pages. However; animportant aspect of these crawlers has been largely overlooked: the selection of the seedpages that serve as the starting points for a crawl. In this paper; we show that the seeds cangreatly influence the performance of crawlers; and propose a new framework forautomatically finding seeds. We describe a system that implements this framework andshow; through a detailed experimental evaluation; that by providing crawlers a seed set thatis large and varied; they not only obtain higher harvest rates but also an improved topic …,World Wide Web,2016,*
Systems and Methods for Social Media Data Mining,*,Systems and methods are provided to collect; analyze and report social media aggregatedfrom a plurality of social media websites. Social media is retrieved from social mediawebsites; analyzed for sentiment; and categorized by topic and user demographics. Thedata is then archived in a data warehouse and various interfaces are provided to query andgenerate reports on the archived data. In some embodiments; the system further recognizesalert conditions and sends alerts to interested users. In some embodiments; the systemfurther recognizes situations where users can be influenced to view a company or itsproducts in a more favorable light; and automatically posts responsive social media to oneor more social media websites.,*,2016,*
Method and apparatus for associating micro-blogs with media programs,*,A system that incorporates teachings of the present disclosure may operate; for example;according to a method for identifying subsets of blogs from a collection of blogs according toa relevance of each subset of blogs to one of a plurality of media programs deliverable to aplurality of subscriber devices; determining for each subset of blogs one or more trends;detecting a subscriber device from the plurality of subscriber devices; selecting a mediaprogram from the plurality of media programs; selecting one of the subsets of blogs that isrelevant to the media program; and supplying the subscriber device the selected subset ofblogs with the one or more trends determined for the select subset of blogs. Otherembodiments are disclosed.,*,2016,*
A Hybrid Deep Network for Name Entity Recognition in Portuguese,Carlos Júnior; Luciano Barbosa; Hendrik Macedo,*,13rd National Meeting on Artificial and Computational Intelligence (ENIAC),2016,*
Method and apparatus for associating micro-blogs with media programs,*,A system that incorporates teachings of the present disclosure may operate; for example;according to a method for identifying subsets of blogs from a collection of blogs according toa relevance of each subset of blogs to one of a plurality of media programs deliverable to aplurality of subscriber devices; determining for each subset of blogs one or more trends;detecting a subscriber device from the plurality of subscriber devices; selecting a mediaprogram from the plurality of media programs; selecting one of the subsets of blogs that isrelevant to the media program; and supplying the subscriber device the selected subset ofblogs with the one or more trends determined for the select subset of blogs. Otherembodiments are disclosed.,*,2015,*
Method and system for adaptive discovery of content on a network,*,A method is provided for identifying documents that include a searchable form relevant to atopic. A document is received. If the received document comprises a form is determined. Aform includes a field presented to a user requesting information from the user. If the receiveddocument is determined to comprise a form; a determination is made concerning whether ornot the form is a searchable form. A searchable form returns non-trivial information to arequester in response to a submission of the form. If the form is determined to be asearchable form; a determination is made concerning whether or not the form is relevant toan identified topic. If the form is determined to be relevant to the identified topic; thedocument is identified as a searchable form relevant to the identified topic.,*,2015,*
VazaBarris and Poxim Corpora: Augmenting NLP Software Ability to Deal with Informal Brazilian Portuguese,F. Santos; N. Silva; L. Barbosa; T. Bispo; C. Junior; H. Macedo,*,12th National Meeting on Artificial and Computational Intelligence (ENIAC),2015,*
Mining Enterprise Websites for Association Thesaurus Construction,Luciano Barbosa,ABSTRACT Enterprise websites are useful resources for obtaining information aboutproducts and services of companies. Typically on these websites; a product is associated toa Web page; and related products are connected by hyperlinks. As a result; the connectivitygraph of an enterprise website exposes the company's products (nodes) and how they areassociated (links). This paper presents a novel approach that mines these graphs in order tobuild association thesauri for enterprises. An association thesaurus represents implicitassociations between concepts (company-related information in the context of this work). Toperform this task; our approach first executes a breadth-search crawl in the website; buildingan initial thesaurus. Next; it employs probabilistic modelling to remove non-relevant contentand assign weights to the associations in the thesaurus. We evaluated the association …,WebDB,2013,*
Understanding the Landscape,Luciano Barbosa; Kien Pham; Claudio Silva; Marcos R Vieira; Juliana Freire,Abstract A growing number of cities are now making urban data freely available to thepublic. Besides promoting transparency; these data can have a transformative effect insocial science research as well as in how citizens participate in governance. Theseinitiatives; however; are fairly recent and the landscape of open urban data is not wellknown. In this study; we try to shed some light on this through a detailed study of over 9;000open data sets from 20 cities in North America. We start by presenting general statisticsabout the content; size; nature; and popularity of the different data sets; and then examine inmore detail structured data sets that contain tabular data. Since a key benefit of having alarge number of data sets available is the ability to fuse information; we investigateopportunities for data integration. We also study data quality issues and time-related …,*,*,*
Uma Arquitetura Híbrida LSTM-CNN para Reconhecimento de Entidades Nomeadas em Textos Naturais em Língua Portuguesa,Carlos AEM Júnior; Luciano A Barbosa; Hendrik T Macedo; SE São Cristóvão,Resumo. Existem modelos de classificadores que realizam bem a tarefa do reconhecimentode entidades nomeadas em texto a partir de grandes esforços em definir muitascaracterísticas baseando-se em conhecimento especializado. Neste trabalho é propostauma arquitetura de Rede Neural Profunda que utiliza uma rede LSTM para detectarentidades nomeadas sobre vários corpora da língua portuguesa do Brasil. Além disso; sãousados word embeddings prétreinados e uma rede CNN para extração automática defeatures das palavras para geração de vetores de char embeddings. No trabalho sãodefinidos cenários para experimentação onde a arquitetura proposta é comparada comuma arquitetura que também utiliza word e char embeddings; obtendo melhor desempenhoutilizando-se apenas word embeddings; e em outro cenário a arquitetura proposta é …,*,*,*
DeepPeep: A Form Search Engine,Luciano Barbosa; Hoa Nguyen; Thanh Nguyen; Ramesh Pinnamaneni; Juliana Freire,We present DeepPeep (http://www. deeppeep. org); a new search engine specialized inWeb forms. DeepPeep uses a scalable infrastructure for discovering; organizing andanalyzing Web forms which serve as entry points to hidden-Web sites. DeepPeep providesan intuitive interface that allows users to explore and visualize large form collections.,*,*,*
