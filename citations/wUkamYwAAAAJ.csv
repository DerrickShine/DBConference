On supporting containment queries in relational database management systems,Chun Zhang; Jeffrey Naughton; David DeWitt; Qiong Luo; Guy Lohman,Abstract Virtually all proposals for querying XML include a class of query we term“containment queries”. It is also clear that in the foreseeable future; a substantial amount ofXML data will be stored in relational database systems. This raises the question of how tosupport these containment queries. The inverted list technology that underlies much ofInformation Retrieval is well-suited to these queries; but should we implement thistechnology (a) in a separate loosely-coupled IR engine; or (b) using the native tables andquery execution machinery of the RDBMS? With option (b); more than twenty years of workon RDBMS query optimization; query execution; scalability; and concurrency control andrecovery immediately extend to the queries and structures that implement these newoperations. But all this will be irrelevant if the performance of option (b) lags that of (a) by …,Acm Sigmod Record,2001,1174
Starburst mid-flight: as the dust clears (database project),Laura M.  Haas; Walter Chang; Guy M.  Lohman; John McPherson; Paul F.  Wilms; George Lapis; Bruce Lindsay; Hamid Pirahesh; Michael J.  Carey; Eugene Shekita,The purpose of the Starburst project is to improve the design of relational databasemanagement systems and enhance their performance; while building an extensible systemto better support nontraditional applications and to serve as a testbed for futureimprovements in database technology. The design and implementation of the Starburstsystem to date are considered. Some key design decisions and how they affect the goal ofimproved structure and performance are examined. How well the goal of extensibility hasbeen met is examined: what aspects of the system are extensible; how extensions can bedone; and how easy it is to add extensions. Some actual extensions to the system; includingthe experiences of the first real customizers; are discussed.,IEEE Transactions on knowledge and data engineering,1990,426
Extensible query processing in Starburst,Laura M Haas; Johann Christoph Freytag; Guy M Lohman; Hamid Pirahesh,Abstract Today's DBMSs are unable to support the increasing demands of the variousapplications that would like to use a DBMS. Each kind of application poses newrequirements for the DBMS. The Starburst project at IBM's Almaden Research Center aimsto extend relational DBMS technology to bridge this gap between applications and theDBMS. While providing a full function relational system to enable sharing acrossapplications; Starburst will also allow (sophisticated) programmers to add many kinds ofextensions to the base system's capabilities; including language extensions (eg; newdatatypes and operations); data management extensions (eg; new access and storagemethods) and internal processing extensions (eg; new join methods and new querytransformations). To support these features; the database query language processor must …,ACM SIGMOD Record,1989,380
Differential Files: Their Application to the Maintenance of Large Data Bases (Abstract),Dennis G.  Severance; Guy M.  Lohman,*,SIGMOD Conference,1976,380
LEO-DB2's learning optimizer,Michael Stillger; Guy M Lohman; Volker Markl; Mokhtar Kandil,Abstract Most modern DBMS optimizers rely upon a cost model to choose the best queryexecution plan (QEP) for any given query. Cost estimates are heavily dependent upon theoptimizer's estimates for the number of rows that will result at each step of the QEP forcomplex queries involving many predicates and/or operations. These estimates rely uponstatistics on the database and modeling assumptions that may or may not be true for a givendatabase. In this paper we introduce LEO; DB2's LEarning Optimizer; as a comprehensiveway to repair incorrect statistics and cardinality estimates of a query execution plan. Bymonitoring previously executed queries; LEO compares the optimizer's estimates withactuals at each step in a QEP; and computes adjustments to cost estimates and statistics thatmay be used during future query optimizations. This analysis can be done either on-line …,VLDB,2001,357
R* optimizer validation and performance evaluation for distributed queries,Lothar F Mackert; Guy M Lohman,Abstract Few database query optimizer models have been validated against actualperformance. This paper extends an earlier optimizer validation and performance evaluationof R* to distributed queries; ie single SQL statements having tables at multiple sites. ActualR* message; I/O; and CPU resources consumed—and the corresponding costs estimated bythe optimizer—were written to database tables using new SQL commands; permittingautomated control from application programs for collecting; reducing; and comparing testdata. A number of tests were run over a wide variety of dynamically-created test databases;SQL queries; and system parameters. Both high-speed networks (comparable to a localarea network) and medium-speed long-haul networks (for linking geographically dispersedhosts) were evaluated. The tests confirmed the accuracy of R*'s message cost model and …,*,1988,351
DB2 design advisor: integrated automatic physical database design,Daniel C Zilio; Jun Rao; Sam Lightstone; Guy Lohman; Adam Storm; Christian Garcia-Arellano; Scott Fadden,Abstract The DB2 Design Advisor in IBM® DB2® Universal Database TM (DB2 UDB)Version 8.2 for Linux®; UNIX® and Windows® is a tool that; for a given workload;automatically recommends physical design features that are any subset of indexes;materialized query tables (also called materialized views); shared-nothing databasepartitionings; and multidimensional clustering of tables. Our work is the very first industrial-strength tool that covers the design of as many as four different features; a significantadvance to existing tools; which support no more than just indexes and materialized views.Building such a tool is challenging; because of not only the large search space introducedby the interactions among features; but also the extensibility needed by the tool to supportadditional features in the future. We adopt a novel" hybrid" approach in the Design …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,335
DB2 advisor: An optimizer smart enough to recommend its own indexes,Gary Valentin; Michael Zuliani; Daniel C Zilio; Guy Lohman; Alan Skelley,This paper introduces the concept of letting an RDBMS optimizer optimize its ownenvironment. In our project; we have used the DB2 optimizer to tackle the index selectionproblem; a variation of the knapsack problem. This paper discusses our implementation ofindex recommendation; the user interface; and provide measurements on the quality of therecommended indexes.,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,297
Grammar-like functional rules for representing query optimization alternatives,Guy M Lohman,Abstract Extensible query optimization requires that the “repertoire” of alternative strategiesfor executing queries be represented as data; not embedded in the optimizer code.Recognizing that query optimizers are essentially expert systems; several researchers havesuggested using strategy rules to transform query execution plans into alternative or betterplans. Though extremely flexible; these systems can be very inefficient at any step in theprocessing; many rules may be eligible for application and complicated conditions must betested to determine that eligibility during unification. We present a constructive;“buildingblocks” approach to defining alternative plans; in which the rules defining alternatives are anextension of the productions of a grammar to resemble the definition of a function inmathematics. The extensions permit each token of the grammar to be parametrized and …,ACM SIGMOD Record,1988,253
Measuring the complexity of join enumeration in query optimization,Kiyoshi Ono; Guy M Lohman,Introduction 314 (the second operand of the join) is the result of a join that must bematerialized in memory or-if it is too big-on disk. The heuristic saves this materialization; butmay exclude better plans for certain qucrics. As an example; suppose a query with four largetables '1'1; T2; 7'3; and T4 has two predicates Tl. Cl='1'2. C2 and '1'3. C3= T4. C4 that areextremely selective (rcstrictive); and one T2. C6= T4. C8 that is not.'I'hcn the plan ((Tl W T2)Cd (T3 fX '1'4)) with a composite inner (T3 W 1'4) is likely to be better than any plan avoidingthe composite inner; such as ((Tl 00 '1'2) W T4) W T3; because the intermediate results of thefirst plan would all be significantly smaller than any in the second plan. Another majorheuristic employed by both System R [% I; 791 and INURES[WON761 always defersCartesian products as late in the join sequence as possiblc; assuming that they result in …,VLDB Conference,1990,248
Extensions to Starburst: Objects; types; functions; and rules,Guy M Lohman; Bruce Lindsay; Hamid Pirahesh; K Bernhard Schiefer,Relational DBMSs; prototyped in the 1970s and now common in the marketplace; gainedpopularity largely because of their simple underlying concepts and high-level set-orientedquery languages that simplified application development. As the traditional recordkeepingapplications of DBMSs (eg; banking; insurance; personnel) have become moresophisticated; and as nontraditional applications (such as engineering databases) haveattempted to adapt off-the-shelf DBMSs; users have been frustrated by the difficulty and poorperformance of adding new features" on top of" commercial DBMSs; ie; in the applicationsrather than the DBMS itself. Yet their demands for new and increasingly complex features tobe built into the DBMS have overwhelmed the capacity of DBMS vendors to enhance theiralready complex products. Influenced by advances in modern programming languages …,Communications of the ACM,1991,240
Robust query processing through progressive optimization,Volker Markl; Vijayshankar Raman; David Simmen; Guy Lohman; Hamid Pirahesh; Miso Cilimdzic,Abstract Virtually every commercial query optimizer chooses the best plan for a query usinga cost model that relies heavily on accurate cardinality estimation. Cardinality estimationerrors can occur due to the use of inaccurate statistics; invalid assumptions about attributeindependence; parameter markers; and so on. Cardinality estimation errors may cause theoptimizer to choose a sub-optimal plan. We present an approach to query processing that isextremely robust because it is able to detect and recover from cardinality estimation errors.We call this approach" progressive query optimization"(POP). POP validates cardinalityestimates against actual values as measured during query execution. If there is significantdisagreement between estimated and actual values; execution might be stopped and re-optimization might occur. Oscillation between optimization and execution steps can occur …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,235
Automating physical database design in a parallel database,Jun Rao; Chun Zhang; Nimrod Megiddo; Guy Lohman,Abstract Physical database design is important for query performance in a shared-nothingparallel database system; in which data is horizontally partitioned among multipleindependent nodes. We seek to automate the process of data partitioning. Given a workloadof SQL statements; we seek to determine automatically how to partition the base data acrossmultiple nodes to achieve overall optimal (or close to optimal) performance for that workload.Previous attempts use heuristic rules to make those decisions. These approaches fail toconsider all of the interdependent aspects of query performance typically modeled bytoday's sophisticated query optimizers. We present a comprehensive solution to the problemthat has been tightly integrated with the optimizer of a commercial shared-nothing paralleldatabase system. Our approach uses the query optimizer itself both to recommend …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,228
Alert: An architecture for transforming a passive DBMS into an active DBMS,Ulf Schreier; Hamid Pirahesh; Rakesh Agrawal; C Mohan,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references …[ADL91] R. Agxaual; LG DeMichiel; and BG Lindsay. Polyglot: An Object-Oriented Type Systemfor Multi-Language Support. Technical Report; IBM Almaden Research Center; 1991. underpreparation … [BW77] DG Bobrov and R. Winograd. An Overview of KRL; a Knowledge RepresentationLanguage. Cognitive Science; 1:3- 46; 1977 … [CBB+89] S. Chakravarthy; B. Blaustein; A.Buchmann; M. Carey; U. Dayal; D. Goldhirsch; M. HSU; R. Jauhari; R. Ladin; N. Livny; D.McCarthy; R. McKee; and A. Rosenthal. HiPA C: A Research Project in Active; Time-ConstrainedDatabase Management - Final Techntcal Report. Technical Report XAIT- 89-02; Xerox AdvancedInformation Technology; July 1989 … [For81] CL Forgy. OPS5 User's Manual. Technical …,Proceedings of the 17th International Conference on Very Large Data Bases,1991,212
Optimization of data repartitioning during parallel query optimization,*,Query evaluation is optimized using parallel optimization techniques to make repartitioningmore efficient. Efficiency is improved by recognizing the possible partitioning requirementsfor achieving parallelism for a query operation; and by recognizing when the partitioningproperty of data satisfies the partitioning requirements of a query operation. A data basemanagement system in accordance with the invention uses parallel query processingtechniques to optimize data repartitioning; or to avoid it altogether.,*,2000,186
R* optimizer validation and performance evaluation for local queries,Lothar F Mackert; Guy M Lohman,Abstract Few database query optimizer models have been validated against actualperformance. This paper presents the methodology and results of a thorough validation ofthe optimizer and evaluation of the performance of the experimental distributed relationaldatabase management system R*; which inherited and extended to a distributedenvironment the optimization algorithms of System R. Optimizer estimated costs and actualR* resources consumed were written to database tables using new SQL commands;permitting automated control from SQL application programs of test data collection andreduction. A number of tests were run over a wide variety of dynamically-created testdatabases; SQL queries; and system parameters. The results for single-table access;sorting; and local 2-table joins are reported here. The tests confirmed the accuracy of the …,ACM SIGMOD Record,1986,176
Query processing in R,Guy M Lohman; C Mohan; Laura M Haas; Bruce G Lindsay; Patricia G Selinger; Paul F Wilms; Dean Daniels,Abstract This chapter describes how statements in the SQL language are processed by theR* distributed relational database management system. R* is an experimental adaptation ofSystem R to the distributed environment. The R* prototype is currently operational onmultiple machines running the MVS operating system; and is undergoing evaluation. The R*system is a confederation of autonomous; locally-administered databases that may begeographically dispersed; yet which appear to the user as a single database. Namingconventions permit R* to access tables at remote sites without resorting to a centralized orreplicated catalog; and without the user having to specify either the current location of or thecommunication commands required to access that table. SQL data definition statementsaffecting remote sites are interpreted through a distributed recursive call mechanism …,Research Report RJ,1985,168
Extensibility in the Starburst database system,P Schwarz; Walter Chang; Johann Christoph Freytag; G Lohman; John McPherson; C Mohan; Hamid Pirahesh,Abstract The goal of the Starburst project is to examine how traditional relational databasesystems must be adapted for new applications and technologies. We believe thatextensibility is an important requirement for future database systems; and have identifiedsome of the problems of building an extensible relational database system. We focus on fivepotential areas for extensibility: external data storage; storage management; accessmethods; abstract data types; and complex objects. Our approach emphasizes extensionswritten by knowledgeable programmers that are invoked as needed from within the primarydatabase system. We show by an example how this method could be used to implementsupport for complex objects. Although many questions remain unanswered; extensiblerelational database systems are a promising area of research.,Proceedings on the 1986 international workshop on Object-oriented database systems,1986,164
Fatma zcan; Hamid Pirahesh; Norman Seemann; Tuong C. Truong; Bert Van der Linden; Brian Vickery; and Chun Zhang. System RX: One Part Relational; One Part...,Kevin S Beyer; Roberta Cochrane; Vanja Josifovski; Jim Kleewein; George Lapis; Guy M Lohman; Bob Lyle,*,Proceedings of the ACM SIGMOD International Conference on Management of Data,2005,162
Relational database query optimization to perform query evaluation plan; pruning based on the partition properties,*,A relational data base management system includes a query processor that uses a queryoperator partition property to perform QEP pruning and to ensure that data input to a queryoperator is partitioned appropriately for the operation. The partition property indicates thegroup of network nodes across which a table is distributed. The query processor also makesuse of partition classes that are designated" interesting classes" to perform preoptimizationplanning and query pruning; and to perform look-ahead partitioning based on partitionclasses that are identified as being of interest to future operations; thereby more efficientlyevaluating complex query statements in an MPP; shared-nothing environment.,*,2000,158
DB2 with BLU acceleration: So much more than just a column store,Vijayshankar Raman; Gopi Attaluri; Ronald Barber; Naresh Chainani; David Kalmuk; Vincent KulandaiSamy; Jens Leenstra; Sam Lightstone; Shaorong Liu; Guy M Lohman; Tim Malkemus; Rene Mueller; Ippokratis Pandis; Berni Schiefer; David Sharpe; Richard Sidle; Adam Storm; Liping Zhang,Abstract DB2 with BLU Acceleration deeply integrates innovative new techniques fordefining and processing column-organized tables that speed read-mostly BusinessIntelligence queries by 10 to 50 times and improve compression by 3 to 10 times; comparedto traditional row-organized tables; without the complexity of defining indexes ormaterialized views on those tables. But DB2 BLU is much more than just a column store.Exploiting frequency-based dictionary compression and main-memory query processingtechnology from the Blink project at IBM Research-Almaden; DB2 BLU performs most SQLoperations-predicate application (even range predicates and IN-lists); joins; and grouping-on the compressed values; which can be packed bit-aligned so densely that multiple valuesfit in a register and can be processed simultaneously via SIMD (single-instruction …,Proceedings of the VLDB Endowment,2013,153
LEO: An autonomic query optimizer for DB2,Volker Markl; Guy M Lohman; Vijayshankar Raman,SQL has emerged as an industry standard for querying relational database managementsystems; largely because a user need only specify what data is wanted; not the details ofhow to access that data. A query optimizer uses a mathematical model of query execution todetermine automatically the best way to access and process any given SQL query. Thismodel is heavily dependent upon the optimizer's estimates for the number of rows that willresult at each step of the query execution plan (QEP); especially for complex queriesinvolving many predicates and/or operations. These estimates rely upon statistics on thedatabase and modeling assumptions that may or may not be true for a given database. Inthis paper we discuss an autonomic query optimizer that automatically self-validates itsmodel without requiring any user interaction to repair incorrect statistics or cardinality …,IBM Systems Journal,2003,128
System; method; and computer program product for progressive query processing,*,A method; system; and computer program product to make query processing more robust inthe face of optimization errors. The invention validates the statistics and assumptions usedfor compiling a query as the query is executed and; when necessary; progressively re-optimizes the query in mid-execution based on the knowledge learned during its partialexecution. The invention selectively places a number of CHECK operators in a queryexecution plan to validate the optimizer's cardinality estimates against actual cardinalities.Errors beyond a threshold trigger re-optimization; and the optimizer decides whether the oldplan is still optimal and whether to re-use previously computed results. The inventionaddresses arbitrary SQL queries whose plans can contain sub-queries; updates; triggerchecking; and view maintenance operations. The invention can handle concurrent update …,*,2010,126
Recommending materialized views and indexes with the IBM DB2 design advisor,Daniel C Zilio; Calisto Zuzarte; Sam Lightstone; Wenbin Ma; Guy M Lohman; Roberta J Cochrane; Hamid Pirahesh; Latha Colby; Jarek Gryz; Eric Alton; Gary Valentin,Materialized views (MVs) and indexes both significantly speed query processing indatabase systems; but consume disk space and need to be maintained when updates occur.Choosing the best set of MVs and indexes to create depends upon the workload; thedatabase; and many other factors; which makes the decision intractable for humans andcomputationally challenging for computer algorithms. Even heuristic-based algorithms canbe impractical in real systems. In this paper; we present an advanced tool that uses thequery optimizer itself to both suggest and evaluate candidate MVs and indexes; and asimple; practical; and effective algorithm for rapidly finding good solutions even for largeworkloads. The algorithm trades off the cost for updates and storing each MV or indexagainst its benefit to queries in the workload. The tool autonomically captures the …,Autonomic Computing; 2004. Proceedings. International Conference on,2004,121
SQAK: doing more with keywords,Sandeep Tata; Guy M Lohman,Abstract Today's enterprise databases are large and complex; often relating hundreds ofentities. Enabling ordinary users to query such databases and derive value from them hasbeen of great interest in database research. Today; keyword search over relationaldatabases allows users to find pieces of information without having to write complicated SQLqueries. However; in order to compute even simple aggregates; a user is required to write aSQL statement and can no longer use simple keywords. This not only requires the ordinaryuser to learn SQL; but also to learn the schema of the complex database in detail in order tocorrectly construct the required query. This greatly limits the options of the user who wishesto examine a database in more depth. As a solution to this problem; we propose aframework called SQAK 1 (SQL Aggregates using Keywords) that enables users to pose …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,118
Learning from empirical results in query optimization,*,An optimizer function of a Relational Database Management System (RDBMS) generatesalternative query execution plans (QEPs) for executing a query; provides an executionmodel of each of the QEPs; chooses one of the QEPs for execution based on the modelassociated therewith; and exploits an empirical measurement from the execution of thechosen QEP to validate the model associated therewith; by determining whether the modelis in error; and by computing one or more adjustments to the model to correct the determinederror.,*,2004,118
GPU join processing revisited,Tim Kaldewey; Guy Lohman; Rene Mueller; Peter Volk,Abstract Until recently; the use of graphics processing units (GPUs) for query processingwas limited by the amount of memory on the graphics card; a few gigabytes at best.Moreover; input tables had to be copied to GPU memory before they could be processed;and after computation was completed; query results had to be copied back to CPU memory.The newest generation of Nvidia GPUs and development tools introduces a commonmemory address space; which now allows the GPU to access CPU memory directly; liftingsize limitations and obviating data copy operations. We confirm that this new technology cansustain 98% of its nominal rate of 6.3 GB/sec in practice; and exploit it to process databasehash joins at the same rate; ie; the join is processed" on the fly" as the GPU reads the inputtables from CPU memory at PCI-E speeds. Compared to the fastest published results for …,Proceedings of the Eighth International Workshop on Data Management on New Hardware,2012,116
Query optimization in the IBM DB2 family,Peter Gassner; Guy M Lohman; K Bernhard Schiefer; Yun Wang,*,Data Engineering Bulletin,1994,115
Query optimization in the IBM DB2 family,Peter Gassner; Guy M.  Lohman; K. Bernhard  Schiefer; Yun Wang,*,Data Engineering Bulletin,1993,115
Dynamic faceted search for discovery-driven analysis,Debabrata Dash; Jun Rao; Nimrod Megiddo; Anastasia Ailamaki; Guy Lohman,Abstract We propose a dynamic faceted search system for discovery-driven analysis on datawith both textual content and structured attributes. From a keyword query; we want todynamically select a small set of" interesting" attributes and present aggregates on them to auser. Similar to work in OLAP exploration; we define" interestingness" as how surprising anaggregated value is; based on a given expectation. We make two new contributions byproposing a novel" navigational" expectation that's particularly useful in the context offaceted search; and a novel interestingness measure through judicious application of p-values. Through a user survey; we find the new expectation and interestingness metric quiteeffective. We develop an efficient dynamic faceted search system by improving a popularopen source engine; Solr. Our system exploits compressed bitmaps for caching the …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,106
System for adapting query optimization effort to expected execution time,*,A system for the automatic adjustment of resources devoted to query optimization accordingto estimated query execution time. The disclosed system permits the query optimizer toautomatically trade off the time spent estimating the execution cost of alternate queryexecution plans against the potential savings in execution time that one of those alternateplans may yield. The number of alternate plans considered is adjusted by selecting compile-time parameters and heuristic criteria for limiting the primitive database operators used inthe alternate plans; thereby establishing a new search space. The parameters and criteriaare adjusted according to the estimate of execution cost for the optimal plan from a firstsearch space. The first search space may be relatively small and quickly evaluated.Evaluation of larger subsequent search spaces is optional according to an automatic …,*,1994,98
Platform-independent method and system for graphically presenting the evaluation of a query in a database management system,*,A method and system for graphically representing a plan for a query in a relational databasemanagement system is disclosed. The method includes receiving and processing an inputquery to form a plurality of plans; selecting at least one plan of the plurality of plans; andtransforming the selected plan into a self-describing formatted file which is platformindependent. The method further includes generating a graph representing the selectedplan from the self-describing formatted file.,*,2004,94
Main-memory scan sharing for multi-core CPUs,Lin Qiao; Vijayshankar Raman; Frederick Reiss; Peter J Haas; Guy M Lohman,Abstract Computer architectures are increasingly based on multi-core CPUs and largememories. Memory bandwidth; which has riot kept pace with the increasing number of cores;has become the primary processing bottleneck; replacing disk I/O as the limiting factor. Toaddress this challenge; we provide novel algorithms for increasing the throughput ofBusiness Intelligence (BI) queries; as well as for ensuring fairness and avoiding starvationamong a concurrent set of such queries. To maximize throughput; we propose a novelFullSharing scheme that allows all concurrent queries; when performing base-table I/O; toshare the cache belonging to a given core. We then generalize this approach to aBatchSharing scheme that avoids thrashing on" agg-tables"---hash tables that are used foraggregation processing---caused by execution of too many queries on a core. This …,Proceedings of the VLDB Endowment,2008,88
Quickly finding known software problems via automated symptom matching,Mark Brodie; Sheng Ma; Guy Lohman; Laurent Mignet; M Wilding; J Champlin; P Sohn,We present an architecture for and prototype of a system for quickly detecting softwareproblem recurrences. Re-discovery of the same problem is very common in many largesoftware products and is a major cost component of product support. At run-time; when aproblem occurs; the system collects the problem symptoms; including the program call-stack;and compares it against a database of symptoms to find the closest matches. The databaseis populated off-line using solved cases and indexed to allow for efficient matching. Thusproblems that occur repeatedly can be easily and automatically resolved without requiringany human problem-solving expertise. We describe a prototype implementation of thesystem; including the matching algorithm; and present some experimental resultsdemonstrating the value of automatically detecting re-occurrence of the same problem for …,Autonomic Computing; 2005. ICAC 2005. Proceedings. Second International Conference on,2005,86
Method for determining optimal database materializations using a query optimizer,*,A method for determining optimal database materializations utilizing a query optimizer in adatabase management system. The method takes one or more queries as inputs and usingthe query optimizer in the database management system generates a series of virtualmaterializations by materializing some subsets of the database. The virtual materializationsare used to consider the relative performance benefits; ie cost-benefits; for the queriesbased on the various virtual materializations. If the query optimizer decides to use any of thematerializations in its plan; then those materializations are recommended to the user; orcreated automatically for the user.,*,2002,86
Statistical learning techniques for costing XML queries,Ning Zhang; Peter J Haas; Vanja Josifovski; Guy M Lohman; Chun Zhang,Abstract Developing cost models for query optimization is significantly harder for XMLqueries than for traditional relational queries. The reason is that XML query operators aremuch more complex than relational operators such as table scans and joins. In this paper;we propose a new approach; called COMET; to modeling the cost of XML operators; to ourknowledge; COMET is the first method ever proposed for addressing the XML query costingproblem. As in relational cost estimation; COMET exploits a set of system catalog statisticsthat summarizes the XML data; the set of" simple path" statistics that we propose is new; andis well suited to the XML setting. Unlike the traditional approach; COMET uses a newstatistical learning technique called" transform regression" instead of detailed analyticalmodels to predict the overall cost. Besides rendering the cost estimation problem …,Proceedings of the 31st international conference on Very large data bases,2005,82
Index scans using a finite LRU buffer: A validated I/O model,Lothar F Mackert; Guy M Lohman,Abstract Indexes are commonly employed to retrieve a portion of a file or to retrieve itsrecords in a particular order. An accurate performance model of indexes is essential to thedesign; analysis; and tuning of file management and database systems; and particularly todatabase query optimization. Many previous studies have addressed the problem ofestimating the number of disk page fetches when randomly accessing k records out of Ngiven records stored on T disk pages. This paper generalizes these results; relaxing twoassumptions that usually do not hold in practice: unlimited buffer and unique records foreach key value. Experiments show that the performance of an index scan is very sensitive tobuffer size limitations and multiple records per key value. A model for these more practicalsituations is presented and a formula derived for estimating the performance of an index …,ACM Transactions on Database Systems (TODS),1989,79
Automatically and adaptively determining execution plans for queries with parameter markers,*,A method for automatically and adaptively determining query execution plans for parametricqueries. A first classifier trained by an initial set of training points is generated using a set ofrandom decision trees (RDTs). A query workload and/or database statistics are dynamicallyupdated. A new set of training points collected off-line is used to modify the first classifier intoa second classifier. A database query is received at a runtime subsequent to the off linephase. The query includes predicates having parameter markers bound to actual values.The predicates are associated with selectivities. The query execution plan is determined byidentifying an optimal average of posterior probabilities obtained across a set of RDTs andmapping the selectivities to a plan. The determined query execution plan is included in anaugmented set of training points that includes the initial set and the new set.,*,2008,78
NUMA-aware algorithms: the case of data shuffling.,Yinan Li; Ippokratis Pandis; Rene Mueller; Vijayshankar Raman; Guy M Lohman,ABSTRACT In recent years; a new breed of non-uniform memory access (NUMA) systemshas emerged: multi-socket servers of multicores. This paper makes the case that datamanagement systems need to employ designs that take into consideration thecharacteristics of modern NUMA hardware. To prove our point; we focus on a primitive that isused as the building block of numerous data management operations: data shuffling. Weperform a comparison of different data shuffling algorithms and show that a naıve datashuffling algorithm can be up to 3× slower than the highest performing; NUMA-aware one.To achieve the highest performance; we employ a combination of thread binding; NUMA-aware thread allocation; and relaxed global coordination among threads. The importance ofsuch NUMA-aware algorithm designs will only increase; as future server systems are …,CIDR,2013,77
Method for recommending indexes and materialized views for a database workload,*,The invention herein provides method and apparatus; including software for determining aset of materialized views or indices of the contents or a subset of the contents of a databasein a data processing system to be created for one or more users of the database. Themethod and apparatus provide method and means for evaluating a workload presented by auser to the database; evaluating the data processing system characteristics; evaluating thedatabase characteristics; and; using the above evaluations for recommending a set ofsuitable materialized views or indices to the user. Another aspect of the invention; whichmay be used for a workload presented by a user of a database in a data processing system;provides method and apparatus; including software for determining a set of materializedviews or indices of the contents or a subset of the contents of the database; by: generating …,*,2006,77
Optimization of SQL queries using early-out join transformations of column-bound relational tables,*,A method and apparatus for optimizing SQL queries in a relational database managementsystem uses early-out join transformations. An early-out join comprises a many-to-oneexistential join; wherein the join scans an inner table for a match for each row of the outertable and terminates the scan for each row of the outer table when a single match is found inthe inner table. To transform a many-to-many join to an early-out join; the query must includea requirement for distinctiveness; either explicitly or implicitly; in one or more result columnsfor the join operation. Distinctiveness can be specified using the DISTINCT keyword in theSELECT clause or can be implied from the predicates present in the query. The early-outjoin transformation also requires that no columns of the inner table be referenced after thejoin; or if an inner table column is referenced after the join; that each referenced column …,*,1996,76
Toward autonomic computing with DB2 universal database,Sam S Lightstone; Guy Lohman; Danny Zilio,Abstract As the cost of both hardware and software falls due to technological advancementsand economies of scale; the cost of ownership for database applications is increasinglydominated by the cost of people to manage them. Databases are growing rapidly in scaleand complexity; while skilled database administrators (DBAs) are becoming rarer and moreexpensive. This paper describes the self-managing or autonomic technology in IBM's DB2Universal Database® for UNIX and Windows to illustrate how self-managing technology canreduce complexity; helping to reduce the total cost of ownership (TCO) of DBMSs andimprove system performance.,ACM Sigmod Record,2002,73
Automated statistics collection in DB2 UDB,Ashraf Aboulnaga; P Haas; M Kandil; S Lightstone; G Lohman; V Markl; I Popivanov; V Raman,Abstract The use of inaccurate or outdated database statistics by the query optimizer in arelational DBMS often results in a poor choice of query execution plans and henceunacceptably long query processing times. Configuration and maintenance of thesestatistics has traditionally been a time-consuming manual operation; requiring that thedatabase administrator (DBA) continually monitor query performance and data changes inorder to determine when to refresh the statistics values and when and how to adjust the setof statistics that the DBMS maintains. In this paper we describe the new Automated StatisticsCollection (ASC) component of IBM® DB2® Universal Database TM (DB2 UDB). Thisautonomic technology frees the DBA from the tedious task of manually supervising thecollection and maintenance of database statistics. ASC monitors both the update-delete …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,68
ROX: relational over XML,Alan Halverson; Vanja Josifovski; Guy Lohman; Hamid Pirahesh; Mathias Mörschel,Abstract An increasing percentage of the data needed by business applications is beinggenerated in XML format. Storing the XML in its native format will facilitate new applicationsthat exchange business objects in XML format and query portions of XML documents usingXQuery. This paper explores the feasibility of accessing natively-stored XML data throughtraditional SQL interfaces; called Relational Over XML (ROX); in order to avoid the costlyconversion of legacy applications to XQuery. It describes the forces that are driving theindustry to evolve toward the ROX scenario as well as some of the issues raised by ROX.The impact of denormalization of data in XML documents is discussed both from a semanticand performance perspective. We also weigh the implications of ROX for manageability andquery optimization. We experimentally compared the performance of a prototype of the …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,64
SMART: making DB2 (More) autonomic,Guy M Lohman; Sam S Lightstone,DB2's Self-Managing And Resource Tuning (SMART) project is developing a stream ofautonomic technology—not just GUIs and handholding—that will actually make DB2 moreself-managing. As part of IBM's Autonomic Computing initiative; SMART will also cooperatewith autonomic features in other IBM components (for example; Web sphere) tosynergistically make the overall system autonomic. While some of SMART's projects areDB2-specific; some of the most challenging problems are much more genetic; and wouldbenefit from academic collaboration.SMART: Making DB2 (More) Autonomic Guy M.Lohman IBM Almaden Research Center K55/B 1;650 Harry Rd. San Jose; CA 95120-6099USA lohman@ almaden. ibm. com Abstract IBM's SMART (Self-Managing And ResourceTuning) project aims to make DB2 self-managing; ie autonomic; to decrease the total cost …,*,2002,62
Optimal policy for batch operations: backup; checkpointing; reorganization; and updating,Guy M Lohman; John A Muckstadt,Abstract Many database maintenance operations are performed periodically in batches;even in realtime systems. The purpose of this paper is to present a general model fordetermining the optimal frequency of these batch operations. Specifically; optimal backup;checkpointing; batch updating; and reorganization policies are derived. The approach usedexploits inventory parallels by seeking the optimal number of items—rather than a timeinterval—to trigger a batch. The Renewal Reward Theorem is used to find the average longrun costs for backup; recovery; and item storage; per unit time; which is then minimized tofind the optimal backup policy. This approach permits far less restrictive assumptions aboutthe update arrival process than did previous models; as well as inclusion of storage costs forthe updates. The optimal checkpointing; batch updating; and reorganization policies are …,ACM Transactions on Database Systems (TODS),1977,62
System and method for automating data partitioning in a parallel database,*,A system for automating data partitioning in a parallel database includes plural nodesconnected in parallel. Each node includes a database server and two databases connectedthereto. Each database server includes a query optimizer. Moreover; a partitioning advisorcommunicates with the database server and the query optimizer. The query optimizer andthe partitioning advisor include a program for recommending and evaluating data tablepartitions that are useful for processing a workload of query statements. The data tablepartitions are recommended and evaluated without requiring the data tables to be physicallyrepartitioned.,*,2009,60
Implementing an interpreter for functional rules in a query optimizer,M Lee; Johann Christoph Freytag; Guy M Lohman,Abstract Query optimizeis translate a high-level; user-submitted query into an efficient planfor executing that query; usually by estimating the execution cost of many differentalternative plans. Existing implementations of these sophisticated but complex componentsof relational database management systems (DBMSs) typically embed the availablestrategies in the optimizer code; making them difficult to modify or enhance as improvedstrategies become available. In the last few years; interest in making DBMSs customizablefor new application areas has magnified this need for flexible specification of executionstrategies in a query optimizer. Several researchers have recently proposed queryoptimizers that are generated from rules for transforming plans into alternative plans.However; little progress has been reported on developing an implementation design that …,Proceeding of the 14th International Conference on Very Large Data Bases,1988,60
Implementing an Interpreter for Functional Rules in a Query Optimizer,Mavis K Leet Johann Christoph FreytagJ; Guy M Lohman,*,Proceedings; very large data bases,1988,60
Star/join query optimization,*,Unwieldy star/join queries are performed more efficiently using a filtered fact table. Suitablequeries include star/join queries with a large fact table joined with multiple subsidiarydimension tables; where indices exist over fact table join columns. The query is analyzed toprepare a query plan for the dimension table accesses. This plan is supplemented byadding nested loop join operations; where the inner table is a dimension table plan and theouter table is an index scan performed over a fact table index of the join column with thedimension table. The plan is also supplemented by filtering records resulting from the nestedloop joins using a sequence of dynamic bit vectors; ultimately yielding a list of probable facttable records. The plan is further supplemented by fetching these records to construct adistilled fact which is used; instead of the large original table; to execute the query in …,*,1999,59
Compiled query execution engine using JVM,Jun Rao; Hamid Pirahesh; C Mohan; Guy Lohman,A conventional query execution engine in a database system essentially uses a SQL virtualmachine (SVM) to interpret a dataflow tree in which each node is associated with a relationaloperator. During query evaluation; a single tuple at a time is processed and passed amongthe operators. Such a model is popular because of its efficiency for pipelined processing.However; since each operator is implemented statically; it has to be very generic in order todeal with all possible queries. Such generality tends to introduce significant runtimeinefficiency; especially in the context of memory-resident systems; because the granularity ofdata commercial system; using SVM. processing (a tuple) is too small compared with theassociated overhead. Another disadvantage in such an engine is that each operator code iscompiled statically; so query-specific optimization cannot be applied. To improve runtime …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,58
Method for detecting and optimizing queries with encoding/decoding tables,*,A join optimizer and method for a relational database management system including a dataprocessor; a stored database; and a plurality of database relations; wherein one or more ofthe relations are retrieved by the processor by means of query commands by performing aplurality of join operations on the relations; the system employing a general purposeheuristic algorithm which excludes or defers Cartesian products as late in the join sequenceas possible; the method includes the steps of determining; in association with the executionof; or preferably prior to executing the general purpose algorithm; whether tables referencedin a query command includes a hub table and at least two encoding tables related to the hubtable and; when the query command references a hub table and at least two encodingtables; determining the best access plan for the hub table; determining whether the best …,*,1999,57
Outerjoin and antijoin reordering using extended eligibility lists,*,An optimization technique that reorders outerjoins and antijoins with inner joins in a bottom-up optimizer of a relational database management system (RDBMS). Each join predicate isassociated with a normal eligibility list (NEL) that includes tables that are referenced in thejoin predicate and an extended eligibility list (EEL) that includes additional tables that arereferenced in conflicting join predicates. An EEL includes all the tables needed by apredicate to preserve the semantics of the original query. During join enumeration; theoptimizer determines whether a join predicate's EEL is a subset of all the tables in twosubplans to be merged; ie; whose EEL is covered. If so; the two subplans are combinedusing the join predicate. Otherwise; the two subplans cannot be joined. Two approaches areused to reordering: without compensation and with compensation. The “without …,*,2003,54
Adaptively reordering joins during query execution,Quanzhong Li; Minglong Shao; Volker Markl; Kevin Beyer; Latha Colby; Guy Lohman,Traditional query processing techniques based on static query optimization are ineffective inapplications where statistics about the data are unavailable at the start of query execution orwhere the data characteristics are skewed and change dynamically. Several adaptive queryprocessing techniques have been proposed in recent years to overcome the limitations ofstatic query optimizers through either explicit re-optimization of plans during execution or byusing a row-routing based approach. In this paper; we present a novel method forprocessing pipelined join plans that dynamically arranges the join order of both inner andouter-most tables at run-time. We extend the Eddies concept of" moments of symmetry" toreorder indexed nested-loop joins; the join method used by all commercial DBMSs forbuilding pipelined query plans for applications for which low latencies are crucial. Unlike …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,48
Parallelizing query optimization,Wook-Shin Han; Wooseong Kwak; Jinsoo Lee; Guy M Lohman; Volker Markl,Abstract Many commercial RDBMSs employ cost-based query optimization exploitingdynamic programming (DP) to efficiently generate the optimal query execution plan.However; optimization time increases rapidly for queries joining more than 10 tables.Randomized or heuristic search algorithms reduce query optimization time for large joinqueries by considering fewer plans; sacrificing plan optimality. Though commercial systemsexecuting query plans in parallel have existed for over a decade; the optimization of suchplans still occurs serially. While modern microprocessors employ multiple cores toaccelerate computations; parallelizing query optimization to exploit multi-core parallelism isnot as straightforward as it may seem. The DP used in join enumeration belongs to thechallenging nonserial polyadic DP class because of its non-uniform data dependencies …,Proceedings of the VLDB Endowment,2008,46
Estimating the compilation time of a query optimizer,*,A compilation time estimator provides a quantified estimate of the optimizer compilation timefor a given query optimizer. The estimator automates the optimizer to choose the right levelof optimization in commercial database systems. The estimator reuses an optimizer's joinenumerator to obtain actual number of joins; but bypasses plan generation to saveestimation overhead; and maintains a small number of interesting physical properties toestimate the number of plans by using a linear regression model. The estimator uses thenumber of generated plans to estimate query compilation time.,*,2008,46
DB2 goes hybrid: Integrating native XML and XQuery with relational data and SQL,K Beyer; Roberta Cochrane; M Hvizdos; Vanja Josifovski; Jim Kleewein; George Lapis; G Lohman; Robert Lyle; Matthias Nicola; Fatma Ozcan; Hamid Pirahesh; Normen Seemann; Ashutosh Singh; T Truong; Robbert C Van der Linden; Brian Vickery; Chun Zhang; Guogen Zhang,Comprehensive and efficient support for XML data management is a rapidly increasingrequirement for database systems. To address this requirement; DB2 UniversalDatabase™(UDB) now combines relational data management with native XML support. Thismakes DB2® a truly hybrid database management system with first-class support for bothXML and relational data processing as well as the integration of the two. This paper presentsthe overall architecture and design aspects of native XML support in DB2 UDB and itsintegration with the relational data-flow engine. We describe the new XML components inDB2 UDB and show how XML processing leverages much of the infrastructure which is usedfor relational data.,IBM Systems Journal,2006,46
Cost-based optimization in DB2 XML,Andrey Balmin; Tom Eliaz; John Hornibrook; L Lim; GM Lohman; D Simmen; M Wang; C Zhang,DB2 XML is a hybrid database system that combines the relational capabilities of DB2Universal Database™(UDB) with comprehensive native XML support. DB2 XML augmentsDB2® UDB with a native XML store; XML indexes; and query processing capabilities forboth XQuery and SQL/XML that are integrated with those of SQL. This paper presents theextensions made to the DB2 UDB compiler; and especially its cost-based query optimizer; tosupport XQuery and SQL/XML queries; using much of the same infrastructure developed forrelational data queried by SQL. It describes the challenges to the relational infrastructurethat supporting XQuery and SQL/XML poses and provides the rationale for the extensionsthat were made to the three main parts of the optimizer: the plan operators; the cardinalityand cost model; and statistics collection.,IBM Systems Journal,2006,45
Using eels; a practical approach to outerjoin and antijoin reordering,Jun Rao; Bruce Lindsay; Guy Lohman; Hamid Pirahesh; David Simmen,Outerjoins and antijoins are two important classes of joins in database systems. Reorderingouterjoins and antijoins with innerjoins is challenging because not all the join orderspreserve the semantics of the original query. Previous work did not consider antijoins andwas restricted to a limited class of queries. We consider using a conventional bottom-upoptimizer to reorder different types of joins. We propose extending each join predicate'seligibility list; which contains all the tables referenced in the predicate. An extended eligibilitylist (EEL) includes all the tables needed by a predicate to preserve the semantics of theoriginal query. We describe an algorithm that can set up the EELs properly in a bottom-uptraversal of the original operator tree. A conventional join optimizer is then modified to checkthe EELs when generating sub-plans. Our approach handles antijoin and can resolve …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,43
Method and system for look ahead query evaluation planning based on interesting partition properties,*,A relational data base management system includes a query processor that uses a queryoperator partition property to perform QEP pruning and to ensure that data input to a queryoperator is partitioned appropriately for the operation. The partition property indicates thegroup of network nodes across which a table is distributed. The query processor also makesuse of partition classes that are designated “interesting classes” to perform preoptimizationplanning and query pruning; and to perform look-ahead partitioning based on partitionclasses that are identified as being of interest to future operations; thereby more efficientlyevaluating complex query statements in an MPP; shared-nothing environment.,*,2002,42
Is query optimization a “solved” problem,Guy Lohman,I asked these same questions almost exactly 25 years ago; in an extended abstract for aWorkshop on Database Query Optimization that was organized by the then-Professor GoetzGraefe at the Oregon Graduate Center [Grae 89a]. Remarkably and quite regrettably; most ofthe issues and critical unsolved problems I identified in that brief rant remain true today.Researchers continue to attack the wrong problems; IMHO: they attack the ones that theycan; ie; that they have ideas for; rather than the ones that they should; ie; that are critical tosuccessfully modeling the true cost of plans and choosing a good one. Perhaps moreimportantly; that will avoid choosing a disastrous plan! At the risk of repeating myself; I'd liketo re-visit these issues; because I'm disappointed that few in the research community havetaken up my earlier challenge.,The ACM SIGMOD Blog,2014,41
Remotely-sensed geophysical databases: experience and implications for generalized DBMS,Guy M Lohman; Joseph C Stoltzfus; Anita N Benson; Michael D Martin; Alfonso F Cardenas,Abstract This paper presents the characteristics of scientific remotely-sensed databases thatare relevant to---and pose unique challenges for---general-purpose database managementsystems (DBMSs). We describe a prototype system that integrates geophysical data and itsmetadata from both satellite and in situ sources; using a relational general-purpose DBMS tomanage the catalog and observational data; and a video optical disk to archive images.Based upon our experience with this application; we suggest augmentations to DBMSs thatwould facilitate their use not only for scientific databases; but also for engineering;document; and even commercial database applications.,ACM Sigmod Record,1983,40
Automatically and adaptively determining execution plans for queries with parameter markers,*,A method and system for automatically and adaptively determining query execution plans forparametric queries. A first classifier trained by an initial set of training points is generated. Aquery workload and/or database statistics are dynamically updated. A new set of trainingpoints is collected off-line. Using the new set of training points; the first classifier is modifiedinto a second classifier. A database query is received at a runtime subsequent to the off-linephase. The query includes predicates having parameter markers bound to actual values.The predicates are associated with selectivities. A mapping of the selectivities into a plandetermines the query execution plan. The determined query execution plan is included in anaugmented set of training points; where the augmented set includes the initial set and thenew set.,*,2011,39
Query optimization method for incrementally estimating the cardinality of a derived relation when statistically correlated predicates are applied,*,A method; apparatus; and article of manufacture for incrementally estimating the cardinalityof a derived relation when statistically correlated predicates are applied. A plurality of queryexecution plans (QEPs) are generated for the query. During the generation of the QEPs; acardinality is computed for any of the QEPs in which two or more predicates are correlated toeach other. The cardinality comprises a number of rows expected to be returned by the QEPand is computed in an incremental fashion for each operator of the QEP. The computationsinclude calculations that may be done prior to the generation of the QEPs and calculationsthat are necessarily done as each operator of a QEP is added to that QEP. Thereafter; one ofthe QEPs is chosen to satisfy the query in a manner that minimizes an estimated cost metric;wherein the cost metric is computed using the cardinality.,*,2004,38
Memory-efficient hash joins,Ronald Barber; Guy Lohman; Ippokratis Pandis; Vijayshankar Raman; Richard Sidle; G Attaluri; Naresh Chainani; Sam Lightstone; David Sharpe,Abstract We present new hash tables for joins; and a hash join based on them; thatconsumes far less memory and is usually faster than recently published in-memory joins.Our hash join is not restricted to outer tables that fit wholly in memory. Key to this hash join isa new concise hash table (CHT); a linear probing hash table that has 100% fill factor; anduses a sparse bitmap with embedded population counts to almost entirely avoid collisions.This bitmap also serves as a Bloom filter for use in multi-table joins. We study the randomaccess characteristics of hash joins; and renew the case for non-partitioned hash joins. Weintroduce a variant of partitioned joins in which only the build is partitioned; but the probe isnot; as this is more efficient for large outer tables than traditional partitioned joins. This alsoavoids partitioning costs during the probe; while at the same time allowing parallel build …,Proceedings of the VLDB Endowment,2014,37
R*: A Research Project on Distributed Relational DBMS.,Laura M.  Haas; Patricia G.  Selinger; Elisa Bertino; Dean Daniels; Bruce G.  Lindsay; Guy M.  Lohman; Yoshifumi Masunaga; C Mohan; Pui Ng; Paul F.  Wilms; Robert A.  Yost,*,Database Engineering Bulletin,1982,35
System and method for matching a plurality of ordered sequences with applications to call stack analysis to identify known software problems,*,The invention finds matches in ordered sequences; eg program function call stacks fromanomalous software program executions for discovering related or identical software flawsand possible known solutions. Call stacks are matched after removing recursive anduninformative subpatterns; eg names of common error handling routines; to see if they weregenerated due to the same possibly known problem. Sequences statistically unlikely to bematches are pruned from the search. Matches found earlier in the sequences may beweighted differently; eg to be more important in call stacks; than other matches. Sequencesare indexed so that those relevant to a query are identified automatically without detailedexhaustive search. Maximum matches are found by optimizing a cost function based onweighted sequence similarity.,*,2010,34
Discovering interestingness in faceted search,*,Exemplary embodiments of the present invention relate to enhanced faceted search supportfor OLAP queries over unstructured text as well as structured dimensions by the dynamicand automatic discovery of dimensions that are determined to be most “interesting” to a userbased upon the data. Within the exemplary embodiments “interestingness” is defined ashow surprising a summary along some dimensions is from a user's expectation. Further;multi-attribute facets are determined and a user is optionally permitted to specify thedistribution of values that she expects; and/or the distance metric by which actual andexpected distributions are to be compared.,*,2008,34
Method; system and program for optimizing compression of a workload processed by a database management system,*,The present invention provides a method; system and program for optimizing compressionof a workload processed by a database management system. In an embodiment of thepresent invention a method of optimizing the compression of database workloads isprovided. Initially; an estimate of a cost of execution for each query according to a definedmetric such as execution time or memory consumption is determined. A sub-set of queries isthen selected from the workload in order of the most costly to least costly relative to thedefined metric for compression according to either a predetermined compression thresholdpercentage or a threshold percentage derived from an allotted workload execution time.Compression is then performed on the selected sub-set of queries (ie those that will benefitthe most from the compression) to achieve a net beneficial trade-off between the cost of …,*,2007,34
System; method; and apparatus for parallelizing query optimization,*,A computer program product that includes a computer useable storage medium to store acomputer readable program that; when executed on a computer; causes the computer toperform operations; including operations to receive a query for which a query execution plan(QEP) is to be computed; divide a search space into a plurality of subproblems for whichconstituent QEPs are to be created; partition the plurality of subproblems into a plurality ofpartitions; and allocate each of the plurality of partitions to a thread of a plurality of threadswithin a multiple thread architecture. Possible QEPs describe a search space. Eachsubproblem references one or more quantifiers and each of the subproblems within apartition references the same number of quantifiers. A partition containing subproblemsreferencing fewer quantifiers is executed before a partition containing subproblems …,*,2012,32
Adaptively reordering joins during query execution,*,A database management system (DBMS) is typically used for storing; querying; and providinginformation for various commercial; industrial; technical; scientific and educationalapplications. As can be appreciated; the volume of information stored in a DBMS increases asa function of the number of users accessing such information by query; and the amount of computingresources required to manage a typical DBMS increases as well … FIG. 1 shows a conventionaldata processing system 10; such as may be embodied in a computer; a computer system; orsimilar programmable electronic system; and can be a stand-alone device or a distributed systemas shown. The data processing system 10 may be responsive to a user input 11 and may compriseat least one computer 13 having a display 15; and a processor 17 in communication with a memory19. The computer 13 may interface with a second computer 21 and a third computer 23 …,*,2010,31
Method for estimating the cost of query processing,*,Provided is a method for modeling the cost of XML as well as relational operators. As withtraditional relational cost estimation; a set of system catalog statistics that summarizes theXML data is exploited; however; the novel use of a set of simple path statistics is alsoproposed. A new statistical learning technique called transform regression is utilized insteadof detailed analytical models to predict the overall cost of an operator. Additionally; a queryoptimizer in a database is enabled to be self-tuning; automatically adapting to changes overtime in the query workload and in the system environment.,*,2009,31
Estimation of column cardinality in a partitioned relational database,*,The present invention is directed to a system; method and computer readable medium forestimating a column cardinality value for a column in a partitioned table stored in a pluralityof nodes in a relational database. According to one embodiment of the present invention; aplurality of column values for the partitioned table stored in each node are hashed; and ahash data set for each node is generated. Each of the hash data sets from each node istransferred to a coordinator node designated from the plurality of nodes. The hash data setsare merged into a merged data set; and an estimated column cardinality value for the tableis calculated from the merged data set.,*,2004,31
Business Analytics in (a) Blink.,Ronald Barber; Peter Bendel; Marco Czech; Oliver Draese; Frederick Ho; Namik Hrle; Stratos Idreos; Min-Soo Kim; Oliver Koeth; Jae-Gil Lee; Tianchao Tim Li; Guy M Lohman; Konstantinos Morfonios; René Müller; Keshava Murthy; Ippokratis Pandis; Lin Qiao; Vijayshankar Raman; Richard Sidle; Knut Stolze; Sandor Szabo,Abstract The Blink project's ambitious goal is to answer all Business Intelligence (BI) queriesin mere seconds; regardless of the database size; with an extremely low total cost ofownership. Blink is a new DBMS aimed primarily at read-mostly BI query processing thatexploits scale-out of commodity multi-core processors and cheap DRAM to retain a (copy ofa) data mart completely in main memory. Additionally; it exploits proprietary compressiontechnology and cache-conscious algorithms that reduce memory bandwidth consumptionand allow most SQL query processing to be performed on the compressed data. Blinkalways scans (portions of) the data mart in parallel on all nodes; without using any indexesor materialized views; and without any query optimizer to choose among them. The Blinktechnology has thus far been incorporated into two IBM accelerator products generally …,IEEE Data Eng. Bull.,2012,30
Method; system and program for selection of database characteristics,*,Disclosed is a method for directing a data processing system; the data processing systemand an article of manufacture. The data processing system has memory for storing adatabase. The method directs the data processing system to configuring the database basedon a database workload having a performance metric. The method includes selecting aplurality of database characteristics based upon the database workload; each of thedatabase characteristics having an associated estimated performance metric for thedatabase workload; selecting a set of database characteristics from the plurality of databasecharacteristics for which the associated estimated performance metric is lower than thedatabase workload performance metric; and configuring the database using the selected setof database characteristics.,*,2008,30
System and Method for Optimizing Query Access to a Database Comprising Hierarchically-Organized Data,*,An cost based optimizer optimizes access to at least a portion of hierarchically-organizeddocuments; such as those formatted using eXtensible Markup Language (XML); byestimating a number of results produced by the access of the hierarchically-organizeddocuments. Estimating the number of results comprises computing the cardinality of eachoperator executing query language expressions and further computing a sequence size ofsequences of hierarchically-organized nodes produced by the query language expressions.Access to the hierarchically-organized documents is optimized using the structure of thequery expression and/or path statistics involving the hierarchically-organized data. Thecardinality and the sequence size are used to calculate a cost estimation for execution ofalternate query execution plans. Based on the cost estimation; an optimal query …,*,2008,30
Information retrieval system and method using index ANDing for improving performance,*,An information retrieval system is adapted to process a query having an associated plan thatincludes applying predicates comprising local predicates and a join predicate; to at least twotables. Index ANDing is used to construct a filter for filtering a first of the tables in accordancewith the local predicates. The join predicate is applied to the tables for determining rowidentifiers of rows satisfying the join predicate. The row identifiers are used to probe the filterfor determining whether the rows satisfying the join predicate also satisfy the localpredicates. The rows that satisfy both the join and local predicates are fetched and the joinpredicate is completed. Results of the join predicate are then returned.,*,2006,30
Predictable query execution through early materialization,*,A method for predictable query execution through early materialization is provided. Themethod deals with the problem of cardinality misestimation in query execution plans; by pre-executing sub-plans on a query execution plan that have questionable estimates andcollecting statistics on the output of these sub-plans. If needed; the overall query executionplan is changed in light of these statistics; before optimizing and executing the remainder ofthe query.,*,2009,28
Estimating compilation time of a query optimizer,Ihab F Ilyas; Jun Rao; Guy Lohman; Dengfeng Gao; Eileen Lin,Abstract A query optimizer compares alternative plans in its search space to find the bestplan for a given query. Depending on the search space and the enumeration algorithm;optimizers vary in their compilation time and the quality of the execution plan they cangenerate. This paper describes a compilation time estimator that provides a quantifiedestimate of the optimizer compilation time for a given query. Such an estimator is useful forautomatically choosing the right level of optimization in commercial database systems. Inaddition; compilation time estimates can be quite helpful for mid-query reoptimization; formonitoring the progress of workload analysis tools where a large number queries need to becompiled (but not executed); and for judicious design and tuning of an optimizer. Previousattempts to estimate optimizer compilation complexity used the number of possible binary …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,28
An extensible processor for an extended relational query language,Thomas J. Watson IBM Research Center. Research Division; LM Haas; WF Cody; JC Freytag; G Lapis; BG Lindsay; GM Lohman; K Ono; H Pirahesh,*,*,1988,28
Computer automated discovery of interestingness in faceted search,*,Exemplary embodiments of the present invention relate to enhanced faceted search supportfor OLAP queries over unstructured text as well as structured dimensions by the dynamicand automatic discovery of dimensions that are determined to be most “interesting” to a userbased upon the data. Within the exemplary embodiments “interestingness” is defined ashow surprising a summary along some dimensions is from a user's expectation. Further;multi-attribute facets are determined and a user is optionally permitted to specify thedistribution of values that she expects; and/or the distance metric by which actual andexpected distributions are to be compared.,*,2009,26
Discovering and exploiting statistical properties for query optimization in relational databases: A survey,Peter J Haas; Ihab F Ilyas; Guy M Lohman; Volker Markl,Abstract Discovering and exploiting statistical features in relational datasets is key to queryoptimization in a relational database management system (RDBMS); and is also needed fordatabase design; cleaning; and integration. This paper surveys a variety of methods forautomatically discovering important statistical features such as correlations; functionaldependencies; keys; and algebraic constraints. We discuss proactive approaches in whichthe data is scanned or sampled (periodically; at optimization time or at query time); or inwhich exploratory queries are executed. Also discussed are reactive approaches thatmonitor the results of the query processing. Finally; we discuss methods for dealing with thepractical challenges of maintaining statistical information in the face of heavy systemutilization; and of dealing with inconsistencies that arise from incomplete cardinality …,*,2009,25
Self-managing technology in IBM DB2 universal database,Daniel Zilio; Sam Lightstone; Kelly Lyons; Guy Lohman,Abstract As the cost of both hardware and software falls due to technological advancementsand economies of scale; the cost of ownership for database applications is increasinglydominated by the cost of people to manage them. Databases are growing rapidly in scaleand complexity; while skilled database administrators (DBAs) are becoming rarer and moreexpensive. The scope of responsibility of DBAs is indeed daunting. This paper describes theself-managing technology in IBM DB2 Universal Database to illustrate how self-managingtechnology can enhance the usability of enterprise middleware and reduce the total cost ofownership (TCO).,Proceedings of the tenth international conference on Information and knowledge management,2001,25
Automatically identifying known software problems,Natwar Modani; Rajeev Gupta; Guy Lohman; Tanveer Syeda-Mahmood; Laurent Mignet,Re-occurrence of the same problem is very common in many large software products. Bymatching the symptoms of a new problem to those in a database of known problems;automated diagnosis and even self-healing for re-occurrences can be (partially) realized.This paper exploits function call stacks as highly structured symptoms of a certain class ofproblems; including crashes; hangs; and traps. We propose and evaluate algorithms forefficiently and accurately matching call stacks by a weighted metric of the similarity of theirfunction names; after first removing redundant recursion and uninformative (poordiscriminator) functions from those stacks. We also describe a new indexing scheme tospeed queries to the repository of known problems; without compromising the quality ofmatches returned. Experiments conducted using call stacks from actual product problem …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,22
Fast and robust optimization of complex database queries,*,A robust way is described for optimizing complex data base queries while retaining theoptimization speed of heuristic methods. The heuristic join-sequencing algorithm is modifiedto permit any of; or a combination of:(1) multiple passes of the heuristic algorithm; each witha different metric; producing multiple plans;(2) complex combinations of the criteria by whichsuch heuristics make their choices; and/or (3) backtracking to consider alternatives to anyparticular decision in the sequence.,*,2006,22
Learning table access cardinalities with LEO,Volker Markl; Guy Lohman,Abstract LEO is a comprehensive way to repair incorrect statistics and cardinality estimatesof a query execution plan. LEO introduces a feedback loop to query optimization thatenhances the available information on the database where the most queries have occurred;allowing the optimizer to actually learn from its past mistakes. We demonstrate how LEOlearns outdated table access statistics on a TPC-H like database schema and show that LEOimproves the estimates for table cardinalities as well as filter factors for single predicates.Thus LEO enables the query optimizer to choose a better query execution plan; resulting inmore efficient query processing. We not only demonstrate learning by repetitive execution ofa single query; but also illustrate how similar; but not identical queries benefit from learnedknowledge. In addition; we show the effect of both learning cardinalities and adjusting …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,22
Discovering interestingness in faceted search,*,Exemplary embodiments of the present invention relate to enhanced faceted search supportfor OLAP queries over unstructured text as well as structured dimensions by the dynamicand automatic discovery of dimensions that are determined to be most “interesting” to a userbased upon the data. Within the exemplary embodiments “interestingness” is defined ashow surprising a summary along some dimensions is from a user's expectation. Further;multi-attribute facets are determined and a user is optionally permitted to specify thedistribution of values that she expects; and/or the distance metric by which actual andexpected distributions are to be compared.,*,2009,21
Efficient sampling of a relational database,*,A system; method and computer readable medium for sampling data from a relationaldatabase are disclosed; where an information processing system chooses rows from a tablein a relational database for sampling; wherein data values are arranged into rows; rows arearranged into pages; and pages are arranged into tables. Pages are chosen for samplingaccording to a probability P and rows in a selected page are chosen for sampling accordingto a probability R; so that the overall probability of choosing a row for sampling is Q= PR.The probabilities P and R are based on the desired precision of estimates computed from asample; as well as processing speed. The probabilities P and R are further based on eithercatalog statistics of the relational database or a pilot sample of rows from the relationaldatabase.,*,2006,21
Extensible enumeration of feasible joins for relational query optimization,Kiyoshi Ono; Guy M Lohman,*,*,1988,21
Usability and design considerations for an autonomic relational database management system,Ric Telford; Randy Horman; Sam Lightstone; Nikolay Markov; Stephen O'Connell; G Lohman,Autonomic systems offer numerous advantages over non-autonomic systems; and many ofthese advantages relate to ease of use. The advantages regarding ease of use includereducing the number of low-level system administration tasks; simplifying the systemadministrator's interface; handling exceptions which would otherwise have resulted insystem alerts; and the learning; by the system; of actions taken by the administrator.However; human intervention must still be factored in; and care must be taken in the designof autonomic systems not to make the system administrator's task more difficult. This paperexamines the ease-of-use ramifications of autonomic computing in the context of relationaldatabases in general; and of the IBM® DB2® Universal Database™ Version 8.1 autonomiccomputing system in particular.,IBM Systems Journal,2003,17
Self-managing technology in database management systems,Surajit Chaudhuri; Benoit Dageville; Guy M Lohman,We are increasingly dependent on information systems for business as well as for personalusage. However; for information systems to provide value to their customers; we mustreduce the complexity associated with their deployment and usage. While the cost ofhardware and software in such systems continue to decrease dramatically throughtechnological advances and competition; the total cost of ownership (TCO) of informationtechnology is increasingly dominated by people costs.,VLDB,2004,16
Method for monitoring dependent metric streams for anomalies,*,A method for monitoring dependent metric streams for anomalies including identifying aplurality of sets of dependent metric streams from a plurality of metric streams of a computersystem by measuring an association of the plurality of metric streams using a statisticaldependency measure analysis; wherein each set includes a plurality of the dependentmetric streams and each metric stream includes a plurality of data; determining a subset ofthe plurality of sets of dependent metric streams to monitor by selecting a quantity of the setsof dependent metric streams that have a highest statistical dependency; cleaning the data ofeach set of dependent metric streams of the subset by identifying and removing outlier data;fitting a probability density function to the cleaned data of each set of dependent metricstreams of the subset; wherein the probability density function is a likelihood function that …,*,2010,15
Autonomic features of the IBM DB2 universal database for linux; UNIX; and windows,Christian M Garcia-Arellano; Sam S Lightstone; Guy M Lohman; Volker Markl; Adam J Storm,Thevast majority of the world's structured data are now stored and managed by relationaldatabase management systems (RDBMSs). These systems provide powerful datamanagement capabilities. However; as the power and functionality of these systems hasgrown; so has the complexity of their administration. In this paper; we show how the IBMDB2 Universal Database for Linux; UNIX; and Windows (DB2 UDB) product has exploitedautonomic computing to reduce this administration complexity and become more self-managing. We survey the major autonomic computing features in the DB2 UDB product anddescribe the benefits; with experimental data in some cases,Systems; Man; and Cybernetics; Part C: Applications and Reviews; IEEE Transactions on,2006,15
Making DB2 products self-managing: Strategies and experiences,Sam Lightstone; G Lohman; P Haas; Volker Markl; Jun Rao; Adam Storm; Maheswaran Surendra; D Zilio,Abstract This paper evaluates the impact of the DB2 Autonomic Computing project at theIBM Toronto Software Lab; Almaden Research Center; and Watson Research Center. Itdescribes the key ideas behind the many self-managing features added to the IBMR сDB2 Rсfor Linux R с; UNIX R с; and Windows R сproducts; and evaluates the degree to whichthese features have been accepted by the DB2 user community. We offer lessons learnedfrom this experience; our conclusions; and future directions for self-managing databases.,IEEE Data Engineering Bulletin,2006,15
Client-based index advisor,*,A common interface to manage heterogeneous databases and develop enterprise classapplications is provided. In particular; it is shown that a client-based system and methodcan:(a) provide a uniform interface for the DBA or the application developer to use across allthe database deployments;(b) provide flexibility in the number and kinds of scenarios it canbe used; and finally (c) reduce the total cost of ownership for the enterprise.,*,2013,14
Method and system for finding structures in multi-dimensional spaces using image-guided clustering,*,A method is provided clustering data points in a multidimensional dataset in amultidimensional image space that comprises generating a multidimensional image from themultidimensional dataset; generating a pyramid of multidimensional images having varyingresolution levels by successively performing a pyramidal sub-sampling of themultidimensional image; identifying data clusters at each resolution level of the pyramid byapplying a set of perceptual grouping constraints; and determining levels of a clusteringhierarchy by identifying each salient bend in a variation curve of a magnitude of identifieddata clusters as a function of pyramid resolution level.,*,2008,14
In-memory BLU acceleration in IBM's DB2 and dashDB: Optimized for modern workloads and hardware architectures,Ronald Barber; Guy Lohman; Vijayshankar Raman; Richard Sidle; Sam Lightstone; Berni Schiefer,Although the DRAM for main memories of systems continues to grow exponentiallyaccording to Moore's Law and to become less expensive; we argue that memory hierarchieswill always exist for many reasons; both economic and practical; and in particular due toconcurrent users competing for working memory to perform joins and grouping. We presentthe in-memory BLU Acceleration used in IBM's DB2 for Linux; UNIX; and Windows; and nowalso the dashDB cloud offering; which was designed and implemented from the ground upto exploit main memory but is not limited to what fits in memory and does not require manualmanagement of what to retain in memory; as its competitors do. In fact; BLU Accelerationviews memory as too slow; and is carefully engineered to work in higher levels of the systemcache by keeping the data encoded and packed densely into bit-aligned vectors that can …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,13
Optimizing GPU-accelerated Group-By and Aggregation.,Tomas Karnagel; Rene Mueller; Guy M Lohman,ABSTRACT The massive parallelism and faster random memory access of GraphicsProcessing Units (GPUs) promise to further accelerate complex analytics operations such asjoins and grouping; but also provide additional challenges to optimizing their performance.There are more implementation alternatives to consider on the GPU; such as exploitingdifferent types of memory on the device and the division of work among processor clustersand threads; and additional performance parameters; such as the size of the kernel grid andthe trade-off between the number of threads and the resulting share of resources eachthread will get. In this paper; we study in depth offloading to a GPU the grouping andaggregation operator; often the dominant operation in analytics queries after joins. Weprimarily focus on the design implications of a hash-based implementation; although we …,ADMS@ VLDB,2015,11
Blink: Not Your Father’s Database!,Ronald Barber; Peter Bendel; Marco Czech; Oliver Draese; Frederick Ho; Namik Hrle; Stratos Idreos; Min-Soo Kim; Oliver Koeth; Jae-Gil Lee; Tianchao Tim Li; Guy Lohman; Konstantinos Morfonios; Rene Mueller; Keshava Murthy; Ippokratis Pandis; Lin Qiao; Vijayshankar Raman; Sandor Szabo; Richard Sidle; Knut Stolze,Abstract The Blink project's ambitious goals are to answer all Business Intelligence (BI)queries in mere seconds; regardless of the database size; with an extremely low total cost ofownership. It takes a very innovative and counter-intuitive approach to processing BIqueries; one that exploits several disruptive hardware and software technology trends.Specifically; it is a new; workload-optimized DBMS aimed primarily at BI query processing;and exploits scale-out of commodity multi-core processors and cheap DRAM to retain a(copy of a) data mart completely in main memory. Additionally; it exploits proprietarycompression technology and cache-conscious algorithms that reduce memory bandwidthconsumption and allow most SQL query processing to be performed on the compresseddata. Ignoring the general wisdom of the last three decades that the only way to scalably …,International Workshop on Business Intelligence for the Real-Time Enterprise,2011,11
Efficient partitioned joins in a database with column-major layout,*,Embodiments of the present invention provide a database processing system for efficientpartitioning of a database table with column-major layout for executing one or more joinoperations. One embodiment comprises a method for partitioning a database table withcolumn-major layout; partitioning only the join-columns by limiting the partitions by size andnumber; executing one or more join operations for joining the partitioned columns; andoptionally de-partitioning the join result to the original order by sequentially writing andrandomly reading table values using P cursors.,*,2016,10
Joins on encoded and partitioned data,Jae-Gil Lee; Gopi Attaluri; Ronald Barber; Naresh Chainani; Oliver Draese; Frederick Ho; Stratos Idreos; Min-Soo Kim; Sam Lightstone; Guy Lohman; Konstantinos Morfonios; Keshava Murthy; Ippokratis Pandis; Lin Qiao; Vijayshankar Raman; Vincent Kulandai Samy; Richard Sidle; Knut Stolze; Liping Zhang,Abstract Compression has historically been used to reduce the cost of storage; I/Os from thatstorage; and buffer pool utilization; at the expense of the CPU required to decompress dataevery time it is queried. However; significant additional CPU efficiencies can be achieved bydeferring decompression as late in query processing as possible and performing queryprocessing operations directly on the still-compressed data. In this paper; we investigate thebenefits and challenges of performing joins on compressed (or encoded) data. Wedemonstrate the benefit of independently optimizing the compression scheme of each joincolumn; even though join predicates relating values from multiple columns may requiretranslation of the encoding of one join column into the encoding of the other. We also showthe benefit of compressing" payload" data other than the join columns" on the fly;" to …,Proceedings of the VLDB Endowment,2014,9
Impliance: A next generation information management appliance,Bishwaranjan Bhattacharjee; Vuk Ercegovac; Joseph Glider; Richard Golding; Guy Lohman; Volke Markl; Hamid Pirahesh; Jun Rao; Robert Rees; Frederick Reiss; Eugene Shekita; Garret Swart,Abstract: ably successful in building a large market and adapting to the changes of the lastthree decades; its impact on the broader market of information management is surprisinglylimited. If we were to design an information management system from scratch; based upontoday's requirements and hardware capabilities; would it look anything like today's databasesystems?" In this paper; we introduce Impliance; a next-generation information managementsystem consisting of hardware and software components integrated to form an easy-to-administer appliance that can store; retrieve; and analyze all types of structured; semi-structured; and unstructured information. We first summarize the trends that will shapeinformation management for the foreseeable future. Those trends imply three majorrequirements for Impliance:(1) to be able to store; manage; and uniformly query all data …,arXiv preprint cs/0612129,2006,9
Finding structures in multi-dimensional spaces using image-guided clustering,*,A data processing system is provided that comprises a processor; a random access memoryfor storing data and programs for execution by the processor; and computer readableinstructions stored in the random access memory for execution by the processor to perform amethod for clustering data points in a multidimensional dataset in a multidimensional imagespace. The method comprises generating a multidimensional image from themultidimensional dataset; generating a pyramid of multidimensional images having varyingresolution levels by successively performing a pyramidal sub-sampling of themultidimensional image; identifying data clusters at each resolution level of the pyramid byapplying a set of perceptual grouping constraints; and determining levels of a clusteringhierarchy by identifying each salient bend in a variation curve of a magnitude of identified …,*,2009,7
Data classification by kernel density shape interpolation of clusters,*,A data processing system is provided that comprises a processor; a random access memoryfor storing data and programs for execution by the processor; and computer readableinstructions stored in the random access memory for execution by the processor to perform amethod for obtaining a shape interpolated representation of shapes of clusters in an imageof a clustered dataset. The method comprises generating a density estimate value of eachgrid point of a set of grid points sampled from the image at a specified resolution for eachcluster using a kernel density function; evaluating the density estimate value of each gridpoint for each cluster to identify a maximum density estimate value of each grid point and acluster associated with the maximum density estimate value; and adding each grid point forwhich the maximum density estimate value exceeds a specified threshold to the …,*,2009,7
Method for data classification by kernel density shape interpolation of clusters,*,A method for obtaining a shape interpolated representation of shapes of one or moreclusters in an image of a dataset that has been clustered comprises generating a densityestimate value of each grid point of a set of grid points sampled from the image at a specifiedresolution for each cluster in the image using a kernel density function; evaluating thedensity estimate value of each grid point for each cluster to identify a maximum densityestimate value of each grid point and a cluster associated with the maximum densityestimate value of each grid point; and adding each grid point for which the maximum densityestimate value exceeds a specified threshold to the cluster associated with the maximumdensity estimate value for the grid point to form a shape interpolated representation of theone or more clusters.,*,2008,7
On-the-fly encoding method for efficient grouping and aggregation,*,Embodiments include a system for encoding data while it is being processed. The systemincludes a processor; an encoder and a decoder. The processor is configured to process aquery request by determining a set of values. The encoder is configured for encoding the setof values; such that a subsequent processing operation can be performed on the encodedvalues. The processor performs the subsequent processing operations. The decoder isconfigured for decoding each value back to its value prior to being encoded uponcompletion of the processor completing the requested query.,*,2016,6
Finding structures in multi-dimensional spaces using image-guided clustering,*,A method executed on a computer for determining a hierarchical clustering of amultidimensional dataset in a multidimensional image space comprises receiving a pyramidof multidimensional images of the multidimensional dataset in which the images of thepyramid representing a first multidimensional image of the multidimensional dataset atsuccessively lower resolution levels; identifying data clusters at each resolution level of thepyramid by applying a set of perceptual grouping constraints; plotting a variation curve of amagnitude of data clusters identified at each resolution level of the pyramid as a function ofresolution level; and generating a clustering hierarchy for the multidimensional dataset byidentifying the resolution level at each salient bend in the variation curve as a level of theclustering hierarchy.,*,2009,6
Progressive optimization in action,Vijayshankar Raman; Volker Markl; David Simmen; Guy Lohman; Hamid Pirahesh,Abstract Progressive Optimization (POP) is a technique to make query plans robust; andminimize need for DBA intervention; by repeatedly re-optimizing a query during runtime ifthe cardinalities estimated during optimization prove to be significantly incorrect. POP worksby carefully calculating validity ranges for each plan operator under which the overall plancan be optimal. POP then instruments the query plan with checkpoints that validate atruntime that cardinalities do lie within validity ranges; and re-optimizes the query otherwise.In this demonstration we showcase POP implemented for a research prototype version ofIBM's DB2 DBMS; using a mix of real-world and synthetic benchmark databases andworkloads. For selected queries of the workload we display the query plans with validityranges as well as the placement of the various kinds of CHECK operators using the DB2 …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,6
Heuristic Method for Joining Relational Database Tables,GM Lohman,*,IBM Technical Disclosure Bulletin,1988,6
On-the-fly encoding method for efficient grouping and aggregation,*,Embodiments include a method and computer program product for encoding data while it isbeing processed as part of a query is provided. The method includes receiving a queryrequest and determining a set of values associated with data to be encoded for completingthe query request. The method also includes encoding those values such that anysubsequent processing operations can be performed on the encoded values to complete therequested query. After performing the subsequent processing operations to complete therequested query; each value is decoded back to its original value.,*,2016,5
Efficient join with one or more large dimension tables,*,Embodiments of the invention relate to processing queries that utilize fact and/or dimensiontables. In one aspect; a pre-join filtering phase precedes a star join. The necessaryconditions for the pre-join filtering are considered for a given SQL query; including anestimated size of the hash table exceeding a threshold and presence of a local predicateeither on the fact table or one or more dimension tables that is not a large dimension table.Once the necessary conditions are satisfied; the execution of the query exploits the pre-joinfiltering to build a pre-join output filter from columns of a reduced fact table that joins witheach large dimension table. Thereafter; all the dimension tables and the fact table are joinedin a star join while exploiting each pre-join filter.,*,2015,5
Query execution in query processing systems,*,A query processing system having a data manager; and a query manager also includes abuffer. The query manager calls the data manager to access data based on a query. Wherethere is no predicate check or consumption operation on the record accessed; the datamanager will notionally return the record to the query manager. However; the data manageraccomplishes the return by writing the relevant portions of the record to a buffer. The datamanager maintains stabilization of the page containing the record while the buffer is beingwritten to. The data manager continues to access records on the stabilized page and to writesuch records to the buffer where appropriate. The query manager retrieves the records fromthe buffer after the data manager has completed its operation resulting from the querymanager call.,*,2006,5
Trends in automating physical database design,Daniel C Zilio; Sam Lightstone; Guy M Lohman,A database administrator (DBA) must make many decisions about what is the best decisionfor a database and its users-including what is the best physical DB design decision. TheDBA must determine what auxiliary data structures to create-such as indexes andmaterialized views-as well as the optimal way to partition each table in a shared-nothingpartitioned system; to maximize workload performance while minimizing any increase in diskspace or maintenance costs. Given the number of possible configurations; the possibleinteractions between each choice; and the associated constraints on the decision; theproblem of selecting an optimal design is daunting for even the most skilled DBA. It has longbeen recognized that DBAs need tools to help them make these complex decisions; but untilrecently little progress was made. We present our vision for the future of automating …,Industrial Informatics; 2003. INDIN 2003. Proceedings. IEEE International Conference on,2003,5
Fast multi-tier indexing supporting dynamic update,*,A method includes performing a lookup using a key into a root node of a multi-tier datastructure; to find a partition for performing an insert. A lookup for the key is performed on afirst level index that is part of a linked data structure. A payload or reference is added to thelinked data structure based on data structure criterion; otherwise the key and the payloadare added to the linked data structure if the key is not found. A new first level index is createdand added to the linked data structure upon the linked data structure remaining unchanged.The key and the payload or reference are added to the new index. Based on merge criterion;a new second level index is created and a portion of content from selected first level andsecond level indexes are merged for combining into the new second level index.,*,2016,4
Wildfire: Concurrent blazing data ingest and analytics,Ronald Barber; Matt Huras; Guy Lohman; C Mohan; Rene Mueller; Fatma Özcan; Hamid Pirahesh; Vijayshankar Raman; Richard Sidle; Oleg Sidorkin; Adam Storm; Yuanyuan Tian; Pinar Tözun,Abstract We demonstrate Hybrid Transactional and Analytics Processing (HTAP) on theSpark platform by the Wildfire prototype; which can ingest up to~ 6 million inserts per secondper node and simultaneously perform complex SQL analytics queries. Here; a simplifiedmobile application uses Wildfire to recommend advertising to mobile customers based upontheir distance from stores and their interest in products sold by these stores; whilecontinuously graphing analytics results as those customers move and respond to the adswith purchases.,Proceedings of the 2016 International Conference on Management of Data,2016,4
On common tools for databases-The case for a client-based index advisor,Sandeep Tata; Lin Qiao; Guy M Lohman,Modern enterprises often deploy multiple databases from different vendors. Managing aheterogeneous mix of databases is a very challenging exercise. To help the DBA tackle thiscomplex administrative task; major database vendors have provided many autonomic tools.The tools help automate common management tasks and even help in performance tuning.However; the DBA now has to face the complexity of dealing with a variety of different toolsfor different tasks; each with different interfaces and capabilities. The application developeralso suffers from a similar problem when trying to use different tools that help him developapplications for different databases. Clearly; there is a need for a common interface tomanage heterogeneous databases and develop enterprise class applications for them. Toaddress this need; we argue for a client-based approach to developing database tools. In …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,4
Is (your) database research having impact?,Guy Lohman,Abstract Is your research having real impact? The ultimate test of the research done by thiscommunity is how it impacts society. Perhaps the most important metric of this impact isacceptance in the marketplace; ie incorporation into products that bring value to thepurchaser. Merely publishing papers and getting them referenced has no intrinsic valueunless the ideas therein are eventually used by someone. So let us ask ourselves candidly–is (my) database research having (positive) impact? Concisely: Are they buying my stuff?Have the “hot topics” of the past withstood the test of time by actually being used in productsthat sold? If so; what characteristics were instrumental in their success? And if not; why didsomething that got so many people excited fail to gain traction with users? Perhaps moreimportantly; what can we learn from our track record of the past in order to have better …,Advances in Databases: Concepts; Systems and Applications,2007,4
Automated statistics collection in action,P Haas; M Kandil; A Lerner; V Markl; I Popivanov; V Raman; D Zilio,Abstract If presented with inaccurate statistics; even the most sophisticated query optimizersmake mistakes. They may wrongly estimate the output cardinality of a certain operation andthus make sub-optimal plan choices based on that cardinality. Maintaining accurate statisticsis hard; both because each table may need a specifically parameterized set of statistics andbecause statistics get outdated as the database changes. Automated Statistic Collection(ASC) is a new component in IBM DB2 UDB that; without any DBA intervention; observesand analyzes the effects of faulty statistics and; in response; it triggers actions thatcontinuously repair the latter. In this demonstration; we will show how ASC works to alleviatethe DBA from the task of maintaining fresh; accurate statistics in several challengingscenarios. ASC is able to reconfigure the statistics collection parameters (eg; number of …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,4
Panel discussion on semantic query optimization,GM Lohman,*,Proc. Data Engineering Conf,1985,4
Optimal data storage and organization in computerized information processing systems subject to failures.,Guy Maring Lohman,*,*,1977,4
Efficient partitioned joins in a database with column-major layout,*,Abstract Embodiments of the present invention provide a database processing system forefficient partitioning of a database table with column-major layout for executing one or morejoin operations. One embodiment comprises a method for partitioning a database table withcolumn-major layout; partitioning only the join-columns by limiting the partitions by size andnumber; executing one or more join operations for joining the partitioned columns; andoptionally de-partitioning the join result to the original order by sequentially writing andrandomly reading table values using P cursors.,*,2018,3
Evolving Databases for New-Gen Big Data Applications.,Ronald Barber; Christian Garcia-Arellano; Ronen Grosman; Rene Mueller; Vijayshankar Raman; Richard Sidle; Matt Spilchen; Adam J Storm; Yuanyuan Tian; Pinar Tözün; Daniel C Zilio; Matt Huras; Guy M Lohman; Chandrasekaran Mohan; Fatma Özcan; Hamid Pirahesh,ABSTRACT The rising popularity of large-scale real-time analytics applications (real-timeinventory/pricing; mobile apps that give you suggestions; fraud detection; risk analysis; etc.)emphasize the need for distributed data management systems that can handle fasttransactions and analytics concurrently. Efficient processing of transactional and analyticalrequests; however; require different optimizations and architectural decisions in a system.This paper presents the Wildfire system; which targets Hybrid Transactional and AnalyticalProcessing (HTAP). Wildfire leverages the Spark ecosystem to enable large-scale dataprocessing with different types of complex analytical requests; and columnar dataprocessing to enable fast transactions and analytics concurrently.,CIDR,2017,3
Massively-parallel lossless data decompression,Evangelia Sitaridi; Rene Mueller; Tim Kaldewey; Guy Lohman; Kenneth A Ross,Today's exponentially increasing data volumes and the high cost of storage makecompression essential for the Big Data industry. Although research has concentrated onefficient compression; fast decompression is critical for analytics queries that repeatedlyread compressed data. While decompression can be parallelized somewhat by assigningeach data block to a different process; break-through speed-ups require exploiting themassive parallelism of modern multi-core processors and GPUs for data decompressionwithin a block. We propose two new techniques to increase the degree of parallelism duringdecompression. The first technique exploits the massive parallelism of GPU and SIMDarchitectures. The second sacrifices some compression efficiency to eliminate datadependencies that limit parallelism during decompression. We evaluate these techniques …,Parallel Processing (ICPP); 2016 45th International Conference on,2016,3
Efficient partitioned joins in a database with column-major layout,*,In database systems; hash joins are a commonly used operation in data warehouse queryprocessing. An important challenge for hash joins is managing the sizes of the hash tableused. Many systems employ partitioning to split hash tables into manageable sized chunks. Oneside of the hash join is chosen as inner and its hash table is partitioned. The inner table is alsoreferred to in the literature as the “build” side. The rows of the other side (the outer table) of thejoin are also partitioned and then used to look up into the partitioned hash tables. The outer tableis also referred to in the literature as the “probe” side. Traditionally; databases were disk-residentwherein the goal was to partition hash tables into memory-sized chunks. However; as main memorieshave grown much larger; the goal is to partition hash tables into cache-sized chunks … Oneembodiment comprises a method for joining database tables in a query. In one …,*,2016,3
Data encoding and processing columnar data,*,Aspects of the invention are provided for accessing a plurality of data elements. A page ofcolumn data is stored in a format that includes compressed and/or non-compressedelements; with the format including a plurality of arrays and a vector. Each of the arraysstores elements with common characteristics; with the vector functioning as a mapping to thestored data elements. The vector is leveraged to identify an array and determine an offset tosupport access to one or more of the data elements.,*,2016,3
Method and system for content-based encrypted access to a database,*,Some aspects of the invention provide methods; systems; and computer program productsfor inserting an encrypted problem signature into a symptom database. A problem signatureis first provided; which is then used to derive a hash value. The problem signature is thenencrypted using the hash value. The hash value is further used to indicate the location in thedatabase to insert the problem signature. The problem signature is then inserted at thelocation in the database indicated by the location.,*,2012,3
You think your DBMS is complex now…?,Guy M Lohman,The last two decades have seen phenomenal growth and progress in databasemanagement systems (DBMSs). With effective standardization on the SQL query language;competition has increased functionality and performance; while driving down cost; at adizzying pace. Relational DBMSs have supplanted all other types of DBMSs; even forextremely large; mission-critical applications of the largest corporations.Products havefocused primarily on the traditional applications: transactions and complex queries of small;uniform records; composed primarily of character and numeric business data. Performanceof these applications has been accelerated by improvements in both hardware and theDBMS software itself. The progress can be seen in benchmarks standardized by theTransaction Processing Council: the TPC-C benchmark for transactions now boasts tens …,ACM Computing Surveys (CSUR),1996,3
Lohman et al. Extensions to Starburst: objects; types; functions and rules,GM LLPS91,*,Com. of the ACM,1991,3
Pooling work across multiple transactions for reducing contention in operational analytics systems,*,A method includes scanning multiple incoming database transaction requests. Eachtransaction includes one or more operations. Operations are clustered into a set ofcombined operations based on type of operation constraints. Log records are prepared andwritten for re-performing operations upon system failures; and for undoing operations uponan operation or a transaction failing to be processed fully. Each set of combined operationsare performed within a thread. Each update operation is marked for a transaction withinwhich the update operation belongs. Recoverable update operations belonging to a pluralityof transactions are performed within a single logical thread of execution.,*,2017,2
Intra-block partitioning for database management,*,A method for storing database information includes storing a table having data values in acolumn major order. The data values are stored in a list of blocks. The method also includesassigning a tuple sequence number (TSN) to each data value in each column of the tableaccording to a sequence order in the table. The data values that correspond to each otheracross a plurality of columns of the table have equivalent TSNs. The method also includesassigning each data value to a partition based on a representation of the data value. Themethod also includes assigning a tuple map value to each data value. The tuple map valueidentifies the partition in which each data value is located.,*,2017,2
Lock-free creation of hash tables in parallel,*,A hash table is created in parallel without requiring a lock or random accesses to memory.The hash table of a database system is logically partitioned and a separate thread isassigned to each partition of the hash table. As many separate threads as can fit theircorresponding hash table partitions into the processor's cache are executed in parallel withother threads without a lock. Execution of a number of separate threads includes: scanningan input data table for a thread's partition and applying a hash function to each key; insertingdata of keys that hash to the thread's partition into the thread's partition; and ignoring keysthat do not hash to the thread's partition.,*,2016,2
Concurrent reads and inserts into a data structure without latching or waiting by readers,*,A method includes performing; by a data structure processor; concurrent read and writeoperations into a hierarchical data structure. Writers acquire latches on the hierarchical datastructure elements that the latches modify. The hierarchical data structure elements aredirectly accessed by readers without acquiring latches. A modify operation is executed by awriter for one or more levels of the hierarchical data structure. When removed portions of thehierarchical data structure are no longer referenced; tracking is performed by use of acombination of a global state value and a copied local state value. The global state valuetransitions through a non-repeating sequence of values. No longer referenced portions ofthe hierarchical data structure are tagged with the current global state value.,*,2016,2
Efficient join on dynamically compressed inner for improved fit into cache hierarchy,*,A method includes joining data between at least two data sets. Values of one or more joinattributes of each of the data sets is represented in a compressed form; indicated by anencoding scheme. A compression scheme for the one or more join attributes is dynamicallyselected.,*,2016,2
Efficient performance of insert and point query operations in a column store,*,A method includes logically organizing; by an object hierarchy processor; data objects in afirst hierarchy. A portion of the data objects in the first hierarchy logically includes groupingsof other data objects. The object hierarchy processor physically organizes the data objectsacross two or more types of memory in a second hierarchy. Another portion of the dataobjects in the second hierarchy physically includes groupings of other data objects.Groupings of the data objects in the second hierarchy are dynamically moved across the twoor more types of memory. Levels of access of the data objects are tracked using a datastructure that maps groupings of the data objects in the first hierarchy onto metadatainformation including combined access frequencies of the data objects; and current numberof accessors to the data objects; in each grouping of the data objects.,*,2016,2
On-the-fly encoding method for efficient grouping and aggregation,*,Embodiments include a method; system; and computer program product for encoding datawhile it is being processed as part of a query is provided. The method includes receiving aquery request and determining a set of values associated with data to be encoded forcompleting the query request. The method also includes encoding those values such thatany subsequent processing operations can be performed on the encoded values tocomplete the requested query. After performing the subsequent processing operations tocomplete the requested query; each value is decoded back to its original value.,*,2016,2
Data shuffling in a non-uniform memory access device,*,Embodiments relate to the orchestration of data shuffling among memory devices of a non-uniform memory access device. An aspect includes a method of orchestrated shuffling ofdata in a non-uniform memory access device includes running an application on a pluralityof threads executing on a plurality of processing nodes and identifying data to be shuffledamong the plurality of processing nodes. The method includes registering the data to beshuffled and generating a plan for orchestrating the shuffling of the data. The method furtherincludes disabling cache coherency of cache memory associated with the processing nodesand shuffling the data among all of the memory devices upon disabling the cachecoherency; the shuffling performed based on the plan for orchestrating the shuffling. Themethod further includes restoring the cache coherency of the cache memory based on …,*,2016,2
ReoptSMART: A Learning Query Plan Cache,Julia Stoyanovich; Kenneth A Ross; Jun Rao; Wei Fan; Volker Markl; Guy Lohman,Abstract: The task of query optimization in modern relational database systems is importantbut can be computationally expensive. Parametric query optimization (PQO) has as its goalthe prediction of optimal query execution plans based on historical results; withoutconsulting the query optimizer. We develop machine learning techniques that can accuratelymodel the output of a query optimizer. Our algorithms handle non-linear boundaries in planspace and achieve high prediction accuracy even when a limited amount of data is availablefor training. We use both predicted and actual query execution times for learning; and arethe first to demonstrate a total net win of a PQO method over a state-of-the-art queryoptimizer for some workloads. ReoptSMART realizes savings not only in optimization time;but also in query execution time; for an over-all improvement by more than an order of …,*,2008,2
The DB2 Universal Database Optimizer,Guy M Lohman,Page 1. © IBM 2007 The DB2 Universal Database* Optimizer Guy M. Lohman lohman@almaden.ibm.com IBM Research Division IBM Almaden Research Center K55/B1; 650 Harry Road SanJose; CA 95120 * Now called; simply; DB2 9 Page 2. © IBM 2007 DB2 UDB Query OptimizerAgenda ∎ Overview of Query Processing ∎ Query ReWrite ∎ Plan Selection Optimization basics ∎Elements of Optimization ∎ Execution Strategies ∎ Cost model & plan properties ∎ Search strategy∎ Parallelism ∎ Special strategies for OLAP & BI ∎ Engineering considerations ∎ Conclusionsand Future ∎ NOTE: Use DB2 for free for academic purposes! See: http://www-304.ibm.com/jct09002c/university/scholars/products/data/ Page 3. © IBM 2007 DB2 UDB Query OptimizerStretching the Boundaries: Query Processing Challenge ∎ Many platforms; but one codebase …,IBM Research Division; IBM Almaden Research Center,2007,2
Reminiscences in influential papers,Richard Snodgrass,Fundamentally; we are in this business to embrace the novel concepts that intrigue anddelight us. These insights are generated or discovered by one or a few; then somehowpropagated through the fabric of the intellectual community; via publication; presentation;and informal conversation. However; the transference is often more accidental thanintentional. The common model is of a researcher having a question and searching for thepaper that answers that question. Seemingly just as prevalent is a haphazard event. Theresearcher comes across a paper that touches something deep inside; triggering a radicalrestructuring of their mental model and allowing them to see things in a new light. Timing isimportant: often that paper or interaction would not have had such a profound effect ifencountered earlier or later. I've asked a few well-known and respected people in the …,ACM SIGMOD Record,1998,2
Measuring the complexity of join enumeration in query optimization,Thomas J. Watson IBM Research Center. Research Division; G Lohman; K Ono,*,*,1990,2
Do semantically equivalent SQL queries perform differently?,Guy M Lohman,The relational database query language SQL (originally;" SEQUEL") was first developed asa non-procedural language: the user should specify only what data is desired; leaving it tothe the system's query optimizer to determine how the data is accessed [ASTR 75]. IBMpioneered the development of query optimizer technology that would achieve this ideal;beginning with the well-known optimizer of System R [SELI 79]; the prototype relationaldatabase management system (DBMS) developed at IBM Research Laboratory in San Jose;California during the late 1970's. The IBM products SQL/DS [SQL 84] and DB2 [CHEN 84];as well as the experimental distributed DBMS prototype R∗[LOHM 85]; contain optimizersthat are based upon the System R prototype.,Data Engineering; 1986 IEEE Second International Conference on,1986,2
A closed network queue model of underground coal mining production; failure; and repair,GM Lohman,Abstract: Underground coal mining system production; failures; and repair cycles weremathematically modeled as a closed network of two queues in series. The model wasdesigned to better understand the technological constraints on availability of currentunderground mining systems; and to develop guidelines for estimating the availability ofadvanced mining systems and their associated needs for spares as well as production andmaintenance personnel. It was found that: mine performance is theoretically limited by themaintainability ratio; significant gains in availability appear possible by means of smallimprovements in the time between failures the number of crews and sections should beproperly balanced for any given maintainability ratio; and main haulage systems closest tothe mine mouth require the most attention to reliability.,*,1978,2
A research report,C Kao; CC Lin,*,A Study of Small and,1976,2
Reducing the cost of update; delete; and append-only insert operations in a database,*,A first request may be received to update a first set of values. The first set of values may bestored at a first location within a first data page of a database. The first location may be read-only. In response to the receiving of the first request; a first set of records may be insertedinto a second data page. The first set of records may include the update of the first set ofvalues. In response to the inserting; a forward pointer may be stored in the first data pagethat points to the first set of records on the second data page. One or more committed valuesmay be identified on the second data page. In response to the identifying; the one or morecommitted values may be merged from the second data page to a third data page.,*,2017,1
Wow: What the world of (data) warehousing can learn from the world of warcraft,Rene Mueller; Tim Kaldewey; Guy M Lohman; John McPherson,Abstract Although originally designed to accelerate pixel monsters; graphics ProcessingUnits (GPUs) have been used for some time as accelerators for selected data baseoperations. However; to the best of our knowledge; no one has yet reported building acomplete system that allows executing complex analytics queries; much less an entire datawarehouse benchmark at realistic scale. In this demo; we showcase such a complete systemprototype running on a high-end GPU paired with an IBM storage system that achieves>90% hardware efficiency. Our solution delivers sustainable high throughput for businessanalytics queries in a realistic scenario; ie; the Star Schema Benchmark at scale factor1;000. Attendees can interact with our system through a graphical user interface on a tabletPC. They will be able to experience first hand how queries that require processing more …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,1
Evolutionary Integration of In-Memory Database Technology into IBM's Enterprise DB2 Database Systems.,Knut Stolze; Guy M Lohman; Vijayshankar Raman; Richard Sidle; Felix Beier,Abstract: Recently; IBM announced Blink Ultra (BLU) as an in-memory enhancement for DB2for Linux; Unix; and Windows. The technology implemented in BLU was tested in variousstages until it founds its current form as DB2 feature. In this paper; we give a brief summaryon the origins of BLU and the adoption process from the BLINK prototype over the IBM SmartAnalytics Optimizer to BLU itself.,GI-Jahrestagung,2013,1
Report on the Second International Workshop on Self-Managing Database Systems (SMDB 2007).,Anastassia Ailamaki; Surajit Chaudhuri; Sam Lightstone; Guy M Lohman; Patrick Martin; Kenneth Salem; Gerhard Weikum,Information management systems are growing rapidly in scale and complexity; while skilleddatabase administrators are becoming rarer and more expensive. Increasingly; the total costof ownership of information management systems is dominated by the cost of people; ratherthan hardware or software costs. This economic dynamic dictates that information systems ofthe future be more automated and simpler to use; with most administration tasks transparentto the user.,IEEE Data Eng. Bull.,2007,1
Foundations of automated database tuning (tutorial),Surajit Chaudhuri; Gerhard Weikum; Ling Liu; Andreas Reuter; Kyu-Young Whang; Jianjun Zhang,Abstract Our society is more dependent on information systems than ever before. However;managing the information systems infrastructure in a cost-effective manner is a growingchallenge. The total cost of ownership (TCO) of information technology is increasinglydominated by people costs. In fact; mistakes in operations and administration of informationsystems are the single most reasons for system outage and unacceptable performance. Forinformation systems to provide value to their customers; we must reduce the complexityassociated with their deployment and usage.,Untitled Event,2006,1
IBM's DB2 Universal Database demonstrations at VLDB'98,Berni Schiefer; Jim Kleewein; Karen Brannon; Guy M Lohman; Gene Fuh,Today's competitive business climate dictates that companies derive more information out oftheir databases. Analysts looking for business trends in their company's database poseincreasingly complex queries; often through query generator front-end tools. Businessesmust extract as much useful information as possible from the large volumes of data that theykeep; making parallel database technology a key component of such business intelligenceapplications. Enterprises and independent software vendors continue to require support formore application productivity and capability. And many growing enterprises have datastored in many systems; often both tile systems and database systems from a variety ofvendors. All of these areas contribute to high performance at low cost. Being able to accessand manage this data with high performance; fast response time and low total cost of …,Proceedings of the 24rd International Conference on Very Large Data Bases,1998,1
Starburst II: the extender strikes back!,Guy M Lohman; George Lapis; Tobin Lehman; Rakesh Agrawal; Roberta Cochrane; John McPherson; C Mohan; Hamid Pirahesh; Jennifer Widom,St: irburst is an extensible relational database system prototype dc> clopcd at lll\I's AlrnadcnResearch Center.'I” hc goal tlu-oughout its dct-clopmcnt has been to build a completerelational database system kernel; engineered throughout with the infrastructure for genericextensions; rather than specific extensions for a single applicxition area not previouslysupportable by relational technology. Grrcntly; Starburst consists of over 300;000 Iincs of C(and sotne C++) source code; and cxccutcs an extended form of the SQL D; itaDefinition[anguagc and the SQL SliI; IIC'f; lXSERI; and DELETE commands; including joinson any num-ber of tables.\Ve demonstrate it on IIl\l's first RISC-tcchnology lvorkstatioo; the 1{1 PC; under the AIX~ pcrating s} stcm (based on~ rli~ J']).,ACM SIGMOD Record,1991,1
Starburst is born,George Lapis; Guy M Lohman; Hamid Pirahesh,*,SIGMOD Record (ACM Special Interest Group on Management of Data),1990,1
The Impact of Site Autonomy on R*: A Distributed Relational DBMS,PG Selinger; E Bertino; D Daniels; LM Haas; B Lindsay; G Lohman; Y Masunaga; C Mohan; P Ng; P Wilms; R Yost,This chapter discusses some of the issues raised in the implementation of a distributedrelational database management system constrained by the design objective of siteautonomy. Ease-of-use; software maintenance; authorization; performance; catalogmanagement; transaction commit protocols; concurrency control and data communicationare all impacted by the notion of site autonomy. These issues are discussed in the context ofR*; an experimental distributed database management system (DDBMS) which emphasizessite autonomy.,Databases-Role and Structure: An Advanced Course,1984,1
A proposed concept for a crustal dynamics information management network,Guy M Lohman; JT Renfrow,Abstract: The findings of a requirements and feasibility analysis of the present and potentialproducers; users; and repositories of space-derived geodetic information are summarized. Aproposed concept is presented for a crustal dynamics information management network thatwould apply state of the art concepts of information management technology to meet theexpanding needs of the producers; users; and archivists of this geodetic information.,*,1980,1
Optimal Policy for Database Batch Operations: Backup; Checkpointing; and Batch Update.,John A Muckstadt; Guy M Lohman,Abstract: The purpose of this paper is to present a general model for determining the optimalfrequency of batch operations. Specifically; optimal backup; checkpointing; and batchupdating policies are derived. Our approach exploits inventory parallels; by seeking theoptimal number of items--rather than a time interval--to trigger a batch. The Renewal RewardTheorem is used to find the average long run costs for backup; recovery; and item storage;per unit time; which is then minimized to find the optimal backup policy. This approachallows us to make far less restrictive assumptions about the updata arrival process than didprevious models; as well as to include storage costs for the updates. The optimalcheckpointing and batch updating policies are shown to be special cases of this optimalbackup policy. The derivation of previous results as special cases of this model; and an …,*,1976,1
Adaptive Payload Management,*,Embodiments relate to payload storage format for storing data in support of an aggregationfunction. As an input is subject to aggregation; the input is evaluated to ascertain a payloadformat for the aggregation. It is understood that there is more than one payload format. Anevaluation of the aggregation key is a factor in the initial payload format. If the key is anaddition to an existing aggregation; the evaluation considers changing the format of thepayload to address processing and/or memory efficiency for the aggregation. The evaluationand the format change takes place dynamically so that the aggregation may continue.,*,2017,*
Adaptive payload management,*,Embodiments of the invention relate to payload storage format for storing data in support ofan aggregation function. As an input is subject to aggregation; the input is evaluated toascertain a payload format for the aggregation. It is understood that there is more than onepayload format. An evaluation of the aggregation key is a factor in the initial payload format.If the key is an addition to an existing aggregation; the evaluation considers changing theformat of the payload to address processing and/or memory efficiency for the aggregation.The evaluation and the format change takes place dynamically so that the aggregation maycontinue.,*,2017,*
Query Optimization–Are We There Yet?,Guy Lohman,After nearly 4 decades and hundreds of scientific papers; relational query optimization canhardly be characterized as anything but a huge scientific and commercial success. Themarket in 2016 for relational database products was estimated by IDC to be about $40 B; outof a total database market of $45.1 B. And SQL still dominates database applicationdevelopment and is widely recognized as the most successful declarative language. Noneof this would have been possible without the success of query optimization; whichtransforms declarative SQL statements of what data the user needs into an “optimal”execution plan; ie; a detailed; procedural specification for how that data will be accessedand processed. So are we “there” yet? Are we done? Are all the big and interestingproblems solved? Is query optimization as an area of scientific inquiry dead; relegated to …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
In-place updates with concurrent reads in a decomposed state,*,A method includes setting; by an update processor; a write latch in a first data structureassociated with an object. The first data structure is copied to a storage structure. A historytuple sequence number (TSN) of the first data structure is set to point to a TSN of the copiedfirst data structure. The version identifier is set to point to a transaction identification for theobject. Data portions are updated for the first data structure. The version identifier is readfrom the first data structure. It is determined whether the version identifier of the first datastructure is visible for a transaction including isolation requirements. If version identifier ofthe first data structure is visible; the first data structure is accessed and it is determinedwhether the version identifier of the first data structure changed since starting thetransaction.,*,2016,*
Logical data shuffling,*,Embodiments relate to data shuffling by logically rotating processing nodes. The nodes arelogically arranged in a two or three dimensional matrix. Every time two of the nodes inadjacent rows of the matrix are positionally aligned; these adjacent nodes exchange data.The positional alignment is a logical alignment of the nodes. The nodes are logicallyarranged and rotated; and data is exchanged in response to the logical rotation.,*,2016,*
Data Encoding and Processing Columnar Data,*,The embodiments described herein relate to accessing a plurality of data elements. A pageof column data is compressed and stored in a format that includes a collection of dataelements. A tuple map is stored; and the collection of data elements is indexed via the tuplemap. A query is processed based on the compressed page by identifying a set of tupleidentifiers mapping to stored data in support of the query. Each tuple identifier correspondsto a location of a respective tuple of the compressed page.,*,2016,*
Data shuffling in a non-uniform memory access device,*,A method of orchestrated shuffling of data in a non-uniform memory access device thatincludes a plurality of processing nodes that are connected by interconnects. The methodincludes running an application on a plurality of threads executing on the plurality ofprocessing nodes. Data to be shuffled is identified from source threads running on sourceprocessing nodes among the processing nodes to target threads executing on targetprocessing nodes among the processing nodes. The method further includes generating aplan for orchestrating the shuffling of the data among the all of the memory devicesassociated with the threads and for simultaneously transmitting data over differentinterconnects to a plurality of different target processing nodes from a plurality of differentsource processing nodes. The data is shuffled among all of the memory devices based on …,*,2016,*
Data shuffling in a non-uniform memory access device,*,A method of orchestrated shuffling of data in a non-uniform memory access device thatincludes a plurality of processing nodes includes running an application on a plurality ofthreads executing on the plurality of processing nodes and identifying data to be shuffledfrom source threads running on source processing nodes among the processing nodes totarget threads executing on target processing nodes among the processing nodes. Themethod further includes generating a plan for orchestrating the shuffling of the data amongthe all of the memory devices associated with the threads and shuffling the data among all ofthe memory devices based on the plan.,*,2016,*
Accurate partition sizing for memory efficient reduction operations,*,Embodiments of the invention relate to processing data records; and for a multi-phasepartitioned data reduction. The first phase relates to processing data records and partitioningthe records into a first partition of records having a common characteristic and a secondpartition of records that are not members of the first partition. The data records in eachpartition are subject to intra-partition data reduction responsive to a resource constraint. Thedata records in each partition are also subject to an inter-partition data reduction; alsoreferred to as an aggregation to reduce a footprint for storing the records. Partitions and/orindividual records are logically aggregated and a data reduction operation for the logicalaggregation of records takes place in response to available resources.,*,2016,*
Multiplication-based method for stitching results of predicate evaluation in column stores,*,A system joins predicate evaluated column bitmaps having varying lengths. The systemincludes a column unifier for querying column values with a predicate and generating anindicator bit for each of the column values that is then joined with the respective columnvalue. The system also includes a bitmap generator for creating a column-major linearbitmap from the column values and indicator bits. The column unifier also determines anoffset between adjacent indicator bits. The system also includes a converter for multiplyingthe column-major linear bitmap with a multiplier to shift the indicator bits into consecutivepositions in the linear bitmap.,*,2015,*
Supporting flexible types in a database,*,Providing database support. A first group of data are received; the first group of data beingexpressed in a first format; and a second group of data are received; the second group ofdata being expressed in a second format; the second format being different from the firstformat. The first and second groups of data are merged; and are represented in at least onecommon column. Such representing includes: maintaining the first and second formats; andproviding a tuple map which provides reference to the first and second formats.,*,2015,*
Message from the ICDE 2015 Program Committee and general chairs,Johannes Gehrke; Wolfgang Lehner; Kyuseok Shim; Sang Kyun Cha; Guy Lohman,Since its inception in 1984; the IEEE International Conference on Data Engineering (ICDE)has become a premier forum for the exchange and dissemination of data managementresearch results among researchers; users; practitioners; and developers. Continuing thislong-standing tradition; the 31st ICDE will be hosted this year in Seoul; South Korea; fromApril 13 to April 17; 2015. It is our great pleasure to welcome you to ICDE 2015 and topresent its proceedings to you.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,*
Multi-level database compression,*,Embodiments of the invention relate to a multi-level database compression technique tocompress table data objects stored in pages. A compact dictionary structure is encoded thatrepresents frequent values of data at any level of granularity. More than one level ofcompression is provided; wherein input to a finer level of granularity is an output of a coarserlevel of granularity. Based upon the encoded dictionary structure; a compression techniqueis applied to a stored page to compress each row on the page. Similarly; a de-compressiontechnique may be applied to decompress the compressed data; utilizing the same dictionarystructures at each level of granularity.,*,2014,*
Go; server; go!: parallel computing with moving servers,Ronald Barber; G Lohman; René Mueller; Ippokratis Pandis; Vijayshankar Raman; W Wilcke,Abstract In data centers today; servers are stationary and data flows on a hierarchicalnetwork of switches and routers. But such static server arrangements require very scalablenetworks; and many applications are bottlenecked by network bandwidth. In addition; serverdensity is kept low to enable maintenance and upgrades; as well as to increase air flow. Inthis paper; we propose a design in which servers move physically; and communicate viapoint-to-point connections (instead of switches). We argue that this allows data transferbandwidth to scale linearly with the number of servers; and that moving servers is not asexpensive as it sounds; at least in terms of power consumption. Moreover; while serversmove around; they regularly reach the perimeters of the system; which helps with heatdissipation and with servicing of failed nodes. This design also helps in traditional switch …,Proceedings of the 4th annual Symposium on Cloud Computing,2013,*
DB2 for Linux; UNIX; and Windows Optimizer,Guy M Lohman,∎ Many platforms; but one codebase!–Software: Unix/Linux (AIX; HP; Sun; Linux); Windows;Sequent; OS/2–Hardware: Uni; SMP; MPP; Clusters; NUMA–Query languages: SQL;SQL/XML; XQuery (New in DB2 V9. 1!)∎ Database volume ranges continue to grow: 1GBto> 10 PB∎ Increasing query complexity:–OLTP DSS OLAP/ROLAP–SQL generated byquery generators; naive users∎ Managing complexity–Fewer skilled administratorsavailable• distributed systems• database design can be complex–Too many knobs!,*,2010,*
32nd International Conference on Very Large Data Bases,Umeshwar Dayal,*,*,2006,*
Industrial and Applications Sessions,Andreas Behm; Serge Rielau; Richard Swagerman; Conor Cunningham; Cesar Galindo-Legaria; Goetz Graefe; Walid Rjaibi; Paul Bird; Nagender Bandi; Chengyu Sun; Divyakant Agrawal; Amr El Abbadi; Sang K Cha; Changbin Song; Meikel Poess; John M Stephens Jr; Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan; Sougata Mukherjea; Bhuvan Bamba; Nick Koudas; Amit Marathe; Divesh Srivastava; Benoit Dageville; Dinesh Das; Karl Dias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin; Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; Arun Marathe; Vivek Narasayya; Manoj Syamala; Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W Warner; Vikas Arora; Susan Kotsovolos; Shankar Pal; Istvan Cseri; Oliver Seeliger; Gideon Schaller; Leo Giakoumakis; Vasili Zolotov; Ashraf Aboulnaga; Peter Haas; Mokhtar Kandil; Sam Lightstone; Guy Lohman; Volker Markl; Ivan Popivanov; Vijayshankar Raman; Marcus Fontoura; Eugene Shekita; Jason Zien; Sridhar Rajagopalan; Andreas Neumann; Bishwaranjan Bhattacharjee; Christof Bornhövd; Tao Lin; Stephan Haller; Joachim Schaper; Managing RDFI Data; Sudarshan S Chawathe; Venkat Krishnamurthyy; Sridhar Ramachandran; David Campbell; Toby Bloom; Ted Sharpe; Rakesh Nagarajan; Mushtaq Ahmed; Aditya Phatak; Raymie Stata; Patrick Hunt; William O'Connell; Ramesh Bhashyam; Roger MacNicol; Blaine French,PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS ConorCunningham; Cesar Galindo-Legaria; Goetz Graefe (Microsoft Corp.) … P*TIME: Highly ScalableOLTP DBMS for Managing Update-Intensive Stream Workload Sang K. Cha (Transact InMemory; Inc); Changbin Song (Seoul National Univ.) … Supporting Ontology-based SemanticMatching in RDBMS Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan(Oracle Corp.) … Automatic SQL Tuning in Oracle 10g Benoit Dageville; Dinesh Das; KarlDias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin (Oracle Corp.) … Database TuningAdvisor for Microsoft SQL Server 2005 Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; ArunMarathe; Vivek Narasayya; Manoj Syamala (Microsoft Corp.) … Query Rewrite for XML in OracleXML DB Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W. Warner; Vikas …,Proceedings of the Thirtieth International Conference on Very Large Data Bases: Toronto; Canada; August 31-September 3; 2004,2004,*
Reality Check: Is Autonomic Really Easier to Use,Guy Lohman,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: Rigi; Search; Register; Log in …,*,2004,*
SIGMOD 2001 Industry Sessions,Guy M.  Lohman; Cesar A.  Galindo-Legaria; Michael J.  Franklin; Leonard J.  Seligman,*,SIGMOD Record,2001,*
Review - Hash Joins and Hash Teams in Microsoft SQL Server,Guy M.  Lohman,*,ACM SIGMOD Digital Review,2000,*
Future Directions in Database Research (Panel),Surajit Chaudhuri; Hector Garcia-Molina; Henry F Korth; Guy M Lohman; David B Lomet; David Maier,Google; Inc. (search). SIGN IN SIGN UP. Future Directions in Database Research (Panel). Authors:Surajit Chaudhuri; Hector Garcia-Molina; Henry F. Korth; Guy M. Lohman; David B. Lomet.,Proceedings of the Fourteenth International Conference on Data Engineering,1998,*
Future Directions in Database Research,Surajit Chaudhuri; Hector Garcia-Molina; Hank Korth; Guy Lohman; David Lomet,*,Proceedings,1998,*
Starburst Mid-Flight: As the Dust Clears,PAUL F WILMS; GEORGE LAPIS; BRUCE LINDSAY; HAMID PIRAHESH; J CAREY MICHAEL; EUGENE SHEKITA,*,Multidatabase systems: an advanced solution for global information sharing,1994,*
VLDB: Very Large Data Bases: International Conference Proceedings; 17th; September 3-6 1991; Barcelona Spain,Guy M Lohman; AC Sernadas; Rafael Camps,*,*,1991,*
Very Large Data Bases: Proceedings: Proceedings of the Seventeenth International Conference on Very Large Data Bases: September 3-6 1991: Barcelona (Catalo...,Rafael Camps; Guy M Lohman; AC Sernadas,*,*,1991,*
Proceedings of the Seventeenth International Conference on Very Large Data Bases; September 3-6; 1991; Barcelona (Catalonia; Spain),Guy M Lohman,*,*,1991,*
Generic functional requirements for a NASA general-purpose data base management system,GM LOHMAN,Abstract: Generic functional requirements for a general-purpose; multi-mission data basemanagement system (DBMS) for application to remotely sensed scientific data bases aredetailed. The motivation for utilizing DBMS technology in this environment is explained. Themajor requirements include:(1) a DBMS for scientific observational data;(2) a multi-missioncapability;(3) user-friendly;(4) extensive and integrated information about data;(5) robustlanguages for defining data structures and formats;(6) scientific data types and structures;(7)flexible physical access mechanisms;(8) ways of representing spatial relationships;(9) ahigh level nonprocedural interactive query and data manipulation language;(10) data basemaintenance utilities;(11) high rate input/output and large data volume storage; andadaptability to a distributed data base and/or data base machine configuration. Detailed …,*,1981,*
Modeling underground coal-mining production,GM Lohman,*,Simul. Counc. Proc. Ser.;(United States),1978,*
Longwall/shortwall mine equipment availability and delay analysis. Final report,KC Curry; GM Lohman; MA Metcalfe,*,*,1978,*
Optimal data storage and organization in computerized information processing systems subject to failures[Ph. D. Thesis],GM LOHMAN,*,*,1977,*
Recommending Materialized Views and Indexes with the IBM DB2 Design Advisor,Guy M Lohman; Roberta J Cochrane; Hamid Pirahesh; Latha Colby; Gary Valentin; Daniel C Zilio; Calisto Zuzarte; Sam Lightstone; Wenbin Ma; Eric Alton; Dongming Liang; Jarek Gryz,Abstract Materialized views (MVs) and indexes both significantly speed query processing indatabase systems; but consume disk space and need to be maintained when updates occur.Choosing the best set of MVs and indexes to create depends upon the workload; thedatabase; and many other factors; which makes the decision intractable for humans andcomputationally challenging for computer algorithms. Even heuristic-based algorithms canbe impractical in real systems. In this paper; we present an advanced tool that uses thequery optimizer itself to both suggest and evaluate candidate MVs and indexes; and asimple; practical; and effective algorithm for rapidly finding good solutions even for largeworkloads. The algorithm trades off the cost for updates and storing each MV or indexagainst its benefit to queries in the workload. The tool autonomically captures the …,*,*,*
吉圭手寺 Rese2rch Division,Guy M Lohman; Bruce G Lindsay; Patricia G Selinger,ABSTRACT: This paper describes how statements in the SQL language are processed bythe R* distri uuti relational database management system. R* is an experimental adaptationof System k to the distributed environment. The R* prototype is currently operational onmultiple machines rurning the MVS operating system; and is undergoing evaluation. The R"system is a confederation of autonomous; locally-administered databases that may begeographically dispersed; yet which appear to the user as a single database. Namingconventions permit R* to access tables at remote sites without resorting to a centralized orreplicated catalog; and without the user having to specify either the current location of or thecommunication commands required to access that table. SQL data definition statementsaffecting remote sites are interpreted through a distributed recursive call mechanism …,*,*,*
Ad d Q O iii Advanced Query Optimization Techniques in a Parallel Computing Environment (Parallelizing Query Optimization),Wook-Shin Han; Wooseong Kwak; Jinsoo Lee; Guy M Lohman; Volker Markl,–By partitioning sub-problems by their sizes; sub-problems of the same resulting size aremutually independent A th bf tifi i th bfb–As the number of quantifiers increases; the numberof subproblems of the same size grows exponentially–Each sub-problem of size S (=smallSZ+ largeSZ) is constructed p mf (mg) using any combination of one smaller sub-problem of size smallSZ and another sub-problem of size largeSZ,*,*,*
Blink: Not Your Father’s Database!,Guy M Lohman,Page 1. Guy M. Lohman Manager; Disruptive Information Management Architectures IBM AlmadenResearch Center lohman@almaden.ibm.com Blink: Not Your Father's Database! Real TimeBusiness Intelligence Page 2. This is joint work with: Ronald Barber; Peter Bendel; Marco Czech;Oliver Draese; Frederick Ho; Namik Hrle; Stratos Idreos; Min-Soo Kim; Oliver Koeth; Jae Gil Lee;Tianchao Tim Lee; Guy Lohman; Konstantinos Morfonios; Keshava Murthy; Lin Qiao; VijayshankarRaman; Richard Sidle; Knut Stolze; … and many more! Page 3. Blink Research Team KonstantinosMorfonios Jae Gil Lee Min-Soo Kim • Lin Qiao Vijayshankar Raman Richard Sidle Ron BarberGuy Lohman Stratos Idreos Page 4. Blink – Agenda • Why and What is Blink • Blink Market –Business Intelligence • Blink Architecture • It's All About Performance! • What's the Big Deal? •Behind the Curtain – The Query Engine Technology …,*,*,*
Query Optimization,Volker Markl; Vijayshankar Raman; David Simmen; Guy Lohman; Hamid Pirahesh,Query optimization is one of the signature components of database technology—the bridgethat connects declarative languages to efficient execution. Query optimizers have areputation as one of the hardest parts of a DBMS to implement well; so it's no surprise theyremain a clear differentiator for mature commercial DBMSs. The best of the opensourcerelational database optimizers are limited by comparison; and some have relatively naiveoptimizers that only work for the simplest of queries. It's important to remember that no queryoptimizer is truly producing “optimal” plans. First; they all use estimation techniques to guessat real plan costs; and it's well known that errors in these estimation techniques can balloon—in some circumstances being as bad as random guesses [7]. Second; optimizers useheuristics to limit the search space of plans they choose; since the problem is NP-hard [6] …,*,*,*
Future Directions in Database Research,Hector Garcia-Molina; Guy Lohman; David Lomet,Theme We will continue the EEE Data Engineering tradition of a panel to discuss the future directionsin database research. This year we will focus on nontraditional and innovative use of databasetechnology. The distinguished panel members are … 0 0 David Lomet; Microsoft ResearchGuy Lohman; IBM Almaden Research Center,*,*,*
Data Engineering,Surajit Chaudhuri Ailamaki; Sam Lightstone; Guy Lohman; Pat Martin; Ken Salem; Gerhard Weikum,Information management systems are growing rapidly in scale and complexity; while skilleddatabase administrators are becoming rarer and more expensive. Increasingly; the total costof ownership of information management systems is dominated by the cost of people; ratherthan hardware or software costs. This economic dynamic dictates that information systems ofthe future be more automated and simpler to use; with most administration tasks transparentto the user. Autonomic; or self-managing; systems provide a promising approach toachieving the goal of systems that are increasingly automated and easier to use. But howcan that be achieved? The aim of this workshop was to present and discuss ideas towardachieving self-managing information systems in an intimate; informal; and interactiveenvironment.,*,*,*
RESEARCH SESSION 1: Query Transformation and Query Optimization,Quanzhong Li; Minglong Shao; Volker Markl; Kevin Beyer; Latha Colby; Guy Lohman; Lucantonio Ghionna; Luigi Granata; Gianluigi Greco; Francesco Scarcello; Matthias Brantner; Norman May; Guido Moerkotte; Per-Ake Larson; Jingren Zhou,Page 1. Papers by Session RESEARCH SESSION 1: Query Transformation and QueryOptimization Adaptively Reordering Joins during Query Execution Quanzhong Li; MinglongShao; Volker Markl; Kevin Beyer; Latha Colby; and Guy Lohman Hypertree Decompositionsfor Query Optimization Lucantonio Ghionna; Luigi Granata; Gianluigi Greco; and FrancescoScarcello Unnesting Scalar SQL Queries in the Presence of Disjunction Matthias Brantner;Norman May; and Guido Moerkotte Efficient Maintenance of Materialized Outer-Join ViewsPer-Ake Larson; and Jingren Zhou MENU Page 2. Papers by Session RESEARCH SESSION2: Temporal; Spatial and Multimedia Data Management (1) A General Cost Model forDimensionality Reduction in High Dimensional Spaces Xiang Lian; and Lei Chen AcceleratingProfile Queries in Elevation Maps Feng Pan; Wei Wang; and Leonard McMillan …,*,*,*
Data Engineering,Ronald Barber; Peter Bendel; Marco Czech; Oliver Draese; Frederick Ho; Namik Hrle; Stratos Idreos; Min-Soo Kim; Oliver Koeth; Jae-Gil Lee; Tianchao Tim Li; Guy Lohman; Konstantinos Morfonios; Rene Mueller; Keshava Murthy; Ippokratis Pandis; Lin Qiao; Vijayshankar Raman; Richard Sidle; Knut Stolze; Sandor Szabo,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
