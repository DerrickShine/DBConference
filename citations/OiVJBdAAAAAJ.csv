Efficient Co-Processor Utilization in Database Query Processing,Sebastian Breß; Felix Beier; Hannes Rauhe; Kai-Uwe Sattler; Eike Schallehn; Gunter Saake,Abstract Specialized processing units such as GPUs or FPGAs provide great opportunities tospeed up database operations by exploiting parallelism and relieving the CPU. However;distributing a workload on suitable (co-) processors is a challenging task; because of theheterogeneous nature of a hybrid processor/co-processor system. In this paper; we presenta framework that automatically learns and adapts execution models for arbitrary algorithmson any (co-) processor. Our physical optimizer uses the execution models to distribute aworkload of database operators on available (co-) processing devices. We demonstrate itsapplicability for two common use cases in modern database systems. Additionally; wecontribute an overview of GPU-co-processing approaches; an in-depth discussion of ourframework's operator model; the required steps for deploying our framework in practice …,Information Systems,2013,52
Why it is time for a HyPE: A hybrid query processing engine for efficient GPU coprocessing in DBMS,Sebastian Breß; Gunter Saake,Abstract GPU acceleration is a promising approach to speed up query processing ofdatabase systems by using low cost graphic processors as coprocessors. Two major trendshave emerged in this area:(1) The development of frameworks for scheduling tasks inheterogeneous CPU/GPU platforms; which is mainly in the context of coprocessing forapplications and does not consider specifics of database-query processing andoptimization.(2) The acceleration of database operations using efficient GPU algorithms;which typically cannot be applied easily on other database systems; because of theiranalytical-algorithm-specific cost models. One major challenge is how to combine traditionaldatabase query processing with GPU coprocessing techniques and efficient databaseoperation scheduling in a GPU-aware query optimizer. In this thesis; we develop a hybrid …,Proceedings of the VLDB Endowment,2013,45
A Framework for Cost based Optimization of Hybrid CPU/GPU Query Plans in Database Systems,Sebastian Breß; Ingolf Geist; Eike Schallehn; Maik Mory; Gunter Saake,Abstrakty EN Current database research identified the use of computational power of GPUsas a way to increase the performance of database systems. As GPU algorithms are notnecessarily faster than their CPU counterparts; it is important to use the GPU only if it isbeneficial for query processing. In a general database context; only few research projectsaddress hybrid query processing; ie; using a mix of CPU-and GPU-based processing toachieve optimal performance. In this paper; we extend our CPU/GPU scheduling frameworkto support hybrid query processing in database systems. We point out fundamentalproblems and propose an algorithm to create a hybrid query plan for a query using ourscheduling framework. Additionally; we provide cost metrics; accounting for the possibleoverlapping of data transfers and computation on the GPU. Furthermore; we present …,Control and Cybernetics,2012,34
The design and implementation of CoGaDB: A column-oriented GPU-accelerated DBMS,Sebastian Breß,Abstract Nowadays; the performance of processors is primarily bound by a fixed energybudget; the power wall. This forces hardware vendors to optimize processors for specifictasks; which leads to an increasingly heterogeneous hardware landscape. Although efficientalgorithms for modern processors such as GPUs are heavily investigated; we also need toprepare the database optimizer to handle computations on heterogeneous processors.GPUs are an interesting base for case studies; because they already offer many difficultieswe will face tomorrow. In this paper; we present CoGaDB; a main-memory DBMS with built-in GPU acceleration; which is optimized for OLAP workloads. CoGaDB uses the self-tuningoptimizer framework HyPE to build a hardware-oblivious optimizer; which learns costmodels for database operators and efficiently distributes a workload on available …,Datenbank-Spektrum,2014,26
Automatic Selection of Processing Units for Coprocessing in Databases,Sebastian Breß; Felix Beier; Hannes Rauhe; Eike Schallehn; Kai-Uwe Sattler; Gunter Saake,Abstract Specialized processing units such as GPUs or FPGAs provide great opportunities tospeed up database operations by exploiting parallelism and relieving the CPU. But utilizingcoprocessors efficiently poses major challenges to developers. Besides finding fine-granulardata parallel algorithms and tuning them for the available hardware; it has to be decided atruntime which (co) processor should be chosen to execute a specific task. Depending oninput parameters; wrong decisions can lead to severe performance degradations sinceinvolving coprocessors introduces a significant overhead; eg; for data transfers. In thispaper; we present a framework that automatically learns and adapts execution models forarbitrary algorithms on any (co) processor to find break-even points and support schedulingdecisions. We demonstrate its applicability for three common use cases in modern …,Advances in Databases and Information Systems,2012,26
GPU-accelerated Database Systems: Survey and Open Challenges,Sebastian Breß; Max Heimel; Norbert Siegmund; Ladjel Bellatreche; Gunter Saake,Abstract The vast amount of processing power and memory bandwidth provided by moderngraphics cards make them an interesting platform for data-intensive applications.Unsurprisingly; the database research community identified GPUs as effective co-processors for data processing several years ago. In the past years; there were manyapproaches to make use of GPUs at different levels of a database system. In this paper; weexplore the design space of GPU-accelerated database management systems. Based onthis survey; we present key properties; important trade-offs and typical challenges of GPU-aware database architectures; and identify major open challenges. Additionally; we surveyexisting GPU-accelerated DBMSs and classify their architectural properties. Then; wesummarize typical optimizations implemented in GPU-accelerated DBMSs. Finally; we …,Transactions on Large-Scale Data and Knowledge-Centered Systems (TLDKS),2014,25
Toward Hardware-Sensitive Database Operations.,David Broneske; Sebastian Breß; Max Heimel; Gunter Saake,ABSTRACT Satisfying the performance needs of tomorrow typically implies using modernprocessor capabilities (such as single instruction; multiple data) and co-processors (such asgraphics processing units) to accelerate database operations. Algorithms are typically hand-tuned to the underlying (co-) processors. This solution is error-prone; introduces highimplementation and maintenance cost and one implementations is not portable to other (co-)processors. To this end; we argue for a combination of database research with modernsoftware-engineering approaches. We emphasize our vision of generating optimizeddatabase algorithms tailored to used (co-) processors from a common code base. With this;we maximize performance while minimizing implementation and maintenance effort ofhardware-tailored database operations.,EDBT,2014,21
Towards Optimization of Hybrid CPU/GPU Query Plans in Database Systems,Sebastian Breß; Eike Schallehn; Ingolf Geist,Abstract Current database research identified the computational power of GPUs as a way toincrease the performance of database systems. Since GPU algorithms are not necessarilyfaster than their CPU counterparts; it is important to use the GPU only if it is beneficial forquery processing. In a general database context; only few research projects address hybridquery processing; ie; using a mix of CPU-and GPU-based processing to achieve optimalperformance. In this paper; we extend our CPU/GPU scheduling framework to support hybridquery processing in database systems. We point out fundamental problems and provide analgorithm to create a hybrid query plan for a query using our scheduling framework.,New Trends in Databases and Information Systems,2012,20
Robust query processing in co-processor-accelerated databases,Sebastian Breß; Henning Funke; Jens Teubner,Abstract Technology limitations are making the use of heterogeneous computing devicesmuch more than an academic curiosity. In fact; the use of such devices is widelyacknowledged to be the only promising way to achieve application-speedups that usersurgently need and expect. However; building a robust and efficient query engine forheterogeneous co-processor environments is still a significant challenge. In this paper; weidentify two effects that limit performance in case co-processor resources become scarce.Cache thrashing occurs when the working set of queries does not fit into the co-processor'sdata cache; resulting in performance degradations up to a factor of 24. Heap contentionoccurs when multiple operators run in parallel on a co-processor and when theiraccumulated memory footprint exceeds the main memory capacity of the co-processor …,Proceedings of the 2016 International Conference on Management of Data,2016,19
Cloud Data Management: A Short Overview and Comparison of Current Approaches,Siba Mohammad; Sebastian Breß; Eike Schallehn,ABSTRACT To meet the storage needs of current cloud applications; new data managementsystems were developed. Design decisions were made by analyzing the applicationsworkloads and technical environment. It was realized that traditional Relational DatabaseManagement Systems (RDBMSs) with their centralized architecture; strong consistency; andrelational model do not fit the elasticity and scalability requirements of the cloud. Differentarchitectures with a variety of data partitioning schemes and replica placement strategieswere developed. As for the data model; the key-value pairs with its variations were adoptedfor cloud storage. The contribution of this paper is to provide a comprehensible overview ofkey-characteristics of current solutions and outline the problems they do and do not address.This paper should serve as an entry point for orientation of future research regarding new …,*,2012,19
Database scan variants on modern CPUs: A performance study,David Broneske; Sebastian Breß; Gunter Saake,Abstract Main-memory databases rely on highly tuned database operations to achieve peakperformance. Recently; it has been shown that different code optimizations for databaseoperations favor different processors. However; it is still not clear how the combination ofcode optimizations (eg; loop unrolling and vectorization) will affect the performance ofdatabase algorithms on different processors. In this paper; we extend prior studies by an in-depth performance analysis of different variants of the scan operator. We find that theperformance of the scan operator for different processors gets even harder to predict whenmultiple code optimizations are combined. Since the scan is the most simple databaseoperator; we expect the same effects for more complex operators such as joins. Based onthese results; we identify practical problems for a query processor and discuss how we …,*,2015,16
Load-aware inter-co-processor parallelism in database query processing,Sebastian Breß; Norbert Siegmund; Max Heimel; Michael Saecker; Tobias Lauer; Ladjel Bellatreche; Gunter Saake,Abstract For a decade; the database community has been exploring graphics processingunits and other co-processors to accelerate query processing. While the developedalgorithms often outperform their CPU counterparts; it is not beneficial to keep processingdevices idle while overutilizing others. Therefore; an approach is needed that efficientlydistributes a workload on available (co-) processors while providing accurate performanceestimates for the query optimizer. In this paper; we contribute heuristics that optimize queryprocessing for response time and throughput simultaneously via inter-device parallelism.Our empirical evaluation reveals that the new approach achieves speedups up to 1.85compared to state-of-the-art approaches while preserving accurate performanceestimations. In a further series of experiments; we evaluate our approach on two new use …,Data & Knowledge Engineering,2014,15
Self-Tuning Distribution of DB-Operations on Hybrid CPU/GPU Platforms,Sebastian Breß; Siba Mohammad; Eike Schallehn,ABSTRACT A current research trend focuses on accelerating database operations with thehelp of GPUs (Graphics Processing Units). Since GPU algorithms are not necessarily fasterthan their CPU counterparts; it is important to use them only if they outperform their CPUcounterparts. In this paper; we address this problem by constructing a decision model for aframework that is able to distribute database operations response time minimal on CPUsand GPUs. Furthermore; we discuss necessary quality measures for evaluating our model.,Grundlagen von Datenbanken,2012,13
Toward efficient variant calling inside main-memory database systems,Sebastian Dorok; Sebastian Breß; Gunter Saake,Mutations in genomes indicate predisposition for diseases or effects on efficacy of drugs. Avariant calling algorithm determines possible mutations in sample genomes. Afterwards;scientists have to decide about the impact of these mutations. Certainly; many differentvariant calling algorithms exist that generate different outputs due to different sequencealignments as input and parameterizations of variant calling algorithms. Thus; a combinationof variant calling results is necessary to provide a more complete set of mutations thansingle algorithm runs can provide. Therefore; a system is required that facilitates theintegration and parameterization of different variant calling algorithms and processing ofdifferent sequence alignments. Moreover; against the backdrop of ever increasing amountsof available genome sequencing data; such a system must provide matured database …,Database and Expert Systems Applications (DEXA); 2014 25th International Workshop on,2014,12
Ocelot/HyPE: optimized data processing on heterogeneous hardware,Sebastian Breß; Bastian Köcher; Max Heimel; Volker Markl; Michael Saecker; Gunter Saake,Abstract The past years saw the emergence of highly heterogeneous server architecturesthat feature multiple accelerators in addition to the main processor. Efficiently exploitingthese systems for data processing is a challenging research problem that comprises manyfacets; including how to find an optimal operator placement strategy; how to estimate runtimecosts across different hardware architectures; and how to manage the code andmaintenance blowup caused by having to support multiple architectures. In prior work; wealready discussed solutions to some of these problems: First; we showed that specifyingoperators in a hardware-oblivious way can prevent code blowup while still maintainingcompetitive performance when supporting multiple architectures. Second; we presentedlearning cost functions and several heuristics to efficiently place operators across all …,Proceedings of the VLDB Endowment,2014,12
How to exploit the device diversity and database interaction to propose a generic cost model?,Ladjel Bellatreche; Salmi Cheikh; Sebastian Breß; Amira Kerkad; Ahcène Boukhorca; Jalil Boukhobza,Abstract Cost models have been following the life cycle of databases. In the first generation;they have been used by query optimizers; where the cost-based optimization paradigm hasbeen developed and supported by most of important optimizers. The spectaculardevelopment of complex decision queries amplifies the interest of the physical design phase(PhD); where cost models are used to select the relevant optimization techniques such asindexes; materialized views; etc. Most of these cost models are usually developed for onestorage device (usually disk) with a well identified storage model and ignore the interactionbetween the different components of databases: interaction between optimizationtechniques; interaction between queries; interaction between devices; etc. In this paper; wepropose a generic cost model for the physical design that can be instantiated for each …,Proceedings of the 17th International Database Engineering & Applications Symposium,2013,10
Exploring the Design Space of a GPU-aware Database Architecture,Sebastian Breß; Max Heimel; Norbert Siegmund; Ladjel Bellatreche; Gunter Saake,Abstract The vast amount of processing power and memory bandwidth provided by moderngraphics cards make them an interesting platform for data-intensive applications.Unsurprisingly; the database research community has identified GPUs as effective co-processors for data processing several years ago. In the past years; there were manyapproaches to make use of GPUs at different levels of a database system. In this paper; wesummarize the major findings of the literature on GPU-accelerated data processing. Basedon this survey; we present key properties; important trade-offs and typical challenges of GPU-aware database architectures; and identify major open research questions.,New Trends in Databases and Information Systems,2013,10
An Operator-Stream-based Scheduling Engine for Effective GPU Coprocessing,Sebastian Breß; Norbert Siegmund; Ladjel Bellatreche; Gunter Saake,Abstract Since a decade; the database community researches opportunities to exploitgraphics processing units to accelerate query processing. While the developed GPUalgorithms often outperform their CPU counterparts; it is not beneficial to keep processingdevices idle while over utilizing others. Therefore; an approach is needed that effectivelydistributes a workload on available (co-) processors while providing accurate performanceestimations for the query optimizer. In this paper; we extend our hybrid query-processingengine with heuristics that optimize query processing for response time and throughputsimultaneously via inter-device parallelism. Our empirical evaluation reveals that the newapproach doubles the throughput compared to our previous solution and state-of-the-artapproaches; because of nearly equal device utilization while preserving accurate …,17th East-European Conference on Advances in Databases and Information Systems (ADBIS),2013,10
Forensics on GPU Coprocessing in Databases – Research Challenges; First Experiments; and Countermeasures,Sebastian Breß; Stefan Kiltz; Martin Schäler,*,In Workshop on Databases in Biometrics; Forensics and Security Applications (DBforBFS); BTW-Workshops,2013,9
Toward efficient and reliable genome analysis using main-memory database systems,Sebastian Dorok; Sebastian Breß; Horstfried Läpple; Gunter Saake,Abstract Improvements in DNA sequencing technologies allow to sequence completehuman genomes in a short time and at acceptable cost. Hence; the vision of genomeanalysis as standard procedure to support and improve medical treatment becomesreachable. In this vision paper; we describe important data-management challenges thathave to be met to make this vision come true. Besides genome-analysis performance; data-management capabilities such as data provenance and data integrity become increasinglyimportant to enable comprehensible and reliable genome analysis. We argue to meet thesechallenges by using main-memory database technologies; which combine fast processingcapabilities with extensive data-management capabilities. Finally; we discuss possibilities ofintegrating genome-analysis tasks into DBMSs and derive new research questions.,Proceedings of the 26th International Conference on Scientific and Statistical Database Management,2014,6
Toward GPU-accelerated database optimization,Andreas Meister; Sebastian Breß; Gunter Saake,Abstract For over three decades; research investigates optimization options in DBMSs.Nowadays; the hardware used in DBMSs become more and more heterogeneous; becauseprocessors are bound by a fixed energy budget leading to increased parallelism. Existingoptimization approaches in DBMSs do not exploit parallelism for a single optimization taskand; hence; can only benefit from the parallelism offered by current hardware by batch-processing multiple optimization tasks. Since a large optimization space often allows us toprocess sub-spaces in parallel; we expect large gains in result quality for optimizationapproaches in DBMSs and; hence; performance for query processing on modern (co-)processors. However; parallel optimization on CPUs is likely to slow down query processing;because DBMSs can fully exploit the CPUs computing resources due to high data …,Datenbank-Spektrum,2015,5
Demonstrating Self-Learning Algorithm Adaptivity in a Hardware-Oblivious Database Engine.,Max Heimel; Filip Haase; Martin Meinke; Sebastian Breß; Michael Saecker; Volker Markl,ABSTRACT The increasingly heterogeneous modern hardware landscape is forcingdatabase vendors to rethink basic design decisions: With more and more architectures tosupport; the traditional approach of building on hand-tuned operators might simply becometoo cost-and labor-intensive. With this problem in mind; we introduced the notion of ahardware-oblivious database engine; which avoids devicespecific optimizations and targetsmultiple different hardware architectures from a single code-base. We demonstrated thefeasibility of this concept through Ocelot; a prototypical hardware-oblivious database thatuses OpenCL to provide operators that can run on multiple architectures. In thisdemonstration; we show how we modified Ocelot to support self-learning algorithmadaptivity: The ability to automatically learn which algorithms are optimal for a given …,EDBT,2014,5
Flexible Analysis of Plant Genomes in a Database Management System.,Sebastian Dorok; Sebastian Breß; Jens Teubner; Gunter Saake,ABSTRACT Analysis of genomes has a wide range of applications from diseasesusceptibility studies to plant breeding research. For example; different types of barley havediffering characteristics regarding draught or salt tolerance. Thus; a typical use case iscomparing two plant genomes and try to deduce which genes are responsible for a certainresistance. For this; we need to find differences in large volumes of aligned genome data;which is already available in large genome databases. The challenge is to efficiently retrievethe genotypes of a certain range of the genome; and then; to determine variants and theirimpact on the plant organism. State-of-the-art tools are fixed pipelines with a fixedparametrization. However; in practice; users want to interactively analyse genome data andneed to customize the parametrization. In this demonstration; we show how we can …,EDBT,2015,4
Roupar: Routinely and mixed query-driven approach for data partitioning,Ladjel Bellatreche; Amira Kerkad; Sebastian Breß; Dominique Geniet,Abstract With the big data era and the cloud; several applications are designed aroundanalytical aspects; where the data warehousing technology is in the heart of theirconstruction chain. The interaction between queries in such environments represents a bigchallenge due to three dimensions:(i) the routinely aspects of queries;(ii) their large number;and (iii) the high operation sharing between queries. In the context of very large databases;these operations are expensive and need to be optimized. The horizontal data partitioning(HDP) is a pre-condition for designing extremely large databases in several environments:centralized; distributed; parallel and cloud. It aims to reduce the cost of these operations. InHDP; the optimization space of potential candidates for partitioning grows exponentially withthe problem size making the problem NP-hard. In this paper; we propose a new approach …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2013,4
The Generalized Physical Design Problem in Data Warehousing Environment: Towards a Generic Cost Model,Ladjel Bellatreche; Sebastian Breß; Amira Kerkad; Ahcene Boukorca; Cheikh Salmi,Over the years; plenty of cost models for physical database design were developed.However; solving the physical design problem in a generic way is still a hard task. In thispaper; we (1) summarize the development stages of cost models for database systems;(2)propose a generic cost model for the generalized physical design problem; and (3) applyour model on the joint query scheduling and buffer management problem.,*,2013,4
Efficient Query Processing in Co-Processor-accelerated Databases,Sebastian Breß; Gunter Saake; Jens Teubner; Kai-Uwe Sattler,Abstract Advancements in hardware changed the bottleneck of modern database systemsfrom disk IO to main memory access and processing power. Since the performance ofmodern processors is primarily limited by a fixed energy budget; hardware vendors areforced to specialize processors. Consequently; processors become increasinglyheterogeneous; which already became commodity in the form of accelerated processingunits or dedicated co-processors such as graphics processing units. However; building arobust and efficient query engine for such heterogeneous co-processor environments is stilla significant challenge. Although the database community developed fast parallel algorithmsfor a large number of heterogeneous processors; we still require methods to use theseprocessors efficiently during query processing.,*,2015,3
Generating Custom Code for Efficient Query Execution on Heterogeneous Processors,Sebastian Breß; Bastian Köcher; Henning Funke; Tilmann Rabl; Volker Markl,Abstract: Processor manufacturers build increasingly specialized processors to mitigate theeffects of the power wall to deliver improved performance. Currently; database engines aremanually optimized for each processor: A costly and error prone process. In this paper; wepropose concepts to enable the database engine to perform per-processor optimizationautomatically. Our core idea is to create variants of generated code and to learn a fastvariant for each processor. We create variants by modifying parallelization strategies;specializing data structures; and applying different code transformations.,arXiv preprint arXiv:1709.00700,2017,2
Efficient storage and analysis of genome data in databases,Sebastian Dorok; Sebastian Breß; Jens Teubner; Horstfried Läpple; Gunter Saake; Volker Markl,Genome-analysis enables researchers to detect mutations within genomes and deduce theirconsequences. Researchers need reliable analysis platforms to ensure reproducible andcomprehensive analysis results. Database systems provide vital support to implement therequired sustainable procedures. Nevertheless; they are not used throughout the completegenome-analysis process; because (1) database systems su er from high storage overheadfor genome data and (2) they introduce overhead during domain-specific analysis. Toovercome these limitations; we integrate genome-specific compression into databasesystems using a specialized database schema. Thus; we can reduce the storage overheadto 30%. Moreover; we can exploit genome-data characteristics during query processingallowing us to analyze real-world data sets up to five times faster than specialized …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,2
A coding template for handling static and incremental horizontal partitioning in data warehouses,Rima Bouchakri; Ladjel Bellatreche; Zoé Faget; Sebastian Breß,Today; data feeding warehouses come from devices with high speed and may affect theselected optimisation techniques such as horizontal data partitioning (HDP). HDP helpsreduce the cost of complex OLAP queries and facilitates warehouse manageability.Selecting a partitioning schema for a given data warehouse is an Non-deterministicPolynomial-time hard (NP-hard) problem. Several studies exist which propose heuristics toselect near-optimal solutions. Most of these heuristics are static; because they assume theexistence of an a priori known set of queries. However; in real life; applications queries maychange dynamically and the partitioning heuristics need to integrate these changes. An easyand naïve way to deal with this problem is to make the heuristics incremental by makingefforts in implementation and coding levels. In this paper; we propose a new vision for …,Journal of Decision Systems,2014,2
Clustering the Cloud-A Model for (Self-) Tuning of Cloud Data Management Systems.,Siba Mohammad; Eike Schallehn; Sebastian Breß,Abstract: Popularity and complexity of cloud data management systems are increasingrapidly. Thus providing sophisticated features becomes more important. The focus of thispaper is on (self-) tuning where we contribute the following:(1) we illustrate why (self-) tuningfor cloud data management is necessary but yet a much more complex task than fortraditional data management; and (2) propose an model to solve some of the outlinedproblems by clustering nodes in zones across data management layers for applications withsimilar requirements.,CLOSER,2013,2
Optimized on-demand data streaming from sensor nodes,Jonas Traub; Sebastian Breß; Tilmann Rabl; Asterios Katsifodimos; Volker Markl,Abstract Real-time sensor data enables diverse applications such as smart metering; trafficmonitoring; and sport analysis. In the Internet of Things; billions of sensor nodes form asensor cloud and offer data streams to analysis systems. However; it is impossible to transferall available data with maximal frequencies to all applications. Therefore; we need to tailordata streams to the demand of applications. We contribute a technique that optimizescommunication costs while maintaining the desired accuracy. Our technique schedulesreads across huge amounts of sensors based on the data-demands of a huge amount ofconcurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques; which decrease theamount of transferred data. Moreover; we share sensor reads and data transfers among …,Proceedings of the 2017 Symposium on Cloud Computing,2017,1
Efficiently storing and analyzing genome data in database systems,Sebastian Dorok; Sebastian Breß; Jens Teubner; Horstfried Läpple; Gunter Saake; Volker Markl,Abstract Genome-analysis enables researchers to detect mutations within genomes anddeduce their consequences. Researchers need reliable analysis platforms to ensurereproducible and comprehensive analysis results. Database systems provide vital support toimplement the required sustainable procedures. Nevertheless; they are not used throughoutthe complete genome-analysis process; because (1) database systems suffer from highstorage overhead for genome data and (2) they introduce overhead during domain-specificanalysis. To overcome these limitations; we integrate genome-specific compression intodatabase systems using a specialized database schema. Thus; we can reduce the storageconsumption of a database approach by up to 35%. Moreover; we exploit genome-datacharacteristics during query processing allowing us to analyze real-world data sets up to …,Datenbank-Spektrum,2017,1
Efficient SIMD Vectorization for Hashing in OpenCL,Tobias Behrens; Viktor Rosenfeld; Jonas Traub; Sebastian Breß; Volker Markl,ABSTRACT Hashing is at the core of many efficient database operators such as hash-basedjoins and aggregations. Vectorization is a technique that uses Single Instruction MultipleData (SIMD) instructions to process multiple data elements at once. Applying vectorization tohash tables results in promising speedups for build and probe operations. However;vectorization typically requires intrinsics–low-level APIs in which functions map toprocessorspecific SIMD instructions. Intrinsics are specific to a processor architecture andresult in complex and difficult to maintain code. OpenCL is a parallel programmingframework which provides a higher abstraction level than intrinsics and is portable todifferent processors. Thus; OpenCL avoids processor dependencies; which results inimproved code maintainability. In this paper; we add efficient; vectorized hashing …,Positions,2018,*
Scalable Detection of Concept Drifts on Data Streams with Parallel Adaptive Windowing,Philipp M Grulich; René Saitenmacher; Jonas Traub; Sebastian Breß; Tilmann Rabl; Volker Markl,ABSTRACT Machine learning techniques for data stream analysis suffer from concept driftssuch as changed user preferences; varying weather conditions; or economic changes.These concept drifts cause wrong predictions and lead to incorrect business decisions.Concept drift detection methods such as adaptive windowing (Adwin) allow for adapting toconcept drifts on the fly. In this paper; we examine Adwin in detail and point out itsthroughput bottlenecks. We then introduce several parallelization alternatives to addressthese bottlenecks. Our optimizations lead to a speedup of two orders of magnitude over theoriginal Adwin implementation. Thus; we explore parallel adaptive windowing to providescalable concept detection for high-velocity data streams with millions of tuples per second.,*,2018,*
Estimating join selectivities using bandwidth-optimized kernel density models,Martin Kiefer; Max Heimel; Sebastian Breß; Volker Markl,Abstract Accurately predicting the cardinality of intermediate plan operations is an essentialpart of any modern relational query optimizer. The accuracy of said estimates has a strongand direct impact on the quality of the generated plans; and incorrect estimates can have anegative impact on query performance. One of the biggest challenges in this field is topredict the result size of join operations. Kernel Density Estimation (KDE) is a statisticalmethod to estimate multivariate probability distributions from a data sample. Previously; weintroduced a modern; self-tuning selectivity estimator for range scans based on KDE that out-performs state-of-the-art multidimensional histograms and is efficient to evaluate on graphicscards. In this paper; we extend these bandwidth-optimized KDE models to estimate theresult size of single and multiple joins. In particular; we propose two approaches:(1) …,Proceedings of the VLDB Endowment,2017,*
Many-Core-Architekturen zur Datenbankbeschleunigung,Kai-Uwe Sattler; Jens Teubner; Felix Beier; Sebastian Breß,Physikalische und technologische Grenzen bei der Erhöhung der Taktfrequenz von Pro- zessorenhaben in den letzten Jahren die Entwicklung von Multi- und Many-Core- Architekturenforciert. Die Ausnutzung dieser Architekturen erfordert jedoch eine weitge- hende Parallelisierungvon Berechnungen. Für den Datenbankbereich bedeutet dies einer- seits ein Überdenken etablierterDatenstrukturen und Verfahren; eröffnet aber gleichzeitig neue Möglichkeiten der Beschleunigungund Skalierung der Datenbankverarbeitung. Ziel des Tutoriums ist es daher; einen Überblicküber den Stand der Forschung und die Ein- satzmöglichkeiten von Many-Core-Architekturenin Datenbanksystemen zu geben. Neben Standard-Prozessoren stehen dabei insbesondereGPGPU-Architekturen im Mittelpunkt; die schon heute die Nutzung von Tausenden Coresermöglichen. Ausgehend von einer Vorstellung aktueller Many-Core- und GPU …,Datenbanksysteme für Business; Technologie und Web (BTW 2015)-Workshopband,2015,*
Cost-Aware Query Optimization during Cloud-Based Complex Event Processing.,Andreas Meister; Sebastian Breß; Gunter Saake,Abstract: Complex Event Processing describes the problem of timely and continuousprocessing of event streams. The load of Complex Event Processing systems can vary (eg;event rates). Static resource provision leads to higher monetary costs because enoughresources have to be provided to efficiently handle peak loads. Therefore; most of the timethe resources will not be fully utilized. One way to achieve scalable processing and elasticalresource allocation fitting varying requirements is to use Cloud Computing. Properties ofCloud Computing are the pay-as-you-go-payment model and high availability. Theseproperties can be used in Complex Event Processing systems to minimize the monetarycosts of systems while satisfying Service Level Agreements. Complex Event Processingsystems must continuously optimize the event processing to adapt to varying loads …,GI-Jahrestagung,2014,*
CoGaDB: A Column-oriented GPU-accelerated DBMS - Reference Manual,Sebastian Breß; Robin Haberkorn; Steven Ladewig,CoGaDB is a prototype of a column-oriented GPU-accelerated database managementsystem developed at the University of Magdeburg. Its purpose is to investigate advancedcoprocessing techniques for effective GPUs utilization during database query processing. Itutilizes our hybrid query processing engine (HyPE) for the physical optimization process.CoGaDB's main purpose is to investigate a GPU-aware database architecture to achieveoptimal performance of DBMS on hybrid CPU/GPU platforms. We are currently working on aarchitecture proposal and try to benefit from past experiences of hybrid CPU/GPU DBMS.Therefore; CoGaDB provides an extensible architecture to enable researchers an easyintegration of their GPU-accelerated operators; coprocessing techniques and queryoptimization heuristics. Note that CoGaDB assumes that the complete database can be …,*,2014,*
Ein selbstlernendes Entscheidungsmodell für die Verteilung von Datenbankoperationen auf CPU/GPU-Systemen,Sebastian Breß,Page 1. Otto-von-Guericke-Universität Magdeburg Fakultät für Informatik Institut fürTechnische und Betriebliche Informationssysteme Arbeitsgruppe Datenbanken MasterarbeitEin selbstlernendes Entscheidungsmodell für die Verteilung von Datenbankoperationenauf CPU/GPU-Systemen Verfasser: Sebastian Breß 26. Februar 2012 Betreuer: Dr.-Ing.Eike Schallehn Universität Magdeburg Fakultät für Informatik Postfach 4120; D–39016Magdeburg Germany Prof. Dr.-Ing. habil. Kai-Uwe Sattler Technische Universität IlmenauFakultät für Informatik und Automatisierung Postfach 100 565; D–98684 Ilmenau GermanyPage 2. Breß; Sebastian: Ein selbstlernendes Entscheidungsmodell für die Verteilungvon Datenbankoperationen auf CPU/GPU-Systemen Masterarbeit; Otto-von-Guericke-Universität Magdeburg; 2012. Page 3. i Danksagung …,*,2012,*
Scotty: Efficient Window Aggregation for out-of-order Stream Processing,Jonas Traub; Philipp Grulich; Alejandro Rodrıguez Cuéllar; Sebastian Breß; Asterios Katsifodimos; Tilmann Rabl; Volker Markl,Abstract—Computing aggregates over windows is at the core of virtually every streamprocessing job. Typical stream processing applications involve overlapping windows and;therefore; cause redundant computations. Several techniques prevent this redundancy bysharing partial aggregates among windows. However; these techniques do not support out-of-order processing and session windows. Out-of-order processing is a key requirement todeal with delayed tuples in case of source failures such as temporary sensor outages.Session windows are widely used to separate different periods of user activity from eachother. In this paper; we present Scotty; a high throughput operator for window discretizationand aggregation. Scotty splits streams into non-overlapping slices and computes partialaggregates per slice. These partial aggregates are shared among all concurrent queries …,*,*,*
A Framework for Cost based Optimization of Hybrid CPU/GPU Query Plans in Database Systems1,Sebastian Breß; Ingolf Geist; Eike Schallehn; Maik Mory; Gunter Saake,*,*,*,*
