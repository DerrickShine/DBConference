Query evaluation techniques for large databases,Goetz Graefe,Abstract Database management systems will continue to manage large data volumes. Thus;efficient algorithms for accessing and manipulating large sets and sequences will berequired to provide acceptable performance. The advent of object-oriented and extensibledatabase systems will not solve this problem. On the contrary; modern data modelsexacerbate the problem: In order to manipulate large sets of complex objects as efficiently astoday's database systems manipulate simple records; query-processing algorithms andsoftware will become more complex; and a solid understanding of algorithm andarchitectural issues is essential for the designer of database management software. Thissurvey provides a foundation for the design and implementation of query execution facilitiesin new database management systems. It describes a wide array of practical query …,ACM Computing Surveys (CSUR),1993,1722
The Volcano optimizer generator: Extensibility and efficient search,Goetz Graefe; William J McKenna,The Volcano project; which provides efficient; extensible tools for query and requestprocessing; particularly for object-oriented and scientific database systems; is reviewed. Inparticular; one of its tools; the optimizer generator; is discussed. The data model; logicalalgebra; physical algebra; and optimization rules are translated by the optimizer generatorinto optimizer source code. It is shown that; compared with the EXODUS optimizer generatorprototype; the search engine of the Volcano optimizer generator is more extensible andpowerful. It provides effective support for non-trivial cost models and for physical propertiessuch as sorting order. At the same time; it is much more efficient; as it combines dynamicprogramming with goal-directed searching and branch-and-bound pruning. Compared withother rule-based optimization systems; it provides complete data model independence …,Data Engineering; 1993. Proceedings. Ninth International Conference on,1993,516
A High Performance Dataflow Database Machine,David J DeWitt; B Gerber; Goetz Graefe; M Heytens; K Kumar; M Muralikrishna,ABSTRACT In this paper; we present the design; implementation techniques; and initialperformance evaluation of Gamma. Gamma is a new relational database machine thatexploits dataﬂow query processing techniques. Gamma is a fully operational prototypeconsisting of 20 VAX 11/750 computers. The design of Gamma is based on what we learnedfrom building our earlier multiprocessor database machine prototype (DIRECT) and severalyears of subsequent research on the problems raised by the DIRECT prototype. In additionto demonstrating that parallelism can really be made to work in a database machine context;the Gamma prototype shows how parallelism can be controlled with minimal controloverhead through a combination of the use of algorithms based on hashing and thepipelining of data between processes. Except for 2 messages to initiate each operator of …,*,1986,502
Encapsulation of parallelism in the Volcano query processing system,Goetz Graefe,Abstract Volcano is a new dataflow query processing system we have developed fordatabase systems research and education. The uniform interface between operators makesVolcano extensible by new operators. All operators are designed and coded as if they weremeant for a single-process system only. When attempting to parallelize Volcano; we had tochoose between two models of parallelization; called here the bracket and operator models.We describe the reasons for not choosing the bracket model; introduce the novel operatormodel; and provide details of Volcano's exchange operator that parallelizes all otheroperators. It allows intra-operator parallelism on partitioned datasets and both vertical andhorizontal inter-operator parallelism. The exchange operator encapsulates all parallelismissues and therefore makes implementation of parallel database algorithms significantly …,ACM SIGMOD Record,1990,472
Multi-table joins through bitmapped join indices,Patrick O'Neil; Goetz Graefe,Abstract This technical note shows how to combine some well-known techniques to create amethod that will efficiently execute common multi-table joins. We concentrate on acommonly occurring type of join known as a star-join; although the method presented willgeneralize to any type of multi-table join. A star-join consists of a central detail table withlarge cardinality; such as an orders table (where an order row contains a single purchase)with foreign keys that join to descriptive tables; such as customers; products; and (sales)agents. The method presented in this note uses join indices with compressed bitmaprepresentations; which allow predicates restricting columns of descriptive tables todetermine an answer set (or foundset) in the central detail table; the method uses differentpredicates on different descriptive tables in combination to restrict the detail table through …,ACM SIGMOD Record,1995,440
The EXODUS optimizer generator,Goetz Graefe; David J DeWitt,Abstract This paper presents the design and an initial performance evaluation of the queryoptimizer generator designed for the EXODUS extensible database system. Algebraictransformation rules are translated into an executable query optimizer; which transformsquery trees and selects methods for executing operations according to cost functionsassociated with the methods. The search strategy avoids exhaustive search and it modifiesitself to take advantage of past experience. Computational results show that an optimizergenerated for a relational system produces access plans almost as good as those producedby exhaustive search; with the search time cut to a small fraction.,ACM SIGMOD Record,1987,440
Volcano/spl minus/an extensible and parallel query evaluation system,Goetz Graefe,To investigate the interactions of extensibility and parallelism in database query processing;we have developed a new dataflow query execution system called Volcano. The Volcanoeffort provides a rich environment for research and education in database systems design;heuristics for query optimization; parallel query execution; and resource allocation. Volcanouses a standard interface between algebra operators; allowing easy addition of newoperators and operator implementations. Operations on individual items; eg; predicates; areimported into the query processing operators using support functions. The semantics ofsupport functions is not prescribed; any data type including complex objects and anyoperation can be realized. Thus; Volcano is extensible with new operators; algorithms; datatypes; and type-specific methods. Volcano includes two novel meta-operators. The …,IEEE Transactions on Knowledge and Data Engineering,1994,437
The architecture of the EXODUS extensible DBMS,Michael J Carey; David J DeWitt; Daniel Frank; M Muralikrishna; Goetz Graefe; Joel E Richardson; Eugene J Shekita,Abstract With non-traditional application areas such as engineering design; image/voicedata management; scientific/statistical applications; and artificial intelligence systems allclamoring for ways to store and efficiently process larger and larger volumes of data; it isclear that traditional database technology has been pushed to its limits. It also seems clearthat no single database system will be capable of simultaneously meeting the functionalityand performance requirements of such a diverse set of applications. In this paper wedescribe the preliminary design of EXODUS; an extensible database system that willfacilitate the fast development of high-performance; application-specific database systems.EXODUS provides certain kernel facilities; including a versatile storage manager and a typemanager. In addition; it provides an architectural framework for building application …,Proceedings on the 1986 international workshop on Object-oriented database systems,1986,377
The cascades framework for query optimization,Goetz Graefe,Abstract This paper describes a new extensible query optimization framework that resolvesmany of the shortcomings of the EXODUS and Volcano optimizer generators. In addition toextensibility; dynamic programming; and memorization based on and extended from theEXODUS and Volcano prototypes; this new optimizer provides (i) manipulation of operatorarguments using rules or functions;(ii) operators that are both logical and physical forpredicates etc.;(iii) schema-specific rules for materialized views;(iv) rules to insert” enforcers”or” glue operators;”(v) rule-specific guidance; permitting grouping of rules;(vi) basic facilitiesthat will later permit parallel search; partially ordered cost measures; and dynamic plans;(vii)extensive tracing support; and (viii) a clean interface and implementation making full use ofthe abstraction mechanisms of C++. We describe and justify our design choices for each …,IEEE Data Eng. Bull.,1995,296
The EXODUS extensible DBMS project: An overview,Michael J Carey; David J DeWitt; Goetz Graefe; David M Haight; Joel E Richardson; Daniel T Schuh; Eugene J Shekita; Scott L Vandenberg,ABSTRACT This paper presents an overview of EXODUS; an extensible database systemproject that is addressing data management problems posed by a variety of challenging newapplications. The goal of the project is to facilitate the fast development of high-performance;application-specific database systems. EXODUS provides certain kernel facilities; includinga versatile storage manager. In addition; it provides an architectural framework for buildingapplication-specific database systems; powerful tools to help automate the generation ofsuch systems; including a rule-based query optimizer generator and a persistentprogramming language; and libraries of generic software components (eg; access methods)that are likely to be useful for many application domains. We briefly describe each of thecomponents of EXODUS in this paper; and we also describe a next-generation DBMS …,Readings in object-oriented database systems,1990,269
Optimization of dynamic query evaluation plans,Richard L Cole; Goetz Graefe,Abstract Traditional query optimizers assume accurate knowledge of run-time parameterssuch as selectivities and resource availability during plan optimization; ie; at compile time. Inreality; however; this assumption is often not justified. Therefore; the “static” plans producedby traditional optimizers may not be optimal for many of their actual run-time invocations.Instead; we propose a novel optimization model that assigns the bulk of the optimizationeffort to compile-time and delays carefully selected optimization decisions until run-time. Ourprevious work defined the run-time primitives;“dynamic plans” using “choose-plan”operators; for executing such delayed decisions; but did not solve the problem ofconstructing dynamic plans at compile-time. The present paper introduces techniques thatsolve this problem. Experience with a working prototype optimizer demonstrates (i) that …,ACM SIGMOD Record,1994,244
Dynamic query evaluation plans,Goetz Graefe; Karen Ward,Abstract In most database systems; a query embedded in a program written in aconventional programming language is optimized when the program is compiled. The queryoptimizer must make assumptions about the values of the program variables that appear asconstants in the query; the resources that can be committed to query evaluation; and thedata in the database. The optimality of the resulting query evaluation plan depends on thevalidity of these assumptions. If a query evaluation plan is used repeatedly over an extendedperiod of time; it is important to determine when reoptimization is necessary. Our work aimsat developing criteria when reoptimization is required; how these criteria can beimplemented efficiently; and how reoptimization can be avoided by using a new techniquecalled dynamic query evaluation plans. We experimentally demonstrate the need for …,ACM SIGMOD Record,1989,224
System and method for optimizing database queries,*,A system and method for optimizing a database query is herein disclosed. The systemconsists of a search engine and a database implementor that determines an optimal plan forexecuting a SQL query. The SQL query is represented as a query tree consisting of anumber of nested expressions. The search engine generates a number of plans from whichan optimal plan is selected. Plans are generated through the application of a set of rulesconsisting of implementation and transformation rules. Implementation rules are used toobtain plans. Transformation rules are used to determine equivalent expressions. A plan forthe query tree entails finding plans for each expression within the tree where each plan isgenerated in accordance with a prescribed set of rules. The database implementor selectsthe set of rules such that more promising plans are generated rather than generating all …,*,1998,207
Data compression and database performance,Goetz Graefe; Leonard D Shapiro,Data compression is widely used in data management to save storage space and networkbandwidth. The authors outline the performance improvements that can be achieved byexploiting data compression in query processing. The novel idea is to leave data incompressed state as long as possible; and to only uncompress data when absolutelynecessary. They show that many query processing algorithms can manipulate compresseddata just as well as decompressed data; and that processing compressed data can speedquery processing by a factor much larger than the compression factor.,Applied Computing; 1991.;[Proceedings of the 1991] Symposium on,1991,177
Experiences building the Open OODB query optimizer,José A Blakeley; William J McKenna; Goetz Graefe,Abstract This paper reports our experiences building the query optimizer for TI's Open OODBsystem. To the best of our knowledge; it is the first working object query optimizer to bebased on a complete extensible optimization framework including logical algebra; executionalgorithms; property enforcers; logical transformation rules; implementation rules; andselectivity and cost estimation. Our algebra incorporates a new materialize operator with itscorresponding logical transformation and implementation rules that enable the optimizationof path expressions. Initial experiments on queries obtained from the object queryoptimization literature demonstrate that our optimizer is able to derive plans that are asefficient as; and often substantially more efficient than; the plans generated by other queryoptimization strategies. These experiments demonstrate that our initial choices for …,ACM SIGMOD Record,1993,173
Query processing techniques for solid state drives,Dimitris Tsirogiannis; Stavros Harizopoulos; Mehul A Shah; Janet L Wiener; Goetz Graefe,Abstract Solid state drives perform random reads more than 100x faster than traditionalmagnetic hard disks; while offering comparable sequential read and write bandwidth.Because of their potential to speed up applications; as well as their reduced powerconsumption; these new drives are expected to gradually replace hard disks as the primarypermanent storage media in large data centers. However; although they may benefitapplications that stress random reads immediately; they may not improve databaseapplications; especially those running long data analysis queries. Database queryprocessing engines have been designed around the speed mismatch between random andsequential I/O on hard disks and their algorithms currently emphasize sequential accessesfor disk-resident data. In this paper; we investigate data structures and algorithms that …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,155
Self-tuning technology in microsoft sql server,Surajit Chaudhuri; Eric Christensen; Goetz Graefe; Vivek R.  Narasayya; Michael J.  Zwilling,Today's databases require database administrators (DBAs) who are responsible forperformance tuning. However; as usage of databases becomes pervasive; it is important thatthe databases are able to automatically tune themselves to application needs and hardwarecapabilities rather than require external intervention by DBAs or applications. Thus self-tuning databases would result in significant reduction in the cost of ownership of databasesand would encourage more widespread use in many nontraditional applications. However;making database management systems self-tuning require significant understanding ofDBMS components and relationships among many of the” tuning knobs” that are exposed tothe application/user. Microsoft SQL Server is committed to the vision of self-tuningdatabases. In this short paper; we review some of the recent advances in making …,IEEE Data Eng. Bull.,1999,155
The five-minute rule ten years later; and other computer storage rules of thumb,Jim Gray; Goetz Graefe,Abstract Simple economic and performance arguments suggest appropriate lifetimes formain memory pages and suggest optimal page sizes. The fundamental tradeoffs are theprices and bandwidths of RAMs and disks. The analysis indicates that with today'stechnology; five minutes is a good lifetime for randomly accessed pages; one minute is agood lifetime for two-pass sequentially accessed pages; and 16 KB is a good size for indexpages. These rules-of-thumb change in predictable ways as technology ratios change. Theyalso motivate the importance of the new Kaps; Maps; Scans; and $/Kaps; $/Maps; $/TBscanmetrics.,ACM Sigmod Record,1997,148
PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS,Conor Cunningham; César A Galindo-Legaria; Goetz Graefe,Abstract PIVOT and UNPIVOT; two operators on tabular data that exchange rows andcolumns; enable data transformations useful in data modeling; data analysis; and datapresentation. They can quite easily be implemented inside a query processor; much likeselect; project; and join. Such a design provides opportunities for better performance; bothduring query optimization and query execution. We discuss query optimization andexecution implications of this integrated design and evaluate the performance of thisapproach using a prototype implementation in Microsoft SQL Server.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,135
On the Efficient Gathering of Sufficient Statistics for Classification from Large SQL Databases.,Goetz Graefe; Usama M Fayyad; Surajit Chaudhuri,Abstract For a wide variety of classification algorithms; scalability to large databases can beachieved by observing that most algorithms are driven by a set of sufficient statistics that aresignificantly smaller than the data. By relying on a SQL backend to compute the sufficientstatistics; we leverage the query processing system of SQL databases and avoid the needfor moving data to the client. We present a new SQL operator (Unpivot) that enables efficientgathering of statistics with minimal changes to the SQL backend. Our approach results insignificant increase in performance without requiring any changes to the physical layout ofthe data. We show analytically how this approach outperforms an alternative that requireschanging in the data layout. We also compare effect of data representation and show that a“dense” representation may be preferred to a “sparse” one; even when the data are fairly …,KDD,1998,128
Electronic database operations for perspective transformations on relational tables using pivot and unpivot columns,*,A “pivot” operation rotates the data items in a relational database table so that certain datavalues in the table become column names of the pivoted table; and the data items of aspecified value column appear in corresponding rows in the new columns of the pivotedtable. A pivot list specifies that only certain values of the pivot column data items participatein the operation. Additional columns of the input table appear as columns in the output table;the rows of the output table are grouped by equal data-item values in these groupingcolumns. An “unpivot” operation provides the inverse of the pivot operation. Both operationsmay be nested in an SQL user query at the algebraic level. The operations occur in thesearch engine of a relational database management system; and may also be invoked aspart of an optimization of another query.,*,2001,127
Sorting And Indexing With Partitioned B-Trees.,Goetz Graefe,Abstract Partitioning within a B-tree; based on an artificial leading key column and combinedwith online reorganization; can be exploited during external merge sort for accurate deepread-ahead and dynamic resource allocation; during index creation for a reduced delay untilthe first query can search the new index; during data loading for streaming integration ofnew data into a fully indexed database; and for miscellaneous other operations. Despiteimproving multiple fundamental database operations using a single basic mechanism; theproposal offers these benefits without requiring data structures or algorithms not yetsupported in modern relational database management systems. While some of the ideasdiscussed here have been touched upon elsewhere; the focus here is on re-thinking therelationship between sorting and B-trees more thoroughly; on exploiting this relationship …,CIDR,2003,122
Efficient assembly for complex objects,Tom Keller; Goetz Graefe; David Maier,Abstract Although obJect-oriented database systems offer advantages over relational orrecord-oriented database systems; such as modelmg facll] tles for complex objects; they arecnticlzed for poor performance and query capabilities on set-oriented applications Theunacceptable performance IS due m part to the ob] ect-at-a-time processing typically usedby object-oriented database systems. We believe that improved performance of ob]ectoriented database systems depends partially on the efficient and se [ectxve retrveval ofsets of complex objects from secondary storage. In this report; we present the method ofcomplex object retrlevai and assembly used in the Volcano query processing system andthe Revelation project. We also present experimental results comparing set-oriented versusobJect-at-at] me complex object assembly,ACM SIGMOD Record,1991,116
Write-optimized B-trees,Goetz Graefe,Abstract Large writes are beneficial both on individual disks and on disk arrays; eg; RAID-5.The presented design enables large writes of internal B-tree nodes and leaves. It supportsboth in-place updates and large append-only (" log-structured") write operations within thesame storage volume; within the same B-tree; and even at the same time. The essence ofthe proposal is to make page migration inexpensive; to migrate pages while writing them;and to make such migration optional rather than mandatory as in log-structured file systems.The inexpensive page migration also aids traditional defragmentation as well asconsolidation of free space needed for future large writes. These advantages are achievedwith a very limited modification to conventional B-trees that also simplifies other B-treeoperations; eg; key range locking and compression. Prior proposals and prototypes …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,113
Implementing sorting in database systems,Goetz Graefe,Abstract Most commercial database systems do (or should) exploit many sorting techniquesthat are publicly known; but not readily available in the research literature. These techniquesimprove both sort performance on modern computer systems and the ability to adaptgracefully to resource fluctuations in multiuser operations. This survey collects many of thesetechniques for easy reference by students; researchers; and product developers. It covers in-memory sorting; disk-based external sorting; and considerations that apply specifically tosorting in database systems.,ACM Computing Surveys (CSUR),2006,110
Merging what's cracked; cracking what's merged: adaptive indexing in main-memory column-stores,Stratos Idreos; Stefan Manegold; Harumi Kuno; Goetz Graefe,Abstract Adaptive indexing is characterized by the partial creation and refinement of theindex as side effects of query execution. Dynamic or shifting workloads may benefit frompreliminary index structures focused on the columns and specific key ranges actuallyqueried---without incurring the cost of full index construction. The costs and benefits ofadaptive indexing techniques should therefore be compared in terms of initialization costs;the overhead imposed upon queries; and the rate at which the index converges to a statethat is fully-refined for a particular workload component. Based on an examination ofdatabase cracking and adaptive merging; which are two techniques for adaptive indexing;we seek a hybrid technique that has a low initialization cost and also converges rapidly. Wefind the strengths and weaknesses of database cracking and adaptive merging …,Proceedings of the VLDB Endowment,2011,99
The five-minute rule twenty years later; and how flash memory changes the rules,Goetz Graefe,Abstract In 1987; Gray and Putzolo presented the five-minute rule; which was reviewed andrenewed ten years later in 1997. With the advent of flash memory in the gap betweentraditional RAM main memory and traditional disk systems; the five-minute rule now appliesto large pages appropriate for today's disks and their fast transfer bandwidths; and it alsoapplies to flash disks holding small pages appropriate for their fast access latency. Flashmemory fills the gap between RAM and disks in terms of many metrics: acquisition cost;access latency; transfer bandwidth; spatial density; and power consumption. Thus; within afew years; flash memory will likely be used heavily in operating systems; file systems; anddatabase systems. Research into appropriate system architectures is urgently needed.,Proceedings of the 3rd international workshop on Data management on new hardware,2007,95
Modern B-tree techniques,Goetz Graefe,Abstract Invented about 40 years ago and called ubiquitous less than 10 years later; B-treeindexes have been used in a wide variety of computing systems from handheld devices tomainframes and server farms. Over the years; many techniques have been added to thebasic design in order to improve efficiency or to add functionality. Examples includeseparation of updates to structure or contents; utility operations such as non-logged yettransactional index creation; and robust query processing such as graceful degradationduring index-to-index navigation.,Foundations and Trends® in Databases,2011,92
Self-selecting; self-tuning; incrementally optimized indexes,Goetz Graefe; Harumi Kuno,Abstract In a relational data warehouse with many tables; the number of possible andpromising indexes exceeds human comprehension and requires automatic index tuning.While monitoring and reactive index tuning have been proposed; adaptive indexing focuseson adapting the physical database layout for and by actual queries." Database cracking" isone such technique. Only if and when a column is used in query predicates; an index for thecolumn is created; and only if and when a key range is queried; the index is optimized forthis key range. The effect is akin to a sort that is adaptive and incremental. This sort is;however; very inefficient; particularly when applied on block-access devices. In contrast;traditional index creation sorts data with an efficient merge sort optimized for block-accessdevices; but it is neither adaptive nor incremental.,Proceedings of the 13th International Conference on Extending Database Technology,2010,92
Hash joins and hash teams in Microsoft SQL Server,Goetz Graefe; Ross Bunker; Shaun Cooper,Abstract The query execution engine in Microsoft SQL Server employs hash-basedalgorithms for inner and outer joins; semi-joins; set operations (such as intersection);grouping; and duplicate removal. The implementation combines many techniques proposedindividually in the research literature but never combined in a single implementation; neitherin a product nor in a research prototype. One of the paper's contributions is a design thatcleanly integrates most existing techniques. One technique; however; which we call hashteams and which has previously been described only in vague terms; has not beenimplemented in prior research or product work. It realizes in hash-based query processingmany of the benefits of interesting orderings in sort-based query processing. Moreover; wedescribe how memory is managed in complex and bushy query evaluation plans with …,VLDB,1998,88
B-tree Indexes and CPU Caches,Goetz Graefe; P-A Larson,Since many existing techniques for exploiting CPU caches in the implementation of B-treeindexes have not been discussed in the literature; most of them are surveyed. Rather thanproviding a detailed performance evaluation for one or two of them on some specificcontemporary hardware; the purpose is to survey and to make widely available thisheretofore-folkloric knowledge in order to enable; structure; and hopefully stimulate futureresearch.,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,84
Auto-parameterization of database queries,*,An auto-parameterization process transforms a database query into a parameterized basicquery form by replacing any constant values in the query with parameters. The auto-parameterization process attempts to generate a safe execution plan from the basic queryform if there is currently no such plan available. A safe execution plan is defined as anexecution plan that is optimal over a range of values for the parameters. If a safe executionplan can be generated; it is passed for execution; along with the constant values that werepresent in the query. If a safe execution plan cannot be generated; the auto-parameterization process passes a specific execution plan for execution. The safe executionplan is cached either at the time it is created or at the time it is executed. The cache issearched each time a parameterized basic query plan is generated by the auto …,*,2002,83
Sort vs. hash revisited,Goetz Graefe; Ann Linville; Leonard D.  Shapiro,Efficient algorithms for processing large volumes of data are very important both forrelational and new object-oriented database systems. Many query-processing operationscan be implemented using sort-or hash-based algorithms; eg intersections; joins; andduplicate elimination. In the early relational database systems; only sort-based algorithmswere employed. In the last decade; hash-based algorithms have gained acceptance andpopularity; and are often considered generally superior to sort-based algorithms such asmerge-join. In this article; we compare the concepts behind sort-and hash-based query-processing algorithms and conclude that (1) many dualities exist between the two types ofalgorithms;(2) their costs differ mostly by percentages rather than by factors;(3) severalspecial cases exist that favor one or the other choice; and (4) there is a strong reason why …,IEEE Transactions on Knowledge and Data Engineering,1994,74
Linked data structure integrity verification system which verifies actual node information with expected node information stored in a table,*,A linked data structure verification system to verify the integrity of at least one linked datastructure simultaneously by way of a verification setup phase and an integrity verificationphase. Individual nodes are retrieved from a memory device and examined seriatim inoptimal memory device location order. Nodes are retrieved and examined in optimalmemory device location order for maximum memory device retrieval performance. Expectedand/or actual node information about nodes in a given linked data structure are temporarilystored as records in an integrity verification table for only as much time as is necessary toverify any part of the node information prior to excising one or more unnecessary recordsfrom the integrity verification table.,*,2001,73
Sort-merge-join: An idea whose time has (h) passed?,Goetz Graefe,Matching two sets of data items is a fundamental operation required in relational; extensible;and object-oriented database systems alike. However; the pros and cons of sort-and hash-based query evaluation techniques in modern query processing systems are still not fullyunderstood. After our earlier research clarified strengths and weaknesses of sort-and hash-based query processing techniques and suggested remedies for the shortcomings of hash-based algorithms; the present paper outlines a number of further differences between sort-merge-join and hybrid hash join that traditionally have been ignored in such comparisonsand render sort-merge-join mostly obsolete. We consolidate old and raise new issuespertinent to the comparison of sort-and hash-based query evaluation techniques and stirsome thought and discussion among both academic and industrial database system …,Data Engineering; 1994. Proceedings. 10th International Conference,1994,73
Fast scans and joins using flash drives,Mehul A Shah; Stavros Harizopoulos; Janet L Wiener; Goetz Graefe,Abstract As access times to main memory and disks continue to diverge; faster non-volatilestorage technologies become more attractive for speeding up data analysis applications.NAND flash is one such promising substitute for disks. Flash offers faster random reads thandisk; consumes less power than disk; and is cheaper than DRAM. In this paper; weinvestigate alternative data layouts and join algorithms suited for systems that use flashdrives as the non-volatile store. All of our techniques take advantage of the fast randomreads of flash. We convert traditional sequential I/O algorithms to ones that use a mixture ofsequential and random I/O to process less data in less time. Our measurements oncommodity flash drives show that a column-major layout of data pages is faster than atraditional row-based layout for simple scans. We present a new join algorithm; RARE …,Proceedings of the 4th international workshop on Data management on new hardware,2008,72
System and method for segmented evaluation of database queries,*,A method of satisfying a database query includes evaluating certain joins on a per-segmentbasis. An expression tree is produced for the query; and the expression tree is evaluated toidentify joins whose operands are two instances of the same relation and whose joinpredicate conjunctively includes an equality comparison between two instances of the samecolumn. When such a join is identified; it may be evaluated by segmenting the operandrelation according to the columns that are compared for equality in the predicate. The join isthen evaluated by performing the join operation separately on each segment. Segmentsmay be spooled separately; thereby exploiting the efficiencies obtained by spooling evenwhere the entire relation is too large to fit in the spool. Execution iterators are provided forspooling successive segments and for applying the join to the spooled segment.,*,2009,65
Extensible query optimization and parallel execution in Volcano,Goetz Graefe; Richard L Cole; Diane L Davison; William J McKenna; Richard H Wolniewicz,*,*,1991,65
A survey of B-tree locking techniques,Goetz Graefe,Abstract B-trees have been ubiquitous in database management systems for severaldecades; and they are used in other storage systems as well. Their basic structure and basicoperations are well and widely understood including search; insertion; and deletion.Concurrency control of operations in B-trees; however; is perceived as a difficult subject withmany subtleties and special cases. The purpose of this survey is to clarify; simplify; andstructure the topic of concurrency control in B-trees by dividing it into two subtopics andexploring each of them in depth.,ACM Transactions on Database Systems (TODS),2010,64
B-tree indexes for high update rates,Goetz Graefe,Abstract In some applications; data capture dominates query processing. For example;monitoring moving objects often requires more insertions and updates than queries. Datagathering using automated sensors often exhibits this imbalance. More generally; indexingstreams is considered an unsolved problem. For those applications; B-tree indexes are goodchoices if some trade-off decisions are tilted towards optimization of updates rather thantowards optimization of queries. This paper surveys some techniques that let B-trees sustainvery high update rates; up to multiple orders of magnitude higher than traditional B-trees; atthe expense of query processing performance. Not surprisingly; some of these techniquesare reminiscent of those employed during index creation; index rebuild; etc.; while othertechniques are derived from well known technologies such as differential files and log …,ACM Sigmod Record,2006,64
Query optimization in object-oriented database systems: a prospectus,Goetz Graefe; David Maier,Abstract We are exploring a scheme that allows optimizing queries over object-orienteddatabases with encapsulated behavior. Objects and classes will be able to reveal theirbehavior in terms of expressions in an algebraic language interpreted by a structural object-oriented database system. An object or class can agree or refuse to reveal its behavior. Thestructural algebra is richer than relational algebra as it includes operators on complex objectcollections; and updates and traversals of individual objects. Objects may reveal to theoptimizer the structural access paths used by their procedures or cost and other statisticsuseful for query optimization. The main features of our approach is that the object-orienteduser interface language is able to perform general computation and to preserve theencapsulation envelope around classes and types.,International Workshop on Object-Oriented Database Systems,1988,62
Parallel nested transactions in transactional memory,*,Various technologies and techniques are disclosed for supporting parallel nestedtransactions in a transactional memory system. Multiple closed nested transactions arecreated for a single parent transaction; and the closed nested transactions are executedconcurrently as parallel nested transactions. Various techniques are used to ensure effectsof the parallel nested transactions are hidden from other transactions outside the parenttransaction until the parent transaction commits. For example; versioned write locks are usedwith parallel nested transactions. When a transactional memory word changes from a writelock to a versioned write lock; an entry is made in a global versioned write lock map to storea pointer to a write log entry that the versioned write lock replaced. When the versioned writelock is encountered during transaction processing; the global versioned write lock map is …,*,2011,60
Database servers tailored to improve energy efficiency,Goetz Graefe,Abstract Database software can be tailored for specific application domains and theirrequired functionality; for specific hardware and its characteristics; or for other purposes.This brief paper collects issues and techniques required or desirable for making a server-class database management system energy-efficient. The opportunities go far beyond costfunctions and general performance improvements. Topics include additional layers in thememory hierarchy; I/O optimizations; data format; scheduling; adaptive query plan execution;and self management in novel ways.,Proceedings of the 2008 EDBT workshop on Software engineering for tailor-made data management,2008,59
Memory management during run generation in external sorting,Per-Åke Larson; Goetz Graefe,Abstract If replacement selection is used in an external mergesort to generate initial runs;individual records are deleted and inserted in the sort operation's workspace. Variable-length records introduce the need for possibly complex memory management and extracopying of records. As a result; few systems employ replacement selection; even though itproduces longer runs than commonly used algorithms. We experimentally compared severalalgorithms and variants for managing this workspace. We found that the simple best fitalgorithm achieves memory utilization of 90% or better and run lengths over 1.8 timesworkspace size; with no extra copying of records and very little other overhead; for widelyvarying record sizes and for a wide range of memory sizes. Thus; replacement selection is aviable algorithm for commercial database systems; even for variable-length records …,ACM SIGMOD Record,1998,55
Rule-based query optimization in extensible database systems,Goetz Graefe,Abstract This thesis presents the problems of query optimization in extensible databasesystems and proposes a solution. It describes the design and an initial evaluation of thequery optimizer generator developed for the EXODUS extensible database system. The goalof the EXODUS system is to provide software tools and libraries to structure and to ease thetask of implementing or extending a database system for a new data model. Our basic modelof optimization is to map a query tree; which consists of operators at the nodes and storeddata at the leaves; to an access plan; which is a tree with implementation methods at thenodes and scans at the leaves. The optimizer generator translates algebraic equivalencerules into an executable optimizer. The equivalence rules are specific to the data model. Thegenerated optimizer reorders query trees and selects implementation methods according …,*,1987,53
Fast algorithms for universal quantification in large databases,Goetz Graefe; Richard L Cole,Abstract Universal quantification is not supported directly in most database systems despitethe fact that it adds significant power to a system's query processing and inferencecapabilities; in particular for the analysis of many-to-many relationships and of set-valuedattributes. One of the main reasons for this omission has been that universal quantificationalgorithms and their performance have not been explored for large databases. In this article;we describe and compare three known algorithms and one recently proposed algorithm forrelational division; the algebra operator that embodies universal quantification. For eachalgorithm; we investigate the performance effects of explicit duplicate removal andreferential integrity enforcement; variants for inputs larger than memory; and parallelexecution strategies. Analytical and experimental performance comparisons illustrate the …,ACM Transactions on Database Systems (TODS),1995,51
Foster B-trees,Goetz Graefe; Hideaki Kimura; Harumi Kuno,Abstract Foster B-trees are a new variant of B-trees that combines advantages of prior B-treevariants optimized for many-core processors and modern memory hierarchies with flashstorage and nonvolatile memory. Specific goals include:(i) minimal concurrency controlrequirements for the data structure;(ii) efficient migration of nodes to new storage locations;and (iii) support for continuous and comprehensive self-testing. Like B link-trees; Foster B-trees optimize latching without imposing restrictions or specific designs on transactionallocking; for example; key range locking. Like write-optimized B-trees; and unlike B link-trees;Foster B-trees enable large writes on RAID and flash devices as well as wear leveling andefficient defragmentation. Finally; they support continuous and inexpensive yetcomprehensive verification of all invariants; including all cross-node invariants of the B …,ACM Transactions on Database Systems (TODS),2012,49
Dynamic resource brokering for multi-user query execution,Diane L Davison; Goetz Graefe,Abstract We propose a new framework for resource allocation based on concepts frommicroeconomics. Specifically; we address the difficult problem of managing resources in amultiple-query environment composed of queries with widely varying resourcerequirements. The central element of the framework is a resource broker that realizes a profitby" selling" resources to competing operators using a performance-based" currency." Theguiding principle for brokering resources is profit maximization. In other words; since thecurrency is derived from the performance objective; the broker can achieve the bestperformance by making the scheduling and resource allocation decisions that maximizeprofit. Moreover; the broker employs dynamic techniques and adapts by changing previousallocation decisions while queries are executing. In a first validation study of the …,ACM SIGMOD Record,1995,49
Adaptive indexing for relational keys,Goetz Graefe; Harumi Kuno,Adaptive indexing schemes such as database cracking and adaptive merging have beeninvestigated to-date only in the context of range queries. These are typical for non-keycolumns in relational databases. For complete self-managing indexing; adaptive indexingmust also apply to key columns. The present paper proposes a design and offers a firstperformance evaluation in the context of keys. Adaptive merging for keys also enablesfurther improvements in B-tree indexes. First; partitions can be matched to levels in thememory hierarchy such as a CPU cache and an in-memory buffer pool. Second; adaptivemerging in merged B-trees enables automatic master-detail clustering.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,47
Algebraic optimization of computations over scientific databases,Richard H Wolniewicz; Goetz Graefe,Abstract Although scientific data analysis increasingly requires access and manipulation oflarge quantities of data; current database technology fails to meet the needs of scientificprocessing in a number of areas. To overcome acceptance problems among scientific users;database systems must provide performance and functionality comparable to currentcombinations of scientific programs and file systems. Therefore; we propose extending theconcept of a database query to include numeric computation over scientific databases. Inthis paper; we examine the specification of an integrated algebra that includes traditionaldatabase operators for pattern matching and search as well as numeric operators forscientific data sets. Through the use of a single integrated algebra; we can performautomatic optimization on scientific computations; realizing all of the traditional benefits of …,IEEE Data Eng. Bull.,1993,46
Encapsulation of parallelism and architecture-independence in extensible database query execution,Goetz Graefe; Diane L Davison,Emerging database application domains demand not only high functionality; but also highperformance. To satisfy these two requirements; the Volcano query execution enginecombines the efficient use of parallelism on a wide variety of computer architectures with anextensible set of query processing operators that can be nested into arbitrarily complexquery evaluation plans. Volcano's novel exchange operator permits designing; developing;debugging; and tuning data manipulation operators in single-process environments butexecuting them in various forms of parallelism. The exchange operator shields the datamanipulation operators from all parallelism issues. The design and implementation of thegeneralized exchange operator are examined. The authors justify their decision to supporthierarchical architectures and argue that the exchange operator offers a significant …,IEEE Transactions on Software Engineering,1993,45
Executing Nested Queries.,Goetz Graefe,Abstract: Optimization of nested queries; in particular finding equivalent “flattened” queriesfor queries that employ the SQL sub-query construct; has been researched extensively. Incontrast; with the exception of nested loops join; execution of nested plans has found littleinterest. Nested execution plans may result from a failure to flatten nested SQL expressionsbut just as likely are created by a query optimizer to exploit all available indexes aseffectively as possible. In fact; if materialized views and index tuning perform as expected;few queries should require large operations such as parallel scans; sorts and hash joins;and most actual query plans will rely entirely on navigating indexes on tables and views.Note that only index navigation plans scale truly gracefully; ie; perform equally well on largeand on very large databases; whereas sorting and hashing scale at best linearly. Since a …,BTW,2003,39
Concurrency control for adaptive indexing,Goetz Graefe; Felix Halim; Stratos Idreos; Harumi Kuno; Stefan Manegold,Abstract Adaptive indexing initializes and optimizes indexes incrementally; as a side effect ofquery processing. The goal is to achieve the benefits of indexes while hiding or minimizingthe costs of index creation. However; index-optimizing side effects seem to turn read-onlyqueries into update transactions that might; for example; create lock contention. This paperstudies concurrency control in the context of adaptive indexing. We show that the design andimplementation of adaptive indexing rigorously separates index structures from indexcontents; this relaxes the constraints and requirements during adaptive indexing comparedto those of traditional index updates. Our design adapts to the fact that an adaptive index isrefined continuously; and exploits any concurrency opportunities in a dynamic way.,Proceedings of the VLDB Endowment,2012,38
A survey of B-tree logging and recovery techniques,Goetz Graefe,Abstract B-trees have been ubiquitous in database management systems for severaldecades; and they serve in many other storage systems as well. Their basic structure andtheir basic operations are well understood including search; insertion; and deletion.However; implementation of transactional guarantees such as all-or-nothing failure atomicityand durability in spite of media and system failures seems to be difficult. High-performancetechniques such as pseudo-deleted records; allocation-only logging; and transactionprocessing during crash recovery are widely used in commercial B-tree implementations butnot widely understood. This survey collects many of these techniques as a reference forstudents; researchers; system architects; and software developers. Central in this discussionare physical data independence; separation of logical database contents and physical …,ACM Transactions on Database Systems (TODS),2012,38
The five-minute rule 20 years later (and how flash memory changes the rules),Goetz Graefe,Y 50 communications of the acm| july 2009| vol. 52| no. 7 practice latency; transferbandwidth; spatial density; power consumption; and cooling costs. 13 Table 1 and somederived metrics in Table 2 illustrate this point (all metrics derived on 4/11/2007 fromdramexchange. com; dvnation. com; buy. com; seagate. com; and samsung. com). Given thenumber of CPU instructions possible during the time required for one disk I/O has steadilyincreased; an intermediate memory in the storage hierarchy is desirable. Flash memoryseems to be a highly probable candidate; as has been observed many times by now. Manyarchitecture details remain to be worked out. For example; in the hardware architecture; willflash memory be accessible via a DIMM slot; a SATA (serial ATA) disk interface; or yetanother hardware interface? Given the effort and delay in defining a new hardware …,Communications of the ACM,2009,37
Parallel external sorting in Volcano,Goetz Graefe,Abstract We add yet another paper on parallel sorting to the large body of literature on thetopic. We briefly survey a new query evaluation system called Volcano developed fordatabase systems research and education; and then focus on Volcano's sort algorithms.Volcano's single-process sort algorithm has several interesting features that make it quiteefficient. Volcano's flexible multi-processing architecture provides efficient mechanisms forsingle-and multi-input and for single-and multi-output sort operations; in any combination.We report experimental performance results sorting medium size and large files on a shared-memory machine.,*,1989,36
Adaptive merging in database indexes,*,A method for implementing adaptive merging in database indexes includes selecting a keyrange from a database query having a range predicate and searching a database for datamatching the key range. The data matching the key range is merged to form a collecteddataset which is stored for future retrieval. A method for optimizing B-tree representation of adatabase using actual queries is also provided.,*,2016,34
Benchmarking adaptive indexing,Goetz Graefe; Stratos Idreos; Harumi Kuno; Stefan Manegold,Abstract Ideally; realizing the best physical design for the current and all subsequentworkloads would impact neither performance nor storage usage. In reality; workloads anddatasets can change dramatically over time and index creation impacts the performance ofconcurrent user and system activity. We propose a framework that evaluates the key premiseof adaptive indexing—a new indexing paradigm where index creation and re-organizationtake place automatically and incrementally; as a side-effect of query execution. We focus onhow the incremental costs and benefits of dynamic reorganization are distributed across theworkload's lifetime. We believe measuring the costs and utility of the stages of adaptationare relevant metrics for evaluating new query processing paradigms and comparing them totraditional approaches.,Technology Conference on Performance Evaluation and Benchmarking,2010,34
System for memory management during run formation for external sorting in database system,*,The memory management system is operational in a database system and functions togracefully transition data from the allocated memory space to run files on disk only asneeded. The memory management system accommodates variable length input records inthe workspace of a database sort operation; requires no extra copying of records in memory;and maintains memory utilization at a high level. This memory management systemtherefore minimizes the amount of data written to disk during run formation and enables theuse of the replacement selection algorithm even with variable length input records; whichimproves performance of sorting and overall operational efficiency of the database system.,*,2000,34
Dynamic query evaluation plans: some course corrections?,Goetz Graefe,Purpose: In database query processing; optimization is commonly regarded as “the hard”part; whether in relational; post-relational; object-oriented; textual-spatial-temporal;federated or web-based database systems. Query execution; on the other hand; isconsidered a mostly straightforward exercise in algorithm implementation; with the currently“hot” twist to consider CPU caches. There is; however; a third piece to the puzzle; namelyphysical database design; eg; the set of useful indexes. The purpose of this brief paper is totie these three pieces together and to encourage students and researchers to adopt abroader perspective of adaptive query processing. Another purpose is to present somecontrarian viewpoints about interesting and worthwhile research topics. Compilation effort: Itis well known that compilation and early binding are good ideas; where they can be …,IEEE Data Eng. Bull.,2000,34
Iterators; Schedulers; and Distributed‐memory Parallelism,Goetz Graefe,Abstract In previous work; we demonstrated the advantages of encapsulating queryevaluation algorithms as 'iterators' for sequential and parallel query evaluation.Unfortunately; those earlier models have a severe drawback with respect to resourceallocation in distributed-memory systems. Since threads may be initiated long before theyactually perform useful work; thread placement decisions may be suboptimal. In this paper;we briefly review the iterator model and then extend it to support bottom-up; just-in-timeactivation of appropriate query plan fragments as well as local and global synchronizationand communication among sibling threads. Some of the algorithms described here mayseem intricate; however; the intricacy is encapsulated entirely in the parallelism or'exchange'iterator; thus freeing developers of data manipulation iterators to focus on the …,Software: Practice and Experience,1996,34
Memory-Contention Responsive Hashjoins; CU-CS-682-93,Goetz Graefe,Abstract Fluctuations in memory contention during query execution may compromise theeffectiveness of previous allocation decisions and result in excessive I/O costs. In order tomaximize system per-formance; memory-intensive algorithms such as hash join mustgracefully adapt to variations in available memory. Responsiveness to memory contention isparticularly important in systems processing mixed workloads due to the erratic frequencyand magnitude of ﬂuctuations. Earlier studies on adaptable hash joins have advocatedlowering I/O costs by reducing the volume; or number of pages; of I/O performed. In thispaper; we present a group of memory-contention responsive hash joins that lower I/O costsby using a large unit of I/O; or cluster; to reduce the amount of time spent on I/() and thatdynamically vary the cluster size in response to ﬂuctuations in memory availability. These …,*,1994,32
Tuning a parallel database algorithm on a shared‐memory multiprocessor,Goetz Graefe; Shreekant S Thakkar,Abstract Database query processing can benefit significantly from parallelism. Paralleldatabase algorithms combine substantial CPU and I/O activity; memory requirements; andmassive data exchange between processes; all of which must be considered to obtainoptimal performance. Since parallel external sorting is a very typical example; we havefocused on sorting to tune Volcano; a new query processing system. The purpose of theVolcano project is to provide efficient; extensible tools for query and request processing innovel application domains; particularly in object-oriented and scientific database systems;and for experimental database performance research. It includes all query processingalgorithms conventionally used in relational database systems as well as several new ones;and can execute all of them in parallel. In this article; we present Volcano's parallel …,Software: Practice and Experience,1992,32
Hierarchical locking in B-tree indexes.,Goetz Graefe,Three designs of hierarchical locking suitable for B-tree indexes are explored in detail andtheir advantages and disadvantages compared. Traditional hierarchies include index; leafpage; and key range or key value. Alternatively; locks on separator keys in interior B-treepages can protect key ranges of different sizes. Finally; for keys consisting of multiplecolumns; key prefixes of different sizes permit a third form of hierarchical locking. Each ofthese approaches requires appropriate implementation techniques. The techniquesexplored here include node splitting and merging; lock escalation and lock de-escalation;and online changes in the granularity of locking. Those techniques are the first designspermitting introduction and removal of levels in a lock hierarchy on demand and withoutdisrupting transaction or query processing. In addition; a simplification of traditional key …,BTW,2007,31
Partitioned B-trees-a user's guide.,Goetz Graefe,Abstract: A recent article introduced partitioned B-trees; in which partitions are defined not inthe catalogs but by distinct values in an artificial leading key column. As there usually is onlya single value in this column; there usually is only a single partition; and queries andupdates perform just like in traditional B-tree indexes. By temporarily permitting multiplevalues; at the expense of reduced query performance; interesting database usage scenariosbecome possible; in particular for bulk insert (database load). The present paper guidesdatabase administrators to exploiting partitioned B-trees even if they are not implemented bytheir DBMS vendor.,BTW,2003,31
Query optimization in revelation; an overview,Scott Daniels; Goetz Graefe; Thomas Keller; David Maier; Duri Schmidt; Bennet Vance,*,Data Engineering,1991,31
Transactional memory using buffered writes and enforced serialization order,*,Various technologies and techniques are disclosed that support buffered writes andenforced serialization order in a software transactional memory system. A buffered writeprocess is provided that performs writes to shadow copies of objects and writes content backto the objects after validating a respective transaction during commit. When a write lock isfirst obtained for a particular transaction; a shadow copy is made of a particular object.Writes are performed to and reads from the shadow copy. After validating the particulartransaction during commit; content is written from the shadow copy to the particular object. Atransaction ordering process is provided that ensures that an order in which the transactionsare committed matches an abstract serialization order of the transactions. Transactions arenot allowed to commit until their ticket number matches a global number that tracks the …,*,2011,29
In-memory performance for big data,Goetz Graefe; Haris Volos; Hideaki Kimura; Harumi Kuno; Joseph Tucek; Mark Lillibridge; Alistair Veitch,Abstract When a working set fits into memory; the overhead imposed by the buffer poolrenders traditional databases non-competitive with in-memory designs that sacrifice thebenefits of a buffer pool. However; despite the large memory available with modernhardware; data skew; shifting workloads; and complex mixed workloads make it difficult toguarantee that a working set will fit in memory. Hence; some recent work has focused onenabling in-memory databases to protect performance when the working data set almost fitsin memory. Contrary to those prior efforts; we enable buffer pool designs to match in-memoryperformance while supporting the" big data" workloads that continue to require secondarystorage; thus providing the best of both worlds. We introduce here a novel buffer pool designthat adapts pointer swizzling for references between system objects (as opposed to …,Proceedings of the VLDB Endowment,2014,28
B-tree indexes; interpolation search; and skew,Goetz Graefe,Abstract Recent performance improvements in storage hardware have benefited bandwidthmuch more than latency. Among other implications; this trend favors large B-tree pages.Recent performance improvements in processor hardware also have benefited processingbandwidth much more than memory latency. Among other implications; this trend favorsadding calculations if they save cache faults. With small calculations guiding the searchdirectly to the desired key; interpolation search complements these trends much better thanbinary search. It performs well if the distribution of key values is perfectly uniform; but it canbe useless and even wasteful otherwise. This paper collects and describes more than adozen techniques for interpolation search in B-tree indexes. Most of them attempt to avoidskew or to detect skew very early and then to avoid its bad effects. Some of these …,Proceedings of the 2nd international workshop on Data management on new hardware,2006,28
Relational division: Four algorithms and their performance,Goetz Graefe,Three known algorithms for relational division; the algebra operator used to expressuniversal quantification (for-all conditions) and an algorithm called hash-division areoutlined. By comparing the algorithms analytically and experimentally; it is shown that thealgorithm provides performance competitive with or superior to that of techniques used todate; namely techniques using sorting or aggregate functions. Furthermore; the algorithmcan eliminate duplicates in the divisor on the fly; ignores duplicates in the dividend; andallows two kinds of partitioning; either of which can be used to resolve hash table overflow orto efficiently implement the algorithm on a multiprocessor system.,Data Engineering; 1989. Proceedings. Fifth International Conference on,1989,28
New algorithms for join and grouping operations,Goetz Graefe,Abstract Traditional database query processing relies on three types of algorithms for joinand for grouping operations. For joins; index nested loops join exploits an index on its innerinput; merge join exploits sorted inputs; and hash join exploits differences in the sizes of thejoin inputs. For grouping; an index-based algorithm has been used in the past whereastoday sort-and hash-based algorithms prevail. Cost-based query optimization chooses themost appropriate algorithm for each query and for each operation. Unfortunately; mistakenalgorithm choices during compile-time query optimization are common yet expensive toinvestigate and to resolve. Our goal is to end mistaken choices among join algorithms andamong grouping algorithms by replacing the three traditional types of algorithms with asingle one. Like merge join; this new join algorithm exploits sorted inputs. Like hash join …,Computer Science-Research and Development,2012,26
Efficient columnar storage in b-trees,Goetz Graefe,Abstract Column-oriented storage formats have been proposed for query processing inrelational data warehouses; specifically for fast scans over non-indexed columns. This shortnote proposes a data compression method that reuses traditional on-disk B-tree structureswith only minor changes yet achieves storage density and scan performance comparable tospecialized columnar designs. The advantage of the proposed method over alternativestorage structures is that traditional algorithms can be reused; eg; for assembling rows withmultiple columns; bulk insertion and deletion; logging and recovery; consistency checking;etc.,ACM SIGMOD Record,2007,26
Transforming query results into hierarchical information,*,A computerized system and method for transforming the results of a query into a hierarchicalinformation stream; such as an extensible Markup Language (XML) data stream isdisclosed. A database server receives a query and generates a rowset. A rowset processor;using the mode specified in the query; processes the rowset and query to generate the XMLdata stream. For the “auto1” mode; the rowset processor transforms a rowset into an XMLdata stream using primary-foreign key information specified in the query to determinenesting. For the “auto2” mode; the rowset processor transforms a rowset into an XML datastream using table ordering information included in the query to determine nesting. For the“explicit” mode; the rowset processor transforms a rowset into an XML data stream using theexplicit organizational information specified in the query.,*,2004,25
Extensibility and search efficiency in the Volcano Optimizer Generator,Goetz Graefe; William J McKenna,*,Intl. Conf. on Data Engineering,1993,25
System for halloween protection in a database system,*,A method for preventing Halloween problem conflicts when updating records in a databasesystem is disclosed. According to the method; in response to an update query; possibleupdate plans are generated; and the most efficient one of the possible update plans isselected. Records in the database are thereafter processed in accordance with the selectedupdate plan such that the records are free from Halloween problem conflicts.,*,2000,23
Transactional support for adaptive indexing,Goetz Graefe; Felix Halim; Stratos Idreos; Harumi Kuno; Stefan Manegold; Bernhard Seeger,Abstract Adaptive indexing initializes and optimizes indexes incrementally; as a side effect ofquery processing. The goal is to achieve the benefits of indexes while hiding or minimizingthe costs of index creation. However; index-optimizing side effects seem to turn read-onlyqueries into update transactions that might; for example; create lock contention. This paperstudies concurrency control and recovery in the context of adaptive indexing. We show thatthe design and implementation of adaptive indexing rigorously separates index structuresfrom index contents; this relaxes constraints and requirements during adaptive indexingcompared to those of traditional index updates. Our design adapts to the fact that anadaptive index is refined continuously and exploits any concurrency opportunities in adynamic way. A detailed experimental analysis demonstrates that (a) adaptive indexing …,The VLDB Journal,2014,22
Combining nested aggregators,*,A method and system for transforming a query tree that includes more than one aggregator.The method includes identifying a first aggregator in the query tree; identifying a secondaggregator that is located below the first aggregator in the query tree; and merging thesecond aggregator with the first aggregator to form a merged aggregator.,*,2009,22
The value of merge-join and hash-join in SQL Server,Goetz Graefe,Abstract Microsoft SQL Server was successful for many years for transaction processing anddecision support workloads with neither merge join nor hash join; relying entirely on nestedloops and index nested loops join. How much difference do additional join algorithms reallymake; and how much system performance do they actually add? In a pure OLTP workloadthat requires only record-to-record navigation; intuition agrees that index nested loops join issufficient. For a DSS workload; however; the question is much more complex. To answer thisquestion; we have analyzed TPC-D query performance using an internal build of SQLServer with merge-join and hash-join enabled and disabled. It shows that merge join andhash join are both required to achieve the best performance for decision support workloads.,VLDB,1999,22
Options in physical database design,Goetz Graefe,Abstract A cornerstone of modern database systems is physical data independence; ie; theseparation of a type and its associated operations from its physical representation inmemory and on storage media. Users manipulate and query data at the logical level; theDBMS translates these logical operations to operations on files; indices; records; and disks.The efficiency of these physical operations depends very much on the choice of datarepresentations. Choosing a physical representation for a logical database is called physicaldatabase design. The number of possible choices in physical database design is very large;moreover; they very often interact with each other. We attempt to list and classify thesechoices and to explore their interactions. The purpose of this paper is to provide an overviewof possible options to the DBMS developer and some guidance to the DBMS …,ACM Sigmod Record,1993,22
Database join optimized for flash storage,*,Computer-implemented systems and associated operating methods implement a fast join fordatabases which is adapted for usage with flash storage. A system comprises a processorthat performs a join of two tables stored in a storage in pages processed in a columnorientation wherein column values for all rows on a page are co-located in mini-pages withinthe page. The processor reduces input/output operations of the join by accessing only joincolumns and mini-pages containing join results.,*,2015,21
Robust query processing.,Goetz Graefe; Arnd Christian König; Harumi Kuno; Volker Markl; Kai-Uwe Sattler,Abstract The 2012 Dagstuhl 12321 Workshop on Robust Query Processing; held from 5–10August 2012; brought together researchers from both academia and industry to discussvarious aspects of robustness in database management systems and ideas for futureresearch. The Workshop was designed as a sequel to an earlier Workshop; DagstuhlWorkshop 10381; that studied a similar set of topics. In this article we summarize some of themain discussion topics of the 12321 Workshop; the results to date; and some open problemsthat remain.,ICDE,2011,21
Combined pessimistic and optimisitic concurrency control,*,Various technologies and techniques are disclosed that improve implementation ofconcurrency control modes in a transactional memory system. A transactional memory wordis provided for each piece of data. The transactional memory word includes a versionnumber; a reader indicator; and an exclusive writer indicator. The transactional memoryword is analyzed to determine if the particular concurrency control mode is proper. Using thetransactional memory word to help with concurrency control allows multiple combinations ofoperations to be performed against the same memory location simultaneously and/or fromdifferent transactions. For example; a pessimistic read operation and an optimistic readoperation can be performed against the same memory location.,*,2008,21
The five-minute rule 20 years later: and how flash memory changes the rules,Goetz Graefe,42 July/August 2008 ACM QUEUE rants: feedback@ acmqueue. com interval is aboutinversely proportional to the record size. Gray and Putzolu gave one hour for 100-byterecords and two minutes for 4-KB pages. The five-minute rule was reviewed and renewed10 years later in 1997. 2 Lots of prices and performance parameters had changed (eg; theprice of RAM had tumbled from $5;000 to $15 per megabyte). Nonetheless; the break-eveninterval for 4-KB pages was still around five minutes. The first purpose of this article is toreview the five-minute rule after another 10 years. Of course; both previous papersacknowledged that prices and performance vary among technologies and devices at anypoint in time (eg; RAM for mainframes versus minicomputers; SCSI versus IDE disks; etc.).Therefore; interested readers are invited to reevaluate the appropriate formulas for their …,Queue,2008,21
The Architecture of the EXODUS Extensible DBMS,M Carey; David J DeWitt; Daniel Frank; Goetz Graefe; M Muralikrishna; Joel E Richardson; Eugene J Shekita,ABSTRACT With non-traditional application areas such as engineering design; image/voicedata management; scientific/statistical applications; and artificial intelligence systems allclamoring for ways to store and efficiently process larger and larger volumes of data; it isclear that traditional database technology has been pushed to its limits. It also seems clearthat no single database system will be capable of simultaneously meeting the functionalityand performance requirements of such a diverse set of applications. In this paper wedescribe the initial design of EXODUS; an extensible database system that will facilitate thefast development of high-performance; applicationspecific database systems. EXODUSprovides certain kernel facilities; including a versatile storage manager and a type manager.In addition; it provides an architectural framework for building application-specific …,Readings in Database Systems,1988,21
Efficient locking techniques for databases on modern hardware.,Hideaki Kimura; Goetz Graefe; Harumi A Kuno,ABSTRACT Traditional database systems are driven by the assumption that disk I/O is theprimary bottleneck; overshadowing all other costs. However; future database systems will bedominated by many-core processors; large main memory; and low-latency semiconductormass storage. In the increasingly common case that the working data set fits in memory orlow-latency storage; new bottlenecks emerge: locking; latching; logging; and critical sectionsin the buffer manager. Prior work has addressed two of these–latching and logging. Thispaper addresses locking and proposes new mechanisms optimized for modern hardware.We devised new algorithms and methods to improve all components of database locking;including key range locking; intent locks; detection and recovery from deadlocks; and earlylock release. Most of the techniques are easily applicable to other database systems …,ADMS@ VLDB,2012,20
Transaction support for indexed summary views,Goetz Graefe; Michael Zwilling,Abstract Materialized views have become a standard technique for performanceimprovement in decision support databases and for a variety of monitoring purposes. Inorder to avoid inconsistencies and thus unpredictable query results; materialized views andtheir indexes should be maintained immediately within user transaction just like indexes onordinary tables. Unfortunately; the smaller a materialized view is; the higher the concurrencycontention between queries and updates as well as among concurrent updates. Therefore;we have investigated methods that reduce contention without forcing users to sacrificeserializability and thus predictable application semantics. These methods extend escrowlocking with multi-granularity (hierarchical) locking; snapshot transactions; multi-versionconcurrency control; key range locking; and system transactions; ie; multiple proven …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,19
Query plan analysis of alternative plans using robustness mapping,*,Computer-implemented and associated operating methods evaluate robustness of a queryplan by measuring performance with regard to a range of runtime conditions and producinga map of relative performance of the given query plan when compared with alternative plansfor a range of conditions. The computer-implemented system comprises logic that evaluatesthe selected query plan in comparison to multiple alternative query plans in a predeterminedrange of runtime conditions that include data characteristics. The logic produces a set ofperformance measurements and analyzes the measured performance to map performanceof the selected query plan in comparison to performance of one or more alternative queryplans.,*,2015,18
System and method for database change notification,*,A client submits a subscription to a database specifying selected data within a data set. Asubscription plan is generated based on the subscription. A query is submitted to thedatabase changing selected data within the data set. A query plan is generated based onthe query. The subscription is matched to the query based on the data set; and the queryplan is supplemented with the subscription plan to generate a notification of the change tothe client.,*,2007,18
Definition; detection; and recovery of single-page failures; a fourth class of database failures,Goetz Graefe; Harumi Kuno,Abstract The three traditional failure classes are system; media; and transaction failures.Sometimes; however; modern storage exhibits failures that differ from all of those. In order tocapture and describe such cases; single-page failures are introduced as a fourth failureclass. This class encompasses all failures to read a data page correctly and with plausiblecontents despite all correction attempts in lower system levels. Efficient recovery seems torequire a new data structure called the page recovery index. Its transactional maintenancecan be accomplished writing the same number of log records as today's efficientimplementations of logging and recovery. Detection and recovery of a single-page failurecan be sufficiently fast that the affected data access is merely delayed; without the need toabort the transaction.,Proceedings of the VLDB Endowment,2012,17
System and method for graceful degradation of a database query,*,A method to achieve acceptable query performance even when a database query optimizerchooses an inefficient query plan due to poor cardinality (row count) estimation includesgenerating a batch sort query plan which includes a row count operation and a reorderoperation. The batch sort; and in particular the reorder operation; is not required to satisfythe query but without these operations the query plan may prove to be inefficient. When therow count operation determines that the query plan is inefficient; the row reorderingoperation reorders rows for more I/O efficient processing. The results of both the row countoperation and the reorder operation are used to produce the desired rowset to satisfy theinput query without discarding any retrieved rows. The combination of the row countoperation and reorder operation allows a graceful degradation of query performance …,*,2009,17
Determining whether change in workload of database system has occurred; and/or whether executing current workload will likely result in problem developing with d...,*,Whether a change in workload of a database system has occurred can be determined.Investigative queries are formulated. Each investigative query is adapted to test a differentresource or a different combination of resources of the database system. The investigativequeries are run when the database system is in an unloaded state to acquire unloadedperformance metrics. The investigative queries are run when the database system is in aloaded state to acquire loaded performance metrics. Whether executing a current workloadof the database system; using a current configuration of the database system; will likelyresult in a problem developing at a later point in time can also be determined.Simultaneously with actual execution of the current workload on the database system usingthe current configuration; a simulation of execution of the current workload using the …,*,2012,16
Exception ordering in contention management to support speculative sequential semantics,*,Various technologies and techniques are disclosed for handling exceptions in sequentialstatements that are executed in parallel. A transactional memory system is provided with acontention manager. The contention manager is responsible for managing exceptions thatoccur within statements that were designed to be executed in an original sequential order;and that were transformed into ordered transactions for speculative execution in parallel.The contention manager ensures that any exceptions that are thrown from one or morespeculatively executed blocks while the statements are being executed speculatively inparallel are handled in the original sequential order.,*,2010,16
Fast loads and fast queries,Goetz Graefe,Abstract For efficient query processing; a relational table should be indexed in multipleways; for efficient database loading; indexes should be omitted. Moerkotte's “smallmaterialized aggregates” can be used to alleviate this tension; notably in the form ofNetezza's “zone maps.” Their most significant advantageous characteristics are that (i) loadbandwidth is maximized by avoiding the cost of index maintenance;(ii) there is no need forcomplex index tuning; and (iii) scans for typical queries are very fast. Their most significantlimiting characteristics are that (iv) they are effective only for query predicates on columnscorrelated with the load sequence;(v) individual outlier values can sharply reduce theireffectiveness; and (vi) they fail to improve search performance within a zone. In thisresearch; we introduce zone filters and zone indexes that address these limitations …,International Conference on Data Warehousing and Knowledge Discovery,2009,16
The microsoft relational engine,Goetz Graefe,Microsoft offers three very successful database products; FoxPro; Access and SQL Server.While SQL Server excels in multi-user transaction performance; Access and its underlyingJet engine excel as an end-user and development tool for desktop and client-serverapplications. One of our top priorities is to improve the integration of these two products. Withrespect to their query processing capabilities; we plan on combining the strengths of SQLServer with those of Access. SQL Server's strengths are focused on management of verylarge tables; server-side cursors; and the use of stored procedures as scripts and as triggers.Access strengths are queries over multiple servers; updatable query results; and bit-mappedprocessing. In our next generation of products; SQL Server will employ new queryprocessing technology. Both optimization and execution will be based on an extensible …,Data Engineering; 1996. Proceedings of the Twelfth International Conference on,1996,16
Efficient Verification of B-tree Integrity.,Goetz Graefe; Ryan Stonecipher,Abstract: The integrity of B-tree structures can become compromised for many reasons.Since these inconsistencies manifest themselves in unpredictable ways; all commercialdatabase management systems include mechanisms to verify the integrity andtrustworthiness of an individual index and of a set of related indexes; and all vendorsrecommend index verification as part of regular database maintenance. This paperintroduces algorithms for B-tree validation; reviews the algorithms' strengths andweaknesses; and proposes a simple yet effective improvement for key verification acrossmultiple B-tree levels. The performance is such that B-tree verification can become part ofscans or backups. Our experimental comparisons include algorithm performance andscalability measured using a shipping product.,BTW,2009,15
Transforming query results into hierarchical information,*,A computerized system and method for transforming the results of a query into a hierarchicalinformation stream; such as an eXtensible Markup Language (XML) data stream isdisclosed. A database server receives a query and generates a rowset. A rowset processor;using the mode specified in the query; processes the rowset and query to generate the XMLdata stream. For the “auto1” mode; the rowset processor transforms a rowset into an XMLdata stream using primary-foreign key information specified in the query to determinenesting. For the “auto2” mode; the rowset processor transforms a rowset into an XML datastream using table ordering information included in the query to determine nesting. For the“explicit” mode; the rowset processor transforms a rowset into an XML data stream using theexplicit organizational information specified in the query.,*,2008,15
Query optimization in object-oriented database systems: The REVELATION project,Goetz Graefe; David Maier,Abstract We are exploring a scheme that allows optimizing queries over object-orienteddatabases with encapsulated behavior. Objects and classes will be able to reveal theirbehavior in terms of expressions in an algebraic language interpreted by a structural object-oriented database system. An object or class can agree or refuse to reveal its behavior. Thestructural algebra is richer than relational algebra as it includes operators on complex objectcollections; and updates and traversals of individual objects. Objects may reveal to theoptimizer the structural access paths used by their procedures or cost and other statisticsuseful for query optimization. The main features of our approach is that the object-orienteduser interface language is able to perform general computation and to preserve theencapsulation envelope around classes and types.,*,1988,15
Software transactional protection of managed pointers,*,Various technologies and techniques are disclosed that provide software transactionalprotection of managed pointers. A software transactional memory system interacts withand/or includes a compiler. At compile time; the compiler determines that there are one ormore reference arguments in one or more code segments being compiled whose sourcecannot be recovered. The compiler executes a procedure to select one or more appropriatetechniques or combinations thereof for communicating the sources of the referencedvariables to the called code segments to ensure the referenced variables can be recoveredwhen needed. Some examples of these techniques include a fattened by-ref technique; astatic fattening technique; a dynamic ByRefInfo type technique; and others. One or morecombinations of these techniques can be used as appropriate.,*,2013,14
Evaluation of set of representative query performance using robustness mapping,*,Computer-implemented systems and associated operating methods use atomic query mapsto identify and evaluate database query plan robustness landmarks. The computer-implemented system comprises logic that evaluates robustness of a selected atomic queryby measuring performance of atomic query execution in a predetermined range of runtimeconditions that include data characteristics. The logic produces a set of measurements thatcan be displayed as one or more performance maps and analyzes the measuredperformance to identify landmarks indicative of database atomic query performancedegradation greater than a predetermined amount.,*,2013,14
Best practices analyzer,*,The Best Practices Analyzer scans one or more instances of the database server forconformance to a set of guidelines and recommendations (“best practices” or “best practicerules”). Best practice rules may be defined by the user and/or may be provided. The BestPractices Analyzer performs analysis of usage of database programming (query) languageand conformance to guidelines. Best Practices Analyzer may check for correct execution ofcommon administrative tasks. Rules may be implemented as procedural code in anylanguage.,*,2011,14
Software transaction commit order and conflict management,*,Various technologies and techniques are disclosed for applying ordering to transactions in asoftware transactional memory system. A software transactional memory system is providedwith a feature to allow a pre-determined commit order to be specified for a plurality oftransactions. The pre-determined commit order is used at runtime to aid in determining anorder in which to commit the transactions in the software transactional memory system. Acontention management process is invoked when a conflict occurs between a firsttransaction and a second transaction. The pre-determined commit order is used in thecontention management process to aid in determining whether the first transaction or thesecond transaction should win the conflict and be allowed to proceed.,*,2010,14
Method for choosing optimal query execution plan for multiple defined equivalent query expressions,*,A query formulates choices of multiple equivalent forms. A choice is represented by aCHOOSE operator having multiple arguments representing the equivalent forms. A lookuptable is generated to include a group for each node other than CHOOSE arguments. ForCHOOSE arguments; the generated table includes a single equivalence group having allchoose arguments as members. The enumeration algorithm of the optimizer is applied togenerate additional members and/or groups; whereby for each group; every member thereofis an equivalent. A cost value is assigned to each member of each group and a member ofeach group is selected as a lowest cost value member. The lowest cost value members ofthe groups define an optimized tree and optimization of the query.,*,2003,14
The one-to-one match operator of the Volcano query processing system,Tom Keller; Goetz Graefe,Abstract Much of the current research on relational database systems focuses on increasingthe functionality and flexibility of query processing. Query processing in relational databasesis based on relational operators. There are a number of theoretical operations; however; thatare typically not included in commercial systems. For example; most commercial systemsprovide the natural join operation but not all provide relational operators such as semi-joinand outer join. We present an operator that exploits the similarity between binary relationaloperators. Binary operators are those that produce a single output relation from two inputrelations. The one-toone matcqoperator has the capability of computing a class of binaryrelational operations. The hash-based implementation of one-to-one match; currently usedin the Volcano query processing system; is described in this paper. This implementation …,*,1989,13
Instant recovery with write-ahead logging: Page repair; system restart; and media restore,Goetz Graefe; Wey Guy; Caetano Sauer,Abstract Download Free Sample Traditional theory and practice of write-ahead logging andof database recovery techniques revolve around three failure classes: transaction failuresresolved by rollback; system failures (typically software faults) resolved by restart with loganalysis;“redo;” and “undo” phases; and media failures (typically hardware faults) resolvedby restore operations that combine multiple types of backups and log replay. The recentaddition of single-page failures and single-page recovery has opened new opportunities farbeyond its original aim of immediate; lossless repair of single-page wear-out in novel ortraditional storage hardware. In the contexts of system and media failures; efficient single-page recovery enables on-demand incremental “redo” and “undo” as part of system restartor media restore operations. This can give the illusion of practically instantaneous restart …,Synthesis Lectures on Data Management,2014,12
Self-diagnosing and self-healing indexes,Goetz Graefe; Harumi Kuno; Bernhard Seeger,Abstract Transactional storage and indexing is the heart of every database; not only forperformance and functionality but also for reliability and availability. For high concurrency;these components must be programmed carefully with short critical sections; a variety ofconsistent states with short transitions; etc. Many-core CPUs exacerbate these requirements.Testing software with 100s or 1; 000s of threads is very difficult. In order to test such code;we suggest verifying the B-tree structure in each traversal. With carefully designed treestructure and node contents; a root-to-leaf pass can verify all nodes along its pathcomprehensively; ie; it can verify all B-tree invariants including its consistency constraintswith respect to its siblings and cousins (defined below). Thus; instead of testing the indeximplementation by running a stress-test and verifying the B-tree structure and contents …,Proceedings of the Fifth International Workshop on Testing Database Systems,2012,12
Deferred maintenance of indexes and of materialized views,Harumi Kuno; Goetz Graefe,Abstract Maintenance of secondary indexes and materialized views can cause the latencyand bandwidth of concurrent information capture to degrade by orders of magnitude. Inorder to preserve performance during temporary bursts of update activity; eg; during loadoperations; many systems therefore support deferred maintenance; at least for materializedviews. However; deferring maintenance means that index or view contents may become out-of-date. In such cases; a seemingly benign choice among alternative query execution plansaffects whether query results represent the latest database contents. We propose here asystem that distinguishes between the maintenance of of logical contents and physicalstructure. This distinction lets us compensate for deferred logical maintenance operationswhile minimizing the impact of deferred physical maintenance operations; and results in …,International Workshop on Databases in Networked Information Systems,2011,12
Selectivity estimation using moments and density functions,Goetz Graefe,Abstract A concise description of the distribution of attribute values is essential in databasequery optimization to estimate the selectivity of database operations and the sizes ofintermediate results of a query. Most current methods used to estimate result sizes dependon assumptions that are rarely justified in real databases; namely the assumptions ofuniform distribution of each attribute and statistical independence between attributes.Moments and density functions are used in statistics to describe the distribution of apopulation. Compared to histograms which are used in some database systems to describevalue distributions; moments and density functions offer the advantages that they requireless storage space and that they can be updated much more efficiently. Statisticaldependencies between attributes can be described using co-moments and multi …,*,1987,12
A Generalized Join Algorithm.,Goetz Graefe,Abstract Database query processing traditionally relies on three alternative join algorithms:index nested loops join exploits an index on its inner input; merge join exploits sorted inputs;and hash join exploits differences in the sizes of the join inputs. Cost-based queryoptimization chooses the most appropriate algorithm for each query and for each operation.Unfortunately; mistaken algorithm choices during compile-time query optimization arecommon yet expensive to investigate and to resolve. Our goal is to end mistaken choicesamong join algorithms by replacing the three traditional join algorithms with a single one.Like merge join; this new join algorithm exploits sorted inputs. Like hash join; it exploitsdifferent input sizes for unsorted inputs. In fact; for unsorted inputs; the cost functions forrecursive hash join and for hybrid hash join have guided our search for the new join …,BTW,2011,11
Fast loads and queries,Goetz Graefe; Harumi Kuno,Abstract For efficient query processing; a relational table should be indexed in multipleways; for efficient database loading; indexes should be omitted. This research introducesnew techniques called zones filters; zone indexes; adaptive merging; and partition filters.The new data structures can be created as side effects of the load process; with all requiredanalyses accomplished while a moderate amount of new data still remains in the buffer pool.Traditional sorting and indexing are not required. Nonetheless; query performance matchesthat of Netezza's zone maps where those apply; exceeds it for the many predicates for whichzone maps are ineffective; and can be comparable to query processing with traditionalindexing; as demonstrated in our simulations.,*,2010,11
Visualizing the robustness of query execution,Goetz Graefe; Harumi Kuno; Janet Wiener,Abstract: In database query processing; actual run-time conditions (eg; actual selectivitiesand actual available memory) very often differ from compile-time expectations of run-timeconditions (eg; estimated predicate selectivities and anticipated memory availability).Robustness of query processing can be defined as the ability to handle unexpectedconditions. Robustness of query execution; specifically; can be defined as the ability toprocess a specific plan efficiently in an unexpected condition. We focus on query execution(run-time); ignoring query optimization (compile-time); in order to complement existingresearch and to explore untapped potential for improved robustness in database queryprocessing. One of our initial steps has been to devise diagrams or maps that show how wellplans perform in the face of varying run-time conditions and how gracefully a system's …,arXiv preprint arXiv:0909.1772,2009,11
Method to increase subscription scalability,*,Systems and methods are provided to increase the scalability of subscriptions in anelectronic database environment. In an illustrative implementation; a computing applicationcomprises at least one instruction set to cooperate with a data environment to optimize theprocessing of subscriptions by the data environment when communicating with cooperatingservices and/or applications. In operation; a subscription is identified. A subscriptiontemplate is created for the subscription and the subscription template is parameterized tocreate a parameter table containing parameters (eg subscription constants). A join is thenperformed between the parameters of the parameter table and the parameterizedsubscription templates to generate application and/or service data required by thecooperating services and/or applications. The data is then processed by the applications …,*,2008,11
Master-detail clustering using merged indexes,Goetz Graefe,Abstract Merged indexes are B-trees that contain multiple traditional indexes and interleavetheir records based on a common sort order. In relational databases; merged indexesimplement ''master-detail clustering''of related records; eg; orders and order details. Thus;merged indexes shift de-normalization from the logical level of tables and rows to thephysical level of indexes and records; which is a much more appropriate place for it. Forobject-oriented applications; clustering can reduce the I/O cost for joining rows in relatedtables to a fraction compared to traditional indexes; with additional beneficial effects onbuffer pool requirements. Prior research has covered merged indexes without providingmuch guidance for their implementation. Enabling the design proposed here is a strictseparation of B-tree and index into two layers of abstraction. In addition; this paper …,Informatik-Forschung und Entwicklung,2007,11
Single-pass restore after a media failure,Caetano Sauer; Goetz Graefe; Theo Härder,When persistent storage fails; traditional media recovery first restores an old backup imagefollowed by replaying the recovery log since the last backup operation. Restoring a backupcan take hours; but log replay often takes much longer due to its random access pattern. Weintroduce single-pass restore; a technique in which restoration of all backups and log replayare performed in a single operation. This allows hiding log replay within the initial restore ofthe backup; thus substantially reducing the time and cost of media recovery and;incidentally; rendering incremental backup techniques unnecessary. Single-pass restore isenabled by a new organization of the log archive; created by a continuous process that iseasily incorporated into the traditional log archiving process. Our empirical analysis showsthat the imposed overhead is negligible in comparison with the substantial benefits …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,10
Fetching optimization in multi-way pipelined database joins,*,A method of performing a multi-way join of a plurality of database relations includesexecuting a plurality of pipelined two-way joins with the database relations. Each two-wayjoin has two sequential phases. In the first phase; missing attributes of the input relations thatare required to evaluate a joining criterion specific to said two-way join are fetched from anon-volatile memory device; and the input relations are joined according to the criterion. Inthe second phase; any additional missing attributes of the input relations are fetched fromthe non-volatile memory device as assigned by an optimization process executed prior tocommencing the multi-way join.,*,2013,10
Adaptive indexing in modern database kernels,Stratos Idreos; Stefan Manegold; Goetz Graefe,Abstract Physical design represents one of the hardest problems for database managementsystems. Without proper tuning; systems cannot achieve good performance. Offline indexingcreates indexes a priori assuming good workload knowledge and idle time. More recently;online indexing monitors the workload trends and creates or drops indexes online. Adaptiveindexing takes another step towards completely automating the tuning process of adatabase system; by enabling incremental and partial online indexing. The main idea is thatphysical design changes continuously; adaptively; partially; incrementally and on demandwhile processing queries as part of the execution operators. As such it brings a plethora ofopportunities for rethinking and improving every single corner of database system design.We will analyze the indexing space between offline; online and adaptive indexing …,Proceedings of the 15th International Conference on Extending Database Technology,2012,10
Designing Database Operators for Flash-enabled Memory Hierarchies.,Goetz Graefe; Stavros Harizopoulos; Harumi A Kuno; Mehul A Shah; Dimitris Tsirogiannis; Janet L Wiener,Abstract Flash memory affects not only storage options but also query processing. In thispaper; we analyze the use of flash memory for database query processing; includingalgorithms that combine flash memory and traditional disk drives. We first focus on flash-resident databases and present data structures and algorithms that leverage the fast randomreads of flash to speed up selection; projection; and join operations. FlashScan andFlashJoin are two such algorithms that leverage a column-based layout to significantlyreduce memory and I/O requirements. Experiments with Postgres and an enterprise SSDdrive show improved query runtimes by up to 6x for queries ranging from simple relationalscans and joins to full TPC-H queries. In the second part of the paper; we use externalmerge sort as a prototypical query execution algorithm to demonstrate that the most …,IEEE Data Eng. Bull.,2010,10
Parallel query execution algorithms,Goetz Graefe,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,10
System and method for improved prefetching,*,The present invention is directed to systems and methods for improved prefetching. Thepresent invention reduces the processing time and associated costs of prefetching byincorporating a number of techniques for efficiently searching a node structure. Suchtechniques eliminate redundant traversal of nodes. Additionally; such techniques do notrequire locking the node structure to prevent concurrent updates. Furthermore; suchtechniques may exploit known key ordering information to optimize a search.,*,2008,10
Performance enhancements for hybrid hash join,Goetz Graefe,*,submitted for publication,1993,10
Hierarchical locking in B-tree indexes,*,Portions of a B-tree index in a database are locked for concurrency control. In one example;hierarchical lock modes are provided that permit locking a key; a gap between the key andthe next key; and a combination of the key and the gap. In another example; key rangelocking may be applied to the B-tree index using locks on separator keys of index nodes. Inanother example; key range locking may be applied to the B-tree index using locks on keyprefixes.,*,2012,9
Hierarchical locking in B-tree indexes,*,Portions of a B-tree index in a database are locked for concurrency control. In one example;hierarchical lock modes are provided that permit locking a key; a gap between the key andthe next key; and a combination of the key and the gap. In another example; key rangelocking may be applied to the B-tree index using locks on separator keys of index nodes. Inanother example; key range locking may be applied to the B-tree index using locks on keyprefixes.,*,2009,9
Five performance enhancements for hybrid hash join,Goetz Graefe,Abstract: In this paper; we focus on set matching algorithms such as intersection; difference;union; and relational join; using join as a representative for all these matching problems. Wediscuss five performance enhancements for hybrid hash join algorithms; namely datacompression; large cluster sizes and multi-level recursion; role reversal of build and probeinputs; histogram methods to exploit non-uniform data and hash value distributions (skew);and join algorithms for multiple inputs. While each of the enhancements is fairly simple; themost surprising result is that hash value skew can be exploited and improve performancerather than being a danger to hybrid hash join performance as conventionally thought. Ourdesign for hash-based N-way matching algorithms is a dual to pipelining data withoutintermediate sorting between multiple merge-joins on the same attribute (interesting …,*,1992,9
Mechanisms for concurrency control and recovery in Prolog—A proposal,Michael J Carey; David J DeWitt; Goetz Graefe,Google; Inc. (search).,Proceedings from the first international workshop on Expert database systems,1986,9
Instant recovery with write-ahead logging: page repair; system restart; media restore; and system failover,Goetz Graefe; Wey Guy; Caetano Sauer,Abstract Traditional theory and practice of write-ahead logging and of database recoveryfocus on three failure classes: transaction failures (typically due to deadlocks) resolved bytransaction rollback; system failures (typically power or software faults) resolved by restartwith log analysis;" redo;" and" undo" phases; and media failures (typically hardware faults)resolved by restore operations that combine multiple types of backups and log replay. Therecent addition of single-page failures and single-page recovery has opened newopportunities far beyond the original aim of immediate; lossless repair of single-page wear-out in novel or traditional storage hardware. In the contexts of system and media failures;efficient single-page recovery enables on-demand incremental" redo" and" undo" as part ofsystem restart or media restore operations. This can give the illusion of practically …,Synthesis Lectures on Data Management,2016,8
An empirical analysis of database recovery costs,Caetano Sauer; Goetz Graefe; Theo Härder,ABSTRACT The time required for recovery from a failure is heavily influenced by hardwaresetup and workload characteristics. In bad but still realistic cases; the recovery requiredduring restart can take hours. For a database system based on write-ahead logging; weperformed a qualitative study of how hardware and software configurations affect thebehavior of the database and; consequently; how this behavior affects recovery time after asystem crash. With the relevant parameters identified in the qualitative study; we performedan empirical quantitative analysis of recovery costs in multiple scenarios. We show thatrecovery costs tend to get worse as hardware and software improve in efficiency; and wediscuss possible approaches to make recovery time independent of system configurationsand workload characteristics.,RDSS (SIGMOD Workshops); Snowbird; UT; USA,2014,8
Hash join and hash aggregation integration system,*,A hash integration system includes a hash join module including build and probe inputs. Ahash aggregation module may aggregate on the probe input of the hash join module; and ahash table generation module may generate an integrated hash table including a recordwith values from the build and aggregated probe inputs. The hash join module may join thebuild and aggregated probe inputs to form a joined output.,*,2013,8
Optimization of query processing with top operations,*,A query processing system performs multiple optimizations of a merge sort for “top”operations. An illustrative query processing system comprises a receiver that receivesdatabase query inputs with a top request; and a sort logic that sorts the inputs usingtemporary files to store intermediate sort data and applies top qualifications to sorted output.An optimizing logic that modifies operation of the sort logic and reduces the number ofrecords in the inputs copied into temporary files.,*,2010,8
Shiftable memory supporting in-memory data structures,*,A shiftable memory supporting in-memory data structures employs built-in data shiftingcapability. The shiftable memory includes a memory having built-in shifting capability to shifta contiguous subset of data from a first location to a second location within the memory. Theshiftable memory further includes a data structure defined on the memory to contain datacomprising the contiguous subset. The built-in shifting capability of the memory to facilitateone or more of movement of the data; insertion of the data and deletion of the data within thedata structure.,*,2017,7
Controlled lock violation for data transactions,*,A system; method; and non-transitory computer readable medium for providing controlledlock violation for data transactions are presented. The system includes a processor forexecuting a first data transaction and a second data transaction; the first and second datatransactions operating on a plurality of data resources. A controlled lock violation modulegrants to the second transaction a conflicting lock to a data resource locked by the firsttransaction with a lock; the conflicting lock granted to the second transaction while the firsttransaction holds its lock. The controlled lock violation module can be applied to distributedtransactions in a two-phase commit and to canned transactions.,*,2016,7
Controlled lock violation,Goetz Graefe; Mark Lillibridge; Harumi Kuno; Joseph Tucek; Alistair Veitch,Abstract In databases with a large buffer pool; a transaction may run in less time than it takesto log the transaction's commit record on stable storage. Such cases motivate a techniquecalled early lock release: immediately after appending its commit record to the log buffer inmemory; a transaction may release its locks. Thus; it cuts overall lock duration to a fractionand reduces lock contention accordingly. Early lock release also has its problems. The initialmention of early lock release was incomplete; the first detailed description andimplementation was incorrect with respect to read-only transactions; and the most recentdesign initially had errors and still does not cover unusual lock modes such as" increment"locks. Thus; we set out to achieve the same goals as early lock release but with a different;simpler; and more robust approach.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,7
Database segment searching,*,A segment encompasses a number of segment records less than the total number of recordsof a database. The segment records have values for a field of the database. Lowest andhighest values of the segment records for the field; and a bitmap for the segment; can bedetermined and stored. Selected bits of the bitmap each correspond to a value for the field.Each selected bit is set to one where at least one segment record has the value to which thebit corresponds. An index relating to just the segment records can be determined and stored.The lowest and highest values; and the bitmap; are adapted to permit determination ofwhether the segment has to be loaded into memory to locate records that satisfy a query.The index is adapted to permit searching of the segment records after the segment has beenloaded into the memory.,*,2012,7
Heap-filter merge join: A new algorithm for joining medium-size inputs,Goetz Graefe,A novel algorithm for relational equijoin is presented. The algorithm is a modification ofmerge join; but promises superior performance for medium-size inputs. In many cases iteven compares favorably with both merge join and hybrid hash join; which is shown usinganalytic cost functions.,IEEE Transactions on Software Engineering,1991,7
Combined join,*,Computer-implemented systems and associated operating methods perform a combinedjoin. A computer-implemented system comprises a processor that performs queryprocessing in a relational database by receiving inputs of a variety of cases and sizes; andperforming a combined database join of two of the received inputs using an index in memoryformed from records of the first input and probed with records from the second input byoptimizing the index for increased-speed searching using fast parent-to-child navigation.The variety of cases comprise combinations of unsorted; sorted; and indexed inputs; and thevariety of sizes comprise input sizes from smaller than the available memory to input sizessubstantially larger than available memory.,*,2015,6
Workload management using robustness mapping,*,Computer-implemented systems and associated operating methods use performance mapscreated by evaluating robustness of a database operator; query plan; or query to analyzehealth of a currently-executing query. The computer-implemented system comprises logicthat receives one or more robustness maps of measured database system performanceacquired during database execution in a predetermined range of runtime conditions. Thelogic analyzes state of a currently-executing query by locating the query's performance onthe robustness maps.,*,2012,6
Facilitating stack read and write operations in a software transactional memory system,*,Various technologies and techniques facilitate stack read and write operations in a softwaretransactional memory system. If the compiler determines that an address for a variable in acode segment is a stack location; the stack location is live on entry; and the address of thevariable has not been taken and passed to another thread; the code is changed to ensurefailure atomicity. One example includes modifying the code so a shadow copy is saved forlocal variables that are live on entry. If the same prior criteria are true except the stacklocation is not live on entry; the code is optimized by ensuring code for logging and softwaretransactional memory operations are not included. If the compiler does not know the addressis the stack location or that the address is not passed to another thread; the code is changedto ensure failure and concurrency atomicity.,*,2011,6
Database system testing using robustness maps,*,Computer-implemented systems and associated operating methods take measurementsand landmarks associated with robustness maps and perform tests evaluating therobustness of a database engine's operator implementations and/or query components. Theillustrative computer-implemented system comprises logic that receives one or morerobustness maps of measured database system performance acquired during databaseexecution in a predetermined range of runtime conditions and uses information from therobustness map or maps to perform regression testing wherein landmarks in the robustnessmaps are operated upon as a robustness bugs describing conditions under which apredetermined implementation of a database operator or query component degrades in amanner different from a predetermined expected manner.,*,2010,6
XML Data Management: Go Native or Spruce Up Relational Systems?,Per-Ake Larson; D Florescu; G Graefe; G Moerkotte; H Pirahesh; H Schoning,Abstract XML data is likely to be widely used as a data exchange format but users also needto store and query XML data. The purpose of this panel is to explore whether and how to bestprovide this functionality … 1. Panelists Dana Florescu (Propel Software Corporation) GoetzGraefe (Microsoft) Guido Moerkotte (University of Mannheim) Hamid Pirahesh (IBM AlmadenResearch Center) Harald Schöning (Software AG) … 2. Panel objectives XML data is rapidlygaining acceptance. Many recommen- dations and standards related to XML have been proposed- see http://www.w3.org/XML for more details. It is com- mon wisdom that XML will be the universaldata exchange format but users also need to be able to store and manipu- late XML dataefficiently. The World Wide Web Consor- tium has recently released a working draft of XQueryhttp://www.w3.org/TR/xquery); a query language for XML data. However; a query …,SIGMOD RECORD,2001,6
The new database imperatives,Goetz Graefe,The market for database systems as well as the science of creating database systems isshifting. The purpose of the paper and presentation is to outline some non conventionalperspectives and to derive imperatives. Some of these perspectives will be shared by some;and some will be controversial: SQL as a language; SQL for business; SQL for scientificanalysis; distributed computing; database storage; and Microsoft SQL Server.,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,6
Optimization of complex dataflows with user-defined functions,Astrid Rheinländer; ULF Leser; Goetz Graefe,Abstract In many fields; recent years have brought a sharp rise in the size of the data to beanalyzed and the complexity of the analysis to be performed. Such analyses are oftendescribed as dataflows specified in declarative dataflow languages. A key technique toachieve scalability for such analyses is the optimization of the declarative programs;however; many real-life dataflows are dominated by user-defined functions (UDFs) toperform; for instance; text analysis; graph traversal; classification; or clustering. This calls forspecific optimization techniques as the semantics of such UDFs are unknown to theoptimizer. In this article; we survey techniques for optimizing dataflows with UDFs. Weconsider methods developed over decades of research in relational database systems aswell as more recent approaches spurred by the popularity of Map/Reduce-style data …,ACM Computing Surveys (CSUR),2017,5
Methods and systems for deadlock detection,*,In at least some examples; a system may include a processor core and a non-transitorycomputer-readable memory in communication with the processor core. The non-transitorycomputer-readable memory may store deadlock detection engine to determine a deadlockcondition; wherein the deadlock detection engine accounts for a set of database lock modes.,*,2014,5
Elasticity in cloud databases and their query processing,Goetz Graefe; Anisoara Nica; Knut Stolze; Thomas Neumann; Todd Eavis; Ilia Petrov; Elaheh Pourabbas; David Fekete,Abstract A central promise of cloud services is elastic; on-demand provisioning. Theprovisioning of data on temporarily available nodes is what makes elastic database servicesa hard problem. The essential task that enables elastic data services is bringing a node andits data up-to-date. Strategies for high availability do not satisfy the need in this contextbecause they bring nodes online and up-to-date by repeating history; eg; by log shipping.Nodes must become up-to-date and useful for query processing incrementally by key range.What is wanted is a technique such that in a newly added node; during each short period oftime; an additional small key range becomes up-to-date; until eventually the entire datasetbecomes up-to-date and useful for query processing; with overall update performancecomparable to a traditional high-availability strategy that carries the entire dataset forward …,International Journal of Data Warehousing and Mining (IJDWM),2013,5
Efficient retry for transactional memory,*,Various technologies and techniques are disclosed for implementing retrying transactions ina transactional memory system. The system allows a transaction to execute a retryoperation. The system registers for waits on every read in a read set of the retryingtransaction. The retrying transaction waits for notification that something in the read set haschanged. A transaction knows if notification is required in one of two ways. If thetransactional memory word contained a waiters bit during write lock acquisition; then duringrelease the transactional memory word is looked up in an object waiters map; and waitingtransactions are signaled. If a writing transaction finds a global count of waiting transactionsto be greater than zero after releasing write locks; a transaction waiters map is used todetermine which waiting transactions need to be signaled. In each case; the write lock is …,*,2011,5
Database system implementation prioritization using robustness maps,*,Computer-implemented systems and associated operating methods take measurementsand landmarks associated with robustness maps and perform tests evaluating therobustness of a database engine's operator implementations and/or query components. Theillustrative computer-implemented system comprises logic that receives one or morerobustness maps of measured database system performance acquired during databaseexecution in a predetermined range of runtime conditions and uses information from therobustness map or maps to prioritize potential changes that improve robustness of adatabase system implementation wherein landmarks in the robustness map or maps areoperated upon as a robustness bug describing conditions under which a predeterminedimplementation of a database operator or query component degrades in a manner …,*,2010,5
Characterizing Queries To Predict Execution In A Database,*,One embodiment is a method that obtains query plans for queries in the workload. Thequery plans include a tree of operators and estimated cardinalities for nodes in the tree. Themethod then groups the operators for the queries and characterizes the workload in terms ofgrouped operators to predict performance of the queries before the queries execute in adatabase.,*,2010,5
Combined pessimistic and optimistic concurrency control,*,Various technologies and techniques are disclosed that improve implementation ofconcurrency control modes in a transactional memory system. A transactional memory wordis provided for each piece of data. The transactional memory word includes a versionnumber; a reader indicator; and an exclusive writer indicator. The transactional memoryword is analyzed to determine if the particular concurrency control mode is proper. Using thetransactional memory word to help with concurrency control allows multiple combinations ofoperations to be performed against the same memory location simultaneously and/or fromdifferent transactions. For example; a pessimistic read operation and an optimistic readoperation can be performed against the same memory location.,*,2010,5
Benchmarking query execution robustness,Janet L Wiener; Harumi Kuno; Goetz Graefe,Abstract Benchmarks that focus on running queries on a well-tuned database system ignorea long-standing problem: adverse runtime conditions can cause database systemperformance to vary widely and unexpectedly. When the query execution engine does notexhibit resilience to these adverse conditions; addressing the resultant performanceproblems can contribute significantly to the total cost of ownership for a database system inover-provisioning; lost efficiency; and increased human administrative costs. For example;focused human effort may be needed to manually invoke workload management actions orfine-tune the optimization of specific queries. We believe a benchmark is needed to measurequery execution robustness; that is; how adverse or unexpected conditions impact theperformance of a database system. We offer a preliminary analysis of barriers to query …,Technology Conference on Performance Evaluation and Benchmarking,2009,5
Support for stack read and write operations,*,Various technologies and techniques facilitate stack read and write operations in a softwaretransactional memory system. If the compiler determines that an address for a variable in acode segment is a stack location; the stack location is live on entry; and the address of thevariable has not been taken and passed to another thread; the code is changed to ensurefailure atomicity. One example includes modifying the code so a shadow copy is saved forlocal variables that are live on entry. If the same prior criteria are true except the stacklocation is not live on entry; the code is optimized by ensuring code for logging and softwaretransactional memory operations are not included. If the compiler does not know the addressis the stack location or that the address is not passed to another thread; the code is changedto ensure failure and concurrency atomicity.,*,2008,5
Storage metrics,Jim Gray; Goetz Graefe,Abstract: Simple economic and performance arguments indicated appropriate lifetimes formain memory pages and sizes of main memory buffer pools. The fundamental tradeoffs arethe prices and bandwidths of DRAMs and disks. They roughly indicate that current systemsshould have about 10 MB of DRAM per randomly accessed disk; and 100 MB of DRAM persequentially accessed disk. They also indicate that 16 KB is a good size for index pagestoday. These rules-of-thumb change in predictable ways as technology ratios change.,Microsoft Research; Draft of March,1997,5
Parallelizing the volcano database query processor,Goetz Graefe,Volcano is a new data flow query processing system developed for database systemsresearch and education. All operators are designed and coded as if they were meant for asingle-process system only. The design implementation of Volcano's exchange operator thatparallelizes all other operators is described. It allows intraoperator parallelism on partitioneddata assets and both vertical and horizontal interoperator parallelism. The exchangeoperator encapsulates all parallelism issues and therefore makes implementation of paralleldatabase algorithms significantly easier and more robust. Included in this encapsulation isthe translation between demand-driven data flow within processes and data-driven data flowbetween processes. In order to evaluate the efficiency of Volcano's exchange operator;parallel sorting was implemented by combining Volcano's single-process sort iterator with …,Compcon Spring'90. Intellectual Leverage. Digest of Papers. Thirty-Fifth IEEE Computer Society International Conference.,1990,5
Instant recovery with write-ahead logging,Theo Härder; Caetano Sauer; Goetz Graefe; Wey Guy,Abstract Instant recovery improves system availability by reducing the mean time to repair;ie; the interval during which a database is not available for queries and updates due torecovery activities. Variants of instant recovery pertain to system failures; media failures;node failures; and combinations of multiple failures. After a system failure; instant restartpermits new transactions immediately after log analysis; before and concurrent to “redo” and“undo” recovery actions. After a media failure; instant restore permits new transactionsimmediately after allocation of a replacement device; before and concurrent to restoringbackups and replaying the recovery log. Write-ahead logging is already ubiquitous in datamanagement software. The recent definition of single-page failures and techniques for log-based single-page recovery enable immediate; lossless repair after a localized wear-out …,Datenbank-Spektrum,2015,4
Orthogonal key-value locking,Goetz Graefe; Hideaki Kimura,B-trees have been ubiquitous for decades; and over the past 20 years; record-level lockinghas been ubiquitous in b-tree indexes. There are multiple designs; each a different tradeoffbetween (i) high concurrency and a fine granularity of locking for updates;(ii) efficient coarselocks for equality and range queries;(iii) run time efficiency with the fewest possibleinvocations of the lock manager; and (iv) conceptual simplicity for efficient development;maintenance; and testing. A new design introduced here is efficient and simple yet supportsboth fine and coarse granularities of locking. A lock request may cover (i) a gap (openinterval) between two (actual) key values;(ii) a key value with its entire list of (actual andpossible) row identifiers (in a non-unique secondary index);(iii) a specific pair of key valueand row identifier; or (iv) a distinct key value and a fraction of all (actual and possible) row …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,4
Logical recovery from single-page failures.,Goetz Graefe; Bernhard Seeger,Abstract: Modern hardware technologies and ever-increasing data sizes increase probabilityand frequency of local storage failures; eg; unrecoverable read errors on individual disksectors or pages on flash storage. Our prior work has formalized singlepage failures andoutlined efficient methods for their detection and recovery. These prior techniques rely onold backup copies of individual pages; eg; as part of a database backup or as old versionsretained after a page migration. Those might not be available; however; eg; after recentindex creation in “non-logged” or “allocation-only logging” mode; which industrial databaseproducts commonly use.,BTW,2013,4
Sorting in a memory hierarchy with flash memory,Goetz Graefe,Abstract Flash memory continues to improve in price; capacity; reliability; durability; andperformance. In addition to consumer and client devices; flash memory is also employed inmany servers. Its optimal usage in various classes of server software; including web servers;file-and-print servers; and database servers; requires evaluation and analysis. The presentpaper analyzes the use of flash memory for database query processing including algorithmsthat combine flash memory and traditional disk drives. External merge sort serves as aprototypical query execution algorithm. The excellent access latency of flash memorypromises substantially better sort performance than sorting with traditional disks.Surprisingly; this is true only in fairly limited cases. Flash memory as intermediate externalstorage improves performance in particular in situations with very limited memory. The …,Datenbank-Spektrum,2011,4
The Five-Minute Rule Ten Years Later; and Other Computer Storage Rules of Thumb,Jim Gray; Goetz Graefe,Abstract: Simple economic and performance arguments suggest appropriate lifetimes formain memory pages and suggest optimal page sizes. The fundamental tradeoffs are theprices and bandwidths of RAMs and disks. The analysis indicates that with today'stechnology; five minutes is a good lifetime for randomly accessed pages; one minute is agood lifetime for two-pass sequentially accessed pages; and 16 KB is a good size for indexpages. These rules-of-thumb change in predictable ways as technology ratios change. Theyalso motivate the importance of the new Kaps; Maps; Scans; and $/Kaps; $/Maps; $/TBscanmetrics.,arXiv preprint cs/9809005,1998,4
Microsoft SQL Server 7.0 query processor,Goetz Graefe; Jim Ewel; Cesar Galindo-Legaria,The information contained in this document represents the current view of MicrosoftCorporation on the issues discussed as of the date of publication. Because Microsoft mustrespond to changing market conditions; it should not be interpreted to be a commitment onthe part of Microsoft; and Microsoft cannot guarantee the accuracy of any informationpresented after the date of publication.,*,1998,4
Fast algorithms for universal quantification in large databases,Goetz Graefe; Richard L Cole,*,submitted for publication,1993,4
Query progress estimation based on processed value packets,*,Abstract A data processing system performs query progress estimation based on processedvalue packets. In the illustrative data processing system; a database query processorcomprises a query optimizer that creates a query plan; and a database plan executor thatexecutes the query plan and observes intermediate result streams processed as the queryplan is executed. A value packet manager anticipates value packets during queryoptimization; creates value packets as the intermediate result streams are processed; andcompares anticipated value packets with created value packets to determine accuracy of theanticipated value packets and estimate query progress.,*,2017,3
Instant recovery for data center savings,Goetz Graefe,Abstract Today's data centers routinely employ triple redundancy; ie; each disk page of adatabase or of a key value store is stored three times (or even more; eg; in database and filesystem backups). In contrast; writeahead logging can reduce the cost of databaseoperations and of data centers; assuming suitable techniques for logging; log archiving;backing up; and recovery. The present paper summarizes our work todate on single-pagerepair; instant restart; and instant restore; it describes our techniques for self-repairingindexes; single-pass restore; and virtual backups; and outlines the opportunities for single-copy databases; for avoidance of ever taking a backup (full or incremental); yet foravailability and reliability matching today's double-or triple-redundant data centers,ACM SIGMOD Record,2015,3
Grouping data,*,A computer-executed method for grouping data comprising; with a processor; generating anumber of sorted runs from an unsorted input; storing the sorted runs in temporary storage;placing pages of data from the sorted runs; one at a time; into a portion of a buffer allocatedto receive that page; and from the allocated portion of the buffer; merging each page of data;one at a time; into a number of aggregated records; the number of aggregated records alsobeing stored in the buffer.,*,2012,3
Merge optimization system,*,A merge optimization system to optimize a run merge process for runs including keys from adata source. The system may include a run generation module to generate a run of the keysin a predetermined order. The system may further include a key characterization module tocharacterize a distribution of the keys across the run at run generation time; and a run mergemodule executed by a computer system to selectively associate the runs based on thecharacterization.,*,2012,3
Sidestep: Co-designed shiftable memory and software,T Kelly; H Kuno; M Pickett; H Boehm; A Davis; W Golab; G Graefe; S Harizopoulos; P Joisha; A Karp; N Muralimanohar; F Perner; G Medeiros-Ribeiro; G Seroussi; A Simitsis; R Tarjan; S Williams,Abstract We have designed computer memories that can shift a long user-specifiedcontiguous region a short; fixed distance in constant time. Such memories make possiblecomplementary co-designed software that not only improves performance but also simplifiesand unifies solutions to fundamental computing problems including sorting; searching; anddata management.,Technical report; HP Labs,2012,3
Handling falsely doomed parents of nested transactions,*,Various technologies and techniques are disclosed for detecting falsely doomed parenttransactions of nested children in transactional memory systems. When rolling back nestedtransactions; a release count is tracked each time that a write lock is released due to rollbackfor a given nested transaction. For example; a write abort compensation map can be used totrack the release count for each nested transaction. The number of times the nestedtransactions releases a write lock is recorded in their respective write abort compensationmap. The release counts can be used during a validation of a parent transaction todetermine if a failed optimistic read is really valid. If an aggregated release count for thenested children transactions accounts for the difference in version numbers exactly; then theoptimistic read is valid.,*,2011,3
Technical perspective Integrating flash devices,Goetz Graefe,Flash memory nowadays seems to be in every discussion about system architecture—notonly in mobile devices from phones to notebook computers; but also in servers; from Webservers to blades and database systems. Sure enough; flash memory boasts multiplequalities and advantages over traditional mass storage; disk drives with rotating platters andmoving access arms. These include no noise or vibration; lower power consumption andcooling requirements during both active and idle times; faster access times; and lower costwhen calculated with a focus on access performance rather than on capacity. For example;a" flash disk" device providing a standard interface and form factor of traditional SATA diskmay cost five times more than a traditional SATA disk; but if it permits 100 times more readoperations per second; the cost for access performance is 20 times less. Similarly; flash …,Communications of the ACM,2009,3
A Performance Evaluation of Histogram-Driven Recursive Hybrid Hash Join,Goetz Graefe,*,submitted for publication,1993,3
Algebraic optimization and parallel execution of computations over scientific databases,Goetz Graefe; Richard H Wolniewicz,*,Proc. Workshop on Metadata Management in Scientific Databases,1992,3
The stability of query evaluation plans and dynamic query evaluation plans,Goetz Graefe,Abstract A database query embedded in a program written in a conventional programminglanguage is optimized when the program is compiled. The query optimizer must makeassumptions about the values of the program variables that appear as constants in the queryand about the data stored in the database. These assumptions include that the query can beoptimized realistically using guessed" typical" values for the program variables and that thedatabase will not change significantly between query optimization and query evaluation.The optimality of the resulting query evaluation plan depends on the validity of theseassumptions. If a query evaluation plan is used repeatedly over an extended period of time;it is important to determine when reoptimization is necessary. We aim at developing criteriawhen reoptimization is required; how these criteria can be implemented efficiently; and …,*,1988,3
The 5 Minute Rule,Jim Gray; Goetz Graefe,*,Techuwal Note; Tandem Computers,1985,3
Fuzzy Planner: Computing Inexactness in a Procedural Problem-solving Languages,Rob Kling,*,*,1973,3
Instant restore after a media failure,Caetano Sauer; Goetz Graefe; Theo Härder,Abstract Media failures usually leave database systems unavailable for several hours untilrecovery is complete; especially in applications with large devices and high transactionvolume. Previous work introduced a technique called single-pass restore; which increasesrestore bandwidth and thus substantially decreases time to repair. Instant restore goesfurther as it permits read/write access to any data on a device undergoing restore—evendata not yet restored—by restoring individual data segments on demand. Thus; the restoreprocess is guided primarily by the needs of applications; and the observed mean time torepair is effectively reduced from several hours to a few seconds.,Advances in Databases and Information Systems,2017,2
Recovering pages of a database,*,Failure of storage media containing at least a portion of a database that has been backed upto backup media is detected. In response to detecting the failure; a log that includestransactions carried out with respect to the database is analyzed. Transactions that accessthe database are run prior to completion of recovery of the portion of the database from thebackup media. Recovery of individual pages is carried out as the individual pages areaccessed by the running transactions.,*,2015,2
Early release of transaction locks based on tags,*,A computing system is associated with a first transaction and a second transaction. The firsttransaction is associated with an update to data and a release of at least one lock on thedata prior to the first transaction being durable. The at least one lock is associated withand/or replaced with at least one tag. The computing system is to identify that the secondtransaction is to acquire the at least one tag based on a read of the data; determine whetherthe first transaction is durable based on the at least one tag; and delay a transaction commitfor the second transaction until the first transaction is durable.,*,2014,2
System and method for executing queries,*,There is provided a computer-implemented method of executing a query plan against adatabase. An exemplary method comprises accessing a first subset of rows of a databasetable using a direct access method. The query plan may comprise the direct access method.The exemplary method also comprises determining a processing cost of accessing the firstsubset of rows. The exemplary method further comprises modifying the direct access methodin response to determining that the processing cost exceeds a specified threshold.Additionally; the exemplary method comprises accessing a second subset of rows of thedatabase table using the modified direct access method.,*,2012,2
Generating progressive query results,*,There is provided a method for generating results for a sort operation. The method includeswriting a subset of input to memory. The subset may be sorted based on the sort operation.The sorted subset may be compared to previous results. The previous results may berecalled from a client of the sort operation based on the comparison.,*,2012,2
Database Workload Management (Dagstuhl Seminar 12282),Shivnath Babu; Goetz Graefe; Harumi Anne Kuno,Abstract This report documents the program and the outcomes of Dagstuhl Seminar 12282"Database Workload Management". Dagstuhl Seminar 12282 was designed to provide avenue where researchers can engage in dialogue with industrial participants for an in-depthexploration of challenging industrial workloads; where industrial participants can challengeresearchers to apply the lessons-learned from their large-scale experiments to multiple realsystems; and that would facilitate the release of real workloads that can be used to drivefuture research; and concrete measures to evaluate and compare workload managementtechniques in the context of these workloads.,Dagstuhl Reports,2012,2
10381 Summary and Abstracts Collection--Robust Query Processing,Götz Graefe; Arnd Christian König; Harumi Anne Kuno; Volker Markl; Kai-Uwe Sattler,Abstract Dagstuhl seminar 10381 on robust query processing (held 19.09. 10-24.09. 10)brought together a diverse set of researchers and practitioners with a broad range ofexpertise for the purpose of fostering discussion and collaboration regarding causes;opportunities; and solutions for achieving robust query processing. The seminar strove tobuild a unified view across the loosely-coupled system components responsible for thevarious stages of database query processing. Participants were chosen for their experiencewith database query processing and; where possible; their prior work in academic researchor in product development towards robustness in database query processing. In order topave the way to motivate; measure; and protect future advances in robust query processing;seminar 10381 focused on developing tests for measuring the robustness of query …,Dagstuhl Seminar Proceedings,2011,2
08281 Executive Summary--Software Engineering for Tailor-made Data Management,Sven Apel; Don Batory; Goetz Graefe; Gunter Saake; Olaf Spinczyk,Abstract Tailor-made data management software (DMS) is not only important in the field ofembedded systems. DMS that incorporates only features that are required bear the potentialto strip down the code base and to improve reliability and maintainability. In the past 20years several new technologies have emerged that aim at lean; efficient; and well-structuredsoftware; which should also be applicable to DMS. In the Dagstuhl Seminar``SoftwareEngineering for Tailor-made Data Management''; July 6h to July 11th; 2008; 29 researchersfrom 7 countries discussed the development; application; and assessment of these newtechnologies in the context of DMS.,Dagstuhl Seminar Proceedings,2008,2
Algorithms for merged indexes.,Goetz Graefe,Within such a merged index; the set of tables; views; and indexes can evolve withoutrestriction. The set of clustering columns can also evolve freely. A relational query processorcan search and update index records just as in traditional indexes. With these abilities; theproposed design may finally bring general masterdetail clustering and its performanceadvantages to traditional databases.,BTW,2007,2
Implementation of Sorting in Database Systems,Goetz Graefe,Abstract It has often been said that sorting algorithms are very instructional in their own rightand representative of a variety of computer algorithms; and that the performance of sorting isindicative of the performance of a variety of other data management tasks. Therefore; thereis a fair amount of literature about the theory of sorting as well as about specific benchmarkresults. On the other hand; most commercial implementations of sorting do (or should!)exploit a number of techniques that are publicly known but not widely discussed in theresearch literature. This survey collects them for easy reference by students; researchers;and product developers. This paper does not contain novel algorithmic techniques and doesnot experimentally evaluate the effectiveness of any one individual technique; instead; itgathers and organizes such techniques in order to organize; stimulate; and focus future …,*,2003,2
The Architecture of Harvey Wiley Corbett,Paul D Stoller,*,*,1995,2
Experiences building the Open OODB query optimizer,W McKenna; Goetz Graefe; J Blakeley,*,Proc. of the Workshop on Database Query Optimizer Generators and Rule-base Optimizers,1993,2
Dynamic Plan Optimization.,Richard L Cole; Goetz Graefe,*,FMLDO,1993,2
Set processing and complex object assembly in Volcano and the REVELATION project,Goetz Graefe,Abstract The REVELATION project aims at combining the conceptual power of encapsulatedbehavior in objectoriented database systems with query optimization and set-orientedprocessing. Volcano is an extensible query processor developed for education andresearch. In this report; we describe evaluation of complex expressions over sets of complexobjects; and selective and efficient retrieval of a set of complex objects and their componentsinto memory. Assembling complex objects in buffer memory is widely regarded as essentialfor increasing performance in object-oriented database systems.,*,1989,2
Update propagation strategies for high-performance OLTP,Caetano Sauer; Lucas Lersch; Theo Härder; Goetz Graefe,Abstract Traditional transaction processing architectures employ a buffer pool where pageupdates are absorbed in main memory and asynchronously propagated to the persistentdatabase. In a scenario where transaction throughput is limited by I/O bandwidth—whichwas typical when OLTP systems first arrived—such propagation usually happens ondemand; as a consequence of evicting a page. However; as the cost of main memorydecreases and larger portions of an application's working set fit into the buffer pool; runningtransactions are less likely to depend on page I/O to make progress. In this scenario; updatepropagation plays a more independent and proactive role; where the main goal is to controlthe amount of cached dirty data. This is crucial to maintain high performance as well as toreduce recovery time in case of a system failure. In this paper; we analyze different …,East European Conference on Advances in Databases and Information Systems,2016,1
System and method for executing queries,*,There is provided a computer-implemented method of executing a query plan against adatabase. An exemplary method comprises accessing a first subset of rows of a databasetable using a direct access method for an index. The query plan may comprise the directaccess method. The exemplary method also comprises determining a processing cost ofaccessing the first subset of rows. The exemplary method further comprises modifying thedirect access method for the index in response to determining that the processing costexceeds a specified threshold. Additionally; the exemplary method comprises accessing asecond subset of rows of the database table using the modified direct access method.,*,2016,1
Parallel nested transactions in transactional memory,*,Various technologies and techniques are disclosed for supporting parallel nestedtransactions in a transactional memory system. Multiple closed nested transactions arecreated for a single parent transaction; and the closed nested transactions are executedconcurrently as parallel nested transactions. Various techniques are used to ensure effectsof the parallel nested transactions are hidden from other transactions outside the parenttransaction until the parent transaction commits. For example; retry is allowed to workcorrectly with parallel nested transactions. When a transaction that is a parallel nestedtransaction or a child transaction of the parallel nested transaction executes a retry; a readset of the transaction is registered for the retry. When a decision is made to propagate theretry past a parallel nested transaction parent of the transaction; keeping the read set …,*,2016,1
Transactions and failure,*,Recovery techniques may be utilized to stabilize a system after a failure. Failures may be causedby hardware faults (eg; power failure or disk failure); software faults (eg; programming errorsor invalid data); or human error. After a failure; applications may resume transactions only afterattempting to restore the system to a consistent state. Such attempts may include repeating andreverting at least some changes that took place before the failure occurred; also known as“redo” recovery and “undo” recovery respectively. After the system is recovered; new transactionsmay begin. Thus; new transactions associated with a corrupted change may executesuccessfully; since those corrupted changes have been restored … FIG. 1 is a block diagramof an example system in accordance with aspects of the present disclosure … FIG. 2 is a flowdiagram of an example method in accordance with aspects of the present disclosure.,*,2015,1
System and method for executing queries,*,There is provided a computer-implemented method of executing a query plan against adatabase. An exemplary method comprises accessing a first subset of rows of a databasetable using a direct access method for an index. The query plan may comprise the directaccess method. The exemplary method also comprises determining a processing cost ofaccessing the first subset of rows. The exemplary method further comprises modifying thedirect access method for the index in response to determining that the processing costexceeds a specified threshold. Additionally; the exemplary method comprises accessing asecond subset of rows of the database table using the modified direct access method.,*,2014,1
A hybrid page layout integrating PAX and NSM,Goetz Graefe; Ilia Petrov; Todor Ivanov; Veselin Marinov,Abstract The paper explores a hybrid page layout (HPL); combining the advantages of NSMand PAX. The design defines a continuum between NSM and PAX supporting both efficientscans minimizing cache faults and efficient insertions and updates. Our evaluation showsthat HPL fills the PAX-NSM performance gap.,Proceedings of the 17th International Database Engineering & Applications Symposium,2013,1
Making Transaction Execution the Bottleneck,Harumi Kuno; Goetz Graefe; Hideaki Kimura,Abstract Traditional database systems rely upon a proven set of tools to guarantee ACIDproperties without compromising performance: a buffer manager to mediate the transfer ofdata between fast in-memory processing and slow disk-based persistent storage; latchingand locking to coordinate concurrent access to data; and logging to enable the recovery;verification; and repair of committed data. These tools are built on code bases that are 10-30years old and designed for hardware assumptions nearly the same age. Modern hardwaretechnologies such as fast persistent memories and multicore break those assumptions;turning the traditional proven tools into the new bottlenecks. Our goal is to rethink thetraditional tools so that they will not be bottlenecks. Here; we review some of theconcurrency-related bottlenecks that face the modern transactional storage management …,International Workshop on Databases in Networked Information Systems,2013,1
Computer indexes with multiple representations,*,The present application is directed to an indexing system. In one example; the indexingsystem includes one or more processors; one or more electronic memories that providerandom access memory; one or more mass storage devices that provide persistent datastorage; and one or more indexing routines; executed by the one or more processors; thatcreate and manage an index data structure comprising nodes that include key-values/reference pairs; the index data structure additionally including a sibling reference foreach index-data-structure node stored in the one or more electronic memories; the one ormore indexing routines removing; from the index data structure; the sibling references forindex-data-structure nodes when transferring the index-data-structure nodes from the one ormore electronic memories to the one or more mass storage devices.,*,2013,1
‘Pause and resume’functionality for index operations,Goetz Graefe; Wey Guy; Harumi Kuno,Online index functionality allows concurrent queries and updates during index maintenancetasks; eg; creation of a new secondary index. Nonetheless; index operations can be bothextremely resource-intensive and also extremely long-running. As such; necessary systemactivities such as adding a new secondary index or changing the primary index mayintroduce significant contention for resources such as CPUs; memory; and space andbandwidth in temporary storage; thus starving the primary database workload. Danger of anadditional workload at inopportune times inhibits adoption of automatic index tuning.Whether initiated by a database administrator or by a soft ware component;'pause andresume'is a step towards self managing database systems. However; realizing even thisstep in a commercial system is more difficult than many researchers may realize. This …,Data Engineering Workshops (ICDEW); 2011 IEEE 27th International Conference on,2011,1
Parallel query optimization,Hans Zeller; Goetz Graefe,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,1
Database Research at the Data-Intensive Systems Center,David Maier; Lois M. L.  Delcambre; Calton Pu; Jonathan Walpole; Goetz Graefe; Leonard D.  Shapiro,This report briefly describes the research activities of the newly formed Data-IntensiveSystems Center (DISC) in Portland; Oregon. DISC includes faculty from the OregonGraduate Institute of Science and Technology and Portland State University. By data-intensive applications we mean applications with high data volume; high data complexity;and/or high data processing demands. Such applications place unique demands on theapplication software; database management systems; operating systems; and networkcommunication facility. Data-intensive applications often require distributed; heterogeneous;and parallel databases.,SIGMOD Record,1993,1
Query Processing Techniques for Large Databases,Goetz Graefe,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; "Query ProcessingTechniques for Large Databases ; CU-CS-579-92" (1992). Computer Science TechnicalReports. Paper 556. http://scholar.colorado.edu/csci_techreports/556,*,1992,1
Architecture-Independent Parallel Query Evaluation in Volcano,Goetz Graefe; Diane Davison,*,*,1990,1
Dagstuhl Reports; Vol. 7; Issue 5 ISSN 2192-5283,Martin Dietzfelbinger; Michael Mitzenmacher; Rasmus Pagh; David P Woodruff; Martin Aumüller; Carola Doerr; Christian Igel; Lothar Thiele; Xin Yao; Tarek R Besold; Artur d’Avila Garcez; Luis C Lamb; Calin A Belta; Rupak Majumdar; Majid Zamani; Matthias Rungger; André Brinkmann; Kathryn Mohror; Weikuan Yu; Falai Chen; Tor Dokken; Thomas A Grandine; Géraldine Morin; Renata Borovica-Gajic; Goetz Graefe; Allison Lee,Theory and Applications of Hashing (Dagstuhl Seminar 17181) Martin Dietzfelbinger; MichaelMitzenmacher; Rasmus Pagh; David P. Woodruff; and Martin Aumüller ........................................… Theory of Randomized Optimization Heuristics (Dagstuhl Seminar 17191) CarolaDoerr; Christian Igel; Lothar Thiele; and Xin Yao ........................ 22 … Human-Like Neural-SymbolicComputing (Dagstuhl Seminar 17192) Tarek R. Besold; Artur d'Avila Garcez; and Luis C. Lamb........................ 56 … Formal Synthesis of Cyber-Physical Systems (Dagstuhl Seminar 17201)Calin A. Belta; Rupak Majumdar; Majid Zamani; and Matthias Rungger ......... 84 … Challengesand Opportunities of User-Level File Systems for HPC (Dagstuhl Seminar 17202) AndréBrinkmann; Kathryn Mohror; and Weikuan Yu ............................ 97 … Geometric Modelling; Interoperabilityand New Challenges (Dagstuhl Seminar 17221) Falai Chen; Tor Dokken; Thomas A …,*,2018,*
Page modification,*,Systems and methods associated with page modification are disclosed. One examplemethod may be embodied on a non-transitory computer-readable medium storing computer-executable instructions. The instructions; when executed by a computer; may cause thecomputer to fetch a page to a buffer pool in a memory. The page may be fetched from atleast one of a log and a backup using single page recovery. The instructions may also causethe computer to store a modification of the page to the log. The modification may be stored tothe log as a log entry. The instructions may also cause the computer to evict the page frommemory when the page is replaced in the buffer pool. Page writes associated with theeviction may be elided.,*,2017,*
Partially sorted log archive,*,Systems and methods associated with an at least partially sorted log archives are disclosed.One example method for restoring a database includes restoring members of a set ofdatabase pages originally stored on a failed media device in the database. Restoring amember of the set of pages includes loading an image of the member of the set of databasepages from a backup. Restoring the member of the set of database pages also includesapplying log entries associated with the member of the set of database pages to the image.The log entries may have been recorded after the image of the member of the set ofdatabase pages was taken. The log entries may be retrieved from the at least partially sortedlog archive. Restoring the member of the set of database pages also includes writing thedatabase page to a replacement media.,*,2017,*
Database transaction replay,*,Systems and methods associated with database transaction replay are described. In oneexample; a computer-readable medium may store computer-executable instructions. Whenexecuted by a computer; the instructions may cause the computer to prevent a databasepage from being written to a persistent portion of a database. The database page may beprevented from being written while there are uncommitted transactions associated with thedatabase page. The instructions may also cause the computer to detect a system failureassociated with the database. In response to the system failure; the instructions may causethe computer to analyze the database to identify a set of transactions that were committedand unwritten to the persistent portion when the system failure occurs. The instructions maycause the computer to then re-perform members of the set of transactions.,*,2017,*
Data restoration,*,An example data restoration approach includes loading a replacement storage media upondetecting a media failure in a failed storage media; detecting a request for data originallystored on the failed storage media that is pending restoration to the replacement storagemedia; and in response to detecting this data request; restoring a data segment associatedwith the data from a backup to the replacement storage media. The approach furthermodifies the data segment in the replacement storage media according to archivedmodifications to the data segment in a log archive and then responds to the data request.,*,2017,*
Tree data structure,*,A method of implementing a tree data structure comprises creating a parent and childrelationship between a first node and a second node; in which the first node and secondnode are at the same hierarchical level of the tree data structure; and maintaining only oneincoming pointer for each of the nodes in the tree data structure at all times. A tree datastructure in which each node in the tree data structure has a single incoming pointer; and inwhich the tree data structure comprises a pointer directed to a local overflow node from anode at the same hierarchical level as the local overflow node.,*,2017,*
Dynamic reordering of operations in a query plan,*,There is provided a computer-implemented method of dynamically reordering operations ina query plan. An exemplary method comprises processing a first set of tuples according to afirst operation. The query plan is pipelined and specifies that the first operation generatesinput for a second operation. The query plan further specifies that the second operation isexecuted after the first operation. The computer-implemented method further includesdetermining that the second operation is to precede the first operation based on a specifiedpolicy. The computer-implemented method further includes executing the second operationfor a second set of tuples before executing the first operation for the second set of tuples.The second operation generates an input for the first operation. The first operation isexecuted after the second operation.,*,2017,*
System for managing address reflectors,*,A system is disclosed for managing address reflectors. In one example; the system disclosesa reflector storage 108; 116; 124; 608; with a set of reflector members 304 associated with aset of reflector names 306. The system also includes a computer 100; 604 programmed withexecutable instructions 500; 610 which operate a set of modules. The modules includesreflector management module 106; 114; 122 which; receives a character-string from userinput 104; 120; and identifies the reflector names 306 whose reflector members 304 arepartially defined by the character-string.,*,2017,*
Structuring page images in a memory,*,Approaches for structuring a plurality of page images in-memory are described in variousexamples of the present disclosure. In one example; a unique page identifier provided withina reference page image is identified. The unique page identifier is associated with a targetpage image stored in-memory. Once identified; the page identifier associated with the targetpage image is replaced with a location specific identifier of the target page image; whereinthe location specific identifier is based on an in-memory location of the target page image.,*,2017,*
Latch-free concurrent searching,*,Systems and methods associated with latch-free searching are disclosed. One examplemethod includes receiving a key identifying data to be retrieved from a tree-based datastructure. The method also includes performing a concurrent; latch-free search of the tree-based data structure until a leaf node is reached. The method also includes validating theleaf node. The method also includes retreading a portion of the search if the leaf node failsvalidation.,*,2017,*
Robust Performance in Database Query Processing (Dagstuhl Seminar 17222),Renata Borovica-Gajic; Goetz Graefe; Allison Lee,Abstract The Dagstuhl Seminar 17222 on" Robust performance in database queryprocessing"; held from 28/May until 02/June 2017; brought together researchers fromacademia and industry to discuss aspects of robustness in database management systemsthat have not been addressed by the previous instances of the seminar. This articlesummarizes the main discussion topics; and presents the summary of the outputs of fourwork groups that discussed: i) updates and database utilities; ii) parallelism; partitioning andskew; iii) dynamic join sequences; and iv) machine learning techniques used to explainunexpected performance observations.,Dagstuhl Reports,2017,*
The Five minute Rule Thirty Years Later and its Impact on the Storage Hierarchy,Raja Appuswamy; Renata Borovica; Goetz Graefe; Anastasia Ailamaki,ABSTRACT In 1987; Jim Gray and Gianfranco Putzolu put forth the five-minute rule fortrading memory to reduce disk-based I/O on the then-current price–performancecharacteristics of DRAM and HDD. The fiveminute rule has gained wide-spread acceptancesince its introduction as an important rule-of-thumb in data engineering and has beenrevisited twice; once in 1997 to account for changes in technology and economic ratio ofHDDs and DRAM; and again in 2007 to investigate the impact of NAND flash-based SSDson the two-tier; DRAM–disk storage hierarchy. In this paper; we revisit the five-minute rulethree decades since its introduction. First; we present changes that have dominated thestorage hardware landscape in the last decade and recompute the break-even intervals fortoday's multi-tiered storage hierarchy. Then; we present recent trends to predict properties …,Proceedings of the 7th International Workshop on Accelerating Analytics and Data Management Systems Using Modern Processor and Storage Architectures,2017,*
Come and crash our database!-Instant recovery in action.,Caetano Sauer; Gilson Souza; Goetz Graefe; Theo Härder,ABSTRACT We present a demonstration of instant recovery; a family of techniques to enableincremental and on-demand recovery from different classes of failures in transactionaldatabase systems. In contrast to traditional ARIES-based algorithms; instant recovery allowstransactions to run concurrently to recovery actions—not only permitting earlier access todata that requires recovery but also using the post-failure access pattern to actually guidethe recovery process. This mechanism prioritizes data needed most urgently after a failure;thus dramatically reducing the mean time to repair perceived by any individual transaction.We have implemented instant recovery in an open-source storage manager and developeda Web-based interface to showcase its recovery capabilities. Users of this demo applicationare able to control the execution of various benchmarks and inject different types of …,EDBT,2017,*
Estimating data,*,Disclosed herein are a system; non transitory computer-readable medium; and method for estimatingdatabase performance. A request for an estimate of data is read. The estimate is calculated basedat least partially on a node located in a data structure … Database management software mayutilize various hierarchical data structures to index and store data. One approach used to predictand improve database performance is to estimate the amount of data it would return in responseto a given predicate. Such estimates may be used to optimize a query plan … FIG. 1 is an examplesystem in accordance with aspects of the disclosure … FIG. 2 is a flow diagram in accordancewith aspects of the disclosure … FIG. 3 is a working example in accordance with aspects ofthe disclosure … FIG. 4 is a further working example in accordance with aspects of thedisclosure … As noted above; the performance of a database may be improved with an …,*,2016,*
Applying write elision,*,A data portion is evicted from a buffer; where the evicted data portion is modified from acorresponding data portion in a persistent storage. Write elision is applied to suppresswriting the evicted data portion to the persistent storage. Subsequent to applying the writeelision and in response to reading a version of the data portion; a redo of a modification ofthe read data portion is applied.,*,2016,*
Data restructuring in multi-level memory hierarchies,*,A method; executed on a suitably programmed processor; provides for ordering recordsusing a memory hierarchy. The memory hierarchy includes and two or more lower levels ofthe memory hierarchy. The method includes the steps of (a) receiving unsorted inputrecords;(b) reading the input records in pages and writing one or more pages of the inputrecords to the primary memory;(c) sorting the pages of input records to create a run;(d) if asize of the run exceeds primary memory capacity; moving data that just exceeds the primarymemory capacity to a secondary level of the memory hierarchy;(e) repeating steps (a)-(d)until either (i) all unsorted input records have been read into primary memory and sorted; or(ii) a capacity of the secondary level of the memory hierarchy is exceeded;(f) following step(e)(ii); moving data that just exceeds the secondary memory capacity from the secondary …,*,2016,*
BaSE (Byte addressable Storage Engine) Transaction Manager.,Sathyanarayanan Manamohan; Krishnaprasad Shastry; Shine Mathew; Ravi Sarveswara; Kirk Bresniker; Goetz Graefe,Abstract Non-Volatile Memory (NVM) is an emerging memory technology that combines thebest properties of current hard disks and main memories by providing non-volatility; highdensity; high speed; and byte addressability. This provides an opportunity to redesignsystems and their software stacks to improve performance and to reduce the complexity.Present-day database systems are designed and optimized for traditional disks and memoryhierarchies. They are very complex because they handle varying levels of storage latencies;from CPU caches to hard disks. Our intention is to build a prototype storage engine that isoptimized for NVM and which takes advantage of the collapsed memory hierarchy. We aredeveloping this storage engine in an incremental way. In this paper; we describe a novelapproach to optimize write-ahead logging (WAL) for NVM based systems. Most database …,COMAD,2016,*
Revisiting optimistic and pessimistic concurrency control,Goetz Graefe,Abstract: Optimistic concurrency control relies on end-of-transaction validation rather thanlock acquisition prior to data accesses. Optimistic concurrency control is popular and varioussystems employ it based on a conviction that it increases concurrency; performance; andscalability. In contrast; we have concluded that optimistic concurrency control permits moreconcurrency than pessimistic concurrency control only if it fails to detect some actualconflicts or if a particular implementation of locking detects false conflicts. An example of theformer is a weak transaction isolation level; eg;“repeatable read” instead of “re-peatablecount;” ie; serializability. An example of the latter is an unnecessarily coarse granularity oflocking; eg; traditional key-value locking. Another example of the latter is unnecessarily longlock retention; eg; while writing a commit log record to stable storage in order to ensure a …,*,2016,*
Query Plan Analysis Of Alternative Plans Using Robustness Mapping,*,Computer-implemented and associated operating methods evaluate robustness of a queryplan by measuring performance with regard to a range of runtime conditions and producinga map of relative performance of the given query plan when compared with alternative plansfor a range of conditions. The computer-implemented system comprises logic that evaluatesthe selected query plan in comparison to multiple alternative query plans in a predeterminedrange of runtime conditions that include data characteristics. The logic produces a set ofperformance measurements and analyzes the measured performance to map performanceof the selected query plan in comparison to performance of one or more alternative queryplans.,*,2015,*
Delta partitions for backup and restore,*,Systems and methods of using delta partitions for backup and restore are disclosed. Anexample method may include after a full backup; separating change information from staticinformation. The method may also include concentrating the change information in a deltapartition prior to a differential backup. The method may also include incrementally backingup only the delta partition during the differential backup.,*,2015,*
Evaluation of database query plan robustness landmarks using operator maps or query maps,*,Computer-implemented systems and associated operating methods create and use anoperator map to identify and evaluate database query plan robustness landmarks. Thecomputer-implemented system comprises logic that creates a map evaluating performanceof an implementation of a database engine operator during execution under actualoperating conditions. The logic measures and maps performance for a selected range ofruntime conditions including resource availability and data characteristics.,*,2015,*
Data store page recovery,*,In one implementation; a data store page recovery process includes selecting a pagereference and an update record reference at a page recovery mapping based on a pageidentifier; accessing a backup page via the page reference; accessing an update record viathe update record reference; and modifying the backup page according to the update record.The page reference is associated with the update record reference at the page recoverymapping.,*,2015,*
Exploitation of correlation between original and desired data sequences during run generation,*,A computer executed method of exploiting correlations between original and desired datasequences during run generation comprises; with a processor; adding a number of datavalues from a data source to a first memory device; the first memory device defining aworkspace; determining whether the data values within the workspace should be output inascending or descending order for a number of runs; and writing a number of the datavalues as a run to a second memory device in the determined order.,*,2014,*
Histogram processing by trend and pattern removal,*,A data processing system compress a histogram with less information loss than simplyreducing the number of steps (or intervals) in the histogram. The data processing systemuses a very detailed histogram as a starting point and comprises histogram compressionlogic that compresses a detailed histogram by detecting trends and periodic patterns in thedetailed histogram. The histogram compression logic extracts the detected trends andperiodic patterns from the detailed histogram; and forms a compressed histogram as adistribution of data remaining after extraction of the trends and periodic patterns.,*,2014,*
Distribution of key values,*,A computer apparatus and related method to reduce database congestion is provided. Inone aspect; the computer apparatus and method may generate a new partition within ahierarchical data structure of interlinked nodes; if a distribution of key values stored in thehierarchical data structure is skewed in favor of a range of key values.,*,2014,*
Parallel aggregation system,*,A parallel aggregation system includes a data analysis module to determine a unique keyvalue of a record to be forwarded to a destination. A pre-processing module may determineexistence of the record in a buffer and priority of the record in a priority queue. Based on theexistence and priority; the pre-processing module may absorb the record in the buffer andselectively forward another record in the buffer to the destination.,*,2014,*
Methods and systems for a deadlock resolution engine,*,In at least some examples; a system may include a processor core and a non-transitorycomputer-readable memory in communication with the processor core. The non-transitorycomputer-readable memory may store a deadlock resolution engine to resolve a deadlockcondition based on an abort shortest pipeline policy.,*,2014,*
BaSE (byte addressable storage engine) access method,Krishnaprasad Shastry; Shine Mathew; Sathyanarayanan Manamohan; Goetz Graefe,Abstract Non-Volatile Memory (NVM) is an emerging memory technology that combines thebest properties of today's hard disks and today's main memory by combining non-volatility;high density; high speed; and byte addressability. This provides an opportunity to redesignsystems and their software stacks to improve performance and to reduce the system andsoftware complexity. Present-day database systems are designed and optimized fortraditional disks and deep memory hierarchies. This makes them very complex because theyhave to handle varying levels of storage latencies; from CPU caches to hard disks. Ourintention is to build a prototype storage engine optimized for NVM to take advantage of thecollapsed memory hierarchy; and to develop this storage engine in an incremental way. Inthis paper; we discuss the optimizations for the data access module. We modified the B …,Proceedings of the 19th International Conference on Management of Data,2013,*
Dagstuhl Reports; Vol. 2; Issue 8 ISSN 2192-5283,Goetz Graefe; Wey Guy; Harumi A Kuno; Glenn Paulley; Christopher W Clifton; Bart Kuijpers; Katharina Morik; Yucel Saygin; Görschwin Fey; Masahiro Fujita; Natasa Miskov-Zivanov; Kaushik Roy; Matteo Sonza Reorda; Jürgen Dix; Koen V Hindriks; Brian Logan; Wayne Wobcke; Samson Abramsky; Jean Krivine; Michael W Mislove,Robust Query Processing (Dagstuhl Seminar 12321) Goetz Graefe; Wey Guy; Harumi A.Kuno; and Glenn Paulley … Mobility Data Mining and Privacy (Dagstuhl Seminar12331) Christopher W. Clifton; Bart Kuijpers; Katharina Morik; and Yucel Saygin ...... 16 … VerifyingReliability (Dagstuhl Seminar 12341) Görschwin Fey; Masahiro Fujita; NatasaMiskov-Zivanov; Kaushik Roy; and Matteo Sonza Reorda .................................................................... 54 … Engineering Multiagent Systems (Dagstuhl Seminar 122342) Jürgen Dix; Koen V.Hindriks; Brian Logan; and Wayne Wobcke ................. 74 … Information Flow and Its Applications(Dagstuhl Seminar 12352) Samson Abramsky; Jean Krivine; and Michael W. Mislove ....................... 99 … Published online and open access by Schloss Dagstuhl – Leibniz-Zentrum für InformatikGmbH; Dagstuhl Publishing; Saarbrücken/Wadern; Germany. Online available at …,*,2013,*
Architectural Analysis of the Moqi Inca Site,Shawn Vaughan,The Inca Empire of the Andes region in South America is widely considered one of the mostinfluential and largest civilizations in the world. They were a highly organized and structuredsociety that managed to create an expansive road system across their territory and weremasters of logistics; yet they had no writing system of their own (though they did have thequipu).,*,2013,*
Dagstuhl Reports; Vol. 2; Issue 7 ISSN 2192-5283,Alan Bundy; Dieter Hutter; Cliff B Jones; J Strother Moore; Peter Feiler; Jérôme Hugues; Oleg Sokolsky; Rüdiger Kapitza; Matthias Schunter; Marc Shapiro; Paulo Verissimo; Michael Waidner; Shivnath Babu; Goetz Graefe; Harumi Anne Kuno; Alberto Apostolico; Andreas Dress; Laxmi Parida,AI meets Formal Software Development (Dagstuhl Seminar 12271) Alan Bundy; DieterHutter; Cliff B. Jones; and J Strother Moore … Architecture-Driven Semantic Analysisof Embedded Systems (Dagstuhl Seminar 12272) Peter Feiler; Jérôme Hugues; and Oleg Sokolsky................................. 30 … Security and Dependability for Federated Cloud Platforms (DagstuhlSeminar 12281) Rüdiger Kapitza; Matthias Schunter; Marc Shapiro; Paulo Verissimo; and MichaelWaidner ......................................................................... 56 … Database Workload Managemen (DagstuhlSeminar 12282) Shivnath Babu; Goetz Graefe; and Harumi Anne Kuno ........................... 73 …Structure Discovery in Biology: Motifs; Networks & Phylogenies (Dagstuhl Seminar 12291) AlbertoApostolico; Andreas Dress; and Laxmi Parida ............................ 92 … Published online and openaccess by Schloss Dagstuhl – Leibniz-Zentrum für Informatik GmbH; Dagstuhl …,*,2013,*
Foundations and Trends® in Databases,Goetz Graefe,*,Foundations and Trends® in Databases,2011,*
Database software for non-volatile byte-addressable memory,Goetz Graefe; Harumi Kuno,Abstract: Most transactional software; eg; in database systems; is written for a 2-levelmemory hierarchy with volatile RAM and persistent disk storage. For example; the standard“writeahead logging” technique relies on an in-memory buffer pool to hold back dirty datapages until the relevant log records have been written to stable storage. It is well-known thata buffer pool enables fast access via locality of reference. In addition; in transactionalsystems; a buffer pool seems an essential component of write-ahead logging; which ensuresthe consistency of persistent data even in the face of recovery from media failure. For atransactional storage system using non-volatile; byteaddressable memory; which does notsuffer from the slow access times of disk; a buffer pool seems unnecessary. However;removing the buffer pool seems to complicate transactional updates. We introduce a …,*,2011,*
B-Tree Locking,Goetz Graefe,The B+-tree is a disk-based; paginated; dynamically updateable; balanced; and tree-likeindex structure. It supports the exact match query as well as insertion/deletion operations inO (logpn) I/Os; where n is the number of records in the tree and p is the page capacity innumber of records. It also supports the range searches in O (logpn+ t∕ p) I/Os; where t is thenumber of records in the query result.,*,2009,*
Parallel Hash Join; Parallel Merge Join; Parallel Nested Loops Join,Goetz Graefe,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,*
Buffer Pool,Goetz Graefe,The B+-tree is a disk-based; paginated; dynamically updateable; balanced; and tree-likeindex structure. It supports the exact match query as well as insertion/deletion operations inO (logpn) I/Os; where n is the number of records in the tree and p is the page capacity innumber of records. It also supports the range searches in O (logpn+ t∕ p) I/Os; where t is thenumber of records in the query result.,*,2009,*
B+-Tree,Donghui Zhang; Kenneth Paul Baclawski; Vassilis J Tsotras,The B+-tree is a disk-based; paginated; dynamically updateable; balanced; and tree-likeindex structure. It supports the exact match query as well as insertion/deletion operations inO (logpn) I/Os; where n is the number of records in the tree and p is the page capacity innumber of records. It also supports the range searches in O (logpn+ t∕ p) I/Os; where t is thenumber of records in the query result.,*,2009,*
Storage Manager,Goetz Graefe,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,*
Buffer Manager,Goetz Graefe,The B+-tree is a disk-based; paginated; dynamically updateable; balanced; and tree-likeindex structure. It supports the exact match query as well as insertion/deletion operations inO (logpn) I/Os; where n is the number of records in the tree and p is the page capacity innumber of records. It also supports the range searches in O (logpn+ t∕ p) I/Os; where t is thenumber of records in the query result.,*,2009,*
ARCHIDAMUS’AND PERICLES’FOREIGN POLICIES: AN APPLICATION OF INTERNATIONAL RELATIONS THEORY,LOGAN HOGGATT,Abstract In 431 BCE; with the outbreak of the Peloponnesian War; the Greek world enteredinto the most violent; divisive; and overall devastating conflict of its history.? Myriad scholarshave attempted to explain the causes of this catastrophic war; the reasons for the course itultimately took; and the forces at work that impeded the peace process; the result of which isa robust body of literature that has substantially increased our knowledge of the war.?However; a new perspective; focusing primarily on key individuals and making use ofinternational relations theory; will give rise to novel explanations to the above issue.?Specifically; the paper explores the foreign policies of two early Peloponnesian Warleaders? the Spartan King Archidamus and the Athenian general Pericles. The applicationof international relations theory to the lives of these leaders and the exclusive focus of the …,*,2008,*
A general and efficient algorithm for “top” queries,Goetz Graefe,“Top K” queries reduce a query result to the most interesting or the most urgent items. Inmany cases; eg; when the result size is unbounded due to duplicate key values; a “top”operation cannot be implemented using the standard algorithm based on an in-memorypriority queue. The default alternative is a full sort. External merge sort admits multiple noveloptimizations specific to “top” operations. These are simple to implement yet greatly reducethe data volume written to runs on temporary storage. Experiments demonstrate substantialperformance improvements; in one case exceeding three orders of magnitude.,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,*
Data Always and Everywhere―Management of Mobile; Ubiquitous; Pervasive; and Sensor Data,Gustavo Alonso; Christian S Jensen; Bernhard Mitschang; Goetz Graefe; Alexander Sinitsyn; Winfried AH Berkvens; Arjan Claassen; Joep P van Gassel,Abstract This report summarizes the important aspects of the workshop on “Management ofMobile; Ubiquitous; Pervasive; and Sensor Data;” which took place from October 16th toOctober21st; 2005. Thirty-seven participants from thirteen countries met during that weekand discussed a broad range of topics related to the management of data in relation tomobile; ubiquitous; and pervasive applications of information technology. The wealth of thecontributions is available at the seminar page at the Dagstuhl server. Here; we provide ashort overview.,Dagstuhl Seminar,2006,*
05421 Abstracts Collection--Data Always and Everywhere--Management of Mobile; Ubiquitous; Pervasive; and Sensor Data,Gustavo Alonso; Christian S Jensen; Bernhard Mitschang,Abstract From 16.10. 05 to 21.10. 05; the Dagstuhl Seminar 05421; Data Always andEverywhere-Management of Mobile; Ubiquitous; Pervasive; and Sensor Data; was held inthe International Conference and Research Center; Schloss Dagstuhl. During the seminar;all participants were given the opportunity to present their current research; and ongoingactivities and open problems were discussed. This document is a collection of the abstractsof the presentations given during the seminar. Some abstracts offer links to extendedabstracts; full papers; and other supporting documents. A separate companion documentsummarizes the seminar. The authors wish to acknowledge Victor Teixeira de Almeida; whoserved as collector for the seminar and thus played a key role in collecting materials from theseminar participants.,Dagstuhl Seminar Proceedings,2006,*
AN EVALUATION OF VETERAN SERVICES AT THE UNIVERSITY OF WISCONSIN-STOUT,Larry Gentaro Graves,The University of Wisconsin-Stout Veterans Services has had a long history of being anentity within the Registration and Records Office; an office under the division of Academicand Student Affairs. The UW-Stout Veterans Services Coordinator is also the AssistantRegistrar and is supervised by the Registrar. The Coordinator is the institution'srepresentative responsible for completing all paperwork necessary to certify the enrollmentand changes in enrollment for students eligible for State and Federal Department ofVeterans Affairs (VA) education benefits (Krecek; 2000). As a representative of theinstitution; the Coordinator not only provides information to the Veteran; but also helpsfacilitate the transition from military to civilian life (Douglass; 2002). The certifying officialmust be familiar with the university calendar; catalogs; majors; and a thorough knowledge …,*,2004,*
The editors-in-chief and the editorial board would like to acknowledge the following people for their expert and continuing assistance in evaluating manuscript submi...,Serge Abiteboul; Charu Agrawal; Sihem Ahmer-Yahia; Gustavo Alonso; Toshiyuki Amagasa; Hiroki Arimura; Masayoshi Aritsugi; Chris Atkinson; Paolo Atzeni; Elena Baralis; Sonia Bergamaschi; Elisa Bertino; Azer Bestavros; Philippe Bonnet; Jerzy Brezinski; Sjaak Brinkkemper; Kurt Brown; Nicolas Bruno; Janis Bubenko; Alejandro Buchmann; Christoph Bussler; Fabio Casati; Sharma Chakravarthy; Don Chamberlin; Yatin Chawathe; Shu-Ching Chen; Jan Chomicki; Vassilis Christophides; Panos Chountas; Lawrence Chung; Peter Dadam; David de Frutos Escrig; Joaquin Delgado; Alex Delis; Eric Dubois; J urgen Ebert; Johann Eder; David Embley; Gregor Engels; Georgios Evangelidis; Ron Fagin; Christian Fahrner; Christos Faloutsos; Patrick Fan; Leonidas Fegaras; Donal Flynn; Johann Christoph Freytag; Hans Fritschi; Norbert Fuhr; Richard Furuta; Dimitrios Georgakopoulos; Andreas Geppert; Michael Gertz; Martin Glinz; Matteo Golfarelli; Goetz Graefe; Peter Green; Sol Greenspan; Paul Grefen; Volker Gruhn; Giovanna Guerrini; G Ralf-Hartmut,*,Information Systems,2004,*
Industrial and Applications Sessions,Andreas Behm; Serge Rielau; Richard Swagerman; Conor Cunningham; Cesar Galindo-Legaria; Goetz Graefe; Walid Rjaibi; Paul Bird; Nagender Bandi; Chengyu Sun; Divyakant Agrawal; Amr El Abbadi; Sang K Cha; Changbin Song; Meikel Poess; John M Stephens Jr; Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan; Sougata Mukherjea; Bhuvan Bamba; Nick Koudas; Amit Marathe; Divesh Srivastava; Benoit Dageville; Dinesh Das; Karl Dias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin; Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; Arun Marathe; Vivek Narasayya; Manoj Syamala; Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W Warner; Vikas Arora; Susan Kotsovolos; Shankar Pal; Istvan Cseri; Oliver Seeliger; Gideon Schaller; Leo Giakoumakis; Vasili Zolotov; Ashraf Aboulnaga; Peter Haas; Mokhtar Kandil; Sam Lightstone; Guy Lohman; Volker Markl; Ivan Popivanov; Vijayshankar Raman; Marcus Fontoura; Eugene Shekita; Jason Zien; Sridhar Rajagopalan; Andreas Neumann; Bishwaranjan Bhattacharjee; Christof Bornhövd; Tao Lin; Stephan Haller; Joachim Schaper; Managing RDFI Data; Sudarshan S Chawathe; Venkat Krishnamurthyy; Sridhar Ramachandran; David Campbell; Toby Bloom; Ted Sharpe; Rakesh Nagarajan; Mushtaq Ahmed; Aditya Phatak; Raymie Stata; Patrick Hunt; William O'Connell; Ramesh Bhashyam; Roger MacNicol; Blaine French,PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS ConorCunningham; Cesar Galindo-Legaria; Goetz Graefe (Microsoft Corp.) … P*TIME: Highly ScalableOLTP DBMS for Managing Update-Intensive Stream Workload Sang K. Cha (Transact InMemory; Inc); Changbin Song (Seoul National Univ.) … Supporting Ontology-based SemanticMatching in RDBMS Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan(Oracle Corp.) … Automatic SQL Tuning in Oracle 10g Benoit Dageville; Dinesh Das; KarlDias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin (Oracle Corp.) … Database TuningAdvisor for Microsoft SQL Server 2005 Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; ArunMarathe; Vivek Narasayya; Manoj Syamala (Microsoft Corp.) … Query Rewrite for XML in OracleXML DB Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W. Warner; Vikas …,Proceedings of the Thirtieth International Conference on Very Large Data Bases: Toronto; Canada; August 31-September 3; 2004,2004,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Tom DeFanti; Cees de Laat; Joe Mambretti; Kees Neggers; Bill St Arnaud,Data-intensive e-science is far removed from transaction-based e-business and streaming-based e-entertainment; yet today's best-effort routed networks must serve all users. Butthese networks are swamped by huge flows of data mixed with normal-bandwidth; short-lived traffic. Each type of traffic has a devastating effect on the other; Web pages take toolong to open; and data is lost; requiring retransmission; especially when the networks do notoffer scheduled services with guarantees of bandwidth or latency.At National ScienceFoundation-sponsored workshops; e-scientists routinely request schedulable high-bandwidth; low-latency connectivity" with known and knowable characteristics"[2; 8]. Many ofthem; along with collaborating computer science researchers; have explored internationalmulti-gigabit connectivity at special high-performance events like iGrid 2002 [3] in …,Communications of the ACM,2003,*
Collaborative Analytical Processing-Dream or Reality?(Panel abstract).,William O'Connell; Andrew Witkowski; Goetz Graefe,Majority of current Business Intelligence Data (BID) is spread out between incompatibletools like RDBMss; OLAP engines; and spreadsheets. Bridges between them allowinterchange of data and metadata thus providing a small degree of BID sharing; but due tolack of metadata standards; this sharing is limited to specific installations. This creates anunacceptable situation where an analyst cannot get a complete picture of a business. Asolution is needed; the Collaborative Analytical Processing (CAP) solution; which imposes avery tight integration between the tools so (meta) data interchange; change management;scalability and data availability is as good as in RDBMss; and performance of OLAP queriesis as good as in the specialized OLAP engines.,VLDB,2001,*
Excerpts from Aab-e Hayat,Muhammad Husain Azad,*,Annual of Urdu Studies,1998,*
Encapsulation of parallelism,Goetz Graefe,*,Readings in database systems (2nd ed.),1994,*
Optimization of Dynamic Query Evaluation Plans Richard L. Cole; University of Colorado at Boulder,Goetz Graefe,*,Proceedings,1994,*
Bulletin of the Technical Committee on,Gerhard Weikum; Arnd Christian; Achim Kraiss; Markus Sinnwell; Surajit Chaudhuri; Eric Christensen; Goetz Graefe; Vivek Narasayya; Michael Zwilling,Membership in the TC on Data Engineering (http: www. is open to all current members of theIEEE Computer Society who are interested in database systems. The web page for the DataEngineering Bulletin is http://www. research. microsoft. com/research/db/debull. The webpage for the TC on Data Engineering is http://www. ccs. neu. edu/groups/IEEE/tcde/index.html.,Urbana,1993,*
Memory-Contention Responsive Hash Joins; CU-CS-682-93,Diane L Davison; Goetz Graefe,Abstract Fluctuations in memory contention during query execution may compromise theeffectiveness of previous allocation decisions and result in excessive I/O costs. In order tomaximize system performance; memory-intensive algorithms such as hash join mustgracefully adapt to variations in available memory. Responsiveness to memory contention isparticularly important in systems processing mixed workloads due to the erratic frequencyand magnitude of fluctuations. Earlier studies on adaptable hash joins have advocatedlowering I/O costs by reducing the volume; or number of pages; of I/O performed. In thispaper; we present a group of memory-contention responsive hash joins that lower I/O costsby using a large unit of I/O; or cluster; to reduce the amount of time spent on I/O and thatdynamically vary the cluster size in response to fluctuations in memory availability. These …,*,1993,*
Dynamic Techniques for Very Complex Database Queries.,Goetz Graefe; Richard L Cole; Diane L Davison,*,FMLDO,1993,*
Optimization of Dynamic Query Evaluation Plans; CU-CS-671-93,Richard L Cole; Goetz Graefe,Abstract................................................................... 1. Introduction.................................................... 2.Motivating Example and Background............... a ia adiak is a at is dº º de a is iia dº si eaat) kahr is y sia i sa t ir is a is i de d» sh iea de his 9 si egasasgsyasty dr isadddd á sa aah ár rhi«a) y A) ģ g. 8 a 9% iba 9 yais 9 as is asaeasas ta is i 4 is ta li ihtttdiaia la 4 a da) aa ia iaaisa is a ty) is ah é iaibdaa ta a sa hnd is a ia a ia ia aaasha ia a is is a,*,1993,*
Join; CU-CS-606-92,Goetz Graefe,Abstract In this paper; we focus on set matching algorithms such as intersection; difference;union; and relational join; using join as a representative for all these matching problems. Wediscuss ﬁve performance enhancements for hybrid hash join algorithms; namely datacompression; large cluster sizes and multi—level recursion; role reversal of build and probeinputs; histogram methods to exploit non-uniform data and hash value distributions (skew);and join algorithms for multiple inputs. While each of the enhancements is fairly simple; themost surprising result is that hash value skew can be exploited and improve performancerather than being a danger to hybrid hash join performance as conventionally thought. Ourdesign for hash-based N-way matching algorithms is a dual to pipelining data withoutintermediate sorting between multiple merge-joins on the same attribute (interesting …,*,1992,*
Algebraic Optimization of Computations over Scientific Databases Richard Wolniewicz; University of Colorado at Boulder,Goetz Graefe,*,Very Large Data Bases: Proceedings,1992,*
Five Performance Enhancements for Hybrid Hash Join; CU-CS-606-92,Goetz Graefe,Abstract In this paper; we focus on set matching algorithms such as intersection; difference;union; and relational join; using join as a representative for all these matching problems. Wediscuss five performance enhancements for hybrid hash join algorithms; namely datacompression; large cluster sizes and multi-level recursion; role reversal of build and probeinputs; histogram methods to exploit non-uniform data and hash value distributions (skew);and join algorithms for multiple inputs. While each of the enhancements is fairly simple; themost surprising result is that hash value skew can be cxploited and improve performancerather than being a danger to hybrid hash join performance as conventionally thought. Ourdesign for hash-based N-way matching algorithms is a dual to pipelining data withoutintermediate sorting between multiple merge-joins on the same attribute (interesting …,*,1992,*
Query Processing Techniques for Large Databases; CU-CS-579-92,Goetz Graefe,Abstract Database management systems will continue to manage large data volumes. Thus;efficient algorithms for accessing and manipulating large sets and sequences will berequired to provide competitive performance. The advent Of object-oriented and extensibledatabase systems will not solve this problem; on the contrary; modern data modelsexacerbate the problem. In order to manipulate large sets of complex objects as efficiently astoday's database systems manipulate simple records; query processing algorithms andsoftware will become more complex; and a Solid understanding of algorithm andarchitectural issues is essential for the designer of database management Software. Thissurvey provides a foundation for the design and implementation of query execution facilitiesin new database management systems. It describes a wide array of practical query …,*,1992,*
T. Fukuda,G Furtan; T Ae; F Agüera; R Ahad; M Ahuja; R Alvaro; H Applerath; B Badrínath; C Barn; С Bani; C Batini; D Batory; G Beiford; A Berre; E Berlino; I Besemel; К Bimson; J Bocca; A Borgida; E Boughter; S Browne; B Buchanan; В Buckles; W Burkhard; A Cabas; J Carlis; S Casner; N Cercone; S Chakravarthy; N Chang; PM Chen; MS Chen; M Chen; S Chen; A Choudhary; С Corti; C Costilla; P De; S Debray; D Dewitl; К Dittrich; D Dolk; P Drew; D Du; J Dugan; A Dutta; E Eder; M Eigh; A El-Abbadi; R Elmasri; D Eshner; R Fagin; P Faudemay; E Fernandez; J Friedman; К Frieson; S Gadia; S Gala; H Ganzinger; J Garza; J Geller; S Ghandeharizadeh; S Ghosh; R Gimeno; M Ginsberg; V Gligor; JA Gonzalez; G Graefe; J Grant; M Guo; G Hall; I Han; E Hanson; Helal J Hendler; S Henkind; A Hevner; S Hikita; B Hillyer; A Hoc; L Hollaar; R Hotaka; M Hsu; S Hudson,Information for Authors ïr'J j^ E T. RANSACTI0NS ON Knowledge and Data Engineering isan archival journal published quarterly. The information published in this TRANSACTIONSis designed to inform researchers; developers; managers; strategic planners; users; andothers interested in state-of-the-art and state-of-the-practice activities in the knowledge anddata engineering area. We are interested in well-defined theoretical results and empiricalstudies that have potential impact on the acquisition; management; storage; and gracefuldegeneration of knowledge and data; as well as in provision of knowledge and dataservices. We welcome treatments of the role of knowledge and data in the development anduse of information systems and in the simplification of software and hardware developmentand maintenance. Since the journal is archival; it is assumed that the ideas presented are …,IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,1991,*
The Volcano Optimizer Generator; CU-CS-563-91,Goetz Graefe; William McKenna,Abstract Novel database applications demand not only high functionality but also highperformance. To combine these two requirements; the Volcano project provides efficient;extensible tools for query and request processing in novel application domains; particularlyin object-oriented and scientific database systems. One of these tools is a new Optimizergenerator. Data model; logical algebra; physical algebra; and rules are translated by theoptimizer gen-erator into optimizer source code. Compared with our earlier EXODUSoptimizer generator prototype; the search engine is more extensible and powerful as itprovides direct and effective support for non-trivial cost models and for physical propertieslike sort order and partitioning; but at the same time it is much more efficient. Compared withother rule-based optimization systems; it provides more extensibility and data model …,*,1991,*
Extensible Query Optimization and Parallel Execution in Volcano; CU-CS-548-91,Goetz Graefe,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; "Extensible QueryOptimization and Parallel Execution in Volcano ; CU-CS-548-91" (1991). Computer ScienceTechnical Reports. 528. http://scholar.colorado.edu/csci_techreports/528,*,1991,*
Sort versus Hash Revisited; CU-CS-534-91,Goetz Graefe; Ann Linville; Leonard D Shapiro,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; Linville; Ann; andShapiro; Leonard D.; "Sort versus Hash Revisited ; CU-CS-534-91" (1991). Computer ScienceTechnical Reports. 514. http://scholar.colorado.edu/csci_techreports/514,*,1991,*
Full-Time Data Compression: An ADT for Database Performance; CU-CS-503-90,Goetz Graefe; Leonard D Shapiro,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz and Shapiro; LeonardD.; "Full-Time Data Compression: An ADT for Database Performance ; CU-CS-503-90"(1990). Computer Science Technical Reports. 484. http://scholar.colorado.edu/csci_techreports/484,*,1990,*
Heap-Filter Merge Join: A New Algorithm for Joining Medium-Size Inputs; CU-CS-471-90,Goetz Graefe,Abstract We present a new algorithm for relational equi-join. The algorithm is a modificationof merge join but promises superior performance for medium-size inputs. In many cases; iteven compares favorably with hybrid hash join. We present I/O cost comparisons for asequential implementation but also discuss parallel versions of the new algorithm.,*,1990,*
Volcano; an Extensible and Parallel Query Evaluation System; CU-CS-481-90,Goetz Graefe,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; "Volcano; an Extensibleand Parallel Query Evaluation System ; CU-CS-481-90" (1990). Computer Science TechnicalReports. 463. http://scholar.colorado.edu/csci_techreports/463,*,1990,*
Efficient Assembly of Complex Objects; CU-CS-502-90,Tom Keller; Goetz Graefe; David Maier,Abstract Although object-oriented database systems offer advantages over relational orrecord-oriented database systems; such as modeling facilities for complex objects; they arecriticized for poor performance and query capabilities on set-oriented applications. Theunacceptable performance is due in part to the object-at-a-time processing typically used byobject-oriented database systems. We believe that improved performance of object-orienteddatabase systems depends partially on the efficient and selective retrieval of sets of complexobjects from secondary storage. In this report; we present the method of complex objectretrieval and assembly used in the Volcano query processing system and the Revelationproject. We also present experimental results comparing set-oriented versus object-at-a-timecomplex object assembly.,*,1990,*
Parallel External Sorting in Volcano; CU-CS-459-90,Goetz Graefe,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; "Parallel ExternalSorting in Volcano ; CU-CS-459-90" (1990). Computer Science Technical Reports. 441.http://scholar.colorado.edu/csci_techreports/441,*,1990,*
Data Compression and Database Performance; CU-CS-496-90,Goetz Graefe; Leonard D Shapiro,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz and Shapiro; LeonardD.; "Data Compression and Database Performance ; CU-CS-496-90" (1990). Computer ScienceTechnical Reports. 477. http://scholar.colorado.edu/csci_techreports/477,*,1990,*
Encapsulation of Parallelism in the Volcano Query Processing System; CU-CS-458-90,Goetz Graefe,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz; "Encapsulation ofParallelism in the Volcano Query Processing System ; CU-CS-458-90" (1990). Computer ScienceTechnical Reports. 440. http://scholar.colorado.edu/csci_techreports/440,*,1990,*
Full-time Data Compression: An ADT for Database Performance,Goetz Graefe; Leonard Shapiro,*,*,1990,*
Tuning a Parallel Database Algorithm on a Shared-Memory Multiprocessor; CU-CS-470-90,Goetz Graefe; Shreekant S Thakkar,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz and Thakkar; ShreekantS.; "Tuning a Parallel Database Algorithm on a Shared-Memory Multiprocessor ; CU-CS-470-90" (1990). Computer Science Technical Reports. 452. http://scholar.colorado.edu/csci_techreports/452,*,1990,*
Logical vs. physical disk shadowing,Goetz Graefe; Leonard D Shapiro,Abstract Since the cost of redundant storage is less important for many applications than thecost of failures and down-times; some database systems offer facilities for maintainingredundant disks. Bitten and Gray analyzed shadowing physical units such as disk pagesover multiple devices. Instead; we propose to replicate logical units such as relations andindices. Logical shadowing allows using different physical organizations; eg; multipledifferent sort orders and clustering indices. Physical organization and clustering areimportant determinants in retrieval costs; having multiple clustering schemes increases thenumber of attributes for which the system achieves optimal performance for range queriesand merge joins.,*,1990,*
Architecture-Independent Parallel Query Evaluation in Volcano; CU-CS-500-90,Goetz Graefe; Diane L Davison,This Technical Report is brought to you for free and open access by Computer Science at CUScholar. It has been accepted for inclusion in Computer Science Technical Reports by an authorizedadministrator of CU Scholar. For more information; please contactcuscholaradmin@colorado.edu … Recommended Citation Graefe; Goetz and Davison; DianeL.; "Architecture-Independent Parallel Query Evaluation in Volcano ; CU-CS-500-90" (1990).Computer Science Technical Reports. 481. http://scholar.colorado.edu/csci_techreports/481,*,1990,*
Parallel processing in database management systems,Sadish Doshi; Gary Kelly; Frank Symonds; Goetz Graefe,Summary form only given. Progress has been made in online transaction processing. It hasbeen proved that comparable performance improvements can be made in decision-supportenvironments. Research done in this area has been examined. Specifically; the authorshave explored various tradeoffs between loosely and tightly coupled multiprocessingarchitectures.,COMPCON Spring'89. Thirty-Fourth IEEE Computer Society International Conference: Intellectual Leverage; Digest of Papers.,1989,*
Database query optimization; proceedings of the ODBF workshop,Goetz Graefe,This Article is brought to you for free and open access by OHSU Digital Commons. It has beenaccepted for inclusion in CSETech by an authorized administrator of OHSU DigitalCommons. For more information; please contact champieu@ohsu.edu … Recommended CitationGraefe; Goetz; "Database query optimization; proceedings of the ODBF workshop" (1989).CSETech. 242. http://digitalcommons.ohsu.edu/csetech/242 … Database QueryOptimization; . Proceedings of the ODBF Workshop … I appreciate this opportunity to bring researchersand practitioners together for this workshop on database query optimization. This topic has beenof great importance for rela- tional database systems; and it will be central for future computersystems. In a narrow scope; the performance of database systems can be improved throughwork in two areas; query planning and query processing. The most significant challenges …,*,1989,*
Heap-Filter Merge Join: A new algorithm for joining medium-size relations,Goetz Graefe,This Article is brought to you for free and open access by OHSU Digital Commons. It has beenaccepted for inclusion in CSETech by an authorized administrator of OHSU DigitalCommons. For more information; please contact champieu@ohsu.edu … Recommended CitationGraefe; Goetz; "Heap-Filter Merge Join: A new algorithm for joining medium-size relations"(1989). CSETech. 197. http://digitalcommons.ohsu.edu/csetech/197 … Heap-Filter MergeJoin: A New Algorithm for Joining Medium-Size Relations … Oregon Graduate Center Departmentof Computer Science and Engineering 19600 NW von Neumann Drive Beaverton; OR97006-1999 USA … A New Algorithm for Joining Medium-Size Relations … Oregon GraduateCenter Beaverton; Oregon 910061999 graefe@cse.ogc .edu … We present a new algorithmfor relational equi-join. The algorithm is a modification of merge join but promises …,*,1989,*
DataCube: an integrated data and compute server based on a hypercube-connected dataflow database machine,Goetz Graefe,Abstract Parallel hardware has been used for both compute-intensive applications and datamanagement; typically as a server for a number of workstations. If both services wereneeded; only one of them was parallelized; or two servers were connected via a network.Instead; we propose to use one highly parallel hardware base and to build system softwarethat provides three abstractions; a compute server; a database machine; and an integrateddata and compute server. The main goal of the integrated server architecture is to exploitparallel communication between servers; and to study the resource management problemsof providing both services by all available processors. The system can also be used tointroduce transactions and checkpoints to compute-intensive applications. Sincehypercubes are suitable for both kinds of servers; we will by using hypercube-connected …,*,1988,*
University Of VVisconsin–,Goetz Graefe,1.1. Extensible Database Systems In recent years; a number of new data models have beenproposed in the database literature that extend the modelling and query processing powerof the relational model (Codd; ısro) For business style applications; the relational data modelhas been used very successfully. Services provided by relational database systems havesignificantly enhanced programmer and data processing productivity. Such services includehigh level query and data manipulation languages; transparent maintenance of secondarysearch structures; data independence; data sharing; control of access privileges;concurrency control; and recovery from program; media; and system failures. While therelation-tuple paradigm with fixedformat records provides sufficient modelling flexibility formost record-keeping applications; it is not well-suited for other database application …,*,1987,*
Fusarium Toxins: Chemistry of Toxic Trichothecenes [I.] II. Organolithium Chemistry,Frank N Kotsonis,*,*,1975,*
Efficient Assembly of Complex Objects Tom Keller,Goetz Graefe; David Maier,Abstract Although object-oriented database systems offer advantages over relational orrecord-oriented database systems; such as modeling facilities for complex objects; they arecriticized for poor performance and query capabilities on set-oriented applications. Theunacceptable performance is due in part to the object-at-a-time processing typically used byobject-oriented database systems. We believe that improved performance of object-orienteddatabase systems depends partially on the efficient and selective retrieval of sets of complexobjects from secondary storage. In this report; we present the method of complex objectretrieval and assembly used in the Volcano query processing system and the Revelationproject. We also present experimental results comparing set-oriented versus object-at-a-timecomplex object assembly.,*,*,*
Efﬁcient Assembly of Complex Objects; CU-CS-502-90,Tom Keller; Goetz Graefe; David Maier,Abstract Although object-oriented database systems offer advantages over relational orrecord-oriented database systems; such as modeling facilities for complex objects; they arecriticized for poor performance and query capabilities on set-oriented applications. Theunacceptable performance is due in part to the object-at-a-time processing typically used byobject-oriented database systems. We believe that improved performance of object-orienteddatabase systems depends partially on the efficient and selective retrieval of sets of complexobjects from secondary storage. In this report; we present the method of complex objectretrieval and assembly used in the Volcano query processing system and the Revelationproject. We also present experimental results comparing set-oriented versus object-at-a-timecomplex object assembly.,*,*,*
Efficient Assembly of Complex Objects Tom Keller Goetz Graefe,David Maier,Abstract Although object-oriented database systems offer advantages over relational orrecord-oriented database systems; such as modeling facilities for complex objects; they arecriticized for poor performance and query capabilities on set-oriented applications. Theunacceptable performance is due in part to the object-at-a-time processing typically used byobject-oriented database systems. We believe that improved performance of object-orienteddatabase systems depends partially on the efficient and selective retrieval of sets of complexobjects from secondary storage. In this report; we present the method of complex objectretrieval and assembly used in the Volcano query processing system and the Revelationproject. We also present experimental results comparing set-oriented versus object-at-a-timecomplex object assembly.,*,*,*
Data Engineering,G Graefe; S Harizopoulos; H Kuno; MA Shah; D Tsirogiannis; JL Wiener; B Bhattacharjee; M Canim; CA Lang; GA Mihaila; KA Ross; M Bjørling; P Bonnet; L Bouganim; B Jònsson,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Data Engineering,Oliver Günther; Wolf-Fritz Riekert; Hans-Peter Kriegel; Thomas Brinkhoff; Ralf Schneider,Letters; conference information; and news should be sent to the Editor-in-Chief. Papers foreach issue are solicited by and should be sent to the Associate Editor responsible for theissue. Opinions expressed in contributions are those of the authors and do not necessarilyreflect the positions of the TC on Data Engineering; the IEEE Computer Society; or theauthors' organizations.,Urbana,*,*
S kkkk of Colorado at Boulder,Goetz Graefe,Abstract Volcano is a new dataflow query processing system we have developed fordatabase systems research and education. The uniform interface between operators makesVolcano extensible by new operators. All operators are designed and coded as if they weremeant for a single-process system only. When attempting to parallclize Volcano; we had tochoose between two models of parallelization; called here the bracket and operator models.We describe the reasons for not choosing thc bracket model; introduce the novel operatormodel; and provide details of Volcano's exchange operator that parallelizes all other opera-tors. It allows intra-operator parallelism on partitioncd datasets and both vertical andhorizontal inter-operator parallelism. The exchange operator encapsulates all parallelismissues and therefore makes implementation of parallel databasc algorithms signifi-cantly …,*,*,*
TllS of Colorado at Boulder,Richard L Cole; Goetz Graefe,Abstract Traditional query optimizers assume accurate knowledge of run-time parameterssuch as selectivities and resource availability during plan optimization; ie; at compile-time. Inreality; however; this assumption is often not justified. Therefore; the “static” plans producedby traditional optimizers may not be optimal for many of their actual run-time invocations.Instead; we propose a novel optimization model that assigns the bulk of the optimizationeffort to compile-time and delays carefully selected optimization decisions until run-time. Ourprevious work defined the run-time primitives;“dynamic plans” using “choose-plan”operators; for executing such delayed decisions; but did not solve the problem ofconstructing dynamic plans at compile-time. The present paper introduces techniques thatsolve this problem. Experience with a working prototype optimizer demonstrates (i) that …,*,*,*
Variants of instant recovery,Goetz Graefe,Abstract Traditional theory and practice of write-ahead logging and of database recoveryfocus on three failure classes. These are transaction failures (typically due to deadlocks)resolved by rollback; system failures (typically power or software faults) resolved by restartwith log analysis;“redo;” and “undo” phases; and media failures (typically hardware faults)resolved by restore operations that combine multiple types of backups and log replay.,*,*,*
学步园,Conor Cunningham; César A Galindo-Legaria; Goetz Graefe,PIVOT 和UNPIVOT 两个运算符可将表中的数据进行行列置换; 本文介绍这两个关系运算的语法;语义; 实现原理; 及与其他关系运算联合使用时; SQL Server 2005 所做的优化.翻译得很烂;并没有严格按照原文翻译; 有些容易理解但却又不好翻译的地方; 直接Copy 原文没有翻译.建议还是直接看原文:,*,*,*
The story of instant recovery–Extended abstract–,Goetz Graefe,This presentation will summarize the history and the technology of instant recovery fromsystem and media failures. The story starts with modern hardware; eg; flash storage; and thedanger of localized failures due to limited write endurance. Initial research sought methodsfor detection and recovery of localized; ie; single-page failures.,GI-Edition,*,*
Q57..,Goetz Graefe,*,*,*,*
FAR EAST,William Armstrong; Christos Faloutsos; Vassos Hadzilacos; HV Jagadish; Ravi Krishnamurthy; David Lomet; Dennis McLeod; Shamkant Navathe; Ekow Otoo; Raghu Ramakrishnan; Betty Salzberg; Jacob Slonim; Irving Traiger; Carlo Zaniolo; Serge Abiteboul; Peter Apers; Horst Biller; Peter Dadam; Robert Demolombe; Frank Eliassen; Georg Gottlob; Peter Lockmann; Robert Meersman; Andreas Reuter; Hans-Joerg Schek; Nicolas Spyratos; Yannis Vasiliou; Bruce Croft; Hector Garcia-Molina; Paula Hawthorn; Randy Katz; Bruce Lindsay; Wo-Shun Luk; Albert Mendelzon; Jack Orenptein; Z Meral Ozsoyoglu; Arnon Rosenthal; Timos Sellis; Toby Teorey; Grant Weddell; Stanley Zdonik; Michele Adiba; Janis Bubenko; Klaus Dittrich; Norbert Fuhr; Theo Haerder; Heikki Mannila; Antoni Olive; Yehoshua Sagiv; Gunter Schlageter; Bernd Walter; Umeshwar Dayal; Goetz Graefe; Yannis Ioannidis; Roger King; Guy Lohman; Ian McLeod; Tim Merrett; Sylvia Osborn; M Tamer Ozsu; Nick Roussopoulos; Kenneth Sevcik; Frank Tompa; Clement Yu; Antonio Albano; Francois Bancilhon; Stefano Ceri; Hartmut Ehrig; Georges Gardarin; Witold Litwin; Rainer Manthey; Peter Pistor; Felix Saltor; Amilcar Sernadas; Henry Tirri; Roberto Zicari,William Armstrong (Canada) Christos Faloutsos (USA) Vassos Hadzilacos (Canada) HV Jagadish(USA) Ravi Krishnamurthy (USA) David Lomet (USA) Dennis McLeod (USA) Shamkant Navathe(USA) Ekow Otoo (Canada) Raghu Ramakrishnan (USA) Betty Salzberg (USA) Jacob Slonim(Canada) Irving Traiger (USA) Carlo Zaniolo (USA) … Serge Abiteboul (France) Peter Apers(Netherlands) Horst Biller (Germany) Peter Dadam (Germany) Robert Demolombe (France) FrankEliassen (Norway) Georg Gottlob (Austria) Peter Lockmann (Germany) Robert Meersman(Netherlands) Andreas Reuter (Germany) Hans-Joerg Schek (Switzerland) Nicolas Spyratos(France) Yannis Vasiliou (Greece) … Ching-Chen Chang (Taiwan; China) Yahiko Kambayashi(Japan) Sukho Lee (Korea) Leszek Maciaszek (Australia) Maria Orlowska (Australia) RonSacks-Davis (Australis) KP Tan (Singapore) KY Whang (Korea),*,*,*
4L"^!'University of Colorado at Boulder,Shared-Memory Multiprocessor; Goetz Graefe; Shreekant S Thakkar,Abstract Database query processing can benefit significantly from parallelism. Paralleldatabase algorithms combine substantial CPU and I/O activity; memory requirements; andmassive data exchange between processes; all of which must be considered to obtainoptimal performance. Since parallel external sorting is a very typical example; we havefocused on sorting to tune Volcano; a new query processing system we have built forresearch and education. Its primary design goals arc extensibility and performance. Itincludes all query processing algorithms conventionally used in relational database systemsas well as several new ones; and can execute all of them in parallel. In this report; wepresent Volcano's parallel external sorting algorithm and a sequence of enhancements toimprove its performance. We observed the best external sort performance reported to …,*,*,*
The work of refereeing was undertaken largely by the Program Committee. Invaluable help was provided; however; by the referees listed below.,C Arapis; B Badrinath; R Balter; D Batory; Y Bernard; J Bocca; S Boettcher; H Boral; R Brachman; H Brueggemann; A Buchmann; E Casais; U Chakravarthy; I Chomicki; H Chou; C Collet; R Cooper; L Dami; U Deppisch; K Dittrich; C Esculier; J Ferrie; S Finkelstein; J Freitag; F Garzotto; G Graefe; G Grahne; H Gunadhi; J Hagelstein; U Hahn; G Hulin; T Imielinski; H Jagadish; P Jeng; M Jeusfeld; H Kangassalo; E Kantorowitz; G Qp; R Kemp; W Kim; W Kohler; D Konstantas; H Korth; R Krishnamurthy; A Kumar; H Lame; T Lehman; V Linnemann; M Livny; G L&man; D Lubinsky; H Mannila; L Mark; S Mazumdar; J McPherson; M Missikoff; S Nagvi; F Olken; J Orenstein; M Papathomas; G Pelagatti; B Pemici; X Pintado; H Pirahesh; P Pistor; R Ramakrishnan; U Reiner; D Roelants; T Rose; D Rotem; B Rubenstein; K Sabnani; Y Sagiv; S Salza; M Scholl; F Schreiber; P Selinger; A Shoshani; E Soisalon-Soininen; S Stemple; M Terranova; B Thalheim; A Tomasic; S Tsur; I Vatton; M Vauclair; M Vernon; M Wallace; M Weigle; D Woelk; N Woo,Arapis; C. Badrinath; B. Balter; R. Batory; D. Bernard; Y. Bocca; J. Boettcher; S. Boral; H.Brachman; R. Brueggemann; H. By; F. Buchmann; A. Casais; E. Chakravarthy; U. Chang; E.Chomicki; I. Chou; H. Collet; C. Cooper; R. Dami; L. Deppisch; U. Dittrich; K. Esculier; C.Ferrie; J. Finkelstein; S. Freitag; J. Garzotto; F. Graefe; G. Grahne; G. Gunadhi; H. Hagelstein;J. Hahn; U. Hermenegildo; M … Hulin; G. Imielinski; T. Jagadish; H. Jeng; P. Jeusfeld; M.Kangassalo; H. Kantorowitz; E. Qp4 G. Kemp; R. Kim; W. Kohler; W. Konstantas; D. Korth; H.Krishnamurthy; R. Kumar; A. Lame; H. Le; c. Lehman; T. Linnemann; V. Livny; M. L&man; G.Lubinsky; D. * Mannila; H. Mark; L. Mazumdar; S. McPherson; J. Missikoff; M. Nagvi; S.Nierstrasz; 0. Olken; F. Orenstein; J. ozarow; L. Papathomas; M … Pelagatti; G. Pemici; B.Pintado; X. Pirahesh; H. Pistor; P. Ramakrishnan; R. Reiner; U. Roelants; D. Rose; T …,*,*,*
University 0f Colorado at Boulder,Goetz Graefe,*,*,*,*
Eliminating memory knobs,Goetz Graefe; Wendy Powley; Kai-Uwe Sattler; Kostas Tzoumas; Jingren Zhou,One of the tasks of workload management is to allocate resources to consumers withconflicting demands. Systems typically contain “knobs” that dictate resource allocation; forexample how much memory should be allocated to the buffer pool; how much memoryshould be allocated to sorting; etc. Eliminating knobs for tuning data management systems isan important task to reduce maintenance costs and make the systems more usable tostandard customers. Eliminating knobs makes the problem of workload managementsignificantly easier by removing free variables. We consider the problem of memory tuning insuch systems as a building block towards this overall goal: given an externally definedamount of memory (which may vary over time); how should this memory be internallydistributed among different heterogeneous consumers? Using a basic model taking utility …,*,*,*
Concurrent queries and updates in summary views and their indexes,Goetz Graefe,Abstract Materialized views have become a standard technique in decision supportdatabases and for a variety of monitoring purposes. In order to avoid inconsistencies andthus unpredictable query results; materialized views and their indexes should be maintainedimmediately within user transactions just like ordinary tables and their indexes.Unfortunately; the smaller and thus the more effective a materialized view is; the higher theconcurrency contention between queries and updates as well as among concurrentupdates. Therefore; we have investigated methods that reduce contention without forcingusers to sacrifice serializability and thus predictable application semantics. These methodsextend escrow locking with snapshot transactions; multi-version concurrency control; multi-granularity (hierarchical) locking; key range locking; and system transactions; ie; with …,*,*,*
CU-CS-458-90 March 1990,Goetz Graefe,Abstract Volcano is a new dataflow query processing system we have developed fordatabase systems research and education. The uniform interface between operators makesVolcano extensible by new operators. All operators are designed and coded as if they weremeant for a single-process system only. When attempting to parallelize Volcano; we had tochoose between two models of parallelization; called here the bracket and operator models.We describe the reasons for not choosing the bracket model; introduce the novel operatormodel; and provide details of Volcano's exchange operator that parallelizes all otheroperators. It allows intra-operator parallelism on partitioned datasets and both vertical andhorizontal inter-operator parallelism. The exchange operator encapsulates all parallelismissues and therefore makes implementation of parallel database algorithms signifi-cantly …,*,*,*
Programme Committee Members,Michele Adiba; Divyakant Agrawal; Peter Apers; Ricardo Baeza-Yates; Jose A Blakeley; Mokrane Bouzeghoub; Alex Buchmann; Peter Buneman; Michael Carey; Marco Casanova; Nick J Cercone; Sharma Chakravarthy; Jan Chomicki; Stavros Christodoulakis; Sophie Cluet; Armin B Cremers; Walter Cunto; Sergio Delgado; Klaus D&rich; Christos Faloutsos; Raymundo Forradellas; Antonio Furtado; Sumit Ganguly; Georges Gardarin; Narain Gehani; Goetz Graefe; Jim Gray; Ehud Gudes; Theo Haerder; Paula Hawthorn; Richard Hull; Sushi Jajodia; Christian Jensen; Manfred Jeusfeld; Yahiko Kambayayshi; Alfons Kemper; Ramamohanarao Kotagiri; Masaru Kitsuregawa; Michel Kuntz; Rosana Lanzelotte; Claudia Medeii; Alberto Mendelzon; Michele Missikoff; C Mohan; Ami Mono; Richard R Muntz; Erich Neuhold; Jack Orenstein; Maria Orlowska; Gultekin Ozsoyoglu; Alain Pirotte; Kenneth Ross; Ron Sacks-Davis; Felix Saltor; Betty Sal&erg; Joachim W Schmidt; Michael Schrefl; Amit P Sheth; Avi Silberschatz; Richard Snodgrass; Stefano Spaccapietra; VS Subrahmanian; TC Tay; Patrick Valduriez; Yannis Vassiliou; P Venkat Rangan; Victor Vianu; Gerhard Weikum; Jennifer Widom; Kam Fai Wong,Michele Adiba (France) Divyakant Agrawal (USA) Peter Apers (Netherlands) RicardoBaeza-Yates (Chile) David Bell (UK) Jose A. Blakeley (Mexico/USA) Mokrane Bouzeghoub(Prance) Alex Buchmann (Mexico/Germany) Peter Buneman (USA) Michael Carey (USA) MarcoCasanova (Brazil) Nick J. Cercone (Canada) Sharma Chakravarthy (USA) Jan Chomicki(USA) Stavros Christodoulakis (Greece) Sophie Cluet (France) Armin B. Cremers (Germany)Walter Cunto (Venezuela) S.Misbah Deen (UK) Sergio Delgado (Mexico) Klaus D&rich(Switzerland) Christos Faloutsos (USA) Raymundo Forradellas (Argentina) Antonio Furtado(Brazil) Sumit Ganguly (USA) Georges Gardarin (France) Narain Gehani (USA) Shahram Ghandeharizadeh(US A) Goetz Graefe (USA) Jim Gray (USA) Ehud Gudes (Israel) Theo Haerder (Germany) PaulaHawthorn (USA) Richard Hull (USA) Sushi1 Jajodia (USA) Christian Jensen (Denmark) …,*,*,*
The EXODUS Optimizer Generator.</title,Goetz Graefe; David J DeWitt,XML (eXtensible Markup Language) has recently emerged as a new standard for datarepresentation and exchange on the Internet. Unlike HTML files; in which tags specify howdata are formatted and displayed; XML files use tags on data elements to identify context ofdata; which provides data in more convenient and usable format. Realizing the benefits thatXML could bring; many people and organizations are now storing their data in XML formaton local servers. For example; Fig. 1 shows an XML file stored in DBLP1 server [Ley]. Similarexamples can be found in many public gene data repositories [GJ00]. Although most XMLresearchers are working on designing XML query languages; query engines and searchengines; more and more people have realize that XML storage management is one ofseveral critical problems in using XML [Wid99]. Currently; there are two typical …,SIGMOD Conference</booktitle,*,*
Computing Web Queries One Page at a Time,Johan Larson; Goetz Graefe; David J DeWitt,*,*,*,*
On the Efficient Gathering of Sufficient Statistics for Classification from Large SQL Databases,Surajit Chaudhuri; Usama Fayyad; Goetz Graefe,*,*,*,*
Recovering semantics of tables on the web,Thomas Neumann; Ruoming Jin; Lin Liu; Bolin Ding; Haixun Wang; Stratos Idreos; Stefan Manegold; Harumi Kuno; Goetz Graefe,@article{2002939; author = {Venetis; Petros and Halevy; Alon and Madhavan; Jayant andPa\c{s}ca; Marius and Shen; Warren and Wu; Fei and Miao; Gengxin and Wu; Chung}; title ={Recovering semantics of tables on the web}; journal = {Proc. VLDB Endow.}; volume = {4}; number ={9}; year = {2011}; issn = {2150-8097}; pages = {528--538}; publisher = {VLDB Endowment}; }@article{2002940; author = {Neumann; Thomas}; title = {Efficiently compiling efficient query plansfor modern hardware}; journal = {Proc. VLDB Endow.}; volume = {4}; number = {9}; year = {2011};issn = {2150-8097}; pages = {539--550}; publisher = {VLDB Endowment}; } @article{2002941;author = {Jin; Ruoming and Liu; Lin and Ding; Bolin and Wang; Haixun}; title = {Distance-constraintreachability computation in uncertain graphs}; journal = {Proc …,Proc. VLDB Endow.,*,*
