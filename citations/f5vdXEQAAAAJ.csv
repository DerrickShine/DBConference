Hadoop++: making a yellow elephant run like a cheetah (without it even noticing),Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Alekh Jindal; Yagiz Kargin; Vinay Setty; Jörg Schad,Abstract MapReduce is a computing paradigm that has gained a lot of attention in recentyears from industry and research. Unlike parallel DBMSs; MapReduce allows non-expertusers to run complex analytical tasks over very large data sets on very large clusters andclouds. However; this comes at a price: MapReduce processes tasks in a scan-orientedfashion. Hence; the performance of Hadoop---an open-source implementation ofMapReduce---often does not match the one of a well-configured parallel DBMS. In thispaper we propose a new type of system named Hadoop++: it boosts task performancewithout changing the Hadoop framework at all (Hadoop does not even'notice it'). To reachthis goal; rather than changing a working system (Hadoop); we inject our technology at theright places through UDFs only and affect Hadoop from inside. This has three important …,Proceedings of the VLDB Endowment,2010,464
Only aggressive elephants are fast elephants,Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Stefan Richter; Stefan Schuh; Alekh Jindal; Jörg Schad,Abstract Yellow elephants are slow. A major reason is that they consume their inputs entirelybefore responding to an elephant rider's orders. Some clever riders have trained their yellowelephants to only consume parts of the inputs before responding. However; the teachingtime to make an elephant do that is high. So high that the teaching lessons often do not payoff. We take a different approach. We make elephants aggressive; only this will make themvery fast. We propose HAIL (Hadoop Aggressive Indexing Library); an enhancement ofHDFS and Hadoop MapReduce that dramatically improves runtimes of several classes ofMapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create differentclustered indexes on each data block replica. An interesting feature of HAIL is that wetypically create a win-win situation: we improve both data upload to HDFS and the …,Proceedings of the VLDB Endowment,2012,122
Trojan data layouts: right shoes for a running elephant,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract MapReduce is becoming ubiquitous in large-scale data analysis. Several recentworks have shown that the performance of Hadoop MapReduce could be improved; forinstance; by creating indexes in a non-invasive manner. However; they ignore the impact ofthe data layout used inside data blocks of Hadoop Distributed File System (HDFS). In thispaper; we analyze different data layouts in detail in the context of MapReduce and arguethat Row; Column; and PAX layouts can lead to poor system performance. We propose anew data layout; coined Trojan Layout; that internally organizes data blocks into attributegroups according to the workload in order to improve data access times. A salient feature ofTrojan Layout is that it fully preserves the fault-tolerance properties of MapReduce. Weimplement our Trojan Layout idea in HDFS 0.20. 3 and call the resulting system Trojan …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,99
Performance and resource modeling in highly-concurrent OLTP workloads,Barzan Mozafari; Carlo Curino; Alekh Jindal; Samuel Madden,Abstract Database administrators of Online Transaction Processing (OLTP) systemsconstantly face difficult questions. For example;" What is the maximum throughput I cansustain with my current hardware?";" How much disk I/O will my system perform if therequests per second double?"; or" What will happen if the ratio of transactions in my systemchanges?". Resource prediction and performance analysis are both vital and difficult in thissetting. Here the challenge is due to high degrees of concurrency; competition for resources;and complex interactions between transactions; all of which non-linearly impactperformance. Although difficult; such analysis is a key component in enabling databaseadministrators to understand which queries are eating up the resources; and how theirsystem would scale under load. In this paper; we introduce our framework; called DBSeer …,Proceedings of the 2013 acm sigmod international conference on management of data,2013,55
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,52
Relax and Let the Database do the Partitioning Online,Alekh Jindal; Jens Dittrich,Abstract Vertical and Horizontal partitions allow database administrators (DBAs) toconsiderably improve the performance of business intelligence applications. However;finding and defining suitable horizontal and vertical partitions is a daunting task even forexperienced DBAs. This is because the DBA has to understand the physical query executionplans for each query in the workload very well to make appropriate design decisions. Tofacilitate this process several algorithms and advisory tools have been developed over thepast years. These tools; however; still keep the DBA in the loop. This means; the physicaldesign cannot be changed without human intervention. This is problematic in situationswhere a skilled DBA is either not available or the workload changes over time; eg due tonew DB applications; changed hardware; an increasing dataset size; or bursts in the …,*,2011,50
Relax and Let the Database do the Partitioning Online,Alekh Jindal; Jens Dittrich,Abstract Vertical and Horizontal partitions allow database administrators (DBAs) toconsiderably improve the performance of business intelligence applications. However;finding and defining suitable horizontal and vertical partitions is a daunting task even forexperienced DBAs. This is because the DBA has to understand the physical query executionplans for each query in the workload very well to make appropriate design decisions. Tofacilitate this process several algorithms and advisory tools have been developed over thepast years. These tools; however; still keep the DBA in the loop. This means; the physicaldesign cannot be changed without human intervention. This is problematic in situationswhere a skilled DBA is either not available or the workload changes over time; eg due tonew DB applications; changed hardware; an increasing dataset size; or bursts in the …,VLDB BIRTE,2011,50
Towards a One Size Fits All Database Architecture.,Jens Dittrich; Alekh Jindal,ABSTRACT We propose a new type of database system coined OctopusDB. Our approachsuggests a unified; one size fits all data processing architecture for OLTP; OLAP; streamingsystems; and scan-oriented database systems. OctopusDB radically departs from existingarchitectures in the following way: it uses a logical event log as its primary storage structure.To make this approach efficient we introduce the concept of Storage Views (SV); iesecondary; alternative physical data representations covering all or subsets of the primarylog. OctopusDB (1) allows us to use different types of SVs for different subsets of the data;and (2) eliminates the need to use different types of database systems for differentapplications. Thus; based on the workload; OctopusDB emulates different types of systems(row stores; column stores; streaming systems; and more importantly; any hybrid …,CIDR,2011,44
Vertexica: your relational friend for graph analytics!,Alekh Jindal; Praynaa Rawlani; Eugene Wu; Samuel Madden; Amol Deshpande; Mike Stonebraker,Abstract In this paper; we present Vertexica; a graph analytics tools on top of a relationaldatabase; which is user friendly and yet highly efficient. Instead of constraining programmersto SQL; Vertexica offers a popular vertex-centric query interface; which is more natural foranalysts to express many graph queries. The programmers simply provide their vertex-compute functions and Vertexica takes care of efficiently executing them in the standard SQLengine. The advantage of using Vertexica is its ability to leverage the relational features andenable much more sophisticated graph analysis. These include expressing graphalgorithms which are difficult in vertex-centric but straightforward in SQL and the ability tocompose end-to-end data processing pipelines; including pre-and post-processing ofgraphs as well as combining multiple algorithms for deeper insights. Vertexica has a …,Proceedings of the VLDB Endowment,2014,35
The uncracked pieces in database cracking,Felix Martin Schuhknecht; Alekh Jindal; Jens Dittrich,Abstract Database cracking has been an area of active research in recent years. The coreidea of database cracking is to create indexes adaptively and incrementally as a side-product of query processing. Several works have proposed different cracking techniques fordifferent aspects including updates; tuple-reconstruction; convergence; concurrency-control;and robustness. However; there is a lack of any comparative study of these differentmethods by an independent group. In this paper; we conduct an experimental study ondatabase cracking. Our goal is to critically review several aspects; identify the potential; andpropose promising directions in database cracking. With this study; we hope to expand thescope of database cracking and possibly leverage cracking in database engines other thanMonetDB. We repeat several prior database cracking works including the core cracking …,Proceedings of the VLDB Endowment,2013,30
A comparison of knives for bread slicing,Alekh Jindal; Endre Palatinus; Vladimir Pavlov; Jens Dittrich,Abstract Vertical partitioning is a crucial step in physical database design in row-orienteddatabases. A number of vertical partitioning algorithms have been proposed over the lastthree decades for a variety of niche scenarios. In principle; the underlying problem remainsthe same: decompose a table into one or more vertical partitions. However; it is not clearhow good different vertical partitioning algorithms are in comparison to each other. In fact; itis not even clear how to experimentally compare different vertical partitioning algorithms. Inthis paper; we present an exhaustive experimental study of several vertical partitioningalgorithms. We categorize vertical partitioning algorithms along three dimensions. Wesurvey six vertical partitioning algorithms and discuss their pros and cons. We identify themajor differences in the use-case settings for different algorithms and describe how to …,Proceedings of the VLDB Endowment,2013,24
Graph Analytics using the Vertica Relational Database,Alekh Jindal; Samuel Madden; Malu Castellanos; Meichun Hsu,Graph analytics is becoming increasingly popular; with a number of new applications andsystems developed in the past few years. In this paper; we study Vertica relational databaseas a platform for graph analytics. We show that vertex-centric graph analysis can betranslated to SQL queries; typically involving table scans and joins; and that modern column-oriented databases are very well suited to running such queries. Furthermore; we show howdevelopers can trade memory footprint for significantly reduced I/O costs in Vertica. Wepresent an experimental evaluation of the Vertica relational database system on a variety ofgraph analytics; including iterative analysis; a combination of graph and relational analyses;and more complex 1-hop neighborhood graph analytics; showing that it is competitive to twopopular vertex-centric graph analytics systems; namely Giraph and GraphLab.,*,2014,18
GRAPHiQL: A graph intuitive query language for relational databases,Alekh Jindal; Samuel Madden,Graph analytics is becoming increasingly popular; driving many important businessapplications from social network analysis to machine learning. Since most graph data iscollected in a relational database; it seems natural to attempt to perform graph analyticswithin the relational environment. However; SQL; the query language for relationaldatabases; makes it difficult to express graph analytics operations. This is because SQLrequires programmers to think in terms of tables and joins; rather than the more naturalrepresentation of graphs as collections of nodes and edges. As a result; even relativelysimple graph operations can require very complex SQL queries. In this paper; we presentGRAPHiQL; an intuitive query language for graph analytics; which allows developers toreason in terms of nodes and edges. GRAPHiQL provides key graph constructs such as …,Big Data (Big Data); 2014 IEEE International Conference on,2014,18
WWHow! Freeing Data Storage from Cages.,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract. Efficient data storage is a key component of data managing systems to achievegood performance. However; currently data storage is either heavily constrained by staticdecisions (eg fixed data stores in DBMSs) or left to be tuned and configured by users (egmanual data backup in File Systems). In this paper; we take a holistic view of data storageand envision a virtual storage layer. Our virtual storage layer provides a unified storageframework for several use-cases including personal; enterprise; and cloud storage.,CIDR,2013,18
Graphframes: an integrated api for mixing graph and relational queries,Ankur Dave; Alekh Jindal; Li Erran Li; Reynold Xin; Joseph Gonzalez; Matei Zaharia,Abstract Graph data is prevalent in many domains; but it has usually required specializedengines to analyze. This design is onerous for users and precludes optimization acrosscomplete workflows. We present GraphFrames; an integrated system that lets users combinegraph algorithms; pattern matching and relational queries; and optimizes work across them.GraphFrames generalize the ideas in previous graph-on-RDBMS systems; such as GraphXand Vertexica; by letting the system materialize multiple views of the graph (not just thespecific triplet views in these systems) and executing both iterative algorithms and patternmatching using joins. To make applications easy to write; GraphFrames provide a concise;declarative API based on the" data frame" concept in R that can be used for both interactivequeries and standalone programs. Under this API; GraphFrames use a graph-aware join …,Proceedings of the Fourth International Workshop on Graph Data Management Experiences and Systems,2016,17
How Achaeans Would Construct Columns in Troy.,Alekh Jindal; Felix Schuhknecht; Jens Dittrich; Karen Khachatryan; Alexander Bunte,ABSTRACT Column stores are becoming popular with data analytics in modern enterprises.However; traditionally; database vendors offer column stores as a different database productall together. As a result there is an all-or-none situation for column store features. To bridgethe gap; a recent effort introduced column store functionality in SQL server (a row store) bymaking deep seated changes in the database system. However; this approach is expensivein terms of time and effort. In addition; it is limited to SQL server. In this paper; we presentTrojan Columns; a novel technique for injecting column store functionality into a givenclosed source row-oriented commercial database system. Trojan Columns does not needaccess to the source code of the database system. Instead; it uses UDFs as a pluggablestorage layer to write and read data. Furthermore; Trojan Columns is transparent to users …,CIDR,2013,11
The mimicking octopus: Towards a one-size-fits-all database architecture,Alekh Jindal,ABSTRACT Modern enterprises need to pick the right DBMSs eg OLTP; OLAP; streamingsystems; scan-oriented systems among others; each tailored to a specific use-caseapplication; for their data managing problems. This makes using specialized solutions foreach application costly due to licensing fees; integration overhead and DBA costs.Additionally; it is tedious to integrate these specialized solutions together. Alternatively;enterprises use a single specialized DBMS for all applications and thereby compromiseheavily on performance. Further; a particular DBMS (eg row store) cannot adapt and changeinto a different DBMS (eg streaming system); as the workload changes; even though much ofthe code and technology is replicated anyways. In this paper we discuss building a new typeof database system which fits several use-cases while reducing costs; boosting …,VLDB PhD Workshop,2010,11
CARTILAGE: adding flexibility to the Hadoop skeleton,Alekh Jindal; Jorge Quiané-Ruiz; Samuel Madden,Abstract Modern enterprises have to deal with a variety of analytical queries over very largedatasets. In this respect; Hadoop has gained much popularity since it scales to thousand ofnodes and terabytes of data. However; Hadoop suffers from poor performance; especially inI/O performance. Several works have proposed alternate data storage for Hadoop in order toimprove the query performance. However; many of these works end up making deepchanges in Hadoop or HDFS. As a result; they are (i) difficult to adopt by several users; and(ii) not compatible with future Hadoop releases. In this paper; we present CARTILAGE; acomprehensive data storage framework built on top of HDFS. CARTILAGE allows users fullcontrol over their data storage; including data partitioning; data replication; data layouts; anddata placement. Furthermore; CARTILAGE can be layered on top of an existing HDFS …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,10
AdaptDB: adaptive partitioning for distributed joins,Yi Lu; Anil Shanbhag; Alekh Jindal; Samuel Madden,Abstract Big data analytics often involves complex join queries over two or more tables.Such join processing is expensive in a distributed setting both because large amounts ofdata must be read from disk; and because of data shuffling across the network. Manytechniques based on data partitioning have been proposed to reduce the amount of datathat must be accessed; often focusing on finding the best partitioning scheme for a particularworkload; rather than adapting to changes in the workload over time. In this paper; wepresent AdaptDB; an adaptive storage manager for analytical database workloads in adistributed setting. It works by partitioning datasets across a cluster and incrementallyrefining data partitioning as queries are run. AdaptDB introduces a novel hyper-join thatavoids expensive data shuffling by identifying storage blocks of the joining tables that …,Proceedings of the VLDB Endowment,2017,4
A moeba: a shape changing storage system for big data,Anil Shanbhag; Alekh Jindal; Yi Lu; Samuel Madden,Abstract Data partitioning significantly improves the query performance in distributeddatabase systems. A large number of techniques have been proposed to efficiently partitiona dataset for a given query workload. However; many modern analytic applications involvead-hoc or exploratory analysis where users do not have a representative query workloadupfront. Furthermore; workloads change over time as businesses evolve or as analysts gainbetter understanding of their data. Static workload-based data partitioning techniques aretherefore not suitable for such settings. In this paper; we describe the demonstration of Amoeba; a distributed storage system which uses adaptive multi-attribute data partitioning toefficiently support ad-hoc as well as recurring queries. A moeba applies a robust partitioningalgorithm such that ad-hoc queries on all attributes have similar performance gains …,Proceedings of the VLDB Endowment,2016,3
An experimental evaluation and analysis of database cracking,Felix Martin Schuhknecht; Alekh Jindal; Jens Dittrich,Abstract Database cracking has been an area of active research in recent years. The coreidea of database cracking is to create indexes adaptively and incrementally as a sideproduct of query processing. Several works have proposed different cracking techniques fordifferent aspects including updates; tuple reconstruction; convergence; concurrency control;and robustness. Our 2014 VLDB paper “The Uncracked Pieces in DatabaseCracking”(PVLDB 7: 97–108; 2013/VLDB 2014) was the first comparative study of thesedifferent methods by an independent group. In this article; we extend our publishedexperimental study on database cracking and bring it to an up-to-date state. Our goal is tocritically review several aspects; identify the potential; and propose promising directions indatabase cracking. With this study; we hope to expand the scope of database cracking …,The VLDB Journal,2016,2
A robust partitioning scheme for ad-hoc query workloads,Anil Shanbhag; Alekh Jindal; Samuel Madden; Jorge Quiane; Aaron J Elmore,Abstract Data partitioning is crucial to improving query performance several workload-basedpartitioning techniques have been proposed in database literature. However; many modernanalytic applications involve ad-hoc or exploratory analysis where users do not have arepresentative query workload a priori. Static workload-based data partitioning techniquesare therefore not suitable for such settings. In this paper; we propose Amoeba; a distributedstorage system that uses adaptive multi-attribute data partitioning to efficiently support ad-hoc as well as recurring queries. Amoeba requires zero set-up and tuning effort; allowinganalysts to get the benefits of partitioning without requiring an upfront query workload. Thekey idea is to build and maintain a partitioning tree on top of the dataset. The partitioningtree allows us to answer queries with predicates by reading a subset of the data. The …,Proceedings of the 2017 Symposium on Cloud Computing,2017,1
Replicated data storage system and methods,*,When analyzing a large web log; for example; the web log may contain different fields like'visitDate'; 'adRevenue' and 'sourceIP' that may serve as filter conditions. In the existing HDFSand Hadoop MapReduce stack; this log file may be uploaded to HDFS using the HDFSclient. HDFS then partitions the file into logical blocks using a constant block size (the HDFSdefault is 64 MB). Each block is then physically stored three times (assuming the default replicationfactor of three). Each physical copy of a block is called a replica. Each replica will sit on a differentdata node. Therefore; at least two node failures may be tolerated by HDFS. Information on thedifferent replicas for an HDFS block is kept in a central name node directory … After uploadinghis log file to HDFS; one may run an actual Map Reduce job. Assuming a user is interested inall source IPs with a visit date from 2011; a map-only MapReduce program may be …,*,2015,1
Robust Data Transformations,Alekh Jindal,ABSTRACT Background. Massively parallel data processing systems are ubiquitous intoday's big data era. Examples include Hadoop; Spark; Stratosphere; and a number of toolsdeveloped on top of them. Users of these systems upload their datasets to a distributed filesystem and run their analysis in a distributed fashion. However; several analyses require avariety of data preparation steps in order to perform the actual analysis efficiently. Examplesinclude creating a variety of data layouts; indexes; or partitioning for analytical queries;collecting random or stratified sampling for approximate queries; compressing; erasurecoding; or flexibly replicating data for tuning the storage space; and cleaning data; ieviolation detection and repair; to guarantee consistency with respect to a given set ofbusiness rules. Traditionally; such data transformations start with the assumption of a …,CIDR (Abstract),2015,1
Method of Storing and Accessing Data in a Database System,*,A method of storing and accessing data in a database system is disclosed. The databasesystem comprises at least one primary data source. The database system is associated withat least one adapted data structure that defines the physical data storage structures (eg; rowstorage and columnar storage) in which the data are stored. Data is allocated from the atleast one primary data source to the at least one adapted data structure in correlation with adatabase query received. For example; based on the data access patterns (eg; queries); thephysical data storage structures in which the data managed by the database system are tobe stored are dynamically determined.,*,2013,1
A METHOD OF STORING AND ACCESSING DATA IN A DATABASE SYSTEM,*,A method of storing and accessing data in a database system is disclosed. The databasesystem comprises at least one primary data source. The database system is associated withat least one adapted data structure that defines the physical data storage structures (eg; rowstorage and columnar storage) in which the data are stored. Data is allocated from the atleast one primary data source to the at least one adapted data structure in correlation with adatabase query received. For example; based on the data access patterns (eg; queries); thephysical data storage structures in which the data managed by the database system are tobe stored are dynamically determined.,*,2012,1
INGESTBASE: A Declarative Data Ingestion System,Alekh Jindal; Jorge-Arnulfo Quiane-Ruiz; Samuel Madden,Abstract: Big data applications have fast arriving data that must be quickly ingested. At thesame time; they have specific needs to preprocess and transform the data before it could beput to use. The current practice is to do these preparatory transformations once the data isalready ingested; however; this is expensive to run and cumbersome to manage. As a result;there is a need to push data preprocessing down to the ingestion itself. In this paper; wepresent a declarative data ingestion system; called INGESTBASE; to allow applicationdevelopers to plan and specify their data ingestion logic in a more systematic manner. Weintroduce the notion of ingestions plans; analogous to query plans; and present adeclarative ingestion language to help developers easily build sophisticated ingestionplans. INGESTBASE provides an extensible ingestion optimizer to rewrite and optimize …,arXiv preprint arXiv:1701.06093,2017,*
Graph Analytics on Relational Databases,Alekh Jindal; Sam Madden; Amol Deshpande; Michael Stonebraker,*,New England Database Summit,2014,*
Replicated data storage system and methods,*,*,*,2013,*
OctopusDB: flexible and scalable storage management for arbitrary database engines,Alekh Jindal,Wir leben in einer dynamischen Zeit; in der sich Wirtschaft; Technologie und Gesellschaftschneller verändern als jemals zuvor. Folglich unterscheiden sich die Anforderungen anDatenverarbeitung heute sehr von dem; was sich die Pioniere dieses Forschungsgebiets inden 70er Jahren ursprünglich ausgemalt hatten. Heutzutage sehen sich Firmen mit derHerausforderung konfrontiert; stark fluktuierende Anfragelasten über einer stetigwachsender Datenmengen zu bewältigen. Daher können es sich moderneDatenbanksysteme; sowohl relationale als auch Big Data Systeme; nicht mehr leisten; wiestarre; in Stein gemeißelte Lösungen zu funktionieren. Stattdessen sollten moderneDatenbanksysteme von Grunde auf für flexible Datenverwaltung konzipiert werden; um mitsich ständig ändernden Anforderungen Schritt halten zu können. Die gegenwärtige …,*,2012,*
Quality in Phrase Mining,Alekh Jindal,Abstract Phrase snippets of large text corpora like news articles or web search results offergreat insight and analytical value. While much of the prior work is focussed on efficientstorage and retrieval of all candidate phrases; little emphasis has been laid on the quality ofthe result set. In this thesis; we define phrases of interest and propose a framework formining and post-processing interesting phrases. We focus on the quality of phrases anddevelop techniques to mine minimal-length maximal-informative sequences of words. Thetechniques developed are streamed into a post-processing pipeline and include exact andapproximate match-based merging; incomplete phrase detection with filtering; and heuristics-based phrase classification. The strategies aim to prune the candidate set of phrases downto the ones being meaningful and having rich content. We characterize the phrases with …,*,2009,*
Teaching Philosophy Statement,Alekh Jindal,The Hindu Vedas place teachers on a high pedestal:“a guru creates and sustainsknowledge; and destroys the weeds of ignorance; just like the creator; the sustainer; and thedestroyer of this universe”. It is no surprise then that students see teachers as authorityfigures that teach them the truth. Thus; the teacher has a profound responsibility to guidestudents by helping them to assimilate and to critically analyze facts. The teacher must alsoprepare students to cope with contradictions between what is being taught in the class andwhat they experience in the real world. Ultimately; the goal of the teacher is to transform thestudents from collective followers of knowledge to independent seekers of it. I believe therole of a teacher is to align teaching activities along the path that works best for his or herclass and to create a constructive learning environment. This requires flexibility and …,*,*,*
Large-Scale Data Analysis: Bridging the Gap,Alekh Jindal; Sarath Yagiz Kargin; Vinay Setty Kumar,● SELECT INTO Temp sourceIP; AVG (pageRank) as avgPageRank; SUM (adRevenue) astotalRevenue FROM Rankings AS R; UserVisits AS UV WHERE R. pageURL= UV. destURLAND UV. visitDate BETWEEN Date ('2000-01-15') AND Date ('2000-01-22') GROUP BY UV.sourceIP;● SELECT sourceIP; totalRevenue; avgPageRank FROM Temp ORDER BYtotalRevenue DESC LIMIT 1;,*,*,*
Preparing Data For The Data Lake,Alekh Jindal; Samuel Madden,Abstract Data preparation is increasingly becoming one of the biggest challenges inprocessing big data. While recent tools such as Tamer and Trifacta address the problem ofintegrating and cleaning the datasets as they come in; preparing these datasets for efficientprocessing over a variety of query workloads is still challenging. In this talk; I will discussthese challenges and describe our tool which allows for fine-grained data preparation; via adata preparation plan; and efficiently runs this plan while uploading the data to HDFS.,*,*,*
Research Overview,Alekh Jindal,My research focuses on building scalable data processing systems that ease the pain of thisproliferation of systems for end users; ie; they support several query workloads; they areeasy to deploy and maintain; and they have comparable query performance as specializedsystems. Along this direction; I have worked on two major themes throughout my Ph. D. atSaarland University and Postdoc at MIT. First; I looked at integrating database concepts intolarge-scale data flow systems; which are highly scalable yet inefficient for several queryworkloads. Hadoop MapReduce; for instance; has became extremely popular due to itsmassive scalability and ease-of-use. However; many researchers and practitioners haveobserved a huge performance gap between Hadoop and well-configured parallel databaseson structured data (ie; involving relational-style operators and queries). To address this; I …,*,*,*
