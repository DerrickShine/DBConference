Using the crowd for top-k and group-by queries,Susan B Davidson; Sanjeev Khanna; Tova Milo; Sudeepa Roy,Abstract Group-by and top-k are fundamental constructs in database queries. However; thecriteria used for grouping and ordering certain types of data--such as unlabeled photosclustered by the same person ordered by age--are difficult to evaluate by machines. Incontrast; these tasks are easy for humans to evaluate and are therefore natural candidatesfor being crowd-sourced. We study the problem of evaluating top-k and group-by queriesusing the crowd to answer either type or value questions. Given two data elements; theanswer to a type question is" yes" if the elements have the same type and therefore belongto the same group or cluster; the answer to a value question orders the two data elements.The assumption here is that there is an underlying ground truth; but that the answersreturned by the crowd may sometimes be erroneous. We formalize the problems of top-k …,Proceedings of the 16th International Conference on Database Theory,2013,101
Tool for translating simulink models into input language of a model checker,B Meenakshi; Abhishek Bhatnagar; Sudeepa Roy,Abstract Model Based Development (MBD) using Mathworks tools like Simulink; Stateflowetc. is being pursued in Honeywell for the development of safety critical avionics software.Formal verification techniques are well-known to identify design errors of safety criticalsystems reducing development cost and time. As of now; formal verification of Simulinkdesign models is being carried out manually resulting in excessive time consumption duringthe design phase. We present a tool that automatically translates certain Simulink modelsinto input language of a suitable model checker. Formal verification of safety critical avionicscomponents becomes faster and less error prone with this tool. Support is also provided forreverse translation of traces violating requirements (as given by the model checker) intoSimulink notation for playback.,International Conference on Formal Engineering Methods,2006,64
On provenance and privacy,Susan B Davidson; Sanjeev Khanna; Sudeepa Roy; Julia Stoyanovich; Val Tannen; Yi Chen,Abstract Provenance in scientific workflows is a double-edged sword. On the one hand;recording information about the module executions used to produce a data item; as well asthe parameter settings and intermediate data items passed between module executions;enables transparency and reproducibility of results. On the other hand; a scientific workflowoften contains private or confidential data and uses proprietary modules. Hence; providingexact answers to provenance queries over all executions of the workflow may reveal privateinformation. In this paper we discuss privacy concerns in scientific workflows--data; module;and structural privacy-and frame several natural questions:(i) Can we formally analyze data;module; and structural privacy; giving provable privacy guarantees for an unlimited/boundednumber of provenance queries?(ii) How can we answer search and structural queries …,Proceedings of the 14th International Conference on Database Theory,2011,63
Provenance views for module privacy,Susan B Davidson; Sanjeev Khanna; Tova Milo; Debmalya Panigrahi; Sudeepa Roy,Abstract Scientific workflow systems increasingly store provenance information about themodule executions used to produce a data item; as well as the parameter settings andintermediate data items passed between module executions. However; authors/owners ofworkflows may wish to keep some of this information confidential. In particular; a modulemay be proprietary; and users should not be able to infer its behavior by seeing mappingsbetween all data inputs and outputs. The problem we address in this paper is the following:Given a workflow; abstractly modeled by a relation R; a privacy requirement? and costsassociated with data. The owner of the workflow decides which data (attributes) to hide; andprovides the user with a view R'which is the projection of R over attributes which have notbeen hidden. The goal is to minimize the cost of hidden data while guaranteeing that …,Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2011,59
A formal approach to finding explanations for database queries,Sudeepa Roy; Dan Suciu,Abstract As a consequence of the popularity of big data; many users with a variety ofbackgrounds seek to extract high level information from datasets collected from varioussources and combined using data integration techniques. A major challenge for research indata management is to develop tools to assist users in explaining observed query outputs. Inthis paper we introduce a principled approach to provide explanations for answers to SQLqueries based on intervention: removal of tuples from the database that significantly affectthe query answers. We provide a formal definition of intervention in the presence of multiplerelations which can interact with each other through foreign keys. First we give a set ofrecursive rules to compute the intervention for any given explanation in polynomial time(data complexity). Then we give simple and efficient algorithms based on SQL queries …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,51
Circuits for Datalog Provenance,Daniel Deutch; Tova Milo; Sudeepa Roy; Val Tannen,ABSTRACT The annotation of the results of database queries with provenance informationhas many applications. This paper studies provenance for datalog queries. We start byconsidering provenance representation by (positive) Boolean expressions; as pioneered inthe theories of incomplete and probabilistic databases. We show that even for linear datalogprograms the representation of provenance using Boolean expressions incurs a super-polynomial size blowup in data complexity. We address this with an approach that is novel inprovenance studies; showing that we can construct in PTIME poly-size (data complexity)provenance representations as Boolean circuits. Then we present optimization techniquesthat embed the construction of circuits into seminaive datalog evaluation; and further reducethe size of the circuits. We also illustrate the usefulness of our approach in multiple …,ICDT (to appear),2014,32
Enabling privacy in provenance-aware workflow systems,Susan B Davidson; Sanjeev Khanna; Sudeepa Roy; Julia Stoyanovich; Val Tannen; Yi Chen; Tova Milo,A new paradigm for creating and correcting scientific analyses is emerging; that ofprovenance-aware workflow systems. In such systems; repositories of workflowspecifications and of provenance graphs that represent their executions will be madeavailable as part of scientific information sharing. This will allow users to search and queryboth workflow specifications and their provenance graphs: Scientists who wish to performnew analyses may search workflow repositories to find specifications of interest to reuse ormodify. They may also search provenance information to understand the meaning of aworkflow; or to debug a specification. Finding erroneous or suspect data; a user may thenask provenance queries to determine what downstream data might have been affected; or tounderstand how the process failed that led to creating the data. With the increased …,*,2011,32
Optimizing user views for workflows,Olivier Biton; Susan B Davidson; Sanjeev Khanna; Sudeepa Roy,Abstract A technique called user views has recently been proposed to focus user attentionon relevant information in response to provenance queries over workflow executions [1; 2]:Given user input on what modules in the workflow specification are relevant to the user; auser view is a concise representation that clusters together modules to create a smallnumber of composite modules (or clusters) such that (1) each composite module in a userview contains at most one relevant (atomic) module; thus assuming the" meaning" of thatmodule; and (2) no control or data dependencies (either direct or indirect) are introduced(soundness) or removed (completeness) between relevant modules. The goal is to find auser view with a smallest number of composite modules. We show that for workflowspecifications that are general graphs; regardless of the number of distinct modules in the …,Proceedings of the 12th International Conference on Database Theory,2009,24
Lower Bounds for Exact Model Counting and Applications in Probabilistic Databases,Paul Beame; Jerry Li; Sudeepa Roy; Dan Suciu,Abstract: The best current methods for exactly computing the number of satisfyingassignments; or the satisfying probability; of Boolean formulas can be seen; either directly orindirectly; as building'decision-DNNF'(decision decomposable negation normal form)representations of the input Boolean formulas. Decision-DNNFs are a special case of'd-DNNF's where'd'stands for'deterministic'. We show that any decision-DNNF can beconverted into an equivalent'FBDD'(free binary decision diagram)--also known as a'read-once branching program'(ROBP or 1-BP)--with only a quasipolynomial increase inrepresentation size in general; and with only a polynomial increase in size in the specialcase of monotone k-DNF formulas. Leveraging known exponential lower bounds for FBDDs;we then obtain similar exponential lower bounds for decision-DNNFs which provide lower …,Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence; Bellevue; WA; USA,2013,22
Faster query answering in probabilistic databases using read-once functions,Sudeepa Roy; Vittorio Perduca; Val Tannen,Abstract A boolean expression is in read-once form if each of its variables appears exactlyonce. When the variables denote independent events in a probability space; the probabilityof the event denoted by the whole expression in read-once form can be computed inpolynomial time (whereas the general problem for arbitrary expressions is# P-complete).Known approaches to checking read-once property seem to require putting theseexpressions in disjunctive normal form. In this paper; we tell a better story for a largesubclass of boolean event expressions: those that are generated by conjunctive querieswithout self-joins and on tuple-independent probabilistic databases. We first show that givena tuple-independent representation and the provenance graph of an SPJ query plan withoutself-joins; we can; without using the DNF of a result event expression; efficiently compute …,Proceedings of the 14th International Conference on Database Theory,2011,22
An optimal labeling scheme for workflow provenance using skeleton labels,Zhuowei Bao; Susan B Davidson; Sanjeev Khanna; Sudeepa Roy,Abstract We develop a compact and efficient reachability labeling scheme for answeringprovenance queries on workflow runs that conform to a given specification. Even though aworkflow run can be structurally more complex and can be arbitrarily larger than thespecification due to fork (parallel) and loop executions; we show that a compact reachabilitylabeling for a run can be efficiently computed using the fact that it originates from a fixedspecification. Our labeling scheme is optimal in the sense that it uses labels of logarithmiclength; runs in linear time; and answers any reachability query in constant time. Ourapproach is based on using the reachability labeling for the specification as an effectiveskeleton for designing the reachability labeling for workflow runs. We also demonstrateempirically the effectiveness of our skeleton-based labeling approach.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,21
Privacy issues in scientific workflow provenance,Susan B Davidson; Sanjeev Khanna; Sudeepa Roy; Sarah Cohen Boulakia,Abstract A scientific workflow often deals with proprietary modules as well as private orconfidential data; such as health or medical information. Hence providing exact answers toprovenance queries over all executions of the workflow may reveal private information. Inthis paper we first study the potential privacy issues in a scientific workflow--module privacy;data privacy; and provenance privacy; and frame several natural questions:(i) can weformally analyze module; data or provenance privacy giving provable privacy guarantees foran unlimited/bounded number of provenance queries?(ii) how can we answer provenancequeries; providing as much information as possible to the user while still guaranteeing therequired privacy? Then we look at module privacy in detail and propose a formal model fromour recent work in [11]. Finally we point to several directions for future work.,Proceedings of the 1st International Workshop on Workflow Approaches to New Data-centric Science,2010,19
Causality and explanations in databases,Alexandra Meliou; Sudeepa Roy; Dan Suciu,Abstract With the surge in the availability of information; there is a great demand for tools thatassist users in understanding their data. While today's exploration tools rely mostly on datavisualization; users often want to go deeper and understand the underlying causes of aparticular observation. This tutorial surveys research on causality and explanation for data-oriented applications. We will review and summarize the research thus far into causality andexplanation in the database and AI communities; giving researchers a snapshot of thecurrent state of the art on this topic; and propose a unified framework as well as directions forfuture research. We will cover both the theory of causality/explanation and someapplications; we also discuss the connections with other topics in database research likeprovenance; deletion propagation; why-not queries; and OLAP techniques.,Proceedings of the VLDB Endowment,2014,17
Top-k and clustering with noisy comparisons,Susan Davidson; Sanjeev Khanna; Tova Milo; Sudeepa Roy,Abstract We study the problems of max/top-k and clustering when the comparisonoperations may be performed by oracles whose answer may be erroneous. Comparisonsmay either be of type or of value: given two data elements; the answer to a type comparisonis “yes” if the elements have the same type and therefore belong to the same group (cluster);the answer to a value comparison orders the two data elements. We give efficient algorithmsthat are guaranteed to achieve correct results with high probability; analyze the cost of thesealgorithms in terms of the total number of comparisons (ie; using a fixed-cost model); andshow that they are essentially the best possible. We also show that fewer comparisons areneeded when values and types are correlated; or when the error model is one in which theerror decreases as the distance between the two elements in the sorted order increases …,ACM Transactions on Database Systems (TODS),2014,13
Automatic translation of simulink models into the input language of a model checker,*,A translator converts an input model; such as resulting from a simulation of a design to beverified; into an output model suitable for verification by a model checker. The input model;for example; may be produced using Simulink; and the output model; for example; may be aNuSMV model.,*,2010,13
Model Counting of Query Expressions: Limitations of Propositional Methods,Paul Beame; Jerry Li; Sudeepa Roy; Suciu; Dan,Abstract: Query evaluation in tuple-independent probabilistic databases is the problem ofcomputing the probability of an answer to a query given independent probabilities of theindividual tuples in a database instance. There are two main approaches to this problem:(1)ingrounded inference'one first obtains the lineage for the query and database instance as aBoolean formula; then performs weighted model counting on the lineage (ie; computes theprobability of the lineage given probabilities of its independent Boolean variables);(2) inmethods known aslifted inference'orextensional query evaluation'; one exploits the high-level structure of the query as a first-order formula. Although it is widely believed that liftedinference is strictly more powerful than grounded inference on the lineage alone; no formalseparation has previously been shown for query evaluation. In this paper we show such a …,ICDT,2014,11
A propagation model for provenance views of public/private workflows,Susan B Davidson; Tova Milo; Sudeepa Roy,Abstract We study the problem of concealing functionality of a proprietary or private modulewhen provenance information is shown over repeated executions of a workflow whichcontains both public and private modules. Our approach is to use provenance views to hidecarefully chosen subsets of data over all executions of the workflow to ensure Γ-privacy: foreach private module and each input x; the module's output f (x) is indistinguishable from Γ--1other possible values given the visible data in the workflow executions. We show that Γ-privacy cannot be achieved simply by combining solutions for individual private modules;data hiding must also be propagated through public modules. We then examine how muchadditional data must be hidden and when it is safe to stop propagating data hiding. Theanswer depends strongly on the workflow topology as well as the behavior of public …,Proceedings of the 16th International Conference on Database Theory,2013,8
Hiding data and structure in workflow provenance,Susan Davidson; Zhuowei Bao; Sudeepa Roy,Abstract In this paper we discuss the use of views to address the problem of providing usefulanswers to provenance queries while ensuring that privacy concerns are met. In particular;we propose a hierarchical workflow model; based on context-free graph grammars; in whichfine-grained dependencies between the inputs and outputs of a module are explicitlyspecified. Using this model; we examine how privacy concerns surrounding data; modulefunction; and workflow structure can be addressed.,International Workshop on Databases in Networked Information Systems,2011,7
Provenance-based dictionary refinement in information extraction,Sudeepa Roy; Laura Chiticariu; Vitaly Feldman; Frederick R Reiss; Huaiyu Zhu,Abstract Dictionaries of terms and phrases (eg common person or organization names) areintegral to information extraction systems that extract structured information fromunstructured text. Using noisy or unrefined dictionaries may lead to many incorrect resultseven when highly precise and sophisticated extraction rules are used. In general; the resultsof the system are dependent on dictionary entries in arbitrary complex ways; and removal ofa set of entries can remove both correct and incorrect results. Further; any such refinementcritically requires laborious manual labeling of the results. In this paper; we study thedictionary refinement problem and address the above challenges. Using provenance of theoutputs in terms of the dictionary entries; we formalize an optimization problem ofmaximizing the quality of the system with respect to the refined dictionaries; study …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,6
Explaining Query Answers with Explanation-Ready Databases,Sudeepa Roy; Laurel Orr; Dan Suciu,Abstract With the increased generation and availability of big data in different domains; thereis an imminent requirement for data analysis tools that are able to'explain'the trends andanomalies obtained from this data to a range of users with different backgrounds. Wu-Madden (PVLDB 2013) and Roy-Suciu (SIGMOD 2014) recently proposed solutions that canexplain interesting or unexpected answers to simple aggregate queries in terms ofpredicates on attributes. In this paper; we propose a generic framework that can supportmuch richer; insightful explanations by preparing the database offline; so that topexplanations can be found interactively at query time. The main idea in such explanation-ready databases is to pre-compute the effects of potential explanations (calledinterventions); and efficiently re-evaluate the original query taking into account these …,Proceedings of the VLDB Endowment,2015,5
Queries with difference on probabilistic databases,Sanjeev Khanna; Sudeepa Roy; Val Tannen,ABSTRACT We study the feasibility of the exact and approximate computation of theprobability of relational queries with difference on tuple-independent databases. We showthat even the difference between two “safe” conjunctive queries without self-joins is “unsafe”for exact computation. We turn to approximation and design an FPRAS for a large class ofrelational queries with difference; limited by how difference is nested and by the nature ofthe subtracted subqueries. We give examples of inapproximable queries outside this class.,*,2011,5
STCON in directed unique-path graphs,Sampath Kannan; Sanjeev Khanna; Sudeepa Roy,Abstract We study the problem of space-efficient polynomial-time algorithms for {\emdirected st-connectivity}(STCON). Given a directed graph $ G $; and a pair of vertices $ s; t $;the STCON problem is to decide if there exists a path from $ s $ to $ t $ in $ G $. For generalgraphs; the best polynomial-time algorithm for STCON uses space that is only slightlysublinear. However; for special classes of directed graphs; polynomial-time poly-logarithmic-space algorithms are known for STCON. In this paper; we continue this thread of researchand study a class of graphs called\emph {unique-path graphs with respect to source $ s $};where there is at most one simple path from $ s $ to any vertex in the graph. For thesegraphs; we give a polynomial-time algorithm that uses $\tilde O (n^{\varepsilon}) $ space forany constant $\varepsilon\in (0; 1] $. We also give a polynomial-time; $\tilde O (n …,LIPIcs-Leibniz International Proceedings in Informatics,2008,4
Exact model counting of query expressions: limitations of propositional methods,Paul Beame; Jerry Li; Sudeepa Roy; Dan Suciu,Abstract We prove exponential lower bounds on the running time of the state-of-the-art exactmodel counting algorithms—algorithms for exactly computing the number of satisfyingassignments; or the satisfying probability; of Boolean formulas. These algorithms can beseen; either directly or indirectly; as building Decision-Decomposable Negation NormalForm (decision-DNNF) representations of the input Boolean formulas. Decision-DNNFs area special case of d-DNNFs where d stands for deterministic. We show that any knowledgecompilation representations from a class (called DLDDs in this article) that contain decision-DNNFs can be converted into equivalent Free Binary Decision Diagrams (FBDDs); alsoknown as Read-Once Branching Programs; with only a quasi-polynomial increase inrepresentation size. Leveraging known exponential lower bounds for FBDDs; we then …,ACM Transactions on Database Systems (TODS),2017,2
On the Complexity of Evaluating Order Queries with the Crowd.,Benoît Groz; Tova Milo; Sudeepa Roy,One of the foremost challenges for information technology over the last few years has beento explore; understand; and extract useful information from large amounts of data. Someparticular tasks such as annotating data or matching entities have been outsourced tohuman workers for many years. But the last few years have seen the rise of a new researchfield called crowdsourcing that aims at delegating a wide range of tasks to human workers;building formal frameworks; and improving the efficiency of these processes. The databasecommunity has thus been suggesting algorithms to process traditional data manipulationoperators with the crowd; such as joins or filtering. This is even more useful when comparingthe underlying “tuples” is a subjective decision–eg; when they are photos; text; or simplynoisy data with different variations and interpretations–and can presumably be done …,IEEE Data Eng. Bull.,2015,2
Refining a dictionary for information extraction,*,A method for refining a dictionary for information extraction; the operations including:inputting a set of extracted results from execution of an extractor comprising the dictionary ona collection of text; wherein the extracted results are labeled as correct results or incorrectresults; processing the extracted results using an algorithm configured to set a score of theextractor above a score threshold; wherein the score threshold balances a precision and arecall of the extractor; and outputting a set of candidate dictionary entries corresponding to afull set of dictionary entries; wherein the candidate dictionary entries are candidates to beremoved from the dictionary based on the extracted results.,*,2014,2
FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference,Sudeepa Roy; Cynthia Rudin; Alexander Volfovsky; Tianyu Wang,Abstract: A classical problem in causal inference is that of matching; where treatment unitsneed to be matched to control units. Some of the main challenges in developing matchingmethods arise from the tension among (i) inclusion of as many covariates as possible indefining the matched groups;(ii) having matched groups with enough treated and controlunits for a valid estimate of Average Treatment Effect (ATE) in each group; and (iii)computing the matched pairs efficiently for large datasets. In this paper we propose a fastand novel method for approximate and exact matching in causal analysis called FLAME(Fast Large-scale Almost Matching Exactly). We define an optimization objective for matchquality; which gives preferences to matching on covariates that can be useful for predictingthe outcome while encouraging as many matches as possible. FLAME aims to optimize …,arXiv preprint arXiv:1707.06315,2017,1
Answering conjunctive queries with inequalities,Paraschos Koutris; Tova Milo; Sudeepa Roy; Dan Suciu,Abstract In this paper; we study the complexity of answering conjunctive queries (CQ) withinequalities (≠). In particular; we are interested in comparing the complexity of the querywith and without inequalities. The main contribution of our work is a novel combinatorialtechnique that enables us to use any Select-Project-Join query plan for a given CQ withoutinequalities in answering the CQ with inequalities; with an additional factor in running timethat only depends on the query. The key idea is to define a new projection operator; whichkeeps a small representation (independent of the size of the database) of the set of inputtuples that map to each tuple in the output of the projection; this representation is used toevaluate all the inequalities in the query. Second; we generalize a result by Papadimitriouand Yannakakis (1997) and give an alternative algorithm based on the color-coding …,Theory of Computing Systems,2017,1
Provenance and uncertainty,Sudeepa Roy,Abstract Data provenance; a record of the origin and transformation of data; explains howoutput data is derived from input data. This dissertation focuses on exploring the connectionbetween provenance and uncertainty in two main directions:(1) how a succinctrepresentation of provenance can help infer uncertainty in the input or the output; and (2)how introducing uncertainty can facilitate publishing provenance information while hidingassociated private information.,*,2012,1
On “Go With the Winners” algorithm,Sudeepa Roy,Abstract Aldous and Vazirani proposed the “Go With The Winners” algorithm [AV94] to boostthe success probability in searching for a leaf at the deepest level of a search tree; byintroducing interactions between simulations. They claim high probability of success usingonly polynomial (in) number of simulations (as against the naıve exponential bound) onsearch trees satisfying certain criteria quantified by a parameter. This search tree modelabstracts the behaviour of Simulated Annealing on a continuous function where a particlemoves down to the leaves as the temperature is lowered making irrevocable choices atbranch points of the tree. In this work; we propose a simple condition which intends tocapture precisely the set of search trees for which the “Go With The Winners” algorithm willfind a deepest node with high probability using only a polynomial number of particles. We …,*,2006,1
Analyzing Query Performance and Attributing Blame for Contentions in a Cluster Computing Framework,Prajakta Kalmegh; Shivnath Babu; Sudeepa Roy,Abstract: Analyzing contention for resources in a cluster computing environment accuratelyis critical in order to understand the performance interferences faced by a query due toconcurrent query executions; and to better manage the workload in the cluster. Today notools exist to help an admin perform a deep analysis of resource contentions taking intoaccount the complex interactions among different queries; their stages; and tasks in ashared cluster. In this paper; we present ProtoXplore-a Proto or first system to eXplore theinteractions between concurrent queries in a shared cluster. We construct a multi-leveldirected acyclic graph called ProtoGraph to formally capture different types of explanationsthat link the performance of concurrent queries. In particular;(a) we designate thecomponents of a query's lost (wait) time as Immediate Explanations towards its observed …,arXiv preprint arXiv:1708.08435,2017,*
A Framework for Inferring Causality from Multi-Relational Observational Data using Conditional Independence,Sudeepa Roy; Babak Salimi,Abstract: The study of causality or causal inference-how much a given treatment causallyaffects a given outcome in a population-goes way beyond correlation or association analysisof variables; and is critical in making sound data driven decisions and policies in a multitudeof applications. The gold standard in causal inference is performing" controlledexperiments"; which often is not possible due to logistical or ethical reasons. As analternative; inferring causality on" observational data" based on the" Neyman-Rubinpotential outcome model" has been extensively used in statistics; economics; and socialsciences over several decades. In this paper; we present a formal framework for soundcausal analysis on observational datasets that are given as multiple relations and where thepopulation under study is obtained by joining these base relations. We study a crucial …,arXiv preprint arXiv:1708.02536,2017,*
Optimizing Iceberg Queries with Complex Joins,Brett Walenz; Sudeepa Roy; Jun Yang,Abstract Iceberg queries; commonly used for decision support; find groups whose aggregatevalues are above or below a threshold. In practice; iceberg queries are often posed overcomplex joins that are expensive to evaluate. This paper proposes a framework forcombining a number of techniques---a-priori; memoization; and pruning---to optimizeiceberg queries with complex joins. A-priori pushes partial GROUP BY and HAVINGcondition before a join to reduce its input size. Memoization caches and reuses joincomputation results. Pruning uses cached results to infer that certain tuples cannotcontribute to the final query result; and short-circuits join computation. We formally deriveconditions for correctly applying these techniques. Our practical rewrite algorithm produceshighly efficient SQL that can exploit combinations of optimization opportunities in ways …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Provenance: Privacy and Security,Susan B Davidson; Sudeepa Roy,Data provenance is information about the origins of data and its movement betweendatabases and processes. It can be used to understand and debug the process by whichdata was obtained and transformed; to ensure reproducibility of results; and to establishtrust. Provenance therefore has implications for both the security and privacy of theassociated data. As metadata; there are also security and privacy concerns associated withprovenance itself; including the integrity; confidentiality; and availability of provenanceinformation.,*,2017,*
connect with us,Paolo Missier; Jun Zhao; Vanessa Braganholo; Adriane Chapman; Sarah Cohen-Boulakia; Vasa Curcin; Tom De Nies; Lois Delcambre; Saumen Dey; Alan Fekete; Irini Fundulaki; Floris Geerts; Ashish Gehani; Boris Glavic; Paul Groth; Melanie Herschel; Bertram Ludaescher; Simon Miles; Luc Moreau; Paolo Papotti; Sudeepa Roy; Perdita Stevens; James Cheney,Abstract: The cost of deriving actionable knowledge from large datasets has beendecreasing thanks to a convergence of positive factors: low cost data generation;inexpensively scalable storage and processing infrastructure (cloud); software frameworksand tools for massively distributed data processing; and parallelisable data analyticsalgorithms. One observation that is often overlooked; however; is that each of theseelements is not immutable; rather they all evolve over time. As those datasets change overtime; the value of their derivative knowledge may decay; unless it is preserved by reacting tothose changes. Our broad research goal is to develop models; methods; and tools forselectively reacting to changes by balancing costs and benefits; ie through complete orpartial re-computation of some of the underlying processes. In this paper we present an …,*,2016,*
Dictionary refinement for information extraction,*,A method for refining a dictionary for information extraction; the operations including:inputting a set of extracted results from execution of an extractor comprising the dictionary ona collection of text; wherein the extracted results are labeled as correct results or incorrectresults; processing the extracted results using an algorithm configured to set a score of theextractor above a score threshold; wherein the score threshold balances a precision and arecall of the extractor; and outputting a set of candidate dictionary entries corresponding to afull set of dictionary entries; wherein the candidate dictionary entries are candidates to beremoved from the dictionary based on the extracted results.,*,2013,*
2008 Preface--IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science,Ramesh Hariharan; Madhavan Mukund; V Vinay,Abstract This volume contains the proceedings of the 28th international conference on theFoundations of Software Technology and Theoretical Computer Science (FSTTCS 2008);organized under the auspices of the Indian Association for Research in Computing Science(IARCS).,LIPIcs-Leibniz International Proceedings in Informatics,2008,*
