Serializable isolation for snapshot databases,Michael J Cahill; Uwe Röhm; Alan D Fekete,Abstract Many popular database management systems implement a multiversionconcurrency control algorithm called snapshot isolation rather than providing fullserializability based on locking. There are well-known anomalies permitted by snapshotisolation that can lead to violations of data consistency by interleaving transactions thatwould maintain consistency if run serially. Until now; the only way to prevent theseanomalies was to modify the applications by introducing explicit locking or artificial updateconflicts; following careful analysis of conflicts between all pairs of transactions. This articledescribes a modification to the concurrency control algorithm of a database managementsystem that automatically detects and prevents snapshot isolation anomalies at runtime forarbitrary applications; thus providing serializable isolation. The new algorithm preserves …,ACM Transactions on Database Systems (TODS),2009,199
Serializable isolation for snapshot databases,Michael J Cahill; Uwe Röhm; Alan D Fekete,Abstract Many popular database management systems implement a multiversionconcurrency control algorithm called snapshot isolation rather than providing fullserializability based on locking. There are well-known anomalies permitted by snapshotisolation that can lead to violations of data consistency by interleaving transactions thatwould maintain consistency if run serially. Until now; the only way to prevent theseanomalies was to modify the applications by introducing explicit locking or artificial updateconflicts; following careful analysis of conflicts between all pairs of transactions. This articledescribes a modification to the concurrency control algorithm of a database managementsystem that automatically detects and prevents snapshot isolation anomalies at runtime forarbitrary applications; thus providing serializable isolation. The new algorithm preserves …,ACM SIGMOD International Conference on Management of Data; SIGMOD 2008,2008,199
FAS: a freshness-sensitive coordination middleware for a cluster of OLAP components,Uwe Röhm; Klemens Böhm; Hans-Jörg Schek; Heiko Schuldt,Data warehouses offer a compromise between freshness of data and query evaluation time.However; a fixed preference ratio between these two variables is too undifferentiated. Datawarehouses are closely tied to online analytical processing (OLAP) of the vast amount ofdata of an organization. They typically offer a compromise between freshness of data andwarehouse maintenance costs. Different application scenarios and users; however; havedifferent preferences in this respect; and a fixed preference ratio is too undifferentiated. Withthe approach presented in this chapter; clients submit query or update transactions to thismiddleware; instead of directly communicating with specific cluster nodes. The middlewareschedules and routes updates and queries to cluster nodes. The scheduler generates acorrect; interleaved execution order. In general; scheduling allows for several cluster …,Proceedings of the 28th international conference on Very Large Data Bases,2002,141
OLAP query routing and physical design in a database cluster,Uwe Rohm; Klemens Bohm; S Hans-Jorg,Abstract This article quantifies the benefit from simple data organization schemes andelementary query routing techniques for the PowerDB engine; a system that coordinates acluster of databases. We report on evaluations for a specific scenario: the workload containsOLAP queries; OLTP queries; and simple updates; borrowed from the TPC-R benchmark.We investigate affinity of OLAP queries and different routing strategies for such queries. Wethen compare two simple data placement schemes; namely full replication and a hybrid onecombining partial replication with partitioning. We run different experiments with queriesonly; with updates only; and with queries concurrently to simple updates. It turns out thathybrid is superior to full replication; even without updates. Our overall conclusion is thatcoordinator-based routing has good scaleup properties for scenarios with complex …,Lecture Notes in Computer Science,2000,140
OLAP query routing and physical design in a database cluster,Uwe Röhm; Klemens Böhm; Hans-Jörg Schek,Abstract This article quantifies the benefit from simple data organization schemes andelementary query routing techniques for the PowerDB engine; a system that coordinates acluster of databases. We report on evaluations for a specific scenario: the workload containsOLAP queries; OLTP queries; and simple updates; borrowed from the TPC-R benchmark.We investigate affinity of OLAP queries and different routing strategies for such queries. Wethen compare two simple data placement schemes; namely full replication and a hybrid onecombining partial replication with partitioning. We run different experiments with queriesonly; with updates only; and with queries concurrently to simple updates. It turns out thathybrid is superior to full replication; even without updates. Our overall conclusion is thatcoordinator-based routing has good scaleup properties for scenarios with complex …,Advances in Database Technology—EDBT 2000,2000,140
Resource-aware online data mining in wireless sensor networks,Nhan Duc Phung; Mohamed Medhat Gaber; Uwe Röhm,Data processing in wireless sensor networks often relies on high-speed data stream input;but at the same time is inherently constrained by limited resource availability. Thus; energyefficiency and good resource management are vital for in-network processing techniques.We propose enabling resource-awareness for in-network processing algorithms by meansof a resource monitoring component and designed a corresponding framework. As proof ofconcept; we implement an online clustering algorithm; which uses the resource monitor toadapt to resource availability; on the Sun SPOT sensor nodes from Sun Microsystem. Werefer to this adaptive clustering algorithm as extended resource-aware cluster (ERA-cluster).Finally; we report on the outcome of several experiments to evaluate the validity of ourapproach in terms of resource adaptiveness and accuracy of the ERA-cluster. Results …,Computational Intelligence and Data Mining; 2007. CIDM 2007. IEEE Symposium on,2007,53
Cache-aware query routing in a cluster of databases,Uwe Röhm; Klemens Böhm; Hans-Jörg Schek,We investigate query routing techniques in a cluster of databases for a query-dominantenvironment. The objective is to decrease query response time. Each component of thecluster runs an off-the-shelf DBMS and holds a copy of the whole database. The cluster hasa coordinator that routes each query to an appropriate component. Considering queries ofrealistic complexity; eg; TPC-R; this article addresses the following questions: Can routingbenefit from caching effects due to previous queries? Since our components are black-boxes; how can we approximate their cache content? How to route a query; given suchcache approximations? To answer these questions; we have developed a cache-awarequery router that is based on signature approximations of queries. We report onexperimental evaluations with the TPC-R benchmark using our PowerDB database …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,53
Serializable snapshot isolation for replicated databases in high-update scenarios,Hyungsoo Jung; Hyuck Han; Alan Fekete; Uwe Röhm,ABSTRACT Many proposals for managing replicated data use sites running the SnapshotIsolation (SI) concurrency control mechanism; and provide 1-copy SI or something similar;as the global isolation level. This allows good scalability; since only ww-conflicts need to bemanaged globally. However; 1-copy SI can lead to data corruption and violation of integrityconstraints [5]. 1-copy serializability is the global correctness condition that prevents datacorruption. We propose a new algorithm Replicated Serializable Snapshot Isolation (RSSI)that uses SI at each site; and combines this with a certification algorithm to guarantee 1-copyserializable global execution. Management of ww-conflicts is similar to what is done in 1-copy SI. But unlike previous designs for 1-copy serializable systems; we do not need toprevent all rw-conflicts among concurrent transactions. We formalize this in a theorem that …,Proceedings of the VLDB Endowment,2011,41
Data management for high-throughput genomics,Uwe Röhm; Jose Blakeley,Abstract: Today's sequencing technology allows sequencing an individual genome within afew weeks for a fraction of the costs of the original Human Genome project. Genomics labsare faced with dozens of TB of data per week that have to be automatically processed andmade available to scientists for further analysis. This paper explores the potential and thelimitations of using relational database systems as the data processing platform for high-throughput genomics. In particular; we are interested in the storage management for high-throughput sequence data and in leveraging SQL and user-defined functions for dataanalysis inside a database system. We give an overview of a database design for high-throughput genomics; how we used a SQL Server database in some unconventional ways toprototype this scenario; and we will discuss some initial findings about the scalability and …,CIDR 2009,2009,36
Evaluating the coordination overhead of replica maintenance in a cluster of databases,Klemens Böhm; Torsten Grabs; Uwe Röhm; Hans-Jörg Schek,Abstract We investigate the design of a coordinator for a cluster of databases. We considerthe following alternatives: TP-Heavy using the TUXEDO TP-monitor; TP-Lite with theORACLE8 database system; and a TP-Less coordinator implemented in EmbeddedSQL/C++. In particular; we investigate the scalability of full replication. We assume thatupdate actions on all replica are executed either synchronously or asynchronously. It turnsout that the TP-Less approach outperforms commercial TP-middleware for small clustersizes already. Another observation is that asynchronous updates are the preferred optioncompared to synchronous updates. The conclusion is that a transaction protocol at thesecond layer must be replication-aware.,Euro-Par 2000 Parallel Processing,2000,34
Corona: Energy-efficient multi-query processing in wireless sensor networks,Raymes Khoury; Tim Dawborn; Bulat Gafurov; Glen Pink; Edmund Tse; Quincy Tse; K Almi’Ani; Mohamed Gaber; Uwe Röhm; Bernhard Scholz,Abstract Wireless sensor networks (WSNs) are a core infrastructure for automaticenvironmental monitoring. We developed Corona as an in-network distributed queryprocessor that allows to share a sensor network between several users with a declarativequery language. It includes a novel approach for minimising sensor activations in sharedwireless sensor networks: we introduce the notion of freshness into WSN so that users canask for cached sensor reading with freshness guarantees. We further integrated a resource-awareness framework that allows the query processor to dynamically adapt to changingresource levels. The capabilities of this system are demonstrated with several aggregationqueries for different users with different freshness and result precision needs.,Database Systems for Advanced Applications,2010,32
Hyperdatabases,H-J Schek; Klemens Böhm; Torsten Grabs; Uwe Röhm; Heiko Schuldt; Roger Weber,When relational database systems were introduced twenty years ago (1980); they were aninfrastructure and main platform for application development. With today's informationsystems; the database system is a storage manager; far away from the applications. Ourvision is that hyperdatabases become available that move up and extend databaseconcepts to a higher level; closer to the applications. A hyperdatabase manages distributedobjects and software components as well as workflows; in analogy to a database system thatmanages data and transactions. In short; hyperdatabases; also called" higher orderdatabases"; will provide" higher order data independence"; eg; immunity of applicationsagainst changes in the implementation of components and workload transparency. They willbe the infrastructure for distributed information systems engineering of the future; and they …,Web Information Systems Engineering; 2000. Proceedings of the First International Conference on,2000,32
YCSB+T: Benchmarking web-scale transactional databases,Akon Dey; Alan Fekete; Raghunath Nambiar; Uwe Röhm,Database system benchmarks like TPC-C and TPC-E focus on emulating databaseapplications to compare different DBMS implementations. These benchmarks use carefullyconstructed queries executed within the context of transactions to exercise specific RDBMSfeatures; and measure the throughput achieved. Cloud services benchmark frameworks likeYCSB; on the other hand; are designed for performance evaluation of distributed NoSQL key-value stores; early examples of which did not support transactions; and so the benchmarksuse single operations that are not inside transactions. Recent implementations of web-scaledistributed NoSQL systems like Spanner and Percolator; offer transaction features to cater tonew web-scale applications. This has exposed a gap in standard benchmarks. We identifythe issues that need to be addressed when evaluating transaction support in NoSQL …,Data Engineering Workshops (ICDEW); 2014 IEEE 30th International Conference on,2014,31
The cost of serializability on platforms that use snapshot isolation,Mohammad Alomari; Michael Cahill; Alan Fekete; Uwe Röhm,Several common DBMS engines use the multiversion concurrency control mechanismcalled Snapshot Isolation; even though application programs can experience non-serializable executions when run concurrently on such a platform. Several proposals existfor modifying the application programs; without changing their semantics; so that they arecertain to execute serializably even on an engine that uses SI. We evaluate the performanceimpact of these proposals; and find that some have limited impact (only a few percent drop inthroughput at a given multi-programming level) while others lead to much greater reductionin throughput of up-to 60% in high contention scenarios. We present experimental results forboth an open-source and a commercial engine. We relate these to the theory; givingguidelines on which conflicts to introduce so as to ensure correctness with little impact on …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,29
A robust technique to ensure serializable executions with snapshot isolation DBMS,Mohammad Alomari; Alan Fekete; U Röhm,Snapshot Isolation (SI) is a popular concurrency control mechanism that has beenimplemented by many commercial and open-source platforms (eg Oracle; Postgre SQL; andMS SQL Server 2005). Unfortunately; SI can result in nonserializable execution; in whichdatabase integrity constraints can be violated. The literature reports some techniques toensure that all executions are serializable when run in an engine that uses SI forconcurrency control. These modify the application by introducing conflicting SQL statements.However; with each of these techniques the DBA has to make a choice among possibletransactions to modify—and as we previously showed; making a bad choice of whichtransactions to modify can come with a hefty performance reduction. In this paper wepropose a novel technique called ELM to introduce conflicts in a separate lock-manager …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,17
Working together in harmony-an implementation of the CORBA object query service and its evaluation,Uwe Röhm; Klemens Böhm,The CORBA standard; together with its service specifications; has gained considerableattention in recent years. The CORBA Object Query Service allows for declarative access toheterogeneous storage systems. We have come up with an implementation of this servicecalled Harmony. The objective of the article is to provide a detailed description andquantitative assessment of Harmony. Its main technical characteristics are data-flowevaluation; bulk transfer and intra-query parallelism. To carry out the evaluation; we haveclassified data exchange between components of applications in several dimensions: one isto distinguish between point-; context-and bulk data access. We have compared Harmonywith:(1) data access through application-specific CORBA objects; and (2) conventionalclient/server communication; ie; Embedded SQL. Our results show that Harmony performs …,Data Engineering; 1999. Proceedings.; 15th International Conference on,1999,17
Scalable transactions across heterogeneous NoSQL key-value data stores,Akon Dey; Alan Fekete; Uwe Röhm,Abstract Many cloud systems provide data stores with limited features; especially they maynot provide transactions; or else restrict transactions to a single item. We propose aapproach that gives multi-item transactions across heterogeneous data stores; using only aminimal set of features from each store such as single item consistency; conditional update;and the ability to include extra metadata within a value. We offer a client-coordinatedtransaction protocol that does not need a central coordinating infrastructure. A prototypeimplementation has been built as a Java library and measured with an extension of YCSBbenchmark to exercise multi-item transactions.,Proceedings of the VLDB Endowment,2013,16
Enabling resource-awareness for in-network data processing in wireless sensor networks,Uwe Röhm; Mohamed Medhat Gaber; Quincy Tse,Abstract The next-generation of wireless sensor platforms allows for more advanced in-network data processing. The central challenge remains energy and communicationefficiency. This paper presents a resource-awareness framework for wireless sensornetworks that allows in-network data processing to adapt to changing resource levels suchas battery power or available memory. We have implemented the proposed framework aspart of a query processing system for the Sun SPOT sensor network platform. As a casestudy; we have applied the framework to the query processor's on-line data clusteringalgorithm; making it resource-aware. In an experimental study; we demonstrate howcommunication costs can be significantly reduced by de-coupling clustering and datacommunication. The results also show the effectiveness of the resource-aware clustering …,Proceedings of the nineteenth conference on Australasian database-Volume 75,2008,16
User Awareness and Policy Compliance of Data Privacy in Cloud Computing,Audrey Mei Yi Quah; Uwe Röhm,Abstract Cloud computing is promising many technical benefits such as enhancedscalability; computing elasticity; and cost efficiency. However; with the benefits of cloud-based; hosted software platforms also comes the responsibility to data privacy. This paperinvestigates the data privacy issues brought about by cloud computing from an Australianperspective with specific focus on two aspects: How does cloud computing affectorganisations' compliance to Australian privacy and data protection regulations? And towhat extent are end-users aware of how cloud computing technologies affect their privacy?We present the results of an online survey among cloud computing users and contrast thesewith the technological possibilities and the cloud provider positions. According to this survey;almost half of the end-users were unaware that they are in fact using one or more cloud …,First Australasian Web Conference (AWC 2013),2013,10
Serializable executions with snapshot isolation: Modifying application code or mixing isolation levels?,Mohammad Alomari; Michael Cahill; Alan Fekete; Uwe Röhm,Abstract Snapshot Isolation concurrency control (SI) allows substantial performance gainscompared to holding commit-duration readlocks; while still avoiding many anomalies suchas lost updates or inconsistent reads. However; for some sets of application programs; SIcan result in non-serializable execution; in which database integrity constraints can beviolated. The literature reports two different approaches to ensuring all executions areserializable while still using SI for concurrency control. In one approach; the applicationprograms are modified (without changing their stand-alone semantics) by introducing extraconflicts. In the other approach; the application programs are not changed; but a smallsubset of them must be run using standard two-phase locking rather than SI. We comparethe performance impact of these two approaches. Our conclusion is that the convenience …,Database Systems for Advanced Applications,2008,10
How to BLAST your database—a study of stored procedures for BLAST searches,Uwe Röhm; Thanh-Mai Diep,Abstract Stored procedures are an important feature of all major database systems thatallows to execute application logic within database servers. This paper reports onexperiences to implement a popular scientific algorithm; the Basic Local Alignment SearchTool (BLAST); as stored procedures within a relational database. We implemented the un-gapped; nucleotide version of the BLAST algorithm with four different relational databaseengines; both commercial and open source. In an experimental evaluation; we comparedour dbBLAST implementations with a standard file-based BLAST implementation from NCBIwith regard to the implementation effort; runtime performance; and scalability. It shows thatalthough our dbBLAST runs faster than the file-based BLAST program for short querysequences; all implementations lack scalability. However; the results also indicate that …,Database Systems for Advanced Applications,2006,10
The Efficacy of Commutativity-Based Semantic Locking in a Real-World Application,Paul Wu; Alan Fekete; Uwe Röhm,While the dominant approach to persistent storage in practice is to use a relational DBMS;there are some specialist applications that rely on object stores. The performance of theseapplications depends on the efficiency of the object store's concurrency control mechanism.Today's predominant concurrency control mechanism is strict two-phase object locking. Inthe 1980s; an interesting alternative was developed: commutativity-based semantic locking.In theory; it can outperform traditional locking schemes in certain scenarios with appropriatecommutativity potential. In this paper; we study the real-world performance of differentlocking strategies in a particular industrial application from the telecommunications sector.We compare object-based locking and commutativity-based semantic locking. We foundthat; in this application; semantic locking performs equally to; but no better than; object …,Knowledge and Data Engineering; IEEE Transactions on,2008,9
Freshness-aware caching in a cluster of J2EE application servers,Uwe Röhm; Sebastian Schmidt,Abstract Application servers rely on caching and clustering to achieve high performance andscalability. While queries benefit from middle-tier caching; updates introduce a distributedcache consistency problem. The standard approaches to this problem; cache invalidationand cache replication; either do not guarantee full cache consistency or impose aperformance penalty. This paper proposes a novel approach: Freshness-Aware Caching(FAC). FAC tracks the freshness of cached data and allows clients to explicitly tradefreshness-of-data for response times. We have implemented FAC in an open-sourceapplication server and compare its performance to cache invalidation and cache replication.The evaluation shows that both cache invalidation and FAC provide better update scalabilitythan cache replication. We also show that FAC can provide a significant better read …,Web Information Systems Engineering–WISE 2007,2007,8
Answering biological questions by querying k‐mer databases,Paul Greenfield; Uwe Röhm,SUMMARY This paper describes a k-mer approach to analysing DNA data and quicklyanswering certain types of ad hoc biological questions. These k-mers (short DNA strings)are stored in a conventional relational database and indexed to support efficient exact matchoperations. We show that k-mers around 20–25 bases long have interesting and usefuluniqueness properties that can be used to compute a 'relatedness' metric and also allow k-mers to be used as 'unique enough'tags to identify organisms and genes. This relatednessmetric is used in SQL queries that can directly answer questions such as how two relatedspecies differ; and what genes are unique to an organism. The k-mer tags have provenuseful in applications; largely metagenomic ones that can quickly process large volumes ofsequencing data to say something about what organisms and genes might be present in …,Concurrency and Computation: Practice and Experience,2013,7
Performance of Serializable Snapshot Isolation on Multicore Servers (TR693),Hyungsoo Jung; Hyuck Han; Alan Fekete; Uwe Röhm; Heon Y Yeom,*,Technical Report TR693,2012,7
PathBank: Web-Based Querying and Visualziation of an Integrated Biological Pathway Database,JWE Ho; Tristan Manwaring; S-H Hong; Uwe Röhm; David Cho Yau Fung; Kai Xu; Tim Kraska; David Hart,PathBank is a Web-based query and visualization system for biological pathways using anintegrated pathway database. To address the needs for biologists to visualize and analyzebiological pathways; PathBank is designed to be user-friendly; flexible and extensible. It is;to the best of our knowledge; the first Web-based system that allows biological pathways tobe visualized in three dimensions. PathBank demonstrates the ability to automaticallygenerate and layout biological pathways in response to Web-based database query aboutproteins; genes; gene ontology and small molecules. Using a novel OWL-to-relationaldatabase schema generation approach; it can automatically integrate biological data fromdifferent sources that support the BioPAX exchange format (eg KEGG; Bio-Cyc). Thesystems Web interface allows both simple keyword and complex query-based searches …,Computer Graphics; Imaging and Visualisation; 2006 International Conference on,2006,7
Genea: Schema-Aware Mapping of Ontologies into Relational Databases.,Tim Kraska; Uwe Röhm,Abstract Ontologies have become an important mechanism for describing and exchangingdata in the semantic web; as well as in application areas such as bioinformatics or healthcare. This paper addresses the problem of how to efficiently store and query ontologyinstance data using an RDBMS. Our approach automatically creates a schema-awarerelational representation of the ontology using generic mapping rules. Part of the ontologyreasoning; such as on subsumption relationships; is done during schema-generation andalso at load-time of the instance data; so that query processing becomes faster. Weimplemented our approach in an OWL-mapping tool; called Genea; that can automaticallycreate a compact relational schema and load the instance data from a given ontology writtenin OWL. We report on results from a quantitative and qualitative evaluation of our …,COMAD,2006,7
Online Analytical Processing with a Cluster of Databases,Uwe Röhm,Online analytical processing (OLAp) aims at analyzing the vast amoum of dara a companyor other organization accumulares duting its day-to-day business. The objective is to prepatestraregic decisions and to gain more insight imo the natute of the operations theorganization patticipares in. Corresponding applications range from online shopsrecommending populat products based on the browsing behavior of theit customers up toinrernational corporation trying to idemify new profirable matker segmems in theit customerrelationship darabase. Respective OLAp sysrems must cope with latge volumes of dara andat the same time must allow for short response times to facilirare inreractive usage. Theymust also be capable to scale; this means; to be easily exrensible with the incteasing daravolumes accumulared. Futthermore; the requitemem that the dara analyzed should be up …,*,2002,7
REST+ T: Scalable Transactions over HTTP,Akon Dey; Alan Fekete; Uwe Röhm,Restful APIs are widely adopted in designing components that are combined to form webinformation systems. The use of REST is growing with the inclusion of smart devices and theInternet of Things; within the scope of web information systems; along with large-scaledistributed NoSQL data stores and other web-based and cloud-hosted services. There is animportant subclass of web information systems and distributed applications which wouldbenefit from stronger transactional support; as typically found in traditional enterprisesystems. In this paper; we propose REST+ T (REST with Transactions); a transactionalRestful data access protocol and API that extends HTTP to provide multi-item transactionalaccess to data and state information across heterogeneous systems. We describe a casestudy called Tora; where we provide access through REST+ T to an existing key-value …,Cloud Engineering (IC2E); 2015 IEEE International Conference on,2015,6
Scalable serializable snapshot isolation for multicore systems,Hyuck Han; SeongJae Park; Hyungsoo Jung; Alan Fekete; Uwe Röhm; Heon Y Yeom,Since 1990's; Snapshot Isolation (SI) has been widely studied; and it was successfullydeployed in commercial and open-source database engines. Berenson et al. showed thatdata consistency can be violated under SI. Recently; a new class of Serializable SIalgorithms (SSI) has been proposed to achieve serializable execution while still allowingconcurrency between reads and updates.,2014 IEEE 30th International Conference on Data Engineering (ICDE),2014,6
Performance of Serializable Snapshot Isolation on Multicore Servers,Hyungsoo Jung; Hyuck Han; Alan Fekete; Uwe Röhm; Heon Y Yeom,Abstract Snapshot isolation (SI) is a widely studied concurrency control approach; with greatimpact in practice within platforms such as Oracle or SQL Server. Berenson et al. showedthough that SI does not guarantee serializable execution; in certain situations; dataconsistency can be violated through concurrency between correct applications. Recently;variants of SI have been proposed; that keep the key properties such as (often) allowingconcurrency between reads and updates; and that also guarantee that every execution willbe serializable. We have had the opportunity to use three implementations of two differentalgorithms of this type; all based on the InnoDB open source infrastructure. We measure theperformance attained by these implementations; on high-end hardware with a substantialnumber of cores. We explore the impact of the differences in algorithm; and also of the …,The 18th International Conference on Database Systems for Advanced Applications (DASFAA 2013),2013,6
Event processing middleware for wireless sensor networks,Selvadurai Selvakennedy; Uwe Röhm; Bernhard Scholz,With the growth in the computation capacity of sensor nodes; they are increasingly equippedto handle more complex functions. Moreover; the need to realize the complete loop of sense-control-actuate as the wired sensing facility; demands for more in-network processing. Tothis end; some primitives related to event data processing can be made available for event-driven applications. In this paper; we propose a middleware to provide efficient basic andhigher-level event processing services through collaborative processing. As concreteservice task instances; we propose an integrated algorithm that performs both event centerlocalization and area estimation. To the best of our knowledge; this is the first such attempt.Using edge detection as a core middleware module; it is demonstrated that the middlewareis able to support accurate estimation services for these event properties.,Parallel Processing Workshops; 2007. ICPPW 2007. International Conference on,2007,6
On the integration of data stream clustering into a query processor for wireless sensor networks,Uwe Röhm; Bernhard Scholz; Mohamed Medhat Gaber,We discuss the integration of on-line data stream clustering into a distributed queryprocessor for wireless sensor networks (WSNs). Our approach is to combine an adaptiveclustering algorithm with in-network data processing by introducing specialised queryoperators that implement stateful stream processing. We have implemented a testbed for anon-line clustering algorithm as part of a query processing system for the Sun SPOT sensornetwork platform. The paper discusses the design alternatives for continuous streamclustering in WSNs and for the integration of the resource-awareness to be able to traderesult accuracy for resource consumption.,Mobile Data Management; 2007 International Conference on,2007,6
An analytical study of central and in-network data processing for wireless sensor networks,Mohamed Medhat Gaber; Uwe Röhm; Karel Herink,Abstract This paper compares the performance of centralized and in-network dataprocessing for wireless sensor networks (WSNs) under various deployment conditions onthe real sensor hardware Sun SPOT from Sun Microsystems. We define several criteria tomeasure the quality of responses in WSN applications. Guided by an extensiveexperimental study; we discuss in detail the performance impacts of different deploymentfactors on algorithms that implement both centralized and in-network computing. Finally;performance guidelines are given to algorithm designers for WSN applications.,Information Processing Letters,2009,5
OLAP with a Database Cluster,Uwe Röhm,Abstract This chapter presents a new approach to online decision support systems that isscalable; fast; and capable of analysing up-to-date data. It is based on a database cluster: acluster of commercial off-the-shelf computers as hardware infrastructure and off-the-shelfdatabase management systems as transactional storage managers. We focus on centralarchitectural issues and on the performance implications of such a cluster-based decisionsupport system. In the first half; we present a scalable infrastructure and discuss physicaldata design alternatives for cluster-based online decision support systems. In the secondhalf of the chapter; we discuss query routing algorithms and freshness-aware scheduling.This protocol enables users to seamlessly decide how fresh the data analysed should be byallowing for different degrees of freshness of the online analytical processing (OLAP) …,*,2009,5
OLAP with a Database Cluster,Uwe Röhm,Abstract This chapter presents a new approach to online decision support systems that isscalable; fast; and capable of analysing up-to-date data. It is based on a database cluster: acluster of commercial off-the-shelf computers as hardware infrastructure and off-the-shelfdatabase management systems as transactional storage managers. We focus on centralarchitectural issues and on the performance implications of such a cluster-based decisionsupport system. In the first half; we present a scalable infrastructure and discuss physicaldata design alternatives for cluster-based online decision support systems. In the secondhalf of the chapter; we discuss query routing algorithms and freshness-aware scheduling.This protocol enables users to seamlessly decide how fresh the data analysed should be byallowing for different degrees of freshness of the online analytical processing (OLAP) …,Data Warehouses and OLAP: Concepts; Architectures and Solutions: Concepts; Architectures and Solutions,2006,5
A survey on data processing issues in wireless sensor networks for enterprise information infrastructure,Kotagiri Ramamohanarao; Lars Kulik; Selvakennedy Selvadurai; Bernhard Scholz; Uwe Röhm; Egemen Tanin; Anastasios Viglas; Albert Zomaya; Chris Leckie,Wireless sensor networks (WSNs) are a key technology for a broad range of applicationssuch as environmental and habitat monitoring; traffic control; health monitoring; supply-chainmanagement; smart homes; security; and surveillance systems [16]. A WSN is collection of alarge number of inexpensive devices capable of sensing certain physical phenomena;carrying out simple processing tasks; as well as communicating with each other usingwireless networking technology in an adhoc manner. As the evolution of sensors followsMoore's law; sensors become smaller; cheaper; and more powerful. This evolution enablesthe deployment of systems that can consist of hundreds or thousands of sensor nodes.Typically; sensor networks are deployed to gather physical information in a robust andautonomous manner. The data collection can be either continuous or selective; ie; detect …,EII Network Taskforce on Wireless Sensor Networks; Survey Technical Report,2006,5
EU-NSF Digital Library Working Group on Interoperability between Digital Libraries Position Paper,Bill Birmingham; Klemens Böhm; Vassilis Christophides; Sophie Cluet; Arturo Crespo; Daniel Kiskis; Barry Leiner; Uwe Roehm; Hans-Jörg Schek; Heiko Schuldt; Peter Weinstein,2 Abstract This paper contains the findings of the EU-NSF working group on interoperabilityin digital libraries. The objective of interoperability is to share digital library sources andservices. Interoperability occurs at different levels of abstraction. We identify these levels fora digital library environment; and we point out open questions corresponding to the differentlevels. Referring to this perspective as' vertical view'; we also come up with a'horizontalview'; which essentially is a reference architecture for digital libraries. We have classified thevarious research issues related to digital libraries; namely; the information model to be usedfor the global interoperability layer; aspects of coordination and control of the underlyingdata sources; transparent distributed information access; and appropriate implementationmechanisms; in terms of this reference architecture.,road map report from the DELOS project,1998,5
Scalable distributed transactions across heterogeneous stores,Akon Dey; Alan Fekete; Uwe Röhm,Typical cloud computing systems provide highly scalable and fault-tolerant data stores thatmay sacrifice other features like general multi-item transaction support. Recently techniquesto implement multi-item transactions in these types of systems have focused on transactionsacross homogeneous data stores. Since applications access data in heterogeneous storagesystems for legacy or interoperability reasons; we propose an approach that enables multi-item transactions with snapshot isolation across multiple heterogeneous data stores usingonly a minimal set of commonly implemented features such as single item consistency;conditional updates; and the ability to store additional meta-data. We define an client-coordinated transaction commitment protocol that does not rely on a central coordinatinginfrastructure. The application can take advantage of the scalability and fault-tolerance …,2015 IEEE 31st International Conference on Data Engineering,2015,4
Plug and play: Interoperability in concert,Lukas Relly; Uwe Röhm,Abstract In order to make database systems interoperate with systems beyond traditionalapplication areas a new paradigm called “exporting database functionality” as a radicaldeparture from traditional thinking has been proposed in research and development.Traditionally; all data are loaded into and owned by the database; whereas according to thenew paradigm data may reside outside the database in external repositories or archives.Nevertheless; database functionality; such as query processing and indexing; is providedexploiting interoperability of the DBMS with the external repositories. Obviously; there is anoverhead involved having the DBMS interoperate with external repositories instead of apriori loading all data into the DBMS. In this paper we discuss alternatives for interoperabilityat different levels of abstraction; and we report on evaluations performed using the …,Interoperating Geographic Information Systems,1999,4
Scalable Distributed Transactions across Heterogeneous Stores,Akon Dey; Alan Fekete; Uwe Röhm,Abstract Modern cloud computing systems usually provide a highly scalable and fault-tolerant data store that sacrifices other features. Often; these systems may not supporttransactions at all or else restrict transactions to one data item each. Recently techniques tosupport multi-item transactions in these types of systems have been successfully developedbut have focused on transactions across homogeneous data stores. However; applicationsoften need to store different data in different storage systems perhaps for legacy orinteroperability reasons. We propose an approach that enables multi-item transactionsacross multiple heterogeneous data stores using only a minimal set of commonlyimplemented features such as single item consistency; conditional updates; and the ability tostore additional meta-data. We define an client-coordinate transaction commitment …,IEEE 31th International Conference on Data Engineering; Seoul; ICDE,2014,2
Performance of program modification techniques that ensure serializable executions with snapshot isolation DBMS,Mohammad Alomari; Alan Fekete; Uwe Röhm,Abstract Snapshot Isolation (SI) is a multiversion concurrency control that has beenimplemented by several open source and commercial database systems (Oracle; MicrosoftSQL Server; and previous releases of PostgreSQL). The main feature of SI is that a readoperation does not block a write operation and vice versa; which allows higher degree ofconcurrency than traditional two-phase locking. SI prevents many anomalies that appear inother isolation levels; but it still can result in non-serializable executions; in which databaseintegrity constraints can be violated. Several techniques are known to modify the applicationcode based on preanalysis; in order to ensure that every execution is serializable onengines running SI. We introduce a new technique called External Lock Manager (ELM). Inusing a technique; there is a choice to make; of which pairs of transactions need to have …,Information Systems,2014,2
Towards serializable replication with snapshot isolation,Michael Cahill; A Fekete; U Röhm,ABSTRACT Replicated database systems necessarily deal with multiple versions of dataitems active concurrently across nodes in a replication group. As a consequence; there is anatural fit between replication and snapshot isolation (SI); which uses multiple versions ofdata within a single site to provide nonblocking read operations. However; snapshotisolation does not guarantee serializable execution for arbitrary applications. Recent theoryhas established necessary and sufficient conditions on applications under which SI doesguarantee serializability. This paper describes a novel replication algorithm using snapshotisolation within each node in a replication group to provide snapshot isolation toapplications using the group. Update transactions can be initiated at any node in the groupand a “master” node is transparently elected to detect conflicts between updating …,PhD Workshop collocated at VLDB Conference,2007,2
Performance of Program Modification Techniques that Ensure Serializable Executions with Snapshot Isolation DBMS,Mohammad Alomari; Alan Fekete; Uwe Röhm,Abstract. Snapshot Isolation (SI) is a multiversion concurrency control that has beenimplemented by several open source and commercial database systems (Oracle;PostgreSQL; Microsoft SQL Server). The main feature of SI is that a read operation does notblock a write operation and vice versa; which allows higher degree of concurrency thantraditional two-phase locking. SI prevents many anomalies that appear in other isolationlevels; but it still can result in non-serializable executions; in which database integrityconstraints can be violated. Several techniques are known to modify the application code inorder to ensure that every execution is serializable on engines running SI. Each modificationtechnique introduces some conflicts; and so prevents certain interleavings; without alteringthe functionality of each application program. In following one of the possible techniques …,*,2011,1
Data Provenance Support in Relational Databases for Stored Procedures,Winly Jurnawan; Uwe Röhm,Abstract The increasing amounts of data produced by automated scientific instrumentsrequire scalable data management platforms for storing; transforming and analyzingscientific data. At the same time; it is paramount for scientific applications to keep track of theprovenance information for quality control purposes and to be able to re-trace workflowsteps. Relational database systems are designed to efficiently manage and analyze largedata volumes; and modern extensible database systems can also host complex datatransformations as stored procedures. However; the relational model does not naturallysupport data provenance or lineage tracking. In this paper; we focus on providing dataprovenance management in relational databases for stored procedures. Our approach;called PSP; leverages the XML capabilities of SQL: 2003 to keep track of the lineage of …,Database Systems for Advanced Applications,2009,1
When serializability comes without cost,Mohammad Alomari; Michael Cahill; Alan Fekete; Uwe Röhm,It is usually expected that performance is reduced by using stricter concurrency control;which reduces the risk of anomalies that can lead to data corruption. For example; the weakisolation level Read Committed allows anomalies that are prevented by Two-Phase Locking(abbreviated 2PL); and because 2PL holds locks for longer than RC; it has lower throughput.We show that sometimes; guaranteed correctness can be obtained along with betterthroughput than RC; by use of the multiversion Snapshot Isolation mechanism along withmodifications to application programs as proposed by Fekete et al. We investigate theconditions under which this effect occurs.,Computer Systems and Applications; 2008. AICCSA 2008. IEEE/ACS International Conference on,2008,1
The Efficacy of Commutativity-Based Semantic Locking in Real-World Applications,Paul Wu; Alan Fekete; Uwe Röhm,Abstract—While the dominant approach to persistent storage in practice is to use a relationalDBMS; there are some specialist applications that rely on object stores. The performance ofthese applications depends heavily on the efficiency of the object store's concurrency controlmechanism. Today's predominant concurrency control mechanism is strict two-phase objectlocking. In the 80s; an interesting alternative was developed: commutativity-based semanticlocking. In theory; it can outperform traditional locking schemes in certain scenarios withappropriate commutativity potential. However; such scenarios are not easily identifiable;hence semantic locking never became popular. In this paper; we study the real-worldperformance potential of different locking strategies in an industrial application from thetelecommunications sector. We compare object-based locking; field-based locking; and …,*,2007,1
A Service-oriented Approach for the Parallelization of Data-intensive Algorithms in a Grid-enabled Cluster,Chun-Wu Chen; Uwe Röhm,We investigate how clusters and grid computing can be combined with a service-orientedarchitecture. An important application is the parallelization of dataintensive algorithms asthey are common in life sciences; such as the sequence alignment problem. We developeda prototype of a service-oriented parallel Basic Local Alignment Search Tool (BLAST)[1].Using a standard grid middleware; the Globus Toolkit [19]; we have distributed data andlogic over several cluster nodes; all of which have access to a shared database. This allowsus to parallelize BLAST by combining both functional and domain decomposition. In anexperimental performance evaluation; we investigate the scalability and performance of thedeveloped BLAST service. Our results show that dataintensive algorithms can be effectivelyparallelized using a service-oriented approach; offering linear scalability. At the same …,Data Engineering Workshops; 2005. 21st International Conference on,2005,1
PBrowse: A web-based platform for real-time collaborative exploration of genomic data,Peter S Szot; Andrian Yang; Xin Wang; Chirag Parsania; Uwe Röhm; Koon Ho Wong; Joshua WK Ho,Abstract Genome browsers are widely used for individually exploring various types ofgenomic data. A handful of genome browsers offer limited tools for collaboration amongmultiple users. Here; we describe PBrowse; an integrated real-time collaborative genomebrowser that enables multiple users to simultaneously view and access genomic data;thereby harnessing the wisdom of the crowd. PBrowse is based on the Dalliance genomebrowser and has a re-designed user and data management system with novel collaborativefunctionalities; including real-time collaborative view; track comment and an integratedgroup chat feature. Through the Distributed Annotation Server protocol; PBrowse can easilyaccess a wide range of publicly available genomic data; such as the ENCODE data sets. Weargue that PBrowse represents a paradigm shift from using a genome browser as a static …,bioRxiv,2016,*
Consistent Freshness-Aware Caching for Multi-Object Requests,Meena Rajani; Uwe Röhm; Akon Dey,Abstract Dynamic websites rely on caching and clustering to achieve high performance andscalability. While queries benefit from middle-tier caching; updates introduce a distributedcache consistency problem. One promising approach to solving this problem is Freshness-Aware Caching (FAC): FAC tracks the freshness of cached data and allows clients toexplicitly trade freshness of data for response times. The original protocol was limited tosingle-object lookups and could only handle complex requests if all requested objects hadbeen loaded into the cache at the same time. In this paper we describe the Multi-ObjectFreshness-Aware Caching (MOFAC) algorithm; an extension of FAC that provides aconsistent snapshot of multiple cached objects even if they are loaded and updated atdifferent points of time. This is done by keeping track of their group valid interval; as …,*,2014,*
Robust Snapshot Replication,Uwe Röhm; Michael J. Cahill; Alan Fekete; Hyungsoo Jung; Seung Woo Baek; Mathew Rodley,Abstract An important technique to ensure the scalability and availability of clusteredcomputer systems is data replication. This paper describes a new approach to datareplication management called Robust Snapshot Replication. It combines an updateanywhere approach (so updates can be evaluated on any replica; spreading their load) withlazy update propagation and snapshot isolation concurrency control. The innovation is howwe employ snapshot isolation in the replicas to provide consistency; fail safety; and also toachieve high scalability for both readers and up-daters; by a system design withoutmiddleware or group communication infrastructure. We implemented our approach using thePostgreSQL database system and conducted an extensive experimental evaluation with asmall database cluster of 8 nodes. Our results demonstrate the scalability of our algorithm …,24th Australasian Database Conference (ADC2013),2013,*
Proceedings of the VLDB Endowment Volume 4 Issue 5,José Blakeley; Joseph M Hellerstein; Nick Koudas; Wolfgang Lehner; Sunita Sarawagi; Uwe Röhm,@article{1952377; author = {Parameswaran; Aditya and Sarma; Anish Das and Garcia-Molina;Hector and Polyzotis; Neoklis and Widom; Jennifer}; title = {Human-assisted graph search:it's okay to ask questions}; journal = {Proc. VLDB Endow.}; volume = {4}; number = {5}; year ={2011}; issn = {2150-8097}; pages = {267--278}; publisher = {VLDB Endowment}; }@article{1952378; author = {Yakout; Mohamed and Elmagarmid; Ahmed K. and Neville;Jennifer and Ouzzani; Mourad and Ilyas; Ihab F.}; title = {Guided data repair}; journal = {Proc.VLDB Endow.}; volume = {4}; number = {5}; year = {2011}; issn = {2150-8097}; pages ={279--289}; publisher = {VLDB Endowment}; } @article{1952379; author = {Venetis; Petrosand Gonzalez; Hector and Jensen; Christian S. and Halevy; Alon}; title = {Hyper-local;directions-based ranking of places}; journal = {Proc …,*,2011,*
Report on the Commercialization of Persistence Technology (RT4R1),MP Atkinson; GNC Kirby; R Morrison; U Röhm; D Sjöberg; HJ Schek; S Gamerman; A Gawecki; M Jordan; B Mathiske; D Plateau; S Stamer,The success of persistence technology transfer projects critically depends not only on thechoice of the best technology available but more importantly also on successful businesstactics and strategies. Therefore; PASTEL established a subgroup which exchangedexperience and knowhow related to these non-technical issues; tried to identify pitfalls andbusiness opportunities; and asked for feedback on this problem from establishedcommercial players outside of the working group.,*,1998,*
EU-NSF Working Group on Interoperability between Digital Libraries,Daniel L. Kiskis; Uwe Röhm,*,ERCIM News,1998,*
Building a Federation of Heterogeneous (Non-Database) Repositories,Markus Tresch; Uwe Röhm,*,The Second Workshop Föderierte Datenbanken,1996,*
