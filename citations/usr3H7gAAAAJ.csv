The XML web: a first study,Laurent Mignet; Denilson Barbosa; Pierangelo Veltri,Abstract Although originally designed for large-scale electronic publishing; XML plays anincreasingly important role in the exchange of data on the Web. In fact; it is expected thatXML will become the lingua franca of the Web; eventually replacing HTML. Not surprisingly;there has been a great deal of interest on XML both in industry and in academia.Nevertheless; to date no comprehensive study on the XML Web (ie; the subset of the Webmade of XML documents only) nor on its contents has been made. This paper is the firstattempt at describing the XML Web and the documents contained in it. Our results are drawnfrom a sample of a repository of the publicly available XML documents on the Web;consisting of about 200;000 documents. Our results show that; despite its short history; XMLalready permeates the Web; both in terms of generic domains and geographically. Also …,Proceedings of the 12th international conference on World Wide Web,2003,178
ToXgene: a template-based data generator for XML,Denilson Barbosa; Alberto Mendelzon; John Keenleyside; Kelly Lyons,Synthetic collections of XML documents can be useful in many applications; such asbenchmarking (eg; Xmark [4]; XOO7 [2]) and algorithm testing and evaluation. We presentToXgene; a template-based tool for facilitating the generation of large; consistent collectionsof synthetic XML documents. ToXgene was designed with the following requirements inmind: it should be declarative; to speed the data generation up; it should be general enoughto generate fairly complex XML content and it should be powerful enough to capture themost common kinds of constraints in popular benchmarks. Preliminary experimental resultsshow that our tool can closely reproduce the data sets for the Xmark and the TPC-Hbenchmarks [6].,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,130
Efficient incremental validation of XML documents,Denilson Barbosa; Alberto O Mendelzon; Leonid Libkin; Laurent Mignet; Marcelo Arenas,We discuss incremental validation of XML documents with respect to DTDs and XMLschema definitions. We consider insertions and deletions of subtrees; as opposed to leafnodes only; and we also consider the validation of ID and IDREF attributes. For arbitraryschemas; we give a worst-case n log n time and linear space algorithm; and show that itoften is far superior to revalidation from scratch. We present two classes of schemas; whichcapture most real-life DTDs; and show that they admit a logarithmic time incrementalvalidation algorithm that; in many cases; requires only constant auxiliary space. We thendiscuss an implementation of these algorithms that is independent of; and can becustomized for different storage mechanisms for XML. Finally; we present extensiveexperimental results showing that our approach is highly efficient and scalable.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,95
ToXgene: An extensible template-based data generator for XML,Denilson Barbosa; Alberto O Mendelzon; John Keenleyside; Kelly Lyons,Abstract. Synthetic collections of XML documents are useful in many applications; such asbenchmarking (eg; Xmark); and algorithm testing and evaluation. We present ToXgene; atemplate-based generator for large; consistent collections of synthetic XML documents.Templates are annotated XML Schema specifications describing both the structure and thecontent of the data to be generated. Our tool was designed to be declarative; and generalenough to generate complex XML content and to capture most common requirements; suchas those embodied in current benchmarks. In the paper; we give an overview of theToXgene template specification language and the extensibility of our tool; we also reportpreliminary experiments with ToXgene carried out at the IBM Toronto Lab; which show thatour tool can closely reproduce the data sets of the TPC-H and Xmark benchmarks.,In WebDB,2002,93
Studying the XML Web: gathering statistics from an XML sample,Denilson Barbosa; Laurent Mignet; Pierangelo Veltri,Abstract XML has emerged as the language for exchanging data on the web and hasattracted considerable interest both in industry and in academia. Nevertheless; to date; littleis known about the XML documents published on the web. This paper presents acomprehensive analysis of a sample of about 200;000 XML documents on the web; and isthe first study of its kind. We study the distribution of XML documents across the web inseveral ways; moreover; we provided a detailed characterization of the structure of real XMLdocuments. Our results provide valuable input to the design of algorithms; tools and systemsthat use XML in one form or another.,World Wide Web,2005,74
Designing information-preserving mapping schemes for XML,Denilson Barbosa; Juliana Freire; Alberto O Mendelzon,Abstract An XML-to-relational mapping scheme consists of a procedure for shreddingdocuments into relational databases; a procedure for publishing databases back asdocuments; and a set of constraints the databases must satisfy. In previous work; we definedtwo notions of information preservation for mapping schemes: losslessness; whichguarantees that any document can be reconstructed from its corresponding database; andvalidation; which requires every legal database to correspond to a valid document. We alsodescribed one information-preserving mapping scheme; called Edge++; and showed that;under reasonable assumptions; losslessness and validation are both undecidable. Thisleads to the question we study in this paper: how to design mapping schemes that areinformation-preserving. We propose to do it by starting with a scheme known to be …,Proceedings of the 31st international conference on Very large data bases,2005,68
ToX-the Toronto XML Engine.,Denilson Barbosa; Attila Barta; Alberto O Mendelzon; George A Mihaila; Flavio Rizzolo; Patricia Rodriguez-Gianolli,We present ToX–the Toronto XML Engine–a repository for XML data and metadata; whichsupports real and virtual XML documents. Real documents are stored as files or mappedinto relational or object databases; depending on their structuredness; indices are definedaccording to the storage method used. Virtual documents can be remote documents; definedas arbitrary WebOQL queries; or views; defined as queries over documents registered in thesystem. The system catalog contains metadata for the documents; especially their schemata;used for query processing and optimization. Queries can range over both the catalog andthe documents; and multiple query languages are supported. In this paper we describe thearchitecture and main of ToX; we present our indexing and storage strategies; including twonovel techniques; and we discuss our query processing strategy. The project started …,Workshop on Information Integration on the Web,2001,53
Effectiveness and efficiency of open relation extraction,Filipe Mesquita; Jordan Schmidek; Denilson Barbosa,Abstract A large number of Open Relation Extraction approaches have been proposedrecently; covering a wide range of NLP machinery; from “shallow”(eg; part-of-speechtagging) to “deep”(eg; semantic role labeling–SRL). A natural question then is what is thetradeoff between NLP depth (and associated computational cost) versus effectiveness. Thispaper presents a fair and objective experimental comparison of 8 state-of-the-artapproaches over 5 different datasets; and sheds some light on the issue. The paper alsodescribes a novel method; EXEMPLAR; which adapts ideas from SRL to less costly NLPmachinery; resulting in substantial gains both in efficiency and effectiveness; over binaryand n-ary relation extraction tasks.,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,2013,44
Robust entity linking via random walks,Zhaochen Guo; Denilson Barbosa,Abstract Entity Linking is the task of assigning entities from a Knowledge Base to textualmentions of such entities in a document. State-of-the-art approaches rely on lexical andstatistical features which are abundant for popular entities but sparse for unpopular ones;resulting in a clear bias towards popular entities and poor accuracy for less popular ones. Inthis work; we present a novel approach that is guided by a natural notion of semanticsimilarity which is less amenable to such bias. We adopt a unified semantic representationfor entities and documents-the probability distribution obtained from a random walk on asubgraph of the knowledge base-which can overcome the feature sparsity issue that affectsprevious work. Our algorithm continuously updates the semantic signature of the documentas mentions are disambiguated; thus focusing the search based on context. Our …,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,42
Open information extraction with tree kernels,Ying Xu; Mi-Young Kim; Kevin Quinn; Randy Goebel; Denilson Barbosa,Abstract Traditional relation extraction seeks to identify pre-specified semantic relationswithin natural language text; while open Information Extraction (Open IE) takes a moregeneral approach; and looks for a variety of relations without restriction to a fixed relationset. With this generalization comes the question; what is a relation? For example; should themore general task be restricted to relations mediated by verbs; nouns; or both? To helpanswer this question; we propose two levels of subtasks for Open IE. One task is todetermine if a sentence potentially contains a relation between two entities? The other tasklooks to confirm explicit relation words for two entities. We propose multiple SVM modelswith dependency tree kernels for both tasks. For explicit relation extraction; our system canextract both noun and verb relations. Our results on three datasets show that our system …,Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2013,40
Identifying controversial articles in Wikipedia: A comparative study,Hoda Sepehri Rad; Denilson Barbosa,Abstract Wikipedia articles are the result of the collaborative editing of a diverse group ofanonymous volunteer editors; who are passionate and knowledgeable about specific topics.One can argue that this plurality of perspectives leads to broader coverage of the topic; thusbenefitting the reader. On the other hand; differences among editors on polarizing topics canlead to controversial or questionable content; where facts and arguments are presented anddiscussed to support a particular point of view. Controversial articles are manually tagged byWikipedia editors; and span many interesting and popular topics; such as religion; history;and politics; to name a few. Recent works have been proposed on automatically identifyingcontroversy within unmarked articles. However; to date; no systematic comparison of theseefforts has been made. This is in part because the various methods are evaluated using …,Proceedings of the eighth annual international symposium on wikis and open collaboration,2012,40
TASM: Top-k approximate subtree matching,Nikolaus Augsten; Denilson Barbosa; Michael Böhlen; Themis Palpanas,We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k bestmatches of a small query tree; eg; a DBLP article with 15 nodes; in a large document tree;eg; DBLP with 26M nodes; using the canonical tree edit distance as a similarity measurebetween subtrees. Evaluating the tree edit distance for large XML trees is difficult: the bestknown algorithms have cubic runtime and quadratic space complexity; and; thus; do notscale. Our solution is TASM-postorder; a memory-efficient and scalable TASM algorithm. Weprove an upper-bound for the maximum subtree size for which the tree edit distance needsto be evaluated. The upper bound depends on the query and is independent of thedocument size and structure. A core problem is to efficiently prune subtrees that are abovethis size threshold. We develop an algorithm based on the prefix ring buffer that allows us …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,34
Identification of speakers in novels,Hua He; Denilson Barbosa; Grzegorz Kondrak,Abstract Speaker identification is the task of attributing utterances to characters in a literarynarrative. It is challenging to automate because the speakers of the majority of utterancesare not explicitly identified in novels. In this paper; we present a supervised machinelearning approach for the task that incorporates several novel features. The experimentalresults show that our method is more accurate and general than previous approaches to theproblem.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2013,33
Topic classification of blog posts using distant supervision,Stephanie D Husby; Denilson Barbosa,Abstract Classifying blog posts by topics is useful for applications such as search andmarketing. However; topic classification is time consuming and error prone; especially in anopen domain such as the blogosphere. The state-of-the-art relies on supervised methods;requiring considerable training effort; that use the whole corpus vocabulary as features;demanding considerable memory to process. We show an effective alternative wherebydistant supervision is used to obtain training data: we use Wikipedia articles labelled withFreebase domains. We address the memory requirements by using only named entities asfeatures. We test our classifier on a sample of blog posts; and report up to 0.69 accuracy formulti-class labelling and 0.9 for binary classification.,Proceedings of the Workshop on Semantic Analysis in Social Media,2012,30
Adaptive record extraction from web pages,Justin Park; Denilson Barbosa,Abstract We describe an adaptive method for extracting records from web pages. Ouralgorithm combines a weighted tree matching metric with clustering for obtaining dataextraction patterns. We compare our method experimentally to the state-of-the-art; and showthat our approach is very competitive for rigidly-structured records (such as productdescriptions) and far superior for loosely-structured records (such as entrieson blogs).,Proceedings of the 16th international conference on World Wide Web,2007,28
Information preservation in XML-to-relational mappings,Denilson Barbosa; Juliana Freire; Alberto O Mendelzon,Abstract We study the problem of storing XML documents using relational mappings. Wepropose a formalization of classes of mapping schemes based on the languages used fordefining functions that assign relational databases to XML documents and vice-versa. Wealso discuss notions of information preservation for mapping schemes; we define losslessmapping schemes as those that preserve the structure and content of the documents; andvalidating mapping schemes as those in which valid documents can be mapped into legaldatabases; and all legal databases are (equivalent to) mappings of valid documents. Wedefine one natural class of mapping schemes that captures all mappings in the literature;and show negative results for testing whether such mappings are lossless or validating.Finally; we propose a lossless and validating mapping scheme; and show that it performs …,International XML Database Symposium,2004,27
Parallelizing MPEG video encoding using multiprocessors,Denilson M Barbosa; Joao Paulo Kitajima; W Weira,Many computer applications are currently using digital video. Recent advances in digitalimaging and faster networking infrastructure made this technology very popular; not only forentertainment; but also in health and education applications. However; high quality videorequires more storage space and communication bandwidth than traditional data. To dealwith this problem; most of the digital video encoding techniques use a compression scheme.The MPEG committee has defined widely used standards for digital video encodingproviding high quality images and high compression rates. However; real-time MPEGencoding also demands high computational power; usually far beyond traditional sequentialcomputers can provide. Fortunately; the algorithms compliant to the MPEG standard can beparallelized. In this work; we propose a novel and simple shared-memory parallel …,Computer Graphics and Image Processing; 1999. Proceedings. XII Brazilian Symposium on,1999,27
Goals and benchmarks for autonomic configuration recommenders,Mariano P Consens; Denilson Barbosa; Adrian Teisanu; Laurent Mignet,Abstract We are witnessing an explosive increase in the complexity of the informationsystems we rely upon; Autonomic systems address this challenge by continuouslyconfiguring and tuning themselves. Recently; a number of autonomic features have beenincorporated into commercial RDBMS; tools for recommending database configurations (ie;indexes; materialized views; partitions) for a given workload are prominent examples of thispromising trend. In this paper; we introduce a flexible characterization of the performancegoals of configuration recommenders and develop an experimental evaluation approach tobenchmark the effectiveness of these autonomic tools. We focus on exploratory queries andpresent extensive experimental results using both real and synthetic data that demonstratethe validity of the approach introduced. Our results identify a specific index configuration …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,26
Leveraging editor collaboration patterns in Wikipedia,Hoda Sepehri Rad; Aibek Makazhanov; Davood Rafiei; Denilson Barbosa,Abstract Predicting the positive or negative attitude of individuals towards each other in asocial environment has long been of interest; with applications in many domains. Weinvestigate this problem in the context of the collaborative editing of articles in Wikipedia;showing that there is enough information in the edit history of the articles that can be utilizedfor predicting the attitude of co-editors. We train a model using a distant supervisionapproach; by labeling interactions between editors as positive or negative depending onhow these editors vote for each other in Wikipedia admin elections. We use the model topredict the attitude among other editors; who have neither run nor voted in an election. Wevalidate our model by assessing its accuracy in the tasks of predicting the results of theactual elections; and identifying controversial articles. Our analysis reveals that the …,Proceedings of the 23rd ACM conference on Hypertext and social media,2012,25
Studying the xml web: Gathering statistics from an xml sample,Denilson Barbosa; Laurent Mignet; Pierangelo Veltri,World Wide Web (2006) 9:187–212 DOI 10.1007/s11280-006-8437-6 ERRATUM … Studyingthe XML Web: Gathering Statistics from an XML Sample … C o Springer Science + BusinessMedia; LLC 2006 … The uncorrected proof of this paper was mistakenly published in volume8; number 4 of this journal; as well as online. The corrected version follows. This was a typesetter'serror … The online version of the original article can be found at http://dx.doi.org/10.1007/s11280-005-1544-y,World Wide Web,2006,24
Extracting information networks from the blogosphere,Yuval Merhav; Filipe Mesquita; Denilson Barbosa; Wai Gen Yee; Ophir Frieder,Abstract We study the problem of automatically extracting information networks formed byrecognizable entities as well as relations among them from social media sites. Our approachconsists of using state-of-the-art natural language processing tools to identify entities andextract sentences that relate such entities; followed by using text-clustering algorithms toidentify the relations within the information network. We propose a new term-weightingscheme that significantly improves on the state-of-the-art in the task of relation extraction;both when used in conjunction with the standard tf &cdot; idf scheme and also when used asa pruning filter. We describe an effective method for identifying benchmarks for openinformation extraction that relies on a curated online database that is comparable to thehand-crafted evaluation datasets in the literature. From this benchmark; we derive a much …,ACM Transactions on the Web (TWEB),2012,21
Extracting information networks from the blogosphere: State-of-the-art and challenges,Filipe Mesquita; Yuval Merhav; Denilson Barbosa,Abstract We study the problem of automatically extracting relational networks ofrecognizable entities from the blogosphere. We describe a highly parallel and efficientsystem capable of processing millions of blog posts in a few hours; and experiments basedon state-of-the-art techniques for the extraction and identification of entities and relations.From these; we devise a tuned approach that achieves substantial precision and recall atthe task at hand. These results are indicate that effective large-scale extraction of suchnetworks from the blogosphere is possible.,Proceedings of the Fourth AAAI Conference on Weblogs and Social Media (ICWSM); Data Challenge Workshop,2010,21
Efficient incremental validation of XML documents after composite updates,Denilson Barbosa; Gregory Leighton; Andrew Smith,Abstract We describe an efficient method for the incremental validation of XML documentsafter composite updates. We introduce the class of Bounded-Edit (BE) DTDs and XMLSchemas; and give a simple incremental revalidation algorithm that yields optimalperformance for them; in the sense that its time complexity is linear in the number ofoperations in the update. We give extensive experimental results showing that our algorithmexhibits excellent scalability. Finally; we provide a statistical analysis of over 250 DTDs andXML Schema specifications found on the Web; showing that over 99% of them are in fact inBE.,International XML Database Symposium,2006,20
Relationship queries on extended knowledge graphs,Mohamed Yahya; Denilson Barbosa; Klaus Berberich; Qiuyue Wang; Gerhard Weikum,Abstract Entity search over text corpora is not geared for relationship queries where answersare tuples of related entities and where a query often requires joining cues from multipledocuments. With large knowledge graphs; structured querying on their relational facts is analternative; but often suffers from poor recall because of mismatches between user queriesand the knowledge graph or because of weakly populated relations. This paper presents theTriniT search engine for querying and ranking on extended knowledge graphs that combinerelational facts with textual web contents. Our query language is designed on the paradigmof SPO triple patterns; but is more expressive; supporting textual phrases for each of theSPO arguments. We present a model for automatic query relaxation to compensate formismatches between the data and a user's query. Query answers--tuples of entities--are …,Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,2016,18
The actortopic model for extracting social networks in literary narrative,Asli Celikyilmaz; Dilek Hakkani-Tur; Hua He; Greg Kondrak; Denilson Barbosa,Abstract We present a generative model for conversational dialogues; namely the actortopicmodel (ACTM); that extend the author-topic model (Rosen-Zvi; et. al; 2004) to identify actorsof given conversation in literary narratives. Thus ACTM assigns each instance of quotedspeech to an appropriate character. We model dialogues in a literary text; which take placebetween two or more actors conversing on different topics; as distributions over topics; whichare also mixtures of the term distributions associated with multiple actors. This follows thelinguistic intuition that rich contextual information can be useful in understanding dialogues;eventually effecting the social network construction. We propose ACTM to ideally lead ourresearch on social network extraction in literary narratives. Our experiments on nineteenthcentury English novels indicate that exploiting content structure of dialogues can yield …,NIPS Workshop: Machine Learning for Social Computing,2010,17
Efficient top-k approximate subtree matching in small memory,Nikolaus Augsten; Denilson Barbosa; Michael Bohlen; Themis Palpanas,We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k bestmatches of a small query tree within a large document tree using the canonical tree editdistance as a similarity measure between subtrees. Evaluating the tree edit distance forlarge XML trees is difficult: the best known algorithms have cubic runtime and quadraticspace complexity; and; thus; do not scale. Our solution is TASM-postorder; a memory-efficient and scalable TASM algorithm. We prove an upper bound for the maximum subtreesize for which the tree edit distance needs to be evaluated. The upper bound depends onthe query and is independent of the document size and structure. A core problem is toefficiently prune subtrees that are above this size threshold. We develop an algorithm basedon the prefix ring buffer that allows us to prune all subtrees above the threshold in a …,IEEE Transactions on Knowledge and Data Engineering,2011,15
Declarative generation of synthetic XML data,Denilson Barbosa; Alberto O Mendelzon,Abstract Synthetic data can be extremely useful in testing and evaluating algorithms; toolsand systems. Most synthetic data generators available today are the result of individualbenchmarking efforts. Typically; these are complex programs in which the specifications ofboth the structure and the contents of the data are hard-coded. As a result; it is often difficultto customize these tools for producing synthetic data tailored for specific needs. In thisarticle; we describe the ToXgene synthetic data generator; which is a declarative tool forgenerating realistic XML data for benchmarking as well as testing purposes. We present ourtemplate specification language; which consists of augmenting XML Schema withprobabilistic models that guide the data-generation process. We discuss the architecture ofour current implementation and we argue about ToXgene's usefulness by discussing …,Software: Practice and Experience,2006,15
Knowledge Base Augmentation using Tabular Data.,Yoones A Sekhavat; Francesco Di Paolo; Denilson Barbosa; Paolo Merialdo,ABSTRACT Large linked data repositories have been built by leveraging semi-structureddata in Wikipedia (eg; DBpedia) and through extracting information from natural languagetext (eg; YAGO). However; the Web contains many other vast sources of linked data; such asstructured HTML tables and spreadsheets. Often; the semantics in such tables is hidden;preventing one from extracting triples from them directly. This paper describes a probabilisticmethod that augments an existing knowledge base with facts from tabular data byleveraging a Web text corpus and natural language patterns associated with relations in theknowledge base. A preliminary evaluation shows high potential for this technique inaugmenting linked data repositories.,LDOW,2014,14
Prequery discovery of domain-specific query forms: A survey,Mauricio C Moraes; Carlos A Heuser; Viviane P Moreira; Denilson Barbosa,The discovery of HTML query forms is one of the main challenges in Deep Web crawling.Automatic solutions for this problem perform two main tasks. The first is locating HTML formson the Web; which is done through the use of traditional/focused crawlers. The second isidentifying which of these forms are indeed meant for querying; which also typically involvesdetermining a domain for the underlying data source (and thus for the form as well). Thisproblem has attracted a great deal of interest; resulting in a long list of algorithms andtechniques. Some methods submit requests through the forms and then analyze the dataretrieved in response; typically requiring a great deal of knowledge about the domain as wellas semantic processing. Others do not employ form submission; to avoid such difficulties;although some techniques rely to some extent on semantics and domain knowledge. This …,IEEE Transactions on Knowledge and Data Engineering,2013,12
An environment for building; exploring and querying academic social networks,Veselin Ganev; Zhaochen Guo; Diego Serrano; Brendan Tansey; Denilson Barbosa; Eleni Stroulia,Abstract Social network analysis aims at uncovering and understanding the structures andpatterns resulting from social interactions among individuals and organizations engaged ina common activity. Since the early days of the field; networks are modeled as graphsmodeling social actors and the relations between them. The field has become very activewith the maturity of computational machinery to handle large-scale graphs; and; morerecently; the automated gathering of social data. We introduce ReaSoN: a comprehensiveset of tools for visualizing and exploring social networks resulting from academic research.In doing so; ReaSoN contributes to the understanding as well as fostering of the socialnetworks underlying academic research. We describe the infrastructure; visualizations andanalysis provided in our system; as well as the process of extracting the social networks …,Proceedings of the International Conference on Management of Emergent Digital EcoSystems,2009,12
Labeling data extracted from the web,Altigran S Da Silva; Denilson Barbosa; Joao MB Cavalcanti; Marco AS Sevalho,Abstract We consider finding descriptive labels for anonymous; structured datasets; such asthose produced by state-of-the-art Web wrappers. We give a probabilistic model to estimatethe affinity between attributes and labels; and describe a method that uses a Web searchengine to populate the model. We discuss a method for finding good candidate labels forunlabeled datasets. Ours is the first unsupervised labeling method that does not rely onmining the HTML pages containing the data. Experimental results with data from 8 differentdomains show that our methods achieve high accuracy even with very few search engineaccesses.,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2007,12
Finding id attributes in XML documents,Denilson Barbosa; Alberto Mendelzon,XML documents may be accompanied by schemas that specify their structure; and impose restrictionson the values of some elements or attributes. The WWW Consortium has defined two schemaformalisms for XML: Document Type Def- initions (DTDs) [13] and XML Schema [14]. Althoughthese languages differ in several aspects; both allow the specification of ID; IDREF and IDREFSat- tributes. ID attributes are unique identifiers for the elements that bear them; IDREF attributesare logical pointers to ID attributes; IDREFS attributes are pointers to sets of ID attributes.IDREF(S) attributes establish references among elements in the document; turning the XML treesinto graphs. XML query lan- guages have ways of “navigating” these graphs seamlessly via treeedges and reference edges (see [1]). In a sense; ID attributes serve as keys for the elements;while IDREF(S) attributes serve as foreign keys. However; as we shall see; there are …,International XML Database Symposium,2003,10
Sociql: A query language for the socialweb,Diego Serrano; Eleni Stroulia; Denilson Barbosa; Victor Guana,Abstract Social-networking sites are becoming increasingly popular with users of all ages.With much of our social activity happening online; these sites are now becoming the subjectof scholarly study and research. Unfortunately; despite the fact that they collect similarcontent and support similar relations and activities; the current generation of these sites arehard to query programmatically; offering limited views of their data; effectively becomingdisconnected islands of information. We describe SociQL; a high-level query language; anda corresponding service; to which social-networking sites can subscribe; that supports theintegrated representation; querying and exploration of disparate social networks. Unlikegeneric web query languages; SociQL is designed specifically to support the integration ofnetworks through a common information model for the purpose of examining sociological …,*,2012,9
Towards identifying arguments in Wikipedia pages,Hoda Sepehri Rad; Denilson Barbosa,Abstract Wikipedia is one of the most widely used repositories of human knowledge today;contributed mostly by a few hundred thousand regular editors. In this open environment;inevitably; differences of opinion arise among editors of the same article. Especially forpolemical topics such as religion and politics; difference of opinions among editors may leadto intense" edit wars" in which editors compete to have their opinions and points of viewaccepted. While such disputes can compromise the reliability of the article (or at leastportions of it); they are recorded in the edit history of the articles. We posit that exposing suchdisputes to the reader; and pointing to the portions of the text where they manifest mostprominently can be beneficial in helping concerned readers in understanding such topics. Inthis paper; we discuss our initial efforts towards the problem of automatic evaluation of …,Proceedings of the 20th international conference companion on World wide web,2011,9
Improving Open Relation Extraction via Sentence Re-Structuring.,Jordan Schmidek; Denilson Barbosa,Abstract Information Extraction is an important task in Natural Language Processing;consisting of finding a structured representation for the information expressed in naturallanguage text. Two key steps in information extraction are identifying the entities mentionedin the text; and the relations among those entities. In the context of Information Extraction forthe World Wide Web; unsupervised relation extraction methods; also called Open RelationExtraction (ORE) systems; have become prevalent; due to their effectiveness without domain-specific training data. In general; these systems exploit part-of-speech tags or semanticinformation from the sentences to determine whether or not a relation exists; and if so; itspredicate. This paper discusses some of the issues that arise when even moderatelycomplex sentences are fed into ORE systems. A process for re-structuring such sentences …,LREC,2014,8
Entity linking with a unified semantic representation,Zhaochen Guo; Denilson Barbosa,Abstract Entity Linking (EL) consists in linking mentions in a document to their referententities in a Knowledge Base. Current approaches fall into two main categories: localapproaches; in which mentions are linked independently of each other; and globalapproaches; in which all mentions are linked collectively. Local approaches often ignore thesemantic relatedness of entities; and while global approaches incorporate the semanticrelatedness; they tend to focus only on directly connected entities; ignoring indirectconnections which might be useful. We present a global EL approach that unifies therepresentation of the semantics of entities and documents--the probability distribution ofentities being visited during a random walk on an entity graph--that accounts for direct andindirect connections. An experimental evaluation shows that our method outperforms five …,Proceedings of the 23rd International Conference on World Wide Web,2014,8
Shallow information extraction for the knowledge web,Denilson Barbosa; Haixun Wang; Cong Yu,A new breed of Information Extraction tools has become popular and shown to be veryeffective in building massive-scale knowledge bases that fuel applications such as questionanswering and semantic search. These approaches rely on Web-scale probabilistic modelspopulated through shallow language processing of the text; pre-existing knowledge; andstructured data already on the Web. This tutorial provides an introduction to thesetechniques; starting from the foundations of information extraction; and covering some of itskey applications.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,8
Analyzing natural-language artifacts of the software process,Maryam Hasan; Eleni Stroulia; Denilson Barbosa; Manar Alalfi,Software teams; as they communicate throughout the life-cycle of their projects; generate asubstantial stream of textual data. Through emails and chats; developers discuss therequirements of their software system; they negotiate the distribution of tasks among them;and they make decisions about the system design; and the internal structure andfunctionalities of its code modules. The software research community has long recognizedthe importance and potential usefulness of such textual information. In this paper; wediscuss our recent work on systematically analyzing several textual streams collectedthrough our WikiDev2. 0 tool. We use two different text-analysis methods to examine fivedifferent sources of textual data. We report on our experience using our method onanalyzing the communications of a nine-member team over four months.,Software Maintenance (ICSM); 2010 IEEE International Conference on,2010,8
Real-time MPEG encoding in shared-memory multiprocessors,Denilson M Barbosa; Joao Paulo Kitajima; Wagner Meira Jr,Abstract Advances in digital imaging and faster networking resources enabled anunprecedent popularization of digital video applications; not only for entertainment; but alsofor health and education. However; capturing; digitizing; storing and displaying video in realtime remains a challenge for nowadays technology. Furthermore; the large number offrames that compose a digital video; each of them occupying hundreds of kilobytes; stresssignificantly both storage and networking resources available currently. A common strategyto reduce the amount of data that has to be handled while providing digital video services iscompression; as specified by the MPEG coding standards; for instance. The compressioncost then becomes the greatest; demanding the use of either specialized hardware orparallel computing to meet the real time demands. In this work; we propose a novel …,2nd International Conference on Parallel Computing Systems,1999,8
Using triads to identify local community structure in social networks,Justin Fagnan; Osmar Zaïane; Denilson Barbosa,We present our novel community mining algorithm that uses only local information toaccurately identify communities; outliers; and hubs in social networks. The main componentof our algorithm is the T metric; which evaluates the relative quality of a community byconsidering the number of internal and external triads (3-node cliques) it contains.Furthermore we propose an intuitive statistical method based on our T metric; whichcorrectly identifies outlier and hub nodes within each discovered community. Finally; weevaluate our approach on a series of ground-truth networks and show that our methodoutperforms the state-of-the-art in community mining algorithms.,Advances in Social Networks Analysis and Mining (ASONAM); 2014 IEEE/ACM International Conference on,2014,7
Systems and Methods for Efficient Top-k Approximate Subtree Matching,*,Systems and method for searching for approximate matches in a database of documentsrepresented by a tree structure. A fast solution to the Top-k Approximate Subtree MatchingProblem involves determining candidate subtrees which will be considered as possiblematches to a query also represented by a tree structure. Once these candidate subtrees arefound; a tree edit distance between each candidate subtree and the query tree is calculated.The results are then sorted to find those with the lowest tree edit distance.,*,2012,7
Automatic evaluation of relation extraction systems on large-scale,Mirko Bronzi; Zhaochen Guo; Filipe Mesquita; Denilson Barbosa; Paolo Merialdo,Abstract The extraction of relations between named entities from natural language text is alongstanding challenge in information extraction; especially in large-scale. A majorchallenge for the advancement of this research field has been the lack of meaningfulevaluation frameworks based on realistic-sized corpora. In this paper we propose aframework for large-scale evaluation of relation extraction systems based on an automaticannotator that uses a public online database and a large web corpus.,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,2012,7
Access control policy translation; verification; and minimization within heterogeneous data federations,Gregory Leighton; Denilson Barbosa,Abstract Data federations provide seamless access to multiple heterogeneous andautonomous data sources pertaining to a large organization. As each source databasedefines its own access control policies for a set of local identities; enforcing such policiesacross the federation becomes a challenge. In this article; we first consider the problem oftranslating existing access control policies defined over source databases in a manner thatallows the original semantics to be observed while becoming applicable across the entiredata federation. We show that such a translation is always possible; and provide analgorithm for automating the translation. We show that verifying whether a translated policyobeys the semantics of the original access control policy defined over a source database isintractable; even under restrictive scenarios. We then describe a practical algorithmic …,ACM Transactions on Information and System Security (TISSEC),2011,7
ToXgene Template Specification Language,Denilson Barbosa,ToXgene is a template-based generator for large collections of synthetic XML documents;under development at the University of Toronto as part of the ToX (Toronto XML Engine)project. This document describes the basic functioning of our tool and the ToXgeneTemplate Specification Language (TSL). TSL templates are essentially XML Schema [1]specifications annotated with specifications of probability distributions and CDATA contentdescriptors; used for generating both attributes and elements. Like XML Schemaspecifications; TSL templates are XML documents. For increased readability; we present theXML Schema notation of TSL in our examples using typewriter face. TSL-specificannotations are attributes or elements added to a schema document; in either case;annotations are prefixed by the string tox-and presented using a different font.,Department of Computer Science; University of Toronto; Version,2003,7
Identifying controversial Wikipedia articles using editor collaboration networks,Hoda Sepehri-Rad; Denilson Barbosa,Abstract Wikipedia is probably the most commonly used knowledge reference nowadays;and the high quality of its articles is widely acknowledged. Nevertheless; disagreementamong editors often causes some articles to become controversial over time. These articlesspan thousands of popular topics; including religion; history; and politics; to name a few; andare manually tagged as controversial by the editors; which is clearly suboptimal. Moreover;disagreement; bias; and conflict are expressed quite differently in Wikipedia compared toother social media; rendering previous approaches ineffective. On the other hand; the socialprocess of editing Wikipedia is partially captured in the edit history of the articles; openingthe door for novel approaches. This article describes a novel controversy model that buildson the interaction history of the editors and not only predicts controversy but also sheds …,ACM Transactions on Intelligent Systems and Technology (TIST),2015,5
Extracting family relationship networks from novels,Aibek Makazhanov; Denilson Barbosa; Grzegorz Kondrak,Abstract: We present an approach to the extraction of family relations from literary narrative;which incorporates a technique for utterance attribution proposed recently by Elson andMcKeown (2010). In our work this technique is used in combination with the detection ofvocatives-the explicit forms of address used by the characters in a novel. We take advantageof the fact that certain vocatives indicate family relations between speakers. The extractedrelations are then propagated using a set of rules. We report the results of the application ofour method to Jane Austen's Pride and Prejudice. Subjects: Computation and Language (cs.CL) Cite as: arXiv: 1405.0603 [cs. CL](or arXiv: 1405.0603 v1 [cs. CL] for this version)Submission history From: Aibek Makazhanov [view email][v1] Sat; 3 May 2014 16: 07: 19GMT (416kb; D),arXiv preprint arXiv:1405.0603,2014,5
Incorporating global information into named entity recognition systems using relational context,Yuval Merhav; Filipe Mesquita; Denilson Barbosa; Wai Gen Yee; Ophir Frieder,Abstract The state-of-the-art in Named Entity Recognition relies on a combination of localfeatures of the text and global knowledge to determine the types of the recognized entities.This is problematic in some cases; resulting in entities being classified as belonging to thewrong type. We show that using global information about the corpus improves the accuracyof type identification. We explore the notion of a global domain frequency that relatesrelation identifying terms with pairs of entity types which are used in that relation. We use thisto identify entities whose types are not compatible with the terms they co-occur in the text.Our results on a large corpus of social media content allows the identification of mistypedentities with 70% accuracy.,Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,2010,5
Optimizing xml compression,Gregory Leighton; Denilson Barbosa,Abstract The eXtensible Markup Language (XML) provides a powerful and flexible means ofencoding and exchanging data. As it turns out; its main advantage as an encoding format(namely; its requirement that all open and close markup tags are present and properlybalanced) yields also one of its main disadvantages: verbosity. XML-conscious compressiontechniques seek to overcome this drawback. Many of these techniques first separate XMLstructure from the document content; and then compress each independently. Furthercompression gains can be realized by identifying and compressing together documentcontent that is highly similar; thereby amortizing the storage costs of auxiliary informationrequired by the chosen compression algorithm. Additionally; the proper choice ofcompression algorithm is an important factor not only for the achievable compression …,International XML Database Symposium,2009,5
Application Benchmark,Denilson Barbosa; Ioana Manolescu; Jeffrey Xu Yu,1. Software non-determinism: A software system is non-deterministic if; when re-executed; itresults in a different execution path than a prior execution. Non-determinism can arise when;for example; paths are determined by relative processor speed or the sequence of externalevents. Such software bugs have been called ''Heisenbugs''(hard failures being ''Bohrbugs'').2. Soft hardware failures: Hardware can also suffer from ''Heisenbugs.''For example; atransient hardware failure may be triggered by an environmental cause; such as a cosmicray changing a memory bit; etc. 3. Operator failures: Systems occasionally require operatorintervention. Operators; being human; make mistakes. An operator is unlikely to make thesame mistake at the same point in a subsequent execution.,*,2009,5
ualberta at TAC-KBP 2012: English and Cross-Lingual Entity Linking.,Zhaochen Guo; Ying Xu; Filipe de Sá Mesquita; Denilson Barbosa; Grzegorz Kondrak,Abstract On one hand; the proliferation of the Web has generated massive information in anunorganized way and is still growing in an accelerating pace. On the other hand; structuredand queryable knowledge bases are very difficult to construct and update. Automaticknowledge base construction techniques are greatly needed to convert the rich Webinformation into useful knowledge bases. Besides information extraction; ambiguities aboutentities and facts also need to be resolved. Entity Linking; which links an extracted namedentity to an entity in a knowledge base; is to solve this ambiguity before populatingknowledge. In this paper; we describe ualberta's system for the 2012 TAC-KBP English andCross-Lingual Entity Linking (EL) task; and report the result on the evaluation datasets.,TAC,2012,4
Extracting meta statements from the blogosphere,Filipe Mesquita; Denilson Barbosa,Abstract Information extraction systems have been recently proposed for organizing andexploring content in large online text corpora as information networks. In such networks; thenodes are named entities (eg; people; organizations) while the edges correspond tostatements indicating relations among such entities. To date; such systems extract ratherprimitive networks; capturing only those relations which are expressed by direct statements.In many applications; it is useful to also extract more subtle relations which are oftenexpressed as meta statements in the text. These can; for instance provide the context for astatement (eg;“Google acquired YouTube on October 2006”); or repercussion about astatement (eg;“The US condemned Russia's invasion of Georgia”). In this work; we report ona system for extracting relations expressed in both direct statements as well as in meta …,Proc. ICWSM 2011,2011,4
Access control policy translation and verification within heterogeneous data federations,Gregory Leighton; Denilson Barbosa,Abstract Data federations provide seamless access to multiple heterogeneous andautonomous data sources pertaining to a large organization. As each source databasedefines its own access control policies for a set of local identities; enforcing such policiesacross the federation becomes a challenge. In this paper; we first consider the problem oftranslating existing access control policies defined over source databases in a manner thatallows the original semantics to be observed; while becoming applicable across the entiredata federation. We show that such a translation is always possible; and provide analgorithm for automating the translation. We then show that verifying that a translated policyobeys the semantics of the original access control policy defined over a source database isintractable; even under restrictive scenarios. Finally; we describe a practical algorithmic …,Proceedings of the 15th ACM symposium on Access control models and technologies,2010,4
A framework for automatic schema mapping verification through reasoning,Paolo Cappellari; Denilson Barbosa; Paolo Atzeni,We advocate an automated approach for verifying mappings between source and targetdatabases in which semantics are taken into account; and that avoids two serious limitationsof current verification approaches: reliance on availability of sample source and targetinstances; and reliance on strong statistical assumptions. We discuss how our approach canbe integrated into the workflow of state-of-the-art mapping design systems; and all itsnecessary inputs. Our approach relies on checking the entailment of verification statementsderived directly from the schema mappings and from semantic annotations to the variablesused in such mappings. We discuss how such verification statements can be produced andhow such annotations can be extracted from different kinds of alignments of schemas intodomain ontologies. Such alignments can be derived semi-automatically; thus; our …,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,4
Microbenchmark,Denilson Barbosa; Ioana Manolescu; Jeffrey Xu Yu,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,4
The Web XML: A First Study,Laurent Mignet; Denilson Barbosa; Pierangelo Veltri,Abstract Although originally designed for large-scale electronic publishing; XML plays anincreasingly important role in the exchange of data on the Web. In fact; it is expected thatXML will become the lingua franca of the Web; eventually replacing HTML. Not surprisingly;there has been a great deal of interest on XML both in industry and in academia. Despitethis interest; to date no comprehensive study on the\emph {XML Web}(ie; the subset of theWeb made of XML documents only) nor on its contents has been made. In this paper wedescribe the XML Web and study the documents contained in it. Our results are drawn froma sample of a repository of the publicly available XML documents on the Web; consisting ofabout 200;000 documents. Our results show that; despite its short history; XML alreadypermeates the Web; both in terms of generic domains and geographically. Also; our …,*,2003,4
Hadoop branching: Architectural impacts on energy and performance,Ivanilton Polato; Denilson Barbosa; Abram Hindle; Fabio Kon,Data centers are notorious energy consumers. In fact; studies have shown that for every $1spent on hardware in the datacenter; $0.50 is spent on powering this hardware over itslifetime. Data centers host real or virtual (ie; cloud) clusters that often execute large computejobs using MapReduce; of which Hadoop is a popular implementation. Like other successfulopen source projects; Hadoop has been maintained and evolved over time with newresource management features being added over time in an effort to improve performance;raising questions as to whether such architectural evolution has achieved its goal; and if so;at what cost. In this work we apply Green Mining to find out that later versions of Hadoop-who exhibit more dynamic resource control-can suffer from serious energy consumptionperformance regressions.,Green Computing Conference and Sustainable Computing Conference (IGSC); 2015 Sixth International,2015,3
Entity Recognition and Linking on Tweets with Random Walks.,Zhaochen Guo; Denilson Barbosa,*,# MSM,2015,3
XML benchmarks,Denilson Barbosa; Ioana Manolescu; Jeffrey Xu Yu,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,Encyclopedia of Database Systems,2009,3
FleDEx: flexible data exchange,Filipe Mesquita; Denilson Barbosa; Eli Cortez; Altigran S da Silva,Abstract We propose a lightweight framework for data exchange that is suitable for non-expert and casual users sharing data on the Web or through peer-to-peer systems. Unlikeprevious work; we consider a simplistic data model and schema formalism that are suitablefor describing typical online data; and propose algorithms for mapping such schemas aswell as for translating the corresponding instances. Our solution requires minimal overheadand setup costs compared to existing data exchange systems; making it very attractive in theWeb data exchange setting. We report experimental results indicating that our method workswell with real Web data from various domains.,Proceedings of the 9th annual ACM international workshop on Web information and data management,2007,3
Hybrid HDFS: decreasing energy consumption and speeding up Hadoop using SSDs,Ivanilton Polato; Denilson Barbosa; Abram Hindle; Fabio Kon,Abstract Apache Hadoop has evolved significantly over the last years; with more than 60releases bringing new features. By implementing the MapReduce programming paradigmand leveraging HDFS; its distributed file system; Hadoop has become a reliable and faulttolerant middleware for parallel and distributed computing over large datasets.Nevertheless; Hadoop may struggle under certain workloads; resulting in poor performanceand high energy consumption. Users increasingly demand that high performance computingsolutions being to address sustainability and limit power consumption. In this paper; weintroduce HDFS H; a hybrid storage mechanism for HDFS; which uses a combination ofHard Disks and Solid-State Disks to achieve higher performance while saving power inHadoop computations. HDFS H brings to middleware the best from HDs (affordable cost …,PeerJ PrePrints,2015,2
Towards scalable summarization and visualization of large text corpora,Tyler Sliwkanich; Douglas Schneider; Aaron Yong; Mitchell Home; Denilson Barbosa,Abstract Society is awash with problems requiring the analysis of vast quantities of text anddata. From detecting flu trends out of twitter conversations to finding scholarly worksanswering specific questions; we rely more and more on computers to process text for us.Text analytics is the application of computational; mathematical; and statistical models toderive information from large quantities of data coming primarily as text. Our project providesfast and effective text-analytics tools for large document collections; such as theblogosphere. We use natural language processing and database techniques to extract;collect; analyze; visualize; and archive information extracted from text. We focus ondiscovering relationships between entities (people; places; organizations; etc.) mentioned inone or more sources (blog posts or news articles). We built a custom solution using …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,2
Accurate fact harvesting from natural language text in wikipedia with Lector,Matteo Cannaviccio; Denilson Barbosa; Paolo Merialdo,Abstract Many approaches have been introduced recently to automatically create oraugment Knowledge Graphs (KGs) with facts extracted from Wikipedia; particularly itsstructured components like the infoboxes. Although these structures are valuable; theyrepresent only a fraction of the actual information expressed in the articles. In this work; wequantify the number of highly accurate facts that can be harvested with high precision fromthe text of Wikipedia articles using information extraction techniques bootstrapped from theentities and relations already in a KG. Our experimental evaluation; which uses Freebase asreference KG; reveals we can augment several relations in the domain of people by morethan 10%; with facts whose accuracy are over 95%. Moreover; the vast majority of thesefacts are missing from the infoboxes; YAGO and DBpedia.,Proceedings of the 19th International Workshop on Web and Databases,2016,1
Robust named entity disambiguation with random walks,Zhaochen Guo; Denilson Barbosa,Abstract Named Entity Disambiguation is the task of assigning entities from a KnowledgeGraph (KG) to mentions of such entities in a textual document. The state-of-the-art for thistask balances two disparate sources of similarity: lexical; defined as the pairwise similaritybetween mentions in the text and names of entities in the KG; and semantic; defined throughsome graph-theoretic property of a subgraph of the KG induced by the choice of entities foreach mention. Departing from previous work; our notion of semantic similarity is rooted inInformation Theory and is defined as the mutual information between random walks on thedisambiguation graph induced by choice of entities for each mention. We describe aniterative algorithm based on this idea; and show an extension that uses learning-to-rank;which yields further improvements. Our experimental evaluation demonstrates that this …,Semantic Web,2016,1
Inferencing in information extraction: Techniques and applications,Denilson Barbosa; Haixun Wang; Cong Yu,Information extraction at Web scale has become one of the most important research topics indata management since major commercial search engines started incorporating knowledgein their search results a couple of years ago [1]. Users increasingly expect structuredknowledge as answers to their search needs. Using Bing as an example; the result page for“Lionel Messi” is full of structured knowledge facts; such as his birthday and awards. Theresearch efforts towards improving the accuracy and coverage of such knowledge baseshave led to significant advances in Information Extraction techniques [2];[3]. As the initialchallenge of accurately extracting facts for popular entities are being addressed; moredifficult challenges have emerged such as extending knowledge coverage to long tailentities and domains; understanding interestingness and usefulness of facts within a …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,1
Advances in Artificial Intelligence,Denilson Barbosa; Evangelos Milios,The 28th Canadian Conference on Artificial Intelligence (AI 2015) built on a long sequenceof successful conferences; bringing together Canadian and international researchers;presenting and discussing original research. The conference was held in Halifax; NovaScotia; Canada; during June 2–2; 2015; and was collocated with the 41st Graphics InterfaceConference (GI 2015); and the 12th Conference on Computer and Robot Vision (CRV2015). AI 2015 attracted 81 submissions from Canada and internationally. Each submissionwas reviewed in double-blind mode by at least three Program Committee members. For theconference and the proceedings 15 regular papers and 12 short papers were accepted; ie;18.5% and 15% of the total number of submissions; respectively. Regular papers wereallocated 16 pages in the proceedings; while short papers were allocated 8 pages. The …,28th Canadian Conference on Artificial Intelligence,2015,1
Automatically training form classifiers,Mauricio C Moraes; Carlos A Heuser; Viviane P Moreira; Denilson Barbosa,Abstract The state-of-the-art in domain-specific Web form discovery relies on supervisedmethods requiring substantial human effort in providing training examples; which limits theirapplicability in practice. This paper proposes an effective alternative to reduce the humaneffort: obtaining high-quality domain-specific training forms. In our approach; the only userinput is the domain of interest; we use a search engine and a focused crawler to locatequery forms which are fed as training data into supervised form classifiers. We tested thisapproach thoroughly; using thousands of real Web forms from six domains; including arepresentative subset of a publicly available form base to validate this approach. The resultsreported in this paper show that it is feasible to mitigate the demanding manual workrequired by some methods of the current state-of-the-art in form discovery; at the cost of a …,International Conference on Web Information Systems Engineering,2013,1
Generating synthetic database schemas for simulation purposes,Carlos Eduardo Pires; Priscilla Vieira; Márcio Saraiva; Denilson Barbosa,Abstract To simulate query answering in Peer Data Management System (PDMSs);simulators need to associate a database schema to each peer in the overlay network.Finding or creating a high number of database schemas can be a time consuming andtendentious task. This work proposes an automatic process to generate multiple syntheticdatabase schemas with semantically coherent variations of a given base schema. Theschemas are obtained through applying different types of modifications to subsets of thebase schema. Our experimental validation has shown that the proposed method is able toproduce random schemas that can be used in realistic simulations.,International Conference on Database and Expert Systems Applications,2011,1
Exploring and visualizing academic social networks,Veselin Ganev; Zhaochen Guo; Diego Serrano; Denilson Barbosa; Eleni Stroulia,Abstract We demonstrate the ReaSoN portal; consisting of interactive web-based tools forvisualizing; exploring; querying; and integrating academic social networks. We describe howthese networks are automatically extracted from bibliographic and citation databases;discuss notions of visibility in such networks which enable a rich set of social networkanalysis; and demonstrate our novel tools for the visualization and exploration of socialnetworks.,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,1
SemaForm: Semantic Wrapper Generation for Querying Deep Web Data Sources,Jagoda Walny; D Barbosa,Abstract A wealth of data on the World Wide Web is hidden behind web form queryinterfaces and cannot be found through regular search engines. Querying across multiplesuch sources is a tedious and error-prone process; it involves manually filling in manyrelated; but different; web forms. SemaForm automates this process by correlating web formlabels to entries in a domain ontology through the use of a continually refined knowledgebase and various techniques for semantic matching.,2009 International Conference on Web Information Systems and Mining,2009,1
XML storage,Denilson Barbosa; Philip Bohannon; Juliana Freire; Carl-Christian Kanne; Ioana Manolescu; Vasilis Vassalos; Masatoshi Yoshikawa,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,1
Towards automatic schema mapping verification through reasoning,Paolo Cappellari; Denilson Barbosa,Abstract. We consider the problem of determining whether a schema mapping producesdata that is compatible with the semantics of the target database schema. We propose alightweight framework based annotating the schemas with conceptual meta-data expressedformally. Based on this framework; we propose a formalization of the problem; and providealgorithms that automatically verify the compatibility of mappings. Also; we discuss a strategyfor automatically deriving the required semantic annotations. Finally; we discuss how ourframework may lead to practical tools to help in the verification of schema mappings.,Logic in Databases (LID 2009),2009,1
Investigations on Knowledge Base Embedding for Relation Prediction and Extraction,Peng Xu; Denilson Barbosa,Abstract: We report an evaluation of the effectiveness of the existing knowledge baseembedding models for relation prediction and for relation extraction on a wide range ofbenchmarks. We also describe a new benchmark; which is much larger and complex thanprevious ones; which we introduce to help validate the effectiveness of both tasks. Theresults demonstrate that knowledge base embedding models are generally effective forrelation prediction but unable to give improvements for the state-of-art neural relationextraction model with the existing strategies; while pointing limitations of existing methods.Subjects: Computation and Language (cs. CL) Cite as: arXiv: 1802.02114 [cs. CL](or arXiv:1802.02114 v1 [cs. CL] for this version) Submission history From: Peng Xu [view email][v1]Tue; 6 Feb 2018 18: 20: 17 GMT (160kb; D),arXiv preprint arXiv:1802.02114,2018,*
Hadoop energy consumption reduction with hybrid HDFS,Ivanilton Polato; Denilson Barbosa; Abram Hindle; Fabio Kon,Abstract Apache Hadoop has evolved significantly over the last years; with more than 60releases bringing new features. By implementing the MapReduce programming paradigmand leveraging HDFS; its distributed file system; Hadoop has become a reliable and faulttolerant middleware for parallel and distributed computing over large datasets.Nevertheless; Hadoop may struggle under certain workloads; resulting in poor performanceand high energy consumption. Users increasingly demand that high performance computingsolutions address sustainability and limit energy consumption. In this paper; we introduceHDFS H; a hybrid storage mechanism for HDFS; which uses a combination of Hard Disksand Solid-State Disks to achieve higher performance while saving power in Hadoopcomputations. HDFS H brings to the middleware the best from HDs (affordable cost per …,Proceedings of the 31st Annual ACM Symposium on Applied Computing,2016,*
Sentiment Analysis for Streams of Web Data: A Case Study of Brazilian Financial Markets,Bruna Neuenschwander; Adriano Pereira; Wagner Meira Jr; Denilson Barbosa,Abstract With the rise of Web 2.0 applications; most people started consuming informationand sharing opinions and ideas about most aspects of their lives on a variety of social mediaplatforms; creating massive and continuous streams of valuable data. While this opened thedoor for information extraction and mining techniques that can help us understand differentaspects of society; extracting useful information from such streams of Web data is far fromtrivial. In this setting; sentiment analysis techniques can be convenient as they are capableof summarizing general feeling about entities people care about; such as products andcompanies. Therefore; they can be quite applicable in scenarios like the stock market; whichalso has tremendous impact on society. This paper describes and evaluates two differenttechniques for sentiment analysis applied to the Brazilian stock market data: lexicon …,Proceedings of the 20th Brazilian Symposium on Multimedia and the Web,2014,*
Perspectives on Business Intelligence,Raymond T Ng; Patricia C Arocena; Denilson Barbosa; Giuseppe Carenini; Luiz Gomes; Jr; Stephan Jou; Rock Anthony Leung; Evangelos Milios; Renée J Miller; John Mylopoulos; Rachel A Pottinger; Frank Tompa; Eric Yu,Abstract Download Free Sample In the 1980s; traditional Business Intelligence (BI) systemsfocused on the delivery of reports that describe the state of business activities in the past;such as for questions like" How did our sales perform during the last quarter?" A decadelater; there was a shift to more interactive content that presented how the business wasperforming at the present time; answering questions like" How are we doing right now?"Today the focus of BI users are looking into the future." Given what I did before and how I amcurrently doing this quarter; how will I do next quarter?" Furthermore; fuelled by the demandsof Big Data; BI systems are going through a time of incredible change. Predictive analytics;high volume data; unstructured data; social data; mobile; consumable analytics; and datavisualization are all examples of demands and capabilities that have become critical …,Synthesis Lectures on Data Management,2013,*
Experimental Evaluation of Autonomic Indexing,Denilson Barbosa; Mariano P Consens; Laurent Mignet,Abstract We are witnessing an explosive increase in the complexity of the informationsystems we rely upon. Autonomic systems address this challenge by continuouslyconfiguring and tuning themselves. Recently; a number of autonomic features have beenincorporated into commercial RDBMS; tools for recommending index configurations for agiven workload are prominent examples of this promising trend. In this paper; we introduce aflexible characterization of the performance goals of an indexing recommender and developan experimental evaluation approach to assess the effectiveness of these tools. We focus onexploratory queries and present extensive experimental results using both real and syntheticdata that demonstrate the validity of the approach introduced. Our results identify a specificindexing configuration based on singlecolumn indexes as a very useful baseline for …,*,2004,*
