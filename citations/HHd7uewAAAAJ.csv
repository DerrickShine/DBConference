BlinkDB: queries with bounded errors and bounded response times on very large data,Sameer Agarwal; Barzan Mozafari; Aurojit Panda; Henry Milner; Samuel Madden; Ion Stoica,Abstract In this paper; we present BlinkDB; a massively parallel; approximate query enginefor running interactive SQL queries on large volumes of data. BlinkDB allows users to trade-off query accuracy for response time; enabling interactive queries over massive data byrunning queries on data samples and presenting results annotated with meaningful errorbars. To achieve this; BlinkDB uses two key ideas:(1) an adaptive optimization frameworkthat builds and maintains a set of multi-dimensional stratified samples from original dataover time; and (2) a dynamic sample selection strategy that selects an appropriately sizedsample based on a query's accuracy or response time requirements. We evaluate BlinkDBagainst the well-known TPC-H benchmarks and a real-world analytic workload derived fromConviva Inc.; a company that manages video distribution over the Internet. Our …,Proceedings of the 8th ACM European Conference on Computer Systems,2013,398
On the Evolution of Wikipedia.,Rodrigo B Almeida; Barzan Mozafari; Junghoo Cho,Abstract A recent phenomenon on the Web is the emergence and proliferation of new socialmedia systems allowing social interaction between people. One of the most popular of thesesystems is Wikipedia that allows users to create content in a collaborative way. Despite itscurrent popularity; not much is known about how users interact with Wikipedia and how ithas evolved over time. In this paper we aim to provide a first; extensive study of the userbehavior on Wikipedia and its evolution. Compared to prior studies; our work differs inseveral ways. First; previous studies on the analysis of the user workloads (for systems suchas peer-to-peer systems [10] and Web servers [2]) have mainly focused on understandingthe users who are accessing information. In contrast; Wikipedia's provides us with theopportunity to understand how users create and maintain information since it provides the …,ICWSM,2007,143
Verifying and mining frequent patterns from large windows over data streams,Barzan Mozafari; Hetal Thakkar; Carlo Zaniolo,Mining frequent itemsets from data streams has proved to be very difficult because ofcomputational complexity and the need for real-time response. In this paper; we introduce anovel verification algorithm which we then use to improve the performance of monitoring andmining tasks for association rules. Thus; we propose a frequent itemset mining method forsliding windows; which is faster than the state-of-the-art methods¿ in fact; its running timethat is nearly constant with respect to the window size entails the mining of much largerwindows than it was possible before. The performance of other frequent itemset miningmethods (including those on static data) can be improved likewise; by replacing theircounting methods (eg; those using hash trees) by our verification algorithm.,ICDE,2008,104
Knowing When You’re Wrong: Building Fast and Reliable Approximate Query Processing Systems,Sameer Agarwal; Henry Milner; Ariel Kleiner; Ameet Talwalkar; Michael Jordan; Samuel Madden; Barzan Mozafari; Ion Stoica,Abstract Modern data analytics applications typically process massive amounts of data onclusters of tens; hundreds; or thousands of machines to support near-real-time decisions.The quantity of data and limitations of disk and memory bandwidth often make it infeasible todeliver answers at interactive speeds. However; it has been widely observed that manyapplications can tolerate some degree of inaccuracy. This is especially true for exploratoryqueries on data; where users are satisfied with" close-enough" answers if they can comequickly. A popular technique for speeding up queries at the cost of accuracy is to executeeach query on a sample of data; rather than the whole dataset. To ensure that the returnedresult is not too inaccurate; past work on approximate query processing has used statisticaltechniques to estimate" error bars" on returned results. However; existing work in the …,SIGMOD,2014,98
Blink and it's done: interactive queries on very large data,Sameer Agarwal; Anand P Iyer; Aurojit Panda; Samuel Madden; Barzan Mozafari; Ion Stoica,Abstract In this demonstration; we present BlinkDB; a massively parallel; sampling-basedapproximate query processing framework for running interactive queries on large volumes ofdata. The key observation in BlinkDB is that one can make reasonable decisions in theabsence of perfect answers. BlinkDB extends the Hive/HDFS stack and can handle thesame set of SPJA (selection; projection; join and aggregate) queries as supported by thesesystems. BlinkDB provides real-time answers along with statistical error guarantees; and canscale to petabytes of data and thousands of machines in a fault-tolerant manner. Ourexperiments using the TPC-H benchmark and on an anonymized real-world video contentdistribution workload from Conviva Inc. show that BlinkDB can execute a wide range ofqueries up to 150x faster than Hive on MapReduce and 10--150x faster than Shark (Hive …,Proceedings of the VLDB Endowment,2012,81
Scaling up crowd-sourcing to very large datasets: a case for active learning,Barzan Mozafari; Purna Sarkar; Michael Franklin; Michael Jordan; Samuel Madden,Abstract Crowd-sourcing has become a popular means of acquiring labeled data for manytasks where humans are more accurate than computers; such as image tagging; entityresolution; and sentiment analysis. However; due to the time and cost of human labor;solutions that rely solely on crowd-sourcing are often limited to small datasets (ie; a fewthousand items). This paper proposes algorithms for integrating machine learning into crowd-sourced databases in order to combine the accuracy of human labeling with the speed andcost-effectiveness of machine learning classifiers. By using active learning as ouroptimization strategy for labeling tasks in crowd-sourced databases; we can minimize thenumber of questions asked to the crowd; allowing crowd-sourced applications to scale (ie;label much larger datasets at lower costs). Designing active learning algorithms for a …,Proceedings of the VLDB Endowment,2014,78
DBSeer: Resource and Performance Prediction for Building a Next Generation Database Cloud.,Barzan Mozafari; Carlo Curino; Samuel Madden,ABSTRACT Cloud computing is characterized by shared infrastructure and a decouplingbetween its operators and tenants. These two characteristics impose new challenges todatabases applications hosted in the cloud; namely:(i) how to price database services;(ii)how to isolate database tenants; and (iii) how to optimize database performance on thisshared infrastructure. We argue that today's solutions; based on virtual-machines; do notproperly address these challenges. We hint at new research directions to tackle theseproblems and argue that these three challenges share a common need for accuratepredictive models of performance and resource utilization. We present our approach; calledDBSeer; with our initial results on predictive models for the important class of OLTP/Webworkloads and show how they can be used to address these challenges.,CIDR,2013,59
Performance and resource modeling in highly-concurrent oltp workloads,Barzan Mozafari; Carlo Curino; S Madden,Abstract Database administrators of Online Transaction Processing (OLTP) systemsconstantly face difficult questions. For example;" What is the maximum throughput I cansustain with my current hardware?";" How much disk I/O will my system perform if therequests per second double?"; or" What will happen if the ratio of transactions in my systemchanges?". Resource prediction and performance analysis are both vital and difficult in thissetting. Here the challenge is due to high degrees of concurrency; competition for resources;and complex interactions between transactions; all of which non-linearly impactperformance. Although difficult; such analysis is a key component in enabling databaseadministrators to understand which queries are eating up the resources; and how theirsystem would scale under load. In this paper; we introduce our framework; called DBSeer …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of data,2013,55
High-performance complex event processing over XML streams,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract Much research attention has been given to delivering high-performance systemsthat are capable of complex event processing (CEP) in a wide range of applications.However; many current CEP systems focus on processing efficiently data having a simplestructure; and are otherwise limited in their ability to support efficiently complex continuousqueries on structured or semi-structured information. However; XML streams represent avery popular form of data exchange; comprising large portions of social network and RSSfeeds; financial records; configuration files; and similar applications requiring advanced CEPqueries. In this paper; we present the XSeq language and system that support CEP on XMLstreams; via an extension of XPath that is both powerful and amenable to an efficientimplementation. Specifically; the XSeq language extends XPath with natural operators to …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,54
The analytical bootstrap: a new method for fast error estimation in approximate query processing,Kai Zeng; Shi Gao; Barzan Mozafari; Carlo Zaniolo,Abstract Sampling is one of the most commonly used techniques in Approximate QueryProcessing (AQP)-an area of research that is now made more critical by the need for timelyand cost-effective analytics over" Big Data". Assessing the quality (ie; estimating the error) ofapproximate answers is essential for meaningful AQP; and the two main approaches used inthe past to address this problem are based on either (i) analytic error quantification or (ii) thebootstrap method. The first approach is extremely efficient but lacks generality; whereas thesecond is quite general but suffers from its high computational overhead. In this paper; weintroduce a probabilistic relational model for the bootstrap process; along with rigoroussemantics and a unified error model; which bridges the gap between these two traditionalapproaches. Based on our probabilistic framework; we develop efficient algorithms to …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,50
Optimal load shedding with aggregates and mining queries,Barzan Mozafari; Carlo Zaniolo,To cope with bursty arrivals of high-volume data; a DSMS has to shed load while minimizingthe degradation of Quality of Service (QoS). In this paper; we show that this problem can beformalized as a classical optimization task from operations research; in ways thataccommodate different requirements for multiple users; different query sensitivities to loadshedding; and different penalty functions. Standard non-linear programming algorithms areadequate for non-critical situations; but for severe overloads; we propose a more efficientalgorithm that runs in linear time; without compromising optimality. Our approach isapplicable to a large class of queries including traditional SQL aggregates; statisticalaggregates (eg; quantiles); and data mining functions; such as k-means; naive Bayesianclassifiers; decision trees; and frequent pattern discovery (where we can even specify a …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,33
Active learning for crowd-sourced databases,Barzan Mozafari; Purnamrita Sarkar; Michael J Franklin; Michael I Jordan; Samuel Madden,Abstract: Crowd-sourcing has become a popular means of acquiring labeled data for a widevariety of tasks where humans are more accurate than computers; eg; labeling images;matching objects; or analyzing sentiment. However; relying solely on the crowd is oftenimpractical even for data sets with thousands of items; due to time and cost constraints ofacquiring human input (which cost pennies and minutes per label). In this paper; wepropose algorithms for integrating machine learning into crowd-sourced databases; with thegoal of allowing crowd-sourcing applications to scale; ie; to handle larger datasets at lowercosts. The key observation is that; in many of the above tasks; humans and machinelearning algorithms can be complementary; as humans are often more accurate but slowand expensive; while algorithms are usually less accurate; but faster and cheaper. Based …,arXiv preprint arXiv:1209.3686,2012,28
SMM: A data stream management system for knowledge discovery,Hetal Thakkar; Nikolay Laptev; Hamid Mousavi; Barzan Mozafari; Vincenzo Russo; Carlo Zaniolo,The problem of supporting data mining applications proved to be difficult for databasemanagement systems and it is now proving to be very challenging for data streammanagement systems (DSMSs); where the limitations of SQL are made even more severeby the requirements of continuous queries. The major technical advances that achievedseparately on DSMSs and on data stream mining algorithms have failed to converge andproduce powerful data stream mining systems. Such systems; however; are essential sincethe traditional pull-based approach of cache mining is no longer applicable; and the push-based computing mode of data streams and their bursty traffic complicate applicationdevelopment. For instance; to write mining applications with quality of service (QoS) levelsapproaching those of DSMSs; a mining analyst would have to contend with many …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,27
Designing an inductive data stream management system: the stream mill experience,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,Abstract There has been much recent interest in on-line data mining. Existing miningalgorithms designed for stored data are either not applicable or not effective on datastreams; where real-time response is often needed and data characteristics changefrequently. Therefore; researchers have been focusing on designing new and improvedalgorithms for on-line mining tasks; such as classification; clustering; frequent itemsetsmining; pattern matching; etc. Relatively little attention has been paid to designing DSMSs;which facilitate and integrate the task of mining data streams---ie; stream systems thatprovide Inductive functionalities analogous to those provided by Weka and MS OLE DB forstored data. In this paper; we propose the notion of an Inductive DSMS---a system thatbesides providing a rich library of inter-operable functions to support the whole mining …,Proceedings of the 2nd international workshop on Scalable stream processing system,2008,23
A Handbook for Building an Approximate Query Engine.,Barzan Mozafari; Ning Niu,Abstract There has been much research on various aspects of Approximate QueryProcessing (AQP); such as different sampling strategies; error estimation mechanisms; andvarious types of data synopses. However; many subtle challenges arise when building anactual AQP engine that can be deployed and used by real world applications. Thesesubtleties are often ignored (or at least not elaborated) by the theoretical literature andacademic prototypes alike. For the first time to the best of our knowledge; in this article; wefocus on these subtle challenges that one must address when designing an AQP system.Our intention for this article is to serve as a handbook listing critical design choices thatdatabase practitioners must be aware of when building or using an AQP system; not toprescribe a specific solution to each challenge.,IEEE Data Eng. Bull.,2015,22
ABS: a system for scalable approximate queries with accuracy guarantees,Kai Zeng; Shi Gao; Jiaqi Gu; Barzan Mozafari; Carlo Zaniolo,Abstract Approximate Query Processing (AQP) based on sampling is critical for supportingtimely and cost-effective analytics over big data. To be applied successfully; AQP must beaccompanied by reliable estimates on the quality of sample-produced approximate answers;the two main techniques used in the past for this purpose are (i) closed-form analytic errorestimation; and (ii) the bootstrap method. Approach (i) is extremely efficient but lacksgenerality; whereas (ii) is general but suffers from high computational overhead. Ourrecently introduced Analytical Bootstrap method combines the strengths of both approachesand provides the basis for our ABS system; which will be demonstrated at the conference.The ABS system models bootstrap by a probabilistic relational model; and extends relationalalgebra with operations on probabilistic relations to predict the distributions of the AQP …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,22
Visualization-aware sampling for very large databases,Yongjoo Park; Michael Cafarella; Barzan Mozafari,Interactive visualizations are crucial in ad hoc data exploration and analysis. However; withthe growing number of massive datasets; generating visualizations in interactive timescalesis increasingly challenging. One approach for improving the speed of the visualization tool isvia data reduction in order to reduce the computational overhead; but at a potential cost invisualization accuracy. Common data reduction techniques; such as uniform and stratifiedsampling; do not exploit the fact that the sampled tuples will be transformed into avisualization for human consumption. We propose a visualization-aware sampling (VAS)that guarantees high quality visualizations with a small subset of the entire dataset. Wevalidate our method when applied to scatter and map plots for three common visualizationgoals: regression; density estimation; and clustering. The key to our sampling method's …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,20
High-performance complex event processing over hierarchical data,Barzan Mozafari; Kai Zeng; Loris D'antoni; Carlo Zaniolo,Abstract While Complex Event Processing (CEP) constitutes a considerable portion of the so-called Big Data analytics; current CEP systems can only process data having a simplestructure; and are otherwise limited in their ability to efficiently support complex continuousqueries on structured or semistructured information. However; XML-like streams represent avery popular form of data exchange; comprising large portions of social network and RSSfeeds; financial feeds; configuration files; and similar applications requiring advanced CEPqueries. In this article; we present the XSeq language and system that support CEP on XMLstreams; via an extension of XPath that is both powerful and amenable to an efficientimplementation. Specifically; the XSeq language extends XPath with natural operators toexpress sequential and Kleene-* patterns over XML streams; while remaining highly …,ACM Transactions on Database Systems (TODS),2013,20
CliffGuard: A principled framework for finding robust database designs,Barzan Mozafari; Eugene Zhen Ye Goh; Dong Young Yoon,Abstract A fundamental problem in database systems is choosing the best physical design;ie; a small set of auxiliary structures that enable the fastest execution of future queries.Almost all commercial databases come with designer tools that create a number of indicesor materialized views (together comprising the physical design) that they exploit duringquery processing. Existing designers are what we call nominal; that is; they assume thattheir input parameters are precisely known and equal to some nominal values. For instance;since future workload is often not known a priori; it is common for these tools to optimize forpast workloads in hopes that future queries and data will be similar. In practice; however;these parameters are often noisy or missing. Since nominal designers do not take theinfluence of such uncertainties into account; they find designs that are sub-optimal and …,Proceedings of the 2015 ACM SIGMOD international conference on management of data,2015,19
From regular expressions to nested words: Unifying languages and query execution for relational and xml sequences,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract There is growing interest in query language extensions for pattern matching overevent streams and stored database sequences; due to the many important applications thatsuch extensions make possible. The push for such extensions has led DBMS vendors andDSMS venture companies to propose Kleene-closure extensions of SQL standards; buildingon seminal research that demonstrated the effectiveness and amenability to efficientimplementation of such constructs. These extensions; however powerful; suffer fromlimitations that severely impair their effectiveness in many real-world applications. Toovercome these problems; we have designed the K* SQL language and system; based onour investigation of the nested words; which are recent models that generalize both wordsand trees. K* SQL extends the existing relational sequence languages; and also enables …,Proceedings of the VLDB Endowment,2010,17
Neighbor-sensitive hashing,Yongjoo Park; Michael Cafarella; Barzan Mozafari,Abstract Approximate kNN (k-nearest neighbor) techniques using binary hash functions areamong the most commonly used approaches for overcoming the prohibitive cost ofperforming exact kNN queries. However; the success of these techniques largely dependson their hash functions' ability to distinguish kNN items; that is; the kNN items retrievedbased on data items' hashcodes; should include as many true kNN items as possible. Awidely-adopted principle for this process is to ensure that similar items are assigned to thesame hashcode so that the items with the hashcodes similar to a query's hashcode are likelyto be true neighbors. In this work; we abandon this heavily-utilized principle and pursue theopposite direction for generating more effective hash functions for kNN tasks. That is; we aimto increase the distance between similar items in the hashcode space; instead of …,Proceedings of the VLDB Endowment,2015,11
K* sql: A unifying engine for sequence patterns and xml,Barzan Mozafari; Kai Zeng; Carlo Zaniolo,Abstract A strong interest is emerging in SQL extensions for sequence patterns using Kleene-closure expressions. This burst of interest from both the research community and thecommercial world is due to the many database and data stream applications made possibleby these extensions; including financial services; RFID-based inventory management; andelectronic health systems. In this demo we will present the K* SQL system that represents amajor step forward in this area. K* SQL supports a more expressive language that allows forgeneralized Kleene-closure queries and also achieves the expressive power of the nestedword model; which greatly expands the application domain to include XML queries; softwaretrace analysis; and genomics. In this demo; we first introduce the core features of ourlanguage in expressing complex pattern queries over both relational and XML data. We …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,11
A data stream mining system,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,On-line data stream mining has attracted much research interest; but systems that can beused as a workbench for online mining have not been researched; since they pose manydifficult research challenges. The proposed system addresses these challenges by anarchitecture based on three main technical advances;(i) introduction of new constructs andsynoptic data structures whereby complex KDD queries can be easily expressed andefficiently supported;(ii) an integrated library of mining algorithms that are fast & light enoughto be effective on data streams; and (iii) support for Mining Model Definition Language(MMDL) that allows users to define new mining algorithms as a set of tasks and flows. Thus;the proposed system provides an extensible workbench for online mining; which is beyondthe existing proposals for even static mining.,Data Mining Workshops; 2008. ICDMW'08. IEEE International Conference on,2008,11
Snappydata: A hybrid transactional analytical store built on spark,Jags Ramnarayan; Barzan Mozafari; Sumedh Wale; Sudhir Menon; Neeraj Kumar; Hemant Bhanawat; Soubhik Chakraborty; Yogesh Mahajan; Rishitesh Mishra; Kishor Bachhav,Abstract In recent years; our customers have expressed frustration in the traditionalapproach of using a combination of disparate products to handle their streaming;transactional and analytical needs. The common practice of stitching heterogeneousenvironments in custom ways has caused enormous production woes by increasingdevelopment complexity and total cost of ownership. With SnappyData; an open sourceplatform; we propose a unified engine for real-time operational analytics; delivering streamanalytics; OLTP and OLAP in a single integrated solution. We realize this platform through aseamless integration of Apache Spark (as a big data computational engine) with GemFire(as an in-memory transactional store with scale-out SQL semantics). In this demonstration;after presenting a few use case scenarios; we exhibit SnappyData as our our in-memory …,Proceedings of the 2016 International Conference on Management of Data,2016,9
SnappyData: A Unified Cluster for Streaming; Transactions and Interactice Analytics.,Barzan Mozafari; Jags Ramnarayan; Sudhir Menon; Yogesh Mahajan; Soubhik Chakraborty; Hemant Bhanawat; Kishor Bachhav,ABSTRACT Many modern applications are a mixture of streaming; transactional andanalytical workloads. However; traditional data platforms are each designed for supporting aspecific type of workload. The lack of a single platform to support all these workloads hasforced users to combine disparate products in custom ways. The common practice ofstitching heterogeneous environments has caused enormous production woes byincreasing complexity and the total cost of ownership. To support this class of applications;we present SnappyData as the first unified engine capable of delivering analytics;transactions; and stream processing in a single integrated cluster. We build this hybridengine by carefully marrying a big data computational engine (Apache Spark) with a scale-out transactional store (Apache GemFire). We study and address the challenges involved …,CIDR,2017,8
DBSherlock: A Performance Diagnostic Tool for Transactional Databases,Dong Young Yoon; Ning Niu; Barzan Mozafari,Abstract Running an online transaction processing (OLTP) system is one of the mostdaunting tasks required of database administrators (DBAs). As businesses rely on OLTPdatabases to support their mission-critical and real-time applications; poor databaseperformance directly impacts their revenue and user experience. As a result; DBAsconstantly monitor; diagnose; and rectify any performance decays. Unfortunately; themanual process of debugging and diagnosing OLTP performance problems is extremelytedious and non-trivial. Rather than being caused by a single slow query; performanceproblems in OLTP databases are often due to a large number of concurrent and competingtransactions adding up to compounded; non-linear effects that are difficult to isolate. Suddenchanges in request volume; transactional patterns; network traffic; or data distribution can …,SIGMOD,2016,8
Database Learning: Toward a Database that Becomes Smarter Every Time,Yongjoo Park; Ahmad Shahab Tajik; Michael Cafarella; Barzan Mozafari,Abstract In today's databases; previous query answers rarely benefit answering futurequeries. For the first time; to the best of our knowledge; we change this paradigm in anapproximate query processing (AQP) context. We make the following observation: theanswer to each query reveals some degree of knowledge about the answer to another querybecause their answers stem from the same underlying distribution that has produced theentire dataset. Exploiting and refining this knowledge should allow us to answer queriesmore analytically; rather than by reading enormous amounts of raw data. Also; processingmore queries should continuously enhance our knowledge of the underlying distribution;and hence lead to increasingly faster response times for future queries. We call this novelidea---learning from past query answers---Database Learning. We exploit the principle of …,ACM SIGMOD,2017,7
Publishing naive Bayesian Classifiers: Privacy without accuracy loss,Barzan Mozafari; Carlo Zaniolo,Abstract We address the problem of publishing a Naïve Bayesian Classifier (NBC) or;equivalently; publishing the necessary views for building an NBC; while protecting privacy ofthe individuals who provided the training data. Our approach completely preserves theaccuracy of the original classifier; and thus significantly improves on current approaches;such as randomization or anonymization; which typically degrade accuracy to preserveprivacy. Current query-view security checkers address the question of'Is the view safe topublish?'and are computationally expensive (often Π p 2-complete). Here instead; we tacklethe question of'How to make a view safe to publish?'and propose a linear-time algorithm topublish safe NBC-enabling views. We first show that a simple measure that restricts theratios between the published NBC statistics is sufficient to prevent any breach of privacy …,Proceedings of the VLDB Endowment,2009,7
DBSeer: Pain-free database administration through workload intelligence,Dong Young Yoon; Barzan Mozafari; Douglas P Brown,Abstract The pressing need for achieving and maintaining high performance in databasesystems has made database administration one of the most stressful jobs in informationtechnology. On the other hand; the increasing complexity of database systems has madequalified database administrators (DBAs) a scarce resource. DBAs are now responsible foran array of demanding tasks; they need to (i) provision and tune their database according totheir application requirements;(ii) constantly monitor their database for any performancefailures or slowdowns;(iii) diagnose the root cause of the performance problem in anaccurate and timely fashion; and (iv) take prompt actions that can restore acceptabledatabase performance. However; much of the research in the past years has focused onimproving the raw performance of the database systems; rather than improving their …,Proceedings of the VLDB Endowment,2015,6
Verdict: A System for Stochastic Query Planning,Barzan Mozafari,Online services; wireless devices; and scientific simulations are all creating massivevolumes of data at unprecedented rates. This abundance of rich datasets has made data-driven discovery the predominant approach across biology; medicine; physics; economicsand even social sciences. Ironically; existing data processing tools have now become thebottleneck in data-driven activities. When faced with large-enough datasets (say; a fewterabytes); even the fastest database systems can take hours or days to answer the simplestqueries (see [1]). This response time is simply unacceptable to many users and applications.The data-driven discovery is often an interactive and iterative process: data scientists form ahypothesis; consult the data; adjust their hypothesis accordingly; and repeat this processuntil a satisfactory answer is discovered. Thus; slow and costly interactions with data can …,Conference on Innovative Data Systems Research (CIDR),2015,5
Continuous post-mining of association rules in a data stream management system,Hetal Thakkar; Barzan Mozafari; Carlo Zaniolo,The real-time (or just-on-time) requirement associated with online association rule miningimplies the need to expedite the analysis and validation of the many candidate rules; whichare typically created from the discovered frequent patterns. Moreover; the mining process;from data cleaning to post-mining; can no longer be structured as a sequence of stepsperformed by the analyst; but must be streamlined into a workflow supported by an efficientsystem providing quality of service guarantees that are expected from modern Data StreamManagement Systems (DSMSs). This chapter describes the architecture and techniquesused to achieve this advanced functionality in the Stream Mill Miner (SMM) prototype; anSQL-based DSMS designed to support continuous mining queries. introduction systems;much ofrecentresearch work has focused,Post-Mining of Association Rules: Techniques for Effective Knowledge Extraction,2009,5
An efficient recursive algorithm and an explicit formula for calculating update vectors of running Walsh-Hadamard transform,Barzan Mozafari; Mohammad H Savoji,Walsh-Hadamard transform (WHT) has many applications in digital signal processingincluding bioinformatics. While there are efficient algorithms for implementing this transformsuch as fast WHT (FWHT); performing WHT continuously on a sliding window over a longsequence is time consuming. As it is not reasonable to compute a separate WHT uponarrival of every new sample; another implementation named running WHT (RWHT) hasbeen introduced in (G. Deng and A. Ling; 1996) which needs to have some update vectorspre-calculated. In this paper we report on an efficient recursive algorithm to find thesevectors. Also we propose an easy-to-compute explicit formula for computing coefficients ofthese update vectors in a computation time independent of the vector size. A proof of theformula and a comparison between the proposed recursive algorithm and an algorithm …,Signal Processing and Its Applications; 2007. ISSPA 2007. 9th International Symposium on,2007,5
A new collision resistant hash function based on optimum dimensionality reduction using Walsh-Hadamard transform,Barzan Mozafari; Mohammad Hasan Savoji,Hash functions play the most important role in various cryptologic applications; ranging fromdata integrity checking to digital signatures. Our goal is to introduce a new hash functionusing Walsh-Hadamard transform for achieving dimensionality reduction (compression) witha regular and one-way distribution. Merkle-Damgard (MD) transform is applied to thiscompression function in order to turn it into a hash function. Our algorithm has a flexibleframework in which some parameters and steps could be changed according to differentneeds for more security or less computation time. Our emphasis is on its resistance againstsome variations of birthday attack. We evaluate collision resistant behavior of this algorithmfor some configurations by calculating the balance factor. As we will see our balance factoris very close to SHA-1's. We present some experimental results to determine the balance …,Information Technology; 2006. ICIT'06. 9th International Conference on,2006,5
A top-down approach to achieving performance predictability in database systems,Jiamin Huang; Barzan Mozafari; Grant Schoenebeck; Thomas F Wenisch,Abstract While much of the research on transaction processing has focused on improvingoverall performance in terms of throughput and mean latency; surprisingly less attention hasbeen given to performance predictability: how often individual transactions exhibit executionlatency far from the mean. Performance predictability is increasingly important whentransactions lie on the critical path of latency-sensitive applications; enterprise software; orinteractive web services. In this paper; we focus on understanding and mitigating thesources of performance unpredictability in today's transactional databases. We conduct thefirst quantitative study of major sources of variance in MySQL; Postgres (two of the largestand most popular open-source products on the market); and VoltDB (a non-conventionaldatabase). We carry out our study with a tool called TProfiler that; given the source code …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,4
Cliffguard: An extended report,Barzan Mozafari; Eugene Zhen Ye Goh; Dong Young Yoon,ABSTRACT A fundamental problem in database systems is choosing the best physicaldesign; ie; a small set of auxiliary structures that enable the fastest execution of futurequeries. Almost all commercial databases come with designer tools that create a number ofindices or materialized views (together comprising the physical design) that they exploitduring query processing. Existing designers are what we call nominal; that is; they assumethat their input parameters are precisely known and equal to some nominal values. Forinstance; since future workload is often not known a priori; it is common for these tools tooptimize for past workloads in hopes that future queries and data will be similar. In practice;however; these parameters are often noisy or missing. Since nominal designers do not takethe influence of such uncertainties into account; they find designs that are sub-optimal …,*,2015,4
Approximate query engines: Commercial challenges and research opportunities,Barzan Mozafari,Abstract Recent years have witnessed a surge of interest in Approximate Query Processing(AQP) solutions; both in academia and the commercial world. In addition to well-known openproblems in this area; there are many new research challenges that have surfaced as aresult of the first interaction of AQP technology with commercial and real-world customers.We categorize these into deployment; planning; and interface challenges. At the same time;AQP settings introduce many interesting opportunities that would not be possible in adatabase with precise answers. These opportunities create hopes for overcoming some ofthe major limitations of traditional database systems. For example; we discuss how adatabase can reuse its past work in a generic way; and become smarter as it answers newqueries. Our goal in this talk is to suggest some of the exciting research directions in this …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
Statistical analysis of latency through semantic profiling,Jiamin Huang; Barzan Mozafari; Thomas F Wenisch,Abstract Most software profiling tools quantify average performance and rely on a program'scontrol flow graph to organize and report results. However; in interactive server applications;performance predictability is often an equally important measure. Moreover; the end user isoften concerned with the performance of a semantically defined interval of execution; suchas a request or transaction; which may not directly map to any single function in the callgraph; especially in high-performance applications that use asynchrony or event-basedprogramming. It is difficult to distinguish functionality that lies on the critical path of asemantic interval from other activity (eg; periodic logging or side operations) that maynevertheless appear prominent in a conventional profile. Existing profilers lack the ability to(i) aggregate results for a semantic interval and (ii) attribute its performance variance to …,Proceedings of the Twelfth European Conference on Computer Systems,2017,2
Complex pattern matching in complex structures: the xseq approach,Kai Zeng; Mohan Yang; Barzan Mozafari; Carlo Zaniolo,There is much current interest in applications of complex event processing over datastreams and of complex pattern matching over stored sequences. While some applicationsuse streams of flat records; XML and various semi-structured information formats arepreferred by many others-in particular; applications that deal with domain science; socialnetworks; RSS feeds; and finance. XSeq and its system improve complex pattern matchingtechnology significantly; both in terms of expressive power and efficient implementation.XSeq achieves higher expressiveness through an extension of XPath based on Kleene-*pattern constructs; and achieves very efficient execution; on both stored and streaming data;using Visibly Pushdown Automata (VPA). In our demo; we will (i) show examples of XSeq indifferent application domains;(ii) explain its compilation/query optimization techniques …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,2
Identifying the Major Sources of Variance in Transaction Latencies: Towards More Predictable Databases,Jiamin Huang; Barzan Mozafari; Grant Schoenebeck; Thomas Wenisch,Abstract: Decades of research have sought to improve transaction processing performanceand scalability in database management systems (DBMSs). However; significantly lessattention has been dedicated to the predictability of performance: how often individualtransactions exhibit execution latency far from the mean? Performance predictability is vitalwhen transaction processing lies on the critical path of a complex enterprise software or aninteractive web service; as well as in emerging database-as-a-service markets wherecustomers contract for guaranteed levels of performance. In this paper; we take severalsteps towards achieving more predictable database systems. First; we propose a profilingframework called VProfiler that; given the source code of a DBMS; is able to identify thedominant sources of variance in transaction latency. VProfiler automatically instruments …,arXiv preprint arXiv:1602.01871,2016,1
Extending relational query languages for data streams,Nikolay Laptev; Barzan Mozafari; Hamid Mousavi; Hetal Thakkar; Haixun Wang; Kai Zeng; Carlo Zaniolo,Abstract The design of continuous query languages for data streams and the extent to whichthese should rely on database query languages represent pivotal issues for data streammanagement systems (DSMSs). The Expressive Stream Language (ESL) of our Stream Millsystem is designed to maximize the spectrum of applications a DSMS can support efficiently;while retaining compatibility with the SQL: 2003 standards. This approach offers significantadvantages; particularly for the many applications that span both data streams anddatabases. Therefore; ESL supports minimal extensions required to overcome SQL'sexpressive power limitations—a critical enhancement since said limitations are quite severeon database applications and are further exacerbated on data stream applications; where;eg; only nonblocking query operators can be used. Thus; ESL builds on user-defined …,*,2016,1
On the Evolution of Wikipedia,and Junghoo Cho Rodrigo B. Almeida; Barzan Mozafari,*,International Conference on Weblogs and Social Media (ICWSM),2007,1
Ensuring Authorized Updates in Multi-user Database-Backed Applications,Kevin Eykholt; Atul Prakash; Barzan Mozafari,Abstract Database-backed applications rely on access control policies based on views toprotect sensitive data from unauthorized parties. Current techniques assume that theapplication's database tables contain a column that enables mapping a user to rows in thetable. This assumption allows database views or similar mechanisms to enforce per-useraccess controls. However; not all database tables contain sufficient information to map auser to rows in the table; as a result of database normalization; and thus; require the joiningof multiple tables. In a survey of 10 popular open-source web applications; on average; 21%of the database tables require a join. This means that current techniques cannot enforcesecurity policies on all update queries for these applications; due to a well-known viewupdate problem. In this paper; we propose phantom extraction; a technique; which …,26th {USENIX} Security Symposium ({USENIX} Security 17),2017,*
Contention-aware lock scheduling for transactional databases,Boyu Tian; Jiamin Huang; Barzan Mozafari; Grant Schoenebeck,ABSTRACT Lock managers are among the most well-studied components in concurrencycontrol and transactional systems. However; one question seems to have been generallyoverlooked:“when there are multiple lock requests on the same object; which one (s) shouldbe granted first?” Nearly all existing systems rely on a FIFO (first in; first out) strategy todecide which transaction (s) to grant the lock to. However; in this paper; we show that thechoice of lock scheduling has significant ramifications on the overall performance of atransactional system. Despite the large body of research on job scheduling outside thedatabase context; lock scheduling presents subtle but challenging requirements that renderexisting results on scheduling inapt for a transactional database. By carefully studying thisproblem; we present the concept of contention-aware scheduling; show the hardness of …,*,2017,*
Approximate Query Engines,Barzan Mozafari,Page 1. 1 © SnappyData Inc. 2017 Approximate Query Engines Commercial Challenges andResearch Opportunities © SIGMOD 2017 University of Michigan; Ann Arbor | SnappyData Inc.Barzan Mozafari Page 2. What Is Approximate Query Processing? Exact Result ComputationApproximate Result I/O Less I/O Less Computation Page 3. Why ApproximaBon? 1. ProducBvity2. Money (Time + Resources) Numerous studies : A latency >2 seconds is no longer interactiveand negatively affects creativity! Human Lme : Money Machine Lme : No one loves their EC2 bill!Massive Market for InteracLve-speed AnalyLcs! ? Page 4. ApproximaLon seems to be a viablepath to interacLvity InteracBve AnalyBcs: Myth or Reality? Q : What about in-memory & columnarDBs? A : Try running a few OLAP queries concurrently on 100GB of data parBBoned across a fewnodes! So ware Inefficiencies excessive copying/ serialization in …,*,2017,*
Workshop on cloud data management (CloudDM)-Front matter,Barzan Mozafari; Ippokratis Pandis,Conference proceedings front matter may contain various advertisements; welcomemessages; committee or program information; and other miscellaneous conferenceinformation. This may in some cases also include the cover art; table of contents; copyrightstatements; title-page or half title-pages; blank pages; venue maps or other generalinformation relating to the conference that was part of the original conference proceedings.,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,*
High-performance complex event processing over {XML} streams and other hierarchical data,Barzan Mozafari; Kai Zeng; Carlo Zaniolo; Loris D’Antoni,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: Mostrare; Search; Register; Log in …,*,2012,*
High-Performance Pattern Detection and Discovery for Databases and Data Streams,Barzan Mozafari,Abstract At the heart of a myriad of data-intensive applications lies the need for (quasi)realtime detection and discovery of complex patterns of interest. Online fraud detection; click-stream analysis; RFID monitoring and stock market analysis are only a few suchapplications. This assortment of complex applications along with recent advances inmonitoring technologies have increased the demand for high-performance systems andtechniques that can detect the existing patterns and discover new ones from massivevolumes of data. These have created many new research challenges and; particularly whenprocessing data streams. For instance; streaming algorithms should only scan the dataonce; must cope with bursty arrivals of data; and provide incremental results in an onlinefashion. Some of these objectives are also highly desirable for large volumes of stored …,*,2011,*
CliffGuard: Towards Workload-Resilient Database Designs,Barzan Mozafari,ABSTRACT A fundamental problem in database systems is choosing the best physicaldesign; ie; a small set of auxiliary structures that enable the fastest execution of futurequeries. Almost all commercial databases come with designer tools that create a number ofindices or materialized views (together comprising the physical design) that they exploitduring query processing. Existing designers are what we call nominal; that is; they assumethat their input parameters are precisely known and equal to some nominal values. Forinstance; since future workload is often not known a priori; it is common for these tools tooptimize for past workloads in hopes that future queries and data will be similar. In practice;however; these parameters are often noisy or missing. Since nominal designers do not takethe influence of such uncertainties into account; they find designs that are sub-optimal …,*,*,*
