Infrastructure for e-government web services,Brahim Medjahed; Abdelmounaam Rezgui; Athman Bouguettaya; Mourad Ouzzani,In efforts to use information and communication technologies for the civil and politicalconduct of government; many countries have begun supporting e-government initiatives.The ultimate goal is to improve government-citizen interactions through an infrastructurebuilt around the" life experience" of citizens. To facilitate the use of welfare applications andexpeditiously satisfy citizens' needs; we wrapped these applications in modular Webservices. Adopting Web services in e-government enables government agencies to providevalue-added services by defining a new service that outsources from other e-governmentservices; to uniformly handle privacy issues: and to standardize the description; discovery;and invocation of social programs. The core of our research is to develop techniques toefficiently access e-government services while preserving citizens' privacy. To that end …,IEEE Internet Computing,2003,210
Efficient access to web services,Mourad Ouzzani; Athman Bouguettaya,For Web services to expand across the Internet; users need to be able to efficiently accessand share Web services. The authors present a query infrastructure that treats Web servicesas first-class objects. It evaluates queries through the invocations of different Web serviceoperations. Because efficiency plays a central role in such evaluations; the authors proposea query optimization model based on aggregating the quality of Web service (QoWS)parameters of different Web services. The model adjusts QoWS through a dynamic ratingscheme and multilevel matching in which the rating provides an assessment of Webservices' behavior. Multilevel matching allows the expansion of the solution space byenabling similar and partial answers.,IEEE Internet Computing,2004,185
NADEEF: a commodity data cleaning system,Michele Dallachiesa; Amr Ebaid; Ahmed Eldawy; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Nan Tang,Abstract Despite the increasing importance of data quality and the rich theoretical andpractical contributions in all aspects of data cleaning; there is no single end-to-end off-the-shelf solution to (semi-) automate the detection and the repairing of violations wrt a set ofheterogeneous and ad-hoc quality constraints. In short; there is no commodity platformsimilar to general purpose DBMSs that can be easily customized and deployed to solveapplication-specific data quality problems. In this paper; we present NADEEF; an extensible;generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between aprogramming interface and a core to achieve generality and extensibility. The programminginterface allows the users to specify multiple types of data quality rules; which uniformlydefine what is wrong with the data and (possibly) how to repair it through writing code that …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,136
Guided data repair,Mohamed Yakout; Ahmed K Elmagarmid; Jennifer Neville; Mourad Ouzzani; Ihab F Ilyas,Abstract In this paper we present GDR; a Guided Data Repair framework that incorporatesuser feedback in the cleaning process to enhance and accelerate existing automatic repairtechniques while minimizing user involvement. GDR consults the user on the updates thatare most likely to be beneficial in improving data quality. GDR also uses machine learningmethods to identify and apply the correct updates directly to the database without the actualinvolvement of the user on these specific updates. To rank potential updates for consultationby the user; we first group these repairs and quantify the utility of each group using thedecision-theory concept of value of information (VOI). We then apply active learning to orderupdates within a group based on their ability to improve the learned model. User feedback isused to repair the database and to adaptively refine the training set for the model. We …,Proceedings of the VLDB Endowment,2011,124
Purdue ionomics information management system. An integrated functional genomics platform,Ivan Baxter; Mourad Ouzzani; Seza Orcun; Brad Kennedy; Shrinivas S Jandhyala; David E Salt,The advent of high-throughput phenotyping technologies has created a deluge ofinformation that is difficult to deal with without the appropriate data management tools.These data management tools should integrate defined workflow controls for genomic-scaledata acquisition and validation; data storage and retrieval; and data analysis; indexedaround the genomic information of the organism of interest. To maximize the impact of theselarge datasets; it is critical that they are rapidly disseminated to the broader researchcommunity; allowing open access for data mining and discovery. We describe here a systemthat incorporates such functionalities developed around the Purdue University high-throughput ionomics phenotyping platform. The Purdue Ionomics Information ManagementSystem (PiiMS) provides integrated workflow control; data storage; and analysis to …,Plant Physiology,2007,122
Preserving privacy in web services,Abdelmounaam Rezgui; Mourad Ouzzani; Athman Bouguettaya; Brahim Medjahed,Abstract Web services are increasingly being adopted as a viable means to access Web-based applications. This has been enabled by the tremendous standardization effort todescribe; advertise; discover; and invoke Web services. Digital government (DG) is a majorapplication domain for Web services. It aims at improving government-citizen interactionsusing information and communication technologies. Government agencies collect; store;process; and share information about millions of citizens who have different preferencesregarding their privacy. This naturally raises a number of legal and technical issues thatmust be addressed to preserve citizens' privacy through the control of the information flowamongst different entities (users; Web services; DBMSs). Solutions addressing this issue arestill in their infancy. They consist; essentially; of enforcing privacy by law or by self …,Proceedings of the 4th international workshop on Web information and data management,2002,119
A visual analytics approach to understanding spatiotemporal hotspots,Ross Maciejewski; Stephen Rudolph; Ryan Hafen; Ahmad Abusalah; Mohamed Yakout; Mourad Ouzzani; William S Cleveland; Shaun J Grannis; David S Ebert,As data sources become larger and more complex; the ability to effectively explore andanalyze patterns among varying sources becomes a critical bottleneck in analytic reasoning.Incoming data contain multiple variables; high signal-to-noise ratio; and a degree ofuncertainty; all of which hinder exploration; hypothesis generation/exploration; and decisionmaking. To facilitate the exploration of such data; advanced tool sets are needed that allowthe user to interact with their data in a visual environment that provides direct analyticcapability for finding data aberrations or hotspots. In this paper; we present a suite of toolsdesigned to facilitate the exploration of spatiotemporal data sets. Our system allows users tosearch for hotspots in both space and time; combining linked views and interactive filteringto provide users with contextual information about their data and allow the user to …,IEEE Transactions on Visualization and Computer Graphics,2010,113
Data pre-processing in liquid chromatography–mass spectrometry-based proteomics,Xiang Zhang; John M Asara; Jiri Adamec; Mourad Ouzzani; Ahmed K Elmagarmid,Abstract Motivation: In a liquid chromatography–mass spectrometry (LC–MS)-basedexpressional proteomics; multiple samples from different groups are analyzed in parallel. Itis necessary to develop a data mining system to perform peak quantification; peak alignmentand data quality assurance. Results: We have developed an algorithm for spectrumdeconvolution. A two-step alignment algorithm is proposed for recognizing peaks generatedby the same peptide but detected in different samples. The quality of LC–MS data isevaluated using statistical tests and alignment quality tests. Availability: Xalign software isavailable upon request from the author. Contact: zhang100@ purdue. edu,Bioinformatics,2005,93
Access control enforcement for conversation-based web services,Massimo Mecella; Mourad Ouzzani; Federica Paci; Elisa Bertino,Abstract Service Oriented Computing is emerging as the main approach to build distributedenterprise applications on the Web. The widespread use of Web services is hindered by thelack of adequate security and privacy support. In this paper; we present a novel frameworkfor enforcing access control in conversation-based Web services. Our approach takes intoaccount the conversational nature of Web services. This is in contrast with existingapproaches to access control enforcement that assume a Web service as a set ofindependent operations. Furthermore; our approach achieves a tradeoff between the needto protect Web service's access control policies and the need to disclose to clients theportion of access control policies related to the conversations they are interested in. This isimportant to avoid situations where the client cannot progress in the conversation due to …,Proceedings of the 15th international conference on World Wide Web,2006,68
Privometer: Privacy protection in social networks,Nilothpal Talukder; Mourad Ouzzani; Ahmed K Elmagarmid; Hazem Elmeleegy; Mohamed Yakout,The increasing popularity of social networks; such as Facebook and Orkut; has raisedseveral privacy concerns. Traditional ways of safeguarding privacy of personal informationby hiding sensitive attributes are no longer adequate. Research shows that probabilisticclassification techniques can effectively infer such private information. The disclosedsensitive information of friends; group affiliations and even participation in activities; such astagging and commenting; are considered background knowledge in this process. In thispaper; we present a privacy protection tool; called Privometer; that measures the amount ofsensitive information leakage in a user profile and suggests self-sanitization actions toregulate the amount of leakage. In contrast to previous research; where inferencetechniques use publicly available profile information; we consider an augmented model …,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,64
Katara: A data cleaning system powered by knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Classical approaches to clean data have relied on using integrity constraints;statistics; or machine learning. These approaches are known to be limited in the cleaningaccuracy; which can usually be improved by consulting master data and involving experts toresolve ambiguity. The advent of knowledge bases KBs both general-purpose and withinenterprises; and crowdsourcing marketplaces are providing yet more opportunities toachieve higher accuracy at a larger scale. We propose KATARA; a knowledge base andcrowd powered data cleaning system that; given a table; a KB; and a crowd; interprets tablesemantics to align it with the KB; identifies correct and incorrect data; and generates top-kpossible repairs for incorrect data. Experiments show that KATARA can be applied tovarious datasets and KBs; and can efficiently annotate data and suggest possible repairs.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,63
Behavior based record linkage,Mohamed Yakout; Ahmed K Elmagarmid; Hazem Elmeleegy; Mourad Ouzzani; Alan Qi,Abstract In this paper; we present a new record linkage approach that uses entity behavior todecide if potentially different entities are in fact the same. An entity's behavior is extractedfrom a transaction log that records the actions of this entity with respect to a given datasource. The core of our approach is a technique that merges the behavior of two possiblematched entities and computes the gain in recognizing behavior patterns as their matchingscore. The idea is that if we obtain a well recognized behavior after merge; then most likely;the original two behaviors belong to the same entity as the behavior becomes morecomplete after the merge. We present the necessary algorithms to model entities' behaviorand compute a matching score for them. To improve the computational efficiency of ourapproach; we precede the actual matching phase with a fast candidate generation that …,Proceedings of the VLDB Endowment,2010,57
Query processing and optimization on the web,Mourad Ouzzani; Athman Bouguettaya,Abstract The advent of the Internet and the Web and their subsequent ubiquity have broughtforth opportunities to connect information sources across all types of boundaries (local;regional; organizational; etc.). Examples of such information sources include databases;XML documents; and other unstructured sources. Uniformly querying those informationsources has been extensively investigated. A major challenge relates to query optimization.Indeed; querying multiple information sources scattered on the Web raises several barriersfor achieving efficiency. This is due to the characteristics of Web information sources thatinclude volatility; heterogeneity; and autonomy. Those characteristics impede astraightforward application of classical query optimization techniques. They add newdimensions to the optimization problem such as the choice of objective function; selection …,Distributed and Parallel Databases,2004,57
Supporting dynamic interactions among web-based information sources,Athman Bouguettaya; Boualem Benatallah; Lily Hendra; Mourad Ouzzani; James Beard,The ubiquity of the World Wide Web offers an ideal opportunity for the deployment of highlydistributed applications. Now that connectivity is no longer an issue; attention has turned toproviding a middleware infrastructure that will sustain data sharing among Web-accessibledatabases. We present a dynamic architecture and system for describing; locating; andaccessing data from Web-accessible databases. We propose the use of flexibleorganizational constructs service links and coalitions to facilitate data organization;discovery; and sharing among Internet-accessible databases. A language is also proposedto support the definition and manipulation of these constructs. The implementation combinesJava; CORBA; database API (JDBC); agent; and database technologies to support ascalable and portable architecture interconnecting large networks of heterogeneous and …,IEEE Transactions on Knowledge and Data Engineering,2000,55
M3: Stream processing on main-memory mapreduce,Ahmed M Aly; Asmaa Sallam; Bala M Gnanasekaran; Long-Van Nguyen-Dinh; Walid G Aref; Mourad Ouzzani; Arif Ghafoor,The continuous growth of social web applications along with the development of sensorcapabilities in electronic devices is creating countless opportunities to analyze theenormous amounts of data that is continuously steaming from these applications anddevices. To process large scale data on large scale computing clusters; MapReduce hasbeen introduced as a framework for parallel computing. However; most of the currentimplementations of the MapReduce framework support only the execution of fixed-inputjobs. Such restriction makes these implementations inapplicable for most streamingapplications; in which queries are continuous in nature; and input data streams arecontinuously received at high arrival rates. In this demonstration; we showcase M3; aprototype implementation of the MapReduce framework in which continuous queries over …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,54
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,52
Usage-based schema matching,Hazem Elmeleegy; Mourad Ouzzani; Ahmed Elmagarmid,Existing techniques for schema matching are classified as either schema-based; instance-based; or a combination of both. In this paper; we define a new class of techniques; calledusage-based schema matching. The idea is to exploit information extracted from the querylogs to find correspondences between attributes in the schemas to be matched. We proposemethods to identify co-occurrence patterns between attributes in addition to other featuressuch as their use in joins and with aggregate functions. Several scoring functions areconsidered to measure the similarity of the extracted features; and a genetic algorithm isemployed to find the highest-score mappings between the two schemas. Our technique issuitable for matching schemas even when their attribute names are opaque. It can further becombined with existing techniques to obtain more accurate results. Our experimental …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,51
BDBMS--a database management system for biological data,Mohamed Y Eltabakh; Mourad Ouzzani; Walid G Aref,Abstract: Biologists are increasingly using databases for storing and managing their data.Biological databases typically consist of a mixture of raw data; metadata; sequences;annotations; and related data obtained from various sources. Current database technologylacks several functionalities that are needed by biological databases. In this paper; weintroduce bdbms; an extensible prototype database management system for supportingbiological data. bdbms extends the functionalities of current DBMSs to include:(1)Annotation and provenance management including storage; indexing; manipulation; andquerying of annotation and provenance as first class objects in bdbms;(2) Local dependencytracking to track the dependencies and derivations among data items;(3) Updateauthorization to support data curation via content-based authorization; in contrast to …,arXiv preprint cs/0612127,2006,48
Syndromic surveillance: STL for modeling; visualizing; and monitoring disease counts,Ryan P Hafen; David E Anderson; William S Cleveland; Ross Maciejewski; David S Ebert; Ahmad Abusalah; Mohamed Yakout; Mourad Ouzzani; Shaun J Grannis,Public health surveillance is the monitoring of data to detect and quantify unusual healthevents. Monitoring pre-diagnostic data; such as emergency department (ED) patient chiefcomplaints; enables rapid detection of disease outbreaks. There are many sources ofvariation in such data; statistical methods need to accurately model them as a basis fortimely and accurate disease outbreak methods. Our new methods for modeling daily chiefcomplaint counts are based on a seasonal-trend decomposition procedure based on loess(STL) and were developed using data from the 76 EDs of the Indiana surveillance programfrom 2004 to 2008. Square root counts are decomposed into inter-annual; yearly-seasonal;day-of-the-week; and random-error components. Using this decomposition method; wedevelop a new synoptic-scale (days to weeks) outbreak detection method and carry out a …,BMC Medical Informatics and Decision Making,2009,42
Rayyan—a web and mobile app for systematic reviews,Mourad Ouzzani; Hossam Hammady; Zbys Fedorowicz; Ahmed Elmagarmid,Synthesis of multiple randomized controlled trials (RCTs) in a systematic review cansummarize the effects of individual outcomes and provide numerical answers about theeffectiveness of interventions. Filtering of searches is time consuming; and no single methodfulfills the principal requirements of speed with accuracy. Automation of systematic reviewsis driven by a necessity to expedite the availability of current best evidence for policy andclinical decision-making. We developed Rayyan (http://rayyan. qcri. org); a free web andmobile app; that helps expedite the initial screening of abstracts and titles using a process ofsemi-automation while incorporating a high level of usability. For the beta testing phase; weused two published Cochrane reviews in which included studies had been selectedmanually. Their searches; with 1030 records and 273 records; were uploaded to Rayyan …,Systematic reviews,2016,41
Supporting annotations on relations,Mohamed Y Eltabakh; Walid G Aref; Ahmed K Elmagarmid; Mourad Ouzzani; Yasin N Silva,Abstract Annotations play a key role in understanding and curating databases. Annotationsmay represent comments; descriptions; lineage information; among several others.Annotation management is a vital mechanism for sharing knowledge and building aninteractive and collaborative environment among database users and scientists. Whatmakes it challenging is that annotations can be attached to database entities at variousgranularities; eg; at the table; tuple; column; cell levels; or more generally; to any subset ofcells that results from a select statement. Therefore; simple comment fields in tuples wouldnot work because of the combinatorial nature of the annotations. In this paper; we presentextensions to current database management systems to support annotations. We proposestorage schemes to efficiently store annotations at multiple granularities; ie; at the table …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,34
Managing government databases,Athman Bouguettaya; Mourad Ouzzani; Brahim Medjahed; Jerry Cameron,Abstract The information revolution has led organizations to rely on databases to conducttheir daily business. The authors explore Indiana's digital government project and explainhow using Web-based techniques to retrieve data can help disadvantaged citizens becomeself-reliant. The authors recommend that empowering novice and experienced users tosubmit queries over database networks provides a sophisticated infrastructure that supportsflexible tools for managing and accessing Internet databases. They propose usingdistributed ontologies of information repositories to meet this challenge. They suggest anorganization and segmentation of databases based on simple ontologies that describecoherent slices of the information space. These distributed ontologies filter interactions;accelerate information searches; and permit data sharing in a tractable manner. The …,Computer,2001,34
Descriptive and Prescriptive Data Cleaning,Anup Chalamalla; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti,Abstract Data cleaning techniques usually rely on some quality rules to identify violatingtuples; and then fix these violations using some repair algorithms. Oftentimes; the rules;which are related to the business logic; can only be defined on some target report generatedby transformations over multiple data sources. This creates a situation where the violationsdetected in the report are decoupled in space and time from the actual source of errors. Inaddition; applying the repair on the report would need to be repeated whenever the datasources change. Finally; even if repairing the report is possible and affordable; this would beof little help towards identifying and analyzing the actual sources of errors for futureprevention of violations at the target. In this paper; we propose a system to address thisdecoupling. The system takes quality rules defined over the output of a transformation …,Proceedings of the 2014 international conference on Management of data,2014,32
Understanding syndromic hotspots-a visual analytics approach,Ross Maciejewski; Stephen Rudolph; Ryan Hafen; Ahmad Abusalah; Mohamed Yakout; Mourad Ouzzani; William S Cleveland; Shaun J Grannis; Michael Wade; David S Ebert,When analyzing syndromic surveillance data; health care officials look for areas withunusually high cases of syndromes. Unfortunately; many outbreaks are difficult to detectbecause their signal is obscured by the statistical noise. Consequently; many detectionalgorithms have a high false positive rate. While many false alerts can be easily filtered bytrained epidemiologists; others require health officials to drill down into the data; analyzingspecific segments of the population and historical trends over time and space. Furthermore;the ability to accurately recognize meaningful patterns in the data becomes morechallenging as these data sources increase in volume and complexity. To facilitate moreaccurate and efficient event detection; we have created a visual analytics tool that providesanalysts with linked geo-spatiotemporal and statistical analytic views. We model …,Visual Analytics Science and Technology; 2008. VAST'08. IEEE Symposium on,2008,32
High-resolution genome-wide scan of genes; gene-networks and cellular systems impacting the yeast ionome,Danni Yu; John MC Danku; Ivan Baxter; Sungjin Kim; Olena K Vatamaniuk; Olga Vitek; Mourad Ouzzani; David E Salt,To balance the demand for uptake of essential elements with their potential toxicity livingcells have complex regulatory mechanisms. Here; we describe a genome-wide screen toidentify genes that impact the elemental composition ('ionome') of yeast Saccharomycescerevisiae. Using inductively coupled plasma–mass spectrometry (ICP-MS) we quantify Ca;Cd; Co; Cu; Fe; K; Mg; Mn; Mo; Na; Ni; P; S and Zn in 11890 mutant strains; including 4940haploid and 1127 diploid deletion strains; and 5798 over expression strains. We identified1065 strains with an altered ionome; including 584 haploid and 35 diploid deletion strains;and 446 over expression strains. Disruption of protein metabolism or trafficking has thehighest likelihood of causing large ionomic changes; with gene dosage also beingimportant. Gene over expression produced more extreme ionomic changes; but over …,BMC genomics,2012,28
A two-phase framework for quality-aware web service selection,Qi Yu; Manjeet Rege; Athman Bouguettaya; Brahim Medjahed; Mourad Ouzzani,Abstract Service-oriented computing is gaining momentum as the next technological tool toleverage the huge investments in Web application development. The expected largenumber of Web services poses a set of new challenges for efficiently accessing theseservices. We propose an integrated service query framework that facilitates users inaccessing their desired services. The framework incorporates a service query model and atwo-phase optimization strategy. The query model defines service communities that areused to organize the large and heterogeneous service space. The service communitiesallow users to use declarative queries to retrieve their desired services without worryingabout the underlying technical details. The two-phase optimization strategy automaticallygenerates feasible service execution plans and selects the plan with the best user …,Service Oriented Computing and Applications,2010,28
Verification of access control requirements in web services choreography,Federica Paci; Mourad Ouzzani; Massimo Mecella,Web services choreography is used to design peer-to-peer applications where each peer ispotentially a Web service. It defines the required behavior of participating Web servicesalong with their interactions through message exchanges. Implementing a complex systemdescribed by a choreography requires selecting actual Web services whose individualbehaviors are compatible with the overall behavior described by the choreography.Although the selected Web services implement the specified behavior; they may not be ableto interact due to the policies they enforce to protect their resources. A Web service'resourcecan be an operation or a credential type to be submitted to be able to invoke an operation. Inthis paper; we propose a novel approach to determine at design time whether achoreography can be implemented by a set of Web services based on their access …,Services Computing; 2008. SCC'08. IEEE International Conference on,2008,28
Efficient delivery of web services,Mourad Ouzzani,This dissertation addresses issues for the efficient access to Web databases and services.We propose a distributed ontology for a meaningful organization of and efficient access toWeb databases. Next; we dedicate most of our work on presenting a comprehensive queryinfrastructure for the emerging concept of Web services. The core of this query infrastructureis to enable the efficient delivery of Web services based on the concept of Quality of WebService. Treating Web services as first class objects is a fundamental step towards achievingthe envisioned Semantic Web. Semantics-aware processing of information requiresintensive use of Web services. In our research; we propose a new query model wherequeries are resolved by combining Web service invocations. To efficiently deploy suchscheme; we propose an optimization strategy based on aggregating Quality of Web …,*,2004,28
Ontological approach for information discovery in internet databases,Mourad Ouzzani; Boualem Benatallah; Athman Bouguettaya,Abstract The Internet has solved the age-old problem of network connectivity and thusenabling the potential access to; and data sharing among large numbers of databases.However; enabling users to discover useful information requires an adequate metadatainfrastructure that must scale with the diversity and dynamism of both users' interests andInternet accessible databases. In this paper; we present a model that partitions theinformation space into a distributed; highly specialized domain ontologies. We alsointroduce inter-ontology relationships to cater for user-based interests across ontologiesdefined over Internet databases. We also describe an architecture that implements these twofundamental constructs over Internet databases. The aim of the proposed model andarchitecture is to eventually facilitate data discovery and sharing for Internet databases.,Distributed and Parallel Databases,2000,27
Locationspark: A distributed in-memory data management system for big spatial data,Mingjie Tang; Yongyang Yu; Qutaibah M Malluhi; Mourad Ouzzani; Walid G Aref,Abstract We present LocationSpark; a spatial data processing system built on top of ApacheSpark; a widely used distributed data processing system. LocationSpark offers a rich set ofspatial query operators; eg; range search; kNN; spatio-textual operation; spatial-join; andkNN-join. To achieve high performance; LocationSpark employs various spatial indexes forin-memory data; and guarantees that immutable spatial indexes have low overhead withfault tolerance. In addition; we build two new layers over Spark; namely a query schedulerand a query executor. The query scheduler is responsible for mitigating skew in spatialqueries; while the query executor selects the best plan based on the indexes and the natureof the spatial queries. Furthermore; to avoid unnecessary network communication overheadwhen processing overlapped spatial data; We embed an efficient spatial Bloom filter into …,Proceedings of the VLDB Endowment,2016,26
Managing biological data using bdbms,Mohamed Y Eltabakh; Mourad Ouzzani; Walid G Aref; Ahmed K Elmagarmid; Yasin Laura-Silva; Muhammad U Arshad; David Salt; Ivan Baxter,We demonstrate bdbms; an extensible database engine for biological databases. bdbmsstarted on the observation that database technology has not kept pace with the specificrequirements of biological databases and that several needed key functionalities are notsupported at the engine level. While bdbms aims at supporting several of thesefunctionalities; this demo focuses on:(1) Annotation and provenance management includingstorage; indexing; querying; and propagation;(2) Local dependency tracking ofdependencies and derivations among data items; and (3) Update authorization to supportdata curation. We demonstrate how bdbms enables biologists to manipulate theirdatabases; annotations; and derivation information in a unified database system using thePurdue Ionomics Information Management System (PiiMS) as a case study.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,26
AQWA: adaptive query workload aware partitioning of big spatial data,Ahmed M Aly; Ahmed R Mahmood; Mohamed S Hassan; Walid G Aref; Mourad Ouzzani; Hazem Elmeleegy; Thamir Qadah,Abstract The unprecedented spread of location-aware devices has resulted in a plethora oflocation-based services in which huge amounts of spatial data need to be efficientlyprocessed by large-scale computing clusters. Existing cluster-based systems for processingspatial data employ static data-partitioning structures that cannot adapt to data changes; andthat are insensitive to the query workload. Hence; these systems are incapable ofconsistently providing good performance. To close this gap; we present AQWA; an adaptiveand query-workload-aware mechanism for partitioning large-scale spatial data. AQWA doesnot assume prior knowledge of the data distribution or the query workload. Instead; as datais consumed and queries are processed; the data partitions are incrementally updated. Withextensive experiments using real spatial data from Twitter; and various workloads of …,Proceedings of the VLDB Endowment,2015,25
NADEEF: A generalized data cleaning system,Amr Ebaid; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang; Si Yin,Abstract We present NADEEF; an extensible; generic and easy-to-deploy data cleaningsystem. NADEEF distinguishes between a programming interface and a core to achievegenerality and extensibility. The programming interface allows users to specify data qualityrules by writing code that implements predefined classes. These classes uniformly definewhat is wrong with the data and (possibly) how to fix it. We will demonstrate the followingfeatures provided by NADEEF.(1) Heterogeneity: The programming interface can be used toexpress many types of data quality rules beyond the well known CFDs (FDs); MDs and ETLrules.(2) Interdependency: The core algorithms can interleave multiple types of rules todetect and repair data errors.(3) Deployment and extensibility: Users can easily customizeNADEEF by defining new types of rules; or by extending the core.(4) Metadata …,Proceedings of the VLDB Endowment,2013,24
GDR: a system for guided data repair,Mohamed Yakout; Ahmed K Elmagarmid; Jennifer Neville; Mourad Ouzzani,Abstract Improving data quality is a time-consuming; labor-intensive and often domainspecific operation. Existing data repair approaches are either fully automated or not efficientin interactively involving the users. We present a demo of GDR; a Guided Data Repairsystem that uses a novel approach to efficiently involve the user alongside automatic datarepair techniques to reach better data quality as quickly as possible. Specifically; GDRgenerates data repairs and acquire feedback on them that would be most beneficial inimproving the data quality. GDR quantifies the data quality benefit of generated repairs bycombining mechanisms from decision theory and active learning. Based on these benefitscores; groups of repairs are ranked and displayed to the user. User feedback is used totrain a machine learning component to eventually replace the user in deciding on the …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,24
Incremental mining for frequent patterns in evolving time series datatabases,Mohamed Y Eltabakh; Mourad Ouzzani; Mohamed A Khalil; Walid G Aref; Ahmed K Elmagarmid,Abstract Several emerging applications warrant mining and discovering hidden frequentpatterns in time series databases; eg; sensor networks; environment monitoring; andinventory stock monitoring. Time series databases are characterized by two features:(1) Thecontinuous arrival of data and (2) the time dimension. These features raise new challengesfor data mining such as the need for online processing and incremental evaluation of themining results. In this paper; we address the problem of discovering frequent patterns indatabases with multiple time series. We propose an incremental technique for discoveringthe complete set of frequent patterns; ie; discovering the frequent patterns over the entiretime series in contrast to a sliding window over a portion of the time series. The proposedapproach updates the mining results with the arrival of every new data item by …,*,2008,23
Ontology-based support for digital government,Athman Bouguettaya; Ahmed Elmagarmid; Brahim Medjahed; Mourad Ouzzani,1 Introduction Digital government can be defined as the civil and political conduct ofgovernment using information and communication technologies [4; 6]. The Web has laid;among other emerging technologies; a broad foundation for digital government. A recentstudy conducted by The Council for Excellence in Government indicates that" 66% ofAmericans favor the President's appointing a high-level official who would oversee andencourage government use of the Web and other technologies to make governmentservices and information more readily available to the public"[2]. The development oftechniques to efficiently access government databases and services is at the core of ourresearch in digital government. For that purpose; we have teamed with the Indiana Familyand Social Services Administration (FSSA) to help the needy citizens collect social …,VLDB,2001,23
Detecting Data Errors: Where are we and what needs to be done?,Ziawasch Abedjan; Xu Chu; Dong Deng; Raul Castro Fernandez; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker; Nan Tang,Abstract Data cleaning has played a critical role in ensuring data quality for enterpriseapplications. Naturally; there has been extensive research in this area; and many datacleaning algorithms have been translated into tools to detect and to possibly repair certainclasses of errors such as outliers; duplicates; missing values; and violations of integrityconstraints. Since different types of errors may coexist in the same data set; we often need torun more than one kind of tool. In this paper; we investigate two pragmatic questions:(1) arethese tools robust enough to capture most errors in real-world data sets? and (2) what is thebest strategy to holistically run multiple tools to optimize the detection effort? To answerthese two questions; we obtained multiple data cleaning tools that utilize a variety of errordetection techniques. We also collected five real-world data sets; for which we could …,Proceedings of the VLDB Endowment,2016,22
Lahva: Linked animal-human health visual analytics,Ross Maciejewski; Benjamin Tyner; Yun Jang; Cheng Zheng; Rimma V Nehme; David S Ebert; William S Cleveland; Mourad Ouzzani; Shaun J Grannis; Lawrence T Glickman,Coordinated animal-human health monitoring can provide an early warning system withfewer false alarms for naturally occurring disease outbreaks; as well as biological; chemicaland environmental incidents. This monitoring requires the integration and analysis of multi-field; multi-scale and multi-source data sets. In order to better understand these data sets;models and measurements at different resolutions must be analyzed. To facilitate theseinvestigations; we have created an application to provide a visual analytics framework foranalyzing both human emergency room data and veterinary hospital data. Our integratedvisual analytic tool links temporally varying geospatial visualization of animal and humanpatient health information with advanced statistical analysis of these multi-source data.Various statistical analysis techniques have been applied in conjunction with a spatio …,Visual Analytics Science and Technology; 2007. VAST 2007. IEEE Symposium on,2007,22
Rayyan: a systematic reviews web app for exploring and filtering searches for eligible studies for Cochrane Reviews,Ahmed Elmagarmid; Zbys Fedorowicz; Hossam Hammady; Ilyas Ihab; Madian Khabsa; Mourad Ouzzani,*,Evidence-Informed Public Health: Opportunities and Challenges. Abstracts of the 22nd Cochrane Colloquium,2014,17
WebDG–a platform for e-government web services,Athman Bouguettaya; Brahim Medjahed; Abdelmounaam Rezgui; Mourad Ouzzani; Xumin Liu; Qi Yu,Abstract Web services are deemed as the natural choice for deploying e-governmentapplications. Their use enables e-government to fully get advantage of the envisionedSemantic Web. In this paper; we propose WebDG; a comprehensive Web ServiceManagement System for e-government applications. It aims to improve government-citizeninteractions through an infrastructure built around the” life experience” of citizens. WebDGprovides a framework for automatically composing e-government services; optimizedquerying services; and preserving privacy.,International Conference on Conceptual Modeling,2004,17
Semantic web enabled e-government services,Brahim Medjahed; Athman Bouguettaya; Mourad Ouzzani,Abstract We propose a novel approach for organizing and describing e-government serviceson the envisioned Semantic Web. We combine the emerging concepts of Web services andontologies to cater for Semantic Web enabled e-government services. This would lay theground for the automatic selection; interoperation; and composition of e-governmentservices.,Proceedings of the 2003 annual national conference on Digital government research,2003,17
WEBFINDIT: An architecture and system for querying web databases,Athman Bouguettaya; B Bentallah; Mourad Ouzzani; Lily Hendra,The Web offers a single user interface to data sharing across heterogeneous andautonomous databases; but it was not designed to handle the rigid DBMS protocols anddata formats used by relational and object-oriented databases. WebFindIt is an ongoingproject to develop the database equivalent of the World Wide Web-namely; a World WideDatabase-through a middleware infrastructure for describing; locating; and accessing datafrom any kind of Web-accessible database. A special-purpose language; Web-Tassili;supports the definition and manipulation of middleware constructs for organizing theinformation space. An implementation of WebFindIt combines Java; CORBA and databasetechnologies.,IEEE Internet Computing,1999,17
Temporal rules discovery for web data cleaning,Ziawasch Abedjan; Cuneyt G Akcora; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract Declarative rules; such as functional dependencies; are widely used for cleaningdata. Several systems take them as input for detecting errors and computing a" clean"version of the data. To support domain experts; in specifying these rules; several tools havebeen proposed to profile the data and mine rules. However; existing discovery techniqueshave traditionally ignored the time dimension. Recurrent events; such as persons reported inlocations; have a duration in which they are valid; and this duration should be part of therules or the cleaning process would simply fail. In this work; we study the rule discoveryproblem for temporal web data. Such a discovery process is challenging because of thenature of web data; extracted facts are (i) sparse over time;(ii) reported with delays; and (iii)often reported with errors over the values because of inaccurate sources or non robust …,Proceedings of the VLDB Endowment,2015,16
Spatial queries with two kNN predicates,Ahmed M Aly; Walid G Aref; Mourad Ouzzani,Abstract The widespread use of location-aware devices has led to countless location-basedservices in which a user query can be arbitrarily complex; ie; one that embeds multiplespatial selection and join predicates. Amongst these predicates; the k-Nearest-Neighbor(kNN) predicate stands as one of the most important and widely used predicates. Unlikerelated research; this paper goes beyond the optimization of queries with single kNNpredicates; and shows how queries with two kNN predicates can be optimized. In particular;the paper addresses the optimization of queries with:(i) two kNN-select predicates;(ii) twokNN-join predicates; and (iii) one kNN-join predicate and one kNN-select predicate. Foreach type of queries; conceptually correct query evaluation plans (QEPs) and newalgorithms that optimize the query execution time are presented. Experimental results …,Proceedings of the VLDB Endowment,2012,16
Lightning fast and space efficient inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which join relational tables on inequality conditions; are used invarious applications. While there have been a wide range of optimization methods for joinsin database systems; from algorithms such as sort-merge join and band join; to variousindices such as B+-tree; R*-tree and Bitmap; inequality joins have received little attentionand queries containing such joins are usually very slow. In this paper; we introduce fastinequality join algorithms. We put columns to be joined in sorted arrays and we usepermutation arrays to encode positions of tuples in one sorted array wrt the other sortedarray. In contrast to sort-merge join; we use space efficient bit-arrays that enableoptimizations; such as Bloom filter indices; for fast computation of the join results. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; and a …,Proceedings of the VLDB Endowment,2015,15
Using Java and CORBA for implementing Internet databases,Athman Bouguettaya; Boualem Benatallah; Mourad Ouzzani; Lily Hendra,We describe an architecture called WebFINDIT that allows dynamic couplings of Webaccessible databases based on their content and interest. We propose an implementationusing WWW; Java; JDBC; and CORBA's ORBs that communicate via the CORBA's IIOPprotocol. The combination of these technologies offers a compelling middlewareinfrastructure to implement fluid-area enterprise applications. In addition to a discussion ofWebFINDIT's core concepts and implementation architecture; we also discuss anexperience of exiting WebFINDIT in a healthcare application.,Data Engineering; 1999. Proceedings.; 15th International Conference on,1999,15
A service computing manifesto: the next 10 years,A Bouguettaya; M Singh; M Huhns; Q Sheng; H Dong; Q Yu; A Ghari Neiat; S Mistry; B Benatallah; B Medjahed; M Ouzzani; F Casati; X Liu; H Wang; D Georgakopoulos; L Chen; S Nepal; Z Malik; A Erradi; Y Wang; B Blake; S Dustdar; F Leymann; M Papazoglou,Y GUI JUN PENG shifts the focus from infrastructure and operations to services. But; as weargue here; service computing has not fully reached its potential. Technological advancesprovide an increasing opportunity for service computing. To avoid the mistakes of the past;we propose a manifestoa—a community declaration of objectives and approaches—as away to establish common ground among researchers in the field and to guide its futuredevelopment. 3 a This manifesto is the culmination of a workshop that was organized atRMIT University (Melbourne; Australia) on Dec 8–9; 2014. A number of experts in servicecomputing from around the world attended. The document was also circulated among otherkey leaders for further input; giving rise to this manifesto. This manifesto first takes stock ofthe current state of service computing and then maps out a strategy for leveraging …,Communications of the ACM,2017,14
Dataxformer: An interactive data transformation tool,John Morcos; Ziawasch Abedjan; Ihab Francis Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract While syntactic transformations require the application of a formula on the inputvalues; such as unit conversion or date format conversions; semantic transformations; suchas" zip code to city"; require a look-up in some reference data. We recently presentedDataXFormer; a system that leverages Web tables; Web forms; and expert sourcing to covera wide range of transformations. In this demonstration; we present the user-interaction withDataXFormer and show scenarios on how it can be used to transform data and explore theeffectiveness and efficiency of several approaches for transformation discovery; leveragingabout 112 million tables and online sources.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,14
Road to Freedom in Big Data Analytics.,Divy Agrawal; Sanjay Chawla; Ahmed K Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,ABSTRACT The world is fast moving towards a data-driven society where data is the mostvaluable asset. Organizations need to perform very diverse analytic tasks using various dataprocessing platforms. In doing so; they face many challenges; chiefly; platform dependence;poor interoperability; and poor performance when using multiple platforms. We presentRHEEM; our vision for big data analytics over diverse data processing platforms. RHEEMprovides a threelayer data processing and storage abstraction to achieve both platformindependence and interoperability across multiple platforms. In this paper; we discuss ourvision as well as present multiple research challenges that we need to address to achieve it.As a case in point; we present a data cleaning application built using some of the ideas ofRHEEM. We show how it achieves platform independence and the performance benefits …,EDBT,2016,13
Dataxformer: Leveraging the Web for Semantic Transformations.,Ziawasch Abedjan; John Morcos; Michael N Gubanov; Ihab F Ilyas; Michael Stonebraker; Paolo Papotti; Mourad Ouzzani,ABSTRACT Data transformation is a crucial step in data integration. While sometransformations; such as liters to gallons; can be easily performed by applying a formula or aprogram on the input values; others; such as zip code to city; require sifting through arepository containing explicit value mappings. There are already powerful systems thatprovide formulae and algorithms for transformations. However; the automated identificationof reference datasets to support value mapping remains largely unresolved. The Web ishome to millions of tables with many containing explicit value mappings. This is in additionto value mappings hidden behind Web forms. In this paper; we present DataXFormer; atransformation engine that leverages Web tables and Web forms to perform transformationtasks. In particular; we describe an inductive; filter-refine approach for identifying explicit …,CIDR,2015,13
ACConv--An Access Control Model for Conversational Web Services,Federica Paci; Massimo Mecella; Mourad Ouzzani; Elisa Bertino,Abstract With organizations increasingly depending on Web services to build complexapplications; security and privacy concerns including the protection of access controlpolicies are becoming a serious issue. Ideally; service providers would like to make sure thatclients have knowledge of only portions of the access control policy relevant to theirinteractions to the extent to which they are entrusted by the Web service and withoutrestricting the client's choices in terms of which operations to execute. We propose ACConv;a novel model for access control in Web services that is suitable when interactions betweenthe client and the Web service are conversational and long-running. The conversation-based access control model proposed in this article allows service providers to limit howmuch knowledge clients have about the credentials specified in their access policies. This …,ACM Transactions on the Web (TWEB),2011,13
The Data Civilizer System.,Dong Deng; Raul Castro Fernandez; Ziawasch Abedjan; Sibo Wang; Michael Stonebraker; Ahmed K Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Nan Tang,ABSTRACT In many organizations; it is often challenging for users to find relevant data forspecific tasks; since the data is usually scattered across the enterprise and ofteninconsistent. In fact; data scientists routinely report that the majority of their effort is spentfinding; cleaning; integrating; and accessing data of interest to a task at hand. In order todecrease the “grunt work” needed to facilitate the analysis of data “in the wild”; we presentDATA CIVILIZER; an end-to-end big data management system. DATA CIVILIZER has alinkage graph computation module to build a linkage graph for the data and a data discoverymodule which utilizes the linkage graph to help identify data that is relevant to user tasks. Italso uses the linkage graph to discover possible join paths that can then be used in a query.For the actual query execution; we use a polystore DBMS; which federates query …,CIDR,2017,12
Challenges in spatiotemporal stream query optimization,Hicham G Elmongui; Mourad Ouzzani; Walid G Aref,Abstract Simplified technology and low costs have spurred the use of location-detectiondevices in moving objects. Usually; these devices will send the moving objects' locationinformation to a spatio-temporal data stream management system; which will be thenresponsible for answering spatio-temporal queries related to these moving objects. A largespectrum of research have been devoted to continuous spatio-temporal query processing.However; we argue that several outstanding challenges have been either addressedpartially or not at all in the existing literature. In particular; in this paper; we focus on theoptimization of multi-predicate spatio-temporal queries on moving objects. We presentseveral major challenges related to the lack of spatio-temporal pipelined operators; and theimpact of time; space; and their combination on the query plan optimality under different …,Proceedings of the 5th ACM international workshop on Data engineering for wireless and mobile access,2006,12
Internet computing support for digital government,Athman Bouguettaya; Abdelmounaam Rezgui; Brahim Medjahed; Mourad Ouzzani,The Web has changed many aspects of our everyday life. The e-revolution has had anunparalleled impact on how people live; communicate; and interact with businesses andgovernment agencies. As a result; many well-established functions of modern society arebeing rethought and redeployed. Amongst all of these functions; the Government function isone where the Web impact is the most tangible. Governments are the most complexorganizations in a society. They provide the legal; political; and economic infrastructure tosupport the daily needs of citizens and businesses [Bouguettaya et al.; 2002]. A governmentgenerally consists of large and complex networks of institutions and agencies. The Web isprogressively; but radically; changing the traditional mechanisms in which these institutionsand agencies operate and interoperate. More importantly; the Web is redefining the …,Practical Handbook of Internet Computing; Chapman & Hall/CRC,2004,12
Supporting data and services access in digital government environments,Athman Bouguettaya; Mourad Ouzzani; Brahim Medjahed; Ahmed K Elmagarmid,Abstract We describe a Web-based architecture; named WebDG; to support emergingDigital Government applications. We use an ontological approach to organize governmentdata and services (applications). The data space is segmented into logically inter-relatedclusters of databases to accelerate metadata and data discovery. Equally; services aresegmented into vocabulary-based taxonomies to provide a fast mechanism for their retrievaland enactment. As a proof of concept; we use government social services; namely; theIndiana Family and Social Services Administration (FSSA); as a case study for thedeployment of our novel techniques. The implementation uses several state-of-the-art; Web-based middleware and e-services technologies as building blocks for WebDG. Theseinclude; CORBA; EJB; and HP's e-speakproducts.,*,2002,12
Learning to identify relevant studies for systematic reviews using random forest and external information,Madian Khabsa; Ahmed Elmagarmid; Ihab Ilyas; Hossam Hammady; Mourad Ouzzani,Abstract We tackle the problem of automatically filtering studies while preparing SystematicReviews (SRs) which normally entails manually inspecting thousands of studies to identifythe few to be included. The problem is modeled as an imbalanced data classification taskwhere the cost of misclassifying the minority class is higher than the cost of misclassifyingthe majority class. This work introduces a novel method for representing systematic reviewsbased not only on lexical features; but also utilizing word clustering and citation features.This novel representation is shown to outperform previously used features in representingsystematic reviews; regardless of the classifier. Our work utilizes a random forest classifierwith the novel features to accurately predict included studies with high recall. Theparameters of the random forest are automatically configured using heuristics methods …,Machine Learning,2016,11
Development and assessment of an interactive web-based breastfeeding monitoring system (LACTOR),Azza Ahmed; Mourad Ouzzani,Abstract The purpose of this study is to describe an interactive web-based breastfeedingmonitoring system (LACTOR); illustrate its components; explain the theoretical framework;and discuss its assessment as a model for an innovative breastfeeding support intervention.Based on the self-regulation model from Bandura Social Cognitive Theory; we havedeveloped an interactive web-based breastfeeding monitoring system using a breastfeedingdiary. The system has two main components: the Mothers' Portal; where mothers can entertheir breastfeeding data and receive notifications; and the Lactation Consultants' Portal;where mothers' data can be monitored. The system is designed to send notifications tomothers in case of breastfeeding problems using triggers such as inability to latch; sleepyinfant; jaundice; and maternal sore nipples. A prospective; descriptive; mixed methods …,Maternal and child health journal,2013,11
The proteome discovery pipeline-a data analysis pipeline for mass spectrometry-based differential proteomics discovery,Catherine P Riley; Erik S Gough; Jing He; Shrinivas S Jandhyala; Brad Kennedy; Seza Orcun; Mourad Ouzzani; Charles Buck; Ali M Roumani; Xiang Zhang,Abstract: Proteomics approaches enable interrogation of large numbers of proteins toprovide a more comprehensive understanding of biological systems. High throughputproteomics typically utilizes liquid chromatography–mass spectrometry technology for dataacquisition. Bioinformatic analysis tools are essential to manage and mine resulting highvolume proteomics data sets. Data analysis is a current bottleneck for many proteomicsresearchers because complete and freely accessible already-developed systems are notavailable. In addition; most analysis systems require experienced bioinformatician inputimmediately upon data acquisition. For proteomics to achieve greatest impact in biology;data analysis must be more efficient and effective. We present the Proteome DiscoveryPipeline (PDP); a web-based analysis platform that provides proteomics data analysis …,The Open Proteomics Journal,2010,11
Rheem: Enabling multi-platform task execution,Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawla; Ahmed Elmagarmid; Hossam Hammady; Yasser Idris; Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,Abstract Many emerging applications; from domains such as healthcare and oil & gas;require several data processing systems for complex analytics. This demo paper showcasessystem; a framework that provides multi-platform task execution for such applications. Itfeatures a three-layer data processing abstraction and a new query optimization approachfor multi-platform settings. We will demonstrate the strengths of system by using real-worldscenarios from three different applications; namely; machine learning; data cleaning; anddata fusion.,Proceedings of the 2016 International Conference on Management of Data,2016,10
Similarity group-by operators for multi-dimensional relational data,Mingjie Tang; Ruby Y Tahboub; Walid G Aref; Mikhail J Atallah; Qutaibah M Malluhi; Mourad Ouzzani; Yasin N Silva,The SQL group-by operator plays an important role in summarizing and aggregating largedatasets in a data analytics stack. While the standard group-by operator; which is based onequality; is useful in several applications; allowing similarity aware grouping provides amore realistic view on real-world data that could lead to better insights. The Similarity SQL-based Group-By operator (SGB; for short) extends the semantics of the standard SQL Group-by by grouping data with similar but not necessarily equal values. While existing similarity-based grouping operators efficiently realize these approximate semantics; they primarilyfocus on one-dimensional attributes and treat multi-dimensional attributes independently.However; correlated attributes; such as in spatial data; are processed independently; andhence; groups in the multi-dimensional space are not detected properly. To address this …,IEEE Transactions on Knowledge and Data Engineering,2016,10
Author disambiguation by hierarchical agglomerative clustering with adaptive stopping criterion,Lei Cen; Eduard C Dragut; Luo Si; Mourad Ouzzani,Abstract Entity disambiguation is an important step in many information retrievalapplications. This paper proposes new research for entity disambiguation with the focus ofname disambiguation in digital libraries. In particular; pairwise similarity is first learned forpublications that share the same author name string (ANS) and then a novel HierarchicalAgglomerative Clustering approach with Adaptive Stopping Criterion (HACASC) is proposedto adaptively cluster a set of publications that share a same ANS to individual clusters ofpublications with different author identities. The HACASC approach utilizes a mixture ofkernel ridge regressions to intelligently determine the threshold in clustering. This obtainsmore appropriate clustering granularity than non-adaptive stopping criterion. We conduct alarge scale empirical study with a dataset of more than 2 million publication record pairs …,Proceedings of the 36th International ACM SIGIR conference on Research and development in information retrieval,2013,10
Semantic integration in geosciences,Zaki Malik; Abdelmounaam Rezgui; Brahim Medjahed; Mourad Ouzzani; A KRISHNA SINHA,We present an approach for the semantic integration of geoscience data; and a systemimplementing this approach. Specifically; we demonstrate the use of data ontologies andapplication of markup languages for semantic integration of data and services. We introducea domain level object ontology; called Earth and Planetary ONTology (EPONT) to explore;extract; and integrate information from heterogeneous geologic data sets. As proof ofconcept; we define the DIA engine; an extensible infrastructure for the Discovery; Integration;and Analysis of geoscience data; tools; and services. DIA provides a collaborativeenvironment where scientists can share their resources (eg; geochemical data; filteringservices; etc.) by registering them through well-defined ontologies. We envision the DIAinfrastructure to also use other classes of ontologies; namely process and service; for …,International Journal of Semantic Computing,2010,10
Query-time record linkage and fusion over web databases,Eduard C Dragut; Mourad Ouzzani; Ahmed K Elmagarmid,Data-intensive Web applications usually require integrating data from Web sources at querytime. The sources may refer to the same real-world entity in different ways and some mayeven provide outdated or erroneous data. An important task is to recognize and merge therecords that refer to the same real world entity at query time. Most existing duplicatedetection and fusion techniques work in the off-line setting and do not meet the onlineconstraint. There are at least two aspects that differentiate online duplicate detection andfusion from its off-line counterpart.(i) The latter assumes that the entire data is available;while the former cannot make such an assumption.(ii) Several query submissions may berequired to compute the “ideal” representation of an entity in the online setting. This paperpresents a general framework for the online setting based on an iterative record-based …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,9
NADEEF/ER: Generic and interactive entity resolution,Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,ABSTRACT Entity resolution (ER); the process of identifying and eventually merging recordsthat refer to the same real-world entities; is an important and long-standing problem. Wepresent Nadeef/Er; a generic and interactive entity resolution system; which is built as anextension over our open-source generalized data cleaning system Nadeef. Nadeef/Erprovides a rich programming interface for manipulating entities; which allows generic;efficient and extensible ER. In this demo; users will have the opportunity to experience thefollowing features:(1) Easy specification–Users can easily define ER rules with a browser-based specification; which will then be automatically transformed to various functions;treated as black-boxes by Nadeef;(2) Generality and extensibility–Users can customize theirER rules by refining and fine-tuning the above functions to achieve both effective and …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,9
Interactive web-based breastfeeding monitoring: feasibility; usability; and acceptability,Azza H Ahmed; Mourad Ouzzani,Background: Strategies that promote higher exclusive breastfeeding rate and duration arehighly recommended. To date; no study has tested the feasibility of Web-based monitoringamong breastfeeding mothers. Goals: To develop an interactive Web-based breastfeedingmonitoring system (LACTOR) and examine its feasibility; usability; and acceptability amongbreastfeeding mothers. Methods: A prospective; descriptive; mixed-methods study wasconducted. Mothers who met the study inclusion criteria were recruited from mother infantunits in 2 Midwestern hospitals in the United States. Mothers were asked to enter theirbreastfeeding data daily through the system for 30 days and then submit an online exitsurvey. This survey consisted of a system usability scale and mothers' perceptions form.Twenty-six mother/infant dyads completed the study. Results: The Feasibility of LACTOR …,Journal of Human Lactation,2012,9
Using web services in e-government applications,B Medjahed; M Ouzzani; A Bouguettaya,*,Proceedings of the 2002 National Sciences Foundation Conference on Digital Government Research,2002,9
Introduction to the special issue on data quality,Mourad Ouzzani; Paolo Papotti; Erhard Rahm,Abstract Poor data quality in databases; data warehouses; and information systems affectsevery application domain. Many data processing tasks; such as information integration; datasharing; information retrieval; information extraction; and knowledge discovery requirevarious forms of data preparation and consolidation with complex data processingtechniques. These tasks usually assume that the data input contains no missing;inconsistent or incorrect values. This leaves a large gap between the available “dirty” dataand the machinery to effectively process the data for the application purposes. In addition;tasks such as data integration and information extraction may themselves introduce errors inthe data.,Information Systems,2013,8
Community-cyberinfrastructure-enabled discovery in science and engineering,Ahmed K Elmagarmid; Arjmand Samuel; Mourad Ouzzani,A community cyberinfrastructure would enable a new era of multidisciplinary research andcollaboration in science and engineering. With such an infrastructure; researchers couldshare knowledge and results along with computing cycles; storage; and bandwidth. Ageneric; transparent cyberinfrastructure would also foster more meaningful analyses of dataand visualization; modeling; and simulation of real-world phenomena.,Computing in Science & Engineering,2008,8
DataXFormer: A robust transformation discovery system,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,In data integration; data curation; and other data analysis tasks; users spend a considerableamount of time converting data from one representation to another. For example US dates toEuropean dates or airport codes to city names. In a previous vision paper; we presented theinitial design of DataXFormer; a system that uses web resources to assist in transformationdiscovery. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. In this paper; we presentthe full fledged system along with several extensions. In particular; we present algorithms tofind (i) transformations that entail multiple columns of input data;(ii) indirect transformationsthat are compositions of other transformations;(iii) transformations that are not functions butrather relationships; and (iv) transformations from a knowledge base of public data. We …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,7
The similarity-aware relational database set operators,Wadha J Al Marri; Qutaibah Malluhi; Mourad Ouzzani; Mingjie Tang; Walid G Aref,Abstract Identifying similarities in large datasets is an essential operation in severalapplications such as bioinformatics; pattern recognition; and data integration. To make arelational database management system similarity-aware; the core relational operators haveto be extended. While similarity-awareness has been introduced in database engines forrelational operators such as joins and group-by; little has been achieved for relational setoperators; namely Intersection; Difference; and Union. In this paper; we propose to extendthe semantics of relational set operators to take into account the similarity of values. Wedevelop efficient query processing algorithms for evaluating them; and implement theseoperators inside an open-source database system; namely PostgreSQL. By extendingseveral queries from the TPC-H benchmark to include predicates that involve similarity …,Information Systems,2016,6
Efficient Processing of Hamming-Distance-Based Similarity-Search Queries Over MapReduce,Mourad Ouzzani MingJie Tang; Yongyang Yu; Walid G. Aref; Qutaibah M. Malluhi,*,International Conference on Extending Database Technology,2015,6
Preserving privacy and fairness in peer-to-peer data integration,Hazem Elmeleegy; Mourad Ouzzani; Ahmed Elmagarmid; Ahmad Abusalah,Abstract Peer-to-peer data integration-aka Peer Data Management Systems (PDMSs)-promises to extend the classical data integration approach to the Internet scale.Unfortunately; some challenges remain before realizing this promise. One of the biggestchallenges is preserving the privacy of the exchanged data while passing through severalintermediate peers. Another challenge is protecting the mappings used for data translation.Protecting the privacy without being unfair to any of the peers is yet a third challenge. Thispaper presents a novel query answering protocol in PDMSs to address these challenges.The protocol employs a technique based on noise selection and insertion to protect thequery results; and a commutative encryption-based technique to protect the mappings andensure fairness among peers. An extensive security analysis of the protocol shows that it …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,6
A query-oriented approach for relevance in citation networks,Luam C Totti; Prasenjit Mitra; Mourad Ouzzani; Mohammed J Zaki,Abstract Finding a relevant set of publications for a given topic of interest is a challengingproblem. We propose a two-stage query-dependent approach for retrieving relevant papersgiven a keyword-based query. In the first stage; we utilize content similarity to select an initialseed set of publications; we then augment them by citation links weighted with informationsuch as citation context relevance and age-based attenuation. In the second stage; weconstruct a multi-layer graph that expands the publications subgraph by including links tothe authors; venues; and keywords. This allows us to return recommendations that are bothhighly authoritative; and also textually related to the query. We show that our stagedapproach gives superior results on three different benchmark query sets.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,5
KATARA: Reliable data cleaning with knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Data cleaning with guaranteed reliability is hard to achieve without accessingexternal sources; since the truth is not necessarily discoverable from the data at hand.Furthermore; even in the presence of external sources; mainly knowledge bases andhumans; effectively leveraging them still faces many challenges; such as aligningheterogeneous data sources and decomposing a complex task into simpler units that can beconsumed by humans. We present K atara; a novel end-to-end data cleaning systempowered by knowledge bases and crowdsourcing. Given a table; a kb; and a crowd; K atara(i) interprets the table semantics wrt the given kb;(ii) identifies correct and wrong data; and(iii) generates top-k possible repairs for the wrong data. Users will have the opportunity toexperience the following features of K atara:(1) Easy specification: Users can define a K …,Proceedings of the VLDB Endowment,2015,5
Cost Estimation of Spatial k-Nearest-Neighbor Operators.,Ahmed M Aly; Walid G Aref; Mourad Ouzzani,ABSTRACT Advances in geo-sensing technology have led to an unprecedented spread oflocation-aware devices. In turn; this has resulted into a plethora of location-based services inwhich huge amounts of spatial data need to be efficiently consumed by spatial queryprocessors. For a spatial query processor to properly choose among the various queryprocessing strategies; the cost of the spatial operators has to be estimated. In this paper; westudy the problem of estimating the cost of the spatial k-nearest-neighbor (k-NN; for short)operators; namely; k-NN-Select and k-NN-Join. Given a query that has a k-NN operator; theobjective is to estimate the number of blocks that are going to be scanned during theprocessing of this operator. Estimating the cost of a k-NN operator is challenging for severalreasons. For instance; the cost of a k-NN-Select operator is directly affected by the value …,EDBT,2015,5
Semantic web services for web databases,Mourad Ouzzani; Athman Bouguettaya,Semantic Web Services for Web Databases introduces an end-to-end framework forquerying Web databases using novel Web service querying techniques. This includes adetailed framework for the query infrastructure for Web databases and services. Casestudies are covered in the last section of this book. Semantic Web Services For WebDatabases is designed for practitioners and researchers focused on service-orientedcomputing and Web databases.,*,2011,5
Detecting inconsistencies in private data with secure function evaluation,Nilothpal Talukder; Mourad Ouzzani; Ahmed K Elmagarmid; Mohamed Yakout,Abstract—Erroneous and inconsistent data; often referred to as 'dirty data'; is a major worryfor businesses. Prevalent techniques to improve data quality consist of discovering dataquality rules; identifying records that violate those rules; and then modifying the data toeither remove those violations. Most of the work described in the literature deals with caseswhere both the data and the rules are visible to the party that is in charge of cleaning thedata. However; consider the case where two parties with data and data quality rules wish tocooperate in data cleaning under two restrictions:(1) neither of the parties is willing to sharetheir data due to its sensitive nature; and (2) the data quality rules may reveal informationabout the content of the data and may be considered as a private asset to the business. Thequestion then is how to clean the data without having to share the data or the rules. While …,*,2011,5
A Top-Down Approach for Two Level Serializability,Mourad Ouzzani; MA Atroun; NL Belkhodja,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references …[AGMS87] Rafael Alonso; Hector Garcia-Molina; Kenneth Salem: Concurrency Control and Recoveryfor Global Procedures in Federated Database Systems. IEEE Data Eng. Bull. 10(3):5-11(1987) … [GM91] H. Garcia-Molina. Global Consistency Considered Harmful for HeterogeneousDatabase Systems. In Proc. of 1st Intl. Workshop on Interoperability in Multidatabase Systems(IMS91); Kyoto; April 91 … Fast; Randomized Join-Order Selection - Why UseTransformations?,Proceedings of the 20th International Conference on Very Large Data Bases,1994,5
A demonstration of aqwa: Adaptive query-workload-aware partitioning of big spatial data,Ahmed M Aly; Ahmed S Abdelhamid; Ahmed R Mahmood; Walid G Aref; Mohamed S Hassan; Hazem Elmeleegy; Mourad Ouzzani,Abstract The ubiquity of location-aware devices; eg; smartphones and GPS devices; has ledto a plethora of location-based services in which huge amounts of geotagged informationneed to be efficiently processed by large-scale computing clusters. This demo presentsAQWA; an adaptive and query-workload-aware data partitioning mechanism for processinglarge-scale spatial data. Unlike existing cluster-based systems; eg; SpatialHadoop; thatapply static partitioning of spatial data; AQWA has the ability to react to changes in the query-workload and data distribution. A key feature of AQWA is that it does not assume priorknowledge of the query-workload or data distribution. Instead; AQWA reacts to changes inboth the data and the query-workload by incrementally updating the partitioning of the data.We demonstrate two prototypes of AQWA deployed over Hadoop and Spark. In both …,Proceedings of the VLDB Endowment,2015,4
Handson db: Managing data dependencies involving human actions,Mohamed Eltabakh; Walid Aref; Ahmed Elmagarmid; Mourad Ouzzani,Consider two values; x and y; in the database; where y= F (x). To maintain the consistency ofthe data; whenever x changes; F needs to be executed to re-compute y and update its valuein the database. This is straightforward in the case where F can be executed by the DBMS;eg; SQL or C function. In this paper; we address the more challenging case where F is ahuman action; eg; conducting a wet-lab experiment; taking manual measurements; orcollecting instrument readings. In this case; when x changes; y remains invalid (inconsistentwith the current value of x) until the human action involved in the derivation is performed andits output result is reflected into the database. Many application domains; eg; scientificapplications in biology; chemistry; and physics; contain multiple such derivations anddependencies that involve human actions. In this paper; we propose HandsOn DB; a …,IEEE Transactions on Knowledge and Data Engineering,2014,4
Duplicate Elimination in Space-partitioning Tree Indexes,Mohamed Y Eltabakh; Mourad Ouzzani; Walid G Aref,Space-partitioning trees; like the disk-based trie; quadtree; kd-tree and their variants; are afamily of access methods that index multi-dimensional objects. In the case of indexing non-zero extent objects; eg; line segments and rectangles; space-partitioning trees may replicateobjects over multiple space partitions; eg; PMR quadtree; expanded MX-CIF quadtree; andextended kd-tree. As a result; the answer to a query over these indexes may includeduplicates that need to be eliminated; ie; the same object may be reported more than once.In this paper; we propose generic duplicate elimination techniques for the class of space-partitioning trees in the context of SP-GiST; an extensible indexing framework for realizingspace-partitioning trees. The proposed techniques are embedded inside the INDEX-SCANoperator. Therefore; duplicate copies of the same object do not propagate in the query …,Scientific and Statistical Database Management; 2007. SSBDM'07. 19th International Conference on,2007,4
Optimized querying of e-government services,Mourad Ouzzani; Athman Bouguettaya; Brahim Medjahed,Abstract Web services are deemed as the natural choice for deploying e-governmentapplications. Their use would open the door for e-government to fully get advantage of theenvisioned Semantic Web. We are investigating the building of a comprehensive WebService Management System that treats Web services as first class objects. In this paper; wefocus on optimized querying of e-government services. Our approach is based on quality ofservice is adjusted through a dynamic rating scheme and multilevel matching.,Proceedings of the 2003 annual national conference on Digital government research,2003,4
Uguide: User-guided discovery of fd-detectable errors,Saravanan Thirumuruganathan; Laure Berti-Equille; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang,Abstract Error detection is the process of identifying problematic data cells that are differentfrom their ground truth. Functional dependencies (FDs) have been widely studied in supportof this process. Oftentimes; it is assumed that FDs are given by experts. Unfortunately; it isusually hard and expensive for the experts to define such FDs. In addition; automatic dataprofiling over dirty data in order to find correct FDs is known to be a hard problem. In thispaper; we propose an end-to-end solution to detect FD-detectable errors from dirty data. Thebroad intuition is that given a dirty dataset; it is feasible to automatically find approximateFDs; as well as data that is possibly erroneous. Arguably; at this point; only experts canconfirm true FDs or true errors. However; in practice; experts never have enough budget tofind all errors. Hence; our problem is; given a limited budget of expert's time; which …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
Fast and scalable inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which is to join relations with inequality conditions; are used invarious applications. Optimizing joins has been the subject of intensive research rangingfrom efficient join algorithms such as sort-merge join; to the use of efficient indices such asB^+ B+-tree; R^* R∗-tree and Bitmap. However; inequality joins have received little attentionand queries containing such joins are notably very slow. In this paper; we introduce fastinequality join algorithms based on sorted arrays and space-efficient bit-arrays. We furtherintroduce a simple method to estimate the selectivity of inequality joins which is then used tooptimize multiple predicate queries and multi-way joins. Moreover; we study an incrementalinequality join algorithm to handle scenarios where data keeps changing. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; a distributed …,The VLDB Journal,2017,3
On order-independent semantics of the similarity group-by relational database operator,Mingjie Tang; Ruby Y Tahboub; Walid G Aref; Qutaibah M Malluhi; Mourad Ouzzani,Abstract: Similarity group-by (SGB; for short) has been proposed as a relational databaseoperator to match the needs of emerging database applications. Many SGB operators thatextend SQL have been proposed in the literature; eg; similarity operators in the one-dimensional space. These operators have various semantics. Depending on how theseoperators are implemented; some of the implementations may lead to different groupings ofthe data. Hence; if SQL code is ported from one database system to another; it is notguaranteed that the code will produce the same results. In this paper; we investigate thevarious semantics for the relational similarity group-by operators in the multi-dimensionalspace. We define the class of order-independent SGB operators that produce the sameresults regardless of the order in which the input data is presented to them. Using the …,arXiv preprint arXiv:1412.4303,2014,3
The similarity-aware relational intersect database operator,Wadha J Al Marri; Qutaibah Malluhi; Mourad Ouzzani; Mingjie Tang; Walid G Aref,Abstract Identifying similarities in large datasets is an essential operation in manyapplications such as bioinformatics; pattern recognition; and data integration. To make theunderlying database system similarity-aware; the core relational operators have to beextended. Several similarity-aware relational operators have been proposed that introducesimilarity processing at the database engine level; eg; similarity joins and similarity group-by. This paper extends the semantics of the set intersection operator to operate over similarvalues. The paper describes the semantics of the similarity-based set intersection operator;and develops an efficient query processing algorithm for evaluating it. The proposedoperator is implemented inside an open-source database system; namely PostgreSQL.Several queries from the TPC-H benchmark are extended to include similarity-based set …,International Conference on Similarity Search and Applications,2014,3
Supporting real-world activities in database management systems,Mohamed Y Eltabakh; Walid G Aref; Ahmed K Elmagarmid; Yasin N Silva; Mourad Ouzzani,The cycle of processing the data in many application domains is complex and may involvereal-world activities that are external to the database; eg; wet-lab experiments; instrumentreadings; and manual measurements. These real-world activities may take long time toprepare for and to perform; and hence introduce inherently long time delays between theupdates in the database. The presence of these long delays between the updates; alongwith the need for the intermediate results to be instantly available; makes supporting real-world activities in the database engine a challenging task. In this paper; we address thesechallenges through a system that enables users to reflect their updates immediately into thedatabase while keeping track of the dependent and potentially invalid data items until theyare re-validated. The proposed system includes:(1) semantics and syntax for interfaces …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,3
Generalization of ACID Properties,Brahim Medjahed; Mourad Ouzzani; Ahmed K Elmagarmid,A simple definition is that gazetteers are dictionaries of placenames. The digital gazetteer asa component of georeferenced information systems; however; is more formally modeled. Agazetteer is defined as a collection of gazetteer entries; each of which contains; at aminimum; the tuple N; F; T where N is a place name; F is a formal expression of geographiclocation–a footprint; and T is a place type expressed with a term (or code) from a typingscheme. Applications often require; in addition; relationships between gazetteer entries;documentation of time frames; and additional information (as described below). Thegazetteer model is a type of knowledge organization system (KOS)–or ontology–which canbe modified to represent other classes of spatial-temporal information; such as named timeperiods and named events [3].,*,2009,3
Correctness criteria beyond serializability,Mourad Ouzzani; Brahim Medjahed; Ahmed K Elmagarmid,Query processing algorithms are designed to efficiently exploit the available cache units inthe memory hierarchy. Cache-conscious algorithms typically employ knowledge ofarchitectural parameters such as cache size and latency. This knowledge can be used toensure that the algorithms have good temporal and/or spatial locality on the target platform.,*,2009,3
A Query Paradigm for Web Services.,Mourad Ouzzani; Athman Bouguettaya,Abstract The widespread adoption of XML standards has prompted an intense activity toaddress Web services research issues. The ability to efficiently access and share Webservices is a critical step towards the full of techniques to address various challengingissues. Efficiently accessing and sharing Web services is critical for the full deployment ofthe new Web economy. An arca of rescarch that we arc investigating is to offer database-likequcrying facilitics whcrc Web scrvices are treated as first class objects that can bemanipudeployment of the new on-line economy. In this paper; we present a new queryparadigm over Web services. lated as if they were pieces of data... Users submit queries thatare resolved by combining invocations of Oērēs ārēTēsõlvēdbõTībining the ivõEāfisõfseveral Web service operations. A threc-level query various Web services. The challenge …,ICWS,2003,3
Adaptive web-based database communities,Athman Bouguettaya; Boualem Benatallah; Brahim Medjahed; Mourad Ouzzani; Lily Hendra,ABSTRACT The evolution into the global information infrastructure and the concomitantincrease in the available information on the Web; is offering a powerful distribution vehiclefor organizations that need to coordinate the use of multiple information sources. However;the technology to organize; search; integrate; and evolve these sources has not kept pacewith the rapid growth of the available information space. In this chapter;,Information modeling for internet applications,2002,3
A large scale study of SVM based methods for abstract screening in systematic reviews,Tanay Kumar Saha; Mourad Ouzzani; Hossam M Hammady; Ahmed K Elmagarmid,Abstract: A major task in systematic reviews is abstract screening; ie; excluding; oftenhundreds or thousand of; irrelevant citations returned from a database search based on titlesand abstracts. Thus; a systematic review platform that can automate the abstract screeningprocess is of huge importance. Several methods have been proposed for this task. However;it is very hard to clearly understand the applicability of these methods in a systematic reviewplatform because of the following challenges:(1) the use of non-overlapping metrics for theevaluation of the proposed methods;(2) usage of features that are very hard to collect;(3)using a small set of reviews for the evaluation; and (4) no solid statistical testing orequivalence grouping of the methods. In this paper; we use feature representation that canbe extracted per citation. We evaluate SVM-based methods (commonly used) on a large …,arXiv preprint arXiv:1610.00192,2016,2
Approving updates in collaborative databases,Khaleel Mershad; Qutaibah M Malluhi; Mourad Ouzzani; Mingjie Tang; Walid G Aref,Data curation activities in collaborative databases mandate that collaborators interact untilthey converge and agree on the content of their data. Typically; updates by a member of thecollaboration are made visible to all collaborators for comments but at the same time arepending the approval or rejection of the data custodian; eg; the principal scientist orinvestigator (PI). In current database technologies; approval and authorization of updates isbased solely on the identity of the user; eg; via the SQL GRANT and REVOKE commands.However; in collaborative environments; the updated data is open for collaborators fordiscussion and further editing and is finally approved or rejected by the PI based on thecontent of the data and not on the identity of the updater. In this paper; we introduce a cloud-based collaborative database system that promotes and enables collaboration and data …,Cloud Engineering (IC2E); 2015 IEEE International Conference on,2015,2
Lonomics Atlas: a tool to explore interconnected ionomic; genomic and environmental data,Eduard C Dragut; Mourad Ouzzani; Amgad Madkour; Nabeel Mohamed; Peter Baker; David E Salt,Ionomics Atlas facilitates access; analysis and interpretation of an existing large-scaleheterogeneous dataset consisting of ionomic (elemental composition of an organism);genetic (heritable changes in the DNA of an organism) and geographic information(geographic location; altitude; climate; soil properties; etc). Ionomics Atlas allowsconnections to be made between the genetic regulation of the ionome of plant populationsand their landscape distribution; allowing scientists to investigate the role of natural ionomicvariation in adaptation of populations to varied environmental conditions in the landscape.The goal of the Ionomics Atlas is twofold:(1) to allow both novice and expert users to easilyaccess and explore layers of interconnected ionomic; genomic and environmental data; and(2) to facilitate hypothesis generation and testing by proving direct querying and browsing …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,2
Mass informatics in differential proteomics,Xiang Zhang; Seza Orcun; Mourad Ouzzani; Cheolhwan Oh,Abstract Systems biology aims to understand biological systems on a comprehensive scale;such that the components that make up the whole are connected to one another and work inharmony. As a major component of systems biology; differential proteomics studies thedifferences between distinct but related proteomes such as normal versus diseased cellsand diseased versus treated cells. High throughput mass spectrometry (MS) basedanalytical platforms are widely used in differential proteomics (Domon; 2006; Fenselau;2007). As a common practice; the proteome is usually digested into peptides first. Thepeptide mixture is then separated using multidimensional liquid chromatography (MDLC)and is finally subjected to MS for further analysis. Thousands of mass spectra are generatedin a single experiment. Discovering the significantly changed proteins from millions of …,*,2009,2
DeepER--Deep Entity Resolution,Muhammad Ebraheem; Saravanan Thirumuruganathan; Shafiq Joty; Mourad Ouzzani; Nan Tang,Abstract: Entity Resolution (ER) is a fundamental problem with many applications. Machinelearning (ML)-based and rule-based approaches have been widely studied for decades;with many efforts being geared towards which features/attributes to select; which similarityfunctions to employ; and which blocking function to use-complicating the deployment of anER system as a turn-key system. In this paper; we present DEEPER; a turn-key ER systempowered by deep learning (DL) techniques. The central idea is that distributedrepresentations and representation learning from DL can alleviate the above human effortsfor tuning existing ER systems. DeepER makes several notable contributions: encoding atuple as a distributed representation of attribute values; building classifiers using theserepresentations and a semantic aware blocking based on LSH; and learning and tuning …,arXiv preprint arXiv:1710.00597,2017,1
A Demonstration of Lusail: Querying Linked Data at Scale,Essam Mansour; Ibrahim Abdelaziz; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Abstract There has been a proliferation of datasets available as interlinked RDF dataaccessible through SPARQL endpoints. This has led to the emergence of variousapplications in life science; distributed social networks; and Internet of Things that need tointegrate data from multiple endpoints. We will demonstrate Lusail; a system that supportsthe need of emerging applications to access tens to hundreds of geo-distributed datasets.Lusail is a geo-distributed graph engine for querying linked RDF data. Lusail delivers out-standing performance using (i) a novel locality-aware query decomposition technique thatminimizes the intermediate data to be accessed by the subqueries; and (ii) selectivity-awareness and parallel query execution to reduce network latency and to increaseparallelism. During the demo; the audience will be able to query actually deployed RDF …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
In-Memory Distributed Matrix Computation Processing and Optimization,Yongyang Yu; Mingjie Tang; Walid G Aref; Qutaibah M Malluhi; Mostafa M Abbas; Mourad Ouzzani,The use of large-scale machine learning and data mining methods is becoming ubiquitousin many application domains ranging from business intelligence and bioinformatics to self-driving cars. These methods heavily rely on matrix computations; and it is hence critical tomake these computations scalable and efficient. These matrix computations are oftencomplex and involve multiple steps that need to be optimized and sequenced properly forefficient execution. This paper presents new efficient and scalable matrix processing andoptimization techniques for in-memory distributed clusters. The proposed techniquesestimate the sparsity of intermediate matrix-computation results and optimize communicationcosts. An evaluation plan generator for complex matrix computations is introduced as well asa distributed plan optimizer that exploits dynamic cost-based analysis and rule-based …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,1
Query Optimizations Over Decentralized RDF Graphs,Ibrahim Abdelaziz; Essam Mansour; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Applications in life sciences; decentralized social networks; Internet of Things; and statisticallinked dataspaces integrate data from multiple decentralized RDF graphs via SPARQLqueries. Several approaches have been proposed to optimize query processing over asmall number of heterogeneous data sources by utilizing schema information. In the case ofschema similarity and interlinks among sources; these approaches cause unnecessary dataretrieval and communication; leading to poor scalability and response time. This paperaddresses these limitations and presents Lusail; a system for scalable and efficient SPARQLquery processing over decentralized graphs. Lusail achieves scalability and low queryresponse time through various optimizations at compile and run times. At compile time; weuse a novel locality-aware query decomposition technique that maximizes the number of …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,1
AUDIT: approving and tracking updates with dependencies in collaborative databases,Khaleel Mershad; Qutaibah M Malluhi; Mourad Ouzzani; Mingjie Tang; Michael Gribskov; Walid G Aref,Abstract Collaborative databases such as genome databases; often involve extensivecuration activities where collaborators need to interact to be able to converge and agree onthe content of data. In a typical scenario; a member of the collaboration makes someupdates and these become visible to all collaborators for possible comments andmodifications. At the same time; these updates are usually pending the approval or rejectionfrom the data custodian based on the related discussion and the content of the data.Unfortunately; the approval and authorization of updates in current databases is basedsolely on the identity of the user; eg; via the SQL GRANT and REVOKE commands. In thispaper; we present a scalable cloud-based collaborative database system to supportcollaboration and data curation scenarios. Our system is based on an Update Pending …,Distributed and Parallel Databases,2017,1
System and method for checking data for errors,*,A system for checking data for errors; the system comprising a checking module operable tocheck tuples of data stored in a target database for errors; the tuples in the target databaseoriginating from the output of at least one query transformation module which applies aquery transformation to tuples of data from at least one data source an identification moduleoperable to identify a problematic tuple from a data source that produces an error in thetarget database; the identification module being operable to quantify the contribution of theproblematic tuple in producing the error in the target database; and a description generationmodule operable to generate a descriptive query which represents at least one of errorsidentified by the checking module in the target database which are produced by the at leastone query transformation module; and problematic tuples identified in a data source by …,*,2016,1
ORLF: A flexible framework for online record linkage and fusion,Eduard C Dragut; Mourad Ouzzani; Ahmed K Elmagarmid; Walid G Aref,With the exponential growth of data on the Web comes the opportunity to integrate multiplesources to give more accurate answers to user queries. Upon retrieving records frommultiple Web databases; a key task is to merge records that refer to the same real-worldentity. We demonstrate ORLF (Online Record Linkage and Fusion); a flexible query-timerecord linkage and fusion framework. ORLF deduplicates newly arriving query results jointlywith previously processed query results. We use an iterative caching solution that leveragesquery locality to effectively deduplicate newly incoming records with cached records. ORLFaims to deliver timely query answers that are duplicate-free and reflect knowledge collectedfrom previous queries.,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,1
Spatial queries with k-nearest-neighbor and relational predicates,Ahmed M Aly; Walid G Aref; Mourad Ouzzani,Abstract The ubiquity of location-aware devices and smartphones has unleashed anunprecedented proliferation of location-based services that require processing queries withboth spatial and relational predicates. Many algorithms and index structures already exist forprocessing k-Nearest-Neighbor (kNN; for short) predicates either solely or when combinedwith textual keyword search. Unfortunately; there has not been enough study on how toefficiently process queries where kNN predicates are combined with general relationalpredicates; ie; ones that have selects; joins and group-by's. One major challenge is thatbecause the kNN is a ranking operation; applying a relational predicate before or after akNN predicate in a query evaluation pipeline (QEP; for short) can result in different outputs;and hence leads to different query semantics. In particular; this renders classical …,Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems,2015,1
iHUB: an information and collaborative management platform for life sciences,David E Salt; Mourad Ouzzani; Eduard C Dragut; Peter Baker; Srivathsava Rangarajan,1. INTRODUCTION We describe ionomicshub; iHUB for short; a large scale cyber-infrastructureto support end-to-end research that aims to improve our understanding of how plants takeup; trans- port and store their nutrient and toxic elements. One of the biggest challenges in thelife sciences is func- tional genomics; which is the study of the molecular mech- anistic functionalcontribution of each of the genes in a genome to the overall biology of an organism. Ionomicsis one example of a functional genomics approach for the study of the genes and gene-networksthat control mineral nutrient and trace element homeostasis (which is the prop- erty of a systemto regulate its variables so that internal conditions remain stable and relatively constant in responseto external conditions; eg; temperature regulation). Quan- titative and qualitative high-throughputmolecular pheno- typing tools have been developed; which; when coupled to similarly …,Proceedings of the 23rd International Conference on World Wide Web,2014,1
JISC: Adaptive Stream Processing Using Just-In-Time State Completion.,Ahmed M Aly; Walid G Aref; Mourad Ouzzani; Hosam M Mahmoud,ABSTRACT The continuous and dynamic nature of data streams may lead a queryexecution plan (QEP) of a long-running continuous query to become suboptimal duringexecution; and hence will need to be altered. The ability to perform an efficient and flawlesstransition to an equivalent; yet optimal QEP is essential for a data stream query processor.Such transition is challenging for plans with stateful binary operators; such as joins; wherethe states of the QEP have to be maintained during query transition without compromisingthe correctness of the query output. This paper presents Just-In-Time State Completion(JISC); a new technique for query plan migration. JISC does not cause any halt to the queryexecution; and thus allows the query to maintain steady output. JISC is applicable topipelined as well as eddy-based query evaluation frameworks. Probabilistic analysis of …,EDBT,2014,1
Guided data repair,*,A computer implemented method for correcting records in a database comprisinggenerating; using a processor; respective candidate replacement entries for multipleinconsistent records of the database; grouping the candidate replacement entries to providemultiple groups of related candidate updates for the database; ranking the groups accordingto a loss function to quantify database quality; receiving input for a selected group; sortingcandidate replacement entries in the selected group; and applying updates from theselected group to the database to correct entries of the inconsistent records.,*,2013,1
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1
U-MAP: a system for usage-based schema matching and mapping,Hazem Elmeleegy; Jaewoo Lee; El Kindi Rezig; Mourad Ouzzani; Ahmed Elmagarmid,Abstract This demo shows how usage information buried in query logs can play a centralrole in data integration and data exchange. More specifically; our system U-Map uses querylogs to generate correspondences between the attributes of two different schemas and thecomplex mapping rules to transform and restructure data records from one of these schemasto another. We introduce several novel features showing the benefit of incorporating querylog analysis into these key components of data integration and data exchange systems.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,1
Data management challenges for computational transportation,Walid G Aref; Mourad Ouzzani,Abstract Computational Transportation is an emerging discipline that poses many datamanagement challenges. Computational transportation is characterized by the existence ofa massive number of moving objects; moving sensors; and moving queries. This paperhighlights important data management challenges for computational transportation andpromising approaches towards addressing them.,Proceedings of the 5th Annual International Conference on Mobile and Ubiquitous Systems: Computing; Networking; and Services,2008,1
Supporting annotated relations,MY Eltabakh; M Ouzzani; Walid G Aref; Ahmed K Elmagarmid; Y Laura-Silva,ABSTRACT Annotations and provenance data play a key role in understanding and curatingscientific databases. However; current database management systems lack adequatesupport for managing annotations and provenance data including:(1) handling annotationsat multiple granularities; ie; at the table; tuple; column and cell levels;(2) propagatingannotations along with query answers;(3) querying data based on their annotations; and (4)providing declarative ways to add; archive; and restore annotations. In this paper; wepropose to treat multi-granular annotations and provenance as first class objects inside thedatabase. We introduce the concept of" Annotated Relations" along with new operators andextended semantics for the standard relational oDerators in support of annotated relations.We present an expressive and declarative extension to SQL to support the processing …,*,2007,1
Database middleware for distributed ontologies in state and federal family & social services,Athman Bouguettaya; Mourad Ouzzani; Ahmed Elmagarmid; Brahim Medjahed,Collecting benefits using current FSSA systems is time-consuming; frustrating; and complexfor needy citizens and social workers. This requires citizens to visit several offices in andoutside their hometowns to receive benefits they are entitled to. In many cases; dealing withthis process prevents underprivileged citizens from devoting adequate time to enhancingtheir prospects for becoming self-supporting with a consequential harmful impact on theirhealth and safety. In the WebDG (Web Digital Government) project; we investigate thedesign and implementation of a middleware for organizing; accessing; and managinggovernment welfare databases. The project employs a variety of strategies to pursue thesimple goal of making life easier for citizens and government employees at Indiana FSSAalike. The disadvantaged citizens would have most of their needs satisfied in one single …,Proceedings of the 2004 annual national conference on Digital government research,2004,1
Pattern-Driven Data Cleaning,El Kindi Rezig; Mourad Ouzzani; Walid G Aref; Ahmed K Elmagarmid; Ahmed R Mahmood,Abstract: Data is inherently dirty and there has been a sustained effort to come up withdifferent approaches to clean it. A large class of data repair algorithms rely on data-qualityrules and integrity constraints to detect and repair the data. A well-studied class of integrityconstraints is Functional Dependencies (FDs; for short) that specify dependencies amongattributes in a relation. In this paper; we address three major challenges in data repairing:(1)Accuracy: Most existing techniques strive to produce repairs that minimize changes to thedata. However; this process may produce incorrect combinations of attribute values (orpatterns). In this work; we formalize the interaction of FD-induced patterns and select repairsthat result in preserving frequent patterns found in the original data. This has the potential toyield a better repair quality both in terms of precision and recall.(2) Interpretability of …,arXiv preprint arXiv:1712.09437,2017,*
Human-Centric Data Cleaning [Vision],El Kindi Rezig; Mourad Ouzzani; Walid G Aref; Ahmed K Elmagarmid,Abstract: Data Cleaning refers to the process of detecting and fixing errors in the data.Human involvement is instrumental at several stages of this process; eg; to identify andrepair errors; to validate computed repairs; etc. There is currently a plethora of data cleaningalgorithms addressing a wide range of data errors (eg; detecting duplicates; violations ofintegrity constraints; missing values; etc.). Many of these algorithms involve a human in theloop; however; this latter is usually coupled to the underlying cleaning algorithms. There iscurrently no end-to-end data cleaning framework that systematically involves humans in thecleaning pipeline regardless of the underlying cleaning algorithms. In this paper; wehighlight key challenges that need to be addressed to realize such a framework. We presenta design vision and discuss scenarios that motivate the need for such a framework to …,arXiv preprint arXiv:1712.08971,2017,*
Lusail: a system for querying linked data at scale,Ibrahim Abdelaziz; Essam Mansour; Mourad Ouzzani; Ashraf Aboulnaga; Panos Kalnis,Abstract The RDF data model allows publishing interlinked RDF datasets; where eachdataset is independently maintained and is queryable via a SPARQL endpoint. Manyapplications would benefit from querying the resulting large; decentralized; geo-distributedgraph through a federated SPARQL query processor. A crucial factor for good performancein federated query processing is pushing as much computation as possible to the localendpoints. Surprisingly; existing federated SPARQL engines are not effective at this tasksince they rely only on schema information. Consequently; they cause unnecessary dataretrieval and communication; leading to poor scalability and response time. This paperaddresses these limitations and presents Lusail; a scalable and efficient federated SPARQLsystem for querying large RDF graphs that are geo-distributed on different endpoints …,Proceedings of the VLDB Endowment,2017,*
Entity Consolidation: The Golden Record Problem,Dong Deng; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract: Four key subprocesses in data integration are: data preparation (ie; transformingand cleaning data); schema integration (ie; lining up like attributes); entity resolution (ie;finding clusters of records that represent the same entity) and entity consolidation (ie;merging each cluster into a" golden record" which contains the canonical values for eachattribute). In real scenarios; the output of entity resolution typically contains multiple dataformats and different abbreviations for cell values; in addition to the omnipresent problem ofmissing data. These issues make entity consolidation challenging. In this paper; we studythe entity consolidation problem. Truth discovery systems can be used to solve this problem.They usually employ simplistic heuristics such as majority consensus (MC) or sourceauthority to determine the golden record. However; these techniques are not capable of …,arXiv preprint arXiv:1709.10436,2017,*
A Demo of the Data Civilizer System,Raul Castro Fernandez; Dong Deng; Essam Mansour; Abdulhakim A Qahtan; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract Finding relevant data for a specific task from the numerous data sources availablein any organization is a daunting task. This is not only because of the number of possibledata sources where the data of interest resides; but also due to the data being scattered allover the enterprise and being typically dirty and inconsistent. In practice; data scientists areroutinely reporting that the majority (more than 80%) of their effort is spent finding; cleaning;integrating; and accessing data of interest to a task at hand. We propose to demonstrateDATA CIVILIZER to ease the pain faced in analyzing data" in the wild". DATA CIVILIZER isan end-to-end big data management system with components for data discovery; dataintegration and stitching; data cleaning; and querying data from a large variety of storageengines; running in large enterprises.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Errata for Lightning Fast and Space Efficient Inequality Joins (PVLDB 8 (13): 2074--2085),Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract This is in response to recent feedback from some readers; which requires someclarifications regarding our IEJ oin algorithm published in [1]. The feedback revolves aroundfour points:(1) a typo in our illustrating example of the join process;(2) a naming error for theindex used by our algorithm to improve the bit array scan;(3) the sort order used in ouralgorithms; and (4) a missing explanation on how duplicates are handled by our self joinalgorithm.,Proceedings of the VLDB Endowment,2017,*
Optimized inequality join method,*,The optimized inequality join method is a method for joining relational tables on inputinequality conditions. The optimized inequality join method is a relatively fast inequality joinmethod using permutation arrays to store positional information for sorted attributed values.Additionally; space efficient bit arrays are used to enable optimization; such as Bloom filterindices; thus providing faster computation of the join results. The method may be used; forexample; for joining various inequalities associated with a variety of measuredenvironmental conditions for raising an alarm when certain conditions are met.,*,2017,*
COACT: a query interface language for collaborative databases,Khaleel Mershad; Qutaibah M Malluhi; Mourad Ouzzani; Mingjie Tang; Michael Gribskov; Walid G Aref; Deo Prakash,Abstract Data curation activities in collaborative databases mandate that collaboratorsinteract until they converge and agree on the content of their data. In a previous work; wepresented a cloud-based collaborative database system that promotes and enablescollaboration and data curation scenarios. Our system classifies different versions of a dataitem to either pending; approved; or rejected. The approval or rejection of a certain version isdone by the database Principle Investigators (or PIs) based on its value. Our system alsoallows collaborators to view the status of each version and help PIs take decisions byproviding feedback based on their experiments and/or opinions. Most importantly; oursystem provided mechanisms for history tracking of different versions to trace themodifications and approval/rejection done by both collaborators and PIs on different …,Distributed and Parallel Databases,2017,*
Behavior based record linkage,*,A computer implemented method for matching data records from multiple entities comprisingproviding respective transaction logs for the entities representing actions performed by or inrespect of the entities; determining a matching score using the transaction logs for respectivepairs of the entities and for predetermined combinations of merged entities by generating ameasure representing a gain in behavior recognition for the entities before and aftermerging; and using the gain as a matching score.,*,2016,*
CYBER SECURITY-PART 1 THE NEED TO SHARE,Ahmed Elmagarmid; Peter Cochrane; Mourad Ouzzani,*,JOURNAL OF THE INSTITUTE OF TELECOMMUNICATIONS PROFESSIONALS,2016,*
A System for Big Data Analytics over Diverse Data Processing Platforms,Jorge Quiane; Divy Agrawal; Sanjay Chawla; Ahmed Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Nan Tang; Mohammed Zaki,Data analytics is at the core of any organization that wants to obtain measurable value fromits growing data assets. Data analytic tasks may range from simple to extremely complexpipelines; such as data extraction; transformation and loading; online analytical processing;graph processing; and machine learning (ML). Following the dictum “one size does not fitall”; academia and industry have embarked on a race of developing data processingplatforms for supporting all of these different tasks; eg; DBMSs and MapReduce-likesystems. Semantic completeness; high performance and scalability are key objectives ofsuch platforms. While there have been major achievements in these objectives; users arestill faced with many road-blocks. MOTIVATING EXAMPLE The first roadblock is thatapplications are tied to a single processing platform; making the migration of an …,Qatar Foundation Annual Research Conference Proceedings,2016,*
Detecting inconsistent data records,*,A computer-implemented method for detecting a set of inconsistent data records in adatabase including multiple records; comprises selecting a data quality rule representing afunctional dependency for the database; transforming the data quality rule into at least onerule vector with hashed components; selecting a set of attributes of the database;transforming at least one record of the database selected on the basis of the selectedattributes into a record vector with hashed components; computing a dot product of the ruleand record vectors to generate a measure representing violation of the data quality rule bythe record.,*,2015,*
Data Science at QCRI,Divy Agrawal; Laure Berti; Hossam Hammady; Prasenjit Mitra; Mourad Ouzzani; Paolo Papotti; Jorge Quiane Ruiz; Nan Tang; Yin Ye; Si Yin; Mohamed Zaki," The Data Analytics group at QCRI has embarked on an ambitious endeavor to become apremiere world-class research group in Data Science by tackling diverse research topicsrelated to information extraction; data quality; data profiling; data integration; and datamining. We will present our ongoing projects to overcome different challenges encounteredin Big Data Curation; Big Data Fusion; and Big Data Analytics.(1) Big Data Curation: Due tocomplex processing and transformation layers; data errors proliferate rapidly and sometimesin an uncontrolled manner; thus compromising the value of information and impacting dataanalysis and decision making. While data quality problems can have crippling effects and noend-to-end off-the-shelf solutions to (semi-) automate error detection and correction existed;we built a commodity platform; NADEEF that can be easily customized and deployed to …,Qatar Foundation Annual Research Conference,2014,*
GlobalHUB: A Model for Sustainable Online Communities,Ali M Roumani; Nathan McNeill; Lalit Patil; Mourad Ouzzani; Edwin Daniel Hirleman,Abstract Recently; sustainability of virtual organizations has been receiving increasingattention in the literature. Most of suggested sustainability pieces have been theoretical innature or based on case studies. The essence of such articles is: build it; and they will come.However; as the authors have discovered in their experience with a virtual organizationcalled GlobalHUB; putting all of the suggested building blocks into place does notnecessarily lead; by itself; to sustainability of a virtual organization. The authors argue thatsustainability has many complex and dynamic dimensions that need to be addressed inexisting models of sustainability for virtual organizations. In this paper the auhtors provideour key learnings in the process of building GlobalHUB as a sustainable online community.,International Journal of Web Portals (IJWP),2014,*
Data Quality Not Your Typical Database Problem.,Mourad Ouzzani,Abstract. Textbook database examples are often wrong and simplistic. Unfortunately Data isnever born clean or pure. Errors; missing values; repeated entries; inconsistent instancesand unsatisfied business rules are the norm rather than the exception. Data cleaning (alsoknown as data cleansing; record linkage and many other terminologies) is growing as amajor application requirement and an interdisciplinary research area. In this talk; we willstart by discussing some of the major issues and challenges facing creating effective andefficient data cleaning solutions. Then we will discuss some challenges and criticize currentconservative approaches to this very critical problem. Finally we will discuss some of ourwork at QCRI in this area.,ICWIT,2012,*
Record Linkage and Fusion over Web Databases,Mourad Ouzzani; Eduard Dragut; El Kindi; Amgad Madkour,Abstract Many data-intensive applications on the Web require integrating data from multiplesources (Web databases) at query time. Online sources may refer to the same real worldentity in different ways and some may provide outdated or erroneous data. An important taskis to recognize and merge the various references that refer to the same entity at query time.Almost all existing duplicate detection and fusion techniques work in the offline setting and;thus; do not meet the online constraint. There are at least two aspects that differentiateonline duplicate detection and fusion from its offline counterpart. First; the latter assumes thatthe entire data is available; while the former cannot make such a hard assumption. Second;several iterations (query submissions) may be required to compute the “ideal”representation of an entity in the online setting. We propose a general framework to …,Qatar Foundation Annual Research Forum Proceedings,2011,*
Guided data repair,Ahmed Elmagarmid; Jennifer Neville; Mourad Ouzzani; Ihab Ilyas; Mohamed Yakout,Abstract In this paper we present GDR; a Guided Data Repair framework that incorporatesuser feedback in the cleaning process to enhance and accelerate existing automatic repairtechniques while minimizing user involvement. GDR consults the user on the updates thatare most likely to be beneficial in improving data quality. GDR also uses machine learningmethods to identify and apply the correct updates directly to the database without the actualinvolvement of the user on these specific updates. To rank potential updates for consultationby the user; we first group these repairs and quantify the utility of each group using thedecision-theory concept of value of information (VOI). We then apply active learning to orderupdates within a group based on their ability to improve the learned model. User feedback isused to repair the database and to adaptively refine the training set for the model. We …,Proceedings of the VLDB Endowment,2011,*
Implementation and Experiments,Mourad Ouzzani; Athman Bouguettaya,Abstract In this chapter; we report on the implementation and experiments for our queryinfrastructure over Web services. The implementation is conducted in the context of WebDG;a digital government prototype that provides access to e-government databases andservices related to social services. As a prototype; WebDG supports access to only few e-government services. Thus; it cannot represent the large number of Web services availableon the Web. It is then necessary to asses our approach through experiments on syntheticdata. The goal of these experiments is to measure the cost of the different algorithms and thequality of the service execution plans they generate. We focus on computing the time it takeseach algorithm to reach a decision. The quality of their results is simply the objectivefunction; F smr; as defined for a service execution plan in the previous chapter. These …,*,2011,*
Web Services Query Model,Mourad Ouzzani; Athman Bouguettaya,Abstract The basic use of Web services consists of invoking operations by sending andreceiving messages. Their definition does not describe potential interactions betweenoperations within the same Web service or other Web services. However; complexapplications accessing diverse Web services (eg; benefits for senior citizens) requireadvanced capabilities to manipulate and deliver Web services' functionalities. In general;users have needs that cannot be fulfilled by simply invoking one single operation or severaloperations independently.,*,2011,*
Web Services Query Execution and Optimization,Mourad Ouzzani; Athman Bouguettaya,Abstract In the previous chapter; we introduced our new web service query model. Usersexpress their requests using the specifications we introduced in the same chapter. Nowgiven such a query and a large pool of Web services along with their QoWS parameters; weneed to transform this query into an executable orchestration of actual web services.,*,2011,*
Current Advances in Semantic Web Services and Web Databases,Mourad Ouzzani; Athman Bouguettaya,Abstract The work we presented in this book relates to several research areas includingWeb databases integration and efficient querying; as well as Web service querying;composition; and optimization. A review of these different areas as they relate to this bookare discussed in this chapter.,*,2011,*
Ontological Organization of Web Databases,Mourad Ouzzani; Athman Bouguettaya,Abstract Organizations rely on a wide variety of databases to conduct their everydaybusiness. Databases are usually designed from scratch if none is found to meetrequirements. This has led to a proliferation of databases obeying different sets ofrequirements oftentimes modeling the same situations. In many instances; and because of alack of any organized conglomeration of databases; users create their own pieces ofinformation that may exist in current databases.,*,2011,*
Conclusions; Open Issues; and Future Directions,Mourad Ouzzani; Athman Bouguettaya,Abstract This book addresses key issues to enable efficient access to Web databases andWeb services. We described a distributed ontology that allows a meaningful organization ofand efficient access to Web databases. We also presented a comprehensive queryinfrastructure for the emerging concept of Web services. The core of this query infrastructurerelates to the efficient delivery of Web services based on the concept of Quality of WebService.,*,2011,*
WS-Query–A Framework to Efficiently Query Semantic Web Service,Mourad Ouzzani; Athman Bouguettaya; Ahmed Elmagarmid,Abstract We propose a query framework; called WS-Query; to efficiently query semantic Webservices using Quality of Web Service (QoWS). Service querying is enabled by a novelservice query model where declarative service queries are resolved by multi-level Webservice invocations. Quality of Web Service is used as a key parameter to select the bestservices. QoWS consists of a set of criteria that characterize the behavior of Web services indelivering their functionalities. We adjust the different QoWS parameters through dynamicrating and multimode matching. The dynamic rating provides a quantitative assessment ofthe Web services in achieving the promised QoWS throughout their interactions with thequery infrastructure. The proposed multimode matching expands the solution space byenabling similar/partial answers and allows assigning a degree of precision for each …,*,2010,*
Location-aware privacy and more: a systems approach using context-aware database management systems,Walid G Aref; Hicham G Elmongui; Mourad Ouzzani,Abstract When a user issues a query; database engines will usually return results basedsolely on the query and the content of the database. However; query issuers have a"context" which if taken into account will certainly change the outcome of the query. Thus;when responding to the query; the database system can consider the query issuer's contextand return only the objects/tuples in the database that not only satisfy the query predicatesbut also are relevant to the query issuer's context. In this paper; we give an overview ofChameleon; a context-aware database management system. Chameleon introduces SQL-level constructs that describe the" context" in which the query is issued as well as thereciprocal contexts of the objects in the database. By tying the query issuer's contexts withthe corresponding contexts of the objects in the database; Chameleon can retrieve the …,Proceedings of the 2nd SIGSPATIAL ACM GIS 2009 International Workshop on Security and Privacy in GIS and LBS,2009,*
Preserving Privacy and Fairness in Peer Data Management Systems,Hazen Elmeleegy; Ahmed Abusalah; Mourad Ouzzani; Ahmed K Elmagarmid,Abstract—Peer Data Management Systems (PDMSs) promise to extend the classical dataintegration approach to the Internet scale. Unfortunately; some challenges remain beforerealizing this promise. One of the biggest challenges is preserving the privacy of theexchanged data while passing through several intermediate peers. Another challenge isprotecting the mappings used for data translation. Achieving privacy preservation withoutbeing unfair to any of the peers is yet a third challenge. This paper presents a novel queryanswering protocol in PDMSs to address these challenges. The protocol employs atechnique based on noise selection and insertion to protect the query results; and acommutative encryption-based technique to protect the mappings and ensure fairnessamong peers. An extensive security analysis of the protocol shows that it is resilient to …,*,2008,*
Record Linkage Based on Entities' Behavior,Mohamed Yakout; Ahmed K Elmagarmid; Hazen Elmeleegy; Mourad Ouzzani,Abstract—Record linkage is the problem of identifying similar records across different datasources. Traditional record linkage techniques focus on using simple database attributes ina textual similarity comparison to decide on matched and non-matched records. Recently;record linkage techniques have considered useful extracted knowledge and domaininformation to help enhancing the matching accuracy. In this paper; we present a newtechnique for record linkage that is based on entity's behavior; which can be extracted from atransaction log. In the matching process; we measure the improvement of identifying abehavior when comparing two entities by merging their transaction log. To do so; we use twomatching phases; first; a candidate generation phase; which is fast and provide almost nofalse negatives; while producing low precision. Second; an accurate matching phase …,*,2008,*
bdbms-A Database Management System for Biological,Mohamed Eltabakh; Mourad Ouzzani; Walid G Aref,Abstract Biologists are increasingly using databases for storing and anaging their data.Biological databases typically consist of a mixture of raw data; metadata; sequences;annotations; and related data obtained from various sources. Current database technologylacks several functionalities that are needed by biological databases. In this paper; weintroduce,*,2007,*
STL and local regression for modeling disease surveillance counts,David E Anderson; Cheng Zheng; Ross Maciejewski; Ryan Hafen; William S Cleveland,Objective: Use the STL local-regression (loess) decomposition procedure andtransformation to model the univariate time-series characteristics of chief-complaint dailycounts as a first step in a time and spatial modeling. Develop visualization tools for modeldisplay and checking. Background: Numerous methods have been applied to the problem ofmodeling temporal properties of disease surveillance data; the ESSENCE system contains awidely used approach (1). STL (2) is a flexible; wellproven method for temporal modelingthat decomposes the series into frequency components (see figure). A periodic componentlike DW can be exactly periodic or evolve through time. STL is based on loess (3); which canmodel a numeric response as a function of any explanatory variables. After the STLmodeling of the counts; we will add patient address and produce a timespace modeling …,Advances in Disease Surveillance,2007,*
Discovering consensus patterns in biological databases,Mohamed Y ElTabakh; Walid G Aref; Mourad Ouzzani; Mohamed H Ali,Abstract Consensus patterns; like motifs and tandem repeats; are highly conserved patternswith very few substitutions where no gaps are allowed. In this paper; we present aprogressive hierarchical clustering technique for discovering consensus patterns inbiological databases over a certain length range. This technique can discover consensuspatterns with various requirements by applying a post-processing phase. The progressivenature of the hierarchical clustering algorithm makes it scalable and efficient. Experiments todiscover motifs and tandem repeats on real biological databases show significantperformance gain over non-progressive clustering techniques.,VLDB Workshop on Data Mining and Bioinformatics,2006,*
ABSTRACT Access Control Enforcement for Conversation-based Web Services,Massimo Mecella; Federica Paci; Mourad Ouzzani; Elisa Bertino,Abstract Service Oriented Computing is emerging as the main approach to build distributedenterprise applications on the Web. The widespread use of Web services is hindered by thelack of adequate security and privacy support. In this paper; we present a novel frameworkfor enforcing access control in conversation-based Web services. Our approach takes intoaccount the conversational nature of Web services. This is in contrast with existingapproaches to access control enforcement that assume a Web service as a set ofindependent operations. Furthermore; our approach achieves a tradeoff between the needto protect Web service's access control policies and the need to disclose to clients theportion of access control policies related to the conversations they are interested in. This isimportant to avoid situations where the client cannot progress in the conversation due to …,*,2006,*
The Indiana Center for Database Systems at Purdue University,Mourad Ouzzani; Walid G Aref; Elisa Bertino; Ann Christine Catlin; Christopher W Clifton; Wing-Kai Hon; Ahmed K Elmagarmid; Arif Ghafoor; Susanne E Hambrusch; Sunil Prabhakar; Jeffrey S Vitter; Xiang Zhang,Abstract The Indiana Center for Database Systems (ICDS) at Purdue University hasembarked in an ambitious endeavor to become a premiere world-class database researchcenter. This goal is substantiated by the diversity of its research topics; the large and diversefunding base; and the steady publication trend in top conferences and journals. ICDS wasfounded with an initial grant from the State of Indiana Corporation of Science andTechnology in 1990. Since then it has grown to now have 9 faculty members and about 30total researchers. This report describes the major research projects underway at ICDS aswell as efforts to move research toward practice.,ACM SIGMOD Record,2005,*
Internet Computing Support for Digital Government.,Brahim Medjahed; Abdelmounaam Rezgui; Athman Bouguettaya; Mourad Ouzzani,*,*,2004,*
Ubiquitous Access to Web Databases.,Athman Bouguettaya; Brahim Medjahed; Mourad Ouzzani; Yao Meng,ABSTRACT With the emergence of the Web; there is a need to provide across-the-boardtransparency for accessing and manipulating data irrespective of platforms; locations; andsystems. The challenge is to build an infrastructure to support flexible tools for informationspace organization; communication facilities; information discovery; content description; andassembly of data from heterogeneous sources. In this chapter; we describe the WebFINDITsystem. WebFINDIT builds a scalable and uniform infrastructure for locating and accessingheterogeneous and autonomous databases in large and dynamic environments. One keyfeature of WebFINDIT is the clustering of Web databases into distributed ontologies. Themain advantage of this ontological organization is filtering interactions and reducing theoverhead of locating information. Another important feature is the large spectrum of …,*,2003,*
Privacy preserving composition of government web services,Athman Bouguettaya; Brahim Medjahed; Abdelmounaam Rezgui; Mourad Ouzzani; Zhaoyang Wen,Abstract The core of our research in digital government is the development of techniques toefficiently access government services and databases. As a case study; we use social andwelfare services within Indiana's Family and Social Services Administration (FSSA). TheFSSA is composed of dozens of autonomous and geographically distant departments. Eachdepartment provides several rehabilitation programs to help disadvantaged citizens. It alsocontains a myriad of databases that store government and citizens information. We proposea framework that enables uniform access to a large number of government services anddatabases. A Web-based prototype called WebDG (Web Digital Government) has beenimplemented. The main features of WebDG are the following:,Proceedings of the 2002 annual national conference on Digital government research,2002,*
Ontological Approach for Information Discovery in Internet Databases,M Ouzzani; B BENATALLAH; A BOUGUETTAYA,Abstract. The Internet has solved the age-old problem of network connectivity and thusenabling the potential access to; and data sharing among large numbers of databases.However; enabling users to discover useful information requires an adequate metadatainfrastructure that must scale with the diversity and dynamism of both users' interests andInternet accessible databases. In this paper; we present a model that partitions theinformation space into a distributed; highly specialized domain ontologies. We alsointroduce inter-ontology relationships to cater for user-based interests across ontologiesdefined over Internet databases. We also describe an architecture that implements these twofundamental constructs over Internet databases. The aim of the proposed model andarchitecture is to eventually facilitate data discovery and sharing for Internet databases.,Distributed and Parallel Databases,2000,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Athman Bouguettaya; Munindar Singh; Michael Huhns; Quan Z Sheng; Hai Dong; Qi Yu; Azadeh Ghari Neiat; Sajib Mistry; Boualem Benatallah; Brahim Medjahed; Mourad Ouzzani; Fabio Casati; Xumin Liu; Hongbing Wang; Dimitrios Georgakopoulos; Liang Chen; Surya Nepal; Zaki Malik; Abdelkarim Erradi; Yan Wang; Brian Blake; Schahram Dustdar; Frank Leymann; Michael Papazoglou,Current it advances can be likened in their effect and impact to the deep economic andsocietal transformations that were brought about by the industrial revolution. Recent yearshave seen the expansion of services in all sectors of the economy; building on theexpansion of the Internet.,Communications of the ACM,*,*
Abstract/Details,Hazem Elmeleegy; Ann Arbor; Ahmed Elmagarmid; Walid Aref; Mourad Ouzzani; Luo Si,*,*,*,*
19th International Conference on Scientific and Statistical Database Management (SSDBM 2007),R Fomkin; T Risch,High energy physics scientists analyze large amounts of data looking for interesting eventswhen particles collide. These analyses are easily expressed using complex queries thatfilter events. We developed a cost model for aggregation operators and other functions usedin such queries and show that it substantially improves performance. However; the queryoptimizer still produces suboptimal plans...,*,*,*
Discovering Data Transformations in Web Resources,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract. In data integration; data curation; and other data analysis tasks; users spend aconsiderable amount of time converting data from one representation to another. Forexample US dates to European dates or airport codes to city names. In practice; datascientists have to code most of the transformation tasks manually; search for the appropriatedictionaries; and involve domain experts. In a previous vision paper; we presented the initialdesign of DataXFormer; a system that uses web resources to assist in transformationdiscovery [1]. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. We demonstrated thesystem at SIGMOD 2015 and deployed an open version of the system; which helped us toincrease our initial workload from 50 to 120 transformations [3]. At the same time we …,*,*,*
Access Control Enforcement for Conversation-based Web Services,Mourad Ouzzani; Elisa Bertino; Massimo Mecella; Federica Paci,Page 1. The Cyber Center Access Control Enforcement for Conversation-based Web ServicesMourad Ouzzani Purdue University; USA Elisa Bertino Purdue University; USA Massimo Mecella *Univ. Roma LA SAPIENZA; Italy Federica Paci Univ. Milano; Italy * while a visiting researcher(fall 2005) in the Department of Computer Science and CERIAS; Purdue University; USA Page2. WWW2006 Conference @ Edinburgh (Scotland) – May 25; 2006 Massimo Mecella The CyberCenter Overview • The conversational model of Web services • Security concerns • Access controlbased on conversations – K-trustworthiness • The technique • The architecture • Conclusions Page3. WWW2006 Conference @ Edinburgh (Scotland) – May 25; 2006 Massimo Mecella The CyberCenter Web Services • … and possibly by constraints on the possible conversations – Using aservice typically involves performing sequences of …,*,*,*
Correctness Criteria Beyond Serializability,Au1 K ELMAGARMID,A transaction is a logical unit of work that includes one or more database access operationssuch as insertion; deletion; modification; and retrieval [8]. A schedule (or history) S of ntransactions T1;...; Tn is an ordering of the transactions that satisfies the following twoconditions:(i) the operations of Ti (i= 1;...; n) in S must occur in the same order in which theyappear in Ti; and (ii) operations from Tj (j6¼i) may be interleaved with Ti's operations in S. Aschedule S is serial if for every two transactions Ti and Tj that appear in S; either alloperations of Ti appear before all operations of Tj; or vice versa. Otherwise; the schedule iscalled nonserial or concurrent. Non-serial schedules of transactions may lead toconcurrency problems such as lost update; dirty read; and unrepeatable read. For instance;the lost update problem occurs whenever two transactions; while attempting to modify a …,*,*,*
RHEEM: Road to Freedom in Big Data Analytics,Divy Agrawal; Sanjay Chawla Ahmed Elmagarmid Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti Jorge-Arnulfo Quiané-Ruiz; Nan Tang Mohammed J Zaki,*,*,*,*
Development of Web-based Breastfeeding Monitoring System,Azza Ahmed; Mourad Ouzzani; Purdue Cyber Center,*,*,*,*
The Proteome Discovery Pipeline–A Data Analysis Pipeline for Mass Spectrometry-Based Differential Proteomics. Catherine P. Riley; Erik S. Gough,Jing He; Shrinivas S Jandhyala; Brad Kennedy; Seza Orcun; Mourad Ouzzani; Charles Buck; Xiang Zhang,*,*,*,*
SNJoin–A Scalable Join in Sensor-Network PhenomenaBases,Mohamed H Ali; Mourad Ouzzani; Walid G Aref; Ibrahim Kamel,Abstract A phenomenon appears in a sensor network when a group of sensors continuouslygenerate a similar behavior over a period of time. PhenomenaBases (or databases ofphenomena) are equipped with Phenomena Detection and Tracking (PDT) techniques thatcontinuously run in the background of a sensor database system to detect and trackphenomena. The process of phenomena detection and tracking depends mainly on a multi-way join operator which is at the core of PDT techniques to report similar sensor readings.With the increase in the sensor network size and to address periods of heavy loads; the joinoperator and; consequently; query processing in the PhenomenaBase face severalchallenges. In this paper; we present a new join operator for PhenomenaBases calledSNJoin that is specially-designed for dynamically-configured large-scale sensor networks …,*,*,*
