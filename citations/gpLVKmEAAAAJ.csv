An improved data stream summary: the count-min sketch and its applications,Graham Cormode; Shan Muthukrishnan,Abstract We introduce a new sublinear space data structure—the count-min sketch—forsummarizing data streams. Our sketch allows fundamental queries in data streamsummarization such as point; range; and inner product queries to be approximatelyanswered very quickly; in addition; it can be applied to solve several important problems indata streams such as finding quantiles; frequent items; etc. The time and space bounds weshow for using the CM sketch to solve these problems significantly improve those previouslyknown—typically from 1/ε 2 to 1/ε in factor.,Journal of Algorithms,2005,1057
Key differences between Web 1.0 and Web 2.0,Graham Cormode; Balachander Krishnamurthy,Abstract Web 2.0 is a buzzword introduced in 2003-04 which is commonly used toencompass various novel phenomena on the World Wide Web. Although largely amarketing term; some of the key attributes associated with Web 2.0 include the growth ofsocial networks; bi-directional communication; various' glue'technologies; and significantdiversity in content types. We are not aware of a technical comparison between Web 1.0 and2.0. While most of Web 2.0 runs on the same substrate as 1.0; there are some keydifferences. We capture those differences and their implications for technical work in thispaper. Our goal is to identify the primary differences leading to the properties of interest in2.0 to be characterized. We identify novel challenges due to the different structures of Web2.0 sites; richer methods of user interaction; new technologies; and fundamentally …,First Monday,2008,837
What's hot and what's not: tracking most frequent items dynamically,Graham Cormode; Shan Muthukrishnan,Abstract Most database management systems maintain statistics on the underlying relation.One of the important statistics is that of the “hot items” in the relation: those that appear manytimes (most frequently; or more than some threshold). For example; end-biased histogramskeep the hot items as part of the histogram and are used in selectivity estimation. Hot itemsare used as simple outliers in data mining; and in anomaly detection in many applications.We present new methods for dynamically determining the hot items at any time in a relationwhich is undergoing deletion operations as well as inserts. Our methods maintain smallspace data structures that monitor the transactions on the relation; and; when required;quickly output all hot items without rescanning the relation in the database. With user-specified probability; all hot items are correctly reported. Our methods rely on ideas from “ …,ACM Transactions on Database Systems (TODS),2005,578
An improved data stream summary: The count-min sketch and its applications,Graham Cormode; S Muthukrishnan,Abstract We introduce a new sublinear space data structure—the Count-Min Sketch—forsummarizing data streams. Our sketch allows fundamental queries in data streamsummarization such as point; range; and inner product queries to be approximatelyanswered very quickly; in addition; it can be applied to solve several important problems indata streams such as finding quantiles; frequent items; etc. The time and space bounds weshow for using the CM sketch to solve these problems significantly improve those previouslyknown—typically from 1/ε 2 to 1/ε in factor.,Latin American Symposium on Theoretical Informatics,2004,261
The string edit distance matching problem with moves,Graham Cormode; S Muthukrishnan,Abstract The edit distance between two strings S and R is defined to be the minimumnumber of character inserts; deletes; and changes needed to convert R to S. Given a textstring t of length n; and a pattern string p of length m; informally; the string edit distancematching problem is to compute the smallest edit distance between p and substrings of t. Werelax the problem so that:(a) we allow an additional operation; namely; substring moves; and(b) we allow approximation of this string edit distance. Our result is a near-linear timedeterministic algorithm to produce a factor of O (log n log* n) approximation to the string editdistance with moves. This is the first known significantly subquadratic algorithm for a stringedit distance problem in which the distance involves nontrivial alignments. Our results areobtained by embedding strings into L 1 vector space using a simplified parsing technique …,ACM Transactions on Algorithms (TALG),2007,259
What's new: Finding significant differences in network data streams,Graham Cormode; S Muthukrishnan,Abstract Monitoring and analyzing network traffic usage patterns is vital for managing IPNetworks. An important problem is to provide network managers with information aboutchanges in traffic; informing them about" what's new." Specifically; we focus on the challengeof finding significantly large differences in traffic: over time; between interfaces and betweenrouters. We introduce the idea of a deltoid: an item that has a large difference; whether thedifference is absolute; relative or variational. We present novel algorithms for finding themost significant deltoids in high-speed traffic data; and prove that they use small space; verysmall time per update; and are guaranteed to find significant deltoids with pre-specifiedaccuracy. In experimental evaluation with real network traffic; our algorithms perform welland recover almost all deltoids. This is the first work to provide solutions capable of …,IEEE/ACM Transactions on Networking (TON),2005,239
Class-based graph anonymization for social network data,Smriti Bhagat; Graham Cormode; Balachander Krishnamurthy; Divesh Srivastava,Abstract The recent rise in popularity of social networks; such as Facebook and MySpace;has created large quantities of data about interactions within these networks. Such datacontains many private details about individuals so anonymization is required prior toattempts to make the data more widely available for scientific research. Prior work hasconsidered simple graph data to be anonymized by removing all non-graph information andadding or deleting some edges. Since social network data is richer in details about the usersand their interactions; loss of details due to anonymization limits the possibility for analysis.We present a new set of techniques for anonymizing social network data based on groupingthe entities into classes; and masking the mapping between entities and the nodes thatrepresent them in the anonymized graph. Our techniques allow queries over the rich data …,Proceedings of the VLDB Endowment,2009,218
Holistic aggregates in a networked world: Distributed tracking of approximate quantiles,Graham Cormode; Minos Garofalakis; S Muthukrishnan; Rajeev Rastogi,Abstract While traditional database systems optimize for performance on one-shot queries;emerging large-scale monitoring applications require continuous tracking of complexaggregates and data-distribution summaries over collections of physically-distributedstreams. Thus; effective solutions have to be simultaneously space efficient (at each remotesite); communication efficient (across the underlying communication network); and providecontinuous; guaranteed-quality estimates. In this paper; we propose novel algorithmicsolutions for the problem of continuously tracking complex holistic aggregates in such adistributed-streams setting---our primary focus is on approximate quantile summaries; butour approach is more broadly applicable and can handle other holistic-aggregate functions(eg;" heavy-hitters" queries). We present the first known distributed-tracking schemes for …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,216
Finding frequent items in data streams,Graham Cormode; Marios Hadjieleftheriou,Abstract The frequent items problem is to process a stream of items and find all itemsoccurring more than a given fraction of the time. It is one of the most heavily studiedproblems in data stream mining; dating back to the 1980s. Many applications rely directly orindirectly on finding the frequent items; and implementations are in use in large scaleindustrial systems. However; there has not been much comparison of the different methodsunder uniform experimental conditions. It is common to find papers touching on this topic inwhich important related work is mischaracterized; overlooked; or reinvented. In this paper;we aim to present the most important algorithms for this problem in a common framework.We have created baseline implementations of the algorithms; and used these to perform athorough experimental study of their properties. We give empirical evidence that there is …,Proceedings of the VLDB Endowment,2008,213
Semantics of ranking queries for probabilistic data and expected ranks,K Yi G Cormode; F Li,*,Data Engineering; 2009. Proceedings. 25th International Conference on,2009,208
Combinatorial algorithms for compressed sensing,Graham Cormode; S Muthukrishnan,Abstract In sparse approximation theory; the fundamental problem is to reconstruct a signalA∈ ℝ n from linear measurements< A ψ i> with respect to a dictionary of ψ i's. Recently;there is focus on the novel direction of Compressed Sensing [9] where the reconstructioncan be done with very few—O (k log n)—linear measurements over a modified dictionary ifthe signal is compressible; that is; its information is concentrated in k coefficients with theoriginal dictionary. In particular; these results [9; 4; 23] prove that there exists a single O (klog n)× n measurement matrix such that any such signal can be reconstructed from thesemeasurements; with error at most O (1) times the worst case error for the class of suchsignals. Compressed sensing has generated tremendous excitement both because of thesophisticated underlying Mathematics and because of its potential applications. In this …,International Colloquium on Structural Information and Communication Complexity,2006,208
Comparing data streams using hamming norms (how to zero in),Graham Cormode; Mayur Datar; Piotr Indyk; S Muthukrishnan,Data streams are fundamental to many data processing applications. Internet routersproduce large-scale diagnostic data streams. Such streams are rarely stored in traditionaldatabases and; instead; must be processed “on the fly” as they are produced. Similarly;sensor networks produce multiple data streams of observations from their sensors. There isgrowing focus on manipulating data streams; and hence; there is a need to identify basicoperations of interest in managing data streams; and to support them efficiently. The chapterproposes computation of the Hamming norm as a basic operation of interest. The Hammingnorm formalizes ideas that are used throughout data processing. When applied to a singlestream; the Hamming norm gives the number of distinct items that are present in that datastream; which is a statistic of great interest in databases. When applied to a pair of …,*,2002,205
Sketching streams through the net: Distributed approximate query tracking,Graham Cormode; Minos Garofalakis,Abstract Emerging large-scale monitoring applications require continuous tracking ofcomplex data-analysis queries over collections of physically-distributed streams. Effectivesolutions have to be simultaneously space/time efficient (at each remote monitor site);communication efficient (across the underlying communication network); and providecontinuous; guaranteed-quality approximate query answers. In this paper; we propose novelalgorithmic solutions for the problem of continuously tracking a broad class of complexaggregate queries in such a distributed-streams setting. Our tracking schemes maintainapproximate query answers with provable error guarantees; while simultaneously optimizingthe storage space and processing time at each remote site; and the communication costacross the network. They rely on tracking general-purpose randomized sketch summaries …,Proceedings of the 31st international conference on Very large data bases,2005,198
Communication-efficient distributed monitoring of thresholded counts,Ram Keralapura; Graham Cormode; Jeyashankher Ramamirtham,Abstract Monitoring is an issue of primary concern in current and next generation networkedsystems. For ex; the objective of sensor networks is to monitor their surroundings for avariety of different applications like atmospheric conditions; wildlife behavior; and troopmovements among others. Similarly; monitoring in data networks is critical not only foraccounting and management; but also for detecting anomalies and attacks. Such monitoringapplications are inherently continuous and distributed; and must be designed to minimizethe communication overhead that they introduce. In this context we introduce and study afundamental class of problems called" thresholded counts" where we must return theaggregate frequency count of an event that is continuously monitored by distributed nodeswith a user-specified accuracy whenever the actual count exceeds a given threshold …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,197
Synopses for massive data: Samples; histograms; wavelets; sketches,Graham Cormode; Minos Garofalakis; Peter J Haas; Chris Jermaine,Abstract Methods for Approximate Query Processing (AQP) are essential for dealing withmassive data. They are often the only means of providing interactive response times whenexploring massive datasets; and are also needed to handle high speed data streams. Thesemethods proceed by computing a lossy; compact synopsis of the data; and then executingthe query of interest against the synopsis rather than the entire dataset. We describe basicprinciples and recent developments in AQP. We focus on four key synopses: randomsamples; histograms; wavelets; and sketches. We consider issues such as accuracy; spaceand time efficiency; optimality; practicality; range of applicability; error bounds on queryanswers; and incremental maintenance. We also discuss the trade-offs between the differentsynopsis types.,Foundations and Trends in Databases,2012,195
Sketching probabilistic data streams,Graham Cormode; Minos Garofalakis,Abstract The management of uncertain; probabilistic data has recently emerged as a usefulparadigm for dealing with the inherent unreliabilities of several real-world applicationdomains; including data cleaning; information integration; and pervasive; multi-sensorcomputing. Unlike conventional data sets; a set of probabilistic tuples defines a probabilitydistribution over an exponential number of possible worlds (ie;" grounded"; deterministicdatabases). This" possibleworlds" interpretation allows for clean query semantics but alsoraises hard computational problems for probabilistic database query processors. To furthercomplicate matters; in many scenarios (eg; large-scale process and environmentalmonitoring using multiple sensor modalities); probabilistic data tuples arrive and need to beprocessed in a streaming fashion; that is; using limited memory and CPU resources and …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,187
Anonymizing bipartite graph data using safe groupings,Graham Cormode; Divesh Srivastava; Ting Yu; Qing Zhang,Abstract Private data often comes in the form of associations between entities; such ascustomers and products bought from a pharmacy; which are naturally represented in theform of a large; sparse bipartite graph. As with tabular data; it is desirable to be able topublish anonymized versions of such data; to allow others to perform ad hoc analysis ofaggregate graph properties. However; existing tabular anonymization techniques do notgive useful or meaningful results when applied to graphs: small changes or masking of theedge structure can radically change aggregate graph properties. We introduce a new familyof anonymizations; for bipartite graph data; called (k; l)-groupings. These groupingspreserve the underlying graph structure perfectly; and instead anonymize the mapping fromentities to nodes of the graph. We identify a class of" safe"(k; l)-groupings that have …,Proceedings of the VLDB Endowment,2008,176
Differentially Private Spatial Decompositions,Graham Cormode; Magda Procopiuc; Entong Shen; Divesh Srivastava; Ting Yu,Differential privacy has recently emerged as the de facto standard for private data release.This makes it possible to provide strong theoretical guarantees on the privacy and utility ofreleased data. While it is well-understood how to release data based on counts and simplefunctions under this guarantee; it remains to provide general purpose techniques to releasedata that is useful for a variety of queries. In this paper; we focus on spatial data such aslocations and more generally any multi-dimensional data that can be indexed by a treestructure. Directly applying existing differential privacy methods to this type of data simplygenerates noise. We propose instead the class of" private spatial decompositions'': theseadapt standard spatial indexing methods such as quad trees and kd-trees to provide aprivate description of the data distribution. Equipping such structures with differential …,Arxiv preprint arXiv:1103.5170,2011,171
Approximation algorithms for clustering uncertain data,Graham Cormode; Andrew McGregor,Abstract There is an increasing quantity of data with uncertainty arising from applicationssuch as sensor network measurements; record linkage; and as output of mining algorithms.This uncertainty is typically formalized as probability density functions over tuple values.Beyond storing and processing such data in a DBMS; it is necessary to perform other dataanalysis tasks such as data mining. We study the core mining problem of clustering onuncertain data; and define appropriate natural generalizations of standard clusteringoptimization criteria. Two variations arise; depending on whether a point is automaticallyassociated with its optimal center; or whether it must be assigned to a fixed cluster no matterwhere it is actually located. For uncertain versions of k-means and k-median; we showreductions to their corresponding weighted versions on data with no uncertainties. These …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,170
-Finding Hierarchical Heavy Hitters in Data Streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Aggregation along hierarchies is a critical summary technique in a large variety of onlineapplications including decision support (OLAP); network management (IP clustering anddenial-of-service attack monitoring); text (on prefixes of strings occurring in the text) and XMLsummarization (on prefixes of root-to-leaf paths in the XML data tree). In these applications;the data is inherently hierarchical and one needs to maintain aggregates at different levelsof the hierarchy over time in a dynamic fashion. It formalizes the problem of finding heavyhitters in massive data streams that considers their hierarchical structure. Such hierarchicalheavy hitters (HHHs) present a “cartogram” summary of the data stream distribution. Thischapter presents comprehensive solutions to the problem of estimating HHHs on datastreams. The resulting summary gives a topological “cartogram” of the hierarchical data. It …,*,2003,167
Algorithms for distributed functional monitoring,Graham Cormode; S Muthukrishnan; Ke Yi,Abstract Consider the following problem: We have k players each receiving a stream ofitems; and communicating with a central coordinator. Let the multiset of items received byplayer i up until time t be A i (t). The coordinator's task is to monitor a given function fcomputed over the union of the inputs∪ i A i (t); continuously at all times t. The goal is tominimize the number of bits communicated between the players and the coordinator. Ofinterest is the approximate version where the coordinator outputs 1 if f &geq; τ and 0 iff&leq;(1− &epsis;) τ. This defines the (k; f; τ; &epsis;) distributed functional monitoringproblem. Functional monitoring problems are fundamental in distributed systems; inparticular sensor networks; where we must minimize communication; they also connect tothe well-studied streaming model and communication complexity. Yet few formal bounds …,ACM Transactions on Algorithms (TALG),2011,151
Communication complexity of document exchange,Graham Cormode; Mike Paterson; Süleyman Cenk Sahinalp; Uzi Vishkin,Page 1. 1 Communication Complexity of Document Exchange Graham Cormode; Mike Paterson;Cenk Sahinalp; Uzi Vishkin Page 2. 2 Document Exchange • Two parties — each have a copyof a (huge) file • The copies differ and there is no record of the changes • Goal: the partiescommunicate to exchange their files • If the files are size n and the “distance” is f; want thecommunication to be f · g(n) • Aim is to minimize communication; and number of rounds Page3. 3 Prior Work Correcting f Hamming Differences • Metzner 83; Metzner 91; Barbará & Lipton91 • Abdel-Ghaffar and Abbadi (1994) communicate O(f log n) bits [based on Reed-Solomoncodes] Protocols fail if there are more than f differences Edit Distance Heuristics given bySchwarz; Bowdidge; Burkhard 90 and the simple Rsync utility (Tridgell; Mackerras 96) Noguarantees on performance Page 4. 4 Correcting Differences …,SODA,2000,146
A near-optimal algorithm for computing the entropy of a stream,Amit Chakrabarti; Graham Cormode; Andrew McGregor,Abstract We describe a simple algorithm for approximating the empirical entropy of a streamof m values in a single pass; using O (ε-2 log (δ-1) log m) words of space. Our algorithm isbased upon a novel extension of a method introduced by Alon; Matias; and Szegedy [1]. Weshow a space lower bound of Ω (ε-2/log (ε-1)); meaning that our algorithm is near-optimal interms of its dependency on ε. This improves over previous work on this problem [8; 13; 17;5]. We show that generalizing to kth order entropy requires close to linear space for all k≥ 1;and give additive approximations using our algorithm. Lastly; we show how to compute amultiplicative approximation to the entropy of a random walk on an undirected graph.,Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,2007,127
Space efficient mining of multigraph streams,Graham Cormode; S Muthukrishnan,Abstract The challenge of monitoring massive amounts of data generated by communicationnetworks has led to the interest in data stream processing. We study streams of edges inmassive communication multigraphs; defined by (source; destination) pairs. The goal is tocompute properties of the underlying graph while using small space (much smaller than thenumber of communicants); and to avoid bias introduced because some edges may appearmany times; while others are seen only once. We give results for three fundamentalproblems on multigraph degree sequences: estimating frequency moments of degrees;finding the heavy hitter degrees; and computing range sums of degree values. In all caseswe are able to show space bounds for our summarizing algorithms that are significantlysmaller than storing complete information. We use a variety of data stream methods …,Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2005,124
Practical Verified Computation with Streaming Interactive Proofs,Graham Cormode; Michael Mitzenmacher; Justin Thaler,Abstract When delegating computation to a service provider; as in the cloud computingparadigm; we seek some reassurance that the output is correct and complete. Yetrecomputing the output as a check is inefficient and expensive; and it may not even befeasible to store all the data locally. We are therefore interested in what can be validated bya streaming (sublinear space) user; who cannot store the full input; or perform the fullcomputation herself. Our aim in this work is to advance a recent line of work on" proofsystems" in which the service provider proves the correctness of its output to a user. Thegoal is to minimize the time and space costs of both parties in generating and checking theproof. Only very recently have there been attempts to implement such proof systems; andthus far these have been quite limited in functionality. Here; our approach is two-fold. First …,Arxiv preprint arXiv:1105.2003,2011,122
Diamond in the rough: Finding hierarchical heavy hitters in multi-dimensional data,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items archived in data warehouses or those that arrive online as streamstypically have attributes which take values from multiple hierarchies (eg; time andgeographic location; source and destination IP addresses). Providing an aggregate view ofsuch data is important to summarize; visualize; and analyze. We develop the aggregate viewbased on certain hierarchically organized sets of large-valued regions (" heavy hitters").Such Hierarchical Heavy Hitters (HHHs) were previously introduced as a crucialaggregation technique in one dimension. In order to analyze the wider range of datawarehousing applications and realistic IP data streams; we generalize this problem tomultiple dimensions. We identify and study two variants of HHHs for multi-dimensional data;namely the" overlap" and" split" cases; depending on how an aggregate computed for a …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,119
Node classification in social networks,Smriti Bhagat; Graham Cormode; S Muthukrishnan,Abstract When dealing with large graphs; such as those that arise in the context of onlinesocial networks; a subset of nodes may be labeled. These labels can indicate demographicvalues; interest; beliefs or other characteristics of the nodes (users). A core problem is to usethis information to extend the labeling so that all nodes are assigned a label (or labels). Inthis chapter; we survey classification techniques that have been proposed for this problem.We consider two broad categories: methods based on iterative application of traditionalclassifiers using graph information as features; and methods which propagate the existinglabels via random walks. We adopt a common perspective on these methods to highlight thesimilarities between different approaches within and across the two categories. We alsodescribe some extensions and related directions to the central problem of node …,*,2011,115
Conquering the divide: Continuous clustering of distributed data streams,Graham Cormode; S Muthukrishnan; Wei Zhuang,Data is often collected over a distributed network; but in many cases; is so voluminous that itis impractical and undesirable to collect it in a central location. Instead; we must performdistributed computations over the data; guaranteeing high quality answers even as new dataarrives. In this paper; we formalize and study the problem of maintaining a clustering of suchdistributed data that is continuously evolving. In particular; our goal is to minimize thecommunication and computational cost; still providing guaranteed accuracy of the clustering.We focus on the k-center clustering; and provide a suite of algorithms that vary based onwhich centralized algorithm they derive from; and whether they maintain a single globalclustering or many local clusterings that can be merged together. We show that thesealgorithms can be designed to give accuracy guarantees that are close to the best …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,115
Methods for finding frequent items in data streams,Graham Cormode; Marios Hadjieleftheriou,Abstract The frequent items problem is to process a stream of items and find all itemsoccurring more than a given fraction of the time. It is one of the most heavily studiedproblems in data stream mining; dating back to the 1980s. Many applications rely directly orindirectly on finding the frequent items; and implementations are in use in large scaleindustrial systems. However; there has not been much comparison of the different methodsunder uniform experimental conditions. It is common to find papers touching on this topic inwhich important related work is mischaracterized; overlooked; or reinvented. In this paper;we aim to present the most important algorithms for this problem in a common framework.We have created baseline implementations of the algorithms and used these to perform athorough experimental study of their properties. We give empirical evidence that there is …,The VLDB Journal,2010,108
Summarizing and mining skewed data streams,Graham Cormode; S Muthukrishnan,Abstract Many applications generate massive data streams. Summarizing such massivedata requires fast; small space algorithms to support post-hoc queries and mining. Animportant observation is that such streams are rarely uniform; and real data sources typicallyexhibit significant skewness. These are well modeled by Zipf distributions; which arecharacterized by a parameter; z; that captures the amount of skew. We present a data streamsummary that can answer point queries with∊ accuracy and show that the space needed isonly O (∊−-min {1; 1/z}). This is the first o (1/∊) space algorithm for this problem; and weshow it is essentially tight for skewed distributions. We show that the same data structure canalso estimate the L 2 norm of the stream in o (1/∊ 2) space for z> ½; another improvementover the existing Ω (1/∊ 2) methods. We support our theoretical results with an …,*,2005,100
Holistic UDAFs at streaming speeds,Graham Cormode; Theodore Johnson; Flip Korn; Shan Muthukrishnan; Oliver Spatscheck; Divesh Srivastava,Abstract Many algorithms have been proposed to approximate holistic aggregates; such asquantiles and heavy hitters; over data streams. However; little work has been done toexplore what techniques are required to incorporate these algorithms in a data stream queryprocessor; and to make them useful in practice. In this paper; we study the performanceimplications of using user-defined aggregate functions (UDAFs) to incorporate selection-based and sketch-based algorithms for holistic aggregates into a data stream managementsystem's query processing architecture. We identify key performance bottlenecks andtradeoffs; and propose novel techniques to make these holistic UDAFs fast and space-efficient for use in high-speed data stream applications. We evaluate performance usinggenerated and actual IP packet data; focusing on approximating quantiles and heavy …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,99
Privbayes: Private data release via bayesian networks,Jun Zhang; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Xiaokui Xiao,Abstract Privacy-preserving data publishing is an important problem that has been the focusof extensive study. The state-of-the-art solution for this problem is differential privacy; whichoffers a strong degree of privacy protection without making restrictive assumptions about theadversary. Existing techniques using differential privacy; however; cannot effectively handlethe publication of high-dimensional data. In particular; when the input dataset contains alarge number of attributes; existing methods require injecting a prohibitive amount of noisecompared to the signal in the data; which renders the published data next to useless. Toaddress the deficiency of the existing methods; this paper presents P riv B ayes; adifferentially private method for releasing high-dimensional data. Given a dataset D; P riv Bayes first constructs a Bayesian network N; which (i) provides a succinct model of the …,ACM Transactions on Database Systems (TODS),2017,92
Finding the frequent items in streams of data,Graham Cormode; Marios Hadjieleftheriou,Abstract Many data generation processes can be modeled as data streams. They producehuge numbers of pieces of data; each of which is simple in isolation; but which takentogether lead to a complex whole. For example; the sequence of queries posed to anInternet search engine can be thought of as a stream; as can the collection of transactionsacross all branches of a supermarket chain. In aggregate; this data can arrive at enormousrates; easily in the realm of hundreds of gigabytes per day or higher. While this data may bearchived and indexed within a data warehouse; it is also important to process the data" as ithappens;" to provide up to the minute analysis and statistics on current trends. Methods toachieve this must be quick to respond to each new piece of information; and use resourceswhich are very small when compared to the total quantity of data. These applications and …,Communications of the ACM,2009,85
Summarizing and mining inverse distributions on data streams via dynamic inverse sampling,Graham Cormode; S Muthukrishnan; Irina Rozenbaum,Abstract Emerging data stream management systems approach the challenge of massivedata distributions which arrive at high speeds while there is only small storage bysummarizing and mining the distributions using samples or sketches. However; datadistributions can be" viewed" in different ways. A data stream of integer values can beviewed either as the forward distribution f (x); ie.; the number of occurrences of x in thestream; or as its inverse; f-1 (i); which is the number of items that appear i times. While bothsuch" views" are equivalent in stored data systems; over data streams that entailapproximations; they may be significantly different. In other words; samples and sketchesdeveloped for the forward distribution may be ineffective for summarizing or mining theinverse distribution. Yet; many applications such as IP traffic monitoring naturally rely on …,Proceedings of the 31st international conference on Very large data bases,2005,80
Sequence distance embeddings,Graham Cormode,In this chapter; embeddings of the permutation distances discussed in Chapter 1 into vectordistances are described. Unlike the embeddings of vector distances we saw in Chapter 2;these have fixed distortion factors (compared to the 1±ϵ distortion factors for Lp distances).On the other hand; these are non-probabilistic embeddings: the distance between anypossible pair of permutations is distorted by no more than the stated distortion factor. Theseembeddings are constructed by using combinatorial observations about the differentpermutation distances. Once we have embedded permutations into vector distances; we cango on and embed these vectors into much smaller vector spaces using the embeddings inChapter 2. This approach yields many other interesting results. In Section 2. 4 we looked atsome geometric problems on vectors. We can ask the same questions for permutations …,*,2003,80
Mergeable Summaries,Pankaj K Agarwal; Graham Cormode; Zengfeng Haung; Jeff M Phillips; Zhewei Wei; Ke Yi,*,*,*,78
Anonymized data: generation; models; usage,Graham Cormode; Divesh Srivastava,Abstract Data anonymization techniques have been the subject of intense investigation inrecent years; for many kinds of structured data; including tabular; graph and item set data.They enable publication of detailed information; which permits ad hoc queries and analyses;while guaranteeing the privacy of sensitive information in the data against a variety ofattacks. In this tutorial; we aim to present a unified framework of data anonymizationtechniques; viewed through the lens of uncertainty. Essentially; anonymized data describesa set of possible worlds; one of which corresponds to the original data. We show thatanonymization approaches such as suppression; generalization; perturbation andpermutation generate different working models of uncertain data; some of which have beenwell studied; while others open new directions for research. We demonstrate that the …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,76
Optimal sampling from distributed streams,Graham Cormode; S Muthukrishnan; Ke Yi; Qin Zhang,Abstract A fundamental problem in data management is to draw a sample of a large data set;for approximate query answering; selectivity estimation; and query planning. With large;streaming data sets; this problem becomes particularly difficult when the data is sharedacross multiple distributed sites. The challenge is to ensure that a sample is drawn uniformlyacross the union of the data while minimizing the communication needed to run the protocoland track parameters of the evolving data. At the same time; it is also necessary to make theprotocol lightweight; by keeping the space and time costs low for each participant. In thispaper; we present communication-efficient protocols for sampling (both with and withoutreplacement) from k distributed streams. These apply to the case when we want a samplefrom the full streams; and to the sliding window cases of only the W most recent items; or …,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,72
Forward decay: A practical time decay model for streaming systems,Graham Cormode; Vladislav Shkapenyuk; Divesh Srivastava; Bojian Xu,Temporal data analysis in data warehouses and datastreaming systems often uses timedecay to reduce the importance of older tuples; without eliminating their influence; on theresults of the analysis. While exponential time decay is commonly used in practice; otherdecay functions (eg polynomial decay) are not; even though they have been identified asuseful. We argue that this is because the usual definitions of time decay are" backwards": thedecayed weight of a tuple is based on its age; measured backward from the current time.Since this age is constantly changing; such decay is too complex and unwieldy for scalableimplementation. In this paper; we propose a new class of" forward" decay functions basedon measuring forward from a fixed point in time. We show that this model captures the morepractical models already known; such as exponential decay and landmark windows; but …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,71
Personal privacy vs population privacy: learning to attack anonymization,Graham Cormode,Abstract Over the last decade great strides have been made in developing techniques tocompute functions privately. In particular; Differential Privacy gives strong promises aboutconclusions that can be drawn about an individual. In contrast; various syntactic methods forproviding privacy (criteria such as k-anonymity and l-diversity) have been criticized for stillallowing private information of an individual to be inferred. In this paper; we consider theability of an attacker to use data meeting privacy definitions to build an accurate classifier.We demonstrate that even under Differential Privacy; such classifiers can be used to infer"private" attributes accurately in realistic data. We compare this to similar approaches forinference-based attacks on other forms of anonymized data. We show how the efficacy of allthese attacks can be measured on the same scale; based on the probability of …,Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,2011,68
Space-and time-efficient deterministic algorithms for biased quantiles over data streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Skew is prevalent in data streams; and should be taken into account by algorithmsthat analyze the data. The problem of finding" biased quantiles"—that is; approximatequantiles which must be more accurate for more extreme values—is a framework forsummarizing such skewed data on data streams. We present the first deterministicalgorithms for answering biased quantiles queries accurately with small—sublinear in theinput size—space and time bounds in one pass. The space bound is near-optimal; and theamortized update cost is close to constant; making it practical for handling high speednetwork data streams. We not only demonstrate theoretical properties of the algorithm; butalso show it uses less space than existing methods in many practical settings; and is fast tomaintain.,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,68
Anonymizing bipartite graph data using safe groupings,Graham Cormode; Divesh Srivastava; Ting Yu; Qing Zhang,Abstract Private data often come in the form of associations between entities; such ascustomers and products bought from a pharmacy; which are naturally represented in theform of a large; sparse bipartite graph. As with tabular data; it is desirable to be able topublish anonymized versions of such data; to allow others to perform ad hoc analysis ofaggregate graph properties. However; existing tabular anonymization techniques do notgive useful or meaningful results when applied to graphs: small changes or masking of theedge structure can radically change aggregate graph properties. We introduce a new familyof anonymizations for bipartite graph data; called (k; ℓ)-groupings. These groupingspreserve the underlying graph structure perfectly; and instead anonymize the mapping fromentities to nodes of the graph. We identify a class of “safe”(k; ℓ)-groupings that have …,The VLDB Journal,2010,66
Robust lower bounds for communication and stream computation,Amit Chakrabarti; Graham Cormode; Andrew McGregor,Abstract We study the communication complexity of evaluating functions when the input datais randomly allocated (according to some known distribution) amongst two or more players;possibly with information overlap. This naturally extends previously studied variable partitionmodels such as the best-case and worst-case partition models [32; 29]. We aim tounderstand whether the hardness of a communication problem holds for almost everyallocation of the input; as opposed to holding for perhaps just a few atypical partitions. A keyapplication is to the heavily studied data stream model. There is a strong connectionbetween our communication lower bounds and lower bounds in the data stream model thatare" robust" to the ordering of the data. That is; we prove lower bounds for when the order ofthe items in the stream is chosen not adversarially but rather uniformly (or near-uniformly) …,Proceedings of the fortieth annual ACM symposium on Theory of computing,2008,66
What’s different: Distributed; continuous monitoring of duplicate-resilient aggregates on data streams,Graham Cormode; S Muthukrishnan; Wei Zhuang,Emerging applications in sensor systems and network-wide IP traffic analysis present manytechnical challenges. They need distributed monitoring and continuous tracking of events.They have severe resource constraints not only at each site in terms of per-updateprocessing time and archival space for highspeed streams of observations; but alsocrucially; communication constraints for collaborating on the monitoring task. Theseelements have been addressed in a series of recent works. A fundamental issue that arisesis that one cannot make the" uniqueness" assumption on observed events which is presentin previous works; since widescale monitoring invariably encounters the same events atdifferent points. For example; within the network of an Internet Service Provider packets ofthe same flow will be observed in different routers; similarly; the same individual will be …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,66
Space-optimal heavy hitters with strong error bounds,Radu Berinde; Piotr Indyk; Graham Cormode; Martin J Strauss,Abstract The problem of finding heavy hitters and approximating the frequencies of items isat the heart of many problems in data stream analysis. It has been observed that severalproposed solutions to this problem can outperform their worst-case guarantees on real data.This leads to the question of whether some stronger bounds can be guaranteed. We answerthis in the positive by showing that a class of counter-based algorithms (including thepopular and very space-efficient Frequent and SpacesSaving algorithms) provides muchstronger approximation guarantees than previously known. Specifically; we show that errorsin the approximation of individual elements do not depend on the frequencies of the mostfrequent elements; but only on the frequency of the remaining tail. This shows that counter-based methods are the most space-efficient (in fact; space-optimal) algorithms having this …,ACM Transactions on Database Systems (TODS),2010,65
Histograms and wavelets on probabilistic data,Graham Cormode; Minos Garofalakis,There is a growing realization that uncertain information is a first-class citizen in moderndatabase management. As such; we need techniques to correctly and efficiently processuncertain data in database systems. In particular; data reduction techniques that canproduce concise; accurate synopses of large probabilistic relations are crucial. Similar totheir deterministic relation counterparts; such compact probabilistic data synopses can formthe foundation for human understanding and interactive data exploration; probabilistic queryplanning and optimization; and fast approximate query processing in probabilistic databasesystems. In this paper; we introduce definitions and algorithms for building histogram-andHaar wavelet-based synopses on probabilistic data. The core problem is to choose a set ofhistogram bucket boundaries or wavelet coefficients to optimize the accuracy of the …,IEEE Transactions on Knowledge and Data Engineering,2010,61
Approximate continuous querying over distributed streams,Graham Cormode; Minos Garofalakis,Abstract While traditional database systems optimize for performance on one-shot queryprocessing; emerging large-scale monitoring applications require continuous tracking ofcomplex data-analysis queries over collections of physically distributed streams. Thus;effective solutions have to be simultaneously space/time efficient (at each remote monitorsite); communication efficient (across the underlying communication network); and providecontinuous; guaranteed-quality approximate query answers. In this paper; we propose novelalgorithmic solutions for the problem of continuously tracking a broad class of complexaggregate queries in such a distributed-streams setting. Our tracking schemes maintainapproximate query answers with provable error guarantees; while simultaneously optimizingthe storage space and processing time at each remote site; and the communication cost …,ACM Transactions on Database Systems (TODS),2008,61
Minimizing minimality and maximizing utility: analyzing method-based attacks on anonymized data,Graham Cormode; Divesh Srivastava; Ninghui Li; Tiancheng Li,Abstract The principle of anonymization for data sharing has become a very popularparadigm for the preservation of privacy of the data subjects. Since the introduction of k-anonymity; dozens of methods and enhanced privacy definitions have been proposed.However; over-eager attempts to minimize the information lost by the anonymizationpotentially allow private information to be inferred. Proof-of-concept of this" minimality attack"has been demonstrated for a variety of algorithms and definitions [16]. In this paper; weprovide a comprehensive analysis and study of this attack; and demonstrate that with care itseffect can be almost entirely countered. The attack allows an adversary to increase his(probabilistic) belief in certain facts about individuals over the data. We show that (a) a largeclass of algorithms are not affected by this attack;(b) for a class of algorithms that have a" …,Proceedings of the VLDB Endowment,2010,60
Time-decaying aggregates in out-of-order streams,Graham Cormode; Flip Korn; Srikanta Tirthapura,Abstract Processing large data streams is now a major topic in data management. The datainvolved can be truly massive; and the required analyses complex. In a stream of sequentialevents such as stock feeds; sensor readings; or IP traffic measurements; data tuplespertaining to recent events are typically more important than older ones. This can beformalized via time-decay functions; which assign weights to data based on the age of data.Decay functions such as sliding windows and exponential decay have been studied underthe assumption of well-ordered arrivals; ie; data arrives in non-decreasing order of timestamps. However; data quality issues are prevalent in massive streams (due to networkasynchrony and delays etc.); and correct arrival order is not guaranteed. We focus on thecomputation of decayed aggregates such as range queries; quantiles; and heavy hitters …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,58
Differentially private summaries for sparse data,Graham Cormode; Cecilia Procopiuc; Divesh Srivastava; Thanh TL Tran,Abstract Differential privacy is fast becoming the method of choice for releasing data understrong privacy guarantees. A standard mechanism is to add noise to the counts incontingency tables derived from the dataset. However; when the dataset is sparse in itsunderlying domain; this vastly increases the size of the published data; to the point ofmaking the mechanism infeasible. We propose a general framework to overcome thisproblem. Our approach releases a compact summary of the noisy data with the same privacyguarantee and with similar utility. Our main result is an efficient method for computing thesummary directly from the input data; without materializing the vast noisy data. Weinstantiate this general framework for several summarization methods. Our experimentsshow that this is a highly practical solution: The summaries are up to 1000 times smaller …,Proceedings of the 15th International Conference on Database Theory,2012,56
Set cover algorithms for very large datasets,Graham Cormode; Howard Karloff; Anthony Wirth,Abstract The problem of Set Cover-to find the smallest subcollection of sets that covers someuniverse-is at the heart of many data and analysis tasks. It arises in a wide range of settings;including operations research; machine learning; planning; data quality and data mining.Although finding an optimal solution is NP-hard; the greedy algorithm is widely used; andtypically finds solutions that are close to optimal. However; a direct implementation of thegreedy approach; which picks the set with the largest number of uncovered items at eachstep; does not behave well when the input is very large and disk resident. The greedyalgorithm must make many random accesses to disk; which are unpredictable and costly incomparison to linear scans. In order to scale Set Cover to large datasets; we provide a newalgorithm which finds a solution that is provably close to that of greedy; but which is much …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,56
Finding hierarchical heavy hitters in streaming data,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items that arrive online as streams typically have attributes which take valuesfrom one or more hierarchies (time and geographic location; source and destination IPaddresses; etc.). Providing an aggregate view of such data is important for summarization;visualization; and analysis. We develop an aggregate view based on certain organized setsof large-valued regions (“heavy hitters”) corresponding to hierarchically discountedfrequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We firstconsider computing (approximate) HHHs over a data stream drawn from a singlehierarchical attribute. We formalize the problem and give deterministic algorithms to findthem in a single pass over the input. In order to analyze a wider range of realistic datastreams (eg; from IP traffic-monitoring applications); we generalize this problem to …,ACM Transactions on Knowledge Discovery from Data (TKDD),2008,55
Privacy in dynamic social networks,Smriti Bhagat; Graham Cormode; Balachander Krishnamurthy; Divesh Srivastava,Abstract Anonymization of social networks before they are published or shared has becomean important research question. Recent work on anonymizing social networks has looked atprivacy preserving techniques for publishing a single instance of the network. However;social networks evolve and a single instance is inadequate for analyzing the evolution of thesocial network or for performing any longitudinal data analysis. We study the problem ofrepeatedly publishing social network data as the network evolves; while preserving privacyof users. Publishing multiple instances of the same network independently has privacy risks;since stitching the information together may allow an adversary to identify users in thenetworks. We propose methods to anonymize a dynamic network such that the privacy ofusers is preserved when new nodes and edges are added to the published network …,Proceedings of the 19th international conference on World wide web,2010,51
Fast approximate wavelet tracking on streams,Graham Cormode; Minos Garofalakis; Dimitris Sacharidis,Abstract Recent years have seen growing interest in effective algorithms for summarizingand querying massive; high-speed data streams. Randomized sketch synopses provideaccurate approximations for general-purpose summaries of the streaming data distribution(eg; wavelets). The focus of existing work has typically been on minimizing spacerequirements of the maintained synopsis—however; to effectively support high-speed data-stream analysis; a crucial practical requirement is to also optimize:(1) the update time forincorporating a streaming data element in the sketch; and (2) the query time for producingan approximate summary (eg; the top wavelet coefficients) from the sketch. Such time costsmust be small enough to cope with rapid stream-arrival rates and the real-time queryingrequirements of typical streaming applications (eg; ISP network monitoring). With cheap …,International Conference on Extending Database Technology,2006,51
Applying link-based classification to label blogs,Smriti Bhagat; Graham Cormode; Irina Rozenbaum,Abstract In analyzing data from social and communication networks; we encounter theproblem of classifying objects where there is explicit link structure amongst the objects. Westudy the problem of inferring the classification of all the objects from a labeled subset; usingonly link-based information between objects. We abstract the above as a labeling problemon multigraphs with weighted edges. We present two classes of algorithms; based on localand global similarities. Then we focus on multigraphs induced by blog data; and carefullyapply our general algorithms to specifically infer labels such as age; gender and locationassociated with the blog based only on the link-structure amongst them. We perform acomprehensive set of experiments with real; large-scale blog data sets and show thatsignificant accuracy is possible from little or no non-link information; and our methods …,*,2009,47
Streaming in a connected world: querying and tracking distributed data streams,Graham Cormode; Minos Garofalakis,Today; a majority of data is fundamentally distributed in nature. Data for almost any task iscollected over a broad area; and streams in at a much greater rate than ever before. Inparticular; advances in sensor technology and miniaturization have led to the concept of thesensor network: a (typically wireless) collection of sensing devices collecting detailed dataabout their surroundings. A fundamental question arises: how to query and monitor this richnew source of data? A similar scenario emerges within more traditional; wired networks: ifdata is collected over remote sites; either about observed external conditions or about thenetwork itself (eg in IP network monitoring); how to process this data in order to answercertain queries? Additionally; other emerging models of distributed computation; such aspeer-to-peer (P2P) networks and grid-based computing face the same problems of …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,47
Estimating dominance norms of multiple data streams,Graham Cormode; S Muthukrishnan,Abstract There is much focus in the algorithms and database communities on designingtools to manage and mine data streams. Typically; data streams consist of multiple signals.Formally; a stream of multiple signals is (i; ai; j) where i's correspond to the domain; j's indexthe different signals and ai; j≥ 0 give the value of the j th signal at point i. We study theproblem of finding norms that are cumulative of the multiple signals in the data stream. Forexample; consider the max-dominance norm; defined as∑ i max j {ai; j}. It may be thought asestimating the norm of the “upper envelope” of the multiple signals; or alternatively; asestimating the norm of the “marginal” distribution of tabular data streams. It is used inapplications to estimate the “worst case influence” of multiple processes; for example in IPtraffic analysis; electrical grid monitoring and financial domain. In addition; it is a natural …,European Symposium on Algorithms,2003,47
Fast mining of massive tabular data via approximate distance computations,Graham Cormode; Piotr Indyk; Nick Koudas; S Muthukrishnan,Tabular data abound in many data stores: traditional relational databases store tables; andnew applications also generate massive tabular datasets. We present methods fordetermining similar regions in massive tabular data. Our methods are for computing the"distance" between any two subregions of tabular data: they are approximate; but highlyaccurate as we prove mathematically; and they are fast; running in time nearly linear in thetable size. Our methods are general since these distance computations can be applied toany mining or similarity algorithms that use L/sub p/norms. A novelty of our distancecomputation procedures is that they work for any L/sub p/norms; not only the traditional p= 2or p= 1; but for all p/spl les/2; the choice of p; say fractional p; provides an interestingalternative similarity behavior! We use our algorithms in a detailed experimental study of …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,47
On estimating frequency moments of data streams,Sumit Ganguly; Graham Cormode,Abstract Space-economical estimation of the p th frequency moments; defined as; for p> 0;are of interest in estimating all-pairs distances in a large data matrix 14; machine learning;and in data stream computation. Random sketches formed by the inner product of thefrequency vector f 1;...; fn with a suitably chosen random vector were pioneered by Alon;Matias and Szegedy 1; and have since played a central role in estimating F p and for datastream computations in general. The concept of p-stable sketches formed by the innerproduct of the frequency vector with a random vector whose components are drawn from ap-stable distribution; was proposed by Indyk for estimating F p; for 0< p< 2; and has beenfurther studied in Li 13. In this paper; we consider the problem of estimating F p; for 0< p< 2.A disadvantage of the stable sketches technique and its variants is that they require O(1ϵ …,*,2007,46
Periosteum‐Mimetic Structures Made from Freestanding Microgrooved Nanosheets,Xuetao Shi; Toshinori Fujie; Akihiro Saito; Shinji Takeoka; Ying Hou; Yiwei Shu; Mingwei Chen; Hongkai Wu; Ali Khademhosseini,DOI: 10.1002/adma. 201305804 and the direction of bone development. The potential of theperiosteum regarding the growth modulation of bone has been verified in vitro and in vivo.The failure of the periosteum in patients who suffer bone defects induced by tumor or traumaposes a considerable challenge.[4] Creating an artificial scaffold that presents the conditionsand morphology necessary to simulate the native periosteum may be an ideal approach forthese cases. Due to the increasing awareness of the periosteum's importance; numerousrecent studies have constructed tissue engineered periosteum using collagen andhydrogels.[5] However; most of these studies have neglected the hierarchical topographicsurface of the periosteum; which is important for bone elongation. Importantly; such artificialperiosteum films cannot effectively anchor to or integrate with bone scaffolds (eg …,Advanced Materials,2014,44
Semantics of ranking queries for probabilistic data,Jeffrey Jestes; Graham Cormode; Feifei Li; Ke Yi,Recently; there have been several attempts to propose definitions and algorithms for rankingqueries on probabilistic data. However; these lack many intuitive properties of a top-k overdeterministic data. We define several fundamental properties; including exact-k;containment; unique rank; value invariance; and stability; which are satisfied by rankingqueries on certain data. We argue that these properties should also be carefully studied indefining ranking queries in probabilistic data; and fulfilled by definition for ranking uncertaindata for most applications. We propose an intuitive new ranking definition based on theobservation that the ranks of a tuple across all possible worlds represent a well-foundedrank distribution. We studied the ranking definitions based on the expectation; the median;and other statistics of this rank distribution for a tuple and derived the expected rank …,IEEE Transactions on Knowledge and Data Engineering,2011,44
Verifying computations with streaming interactive proofs,Graham Cormode; Justin Thaler; Ke Yi,Abstract When computation is outsourced; the data owner would like to be assured that thedesired computation has been performed correctly by the service provider. In theory; proofsystems can give the necessary assurance; but prior work is not sufficiently scalable orpractical. In this paper; we develop new proof protocols for verifying computations which arestreaming in nature: the verifier (data owner) needs only logarithmic space and a singlepass over the input; and after observing the input follows a simple protocol with a prover(service provider) that takes logarithmic communication spread over a logarithmic number ofrounds. These ensure that the computation is performed correctly: that the service providerhas not made any errors or missed out some data. The guarantee is very strong: even if theservice provider deliberately tries to cheat; there is only vanishingly small probability of …,Proceedings of the VLDB Endowment,2011,44
Estimating the confidence of conditional functional dependencies,Graham Cormode; Lukasz Golab; Korn Flip; Andrew McGregor; Divesh Srivastava; Xi Zhang,Abstract Conditional functional dependencies (CFDs) have recently been proposed asextensions of classical functional dependencies that apply to a certain subset of the relation;as specified by a pattern tableau. Calculating the support and confidence of a CFD (ie; thesize of the applicable subset and the extent to which it satisfies the CFD) gives valuableinformation about data semantics and data quality. While computing the support is easier;computing the confidence exactly is expensive if the relation is large; and estimating it from arandom sample of the relation is unreliable unless the sample is large. We study how toefficiently estimate the confidence of a CFD with a small number of passes (one or two) overthe input using small space. Our solutions are based on a variety of sampling and sketchingtechniques; and apply when the pattern tableau is known in advance; and also the harder …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,42
The continuous distributed monitoring model,Graham Cormode,Abstract In the model of continuous distributed monitoring; a number of observers each seea stream of observations. Their goal is to work together to compute a function of the union oftheir observations. This can be as simple as counting the total number of observations; ormore complex non-linear functions such as tracking the entropy of the induced distribution.Assuming that it is too costly to simply centralize all the observations; it becomes quitechallenging to design solutions which provide a good approximation to the current answer;while bounding the communication cost of the observers; and their other resources such astheir space usage. This survey introduces this model; and describe a selection results in thissetting; from the simple counting problem to a variety of other functions that have beenstudied.,ACM SIGMOD Record,2013,41
Prediction Promotes Privacy in Dynamic Social Networks.,Smriti Bhagat; Graham Cormode; Divesh Srivastava; B Krishnamurthy,Abstract Recent work on anonymizing online social networks (OSNs) has looked at privacypreserving techniques for publishing a single instance of the network. However; OSNsevolve and a single instance is inadequate for analyzing their evolution or performinglongitudinal data analysis. We study the problem of repeatedly publishing OSN data as thenetwork evolves while preserving privacy of users. Publishing multiple instancesindependently has privacy risks; since stitching the information together may allow anadversary to identify users. We provide methods to anonymize a dynamic network when newnodes and edges are added to the published network. These methods use link predictionalgorithms to model the evolution. Using this predicted graph to perform group-basedanonymization; the loss in privacy caused by new edges can be eliminated almost …,WOSN,2010,41
Towards an algorithmic theory of compressed sensing,Graham Cormode; S Muthukrishnan,ABSTRACT In Approximation Theory; the fundamental problem is to reconstruct a signal A∈Rn from linear measurements〈 A; ψi〉 with respect to a dictionary Ψ for Rn. Recently; therehas been tremendous excitement about the novel direction of Compressed Sensing [10]where the reconstruction can be done with very few—O (k)—linear measurements over amodified dictionary Ψ if the information of the signal is concentrated in k coefficients over anorthonormal basis Ψ. These results have reconstruction error on any given signal that isoptimal with respect to a broad class of signals. In a series of papers and meetings over thepast year; a theory of Compressed Sensing has been developed by mathematicians. Wedevelop an algorithmic perspective for the Compressed Sensing problem; showing thatCompressed Sensing results resonate with prior work in Group Testing; Learning theory …,Center for Discrete Math. and Comp. Sci.(DIMACS); Tech. Rep. TR,2005,41
Differentially private publication of sparse data,Graham Cormode; Magda Procopiuc; Divesh Srivastava; Thanh TL Tran,Abstract: The problem of privately releasing data is to provide a version of a dataset withoutrevealing sensitive information about the individuals who contribute to the data. The modelof differential privacy allows such private release while providing strong guarantees on theoutput. A basic mechanism achieves differential privacy by adding noise to the frequencycounts in the contingency tables (or; a subset of the count data cube) derived from thedataset. However; when the dataset is sparse in its underlying space; as is the case for mostmulti-attribute relations; then the effect of adding noise is to vastly increase the size of thepublished data: it implicitly creates a huge number of dummy data points to mask the truedata; making it almost impossible to work with. We present techniques to overcome thisroadblock and allow efficient private release of sparse data; while maintaining the …,arXiv preprint arXiv:1103.0825,2011,39
Flexible aggregate similarity search,Yang Li; Feifei Li; Ke Yi; Bin Yao; Min Wang,Abstract Aggregate similarity search; aka aggregate nearest neighbor (Ann) query; findsmany useful applications in spatial and multimedia databases. Given a group Q of M queryobjects; it retrieves the most (or top-k) similar object to Q from a database P; where thesimilarity is an aggregation (eg; sum; max) of the distances between the retrieved object pand all the objects in Q. In this paper; we propose an added flexibility to the query definition;where the similarity is an aggregation over the distances between p and any subset of ÆMobjects in Q for some support 0< Æ d 1. We call this new definition flexible aggregatesimilarity (Fann) search; which generalizes the Ann problem. Next; we present algorithms foranswering Fann queries exactly and approximately. Our approximation algorithms areespecially appealing; which are simple; highly efficient; and work well in both low and …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,38
Continuous sampling from distributed streams,Graham Cormode; S Muthukrishnan; Ke Yi; Qin Zhang,Abstract A fundamental problem in data management is to draw and maintain a sample of alarge data set; for approximate query answering; selectivity estimation; and query planning.With large; streaming data sets; this problem becomes particularly difficult when the data isshared across multiple distributed sites. The main challenge is to ensure that a sample isdrawn uniformly across the union of the data while minimizing the communication needed torun the protocol on the evolving data. At the same time; it is also necessary to make theprotocol lightweight; by keeping the space and time costs low for each participant. In thisarticle; we present communication-efficient protocols for continuously maintaining a sample(both with and without replacement) from k distributed streams. These apply to the casewhen we want a sample from the full streams; and to the sliding window cases of only the …,Journal of the ACM (JACM),2012,36
Small synopses for group-by query verification on outsourced data streams,Ke Yi; Feifei Li; Graham Cormode; Marios Hadjieleftheriou; George Kollios; Divesh Srivastava,Abstract Due to the overwhelming flow of information in many data stream applications; dataoutsourcing is a natural and effective paradigm for individual businesses to address theissue of scale. In the standard data outsourcing model; the data owner outsources streamingdata to one or more third-party servers; which answer queries posed by a potentially largenumber of clients on the data owner's behalf. Data outsourcing intrinsically raises issues oftrust; making outsourced query assurance on data streams a problem with importantpractical implications. Existing solutions proposed in this model all build upon cryptographicprimitives such as signatures and collision-resistant hash functions; which only work forcertain types of queries; for example; simple selection/aggregation queries. In this article; weconsider another common type of queries; namely;“GROUP BY; SUM” queries; which …,ACM Transactions on Database Systems (TODS),2009,36
Annotations in data streams,Amit Chakrabarti; Graham Cormode; Andrew Mcgregor,Abstract The central goal of data stream algorithms is to process massive streams of datausing sublinear storage space. Motivated by work in the database community onoutsourcing database and data stream processing; we ask whether the space usage of suchalgorithms be further reduced by enlisting a more powerful “helper” who can annotate thestream as it is read. We do not wish to blindly trust the helper; so we require that thealgorithm be convinced of having computed a correct answer. We show upper bounds thatachieve a non-trivial tradeoff between the amount of annotation used and the spacerequired to verify it. We also prove lower bounds on such tradeoffs; often nearly matching theupper bounds; via notions related to Merlin-Arthur communication complexity. Our resultscover the classic data stream problems of selection; frequency moments; and …,International Colloquium on Automata; Languages; and Programming,2009,35
Exponentially decayed aggregates on data streams,Graham Cormode; Flip Korn; Srikanta Tirthapura,In a massive stream of sequential events such as stock feeds; sensor readings; or IP trafficmeasurements; tuples pertaining to recent events are typically more important than olderones. It is important to compute various aggregates over such streams after applying adecay function which assigns weights to tuples based on their age. We focus on thecomputation of exponentially decayed aggregates in the form of quantiles and heavy hitters.Our techniques are based on extending existing data stream summaries; such as the q-digest [1] and the" space-saving" algorithm [2]. Our experiments confirm that our methodscan be applied in practice; and have similar space and time costs to the non-decayedaggregate computation.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,35
Information cost tradeoffs for augmented index and streaming language recognition,Amit Chakrabarti; Graham Cormode; Ranganath Kondapally; Andrew McGregor,This paper makes three main contributions to the theory of communication complexity andstream computation. First; we present new bounds on the information complexity ofaugmented-index. In contrast to analogous results for index by Jain; Radhakrishnan; andSen [J. ACM; 56 (2009); article 33]; we have to overcome the significant technical challengethat protocols for augmented-index may violate the “rectangle property” due to the inherentinput sharing. Second; we use these bounds to resolve an open problem of Magniez;Mathieu; and Nayak [Proceedings of the 42 nd Annual ACM Symposium on Theory ofComputing; 2010; pp. 261--270] that asked about the multipass complexity of recognizingDyck languages. This results in a natural separation between the standard multipass modeland the multipass model that permits reverse passes. Third; we present the first passive …,SIAM Journal on Computing,2013,34
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Benoit Baudry; Sudipto Ghosh; Franck Fleurey; Robert France; Yves Le Traon; Jean-Marie Mottu,Model Driven Engineering (MDE) techniques 10 support extensive use of models in order tomanage the increasing complexity of software systems. Appropriate abstractions of softwaresystem elements can ease reasoning and understanding and thus limit the risk of errors inlarge systems. Automatic model transformations play a critical role in MDE since theyautomate complex; tedious; error-prone; and recurrent software development tasks. Airbususes automatic code synthesis from SCADE models to generate the code for embeddedcontrollers in the Airbus A380. Commercial tools for model transformations exist.Objecteering and Together from Borland are tools that can automatically add designpatterns in a UML class model. Esterel Technologies have a tool for automatic codesynthesis for safety critical systems.Other examples of transformations are refinement of a …,Communications of the ACM,2010,33
Effective computation of biased quantiles over data streams,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Skew is prevalent in many data sources such as IP traffic streams. To continually summarizethe distribution of such data; a high-biased set of quantiles (eg; 50th; 90th and 99thpercentiles) with finer error guarantees at higher ranks (eg; errors of 5; 1 and 0.1 percent;respectively) is more useful than uniformly distributed quantiles (eg; 25th; 50th and 75thpercentiles) with uniform error guarantees. In this paper; we address the following twoproblems. First; can we compute quantiles with finer error guarantees for the higher ranks ofthe data distribution effectively using less space and computation time than computing allquantiles uniformly at the finest error? Second; if specific quantiles and their error boundsare requested a priori; can the necessary space usage and computation time be reduced?We answer both questions in the affirmative by formalizing them as the" high-biased" and …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,33
Time-decaying sketches for sensor data aggregation,Graham Cormode; Srikanta Tirthapura; Bojian Xu,Abstract We present a new sketch for summarizing network data. The sketch has thefollowing properties which make it useful in communication-efficient aggregation indistributed streaming scenarios; such as sensor networks: the sketch is duplicate-insensitive; ie re-insertions of the same data will not affect the sketch; and hence theestimates of aggregates. Unlike previous duplicate-insensitive sketches for sensor dataaggregation [26; 12]; it is also time-decaying; so that the weight of a data item in the sketchcan decrease with time according to a user-specified decay function. The sketch can giveprovably approximate guarantees for various aggregates of data; including the sum; median;quantiles; and frequent elements. The size of the sketch and the time taken to update it areboth polylogarithmic in the size of the relevant data. Further; multiple sketches computed …,Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing,2007,32
Radial histograms for spatial streams,Graham Cormode; S Muthukrishnan,ABSTRACT Many data streams relate to geographic or spatial information such as thetracking of moving objects; or location-specific measurements and queries. While severaltechniques have been developed recently for processing numerical; text or XML streams;new techniques are needed to process spatial queries on streaming geometric data. Wepropose a novel data structure; the Radial Histogram; to process streams including spatialdata points. It allows a number of geometric aggregates involving the spread and extent ofthe points in the data streams—diameter; furthest neighbors; convex hulls—to be accuratelyestimated to arbitrary precision. By using multiple Radial Histograms for a set of givenfacilities; we can process data streams consisting of massive numbers of client points. Wecan then accurately estimate number of spatial aggregates involving relative placement of …,DIM ACS Technical Report,2003,31
DPT: differentially private trajectory synthesis using hierarchical reference systems,Xi He; Graham Cormode; Ashwin Machanavajjhala; Cecilia M Procopiuc; Divesh Srivastava,Abstract GPS-enabled devices are now ubiquitous; from airplanes and cars to smartphonesand wearable technology. This has resulted in a wealth of data about the movements ofindividuals and populations; which can be analyzed for useful information to aid in city andtraffic planning; disaster preparedness and so on. However; the places that people go candisclose extremely sensitive information about them; and thus their use needs to be filteredthrough privacy preserving mechanisms. This turns out to be a highly challenging task: rawtrajectories are highly detailed; and typically no pair is alike. Previous attempts fail either toprovide adequate privacy protection; or to remain sufficiently faithful to the original behavior.This paper presents DPT; a system to synthesize mobility data based on raw GPStrajectories of individuals while ensuring strong privacy protection in the form of ε …,Proceedings of the VLDB Endowment,2015,30
Conservative or liberal? personalized differential privacy,Zach Jorgensen; Ting Yu; Graham Cormode,Differential privacy is widely accepted as a powerful framework for providing strong; formalprivacy guarantees for aggregate data analysis. A limitation of the model is that the samelevel of privacy protection is afforded for all individuals. However; it is common that the datasubjects have quite different expectations regarding the acceptable level of privacy for theirdata. Consequently; differential privacy may lead to insufficient privacy protection for someusers; while over-protecting others. We argue that by accepting that not all users require thesame level of privacy; a higher level of utility can often be attained by not providing excessprivacy to those who do not want it. We propose a new privacy definition called personalizeddifferential privacy (PDP); a generalization of differential privacy in which users specify apersonal privacy requirement for their data. We then introduce several novel mechanisms …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,30
Accurate and efficient private release of datacubes and contingency tables,Grigory Yaroslavtsev; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava,A central problem in releasing aggregate information about sensitive data is to do soaccurately while providing a privacy guarantee on the output. Recent work focuses on theclass of linear queries; which include basic counting queries; data cubes; and contingencytables. The goal is to maximize the utility of their output; while giving a rigorous privacyguarantee. Most results follow a common template: pick a “strategy” set of linear queries toapply to the data; then use the noisy answers to these queries to reconstruct the queries ofinterest. This entails either picking a strategy set that is hoped to be good for the queries; orperforming a costly search over the space of all possible strategies. In this paper; wepropose a new approach that balances accuracy and efficiency: we show how to improvethe accuracy of a given query set by answering some strategy queries more accurately …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,30
A near-optimal algorithm for estimating the entropy of a stream,Amit Chakrabarti; Graham Cormode; Andrew McGregor,Abstract We describe a simple algorithm for approximating the empirical entropy of a streamof m values up to a multiplicative factor of (1+ &epsi;) using a single pass; O (&epsi;− 2 log(δ− 1) log m) words of space; and O (log &epsi;− 1+ log log δ− 1+ log log m) processing timeper item in the stream. Our algorithm is based upon a novel extension of a methodintroduced by Alon et al.[1999]. This improves over previous work on this problem. We showa space lower bound of Ω (&epsi;− 2/log 2 (&epsi;− 1)); demonstrating that our algorithm isnear-optimal in terms of its dependency on &epsi;. We show that generalizing tomultiplicative-approximation of the kth-order entropy requires close to linear space for k≥ 1.In contrast we show that additive-approximation is possible in a single pass using only poly-logarithmic space. Lastly; we show how to compute a multiplicative approximation to the …,ACM Transactions on Algorithms (TALG),2010,29
METHOD AND APPARATUS FOR PROVIDING ANONYMIZATION OF DATA,*,*,*,2009,29
Substring compression problems,Graham Cormode; S Muthukrishnan,Abstract We initiate a new class of string matching problems called Substring CompressionProblems. Given a string S that may be preprocessed; the problem is to quickly find thecompressed representation or the compressed size of any query substring of S (SubstringCompression Query or SCQ) or to find the length l substring of S whose compression is theleast (Least Compressible Substring or LCS problem). Starting from the seminal paper ofLempel and Ziv over 25 years ago; many different methods have emerged for compressingentire strings. Determining substring compressibility is a natural variant that iscombinatorially and algorithmically challenging; yet surprisingly has not been studiedbefore. In addition; compressibility of strings is emerging as a tool to compare biologicalsequences and analyze their information content. However; typically; the compressibility …,Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,2005,29
Kernelization via sampling with applications to finding matchings and related problems in dynamic graph streams,Rajesh Chitnis; Graham Cormode; Hossein Esfandiari; MohammadTaghi Hajiaghayi; Andrew McGregor; Morteza Monemizadeh; Sofya Vorotnikova,Abstract In this paper we present a simple but powerful subgraph sampling primitive that isapplicable in a variety of computational models including dynamic graph streams (where theinput graph is defined by a sequence of edge/hyperedge insertions and deletions) anddistributed systems such as MapReduce. In the case of dynamic graph streams; we use thisprimitive to prove the following results:• Matching: Our main result for matchings is that thereexists an Õ (k 2) space algorithm that returns the edges of a maximum matching on theassumption the cardinality is at most k. The best previous algorithm used Õ (kn) spacewhere n is the number of vertices in the graph and we prove our result is optimal up tologarithmic factors. Our algorithm has Õ (1) update time. We also show that there exists an Õ(n 2/α 3) space algorithm that returns an α-approximation for matchings of arbitrary size …,Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms,2016,27
Link-based classification of graph nodes,*,A method of labeling unlabeled nodes in a graph that represents objects that have anexplicit structure between them. A computing device can use a labeling engine to labelednodes in a graph that are labeled and can identify an unlabeled node in the graph that isstructurally associated with the labeled nodes. The labeling engine can label the unlabelednode with the label of the labeled node based on the structural association between theunlabeled node and the labeled node.,*,2009,27
Permutation editing and matching via embeddings,Graham Cormode; S Muthukrishnan; Süleyman Cenk Sahinalp,Abstract If the genetic maps of two species are modelled as permutations of (homologous)genes; the number of chromosomal rearrangements in the form of deletions; block moves;inversions etc. to transform one such permutation to another can be used as a measure oftheir evolutionary distance. Motivated by such scenarios; we study problems of computingdistances between permutations as well as matching permutations in sequences; andfinding most similar permutation from a collection (“nearest neighbor”). We adopt a generalapproach: embed permutation distances of relevance into well-known vector spaces in anapproximately distance-preserving manner; and solve the resulting problems on the well-known spaces. Our results are as follows:—We present the first known approximatelydistance preserving embeddings of these permutation distances into well-known spaces …,International Colloquium on Automata; Languages; and Programming,2001,27
Private release of graph statistics using ladder functions,Jun Zhang; Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Xiaokui Xiao,Abstract Protecting the privacy of individuals in graph structured data while making accurateversions of the data available is one of the most challenging problems in data privacy. Mostefforts to date to perform this data release end up mired in complexity; overwhelm the signalwith noise; and are not effective for use in practice. In this paper; we introduce a new methodwhich guarantees differential privacy. It specifies a probability distribution over possibleoutputs that is carefully defined to maximize the utility for the given input; while still providingthe required privacy level. The distribution is designed to form a'ladder'; so that each outputachieves the highest'rung'(maximum probability) compared to less preferable outputs. Weshow how our ladder framework can be applied to problems of counting the number ofoccurrences of subgraphs; a vital objective in graph analysis; and give algorithms whose …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,26
Lightweight authentication of linear algebraic queries on data streams,Stavros Papadopoulos; Graham Cormode; Antonios Deligiannakis; Minos Garofalakis,Abstract We consider a stream outsourcing setting; where a data owner delegates themanagement of a set of disjoint data streams to an untrusted server. The ownerauthenticates his streams via signatures. The server processes continuous queries on theunion of the streams for clients trusted by the owner. Along with the results; the server sendsproofs of result correctness derived from the owner's signatures; which are easily verifiableby the clients. We design novel constructions for a collection of fundamental problems overstreams represented as linear algebraic queries. In particular; our basic schemesauthenticate dynamic vector sums and dot products; as well as dynamic matrix products.These techniques can be adapted for authenticating a wide range of important operations instreaming environments; including group by queries; joins; in-network aggregation …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,26
Empirical privacy and empirical utility of anonymized data,Graham Cormode; Cecilia M Procopiuc; Entong Shen; Divesh Srivastava; Ting Yu,Procedures to anonymize data sets are vital for companies; government agencies and otherbodies to meet their obligations to share data without compromising the privacy of theindividuals contributing to it. Despite much work on this topic; the area has not yet reachedstability. Early models (k-anonymity and ℓ-diversity) are now thought to offer insufficientprivacy. Noise-based methods like differential privacy are seen as providing strongerprivacy; but less utility. However; across all methods sensitive information of someindividuals can often be inferred with relatively high accuracy. In this paper; we reverse theidea of aprivacy attack;'by incorporating it into a measure of privacy. Hence; we advocate thenotion of empirical privacy; based on the posterior beliefs of an adversary; and their ability todraw inferences about sensitive values in the data. This is not a new model; but rather a …,Data Engineering Workshops (ICDEW); 2013 IEEE 29th International Conference on,2013,26
A manifesto for modeling and measurement in social media,Graham Cormode; Balachander Krishnamurthy; Walter Willinger,Abstract Online Social Networks (OSNs) have been the subject of a great deal of study inrecent years. The majority of this study has used simple models; such as node-and-edgegraphs; to describe the data. In this paper; we argue that such models; which necessarilylimit the structures that can be described and omit temporal information; are insufficient todescribe and study OSNs. Instead; we propose that a richer class of Entity InteractionNetwork models should be adopted. We outline a checklist of features that can help buildsuch a model; and apply it to three popular networks (Twitter; Facebook and YouTube) tohighlight important features. We also discuss important considerations for the collection;validation and sharing of OSN data.,First Monday,2010,26
Correlation clustering in data streams,KookJin Ahn; Graham Cormode; Sudipto Guha; Andrew McGregor; Anthony Wirth,Abstract In this paper; we address the problem of\emphcorrelation clustering in the dynamicdata stream model. The stream consists of updates to the edge weights of a graph on nnodes and the goal is to find a node-partition such that the end-points of negative-weightedges are typically in different clusters whereas the end-points of positive-weight edges aretypically in the same cluster. We present polynomial-time; O (n⋅\textpolylog n)-spaceapproximation algorithms for natural problems that arise. We first develop data structuresbased on linear sketches that allow the “quality” of a given node-partition to be measured.We then combine these data structures with convex programming and sampling techniquesto solve the relevant approximation problem. However the standard LP and SDPformulations are not obviously solvable in O (n⋅\textpolylog n)-space. Our work presents …,International Conference on Machine Learning,2015,25
Sketch Techniques for Approximate Query Processing,Graham Cormode; Graham Cormode; S Bhagat; Graham Cormode; S Muthukrishnan; Graham Cormode; Justin Thaler; Ke Yi; Graham Cormode; Graham Cormode; Graham Cormode; Minos Garofalakis; G Cormode; F Korn; S Tirthapura,*,Paper accompanying invited talk at Algorithms and Models for Distributed Event Processing (AlMoDEP)},*,25
Parameterized streaming: Maximal matching and vertex cover,Rajesh Chitnis; Graham Cormode; MohammadTaghi Hajiaghayi; Morteza Monemizadeh,Abstract As graphs continue to grow in size; we seek ways to effectively process such data atscale. The model of streaming graph processing; in which a compact summary is maintainedas each edge insertion/deletion is observed; is an attractive one. However; few results areknown for optimization problems over such dynamic graph streams. In this paper; weintroduce a new approach to handling graph streams; by instead seeking solutions for theparameterized versions of these problems. Here; we are given a parameter k and theobjective is to decide whether there is a solution bounded by k. By combining kernelizationtechniques with randomized sketch structures; we obtain the first streaming algorithms forthe parameterized versions of Maximal Matching and Vertex Cover. We consider variousmodels for a graph stream on n nodes: the insertion-only model where the edges can …,Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms,2015,24
Tracking distributed aggregates over time-based sliding windows,Graham Cormode; Ke Yi,Abstract The area of distributed monitoring requires tracking the value of a function ofdistributed data as new observations are made. An important case is when attention isrestricted to only a recent time period; such as the last hour of readings—the sliding windowcase. In this paper; we introduce a novel paradigm for handling such monitoring problems;which we dub the “forward/backward” approach. This view allows us to provide optimal ornear-optimal solutions for several fundamental problems; such as counting; trackingfrequent items; and maintaining order statistics. The resulting protocols improve on previouswork or give the first solutions for some problems; and operate efficiently in terms of spaceand time needed. Specifically; we obtain optimal O(kϵ\log(ϵn/k)) communication perwindow of n updates for tracking counts and heavy hitters with accuracy ε across k sites; …,Proceedings of the 30th annual ACM SIGACT-SIGOPS symposium on Principles of distributed computing,2011,23
The hardness of the Lemmings game; or Oh no; more NP-completeness proofs,Graham Cormode,Abstract In the computer game 'Lemmings'; the player must guide a tribe of green-hairedLemming creatures to safety; and hence save them from an untimely demise. We formulatethe decision problem which is; given a level of the game; to decide whether it is possible tocomplete the level (and hence to find a solution to that level). Under certain limitations; thiscan be decided in polynomial time; but in general the problem is shown to be NP-Hard. Thiscan hold even if there is only a single lemming to save; thus showing that it is hard toapproximate the number of saved lemmings to any factor.,Proceedings of Third International Conference on Fun with Algorithms,2004,23
Sketch algorithms for estimating point queries in NLP,Amit Goyal; Hal Daumé III; Graham Cormode,Abstract Many NLP tasks rely on accurate statistics from large corpora. Tracking completestatistics is memory intensive; so recent work has proposed using compact approximate"sketches" of frequency distributions. We describe 10 sketch methods; including existing andnovel variants. We compare and study the errors (over-estimation and underestimation)made by the sketches. We evaluate several sketches on three important NLP problems. Ourexperiments show that one sketch performs best for all the three tasks.,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,22
Continuous distributed monitoring: a short survey,Graham Cormode,Abstract In the model of continuous distributed monitoring; a number of observers each seea stream of observations. Their goal is to work together to compute a function of the union oftheir observations. This can be as simple as counting the total number of observations; ormore complex non-linear functions such as tracking the entropy of the induced distribution.Assuming that it is too costly to simply centralize all the observations; it becomes quitechallenging to design solutions which provide a good approximation to the current answer;while bounding the communication cost of the observers; and their other resources such astheir space usage. This survey introduces this model; and describe a selection results in thissetting; from the simple counting problem to a variety of other functions that have beenstudied.,Proceedings of the First International Workshop on Algorithms and Models for Distributed Event Processing,2011,22
Sampling for big data: a tutorial,Graham Cormode; Nick Duffield,Abstract One response to the proliferation of large datasets has been to develop ingeniousways to throw resources at the problem; using massive fault tolerant storage architectures;parallel and graphical computation models such as MapReduce; Pregel and Giraph.However; not all environments can support this scale of resources; and not all queries needan exact response. This motivates the use of sampling to generate summary datasets thatsupport rapid queries; and prolong the useful life of the data in storage. To be effective;sampling must mediate the tensions between resource constraints; data characteristics; andthe required query accuracy. The state-of-the-art in sampling goes far beyond simpleuniform selection of elements; to maximize the usefulness of the resulting sample. Thistutorial reviews progress in sample design for large datasets; including streaming and …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,21
Method for distributed tracking of approximate join size and related summaries,*,A method of distributed approximate query tracking relies on tracking general-purposerandomized sketch summaries of local streams at remote sites along with concise predictionmodels of local site behavior in order to produce highly communication-efficient andspace/time-efficient solutions. A powerful approximate query tracking framework readilyincorporates several complex analysis queries; including distributed join and multi-joinaggregates and approximate wavelet representations; thus giving the first known low-overhead tracking solution for such queries in the distributed-streams model.,*,2010,21
Probabilistic histograms for probabilistic data,Graham Cormode; Antonios Deligiannakis; Minos Garofalakis; Andrew McGregor,Abstract There is a growing realization that modern database management systems(DBMSs) must be able to manage data that contains uncertainties that are represented inthe form of probabilistic relations. Consequently; the design of each core DBMS componentmust be revisited in the presence of uncertain and probabilistic information. In this paper; westudy how to build histogram synopses for probabilistic relations; for the purposes ofenabling both DBMS-internal decisions (such as indexing and query planning); and(possibly; user-facing) approximate query processing tools. In contrast to initial work in thisarea; our probabilistic histograms retain the key possible-worlds semantics of probabilisticdata; allowing for more accurate; yet concise; representation of the uncertaintycharacteristics of data and query results. We present a variety of techniques for building …,Proceedings of the VLDB Endowment,2009,21
No Blog is an Island-Analyzing Connections Across Information Networks.,Smriti Bhagat; Graham Cormode; S Muthukrishnan; Irina Rozenbaum; Hongyi Xue,Abstract There are multiple information and social networks among individuals; fromtelephone to email; web; blog; instant message (IM) and chat networks. Prior work hasstudied each of these individual networks quite extensively; including telephone networks[7]; postal network [20]; web communities [17]; and so on. Still; what is of great interest is howthese networks collide and interact with each other. Each individual has presence in multiplenetworks and it will be of interest to understand how the references and presence in oneaffects that in the others. Previously such studies would have been difficult to do since eachsource of data was often “owned” by a specific entity. Blogs now provide a unique; publicsource of data that naturally provides visibility into multiple networks. For example; blogentries cite web pages; blogs have friends and communities; as well as blog profiles that …,ICWSM,2007,21
Quantiles over data streams: an experimental study,Lu Wang; Ge Luo; Ke Yi; Graham Cormode,Abstract A fundamental problem in data management and analysis is to generatedescriptions of the distribution of data. It is most common to give such descriptions in termsof the cumulative distribution; which is characterized by the quantiles of the data. The designand engineering of efficient methods to find these quantiles has attracted much study;especially in the case where the data is described incrementally; and we must compute thequantiles in an online; streaming fashion. Yet while such algorithms have proved to betremendously useful in practice; there has been limited formal comparison of the competingmethods; and no comprehensive study of their performance. In this paper; we remedy thisdeficit by providing a taxonomy of different methods; and describe efficient implementations.In doing so; we propose and analyze variations that have not been explicitly studied …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,20
Methods and apparatus for representing probabilistic data using a probabilistic histogram,*,Methods and apparatus for representing probabilistic data using a probabilistic histogramare disclosed. An example method comprises partitioning a plurality of ordered data itemsinto a plurality of buckets; each of the data items capable of having a data value from aplurality of possible data values with a probability characterized by a respective individualprobability distribution function (PDF); each bucket associated with a respective subset ofthe ordered data items bounded by a respective beginning data item and a respectiveending data item; and determining a first representative PDF for a first bucket associatedwith a first subset of the ordered data items by partitioning the plurality of possible datavalues into a first plurality of representative data ranges and respective representativeprobabilities based on an error between the first representative PDF and a first plurality of …,*,2012,20
Electronic books in digital libraries,Gultekin Ozsoyoglu; Nevzat Hurkan Balkir; Graham Cormode; Z Meral Ozsoyoglu,An electronic book is an application with a multimedia database of instructional resources;which include hyperlinked text; instructor's audio/video clips; slides; animation; still images;etc. As well as content-based information about these data; and metadata such asannotations; tags; and cross-referencing information. Electronic books in the Internet or onCDs today are not easy to learn from. We propose the use of a multimedia database ofinstructional resources in constructing and delivering multimedia lessons about topics in anelectronic book. We introduce an electronic book data model containing (a) topic objectsand (b) instructional resources; called instruction module objects; which are multimediapresentations possibly capturing real-life lectures of instructors. We use the notion of topicprerequisites for topics at different detail levels; to allow electronic book users to request …,Advances in Digital Libraries; 2000. Proceedings. IEEE,2000,19
Annotations in data streams,Amit Chakrabarti; Graham Cormode; Andrew McGregor; Justin Thaler,Abstract The central goal of data stream algorithms is to process massive streams of datausing sublinear storage space. Motivated by work in the database community onoutsourcing database and data stream processing; we ask whether the space usage of suchalgorithms can be further reduced by enlisting a more powerful “helper” that can annotatethe stream as it is read. We do not wish to blindly trust the helper; so we require that thealgorithm be convinced of having computed a correct answer. We show upper bounds thatachieve a nontrivial tradeoff between the amount of annotation used and the space requiredto verify it. We also prove lower bounds on such tradeoffs; often nearly matching the upperbounds; via notions related to Merlin-Arthur communication complexity. Our results cover theclassic data stream problems of selection; frequency moments; and fundamental graph …,ACM Transactions on Algorithms (TALG),2014,18
Method and apparatus for providing anonymization of data,*,A method and apparatus for providing an anonymization of data are disclosed. For example;the method receives a request for anonymizing; wherein the request comprises a bipartitegraph for a plurality of associations or a table that encodes the plurality of associations forthe bipartite graph. The method places each node in the bipartite graph in a safe group andprovides an anonymized graph that encodes the plurality of associations of the bipartitegraph; if a safe group for all nodes of the bipartite graph is found.,*,2013,18
Interactive proof to validate outsourced data stream processing,*,A method for validating outsourced processing of a data stream arriving at a streaming datawarehouse of a data service provider includes a proof protocol. A verifier acting on behalf ofa data owner of the data stream may interact with a prover acting on behalf of the dataservice provider. The verifier may calculate a first root hash value of a binary tree duringsingle-pass processing of the original data stream with limited computational effort. Asecond root hash value may be calculated using the proof protocol between the verifier andthe prover. The prover is requested to provide certain queried values before receivingrandom numbers used to generate subsequent responses dependent on the providedvalues. The proof protocol may be used to validate the data processing performed by thedata service provider.,*,2013,17
Streaming graph computations with a helpful advisor,Graham Cormode; Michael Mitzenmacher; Justin Thaler,Abstract Motivated by the trend to outsource work to commercial cloud computing services;we consider a variation of the streaming paradigm where a streaming algorithm can beassisted by a powerful helper that can provide annotations to the data stream. We extendprevious work on such annotation models by considering a number of graph streamingproblems. Without annotations; streaming algorithms for graph problems generally requiresignificant memory; we show that for many standard problems; including all graph problemsthat can be expressed with totally unimodular integer programming formulations; onlyconstant space (measured in words) is needed for single-pass algorithms given linear-sizedannotations. We also obtain protocols achieving essentially optimal tradeoffs betweenannotation length and memory usage for several important problems; including integer …,Algorithmica,2013,17
Time-decaying sketches for robust aggregation of sensor data,Graham Cormode; Srikanta Tirthapura; Bojian Xu,We present a new sketch for summarizing network data. The sketch has the followingproperties which make it useful in communication-efficient aggregation in distributedstreaming scenarios; such as sensor networks: the sketch is duplicate insensitive; ie;reinsertions of the same data will not affect the sketch and hence the estimates ofaggregates. Unlike previous duplicate-insensitive sketches for sensor data aggregation [S.Nath et al.; Synposis diffusion for robust aggregation in sensor networks; in Proceedings ofthe 2nd International Conference on Embedded Network Sensor Systems;(2004); pp. 250–262];[J. Considine et al.; Approximate aggregation techniques for sensor databases; inProceedings of the 20th International Conference on Data Engineering (ICDE); 2004; pp.449–460]; it is also time decaying; so that the weight of a data item in the sketch can …,SIAM Journal on Computing,2009,17
On automated lesson construction from electronic textbooks,Gultekin Ozsoyoglu; Nevzat Hurkan Balkir; Z Meral Ozsoyoglu; Graham Cormode,An electronic book may be viewed as an application with a multimedia database. We definean electronic textbook as an electronic book that is used in conjunction with instructionalresources such as lectures. We propose an electronic textbook data model with topics; topicsources; metalinks (relationships among topics); and instructional modules; which aremultimedia presentations possibly capturing real-life lectures of instructors. Using the datamodel; the system provides users a topic-guided multimedia lesson construction. Weconcentrate; in detail; on the use of one metalink type in lesson construction; namely;prerequisite dependencies; and provide a sound and complete axiomatization ofprerequisite dependencies. We present a simple automated way of constructing lessons forusers where the user lists a set of topic names (s) he is interested in; and the system …,IEEE Transactions on knowledge and data engineering,2004,17
Verifiable stream computation and Arthur–Merlin communication,Amit Chakrabarti; Graham Cormode; Andrew McGregor; Justin Thaler; Suresh Venkatasubramanian,Abstract In the setting of streaming interactive proofs (SIPs); a client (verifier) needs tocompute a given function on a massive stream of data; arriving online; but is unable to storeeven a small fraction of the data. It outsources the processing to a third party service(prover); but is unwilling to blindly trust answers returned by this service. Thus; the servicecannot simply supply the desired answer; it must convince the verifier of its correctness via ashort interaction after the stream has been seen. In this work we study" barely interactive"SIPs. Specifically; we show that two or three rounds of interaction suffice to solve severalquery problems-including Index; Median; Nearest Neighbor Search; Pattern Matching; andRange Counting-with polylogarithmic space and communication costs. Such efficiency withO (1) rounds of interaction was thought to be impossible based on previous work. On the …,LIPIcs-Leibniz International Proceedings in Informatics,2015,16
Methods and apparatus for ranking uncertain data in a probabilistic database,*,Methods and apparatus for ranking uncertain data in a probabilistic database are disclosed.An example method disclosed herein comprises using a set of data tuples representing aplurality of possible data set instantiations associated with a respective plurality ofinstantiation probabilities to store non-deterministic data in a database; each data tuplecorresponding to a set of possible data tuple instantiations; each data set instantiationrealizable by selecting a respective data tuple instantiation for at least some of the datatuples; the method further comprising determining an expected rank for each data tupleincluded in at least a subset of the set of data tuples; the expected rank for a particular datatuple representing a combination of weighted component ranks of the particular data tuple;each component rank representing a ranking of the data tuple in a corresponding data set …,*,2014,16
A unifying framework for ℓ 0-sampling algorithms,Graham Cormode; Donatella Firmani,Abstract The problem of building an ℓ 0-sampler is to sample near-uniformly from thesupport set of a dynamic multiset. This problem has a variety of applications within dataanalysis; computational geometry and graph algorithms. In this paper; we abstract a set ofsteps for building an ℓ 0-sampler; based on sampling; recovery and selection. We analyzethe implementation of an ℓ 0-sampler within this framework; and show how priorconstructions of ℓ 0-samplers can all be expressed in terms of these steps. Our experimentalcontribution is to provide a first detailed study of the accuracy and computational cost of ℓ 0-samplers.,Distributed and Parallel Databases,2014,16
Communication-efficient distributed monitoring of thresholded counts,*,A system; method; and computer program product for distributed monitoring of localthresholds at each of a number of monitoring nodes and initiating communication only afterthe locally observed data exceeds the local threshold. Both static thresholds and adaptivethresholds are considered. In the static case; a combination of two alternate strategies forconsidering thresholds minimizes communication overhead. In the adaptive case; localthresholds are adjusted based on the observed distributions of updated information in thedistributed monitoring system. Both approaches yield significant savings over the naïveapproach of performing processing at a centralized location.,*,2010,16
Verification of outsourced data streams,*,Embodiments disclosed herein are directed to verifying query results of an untrusted server.A data owner outsources a data stream to the untrusted server; which is configured torespond to a query from a client with the query result; which is returned to the client. Thedata owner can maintain a vector associated with query results returned by the server andcan generate a verification synopsis using the vector and a seed. The verification synopsisincludes a polynomial; where coefficients of the polynomial are determined based on theseed. The data owner outputs the verification synopsis and the seed to a client forverification of the query results.,*,2012,15
Fundamentals of analyzing and mining data streams,Graham Cormode,∎ Many things are hard to compute exactly over a stream–Is the count of all items the samein two different streams?–Requires linear space to compute exactly∎ Approximation: find ananswer correct within some factor–Find an answer that is within 10% of correct result–Moregenerally; a (1±ε) factor approximation∎ Randomization: allow a small probability of failure–Answer is correct; except with probability 1 in 10;000–More generally; success probability (1-δ)∎ Approximation and Randomization:(ε; δ)-approximations,Tutorial at Workshop on Data Stream Analysis; Caserta; Italy,2007,15
Don't let the negatives bring you down: sampling from streams of signed updates,Edith Cohen; Graham Cormode; Nick Duffield,Abstract Random sampling has been proven time and time again to be a powerful tool forworking with large data. Queries over the full dataset are replaced by approximate queriesover the smaller (and hence easier to store and manipulate) sample. The sample constitutesa flexible summary that supports a wide class of queries. But in many applications; datasetsare modified with time; and it is desirable to update samples without requiring access to thefull underlying datasets. In this paper; we introduce and analyze novel techniques forsampling over dynamic data; modeled as a stream of modifications to weights associatedwith each key. While sampling schemes designed for stream applications can often readilyaccommodate positive updates to the dataset; much less is known for the case of negativeupdates; where weights are reduced or items deleted altogether. We primarily consider …,ACM SIGMETRICS Performance Evaluation Review,2012,14
Methods and apparatus to determine statistical dominance point descriptors for multidimensional data,*,Methods and apparatus to determine statistical dominance point descriptors formultidimensional data are disclosed. An example method disclosed herein comprisesdetermining a first joint dominance value for a first data point in a multidimensional data set;data points in the multidimensional data set comprising multidimensional values; eachdimension corresponding to a different measurement of a physical event; the first jointdominance value corresponding to a number of data points in the multidimensional data setdominated by the first data point in every dimension; determining a first skewness value forthe first data point; the first skewness value corresponding to a size of a first dimension of thefirst data point relative to a combined size of all dimensions of the first data point; andcombining the first joint dominance and first skewness values to determine a first …,*,2012,14
Streaming graph computations with a helpful advisor,Graham Cormode; Michael Mitzenmacher; Justin Thaler,Abstract Motivated by the trend to outsource work to commercial cloud computing services;we consider a variation of the streaming paradigm where a streaming algorithm can beassisted by a powerful helper that can provide annotations to the data stream. We extendprevious work on such annotation models by considering a number of graph streamingproblems. Without annotations; streaming algorithms for graph problems generally requiresignificant memory; we show that for many standard problems; including all graph problemsthat can be expressed with totally unimodular integer programming formulations; onlyconstant memory is needed for single-pass algorithms given linear-sized annotations. Wealso obtain a protocol achieving optimal tradeoffs between annotation length and memoryusage for matrix-vector multiplication; this result contributes to a trend of recent research …,European Symposium on Algorithms,2010,14
Algorithms for next generation networks,Graham Cormode; Marina Thottan,Data networking now plays a major role in everyday life and new applications continue toappear at a blinding pace. Yet we still do not have a sound foundation for designing;evaluating and managing these networks. This book covers topics at the intersection ofalgorithms and networking. It builds a complete picture of the current state of research onNext Generation Networks and the challenges for the years ahead. Particular focus is givento evolving research initiatives and the architecture they propose and implications fornetworking. Topics: Network design and provisioning; hardware issues; layer-3 algorithmsand MPLS; BGP and Inter AS routing; packet processing for routing; security and networkmanagement; load balancing; oblivious routing and stochastic algorithms; network codingfor multicast; overlay routing for P2P networking and content delivery. This timely volume …,*,2010,14
Time‐decayed correlated aggregates over data streams,Graham Cormode; Srikanta Tirthapura; Bojian Xu,Abstract Data stream analysis frequently relies on identifying correlations and posingconditional queries on the data after it has been seen. Correlated aggregates form animportant example of such queries; which ask for an aggregation over one dimension ofstream elements which satisfy a predicate on another dimension. Since recent events aretypically more important than older ones; time decay should also be applied to downweightless significant values. We present space-efficient algorithms as well as space lower boundsfor the time-decayed correlated sum; a problem at the heart of many related aggregations.By considering different fundamental classes of decay functions; we separate cases whereefficient approximations with relative error or additive error guarantees are possible; fromother cases where linear space is necessary to approximate. In particular; we show that …,Statistical Analysis and Data Mining: The ASA Data Science Journal,2009,14
System and Method for Identifying Hierarchical Heavy Hitters in Multi-Dimensional Data,*,A method including receiving a plurality of elements of a data stream; storing a multi-dimensional data structure in a memory; said multi-dimensional data structure storing theplurality of elements as a hierarchy of nodes; each node having a frequency countcorresponding to the number of elements stored therein; comparing the frequency count ofeach node to a threshold value based on a total number of the elements stored in the nodesand identifying each node for which the frequency count is at least as great as the thresholdvalue as a hierarchical heavy hitter (HHH) node and propagating the frequency count ofeach non-HHH nodes to its corresponding parent nodes.,*,2009,14
Annotations for sparse data streams,Amit Chakrabarti; Graham Cormode; Navin Goyal; Justin Thaler,Abstract Motivated by the surging popularity of commercial cloud computing services; anumber of recent works have studied annotated data streams and variants thereof. In thissetting; a computationally weak verifier (cloud user); lacking the resources to store andmanipulate his massive input locally; accesses a powerful but untrusted prover (cloudservice). The verifier must work within the restrictive data streaming paradigm. The prover;who can annotate the data stream as it is read; must not just supply the final answer but alsoconvince the verifier of its correctness. Ideally; both the amount of annotation from the proverand the space used by the verifier should be sublinear in the relevant input size parameters.A rich theory of such algorithms---which we call schemes---has started to emerge. Prior workhas shown how to leverage the prover's power to efficiently solve problems that have no …,Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms,2014,13
Approximating data with the count-min sketch,Graham Cormode; Muthu Muthukrishnan,Faced with handling multiple large data sets in modern data-processing settings;researchers have proposed sketch data structures that capture salient properties whileoccupying little memory and that update or probe quickly. In particular; the Count-Min sketchhas proven effective for a variety of applications. It concurrently tracks many item counts withsurprisingly strong accuracy.,IEEE software,2012,13
Structure-aware sampling: Flexible and accurate summarization,Edith Cohen; Graham Cormode; Nick Duffield,Abstract: In processing large quantities of data; a fundamental problem is to obtain asummary which supports approximate query answering. Random sampling yields flexiblesummaries which naturally support subset-sum queries with unbiased estimators and well-understood confidence bounds. Classic sample-based summaries; however; are designedfor arbitrary subset queries and are oblivious to the structure in the set of keys. The particularstructure; such as hierarchy; order; or product space (multi-dimensional); makes rangequeries much more relevant for most analysis of the data.,arXiv preprint arXiv:1102.5146,2011,13
Method and apparatus for globally approximating quantiles in a distributed monitoring environment,*,The invention comprises a method and apparatus for determining a rank of a query value.Specifically; the method comprises receiving a rank query request; determining; for each ofthe at least one remote monitor; a predicted lower-bound rank value and upper-bound rankvalue; wherein the predicted lower-bound rank value and upper-bound rank value aredetermined according to at least one respective prediction model used by each of the atleast one remote monitor to compute the at least one local quantile summary; computing apredicted average rank value for each of the at least one remote monitor using the at leastone predicted lower-bound rank value and the at least one predicted upper-bound rankvalue associated with the respective at least one remote monitor; and computing the rank ofthe query value using the at least one predicted average rank value associated with the …,*,2010,13
Count-min sketch,Graham Cormode,Query processing algorithms are designed to efficiently exploit the available cache units inthe memory hierarchy. Cache-conscious algorithms typically employ knowledge ofarchitectural parameters such as cache size and latency. This knowledge can be used toensure that the algorithms have good temporal and/or spatial locality on the target platform.,*,2009,13
Method and apparatus for monitoring functions of distributed data,*,A method and system of monitoring computer network activity including determining a firstphase frequency estimate; associated with a first frequency vector; determined in responseto receiving first bits from a first plurality of remote computer network devices. The first bitsreceived from the first plurality of remote devices in response to satisfying a first activitythreshold. Also; determining a second phase frequency estimate associated with a secondfrequency vector and determined in response to receiving second bits from a secondplurality of remote devices. The second bits received from the second plurality of remotedevices in response to a second activity threshold being satisfied. The second phasefrequency estimate determined in response to the first phase frequency estimate exceedinga global threshold. Further; providing a frequency moment Fp in response to the second …,*,2013,12
Fast approximate wavelet tracking on streams,*,The first fast solution to the problem of tracking wavelet representations of one-dimensionaland multi-dimensional data streams based on a stream synopsis; the Group-Count Sketch(GCS) is provided. By imposing a hierarchical structure of groups over the data and applyingthe GCS; our algorithms can quickly recover the most important wavelet coefficients withguaranteed accuracy. A tradeoff between query time and update time is established; byvarying the hierarchical structure of groups; allowing the right balance to be found forspecific data streams. Experimental analysis confirmed this tradeoff; and showed that all themethods significantly outperformed previously known methods in terms of both update timeand query time; while maintaining a high level of accuracy.,*,2011,12
How not to review a paper: The tools and techniques of the adversarial reviewer,Graham Cormode,Abstract There are several useful guides available for how to review a paper in ComputerScience [10; 6; 12; 7; 2]. These are soberly presented; carefully reasoned and sensiblyargued. As a result; they are not much fun. So; as a contrast; this note is a checklist of hownot to review a paper. It details techniques that are unethical; unfair; or just plain nasty. Sincein Computer Science we often present arguments about how an adversary would approacha particular problem; this note describes the adversary's strategy.,ACM SIGMOD Record,2009,12
Conditional heavy hitters: detecting interesting correlations in data streams,Katsiaryna Mirylenka; Graham Cormode; Themis Palpanas; Divesh Srivastava,Abstract The notion of heavy hitters—items that make up a large fraction of the population—has been successfully used in a variety of applications across sensor and RFID monitoring;network data analysis; event mining; and more. Yet this notion often fails to capture thesemantics we desire when we observe data in the form of correlated pairs. Here; we areinterested in items that are conditionally frequent: when a particular item is frequent withinthe context of its parent item. In this work; we introduce and formalize the notion ofconditional heavy hitters to identify such items; with applications in network monitoring andMarkov chain modeling. We explore the relationship between conditional heavy hitters andother related notions in the literature; and show analytically and experimentally theusefulness of our approach. We introduce several algorithm variations that allow us to …,The VLDB Journal,2015,11
A dataset search engine for the research document corpus,Meiyu Lu; Srinivas Bangalore; Graham Cormode; Marios Hadjieleftheriou; Divesh Srivastava,A key step in validating a proposed idea or system is to evaluate over a suitable dataset.However; to this date there have been no useful tools for researchers to understand whichdatasets have been used for what purpose; or in what prior work. Instead; they have tomanually browse through papers to find the suitable datasets and their corresponding URLs;which is laborious and inefficient. To better aid the dataset discovery process; and provide abetter understanding of how and where datasets have been used; we propose a frameworkto effectively identify datasets within the scientific corpus. The key technical challenges areidentification of datasets; and discovery of the association between a dataset and the URLswhere they can be accessed. Based on this; we have built a user friendly web-based searchinterface for users to conveniently explore the dataset-paper relationships; and find …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,11
Structure-aware sampling on data streams,Edith Cohen; Graham Cormode; Nick Duffield,Abstract The massive data streams observed in network monitoring; data processing andscientific studies are typically too large to store. For many applications over such data; wemust obtain compact summaries of the stream. These summaries should allow accurateanswering of post hoc queries with estimates which approximate the true answers over theoriginal stream. The data often has an underlying structure which makes certain subsetqueries; in particular range queries; more relevant than arbitrary subsets. Applications suchas access control; change detection; and heavy hitters typically involve subsets that areranges or unions thereof. Random sampling is a natural summarization tool; being easy toimplement and flexible to use. Known sampling methods are good for arbitrary queries butfail to optimize for the common case of range queries. Meanwhile; specialized …,Proceedings of the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems,2011,11
System and method for generating statistical descriptors for a data stream,*,Described is a system and method for receiving a data stream of multi-dimensional items;collecting a sample of the data stream having a predetermined number of items and dividingthe sample into a plurality of subsamples; each subsample corresponding to a singledimension of each of the predetermined number of items. A query is then executed on aparticular item in at least two of the subsamples to generate data for the correspondingsubsample. This data is combined into a single value.,*,2010,11
System and method for encoding a signal using compressed sensor measurements,*,Described is a system and method for receiving a signal for transmission and encoding thesignal into a plurality of linear projections representing the signal. The encoding includesdefining a transform matrix. The transform matrix being defined by processing the signalusing a macroseparation matrix; processing the signal using a microseparation matrix andprocessing the signal using an estimation vector.,*,2008,11
On signatures for communication graphs,Graham Cormode; Flip Korn; S Muthukrishnan; Yihua Wu,Communications between individuals can be represented by (weighted; multi-) graphs.Many applications operate on communication graphs associated with telephone calls;emails; instant messages (IM); blogs; web forums; e-business relationships and so on.These applications include identifying repetitive fraudsters; message board aliases;multiusage of IP addresses; etc. Tracking electronic identities in communication networkscan be achieved if we have a reliable" signature" for nodes and activities. While manyexamples of ad hoc signatures can be proposed for particular tasks; what is needed is asystematic study of the principles behind the usage of signatures for any task. We develop aformal framework for the use of signatures in communication graphs and identify threefundamental properties that are natural to signature schemes: persistence; uniqueness …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,11
Efficient Strategies for Continuous Distributed Tracking Tasks.,Graham Cormode; Minos N Garofalakis,Abstract While traditional databases have focused on single query evaluation in acentralized setting; emerging applications require continuous tracking of queries on datathat is widely distributed and constantly updated. We describe such scenarios; and describethe challenges involved in designing communication-efficient protocols for the tracking taskswe define. We outline some solutions to these problems; by abstracting a model of thecommunication system; defining the tracking tasks of interest; and building query-trackingschemes based on three guiding principles of minimizing global information; usingsummaries to capture whole data streams; and seeking stability of the protocols.,IEEE Data Eng. Bull.,2005,11
Stable distributions for stream computations: It’s as easy as 0; 1; 2,Graham Cormode,A surprising number of data stream problems are solved by methods involving computationswith stable distributions. This paper will give a short summary of some of these problems;and how the best known solutions depend on use of stable distributions; it also lists somerelated open problems. Stable distributions arise from seeking statistics with the propertythat,Workshop on Management and Processing of Massive Data Streams (MPDS) at FCRC,2003,11
Kernelization via sampling with applications to dynamic graph streams,Rajesh Chitnis; Graham Cormode; Hossein Esfandiari; MohammadTaghi Hajiaghayi; Andrew McGregor; Morteza Monemizadeh; Sofya Vorotnikova,Abstract: In this paper we present a simple but powerful subgraph sampling primitive that isapplicable in a variety of computational models including dynamic graph streams (where theinput graph is defined by a sequence of edge/hyperedge insertions and deletions) anddistributed systems such as MapReduce. In the case of dynamic graph streams; we use thisprimitive to prove the following results:--Matching: First; there exists an $\tilde {O}(k^ 2) $space algorithm that returns an exact maximum matching on the assumption the cardinalityis at most $ k $. The best previous algorithm used $\tilde {O}(kn) $ space where $ n $ is thenumber of vertices in the graph and we prove our result is optimal up to logarithmic factors.Our algorithm has $\tilde {O}(1) $ update time. Second; there exists an $\tilde {O}(n^2/\alpha^ 3) $ space algorithm that returns an $\alpha $-approximation for matchings of …,arXiv preprint arXiv:1505.01731,2015,10
Method and apparatus for providing real friends count,*,A method and apparatus for tracking communications in a network are disclosed. Forexample; the method receives a subscription from a customer for a service to track at leastone variable associated with a plurality of communicants of the customer. The methodidentifies a plurality of members of a social network of the customer; and gatherscommunication data associated with the plurality of members for tracking the at least onevariable. The method then displays at least one result derived from the communication datato the customer.,*,2014,10
Socializing the h-index,Graham Cormode; Qiang Ma; S Muthukrishnan; Brian Thompson,Abstract A variety of bibliometric measures have been proposed to quantify the impact ofresearchers and their work. The h-index is a notable and widely used example which aimsto improve over simple metrics such as raw counts of papers or citations. However; alimitation of this measure is that it considers authors in isolation and does not account forcontributions through a collaborative team. To address this; we propose a natural variantthat we dub the Social h-index. The idea is to redistribute the h-index score to reflect anindividual's impact on the research community. In addition to describing this new measure;we provide examples; discuss its properties; and contrast with other measures.,Journal of Informetrics,2013,10
Automatic gleaning of semantic information in social networks,*,Disclosed are method and apparatus for identifying members of a social network who havea high likelihood of providing a useful response to a query. A query engine examines thepersonal pages of a set of members and automatically gleans semantic information relevantto the query. From the automatically-gleaned semantic information; a score indicative of thelikelihood that the member may provide a useful response is calculated.,*,2013,10
Methods and apparatus to construct histogram and wavelet synopses for probabilistic data,*,Example methods and apparatus to construct histogram and wavelet synopses forprobabilistic data are disclosed. A disclosed example method involves receivingprobabilistic data associated with probability measures and generating a plurality ofhistograms based on the probabilistic data. Each histogram is generated based on itemsrepresented by the probabilistic data. In addition; each histogram is generated using adifferent quantity of buckets containing different ones of the items. An error measureassociated with each of the plurality of histograms is determined and one of the plurality ofhistograms is selected based on its associated error measure. The method also involvesdisplaying parameter information associated with the one of the plurality of histograms torepresent the data.,*,2013,10
System and method for identifying hierarchical heavy hitters in a multidimensional environment,*,A method including receiving a plurality of elements of a data stream; storing a multi-dimensional data structure in a memory; said multi-dimensional data structure storing theplurality of elements as a hierarchy of nodes; each node having a frequency countcorresponding to the number of elements stored therein; comparing the frequency count ofeach node to a threshold value based on a total number of the elements stored in the nodesand identifying each node for which the frequency count is at least as great as the thresholdvalue as a hierarchical heavy hitter (HHH) node and propagating the frequency count ofeach non-HHH nodes to its corresponding parent nodes.,*,2009,10
Method and apparatus for identifying hierarchical heavy hitters in a data stream,*,A method; apparatus; and computer readable medium for processing a data stream isdescribed. In one example; a set of elements of a data stream are received. The set ofelements are stored in a memory as a hierarchy of nodes. Each of the nodes includesfrequency data associated with either an element in the set of elements or a prefix of anelement in the set of elements. A set of hierarchical heavy hitters is then identified among thenodes in the hierarchy. The frequency data of each of the hierarchical heavy hitter nodes;after discounting any portion thereof attributed to a descendent hierarchical heavy hitternode in said set of hierarchical heavy hitter nodes; being greater than or equal to a fractionof the number of elements in the set of elements.,*,2005,10
Forward decay temporal data analysis,*,A disclosed method for implementing time decay in the analysis of streaming data objects isbased on the age; referred to herein as the forward age; of a data object measured from alandmark time in the past to a time associated with the occurrence of the data object; eg; anobject's timestamp. A forward time decay function is parameterized on the forward age.Because a data object's forward age does not depend on the current time; a value of theforward time decay function is determined just once for each data object. A scaling factor orweight associated with a data object may be weighted according to its decay function value.Forward time decay functions are beneficial in determining decayed aggregates; includingdecayed counts; sums; and averages; decayed minimums and maximums; and for drawingdecay-influenced samples.,*,2013,8
Finding interesting correlations with conditional heavy hitters,Katsiaryna Mirylenka; Themis Palpanas; Graham Cormode; Divesh Srivastava,The notion of heavy hitters-items that make up a large fraction of the population-has beensuccessfully used in a variety of applications across sensor and RFID monitoring; networkdata analysis; event mining; and more. Yet this notion often fails to capture the semantics wedesire when we observe data in the form of correlated pairs. Here; we are interested in itemsthat are conditionally frequent: when a particular item is frequent within the context of itsparent item. In this work; we introduce and formalize the notion of Conditional Heavy Hittersto identify such items; with applications in network monitoring; and Markov chain modeling.We introduce several streaming algorithms that allow us to find conditional heavy hittersefficiently; and provide analytical results. Different algorithms are successful for differentinput characteristics. We perform experimental evaluations to demonstrate the efficacy of …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,8
Publishing attributed social graphs with formal privacy guarantees,Zach Jorgensen; Ting Yu; Graham Cormode,Abstract Many data analysis tasks rely on the abstraction of a graph to represent relationsbetween entities; with attributes on the nodes and edges. Since the relationships encodedare often sensitive; we seek effective ways to release representative graphs whichnevertheless protect the privacy of the data subjects. Prior work on this topic has focusedprimarily on the graph structure in isolation; and has not provided ways to handle richergraphs with correlated attributes. We introduce an approach to release such graphs underthe strong guarantee of differential privacy. We adapt existing graph models; and introduce anew one; and show how to augment them with meaningful privacy. This provides a completeworkflow; where the input is a sensitive graph; and the output is a realistic synthetic graph.Our experimental study demonstrates that our process produces useful; accurate …,Proceedings of the 2016 International Conference on Management of Data,2016,7
Generating minimality-attack-resistant data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for generating data and data sets that are resistant to minimality attacks. Data setshaving a number of tuples are received; and the tuples are ordered according to an aspectof the tuples. The tuples can be split into groups of tuples; and each of the groups may beanalyzed to determine if the group complies with a privacy requirement. Groups that satisfythe privacy requirement may be output as new data sets that are resistant to minimalityattacks.,*,2014,7
Methods and apparatus to anonymize a dataset of spatial data,*,Methods and apparatus are disclosed to anonymize a dataset of spatial data. An examplemethod includes generating a spatial indexing structure with spatial data; establishing aheight value associated with the spatial indexing structure to generate a plurality of treenodes; each of the plurality of tree nodes associated with spatial data counts; calculating alocalized noise budget value for respective ones of the tree nodes based on the height valueand an overall noise budget; and anonymizing the plurality of tree nodes with aanonymization process; the anonymization process using the localized noise budget valuefor respective ones of the tree nodes.,*,2014,7
On Interactivity in Arthur-Merlin Communication and Stream Computation.,Amit Chakrabarti; Graham Cormode; Andrew McGregor; Justin Thaler; Suresh Venkatasubramanian,Abstract We introduce online interactive proofs (OIP); which are a hierarchy ofcommunication complexity models that involve both randomness and nondeterminism (thus;they belong to the Arthur–Merlin family); but are online in the sense that the basiccommunication flows from Alice to Bob alone. The complexity classes defined by these OIPmodels form a natural hierarchy based on the number of rounds of interaction betweenverifier and prover. We give upper and lower bounds that (1) characterize every finite level ofthe OIP hierarchy in terms of previously-studied communication complexity classes; and (2)separate the first four levels of the hierarchy. These results show marked contrasts and someparallels with the classic Turing Machine theory of interactive proofs. Our motivation forstudying OIP is to address computational complexity questions arising from the growing …,Electronic Colloquium on Computational Complexity (ECCC),2013,7
Scienceography: the study of how science is written,Graham Cormode; S Muthukrishnan; Jinyun Yan,Abstract Scientific literature has itself been the subject of much scientific study; for a varietyof reasons: understanding how results are communicated; how ideas spread; and assessingthe influence of areas or individuals. However; most prior work has focused on extractingand analyzing citation and stylistic patterns. In this work; we introduce the notion of'scienceography'; which focuses on the writing of science. We provide a first large scalestudy using data derived from the arXiv e-print repository. Crucially; our data includes the“source code” of scientific papers—the \LaTeX source—which enables us to study featuresnot present in the “final product”; such as the tools used and private comments betweenauthors. Our study identifies broad patterns and trends in two example areas—computerscience and mathematics—as well as highlighting key differences in the way that science …,International Conference on Fun with Algorithms,2012,7
Aggregate query answering on possibilistic data with cardinality constraints,Graham Cormode; Divesh Srivastava; Entong Shen; Ting Yu,Uncertainties in data can arise for a number of reasons: when data is incomplete; containsconflicting information or has been deliberately perturbed or coarsened to remove sensitivedetails. An important case which arises in many real applications is when the data describesa set of possibilities; but with cardinality constraints. These constraints represent correlationsbetween tuples encoding; eg that at most two possible records are correct; or that there is an(unknown) one-to-one mapping between a set of tuples and attribute values. Although therehas been much effort to handle uncertain data; current systems are not equipped to handlesuch correlations; beyond simple mutual exclusion and co-existence constraints. Vitally; theyhave little support for efficiently handling aggregate queries on such data. In this paper; weaim to address some of these deficiencies; by introducing LICM (Linear Integer Constraint …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,7
System and method for generating statistical descriptors for a data stream,*,Described is a system and method for receiving a data stream of multi-dimensional items;collecting a sample of the data stream having a predetermined number of items and dividingthe sample into a plurality of subsamples; each subsample corresponding to a singledimension of each of the predetermined number of items. A query is then executed on aparticular item in at least two of the subsamples to generate data for the correspondingsubsample. This data is combined into a single value.,*,2012,7
The sparse awakens: Streaming algorithms for matching size estimation in sparse graphs,Graham Cormode; Hossein Jowhari; Morteza Monemizadeh; S Muthukrishnan,Abstract: Estimating the size of the maximum matching is a canonical problem in graphalgorithms; and one that has attracted extensive study over a range of differentcomputational models. We present improved streaming algorithms for approximating thesize of maximum matching with sparse (bounded arboricity) graphs.* Insert-Only Streams:We present a one-pass algorithm that takes O (c log^ 2 n) space and approximates the sizeof the maximum matching in graphs with arboricity c within a factor of O (c). This improvessignificantly on the state-of-the-art O~(cn^{2/3})-space streaming algorithms.,arXiv preprint arXiv:1608.03118,2016,6
The tears of donald knuth,Thomas Haigh,Has the history of computing taken a tragic turn? the much smaller community of historiansof computing but; even by Google Scholar's generous definitions; the paper that saddenedKnuth has been cited only nine times. Knuth then enumerated his motivations; as acomputer scientist; to read the history of science. First; reading history helped him tounderstand the process of discovery. Second; understanding the difficulty and false startsexperienced by brilliant historical scientists in making discoveries that specialists now findobvious helped him to,Communications of the ACM,2015,6
Anonymization of data over multiple temporal releases,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for anonymizing data over multiple temporal releases. Data is received; and nodesand connections in the data are identified. The data also is analyzed to identify predictedconnections. The nodes; the connections; and the predicted connections are analyzed todetermine how to group the nodes in the data. The data is published; and the grouping ofthe nodes is extended to subsequent temporal releases of the data; the nodes of which aregrouped in accordance with the grouping used with the data.,*,2014,6
A second look at counting triangles in graph streams,Graham Cormode; Hossein Jowhari,Abstract In this paper we present improved results on the problem of counting triangles inedge streamed graphs. For graphs with m edges and at least T triangles; we show that anextra look over the stream yields a two-pass streaming algorithm that uses O (m ϵ 4.5 T)space and outputs a (1+ ϵ) approximation of the number of triangles in the graph. Thisimproves upon the two-pass streaming tester of Braverman et al.[2]; which distinguishesbetween triangle-free graphs and graphs with at least T triangles using O (m T 1/3) space.Also; in terms of dependence on T; we show that more passes would not lead to a betterspace bound. In other words; we prove there is no constant pass streaming algorithm thatdistinguishes between triangle-free graphs from graphs with at least T triangles using O (m T1/2+ ρ) space for any constant ρ≥ 0.,Theoretical Computer Science,2014,6
Modeling collaboration in academia: A game theoretic approach,Qiang Ma; S Muthukrishnan; Brian Thompson; Graham Cormode,Abstract In this work; we aim to understand the mechanisms driving academic collaboration.We begin by building a model for how researchers split their effort between multiple papers;and how collaboration affects the number of citations a paper receives; supported byobservations from a large real-world publication and citation dataset; which we call the h-Reinvestment model. Using tools from the field of Game Theory; we study researchers'collaborative behavior over time under this model; with the premise that each researcherwants to maximize his or her academic success. We find analytically that there is a strongincentive to collaborate rather than work in isolation; and that studying collaborativebehavior through a game-theoretic lens is a promising approach to help us betterunderstand the nature and dynamics of academic collaboration.,Proceedings of the 23rd International Conference on World Wide Web,2014,6
People like us: mining scholarly data for comparable researchers,Graham Cormode; S Muthukrishnan; Jinyun Yan,Abstract There are many situations that one needs to find comparable others for a givenresearcher. Examples include finding peer reviewers; programming committees forconferences; and comparable individual asked in recommendation letters for tenureevaluation. The task is often done on an ad hoc and informal basis. In this paper; weaddress an interesting problem that has not been adequately studied so far: miningcumulated large scale scholarly data to find comparable researchers. We propose astandard to quantify the quality of individual's research output; through the quality ofpublishing venues. We represent a researcher as a sequence of her publication records;and develop methods to compute the distance between two researchers through sequencematching. Multiple variations of distances are considered to target different scenarios. We …,Proceedings of the 23rd International Conference on World Wide Web,2014,5
Computing time-decayed aggregates in data streams,*,Aggregates are calculated from a data stream in which data is sent in a sequence of tuples;in which each tuple comprises an item identifier and a timestamp indicating when the tuplewas transmitted. The tuples may arrive out-of-order; that is; the sequence in which the tuplesarrive are not necessarily in the sequence of their corresponding timestamps. In calculatingaggregates; more recent data may be given more weight by multiplying each tuple by adecay function which is a function of the timestamp associated with the tuple and the currenttime. The tuples are recorded in a quantile-digest data structure. Aggregates are calculatedfrom the data stored in the quantile-digest data structure.,*,2013,5
Accurate and efficient private release of datacubes and contingency tables,Graham Cormode; Cecilia M Procopiuc; Divesh Srivastava; Grigory Yaroslavtsev,Abstract: A central problem in releasing aggregate information about sensitive data is to doso accurately while providing a privacy guarantee on the output. Recent work focuses on theclass of linear queries; which include basic counting queries; data cubes; and contingencytables. The goal is to maximize the utility of their output; while giving a rigorous privacyguarantee. Most results follow a common template: pick a" strategy" set of linear queries toapply to the data; then use the noisy answers to these queries to reconstruct the queries ofinterest. This entails either picking a strategy set that is hoped to be good for the queries; orperforming a costly search over the space of all possible strategies. In this paper; wepropose a new approach that balances accuracy and efficiency: we show how to improvethe accuracy of a given query set by answering some strategy queries more accurately …,arXiv preprint arXiv:1207.6096,2012,5
Method and apparatus for finding biased quantiles in data streams,*,A method and apparatus for computing biased or targeted quantiles are disclosed. Forexample; the present invention reads a plurality of items from a data stream and inserts eachof the plurality of items that was read from the data stream into a data structure. Periodically;the data structure is compressed to reduce the number of stored items in the data structure.In turn; the compressed data structure can be used to output a biased or targeted quantile.,*,2006,5
Some Key Concepts in Data Mining–Clustering,Graham Cormode,The notion of 'clusters' is a very natural one; and occurs frequently in discussions ofepidemiology. We hear about 'cancer clusters'; areas where the number of reported cancercases within an area or group of people exceeds the expected amount. Such clusters lead toinvestigation of possible carcinogens or explanation for greater susceptibility amongstcertain groups. When thinking about clusters; people most often think of geographicalclusters—the image of “pins in a map” is a resonant one. In fact; this approach has its rootsin the very foundation of epidemiology. The famed work of John Snow in 1854 was to plotcases of cholera on a map of London; and to observe that these were centered aroundcertain water pumps. Thus the result of this early instance of clustering was to produce ahypothesis on the source of the cholera cases; which was subsequently verified. The …,DIMACS Series in Discrete Mathematics and Theoretical Computer Science,2006,5
Summarizing two-dimensional data with skyline-based statistical descriptors,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Much real data consists of more than one dimension; such as financial transactions(eg; price× volume) and IP network flows (eg; duration× numBytes); and capturerelationships between the variables. For a single dimension; quantiles are intuitive androbust descriptors. Processing and analyzing such data; particularly in data warehouse ordata streaming settings; requires similarly robust and informative statistical descriptors thatgo beyond one-dimension. Applying quantile methods to summarize a multidimensionaldistribution along only singleton attributes ignores the rich dependence amongst thevariables. In this paper; we present new skyline-based statistical descriptors for capturingthe distributions over pairs of dimensions. They generalize the notion of quantiles in theindividual dimensions; and also incorporate properties of the joint distribution. We …,International Conference on Scientific and Statistical Database Management,2008,4
Using transmission dynamics models to validate vaccine efficacy measures prior to conducting HIV vaccine efficacy trials.,Kamal Desai; Marie-Claude Boily; Benoît R Mâsse; Roy M Anderson,Abstract. Mathematical models of disease transmission have proven useful in gainingepidemiological insights into dynamics of infection and informing public health decisions fortheir control. Such models are now finding applications in the design of expensive clinicaltrials against HIV to inform statistical parameters; such as sample size; trial duration andefficacy analyses. We illustrate the use of clinical trial simulation to address amethodological problem in estimating efficacy in trials of vaccines against HIV. Because ofunknown or poorly understood models of vaccine action; uncertain time-lags in immunity;and the many heterogeneities that influence exposure to HIV; traditional efficacy measures;based on the estimation of hazard rates or on cumulative incidence; produce estimates thatexhibit time-dependent biases; even if the true efficacy is constant over time. This …,Discrete methods in epidemiology,2004,4
Data stream methods,Graham Cormode; S Muthukrishnan,Page 1. Data Stream Methods Graham Cormode graham@dimacs.rutgers.edu S.Muthukrishnan muthu@cs.rutgers.edu Page 2. Plan of attack • Frequent Items / Heavy Hitters •Counting Distinct Elements • Clustering items in Streams Page 3. Motivating Distinct ElementsMany network flows between (source; dest) pairs Want a snapshot at time t of the flows Thisdefines a (massive) vector; and we ask: • Summarise the current state • How does state attime t compare with at t'? • Which past situation does this most resemble; etc.? Page 4. CountingDistinct Values Application 1: Maintaining number of distinct values in a relation with insertsand deletes Important to know number of values for query optimization; approximate queryanswering; join size estimation etc. Fully dynamic case; with inserts and deletes: samplingfrom the relation itself has been shown to be inaccurate …,*,2003,4
Computing time-decayed aggregates under smooth decay functions,*,Aggregates are calculated from a data stream in which data is sent in a sequence of tuples;in which each tuple comprises an item identifier and a timestamp indicating when the tuplewas transmitted. The tuples may arrive at a data receiver out-of-order; that is; the sequencein which the tuples arrive are not necessarily in the same sequence as their correspondingtimestamps. In calculating aggregates; more recent data may be given more weight by adecay function which is a function of the timestamp associated with the tuple and the currenttime. The statistical characteristics of the tuples are summarized by a set of linear datasummaries. The set of linear data summaries are generated such that only a single lineardata summary falls between a set of boundaries calculated from the decay function and a setof timestamps. Aggregates are calculated from the set of linear data summaries.,*,2013,3
On unifying the space of ℓ 0-sampling algorithms,Graham Cormode; Donatella Firmani,Abstract The problem of building an &ell; 0-sampler is to sample near-uniformly from thesupport set of a dynamic multiset. This problem has a variety of applications within dataanalysis; computational geometry and graph algorithms. In this paper; we abstract a set ofsteps for building an &ell; 0-sampler; based on sampling; recovery and selection. Weanalyze the implementation of an &ell; 0-sampler within this framework; and show how priorconstructions of &ell; 0-samplers can all be expressed in terms of these steps. Ourexperimental contribution is to provide a first detailed study of the accuracy andcomputational cost of &ell; 0-samplers.,Proceedings of the Meeting on Algorithm Engineering & Expermiments,2013,3
On unifying the space of ℓ 0-sampling algorithms,Graham Cormode; Donatella Firmani,Abstract The problem of building an &ell; 0-sampler is to sample near-uniformly from thesupport set of a dynamic multiset. This problem has a variety of applications within dataanalysis; computational geometry and graph algorithms. In this paper; we abstract a set ofsteps for building an &ell; 0-sampler; based on sampling; recovery and selection. Weanalyze the implementation of an &ell; 0-sampler within this framework; and show how priorconstructions of &ell; 0-samplers can all be expressed in terms of these steps. Ourexperimental contribution is to provide a first detailed study of the accuracy andcomputational cost of &ell; 0-samplers.,Proceedings of the Meeting on Algorithm Engineering & Expermiments,2013,3
Streaming in a Connected World.,Graham Cormode; Minos N Garofalakis,*,VLDB,2006,3
Brief Announcement: Tracking Distributed Aggregates over Time-based Sliding Windows,Graham Cormode; Ke Yi,ABSTRACT The area of distributed monitoring requires tracking the value of a function ofdistributed data as new observations are made. An important case is when attention isrestricted to only a recent time period; such as the last hour of readings—the sliding windowcase. In this announcement; we outline a novel paradigm for handling such monitoringproblems; which we dub the “forward/backward” approach. This provides clean solutions forseveral fundamental problems; such as counting; tracking frequent items; and maintainingorder statistics. We obtain efficient protocols for these problems that improve on previouswork; and are easy to implement. Specifically; we obtain optimal O (k ε log (εn/k))communication per window of n updates for tracking counts and heavy hitters with accuracyε across k sites; and near-optimal communication of O (k ε log2 (1/ε) log (n/k)) for …,PODC,*,3
Marginal Release Under Local Differential Privacy,Tejas Kulkarni; Graham Cormode; Divesh Srivastava,Abstract: Many analysis and machine learning tasks require the availability of marginalstatistics on multidimensional datasets while providing strong privacy guarantees for thedata subjects. Applications for these statistics range from finding correlations in the data tofitting sophisticated prediction models. In this paper; we provide a set of algorithms formaterializing marginal statistics under the strong model of local differential privacy. Weprove the first tight theoretical bounds on the accuracy of marginals compiled under eachapproach; perform empirical evaluation to confirm these bounds; and evaluate them fortasks such as modeling and correlation testing. Our results show that releasing informationbased on (local) Fourier transformations of the input is preferable to alternatives baseddirectly on (local) marginals. Subjects: Databases (cs. DB) Cite as: arXiv: 1711.02952 [cs …,arXiv preprint arXiv:1711.02952,2017,2
On the tradeoff between stability and fit,Edith Cohen; Graham Cormode; Nick Duffield; Carsten Lund,Abstract In computing; as in many aspects of life; changes incur cost. Many optimizationproblems are formulated as a one-time instance starting from scratch. However; a commoncase that arises is when we already have a set of prior assignments and must decide how torespond to a new set of constraints; given that each change from the current assignmentcomes at a price. That is; we would like to maximize the fitness or efficiency of our system;but we need to balance it with the changeout cost from the previous state. We provide aprecise formulation for this tradeoff and analyze the resulting stable extensions of somefundamental problems in measurement and analytics. Our main technical contribution is astable extension of Probability Proportional to Size (PPS) weighted random sampling; withapplications to monitoring and anomaly detection problems. We also provide a general …,ACM Transactions on Algorithms (TALG),2016,2
Efficient publication of sparse data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for publishing data. A data summary summarizing the data can be generated andpublished according to several publishing schemes. In some embodiments; non-zero entriesare selected and modified and zero entries are sampled according to one or moredistribution functions. The sampled and modified values are added to a data summary; or asample of the sampled and modified values are added to the data summary. The datasummary is published; released; used; or otherwise output. In other embodiments; priorityvalues are assigned to each value associated with the data; and a number of entries with thehighest values are selected and added to the data summary.,*,2016,2
Brief announcement: New streaming algorithms for parameterized maximal matching & beyond,Rajesh Chitnis; Graham Cormode; Hossein Esfandiari; MohammadTaghi Hajiaghayi; Morteza Monemizadeh,Abstract Very recently at SODA'15 [2]; we studied maximal matching via the framework ofparameterized streaming; where we sought solutions under the promise that no maximalmatching exceeds k in size. In this paper; we revisit this problem and provide a much simpleralgorithm for this problem. We are also able to apply the same technique to the Point LineCover problem [3].,Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures,2015,2
Method and apparatus for providing an electronic commerce website,*,A method and an apparatus for providing an electronic commerce website over a networkare disclosed. For example; the method receives a request for a product or a service from acustomer via an electronic commerce website; and identifies internal information pertainingto the request; wherein the internal information is information known by a businessenterprise of the electronic commerce website. The method also identifies externalinformation pertaining to the request; wherein the external information is informationobtained by the business enterprise from another entity; and sends the internal informationand the external information pertaining to the request to the customer.,*,2010,2
Individual privacy vs population privacy: Learning to attack anonymization,Graham Cormode,Abstract: Over the last decade there have been great strides made in developing techniquesto compute functions privately. In particular; Differential Privacy gives strong promises aboutconclusions that can be drawn about an individual. In contrast; various syntactic methods forproviding privacy (criteria such as kanonymity and l-diversity) have been criticized for stillallowing private information of an individual to be inferred. In this report; we consider theability of an attacker to use data meeting privacy definitions to build an accurate classifier.We demonstrate that even under Differential Privacy; such classifiers can be used toaccurately infer" private" attributes in realistic data. We compare this to similar approachesfor inferencebased attacks on other forms of anonymized data. We place these attacks onthe same scale; and observe that the accuracy of inference of private attributes for …,arXiv preprint arXiv:1011.2511,2010,2
How to Increase the Acceptance Ratios of Top Conferences?,Graham Cormode; Artur Czumaj; S Muthukrishnan,ABSTRACT In the beginning was the pub. This work was triggered by a pub conversationwhere the authors observed that many resumes list acceptance ratios of conferences wheretheir papers appear; boasting the low acceptance ratio. The lower the ratio; the better yourpaper looks. The list might look equally impressive if one listed the rejection ratio ofconferences where one's paper was submitted and rejected. We decided to lampoon ratherthan lament the effort the PC typically put in: wouldn't the world be better if we couldencourage only high quality submissions and so run top conferences with very highacceptance ratios? This paper captures our thoughts; and it is best consumed in a pub (andin color).,Proceedings of the 3rd International Conference on Fun with Algorithms (FUN 2004); Isola d’Elba; Tuscany; Italy,2004,2
Quantiles over data streams: experimental comparisons; new analyses; and further improvements,Ge Luo; Lu Wang; Ke Yi; Graham Cormode,Abstract A fundamental problem in data management and analysis is to generatedescriptions of the distribution of data. It is most common to give such descriptions in termsof the cumulative distribution; which is characterized by the quantiles of the data. The designand engineering of efficient methods to find these quantiles has attracted much study;especially in the case where the data are given incrementally; and we must compute thequantiles in an online; streaming fashion. While such algorithms have proved to beextremely useful in practice; there has been limited formal comparison of the competingmethods; and no comprehensive study of their performance. In this paper; we remedy thisdeficit by providing a taxonomy of different methods and describe efficient implementations.In doing so; we propose new variants that have not been studied before; yet which …,The VLDB Journal,2016,1
Stable distributions in streaming computations,Graham Cormode; Piotr Indyk,Abstract In many streaming scenarios; we need to measure and quantify the data that isseen. For example; we may want to measure the number of distinct IP addresses seen overthe course of a day; compute the difference between incoming and outgoing transactions ina database system or measure the overall activity in a sensor network. In all of theseexamples; data can be modeled as massive; dynamic vectors. Here; we are interested in thewell-known and widely used L p L_p norms of such vectors. These encompass the familiarEuclidean (root of sum of squares) and Manhattan (sum of absolute values) norms. We showhow such L p L_p norms can be efficiently estimated for massive vectors presented in thestreaming model. This is achieved by making succinct sketches of the data; which can beused as synopses of the vectors they summarize.,*,2016,1
Computing time-decayed aggregates under smooth decay functions,*,Aggregates are calculated from a data stream in which data is sent in a sequence of tuples;in which each tuple comprises an item identifier and a timestamp indicating when the tuplewas transmitted. The tuples may arrive at a data receiver out-of-order; that is; the sequencein which the tuples arrive are not necessarily in the same sequence as their correspondingtimestamps. In calculating aggregates; more recent data may be given more weight by adecay function which is a function of the timestamp associated with the tuple and the currenttime. The statistical characteristics of the tuples are summarized by a set of linear datasummaries. The set of linear data summaries are generated such that only a single lineardata summary falls between a set of boundaries calculated from the decay function and a setof timestamps. Aggregates are calculated from the set of linear data summaries.,*,2015,1
The confounding problem of private data release (Invited Talk),Graham Cormode,Abstract The demands to make data available are growing ever louder; including open datainitiatives and" data monetization". But the problem of doing so without disclosingconfidential information is a subtle and difficult one. Is" private data release" an oxymoron?This paper (accompanying an invited talk) aims to delve into the motivations of data release;explore the challenges; and outline some of the current statistical approaches developed inresponse to this confounding problem.,LIPIcs-Leibniz International Proceedings in Informatics,2015,1
Lightweight query authentication on streams,Stavros Papadopoulos; Graham Cormode; Antonios Deligiannakis; Minos Garofalakis,Abstract We consider a stream outsourcing setting; where a data owner delegates themanagement of a set of disjoint data streams to an untrusted server. The ownerauthenticates his streams via signatures. The server processes continuous queries on theunion of the streams for clients trusted by the owner. Along with the results; the server sendsproofs of result correctness derived from the owner's signatures; which are verifiable by theclients. We design novel constructions for a collection of fundamental problems over streamsrepresented as linear algebraic queries. In particular; our basic schemes authenticatedynamic vector sums; matrix products; and dot products. These techniques can be adaptedfor authenticating a wide range of important operations in streaming environments; includinggroup-by queries; joins; in-network aggregation; similarity matching; and event …,ACM Transactions on Database Systems (TODS),2014,1
Parameterized streaming algorithms for vertex cover,Rajesh Chitnis; Graham Cormode; MohammadTaghi Hajiaghayi; Morteza Monemizadeh,Abstract: As graphs continue to grow in size; we seek ways to effectively process such dataat scale. The model of streaming graph processing; in which a compact summary ismaintained as each edge insertion/deletion is observed; is an attractive one. However; fewresults are known for optimization problems over such dynamic graph streams. In this paper;we introduce a new approach to handling graph streams; by instead seeking solutions forthe parameterized versions of these problems where we are given a parameter $ k $ and theobjective is to decide whether there is a solution bounded by $ k $. By combiningkernelization techniques with randomized sketch structures; we obtain the first streamingalgorithms for the parameterized versions of the Vertex Cover problem. We consider thefollowing three models for a graph stream on $ n $ nodes:,arXiv preprint arXiv:1405.0093,2014,1
Summary data structures for massive data,Graham Cormode,Abstract Prompted by the need to compute holistic properties of increasingly large data sets;the notion of the “summary” data structure has emerged in recent years as an importantconcept. Summary structures can be built over large; distributed data; and provideguaranteed performance for a variety of data summarization tasks. Various types ofsummaries are known: summaries based on random sampling; summaries formed as linearsketches of the input data; and other summaries designed for a specific problem at hand.,Conference on Computability in Europe,2013,1
What does an associate editor actually do?,Graham Cormode,Journal publications are an important part of the prop- agation of results and ideas in computerscience. Papers in prestigious journals reflect well on their authors; and serve to provide afull; detailed and peer-reviewed de- scription of their research. Yet; the process from sub- missionto decision is opaque. A researcher typically submits their paper to a journal and then waits months(sometimes many months) before receiving a set of re- views and a decision on whether thejournal will pursue publication of the submission. It is far from obvious to the researcher exactlywhat is going on during this time. The purpose of this article is to shed more light on thisprocess; by describing the typical sequence of events from the perspective of the associateeditor. The hope is that this serves multiple purposes … • To help authors understand theprocess; and allow them to make their submissions with this knowl- edge.,ACM SIGMOD Record,2013,1
Discrete Methods in Epidemiology,James Abello; Graham Cormode,Selected data mining concepts by J. Abello; G. Cormode; D. Fradkin; D. Madigan; O. Melnik;and I. Muchnik Descriptive epidemiology: A brief introduction by D. Schneider Biostatisticalchallenges in molecular data analysis by WD Shannon Mining online media for globaldisease outbreak monitoring by L. Hirschman and LE Damianos Generalized contingencytables and concept lattices by D. Ozonoff; A. Pogel; and T. Hannan Graph partitions andconcept lattices by J. Abello and A. Pogel Using transmission dynamics models to validatevaccine efficacy measures prior to conducting HIV vaccine efficacy trials by K. Desai; M.-C.Boily; B. Masse; and RM Anderson Causal tree of disease transmission and the spreading ofinfectious diseases by A. Vazquez Structure of social contact networks and their impact onepidemics by S. Eubank; VS Anil Kumar; MV Marathe; A. Srinivasan; and N. Wang …,*,2007,1
Obliviously Approximating Sequence Distances,Graham Cormode; Cenk Sahinalp,Abstract There are several applications for schemes which approximately nd the distancebetween two sequences in a way that isoblivious' of one of the sequences up until a nalsublinear number of comparisons. This paper shows how sequences can be preprocessedobliviously to give a binary string; so that a simple vector distance between two bitstringsgives an approximation to a sequence distance of interest. Small constant factorapproximations are given for the Reversal; Transposition and Swap Distances; and alogarithmic factor approximation for Sequence Edit Distance which is analagous toLevenstein Edit Distance. By embedding these distances into vector distances (such as theHamming metric) existing methods can then be used for the Approximate Nearest Neighborsand Clustering problems on these vector distances to solve the corresponding problems …,*,2000,1
Topic Dependencies for Electronic Books,Graham Cormode,Abstract Electronics books consist of a number of topics; and information aboutdependencies between topics. We examine some theoretical problems related to handlingthese topic dependencies. In particular we consider the case where these topicdependencies are acyclic; and nondecomposable (that is; if topic a and b together require c;it is not necessarily the case that a or b alone require c). We formalise the semantics of thissystem; and give an axiomatisation which we show is sound and complete. We then showthat we can easily compute the closure of a set of all topics in this model; which is the set oftopics which are required by that set. This closure is computed in an identical manner to thatfor other formalisations of topic dependencies; showing that for this purpose no distinctionneed be drawn. We then consider a di erent kind of closure; where given a set of desired …,Unpublished manuscript,1999,1
An evaluation of multi-probe locality sensitive hashing for computing similarities over web-scale query logs,Graham Cormode; Anirban Dasgupta; Amit Goyal; Chi Hoon Lee,Many modern applications of AI such as web search; mobile browsing; image processing;and natural language processing rely on finding similar items from a large database ofcomplex objects. Due to the very large scale of data involved (eg; users' queries fromcommercial search engines); computing such near or nearest neighbors is a non-trivial task;as the computational cost grows significantly with the number of items. To address thischallenge; we adopt Locality Sensitive Hashing (aka; lsh) methods and evaluate fourvariants in a distributed computing environment (specifically; Hadoop). We identify severaloptimizations which improve performance; suitable for deployment in very large scalesettings. The experimental results demonstrate our variants of lsh achieve the robustperformance with better recall compared with “vanilla” lsh; even when using the same …,PloS one,2018,*
Learning Graphical Models from a Distributed Stream,Yu Zhang; Srikanta Tirthapura; Graham Cormode,Abstract: A current challenge for data management systems is to support the constructionand maintenance of machine learning models over data that is large; multi-dimensional; andevolving. While systems that could support these tasks are emerging; the need to scale todistributed; streaming data requires new models and algorithms. In this setting; as well ascomputational scalability and model accuracy; we also need to minimize the amount ofcommunication between distributed processors; which is the chief component of latency. Westudy Bayesian networks; the workhorse of graphical models; and present a communication-efficient method for continuously learning and maintaining a Bayesian network model overdata that is arriving as a distributed stream partitioned across multiple processors. We showa strategy for maintaining model parameters that leads to an exponential reduction in …,arXiv preprint arXiv:1710.02103,2017,*
Fast Sketch-based Recovery of Correlation Outliers,Graham Cormode; Jacques Dark,Abstract: Many data sources can be interpreted as time-series; and a key problem is toidentify which pairs out of a large collection of signals are highly correlated. We expect thatthere will be few; large; interesting correlations; while most signal pairs do not have anystrong correlation. We abstract this as the problem of identifying the highly correlated pairs ina collection of n mostly pairwise uncorrelated random variables; where observations of thevariables arrives as a stream. Dimensionality reduction can remove dependence on thenumber of observations; but further techniques are required to tame the quadratic (in n) costof a search through all possible pairs. We develop a new algorithm for rapidly finding largecorrelations based on sketch techniques with an added twist: we quickly generate sketchesof random combinations of signals; and use these in concert with ideas from coding …,arXiv preprint arXiv:1710.01985,2017,*
Constrained Differential Privacy for Count Data,Graham Cormode; Tejas Kulkarni; Divesh Srivastava,Abstract: Concern about how to aggregate sensitive user data without compromisingindividual privacy is a major barrier to greater availability of data. The model of differentialprivacy has emerged as an accepted model to release sensitive information while giving astatistical guarantee for privacy. Many different algorithms are possible to address differenttarget functions. We focus on the core problem of count queries; and seek to designmechanisms to release data associated with a group of n individuals. Prior work has focusedon designing mechanisms by raw optimization of a loss function; without regard to theconsequences on the results. This can leads to mechanisms with undesirable properties;such as never reporting some outputs (gaps); and overreporting others (spikes). We tamethese pathological behaviors by introducing a set of desirable properties that …,arXiv preprint arXiv:1710.00608,2017,*
What is Data Sketching; and Why Should I Care?,Graham Cormode,Do you ever feel overwhelmed by a constant stream of information? It can seem like there isa barrage of new email and text messages arriving; phone calls; articles to read; and knockson the door. Putting these pieces together to keep track of what's important can be a realchallenge. The same information overload is of concern in many computational settings. Forexample; telecommunications companies want to keep track of the activity on their network;to identify the overall network health; and spot anomalies or changes in behavior. Yet; thescale of events occurring is huge: many millions of network events per hour; per networkelement. And while new technologies allow the scale and granularity of events beingmonitored to increase by orders of magnitude; the capacity of computing elements to makesense of these (processors; memory and disks) is barely increasing. Even on a small …,*,2017,*
Data sketching,Graham Cormode,AFFPIXTURE a strong temptation just to ignore it entirely. A slightly more principledapproach is just to ignore most of it—that is; take a small number of examples from the fulldataset; perform the computation on this subset; and then try to extrapolate to the fulldataset. To give a good estimation; the examples must be randomly chosen. This is therealm of sampling. There are many variations of sampling; but this article uses the mostbasic: uniform random sampling. Consider a large collection of customer records. Randomlyselecting a small number of records provides the sample. Then various questions can beanswered accurately by looking only at the sample: for example; estimating what fraction ofcustomers live in a certain city or have bought a certain product. The method. To flesh thisout; let's fill in a few gaps. First; how big should the sample be to supply good answers?,Communications of the ACM,2017,*
A second look at counting triangles in graph streams (corrected),Graham Cormode; Hossein Jowhari,Abstract In this paper we present improved results on the problem of counting triangles inedge streamed graphs. For graphs with m edges and at least T triangles; we show that anextra look over the stream yields a two-pass streaming algorithm that uses O (m ε 2.5 Tpolylog (m)) space and outputs a (1+ ε) approximation of the number of triangles in thegraph. This improves upon the two-pass streaming tester of Braverman; Ostrovsky andVilenchik; ICALP 2013; which distinguishes between triangle-free graphs and graphs with atleast T triangle using O (m T 1/3) space. Also; in terms of dependence on T; we show thatmore passes would not lead to a better space bound. In other words; we prove there is noconstant pass streaming algorithm that distinguishes between triangle-free graphs fromgraphs with at least T triangles using O (m T 1/2+ ρ) space for any constant ρ≥ 0.,Theoretical Computer Science,2017,*
Independent Set Size Approximation in Graph Streams,Graham Cormode; Jacques Dark; Christian Konrad,Abstract: We study the problem of estimating the size of independent sets in a graph $ G $defined by a stream of edges. Our approach relies on the Caro-Wei bound; which expressesthe desired quantity in terms of a sum over nodes of the reciprocal of their degrees; denotedby $\beta (G) $. Our results show that $\beta (G) $ can be approximated accurately; basedon a provided lower bound on $\beta $. Stronger results are possible when the edges arepromised to arrive grouped by an incident node. In this setting; we obtain a value that is atmost a logarithmic factor below the true value of $\beta $ and no more than the trueindependent set size. To justify the form of this bound; we also show an $\Omega (n/\beta) $lower bound on any algorithm that approximates $\beta $ up to a constant factor. Subjects:Data Structures and Algorithms (cs. DS) Cite as: arXiv: 1702.08299 [cs. DS](or arXiv …,arXiv preprint arXiv:1702.08299,2017,*
Engineering Streaming Algorithms,Graham Cormode,Abstract Streaming algorithms must process a large quantity of small updates quickly toallow queries about the input to be answered from a small summary. Initial work onstreaming algorithms laid out theoretical results; and subsequent efforts have involvedengineering these for practical use. Informed by experiments; streaming algorithms havebeen widely implemented and used in practice. This talk will survey this line of work; andidentify some lessons learned.,LIPIcs-Leibniz International Proceedings in Informatics,2017,*
Corrigendum to “A second look at counting triangles in graph streams”[Theoret. Comput. Sci. 552 (2014) 44–51],Graham Cormode; Hossein Jowhari,*,Theoretical Computer Science,2017,*
Robust Lower Bounds for Communication and Stream Computation.,Amit Chakrabarti; Graham Cormode; Andrew McGregor,Abstract: We study the communication complexity of evaluating functions when the inputdata is randomly allocated (according to some known distribution) amongst two or moreplayers; possibly with information overlap. This naturally extends previously studied variablepartition models such as the best-case and worst-case partition models. We aim tounderstand whether the hardness of a communication problem holds for almost everyallocation of the input; as opposed to holding for perhaps just a few atypical partitions. A keyapplication is to the heavily studied data stream model. There is a strong connectionbetween our communication lower bounds and lower bounds in the data stream model thatare “robust” to the ordering of the data. That is; we prove lower bounds for when the order ofthe items in the stream is chosen not adversarially but rather uniformly (or near-uniformly) …,Theory of Computing,2016,*
Join Sizes; Frequency Moments; and Applications,Graham Cormode; Minos Garofalakis,Abstract We focus on a set of problems chiefly inspired by the problem of estimating the sizeof the (equi-) join between two relational data streams. This problem is at the heart of a widevariety of other problems; both in databases/data streams and beyond; includingapproximating range-query aggregates; quantiles; and heavy-hitter elements; and buildingapproximate histograms and wavelet representations. Our discussion focuses on efficient;sketch-based streaming algorithms for join-size and self-join-size estimation problems;based on the influential papers of Alon; Matias; Gibbons; and Szegedy.,*,2016,*
AMS Sketch,Graham Cormode,The Abelian hidden subgroup problem is the problem of finding generators for a subgroup Kof an Abelian group G; where this subgroup is defined implicitly by a function f WG! X; forsome finite set X. In particular; f has the property that f. v/D f. w/if and only if the cosets (weare assuming additive notation for the group operation here.) vCK and wCK are equal. Inother words; f is constant on the cosets of the subgroup K and distinct on each coset. It isassumed that the group G is finitely generated and that the elements of G and X have uniquebinary encodings. The binary assumption is only for convenience; but it is important to haveunique encodings (eg; in [22] Watrous uses a quantum state as the unique encoding ofgroup elements). When using variables g and h (possibly with subscripts); multiplicativenotation is used for the group operations. Variables x and y (possibly with subscripts) will …,Encyclopedia of Algorithms,2016,*
Sampling from distributed streams of data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for sampling from distributed data streams. Data elements are received at site serversconfigured to collect and report data to a coordinator device. The site servers assign a binarystring to each of the data elements. Each bit of the binary strings can be independently set toa 0 or a 1 with a probability of one half. The binary string is used to sample from the receiveddata elements; and the data elements and/or the sampled data elements can be transmittedto a coordinator device. The coordinator device can examine one or more bits of the binarystring to draw samples of the received data elements in accordance with desiredprobabilities.,*,2015,*
Methods and apparatus to sample data connections,*,Methods; apparatus; and articles of manufacture are disclosed to sample signed weightedupdates. Example methods disclosed herein include identifying a first key associated with adata update obtained by sampling a stream of data received from a plurality of nodes in anetwork; the first key being representative of a first node in the plurality of nodes; adjusting afirst value associated with the first key based on a weight associated with the data update;the weight being positive when the data update corresponds to opening of a new dataconnection with the first node; the weight being negative when the data update correspondsto closing of an existing data connection with the first node; and estimating a status of thefirst node based on the first value.,*,2015,*
Streaming Methods in Data Analysis,Graham Cormode,Abstract A fundamental challenge in processing the massive quantities of informationgenerated by modern applications is in extracting suitable representations of the data thatcan be stored; manipulated and interrogated on a single machine. A promising approach isin the design and analysis of compact summaries: data structures which capture keyfeatures of the data; and which can be created effectively over distributed; streaming data.Popular summary structures include the count distinct algorithms; which compactlyapproximate item set cardinalities; and sketches which allow vector norms and products tobe estimated. These are very attractive; since they can be computed in parallel andcombined to yield a single; compact summary of the data. This talk introduces the conceptsand examples of compact summaries.,British International Conference on Databases,2015,*
Compact Summaries over Large Datasets,Graham Cormode,Abstract A fundamental challenge in processing the massive quantities of informationgenerated by modern applications is in extracting suitable representations of the data thatcan be stored; manipulated and interrogated on a single machine. A promising approach isin the design and analysis of compact summaries: data structures which capture keyfeatures of the data; and which can be created effectively over distributed data sets. Popularsummary structures include the count distinct algorithms; which compactly approximate itemset cardinalities; and sketches which allow vector norms and products to be estimated.These are very attractive; since they can be computed in parallel and combined to yield asingle; compact summary of the data. This tutorial introduces the concepts and examples ofcompact summaries.,Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2015,*
Computing time-decayed aggregates in data streams,*,Aggregates are calculated from a data stream in which data is sent in a sequence of tuples;in which each tuple comprises an item identifier and a timestamp indicating when the tuplewas transmitted. The tuples may arrive out-of-order; that is; the sequence in which the tuplesarrive are not necessarily in the sequence of their corresponding timestamps. In calculatingaggregates; more recent data may be given more weight by multiplying each tuple by adecay function which is a function of the timestamp associated with the tuple and the currenttime. The tuples are recorded in a quantile-digest data structure. Aggregates are calculatedfrom the data stored in the quantile-digest data structure.,*,2014,*
Automatic gleaning of semantic information in social networks,*,Disclosed are method and apparatus for identifying members of a social network who havea high likelihood of providing a useful response to a query. A query engine examines thepersonal pages of a set of members and automatically gleans semantic information relevantto the query. From the automatically-gleaned semantic information; a score indicative of thelikelihood that the member may provide a useful response is calculated.,*,2014,*
Verification of data stream computations using third-party-supplied annotations,*,A third party that performs data stream computation is requested to return not only thesolution to the computation; but also “annotations” to the original data stream. Theannotations are then used by the data owner (in actuality; a “verifier” associated with thedata owner) to check the results of the third party's computations. As implemented; theverifier combines the annotations with the original data; performs some computations; and isthen assured of the correctness of the provided solution. The cost of verification issignificantly lower to the data owner than the cost of fully processing the data “in house”.,*,2014,*
Streams; Sketching and Big Data–References,Graham Cormode,• Survey of sketching algorithms: G. Cormode. Sketch techniques for massive data. In G.Cormode; M. Garofalakis; P. Haas; and C. Jermaine; editors; Synposes for Massive Data:Samples; Histograms; Wavelets and Sketches; Foundations and Trends in Databases. NOWpublishers; 2012 … • Textbook on randomized algorithms: R. Motwani and P. Raghavan. RandomizedAlgo- rithms. Cambridge University Press; 1995 … • Textbook on randomized algorithms: M.Mitzenmacher and E. Upfal. Probability and Computing: Randomized Algorithms and ProbabilisticAnalysis. Cambridge University Press; 2005 … • Count-Min sketch: G. Cormode and S.Muthukrishnan. An improved data stream sum- mary: The Count-Min sketch and itsapplications. Journal of Algorithms; 55(1):58–75; 2005 … • Count sketch: M. Charikar; K.Chen; and M. Farach-Colton. Finding frequent items in data streams. In Procedings of the …,*,2014,*
Modeling Collaboration in Academia: A Game Theoretic Approach,Graham Cormode; Qiang Ma; S Muthukrishnan; Brian Thompson,Abstract: In this work; we aim to understand the mechanisms driving academic collaboration.We begin by building a model for how researchers split their effort between multiple papers;and how collaboration affects the number of citations a paper receives; supported byobservations from a large real-world publication and citation dataset; which we call the h-Reinvestment model. Using tools from the field of Game Theory; we study researchers'collaborative behavior over time under this model; with the premise that each researcherwants to maximize his or her academic success. We find analytically that there is a strongincentive to collaborate rather than work in isolation; and that studying collaborativebehavior through a game-theoretic lens is a promising approach to help us betterunderstand the nature and dynamics of academic collaboration.,arXiv preprint arXiv:1407.2220,2014,*
Validation of priority queue processing,*,A method for validating outsourced processing of a priority queue includes configuring averifier for independent; single-pass processing of priority queue operations that includeinsertion operations and extraction operations and priorities associated with each operation.The verifier may be configured to validate N operations using a memory space having a sizethat is proportional to the square root of N using an algorithm to buffer the operations as aseries of R epochs. Extractions associated with each individual epoch may be monitoredusing arrays Y and Z. Insertions for the epoch k may monitored using arrays X and Z. Theprocessing of the priority queue operations may be verified based on the equality orinequality of the arrays X; Y; and Z. Hashed values for the arrays may be used to test theirequality to conserve storage requirements.,*,2013,*
First author advantage: citation labeling in research,Graham Cormode; S Muthukrishnan; Jinyun Yan,Abstract Citations among research papers; and the networks they form; are the primaryobject of study in scientometrics. The act of making a citation reflects the citer's knowledge ofthe related literature; and of the work being cited. We aim to gain insight into this process bystudying citation keys: user-chosen labels to identify a cited work. Our main observation isthat the first listed author is disproportionately represented in such labels; implying a strongmental bias towards the first author.,Proceedings of the 2013 workshop on Computational scientometrics: theory & applications,2013,*
UMicS: from anonymized data to usable microdata,Graham Cormode; Entong Shen; Xi Gong; Ting Yu; Cecilia M Procopiuc; Divesh Srivastava,Abstract There is currently a tug-of-war going on surrounding data releases. On one side;there are many strong reasons pulling to release data to other parties: business factors;freedom of information rules; and scientific sharing agreements. On the other side; concernsabout individual privacy pull back; and seek to limit releases. Privacy technologies such asdifferential privacy have been proposed to resolve this deadlock; and there has been muchstudy of how to perform private data release of data in various forms. The focus of suchworks has been largely on the data owner: what process should they apply to ensure thatthe released data preserves privacy whilst still capturing the input data distributionaccurately. Almost no attention has been paid to the needs of the data user; who wants tomake use of the released data within their existing suite of tools and data. The difficulty of …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,*
Staff,Divesh Srivastava,For close to 50 years; Neil J. Sloane has been collecting and cataloguing integersequences. The result is The Online Encyclopedia of Integer Sequences (OEIS); acomprehensive; constantly updated repository of 200;000+ sequences and one of the firstlarge-scale exercises in crowd-sourcing to expand mathematical and scientificinformation.The OEIS; for years hosted on AT&T Research servers; is now under thestewardship of The OEIS Foundation; which is charged with maintaining the OEIS as a freeand open resource to the world community. Read more,Journal of Computational Methods in Science and Engineering,2013,*
Sampling from distributed streams of data,*,The present disclosure is directed to systems; methods; and computer-readable storagemedia for sampling from distributed data streams. Data elements are received at site serversconfigured to collect and report data to a coordinator device. The site servers assign a binarystring to each of the data elements. Each bit of the binary strings can be independently set toa 0 or a 1 with a probability of one half. The binary string is used to sample from the receiveddata elements; and the data elements and/or the sampled data elements can be transmittedto a coordinator device. The coordinator device can examine one or more bits of the binarystring to draw samples of the received data elements in accordance with desiredprobabilities.,*,2013,*
Studying the source code of scientific research,Graham Cormode; S Muthukrishnan; Jinyun Yan,Abstract Just as inspecting the source code of programs tells us a lot about the process ofprogramming; inspecting the" source code" of scientific papers informs on the process ofscientific writing. We report on our study of the source of tens of thousands of papers fromComputer Science and Mathematics.,ACM SIGKDD Explorations Newsletter,2013,*
Development of a Test System for In-Process Form Profile Measurement,Yongsheng Gao; Ruipeng Li; Jiaxi Wang,Development of a Test System for In-Process Form Profile Measurement.,The 11th International Symposium of Measurement Technology and Intelligent Instruments; RWTH Aachen University; Aachen; Germany,2013,*
Database Systems and Theory-Article 10 (25 pages)-Continuous Sampling from Distributed Streams,G Cormode; S Muthukrishnan; K Yi; Q Zhang,*,Journal of the ACM-Association for ComputingMachinery,2012,*
Mergeable Coresets,Pankaj K Agarwal; Graham Cormode; Zengfeng Haung; Jeff M Phillips; Zhewei Wei; Ke Yi,Abstract We study the mergeability of data summaries. Informally speaking; mergeabilityrequires that; given two summaries on two data sets; there is a way to merge the twosummaries into a summary on the two data sets combined together; while preserving theerror and size guarantees. This property means that the summary can be treated like otheralgebraic objects such as sum and max; which is especially useful for computing summarieson massive distributed data. Many data summaries are trivially mergeable by construction;most notably those based on linear transformations. But some other fundamental ones likethose for heavy hitters and quantiles; are not (known to be) mergeable. In this paper; wedemonstrate that these summaries are indeed mergeable or can be made mergeable afterappropriate modifications. Specifically; we show that for ε-approximate heavy hitters …,Third Workshop on Massive Data Algorithmics (MASSIVE),2011,*
A transcriptional regulator NIF-1 regulates neurite outgrowth and undergoes p35-dependent nuclear export,Xiao Su Zhao; Ada WY Fu; Winnie WY Chien; Zhen Li; Amy KY Fu; Nancy Y Ip,A transcriptional regulator NIF-1 regulates neurite outgrowthand undergoes p35-dependent nuclear export.,The Gordon Research Conference on Molecular and Cellular Neurobiology; Hong Kong,2008,*
Misra-Gries Summaries,Graham Cormode,The frequent items problem is to process a stream of items and find all items occurring morethan a given fraction of the time. It is one of the most heavily studied problems in data streamalgorithms; dating back to the 1980s. Many applications rely directly or indirectly on findingthe frequent items; and implementations are in use in large-scale industrial systems.Informally; given a sequence of items; the problem is simply to find those items which occurmost frequently. Typically; this is formalized as finding all items whose frequency exceeds aspecified fraction of the total number of items. Variations arise when the items have weightsand further when these weights can also be negative.,*,2008,*
Continuous Distributed Stream Querying using Sketches,Graham Cormode; Minos Garofalakis,While traditional database systems optimize for performance on one-shot query processing;emerging largescale monitoring applications require continuous tracking of complex data-analysis queries over collections of physically-distributed streams. Thus; effective solutionshave to be simultaneously space/time efficient (at each remote monitor site); communicationefficient (across the underlying communication network); and provide continuous;guaranteed-quality approximate query answers. In this paper; we propose novel algorithmicsolutions for the problem of continuously tracking a broad class of complex aggregatequeries in such a distributed-streams setting. Our tracking schemes maintain approximatequery answers with provable error guarantees; while simultaneously optimizing the storagespace and processing time at each remote site; and the communication cost across the …,*,2008,*
Finding hierarchical heavy hitters...,Graham Cormode; Flip Korn; S Muthukrishnan; Divesh Srivastava,Abstract Data items that arrive online as streams typically have attributes which take valuesfrom one or more hierarchies (time and geographic location; source and destination IPaddresses; etc.). Providing an aggregate view of such data is important for summarization;visualization; and analysis. We develop an aggregate view based on certain organized setsof large-valued regions (“heavy hitters”) corresponding to hierarchically discountedfrequency counts. We formally define the notion of Hierarchical Heavy Hitters (HHHs). Wefirst consider computing (approximate) HHHs over a data stream drawn from a singlehierarchical attribute. We formalize the problem and give deterministic algorithms to findthem in a single pass over the input. In order to analyze a wider range of realistic datastreams (eg; from IP traffic monitoring applications); we generalize this problem to …,*,2007,*
Computer Thanks Its Expert Reviewers,Bob Ward,In this last issue of Computer in 2006; I would like to extend my gratitude to the dedicatedreview-ers whose efforts make possible each issue of Computer. Computer relies on andhighly values the peer review process and the reviewers who are at its very core. Ourreviewers contribute by reviewing papers as assigned in their technical area of expertise.They often comment on organization and clarity; questions of accuracy; disputed definitions;and the effectiveness of visual aids; figures; or other ancillary materials. Reviewers forComputer work under the direction of Associate Editors in Chief Kathleen Swigger and BillSchilit. Kathy and Bill have contributed extensively to Computer and to the Computer Societythroughout 2006. Their expertise and guidance; combined with their commitment; are criticalcomponents of our review process.,Computer,2006,*
Discrete Methods in Epidemiology: This DIMACS Volume Presents the Proceedings from the DIMACS Workshop on Data Mining and Epidemiology Held at the DIM...,Graham Cormode; James Abello,*,*,2006,*
Selected Data Mining Concepts,James Abello; Graham Cormode; Dmitriy Fradkin; David Madigan; Ofer Melnik; I Muchnik,*,DIMACS SERIES IN DISCRETE MATHEMATICS AND THEORETICAL COMPUTER SCIENCE,2006,*
An evaluation of the edit-distance-with-moves similarity metric for comparing genetic sequences,Shiri Azenkot; Tzu-Yi Chen; Graham Cormode,ABSTRACT We describe the first known implementation of an approximation algorithm forthe string edit distance with moves similarity metric. This is the first algorithm to considernontrivial alignment and run in substantially sub-quadratic time [2]. Extensiveexperimentation demonstrates that the algorithm produces a good approximation for the editdistance with moves; especially on strings of length 500 B to 10 KB. We also found that thealgorithm has high potential for use in computational biology. When comparing texts ofgenetic sequences; our algorithm outperforms the q-grams heuristic in predicting results ofthe Smith-Waterman algorithm. Finally; we propose additional application areas for ourimplementation.,*,2005,*
Ashish Goel; Monika R. Henzinger; Serge Plotkin. An online throughput-competitive,Michael Filaseta; Douglas B Meade; Petra Šparl; Janez Žerovnik; Yoo-Ah Kim; Graham Cormode; S Muthukrishnan; Reuven Bar-Yehuda; Guy Even; Shimon Moni Shahar; Moni Naor,*,Journal of Algorithms,2005,*
Goel; Ashish,Micah Adler; William A Aiello; Reuven Bar-Yehuda; Yair Bartal; Graham Cormode; Alberto Del Lungo; Guy Even; Michael Filaseta; Monika R Henzinger; Yoo-Ah Kim; Benoit Larose; Cynthia Loten; Guy Louchard; Yishay Mansour; Claudio Marini; Douglas B Meade; Manor Mendel; Franco Montagna; Moni Naor; Serge Plotkin; S Rajagopolan; Adi Rosén; Shimon Moni Shahar; Petra Šparl; László Zádori; Janez Žerovnik,*,Journal of Algorithms,2005,*
Report on DIMACS Working Group Meeting: Data Mining and Epidemiology,James Abello; Graham Cormode,Abstract Epidemiology is an observational science that concerns itself with finding andexplaining patterns of health and disease in populations; usually of humans; but alsopopulations of animals; insects and plants. Data mining is an active area of researchinterested in finding algorithms for describing latent patterns in often very large data sets.This Working Group had the objective of fostering collaboration between these twodisciplines. In March of 2004 it organized a twoday meeting at DIMACS to bring these twogroups together in a format designed to initiate such collaborations.,*,2004,*
Semantics of Ranking Queries for Probabilistic Data,Jeffrey Jestes; Graham Cormode; Feifei Li; Ke Yi,Abstract—Recently; there have been several attempts to propose definitions and algorithmsfor ranking queries on probabilistic data. However; these lack many intuitive properties of atop-k over deterministic data. We define numerous fundamental properties; including exact-k; containment; unique-rank; value-invariance; and stability; which are satisfied by rankingqueries on certain data. We argue these properties should also be carefully studied indefining ranking queries in probabilistic data; and fulfilled by definition for ranking uncertaindata for most applications. We propose an intuitive new ranking definition based on theobservation that the ranks of a tuple across all possible worlds represent a well-foundedrank distribution. We studied the ranking definitions based on the expectation; the medianand other statistics of this rank distribution for a tuple and derived the expected rank …,IEEE Transactions on Communications,1991,*
warwick. ac. uk/lib-publications,Kook-Jin Ahn; Graham Cormode; Sudipto Guha; Andrew McGregor; Anthony Ian Wirth,Abstract In this paper; we address the problem of correlation clustering in the dynamic datastream model. The stream consists of updates to the edge weights of a graph on n nodesand the goal is to find a node-partition such that the end-points of negative-weight edges aretypically in different clusters whereas the end-points of positive-weight edges are typically inthe same cluster. We present polynomial-time; O (n· polylog n)-space approximationalgorithms for natural problems that arise. We first develop data structures based on linearsketches that allow the “quality” of a given nodepartition to be measured. We then combinethese data structures with convex programming and sampling techniques to solve therelevant approximation problem. However the standard LP and SDP formulations are notobviously solvable in O (n· polylog n)-space. Our work presents spaceefficient algorithms …,*,*,*
Deterministic Distributed and Streaming Algorithms for Linear Algebra Problems,Graham Cormode; Charlie Dickens; David P Woodruff,Problem: Find εbp additive error approximation to minx∈ Rd Ax− b∞. Idea: Store all rows inA of high leverage and set all others to zero; call this A. Keep b on these indices; otherwiseset to zero; call this b. Solve the l∞-regression on A and b. This provides a goodapproximation (theoretically and empirically) as we can prove low leverage rows which aredropped do not contribute much to the l∞ cost.,*,*,*
Participant List,James Abello; Sanjeev Arora; Jean-Francois Baget; Tugkan Batu; Michael A Bender; William Brinkman; Adam L Buchsbaum; Amit Chakrabarti; Bernard Chazelle; Jie Chen; Graham Cormode; Artur Czumaj; Sanjoy Dasgupta; Ted Diament; Giovanni DiCrescenzo; Yevgeniy Dodis; Petros S Drineas; William H DuMouchel; Edith Elkind; Funda Ergun; Joan Feigenbaum; Eldar Fischer; Jessica H Fong; Lance Fortnow; Nicola Galesi; Loukas Georgiadis; Phillip B Gibbons; Oded Goldreich; Rehovot Israel; Shafi Goldwasser; Gary Gordon; Navin Goyal; Sudipto Guha; Joel Hass; Johan Hastad; Piotr Indyk; Yuval Ishai; Ravi Kannan; Sampath Kannan; George Karakostas; Marek Karpinski; Carl Kingsford; Marcos A Kiwi; Michal Koucky; Michael Krivelevich; Ravi Kumar; Ding Liu; Satyanarayana V Lokan; Frederic Magniez; Elizabeth McMahon; Silvio Micali; Nina Mishra Fox; Moni Naor; Liadan Ita O'Callaghan; Francis Opoku; Elena Oranskaya; Rafail Ostrovsky; Michal Parnas; Benny Pinkas; Manoj Malathi Prabhakaran; Yuval Rabani; Sridhar Rajagopalan; Sofya Raskhodnikova; Omer Reingold; Dana Ron; Ronitt Rubinfeld; Amit Sahai; Cenk Sahinalp; Michael Saks; Alex Samorodnitsky; Christian Sohler; Paderborn Germany; Venkatesh Srinivasan; Daniel Stefankovic; Martin J Strauss; Khot Subhash; Xiaodong Sun; Srikanta Tirthapura; Andrew Tomkins; Iannis Tourlakis; Luca Trevisan; Soda Hall; Kwadwo Twumasi-Ampofo; Salil Vadhan; Dieter Van Melkebeek; Rytchkov Viatcheslav; Taso Viglas; Mahesh Viswanathan; Patrick E White; Avi Wigderson; Anthony I Wirth; Beini Zhou,Participant List James Abello AT&T Research Shannon Laboratories Information Visualization180 Park Avenue Florham Park; NJ 07932 973-360-8649 abello@research.att.com SanjeevArora Princeton University Computer Science 35 Olden Street Princeton; NJ 08544609-258-3869 arora@cs.princeton.edu Jean-Francois Baget LIRMM 161; rue Ada 34000Montpellier France (33) 4 67 41 85 79 baget@lirmm.fr Tugkan Batu Cornell University 4104 UpsonHall Ithaca; NY 14850 607-255-9537 batu@cs.cornell.edu Michael A. Bender SUNY Stony BrookDepartment of Computer Science Stony Brook; NY 11794-4400 631-632-7835 bender@cs.sunysb.edu William Brinkman Princeton University The Graduate College 88 College Road WestPrinceton; NJ 08544 609-986-9049 brinkman@cs.princeton.edu Adam L. Buchsbaum AT&TLabs-Research 180 Park Avenue Florham Park; NJ 07932 973-360-8674 alb …,Computer Science,*,*
Streaming Graph Computations with a Helpful Advisor,Justin Thaler; Graham Cormode; Michael Mitzenmacher, A few slides borrowed from IITK Workshop on Algorithms …  Stream: m elements from universeof size n …  eg; S=<x 1 ; x 2 ; ... ; x m > = 3;5;3;7;5;4;8;7;5;4;8;6;3;2 … • Goal: Computea function of stream; eg; median; number of … (i) Limited working memory; ie;sublinear(n;m) … (ii) Sequential access to adversarially ordered data …  S = <x 1 ; x 2 ;…; x m >; x i ∈[n] x [n] …  A defines a graph G on n vertices …  Eg Ω(n) space needed forconnectivity …  Stream Punctuation [Tucker et al. 05]; Proof Infused Streams … [Li et al.07]; Stream Outsourcing [Yi et al. 08]; Best-Order … Model [Das Sarma et al. 09] (is a specialcase of our model) …  Stream Punctuation [Tucker et al. 05]; Proof Infused Streams … [Li etal. 07]; Stream Outsourcing [Yi et al. 08]; Best-Order … Model [Das Sarma et al. 09] (is a specialcase of our model) …  [Chakrabarti et al. 09] Online Annotation Model: Give,*,*,*
Tracking Inverse Distributions of Massive Data Streams,Graham Cormode,Page 1. Tracking Inverse Distributions of Massive Data Streams Graham Cormodecormode@bell-labs.com Page 2. Network Monitoring Today's converged networks bring manynew challenges for monitoring Massive scale of data and connections No centralized control;inability to police what is connected Attacks; malicious usage; malware; misconfigurations… Noper-connection records or infrastructure IMS Network CSCF HSS AS Cable; DSL PSTN EnterpriseWireless RNC PSTN Page 3. Scale of Data • IP Network Traffic: up to 1 Billion packets per hourper router. Each ISP has many (hundreds) of routers • Scientific data: NASA's observation satelliteseach generate billions of readings per day. • Compare to "human scale" data: “only” 1 billionworldwide credit card transactions per month. “Only” 3 Billion Telephone Calls in US each day“Only” 30 Billion emails daily; 1 Billion SMS; IMs. CC trans US Phone Satellite …,*,*,*
Cluster and Data Stream Analysis,Graham Cormode,Page 1. Cluster and Data Stream Analysis Graham Cormode cormode@bell-labs.com Page2. 2 Outline ▪ Cluster Analysis – Clustering Issues – Clustering algorithms: HierarchicalAgglomerative Clustering; K-means; Expectation Maximization; Gonzalez approximation forK-center ▪ Data Stream Analysis – Massive Data Scenarios – Distance Estimates for HighDimensional Data: Count-Min Sketch for L∞; AMS sketch for L 2 ; Stable sketches for L p ;Experiments on tabular data – Too many data points to store: Doubling Algorithm for k-centerclustering; Hierarchical Algorithm for k-median; Grid algorithms for k-median ▪ Conclusion andSummary Page 3. 3 1. Cluster Analysis Page 4. 4 An Early Application of Clustering John Snowplotted the location of cholera cases on a map during an outbreak in the summer of 1854. Hishypothesis was that the disease was carried in water; so he plotted location of cases and water …,*,*,*
Tracking Frequent Items Dynamically:” What’s Hot and What’s Not”,Graham Cormode; S Muthukrishnan,Background: A does not believe B is telling the truth; so A sets a trap. A: Did you do the onewe always called the" Hell Paper". You know the one; where we prove P= NP? B: I did that! Iproved P= NP! I placed near the top of the class; and the professor used my paper as anexample!,*,*,*
198: 671 Processing Massive Data Sets,S Muthu Muthukrishnan; Graham Cormode,• Webonym; webmorphism.• Some questions that arose (badri asked them?) œ How tocollect email info; cutting across IP network layers? Routinely done in IP business. 100%inter-ISP email is SMTP which can be logged. Imap; pop3;… œ Credit card transactions inUS. Visa did apparently 6.2 billion transactions last year in US. What is the number ofpackets sent by a 1Gb/s link in one hour assuming average packet size is 40 bytes? œ Dowe have to look at large datasets for streaming algorithms to be interesting? NO: databasemonitoring.,*,*,*
7UDFNLQJ'LVWULEXWHG $ JJUHJDWHV RYHU 7LPH EDVHG 6OLGLQJ: LQGRZV,Graham Cormode; Ke Yi,Page 1. %ULHI $QQRXQFHPHQW 7UDFNLQJ 'LVWULEXWHG $JJUHJDWHV RYHU7LPH EDVHG 6OLGLQJ :LQGRZV Graham Cormode AT&T Labs Ke Yi HKUST Page 2.&RQWLQXRXV 'LVWULEXWHG 0RGHO k sites local stream(s) seen at each site d ^ ^ 2 ■K ■ ^ ■ ' – ε ^ ^ Page 3. 3UREOHPV LQ 'LVWULEXWHG 0RQLWRULQJ ■ D d ^ ■ d –Y [C; Garofalakis; Muthukrishnan; Rastogi 05] – [Arackaparambil Brody Chakrabarti 09] –& D [C; Muthukrishnan; Yi 08] – & D [C; Muthukrishnan; Yi 08] – ' [Sharman; Schuster; Keren06] ■ d – ^ [C; Muthukrishnan; Yi; Zhang 10] – [Chan Lam Lee Ting 10] ■ d 3 Page 4.)RUZDUG EDFNZDUG IUDPHZRUN ■ < Current window Departing Arriving T 2T 3T 4T ■ <– – – K – K ε ε K ε ε – ; K ε ε K ε ε – Y K ε ε ε K ε ε ε 4,*,*,*
Lightweight Query Authentication on Streams,Graham Cormode; ANTONIOS DELIGIANNAKIS; MINOS GAROFALAKIS,We consider a stream outsourcing setting; where a data owner delegates the managementof a set of disjoint data streams to an untrusted server. The owner authenticates his streamsvia signatures. The server processes continuous queries on the union of the streams forclients trusted by the owner. Along with the results; the server sends proofs of resultcorrectness derived from the owner's signatures; which are verifiable by the clients. Wedesign novel constructions for a collection of fundamental problems over streamsrepresented as linear algebraic queries. In particular; our basic schemes authenticatedynamic vector sums; matrix products; and dot products. These techniques can be adaptedfor authenticating a wide range of important operations in streaming environments; includinggroup-by queries; joins; in-network aggregation; similarity matching; and event …,*,*,*
Anonymization and Uncertainty in Social Network Data,Graham Cormode,1. Anonymize the title page 2. Remove mention of funding sources and personalacknowledgments 3. Anonymize references found in running prose that cite your papers 4.Anonymize citations of submitted work in the bibliography 5. Ambiguate statements onsystems that identify an author 6. Name your files with care; document properties are alsoanonymized,*,*,*
Space-optimal Heavy Hitters with Strong Error Bounds,Graham Cormode,♦ The Frequent Items Problem (aka Heavy Hitters): given stream of N items; find those thatoccur most frequently♦ Eg Find all items occurring more than 1% of the time♦ Formally “hard”in small space; so allow approximation♦ Find all items with count≥ φN; none with count<(φ−ε) N,*,*,*
Springs and Sound Layouts,Graham Cormode,The problem of graph drawing is of relevance to computer science for a number of reasons.It touches on many aspects of the subject; from algorithms and complexity; through graphicsand data visualisation to less obviously related areas; such as VLSI. As well as being aproblem encountered whenever there is graph data that needs to be displayed; it is closelyrelated to the problems of circuit layout and map labelling. With the number of variations andapproaches to the problem; it is perhaps no wonder that there is an entire annualsymposium dedicated to graph drawing [GD98]. The problem of graph drawing can besummarised as “given a vertex list; V; and edge list; E; produce an aesthetic layout”. Such abrief description hides a great deal of detail; notably in the word “aesthetic”; which is notoften found used in relation to Computer Science. Some aspects can be dealt with …,*,*,*
Program Committees,Sihem Amer Yahia; Kevin Beyer; CWI Peter Boncz; Netherlands Angela Bonifati; Arbee Chen; Jan Chomicki; Bobbie Cochrane; Latha Colby; Ada Fu; Sumit Ganguly; Torsten Grust; Raghav Kaushik; Arnd Christian Konig; Sailesh Krishnamurthy; Amalgamated Insight; David Lomet; Christopher Olston; Fatma Ozcan; Neoklis Polyzotis; Raghu Ramakrishnan; Krithi Ramamritham; Vijayshankar Raman; Rajeev Rastogi; Mary Roth; Jerome Simeon; Ioana Stanoi; Andrew Tomkins; AUEB Vasilis Vassalos; Greece Victor Vianu; Min Wang; Aidong Zhang,Foto Afrati; NTUA; Greece Natassa Ailamaki; CMU; USA Sihem Amer Yahia; YahooResearch; USA Paolo Atzeni; Univ. di Roma Tre; Italy Shivnath Babu; Duke Univ.; USA JamesA. Bailey; Univ. of Melbourne; Australia Magdalena Balazinska; Univ. of Washington; USA KevinBeyer; IBM Research; USA Michael Boehlen; Univ. of Bolzano; Italy Peter Boncz; CWI; NetherlandsAngela Bonifati; CNR; Italy Arbee Chen; National Tsing Hua Univ.; Taiwan Mitch Cherniack;Brandeis Univ.; USA Jan Chomicki; SUNY Buffalo; USA Stavros Christodoulakis; Tech Univ.of Crete; Greece Bobbie Cochrane; IBM Research; USA Edith Cohen; AT&T Labs; USA LathaColby; IBM Research; USA Graham Cormode; AT&T Labs; USA Abhinandan Das; GoogleLabs; USA Amol Deshpande; Univ. of Maryland; USA Alin Dobra; Univ. of Florida; USA ElenaFerrari; Univ. of Insubria; Italy J. Christoph Freytag; Humboldt Univ.; Germany Ada Fu …,*,*,*
First International Workshop on Big Dynamic Distributed Data (BD3),Graham Cormode; Ke Yi; Antonios Deligiannakis Minos Garofalakis,As the amount of streaming data produced by large-scale systems such as environmentalmonitoring; scientific experiments and communication networks grows rapidly; newapproaches are needed to effectively process and analyze such data. There are severalpromising directions in the area of large-scale distributed computation; that is; wheremultiple computing entities work together over partitions of the massive; streaming data toperform complex computations. Two important paradigms in this realm are continuousdistributed monitoring (ie; continually maintaining an accurate estimate of a complex query);and distributed and cluster-based systems that allow the processing of big; streaming data(eg; IBM System S; Apache S4; and Twitter Storm). The aim of the BD3 workshop is to bringtogether computer scientists with interests in this field to present recent innovations; find …,*,*,*
Q uerying and Tracking D istributed D ata Streams,Minos Garofalakis; Graham Cormode,Page 1. 1 Streaming in a Connected World: Q uerying and Tracking D istributed D ata Streams MinosGarofalakis Yahoo! Research & UC Berkeley minos@acm.org 1;1 f s;1 f 1;k f s;k f sf 1f local updatestreams local update streams Site 1 Site k State−Update Coordinator Global Streams ApproximateAnswer User Query Q(fi; fj; ...) for Q(fi; fj; ...) Messages Graham Cormode AT&T Labs - Researchgraham@research.att.com Streaming in a Connected World — Cormode & Garofalakis 2 Streams –A Brave N ew World ∎ Traditional DBMS: data stored in finite; persistent data sets ∎ Data Streams:distributed; continuous; unbounded; rapid; time varying; noisy; . . . ∎ Data-Stream Management:variety of modern applications – Network monitoring …,*,*,*
Continuous Distributed Monitoring,Graham Cormode,There are many scenarios where we need to track events:∎ Network health monitoringwithin a large ISP∎ Collecting and monitoring environmental data with sensors∎ Observingusage and abuse of distributed data centers All can be abstracted as a collection ofobservers who want to All can be abstracted as a collection of observers who want tocollaborate to compute a function of their observations,*,*,*
Introduction to Distributed Data Streams,Graham Cormode,Page 1. Introduction to Distributed Data Streams Graham Cormode graham@research.att.comPage 2. ∎ Data is growing faster than our ability to store or index it ∎ There are 3 Billion TelephoneCalls in US each day (100BN minutes); 30B emails daily; 4B SMS; IMs. ∎ Scientific data: NASA'sobservation satellites generate billions of readings each per day. ∎ IP Network Traffic: can bebillions packets per hour per router. Each ISP has many (hundreds) routers! ∎ Whole genomesequences for individual humans now available: each is gigabytes in size Data is Massive 2 Page3. Massive Data Analysis Must analyze this massive data: ∎ Scientific research (monitorenvironment; species) ∎ System management (spot faults; drops; failures) ∎ Customer research(association rules; new offers) ∎ For revenue protection (phone fraud; service abuse) Else; whyeven measure this data? 3 Page 4. Example: Network Data …,*,*,*
Abiteboul; S. 41 Aggarwal; CC 261;593 Agrawal; D 93; 274;496;639 Agrawal; S. 5,M Akinde; S AI-Khalifa; G Alonso; M Areal; WG Aref; V Atluri; I Atmosukarto; D Baker; R Barga; K Barker; B Benatallah; G Bhalotia; HE Blok; M Bohlen; A Bonifati; D Braga; S Bressan; N Bruno; F Buccafurri; A Campi; F Casati; AC Catlin; S Ceri; S Chakrabarti; NH Chan; S Chaudhuri; B Chen; CM Chen; J Chen; MS Chen; F Chiu; J Cho; HD Chon; L Cohen; B Cooper; R Cordova; G Cormode; G Das; S Davey; U Daya; S Decker; A Descour; A Deshpande; J Desmarais; DJ DeWitt; A Doan; M Dumas; J Dunn; MG Elfeky; CJ El1mann; AK Elmagarmid; R Elmasri; C Fa1outsos; J Fan; A Faradjian; P Felber; J Feng; S Flesca; I Foudos; J Freire; AW Fu; F Furfaro; A Gal; H Garcia-Molina; M Garofalakis; J Gehrke; D Georgakopoulos; M Gertz; A Goel,333 369 264 331 129 567; 685 ; 673 ; 490 ; 266 ; 271 263 29 393 490 212;276 716 498 492271 498 431 176 490 605 29 141 278;463 265; 663 335 488 262 ; 494 ; 266 ; 166 129 309 309333 494 275 673 309 141;567;605 309 262 453 706 329 … 267 269 176 583 269 617 268; 268 543 269;327 155;331;335 155 697 555 507 279 333 267 583 41;369 490 271 29 117 331685 333 212;498 685 685 ; 245 605 333 270 431 529 345;697 271 ;498 273 583 272 297 ;485685 274 275 333 265;663 327 … Ounopulos; D PUO; J. ..; Gtirel; A. Haas; L. ãas; p .J.Haclgtimti; H. I … ¥alevy; A. ãmmad; M. ¥aritsa; JR ¥ellerstein; J .M … Lee; D. Lee; MLLehner; W Leung; CK-S Ling; T. W ' … Ling; Y. Liu; B Liu; J Lomet; D. Low; WL Lu; H. ; Lu; JXLuo; G. Madden; S. Madhyastha; T. Maier; D. Major; G. Mani; M. Mannila; H Marian; A.,*,*,*
Techniques and Applications for Approximating String Distances {Rough Draft (April 11 2000),Graham Cormode; S Muthukrishnan; Mike Paterson,In this paper; we introduce techniques which allow us to approximate the distance betweenstrings under a variety of distance measures. The rst technique is Hamming Signatures;which computes an O (logn) signature for a string such that the Hamming distance betweentwo strings can be estimated up to a constant factor by a linear processing of theirsignatures. The second technique is Histogram Transformations; which transformsgeneralized edit distances involving powerful block operations into problems of nding theHamming distance between histograms. Combining these two techniques allows us to solvea number of problems. We also show how these methods can be used to solve previouslyunanswered Nearest Neighbour problems. Namely; given a set of strings; we canpreprocess them in polynomial time in order to nd the approximate nearest neighbour of a …,*,*,*
DIMACS Technical Report 2002-35,Graham Cormode; S Muthukrishnan,ABSTRACT Data streams often consist of multiple signals. Consider a stream of multiplesignals (i; ai; ϳ) w here i's correspond to the domain; j's inde x the di ff erent signals and ai;ϳ> 0 to the value of the j th signal at point i. W e study the problem of determining thedominance norms over the multiple signals; in particular the max-dominance norm; de fi nedas Σi ma xϳ {ai; ϳ}. It is used in applications to estimate the" w orst case in fl uence" ofmultiple processes; for ex ample in IP tra ffi c analysis; electrical grid monitoring and financial domain. Besides fi nding many applications; it is a natural measure: it generali z esthe notion of union of data streams and may be alternately thought of as estimating the L1norm of the upper envelope of multiple signals. W e present the fi rst k no wn data streamalgorithm for estimating ma x-dominance of multiple signals. In particular; we use w or k …,*,*,*
ICDM 2008,Yip Chi Lap; Rezwan Ahmed; Panayiotis Andreou; Maria Andreou; Anelia Angelova; Maria-Luiza Antonie; Gowtham Atluri; Mohammad Salahuddin Aziz; Xiang Bai; Jing Bai; Alexander Behm; Alessio Bertone; Smriti Bhagat; Runa Bhaumik; Arnold Boedihardjo; Mario Boley; Shyam Boriah; Serdar Bozdag; Sandra Bringay; Alexei Brodsky; Fabian Buchwald; Yundong Cai; Deng Cai; Guadalupe Canahuate; Mustafa Canim; Bin Cao; Sinno Jialin Pan; Nathan Liu; Nicolas Cebron; Aaron Cederquist; Oner Ulvi Celepcikay; Loic Cerf; Varun Chandola; Vineet Chaoji; Rui Chen; Rachsuda Jiamthapthaksin; Chun-shen Chen; Jiyang Chen; Feng Chen; Chien Chin Chen; Hyuk Cho; James Tan Swee Chuan; Chun Kit Chui; Graham Cormode; Fabrizio Costa; Grace Cui; Claudia d'Amato; Jing Dai; Bi-Ru Dai; Xiao Dan; Arjun Dasgupta; Vanesa Daza; Meghana Deodhar; Kevin DeRonne; David Dineen; Wei Ding; Marcos A Domingues; Nguyen Luong Dong; Nurcan Durak; Christoph F Eick; Magdalini Eirinaki; Gang Fang; Nicola Fanizzi; Fernando Farfan; Fabio Fassetti; Lukas Faulstich; Ad Feelders; Celine Fiot; Andrew Foss; Arik Friedman; Hans-Henning Gabriel; Johannes Gaertner; Jing Gao; Chuancong Gao; Cibin George; Georgios Giannakopoulos; Mike Gibas,Yip Chi Lap Rezwan Ahmed Panayiotis Andreou Maria Andreou Anelia Angelova Maria-LuizaAntonie Gowtham Atluri Mohammad Salahuddin Aziz Xiang Bai Jing Bai Alexander Behm AlessioBertone Smriti Bhagat Runa Bhaumik Arnold Boedihardjo Mario Boley Shyam Boriah SerdarBozdag Sandra Bringay Alexei Brodsky Fabian Buchwald Yundong Cai Deng Cai GuadalupeCanahuate Mustafa Canim Bin Cao Sinno Jialin Pan Bin Cao Nathan Liu Nicolas Cebron AaronCederquist Oner Ulvi Celepcikay Loic Cerf Varun Chandola Vineet Chaoji Rui Chen RachsudaJiamthapthaksin Chun-shen Chen Jiyang Chen … Feng Chen Chien Chin Chen Hyuk ChoJames Tan Swee Chuan Chun Kit Chui Graham Cormode Fabrizio Costa Grace Cui Claudiad'Amato Jing Dai Bi-Ru Dai Xiao Dan Arjun Dasgupta Vanesa Daza Meghana Deodhar KevinDeRonne David Dineen Wei Ding Marcos A. Domingues Nguyen Luong Dong Nurcan …,*,*,*
Participant List,James Alt; Scott Ashworth; Marco Battaglini; Roland Benabou; Carles Boix; Bruce Bueno de Mesquita; Ernesto Dal Bo; Avinash Dixit; Jeffry Frieden; Joanne Gowa; Catherine Hafer; Philip Keefer; John Londregan; Nolan McCarty; Helen Milner; Grigore Pop-Eleches; Ronald Rogowski; Thomas Romer; Kenneth Shepsle; Enrico Spolaore; Susan Stokes; Andrea Vindigni; Romain Wacziarg; Jennifer Widner,Page 1. PARTICIPANT LIST As of June 12; 2016 A Margie Alt Executive Director; EnvironmentAmerica margie@environmentamerica.org Advanced Energy Frank Altman President and CEO;Community Reinvestment Fund; Inc. frank@crfusa.com Financial Opportunity Martha AmramChief Executive Officer; WattzOn martha@wattzon.com Advanced Energy Stuart AndreasonSenior Community and Economic Development Policy Advisor; Federal Reserve Bank of Atlantastuart.andreason@atl.frb.org Workforce Development Larry Angelilli Chief Financial Officer;MoneyGram International langelilli@moneygram.com Financial Opportunity Claire AngelleDirector; Atlanta Mayor's Office of International Affairs ccangelle@atlantaga.gov Small andMedium Enterprises John Arensmeyer Founder and CEO; Small Business Majorityjarensmeyer@smallbusinessmajority.org Small and Medium Enterprises …,*,*,*
Program Committee Vice Chairs,Daniel Barbara; Tamraparni Dasu; Inderjit Dhillon; Venkatesh Ganti; Bart Goethals; Dimitrios Gunopulos; Hillol Kargupta; George Karypis; S Muthu Muthukrishnan; Dino Pedreschi; Jian Pei; Sunita Sarawagi; Arno Siebes; Jeffrey Xu Yu; Dimitris Achlioptas; Gediminas Adomavicius; Gagan Agarwal; Charu Aggarwal; Eugene Agichtein; Hiroki Arimura; Arindam Banerjee; Francesco Bonchi; Jean-Francois Boulicaut; Paul Bradley; Erick Cantu-Paz; Philip Chan; Kevin Chang; Sanjay Chawla; Hsinchun Chen; Ming-Syan Chen; David Wai-lok Cheung; Chris Clifton; Frans Coenen; Diane Cook; Rob Cooley; Graham Cormode; Honghua Dai; Gautam Das; Chris Ding; Alin Dobra; Carlotta Domeniconi,Program Committee Vice Chairs Daniel Barbara; George Mason University; USA TamraparniDasu; AT&T Research Labs; USA Inderjit Dhillon; University of Texas at Austin; USA VenkateshGanti; Microsoft Research; USA Bart Goethals; University of Antwerp; Belgium DimitriosGunopulos; University of California; Riverside; USA Hillol Kargupta; University of Maryland; BaltimoreCounty & Agnik; LLC; USA George Karypis; University of Minnesota; USA S. MuthuMuthukrishnan; Rutgers University; USA Dino Pedreschi; Univ. of Pisa; Italy Jian Pei; State Universityof New York at Buffalo; USA Sunita Sarawagi; Indian Institute of Technology; Bombay; India ArnoSiebes; Utrecht University; Netherlands Jeffrey Xu Yu; Chinese University of Hong Kong; PRChina … Program Committee Members Dimitris Achlioptas; Microsoft Research; USA GediminasAdomavicius; University of Minnesota; USA Gagan Agarwal; Ohio State University; USA …,*,*,*
The Hardness of the Lemmings Game,Graham Cormode,ABSTRACT In the computer gameLemmings'; the player must guide a tribe of green-hairedlemming creatures to safety; and save them from an untimely demise. We formulate thedecision problem which is; given a level of the game; to decide whether it is possible tocomplete the level (and hence to find a solution to that level). Under certain limitations; thiscan be decided in polynomial time; but in general the problem is shown to be NP-Hard. Thiscan hold even if there is only a single lemming to save; thus showing that it is hard toapproximate the number of saved lemmings to any factor.,*,*,*
What’s New: Finding Significant Differences in Network Data Streams,Graham Cormode S Muthukrishnan; Graham Cormode,Abstract—Monitoring and analyzing network traffic usage pat-terns is vital for managing IPNetworks. An important problem is to provide network managers with information aboutchanges in traffic; informing them about “what's new”. Specifically; we focus on the challengeof finding significantly large differences in traffic: over time; between interfaces and betweenrouters. We introduce the idea of a deltoid: an item that has a large difference; whether thedifference is absolute; relative or variational. We present novel algorithms for finding themost significant deltoids in high speed traffic data; and prove that they use small space; verysmall time per update; and are guaranteed to find significant deltoids with pre-specifiedaccuracy. In experimental evaluation with real network traffic; our algorithms perform welland recover almost all deltoids. This is the first work to provide solutions capable of …,*,*,*
DIMACS TR: 2004-37 Report on DIMACS Working Group Meeting: Data Mining and Epidemiology; March 18-19; 2004,James Abello; Graham Cormode,ABSTRACT Epidemiology is an observational science that concerns itself with finding andexplaining patterns of health and disease in populations; usually of humans; but alsopopulations of animals; insects and plants. Data mining is an active area of researchinterested in finding algorithms for describing latent patterns in often very large data sets.This Working Group had the objective of fostering collaboration between these twodisciplines. In March of 2004 it organized a two-day meeting at DIMACS to bring these twogroups together in a format designed to initiate such collaborations.,*,*,*
DIMACS TR: 2005-39 An evaluation of the edit-distance-with-moves similarity metric for comparing genetic sequences,Shiri Azenkot; Tzu-Yi Chen; Graham Cormode,ABSTRACT We describe the first known implementation of an approximation algorithm forthe string edit distance with moves similarity metric. This is the first algorithm to considernontrivial alignment and run in substantially sub-quadratic time [2]. Extensiveexperimentation demonstrates that the algorithm produces a good approximation for the editdistance with moves metric; especially on strings of length 500B to 10KB. We also found thatthe algorithm has high potential for use in computational biology. When comparing texts ofgenetic sequences; our algorithm outperforms the q-grams heuristic in predicting results ofthe Smith-Waterman algorithm. Finally; we propose additional application areas for ourimplementation.,*,*,*
DIMACS TR: 2007-10 Time-Decaying Aggregates in Out-of-order Streams,Graham Cormode; Flip Korn; Srikanta Tirthapura,ABSTRACT Processing large data streams is now a major topic in data management. Thedata involved can be truly massive; and the required analyses complex. In a stream ofsequential events such as stock feeds; sensor readings; or IP traffic measurements; tuplespertaining to recent events are typically more important than older ones. This can beformalized via time decay functions; which assign weights based on age. Decay functionssuch as sliding windows and exponential decay have been well studied under theassumption of well-ordered arrivals; ie; data arrives in the order of increasing time stamps.However; data quality issues are prevalent in massive streams (due to network asynchronyand delays or possibly due to features inherent to the measurement process); and correctorder of arrival cannot be guaranteed. We focus on the computation of decayed …,*,*,*
Approximate Query Processing in Database Systems,Graham Cormode; Minos Garofalakis; Peter J Haas; Chris Jermaine,*,*,*,*
DIMACS TR: 2001-26 The String Edit Distance Matching Problem with Moves,Graham Cormode; S Muthukrishnan,ABSTRACT The edit distance between two strings S and R is defined to be the minimumnumber of character inserts; deletes and changes needed to convert R to S. Given a textstring t of length n; and a pattern string p of length m; informally; the string edit distancematching problem is to compute the smallest edit distance between p and substrings of t. Awell known dynamic programming algorithm takes time O (nm) to solve this problem; and itis an important open problem in Combinatorial Pattern Matching to significantly improve thisbound.,*,*,*
Queries on the Inverse Distribution Using Dynamic Sampling,Graham Cormode; S Muthukrishnan; Irina Rozenbaum,*,*,*,*
