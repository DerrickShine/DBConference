An introduction to duplicate detection,Felix Naumann; Melanie Herschel,Abstract With the ever increasing volume of data; data quality problems abound. Multiple; yetdifferent representations of the same real-world objects in data; duplicates; are one of themost intriguing data quality problems. The effects of such duplicates are detrimental; forinstance; bank customers can obtain duplicate identities; inventory levels are monitoredincorrectly; catalogs are mailed multiple times to the same household; etc. Automaticallydetecting duplicates is difficult: First; duplicate representations are usually not identical butslightly differ in their values. Second; in principle all pairs of records should be compared;which is infeasible for large volumes of data. This lecture examines closely the two maincomponents to overcome these difficulties:(i) Similarity measures are used to automaticallyidentify duplicates when comparing two records. Well-chosen similarity measures …,Synthesis Lectures on Data Management,2010,221
DogmatiX tracks down duplicates in XML,Melanie Weis; Felix Naumann,Abstract Duplicate detection is the problem of detecting different entries in a data sourcerepresenting the same real-world entity. While research abounds in the realm of duplicatedetection in relational data; there is yet little work for duplicates in other; more complex datamodels; such as XML. In this paper; we present a generalized framework for duplicatedetection; dividing the problem into three components: candidate definition defining whichobjects are to be compared; duplicate definition defining when two duplicate candidates arein fact duplicates; and duplicate detection specifying how to efficiently find those duplicates.Using this framework; we propose an XML duplicate detection method; DogmatiX; whichcompares XML elements based not only on their direct data values; but also on the similarityof their parents; children; structure; etc. We propose heuristics to determine which of these …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,179
Data fusion in three steps: Resolving schema; tuple; and value inconsistencies,Felix Naumann; Alexander Bilke; Jens Bleiholder; Melanie Weis,Abstract Heterogeneous and dirty data is abundant. It is stored under different; often opaqueschemata; it represents identical real-world objects multiple times; causing duplicates; and ithas missing values and conflicting values. Without suitable techniques for integrating andfusing such data; the data quality of an integrated system remains low. We present a suite ofmethods; combined in a single tool; that allows ad-hoc; declarative fusion of such data byemploying schema matching; duplicate detection and data fusion. Guided by a SQL-likequery against one or more tables; we proceed in three fully automated steps: First; instance-based schema matching bridges schematic heterogeneity of the tables by aligningcorresponding attributes. Next; duplicate detection techniques find multiple representationsof identical real-world objects. Finally; data fusion and conflict resolution merges each …,IEEE Data Eng. Bull,2006,103
Detecting duplicate objects in XML documents,Melanie Weis; Felix Naumann,Abstract The problem of detecting duplicate entities that describe the same real-world object(and purging them) is an important data cleansing task; necessary to improve data quality.For data stored in a flat relation; numerous solutions to this problem exist. As XML becomesincreasingly popular for data representation; algorithms to detect duplicates in nested XMLdocuments are required. In this paper; we present a domain-independent algorithm thateffectively identifies duplicates in an XML document. The solution adopts a top-downtraversal of the XML tree structure to identify duplicate elements on each level. Pairs ofduplicate elements are detected using a thresholded similarity function; and are thenclustered by computing the transitive closure. To minimize the number of pairwise elementcomparisons; an appropriate filter function is used. The similarity measure involves string …,Proceedings of the 2004 international workshop on Information quality in information systems,2004,96
Explaining missing answers to SPJUA queries,Melanie Herschel; Mauricio A Hernández,Abstract This paper addresses the problem of explaining missing answers in queries thatinclude selection; projection; join; union; aggregation and grouping (SPJUA). Explainingmissing answers of queries is useful in various scenarios; including query understandingand debugging. We present a general framework for the generation of these explanationsbased on source data. We describe the algorithms used to generate a correct; finite; and;when possible; minimal set of explanations. These algorithms are part of Artemis; a systemthat assists query developers in analyzing queries by; for instance; allowing them to ask whycertain tuples are not in the query results. Experimental results demonstrate that Artemisgenerates explanations of missing tuples at a pace that allows developers to effectively usethem for query analysis.,Proceedings of the VLDB Endowment,2010,86
XStruct: efficient schema extraction from multiple and large XML documents,Jan Hegewald; Felix Naumann; Melanie Weis,XML is the de facto standard format for data exchange on the Web. While it is fairly simple togenerate XML data; it is a complex task to design a schema and then guarantee that thegenerated data is valid according to that schema. As a consequence much XML data doesnot have a schema or is not accompanied by its schema. In order to gain the benefits ofhaving a schema-efficient querying and storage of XML data; semantic verification; dataintegration; etc.-this schema must be extracted. In this paper we present an automatictechnique; XStruct; for XML Schema extraction. Based on ideas of [5]; XStruct extracts aschema for XML data by applying several heuristics to deduce regular expressions that are1-unambiguous and describe each element's contents correctly but generalized to areasonable degree. Our approach features several advantages over known techniques …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,81
Automatic data fusion with HumMer,Alexander Bilke; Jens Bleiholder; Felix Naumann; Christoph Böhm; Karsten Draba; Melanie Weis,Abstract Heterogeneous and dirty data is abundant. It is stored under different; often opaqueschemata; it represents identical real-world objects multiple times; causing duplicates; and ithas missing values and conflicting values. The Humboldt Merger (HumMer) is a tool thatallows ad-hoc; declarative fusion of such data using a simple extension to SQL. Guided by aquery against multiple tables; HumMer proceeds in three fully automated steps: First;instance-based schema matching bridges schematic heterogeneity of the tables by aligningcorresponding attributes. Next; duplicate detection techniques find multiple representationsof identical real-world objects. Finally; data fusion and conflict resolution merges duplicatesinto a single; consistent; and clean representation.,Proceedings of the 31st international conference on Very large data bases,2005,72
Structure-based inference of XML similarity for fuzzy duplicate detection,Luís Leitão; Pável Calado; Melanie Weis,Abstract Fuzzy duplicate detection aims at identifying multiple representations of real-worldobjects stored in a data source; and is a task of critical practical relevance in data cleaning;data mining; or data integration. It has a long history for relational data stored in a singletable (or in multiple tables with equal schema). Algorithms for fuzzy duplicate detection inmore complex structures; eg; hierarchies of a data warehouse; XML data; or graph datahave only recently emerged. These algorithms use similarity measures that consider theduplicate status of their direct neighbors; eg; children in hierarchical data; to improveduplicate detection effectiveness. In this paper; we propose a novel method for fuzzyduplicate detection in hierarchical and semi-structured XML data. Unlike previousapproaches; it not only considers the duplicate status of children; but rather the probability …,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,60
Industry-scale duplicate detection,Melanie Weis; Felix Naumann; Ulrich Jehle; Jens Lufter; Holger Schuster,Abstract Duplicate detection is the process of identifying multiple representations of a samereal-world object in a data source. Duplicate detection is a problem of critical importance inmany applications; including customer relationship management; personal informationmanagement; or data mining. In this paper; we present how a research prototype; namelyDogmatiX; which was designed to detect duplicates in hierarchical XML data; wassuccessfully extended and applied on a large scale industrial relational database incooperation with Schufa Holding AG. Schufa's main business line is to store and retrievecredit histories of over 60 million individuals. Here; correctly identifying duplicates is criticalboth for individuals and companies: On the one hand; an incorrectly identified duplicatepotentially results in a false negative credit history for an individual; who will then not be …,Proceedings of the VLDB Endowment,2008,56
XML duplicate detection using sorted neighborhoods,Sven Puhlmann; Melanie Weis; Felix Naumann,Abstract Detecting duplicates is a problem with a long tradition in many domains; such ascustomer relationship management and data warehousing. The problem is twofold: Firstdefine a suitable similarity measure; and second efficiently apply the measure to all pairs ofobjects. With the advent and pervasion of the XML data model; it is necessary to find newsimilarity measures and to develop efficient methods to detect duplicate elements in nestedXML data. A classical approach to duplicate detection in flat relational data is the sortedneighborhood method; which draws its efficiency from sliding a window over the relation andcomparing only tuples within that window. We extend the algorithm to cover not only a singlerelation but nested XML elements. To compare objects we make use of XML parent andchild relationships. For efficiency; we apply the windowing technique in a bottom-up …,International Conference on Extending Database Technology,2006,53
Artemis: A system for analyzing missing answers,Melanie Herschel; Mauricio A Hernández; Wang-Chiew Tan,Abstract A central feature of relational database management systems is the ability to definemultiple different views over an underlying database schema. Views provide a method ofdefining access control to the underlying database; since a view exposes a part of thedatabase and hides the rest. Views also provide logical data independence to applicationprograms that access the database. For most cases; the process of specifying the desiredviews in SQL is typically tedious and error-prone. While numerous tools exist to supportdevelopers in debugging program code; we are not aware of any tool that supportsdevelopers in verifying the correctness of their views defined in SQL.,Proceedings of the VLDB Endowment,2009,46
Query-Based Why-Not Provenance with NedExplain,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,With the increasing amount of available data and transformations manipulating the data; ithas become essential to analyze and debug data transformations. A sub-problem of datatransformation analysis is to understand why some data are not part of the result of arelational query. One possibility to explain the lack of data in a query result is to identifywhere in the query we lost data pertinent to the expected outcome. A first approach to this socalled why-not provenance has been recently proposed; but we show that this first approachhas some shortcomings. To overcome these shortcomings; we propose\ned; an algorithm toexplain data missing from a query result. NedExplain computes the why-not provenance formonotone relational queries with aggregation. After providing necessary definitions; thispaper contributes a detailed description of the algorithm. A comparative evaluation shows …,Extending Database Technology (EDBT),2014,39
A duplicate detection benchmark for XML (and relational) data,Melanie Weis; Felix Naumann; Franziska Brosy,Abstract Duplicate detection; which is an important subtask of data cleaning; is the task ofidentifying multiple representations of a same real-world object. Numerous approaches bothfor relational and XML data exist. Their goals are either on improving the quality of thedetected duplicates (effectiveness) or on saving computation time (efficiency). In particularfor the first goal; the “goodness” of an approach is usually evaluated based on experimentalstudies. Although some methods and data sets have gained popularity; it is still difficult tocompare different approaches or to assess the quality of one own's approach. This difficultyof comparison is mainly due to lack of documentation of algorithms and the data; softwareand hardware used and/or limited resources not allowing to rebuild systems described byothers. In this paper; we propose a benchmark for duplicate detection; specialized to XML …,Proc. of Workshop on Information Quality for Information Systems (IQIS),2006,39
Detecting duplicates in complex XML data,Melanie Weis; Felix Naumann,Recent work both in the relational and the XML world have shown that the efficacy andefficiency of duplicate detection is enhanced by regarding relationships between entities.However; most approaches for XML data rely on 1: n parent/child relationships; and do notapply to XML data that represents m: n relationships. We present a novel comparisonstrategy; which performs duplicate detection effectively for all kinds of parent/childrelationships; given dependencies between different XML elements. Due to cyclicdependencies; it is possible that a pairwise classification is performed more than once;which compromises efficiency. We propose an order that reduces the number of suchreclassifications and apply it to two algorithms. The first algorithm performs reclassifications;and efficiency is increased by using the order reducing the number of reclassifications …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,36
Efficient and Effective Duplicate Detection in Hierarchical Data,Luis Leitão; Pável Calado; Melanie Herschel,Although there is a long line of work on identifying duplicates in relational data; only a fewsolutions focus on duplicate detection in more complex hierarchical structures; like XMLdata. In this paper; we present a novel method for XML duplicate detection; called XMLDup.XMLDup uses a Bayesian network to determine the probability of two XML elements beingduplicates; considering not only the information within the elements; but also the way thatinformation is structured. In addition; to improve the efficiency of the network evaluation; anovel pruning strategy; capable of significant gains over the unoptimized version of thealgorithm; is presented. Through experiments; we show that our algorithm is able to achievehigh precision and recall scores in several data sets. XMLDup is also able to outperformanother state-of-the-art duplicate detection solution; both in terms of efficiency and of …,IEEE Transaction On Knowledge and Data Engineering,2013,33
Efficient and Effective Duplicate Detection in Hierarchical Data,Melanie Herschel; Pável Calado; Luís Leitão,*,IEEE Transactions on Knowledge and Data Engineering,2012,31
Scalable Iterative Graph Duplicate Detection,Melanie Herschel; Felix Naumann; Sascha Szott; Maik Taubert,Duplicate detection determines different representations of real-world objects in a database.Recent research has considered the use of relationships among object representations toimprove duplicate detection. In the general case where relationships form a graph; researchhas mainly focused on duplicate detection quality/effectiveness. Scalability has beenneglected so far; even though it is crucial for large real-world duplicate detection tasks. Wescale-up duplicate detection in graph data (DDG) to large amounts of data and pairwisecomparisons; using the support of a relational database management system. To this end;we first present a framework that generalizes the DDG process. We then present algorithmsto scale DDG in space (amount of data processed with bounded main memory) and in time.Finally; we extend our framework to allow batched and parallel DDG; thus further …,Knowledge and Data Engineering; IEEE Transactions on,2011,31
Subsumption and complementation as data fusion operators,Jens Bleiholder; Sascha Szott; Melanie Herschel; Frank Kaufer; Felix Naumann,Abstract The goal of data fusion is to combine several representations of one real worldobject into a single; consistent representation; eg; in data integration. A very popularoperator to perform data fusion is the minimum union operator. It is defined as the outerunion and the subsequent removal of subsumed tuples. Minimum union is used in otherapplications as well; for instance in database query optimization to rewrite outer join queries;in the semantic web community in implementing Sparql's optional operator; etc. Despite itswide applicability; there are only few efficient implementations; and until now; minimumunion is not a relational database primitive. This paper fills this gap as we presentimplementations of subsumption that serve as a building block for minimum union.Furthermore; we consider this operator as database primitive and show how to perform …,Proceedings of the 13th International Conference on Extending Database Technology,2010,25
Declarative XML data cleaning with XClean,Melanie Weis; Ioana Manolescu,Abstract Data cleaning is the process of correcting anomalies in a data source; that may forinstance be due to typographical errors; or duplicate representations of an entity. It is acrucial task in customer relationship management; data mining; and data integration. Withthe growing amount of XML data; approaches to effectively and efficiently clean XML areneeded; an issue not addressed by existing data cleaning systems that mostly specialize onrelational data. We present XClean; a data cleaning framework specifically geared towardscleaning XML data. XClean's approach is based on a set of cleaning operators; whosesemantics is well-defined in terms of XML algebraic operators. Users may specify cleaningprograms by combining operators by means of a declarative XClean/PL program; which isthen compiled into XQuery. We describe XClean's operators; language; and compilation …,Proceedings of the 19th international conference on Advanced information systems engineering,2007,25
Wondering why data are missing from query results?: ask conseil why-not,Melanie Herschel,Abstract In analyzing and debugging data transformations; or more specifically relationalqueries; a subproblem is to understand why some data are not part of the query result. Thisproblem has recently been addressed from different perspectives for various fragments ofrelational queries. The different perspectives yield different; yet complementary explanationsof such missing-answers. This paper first aims at unifying the different approaches bydefining a new type of explanation; called hybrid explanation; that encompasses the varietyof previously defined types of explanations. This solution goes beyond simply forming theunion of explanations produced by different algorithms and is shown to be able to explain alarger set of missing-answers. Second; we present Conseil; an algorithm to generate hybridexplanations. Conseil is also the first algorithm to handle non-monotonic queries …,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,21
Scaling up duplicate detection in graph data,Melanie Herschel; Felix Naumann,Abstract Duplicate detection determines different representations of real-world objects in adatabase. Recent research has considered the use of relationships among objectrepresentations to improve duplicate detection. In the general case where relationships forma graph; research has mainly focused on duplicate detection quality/effectiveness.Scalability has been neglected so far; even though it is crucial for large real-world duplicatedetection tasks. We scale up duplicate detection in graph data (DDG) to large amounts ofdata using the support of a relational database system. We first generalize the process ofDDG and then present how to scale DDG in space (amount of data processed with limitedmain memory) and in time. Finally; we explore how complex similarity computation can beperformed efficiently. Experiments on data an order of magnitude larger than data …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,16
EFQ: Why-not answer polynomials in action,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,Abstract One important issue in modern database applications is supporting the user withefficient tools to debug and fix queries because such tasks are both time and skilldemanding. One particular problem is known as Why-Not question and focusses on thereasons for missing tuples from query results. The EFQ platform demonstrated here hasbeen designed in this context to efficiently leverage Why-Not Answers polynomials; a novelapproach that provides the user with complete explanations to Why-Not questions andallows for automatic; relevant query refinements.,Proceedings of the VLDB Endowment,2015,15
Entity Resolution in the Web of Data,Kostas Stefanidis; Vasilis Efthymiou; Melanie Herschel; Vassilis Christophides,Page 1. Entity Resolution in the Web of Data Kostas Stefanidis1; Vasilis Efthymiou1;2; MelanieHerschel3;4; Vassilis Christophides5 kstef@ics.forth.gr; vefthym@ics.forth.gr; melanie.herschel@lri.fr vassilis.christophides@technicolor.com 1FORTH; 2University of Crete; 3Université ParisSud;4Inria Saclay; 5Paris R&I Center; Technicolor Page 2. LOD Cloud and the Web of Data 2Media Government Geographic Publications User-generated Life sciences Cross-domain Page3. LOD Cloud and the Web of Data 3 adapted from Suchanek & Weikum tutorial@SIGMOD 2013Page 4. LOD Cloud and the Web of Data 4 24.9M entities 2.16 Billion triples 50;000 properties40M entities 1.9 Billion triples 10M entities 120M triples adapted from Suchanek & Weikumtutorial@SIGMOD 2013 Page 5. Entities: An Invaluable Asset 5 Monuments “Entities” is whata large part of our knowledge is about Page 6. Entities: An Invaluable Asset …,International Conference on Information and Knowledge Management (CIKM),2013,15
Fuzzy Duplicate Detection on XML Data,Melanie Weis,Abstract XML is popular for data exchange and data publishing on the Web; but it comeswith errors and inconsistencies inherent to real-world data. Hence; there is a need for XMLdata cleansing; which requires solutions for fuzzy duplicate detection in XML. Thehierarchical and semi-structured nature of XML strongly differs from the flat and structuredrelational model; which has received the main attention in duplicate detection so far. Weconsider four major challenges of XML duplicate detection to develop effective; efficient; andscalable solutions to the problem.,Proceedings of VLDB 2005 PhD Workshop,2005,15
A Hybrid Approach to Answering Why-Not Questions on Relational Query Results,Melanie Herschel,Abstract In analyzing and debugging data transformations; or more specifically relationalqueries; a subproblem is to understand why some data are not part of the query result. Thisproblem has recently been addressed from different perspectives for various fragments ofrelational queries. The different perspectives yield different yet complementary explanationsof such missing answers. This article first aims at unifying the different approaches bydefining a new type of explanation; called hybrid explanation; that encompasses the varietyof previously defined types of explanations. This solution goes beyond simply forming theunion of explanations produced by different algorithms and is shown to be able to explain alarger set of missing answers. Second; we present Conseil; an algorithm to generate hybridexplanations. Conseil is also the first algorithm to handle nonmonotonic queries …,Journal of Data and Information Quality (JDIQ),2015,14
Efficient computation of polynomial explanations of why-not questions,Nicole Bidoit; Melanie Herschel; Aikaterini Tzompanaki,Abstract Answering a Why-Not question consists in explaining why a query result does notcontain some expected data; called missing answers. This paper focuses on processingWhy-Not questions in a query-based approach that identifies the culprit query components.Our first contribution is a general definition of a Why-Not explanation by means of apolynomial. Intuitively; the polynomial provides all possible explanations to explore in orderto recover the missing answers; together with an estimation of the number of recoverableanswers. Moreover; this formalism allows us to represent Why-Not explanations in a unifiedway for extended relational models with probabilistic or bag semantics. We further presentan algorithm to efficiently compute the polynomial for a given Why-Not question. Anexperimental evaluation demonstrates the practicality of the solution both in terms of …,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,13
Immutably Answering Why-Not Questions for Equivalent Conjunctive Queries,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,Abstract Answering Why-Not questions consists in explaining to developers of complex datatransformations or manipulations why their data transformation did not produce somespecific results; although they expected them to do so. Different types of explanations thatserve as Why-Not answers have been proposed in the past and are either based on theavailable data; the query tree; or both. Solutions (partially) based on the query tree aregenerally more efficient and easier to interpret by developers than solutions solely based ondata. However; algorithms producing such query-based explanations so far may returndifferent results for reordered conjunctive query trees; and even worse; these results may beincomplete. Clearly; this represents a significant usability problem; as the explanationsdevelopers get may be partial and developers have to worry about the query tree …,USENIX Workshop on the Theory and Practice of Provenance (TAPP),2014,13
Pushing the limits of instance matching systems: A semantics-aware benchmark for linked data,Tzanina Saveta; Evangelia Daskalaki; Giorgos Flouris; Irini Fundulaki; Melanie Herschel; Axel-Cyrille Ngonga Ngomo,Abstract The architectural choices behind the Data Web have led to the publication of largeinterrelated data sets that contain different descriptions for the same real-world objects. Dueto the mere size of current online datasets; such duplicate instances are most commonlydetected (semi-) automatically using instance matching frameworks. Choosing the rightframework for this purpose remains tedious; as current instance matching benchmarks fail toprovide end users and developers with the necessary insights pertaining to how currentframeworks behave when dealing with real data. In this poster; we present the SemanticPublishing Instance Matching Benchmark (SPIMBENCH) which allows the benchmarking ofinstance matching systems against not only structure-based and value-based test cases; butalso against semantics-aware test cases based on OWL axioms. SPIMBENCH features a …,Proceedings of the 24th International Conference on World Wide Web,2015,12
An overview of XML duplicate detection algorithms,Pável Calado; Melanie Herschel; Luís Leitão,Abstract Fuzzy duplicate detection aims at identifying multiple representations of real-worldobjects in a data source; and is a task of critical relevance in data cleaning; data mining; anddata integration tasks. It has a long history for relational data; stored in a single table or inmultiple tables with an equal schema. However; algorithms for fuzzy duplicate detection inmore complex structures; such as hierarchies of a data warehouse; XML data; or graph datahave only recently emerged. These algorithms use similarity measures that consider theduplicate status of their direct neighbors to improve duplicate detection effectiveness. In thischapter; we study different approaches that have been proposed for XML fuzzy duplicatedetection. Our study includes a description and analysis of the different approaches; as wellas a comparative experimental evaluation performed on both artificial and real-world data …,Soft Computing in XML Data Management,2010,12
LANCE: Piercing to the heart of instance matching tools,Tzanina Saveta; Evangelia Daskalaki; Giorgos Flouris; Irini Fundulaki; Melanie Herschel; Axel-Cyrille Ngonga Ngomo,Abstract One of the main challenges in the Data Web is the identification of instances thatrefer to the same real-world entity. Choosing the right framework for this purpose remainstedious; as current instance matching benchmarks fail to provide end users and developerswith the necessary insights pertaining to how current frameworks behave when dealing withreal data. In this paper; we present lance; a domain-independent instance matchingbenchmark generator which focuses on benchmarking instance matching systems forLinked Data. lance is the first Linked Data benchmark generator to support complexsemantics-aware test cases that take into account expressive OWL constructs; in addition tothe standard test cases related to structure and value transformations. lance supports thedefinition of matching tasks with varying degrees of difficulty and produces a weighted …,International Semantic Web Conference,2015,10
The nautilus analyzer: understanding and debugging data transformations,Melanie Herschel; Hanno Eichelberger,Abstract When developing data transformations-a task omnipresent in applications like dataintegration; data migration; data cleaning; or scientific data processing-developers quicklyface the need to verify the semantic correctness of the transformation. Declarativespecifications of data transformations; eg; SQL or ETL tools; increase developer productivitybut usually provide limited or no means for inspection or debugging. In this situation;developers today have no choice but to manually analyze the transformation and; in case ofan error; to (repeatedly) fix and test the transformation. The goal of the Nautilus project is tosemi-automatically support this analysis-fix-test cycle. This demonstration focuses on onemain component of Nautilus; namely the Nautilus Analyzer that helps developers inunderstanding and debugging their data transformations. The demonstration will show …,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,10
XClean in Action,Melanie Weis; Ioana Manolescu,ABSTRACT We demonstrate XClean; a data cleaning system specifically geared towardscleaning XML data. XClean's approach is based on a set of cleaning operators. Users mayspecify cleaning programs by combining operators using the declarative XClean/PLlanguage; which is then compiled into XQuery. We plan to show XClean in action on severalscenarios based on real-world data. A graphical user interface supports users in writingXClean/PL programs and guides them through the process to obtain the clean data.,of: Proceedings of the 3rd Biennial Conference on Innovative Data Systems Research; CIDR,2007,10
DataBridges: data integration for digital cities,Melanie Herschel; Ioana Manolescu,Abstract The European Union has created the European Institute of Technology (EIT); withinwhich ICT Labs focuses on fostering exchange and new result creation in the sphere ofInformation and Communication Technology; across the areas of research; highereducation; and industrial innovation." Digital Cities of the Future" is an action line (orchapter) of EIT ICT Labs. Within this action line; we coordinated an activity called"DataBridges: Data Integration for Digital cities"; whose aim is to produce; link; integrate andexploit open data in the Digital Cities data space; for the benefit of both citizens andadministrations. In that context; DataBridges addresses many research challenges such asacquiring (or producing) Open City Data in RDF; data linking and integration; dataprovenance; and visualization. This position paper describes our efforts within …,Proceedings of the 2012 ACM workshop on City data management workshop,2012,9
Complement union for data integration,Jens Bleiholder; Sascha Szott; Melanie Herschel; Felix Naumann,A data integration process consists of mapping source data into a target representation(schema mapping); identifying multiple representations of the same real-word object(duplicate detection); and finally combining these representations into a single consistentrepresentation (data fusion). Clearly; as multiple representations of an object are generallynot exactly equal; during data fusion; we have to take special care in handling data conflicts.This paper focuses on the definition and implementation of complement union; an operatorthat defines a new semantics for data fusion.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,7
Duplicate detection in XML,Melanie Weis,*,*,2008,7
Transformation lifecycle management with nautilus,Melanie Herschel; Torsten Grust,When developing data transformations—a task omnipresent in applications like dataintegration; data migration; data cleaning; or scientific data processing—developers quicklyface the need to verify the semantic correctness of the transformation. Declarativespecifications of data transformations; eg SQL or ETL tools; increase developer productivitybut usually provide limited or no means for inspection or debugging. In this situation;developers today have no choice but to manually analyze the transformation and; in case ofan error; to (repeatedly) fix and test the transformation. As a simple example; consider adeveloper who wonders why some products are missing from a SQL query result (making itobvious for him that the query is faulty). Possible reasons abound; eg were product tuplesfiltered by a particular selection or are expected join partners missing? Usually; the …,VLDB Workshop on the Quality of Data (QDB),2011,6
Relationship-based duplicate detection,Melanie Weis; Felix Naumann,Recent work both in the relational and the XML world have shown that the efficacy andefficiency of duplicate detection is enhanced by regarding relationships between ancestorsand descendants. We present a novel comparison strategy that uses relationships butdisposes of the strict bottom-up and topdown approaches proposed for hierarchical data.Instead; pairs of objects at any level of the hierarchy are compared in an order that dependson their relationships: Objects with many dependants influence many other duplicity-decisions and thus it should be decided early if they are duplicates themselves. We applythis ordering strategy to two algorithms. RECONA allows to re-examine an object if itsinfluencing neighbors turn out to be duplicates. Here ordering reduces the number of suchre-comparisons. ADAMA is more efficient by not allowing any re-comparison. Here the …,*,2006,6
Provenance: On and behind the screens,Melanie Herschel; Marcel Hlawatsch,Abstract Collecting and processing provenance; ie; information describing the productionprocess of some end product; is important in various applications; eg; to assess quality; toensure reproducibility; or to reinforce trust in the end product. In the past; different types ofprovenance meta-data have been proposed; each with a different scope. The first part of theproposed tutorial provides an overview and comparison of these different types ofprovenance. To put provenance to good use; it is essential to be able to interact with andpresent provenance data in a user-friendly way. Often; users interested in provenance arenot necessarily experts in databases or query languages; as they are typically domainexperts of the product and production process for which provenance is collected (biologists;journalists; etc.). Furthermore; in some scenarios; it is difficult to use solely queries for …,Proceedings of the 2016 International Conference on Management of Data,2016,5
Eliminating NULLs with Subsumption and Complementation.,Jens Bleiholder; Melanie Herschel; Felix Naumann,Abstract In a data integration process; an important step after schema matching andduplicate detection is data fusion. It is concerned with the combination or merging of differentrepresentations of one real-world object into a single; consistent representation. In order tosolve potential data conflicts; many different conflict resolution strategies can be applied. Inparticular; some representations might contain missing values (NULL-values) where othersprovide a non-NULL-value. A common strategy to handle such NULL-values; is to replacethem with the existing values from other representations. Thus; the conciseness of therepresentation is increased without losing information. Two examples for relationaloperators that implement such a strategy are minimum union and complement union andtheir unary building blocks subsumption and complementation. In this paper; we define …,IEEE Data Eng. Bull.,2011,5
Space and time scalability of duplicate detection in graph data,Melanie Weis; Felix Naumann,Abstract The task of duplicate detection consists in determining different representations of asame real-world object in a database; and that for every object in the database. Recentresearch has considered to use relationships among object representations to improveduplicate detection. In the general case where relationships form a graph; research hasmainly focused on duplicate detection effectiveness. Scalability has been neglected so far;even though it is crucial for large real-world duplicate detection tasks to scale up. In thisreport; we present how duplicate detection in graph data scales up to large amounts of dataand pairwise comparisons; using the support of a relational database system. To this end;we generalize the process of duplicate detection in graphs (DDG). We then define twomethods to scale algorithms for DDG in space (amount of data processed with limited …,Hasso-Plattner-Insitut Potsdam,2007,5
Erkennen und Bereinigen von Datenfehlern in naturwissenschaftlichen Daten.,Heiko Müller; Melanie Weis; Jens Bleiholder; Ulf Leser,Die Ausführung zusätzlicher Experimente ist aber vergleichsweise teuer und unterUmständen auch unmöglich. Im naturwissenschaftlichen Bereich kommt es uns dabeizugute; dass weltweit unterschiedliche Arbeitsgruppen oftmals eine teilweise odervollständig überlappende Menge an Objekten untersuchen; wie zB eine bestimmteProteinfamilie oder das gleiche Genom. Es handelt sich dabei sowohl um die Ausführungvon Experimenten als auch um die überlappende Auswertung experimenteller Ergebnisse.Dies erfolgt häufig unter Ausnutzung unterschiedlicher Techniken und Reagenzien. Somitstehen oftmals genügend überlappende Datensätze bereit. In einigen Fällen können unterAusnutzung von Domänenwissen die benötigten Daten aus existierenden Daten abgeleitetwerden; wie dies in [Müller 2003] zur Korrektur fehlerhaft annotierter Startpositionen der …,Datenbank-Spektrum,2005,4
Instance matching benchmarks for linked data,Evangelia Daskalaki; I Fundulaki; M Herschel; J Saveta,Page 1. 1 Instance Matching Benchmarks for Linked Data Evangelia Daskalaki; Institute ofComputer Science – FORTH ; Greece Tzanina Saveta; Institute of Computer Science – FORTH ;Greece Irini Fundulaki; Institute of Computer Science – FORTH ; Greece Melanie Herschel; InriaISWC 2014 ; October 19th; Riva del Garda; Italy http://www.ics.forth.gr/isl/BenchmarksTutorial/Page 2. 2 Instance Matching Benchmarks for Linked Data Evangelia Daskalaki; Irini Fundulaki;Melanie Herschel; Tzanina Saveta Teaser Slide • We will talk about Benchmarks • Benchmarksare generally a set of tests to assess computer systems' performances • Specifically we will talkabout: Instance Matching (IM) Benchmark for Linked Data. Page 3. 3 Instance MatchingBenchmarks for Linked Data Evangelia Daskalaki; Irini Fundulaki; Melanie Herschel; TzaninaSaveta Overview • Introduction into Linked Data • Instance Matching …,ISWC (Tutorial),2014,3
Reuse-based Optimization for Pig Latin,Jesús Camacho-Rodríguez; Dario Colazzo; Melanie Herschel; Ioana Manolescu; Soudip Roy Chowdhury,Abstract Pig Latin is a popular language which is widely used for parallel processing ofmassive data sets. Currently; subexpressions occurring repeatedly in Pig Latin scripts areexecuted as many times as they appear; and the current Pig Latin optimizer does not identifyreuse opportunities. We present a novel optimization approach aiming at identifying andreusing repeated subexpressions in Pig Latin scripts. Our optimization algorithm; namedPigReuse; identifies subexpression merging opportunities; selects the best ones to executebased on a cost function; and reuses their results as needed in order to compute exactly thesame output as the original scripts. Our experiments demonstrate the effectiveness of ourapproach.,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,2
PigReuse: A Reuse-based Optimizer for Pig Latin,Jesús Camacho-Rodríguez; Dario Colazzo; Melanie Herschel; Ioana Manolescu; Soudip Roy Chowdhury,Pig Latin is a popular language which is widely used for parallel processing of massive datasets. Currently; subexpressions occurring repeatedly in Pig Latin scripts are executed asmany times as they appear; and the current Pig Latin optimizer does not identify reuseopportunities. We present a novel optimization approach aiming at identifying and reusingrepeated subexpressions in Pig Latin scripts. Our optimization algorithm; named PigReuse;operates on a particular algebraic representation of Pig Latin scripts. PigReuse identifiessubexpression merging opportunities; selects the best ones to execute based on a costfunction; and reuses their results as needed in order to compute exactly the same output asthe original scripts. Our experiments demonstrate the effectiveness of our approach.,*,2016,2
Refining SQL queries based on why-not polynomials,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,*,USENIX Workshop on the Theory and Practice of Provenance (TAPP); Washington D.C.; USA,2016,1
Answering Why-Not questions,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,With the increasing amount of available data and transformations manipulating the data; ithas become essential to analyze and debug data transformations. A sub-problem of datatransformation analysis is to understand why some data are not part of the result of arelational query. One possibility to explain the lack of data in a query result is to identifywhere in the query data pertinent to the expected; but missing output is lost during queryprocessing. A first approach to this so called why-not provenance has been recentlyproposed; but we show that this first approach has some shortcomings. To overcome theseshortcomings; we propose an algorithm to explain non-existing data in a query result. Thisalgorithm allows to compute the why-not provenance for rela-tional queries involvingselection; projection; join and union. After providing necessary definitions; this paper …,Bases de Données Avancées (BDA),2013,1
A survey on provenance: What for? What form? What from?,Melanie Herschel; Ralf Diestelkämper; Houssem Ben Lahmar,Abstract Provenance refers to any information describing the production process of an endproduct; which can be anything from a piece of digital data to a physical object. While thissurvey focuses on the former type of end product; this definition still leaves room for manydifferent interpretations of and approaches to provenance. These are typically motivated bydifferent application domains for provenance (eg; accountability; reproducibility; processdebugging) and varying technical requirements such as runtime; scalability; or privacy. As aresult; we observe a wide variety of provenance types and provenance-generating methods.This survey provides an overview of the research field of provenance; focusing on whatprovenance is used for (what for?); what types of provenance have been defined andcaptured for the different applications (what form?); and which resources and system …,The VLDB Journal,2017,*
Provenance in DISC Systems: Reducing Space Overhead at Runtime,Ralf Diestelkämper; Melanie Herschel; Priyanka Jadhav; Ralf Diestelk,Abstract Data intensive scalable computing (DISC) systems; such as Apache Hadoop orSpark; allow to process large amounts of heterogenous data. For varying provenanceapplications; emerging provenance solutions for DISC systems track all source data itemsthrough each processing step; imposing a high space and time overhead during programexecution. We introduce a provenance collection approach that reduces the space overheadat runtime by sampling the input data based on the definition of equivalence classes. Apreliminary empirical evaluation shows that this approach allows to satisfy many use casesof provenance applications in debugging and data exploration; indicating that provenancecollection for a fraction of the input data items often suffices for selected provenanceapplications. When additional provenance is required; we further outline a method to …,9th USENIX Workshop on the Theory and Practice of Provenance (TaPP 17),2017,*
Provenance-based Recommendations for Visual Data Exploration,Ben Lahmar; Melanie Herschel,Abstract Visual data exploration allows users to analyze datasets based on visualizations ofinteresting data characteristics; to possibly discover interesting information about the datathat users are a priori unaware of. In this context; both recommendations of queries selectingthe data to be visualized and recommendations of visualizations that highlight interestingdata characteristics support users in visual data exploration. So far; these two types ofrecommendations have been mostly considered in isolation of one another. We present arecommendation approach for visual data exploration that unifies query recommendationand visualization recommendation. The recommendations rely on two types of provenance;ie; data provenance (aka lineage) and evolution provenance that tracks users' interactionswith a data exploration system. This paper presents the provenance data model as well …,9th USENIX Workshop on the Theory and Practice of Provenance (TaPP 17),2017,*
Répondre à des requêtes Why-Not indépendamment de la représentation des requêtes,Nicole Bidoit; Melanie Herschel; Katerina Tzompanaki,Dans le contexte de développement de transformations complexes; les réponses à unequestion de type'Why-Not'ont pour objectif d'expliquer au développeur les raisons del'absence de certaines réponses dans le résultat d'une transformation. Plusieurs typesd'explications ont été proposées et étudiées: des explications basées sur les données; desexplications basées sur l'arbre de la requête; des expli-cations hybrides. Les explicationsqui s' appuient sur l'arbre de la requête; appelées explications' query-based'(query-basedexplanations) peuvent être calculées plus efficacement et sont aussi plus faciles àinterpréter par le développeur. Cependant; les algorithmes connus produisant desexplications' query-based'donnent des résultats (1) qui sont dépendants des arbres derequêtes considérés;(2) qui ne sont pas toujours complets. À l'évidence; cela pose un …,Bases de données avancées (BDA14),2014,*
Datenintegration,Melanie Herschel,Data integration aims at combining data that resides in distributed; autonomous; andheterogeneous databases into a single consistent view of the data. Its applications areabundant; ranging from data integration in scientific data (helping scientists share;understand; reuse and complement past results) to data integration in enterprises (forinstance to set up data warehouses; perform business intelligence or implement master datamanagement) and data integration on the Web (comparison online shopping or linking opendata). In order to achieve data integration; three major problems have been of particularinterest to both the database research community and IT industry. First; the heterogeneitybetween data models and schemas of data sources has to be overcome. Second; datasources may overlap in the sets of real-world entities such as persons or products they …,it-Information Technology,2012,*
Application de mesures de distance pour la détection de problèmes de qualité de données,Melanie Herschel; Laure Berti-Équille,Avec la multiplication des sources d'informations disponibles et l'accroissement desvolumes et flux de données potentiellement accessibles; la qualité des données et; au senslarge; la qualité des informations n'ont cessé de prendre une place de premier plan tant auniveau académique qu'au sein des entreprises. Si l'analyse des données; l'extraction deconnaissances à partir des données et la prise de décision peuvent être réalisées sur desdonnées inexactes; incomplètes; ambiguës et de qualité médiocre; on peut alors s'interroger sur le sens à donner aux résultats de ces analyses et remettre en cause; à justetitre; la qualité des connaissances ainsi" élaborées"; tout comme le bien-fondé desdécisions prises. Aujourd'hui; il n'est donc plus question de négliger les données mais; bienau contraire; d'évaluer et de contrôler leur qualité dans les systèmes d'information; les …,La qualité et la gouvernance de données au service de la performance des entreprises,2012,*
it-Information technology Special issue on data integration,Melanie Herschel,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion;Connexion avec ORCID; Créer un compte; Mot de passe oublié ? Login oublié ? fr; en.Accueil; Dépôt; Consultation: Les derniers dépôts; Par type de publication; Par discipline;Par année de publication; Par structure de recherche; Les portails de l'archive; Lescollections. Recherche; Documentation: Tutoriels; Compte et profil: Pourquoi créer uncompte et un profil dans HAL; Créer son compte et son profil dans HAL; Modifier son compteou son profil dans HAL; Modifier son mot de passe; Login ou mot de passe oublié; Lesdroits associés au profil. Déposer: Avant de commencer; Les …,*,2012,*
Dublettenerkennung unter Berücksichtigung von Datenabhängigkeiten Duplicate Detection Exploiting Data Relationships,Melanie Herschel,Zusammenfassung Dublettenerkennung befasst sich mit dem Problem; verschiedeneDatenbankrepräsentationen des gleichen Objekts zu identifizieren. Bisherige Algorithmenzur Dublettenerkennung ignorieren den Kontext; in dem sich die Daten befinden. Dabeienthält dieser oftmals weitere nützliche Daten; die das Ergebnis positiv beeinflussenkönnen. In diesem Beitrag stellen wir Algorithmen zur Dublettenerkennung vor; dieBeziehungen innerhalb der Daten ausnutzen. Abstract Duplicate detection consists inidentifying multiple; different data base representations of a same real-world object. State-of-the-art duplicate detection systems usually concentrate on identifying duplicates in a singlerelational table and thereby ignore that the data may exist in a larger context that; whenconsidered; can significantly improve the performance of duplicate detection. In this paper …,it-Information Technology,2009,*
Space and time scalability of duplicate detection in graph data,Melanie Herschel; Felix Naumann,Abstract Duplicate detection consists in determining different representations of real-worldobjects in a database. Recent research has considered the use of relationships amongobject representations to improve duplicate detection. In the general case whererelationships form a graph; research has mainly focused on duplicate detectionquality/effectiveness. Scalability has been neglected so far; even though it is crucial for largereal-world duplicate detection tasks. In this paper we scale up duplicate detection in graphdata (DDG) to large amounts of data and pairwise comparisons; using the support of arelational database system. To this end; we first generalize the process of DDG. We thenpresent how to scale algorithms for DDG in space (amount of data processed with limitedmain memory) and in time. Finally; we explore how complex similarity computation can be …,*,2008,*
Data Engineering,Hideki Kawai; Tait Eliott Larson; David Menestrina; Qi Su; Sutthipong Thavisomboon; Jennifer Widom; Felix Naumann; Alexander Bilke; Jens Bleiholder; Melanie Weis,Abstract An important aspect of maintaining information quality in data repositories isdetermining which sets of records refer to the same real world entity. This so called entityresolution problem comes up frequently for data cleaning and integration. In many domains;the underlying entities exhibit strong ties between themselves. Friendships in socialnetworks and collaborations between researchers are examples of such ties. In such cases;we stress the need for collective entity resolution where; instead of independently taggingpairs of records as duplicates or non-duplicates; related entities are resolved collectively.We present different algorithms for collective entity resolution that combine relationalevidence with traditional attribute-based approaches to improve entity resolutionperformance in a scalable manner.,*,*,*
