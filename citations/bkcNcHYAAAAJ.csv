Pig latin: a not-so-foreign language for data processing,Christopher Olston; Benjamin Reed; Utkarsh Srivastava; Ravi Kumar; Andrew Tomkins,Abstract There is a growing need for ad-hoc analysis of extremely large data sets; especiallyat internet companies where innovation critically depends on being able to analyzeterabytes of data collected every day. Parallel database products; eg; Teradata; offer asolution; but are usually prohibitively expensive at this scale. Besides; many of the peoplewho analyze this data are entrenched procedural programmers; who find the declarative;SQL style to be unnatural. The success of the more procedural map-reduce programmingmodel; and its associated scalable implementations on commodity hardware; is evidence ofthe above. However; the map-reduce paradigm is too low-level and rigid; and leads to agreat deal of custom user code that is hard to maintain; and reuse. We describe a newlanguage called Pig Latin that we have designed to fit in a sweet spot between the …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,2082
Query Processing; Resource Management; and Approximation in a Data Stream Management System–,Rajeev Motwani; Jennifer Widom; Arvind Arasu; Brian Babcock; Shivnath Babu; Mayur Datar; Gurmeet Manku; Chris Olston; Justin Rosenstein; Rohit Varma,Abstract This paper describes our ongoing work developing the Stanford Stream DataManager (STREAM); a system for executing continuous queries over multiple continuousdata streams. The STREAM system supports a declarative query language; and it copes withhigh data rates and query workloads by providing approximate answers when resources arelimited. This paper describes specific contributions made so far and enumerates our nextsteps in developing a general-purpose Data Stream Management System.,IN CIDR,2003,996
What's new on the web?: the evolution of the web from a search engine perspective,Alexandros Ntoulas; Junghoo Cho; Christopher Olston,Abstract We seek to gain improved insight into how Web search engines shouldcope withthe evolving Web; in an attempt to provide users with themost up-to-date results possible.For this purpose we collectedweekly snapshots of some 150 Web sites over the course ofone year; and measured the evolution of content and link structure. Our measurements focuson aspects of potential interest to search engine designers: the evolution of link structureover time; the rate ofcreation of new pages and new distinct content on the Web; and the rateof change of the content of existing pages under search-centric measures of degree ofchange. Our findings indicate a rapid turnover rate of Web pages; ie; high rates of birth anddeath; coupled with an even higher rate ofturnover in the hyperlinks that connect them. Forpages that persistover time we found that; perhaps surprisingly; the degree of contentshift …,Proceedings of the 13th international conference on World Wide Web,2004,579
Adaptive filters for continuous queries over distributed data streams,Chris Olston; Jing Jiang; Jennifer Widom,Abstract We consider an environment where distributed data sources continuously streamupdates to a centralized processor that monitors continuous queries over the distributeddata. Significant communication overhead is incurred in the presence of rapid updatestreams; and we propose a new technique for reducing the overhead. Users registercontinuous queries with precision requirements at the central stream processor; whichinstalls filters at remote data sources. The filters adapt to changing conditions to minimizestream rates while guaranteeing that all continuous queries still receive the updatesnecessary to provide answers of adequate precision at all times. Our approach enablesapplications to trade precision for communication overhead at a fine granularity byindividually adjusting the precision constraints of continuous queries over streams in a …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,569
Distributed top-k monitoring,Brian Babcock; Chris Olston,Abstract The querying and analysis of data streams has been a topic of much recent interest;motivated by applications from the fields of networking; web usage analysis; sensorinstrumentation; telecommunications; and others. Many of these applications involvemonitoring answers to continuous queries over data streams produced at physicallydistributed locations; and most previous approaches require streams to be transmitted to asingle location for centralized processing. Unfortunately; the continual transmission of alarge number of rapid data streams to a central location can be impractical or expensive. Westudy a useful class of queries that continuously report the k largest values obtained fromdistributed data streams (" top-k monitoring queries"); which are of particular interestbecause they can be used to reduce the overhead incurred while running other types of …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,514
Building a high-level dataflow system on top of Map-Reduce: the Pig experience,Alan F Gates; Olga Natkovich; Shubham Chopra; Pradeep Kamath; Shravan M Narayanamurthy; Christopher Olston; Benjamin Reed; Santhosh Srinivasan; Utkarsh Srivastava,Abstract Increasingly; organizations capture; transform and analyze enormous data sets.Prominent examples include internet companies and e-science. The Map-Reduce scalabledataflow paradigm has become popular for these applications. Its simple; explicit dataflowprogramming model is favored by some over the traditional high-level declarative approach:SQL. On the other hand; the extreme simplicity of Map-Reduce leads to much low-levelhacking to deal with the many-step; branching dataflows that arise in practice. Moreover;users must repeatedly code standard operations such as join by hand. These practiceswaste time; introduce bugs; harm readability; and impede optimizations. Pig is a high-leveldataflow system that aims at a sweet spot between SQL and Map-Reduce. Pig offers SQL-style high-level data manipulation constructs; which can be assembled in an explicit …,Proceedings of the VLDB Endowment,2009,466
Web crawling,Christopher Olston; Marc Najork,Abstract This is a survey of the science and practice of web crawling. While at first glanceweb crawling may appear to be merely an application of breadth-first-search; the truth is thatthere are many challenges ranging from systems concerns such as managing very largedata structures to theoretical questions such as how often to revisit evolving content sources.This survey outlines the fundamental challenges and describes the state-of-the-art modelsand solutions. It also highlights avenues for future work.,Foundations and Trends in Information Retrieval,2010,302
ScentTrails: Integrating browsing and searching on the Web,Christopher Olston; Ed H Chi,Abstract The two predominant paradigms for finding information on the Web are browsingand keyword searching. While they exhibit complementary advantages; neither paradigmalone is adequate for complex information goals that lend themselves partially to browsingand partially to searching. To integrate browsing and searching smoothly into a singleinterface; we introduce a novel approach called ScentTrails. Based on the concept ofinformation scent developed in the context of information foraging theory; ScentTrailshighlights hyperlinks to indicate paths to search results. This interface enables users tointerpolate smoothly between searching and browsing to locate content matching complexinformation goals effectively. In a preliminary user study; ScentTrails enabled subjects to findinformation more quickly than by either searching or browsing alone.,ACM Transactions on Computer-Human Interaction (TOCHI),2003,289
Adaptive precision setting for cached approximate values,Chris Olston; Boon Thau Loo; Jennifer Widom,Abstract Caching approximate values instead of exact values presents an opportunity forperformance gains in exchange for decreased precision. To maximize the performanceimprovement; cached approximations must be of appropriate precision: approximations thatare too precise easily become invalid; requiring frequent refreshing; while overly impreciseapproximations are likely to be useless to applications; which must then bypass the cache.We present a parameterized algorithm for adjusting the precision of cached approximationsadaptively to achieve the best performance as data values; precision requirements; orworkload vary. We consider interval approximations to numeric values but our ideas can beextended to other kinds of data and approximations. Our algorithm strictly generalizesprevious adaptive caching algorithms for exact copies: we can set parameters to require …,ACM SIGMOD Record,2001,257
Finding (recently) frequent items in distributed data streams,Amit Manjhi; Vladislav Shkapenyuk; Kedar Dhamdhere; Christopher Olston,We consider the problem of maintaining frequency counts for items occurring frequently inthe union of multiple distributed data streams. Naive methods of combining approximatefrequency counts from multiple nodes tend to result in excessively large data structures thatare costly to transfer among nodes. To minimize communication requirements; the degree ofprecision maintained by each node while counting item frequencies must be managedcarefully. We introduce the concept of a precision gradient for managing precision whennodes are arranged in a hierarchical communication structure. We then study theoptimization problem of how to set the precision gradient so as to minimize communication;and provide optimal solutions that minimize worst-case communication load over allpossible inputs. We then introduce a variant designed to perform well in practice; with …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,210
Offering a precision-performance tradeoff for aggregation queries over replicated data,Chris Olston; Jennifer Widom,Strict consistency of replicated data is infeasible or not required by many distributedapplications; so current systems often permit stale replication; in which cached copies ofdata values are allowed to become out of date. Queries over cached data return an answerquickly; but the stale answer may be unboundedly imprecise. Alternatively; queries overremote master data return a precise answer; but with potentially poor performance. Tobridge the gap between these two extremes; we propose a new class of replication systemscalled TRAPP (Tradeoff in Replication Precision and Performance). TRAPP systems giveeach user fine-grained control over the tradeoff between precision and performance:Caches store ranges that are guaranteed to bound the current data values; instead of storingstale exact values. Users supply a quantitative precision constraint along with each query …,*,2000,195
Interactive data analysis: The control project,Joseph M Hellerstein; Ron Avnur; Andy Chou; Christian Hidber; Chris Olston; Vijayshankar Raman; Tali Roth; Peter J Haas,Data analysis is fundamentally an iterative process in which you issue a query; receive aresponse; formulate the next query based on the response; and repeat. You usually don'tissue a single; perfectly chosen query and get the information you want from a database;indeed; the purpose of data analysis is to extract unknown information; and in mostsituations; there is no one perfect query. People naturally start by asking broad; big-picturequestions and then continually refine their questions based on feedback and domainknowledge. In the Control (Continuous Output and Navigation Technology with RefinementOnline) project at the University of California; Berkeley; the authors are working withcollaborators at IBM; Informix; and elsewhere to explore ways to improve human-computerinteraction during data analysis. The Control project's goal is to develop interactive …,Computer,1999,194
Best-effort cache synchronization with source cooperation,Chris Olston; Jennifer Widom,Abstract In environments where exact synchronization between source data objects andcached copies is not achievable due to bandwidth or other resource constraints; stale (out-of-date) copies are permitted. It is desirable to minimize the overall divergence between sourceobjects and cached copies by selectively refreshing modified objects. We call the onlineprocess of selecting which objects to refresh in order to minimize divergence best-effortsynchronization. In most approaches to best-effort synchronization; the cache coordinatesthe process and selects objects to refresh. In this paper; we propose a best-effortsynchronization scheduling policy that exploits cooperation between data sources and thecache. We also propose an implementation of our policy that incurs low communicationoverhead even in environments with very large numbers of sources. Our algorithm is …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,166
Stateful bulk processing for incremental analytics,Dionysios Logothetis; Christopher Olston; Benjamin Reed; Kevin C Webb; Ken Yocum,Abstract This work addresses the need for stateful dataflow programs that can rapidly siftthrough huge; evolving data sets. These data-intensive applications perform complex multi-step computations over successive generations of data inflows; such as weekly web crawls;daily image/video uploads; log files; and growing social networks. While programmers maysimply re-run the entire dataflow when new data arrives; this is grossly inefficient; increasingresult latency and squandering hardware resources and energy. Alternatively; programmersmay use prior results to incrementally incorporate the changes. However; current large-scaledata processing tools; such as Map-Reduce or Dryad; limit how programmers incorporateand use state in data-parallel programs. Straightforward approaches to incorporating statecan result in custom; fragile code and disappointing performance. This work presents a …,Proceedings of the 1st ACM symposium on Cloud computing,2010,148
Nova: continuous pig/hadoop workflows,Christopher Olston; Greg Chiou; Laukik Chitnis; Francis Liu; Yiping Han; Mattias Larsson; Andreas Neumann; Vellanki BN Rao; Vijayanand Sankarasubramanian; Siddharth Seth; Chao Tian; Topher ZiCornell; Xiaodan Wang,Abstract This paper describes a workflow manager developed and deployed at Yahoocalled Nova; which pushes continually-arriving data through graphs of Pig programsexecuting on Hadoop clusters.(Pig is a structured dataflow language and runtime for theHadoop map-reduce system.) Nova is like data stream managers in its support for statefulincremental processing; but unlike them in that it deals with data in large batches using disk-based processing. Batched incremental processing is a good fit for a large fraction ofYahoo's data processing use-cases; which deal with continually-arriving data and benefitfrom incremental algorithms; but do not require ultra-low-latency processing.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,133
Recrawl scheduling based on information longevity,Christopher Olston; Sandeep Pandey,Abstract It is crucial for a web crawler to distinguish between ephemeral and persistentcontent. Ephemeral content (eg; quote of the day) is usually not worth crawling; because bythe time it reaches the index it is no longer representative of the web page from which it wasacquired. On the other hand; content that persists across multiple page updates (eg; recentblog postings) may be worth acquiring; because it matches the page's true content for asustained period of time. In this paper we characterize the longevity of information found onthe web; via both empirical measurements and a generative model that coincides with thesemeasurements. We then develop new recrawl scheduling policies that take longevity intoaccount. As we show via experiments over real web data; our policies obtain betterfreshness at lower cost; compared with previous approaches.,Proceedings of the 17th international conference on World Wide Web,2008,130
User-centric web crawling,Sandeep Pandey; Christopher Olston,Abstract Search engines are the primary gateways of information access on the Web today.Behind the scenes; search engines crawl the Web to populate a local indexed repository ofWeb pages; used to answer user search queries. In an aggregate sense; the Web is verydynamic; causing any repository of Web pages to become out of date over time; which inturn causes query answer quality to degrade. Given the considerable size; dynamicity; anddegree of autonomy of the Web as a whole; it is not feasible for a search engine to maintainits repository exactly synchronized with the Web. In this paper we study how to scheduleWeb pages for selective (re) downloading into a search engine repository. The schedulingobjective is to maximize the quality of the user experience for those who query the searchengine. We begin with a quantitative characterization of the way in which the discrepancy …,Proceedings of the 14th international conference on World Wide Web,2005,125
Automatic Optimization of Parallel Dataflow Programs.,Christopher Olston; Benjamin Reed; Adam Silberstein; Utkarsh Srivastava,Abstract Large-scale parallel dataflow systems; eg; Dryad and Map-Reduce; have attractedsignificant attention recently. High-level dataflow languages such as Pig Latin and Sawzallare being layered on top of these systems; to enable faster program development and moremaintainable code. These languages engender greater transparency in program structure;and open up opportunities for automatic optimization. This paper proposes a set ofoptimization strategies for this context; drawing on and extending techniques from thedatabase community.,USENIX Annual Technical Conference,2008,106
Visualizing data with bounded uncertainty,Chris Olston; Jock D Mackinlay,Visualization is a powerful way to facilitate data analysis; but it is crucial that visualizationsystems explicitly convey the presence; nature; and degree of uncertainty to users.Otherwise; there is a danger that data will be falsely interpreted; potentially leading toinaccurate conclusions. A common method for denoting uncertainty is to use error bars orsimilar techniques designed to convey the degree of statistical uncertainty. While uncertaintycan often be modeled statistically; a second form of uncertainty; bounded uncertainty; canalso arise that has very different properties than statistical uncertainty. Error bars should notbe used for bounded uncertainty because they do not convey the correct properties; so adifferent technique should be used instead. We describe a technique for conveying boundeduncertainty in visualizations and show how it can be applied systematically to common …,Information Visualization; 2002. INFOVIS 2002. IEEE Symposium on,2002,97
Shuffling a stacked deck: the case for partially randomized ranking of search engine results,Sandeep Pandey; Sourashis Roy; Christopher Olston; Junghoo Cho; Soumen Chakrabarti,Abstract In-degree; PageRank; number of visits and other measures of Web page popularitysignificantly influence the ranking of search results by modern search engines. Theassumption is that popularity is closely correlated with quality; a more elusive concept that isdifficult to measure directly. Unfortunately; the correlation between popularity and quality isvery weak for newly-created pages that have yet to receive many visits and/or in-links.Worse; since discovery of new content is largely done by querying search engines; andbecause users usually focus their attention on the top few results; newly-created but high-quality pages are effectively" shut out;" and it can take a very long time before they becomepopular. We propose a simple and elegant solution to this problem: the introduction of acontrolled amount of randomness into search result ranking methods. Doing so offers …,Proceedings of the 31st international conference on Very large data bases,2005,89
The discoverability of the web,Anirban Dasgupta; Arpita Ghosh; Ravi Kumar; Christopher Olston; Sandeep Pandey; Andrew Tomkins,Abstract Previous studies have highlighted the high arrival rate of new contenton the web.We study the extent to which this new content can beefficiently discovered by a crawler. Ourstudy has two parts. First; we study the inherent difficulty of the discovery problem usingamaximum cover formulation; under an assumption of perfect estimates oflikely sources oflinks to new content. Second; we relax thisassumption and study a more realistic setting inwhich algorithms mustuse historical statistics to estimate which pages are most likely toyieldlinks to new content. We recommend a simple algorithm thatperforms comparably to allapproaches we consider. We measure the emphoverhead of discovering new content;defined asthe average number of fetches required to discover one new page. Weshow firstthat with perfect foreknowledge of where to explore forlinks to new content; it is possible …,Proceedings of the 16th international conference on World Wide Web,2007,88
Scheduling shared scans of large data files,Parag Agrawal; Daniel Kifer; Christopher Olston,Abstract We study how best to schedule scans of large data files; in the presence of manysimultaneous requests to a common set of files. The objective is to maximize the overall rateof processing these files; by sharing scans of the same file as aggressively as possible;without imposing undue wait time on individual jobs. This scheduling problem arises inbatch data processing environments such as Map-Reduce systems; some of which handletens of thousands of processing requests daily; over a shared set of files. As wedemonstrate; conventional scheduling techniques such as shortest-job-first do not performwell in the presence of cross-job sharing opportunities. We derive a new family ofscheduling policies specifically targeted to sharable workloads. Our scheduling policiesrevolve around the notion that; all else being equal; it is good to schedule nonsharable …,Proceedings of the VLDB Endowment,2008,86
Handling advertisements of unknown quality in search advertising,Sandeep Pandey; Christopher Olston,Abstract We consider how a search engine should select advertisements to display withsearch results; in order to maximize its revenue. Under the standard “pay-per-click”arrangement; revenue depends on how well the displayed advertisements appeal to users.The main difficulty stems from new advertisements whose degree of appeal has yet to bedetermined. Often the only reliable way of determining appeal is exploration via display tousers; which detracts from exploitation of other advertisements known to have high appeal.Budget constraints and finite advertisement lifetimes make it necessary to explore as well asexploit. In this paper we study the tradeoff between exploration and exploitation; modelingadvertisement placement as a multi-armed bandit problem. We extend traditional banditformulations to account for budget constraints that occur in search engine advertising …,Advances in neural information processing systems,2007,79
WIC: A general-purpose algorithm for monitoring web information sources,Sandeep Pandey; Kedar Dhamdhere; Christopher Olston,Abstract The Web is becoming a universal information dissemination medium; due to anumber of factors including its support for content dynamicity. A growing number of Webinformation providers post near real-time updates in domains such as auctions; stockmarkets; bulletin boards; news; weather; roadway conditions; sports scores; etc. Externalparties often wish to capture this information for a wide variety of purposes ranging fromonline data mining to automated synthesis of information from multiple sources. There hasbeen a great deal of work on the design of systems that can process streams of data fromWeb sources; but little attention has been paid to how to produce these data streams; giventhat Web pages generally require" pull-based" access. In this paper we introduce a newgeneral-purpose algorithm for monitoring Web information sources; effectively converting …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,64
Generating example data for dataflow programs,Christopher Olston; Shubham Chopra; Utkarsh Srivastava,Abstract While developing data-centric programs; users often run (portions of) theirprograms over real data; to see how they behave and what the output looks like. Doing somakes it easier to formulate; understand and compose programs correctly; compared withexamination of program logic alone. For large input data sets; these experimental runs canbe time-consuming and inefficient. Unfortunately; sampling the input data does not alwayswork well; because selective operations such as filter and join can lead to empty results oversampled inputs; and unless certain indexes are present there is no way to generate biasedsamples efficiently. Consequently new methods are needed for generating example inputdata for data-centric programs. We focus on an important category of data-centric programs;dataflow programs; which are best illustrated by displaying the series of intermediate data …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,56
Computing the median with uncertainty,Tomas Feder; Rajeev Motwani; Rina Panigrahy; Chris Olston; Jennifer Widom,We consider a new model for computing with uncertainty. It is desired to compute a function f(X 1;...; Xn); where X 1;...; Xn are unknown but guaranteed to lie in specified intervals I 1;...;In. It is possible to query the precise value of any Xj at a cost cj. The goal is to pin down thevalue of f to within a precision δ at a minimum possible cost. We focus on the selectionfunction f which returns the value of the k th smallest argument. We present optimal offlineand online algorithms for this problem.,SIAM Journal on Computing,2003,53
Search result diversity for informational queries,Michael J Welch; Junghoo Cho; Christopher Olston,Abstract Ambiguous queries constitute a significant fraction of search instances and posereal challenges to web search engines. With current approaches the top results for thesequeries tend to be homogeneous; making it difficult for users interested in less popularaspects to find relevant documents. While existing research in search diversification offersseveral solutions for introducing variety into the results; the majority of such work ispredicated; implicitly or otherwise; on the assumption that a single relevant document willfulfill a user's information need; making them inadequate for many informational queries. Inthis paper we present a search-diversification algorithm particularly suitable for informationalqueries by explicitly modeling that the user may need more than one page to satisfy theirneed. This modeling enables our algorithm to make a well-informed tradeoff between a …,Proceedings of the 20th international conference on World wide web,2011,50
System and method for providing a change profile of a web page,*,An improved system and method is provided for adaptively refreshing a web page. A baseversion of the web page may be partitioned into a collection of fragments. Then thecollection of fragments may be compared with the corresponding fragments of a recentversion of the web page to determine a divergence measurement of the difference betweenthe base version and the recent version of the web page. The divergence measurement maybe recorded in a change profile representing a change history of the web page that includesa sequence of numeric pairs indicating a time offset and a divergence measurement of thedifference between a version of the web page at the time offset and a base version of theweb page. The refresh period for the web page may be adjusted by applying an adaptiverefresh policy using the divergence measurements recorded in the change profile.,*,2008,50
A scalability service for dynamic web applications,Christopher Olston; Amit Manjhi; Charles Garrod; Anastassia Ailamaki; Bruce M Maggs; Todd C Mowry,Abstract Providers of dynamic Web applications are currently unable to accommodate heavyusage without significant investment in infrastructure and in-house management capability.Our goal is to develop technology to enable a third party to offer scalability as a subscriptionservice with “per-click” pricing to application providers. To this end we have developed aprototype proxy caching system able to scale delivery of dynamic Web content to a largenumber of users. In this paper we report initial positive results obtained from our prototypethat point to the feasibility of our goal. We also report the shortcomings of our currentprototype; the chief one being the lack of a scalable method of managing data consistency.We then present our initial work on a novel approach to scalable consistency management.Our approach is based on a fully distributed mechanism that does not require content …,*,2005,50
Scalable query result caching for web applications,Charles Garrod; Amit Manjhi; Anastasia Ailamaki; Bruce Maggs; Todd Mowry; Christopher Olston; Anthony Tomasic,Abstract The backend database system is often the performance bottleneck when runningweb applications. A common approach to scale the database component is query resultcaching; but it faces the challenge of maintaining a high cache hit rate while efficientlyensuring cache consistency as the database is updated. In this paper we introduceFerdinand; the first proxy-based cooperative query result cache with fully distributedconsistency management. To maintain a high cache hit rate; Ferdinand uses both a localquery result cache on each proxy server and a distributed cache. Consistency managementis implemented with a highly scalable publish/subscribe system. We implement a fullyfunctioning Ferdinand prototype and evaluate its performance compared to severalalternative query-caching approaches; showing that our high cache hit rate and …,Proceedings of the VLDB Endowment,2008,48
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47
CoScan: cooperative scan sharing in the cloud,Xiaodan Wang; Christopher Olston; Anish Das Sarma; Randal Burns,Abstract We present CoScan; a scheduling framework that eliminates redundant processingin workflows that scan large batches of data in a map-reduce computing environment.CoScan merges Pig programs from multiple users at runtime to reduce I/O contention whileadhering to soft deadline requirements in scheduling. This includes support for joinworkflows that operate on multiple data sources. Our solution maps well to workflows atmany Internet companies which reuse data from a common set of inputs. Experiments on thePigMix data analytics benchmark exhibit orders of magnitude reduction in resourcecontention with minimal impact on latency.,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,46
Navigationaided retrieval,Shashank Pandit; Christopher Olston,Abstract Users searching for information in hypermedia environments often performquerying followed by manual navigation. Yet; the conventional text/hypertext retrievalparadigm does not explicity take post-query navigation into account. This paper proposes anew retrieval paradigm; called navigation-aided retrieval (NAR); which treats both queryingand navigation as first-class activities. In the NAR paradigm; querying is seen as a means toidentify starting points for navigation; and navigation is guided based on informationsupplied in the query. NAR is a generalization of the conventional probabilistic informationretrieval paradigm; which implicitly assumes no navigation takes place. This paper presentsa formal model for navigation-aided retrieval; and reports empirical results that point to thereal-world applicability of the model. The experiments were performed over a large Web …,Proceedings of the 16th international conference on World Wide Web,2007,46
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45
Crawl ordering by search impact,Sandeep Pandey; Christopher Olston,Abstract We study how to prioritize the fetching of new pages under the objective ofmaximizing the quality of search results. In particular; our objective is to fetch new pages thathave the most impact; where the impact of a page is equal to the number of times the pageappears in the top K search results for queries; for some constant K; eg; K= 10. Since theimpact of a page depends on its relevance score for queries; which in turn depends on thepage content; the main difficulty lies in estimating the impact of the page before actuallyfetching it. Hence; impact must be estimated based on the limited information that isavailable prior to fetching page content; eg; the URL string; number of in-links; referringanchortext We formally characterize this problem and study its hardness. We leverage ourformalism to design a new impact-driven crawling policy; and demonstrate its …,Proceedings of the 2008 International Conference on Web Search and Data Mining,2008,39
DataSplash,Chris Olston; Allison Woodruff; Alexander Aiken; Michael Chu; Vuk Ercegovac; Mark Lin; Mybrid Spalding; Michael Stonebraker,Abstract Database visualization is an area of growing importance as database systemsbecome larger and more accessible. DataSplash is an easy-to-use; integrated environmentfor navigating; creating; and querying visual representations of data. We will demonstratethe three main components which make up the DataSplash environment: a navigationsystem; a direct-manipulation interface for creating and modifying visualizations; and a direct-manipulation visual query system.,ACM SIGMOD Record,1998,36
Efficient Monitoring and Querying of Distributed; Dynamic Data via Approximate Replication.,Christopher Olston; Jennifer Widom,Abstract It is increasingly common for an application's data to reside at multiple disparatelocations; while the application requires centralized access to its data. A simple solution is toreplicate all relevant data at a central point; forwarding updates from master copies toreplicas without any special processing or filtering along the way. This scheme maintains up-to-date centralized data; but incurs signficant communication overhead when the data ishighly dynamic; because the volume of updates is large. If communication resources areprecious; communication can be reduced by prioritizing and filtering updates inside thenetwork; at or near the data sources. When updates are dropped; the replicas becomeapproximate rather than exact. Fortunately; many real-world applications involvingdistributed; dynamic data can tolerate approximate data values to some extent; so …,IEEE Data Eng. Bull.,2005,35
Interactive Analysis of Web-Scale Data.,Christopher Olston; Edward Bortnikov; Khaled Elmeleegy; Flavio Junqueira; Benjamin Reed,ABSTRACT We consider how to support interactive querying over webscale data. The basicapproach is to view querying as a two-phase activity: first supply a query template; and latersupply specific instantiations of the template. Interactive responsiveness is offered in thesecond phase only. While instances of this problem have been studied in the past; eg; OLAPand web search; we pursue a more general formulation. Our aim is to build a general two-phase query system.,CIDR,2009,33
DataSplash: A direct manipulation environment for programming semantic zoom visualizations of tabular data,Allison Woodruff; Chris Olston; Alexander Aiken; Michael Chu; Vuk Ercegovac; Mark Lin; Mybrid Spalding; Michael Stonebraker,Abstract We describe DataSplash; a direct manipulation system for creating semantic zoomvisualizations of tabular (relational) data. DataSplash makes contributions in three areas thatare key to the construction of such visualizations. First; DataSplash helps users graphicallyspecify the visual appearance of groups of objects. Second; the system helps users visuallyprogram the way the appearance of groups of objects changes as users browse thevisualization. Third; DataSplash allows users to create groups of graphical links betweencanvases. These direct manipulation facilities simplify the process of constructing semanticzoom applications; particularly ones that display large data sets.,Journal of Visual Languages & Computing,2001,33
Inspector gadget: A framework for custom monitoring and debugging of distributed dataflows,Christopher Olston; Benjamin Reed,Abstract We demonstrate a novel dataflow introspection framework called Inspector Gadget;which makes it easy to create custom monitoring and debugging add-ons to an existingdataflow engine such as Pig. The framework is motivated by a series of informal userinterviews; which revealed that dataflow monitoring and debugging needs are both pressingand diverse. Of the 14 monitoring/debugging behaviors requested by users; we were able toimplement 12 in Inspector Gadget; in just a few hundred lines of (Java) code each.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,32
Approximate replication,Christopher Alden Remi Olston,Abstract In distributed environments that collect or monitor data; useful data may be spreadacross multiple distributed nodes; but users or applications may wish to access that datafrom a central location. A common way to facilitate centralized access to distributed data is tomaintain replicas of data objects of interest at a central location. When data collections arelarge or volatile; keeping replicas consistent with remote master copies poses a significantchallenge due to the large communication cost incurred. Consequently; in many real-worldenvironments exact replica consistency is not maintained; and some form of inexact; orapproximate; replication is typically used instead. Approximate replication is often performedby refreshing replicas periodically. Periodic refreshing allows communication cost to becontrolled; but it does not always make good use of communication resources: In …,*,2003,31
Control: Continuous output and navigation technology with refinement on-line,Ron Avnur; Joseph M Hellerstein; Bruce Lo; Chris Olston; Bhaskaran Raman; Vijayshankar Raman; Tali Roth; Kirk Wylie,Abstract The CONTROL project at UC Berkeley has developed technologies to provideonline behavior for data-intensive applications. Using new query processing algorithms;these technologies continuously improve estimates and confidence statistics. In addition;they react to user feedback; thereby giving the user control over the behavior of long-runningoperations. This demonstration displays the modifications to a database system and theresulting impact on aggregation queries; data visualization; and GUI widgets. We thencompare this interactive behavior to batch-processing alternatives.,ACM SIGMOD Record,1998,30
Goods: Organizing google's datasets,Alon Halevy; Flip Korn; Natalya F Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract Enterprises increasingly rely on structured datasets to run their businesses. Thesedatasets take a variety of forms; such as structured files; databases; spreadsheets; or evenservices that provide access to the data. The datasets often reside in different storagesystems; may vary in their formats; may change every day. In this paper; we present GOODS;a project to rethink how we organize structured datasets at scale; in a setting where teamsuse diverse and often idiosyncratic ways to produce the datasets and where there is nocentralized system for storing and querying them. GOODS extracts metadata ranging fromsalient information about each dataset (owners; timestamps; schema) to relationshipsamong datasets; such as similarity and provenance. It then exposes this metadata throughservices that allow engineers to find datasets within the company; to monitor datasets; to …,Proceedings of the 2016 International Conference on Management of Data,2016,27
Computing shortest paths with uncertainty,Tomás Feder; Rajeev Motwani; Liadan O'Callaghan; Chris Olston; Rina Panigrahy,Abstract We consider the problem of estimating the length of the shortest path from a vertex sto a vertex t in a DAG whose edge lengths are known only approximately but can bedetermined exactly at a cost. Initially; for each edge e; the length of e is known only to liewithin an interval [le; he]; the estimation algorithm can pay we to find the exact length of e.We study the problem of finding the cheapest set of edges such that; if exactly these edgesare queried; the length of the shortest s–t path will be known; within an additive κ⩾ 0; aninput parameter. An actual s–t path; whose true length exceeds that of the shortest s–t pathby at most κ; will be obtained as well. The problem of finding a cheap set of edge queries isin neither NP nor co-NP unless NP= co-NP. We give positive and negative results for twospecial cases and for the general case; which we show is in Σ 2.,Journal of Algorithms,2007,27
System for query scheduling to maximize work sharing,*,A system of query scheduling to maximize work sharing. The system schedules queries toaccount for future queries possessing a sharability component. Included in the system areoperations for assigning an incoming query to a query queue based on a sharabilitycharacteristic of the incoming query; and evaluating a priority function for each member of aplurality of query queues to identify one highest priority query queue. The priority functionaccounts for the probability that a future incoming query will contain the sharabilitycharacteristic common to a member of the plurality of query queues. The system of queryscheduling to maximize work sharing selects a batch of queries from the highest priorityquery queue; and dispatches the batch to one or more query execution units.,*,2011,22
Adaptive file placement in a distributed file system,*,In a distributed system that includes multiple machines; a scheduler attempts to schedule atask on a machine that is not currently overloaded with work. If a task is scheduled on amachine that does not yet have copies of the portions of the data set on which the taskneeds to operate; then that machine obtains copies of those portions from other machinesthat already have them. Whenever a “source” machine ships a copy of a portion to another“destination” machine in the distributed system; the destination machine persistently storesthat copy on the destination machine's persistent storage mechanism. The copy alsoremains on the source machine. Thus; portions of the data set are automatically replicatedwhenever those portions are shipped between machines of the distributed system. Eachmachine in the distributed system has access to “global” information that indicates which …,*,2009,22
System and method for adaptively refreshing a web page,*,An improved system and method is provided for adaptively refreshing a web page. A baseversion of the web page may be partitioned into a collection of fragments. Then thecollection of fragments may be compared with the corresponding fragments of a recentversion of the web page to determine a divergence measurement of the difference betweenthe base version and the recent version of the web page. The divergence measurement maybe recorded in a change profile representing a change history of the web page that includesa sequence of numeric pairs indicating a time offset and a divergence measurement of thedifference between a version of the web page at the time offset and a base version of theweb page. The refresh period for the web page may be adjusted by applying an adaptiverefresh policy using the divergence measurements recorded in the change profile.,*,2014,21
System and method for budgeted generalization search in hierarchies,*,An improved system and method is provided for searching a collection of objects that maybe located in hierarchies of auxiliary information for retrieval of response objects. Aframework to perform a generalization search in hierarchies may be used to generalize asearch by moving up to a higher level in a hierarchy of taxonomies or to specialize a searchby moving down to a lower level in the hierarchy of taxonomies. Once the system maydecide to enumerate response objects at a particular level of generalization; a budgetedgeneralization search may be used for enumerating a set of response objects within abudgeted cost.,*,2011,21
Relaxation in text search using taxonomies,Marcus Fontoura; Vanja Josifovski; Ravi Kumar; Christopher Olston; Andrew Tomkins; Sergei Vassilvitskii,Abstract In this paper we propose a novel document retrieval model in which text queries areaugmented with multi-dimensional taxonomy restrictions. These restrictions may be relaxedat a cost to result quality. This new model may be applicable in many arenas; includingmultifaceted; product; and local search; where documents are augmented with hierarchicalmetadata such as topic or location. We present efficient algorithms for indexing and queryprocessing in this new retrieval model. We decompose query processing into two sub-problems: first; an online search problem to determine the correct overall level of relaxationcost that must be incurred to generate the top k results; and second; a budgeted relaxationsearch problem in which all results at a particular relaxation cost must be produced atminimal cost. We show the latter problem is solvable exactly in two hierarchical …,Proceedings of the VLDB Endowment,2008,18
Parallel evaluation of composite aggregate queries,Lei Chen; Christopher Olston; Raghu Ramakrishnan,Aggregate measures summarizing subsets of data are valuable in exploratory analysis anddecision support; especially when dependent aggregations can be easily specified andcomputed. A novel class of queries; called composite subset measures; was previouslyintroduced to allow correlated aggregate queries to be easily expressed. This paperconsiders how to evaluate composite subset measure queries using a large distributedsystem. We describe a cross-node data redistribution strategy that takes into account thenested structure of a given query. The main idea is to group data into blocks in" cube space";such that aggregations can be generated locally within each block; leveraging previouslyproposed optimizations per-block. The partitioning scheme allows overlap among blocks sothat sliding window aggregation can be handled. Furthermore; it also guarantees that the …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,18
Invalidation clues for database scalability services,Amit Manjhi; Phillip B Gibbons; Anastassia Ailamaki; Charles Garrod; Bruce M Maggs; Todd C Mowry; Christopher Olston; Anthony Tomasic; Haifeng Yu,For their scalability needs; data-intensive Web applications can use a database scalabilityservice (DBSS); which caches applications' query results and answers queries on theirbehalf. One way for applications to address their security/privacy concerns when using aDBSS is to encrypt all data that passes through the DBSS. Doing so; however; causes theDBSS to invalidate large regions of its cache when data updates occur. To invalidate moreprecisely; the DBSS needs help in order to know which results to invalidate; such helpinevitably reveals some properties about the data. In this paper; we present invalidationclues; a general technique that enables applications to reveal little data to the DBSS; yetlimit the number of unnecessary invalidations. Compared with previous approaches;invalidation clues provide applications significantly improved tradeoffs between security …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,18
Simultaneous scalability and security for data-intensive web applications,Amit Manjhi; Anastassia Ailamaki; Bruce M Maggs; Todd C Mowry; Christopher Olston; Anthony Tomasic,Abstract For Web applications in which the database component is the bottleneck; scalabilitycan be provided by a third-party Database Scalability Service Provider (DSSP) that cachesapplication data and supplies query answers on behalf of the application. Cost-effectiveDSSPs will need to cache data from many applications; inevitably raising concerns aboutsecurity. However; if all data passing through a DSSP is encrypted to enhance security; thendata updates trigger invalidation of large regions of cache. Consequently; achieving goodscalability becomes virtually impossible. There is a tradeoff between security and scalability;which requires careful consideration. In this paper we study the security-scalability tradeoff;both formally and empirically. We begin by providing a method for statically identifyingsegments of the database that can be encrypted without impacting scalability …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,18
System and method for determining a quantitative measure of qualitative usability of related Web pages,*,A system and method for determining a quantitative measure of qualitative usability ofrelated Web pages. Web pages are accepted that each include at least one hyperlinkreferencing and proximal cues relating to distal content included in another Web page. Aninformation goal identifying a target Web page is specified. An activation network is formed.A directed graph including nodes corresponding to the Web pages and arcs correspondingto the hyperlinks is built. A weight is assigned to each arc to represent a probability oftraversal of the corresponding hyperlink based on a relatedness of keywords in theinformation goal to the proximal cues included in the referenced Web page. A traversalthrough the activation network to the node corresponding to the target Web page isevaluated as a quantitative measure of usability.,*,2010,17
Spongefiles: Mitigating data skew in mapreduce using distributed memory,Khaled Elmeleegy; Christopher Olston; Benjamin Reed,Abstract Data skew is a major problem for data processing platforms like MapReduce. Skewcauses worker tasks to spill to disk what they cannot fit in memory; which slows down thetask and the overall job. Moreover; performance of other jobs sharing same disk degrades.In many cases; this situation occurs even as the cluster has plenty of spare memory it is justnot used evenly. We introduce SpongeFiles; a novel distributed-memory abstraction tailoredto data processing environments like MapReduce. A SpongeFile is a logical byte array;comprised of large chunks that can be stored in a variety of locations in the cluster. Spilleddata goes to SpongeFiles; which route it to the nearest location with sufficient capacity (localmemory; remote memory; local disk; or remote disk as a last resort). By enabling memory-sapped nodes to tap into the spare capacity of their neighbors; SpongeFiles minimize …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,16
System and method for crawl ordering by search impact,*,An improved system and method for crawl ordering of a web crawler by impact upon searchresults of a search engine is provided. Content-independent features of uncrawled webpages may be obtained; and the impact of uncrawled web pages may be estimated forqueries of a workload using the content-independent features. The impact of uncrawled webpages may be estimated for queries by computing an expected impact score for uncrawledweb pages that match needy queries. Query sketches may be created for a subset of thequeries by computing an expected impact score for crawled web pages and uncrawled webpages matching the queries. Web pages may then be selected to fetch using a combinedquery-based estimate and query-independent estimate of the impact of fetching the webpages on search query results.,*,2011,16
Yedalog: Exploring knowledge at scale,Brian Chin; Daniel von Dincklage; Vuk Ercegovac; Peter Hawkins; Mark S Miller; Franz Och; Christopher Olston; Fernando Pereira,Abstract With huge progress on data processing frameworks; human programmers arefrequently the bottleneck when analyzing large repositories of data. We introduce Yedalog; adeclarative programming language that allows programmers to mix data-parallel pipelinesand computation seamlessly in a single language. By contrast; most existing tools for data-parallel computation embed a sublanguage of data-parallel pipelines in a general-purposelanguage; or vice versa. Yedalog extends Datalog; incorporating not only computationalfeatures from logic programming; but also features for working with data structured as nestedrecords. Yedalog programs can run both on a single machine; and distributed across acluster in batch and interactive modes; allowing programmers to mix different modes ofexecution easily.,LIPIcs-Leibniz International Proceedings in Informatics,2015,15
Computing shortest paths with uncertainty,Tomás Feder; Rajeev Motwani; Liadan O’Callaghan; Chris Olston; Rina Panigrahy,Abstract We consider the problem of estimating the length of a shortest path in a DAG whoseedge lengths are known only approximately but can be determined exactly at a cost. Initially;each edge e is known only to lie within an interval [le; he]; the estimation algorithm can payce to find the exact length of e. In particular; we study the problem of finding the cheapest setof edges such that; if exactly these edges are queried; the length of the shortest path will beknown; within an additive k> 0 that is given as an input parameter. We study both thegeneral problem and several special cases; and obtain both easiness and hardnessapproximation results.,Annual Symposium on Theoretical Aspects of Computer Science,2003,13
Formal language and translator for parallel processing of data,*,The present invention; in an example embodiment; provides a special-purpose formallanguage and translator for the parallel processing of large databases in a distributedsystem. The special-purpose language has features of both a declarative programminglanguage and a procedural programming language and supports the co-grouping of tables;each with an arbitrary alignment function; and the specification of procedural operations tobe performed on the resulting co-groups. The language's translator translates a program inthe language into optimized structured calls to an application programming interface forimplementations of functionality related to the parallel processing of tasks over a distributedsystem. In an example embodiment; the application programming interface includesinterfaces for MapReduce functionality; whose implementations are supplemented by the …,*,2011,12
Scalable consistency management for web database caches,Charles Garrod; Amit Manjhi; Anastassia Ailamaki; Phil Gibbons; Bruce M Maggs; Todd Mowry; Christopher Olston; Anthony Tomasic,Abstract We have built a prototype of a scalable dynamic web-content delivery system;which we call S3. Initial experiments with S3 led us to conclude that the key to achievingscalability lay in reducing the workload on back-end databases. S3 utilizes proxy servers togenerate dynamic content and cache the results of queries forwarded to the back-enddatabase. This approach introduces the challenge of maintaining cache consistency whenthe database is updated. In this paper we introduce a fully-distributed consistencymanagement infrastructure that uses a scalable publish/subscribe substrate to propagateupdate notifications. We use static analysis of the database workload to introduce severaldesign alternatives for how to map database requests to publish/subscribe groups. Finally;we develop a simulation framework and use both simulation and our S3 prototype …,*,2006,11
Getting portals to behave,Chris Olston; Allison Woodruff,Data visualization environments help users understand and analyze their data by permittinginteractive browsing of graphical representations of the data. To further facilitateunderstanding and analysis; many visualization environments have special features knownas portals; which are sub-windows of a data canvas. Portals provide a way to displaymultiple graphical representations simultaneously; in a nested fashion. This makes portalsan extremely powerful and flexible paradigm for data visualization. Unfortunately; with thisflexibility comes complexity. There are over a hundred possible ways each portal can beconfigured to exhibit different behaviors. Many of these behaviors are confusing and certainbehaviors can be inappropriate for a particular setting. It is desirable to eliminate confusingand inappropriate behaviors. The authors construct a taxonomy of portal behaviors and …,Information Visualization; 2000. InfoVis 2000. IEEE Symposium on,2000,10
Ibis: A Provenance Manager for Multi-Layer Systems.,Christopher Olston; Anish Das Sarma,ABSTRACT End-to-end data processing environments are often comprised of severalindependently-developed (sub-) systems; eg for engineering; organizational or historicalreasons. Unfortunately this situation harms usability. For one thing; systems createdindependently tend to have disparate capabilities in terms of what metadata is retained andhow it can be queried. If something goes wrong it can be very difficult to trace executionhistories across the various sub-systems. One solution is to ship each sub-system'smetadata to a central metadata manager that integrates it and offers a powerful and uniformquery interface. This paper describes a metadata manager we are building; called Ibis.Perhaps the greatest challenge in this context is dealing with data provenance queries in thepresence of mixed granularities of metadata—eg rows vs. column groups vs. tables; …,CIDR,2011,9
Method or system for spilling in query environments,*,The Internet is ubiquitous; the World Wide Web provided by the Internet continues to grow withnew content seemingly being added every second. To provide access to stored content; toolsor services may be provided which allow for copious amounts of stored content to besearched. For example; service providers may allow for users to search the World Wide Webor other like networks using search engines. Similar tools or services may allow for one or moredatabases or other like repositories to be searched … Some processing systems access voluminousstored content available via web sites or otherwise associated with websites; such as viahyperlinks; for example; so that the websites may be added to search query rankings; forexample. Stored content may be compiled in groupings of associated records; for example.However; some groupings of records may be large and a processor or similar processing …,*,2013,8
Adaptive materialized view selection for databases,*,Techniques described herein adaptively select materialized view fragments for persistentmaintenance. During an interval of time; the selected fragments are persistently maintainedin the database system; while the other non-selected fragments are not persistentlymaintained as materialized view fragments. Over time; the composition of the set of selectedfragments may change. As queries are executed in the database system over an interval oftime; statistics including the frequency of access of each currently selected fragment duringthat interval are generated. At the start of the next interval of time; based on these statistics;some currently selected fragments may be unselected. Some currently non-selectedfragments of one or more candidate materialized views may be selected based on thestatistics. For the next interval; the newly unselected fragments cease to be persistently …,*,2011,7
Modular web crawling policies and metrics,*,A web crawler loads a policy from a customizable stored module that is separate and distinctfrom the web crawler's source code. The web crawler follows these policies in determiningthe order in which the web crawler will visit and index web pages in an index used by anInternet search engine. As a result; the web crawler's behavior can be modified more easily.The web crawler's behavior can be finely tuned to be more efficient and/or to accommodatethe particular needs of the search engine. Multiple different policies may be maintainedconcurrently in separate stored modules; and the web crawler can be instructed to usedifferent modules' policies at different specified times or under different specifiedcircumstances.,*,2009,7
Configurations: Understanding alternatives for safeguarding data,Bob Mungamuru; Hector Garcia-Molina; Christopher Olston,Configurations are introduced as a new model for the description and analysis of securedata systems. Both the longevity and privacy of sensitive data are considered. The modeluses two basic operators: copy; which replicates data for longevity; and split; whichdecomposes data (eg; into ciphertext and a key) for privacy. The operators can berecursively composed to describe how data and their associated``keys''are managed.Various classes of configurations are defined that have desirable properties with respect tophysical realizability and semantic correctness. Formal techniques are provided to verifythese properties for a given configuration.,*,2005,7
System and method using a refresh policy for incremental updating of web pages,*,An improved system and method is provided for adaptively refreshing a web page. A baseversion of the web page may be partitioned into a collection of fragments. Then thecollection of fragments may be compared with the corresponding fragments of a recentversion of the web page to determine a divergence measurement of the difference betweenthe base version and the recent version of the web page. The divergence measurement maybe recorded in a change profile representing a change history of the web page that includesa sequence of numeric pairs indicating a time offset and a divergence measurement of thedifference between a version of the web page at the time offset and a base version of theweb page. The refresh period for the web page may be adjusted by applying an adaptiverefresh policy using the divergence measurements recorded in the change profile.,*,2008,6
Managing Google's data lake: an overview of the Goods system.,Alon Y Halevy; Flip Korn; Natalya Fridman Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract For most large enterprises today; data constitutes their core asset; along with codeand infrastructure. For most enterprises; the amount of data that they produce internally hasexploded in recent years. At the same time; in many cases; engineers and data scientists donot use centralized data-management systems and end up creating what became known asa data lake—a collection of datasets that often are not well organized or not organized at alland where one needs to “fish” for useful datasets. In this paper; we describe our experiencebuilding and deploying GOODS; a system to manage Google's internal data lake. GOODScrawls Google's infrastructure and builds a catalog of discovered datasets; includingstructured files; databases; spreadsheets; and even services that provide access to the data.GOODS extracts metadata about datasets in a post-hoc way: engineers continue to …,IEEE Data Eng. Bull.,2016,5
System and method for generalization search in hierarchies,*,An improved system and method is provided for searching a collection of objects that maybe located in hierarchies of auxiliary information for retrieval of response objects. Aframework to perform a generalization search in hierarchies may be used to generalize asearch by moving up to a higher level in a hierarchy of taxonomies or to specialize a searchby moving down to a lower level in the hierarchy of taxonomies. Once the system maydecide to enumerate response objects at a particular level of generalization; a budgetedgeneralization search may be used for enumerating a set of response objects within abudgeted cost.,*,2008,5
Modeling and scheduling asynchronous incremental workflows,*,Disclosed are methods and apparatus for scheduling an asynchronous workflow having aplurality of processing paths. In one embodiment; one or more predefined constraint metricsthat constrain temporal asynchrony for one or more portions of the workflow may be receivedor provided. Input data is periodically received or intermediate or output data is generatedfor one or more of the processing paths of the workflow; via one or more operators; based ona scheduler process. One or more of the processing paths for generating the intermediate oroutput data are dynamically selected based on received input data or generatedintermediate or output data and the one or more constraint metrics. The selected one ormore processing paths of the workflow are then executed so that each selected processingpath generates intermediate or output data for the workflow.,*,2015,4
Method and system for data provenance management in multi-layer systems,*,Method; system; and programs for heterogeneous data management. Information frommultiple data sources is first obtained. Data/metadata from each of the data sources ismodeled based on the source and/or granularity information of the data/metadata togenerate data/metadata models. The data/metadata from multiple data sources areintegrated; by applying one or more processes to the data/metadata from different datasources based on the data/metadata models; to generate integrated data/metadata. Aprovenance representation for the integrated data/metadata is created tracing sources;granularities; and/or processes applied and archived for enabling an query associated withthe integrated data/metadata.,*,2014,4
Iconification and omission in information exploration,Allison Woodruff; Chris Olston,Because users find it difficult and unpleasant to explore cluttered displays; a number ofvisualization systems reduce clutter by allowing users to view detail selectively. Early workincludes fisheye views and the Spatial Data Management System (SDMS)[2; 3]. More recentparadigms include the Pad zoomable interface and Magic Lenses [4; 1].By its very nature;this strategy for clutter reduction eliminates certain details from the display. Two processesare common. First; a graphical representation may be iconified. Iconification is the process oftransforming a graphical representation into a more compressed graphical representation.This compression may take a number of forms. For example; the graphical representationmay change shape or detail may be omitted. Note that information may be lost duringiconification. Consider a circle that represents a city. Suppose the radius of the circle is …,SIGCHI’98 Workshop on Innovation and Evaluation in Information Exploration Interfaces,1998,4
Generating example data for testing database queries,*,To avoid this slowness; users sometimes create a side database consisting of a small sampleof the original one; for experimentation. Unfortunately; this method does not always workwell. For example; suppose the query performs a join of two tables A(x;y) and B(x;z) with thejoin predicate specifying equality of the two x attributes. If the original data contains many distinctvalues for x; then it is unlikely that a small sample of A and a small sample of B contain any matchingx values. Hence the join query over the sample database may well produce an empty result;even if the query is correct. Similarly; a query with a selective filter executed on a data samplemay produce an empty result. In general it can be difficult to test the semantics of a query overa sample database … In one aspect; embodiments relate to computer-implemented methodsfor testing database query semantics. For example; some embodiments relate to a …,*,2010,3
The case for being lazy: How to leverage lazy evaluation in MapReduce,Kristi Morton; Magdalena Balazinska; Dan Grossman; Christopher Olston,Abstract In this paper; we study the benefits and overheads of lazy MapReduce processing;where the input data is partitioned and only the smallest subset of these partitions areprocessed to meet a user's need at any time. We also develop guidelines for successfullyapplying the lazy MapReduce computation technique to reduce processing times of analysistasks.,Proceedings of the 2nd international workshop on Scientific cloud computing,2011,2
Graceful logic evolution in web data processing workflows,Christopher Olston,ABSTRACT Data processing workflows evolve over time. For example; operators of webinformation extraction workflows change them continually by retraining classifiers; adjustingconfidence thresholds; extracting additional structured data fields; and incorporating newinformation sources. In most workflow systems; even minor workflow logic changes triggerrecomputation of derived data products from scratch. When data volumes are large thisapproach is grossly wasteful and generates huge latency hiccups.,environments,*,2
System and method for automatic detection of needy queries,*,The present invention relates to methods; systems; and computer readable mediacomprising instructions for identifying needy queries for which additional responsive contentis needed. The method of the present invention comprises receiving a query comprising oneor more terms and retrieving one or more content items identified as responsive to the query;the one or more content items ranked according to one or more ranking techniques. A scoreis generated for the one or more ranked content items identified as responsive to the query.A determination is thereafter made as to whether the query is needy based upon acomparison of the one or more scores associated with the one or more content itemsidentified as responsive to the query and a needy query score threshold.,*,2012,1
Configurations: a model for distributed data storage,Bob Mungamuru; Hector Garcia-Molina; Christopher Olston,There are many ways to safeguard data from loss and unauthorized access; but there aretwo fundamental operations that cover many of the options: Copy and what we call Split.Making copies of data (ie; replicating across n sites) safeguards against the loss of data;whereas splitting data safeguards against unauthorized access. With a Split; data is“decomposed” into n shares (eg; ciphertext and n− 1 keys) and distributed across n sites; insuch a way that all n shares are needed to reconstruct the original data; and access by anadversary to any proper subset of the shares is not a security breach. The Split operator canbe implemented in many ways. For instance; in a 3-way Split (ie; n= 3) of a file; two sharescan be randomly generated sequences of bits and the third share can be the bits of our fileXOR-ed with the random sequences. From our point of view; a Split might even be a …,Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing,2007,1
TensorFlow-Serving: Flexible; High-Performance ML Serving,Christopher Olston; Noah Fiedel; Kiril Gorovoy; Jeremiah Harmsen; Li Lao; Fangwei Li; Vinu Rajashekhar; Sukriti Ramesh; Jordan Soyke,Abstract: We describe TensorFlow-Serving; a system to serve machine learning modelsinside Google which is also available in the cloud and via open-source. It is extremelyflexible in terms of the types of ML platforms it supports; and ways to integrate with systemsthat convey new models and updated versions from training to serving. At the same time; thecore code paths around model lookup and inference have been carefully optimized to avoidperformance pitfalls observed in naive implementations. Google uses it in many productiondeployments; including a multi-tenant model hosting service called TFS.,arXiv preprint arXiv:1712.06139,2017,*
Post-hoc management of datasets,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for generating a catalog for multiple datasets; the method comprisingaccessing multiple extant data sets; the extant data sets including data sets that areindependently generated and structurally dissimilar; organizing the data sets intocollections; each data set in each collection belonging to the collection based on collectiondata associated with the data set; for each collection of data sets: determining; from a subsetof the data sets that belong to the collection; metadata that describe the data sets that belongto the collection; wherein the metadata does not include the collection data; and attributing;to other data sets in the collection; the metadata determined from the subset of data sets;and generating; from the collections of data sets and the determined metadata; a catalog …,*,2017,*
Batching inputs to a machine learning model,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for batching inputs to machine learning models. One of the methods includesreceiving a stream of requests; each request identifying a respective input for processing bya first machine learning model; adding the respective input from each request to a firstqueue of inputs for processing by the first machine learning model; determining; at a firsttime; that a count of inputs in the first queue as of the first time equals or exceeds a maximumbatch size and; in response: generating a first batched input from the inputs in the queue asof the first time so that a count of inputs in the first batched input equals the maximum batchsize; and providing the first batched input for processing by the first machine learning model.,*,2017,*
Automatically Scaling Multi-Tenant Machine Learning,Steven J Ross; Christopher Olston; Noah Fiedel,Abstract Generally; the present disclosure is directed to optimizing use of computingresources in a system. In particular; in some implementations; the systems and methods ofthe present disclosure can include or otherwise leverage one or more machine-learnedmodels to predict task allocation for a job serving a plurality of machine-learned modelsbased on current system state and queries per second (QPS) data for the plurality of models.Alternatively; the tasks can be allocated according to one or more rules (eg; a new task isallocated to a job until the compute usage for the job falls below a scaling threshold). Thus;the systems and methods of the present disclosure are able to efficiently serve a mix of high-QPS and low-QPS machine-learned models at low latency with minimal waste of computeresources (eg; CPU; GPU; TPU; etc.) and memory (eg; RAM).,*,2017,*
Elastic multi-resolution model-serving to compute inferences,Christopher Olston; Noah Fiedel; Ed H Chi; Alexander Beutel,Abstract Machine-learning models are consuming an increasing fraction of the world'scomputing resources. The cost of computing inferences with some machine-learning modelsis extremely high. Provisioning computing resources for peak performance; eg; highavailability and quality of service; entails the creation of headroom for traffic spikes(increases in demand) and preparing for the possibility of outages (decreases in capacity).Executing computer applications that utilize machine-learning models; also known asmachine-learned models; can require significant capital and operational expenses.,*,2017,*
Virtual environment spanning desktop and cloud,*,A method and system are given for providing a virtual environment spanning a desktop anda cloud. In one example; the method includes receiving a query template over a data set thatresides in the cloud; optimizing the query template to segment the query template into anoffline phase and an online phase; executing the offline phase on the cloud to build one ormore indexes; and sending the one or more indexes to the desktop.,*,2015,*
Virtual environment spanning desktop and cloud,*,A method and system are given for providing a virtual environment spanning a desktop anda cloud. In one example; the method includes receiving a query template over a data set thatresides in the cloud; optimizing the query template to segment the query template into anoffline phase and an online phase; executing the offline phase on the cloud to build one ormore indexes; and sending the one or more indexes to the desktop.,*,2014,*
Exploring large textual data sets via interactive aggregation,*,A method and a system are provided for exploring a large textual data set via interactiveaggregation. In one example; the method includes receiving the large textual data set andan original query template; building an index for the query template; wherein the building theindex comprises ordering the index a particular way to optimize query time; receiving one ormore bindings for the original query template; computing an answer to the original querytemplate using the index and the one or more bindings; and anticipating one or more futurequeries that a user may submit and that are related to the original query template.,*,2014,*
This Presentation,Rajeev Motwani; Jennifer Widom; Arvind Arasu; Brian Babcock; Shivnath Babu; Mayur Datar; Gurmeet Manku; Chris Olston; Justin Rosenstein; Rohit Varma; Abram Hindle,Page 1. Query Processing; Approximation; and Resource Management in a DSMS CS856Query Processing; Approximation; and Resource Management in a Data Stream ManagementSystem Rajeev Motwani; Jennifer Widom; Arvind Arasu; Brian Babcock; Shivnath Babu; MayurDatar; Gurmeet Manku; Chris Olston; Justin Rosenstein; Rohit Varma Presentation by: AbramHindle Department of Computer Science University of Waterloo ahindle@cs.uwaterloo.caOctober 4; 2005 Abram Hindle 1 Query Processing; Approximation; and Resource Managementin a DSMS CS856 This Presentation • What am I going to cover? – Authors – Introduction –DSMS – STREAM – Summary Abram Hindle 2 Query Processing; Approximation; and ResourceManagement in a DSMS CS856 Authors …,*,2005,*
WIC: A General-Purpose Algorithm for Monitoring,Sandeep Pandey; Kedar Dhamdhere; Christopher Olston,Abstract The Web is becoming a universal information dis-semination medium; due to anumber of factors including its support for content dynamicity. A growing number of Webinformation providers post near real-time updates in domains such as auctions; stockmarkets; bulletin boards; news; weather; roadway conditions; sports scores; etc. Externalparties often wish to capture this information for a wide variety of purposes ranging fromonline data mining to automated synthesis of information from multiple sources. There hasbeen a great deal of work on the design of sys-tems that can process streams of data fromWeb sources; but little attention has been paid to how to produce these data streams; giventhat Web pages generally require “pull-based” access. In this paper we introduce a newgeneralpurpose algorithm for monitoring Web information sources; effectively converting …,Proceedings 2004 VLDB Conference: The 30th International Conference on Very Large Databases (VLDB),2004,*
Feder; Tomás,Surender Baswana; Benjamín Callejas Bedregal; Christiano Braga; Sergio Cabello; Amin Coja-Oghlan; Robson da Luz; Clare Dixon; Mírian Halfeld Ferrari; Ramesh Hariharan; Narciso Martí-Oliet; Rajeev Motwani; Martin A Musicante; Cláudia Nalon; Liadan O’Callaghan; Chris Olston; Miguel Palomino; Rina Panigrahy; Sandeep Sen; Alberto Verdejo,*,*,*,*
15-740/18-740 Computer Architecture; Fall 2012 Papers for In-Class Discussions,Christopher Olston; Benjamin Reed; Utkarsh Srivastava; Ravi Kumar; Andrew Tomkins; John Nickolls; Ian Buck; Michael Garland; Kevin Skadron,• Jeffrey Dean and Sanjay Ghemawat. “MapReduce: simplified data processing on largeclus- ters;” in Commun. ACM 51; 1 (January 2008); 107-113. • Michael Isard; Mihai Budiu; YuanYu; Andrew Birrell; and Dennis Fetterly. “Dryad: dis- tributed data-parallel programs from sequentialbuilding blocks;” in Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer Systems 2007 (EuroSys '07). • Christopher Olston; Benjamin Reed; UtkarshSrivastava; Ravi Kumar; and Andrew Tomkins. “Pig latin: a not-so-foreign language for dataprocessing;” in Proceedings of the 2008 ACM SIGMOD international conference on Managementof data (SIGMOD '08). • Sungpack Hong; Hassan Chafi; Edic Sedlar; and Kunle Olukotun.“Green-Marl: a DSL for easy and efficient graph analysis;” in Proceedings of the seventeenthinternational conference on Architectural Support for Programming Languages and …,*,*,*
Program Committees,Sihem Amer Yahia; Kevin Beyer; CWI Peter Boncz; Netherlands Angela Bonifati; Arbee Chen; Jan Chomicki; Bobbie Cochrane; Latha Colby; Ada Fu; Sumit Ganguly; Torsten Grust; Raghav Kaushik; Arnd Christian Konig; Sailesh Krishnamurthy; Amalgamated Insight; David Lomet; Christopher Olston; Fatma Ozcan; Neoklis Polyzotis; Raghu Ramakrishnan; Krithi Ramamritham; Vijayshankar Raman; Rajeev Rastogi; Mary Roth; Jerome Simeon; Ioana Stanoi; Andrew Tomkins; AUEB Vasilis Vassalos; Greece Victor Vianu; Min Wang; Aidong Zhang,Foto Afrati; NTUA; Greece Natassa Ailamaki; CMU; USA Sihem Amer Yahia; YahooResearch; USA Paolo Atzeni; Univ. di Roma Tre; Italy Shivnath Babu; Duke Univ.; USA JamesA. Bailey; Univ. of Melbourne; Australia Magdalena Balazinska; Univ. of Washington; USA KevinBeyer; IBM Research; USA Michael Boehlen; Univ. of Bolzano; Italy Peter Boncz; CWI; NetherlandsAngela Bonifati; CNR; Italy Arbee Chen; National Tsing Hua Univ.; Taiwan Mitch Cherniack;Brandeis Univ.; USA Jan Chomicki; SUNY Buffalo; USA Stavros Christodoulakis; Tech Univ.of Crete; Greece Bobbie Cochrane; IBM Research; USA Edith Cohen; AT&T Labs; USA LathaColby; IBM Research; USA Graham Cormode; AT&T Labs; USA Abhinandan Das; GoogleLabs; USA Amol Deshpande; Univ. of Maryland; USA Alin Dobra; Univ. of Florida; USA ElenaFerrari; Univ. of Insubria; Italy J. Christoph Freytag; Humboldt Univ.; Germany Ada Fu …,*,*,*
A Spatial Model for Nested Multiscale Interfaces,Chris Olston,Abstract Significant recent interest has been devoted to the multiscale interface as aparadigm for user-driven exploration of large or complex information worlds. Multiscaleinterfaces sometimes permit nesting of worlds; a notion that underpins a diverse variety ofnavigation aides and multiple-view tools such as visual hyperlinks; bookmarks; filters;magnifying glasses; overview and detail views; and coordinated views; all of which can beused to enhance the effectiveness of information exploration. These constructs often behavein accordance with underlying spatial relationships among nested views. Unfortunately;researchers and designers currently lack tools to help them describe and reason formallyabout spatial relationships tying together nested virtual worlds. The absence of appropriateformalisms is problematic because multiple spatial models are available and; even under …,*,*,*
