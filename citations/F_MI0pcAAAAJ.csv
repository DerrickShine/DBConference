Answering queries using views: A survey,Alon Y Halevy,Abstract. The problem of answering queries using views is to find efficient methods ofanswering a query using a set of previously defined materialized views over the database;rather than accessing the database relations. The problem has recently received significantattention because of its relevance to a wide variety of data management problems. In queryoptimization; finding a rewriting of a query using a set of materialized views can yield a moreefficient query execution plan. To support the separation of the logical and physical views ofdata; a storage schema can be described using views over the logical schema. As a result;finding a query execution plan that accesses the storage amounts to solving the problem ofanswering queries using views. Finally; the problem arises in data integration systems;where data sources can be described as precomputed views over a mediated schema …,The VLDB Journal,2001,1787
Querying heterogeneous information sources using source descriptions,Alon Levy; Anand Rajaraman; Joann Ordille,We witness a rapid increase in the number of structured information sources that areavailable online; especially on the WWW. These sources include commercial databases onproduct information; stock market information; real estate; automobiles; and entertainment.We would like to use the data stored in these databases to answer complex queries that gobeyond keyword searches. We face the following challenges:(1) Several informationsources store interrelated data; and any query-answering system must understand therelationships between their contents.(2) Many sources are not full-featured databasesystems and can answer only a small set of queries over their data (for example; forms onthe WWW restrict the set of queries one can (3) Since the number of sources is very large;effective techniques are needed to prune the set of information sources accessed to …,*,1996,1616
A query language for XML,Alin Deutsch; Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,Abstract An important application of XML is the interchange of electronic data (EDI) betweenmultiple data sources on the Web. As XML data proliferates on the Web; applications willneed to integrate and aggregate data from multiple source and clean and transform data tofacilitate exchange. Data extraction; conversion; transformation; and integration are all well-understood database problems; and their solutions rely on a query language. We present aquery language for XML; called XML-QL; which we argue is suitable for performing theabove tasks. XML-QL is a declarative;relational complete'query language and is simpleenough that it can be optimized. XML-QL can extract data from existing XML documents andconstruct new XML documents.,Computer networks,1999,1343
Learning to map between ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Abstract Ontologies play a prominent role on the Semantic Web. They make possible thewidespread publication of machine understandable data; opening myriad opportunities forautomated information processing. However; because of the Semantic Web's distributednature; data on it will inevitably come from many different ontologies. Information processingacross ontologies is not possible without knowing the semantic mappings between theirelements. Manually finding such mappings is tedious; error-prone; and clearly not possibleat the Web scale. Hence; the development of tools to assist in the ontology mapping processis crucial to the success of the Semantic Web. We describe glue; a system that employsmachine learning techniques to find such mappings. Given two ontologies; for each conceptin one ontology glue finds the most similar concept in the other ontology. We give well …,Proceedings of the 11th international conference on World Wide Web,2002,1319
Crowdsourcing systems on the world-wide web,Anhai Doan; Raghu Ramakrishnan; Alon Y Halevy,As is typical for an emerging area; this effort has appeared under many names; including peerproduction; user-powered systems; user-generated content; collaborative systems; communitysystems; social systems; social search; social media; collective intelligence; wikinomics; crowdwisdom; smart mobs; mass collaboration; and human computation. The topic has been discussedextensively in books; popular press; and academia. 1;5;15;23;29;35 But this body of work hasconsidered mostly efforts in the physical world. 23;29;30 Some do consider crowdsourcing systemson the Web; but only certain system types 28;33 or challenges (for example; how to evaluateusers 12 ) … This survey attempts to provide a global picture of crowdsourcing systems on theWeb. We define and classify such systems; then describe a broad sample of systems. The sampleranges from relatively simple well-established systems such as reviewing books to …,Communications of the ACM,2011,1291
Reconciling schemas of disparate data sources: A machine-learning approach,AnHai Doan; Pedro Domingos; Alon Y Halevy,Abstract A data-integration system provides access to a multitude of data sources through asingle mediated schema. A key bottleneck in building such systems has been the laboriousmanual construction of semantic mappings between the source schemas and the mediatedschema. We describe LSD; a system that employs and extends current machine-learningtechniques to semi-automatically find such mappings. LSD first asks the user to provide thesemantic mappings for a small set of data sources; then uses these mappings together withthe sources to train a set of learners. Each learner exploits a different type of informationeither in the source schemas or in their data. Once the learners have been trained; LSDfinds semantic mappings for a new data source by applying the learners; then combiningtheir predictions using a meta-learner. To further improve matching accuracy; we extend …,ACM Sigmod Record,2001,996
Answering queries using views,Alon Y Levy; Alberto O Mendelzon; Yehoshua Sagiv,Abstract We consider the problem of computing answers to queries by using materializedviews. Aside from its potential in optimizing query evaluation; the problem also arises inapplications such as Global Information Systems; Mobile Computing and maintainingphysical data independence. We consider the problem of finding a rewriting of a query thatuses the materialized views; the problem of finding minimal rewritings; and finding completerewritings(ie; rewritings that use only the views). We show that all the possible rewritings canbe obtained by considering cent ainment mappings from the views to the query; and that theproblems we consider are NP-complete when both the query and the views are conjunctiveand don't involve builtin comparison predicates. We show that the problem has twoindependent sources of complexity(the number of possible containment mappings; and …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,921
Similarity search for web services,Xin Dong; Alon Halevy; Jayant Madhavan; Ema Nemes; Jun Zhang,Abstract Web services are loosely coupled software components; published; located; andinvoked across the web. The growing number of web services available within anorganization and on the Web raises a new and challenging search problem: locatingdesired web services. Traditional keyword search is insufficient in this context: the specifictypes of queries users require are not captured; the very small text fragments in web servicesare unsuitable for keyword search; and the underlying structure and semantics of the webservices are not exploited. We describe the algorithms underlying the Woogle search enginefor web services. Woogle supports similarity search for web services; such as finding similarweb-service operations and finding operations that compose with a given one. We describenovel techniques to support these types of searches; and an experimental study on a …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,896
Database techniques for the World-Wide Web: A survey,Daniela Florescu; Alon Y Levy; Alberto O Mendelzon,The popularity of the World-Wide Web (WWW) has made it a prime vehicle for disseminatinginformation. The relevance of database concepts to the problems of managing and queryingthis information has led to a signi cant body of recent research addressing these problems.Even though the underlying challenge is the one that has been traditionally addressed bythe database community {how to manage large volumes of data {the novel context of theWWW forces us to signi cantly extend previous techniques. The primary goal of this survey isto classify the di erent tasks to which database concepts have been applied; and toemphasize the technical innovations that were required to do so. We do not claim thatdatabase technology is the magic bullet that will solve all web information managementproblems; other technologies; such as Information Retrieval; Arti cial Intelligence; and …,SIGMOD record,1998,882
Data integration: the teenage years,Alon Halevy; Anand Rajaraman; Joann Ordille,Data integration is a pervasive challenge faced in applications that need to query acrossmultiple autonomous and heterogeneous data sources. Data integration is crucial in largeenterprises that own a multitude of data sources; for progress in large-scale scientificprojects; where data sets are being produced independently by multiple researchers; forbetter cooperation among government agencies; each with their own data sources; and inoffering good search quality across the millions of structured data sources on the World-Wide Web. Ten years ago we published “Querying Heterogeneous Information Sourcesusing Source Descriptions”[73]; a paper describing some aspects of the Information Manifolddata integration project. The Information Manifold and many other projects conducted at thetime [5; 6; 20; 25; 38; 43; 51; 66; 100] have led to tremendous progress on data …,Proceedings of the 32nd international conference on Very large data bases,2006,850
From databases to dataspaces: a new abstraction for information management,Michael Franklin; Alon Halevy; David Maier,Abstract The development of relational database management systems served to focus thedata management community for decades; with spectacular results. In recent years;however; the rapidly-expanding demands of" data everywhere" have led to a field comprisedof interesting and productive efforts; but without a central focus or coordinated agenda. Themost acute information management challenges today stem from organizations (eg;enterprises; government agencies; libraries;" smart" homes) relying on a large number ofdiverse; interrelated data sources; but having no way to manage their dataspaces in aconvenient; integrated; or principled fashion. This paper proposes dataspaces and theirsupport systems as a new agenda for data management. This agenda encompasses muchof the work going on in data management today; while posing additional research …,ACM Sigmod Record,2005,788
The unreasonable effectiveness of data,Alon Halevy; Peter Norvig; Fernando Pereira,At Brown University; there is excitement of having access to the Brown Corpus; containingone million English words. Since then; we have seen several notable corpora that are about100 times larger; and in 2006; Google released a trillion-word corpus with frequency countsfor all sequences up to five words long. In some ways this corpus is a step backwards fromthe Brown Corpus: it's taken from unfiltered Web pages and thus contains incompletesentences; spelling errors; grammatical errors; and all sorts of other errors. It's not annotatedwith carefully hand-corrected part-of-speech tags. But the fact that it's a million times largerthan the Brown Corpus outweighs these drawbacks. A trillion-word corpus-along with otherWeb-derived corpora of millions; billions; or trillions of links; videos; images; tables; and userinteractions-captures even very rare aspects of human behavior. So; this corpus could …,IEEE Intelligent Systems,2009,748
Reference reconciliation in complex information spaces,Xin Dong; Alon Halevy; Jayant Madhavan,Abstract Reference reconciliation is the problem of identifying when different references (ie;sets of attribute values) in a dataset correspond to the same real-world entity. Most previousliterature assumed references to a single class that had a fair number of attributes (eg;research publications). We consider complex information spaces: our references belong tomultiple related classes and each reference may have very few attribute values. A primeexample of such a space is Personal Information Management; where the goal is to providea coherent view of all the information on one's desktop. Our reconciliation algorithm hasthree principal features. First; we exploit the associations between references to design newmethods for reference comparison. Second; we propagate information betweenreconciliation decisions to accumulate positive and negative evidences. Third; we …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,636
Learning to match ontologies on the semantic web,AnHai Doan; Jayant Madhavan; Robin Dhamankar; Pedro Domingos; Alon Halevy,Abstract. On the Semantic Web; data will inevitably come from many different ontologies;and information processing across ontologies is not possible without knowing the semanticmappings between them. Manually finding such mappings is tedious; error-prone; andclearly not possible on the Web scale. Hence the development of tools to assist in theontology mapping process is crucial to the success of the Semantic Web. We describeGLUE; a system that employs machine learning techniques to find such mappings. Giventwo ontologies; for each concept in one ontology GLUE finds the most similar concept in theother ontology. We give well-founded probabilistic definitions to several practical similaritymeasures and show that GLUE can work with all of them. Another key feature of GLUE isthat it uses multiple learning strategies; each of which exploits well a different type of …,The VLDB Journal,2003,618
Semantic integration research in the database community: A brief survey,AnHai Doan; Alon Y Halevy,Abstract Semantic integration has been a long-standing challenge for the databasecommunity. It has received steady attention over the past two decades; and has nowbecome a prominent area of database research. In this article; we first review databaseapplications that require semantic integration and discuss the difficulties underlying theintegration process. We then describe recent progress and identify open research issues.We focus in particular on schema matching; a topic that has received much attention in thedatabase community; but also discuss data matching (for example; tuple deduplication) andopen issues beyond the match discovery context (for example; reasoning with matches;match verification and repair; and reconciling inconsistent data values). For previoussurveys of database research on semantic integration; see Rahm and Bernstein (2001); …,AI magazine,2005,617
Ontology matching: A machine learning approach,AnHai Doan; Jayant Madhavan; Pedro Domingos; Alon Halevy,Summary This chapter studies ontology matching: the problem of finding the semanticmappings between two given ontologies. This problem lies at the heart of numerousinformation processing applications. Virtually any application that involves multipleontologies must establish semantic mappings among them; to ensure interoperability.Examples of such applications arise in myriad domains; including e-commerce; knowledgemanagement; e-learning; information extraction; bio-informatics; web services; and tourism(see Part D of this book on ontology applications). Despite its pervasiveness; today ontologymatching is still largely conducted by hand; in a labor-intensive and error-prone process.The manual matching has now become a key bottleneck in building large-scale informationmanagement systems. The advent of technologies such as the WWW; XML; and the …,*,2004,598
An adaptive query execution system for data integration,Zachary G Ives; Daniela Florescu; Marc Friedman; Alon Levy; Daniel S Weld,Abstract Query processing in data integration occurs over network-bound; autonomous datasources. This requires extensions to traditional optimization and execution techniques forthree reasons: there is an absence of quality statistics about the data; data transfer rates areunpredictable and bursty; and slow or unavailable data sources can often be replaced byoverlapping or mirrored sources. This paper presents the Tukwila data integration system;designed to support adaptivity at its core using a two-pronged approach. Interleavedplanning and execution with partial optimization allows Tukwila to quickly recover fromdecisions based on inaccurate estimates. During execution; Tukwila uses adaptive queryoperators such as the double pipelined hash join; which produces answers quickly; and thedynamic collector; which robustly and efficiently computes unions across overlapping …,ACM SIGMOD Record,1999,590
ULDBs: Databases with uncertainty and lineage,Omar Benjelloun; Anish Das Sarma; Alon Halevy; Jennifer Widom,Abstract This paper introduces ULDBs; an extension of relational databases with simple yetexpressive constructs for representing and manipulating both lineage and uncertainty.Uncertain data and data lineage are two important areas of data management that havebeen considered extensively in isolation; however many applications require the features intandem. Fundamentally; lineage enables simple and consistent representation of uncertaindata; it correlates uncertainty in query results with uncertainty in the input data; and queryprocessing with lineage and uncertainty together presents computational benefits overtreating them separately. We show that the ULDB representation is complete; and that itpermits straightforward implementation of many relational operations. We define two notionsof ULDB minimality--data-minimal and lineage-minimal--and study minimization of ULDB …,Proceedings of the 32nd international conference on Very large data bases,2006,583
Schema mediation in peer data management systems,Alon Y Halevy; Zachary G Ives; Dan Suciu; Igor Tatarinov,Intuitively; data management and data integration tools should be well-suited for exchanginginformation in a semantically meaningful way. Unfortunately; they suffer from two significantproblems: they typically require a comprehensive schema design before they can be used tostore or share information; and they are difficult to extend because schema evolution isheavyweight and may break backwards compatibility. As a result; many small-scale datasharing tasks are more easily facilitated by nondatabase-oriented tools that have littlesupport for semantics. The goal of the peer data management system (PDMS) is to addressthis need: we propose the use of a decentralized; easily extensible data managementarchitecture in which any user can contribute new data; schema information; or evenmappings between other peer's schemas. PDMSs represent a natural step beyond data …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,571
The information manifold,Thomas Kirk; Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,Abstract We describe the Information Manifold (IM); a system for browsing and querying ofmultiple networked information sources. As a first contribution; the system demonstrates theviability of knowledge representation technology for retrieval and organization of informationfrom disparate (structured and unstructured) information sources. Such an organizationallows the user to pose high-level queries that use data from multiple information sources.As a second contribution; we describe novel query processing algorithms used to combineinformation from multiple sources. In particular; our algorithms are guaranteed to find exactlythe set of information sources relevant to a query; and to completely exploit knowledgeabout local closed world information (Etzioni et al. 1994).,Proceedings of the AAAI 1995 Spring Symp. on Information Gathering from Heterogeneous; Distributed Enviroments,1995,554
iMAP: discovering complex semantic matches between database schemas,Robin Dhamankar; Yoonkyong Lee; AnHai Doan; Alon Halevy; Pedro Domingos,Abstract Creating semantic matches between disparate data sources is fundamental tonumerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. Todate; however; virtually all of these works deal only with one-to-one (1-1) matches; such asaddress= location. They do not consider the important class of more complex matches; suchas address= concat (city; state) and room-pric= room-rate*(1+ tax-rate). We describe theiMAP system which semi-automatically discovers both 1-1 and complex matches. iMAPreformulates schema matching as a search in an often very large or infinite match space. Tosearch effectively; it employs a set of searchers; each discovering specific types of complexmatches. To further improve matching accuracy; iMAP exploits a variety of domain …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,531
Webtables: exploring the power of tables on the web,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang,Abstract The World-Wide Web consists of a huge number of unstructured documents; but italso contains structured data in the form of HTML tables. We extracted 14.1 billion HTMLtables from Google's general-purpose web crawl; and used statistical classificationtechniques to find the estimated 154M that contain high-quality relational data. Becauseeach relational table has its own" schema" of labeled and typed columns; each such tablecan be considered a small structured database. The resulting corpus of databases is largerthan any other corpus we are aware of; by at least five orders of magnitude.,Proceedings of the VLDB Endowment,2008,522
Piazza: data management infrastructure for semantic web applications,Alon Y Halevy; Zachary G Ives; Peter Mork; Igor Tatarinov,Abstract The Semantic Web envisions a World Wide Web in which data is described withrich semantics and applications can pose complex queries. To this point; researchers havedefined new languages for specifying meanings for concepts and developed techniques forreasoning about them; using RDF as the data model. To flourish; the Semantic Web needsto be able to accommodate the huge amounts of existing data and the applicationsoperating on them. To achieve this; we are faced with two problems. First; most of the world'sdata is available not in RDF but in XML; XML and the applications consuming it rely not onlyon the domain structure of the data; but also on its document structure. Hence; to provideinteroperability between such sources; we must map between both their domain structuresand their document structures. Second; data management practitioners often prefer to …,Proceedings of the 12th international conference on World Wide Web,2003,471
Corpus-based schema matching,Jayant Madhavan; Philip A Bernstein; AnHai Doan; Alon Halevy,Schema matching is the problem of identifying corresponding elements in different schemas.Discovering these correspondences or matches is inherently difficult to automate. Pastsolutions have proposed a principled combination of multiple algorithms. However; thesesolutions sometimes perform rather poorly due to the lack of sufficient evidence in theschemas being matched. In this paper we show how a corpus of schemas and mappingscan be used to augment the evidence about the schemas being matched; so they can bematched better. Such a corpus typically contains multiple schemas that model similarconcepts and hence enables us to learn variations in the elements and their properties. Weexploit such a corpus in two ways. First; we increase the evidence about each element beingmatched by including evidence from similar elements in the corpus. Second; we learn …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,459
Combining Horn rules and description logics in CARIN,Alon Y Levy; Marie-Christine Rousset,Abstract We describe carin; a novel family of representation languages; that combine theexpressive power of Horn rules and of description logics. We address the issue of providingsound and complete inference procedures for such languages. We identify existentialentailment as a core problem in reasoning in carin; and describe an existential entailmentalgorithm for the ALCNR description logic. As a result; we obtain a sound and completealgorithm for reasoning in non-recursive carin ALCNR knowledge bases; and an algorithmfor rule subsumption over ALCNR. We show that in general; the reasoning problem forrecursive carin-ALCNR knowledge bases is undecidable; and identify the constructors ofALCNR causing the undecidability. We show two ways in which carin-ALCNR knowledgebases can be restricted while obtaining sound and complete reasoning.,Artificial Intelligence,1998,459
Google's deep web crawl,Jayant Madhavan; David Ko; Łucja Kot; Vignesh Ganapathy; Alex Rasmussen; Alon Halevy,Abstract The Deep Web; ie; content hidden behind HTML forms; has long beenacknowledged as a significant gap in search engine coverage. Since it represents a largeportion of the structured data on the Web; accessing Deep-Web content has been a long-standing challenge for the database community. This paper describes a system for surfacingDeep-Web content; ie; pre-computing submissions for each HTML form and adding theresulting HTML pages into a search engine index. The results of our surfacing have beenincorporated into the Google search engine and today drive more than a thousand queriesper second to Deep-Web content. Surfacing the Deep Web poses several challenges. First;our goal is to index the content behind many millions of HTML forms that span manylanguages and hundreds of domains. This necessitates an approach that is completely …,Proceedings of the VLDB Endowment,2008,458
User interface for information retrieval system,*,An improved information retrieval system user interface for retrieving information from aplurality of sources and for storing information source descriptions in a knowledge base. Theuser interface includes a hypertext browser and a knowledge base browser/editor. Thehypertext browser allows a user to browse an unstructured information space through theuse of interactive hypertext links. The knowledge base browser/editor displays a directedgraph representing a generalization taxonomy of the knowledge base; with the nodesrepresenting concepts and edges representing relationships between concepts. The systemallows users to store information source descriptions in the knowledge base via graphicalpointing means. By dragging an iconic representation of an information source from thehypertext browser to a node in the directed graph; the system will store an information …,*,1998,445
Catching the boat with Strudel: Experiences with a web-site management system,Mary Fernandez; Daniela Florescu; Jaewoo Kang; Alon Levy; Dan Suciu,Abstract The Strudel system applies concepts from database management systems to theprocess of building Web sites. Strudel's key idea is separating the management of the site'sdata; the creation and management of the site's structure; and the visual presentation of thesite's pages. First; the site builder creates a uniform model of all data available at the site.Second; the builder uses this model to declaratively define the Web site's structure byapplying a “site-definition query” to the underlying data. The result of evaluating this query isa “site graph”; which represents both the site's content and structure. Third; the builderspecifies the visual presentation of pages in Strudel's HTML-template language. The datamodel underlying Strudel is a semi-structured model of labeled directed graphs. Wedescribe Strudel's key characteristics; report on our experiences using Strudel; and …,ACM SIGMOD Record,1998,442
Updating xml,Igor Tatarinov; Zachary G Ives; Alon Y Halevy; Daniel S Weld,Abstract As XML has developed over the past few years; its role has expanded beyond itsoriginal domain as a semantics-preserving markup language for online documents; and it isnow also the de facto format for interchanging data between heterogeneous systems. Datasources expert XML “views” over their data; and other system can directly import or querythese views. As a result; there has been great interest in languages and systems forexpressing queries over XML data; whether the XML is stored in a repository or generatedas a view over some other data storage format. Clearly; in order to fully evolve XML into auniversal data representation and sharing format; we must allow users to specify updates toXML documents and must develop techniques to process them efficiently. Updatecapabilities are important not only for modifying XML documents; but also for propagating …,ACM SIGMOD Record,2001,441
Web-scale data integration: You can only afford to pay as you go,Jayant Madhavan; Shawn R Jeffery; Shirley Cohen; Xin Dong; David Ko; Cong Yu; Alon Halevy,ABSTRACT The World Wide Web is witnessing an increase in the amount of structuredcontent–vast heterogeneous collections of structured data are on the rise due to the DeepWeb; annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper; we highlight these challenges intwo scenarios–the Deep Web and Google Base. We contend that traditional data integrationtechniques are no longer valid in the face of such heterogeneity and scale. We propose anew data integration architecture; PAYGO; which is inspired by the concept of dataspacesand emphasizes pay-as-you-go data management as means for achieving web-scale dataintegration.,*,2007,429
A vision for management of complex models,Phillip A Bernstein; Alon Y Halevy; Rachel A Pottinger,Abstract Many problems encountered when building applications of database systemsinvolve the manipulation of models. By" model;" we mean a complex structure thatrepresents a design artifact; such as a relational schema; object-oriented interface; UMLmodel; XML DTD; web-site schema; semantic network; complex document; or softwareconfiguration. Many uses of models involve managing changes in models andtransformations of data from one model into another. These uses require an explicitrepresentation of" mappings" between models. We propose to make database systemseasier to use for these applications by making" model" and" model mapping" first-classobjects with special operations that simplify their use. We call this capability modelmanagement. In addition to making the case for model management; our main …,ACM Sigmod Record,2000,413
Navigational plans for data integration,Marc Friedman; Alon Y Levy; Todd D Millstein,*,AAAI/IAAI,1999,408
Working models for uncertain data,Anish Das Sarma; Omar Benjelloun; Alon Halevy; Jennifer Widom,This paper explores an inherent tension in modeling and querying uncertain data: simple;intuitive representations of uncertain data capture many application requirements; but theserepresentations are generally incomplete―standard operations over the data may result inunrepresentable types of uncertainty. Complete models are theoretically attractive; but theycan be nonintuitive and more complex than necessary for many applications. To addressthis tension; we propose a two-layer approach to managing uncertain data: an underlyinglogical model that is complete; and one or more working models that are easier tounderstand; visualize; and query; but may lose some information. We explore the space ofincomplete working models; place several of them in a strict hierarchy based on expressivepower; and study their closure properties. We describe how the two-layer approach is …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,406
Answering queries with aggregation using views,Divesh Srivastava; Shaul Dar; Hosagrahar V Jagadish; Alon Y Levy,Abstract We present novel algorithms for the problem of using materialized views to computeanswers to SQL queries with grouping and aggregation; in the presence of multiset tables. Inaddition to its obvious potential in query optimization; this problem is important in manyapplications; such as data warehousing; very large transaction recording systems; globalinformation systems and mobile computing; where access to local or cached materializedviews may be cheaper than access to the underlying database. Our contributions are thefollowing: First; we show that in the case where the query has grouping and aggregation butthe views do not; a view is usable in answering a query only if there is an isomorphismbetween the view and a portion of the query. Second; when the views also have groupingand aggregation we identify conditions under which the aggregation information present …,VLDB,1996,381
Principles of dataspace systems,Alon Halevy; Michael Franklin; David Maier,Abstract The most acute information management challenges today stem from organizationsrelying on a large number of diverse; interrelated data sources; but having no means ofmanaging them in a convenient; integrated; or principled fashion. These challenges arise inenterprise and government data management; digital libraries;" smart" homes and personalinformation management. We have proposed dataspaces as a data managementabstraction for these diverse applications and DataSpace Support Platforms (DSSPs) assystems that should be built to provide the required services over dataspaces. Unlike dataintegration systems; DSSPs do not require full semantic integration of the sources in order toprovide useful services. This paper lays out specific technical challenges to realizing DSSPsand ties them to existing work in our field. We focus on query answering in DSSPs; the …,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,356
What can database do for peer-to-peer?,Steven D Gribble; Alon Y Halevy; Zachary G Ives; Maya Rodrig; Dan Suciu,Abstract The Internet community has recently been focused on peer-to-peer systems likeNapster; Gnutella; and Freenet. The grand vision—a decentralized community of machinespooling their resources to benefit everyone—is compelling for many reasons: scalability;robustness; lack of need for administration; and even anonymity and resistance tocensorship. Existing peer-to-peer (P2P) systems have focused on specific applicationdomains (eg music files) or on providing filesystem-like capabilities; these systems ignorethe semantics of data. An important question for the database community is how datamanagement can be applied to P2P; and what we can learn from and contribute to the P2Parea. We address these questions; identify a number of potential research ideas in theoverlap between data management and P2P systems; present some preliminary …,WebDB,2001,355
Enterprise information integration: successes; challenges and controversies,Alon Y Halevy; Naveen Ashish; Dina Bitton; Michael Carey; Denise Draper; Jeff Pollock; Arnon Rosenthal; Vishal Sikka,Abstract The goal of EII systems is to provide uniform access to multiple data sources withouthaving to first load them into a data warehouse. Since the late 1990's; several EII productshave appeared in the marketplace and significant experience has been accumulated fromfielding such systems. This collection of articles; by individuals who were involved in thisindustry in various ways; describes some of these experiences and points to the challengesahead.,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,353
Principles of data integration,AnHai Doan; Alon Halevy; Zachary Ives,Principles of Data Integration is the first comprehensive textbook of data integration;covering theoretical principles and implementation issues as well as current challengesraised by the semantic web and cloud computing. The book offers a range of dataintegration solutions enabling you to focus on what is most relevant to the problem at hand.Readers will also learn how to build their own algorithms and implement their own dataintegration application. Written by three of the most respected experts in the field; this bookprovides an extensive introduction to the theory and concepts underlying today's dataintegration techniques; with detailed; instruction for their application using concreteexamples throughout to explain the concepts. This text is an ideal resource for databasepractitioners in industry; including data warehouse engineers; database system …,*,2012,343
Data model and query evaluation in global information systems,Alon Y Levy; Divesh Srivastava; Thomas Kirk,Abstract Global information systems involve a large number of information sourcesdistributed over computer networks. The variety of information sources and disparity ofinterfaces makes the task of easily locating and efficiently accessing information over thenetwork very cumbersome. We describe an architecture for global information systems thatis especially tailored to address the challenges raised in such an environment; anddistinguish our architecture from architectures of multidatabase and distributed databasesystems. Our architecture is based on presenting a conceptually unified view of theinformation space to a user; specifying rich descriptions of the contents of the informationsources; and using these descriptions for optimizing queries posed in the unified view. Thecontributions of this paper include:(1) we identify aspects of site descriptions that are …,Journal of Intelligent Information Systems,1995,341
Logic-based techniques in data integration,Alon Y Levy,Abstract The data integration problem is to provide uniform access to multipleheterogeneous information sources available online (eg; databases on the WWW). Thisproblem has recently received considerable attention from researchers in the fields ofArtificial Intelligence and Database Systems. The data integration problem is complicated bythe facts that (1) sources contain closely related and overlapping data;(2) data is stored inmultiple data models and schemas; and (3) data sources have differing query processingcapabilities. A key element in a data integration system is the language used to describe thecontents and capabilities of the data sources. While such a language needs to be asexpressive as possible; it should also enable to efficiently address the main inferenceproblem that arises in this context: to translate a user query that is formulated over a …,*,2000,335
A query language and processor for a web-site management system,Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,We have designed a system; called Strudel (to be demonstrated at SIGMOD-97 FFK+97]);which applies familiar concepts from database management systems; to the process ofbuilding web sites. The main motivation for developing Strudel is the observation that withcurrent technology; creating and managing large sites is tedious; because a site designermust simultaneously perform (at least) three tasks:(1) choosing what information will beavailable at the site;(2) organizing that information in individual pages or in graphs of linkedpages; and (3) specifying the visual presentation of pages in HTML. Furthermore; sincethere is no separation between the physical organization of the information underlying aweb site and the logical view we have on it; changing or restructuring a site are unwieldytasks. In Strudel; the web site manager can separate the logical view of information …,Proc. of the Workshop on Semi-structured Data; Tucson; Arizona,1997,330
The piazza peer data management system,Alon Y Halevy; Zachary G Ives; Jayant Madhavan; Peter Mork; Dan Suciu; Igor Tatarinov,Intuitively; data management and data integration tools are well-suited for exchanginginformation in a semantically meaningful way. Unfortunately; they suffer from two significantproblems: They typically require a comprehensive schema design before they can be usedto store or share information and they are difficult to extend because schema evolution isheavyweight and may break backward compatibility. As a result; many small-scale datasharing tasks are more easily facilitated by nondatabase-oriented tools that have littlesupport for semantics. The goal of the peer data management system (PDMS) is to addressthis need: We propose the use of a decentralized; easily extensible data managementarchitecture in which any user can contribute new data; schema information; or evenmappings between other peers' schemes. PDMSs represent a natural step beyond data …,IEEE Transactions on Knowledge and Data Engineering,2004,322
P-CLASSIC: A tractable probablistic description logic,Daphne Koller; Alon Levy; Avi Pfeffer,Abstract Knowledge representation languages invariably reflect a trade-off betweenexpressivity and tractability. Evidence suggests that the compromise chosen by descriptionlogics is a particularly successful one. However; description logic (as for all variants of first-order logic) is severely limited in its ability to express uncertainty. In this paper; we presentPClassic; a probabilistic version of the description logic CLASSIC. In addition toterminological knowledge; the language utilizes Bayesian networks to express uncertaintyabout the basic properties of an individual; the number of fillers for its roles; and theproperties of these fillers. We provide a semantics for PClassic and an effective inferenceprocedure for probabilistic subsumption: computing the probability that a random individualin class C is also in class D. The effectiveness of the algorithm relies on independence …,AAAI/IAAI,1997,321
Representing and reasoning about mappings between domain models,Jayant Madhavan; Philip A Bernstein; Pedro Domingos; Alon Y Halevy,Abstract Mappings between disparate models are fundamental to any application thatrequires interoperability between heterogeneous data and applications. Generatingmappings is a laborintensive and error prone task. To build a system that helps usersgenerate mappings; we need an explicit representation of mappings. This representationneeds to have well-defined semantics to enable reasoning and comparison betweenmappings. This paper first presents a powerful framework for defining languages forspecifying mappings and their associated semantics. We examine the use of mappings andidentify the key inference problems associated with mappings. These properties can beused to determine whether a mapping is adequate in a particular context. Finally; weconsider an instance of our framework for a language representing mappings between …,AAAI/IAAI,2002,314
Learning to match the schemas of data sources: A multistrategy approach,Anhai Doan; Pedro Domingos; Alon Halevy,Abstract The problem of integrating data from multiple data sources—either on the Internetor within enterprises—has received much attention in the database and AI communities. Thefocus has been on building data integration systems that provide a uniform query interface tothe sources. A key bottleneck in building such systems has been the laborious manualconstruction of semantic mappings between the query interface and the source schemas.Examples of mappings are “element location maps to address” and “price maps to listed-price”. We propose a multistrategy learning approach to automatically find such mappings.The approach applies multiple learner modules; where each module exploits a different typeof information either in the schemas of the sources or in their data; then combines thepredictions of the modules using a meta-learner. Learner modules employ a variety of …,Machine Learning,2003,308
Query-answering algorithms for information agents,Alon Y Levy; Anand Rajaraman; Joann J Ordille,Abstract We describe the architecture and query-answering algorithms used in theInformation Manifold; an implemented information gathering system that provides uniformaccess to structured information sources on the World-Wide Web. Our architecture providesan expressive language for describing information sources; which makes it easy to add newsources and to model the fine-grained distinctions between their contents. The query-answering algorithm guarantees that the descriptions of the sources are exploited to accessonly sources that are relevant to a given query. Accessing only relevant sources is crucial toscale up such a system to large numbers of sources. In addition; our algorithm can exploitrun-time information to further prune information sources and to reduce the cost of queryplanning.,*,1996,308
-Composing Mappings Among Data Sources,Jayant Madhavan; Alon Y Halevy,Semantic mappings between data sources play a key role in several data sharingarchitectures. Mappings provide the relationships between data stored in different sources;and therefore enable answering queries that require data from other nodes in a data-sharingnetwork. This chapter investigates the theoretical underpinnings of mapping composition.The problem of sharing data from multiple sources within or between enterprises hasrecently received significant attention in research and in the commercial world. This chapterstudies the problem for a rich mapping language; GLAV that combines the advantages of theknown mapping formalisms global-as-view and local-as-view. The chapter explores thateven when composing two simple GLAV mappings; the full composition may be an infiniteset of GLAV formulas. It also explores that if one restricts the set of queries to be in CQ k …,*,2003,304
A scalable algorithm for answering queries using views,Rachel Pottinger; Alon Y Levy,Abstract The problem of answering queries using views is to find efficient methods ofanswering a query using a set of previously materialized views over the database; ratherthan accessing the database relations. The problem has received significant attentionbecause of its relevance to a wide variety of data management problems; such as dataintegration; query optimization; and the maintenance of physical data independence. Todate; the performance of proposed algorithms has received very little attention; and inparticular; their scale up in the presence of a large number of views is unknown. We firstanalyze two previous algorithms; the bucket algorithm and the inverse-rules algorithm; andshow their deficiencies. We then describe the MiniCon algorithm; a novel algorithm forfinding the maximally-contained rewriting of a conjunctive query using a set of conjunctive …,VLDB,2000,290
Efficient query reformulation in peer data management systems,Igor Tatarinov; Alon Halevy,Abstract Peer data management systems (PDMS) offer a flexible architecture fordecentralized data sharing. In a PDMS; every peer is associated with a schema thatrepresents the peer's domain of interest; and semantic relationships between peers areprovided locally between pairs (or small sets) of peers. By traversing semantic paths ofmappings; a query over one peer can obtain relevant data from any reachable peer in thenetwork. Semantic paths are traversed by reformulating queries at a peer into queries on itsneighbors. Naively following semantic paths is highly inefficient in practice. We describeseveral techniques for optimizing the reformulation process in a PDMS and validate theireffectiveness using real-life data sets. In particular; we develop techniques for pruning pathsin the reformulation process and for minimizing the reformulated queries as they are …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,287
Bootstrapping pay-as-you-go data integration systems,Anish Das Sarma; Xin Dong; Alon Halevy,Abstract Data integration systems offer a uniform interface to a set of data sources. Despiterecent progress; setting up and maintaining a data integration application still requiressignificant upfront effort of creating a mediated schema and semantic mappings from thedata sources to the mediated schema. Many application contexts involving multiple datasources (eg; the web; personal information management; enterprise intranets) do not requirefull integration in order to provide useful services; motivating a pay-as-you-go approach tointegration. With that approach; a system starts with very few (or inaccurate) semanticmappings and these mappings are improved over time as deemed necessary. This paperdescribes the first completely self-configuring data integration system. The goal of our workis to investigate how advanced of a starting point we can provide a pay-as-you-go system …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,280
Pay-as-you-go user feedback for dataspace systems,Shawn R Jeffery; Michael J Franklin; Alon Y Halevy,Abstract A primary challenge to large-scale data integration is creating semanticequivalences between elements from different data sources that correspond to the samereal-world entity or concept. Dataspaces propose a pay-as-you-go approach: automatedmechanisms such as schema matching and reference reconciliation provide initialcorrespondences; termed candidate matches; and then user feedback is used toincrementally confirm these matches. The key to this approach is to determine in what orderto solicit user feedback for confirming candidate matches. In this paper; we develop adecision-theoretic framework for ordering candidate matches for user confirmation using theconcept of the value of perfect information (VPI). At the core of this concept is a utility functionthat quantifies the desirability of a given state; thus; we devise a utility function for …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,267
The Piazza peer data management project,Igor Tatarinov; Zachary Ives; Jayant Madhavan; Alon Halevy; Dan Suciu; Nilesh Dalvi; Xin Luna Dong; Yana Kadiyska; Gerome Miklau; Peter Mork,Abstract A major problem in today's information-driven world is that sharing heterogeneous;semantically rich data is incredibly difficult. Piazza is a peer data management system thatenables sharing heterogeneous data in a distributed and scalable way. Piazza assumes theparticipants to be interested in sharing data; and willing to define pairwise mappingsbetween their schemas. Then; users formulate queries over their preferred schema; and aquery answering system expands recursively any mappings relevant to the query; retrievingdata from other peers. In this paper; we provide a brief overview of the Piazza projectincluding our work on developing mapping languages and query reformulation algorithms;assisting the users in defining mappings; indexing; and enforcing access control overshared data.,ACM Sigmod Record,2003,263
Queries independent of updates,Alon Y Levy; Yehoshua Sagiv,Abstract This paper considers the problem of detecting independence of a queriesexpressed by datalog programs from updates. We provide new insight into theindependence problem by reducing it to the equivalence problem for datalog programs(both for the case of insertion and deletion updates). Equivalence; as well as independence;is undecidable in general. However; algorithms for detecting subclasses of equivalenceprovide sufficient (and sometimes also necessary) conditions for independence. Weconsider two such subclasses. The first; query-reachability; general-izes previous work onindependence [BCL89; E190]; which dealt with nonrecursive programs with a singleoccurrence of the updated predicate. Using recent results on queryreachability [LS92;LMSS93]; we generalize these earlier independence tests to arbitrary recursive datalog …,VLDB,1993,253
Data integration with uncertainty,Xin Dong; Alon Y Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levels;and do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; queries to the system may be posed with keywordsrather than in a structured form. Third; the data from the sources may be extracted usinginformation extraction techniques and so may yield imprecise data. As a first step to buildingsuch a system; we introduce the concept of probabilistic schema mappings and analyzetheir formal foundations. We show that there are two possible semantics for such …,Proceedings of the 33rd international conference on Very large data bases,2007,251
Learning Source Description for Data Integration.,AnHai Doan; Pedro M Domingos; Alon Y Levy,ABSTRACT To build a data-integration system; the application designer must specify amediated schema and supply the descriptions of data sources. A source descriptioncontains a source schema that describes the content of the source; and a mapping betweenthe corresponding elements of the source schema and the mediated schema. Manuallyconstructing these mappings is both labor-intensive and error-prone; and has proven to be amajor bottleneck in deploying large-scale data integration systems in practice. In this paperwe report on our initial work toward automatically learning mappings between sourceschemas and the mediated schema. Specifically; we investigate finding one-to-onemappings for the leaf elements of source schemas. We describe LSD; a system thatautomatically finds such mappings. LSD consults a set of learner modules–where each …,WebDB (Informal Proceedings),2000,237
Query optimization in the presence of limited access patterns,Daniela Florescu; Alon Levy; Ioana Manolescu; Dan Suciu,Abstract We consider the problem of query optimization in the presence of limitations onaccess patterns to the data (ie; when one must provide values for one of the attributes of arelation in order to obtain tuples). We show that in the presence of limited access patternswe must search a space of annotated query plans; where the annotations describe theinputs that must be given to the plan. We describe a theoretical and experimental analysis ofthe resulting search space and a novel query optimization algorithm that is designed toperform well under the different conditions that may arise. The algorithm searches the set ofannotated query plans; pruning invalid and non-viable plans as early as possible in thesearch space; and it also uses a best-first search strategy in order to produce a first completeplan early in the search. We describe experiments to illustrate the performance of our …,ACM SIGMOD Record,1999,235
Recovering semantics of tables on the web,Petros Venetis; Alon Halevy; Jayant Madhavan; Marius Paşca; Warren Shen; Fei Wu; Gengxin Miao; Chung Wu,Abstract The Web offers a corpus of over 100 million tables [6]; but the meaning of eachtable is rarely explicit from the table itself. Header rows exist in few cases and even whenthey do; the attribute names are typically useless. We describe a system that attempts torecover the semantics of tables by enriching the table with additional annotations. Ourannotations facilitate operations such as searching for tables and finding related tables. Torecover semantics of tables; we leverage a database of class labels and relationshipsautomatically extracted from the Web. The database of classes and relationships has verywide coverage; but is also noisy. We attach a class label to a column if a sufficient number ofthe values in the column are identified with that label in the database of class labels; andanalogously for binary relationships. We describe a formal model for reasoning about …,Proceedings of the VLDB Endowment,2011,228
A platform for personal information management and integration,Xin Luna Dong; Alon Halevy,Abstract The explosion of the amount of information available in digital form has madesearch a hot research topic for the Information Management Community. While most of theresearch on search is focused on the WWW; individual computer users have developed theirown vast collections of data on their desktops; and these collections are in critical need forgood search tools. We study the Personal Information Management (PIM) problem from thedata management point of view. We argue that the key for building a successful PIM systemis to provide a logical view of one's personal information; consisting of semanticallymeaningful objects and associations. The thesis of this research is to build a prototype of aPIM system based upon this logical view; and demonstrate how we can leverage such aview to address the many PIM challenges.,Proceedings of VLDB 2005 PhD Workshop,2005,226
STRUDEL: a Web site management system,Mary Fernandez; Daniela Florescu; Jaewoo Kang; Alon Levy; Dan Suciu,The growth of the World-Wide Web has created a new kind of data management problem: buildingand main- taining Web sites. Building a Web site involves several tasks; such as choosing whatinformation will be avail- able at the site; organizing that information in individ- ual pages or ingraphs of linked pages; and specifying the visual presentation of pages in HTML. Creating andmanaging large sites is tedious; because a user often must perform these tasks simultaneouslywhen creat- ing HTML pages. The task of building and manag- ing Web sites presents a uniqueopportunity for apply- ing concepts from database management systems; such as the separationbetween the logical view of informa- tion and its storage and maintenance and the ability to restructureinformation via queries. Furthermore; re- cent research results on information integration[U1197] … Web site by creating different views of the underlying data. Building …,ACM SIGMOD Record,1997,224
Recursive query plans for data integration,Oliver M Duschka; Michael R Genesereth; Alon Y Levy,Abstract Generating query-answering plans for data integration systems requires to translatea user query; formulated in terms of a mediated schema; to a query that uses relations thatare actually stored in data sources. Previous solutions to the translation problem producedsets of conjunctive plans; and were therefore limited in their ability to handle recursivequeries and to exploit data sources with binding-pattern limitations and functionaldependencies that are known to hold in the mediated schema. As a result; these plans wereincomplete wrt sources encountered in practice (ie; produced only a subset of the possibleanswers). We describe the novel class of recursive query answering plans; which enablesus to settle three open problems. First; we describe an algorithm for finding a query plan thatproduces the maximal set of answers from the sources for arbitrary recursive queries …,The Journal of Logic Programming,2000,215
Databases with uncertainty and lineage,Omar Benjelloun; Anish Das Sarma; Alon Halevy; Martin Theobald; Jennifer Widom,Abstract This paper introduces uldb s; an extension of relational databases with simple yetexpressive constructs for representing and manipulating both lineage and uncertainty.Uncertain data and data lineage are two important areas of data management that havebeen considered extensively in isolation; however many applications require the features intandem. Fundamentally; lineage enables simple and consistent representation of uncertaindata; it correlates uncertainty in query results with uncertainty in the input data; and queryprocessing with lineage and uncertainty together presents computational benefits overtreating them separately. We show that the uldb representation is complete; and that itpermits straightforward implementation of many relational operations. We define two notionsof uldb minimality--data-minimal and lineage-minimal--and study minimization of uldb …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,204
The Lowell database research self-assessment,Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk; David Maier; Jeff Naughton; Hans Schek; Timos Sellis; Avi Silberschatz; Mike Stonebraker; Rick Snodgrass; Jeff Ullman; Gerhard Weikum; Jennifer Widom; Stan Zdonik,A group of senior database researchers gathers every few years to assess the state of databaseresearch and to point out problem areas that deserve additional focus. This article summarizesthe discussion and conclusions of the sixth such meeting in Lowell; MA; in May 2003. It followsa number of earlier reports with similar goals; including [1; 2; 5-7] … Continuing thistradition; 25 senior database researchers representing a broad cross section of the field in termsof research interests; affiliations; and geography gathered in Lowell for two days of intensivediscussion on where the database field is and where it should be going … Several importantobservations came out of this meeting. Information management continues to be a critical componentof most complex software systems. We recommend that database researchers increase theirfocus on the integration of text; data; code; and streams; fusion of information from …,Communications of the ACM,2005,203
CARIN: A representation language combining horn rules and description logics',Alon Y Levy; Marie-Christine Rousset,Abstract We describe carin; a novel family of representation languages; that combine theexpressive power of Horn rules and of description logics. We address the issue of providingsound and complete inference procedures for such languages. We identify existentialentailment as a core problem in reasoning in carin; and describe an existential entailmentalgorithm for the ALCN R description logic. As a result; we obtain a sound and completealgorithm for reasoning in non recursive carin-ALCN R knowledge bases; and an algorithmfor rule subsumption over ALCN R. We show that in general; the reasoning problem forrecursive carin-ALCN R knowledge bases is undecidable; and identify the constructors ofALCN R causing the undecidability. We show two ways in which carin-ALCN R knowledgebases can be restricted while obtaining sound and complete reasoning.,ECAI,1996,200
Rewriting queries using views in description logics,Catriel Beeri; Alon Y Levy; Marie-Christine Rousset,Abstract The problem of rewriting queries using views is to iind a query expression that usesonly a set of views V and is equivalent to (or maximally contained in) a given query Q.Rewriting queries using views is important for query optimization and for applications suchas information integration and data warehousing. Description logics are a family of logicsthat were developed for modeling complex hierarchical structures; and can also be viewedas a query language with an interesting tradeoff between complexity and expressive power.We consider the problem of rewriting queries using views expressed in description logicsand conjunctive queries over description logics. We show that if the view definitions do notcontain existential variables; then it is always possible to find a rewriting that is a union ofconjunctive queries; and furthermore; this rewriting produces the maximal set of answers …,Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1997,198
Theory of answering queries using views,Alon Y Halevy,Abstract The problem of answering queries using views is to find efficient methods ofanswering a query using a set of previously materialized views over the database; ratherthan accessing the database relations. The problem has recently received significantattention because of its relevance to a wide variety of data management problems; such asquery optimization; the maintenance of physical data independence; data integration anddata warehousing. This article surveys the theoretical issues concerning the problem ofanswering queries using views.,ACM SIGMOD Record,2000,192
Query containment for conjunctive queries with regular expressions,Daniela Florescu; Alon Levy; Dan Suciu,Abstract The management of semistructured data has recently rccoivcd significant attentionbecause of the need of several applications to model and query large volumes of irregulardata. This paper considers the problem of query containment for a query language oversemistructured data; STRUQLO; that contains the essential feature common to all suchlanguages; namely the ability to specify regular path expressions over the data. We showhcrc that containment of STRUQLO queries is decidable. First; we give a semantic criterionfor STRUQLO query containment: WC show that it suffices to check containment on onlyfinitely many canonical databases. Second; we give a syntactic criteria for querycontainment; based on a notion of query mappings; which extends containment mappingsfor conjunctive queries. Third; wc consider a certain fragment of STRUQLO; obtained by …,Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1998,187
Declarative specification of Web sites with S,Mary Fernández; Daniela Florescu; Alon Levy; Dan Suciu,Abstract S is a system for implementing data-intensive Web sites; which typically integrateinformation from multiple data sources and have complex structure. S's key idea isseparating the management of a Web site's data; the specification of its content andstructure; and the visual representation of its pages. S provides a declarative querylanguage for specifying a site's content and structure; and a simple template language forspecifying a site's HTML representation. This paper contains a comprehensive description ofthe S system and details the benefits of declarative site specification. We describe ourexperiences using S in a production application and describe three different; butcomplementary; systems that extend and improve upon S's original ideas.,The VLDB Journal—The International Journal on Very Large Data Bases,2000,181
Indexing dataspaces,Xin Dong; Alon Halevy,Abstract Dataspaces are collections of heterogeneous and partially unstructured data.Unlike data-integration systems that also offer uniform access to heterogeneous datasources; dataspaces do not assume that all the semantic relationships between sources areknown and specified. Much of the user interaction with dataspaces involves exploring thedata; and users do not have a single schema to which they can pose queries. Consequently;it is important that queries are allowed to specify varying degrees of structure; spanningkeyword queries to more structure-aware queries. This paper considers indexing support forqueries that combine keywords and structure. We describe several extensions to invertedlists to capture structure when it is present. In particular; our extensions incorporate attributelabels; relationships between data items; hierarchies of schema elements; and synonyms …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,178
Data integration and genomic medicine,Brenton Louie; Peter Mork; Fernando Martin-Sanchez; Alon Halevy; Peter Tarczy-Hornoch,Abstract Genomic medicine aims to revolutionize health care by applying our growingunderstanding of the molecular basis of disease. Research in this arena is data intensive;which means data sets are large and highly heterogeneous. To create knowledge from data;researchers must integrate these large and diverse data sets. This presents dauntinginformatic challenges such as representation of data that is suitable for computationalinference (knowledge representation); and linking heterogeneous data sets (dataintegration). Fortunately; many of these challenges can be classified as data integrationproblems; and technologies exist in the area of data integration that may be applied to thesechallenges. In this paper; we discuss the opportunities of genomic medicine as well asidentify the informatics challenges in this domain. We also review concepts and …,*,2007,176
Google fusion tables: web-centered data management and collaboration,Hector Gonzalez; Alon Y Halevy; Christian S Jensen; Anno Langen; Jayant Madhavan; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,Abstract It has long been observed that database management systems focus on traditionalbusiness applications; and that few people use a database management system outsidetheir workplace. Many have wondered what it will take to enable the use of datamanagement technology by a broader class of users and for a much wider range ofapplications. Google Fusion Tables represents an initial answer to the question of how datamanagement functionality that focused on enabling new users and applications would lookin today's computing environment. This paper characterizes such users and applicationsand highlights the resulting principles; such as seamless Web integration; emphasis onease of use; and incentives for data sharing; that underlie the design of Fusion Tables. Wedescribe key novel features; such as the support for data acquisition; collaboration …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,175
Obtaining complete answers from incomplete databases,Alon Y Levy,Abstract We consider the problem of answering queries from databases that may beincomplete. A database is incomplete if some tuples may be missing from some relations;and only a part of each relation is known to be complete. This problem arises in severalcontexts. For example; systems that provide access to multiple heterogeneous informationsources often encounter incomplete sources. The question we address is to determinewhether the answer to a speci c given query is complete even when the database isincomplete. We present a novel sound and complete algorithm for the answer-completenessproblem by relating it to the problem of independence of queries from updates. We alsoshow an important case of the independence problem (and therefore of the answer-completeness problem) that can be decided in polynomial time; whereas the best known …,VLDB,1996,172
Clustering query refinements by user intent,Eldar Sadikov; Jayant Madhavan; Lu Wang; Alon Halevy,Abstract We address the problem of clustering the refinements of a user search query. Theclusters computed by our proposed algorithm can be used to improve the selection andplacement of the query suggestions proposed by a search engine; and can also serve tosummarize the different aspects of information relevant to the original user query. Ouralgorithm clusters refinements based on their likely underlying user intents by combiningdocument click and session co-occurrence information. At its core; our algorithm operates byperforming multiple random walks on a Markov graph that approximates user searchbehavior. A user study performed on top search engine queries shows that our clusters arerated better than corresponding clusters computed using approaches that use onlydocument click or only sessions co-occurrence information.,Proceedings of the 19th international conference on World wide web,2010,166
Apparatus and methods for retrieving information by modifying query plan based on description of information sources,*,Techniques for optimizing queries in a system in which executing the query requiresretrieval of information from a number of different data bases which are accessible via anetwork. In the techniques; a query results in a query plan which includes subplans forquerying the data bases which contain the required information. When a subplan isexecuted in one of the data bases; the data base returns not only the information whichresults from the execution of the subplan; but also source and constraint information aboutthe data in the data base. The source and constraint information is then used to optimize thequery plan by pruning redundant subplans. An embodiment is disclosed in which queriesare made to a domain model implemented using a knowledge base system. The domainmodel includes a world view of the data; a set of descriptions of the data bases; and a set …,*,1997,164
Why your data won't mix,Alon Halevy,Abstract When independent parties develop database schemas for the same domain; theywill almost always be quite different from each other. These differences are referred to assemantic heterogeneity; which also appears in the presence of multiple XML documents;Web services; and ontologies—or more broadly; whenever there is more than one way tostructure a body of data. The presence of semi-structured data exacerbates semanticheterogeneity; because semi-structured schemas are much more flexible to start with. Formultiple data systems to cooperate with each other; they must understand each other'sschemas. Without such understanding; the multitude of data sources amounts to a digitalversion of the Tower of Babel.,Queue,2005,163
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; AnHai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Abstract In late May; 2008; a group of database researchers; architects; users and punditsmet at the Claremont Resort in Berkeley; California to discuss the state of the research fieldand its impacts on practice. This was the seventh meeting of this sort in twenty years; andwas distinguished by a broad consensus that we are at a turning point in the history of thefield; due both to an explosion of data and usage scenarios; and to major shifts in computinghardware and platforms. Given these forces; we are at a time of opportunity for researchimpact; with an unusually large potential for influential results across computing; thesciences and society. This report details that discussion; and highlights the group'sconsensus view of new focus areas; including new database engine architectures;declarative programming languages; the interplay of structured and unstructured data …,ACM Sigmod Record,2008,160
An XML query engine for network-bound data,Zachary G Ives; Alon Y Halevy; Daniel S Weld,Abstract XML has become the lingua franca for data exchange and integration acrossadministrative and enterprise boundaries. Nearly all data providers are adding XML importor export capabilities; and standard XML Schemas and DTDs are being promoted for alltypes of data sharing. The ubiquity of XML has removed one of the major obstacles tointegrating data from widely disparate sources-namely; the heterogeneity of data formats.However; general-purpose integration of data across the wide are a also requires a queryprocessor that can query data sources on demand; receive streamed XML data from them;and combine and restructure the data into new XML output-while providing goodperformance for both batch-oriented and ad hoc; interactive queries. This is the goal of theTukwila data integration system; the first system that focuses on network-bound; dynamic …,The VLDB Journal—The International Journal on Very Large Data Bases,2002,158
Using probabilistic information in data integration,Daniela Florescu; Daphne Koller; Alon Levy; Avi Pfeffer,Abstract The goal of a mediator system is to provide users a uniform interface to themultitude of information sources. To translate user queries; given in a mediated schema; toqueries on the data sources; mediators rely on explicit mappings between the contents ofthe data sources and the meanings of the relations in the mediated schema. Thus far;contents of data sources were described qualitatively. In this paper we describe the use ofquantitative information in the form of probabilistic knowledge in mediator systems. Weconsider several kinds of probabilistic information: information about overlap betweencollections in the mediated schema; coverage of the information sources; and degrees ofoverlap between information sources. We address the problem of ordering accesses tomultiple information sources; in order to maximize the likelihood of obtaining answers as …,VLDB,1997,153
Recursive plans for information gathering,Oliver M Duschka; Alon Y Levy,Abstract Generating query-answering plans for information gathering agents requires totranslate a user query; formulated in terms of a set of virtual relations; to a query that usesrelations that are actually stored in information sources. Previous solutions to the translationproblem produced sets of conjunctive plans; and were therefore limited in their ability tohandle information sources with binding-pattern limitations; and to exploit functionaldependencies in the domain model. As a result; these plans were incomplete wrt sourcesencountered in practice ie; produced only a subset of the possible answers. We describe thenovel class of recursive information gathering plans; which enables us to settle two openproblems. First; we describe an algorithm for finding a query plan that produces the maximalset of answers from the sources in the presence of functional dependencies. Second; we …,IJCAI (1),1997,147
Data integration for the relational web,Michael J Cafarella; Alon Halevy; Nodira Khoussainova,Abstract The Web contains a vast amount of structured information such as HTML tables;HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However; integrating data from this relational webraises several challenges that are not addressed by current data integration systems ormash-up tools. First; the structured data is usually not published cleanly and must beextracted (say; from an HTML list) before it can be used. Second; due to the vastness of thecorpus; a user can never know all of the potentially-relevant databases ahead of time (muchless write a wrapper or mapping for each one); the source databases must be discoveredduring the integration process. Third; some of the important information regarding the data isonly present in its enclosing web page and needs to be extracted appropriately. This …,Proceedings of the VLDB Endowment,2009,146
Data integration with uncertainty,Xin Luna Dong; Alon Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levelsand do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; the data from the sources may be extracted usinginformation extraction techniques and so may yield erroneous data. Third; queries to thesystem may be posed with keywords rather than in a structured form. As a first step tobuilding such a system; we introduce the concept of probabilistic schema mappings andanalyze their formal foundations. We show that there are two possible semantics for such …,The VLDB Journal,2009,140
Query optimization by predicate move-around,Alon Y Levy; Inderpal Singh Mumick; Yehoshua Sagiv,Abstract A new type of optimization; called predicate move-around; ia introduced. It is shownhow this optimization 'considerably improvea the efficiency of evaluating SQL queries thathave query graphs with a large number of query blocks (which ie a typical situation whenqueries are defined in terms of multiple views and subqueries). Predicate move-aroundworks by moving predicates across query blocks (in the query graph) that cannot be mergedinto one block. Predicate move-around is a generalization of and has many advantages overthe traditional predicate pushdotin. One key advantage arises from the fact that predicatemove-around precedes pushdown by pulling predicates up the query graph. As a result;predicates that appear in the query in one part of the graph can be moved around the graphand applied alao in other parts of graph. Moreover; predicate movearound optimization …,VLDB,1994,139
Answering queries using limited external query processors,Alon Y Levy; Anand Rajaraman; Jeffrey D Ullman,Abstract When answering queries using external information sources; the contents of thequeries can be described by views. To answer a query; we must rewrite it using the set ofviews presented by the sources. When the external information sources also have the abilityto answer some (perhaps limited) sets of queries that require performing operations on theirdata; the set of views presented by the source may be infinite (albeit encoded in some finitefashion). Previous work on answering queries using views has only considered the casewhere the set of views is finite. In order to exploit the ability of information sources to answermore complex queries; we consider the problem of answering conjunctive queries usinginfinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views canbe partitioned into a finite number of equivalence classes; such that picking one view from …,Journal of Computer and System Sciences,1999,138
Uncovering the Relational Web.,Michael J Cafarella; Alon Y Halevy; Yang Zhang; Daisy Zhe Wang; Eugene Wu,ABSTRACT The World-Wide Web consists of a huge number of unstructured hypertextdocuments; but it also contains structured data in the form of HTML tables. Many of thesetables contain both relational-style data and a small “schema” of labeled and typed columns;making each such table a small structured database. The WebTables project is an effort toextract and make use of the huge number of these structured tables on the Web. A cleancollection of relational-style tables could be useful for improving web search; schemadesign; and many other applications. This paper describes the first stage of the WebTablesproject. First; we give an in-depth study of the Web's HTML table corpus. For example; weextracted 14.1 billion HTML tables from a several-billion-page portion of Google'sgeneralpurpose web crawl; and estimate that 154 million of these tables contain high …,WebDB,2008,133
Adapting to source properties in processing data integration queries,Zachary G Ives; Alon Y Halevy; Daniel S Weld,Abstract An effective query optimizer finds a query plan that exploits the characteristics of thesource data. In data integration; little is known in advance about sources' properties; whichnecessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays andadjusting for mis-estimated cardinality or selectivity values. In this paper; we present ageneralized architecture for adaptive query processing and introduce a new technique;called adaptive data partitioning (ADP); which is based on the idea of dividing the sourcedata into regions; each executed by different; complementary plans. We show how thismodel can be applied in novel ways to not only correct for underestimated selectivity andcardinality values; but also to discover and exploit order in the source data; and to detect …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,131
The information manifold approach to data integration,Alon Levy,A data integration system provides a uniform interface to a multitude of data sources.Consider; for example; an data integration system providing information about movies fromdata sources on the World-Wide Web (WWW). There are numerous sources on the WWWconcerning movies; such as the Internet Movie Database (providing comprehensive listingsof movies; their casts; directors; genres; etc.); MovieLink (providing playing times of moviesin US cities); and several sites providing textual reviews for selected movies. Suppose wewant to nd which movies directed by Woody Allen are playing tonight in Seattle; and theirrespective reviews. None of these data sources in isolation can answer this query. However;by combining data from multiple sources; we can answer queries like this one; and evenmore complex ones. To answer our query; we would rst query the Internet Movie …,IEEE Intelligent Systems,1998,131
Harvesting relational tables from lists on the web,Hazem Elmeleegy; Jayant Madhavan; Alon Halevy,Abstract A large number of web pages contain data structured in the form of" lists". Manysuch lists can be further split into multi-column tables; which can then be used in moresemantically meaningful tasks. However; harvesting relational tables from such lists can bea challenging task. The lists are manually generated and hence need not have well definedtemplates--they have inconsistent delimiters (if any) and often have missing information. Wepropose a novel technique for extracting tables from lists. The technique is domain-independent and operates in a fully unsupervised manner. We first use multiple sources ofinformation to split individual lines into multiple fields; and then compare the splits acrossmultiple lines to identify and fix incorrect splits and bad alignments. In particular; we exploit acorpus of HTML tables; also extracted from the Web; to identify likely fields and good …,Proceedings of the VLDB Endowment,2009,130
Personal information management with SEMEX,Yuhan Cai; Xin Luna Dong; Alon Halevy; Jing Michelle Liu; Jayant Madhavan,Abstract The explosion of information available in digital form has made search a hotresearch topic for the Information Management Community. While most of the research onsearch is focused on the WWW; individual computer users have developed their own vastcollections of data on their desktops; and these collections are in critical need for goodsearch and query tools. The problem is exacerbated by the proliferation of varied electronicdevices (laptops; PDAs; cellphones) that are at our disposal; which often hold subsets orvariations of our data. In fact; several recent venues have noted Personal InformationManagement (PIM) as an area of growing interest to the data management community [1; 8;6],Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,129
Information integration,Marti A.  Hearst; AY Levy; C Knoblock; S Minton; W Cohen,Despite the Web's current disorganized and anarchic state; many AI researchers believe thatit will become the world's largest knowledge base. We examine a line of research whosefinal goal is to make disparate data sources work together to better serve users' informationneeds. This work is known as information integration. The authors talk about its applicationto datasets made available over the Web. A. Levy discusses the relationship betweeninformation-integration and traditional database systems. He then enumerates importantissues in the field and demonstrates how the Information Manifold project has addressedsome of these. C. Knoblock and S. Minton describe the Ariadne system. Two of itsdistinguishing features are its use of wrapper algorithms to extract structured informationfrom semistructured data sources and its use of planning algorithms to determine how to …,IEEE Intelligent Systems and their Applications,1998,129
Google fusion tables: data management; integration and collaboration in the cloud,Hector Gonzalez; Alon Halevy; Christian S Jensen; Anno Langen; Jayant Madhavan; Rebecca Shapley; Warren Shen,Abstract Google Fusion Tables is a cloud-based service for data management andintegration. Fusion Tables enables users to upload tabular data files (spreadsheets; CSV;KML); currently of up to 100MB. The system provides several ways of visualizing the data(eg; charts; maps; and timelines) and the ability to filter and aggregate the data. It supportsthe integration of data from multiple sources by performing joins across tables that maybelong to different users. Users can keep the data private; share it with a select set ofcollaborators; or make it public and thus crawlable by search engines. The discussionfeature of Fusion Tables allows collaborators to conduct detailed discussions of the data atthe level of tables and individual rows; columns; and cells. This paper describes the innerworkings of Fusion Tables; including the storage of data in the system and the tight …,Proceedings of the 1st ACM symposium on Cloud computing,2010,128
Introduction to the special issue on semantic integration,AnHai Doan; Natalya F Noy; Alon Y Halevy,Abstract Semantic heterogeneity is one of the key challenges in integrating and sharing dataacross disparate sources; data exchange and migration; data warehousing; modelmanagement; the Semantic Web and peer-to-peer databases. Semantic heterogeneity canarise at the schema level and at the data level. At the schema level; sources can differ inrelations; attribute and tag names; data normalization; levels of detail; and the coverage of aparticular domain. The problem of reconciling schema-level heterogeneity is often referredto as schema matching or schema mapping. At the data level; we find differentrepresentations of the same real-world entities (eg; people; companies; publications; etc.).Reconciling data-level heterogeneity is referred to as data deduplication; record linkage;and entity/object matching. To exacerbate the heterogeneity challenges; schema …,ACM Sigmod Record,2004,123
Querying XML data,Alin Deutsch; Mary F.  Fernandez; Daniela Florescu; Alon Y.  Levy; David Maier; Dan Suciu,XML threatens to expand beyond its document markup origins to become the basis for datainterchange on the Internet. One highly anticipated application of XML is the interchange ofelectronic data (EDI). Unlike existing Web documents; electronic data is primarily intendedfor computer; not human; consumption. For example; businesses could publish data abouttheir products and services; and potential customers could compare and process thisinformation automatically; business partners could exchange internal operational databetween their information systems on secure channels; search robots could integrateautomatically information from related sources that publish their data in XML format; likestock quotes from financial sites; sports scores from news sites. New opportunities will arisefor third parties to add value by integrating; transforming; cleaning; and aggregating XML …,IEEE Data Eng. Bull.,1999,118
Crossing the Structure Chasm.,Alon Y Halevy; Oren Etzioni; AnHai Doan; Zachary G Ives; Jayant Madhavan; Luke K McDowell; Igor Tatarinov,Online information comes in two flavors: unstructured corpora of text on the one hand; andstructured data managed by databases and knowledge bases on the other. These twodifferent kinds of data lead to very different authoring; management and search paradigms.In the first; search is based on keywords and answers are ranked according to relevance. Inthe second; search is based on queries in a formal language (eg; SQL); and all the answersreturned for the query are correct according to the underlying semantics of the system. In theu-world of unstructured data; authoring data is straightforward. In contrast; in the s-world ofstructured data; authoring data is a conceptual effort that requires technical expertise andsubstantial up front effort; the author is required to provide a comprehensive structure (ie;schema) of the domain before entering data. This paper is focused on the profound …,CIDR,2003,116
Intelligent internet systems,Alon Y Levy; Daniel S Weld,*,*,2000,113
Supporting executable mappings in model management,Sergey Melnik; Philip A Bernstein; Alon Halevy; Erhard Rahm,Abstract Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators; such as Compose; Diff; andMerge; that are applied to models; such as database schemas or interface specifications;and to mappings between models. Prior model management solutions focused on a simpleclass of mappings that do not have executable semantics. Yet many metadata applicationsrequire that mappings be executable; expressed in SQL; XSLT; or other data transformationlanguages. In this paper; we develop a semantics for model-management operators thatallows applying the operators to executable mappings. Our semantics captures previously-proposed desiderata and is language-independent: the effect of the operators is expressedin terms of what they do to the instances of models and mappings. We describe an …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,110
Query containment for data integration systems,Todd Millstein; Alon Halevy; Marc Friedman,Abstract The problem of query containment is fundamental to many aspects of databasesystems; including query optimization; determining independence of queries from updates;and rewriting queries using views. In the data-integration framework; however; the standardnotion of query containment does not suffice. We define relative containment; whichformalizes the notion of query containment relative to the sources available to the data-integration system. First; we provide optimal bounds for relative containment for severalimportant classes of datalog queries; including the common case of conjunctive queries.Next; we provide bounds for the case when sources enforce access restrictions in the form ofbinding pattern constraints. Surprisingly; we show that relative containment for conjunctivequeries is still decidable in this case; even though it is known that finding all answers to …,Journal of Computer and System Sciences,2003,108
Challenges and Opportunities with big data 2011-1,Divyakant Agrawal; Philip Bernstein; Elisa Bertino; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jagadish; Alexandros Labrinidis; Sam Madden; Yannis Papakonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of``Big Data.''While the promise of Big Data is real--for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009--there is currently a wide gap between its potential and its realization.,*,2011,107
Semantic integration,Natalya F Noy; AnHai Doan; Alon Y Halevy,*,AI magazine,2005,103
Learning to map between structured representations of data,Anhai Doan; Alon Y Halevy; Pedro M Domingos,This dissertation studies representation matching: the problem of creating semanticmappings between two data representations. Examples of mappings are “element locationof one representation maps to element address of the other”;“contact-phone maps to agent-phone”; and “listed-price maps to price*(1+ tax-rate)”. We begin this chapter by showing thatrepresentation matching is a fundamental step in numerous data management applications.Next; we show that the manual creation of semantic mappings is extremely labor intensiveand hence has become a key bottleneck hindering the widespread deployment of the aboveapplications (Sections 1.2-1.3). We then outline our semi-automatic solutions torepresentation matching (Sections 1.4-1.5). Finally; we list the contributions and give a roadmap to the rest of the dissertation (Section 1.6-1.7).,*,2002,102
Harnessing the deep web: Present and future,Jayant Madhavan; Loredana Afanasiev; Lyublena Antova; Alon Halevy,Abstract: Over the past few years; we have built a system that has exposed large volumes ofDeep-Web content to Google. com users. The content that our system exposes contributes tomore than 1000 search queries per-second and spans over 50 languages and hundreds ofdomains. The Deep Web has long been acknowledged to be a major source of structureddata on the web; and hence accessing Deep-Web content has long been a problem ofinterest in the data management community. In this paper; we report on where we believethe Deep Web provides value and where it does not. We contrast two very differentapproaches to exposing Deep-Web content--the surfacing approach that we used; and thevirtual integration approach that has often been pursued in the data management literature.We emphasize where the values of each of the two approaches lie and caution against …,arXiv preprint arXiv:0909.1785,2009,101
The Claremont report on database research,Rakesh Agrawal; Anastasia Ailamaki; Philip A Bernstein; Eric A Brewer; Michael J Carey; Surajit Chaudhuri; Anhai Doan; Daniela Florescu; Michael J Franklin; Hector Garcia-Molina; Johannes Gehrke; Le Gruenwald; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; Hank F Korth; Donald Kossmann; Samuel Madden; Roger Magoulas; Beng Chin Ooi; Tim O'Reilly; Raghu Ramakrishnan; Sunita Sarawagi; Michael Stonebraker; Alexander S Szalay; Gerhard Weikum,Here; we explore the conclusions of this self-assessment. It is by definition somewhatinward-focused but may be of interest to the broader computing community as both a windowinto upcoming directions in database research and a description of some of the community issuesand initiatives that surfaced. We describe the group's consensus view of new focus areas forresearch; including database engine architectures; declarative programming languages; interplayof structured data and free text; cloud data services; and mobile and virtual worlds. We also reporton discussions of the database community's growth and processes that may be of interest toother research areas facing similar challenges … Over the past 20 years; small groups of databaseresearchers have periodically gathered to assess the state of the field and propose directionsfor future research. 1;3;4;5;6;7 Reports of the meetings served to foster debate within the …,Communications of the ACM,2009,100
A formal perspective on the view selection problem,Rada Chirkova; Alon Y Halevy; Dan Suciu,Abstract The view selection problem is to choose a set of views to materialize over adatabase schema; such that the cost of evaluating a set of workload queries is minimizedand such that the views fit into a prespecified storage constraint. The two main applicationsof the view selection problem are materializing views in a database to speed up queryprocessing; and selecting views to materialize in a data warehouse to answer decisionsupport queries. We describe several fundamental results concerning the view selectionproblem. We consider the problem for views and workloads that consist of equalityselection;project and join queries; and show that the complexity of the problem depends crucially onthe quality of the estimates that a query optimizer has on the size of the views it isconsidering to materialize. When a query optimizer has good estimates of the sizes of the …,VLDB,2001,98
Query optimization by predicate move-around,*,Query optimization which is done by making a graph of the query and moving predicatesaround in the graph so that they will be applied early in the optimized query generated fromthe graph. Predicates are first propagated up from child nodes of the graph to parent nodesand then down into different child nodes. After the predicates have been moved; redundantpredicates are detected and removed. Predicates are moved through aggregationoperations and new predicates are deduced from aggregation operations and fromfunctional dependencies. The optimization is not dependent on join order and works wherenodes of the graph cannot be merged.,*,1997,97
The Nimble XML data integration system,Denise Draper; Alon Y Halevy; Daniel S Weld,For better or for worse; XML has emerged as a de facto standard for data interchange. Thisconsensus is likely to lead to increased demand for technology that allows users to integratedata from a variety of applications; repositories; and partners; which are located across thecorporate intranet or on the Internet. Nimble Technology has spent two years developing aproduct to service this market. Originally conceived after decades of person-years ofresearch on data integration; the product is now being deployed at several Fortune-500 beta-customer sites. The article reports on the key challenges faced in the design of our productand highlights some issues which require more attention from the research community. Inparticular we address architectural issues arising from designing a product to support XMLas its core representation; choices in the design of the underlying algebra; on-the-fly data …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,95
A formal perspective on the view selection problem,Rada Chirkova; Alon Y Halevy; Dan Suciu,Abstract The view selection problem is to choose a set of views to materialize over adatabase schema; such that the cost of evaluating a set of workload queries is minimizedand such that the views fit into a prespecified storage constraint. The two main applicationsof the view selection problem are materializing views in a database to speed up queryprocessing; and selecting views to materialize in a data warehouse to answer decisionsupport queries. In addition; view selection is a core problem for intelligent data placementover a wide-area network for data integration applications and data management forubiquitous computing. We describe several fundamental results concerning the viewselection problem. We consider the problem for views and workloads that consist of equality-selection; project and join queries; and show that the complexity of the problem depends …,The VLDB Journal—The International Journal on Very Large Data Bases,2002,93
Structured data on the web,Michael J Cafarella; Alon Halevy; Jayant Madhavan,Though the web is best known as a vast repository of shared documents; it also contains a significantamount of structured data covering a complete range of topics; from product to financial;public-record; scientific; hobby-related; and government. Structured data on the Web sharesmany similarities with the kind of data traditionally managed by commercial database systemsbut also reflects some unusual characteristics of its own; for example; it is embedded in textualWeb pages and must be extracted prior to use; there is no centralized data design as there isin a traditional database; and; unlike traditional databases that focus on a single domain; it coverseverything. Existing data-management systems do not address these challenges and assumetheir data is modeled within a well-defined domain … This article discusses the nature ofWeb-embedded structured data and the challenges of managing it. To begin; we present …,Communications of the ACM,2011,92
Web-scale extraction of structured data,Michael J Cafarella; Jayant Madhavan; Alon Halevy,Abstract A long-standing goal of Web research has been to construct a unified Webknowledge base. Information extraction techniques have shown good results on Web inputs;but even most domain-independent ones are not appropriate for Web-scale operation. Inthis paper we describe three recent extraction systems that can be operated on the entireWeb (two of which come from Google Research). The TextRunner system focuses on rawnatural language text; the WebTables system focuses on HTML-embedded tables; and thedeep-web surfacing system focuses on" hidden" databases. The domain; expressiveness;and accuracy of extracted data can depend strongly on its source extractor; we describedifferences in the characteristics of data produced by the three extractors. Finally; we discussa series of unique data applications (some of which have already been prototyped) that …,ACM SIGMOD Record,2009,92
Adaptive query processing for internet applications,Zachary G Ives; Alon Y Levy; Daniel S Weld; Daniela Florescu; Marc Friedman,Abstract As the area of data management for the Internet has gained in popularity; recentwork has focused on effectively dealing with unpredictable; dynamic data volumes andtransfer rates using adaptive query processing techniques. Important requirements of theInternet domain include:(1) the ability to process XML data as it streams in from the network;in addition to working on locally stored data;(2) dynamic scheduling of operators to adjust toI/O delays and flow rates;(3) sharing and re-use of data across multiple queries; wherepossible;(4) the ability to output results and later update them. An equally importantconsideration is the high degree of variability in performance needs for different queryprocessing domains: perhaps an ad-hoc query application should optimize for display ofincomplete and partial incremental results; whereas a corporate data integration …,*,2000,92
Schema mediation for large-scale semantic data sharing,Y Halevy; G Ives; Dan Suciu; Igor Tatarinov,Abstract Intuitively; data management and data integration tools should be well suited forexchanging information in a semantically meaningful way. Unfortunately; they suffer fromtwo significant problems: they typically require a common and comprehensive schemadesign before they can be used to store or share information; and they are difficult to extendbecause schema evolution is heavyweight and may break backward compatibility. As aresult; many large-scale data sharing tasks are more easily facilitated by non-database-oriented tools that have little support for semantics. The goal of the peer data managementsystem (PDMS) is to address this need: we propose the use of a decentralized; easilyextensible data management architecture in which any user can contribute new data;schema information; or even mappings between other peers' schemas. PDMSs represent …,The VLDB Journal—The International Journal on Very Large Data Bases,2005,90
Finding related tables,Anish Das Sarma; Lujun Fang; Nitin Gupta; Alon Halevy; Hongrae Lee; Fei Wu; Reynold Xin; Cong Yu,Abstract We consider the problem of finding related tables in a large corpus of heterogenoustables. Detecting related tables provides users a powerful tool for enhancing their tables withadditional data and enables effective reuse of available public data. Our first contribution is aframework that captures several types of relatedness; including tables that are candidatesfor joins and tables that are candidates for union. Our second contribution is a set ofalgorithms for detecting related tables that can be either unioned or joined. We describe aset of experiments that demonstrate that our algorithms produce highly related tables. Wealso show that we can often improve the results of table search by pulling up tables that areranked much lower based on their relatedness to top-ranked tables. Finally; we describehow to scale up our algorithms and show the results of running it on a corpus of over a …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,89
A model for data integration systems of biomedical data applied to online genetic databases.,Peter Mork; Alon Halevy; Peter Tarczy-Hornoch,Abstract We present a general model for data integration systems using a mediated schemato represent commonalities in the underlying sources. These sources are mapped to themediated schema using source descriptions. Users can pose queries against the mediatedschema; allowing the system to generate automatically a query plan that enumerates andranks all possible ways in which the query could be answered. We apply this approach tothe domain of online genetic databases; demonstrating the system s ability to answerrelevant queries across multiple sources.,Proceedings of the AMIA Symposium,2001,89
Semantic query optimization in datalog programs,Alon Y Levy; Yehoshua Sagiv,Abstract Semantic query optimization refers to the process of using integrity constraints(ic's)in order to optimize the evaluation of queries. The process is well understood in the case ofunions of select-project-join queries(ie; nonrecursive datalog). For arbitrary datalogprograms; however; the issue has largely remained an unsolved problem. This paperstudies this problem and shows when semantic query optimization can be completely donein recursive rules provided that order constraints and negated EDB subgoals appear only inthe recursive rules; but not in the it's. If either order constraints or negated EDB subgoals areintroduced in it's; then the problem of semantic query optimization becomes undecidable.Since semantic query optimization is closely related to the containment problem of a datalogprogram in a union of conjunctive queries; our results also imply new decidability and …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,86
Deciding containment for queries with complex objects,Alon Y Levy; Dan Suciu,Abstract We address the problem of query containment and query equivalence for complexobjects. We show that for a certain conjunctive query language for complex objects; querycontainment and weak query equivalence are decidable. Our results also have twoimportant consequences. Fist; when the answers of the two queries are guaranteed not tocontain empty sets; then weak equivalence coincides with equivalence; and our resultanswers partially an open problem about the equivalence of nest; unmst queries for complexobjects [24]. Second; we show that checking the equivalence of conjunctive queries withgrouping and aggregates is NP-complete. Our results rely on a translation of thecontainment and equivalence conditions for complex objects into novel conditions onconjunctive queries; which we call simulation and strong simulation respectively. These …,Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1997,83
Data Integration: A Status Report.,Alon Y Halevy,*,BTW,2003,82
Combining artificial intelligence and databases for data integration,Alon Y Levy,Abstract Data integration is a problem at the intersection of the fields of Artificial Intelligenceand Database Systems. The goal of a data integration system is to provide a uniforminterface to a multitude of data sources; whether they are within one enterprise or on theWorld-Wide Web. The key challenges in data integration arise because the data sourcesbeing integrated have been designed independently for autonomous applications; and theircontents are related in subtle ways. As a result; a data integration system requires richformalisms for describing contents of data sources and relating between contents of differentsources. This paper discusses works aimed at applying techniques from ArtificialIntelligence to the problem of data integration. In addition to employing KnowledgeRepresentation techniques for describing contents of information sources; projects have …,*,1999,82
Answering queries using views: A survey,Alon Levy,Abstract The problem of answering queries using views is to find efficient methods ofanswering a query using a set of previously materialized views over the database; ratherthan accessing the database relations. The problem has recently received significantattention because of its relevance to a wide variety of data management problems. In queryoptimization; finding a rewriting of a query using a set of materialized views can yield a moreefficient query execution plan. To support the separation of the logical and physical views ofdata; a storage schema can be described using views over the logical schema. As a result;finding a query execution plan that accesses the storage amounts to solving the problem ofanswering queries using views. Finally; the problem arises in data integration systems;where data sources can be described as precomputed views. This article surveys the …,*,2000,81
XML query languages: Experiences and exemplars,Mary Fernandez; Jérôme Siméon; Philip Wadler; S Cluet; A Deutsch; D Florescu; A Levy; D Maier; J McHugh; J Robie; D Suciu; J Widom,Abstract This paper identi es essential features of an XML query language by examining fourexisting query languages: XML-QL; YATL; Lorel; and XQL. The rst three languages comefrom the database community and possess striking similarities. The fourth comes from thedocument community and lacks some key functionality of the other three.,*,1999,81
Apparatus and methods for retrieving information,*,A query translator translates a query between a graphical user interface and a knowledgerepresentation system. The knowledge representation system reformulates the query andgenerates an access plan to access data requested by the query. The access plan utilizesseveral different protocols to access the query information located in dissimilar databasesdistributed throughout a network. The knowledge representation system generates theaccess plan by first processing the query through a world view which defines the informationin conceptual terms that a human being would understand and then processes the querythrough a system/network view which redefines the query into network and database accessinformation so that the data requested by the query can be located. Placing the world viewand the system network view in the knowledge representation system enables real-time …,*,1997,79
Constraints and redundancy in datalog,Alon Levy; Yehoshua Sagiv,Abstract Two types of redundancies in datalog program are considered. Redundancy basedon reachability eliminates rules and predicates that do not participate in any derivation treeof a fact for the query predicate. Redundancy based on irrelevance is similar; but considersonly minimal derivation trees; that is; derivation trees having no pair of identical atoms; suchthat one is an ancestor of the other. Algorithms for detecting these redundancies are given;including the case of programs with constraint literals. These algorithms not only detectredundancies in the presence of constraints; but also push constraints from the given queryand rules to the EDB predicates. Under certain assumptions discussed in the paper; theconstraints are pushed to the EDB as tightly as possible.,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,77
Digital library information-technology infrastructures,Yannis Ioannidis; David Maier; Serge Abiteboul; Peter Buneman; Susan Davidson; Edward Fox; Alon Halevy; Craig Knoblock; Fausto Rabitti; Hans Schek; Gerhard Weikum,Abstract This paper charts a research agenda on systems-oriented issues in digital libraries.It focuses on the most central and generic system issues; including system architecture; user-level functionality; and the overall operational environment. With respect to user-levelfunctionality; in particular; it abstracts the overall information lifecycle in digital libraries tofive major stages and identifies key research problems that require solution in each stage.Finally; it recommends an explicit set of activities that would help achieve the research goalsoutlined and identifies several dimensions along which progress of the digital library fieldcan be evaluated.,International journal on digital libraries,2005,75
Equivalence; query-reachability and satisfiability in Datalog extensions,Alon Levy; Inderpal Singh Mumick; Yehoshua Sagiv; Oded Shmueli,Abstract We consider the problems of equivalence; satisfiability and query-reachability fordatalog programs with negation and dense-order constraints. These problems are importantfor optimizing datalog programs. We show that both query-reachability and satisfiability aredecidable for programs with stratified negation provided that negation is applied only to EDBpredicates or that all EDB predicates are unary. In the latter case; we show that equivalenceis also decidable. The algorithms we present are also used to push constraints from a givenquery to the EDB predicates. Finally; we show that satisfiability is undecidable for datalogprograms with unary IDB predicates; stratified negation and the interpreted predicate≠,Proceedings of the twelfth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1993,75
Verification of knowledge bases based on containment checking,Alon Y Levy; Marie-Christine Rousset,Abstract Building complex knowledge based applications requires encoding large amountsof domain knowledge. After acquiring knowledge from domain experts; much of the effort inbuilding a knowledge base goes into verifying that the knowledge is encoded correctly. Aknowledge base is verified if it can be shown that certain constraints always hold betweenthe inputs and the outputs. We consider the knowledge base verification problem for Hornrule knowledge bases and for three kinds of constraints: I/O consistency constraints; I/Odependency constraints; and input completeness constraints. For the first two cases; weestablish tight complexity results on the problem; and show in what cases it is decidable. Inthe third case; we show that the problem is; in general; undecidable; and we identify twodecidable cases. In our analysis we show how the properties of the problem vary …,Artificial Intelligence,1998,73
Openii: an open source information integration toolkit,Len Seligman; Peter Mork; Alon Halevy; Ken Smith; Michael J Carey; Kuang Chen; Chris Wolf; Jayant Madhavan; Akshay Kannan; Doug Burdick,Abstract OpenII (openintegration. org) is a collaborative effort to create a suite of open-source tools for information integration (II). The project is leveraging the latest developmentsin II research to create a platform on which integration tools can be built and further researchconducted. In addition to a scalable; extensible platform; OpenII includes industrial-strengthcomponents developed by MITRE; Google; UC-Irvine; and UC-Berkeley that interoperatethrough a common repository in order to solve II problems. Components of the toolkit havebeen successfully applied to several large-scale US government II challenges.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,72
Piazza: mediation and integration infrastructure for semantic web data,Zachary G Ives; Alon Y Halevy; Peter Mork; Igor Tatarinov,Abstract The Semantic Web envisions a World Wide Web in which data is described withrich semantics and applications can pose complex queries. To this point; researchers havedefined new languages for specifying meanings for concepts and developed techniques forreasoning about them; using RDF as the data model. To flourish; the Semantic Web needsto provide interoperability—both between sites with different terminologies and with existingdata and the applications operating on them. To achieve this; we are faced with twoproblems. First; most of the world's data is available not in RDF but in XML; XML and theapplications consuming it rely not only on the domain structure of the data; but also on itsdocument structure. Hence; to provide interoperability between such sources; we must mapbetween both their domain structures and their document structures. Second; data …,Web Semantics: Science; Services and Agents on the World Wide Web,2004,72
Method and system for using materialized views to evaluate queries involving aggregation,*,The present invention is a method and system for using materialized views to computeanswers to SQL queries with grouping and aggregation. A query is evaluated a using amaterialized view. The materialized view is semantically analyzed to determine whether thematerialized view is usable in evaluating an input query. The semantic analysis includesdetermining that the materialized view does not project out any columns needed to evaluatethe input query and determining that the view does not discard any tuple that satisfies acondition enforced in the input query. If the view is usable; the input query is rewritten toproduce an output query that is multi-set equivalent to the input query and that specifies oneor more occurrences of the materialized view as a source of information to be returned bythe output query. The output query is then evaluated. The semantic analysis and rewriting …,*,1999,72
A Semantic Theory of Abstractions.,P Pandurang Nayak; Alon Y Levy,*,IJCAI,1995,72
Integration of an information server database schema by generating a translation map from exemplary files,*,A method of reducing the burden on database administrators when integrating informationfrom a database system with a computer system over a computer network is disclosed.Rather than requiring the database administrator to express the meaning of databaseattribute names in a new language; the database administrators needs only to specifymappings between different database schemas by creating database example files. Thedatabase example files contain a common body of information values stored using thecorresponding attribute names of the different database schemas. The database examplefiles then become the basis for generation of a translation map between the computersystem and the remote database system. Then; information queries from a user aretranslated with the translation map to the database schema of the remote database …,*,1998,71
Optimization of run-time management of data intensive web-sites,Daniela Florescu; Alon Levy; Dan Suciu; Khaled Yagoub,Abstract An increasing number of web sites have their data extracted from relationaldatabases. Several commercial products and research prototypes have been moving in thedirection of declarative speciﬁcation of the sites' structure and content. Speciﬁcally; the entiresite is speciﬁed using a collection of queries describing the site's nodes (corresponding toweb pages and the data contained in them) and edges (corresponding to the hyperlinks).Given this paradigm; an important issue is when to compute the site's pages. Two extremeapproaches; with obvious drawbacks; are (1) to precompute the entire site in advance; and(2) to evaluate on demand all the queries necessary to construct a given page. We considerthe problem of automatically optimizing the run-time management of declaratively speciﬁedweb sites. In our approach; given a declarative site speciﬁcation and constraints on the …,VLDB,1999,70
Mangrove: Enticing ordinary people onto the semantic web via instant gratification,Luke McDowell; Oren Etzioni; Steven D Gribble; Alon Halevy; Henry Levy; William Pentney; Deepak Verma; Stani Vlasseva,Abstract Despite numerous efforts; the semantic web has yet to achieve widespreadadoption. Recently; some researchers have argued that participation in the semantic web istoo difficult for “ordinary” people; limiting its growth and popularity. In response; this paperintroduces Mangrove; a system whose goal is to entice non-technical people to semanticallyannotate their existing HTML data. Mangrove seeks to alter the cost-benefit equation ofauthoring semantic content. To increase the benefit; Mangrove is designed to makesemantic content instantly available to services that consume the content and yieldimmediate; tangible benefit to authors. To reduce the cost; Mangrove makes semanticauthoring as painless as possible by transferring some of the burden of schema design; datacleaning; and data structuring from content authors to the programmers who create …,International Semantic Web Conference,2003,69
Efficient evaluation of regular path expressions on streaming XML data,Zachary Ives; Alon Levy; D Weld,Abstract The adoption of XML promises to accelerate construction of systems that integratedistributed; heterogeneous data. Query languages for XML are typically based on regularpath expressions that traverse the logical XML graph structure; the efficient evaluation ofsuch path expressions is central to good query processing performance. Most existing XMLquery processing systems convert XML documents to an internal representation; generally aset of tables or objects; path expressions are evaluated using either index structures or joinoperations across the tables or objects. Unfortunately; the required index creation or joinoperations are often costly even with locally stored data; and they are especially expensivein the data integration domain; where the system reads data streamed from remote sourcesacross a network; and seldom reuses results for subsequent queries. This paper presents …,*,2000,67
Verifying integrity constraints on web sites,Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,Abstract Data-intensive Web sites have created a new form of knowledge base; as richlystructured bodies of data. Several novel systems for creating dataintensive Web sitessupport declarative specification of a site's structure and content (ie; the pages; the dataavailable in each page; and the links between pages). Declarative systems provide aplatform on which A1 techniques can be developed that; further simplify the tasks ofconstructing and maintaining Web sites. This paper addresses the problem of specifying andverifying integrity constraints on a Web site's structure. We describe a language that cancapture many practical constraints and an accompanying sound and complete verificationalgorithm. The algorithm has the important property that if the constraints are violated; itproposes fixes to either the constraints or to the site definition. Finally; we establish tight …,IJCAI,1999,64
Xml-ql: A query language for XML; 1998,Alin Deutsch; Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,*,*,1999,60
Semantic email,Luke McDowell; Oren Etzioni; Alon Halevy; Henry Levy,Abstract This paper investigates how the vision of the Semantic Web can be carried overtothe realm of email. We introduce a general notion of semantice mail; in which an emailmessage consists of an RDF query or update coupled with corresponding explanatory text.Semantic email opens the door to a wide range of automated; email-mediated applicationswith formally guaranteed properties. In particular; this paper introduces a broad class ofsemantic email processes. For example consider the process of sending an email to aprogram committee asking who will attend the PC dinner automatically collecting theresponses and tallying them up. We define bothlogical and decision-theoretic models wherean email process ismodeled as a set of updates to a data set on which we specify goals viacertain constraints or utilities. We then describe a set ofinference problems that arise …,Proceedings of the 13th international conference on World Wide Web,2004,59
Method and apparatus for optimizing database queries involving aggregation predicates,*,A method and apparatus for optimizing a query involving aggregation. Aggregationpredicates are represented by a constraint language through which relationships involvingaggregation predicates can be shown. As a result; new predicates can be inferred from aninitial set of aggregation predicates and optimization techniques which involve inferring newpredicates can be applied. It is also shown how to restrict the inferences involvingaggregation predicates to only those predicates which will lead to new predicates which arerelevant to query optimization. In general; the technique is not limited to query optimization;but may be applied to various applications involving the manipulation of aggregationpredicates.,*,2000,59
Efficient query processing for data integration,Zachary G Ives; Alon Halevy,The processing of queries written in a declarative language (eg; SQL or XQuery) has been asubject of intense study since the origins of the relational database system; with IBM'sSystem-R [SAC+ 79] and Berkeley's Ingres [SWKH76] projects from the 1970s. The standardapproach has been to take a declarative; user-supplied query and to try to select an order ofevaluation and the most appropriate algorithmic implementations for the operations in thequery—these are expressed within a query plan. The query plan is then executed; fetchingdata from source relations and combining it according to the operators to produce results.System-R established a standard approach to query processing that is still followed today.This approach is very similar to compilation and execution of traditional languages: a queryoptimizer statically compiles the query into a plan; attempting to pick the most efficient …,*,2002,57
Static analysis in datalog extensions,Alon Y Halevy; Inderpal Singh Mumick; Yehoshua Sagiv; Oded Shmueli,Abstract We consider the problems of containment; equivalence; satisfiability and query-reachability for datalog programs with negation. These problems are important for optimizingdatalog programs. We show that both query-reachability and satisfiability are decidable forprograms with stratified negation provided that negation is applied only to EDB predicates orthat all EDB predicates are unary. In the latter case; we show that equivalence is alsodecidable. The algorithms we present can also be used to push constraints from a givenquery to the EDB predicates. In showing our decidability results we describe a powerful tool;the query-tree; which is used for several optimization problems for datalog programs. Finally;we show that satisfiability is undecidable for datalog programs with unary IDB predicates;stratified negation and the interpreted predicate≠.,Journal of the ACM (JACM),2001,56
Method and apparatus for web site management,*,A method and apparatus for managing a web-site. An information definition query isreceived for defining an integrated view of non-uniform information retrieved from a multiplesources across a network; and stored in multiple formats. An integrated view is createdaccording to the information definition query. A site definition query is received for defining asite view for the non-uniform information at the web-site. The site view is created accordingto the site definition query. The non-uniform information is then presented on the web-site;typically in the form of a HTML document.,*,1999,56
Declarative Web Site Management with Tiramisu.,Corin R Anderson; Alon Y Levy; Daniel S Weld,Abstract Early research in declarative web-site management identi ed a key principle: theseparation of data selection; site structure; and page presentation through the introduction ofa logical representation; the site graph; de ned as a view over underlying data. While theseparation of these three tasks provides many bene ts; existing systems require that the userimplement the web site with the same system they use to design it; and this is problematic forthree reasons. First; many users are familiar with and prefer another implementation tool.Second; other tools may provide useful; new features. Third; a complex site may becontrolled by multiple organizations whose standards require di erent tools. In this paper wepresent a new architecture for declarative web-site management which separates designand implementation. In addition; we describe the challenges we faced during the …,WebDB (Informal Proceedings),1999,56
The limits on combining recursive Horn rules with description logics,Alon Y Levy; Marie-Christine Rousset,Abstract Horn rule languages have formed the basis for many Arti cial Intelligenceapplication languages; but are not expressive enough to model domains with a richhierarchical structure. Description logics have been designed especially to model richhierarchies. Several applications would signi cantly bene t from combining the expressivepower of both formalisms. This paper focuses on combining recursive function-free Hornrules with the expressive description logic ALCN R; and shows exactly when a hybridlanguage with decidable inference can be obtained. First; we show that several of the coreconstructors of description logics lead by themselves to undecidability of inference whencombined with recursive function-free Horn rules. We then show that without theseconstructors we obtain a maximal subset of ALCN Rthat yields a decidable hybrid …,AAAI/IAAI; Vol. 1,1996,56
Automated model selection for simulation,Yumi Iwasaki; Alon Y Levy,Abstract Constructing an appropriate model is crucial in reasoning successfully about thebehavior of a physical situation to answer a query. In compositional modeling; a system isprovided with a library of composible pieces of knowledge about the physical world calledmodel fragments. Its task is to select appropriate model fragments to describe the situation;either for static analysis of a single state; or for the more complicated case simulation ofdynamic behavior over a sequence of states. In previous work we showed how the modelconstruction problem in general can advantageously be formulated as a problem ofreasoning about relevance. This paper presents an actual algorithm; based on relevancereasoning; for selecting model fragments e ciently for the case of simulation. We show thatthe algorithm produces an adequate model for a given query and moreover; it is the …,AAAI,1994,56
Towards efficient information gathering agents,Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,*,Working Notes of the AAAI Spring Symposium on Software Agents,1994,56
Answering Structured Queries on Unstructured Data.,Jing Liu; Xin Dong; Alon Y Halevy,Abstract There is growing number of applications that require access to both structured andunstructured data. Such collections of data have been referred to as dataspaces; andDataspace Support Platforms (DSSPs) were proposed to offer several services overdataspaces; including search and query; source discovery and categorization; indexing andsome forms of recovery. One of the key services of a DSSP is to provide seamless queryingon the structured and unstructured data. Querying each kind of data in isolation has beenthe main subject of study for the fields of databases and information retrieval. Recently thedatabase community has studied the problem of answering keyword queries on structureddata such as relational data or XML data. The only combination that has not been fullyexplored is answering structured queries on unstructured data. This paper explores an …,WebDB,2006,55
Representing uncertain data: models; properties; and algorithms,Anish Das Sarma; Omar Benjelloun; Alon Halevy; Shubha Nabar; Jennifer Widom,Abstract In general terms; an uncertain relation encodes a set of possible certain relations.There are many ways to represent uncertainty; ranging from alternative values for attributesto rich constraint languages. Among the possible models for uncertain data; there is atension between simple and intuitive models; which tend to be incomplete; and completemodels; which tend to be nonintuitive and more complex than necessary for manyapplications. We present a space of models for representing uncertain data based on avariety of uncertainty constructs and tuple-existence constraints. We explore a number ofproperties and results for these models. We study completeness of the models; as well asclosure under relational operations; and we give results relating closure and completeness.We then examine whether different models guarantee unique representations of …,The VLDB Journal—The International Journal on Very Large Data Bases,2009,54
Visualization of heterogeneous data,Mike Cammarano; Xin Dong; Bryan Chan; Jeff Klingner; Justin Talbot; Alon Halevey; Pat Hanrahan,Both the resource description framework (RDF); used in the semantic web; and Maya Viz u-forms represent data as a graph of objects connected by labeled edges. Existing systems forflexible visualization of this kind of data require manual specification of the possiblevisualization roles for each data attribute. When the schema is large and unfamiliar; thisrequirement inhibits exploratory visualization by requiring a costly up-front data integrationstep. To eliminate this step; we propose an automatic technique for mapping data attributesto visualization attributes. We formulate this as a schema matching problem; findingappropriate paths in the data model for each required visualization attribute in avisualization template.,IEEE Transactions on Visualization and Computer Graphics,2007,54
Hyper-local; directions-based ranking of places,Petros Venetis; Hector Gonzalez; Christian S Jensen; Alon Halevy,Abstract Studies find that at least 20% of web queries have local intent; and the fraction ofqueries with local intent that originate from mobile properties may be twice as high. Theemergence of standardized support for location providers in web browsers; as well as ofproviders of accurate locations; enables so-called hyper-local web querying where thelocation of a user is accurate at a much finer granularity than with IP-based positioning. Thispaper addresses the problem of determining the importance of points of interest; or places;in local-search results. In doing so; the paper proposes techniques that exploit loggeddirections queries. A query that asks for directions from a location a to a location b is taken tosuggest that a user is interested in traveling to b and thus is a vote that location b isinteresting. Such user-generated directions queries are particularly interesting because …,Proceedings of the VLDB Endowment,2011,53
Structured data meets the Web: a few observations.,Jayant Madhavan; Alon Y Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu,Abstract The World Wide Web is witnessing an increase in the amount of structured content–vast heterogeneous collections of structured data are on the rise due to the Deep Web;annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper we articulate challenges based onour experience with addressing them at Google; and offer some principles for addressingthem in a general fashion.,IEEE Data Eng. Bull.,2006,52
Containment of nested XML queries,Xin Dong; Alon Y Halevy; Igor Tatarinov,Abstract Query containment is the most fundamental relationship between a pair of databasequeries: a query Q is said to be contained in a query Q′ if the answer for Q is always asubset of the answer for Q′; independent of the current state of the database. Querycontainment is an important problem in a wide variety of data management applications;including verification of integrity constraints; reasoning about contents of data sources indata integration; semantic caching; verification of knowledge bases; determining queriesindependent of updates; and most recently; in query reformulation for peer datamanagement systems. Query containment has been studied extensively in the relationalcontext and for XPath queries; but not for XML queries with nesting. We consider thetheoretical aspects of the problem of query containment for XML queries with nesting. We …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,50
Biperpedia: An ontology for search applications,Rahul Gupta; Alon Halevy; Xuezhi Wang; Steven Euijong Whang; Fei Wu,Abstract Search engines make significant efforts to recognize queries that can be answeredby structured data and invest heavily in creating and maintaining high-precision databases.While these databases have a relatively wide coverage of entities; the number of attributesthey model (eg; GDP; CAPITAL; ANTHEM) is relatively small. Extending the number ofattributes known to the search engine can enable it to more precisely answer queries fromthe long and heavy tail; extract a broader range of facts from the Web; and recover thesemantics of tables on the Web. We describe Biperpedia; an ontology with 1.6 M (class;attribute) pairs and 67K distinct attribute names. Biperpedia extracts attributes from the querystream; and then uses the best extractions to seed attribute extraction from text. For everyattribute Biperpedia saves a set of synonyms and text patterns in which it appears …,Proceedings of the VLDB Endowment,2014,49
Information manifold for query processing,*,A system and method for accepting and responding to queries based on information storedon multiple heterogenous information sources. A uniform query interface to large collectionsof structured information sources is provided to a user to pose queries using a uniformschema of the available information. A query plan for answering the query is formulated fromdescriptions of the contents and capabilities of the available information sources. Based onthese descriptions logical solutions which are subsets of the complete solution to the queryare derived. An order for executing these solutions is determined based on the inputrequirements and other capabilities of the relevant information sources.,*,1999,49
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47
Renoun: Fact extraction for nominal attributes,Mohamed Yahya; Steven Whang; Rahul Gupta; Alon Halevy,Abstract Search engines are increasingly relying on large knowledge bases of facts toprovide direct answers to users' queries. However; the construction of these knowledgebases is largely manual and does not scale to the long and heavy tail of facts. Openinformation extraction tries to address this challenge; but typically assumes that facts areexpressed with verb phrases; and therefore has had difficulty extracting facts for noun-basedrelations. We describe ReNoun; an open information extraction system that complementsprevious efforts by focusing on nominal attributes and on the long tail. ReNoun's approach isbased on leveraging a large ontology of noun attributes mined from a text corpus and fromuser queries. ReNoun creates a seed set of training data by using specialized patterns andrequiring that the facts mention an attribute in the ontology. ReNoun then generalizes …,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014,46
PQL: a declarative query language over dynamic biological schemata.,Peter Mork; Ron Shaker; Alon Halevy; Peter Tarczy-Hornoch,Abstract We introduce the PQL query language (PQL) used in the GeneSeek genetic dataintegration project. PQL incorporates many features of query languages for semi-structureddata. To this we add the ability to express metadata constraints like intended semantics anddatabase curation approach. These constraints guide the dynamic generation of potentialquery plans. This allows a single query to remain relevant even in the presence of sourceand mediated schemas that are continually evolving; as is often the case in data integration.,Proceedings of the AMIA Symposium,2002,46
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45
Integrating network-bound XML data,Zachary G Ives; Alon Y Halevy; Daniel S Weld,Abstract Although XML was originally envisioned as a replacement for HTML on the web; tothis point it has instead been used primarily as a format for on-demand interchange of databetween applications and enterprises. The web is rather sparsely populated with static XMLdocuments; but nearly every data management application today can export XML data.There is great interest in integrating such exported data across applications andadministrative boundaries; and as a result; efficient techniques for integrating XML dataacross local-and wide-area networks are an important research focus.,Departmental Papers (CIS),2001,45
Automated model selection for simulation based on relevance reasoning,Alon Y Levy; Yumi Iwasaki; Richard Fikes,Abstract Constructing an appropriate model is a crucial step in performing the reasoningrequired to successfully answer a query about the behavior of a physical situation. In thecompositional modeling approach of Falkenhainer and Forbus (1991); a system is providedwith a library of composable pieces of knowledge about the physical world called modelfragments. The model construction problem involves selecting appropriate model fragmentsto describe the situation. Model construction can be considered either for static analysis of asingle state or for simulation of dynamic behavior over a sequence of states. The latter issignificantly more difficult than the former since one must select model fragments withoutknowing exactly what will happen in the future states. The model construction problem ingeneral can advantageously be formulated as a problem of reasoning about relevance of …,Artificial Intelligence,1997,45
Irrelevance reasoning in knowledge based systems,Alon Yitzchak Levy,Abstract: This dissertation considers the problem of reasoning about irrelevance ofknowledge in a principled and efficient manner. Specifically; it is concerned with two keyproblems:(1) developing algorithms for automatically deciding what parts of a knowledgebase are irrelevant to a query and (2) the utility of relevance reasoning. The dissertationdescribes a novel tool; the query-tree; for reasoning about irrelevance. Based on the query-tree; we develop several algorithms for deciding what formulas are irrelevant to a query. Ourgeneral framework sheds new light on the problem of detecting independence of queriesfrom updates. We present new results that significantly extend previous work in this area.The framework also provides a setting in which to investigate the connection between thenotion of irrelevance and the creation of abstractions. We propose a new approach to …,*,1993,45
Functional Dependency Generation and Applications in Pay-As-You-Go Data Integration Systems.,Daisy Zhe Wang; Xin Luna Dong; Anish Das Sarma; Michael J Franklin; Alon Y Halevy,ABSTRACT Recently; the opportunity of extracting structured data from the Web has beenidentified by a number of research projects. One such example is that millions of relational-style HTML tables can be extracted from the Web. Traditional data integration approachesdo not scale over such corpora with hundreds of small tables in one domain. To solve thisproblem; previous work has proposed pay-as-you-go data integration systems to provide;with little up-front cost; base services over loosely-integrated information. One keycomponent of such systems; which has received little attention to date; is the need for aframework to gauge and improve the quality of the integration. We propose a frameworkbased on functional dependencies (FDs). Unlike in traditional database design; where FDsare specified as statements of truth about all possible instances of the database; in web …,WebDB,2009,44
Data management projects at Google,Michael Cafarella; Edward Chang; Andrew Fikes; Alon Halevy; Wilson Hsieh; Alberto Lerner; Jayant Madhavan; S Muthukrishnan,Abstract This article describes some of the ongoing research projects related to structureddata management at Google today. The organization of Google encourages researchscientists to work closely with engineering teams. As a result; the research projects tend tobe motivated by real needs faced by Google's products and services; and solutions are putinto production and tested rapidly. In addition; because of the sheer scale at which Googleoperates; the engineering challenges faced by Google's services often require researchinnovations.,ACM SIGMOD Record,2008,43
Speeding up inferences using relevance reasoning: a formalism and algorithms,Alon Y Levy; Richard E Fikes; Yehoshua Sagiv,Abstract Irrelevance reasoning refers to the process in which a system reasons about whichparts of its knowledge are relevant (or irrelevant) to a specific query. Aside from itsimportance in speeding up inferences from large knowledge bases; relevance reasoning iscrucial in advanced applications such as modeling complex physical devices andinformation gathering in distributed heterogeneous systems. This article presents a novelframework for studying the various kinds of irrelevance that arise in inference and efficientalgorithms for relevance reasoning. We present a proof-theoretic framework for analyzingdefinitions of irrelevance. The framework makes the necessary distinctions between differentnotions of irrelevance that are important when using them for speeding up inferences. Wedescribe the query-tree algorithm which is a sound; complete and efficient algorithm for …,Artificial Intelligence,1997,42
Malleable Schemas: A Preliminary Report.,Xin Dong; Alon Y Halevy,ABSTRACT Large-scale information integration; and in particular; search on the World WideWeb; is pushing the limits on the combination of structured data and unstructured data. By itsvery nature; as we combine a large number of information sources; our ability to model thedomain in a completely structured way diminishes. We argue that in order to buildapplications that combine structured and unstructured data; there is a need for a newmodeling tool. We consider the question of modeling an application domain whose datamay be partially structured and partially unstructured. In particular; we are concerned withapplications where the border between the structured and unstructured parts of the data isnot well defined; not well known in advance; or may evolve over time. We propose theconcept of malleable schemas as a modeling tool that enables incorporating both …,WebDB,2005,38
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2013,37
Crowd-powered find algorithms,Anish Das Sarma; Aditya Parameswaran; Hector Garcia-Molina; Alon Halevy,We consider the problem of using humans to find a bounded number of items satisfyingcertain properties; from a data set. For instance; we may want humans to identify a selectnumber of travel photos from a data set of photos to display on a travel website; or acandidate set of resumes that meet certain requirements from a large pool of applicants.Since data sets can be enormous; and since monetary cost and latency of data processingwith humans can be large; optimizing the use of humans for finding items is an importantchallenge. We formally define the problem using the metrics of cost and time; and designoptimal algorithms that span the skyline of cost and time; ie; we provide designers the abilityto control the cost vs. time trade-off. We study the deterministic as well as error-prone humananswer settings; along with multiplicative and additive approximations. Lastly; we study …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,34
Harvesting relational tables from lists on the web,Hazem Elmeleegy; Jayant Madhavan; Alon Halevy,Abstract A large number of web pages contain data structured in the form of" lists". Manysuch lists can be further split into multi-column tables; which can then be used in moresemantically meaningful tasks. However; harvesting relational tables from such lists can bea challenging task. The lists are manually generated and hence need not have well-definedtemplates--they have inconsistent delimiters (if any) and often have missing information. Wepropose a novel technique for extracting tables from lists. The technique is domainindependent and operates in a fully unsupervised manner. We first use multiple sources ofinformation to split individual lines into multiple fields and then; compare the splits acrossmultiple lines to identify and fix incorrect splits and bad alignments. In particular; we exploit acorpus of HTML tables; also extracted from the web; to identify likely fields and good …,The VLDB Journal—The International Journal on Very Large Data Bases,2011,34
Data modeling in dataspace support platforms,Anish Das Sarma; Xin Luna Dong; Alon Y Halevy,Abstract Data integration has been an important area of research for several years.However; such systems suffer from one of the main drawbacks of database systems: theneed to invest significant modeling effort upfront. Dataspace Support Platforms (DSSP)envision a system that offers useful services on its data without any setup effort; and improvewith time in a pay-as-you-go fashion. We argue that in order to support DSSPs; the systemneeds to model uncertainty at its core. We describe the concepts of probabilistic mediatedschemas and probabilistic mappings as enabling concepts for DSSPs.,*,2009,34
Semex: Toward on-the-fly personal information integration,Xin Dong; Alon Halevy; Ema Nemes; Stephan Sigurdsson; Pedro Domingos,Abstract On-the-fly information integration attempts to change the basic cost-benefit equationassociation with building information integration applications. This paper argues that on-the-fly can be supported by extending one's personal information space. As a first step in thisdirection; we describe the Semex system that provides a logical and integrated view of one'spersonal information.,Proc. of the VLDB IIWeb Workshop,2004,34
Scalable rendering of large spatial databases,*,Aspects of the invention provide a service for data management and integration across awide range of applications. Clustered computers may be arranged in a cloud-typeconfiguration for storing and handling large amounts of user data under the control of a front-end management server. Communities of distributed users may collaborate on the dataacross multiple enterprises. Very large tabular data files are uploaded to the storagefacilities. The data files are maintained as tables; and a composite table of relatedinformation is created and maintained in response to user queries. Different ways ofvisualizing the data are provided. Depending on the amount of information that can bedisplayed; features in a spatial index may the thinned for presentation. Spatial andstructured queries are processing and results are intersected to obtain information for …,*,2015,33
Data integration with dependent sources,Anish Das Sarma; Xin Luna Dong; Alon Halevy,Abstract Data integration systems offer users a uniform interface to a set of data sources.Previous work has typically assumed that the data sources are independent of each other;however; in scenarios involving large numbers of sources; such as the Web or largeenterprises; there is an eco-system of dependent sources; where some sources copy partsof their data from others. This paper considers the new optimization problems that arisewhile answering queries over large number of dependent sources. These are the (1) cost-minimization problem: what is the minimum cost we must incur to get all answer tuples;(2)maximum-coverage problem: given a bound on the cost; how can we get the maximumpossible coverage; and (3) the source-ordering problem: for a set of data sources; what isthe best order to query them so as to retrieve answer tuples as fast as possible.,Proceedings of the 14th International Conference on Extending Database Technology,2011,33
The Nimble integration engine,Denise Draper; Alon Y Halevy; Daniel S Weld,Abstract The consensus that XML has become the de facto standard for data interchangewill spur demand for technology that allows users to integrate data from a variety ofapplications; repositories; and legacy systems which are located across the corporateintranet or at partner companies on the Internet. In the past two years; Nimble Technologyhas developed a product for this market. Spawned from over a person-decade of dataintegration research; the product has been deployed at several Fortune-500 beta-customersites. This abstract reports on the key challenges we faced in the design of our product andhighlights some issues we think require more attention from the research community.,ACM SIGMOD Record,2001,33
Reasoning with aggregation constraints,Alon Y Levy; Inderpal Singh Mumick,Abstract Aggregation queries are becoming increasingly common as databases continue togrow and provide parallel execution engines to enable complex queries over larger andlarger amounts of data. Consequently; optimization of aggregation queries is becoming veryimportant. In this paper we present a framework for reasoning with constraints arising fromthe use of aggregations. The framework introduces a constraint language; three types ofinference rules to derive constraints that must hold given a set of aggregations andconstraints in the query; and a sound and tractable inference procedure. The constraintlanguage and inference procedure can be used by any system that deals with aggregations—be it constraint programming; databases; or global information systems. However; the primeapplication of aggregation reasoning is in database query optimizers to optimize SQL (or …,International Conference on Extending Database Technology,1996,33
A query language for {XML},D Florescu; A Deutsch; A Levy; D Suciu; M Fernandez,*,Proceedings of Eighth International World Wide Web Conference,1999,30
Information gathering plans with sensing actions,Naveen Ashish; Craig A Knoblock; Alon Levy,Abstract Information gathering agents can automate the task of retrieving and integratingdata from a large number of diverse information sources. The key issue in their performanceis efficient query planning that minimizes the number of information sources used to answera query. Previous work on query planning has considered generating information gatheringplans solely based on compile-time analysis of the query and the models of the informationsources. We argue that at compile-time it may not be possible to generate an efficient planfor retrieving the requested information because of the large number of possibly relevantsources. We describe an approach that naturally extends query planning to use run-timeinformation to optimize queries that involve many sources. First; we describe an algorithm forgenerating a discrimination matrix; which is a data structure that identifies the information …,European Conference on Planning,1997,30
Applying WebTables in Practice.,Sreeram Balakrishnan; Alon Y Halevy; Boulos Harb; Hongrae Lee; Jayant Madhavan; Afshin Rostamizadeh; Warren Shen; Kenneth Wilder; Fei Wu; Cong Yu,*,CIDR,2015,29
Efficient spatial sampling of large geographical tables,Anish Das Sarma; Hongrae Lee; Hector Gonzalez; Jayant Madhavan; Alon Halevy,Abstract Large-scale map visualization systems play an increasingly important role inpresenting geographic datasets to end users. Since these datasets can be extremely large;a map rendering system often needs to select a small fraction of the data to visualize them ina limited space. This paper addresses the fundamental challenge of thinning: determiningappropriate samples of data to be shown on specific geographical regions and zoom levels.Other than the sheer scale of the data; the thinning problem is challenging because of anumber of other reasons:(1) data can consist of complex geographical shapes;(2) renderingof data needs to satisfy certain constraints; such as data being preserved across zoom levelsand adjacent regions; and (3) after satisfying the constraints; an optimal solution needs to bechosen based on objectives such as maximality; fairness; and importance of data. This …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,29
Exploring schema repositories with schemr,Kuang Chen; Akshay Kannan; Jayant Madhavan; Alon Halevy,Abstract Schemr is a search engine for users to search for and visualize schemas in ametadata repository. Users may search by keywords and by example; using schemafragments as query terms. Schemr uses a novel search algorithm; based on a combinationof text search and schema matching techniques; coupled with a structurally-aware scoringmetric. Schemr presents search results in a GUI that allows users to explore which elementsmatch and how well they do. The GUI supports interactions; including panning; zooming;layout and drilling-in. This paper introduces Schemr as a new component of the informationintegration toolbox and discusses its benefits in several applications.,ACM SIGMOD Record,2011,29
Corpus-based knowledge representation,Alon Y Halevy; Jayant Madhavan,Abstract A corpus-based knowledge representation system consists of a large collection ofdisparate knowledge fragments or schemas; and a rich set of statistics computed over thecorpus. We argue that by collecting such a corpus and computing the appropriate statistics;corpus-based representation offers an alternative to traditional knowledge representation fora broad class of applications. The key advantage of corpusbased representation is that weavoid the laborious process of building a (often brittle) knowledge base. We describe thebasic building blocks of a corpus-based representation system and a set of applications forwhich such a paradigm is appropriate; including one application where the approach isalready showing promising results.,IJCAI,2003,29
Semantic email: theory and applications,Luke McDowell; Oren Etzioni; Alon Halevy,Abstract This paper investigates how the vision of the Semantic Web can be carried over tothe realm of email. We introduce a general notion of semantic email; in which an emailmessage consists of a structured query or update coupled with corresponding explanatorytext. Semantic email opens the door to a wide range of automated; email-mediatedapplications with formally guaranteed properties. In particular; this paper introduces a broadclass of semantic email processes. For example; consider the process of sending an emailto a program committee; asking who will attend the PC dinner; automatically collecting theresponses; and tallying them up. We define both logical and decision-theoretic modelswhere an email process is modeled as a set of updates to a data set on which we specifygoals via certain constraints or utilities. We then describe a set of inference problems that …,Web Semantics: Science; Services and Agents on the World Wide Web,2004,28
Mining structures for semantics,Xin Dong; Jayant Madhavan; Alon Halevy,Abstract Online data is available in two avors: unstructured data that resides as free text inHTML pages; and structured data that resides in databases and knowledge bases.Unstructured data is easily accessed as human-readable text on a browser; while structureddata is hidden behind web query interfaces (web forms); web services; and customdatabase APIs. Access to this data; popularly referred to as the hidden web; entailssubmitting correctly completed web forms or writing code to access web services usingprotocols such as SOAP.,ACM SIGKDD Explorations Newsletter,2004,28
Goods: Organizing google's datasets,Alon Halevy; Flip Korn; Natalya F Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract Enterprises increasingly rely on structured datasets to run their businesses. Thesedatasets take a variety of forms; such as structured files; databases; spreadsheets; or evenservices that provide access to the data. The datasets often reside in different storagesystems; may vary in their formats; may change every day. In this paper; we present GOODS;a project to rethink how we organize structured datasets at scale; in a setting where teamsuse diverse and often idiosyncratic ways to produce the datasets and where there is nocentralized system for storing and querying them. GOODS extracts metadata ranging fromsalient information about each dataset (owners; timestamps; schema) to relationshipsamong datasets; such as similarity and provenance. It then exposes this metadata throughservices that allow engineers to find datasets within the company; to monitor datasets; to …,Proceedings of the 2016 International Conference on Management of Data,2016,27
Evolving the Semantic Web with Mangrove.,Luke K McDowell; Oren Etzioni; Steven D Gribble; Alon Y Halevy; Henry M Levy; William Pentney; Deepak Verma; Stani Vlasseva,ABSTRACT Despite numerous proposals for its creation; the semantic web has yet toachieve widespread adoption. Recently; some researchers have argued that participation inthe semantic web is too difficult for “ordinary” people; limiting its growth and popularity. Inresponse; this paper introduces MANGROVE; a system whose goal is to evolve a portion ofthe semantic web from the enormous volume of facts already available in HTML documents.MAN-GROVE seeks to emulate three key conditions that contributed to the explosive growthof the web: ease of authoring; instant gratification for authors; and robustness of services tomalformed and malicious information. In the HTML world; a newly authored page isimmediately accessible through a browser; we mimic this feature in MANGROVE by makingsemantic content instantly available to services that consume the content and yield …,WWW (Posters),2003,27
Creating abstractions using relevance reasoning,Alon Y Levy,*,AAAI,1994,27
The view selection problem for XML content based routing,Ashish Kumar Gupta; Dan Suciu; Alon Y Halevy,Abstract We consider the view selection problem for XML content based routing: given anetwork; in which a stream of XML documents is routed and the routing decisions are takenbased on results of evaluating XPath predicates on these documents; select a set of viewsthat maximize the throughput of the network. While in view selection for relational queriesthe speedup comes from eliminating joins; here the speedup is obtained from gaining directaccess to data values in an XML packet; without parsing that packet. The views in ourcontext can be seen as a binary representation of the XML document; tailored for thenetwork's workload. In this paper we define formally the view selection problem in thecontext of XML content based routing; and provide a practical solution for it. First; weformalize the problem; while the exact formulation is too complex to admit practical …,Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2003,26
View Selection for Stream Processing.,Ashish Kumar Gupta; Alon Y Halevy; Dan Suciu,Abstract Consider XML content-based document routing: a stream of XML documents arerouted through a network; and routing decisions are taken based on the result of evaluatingXPath predicates on these documents. Parsing XML documents and interpreting XPathexpressions is the main bottleneck in such systems. We propose a novel solution tospeedup the evaluation of XPath predicates based on precomputing views for the XMLdocuments. There are both similarities and differences from the” view selection problem” inrelational databases. We describe an architecture for using these views; discuss severaldesign choices and make a brief theoretical analysis for one special case. Finally; we reportsome initial experiments; showing the potential for query speedup by using stream views.,WebDB,2002,26
A first tutorial on dataspaces,Michael Franklin; Alon Halevy; David Maier,Abstract Dataspace systems offer services on data without requiring upfront semanticintegration. In sharp contrast with existing information-integration systems; dataspacessystems offer best-effort answers even before semantic mappings are provided to thesystem. Dataspaces offer a pay-as-you-go approach to data management. Users (oradministrators) of the system decide where and when it is worthwhile to invest more effort inidentifying semantic relationships. As such; dataspaces offer services on the data in place;without losing the context surrounding the data.,Proceedings of the VLDB Endowment,2008,25
Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data,Zachary G Ives; Yannis Papakonstantinou; Alon Halevy,*,*,2003,25
More on data management for xml,Alon Levy,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,http://www. cs. washington. edu/homes/alon/widom-response. html,1999,25
Overview of Strudel - A Web-Site Management System,Mary F.  Fernandez; Daniela Florescu; Jaewoo Kang; Alon Y.  Levy; Dan Suciu,ABSTRACT. The Strudel system applies concepts from database management systems tothe process of building Web sites. Strudel's key idea is separating the management of thesite's data; the creation and management of the site's structure; and the visual presentationof the site's pages. First; the site builder creates a uniform model of all data available at thesite. Second; the builder uses this model to declaratively define the Web site's structure byapplying a “sitedefinition query” to the underlying data. Third; the builder specifies the visualpresentation of pages in Strudel's HTML-template language. The data model underlyingStrudel is a semistructured model of labeled directed graphs.,Networking and Information Systems,1998,24
Databases and Web 2.0 panel at VLDB 2007,Sihem Amer-Yahia; Volker Markl; Alon Halevy; AnHai Doan; Gustavo Alonso; Donald Kossmann; Gerhard Weikum,Abstract Web 2.0 refers to a set of technologies that enables indviduals to create and sharecontent on the Web. The types of content that are shared on Web 2.0 are quite varied andinclude photos and videos (eg; Flickr; YouTube); encyclopedic knowledge (eg; Wikipedia);the blogosphere; social book-marking and even structured data (eg; Swivel; Many-eyes).One of the important distinguishing features of Web 2.0 is the creation of communities ofusers. Online communities such as LinkedIn; Friendster; Facebook; MySpace and Orkutattract millions of users who build networks of their contacts and utilize them for social andprofessional purposes. In a nutshell; Web 2.0 offers an architecture of participation anddemocracy that encourages users to add value to the application as they use it.,ACM SIGMOD Record,2008,22
Challenges and Opportunities with Big Data,Elisa Bertino; Philip Bernstein; Divyakant Agrawal; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jadadish; Alexandros Labrinidis; Sam Madden; Yannis Papokonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of" Big Data". While the promise of Bid Data isreal-for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009-there is currently a wide gap between its potential and its realization.,*,2011,21
Exploiting irrelevance reasoning to guide problem solving,Alon Y Levy; Yehoshua Sagiv,Abstract Identifying that parts of a knowledge base (KB) are irrelevant to a speci c query is apowerful method of controlling search during problem solving. However; nding methods ofsuch irrelevance reasoning and analyzing their utility are open problems. We present aframework based on a proof-theoretic analysis of irrelevance that enables us to addressthese problems. Within the framework; we focus on a class of strong-irrelevance claims andshow that they have several desirable properties. For example; in the context of Horn-ruletheories; we show that strong-irrelevance claims can be derived e ciently either byexamining the KB or as logical consequences of other strongirrelevance claims. Animportant aspect is that our algorithms reason about irrelevance using only a small part ofthe KB. Consequently; the reasoning is e cient and the derived irrelevance claims are …,IJCAI,1993,20
Clustering query refinements by inferred user intent,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for clustering query refinements. One method includes building arepresentation of a graph for a first query; wherein the graph has a node for the first query; anode for each of a plurality of refinements for the first query; and a node for each documentin the document sets of the refinements; and wherein the graph has edges from the firstquery node to each of the refinement nodes; edges from the first query to each document inthe respective document set of the first query; edges from each refinement to each documentin the respective document set of the refinement; and edges from each refinement to each co-occurring query of the refinement. The method further includes clustering the refinementsinto refinement clusters by partitioning the refinement nodes in the graph into proper …,*,2013,19
Semantic Email: Adding Lightweight Data Manipulation Capabilities to the Email Habitat.,Oren Etzioni; Alon Y Halevy; Henry M Levy; Luke K McDowell,ABSTRACT The Semantic Web envisions a portion of the World Wide Web in which theunderlying data is machine understandable and applications can exploit this data forimproved querying; aggregation; and interaction. This paper investigates whether the samevision can be carried over to the realm of email; the adjacent information space in which wespend significant amounts of time. We introduce a general notion of semantic email; in whichemail messages consist of a database query or update coupled with correspondingexplanatory text. Semantic email opens the door to a wide range of automated; email-mediated applications. In particular; this paper introduces a class of semantic emailprocesses. For example; consider the process of sending an email to a program committee;asking who will attend the PC dinner; automatically collecting the responses; and tallying …,WebDB,2003,19
Argumentation in negotiation: A formal model and implementation,Sarit Kraus; Katia Sycara; A Evanchik,*,Artificial Intelligence,1998,19
Facilitating searches through content which is accessible through web-based forms,*,One embodiment of the present invention provides a system that facilitates crawling throughweb-based forms to gather information to facilitate subsequent searches through contentwhich is accessible though the web-based forms. During operation; the system first obtainsweb-based forms to be searched. Note that the system can obtain these web-based formsfrom a number of sources. For example; the system can crawl through web sites to identifyweb-based forms; the system can receive manually provided web-based forms; or thesystem can find web-based forms through methods other than crawling. Next; the systemcreates database entries for the identified forms. This involves obtaining and storingmetadata describing the identified forms into database entries and then storing thesedatabase entries in a form database to facilitate searches through content which is …,*,2010,18
Structures; semantics and statistics,Alon Y Halevy,Abstract At a fundamental level; the key challenge in data integration is to reconcile thesemantics of disparate data sets; each expressed with a different database structure. I arguethat computing statistics over a large number of structures offers a powerful methodology forproducing semantic mappings; the expressions that specify such reconciliation. In essence;the statistics offer hints about the semantics of the symbols in the structures; therebyenabling the detection of semantically similar concepts. The same methodology can beapplied to several other data management tasks that involve search in a space of complexstructures and in enabling the next-generation on-the-fly data integration systems.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,18
Web-site management: The Strudel approach,Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,Documents; Authors; Tables. Log in; Sign up; MetaCart; Donate. CiteSeerX logo. Documents:Advanced Search Include Citations. Authors: Advanced Search Include Citations | Disambiguate.Tables: Web-Site Management: The Strudel Approach (1998). Cached. Download as a PDF;Download as a PS. Download Links. [www.research.microsoft.com]; [www.cs.washington.edu]Other Repositories/Bibliography. DBLP. Save to List; Add to Collection; Correct Errors; MonitorChanges. by Mary Fernández ; Daniela Florescu ; Alon Levy ; Dan Suciu. Citations: 8 - 0 self.Summary; Citations; Active Bibliography; Co-citation; Clustered Documents; Version History.BibTeX. @MISC{Fernández98web-sitemanagement:; author = {Mary Fernández and DanielaFlorescu and Alon Levy and Dan Suciu}; title = {Web-Site Management: The Strudel Approach};year = {1998} }. Share. Facebook; Twitter; Reddit; Bibsonomy. OpenURL. Abstract …,*,1998,18
Table search using recovered semantic information,*,Methods; systems; and apparatus; including computer programs encoded on a computerstorage medium; for searching tables using recovered semantic information. In general; oneaspect of the subject matter described in this specification can be embodied in methods thatinclude the actions of receiving a collection of tables; each table including a plurality of rows;each row including a plurality of cells; recovering semantic information associated with eachtable of the collection of tables; the recovering including determining a class associated witheach respective table according to a class-instance hierarchy including identifying a subjectcolumn of each table of the collection of tables; and labeling each table in the collection oftables with the respective class.,*,2012,16
Dataspaces: A new abstraction for information management,Alon Y Halevy; Michael J Franklin; David Maier,Abstract Most data management scenarios today rarely have a situation in which all the datathat needs to be managed can fit nicely into a conventional relational DBMS; or into anyother single data model or system. Instead; we see a set of loosely connected data sources;typically with the following recurring challenges:–Users want be able to search the entirecollection without having knowledge of individual sources; their schemas or interfaces. Insome cases; they merely want to know where the information exists as a starting point tofurther exploration.–An organization may want to enforce certain rules; integrity constraints;or conventions (eg; on naming entities) across the entire collection; or track flow and lineagebetween systems. Furthermore; the organization needs to create a coherent external view ofthe data.–The administrators may want to impose a single “support system” in terms of …,International Conference on Database Systems for Advanced Applications,2006,16
Directions-based ranking of places returned by local search queries,*,A system and a method for ranking search results of local search queries. A local searchquery and a current location of a user are received. Next; two or more places that satisfy thelocal search query are identified; and for each respective place a corresponding distancefrom the current location of the user to the respective place is also identified. The two ormore places are then ranked in accordance with scores that are based; at least in part; onpopularity of the two or more places and the corresponding distances from the currentlocation of the user; to produce a set of ranked places. The ranked set of places is thenprovided to the user.,*,2013,15
Information integration,Alon Halevy,An example of index oriented towards human navigation is the web directory. In such anindex; links to sites are organized into hierarchical categories; according to the sites'contents. In web directories; normally the tasks of collecting and categorizing pages arecarried out under supervision of human editors. An example of index not oriented to humansis a hidden list of metadata. Metadata are data about data. As a mean of assisting a searchengine to locate content or an information entity; they can be used to describe that content orentity. While not visible to humans; this information can provide contextual clues to automaticalgorithms used by search engines.,*,2009,13
Self-organizing data sharing communities with SAGRES,Zachary Ives; Alon Levy; Jayant Madhavan; Rachel Pottinger; Stefan Saroiu; Igor Tatarinov; Shiori Betzler; Qiong Chen; Ewa Jaslikowska; Jing Su; Wai Tak Theodora Yeung,An increasing number of devices (eg; household appliances; PDAs; cell phones) havemicroprocessors and will soon be able to exhibit sophisticated behaviors and interactionswith other devices: a home heating system will monitor its residents' alarm clocks andschedules to set the temperature optimally; a car's GPS system will use local traffic reports tooptimize its driv er'sroute based on road conditions. The Sagres project at the University ofWashington addresses the key issues of data sharing and management in the realm ofinvisible computing. In the con text of invisible computing; data exchange and computationoccur in the background in response to cues from users. Devices are added and removedfrom the net w ork on a regular basis; and they must be able to interoperate with little humanintervention. The collection of devices that exist around a particular individual or in a …,ACM SIGMOD Record,2000,13
Identifying aspects for web-search queries,Fei Wu; Jayant Madhavan; Alon Halevy,Abstract Many web-search queries serve as the beginning of an exploration of an unknownspace of information; rather than looking for a specific web page. To answer such querieseffectively; the search engine should attempt to organize the space of relevant information ina way that facilitates exploration. We describe the Aspector system that computes aspects fora given query. Each aspect is a set of search queries that together represent a distinctinformation need relevant to the original search query. To serve as an effective means toexplore the space; Aspector computes aspects that are orthogonal to each other and to havehigh combined coverage. Aspector combines two sources of information to computeaspects. We discover candidate aspects by analyzing query logs; and cluster them toeliminate redundancies. We then use a mass-collaboration knowledge base (eg …,Journal of Artificial Intelligence Research,2011,12
Google fusion tables,Alon Halevy; Rebecca Shapley,Page 1. Google Fusion Tables Alon Halevy May 5; 2011 google.com/fusiontables Page 2. SEARCHAlon Halevy Page 3. Page 4. Discover Manage; Analyze; Combine Extract Publish Fusion Tables:collaborating on data in the cloud; easy data publishing Web-form crawling; Finding all HTML tablesLists -> tables Extracting context Structured data on the Web Page 5. The Deep Web store locationsused cars radio stations patents recipes • Deep = not accessible through general purpose searchengines – Major gap in the coverage of search engines. Page 6. HTML Tables on the Web Page7. Discover Manage; Analyze; Combine Extract Publish Fusion Tables: collaborating on data inthe cloud; easy data publishing Web-form crawling; Finding all HTML tables Lists -> tables Extractingcontext Structured data on the Web Page 8. Fusion Tables Goals • Ease of use: – empower userswith no IT capabili es/support • En cing …,Research Blog; June,2009,12
Learning mappings between data schemas,Anhai Doan; Pedro Domingos; Alon Y Levy,*,Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data,2000,12
Using Description Logics to Model and Reason About Views.,Alon Y Levy; Marie-Christine Rousset,Several advanced applications of database systems require the modeling; maintenance;and usage of large collections of views. Prime examples include mediator systems thatprovide access to multiple information sources; data mining and archeology; mobiledatabases; data warehouses; and decision support systems. Furthermore; some databasevendors are considering the maintenance of materialized views also as a means for queryoptimization. As a result; problems concerning materialized views have recently received alot of attention in the database community. A view in a relational database is essentially ananswer to a query. If the answers to the query are physically maintained; the view is said tobe materialized. Naturally; when there are many views (either materialized or not) there arecomplex relationships between the contents of various views. For example; one view may …,KRDB,1996,12
Query processing in the information manifold,A Levy; Anand Rajaraman; J Ordille,Abstract The World Wide Web (WWW) provides access to a multitude of information sources.A growing number of these sources are structured; and provide a query interface; and evenmore sources behave as structured sources if we use an appropriate interface program (eg;for parsing structured text les). Unfortunately; even though these sources; when put together;have the potential of answering complex user queries; the current interaction with them ismuch like browsing. A user must consider the list of sources available; decide which ones toaccess; and then interact with each one individually. Furthermore; combining informationfrom di erent sources can only be done manually. This paper describes the InformationManifold; a system that provides a uniform query interface to large collections of structuredinformation sources. A user poses queries to the Information Manifold using a uniform …,Proc. VLDB,1996,12
Information Gathering From Heterogeneous Distributed Environments,Craig Knoblock; Alon Levy,*,AAAI Symposium,1995,12
Relevance reasoning to guide compositional modeling,Alon Y Levy; Yumi Iwasaki; Hiroshi Motoda,Abstract The ability to choose an appropriate manner in which to model a given device iscrucial in making a compositional modeling [3] approach successful. In compositionalmodeling; a system is provided with a library of composible pieces of knowledge about thephysical world; called model fragments; each representing a conceptually distinctphenomenon such as a physical process or one aspect of a component behavior. Given aspecific query about a device; the system chooses among those model fragments tocompose a model of the device that is most adequate to answer the query. Selection ofappropriate model fragments can be viewed as a special case ofa more general problem ofreasoning about relevance of knowledge to a given goal. In this paper we pursue this viewby applying a general framework for reasoning about relevance to the problem of model …,*,1992,12
Channeling the deluge: research challenges for big data and information systems,Paul Bennett; Lee Giles; Alon Halevy; Jiawei Han; Marti Hearst; Jure Leskovec,Abstract With massive amounts of data being generated and stored ubiquitously in everydiscipline and every aspect of our daily life; how to handle such big data poses manychallenging issues to researchers in data and information systems. The participants of CIKM2013 are active researchers on large scale data; information and knowledge management;from multiple disciplines; including database systems; data mining; information retrieval;human-computer interaction; and knowledge or information management. As a group ofexperienced researchers in academia and industry; we will present at this panel our visionson what should be the challenging research issues in this promising research frontier andhope to attract heated discussions and debates from the audience. We expect panelists withdiverse backgrounds raise different challenging research problems and exchange their …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,11
Big Data Storytelling Through Interactive Maps.,Jayant Madhavan; Sreeram Balakrishnan; Kathryn Brisbin; Hector Gonzalez; Nitin Gupta; Alon Y Halevy; Karen Jacqmin-Adams; Heidi Lam; Anno Langen; Hongrae Lee; Rod McChesney; Rebecca Shapley; Warren Shen,Abstract Google Fusion Tables (GFT) brings big data collaboration and visualization to data-experts who possess neither large data-processing resources nor expertise. In this paper wehighlight our support for map visualizations over large complex geospatial datasets.Interactive maps created using GFT have already been used by journalists in numerous high-profile stories.,IEEE Data Eng. Bull.,2012,11
Uncertainty in data integration and dataspace support platforms,Anish Das Sarma; Xin Luna Dong; Alon Y Halevy,Abstract Data integration has been an important area of research for several years.However; such systems suffer from one of the main drawbacks of database systems: theneed to invest significant modeling effort upfront. Dataspace support platforms (DSSP)envision a system that offers useful services on its data without any setup effort and thatimproves with time in a pay-as-you-go fashion. We argue that to support DSSPs; the systemneeds to model uncertainty at its core. We describe the concepts of probabilistic mediatedschemas and probabilistic mappings as enabling concepts for DSSPs.,*,2011,11
A semantics for model management operators,Sergey Melnik; Philip A Bernstein; Alon Halevy; Erhard Rahm,Abstract Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators; such as Compose; Extract;and Merge; that are applied to models; such as database schemas or interfacespecifications; and to mappings between models. To be used in practice; these operatorsneed to be implemented for particular schema definition languages and mappinglanguages. To guide that implementation; we need a language-independent semantics thattells what the operators should do. In this paper we develop a state-based semantics of theoperators. That is; we express the effect of applying the operators to models in terms of whatthe operators do to instances of these models. We show that our semantics capturespreviouslyproposed desiderata for the operators. We study formal properties of the …,Microsoft Technical Report,2004,11
Information integration research: Summary of nsf idm workshop breakout session,Alon Halevy; Chen Li; Contributions From Philip Bernstein; Kevin Chang; Jayavel Shanmugasundaram; Mike Uschold,Abstract Information integration systems provide users a uniform interface to a multitude ofheterogeneous; independently developed data sources. They free the user from having tolocate the data sources; interact with each one in isolation and manually combine data frommultiple sources. The applications of information integration systems range frommanagement of data in large enterprises; data sharing amongst government agencies andlarge scientific projects (eg; biological research and astronomy); and integration of datasources on the World-Wide Web. In the past few years we have seen significant progress onmany aspects of data integration; including languages for mediation between data sources;query processing techniques for data integration; and the construction of wrappers to datasources. In addition; recent commercial activities have produced tools that efficiently …,*,2004,11
An evolutionary approach to the semantic web,Oren Etzioni; Steve Gribble; Alon Halevy; Henry Levy; Luke McDowell,Proposals for creating a semantic web have been around at least since 1995 [Dobson andBurrill; 1995]. A wide range of semantic markup languages have been proposed includingRDF; N3; SHOE; DAML; and OIL. Yet; in contrast with the explosive growth of HTML; thesemantic web has been slow to materialize. Below; we attempt to explain why by articulatingseveral hypotheses. We then sketch our own approach which seeks to facilitate gradualevolution from HTML to a semantic web. People have not been tagging their web pagesbecause they have had no reason to do so. 1 After all; HTML became popular only afterMOSAIC came along. Tagging will be driven by applications that consume the tags andresult in immediate; tangible satisfaction for the author; Instant gratification is a requirementfor rapid adoption. Another factor to consider is ease of authoring. HTML spread like wild …,Poster presentation at the First International Semantic Web Conference,2002,11
What Can Peerto-Peer Do for Databases; and Vice Versa,Steven Gribble; Alon Halevy; Zachary Ives; Maya Rodrig; Dan Suciu,*,Proc. WebDB,2001,11
Run-time management of data intensive Web-sites,Daniela Florescu; Alon Levy; Dan Suciu; Khaled Yagoub,An increasing number of web sites have their data extracted from relational databases.Several commercial products and research prototypes have been moving in the direction ofdeclarative specification of the structure and content of sites. Specifically; the entire site isspecified using a collection of queries describing the site's nodes (corresponding to webpages and the data contained in them) and edges (corresponding to the hyperlinks). Giventhis paradigm; an important issue is when to compute the site's pages. In one extremeapproach; the site is precomputed in advance; while in the other extreme; the queriesnecessary to construct a given page are computed on demand. Both approaches have theirobvious drawbacks: large space and maintenance overhead in the first approach; and poorrun-time performance and unnecessary repeated computations in the second. In this …,*,1999,11
An experiment in integrating internet information sources,Alon Y Levy; Joann J Ordille,The number of online information sources is growing rapidly. Though much of thisinformation is unstructured (eg; text; images) the number of structured information sources(eg; databases) is also increasing. The advantage of structured information sources is thatwe can actually query their contents and use them to answer queries; perhaps incombination with other structured sources; rather than just viewing the whole informationsource. Given the variability in the information sources available; it is impractical for a user tointeract with each source using its speci c terminology. As a consequence; several systems(such as Nomenclator Ordille and Miller; 1993b; Ordille; 1994]; The Information Manifold Kirket al.; 1995]; TSIMMIS Chawathe et al.; 1994]; SIMS Arens et al.; 1994]) have beendeveloped based on the notion of a mediator Wiederhold; 1992]. The key idea behind a …,AAAI Fall Symposium on AI Applications on Knowledge Navigation and Retrieval; Cambridge; MA,1995,11
Discovering structure in the universe of attribute names,Alon Halevy; Natalya Noy; Sunita Sarawagi; Steven Euijong Whang; Xiao Yu,Abstract Recently; search engines have invested significant effort to answering entity--attribute queries from structured data; but have focused mostly on queries for frequentattributes. In parallel; several research efforts have demonstrated that there is a long tail ofattributes; often thousands per class of entities; that are of interest to users. Researchers arebeginning to leverage these new collections of attributes to expand the ontologies thatpower search engines and to recognize entity--attribute queries. Because of the sheernumber of potential attributes; such tasks require us to impose some structure on this longand heavy tail of attributes. This paper introduces the problem of organizing the attributes byexpressing the compositional structure of their names as a rule-based grammar. These rulesoffer a compact and rich semantic interpretation of multi-word attributes; while …,Proceedings of the 25th International Conference on World Wide Web,2016,10
Principles of data integration,Alon Halevy; A Doan; Z Ives,*,Morgan Kaufmann,2012,10
Searching through content which is accessible through web-based forms,*,One embodiment of the present invention provides a system that facilitates searchingthrough content which is accessible though web-based forms. During operation; the systemreceives a query containing keywords. Next; the system analyzes the query to create astructured query. The system then performs a lookup based on the structured query in adatabase containing entries describing the web-based forms. Next; the system ranks formsreturned by the lookup; and uses the rankings and associated database entries to facilitate asearch through content which is accessible through the forms.,*,2011,10
Technical perspective Schema mappings: rules for mixing data,Alon Halevy,When you search for flight tickets on you favorite Web site; your query is often dispatched totens of databases to produce an answer. When you search for products on Amazon. com;you are seeing results from thousands of vendor databases that were developed beforeAmazon existed. Did you ever wonder how that happens? What is the theory behind it all? Atthe core; these systems are powered by schema mappings that provide the glue to tie allthese databases together. The following paper by ten Cate and Kolaitis will give you aglimpse into the theoretical foundations underlying schema mappings and might eveninspire you to work in the area.The scenarios I've noted here are examples of datamanagement applications that require access to multiple heterogeneous data sets. Dataintegration is the field that develops architectures; systems; formalisms; and algorithms for …,Communications of the ACM,2010,10
Mining subjective properties on the web,Immanuel Trummer; Alon Halevy; Hongrae Lee; Sunita Sarawagi; Rahul Gupta,Abstract Even with the recent developments in Web search of answering queries fromstructured data; search engines are still limited to queries with an objective answer; such asEUROPEAN CAPITALS or WOODY ALLEN MOVIES. However; many queries are subjective;such as SAFE CITIES; or CUTE ANIMALS. The underlying knowledge bases of searchengines do not contain answers to these queries because they do not have a ground truth.We describe the Surveyor system that mines the dominant opinion held by authors of Webcontent about whether a subjective property applies to a given entity. The evidence on whichSURVEYOR relies is statements extracted from Web text that either support the property orclaim its negation. The key challenge that SURVEYOR faces is that simply counting thenumber of positive and negative statements does not suffice; because there are multiple …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,9
Processing queries and merging schemas in support of data integration,Rachel Amanda Pottinger; Philip A Bernstein; Alon Y Halevy,In Chapter 1; we motivated the problem of answering queries using views as a method forreformulating queries asked in a Local-As-View (LAV) data integration system. In thischapter; we assume that a data integration system has been set up using LAV as themapping language between the mediated schema and the data sources; and focus on howto translate user queries into queries over the data sources using a technique calledanswering queries using views (aka rewriting queries using views). We defined answeringqueries using views in Chapter 2; both for finding equivalent rewritings (Definition 2.4) inquery optimization of physical data independence and for data integration where the goal isto find maximally-contained rewritings (Definition 2.8). While the problem is NP-Complete inthe number of subgoals of the query; the number of query subgoals is generally quite …,*,2004,9
Exploiting run-time information for efficient processing of queries,Craig A Knoblock; Alon Levy,Abstract Information agents answer user queries using a large number of diverseinformation sources. The key issue in their perfomance is finding the set of informationsources relevant to a query. Previous work has considered determining relevance soleybased on compile-time analysis of the query. We argue that at compile-time; it is often notpossible to significantly prune the set of sources relevant to a query; and that run-timeinformation is needed. We make the following contributions. First; we identify the differenttypes of information that can be obtained at run-time; and how they can be used to pruneinformation sources. Second; we describe an algorithm which naturally extends queryplanning algorithms to exploit run-time information. Third; we describe the discriminationmatrix; which is a data structure that identifies the information that can be used to help …,Working Notes of the AAAI Spring Symposium on Information Gathering in Heterogeneous; Distributed Environments;(Palo Alto; CA),1995,9
Synthesizing Union Tables from the Web.,Xiao Ling; Alon Y Halevy; Fei Wu; Cong Yu,Abstract Several recent works have focused on harvesting HTML tables from the Web andrecovering their semantics [Cafarella et al.; 2008a; Elmeleegy et al.; 2009; Limaye et al.;2010; Venetis et al.; 2011]. As a result; hundreds of millions of high quality structured datatables can now be explored by the users. In this paper; we argue that those efforts onlyscratch the surface of the true value of structured data on the Web; and study the challengingproblem of synthesizing tables from the Web; ie; producing never-before-seen tables fromraw tables on the Web. Table synthesis offers an important semantic advantage: when a setof related tables are combined into a single union table; powerful mechanisms; such astemporal or geographical comparison and visualization; can be employed to understandand mine the underlying data holistically. We focus on one fundamental task of table …,IJCAI,2013,8
Principles of data integration,Z Ives; A Halevy; A Doan,*,*,2012,8
Finding with the crowd,Anish Das Sarma; Aditya Parameswaran; Hector Garcia-Molina; Alon Halevy,We consider the problem of using humans to find a bounded number of items satisfyingcertain properties; from a data set. For instance; we may want humans to identify a selectnumber of travel photos from a data set of photos to display on a travel website; or acandidate set of resumes that meet certain requirements from a large pool of applicants.Since data sets can be enormous; and since monetary cost and latency of data processingwith humans can be large; optimizing the use of humans for finding items is an importantchallenge. We formally define the problem using the metrics of cost and time; and designoptimal algorithms that span the skyline of cost and time; ie; we provide designers the abilityto control the cost vs. time trade-off. We study the deterministic as well as error-prone humananswer settings; along with multiplicative and additive approximations. Lastly; we study …,*,2012,8
Uncertainty in data integration,A Das Sarma; L Dong; Alon Halevy,Abstract Data integration has been an important area of research for several years. In thischapter; we argue that supporting modern data integration applications requires systems tohandle uncertainty at every step of integration. We provide a formal framework for dataintegration systems with uncertainty. We define probabilistic schema mappings andprobabilistic mediated schemas; show how they can be constructed automatically for a set ofdata sources; and provide techniques for query answering. The foundations laid out in thischapter enable bootstrapping a pay-as-you-go integration system completely automatically.,Managing and Mining Uncertain Data,2009,8
Managing change in large-scale data sharing systems,Peter Mork; Steven D Gribble; Alon Y Halevy,Abstract The problem of sharing data across multiple sources has received considerableattention in recent years because of its relevance to enterprise data management; scientificdata management; and information integration on the WWW. However; the management ofupdates in such systems has received very little attention. In a data sharing system; the set ofsources and clients is not fixed; and therefore the sources publishing the updates do notnecessarily know exactly who will consume them. Consequently; the system needs tosupport a variety of update propagation strategies. In this paper; our approach is based onidentifying two kinds of objects of interest; which are treated as first-class citizens in thesystem: updategrams; which are descriptions of updates over base relations; and boosters;which complement updategrams to speed up the processing of join views. We derive a …,UW CS\&E Technical Reports UW-CSE-04-04-01. pdf,2004,8
Verification of Knowledge Bases: a Unifying Logical View.,Alon Y Levy; Marie-Christine Rousset,ABSTRACT: Notions of correctness and completeness of a KB are impossible to capturecompletely by a formal de nition. However; when the knowledge base is represented in adeclarative logical formalism; they can be approached by a logical analysis of its contents. Alogical analysis of the knowledge base and constraints that are known to hold on the domainenables us to detect anomalies or discrepancies between the knowledge represented in theKB and the domain. This paper describes a uni ed logical framework for the veri cationproblem of knowledge bases represented by logical rules (ie; Horn rules and someextensions). We consider several instances of the veri cation problem; describe algorithmsfor veri cation; and establish the computational complexity of the veri cation problem. Inparticular; we consider the veri cation wrt consistency constraints that are speci ed …,EUROVAV,1997,8
Consistent thinning of large geographical data for map visualization,Anish Das Sarma; Hongrae Lee; Hector Gonzalez; Jayant Madhavan; Alon Halevy,Abstract Large-scale map visualization systems play an increasingly important role inpresenting geographic datasets to end-users. Since these datasets can be extremely large;a map rendering system often needs to select a small fraction of the data to visualize them ina limited space. This article addresses the fundamental challenge of thinning: determiningappropriate samples of data to be shown on specific geographical regions and zoom levels.Other than the sheer scale of the data; the thinning problem is challenging because of anumber of other reasons:(1) data can consist of complex geographical shapes;(2) renderingof data needs to satisfy certain constraints; such as data being preserved across zoom levelsand adjacent regions; and (3) after satisfying the constraints; an optimal solution needs to bechosen based on objectives such as maximality; fairness; and importance of data. This …,ACM Transactions on Database Systems (TODS),2013,7
Analyzing a form page for indexing,*,Among other disclosure; a computer-implemented method of analyzing a form page forindexing includes identifying a form page that is configured for use in requesting any ofmultiple target pages. The form page includes multiple input controls. The method includesidentifying at least one of the multiple input controls as being informative with regard torequesting the multiple target pages. The method includes updating an indexing recordassociated with the form page to reflect the identification.,*,2013,7
Web data management,Michael J Cafarella; Alon Y Halevy,Abstract Web Data Management (or WDM) refers to a body of work concerned withleveraging the large collections of structured data that can be extracted from the Web. Overthe past few years; several research and commercial efforts have explored these collectionsof data with the goal of improving Web search and developing mechanisms for surfacingdifferent kinds of search answers. This work has leveraged (1) collections of structured datasuch as HTML tables; lists and forms;(2) recent ontologies and knowledge bases created bycrowd-sourcing; such as Wikipedia and its derivatives; DBPedia; YAGO and Freebase; and(3) the collection of text documents from the Web; from which facts could be extracted in adomain-independent fashion. The promise of this line of work is based on the observationthat new kinds of results can be obtained by leveraging a huge collection of …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,7
Semantic data sharing with a peer data management system,Igor Tatarinov; Alon Halevy,Data sharing is a ubiquitous problem. It is crucial in a wide range of applications such asenterprise data management; large-scale scientific projects; sharing data betweengovernment agencies; and personal information sharing. The goal of data sharing is to allowtransparent access to shared data independently of where the data is stored. For example;in a large company information about a particular customer can appear in the marketing;sales; service and many other databases. A recent study has found that a typical largeenterprise has; on average; 49 different databases [Met01]. The users often need to find allavailable information about a customer no matter where it is stored (to detect fraud; forexample). In scientific data sharing; researchers in biology; chemistry; astronomy and otherfields produce large amounts of experimental data. Multiple research groups often work …,*,2004,7
XML and Object Technology,Barbara Catania; Elena Ferrari; Alon Levy; Alberto O Meldelzon,Abstract The aim of the First ECOOP Workshop on XML and Object Technology was to bringtogether researchers working in the object-oriented field to discuss how object-orientedtechnology can be exploited in data management solutions for XML [14] and which issuesrequire new developments and further investigation. The workshop consisted of a number ofpresentations of reviewed papers and of discussions on various aspects related to the maintopic of the workshop.,European Conference on Object-Oriented Programming,2000,7
System and method for obtaining complete and correct answers from incomplete and/or incorrect databases,*,A system and method for obtaining complete and correct answers from incomplete and/orpartially incorrect databases determine if the answer to a query will be complete bydetermining whether the answer to the query is independent of an insertion update to thedatabase. If the answer to the query is independent of an insertion update; the system andthe method determine that the answer to the query will be complete. In the case ofdatabases that may be incorrect; the system and the method determine if the answer to agiven query will be correct by determining if the answer to the query is independent of adeletion update to the database. If the answer to the query is independent of a deletionupdate; the system and the method determine that the answer to the query will be correct.,*,1999,7
Recent progress in data integration—A tutorial,Daniela Florescu; Alon Levy,In the last few years there has been considerable interest in the problem of providing accessto large collections of distributed heterogeneous information sources (eg; sources on theWorld-Wide Web; company-wide databases). This interest has spawned a significantamount of research in Database Systems and related fields (eg; Artificial Intelligence;Operating Systems; Human Computer Interaction). This has led to the development ofseveral research prototypes for information integration and recently; we are seeing thebeginnings of an industry addressing this problem. The goal of this tutorial is to survey thework on information integration; to illustrate the common principles underlying this body ofwork; to assess the state of the art; and identify the open research problems in this area. Thetutorial will illustrate the issues involved in information integration through several …,East European Symposium on Advances in Databases and Information Systems,1998,7
Reasoning About Web-Site Structure.,Mary F Fernandez; Daniela Florescu; Alon Y Levy; Dan Suciu,*,KRDB,1998,7
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2015,6
Recent progress towards an ecosystem of structured data on the Web,Nitin Gupta; Alon Y Halevy; Boulos Harb; Heidi Lam; Hongrae Lee; Jayant Madhavan; Fei Wu; Cong Yu,Google Fusion Tables aims to support an ecosystem of structured data on the Web byproviding a tool for managing and visualizing data on the one hand; and for searching andexploring for data on the other. This paper describes a few recent developments in ourefforts to further the ecosystem.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,6
Data Management for Journalism.,Alon Y Halevy; Susan McGregor,Abstract We describe the power and potential of data journalism; where news stories arereported and published with data and dynamic visualizations. We discuss the challengesfacing data journalism today and how recent data management tools such as Google FusionTables have helped in the newsroom. We then describe some of the challenges that need tobe addressed in order for data journalism to reach its full potential.,IEEE Data Eng. Bull.,2012,6
Discovering Structure in a Corpus of Schemas.,Alon Y Halevy; Jayant Madhavan; Philip A Bernstein,Abstract This paper describes a research program that exploits a large corpus of databaseschemas; possibly with associated data and meta-data; to build tools that facilitate thecreation; querying and sharing of structured data. The key insight is that given a largecorpus; we can discover patterns concerning how designers create structures forrepresenting domains. Given these patterns; we can more easily map between disparatestructures or propose structures that are appropriate for a given domain. We describe thefirst application of our approach to the problem of semi-automatic schema matching.,IEEE Data Eng. Bull.,2003,6
Harvesting relational tables from lists on the web,*,Computer implemented methods and apparatus for extracting list information into databasetables. A number of fields are independently determined for items in list. A number ofdatabase table columns are determined from most common number of list item fields. Newfields are determined for items with more fields than database columns. Null fields areinserted into items with fewer fields than database columns. Information from items havingthe same number of fields as database columns is written to database table rows.Information from each field is written to a corresponding database table column. Streaks ofpoorly matching cells in a database table row are determined. Streak cells are merged andnew cells are determined. Null cells are inserted if number of new cells is less than numberof cells in the streak. Information from the new cells is written to the table row and columns …,*,2012,5
Answering web questions using structured data: dream or reality?,Fernando Pereira; Anand Rajaraman; Sunita Sarawagi; William Tunstall-Pedoe; Gerhard Weikum; Alon Halevy,Abstract The question of which role structured data can play in Web search has been raisedfrom the early days of the Web. On the one hand; structured data can be used to answerfactual queries. On the other; large amounts of structured data can be used to betterorganize web-content and therefore to improve search on a wide range of queries.,Proceedings of the VLDB Endowment,2009,5
Soliciting User Feedback in a Dataspace System,Shawn Jeffery; Michael Franklin; Alon Halevy; Shawn R Jeffery,ABSTRACT A primary challenge to large-scale data integration is creating semanticequivalences between elements from different data sources that correspond to the samereal-world entity or concept. Dataspaces propose a pay-as-you-go approach: automatedmechanisms such as schema matching and reference reconciliation provide a initialcorrespondences; termed candidate matches; and then user feedback is used toincrementally confirm these matches. The key to this approach is to determine in what orderto solicit user feedback for confirming candidate matches. In this paper; we develop adecision-theoretic framework for ordering candidate matches for user confirmation using theconcept of the value of perfect information (VPI). At the core of this concept is a utility functionthat quantifies the desirability of a given state; thus; we devise a utility function for …,Electrical Engineering and Computer Sciences University of California at Berkeley,2007,5
Specifying Semantic Email Processes.,Luke K McDowell; Oren Etzioni; Alon Y Halevy,Abstract Prior work has shown that semantic email processes (SEPs) can be an effective toolfor automating emailmediated tasks that are currently performed manually in a tedious; time-consuming; and error-prone manner. However; specifying a SEP can be difficult toaccomplish; even for users familiar with RDF and semantic email. In response; this paperconsiders an approach for specifying SEP templates that can be authored once but theninstantiated many times by untrained users. We describe the template language and providea complete example; highlighting the key features needed to enable general SEPs. We thenexamine a number of challenges related to SEP authoring. In particular; we discuss theproblem of verifying that a given template will always produce a valid instantiation and givethe computational complexity of this problem. In addition; we discuss how to simplify the …,WWW Workshop On Application Design; Development and Implementation Issues in the Semantic Web,2004,5
Crossing the structure chasm,Oren Etzioni; Alon Halevy; Anhai Doan; Zachary G Ives; Jayant Madhaven; Luke McDowell; Igor Tatarinov,Abstract It has frequently been observed that most of the world's data lies outside databasesystems. The reason is that database systems focus on structured data; leaving theunstructured realm to others. The world of unstructured data has several very appealingproperties; such as ease of authoring; querying and data sharing. In contrast; authoring;querying and sharing structured data require significant effort; albeit with the benefit of richquery languages and exact answers. We argue that in order to broaden the use of datamanagement tools; we need a concerted effort to cross this structure chasm; by importing theattractive properties of the unstructured world into the structured one. As an initial effort inthis direction; we introduce the REVERE System; which offers several mechanisms forcrossing the structure chasm; and considers as its first application the chasm on the …,*,2003,5
Special issue on XML data management,A Halevy,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Bulletin of the TCDE; IEEE CS,2001,5
Challenges for global information systems,Alon Y Levy; Abraham Silberschatz; Divesh Srivastava; Maria Zemankova,Currently; the Internet provides access to a very large number and wide variety ofinformation sources (eg; textual databases; sites containing technical reports; directorylistings); and systems to access these sources (eg; World Wide Web; Gopher; WAIS). Thechallenge is to provide easy; efficient; robust and secure access to this information and otherkinds (eg; relational and object oriented databases). This aim of this panel is to explorewhether there are any new technical problems; relevant to the Database field; that need tobe solved in order to realize such global information systems. In particular; we debatewhether existing techniques from database systems (eg; multidatabases and distributeddatabases) can be applied or straigtitforwardly extended to global information systems.Furthermore; we attempt to establish realistic goals for database technologies in global …,VLDB,1994,5
A semantic theory of abstractions: A preliminary report,P Pandurang Nayak; Alon Y Levy; Henry Lum Jr,Abstract: In this paper we present a semantic theory of abstractions based on viewingabstractions as interpretations between theories. This theory captures important aspects ofabstractions not captured in the theory of abstractions presented by Giunchiglia and Walsh.Instead of viewing abstractions as syntactic mappings; we view abstractions as a two stepprocess: the intended domain model is first abstracted and then a set of (abstract) formulasis constructed to capture the abstracted domain model. Viewing and justifying abstractionsas model level transformations is both natural and insightful. We provide a precisecharacterization of the abstract theory that exactly implements the intended abstraction; andshow that this theory; while being axiomatizable; is not always finitely axiomatizable. Asimple corollary of the latter result disproves a conjecture made by Tenenberg that if a …,*,1994,5
Managing Google's data lake: an overview of the Goods system.,Alon Y Halevy; Flip Korn; Natalya Fridman Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract For most large enterprises today; data constitutes their core asset; along with codeand infrastructure. For most enterprises; the amount of data that they produce internally hasexploded in recent years. At the same time; in many cases; engineers and data scientists donot use centralized data-management systems and end up creating what became known asa data lake—a collection of datasets that often are not well organized or not organized at alland where one needs to “fish” for useful datasets. In this paper; we describe our experiencebuilding and deploying GOODS; a system to manage Google's internal data lake. GOODScrawls Google's infrastructure and builds a catalog of discovered datasets; includingstructured files; databases; spreadsheets; and even services that provide access to the data.GOODS extracts metadata about datasets in a post-hoc way: engineers continue to …,IEEE Data Eng. Bull.,2016,4
Discovering subsumption relationships for web-based ontologies,Dana Movshovitz-Attias; Steven Euijong Whang; Natalya Noy; Alon Halevy,Abstract As search engines are becoming smarter at interpreting user queries and providingmeaningful responses; they rely on ontologies to understand the meaning of entities.Creating ontologies manually is a laborious process; and resulting ontologies may notreflect the way users think about the world; as many concepts used in queries are noisy; andnot easily amenable to formal modeling. There has been considerable effort in generatingontologies from Web text and query streams; which may be more reflective of how usersquery and write content. In this paper; we describe the LATTE system that automaticallygenerates a subconcept--superconcept hierarchy; which is critical for using ontologies toanswer queries. LATTE combines signals based on word-vector representations of conceptsand dependency parse trees; however; LATTE derives most of its power from an ontology …,Proceedings of the 18th International Workshop on Web and Databases,2015,4
Determining keyword for a form page,*,Among other disclosed subject matter; a computer-implemented method of analyzing a formpage for indexing includes identifying a form page that is configured for use in requestingany of multiple target pages; the form page including at least one text input control forretrieving any of the multiple target pages. The method includes identifying at least onekeyword as being informative with regard to the text input control. The method includesupdating an indexing record associated with the form page to reflect the identified keyword.,*,2013,4
A Measurement Study of Two Web-based Collaborative Visual Analytics Systems,Kristi Morton; Magdalena Balazinska; Dan Grossman; Robert Kosara; Jock Mackinlay; Alon Halevy,ABSTRACT In this paper; we present a longitudinal study of the use of two popular Web-based; collaborative visual analytics systems; Tableau Public and Many Eyes. As data hasbecome more widely accessible through the Web; online visual analytics systems haveemerged as a popular tool for data analysis and sharing. In spite of their growing popularity;however; little is known about how these systems are being utilized. The study presented inthis paper addresses this shortcoming and shows details about the workloads of thesesystems; their users; the types of analysis they perform over single or integrated datasets;and their degree of collaboration. To the best of our knowledge; this is the first study of itskind; and presents important details about the user of online; visual analytics systems.,*,2012,4
Ants meeting algorithms,Asaf Shiloni; Alon Levy; Ariel Felner; Meir Kalech,Abstract Ant robots have very low computational power and limited memory. Theycommunicate by leaving pheromones in the environment. In order to create a cooperativeintelligent behavior; ants may need to get together; however; they may not know thelocations of other ants. Hence; we focus on an ant variant of the rendezvous problem; inwhich two ants are to be brought to the same location in finite time. We introduce twoalgorithms that solve this problem for two ants by simulating a bidirectional search indifferent environment settings. An algorithm for an environment with no obstacles and ageneral algorithm that handles all types of obstacles. We provide detailed discussion on thedifferent attributes; size of pheromone required; and the performance of these algorithms.,Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 2-Volume 2,2010,4
Socialising Data with Google Fusion Tables.,Hector Gonzalez; Alon Y Halevy; Anno Langen; Jayant Madhavan; Rod McChesney; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,Abstract We describe the social features of Google Fusion Tables; a cloud-based datamanagement service whose goal is to facilitate collaboration around data sets. The socialfeatures include the ability to specify attribution of data sets; a mechanism for conductingdiscussions on data (at fine granularity; such as row; column or cell); the ability to mergetables that belong to different owners; and the ability to share specific queries andvisualizations and embed them in other properties on the Web. We describe the rationale fordesigning these features and our experiences after our first year of interacting with users.,IEEE Data Eng. Bull.,2010,4
Structured Data on the Web.,Alon Y Halevy,Page 1. Structured Data on the Web Alon Halevy Google Australasian Computer Science WeekJanuary; 2010 Page 2. Structured Data & The Web Page 3. Andree Hudson; 4th of July Page4. Page 5. Page 6. Page 7. Page 8. Discover Manage; Analyze; Combine Extract Publish Hardto query; visualize; combine data across organizations Requires infrastructure; concerns aboutlosing control Hard to find structured data via search engines Data is embedded in web page;behind forms Page 9. Discover Manage; Analyze; Combine Extract Publish Fusion Tables:collaborating on data in the cloud; easy data publishing Web-form crawling; Finding all HTMLtables Lists -> tables Extracting context Page 10. Two Over-Arching Questions • How do wecombine techniques for structured and unstructured data? – Can't think of structured data inisolation on the Web • How to design a data management …,NGITS,2009,4
User-focused database management,Alon Y Halevy,Abstract This talk describes two projects whose over goal is to make database managementsystems usable by a wider audience. Dataspaces aim to eliminate the upfront effort involvedin creating a database. Data mangement for collaboration attempts to shift the focus of datamangement to supporting users in their natural environments and workflow.,Proceedings of the 14th international conference on Intelligent user interfaces,2009,4
Dataspaces: Co-existence with Heterogeneity.,David Maier; Alon Y Halevy; Michael J Franklin,Most information management scenarios today rarely have a situation in which all the datathat needs to be managed can fit nicely into a single management system; such as arelational database or a knowledge base. Instead; we need to manage a set of looselyconnected data sources; and typically face the following recurring challenges:,KR,2006,4
Enhancements of personal information,David Maier; Alon Halevy; Marcia Bates; Harry Bruce; Ben Bederson; Mel Knox,Personal information as initially encountered can often be very raw; fragmentary; or partiallyrelevant; it may come from disparate sources with differing format and structures. Hencethere have been many proposals to enhance it in various ways to make it more useful for thetask at hand; to improve later findability; or to record and reuse human analysis andjudgment connected with it. Enhancements typically involve adding more data to personalinformation or adding links between previously unconnected pieces of personal information;but can involve deletions or removal of extraneous relationships. This breakout groupdiscussed the kinds of enhancements that have been considered (or should be considered);the variety of reasons for enhancing personal information; and the issues that arise indevising enhancement methods. We recount our discussions on each in turn.,Breakout Group Summary; PIM Workshop,2005,4
The specification of agent behavior by ordinary people: A case study,Luke McDowell; Oren Etzioni; Alon Halevy,Abstract The development of intelligent agents is a key part of the Semantic Web vision; buthow does an ordinary person tell an agent what to do? One approach to this problem is touse RDF templates that are authored once but then instantiated many times by ordinaryusers. This approach; however; raises a number of challenges. For instance; how cantemplates concisely represent a broad range of potential uses; yet ensure that each possibleinstantiation will function properly? And how does the agent explain its actions to thehumans involved? This paper addresses these challenges in the context of a case studycarried out on our fully-deployed system for semantic email agents. We describe how high-level features of our template language enable the concise specification of flexible goals. Inresponse to the first question; we show that it is possible to verify; in polynomial time; that …,International Semantic Web Conference,2004,4
Rethinking the conference reviewing process,Michael J Franklin; Jennifer Widom; Anastassia Ailamaki; Philip A Bernstein; David DeWitt; Alon Halevy; Zachary Ives; Gerhard Weikum,In recent years the database research community has endeavored to expand the scope ofthe field and attract a larger and more varied base of participants. We have also long workedat “educating” academic tenure committees and research management about theimportance of our major conferences. We may now be seeing some unintended effects ofour success. There is a growing dissatisfaction with conference reviewing from all sides ofthe process. Many now perceive the process to be" broken". A number of factors can beidentified as precipitating the discontent:• The number of submitted papers has spikeddramatically in recent years (see Figure 1).,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,4
Warehousing and incremental evaluation for Web Site management.,Mary F Fernández; Daniela Florescu; Alon Y Levy; Dan Suciu,Abstract Recently; several systems have been proposed for building Web sites fromdeclarative speci cations. The salient feature of these systems is that they model both thestructure and the content of a Web site as a view over some existing raw data. In this\Website as view" paradigm; a critical issue is when to compute the site. One approach is tomaterialize the site completely before the users browse it: the disadvantages of thisapproach are high maintenance costs and stale data. Moreover; this approach is notapplicable if the Web site has forms; because then the queries de ning the Web sitedepends on user input. The other extreme approach is to precompute only the root (s) of theWeb site; and then compute dynamically (at\click time") the queries that retrieves theinformation required to display the next page. In this paper we address the problem of …,BDA,1998,4
A proof-theoretic approach to irrelevance: Foundations and applications,Alon Y Levy; Richard E Fikes; Yehoshua Sagiv,*,Proc. of the AAAI Fall Symposium on Relevance,1994,4
E cient query processing for information gathering agents,Craig A Knoblock; Alon Levy,In the not too distant future; we envision information gathering agents that will have accessto a large set of diverse information sources. These information sources will not belong tothe agent; rather they will be information resources that are made available across thenetwork (possibly for a fee). The agents will not maintain any real data; they will only havedescriptions of the contents of these information sources. An agent will have a domainmodel of its area of expertise (eg; a class hierarchy describing the objects in it domain); anda description of an information source relates the contents of the source to the domain modelof the agent. An agent will serve as a mediator for processing information requests; sendingrequests to the appropriate information sources and possibly processing the intermediatedata; thus freeing a user from being aware of and sending queries directly to the …,Proceedings of the Workshop on Intelligent Information Agents; Gaithersburg; MD,1994,4
Special issue on structured and crowd-sourced data on the Web,Marco Brambilla; Stefano Ceri; Alon Halevy,The abundance of structured and social data on the Web coupled with ability to solicitfeedback from crowds has the potential of changing the way we search for information andenabling new classes of applications on the Web. This special issue of the VLDB Journalfeatures original contributions that advance the state of the art in this topic area. Broadly; thespecial issue is concerned with methods for analyzing and serving structured data on theWeb and methods for enhancing data by soliciting feedback from crowds. Structured dataappear on the Web in several forms; including hidden Web sources exposed through HTMLform interfaces; tables; lists; and pages with repeating semistructured cards. Currentresearch efforts for leveraging this data include approaches for extracting and combiningresults from multiple sources; for surfacing the deep web; and for exposing data through …,The VLDB Journal,2013,3
Dataspaces: The tutorial,AY Halevy; David Maier; MJ Franklin,• Dataspaces: why? What are they? Dataspaces: why? What are they? – Examples and motivation• Dataspace techniques: – Locating and understanding data sources – Creating mappings andmediated schemas – Pay-as-you-go: improving with time – Query processing for dataspacesQuery processing for dataspaces • Research challenges on specific dataspaces: – Science;the desktop; the Web … [Doan et al.; U. Wisc; CIMPLE; UW + Y!] … • A data space for the databaseresearch A data space for the database research community … • Started with 846 data sourcesin May 2005 … – researcher homepages; CS dept homepages; etc • Immediately provided somebasic service … – crawl sources daily to obtain 11000+ pages – index & provide keyword search• Incrementally extract & integrate data … – provide more services & better services – leverageuser feedback to further improve the system … Forecasting System Atmospheric models …,PVLDB,2008,3
What does Web 2.0 have to do with databases?,Sihem Amer-Yahia; Alon Halevy,Abstract Web 2.0 is a buzzword we have been hearing for over 2 years. According toWikipedia; it hints at an improved form of the World Wide Web where technologies such asweblogs; social bookmarking; RSS feeds; photo and video sharing; based on anarchitecture of participation and democracy that encourages users to add value to theapplication as they use it. Web 2.0 enables social networking on the Web by allowing usersto contribute content; share it; rate it; create a network of friends; and decide what they like tosee and how they want it to look like.,Proceedings of the 33rd international conference on Very large data bases,2007,3
Learning about data integration challenges from day one,Alon Y Halevy,ABSTRACT I describe the format of the new version of an introductory database course that Itaught at the University of Washington in Winter; 2003. The key idea underlying the course isto expose the students to some of the challenges that arise when working with andintegrating data from multiple database systems and applications.,ACM SIGMOD Record,2003,3
Querying heterogeneous information sources using source descriptions,Anand Rajaraman; Alon Y Levy; J Ordill Joann,Abstract We witness a rapid increase in the number of structured information sources thatare available online; especially on the WWW. These sources include commercial databaseson product information; stock market information; real estate; automobiles; andentertainment. We would like to use the data stored in these databases to answer complexqueries that go beyond keyword searches. We face the following challenges:(1) Severalinformation sources store interrelated data; and any query-answering system mustunderstand the relationships between their contents.(2) Many sources are not full-featureddatabase systems and can answer only a small set of queries over their data (for example;forms on the WWW restrict the set of queries one can ask). Since the number of sources isvery large; effective techniques are needed to prune the set of information sources …,Proceedings of the 22nd International Conference on Very Large Databases; VLDB-96; Bombay; India,1996,3
Irrelevance in problem solving,Alon Y Levy,Abstract: The notion of irrelevance underlies many different works in AI; such as detectingredundant facts; creating abstraction hierarchies and reformulation and modeling physicaldevices. However; in order to design problem solvers that exploit the notion of irrelevance;either by automatically detecting irrelevance or by being given knowledge about irrelevance;a formal treatment of the notion is required. In this paper we present a general framework foranalyzing irrelevance. We discuss several properties of irrelevance and show how they varyin a space of definitions outlined by the framework. We show how irrelevance claims can beused to justify the creation of abstractions thereby suggesting a new view on the work onabstraction.,*,1992,3
Data integration: After the teenage years,Behzad Golshan; Alon Halevy; George Mihaila; Wang-Chiew Tan,Abstract The field of data integration has expanded significantly over the years; fromproviding a uniform query and update interface to structured databases within an enterpriseto the ability to search; ex-change; and even update; structured or unstructured data that arewithin or external to the enterprise. This paper describes the evolution in the landscape ofdata integration since the work on rewriting queries using views in the mid-1990's. Inaddition; we describe two important challenges for the field going forward. The firstchallenge is to develop good open-source tools for different components of data integrationpipelines. The second challenge is to provide practitioners with viable solutions for the long-standing problem of systematically combining structured and unstructured data.,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,2
Searching for join candidates,*,Systems and techniques are provided for receiving an input column and a search keywordand providing one or more suggested columns with which to merge the input column. Acoverage score and a refinity score are calculated for potential columns based on the inputcolumn as well as a search score based on the search keyword. The one or more suggestedcolumns may be determined based on the coverage score; refinity score; and/or the searchscore. The input column and/or a potential column may be modified based on a function andthe modification may result in a plurality of modified input and/or potential columns.Coverage; refinity; and search scores may be calculated based on the modified columns.,*,2015,2
Why should you care about structured data on the web,Alon Halevy,ABSTRACT: For the first time since the emergence of the Web; structured data is playing akey role in search engines and is therefore being collected via a concerted effort. Much ofthis data is being extracted from the Web; which contains vast quantities of structured dataon a variety of domains; such as hobbies; products and reference data. Moreover; the Webprovides a platform that encourages publishing more data sets from governments and otherpublic organizations. The Web also supports new data management opportunities; such aseffective crisis response; data journalism and crowd-sourcing data sets. The emerging eco-system around structured data can have profound implications on various industries andopen up new opportunities for the simulation community. I will describe some of the effortswe are conducting at Google to collect structured data; filter the highquality content; and …,SummerSim Keynote. Abstract available in: http://www. duniptechnologies. com/summersim14/SummerSim,2014,2
When does cotraining work in real data? Knowledge and Data Engineering,J Du; C Ling; ZH Zhou,*,IEEE Transactions on,2011,2
Robocup 2010 standard platform league team burst description,Matan Keidar; Inbar Aharon; Danielle Barda; Or Kamara; Alon Levy; Eran Polosetski; Dikla Ramati; Lior Shlomov; Jeremy Shoshan; Ari Yakir; Avishay Zilka; Gal A Kaminka; Eli Kolberg,Abstract. Team BURST was founded in 2009. It consists mostly of undergraduate students;whose participation is a part of a robotics course at the Computer Science department at theuniversity. Team BURST which competed in the RoboCup 2009 standard platform league; isfirst RoboCup senior team from Israel; ever. Our main research interests within the scope ofthe SPL are robust; realtime vision; machine learning for gait generation and adaptation andarchitectures for humanoid decision-making (including teamwork).,RoboCup,2010,2
Fusion Tables: new ways to collaborate on structured data,Jonathan Goldberg Kidon,Fusion Tables allows data collaborators to create; merge; navigate and set access controlpermissions on structured data. This thesis focuses on the collaboration tools that wereadded to Googles Fusion Tables. The collaboration tools provided additional functionality:first; the ability to view; sort and filter all the threaded discussions on the differentgranularities of the data set; second; the ability to take Snaps; dynamic state bookmarkingthat allows collaborators to save queries and visualizations and share them with other users.In addition; this thesis initiates a discussion about data collaboration on different platformsoutside the Data Management System (DMS); and the implementation of the Fusion Table-Google Wave gadget that provides this functionality. To evaluate these added features; weconducted a user survey based on three sources: Google Analytics; field study of …,*,2010,2
Semantic integration workshop at the second international semantic web conference (ISWC-2003),AnHai Doan; Alon Y Halevy; Natalya F Noy,*,AI Magazine,2004,2
Semantic Integration,AnHai Doan; Alon Halevy; Natasha Noy,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: semantics & computational sc... Search; Register; Log in …,*,2003,2
Research on statistical relational learning at the university of washington,Pedro Domingos; Yeuhi Abe; Corin Anderson; AnHai Doan; Dieter Fox; Alon Halevy; Geoff Hulten; Henry Kautz; Tessa Lau; Lin Liao; Jayant Madhavan; D Patterson Mausam; Matthew Richardson; Sumit Sanghai; Daniel Weld; Steve Wolfman,Abstract This paper presents an overview of the research on learning statistical models fromrelational data being carried out at the University of Washington. Our work falls into five maindirections: learning models of social networks; learning models of sequential relationalprocesses; scaling up statistical relational learning to massive data sources; learning forknowledge integration; and learning programs in procedural languages. We describe someof the common themes and research issues arising from this work.,Proceedings of the IJCAI-2003 Workshop on Learning Statistical Models from Relational Data,2003,2
Data Integration: A “Killer App” for Multistrategy Learning,AnHai Doan; Pedro Domingos; A Levy,Abstract To build a data-integration system; the application designer must specify amediated schema and supply the descriptions of data sources. A source descriptioncontains a source schema that describes the content of the source; and a mapping betweenthe corresponding elements of the source schema and the mediated schema. Manuallyconstructing these mappings is both laborintensive and error-prone; and has proven to be amajor bottleneck in deploying large-scale data integration systems in practice. In this paperwe report on our initial work toward automatically learning mappings between sourceschemas and the mediated schema. Specifically; we investigate finding one-to-onemappings for the leaf elements of source schemas. We describe LSD; a system thatautomatically finds such mappings. LSD consults a set of learner modules; where each …,*,2000,2
Data engineering special issue on adaptive query processing; june 2000,Alon Levy; Adaptive Query,Abstract As query engines are scaled and federated; they must cope with highlyunpredictable and changeable environments. In the Telegraph project; we are attempting toarchitect and implement a continuously adaptive query engine suitable for global-areasystems; massive parallelism; and sensor networks. To set the stage for our research; wepresent a survey of prior work on adaptive query processing; focusing on threecharacterizations of adaptivity: the frequency of adaptivity; the effects of adaptivity; and theextent of adaptivity. Given this survey; we sketch directions for research in the Telegraphproject. 1 Introduction Adaptivity has been an inherent--though largely latent--aspect ofdatabase research for the last three decades. Codd's vision of data independence waspredicated on the development of systems that could adapt gracefully and opaquely to …,*,2000,2
A query language for XML,M Fernandez; D Florescu; A Levy; D Suciu,*,Computer Networks,1999,2
Information Gathering from Heterogeneous,Craig A Knoblock; Alon Levy,*,*,1995,2
Extracting facts from documents,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for extracting facts from a collection of documents. One of the methodsincludes obtaining a plurality of seed facts; generating a plurality of patterns from the seedfacts; wherein each of the plurality of patterns is a dependency pattern generated from adependency parse; applying the patterns to documents in a collection of documents toextract a plurality of candidate additional facts from the collection of documents; andselecting one or more additional facts from the plurality of candidate additional facts.,*,2017,1
Technical perspective: incremental knowledge base construction using DeepDive,Alon Halevy,Imagine the task of creating a database of all the highquality specialty cafes around theworld so you never have to settle for an imperfect brew. There are plenty of online sourceswith content relevant to your envisioned database. Cafes may be featured in well-respectedcoffee publications such as sprudge. com or baristamagazine. com. Data of more fleetingnature may pop up when your coffee-savvy friends note their location by checking in onFacebook or tweeting. Naturally; there is a plethora of books that studied cafes around theworld in even more detail. The task of creating such a database is surprisingly hard. Youwould begin by deciding which attributes of cafes the database should model. Attributessuch as address and opening hours would be obvious even to a novice; but you will need toconsult a coffee expert who will suggest more refined attributes such as roast profile and …,ACM SIGMOD Record,2016,1
Systems; methods; and computer-readable media for searching tabular data,*,Systems; methods; and computer-readable media are provided for searching a tabulardatabase. According to certain embodiments; search parameters for searching a tabulardatabase are received from a user device and a row of a tabular database that correspondsto the search parameters is determined. In certain embodiments; the row may be determinedby comparing the search parameters with a plurality of stored exemplar search queries;each of the plurality of stored exemplar search queries comprising a search queryassociated with a row and a column of the tabular database. A column of the tabulardatabase that corresponds to the search parameters is determined by comparing the searchparameters with the plurality of stored exemplar search queries. In certain embodiments; atleast one cell of the tabular database is determined. The determined cell may be located …,*,2016,1
Structured data on the web (or; a personal journey away from and back to ontologies),Alon Halevy,For the first time since the emergence of the Web; structured data is playing a key role insearch engines and is therefore being collected via a concerted effort. Much of this data isbeing extracted from the Web; which contains vast quantities of structured data on a varietyof domains; such as hobbies; products and reference data. Moreover; the Web provides aplatform that encourages publishing more data sets from governments and other publicorganizations. The Web also supports new data management opportunities; such aseffective crisis response; data journalism and crowd-sourcing data sets. I will describe someof the efforts we are conducting at Google to collect structured data; filter the high-qualitycontent; and serve it to our users. These efforts include providing Google Fusion Tables; aservice for easily ingesting; visualizing and integrating data; mining the Web for high …,Informal Proceedings of the 27th International Workshop on Description Logics. CEUR Workshop Proceedings,2014,1
Opportunities and challenges in data journalism,Susan McGregor; Alon Halevy,Since the Arab Spring earlier that year; the role of social media as an organizing tool fordemonstrations and protests had become accepted wisdom in many circles; providing abasis for Cameron's assertion that the government should have the right “to stop peoplecommunicating via these Websites and services when we know they are plotting violence;disorder and criminality”[11]. Less than 2 weeks later; however; a preliminary analysis andvisualization of more than 2.5 million tweets by the UK newspaper The Guardian [3]indicated that riot-related traffic on the web service tended to spike after the violence beganin a particular neighborhood; as shown in Figure 16.1. Over the following months; TheGuardian partnered with London School of Economics and the University of Manchester;where more formal analyses confirmed that social media outlets such as Twitter and …,Computation for humanity: Information technology to advance society,2013,1
Determining a geographic location relevant to a web page,*,One embodiment of the present invention provides a system that facilitates searchingthrough content which is accessible though web-based forms. During operation; the systemreceives a query containing keywords. Next; the system analyzes the query to create astructured query. The system then performs a lookup based on the structured query in adatabase containing entries describing the web-based forms. Next; the system ranks formsreturned by the lookup; and uses the rankings and associated database entries to facilitate asearch through content which is accessible through the forms.,*,2013,1
Towards an ecosystem of structured data on the web,Alon Y Halevy,Abstract We are in the midst of very exciting times in which structured data is having aprofound impact on many aspects of our lives. In many countries; citizens take for grantedthe fact that governments; local authorities; and non-government organizations should makea variety of data sets available to the public. These data sets span a variety of topics such aseconomic indicators; crime statistics; educational data; government spending and campaigncontributions. Journalists and other data aficionados are fueling this trend by turning thisdata into visualizations and stories that are spread by social networks and seen by millionsof people [8]. These visualizations; stories and public attention; in turn; lead to newquestions and hence a demand for additional data.,Proceedings of the 15th International Conference on Extending Database Technology,2012,1
Best-effort modeling of structured data on the web,Alon Halevy,Conceptual modeling has been used mainly for supporting information systems (IS)development. In order to better capture requirements for developing IS; we have been extendingconceptual models to include more business context (eg; mission of the organization). This seemsto interest organizational workers in using those conceptual models to solve problems. We proposedual roles for conceptual modeling: developing IS; and managing the changes occurring in thebusiness. To fulfill the second role; conceptual modeling must provide constructs that are notbiased toward IS background and thinking; but assist organizational workers to better understandthe business and its operations. Research and literature on management will be useful to accomplishthis objective. Our research in this direction suggests much potential in expanding conceptualmodeling to support organizational workers … Mobility data is becoming an important …,Proceedings of the 30th international conference on Conceptual modeling,2011,1
Reports of the AAAI 2010 conference workshops,David W Aha; Mark Boddy; Vadim Bulitko; Artur S d'Avila Garcez; Prashant Doshi; Stefan Edelkamp; Christopher Geib; Piotr Gmytrasiewicz; Robert P Goldman; Pascal Hitzler; Charles Isbell; Darsana Josyula; Leslie Pack Kaelbling; Kristian Kersting; Maithilee Kunda; Luis C Lamb; Bhaskara Marthi; Keith McGreggor; Vivi Nastase; Gregory Provan; Anita Raja; Ashwin Ram; Mark Riedl; Stuart Russell; Ashish Sabharwal; Jan-Georg Smaus; Gita Sukthankar; Karl Tuyls; Ron van der Meyden; Alon Halevy; Lilyana Mihalkova; Sriraam Natarajan,Abstract The AAAI-10 Workshop program was held Sunday and Monday; July 11–12; 2010at the Westin Peachtree Plaza in Atlanta; Georgia. The AAAI-10 workshop program included13 workshops covering a wide range of topics in artificial intelligence. The titles of theworkshops were AI and Fun; Bridging the Gap between Task and Motion Planning;Collaboratively-Built Knowledge Sources and Artificial Intelligence; Goal-DirectedAutonomy; Intelligent Security; Interactive Decision Theory and Game Theory; Metacognitionfor Robust Social Systems; Model Checking and Artificial Intelligence; Neural-SymbolicLearning and Reasoning; Plan; Activity; and Intent Recognition; Statistical Relational AI;Visual Representations and Reasoning; and Abstraction; Reformulation; and Approximation.This article presents short summaries of those events.,AI Magazine,2010,1
XML and data integration,Alon Halevy,Page 1. XML and Data Integration Alon Halevy University of Washington; Seattle EDBT SummerSchool; Sardegna; Italy; September; 2004 Page 2. Abstractions 'R Us Logical vs. Physical; Whatvs. How. SSN Name Category 123-45-6789 Charles undergrad 234-56-7890 Dan grad … … SSNCID 123-45-6789 CSE444 123-45-6789 CSE444 234-56-7890 CSE142 … Students: Takes: CIDName Quarter CSE444 Databases fall CSE541 Operating systems winter Courses: SELECTC.name FROM Students S; Takes T; Courses C WHERE S.name=“Mary” and S.ssn = T.ssn andT.cid = C.cid Page 3. Data Integration: A Higher-level Abstraction Mediated Schema Query S1 S2S3 SSN Name Category 123-45-6789 Charles undergrad 234-56-7890 Dan grad … … SSN CID123-45-6789 CSE444 123-45-6789 CSE444 234-56-7890 CSE142 … CID Name Quarter CSE444Databases fall CSE541 Operating systems winter …,EDBT Summerschool presentation,2004,1
Learning Complex Semantic Mappings between Structured Representations,AnHai Doan; Pedro Domingos; Alon Y Halevy,Abstract Establishing semantic mappings between multiple representations is fundamentalto interoperability efforts in data integration; the Semantic Web; knowledge sharing; andinformation agents. Manually creating such mappings is extremely tedious and error-prone.Hence many recent works have focused on developing techniques to automate the mappingprocess. However; these works deal only with one-to-one (1-1) mappings; the simplest typeof mapping. They do not consider the important class of more complex mappings. Wedescribe the COMAP system which semi-automatically discovers both 1-1 and morecomplex mappings. The key distinguishing feature of COMAP is that it incorporates multipletypes of knowledge; search; and learning techniques in every important stage of themapping process. This feature helps maximize mapping accuracy; and makes the sytem …,Unpublished Manuscript,2002,1
E ciently Ordering Query Plans for Data Integration,AnHai Doan; Alon Levy,Abstract We describe Streamer; the query-reformulation component of a data integrationsystem. Given a utility measure and a user query; Streamer uses abstraction-based renement planning and exploits information on plan independence to produce; in decreasingorder of utility; a set of plans that access data sources to obtain answers to the query. Wethen focus on plan coverage as an important utility measure. We show how to use statisticinformation about the domain and data sources to estimate plan coverage; and how toincorporate the plancoverage framework into Streamer. In doing so; we provide the rstmethod for e ectively integrating the use of quantitative information into the query optimizerof a data-integration system. We present preliminary experimental results suggesting thatStreamer runs an order of magnitude faster than brute-force plan-ordering methods …,Proceedings of IEEE International Conference on Data Engineering,2002,1
Reminiscences on influential papers,Kenneth A Ross,First; together with Agrawal; Imielinski; and Swami's SIGMOD'93 paper:" Mining AssociationRules between Sets of Items in Large Databases;" it identifies a new and important task indata mining: association rule mining; ie; finding frequent patterns or itemsets (sets of items)that occur frequently together in large databases. This has proven truly useful for frequentpattern or association mining; dependency or correlation analysis; etc.; with manyapplications. Some following studies have shown that it is also usefifl for associatiombasedclassification; sequential or structured pattern analysis; constraint-based mining; clusteranalysis; semantic data compression; data cube computation; and so on. Identification of acrucial research problem itself makes the paper distinct from many others. Second; itdiscovers a nice and elegant property in association mining; namely Apriori; which states …,ACM SIGMOD Record,2001,1
Answering queries using views,Alon Y Levy; Alberto O Mendelzon; Yehoshua Sagiv; Divesh Srivastava,*,Materialized views,1999,1
Exploiting Run-Time Information to Locate Relevant Data Sources,Craig A Knoblock; Alon Y Levy,Abstract Information agents answer user queries using a large number of diverseinformation sources. The key issue in their perfomance is nding the set of informationsources relevant to a query. Previous work has considered determining relevance soleybased on compile-time analysis of the query. We argue that at compile-time; it is often notpossible to signi cantly prune the set of sources relevant to a query; and that run-timeinformation is needed. We make the following contributions. First; we identify the di erenttypes of information that can be obtained at run-time; and how they can be used to pruneinformation sources. Second; we describe an algorithm which naturally extends queryplanning algorithms to exploit run-time information. Third; we describe the discriminationmatrix; which is a data structure that identi es the information that can be used to help …,Working Notes of the AAAI Spring Symposium on Information Gathering from Heterogeneous; Distributed Environments; Stanford; CA,1995,1
HappyDB: A Corpus of 100;000 Crowdsourced Happy Moments,Akari Asai; Sara Evensen; Behzad Golshan; Alon Halevy; Vivian Li; Andrei Lopatenko; Daniela Stepanov; Yoshihiko Suhara; Wang-Chiew Tan; Yinzhan Xu,Abstract: The science of happiness is an area of positive psychology concerned withunderstanding what behaviors make people happy in a sustainable fashion. Recently; therehas been interest in developing technologies that help incorporate the findings of thescience of happiness into users' daily lives by steering them towards behaviors that increasehappiness. With the goal of building technology that can understand how people expresstheir happy moments in text; we crowd-sourced HappyDB; a corpus of 100;000 happymoments that we make publicly available. This paper describes HappyDB and its properties;and outlines several important NLP problems that can be studied with the help of the corpus.We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our resultsdemonstrate the need for deeper NLP techniques to be developed which makes …,arXiv preprint arXiv:1801.07746,2018,*
Identifying entity attributes,*,Abstract Methods; systems; and apparatus; including computer programs encoded oncomputer storage media; for generating an ontology of entity attributes. One of the methodsincludes extracting a plurality of attributes based upon a plurality of queries; andconstructing an ontology based upon the plurality of attributes and a plurality of entityclasses.,*,2018,*
Post-hoc management of datasets,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for generating a catalog for multiple datasets; the method comprisingaccessing multiple extant data sets; the extant data sets including data sets that areindependently generated and structurally dissimilar; organizing the data sets intocollections; each data set in each collection belonging to the collection based on collectiondata associated with the data set; for each collection of data sets: determining; from a subsetof the data sets that belong to the collection; metadata that describe the data sets that belongto the collection; wherein the metadata does not include the collection data; and attributing;to other data sets in the collection; the metadata determined from the subset of data sets;and generating; from the collections of data sets and the determined metadata; a catalog …,*,2017,*
Synthesizing union tables from the web,*,Systems and techniques are provided for generating a union table with from stitchabletables. Tables may be extracted from web pages to obtain extracted tables. Stitchable tablesmay be determined from the extracted tables. Hidden attributes for the stitchable tables maybe extracted from the web pages from which the stitchable tables were extracted usingsegmentation of text for contextual data from the web pages into segment sequences; andalignment of the segment sequences. Iterative pairwise alignment may be used to align thesegment sequences and obtain aligned segments. The stitchable tables may be joined intoa union table. Hidden attributes from the aligned segments may be added to the union table.Headers for the hidden attributes in the union table may be labeled using a database ofentities and class labels.,*,2017,*
Building knowledge bases from messy data: technical perspective,Alon Halevy,Imagine the task of creating a database of all the high-quality specialty cafés around theworld so you never have to settle for an imperfect brew. Relying on reviews from sites suchas Yelp will not do the job because there is no restriction on who can post reviews there.You; on the other hand; are interested only in cafés that are reviewed by the coffeeintelligentsia. There are several online sources with content relevant to your envisioneddatabase. Cafés may be featured in well-respected coffee publications such as sprudge.com or baristamagazine. com; and data of more fleeting nature may pop up on your socialmedia stream from coffee-savvy friends.The task of creating such a database is surprisinglydifficult. You would begin by deciding which attributes of cafés the database should model.Attributes such as address and opening hours would be obvious even to a novice; but …,Communications of the ACM,2017,*
What I Wish I Knew When I Finished my PhD,Alon Halevy,Abstract You're about to finish your Ph. D and looking forward to a bright career. You mighthave some plans for what that career will look like; but the truth is; you're about to embark ona fascinating journey you know little about. You think that in 5 or 10 years you'll be all set; butactually; careers take interesting twists at many stages. In this talk I will share a few of thelessons I learned in the first 20+ years of my journey.,Proceedings of the 2016 on SIGMOD'16 PhD Symposium,2016,*
Identifying query aspects,*,Methods; systems; and apparatus; including computer program products; for generatingaspects associated with entities. In some implementations; a method includes receiving dataidentifying an entity; generating a group of candidate aspects for the entity; modifying thegroup of candidate aspects to generate a group of modified candidate aspects comprisingcombining similar candidate aspects and grouping candidate aspects using one or moreaspect classes each associated with one or more candidate aspects; ranking one or moremodified candidate aspects in the group of modified candidate aspects based on a diversityscore and a popularity score; and storing an association between one or more highestranked modified candidate aspects and the entity. The aspects can be used to organize andpresent search results in response to queries for the entity.,*,2016,*
Harvesting relational tables from lists on the web,*,List information can be extracted into database tables. A number of fields are independentlydetermined for items in list. A number of database table columns are determined from mostcommon number of list item fields. New fields are determined for items with more fields thandatabase columns. Null fields are inserted into items with fewer fields than databasecolumns. Information from items having the same number of fields as database columns iswritten to database table rows. Information from each field is written to a correspondingdatabase table column. Streaks of poorly matching cells in a database table row aredetermined. Streak cells are merged and new cells are determined. Null cells are inserted ifnumber of new cells is less than number of cells in the streak. Information from the new cellsis written to the table row and columns that define the streak.,*,2014,*
Biperpedia: An Ontology for Search Applications,Alon Halevy; Xuezhi Wang; Steven Whang; Fei Wu,*,*,2014,*
Structured data in web search,Alon Halevy,Abstract For the first time since the emergence of the Web; structured data is playing a keyrole in search engines and is therefore being collected via a concerted effort. Much of thisdata is being extracted from the Web; which contains vast quantities of structured data on avariety of domains; such as hobbies; products and reference data. Moreover; the Webprovides a platform that encourages publishing more data sets from governments and otherpublic organizations. The Web also supports new data management opportunities; such aseffective crisis response; data journalism and crowd-sourcing data sets. I will describe someof the efforts we are conducting at Google to collect structured data; filter the high-qualitycontent; and serve it to our users. These efforts include providing Google Fusion Tables; aservice for easily ingesting; visualizing and integrating data; mining the Web for high …,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,*
VLDB Endowment,Michael Böhlen; Christoph Koch; Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu; Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; Dan Olteanu; Divesh Srivastava; Jens Teubner; Stefan Manegold; Peer Kröger; Stratis D Viglas,39th International Conference on Very Large Data Bases; Riva del Garda; Trento; Italy … Proceedingsof the 39th International Conference on … Very Large Data Bases; Riva del Garda; Trento; Italy… Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu;Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; DanOlteanu; Divesh Srivastava; Jens Teubner … The 39th International Conference on Very LargeData Bases; Riva del Garda; Trento; Italy … Permission to make digital or hard copies of portionsof this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyright for components of this work owned by others than VLDBEndowment must be honored. Abstracting with credit is permitted. To copy otherwise; to …,*,2013,*
Bringing (Web) Databases to the Masses.,Alon Y Halevy,–You're an analyst looking at sales data. Bring in a column with popula/on by zipcode–You're a high-‐school student looking at bird species in danger of ex/nc/on. Bring in a dataset about the popula/on of different species–Relate coffee consump/on data with coffeeproduc/on data,ESWC,2012,*
Infrastructure for Detector Research and Development towards the International Collider,J Aguilar; H Henschel; F Fehr; M Lupberger; J Fleury; G Eigen; B Lutz; L Musa; FJ Munoz; H Kruger; U Mjornmark; J Kotula; L Raux; P Gross; M Winter; M Schumacher; M Caccia; J Alozy; L Jonsson; F Gastaldi; S Callier; X Janssen; M Reinecke; W Da Silva; M Marcisovsky; A Savoy-Navarro; A Mathieu; LBA Hommels; Y Degerli; R Poschl; P Kodys; J Ninkovich; M Ohlerich; D Dzahini; F Senee; M Lozano; P Mehtaelae; R Shaw Ward; D Calvet; I Vila; Y Yang; JY Hostachy; O Bachynska; Y Bilevych; E Stenlund; V Uzhinskiy; P Aspell; K Oliwa; C Irmler; P Gottlicher; F Dulucq; Ch de la Taille; T Bergauer; I Valin; R Diener; Ch Rosemann; E Kielar; M Goffe; R Orava; P Cornebise; G Pellegrini; T Wu; P Ambalathankandy; D Jeans; F Wicek; K Swientek; S Mannen; J Furletova; E Corrin; M Krammer; A Bonnemaison; G Doziere; M Kohli; I Peric; DS Bailey; S Rummel; N van Remortel; L Reuen; V Ivantchenko; J Smolik; S Kananov; R Zimmermann; E Garutti; A Bulgheroni; L de Nooij; M Gelin; A Charpy; W Lohmann; R Jaramillo; A Karar; G Traversi; IM Gregor; M Faucci Giannelli; C Jauffret; A Kaukher; D Haas; C Clerc; G Trampitsch; R Wurth; JC Vanel; C Muhl; V Boisvert; W Daniluk; P Ghislain; M Dhellot; A Oskarsson; W Wierba; K Gadow; W Wang; P Colas; S Haensel; T Behnke; T Fiutowski; P Sicho; D Imbault; P Wienemann; U Koetz; AF Zarnecki; P Schade; JC Brient; Ch Brezina; G Claus de Masi; M Warren; J Timmermans; V Bartsch; W Lange; M Friedl; HG Moser; J David; A Rouge; D Quirion; A Besson; P Gay; JJ Velthuis; M Pohl; R Volkenborn; I Jikhleb; L Bryngemark; J Bonnard; M Hauschild; M Terwort; T Hung Pham; A Calderone; V Grichine; M Postranecky; L Royer; H Videau; A Kehrli; F Formenti; S Aplin,The EUDET-project was launched to create an infrastructure for developing and testing newand advanced detector technologies to be used at a future linear collider. The aim was tomake possible experimentation and analysis of data for institutes; which otherwise could notbe realized due to lack of resources. The infrastructure comprised an analysis and softwarenetwork; and instrumentation infrastructures for tracking detectors as well as for calorimetry.,*,2012,*
Kristian Kersting; Stuart Russell; Leslie Pack Kaelbling,Alon Halevy; Sriraam Natarajan; Lilyana Mihalkova,Much has been achieved in the field of AI; yet much remains to be done if we are to reachthe goals we all imagine. One of the key challenges with moving ahead is closing the gapbetween logical and statistical AI. Logical AI has mainly focused on complexrepresentations; and statistical AI on uncertainty. Intelligent agents; however; must be able tohandle both the complexity and the uncertainty of the real world. Recent years have seen anexplosion of successes in combining probability and (subsets of) first-order logicrespectively programming languages and databases in several subfields of AI such asreasoning; learning; knowledge representation; planning; databases; NLP; robotics; vision;etc. Nowadays; we can learn probabilistic relational models automatically from millions ofinter-related objects. We can generate optimal plans and learn to act optimally in …,*,2010,*
Building Data Integration Systems for the Web,Alon Halevy,Page 1. Building Data Integration Systems for the Web Alon Halevy Google NSF InformationIntegration Workshop April 22; 2010 Page 2. Without (too much) Loss of Generality Web Enterprise;Science projects; … ⊇ Information integration ≅ data management Page 3. A Few Principles •Data management “in situ” – Data meaning is derived from its context – Manipulate data in itsnatural location • Pay-as-you-go data management – Provide services before modeling is done –Data can be about any domain • Collaboration should be built in – Query answering is only stepthe first step Page 4. Alex Labrinidis @via Facebook Page 5. Structured Data & The Web Page6. Discover Manage; Analyze; Combine Extract Publish Hard to query; visualize; combine dataacross organizations Requires infrastructure; concerns about losing control Hard to find structureddata via search engines Data is embedded in web page …,*,2010,*
Table Search Using Recovered Semantics,Petros Venetis; Alon Halevy; Jayant Madhavan; Marius Pasca; Warren Shen; Fei Wu; Gengxin Miao; Chung Wu,ABSTRACT We consider the problem of searching for tables in a large table corpus. TheWeb offers a corpus of 100 million tables; and smaller but sizable corpora are found withinenterprises or individual repositories (eg; data. gov). Table search is challenging becausethe semantics of the data are typically not explicit in the table itself; and signals that workwell for search over document corpora do not apply as well to table corpora. We describethe TableFinder system that partially recovers the semantics of the tables in the corpus; bymapping tables into a database of class labels that is automatically extracted from the Webitself. The database of classes has very wide coverage; but is also noisy. TableFinderidentifies a column in each table corresponding to the table's subject and identifies theclasses describing the values in that column. Query answering proceeds by considering …,*,2010,*
Discovering Functional Dependencies in Pay-As-You-Go Data Integration Systems,Daisy Zhe Wang; Michael Franklin; Luna Dong; Anish Das Sarma; Alon Halevy,ABSTRACT Functional dependency is one of the most extensively researched subjects indatabase theory; originally for improving quality of schemas; and recently for improvingquality of data. In a payas-you-go data integration system; where the goal is to provide best-effort service even without thorough understanding of the underlying domain and the variousdata sources; functional dependency can play an even more important role; applied innormalizing an automatically generated mediated schema; pinpointing sources of lowquality; resolving conflicts in data from different sources; improving efficiency of queryanswering; and so on. Despite its importance; discovering functional dependencies in sucha context is challenging: we cannot assume upfront domain knowledge for specifyingdependencies; and the data can be dirty; incomplete; or even misinterpreted; so make …,*,2009,*
Surfacing the Deep Web,Alon Halevy; Jayant Madhaven,THE TERM “DEEP WEB” REFERS TO WEB CONTENT THAT LIES HIDDEN BEHIND HTMLFORMS. IN ORDER to get to such content; a user has to perform a form submission withvalid input values. Take; for example; the store locator form in Figure 9-1. Searching forstores in the zip code 94043 results in a web page with a listing of stores. The result page isan example of a web page in the Deep Web.,Beautiful Data: The Stories Behind Elegant Data Solutions,2009,*
Ant meeting in an unknown environment,Asaf Shiloni; Alon Levy; Ariel Felner; Meir Kalech,Abstract. Ant robots have very low computational power and limited memory. Theycommunicate by leaving pheromones in the environment. In order to create a cooperativeintelligent behavior; ants may need to get together; however; they may not know thelocations of other ants. Hence; we define the ant meeting problem of bringing two ants to acommon position in finite time. We introduce three algorithms (all equivalent to finite statemachines) that solve this problem for two ants by simulating a bidirectional breadth-firstsearch in different environment settings. A basic algorithm for a grid with no obstacles; analgorithm for a grid with finite rectangular obstacles and a general algorithm that handles alltypes of obstacles. We provide detailed discussion on the different attributes; size ofpheromone required; and the performance of these algorithms.,*,2009,*
XML Information Integration,Alon Halevy,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,*
A Tamper-Resistant and Portable Healthcare Folder,Luc Bouganim; Georges Gardarin; Philippe Kesmarszky; Philippe Pucheral; Iulian Sandu-Popa; Karine Zeitouni; Georges Gardarin; Didier Nakache; Elisabeth Métais; Daniela Florescu; Alon Levy; Dan Suciu; Khaled Yagoub; Daniela Florescu; Donald Kossmann; Esther Pacitti; Daniela Florescu; Alon Levy; Ioana Manolescu; Dan Suciu; Esther Pacitti; Pascale Minet; Eric Simon; Luc Bouganim; Françoise Fabret; C Mohan; Patrick Valduriez; Philippe Bonnet; Anthony Tomasic,Lets's discover the men and women who contribute to innovative computer science andmathematics and drive the development of our digital world. The Inria awards also underlinethe contributions of research and innovation support teams who play a significant part in theefficiency and successes of Inria. All the 2017 awards have been honoured during the Inria50th birthday event at the CENTQUATRE-PARIS on novembre 7th 2017.,International Journal of Telemedicine and Applications,2008,*
Keynote talks,Alon Halevy,*,International Conference on Management of Data: Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,*
Scalable; Peer-Based Mediation Across XML Schemas and Ontologies,Zachary G Ives; Alon Y Halevy; Peter Mork; Igor Tatarinov,Summary Research on the Semantic Web has focused on reasoning about data that issemantically annotated in the RDF data model; with concepts and properties specified inrich ontology languages such as OWL. However; to flourish; the Semantic Web needs toprovide interoperability both between sites with different ontologies and with existing; non-RDF data and the applications operating on them. To achieve this; we are faced with twoproblems. First; most of the world's data is available not in RDF but in XML; XML and theapplications consuming it rely not only on the domain structure of the data; but also on itsdocument structure. Hence; to provide interoperability between such sources; we must mapbetween both their domain structures and their document structures. Second; datamanagement practitioners often prefer to exchange data through local point-to-point data …,*,2006,*
1 Department of Computer and Information Science; University of Pennsylvania; Philadelphia; United States zives@ cis. upenn. edu 2 Department of Computer Scie...,Zachary G Ives; Alon Y Halevy; Peter Mork; Igor Tatarinov,Summary. Research on the Semantic Web has focused on reasoning about data that issemantically annotated in the RDF data model; with concepts and properties specified inrich ontology languages such as OWL. However; to flourish; the Semantic Web needs toprovide interoperability both between sites with different ontologies and with existing; non-RDF data and the applications operating on them. To achieve this; we are faced with twoproblems. First; most of the world's data is available not in RDF but in XML; XML and theapplications consuming it rely not only on the domain structure of the data; but also on itsdocument structure. Hence; to provide interoperability between such sources; we must mapbetween both their domain structures and their document structures. Second; datamanagement practitioners often prefer to exchange data through local point-to-point data …,Semantic Web and Peer-to-Peer: Decentralized Management and Exchange of Knowledge and Information,2005,*
Published online: 8 March 2005 M. Tamer Özsu: Coordinating Editor-in-Chief,Alon Y Halevy; Zachary G Ives; Dan Suciu; Igor Tatarinov; Avigdor Gal; Ateret Anaby-Tavor; Alberto Trombetta; Danilo Montesi; Dengfeng Gao; Christian S Jensen; Richard T Snodgrass; Michael D Soo; Yannis Tzitzikas; Nicolas Spyratos; Panos Constantopoulos,. One of the challenging problems that Web service technology faces is the ability toeffectively discover services based on their capabilities. We present an approach to tacklingthis problem in the context of description logics (DLs). We formalize service discovery as anew instance of the problem of rewriting concepts using terminologies. We call this newinstance the best covering problem. We...,The VLDB Journal,2005,*
Digital library information-technology infrastructures,Γιάννης Ιωαννίδης; David Maier; Serge Abiteboul; Gerhard Weikum; Hans Schek; Fausto Rabitti; Craig Knoblock; Alon Halevy; Edward A Fox; Susan Davidson; Peter Buneman; Yannis Ioannidis,This paper charts a research agenda on systems-oriented issues in digital libraries. Itfocuses on the most central and generic system issues; including system architecture; user-level functionality; and the overall operational environment. With respect to user-levelfunctionality; in particular; it abstracts the overall information lifecycle in digital libraries tofive major stages and identifies key research problems that require solution in each stage.Finally; it recommends an explicit set of activities that would help achieve the research goalsoutlined and identifies several dimensions along which progress of the digital library fieldcan be evaluated.,*,2005,*
Editors and Staff; 2005,Moshe Tennenholtz; Toby Walsh,*,*,2005,*
KNOWLEDGE AND DATA ENGINEERING,B Cui; BC Ooi; J Su; KL Tan; YK Woon; WK Ng; EP Lim; KL Tan; AY Halevy; ZG Ives; J Madhavan; P Mork; D Suciu; I Tatarinov; S Shah; K Ramamritham; P Shenoy; HT Shen; Y Shu; B Yu; E Bertino; E Ferrari; AC Squicciarini; L Xiong; L Liu; K Aberer; A Datta; M Hauswirth,CONCISE PAPERS Databases Main Memory Indexing: The Case for BD-Tree B. Cui; BCOoi; J. Su; and K.-L. Tan ................................................................................................................................ Data Mining A Support-Ordered Trie for Fast Frequency Itemset Discovery Y.-K. Woon;W.-K. Ng; and E.-P. Lim … SPECIAL SECTION ON PEER-TO-PEER-BASED DATA MANAGEMENTGuest Editors' Introduction: Special Section on Peer-to-Peer-Based Data Management BC Ooiand K.-L. Tan ...................................................................................................................................................... The Piazza Peer Data Management System AY Halevy; ZG Ives …,*,2004,*
SEMEX: Mining for Personal Information Integration,Xin Dong; Alon Halevy; Ema Nemes; Stephan B Sigurdsson; Pedro Domingos,Abstract. Personal information management is one of the key applications of the semanticweb. Whereas today's devices store data according to applications; ideal personalinformation management system should treat all data as a set of meaningful objects andassociations between the objects. To ensure extensibility; a personal informationmanagement system should automatically incorporate associations generated in multipleways: mining specific personal data sources; or integrating with external data. As a first stepin this direction; we describe the Semex system that provides a logical and integrated viewof one's personal information.,Mining for and from the Semantic Web 2004 (SWM 2004),2004,*
The Semantic Integration Workshop at ISWC 2003,A Doan; A Halevy; N Noy,*,SIGMOD RECORD,2004,*
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 The Piazza Peer Data Management System,Alon Y Halevy; Zachary G Ives; Jayant Madhavan; Peter Mork; Dan Suciu; Igor Tatarinov,Abstract Intuitively; data management and data integration tools should be well-suited forexchanging information in a semantically meaningful way. Unfortunately; they suffer fromtwo significant problems: they typically require a comprehensive schema design before theycan be used to store or share information; and they are difficult to extend because schemaevolution is heavyweight and may break backward compatibility. As a result; many small-scale data sharing tasks are more easily facilitated by nondatabase-oriented tools that havelittle support for semantics. The goal of the peer data management system (PDMS) is toaddress this need: we propose the use of a decentralized; easily extensible datamanagement architecture in which any user can contribute new data; schema information; oreven mappings between other peers' schemas. PDMSs represent a natural step beyond …,*,2004,*
iMAP,Robin Dhamankar; Yoonkyong Lee; AnHai Doan; Alon Halevy; Pedro Domingos,Abstract Creating semantic matches between disparate data sources is fundamental tonumerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. Todate; however; virtually all of these works deal only with one-to-one (1-1) matches; such asaddress= location. They do not consider the important class of more complex matches; suchas address= concat (city; state) and room-price= room-rate*(1+ tax-rate). We describe theiMAP system which semi-automatically discovers both 1-1 and complex matches. iMAPreformulates schema matching as a search in an often very large or infinite match space. Tosearch effectively; it employs a set of searchers; each discovering specific types of complexmatches. To further improve matching accuracy; iMAP exploits a variety of domain …,Proceedings of the ACM SIGMOD International Conference on Management of Data; SIGMOD 2004,2004,*
Semantic Integration Workshop (SI-2003),AnHai Doan; Alon Halevy; Natasha Noy,In numerous distributed environments; including today's World-Wide Web; organizationalintranets; and the emerging Semantic Web; the applications will inevitably use theinformation described by multiple ontologies and schemas. Interoperability amongapplications depends critically on the ability to map between them. Today; matchingbetween ontologies and schemas is still largely done by hand; in a laborintensive and error-prone process. As a consequence; semantic integration issues have now become a keybottleneck in the deployment of a wide variety of information management applications. Thehigh cost of this bottleneck has motivated numerous research activities on methods fordescribing mappings; manipulating them; and generating them semi-automatically. Thisresearch has spanned several communities (Databases; AI; WWW); but unfortunately …,*,2003,*
Next Generation Information Technologies and Systems: 5th International Workshop; NGITS 2002; Caesarea; Israel; June 24-25; 2002. Proceedings,Alon Halevy; Avigdor Gal,NGITS2002 was the? fth workshop of its kind; promoting papers that discuss newtechnologies in information systems. Following the success of the four p-vious workshops(1993; 1995; 1997; and 1999); the? fth NGITS Workshop took place on June 24–25; 2002; inthe ancient city of Caesarea. In response to the Call for Papers; 22 papers were submitted.Each paper was evaluated by three Program Committee members. We accepted 11 papersfrom 3 continents and 5 countries; Israel (5 papers); US (3 papers); Germany; Cyprus; andThe Netherlands (1 paper from each). The workshop program consisted of? ve papersessions; two keynote lectures; and one panel discussion. The topics of the paper sessionsare: Advanced Query Processing; Web Applications; Moving Objects; Advanced InformationModels; and Advanced Software Engineering. We would like to thank all the authors who …,*,2003,*
The Lowell Database Research Self Assessment,Stan Zdonik; Jennifer Widom; Gerhard Weikum; Jeff Ullman; Rick Snodgrass; Mike Stonebraker; Avi Silberschatz; Timos Sellis; Hans Schek; Jeff Naughton; David Maier; Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk,Abstract A group of senior database researchers gathers every few years to assess the stateof database research and to point out problem areas that deserve additional focus. Thisreport summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6;2003 in Lowell; Mass. It observes that information management continues to be a criticalcomponent of most complex software systems. It recommends that database researchersincrease focus on: integration of text; data; code; and streams; fusion of information fromheterogeneous data sources; reasoning about uncertain data; unsupervised data mining forinteresting correlations; information privacy; and self-adaptation and repair.,*,2003,*
Proceedings of the ACM SIGMOD International Conference on Management of Data: ACM SIGMOD 2003; June 9-12; 2003; San Diego; California,International Conference on Management of Data; Special Interest Group on Management of Data Association for Computing Machinery,*,*,2003,*
Special Issue on PODS 2000,James F Lynch; Todd Millstein; Alon Halevy; Marc Friedman; Marc Spielmann; Tova Milo; Dan Suciu; Victor Vianu; Zhiyuan Chen; Flip Korn; Nick Koudas; S Muthukrishnan; Stéphane Grumbach; Leonardo Tininini; Michael Benedikt; Martin Grohe; Leonid Libkin; Luc Segoufin; Pankaj K Agarwal; Lars Arge; Jeff Erickson; Jon Kleinberg; Christos Papadimitriou; Prabhakar Raghavan; Wenfei Fan; Jérôme Siméon,*,*,2003,*
Special issue on XML query processing,AY Halevy,*,*,2002,*
Peer-Data Management Systems: Plumbing for the Semantic Web,Alon Y Halevy,The vision of the Semantic Web [Berners-Lee et al. 2001] offers exciting challenges forresearch in Artificial Intelligence. In a nutshell; the Semantic Web envisions agentscoordinating complex tasks over the Intemet. The coordination is facilitated by annotating thedata and services on a web destination with rich ontologies; hence enabling semanticinteroperation. In this talk I will discuss some of the challenges raised by the Semantic Weband highlight some dangerous pitfalls. I will illustrate some of the challenges using thePiazza Peer-Data Management System [Gribble et a/. 2001](PDMS) being developed at theUniversity of Washington. At a high level; the Piazza project seeks to facilitate widescaledata sharing among cooperating communities. By data sharing; we mean that participantscan export and share schemas; base data; and data views constructed through …,*,2002,*
Next Generation Information Technologies and Systems: 5th International Workshop; NGITS 2002: Proceedings,Alon Halevy; Avigdor Gal,*,*,2002,*
The Nimble XML Data Integration System,Alon Y HaLevy,Abstract: For better or for worse; XML has emerged as a de facto standard for datainterchange. This consensus is likely to lead to increased demand for technology that allowsusers to integrate data from a variety of applications; repositories; and partners which arelocated across the corporate intranet or on the Internet. Nimble Technology has spent twoyears developing a product to service this market. Originally conceived after decades ofperson-years of research on data integration; the product is now being deployed at severalFortune-500 beta-customer sites. This abstract reports on the key challenges we faced in thedesign of our product and highlights some issues we think require more attention from theresearch community. In particular; we address architectural issues arising from designing aproduct to support XML as its core representation; choices in the design of the underlying …,Proceedings of the 17th International Conference on Data Engineering,2001,*
Ь Ц б а Сви ж и гв в в иб,Denise Draper; Alon Y Halevy; Daniel S Weld,ABSTRACT Ь гвз взйз и и ХФ з гб и иг зи в ж гж и ви ж в л аа здйж б в гж и вгаг н и и ааглзйз жз иг ви ж и и жгб к ж ин г дда и гвзИ ж дгз игж зИ в а н знзи бз л ж аг и жгзз и гждгж ивиж в и гж и д жив ж гбд в з гв и Сви жв иК Св и д зи илг н жзИ Ц б а Ь вгаг н з к агд джг й игж и з б ж иК Ыд лв жгб гк ж д жзгвЙ г и ви ж и гв ж з ж И и джг й и з в дагн и з к ж а гжийв ЙММ и Й йзигб ж з и зК Ь з зиж и ж дгжиз гв и н аа в з л в и з в г гйж джг й и в а из згб ззй зл и в ж ей ж бгж ии ви гв жгб и ж з ж гббйв инК,*,2001,*
Erratum to:``Conjunctive partial deduction: foundations; control; algorithms and experiments''[The Journal of Logic Programming 41 (1999) 231](3) 265 Dekhtyar; A....,M DucassÈ; J NoyÈ; OM Duschka; MR Genesereth; AY Levy,*,The Journal of Logic Programming,2000,*
Ambite; JL and CA Knoblock Flexible and scalable cost-based query planning in mediators: A trans-formational approach 115–161 Cohen; WW WHIRL: A word-bas...,M Craven; D DiPasquo; D Freitag; A McCallum; T Mitchell; K Nigam; S Slattery; R Krovetz; N Kushmerick; V Lesser; B Horling; F Klassner; A Raja; T Wagner; SXQ Zhang; AY Levy; DS Weld; M Perkowitz; O Etzioni,*,Artificial Intelligence,2000,*
Reminiscences on Influential Papers,Christos Faloutsos; A Levy; P O'Neil; E Simon; D Srivastava; V Vianu; G Weikum,I continue to invite unsolicited contributions to this column. (I haven't received any so far; butthe previous issue has only been out a month or so at the time of writing.) See http://www.acm.org/sigmod/record/author.html for submission guidelines … Christos Faloutsos; Carnegie MellonUniversity; christos@cs.cmu.edu … [Manfred Schroeder. Fractals; Chaos; Power Laws: Minutesfrom an Infinite Paradise. WH Freeman and Company; 1991.] … "What was the single most influentialwork for your research?" There is a handful of truly in- fluential papers: the R-tree; RAID; the AssociationRules; each started a revolution. However; I find myself citing repeatedly this masterpiecebook. Beyond George Kingsley Zipf and his famous 'law'; and beyond the milestone book byMandelbrot on fractals; Schroeder's book explains how self-similarity and power laws appearin countless phenomena; it shows how to measure the fractal dimensions; and; it gives a …,SIGMOD RECORD,2000,*
Next Generation Information Technologies and Systems: 4th International Workshop; NGITS'99 Zikhron-Yaakov; Israel; July 5-7; 1999 Proceedings,Ron Y Pinter,The Next Generation Information Technologies and Systems (NGITS) wo-shop series is abiannual event held in Israel since 1993. Like its predecessors; NGITS'99 brings togetheractive members of the international research com-nity interested in information technologyand knowledge based systems. Many of the base technologies in the traditional areas ofdatabase management systems; information retrieval; and resource optimization; are beingdeployed nowadays in novel systems and applications that? ourish with the astonishingincrease in computational power; storage capacity; communication; and-of course-theadvent of the world-wide web. These new fronts; in turn; present an ever gr-ing set ofchallenges to the technologies; such as data availability; information integrity; andknowledge extraction; fuelling an exciting set of activities. Our workshop clearly re? ects …,*,1999,*
FIFTEENTH ACM SIGACT-SIGMOD-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS-Answering Queries Using Limited External Query Processors,Alon Y Levy; Anand Rajaraman; Jeffrey D Ullman,*,Journal of Computer and System Sciences,1999,*
Verifying Integrity Constraints on Web Sites,Daniela Florescu; Alon Levy; Dan Suciu,Abstract Data-intensive Web sites have created a new form of knowledge base; as richlystructured bodies of data. Several novel systems for creating dataintensive Web sitessupport declarative speci cation of a site's structure and content (ie; the pages; the dataavailable in each page; and the links between pages). Declarative systems provide aplatform on which AI techniques can be developed that further simplify the tasks ofconstructing and maintaining Web sites. This paper addresses the problem of specifying andverifying integrity constraints on a Web site's structure. We describe a language that cancapture many practical constraints and an accompanying sound and complete veri cationalgorithm. The algorithm has the important property that if the constraints are violated; itproposes xes to either the constraints or to the site de nition. Finally; we establish tight …,In IJCAI,1999,*
Is Web-site Management a Database Problem?,Daniela Florescu; Alon Levy; Dan Suciu,Several recent events have heatedly discussed the applicability of database technology tothe Internet and the World-Wide Web (eg; Dewitt's VLDB-95 talk; a 1996 DIMACS Web/DBworkshop; ICDE-98 panel). One of the areas that has emerged from these discussions as acandidate for impact of the database community is that of of Web site construction andmanagement. In parallel; several research projects have been started with the goal ofaddressing this problem (eg; Strudel (AT&T Research); Araneus (University of Rome); YAT(INRIA; France) and WebOQL (University of Toronto)). The common theme of these projectsis the declarative management of the content and structure of web sites. These projects feedoff previous relevant work on management of semistructed data and on data integration. Inaddition to the research activity; there has been a flurry of activity among database …,VLDB,1998,*
Warehousing and Incremental Evaluation for Web Site Management,Daniela Florescu; Alon Levy; Dan Suciu,Abstract Recently; several systems have been proposed for building Web sites fromdeclarative speci cations. The salient feature of these systems is that they model both thestructure and the content of a Web site as a view over some existing raw data. In this\Website as view" paradigm; a critical issue is when to compute the site. One approach is tomaterialize the site completely before the users browse it: the disadvantages of thisapproach are high maintenance costs and stale data. Moreover; this approach is notapplicable if the Web site has forms; because then the queries de ning the Web sitedepends on user input. The other extreme approach is to precompute only the root (s) of theWeb site; and then compute dynamically (at\click time") the queries that retrieves theinformation required to display the next page. In this paper we address the problem of …,*,1998,*
XML Query and transformation language.,Adam Bosworth; Andrew Layman; Adriana Ardeleanu; David Schach,We believe that it will be enormously useful to have a single language for moving any type ofinformation around the web and have worked hard to enable XML to be this language.Similarly; we believe that it will be enormously useful to have a single language for queryingXML. We further believe that in the web; it will not be practical for data providers to exposetheir underlying physical implementations of storage as SQL; for example; does todaybecause:,QL,1998,*
Proceedings of the International Workshop on Description Logics [DL97]; Gif sur Yvette; September 27-29; 1997,MC Rousset; R Brachman; F Donini; E Franconi; I Horrocks; A Levy,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,Proceedings of the International Workshop on Description Logics [DL97],1997,*
Recursive Plans for Information Gathering,Alon Y Levy,Abstract Generating query-ansWering plans for information gathering agents requires totranslate a user query; formulated in terms of a set of virtual relations; to a query that usesrelations that are actually stored in information sources. Previous solutions to the translationproblem produced sets of conjunctive plans; and Were therefore limited in their ability tohandle information sources With binding-pattern limitations; and to exploit functionaldependencies in the domain model. As a result; these plans Were incomplete Wrt sourcesencountered in practice (ie; produced only a subset of the possible ansWers). We describethe novel class of recursive information gathering plans; Which enables us to settle tWoopen problems. First; We describe an algorithm for ﬁnding a query plan that produces themaximal set of ansWers from the sources in the presence of functional dependencies …,*,1997,*
A. Hossain and KS Ray; An extension of QSIM with qualitative curvature,AY Levy; Y Iwasaki; R Fikes,*,Artificial Intelligence,1997,*
Switching Terminologies-Creating a New View of An Old World: Preliminary Report.,Catriel Beeri; Alon Y Levy; Marie-Christine Rousset,Several advanced applications of database systems require the modeling; maintenance;and usage of large collections of views. Prime examples include mediator systems thatprovide access to multiple information sources; data mining and archeology; mobiledatabases; data warehouses; and decision support systems. Furthermore; some databasevendors are considering the maintenance of materialized views also as a means for queryoptimization. As a result; problems concerning materialized views have recently received alot of attention in the database community. A view is essentially a query. If the answers to thequery are physically maintained; the view is said to be materialized. Naturally; when thereare many views (either materialized or not) there are complex relationships between theanswers to di erent views. For example; one view may be guaranteed to be a superset of …,Description Logics,1996,*
INFONA-science communication portal,Michael E Mura; Nicholas C Handy,Summary. An evaluation of the coulomb integral for a cuboid with uniform density ispresented in analytic form; leading to the development of non-overlapping cube basisfunctions. The coulomb energy of the hydrogen molecule is determined with these functionsfitted to the molecular orbital; and this result is compared with the ab initio coulomb energy.,Theoretical Chemistry Accounts,1995,*
Irrelevance reasoning in knowledge-based systems(Ph. D. Thesis),ALONYITZCHAK LEVY,*,*,1993,*
Speeding Up Inferences in Large Knowledge Bases,Alon Y Levy; Richard E Fikes; Yehoshua Sagiv,Abstract Speeding up inferences made from large knowledge bases is a key to scaling up AIsystems. The query-tree is a powerful tool for analyzing KBs containing Horn rules whichtakes into consideration the semantics of interpreted predicates that appear in the rules (eg;order and sort predicates). It is a nite structure that encodes all derivations of a given set ofqueries and tells us which rules and ground facts can be used in deriving answers to thequeries and which can be ignored. This paper investigates experimentally the impact ofseveral methods of employing the query-tree on speeding up inference. Speedups areobtained by creating specialized indices that point only to relevant facts in the KB and byfollowing only sequences of rule applications that are allowed by the query-tree. Theexperiments show that signi cant speedups (often orders of magnitude) are obtained by …,*,1993,*
Acquiring (Ir) relevance Knowledge for Problem Solving,Alon Y Levy; Yumi Iwasaki; Hiroshi Motoda,Abstract A major drawback of arti cial intelligence systems that rely on declarativerepresentations is that the e ciency of reasoning degrades quickly as the size of theknowledge base increases. To address this problem when building a system; we need toacquire not only knowledge about the domain; but also knowledge about control ofreasoning. In this paper; we discuss one type of such control knowledge; namely; relevanceof our knowledge to speci c problem solving goals. We show how this knowledge can beused by the problem solver either to ignore part of its knowledge or to automatically createabstractions and how the system can guide the acquisition of such knowledge. We groundour discussion in a framework in which knowledge about relevance can be stated; reasonedwith and analyzed. We apply the framework to the problem of modeling physical devices …,*,1992,*
Exploiting (ir) relevence to Guide Problem-solving,Alon Y Levy,*,*,1991,*
Foundations for Information Integration,Serge Abiteboul; Dana Florescu; Alon Levy; Guido Moerkotte,We are currently witnessing an explosion in the amount of information that is available on-line (eg; sources on the Internet; company-wide intranets; etc). Providing easy and efficientaccess to this information| known as the problem of data integration raises an importantchallenge to several fields of Computer Science including Database Systems; ArtificialIntelligence; Operating Systems; Networking and Human Computer Interaction. Thechallenge is to develop techniques for providing uniform access to the wealth of availableinformation. Usually; data integration is achieved by providing the user a mediated schemathat hides the details of each of the data sources; and lets the user focus on specifying whathe wants; rather than specifying how or where to find the information. The data integrationproblem is complicated by the fact that the data sources are autonomous; employ different …,*,*,*
BIPERPEDIA,Rahul Gupta; Alon Halevy; Xuezhi Wang; Steven Euijong Whang; Fei Wu,*,*,*,*
Paul Young Retires,Tom Anderson; Brian Curless; Chris Diorio; Alon Levy,After a distinguished and influential career; 15 years of which were spent on the UW faculty;Paul Young has decided to retire. At 62 years; Young explained his objective as “wanting tospend more time doing only the things I want to do.” He will leave Seattle for his 80 acre farmin Wisconsin; where he and his wife; Debbie Joseph; a computer science professor at theUniversity of Wisconsin; will restore much of the land to its original prairie habitat. Young; agraduate of Antioch College ('59) who earned his PhD from MIT ('63); joined the UW facultyin 1983 from Purdue; where he had spent most of his early career. He was UW ComputerScience chair from 1983 to 1988. Though his leadership style emphasized consensus andcooperation; Richard Ladner claimed at Young's retirement party; that it was neverthelessdifficult having Paul as chair.“None of us who worked with him ever learned to read Paul's …,*,*,*
What Is the Deep Web?,Alon Halevy; Jayant Madhaven,THE TERM “DEEP WEB” REFERS TO WEB CONTENT THAT LIES HIDDEN BEHIND HTMLFORMS. IN ORDER to get to such content; a user has to perform a form submission withvalid input values. Take; for example; the store locator form in Figure 9-1. Searching forstores in the zip code 94043 results in a web page with a listing of stores. The result page isan example of a web page in the Deep Web.,Edited by Toby Segaran and Jeff Hammerbacher,*,*
Dime qué datos consultas y te diré quién eres,Alon Halevy,La World Wide Web contiene una inabarcable cantidad de información y estructuras dedatos con una diversidad de categorías; desde hobbies hasta productos; informaciónpersonal; datos estadísticos o enciclopédicos. Datos que no sólo son texto o números sinoque también conforman mapas; gráficos o visualizaciones sofisticadas. Sin embargo; esosdatos aislados son sólo una porción de lo que nos interesa en la práctica.,*,*,*
Web Data,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang WebTables,Since the previous edition of this collection; the World Wide Web has unequivocally laid anylingering questions regarding its longevity and global impact to rest. Several multi-Billion-user services including Google and Facebook have become central to modern life in the firstworld; while Internet-and Web-related technology has permeated both business andpersonal interactions. The Web is undoubtedly here to stayat least for the foreseeable future.Web data systems bring a new set of challenges; including high scale; data heterogeneity;and a complex and evolving set of user interaction modes. Classical relational databasesystem designs did not have the Web workload in mind; and are not the technology ofchoice in this context. Rather; Web data management requires a melange of techniquesspanning information retrieval; database internals; data integration; and distributed …,*,*,*
ICDE ‘98 Program Committee Members,Nabil Adam; Gustavo Alonso; BR Badrinath; Sujata Banerjee; Lubomir F Bit; Alexandros Biliris; Patrick Bobbie; Michael H Boehlen; Arbee LP Chen; Ming-Syan Chen; Boris Chidlosvkii; Munir Cochinwala; Robert Demolombe; Suzanne Dietrich; Klaus Dittrich; Asuman Dogac; Maggie Dunham; Curtis Dyreson; Ophir Frieder; Narain Gehani; Shahram Ghandeharizadeh; Joachim Hammer; Jiawei Han; Ralf Hartmut Gueting; Waqar Hasan; Sandra Heiler; HV Jagadish; Yahiko Kambayashi; Vijay Kumar; Alon Levy,*,*,*,*
Guest Editorial Peter Tarczy-Hornoch; Mia K. Markey; John A. Smith; Tadaaki Hiruki. Bio* Medical informatics and genomic medicine: Research and training...................,Brenton Louie; Peter Mork; Fernando Martin-Sanchez; Alon Halevy; Peter Tarczy-Hornoch Data; Nicholas Sioutos; Sherri de Coronado; Margaret W Haber; Frank W Hartel; Wen-Ling Shaiu,*,*,*,*
Ben-Eliyahu-Zohary; R. and L. Palopoli Reasoning with minimal models: efficient algorithms and applica-tions Castillo; E.; C. Solares and P Gomez Tail uncertainty...,A Hossain; KS Ray; M Kaminski; AY Levy; Y Iwasaki; R Fikes; V Lifschitz,*,*,*,*
Special Issue: Bio* Medical Informatics,Brenton Louie; Peter Mork; Fernando Martin-Sanchez; Alon Halevy; Peter Tarczy-Hornoch Data,*,*,*,*
ICDE ‘98 General Chair’s Message,S Yu Philip,*,*,*,*
Declarative Specification of Web Sites with,Mary Fernandez; Daniela Florescu; Alon Levy; Dan Suciu,*,*,*,*
Mining Structures for Semantics,Alon Halevy; Jayant Madhavan; Xin Dong,Online data is available in two flavors: unstructured data that resides as free text in HTMLpages; and structured data that resides in databases and knowledge bases. Unstructureddata is easily accessed as human-readable text on a browser; while structured data ishidden behind web query interfaces (web forms); web services; and custom database APIs.Access to this data; popularly refered to as the hidden web; entails submitting correctlycompleted web forms or writing source code using protocols such as SOAP. Unstructuredtext while being human readable; is not readily machine understandable. The need toaccurately identify semantics from natural language text makes is very hard to automaticallyprocess such data. Structured data; with an accompanying schema defining its semantics;can be automatically processed using existing rich query languages.(Luna: Suggest …,*,*,*
Data Engineering,Amitanand Aiyer; Mikhail Bautin; Guoqiang Jerry Chen; Pritam Damania; Prakash Khemani; Kannan Muthukkaruppan; Karthik Ranganathan; Nicolas Spiegelberg; Liyin Tang; Madhuwanti Vaidya; Arvind Arasu; Surajit Chaudhuri; Zhimin Chen; Kris Ganjam; Raghav Kaushik; Vivek Narasayya; Michael J Carey Bu; Joshua Rosen; Neoklis Polyzotis; Tyson Condie; Markus Weimer; Raghu Ramakrishnan; Ken Goodhope; Joel Koshy; Jay Kreps; Neha Narkhede; Richard Park; Jun Rao; Victor Yang Ye; Sreeram Balakrishnan Madhavan; Kathryn Brisbin; Hector Gonzalez; Nitin Gupta; Alon Halevy; Karen Jacqmin-Adams; Heidi Lam; Anno Langen; Hongrae Lee; Rod McChesney; Rebecca Shapley; Warren Shen,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Querying and Searching a Dataspace,Xin Dong; Alon Halevy; Jing Liu,Many data management applications involve managing dataspaces; which are largecollections of highly heterogeneous data sources and partially unstructured data [5]. Thedistinguishing aspect of dataspaces from data integration systems is that dataspaces do notnecessarily include semantic mappings between data sources; and hence the data sourcesare only loosely coupled. Examples of dataspaces are ubiquitous; including enterprises andgovernment agencies; personal information on the desktop or “smart homes”; digital librariesand scientific data. The goal of Dataspace Support Platforms (DSSPs) is to provide a set ofservices that recur in many dataspace contexts; such as search and query; discovery ofsources; and dataspace evolution. The key idea behind DSSPs is a “pay as you go”approach to integration: provide some services from the outset; and evolve the …,*,*,*
Convergent Query Processing,Zachary G Ives; Alon Y Halevy; Daniel S Weld,Abstract The widely used paradigm of cost-based optimization divides query processing intoseparate optimization and execution stages. Performance is highly dependent on pre-existing statistics; and cumulative errors in the cost model rapidly increase with thecomplexity of the query. In response; researchers have investigated a variety of adaptivequery processing techniques—incorporating feedback into a system so it can adapt thequery plan being executed. While these techniques are effective in certain situations; mostare restricted to SPJ queries or limited in when they can trigger adaptive behavior; eachfocuses on a specific type of adaptivity; and there is no easy way to combine the techniquesto form a comprehensive solution. This paper presents convergent query processing; anovel form of adaptive query processing that subsumes previous approaches and …,*,*,*
Intelligent Internet Systems,Daniel S Weld; Alon Levy,*,*,*,*
Reasoning with Aggregation Constraints in Views,Shaul Dar; HV Jagadish; Alon Y Levy; Divesh Srivastava,Abstract We investigate the problem of using materialized views to compute answers to SQLqueries with grouping and aggregation; in the presence of multiset tables. This problem isimportant in many applications; such as data warehousing; mobile computing; globalinformation systems; and maintaining physical data independence; where access to local orcached materialized views may be cheaper than access to the underlying database. Inaddition; this problem has obvious potential in optimizing query evaluation. The problem isformally stated as nding a rewriting of an SQL query Q where the materialized views occur inthe FROM clause; and the rewritten query is multiset-equivalent to Q. First; we study the casewhere the query has grouping and aggregation but the views do not; and show that usabilityof a view in evaluating a query essentially requires an isomorphism between the view …,*,*,*
Overview of Strudel-A Web-Site Management System,Mary Fernandez—Daniela Florescu—Jaewoo Kang; Alon Levy; Dan Suciu,ABSTRACT. The Strudel system applies concepts from database management systems tothe process of building Web sites. Strudel's key idea is separating the management of thesite's data; the creation and management of the site's structure; and the visual presentationof the site's pages. First; the site builder creates a uniform model of all data available at thesite. Second; the builder uses this model to declaratively define the Web site's structure byapplying a “sitedefinition query” to the underlying data. Third; the builder specifies the visualpresentation of pages in Strudel's HTML-template language. The data model underlyingStrudel is a semistructured model of labeled directed graphs.,*,*,*
Inria Roquencourt dana@ rodin. inria. fr,Daniela Florescu; Alon Levy; Alberto Mendelzon,*,*,*,*
Discussion of: Forget It!,Fangzhen Lin; Ray Reiter; Alon Y Levy,I. Although the definition given for the notion of forgetting is rather intuitive; are there anydesiderata from any definition of forgetting? ie; are there any intuitive properties that wewould like any definition to satisfy? How does the notion of forgetting differ from othernotions investigated in the context of belief revision? In particular; howdoes forgetting differfrom belief erasure[Katsuno and Mendelzon; 1991] and belief contraction [Alchourron et al.;1985]? 2. Clearly; one motivation for investigating the notion of forgetting is to obtain atheory with which it is easier to reason. Can it be shown that theories resulting fromforgetting sentences are (in some sense) simpler?(eg; not much larger; or do not requiremore expressive language).,*,*,*
CARIN: Description Logics as a Constraint Language in Horn Rule Languages,Alon Y Levy; Marie-Christine Rousset,Abstract We describe carin; a novel family of hybrid declarative programming languages;that extend the expressive power of Horn rule languages by incorporating description logicsas a constraint sublanguage. Description logics are declarative object-oriented languagesthat have been designed especially for the purpose of representing domains with richhierarchical structure; and have been used in several applications. We address the keyissue in such a hybrid language like carin; which is to develop a sound and completeprocedure for making inferences. We identify the problem of existential entailment; which isat the core of several reasoning algorithms for carin. Our main result is a an existentialentailment algorithm for the expressive language of ALCN R-carin. This algorithm entailsseveral results. In particular; it provides a sound and complete inference procedure for …,*,*,*
Panel Session,Alon Levy; Susan Malaika; Santa Teresa; Albert Mendelson,Page 1. Panel Session 1 WW and the Internet - Did We (the Database Community) Miss theBoat? Chair Michael Rabinovich; AT&T Laboratories Panelists Mic Bowman; Transarc AlonLevy; University of Washington Susan Malaika; IBM Santa Teresa Lab Albert0 Mendelson;University of Toronto Hector Garcia-Molina; Stanford University,*,*,*
Towards E cient Information Gathering Agents,Alon Y Levy; Yehoshua Sagiv; Divesh Srivastava,Abstract Information gathering agents are required in many software agent applications toanswer queries; posed by other agents; using a variety of available information sources. Weformally consider the problem of designing information gathering agents; and make twoimportant contributions. First; we examine the key issue of integrating knowledge fromexternal sites into our knowledge base; and present an expressive language for thispurpose. A noteworthy feature of our language is its ability to capture the knowledge thatsome external sites have complete information of a certain kind; using rich semanticconstraints. Given a query on the knowledge base; it is important for the agent to rstdetermine the set of external sites that contain information relevant to answering the query;and then access those sites. Our second contribution is to show that; given a query and …,*,*,*
Bulletin of the Technical Committee on,Sirish Chandrasekaran; Amol Deshpande; Kris Hildrum; Sam Madden; Vijayshankar Raman; Mehul A Shah; Zachary G Ives; Alon Y Levy; Daniel S Weld; Daniela Florescu; Marc Friedman,Bulletin of the Technical Committee on Ø Ò Ò Ö Ò June 2000 Vol. 23 No. 2 IEEE Computer SocietyLetters Letter from the Editor-in-Chief...................................................... David Lomet 1 Letter fromthe Special Issue Editor...................................................... Alon Levy 2 Special Issue on AdaptiveQuery Processing Dynamic Query Evaluation Plans: Some Course Corrections?.............................. Goetz Graefe 3 Adaptive … Editorial Board Editor-in-Chief David B. Lomet MicrosoftResearch One Microsoft Way; Bldg. 9 Redmond WA 98052-6399 lomet@ microsoft. com AssociateEditors Luis Gravano Computer Science Department Columbia University 1214 Amsterdam AvenueNew York; NY 10027 Alon Levy University of Washington Computer Science and EngineeringDept. Sieg Hall; Room 310 Seattle; WA 98195 Sunita Sarawagi School of Information TechnologyIndian Institute of Technology; Bombay Powai Street Mumbai; India 400076 Gerhard …,*,*,*
AI Principles Research Department; AT&T Laboratories 600 Mountain Avenue; Room 2C-406 Murray Hill; NJ; 07974; USA email: levy@ research. att. com &,Alon Levy; Marie-Christine Rousset,*,*,*,*
Data Complexity of Query An-swering in Description Logics; 260 Dataspaces: Co-existence with Heterogeneity; 3,Natasha Alechina; Arnon Avron; Jorge A Baier; Jonathan Ben-Naim; Brandon Bennett; Philippe Besnard; Meghyn Bienvenu; Lawrence E Blume; Piero Bonatti; Blai Bonet; Richard Booth; Gerhard Brewka; Pedro Cabalar; Diego Calvanese; Yin Chen; Yann Chevaleyre; Jens Claßen; Ernesto Compatangelo; Willem Conradie; Sylvie Coste-Marquis; Madalina Croitoru; Giuseppe De Giacomo; James Delgrande; Caroline Devred; Jon Doyle; Didier Dubois; Florence Dupin de Saint-Cyr; David A Easley; Thomas Eiter; Ulle Endriss; Selim T Erdogan; Even More Irresistible SROIQ; Hélène Fargier; Robert Feldmann; Michael Fink; Michael Franklin; Christian Fritz; Benoit Gaudou; Héctor Geffner; Silvio Ghilardi; Angelo Gilio; Bernardo Cuenca Grau; Rolf Haenni; Alon Halevy; Joseph Y Halpern; Andreas Herzig; Ian Horrocks; Anthony Hunter; Iterated Revision as Prioritized; Victor Jauregui; Aditya Kalyanpur; Gabriele Kern-Isberner; Sébastien Konieczny; Oliver Kutz,*,*,*,*
Workshop Officers,Laura Haas; Zachary Ives; Mukesh Mohania; Manish Bhide; Divy Agrawal; Phil Bernstein; Kevin Chang; Yi Chen; Alin Deutsch; AnHai Doan; Alon Halevy; Mizuho Iwaihara; Masaru Kitsuregawa; Craig Knoblock; Sergey Melnik; Ullas Nambiar; Felix Naumann; Evaggelia Pitoura; Prasan Roy; Michael Schrefl; Kohichi Takeda; Wang-Chiew Tan; Millist Vincent; Ji-Rong Wen,*,*,*,*
NGITS 2002: next generation information technologies and systems(Caesarea; 24-25 June 2002),Alon Halevy; Avigdor Gal,*,Lecture notes in computer science,*,*
Corpus-based Schema Matching,Jay ant Madhavan; Philip Bernstein; Kuang Chen; Alon Halevy; Pradeep Shenoy,Abstract Schema matching is the problem of determining a set of correspondences thatidentify similar elements in two different schémas. In this paper we propose a novel methodfor matching schémas that leverages previous matching experiences by extractingknowledge from a corpus of known schémas and mappings; and applying this knowledge tomatch new schémas. We describe the Mapping Knowledge Base that captures theknowledge gleaned from the past; methods to apply the gleaned knowledge to newmatching tasks; and interesting tradeoffs in building such a system. Finally; we presentpreliminary experimental results that demonstrate that the use of past experience can resultin an improvement in the generated matches.,*,*,*
Evaluating Temporal Graphs Built from Texts via Transitive Reduction............................... 375 X. Tannier; P. Muller On-Line Planning and Scheduling: An Application t...,A Rahman; V Ng; R He; E Brunskill; N Roy; IA Kash; EJ Friedman; JY Halpern; V Aravantinos; R Caferra; N Peltier; L Bordeaux; G Katsirelos; N Narodytska; MY Vardi; F Wu; J Madhavan; A Halevy,Non-Deterministic Policies in Markovian Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . 1 M. Milani Fard; J. Pineau A Logical Study of Partial Entailment . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Y. Zhou; Y. Zhang False-Name Manipulationsin Weighted Voting Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 H. Aziz; Y.Bachrach; E. Elkind; M. Paterson … Computing Small Unsatisfiable Cores in SatisfiabilityModulo Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701 A. Cimatti; A. Griggio; R. Sebastiani ExploitingStructure in Weighted Model Counting Approaches to Probabilistic Inference . . . . . . . . . . . . . .. 729 W. Li; P. Poupart; P. van Beek Scaling up Heuristic Planning with Relational Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 767 T. de la Rosa; S. Jiménez; R. Fuentetaja; D. BorrajoRegression Conformal Prediction with Nearest Neighbours …,*,*,*
Data Integration,Alon Halevy,Data integration systems offer uniform access to a set of autonomous and heterogeneousdata sources. Sources can range from database systems and legacy systems to forms on theWeb; web services and flat files. The data in the sources need not be completely structuredas in relational databases. The number of sources in a data integration application canrange from a handful to thousands.,*,*,*
ICDM 2005 Workshop Proposal,Katy Borner; Alon Halevy; H Jagadish; Munindar Singh,Recent advances in high performance computing; high speed and high bandwidthcommunication; massive storage; and software (eg; web services) that can be remotelyinvoked on the Internet present unprecedented opportunities in data-driven knowledgeacquisition in a broad range of applications in virtually all areas of human endeavorincluding collaborative cross-disciplinary discovery in e-science; bioinformatics; e-government; environmental informatics; health informatics; security informatics; e-business;education; social informatics; among others. Given the explosive growth in the number anddiversity of potentially useful information sources in many domains; there is an urgent needfor sound approaches to integrative and collaborative analysis and interpretation ofdistributed; autonomous (and hence; inevitably semantically heterogeneous) data …,*,*,*
DIMACS Workshop on Algorithms in the Field,Deepak Agarwal; Stephen E Fienberg; Sergei Vassilvitskii,Algorithmically matching items to users in a given context is essential for the success andprofitability of large scale recommender systems like content optimization; computationaladvertising; search; shopping; movie recommendation; and many more. The objective is tomaximize some utility (eg total revenue; total engagement) of interest over a long timehorizon; hence this is a bandit problem since there is positive utility in displaying items thatmay have low mean but high variance. A key challenge in such bandit problems is the curseof dimensionality---data is obtained via interactions among several heavy-tailed categoricalvariables and the goal is to estimate response rates of some rare event like click; buy; andso on. Other than statistical challenges due to high dimensionality and data sparsity; banditproblems are difficult to work with for response that are observed with considerable delay …,*,*,*
Schemr: a Schema Search Engine for Information Integration,Kuang Chen; Jayant Madhavan; Alon Halevy,Schemr is a schema search engine; and provides users the ability to search for andvisualize schemas stored in a metadata repository. Users may search by keywords and byexample–using schema fragments as query terms. Schemr uses a novel search algorithm;based on a combination of text search and schema matching techniques; as well as astructurally-aware scoring metric. Schemr presents search results in a GUI that allows usersto explore which elements match and how well they do. The GUI supports interactions;including panning; zooming; layout and drilling-in. We demonstrate schema search andvisualization; introduce Schemr as a new component of the information integration toolbox;and discuss its benefits in several applications.,*,*,*
Ullas Nambiar (IBM India Research Lab; New Delhi; India); Cochair Zaiqing Nie (Microsoft Research Asia; Beijing; PR China); Cochair,Alon Halevy; Kevin Chen-Chuan Chang; Subbarao Kambhampati; Avigdor Gal; Andrew McCallum; Biplav Srivastava; Bing Liu; Craig Knoblock; Chen Li; Felix Naumann; Ganesh Ramakrishnan; Gautam Das; Hasan Davulcu; Ji-Rong Wen; Kamal Karlapalem; Louiqa Raschid; Michael Cafarella; Misha Bilenko; Mong Li Lee; Nicholas Kushmerick; Robert Grossman; Steven Minton; Thomas Y Lee; Vanja Josifovski; Weiyi Meng; William Cohen,Alon Halevy (Google Inc. Mountain View; California; USA) Kevin Chen-Chuan Chang (Universityof Illinois at Urbana-Champaign; Illinois; USA) Subbarao Kambhampati (Arizona StateUniversity; Tempe; Arizona; USA) … Avigdor Gal (Technion – Israel Institute of Technology)Andrew McCallum (University of Massachusetts Amherst; USA) Biplav Srivastava (IBM IndiaResearch Lab) Bing Liu (University of Illinois at Chicago; USA) Craig Knoblock (University ofSouthern California; USA) Chen Li (University of California; Irvine; USA) Felix Naumann (HassoPlattner Institut; Postdam; Germany) Ganesh Ramakrishnan (IBM India Research Lab) GautamDas (University of Texas; Arlington; USA) Hasan Davulcu (Arizona State University; USA) Ji-RongWen (Microsoft Research Asia) Kamal Karlapalem (IIIT – Hyderabad; India) Louiqa Raschid(University of Maryland College Park; USA) Michael Cafarella (University of Washington …,*,*,*
Author-Title Index Volume 40; 2011,V Aravantinos; A Atserias; H Aziz; Y Bachrach; L Bordeaux; D Borrajo; E Brunskill; R Caferra; S Cardon; A Cimatti; Bounded-Width Resolution; T de la Rosa; JP Delgrande; MB Do; E Elkind; U Endriss; P Faliszewski; JK Fichte; EJ Friedman; MPJ Fromherz; R Fuentetaja; A Gammerman; C Geist; A Griggio; A Halevy; JY Halpern; CW Hang; R He; E Hemaspaandra; LA Hemaspaandra; A Hunter; M Hutter; S Jiménez; IA Kash; G Katsirelos; C Lecoutre; W Li; J Madhavan; M Milani Fard,Aravantinos; V.; 599 Atserias; A.; 353 Automated Search for Impossibility Theorems in So- cialChoice Theory: Ranking Sets of Objects; 143 Aziz; H.; 57 … Bachrach; Y.; 57 Bordeaux; L.; 657Borrajo; D.; 767 Brunskill; E.; 523 … Caferra; R.; 599 Cardon; S.; 175 Cimatti; A.; 701Clause-Learning Algorithms with Many Restarts and Bounded-Width Resolution; 353 Complexityof Integer Bound Propagation; The; 657 Computing Small Unsatisfiable Cores in SatisfiabilityModulo Theories; 701 … Decidability and Undecidability Results for Proposi- tionalSchemata; 599 de la Rosa; T.; 767 Delgrande; JP; 269 Do; MB; 415 … Efficient Planning underUncertainty with Macro-Ac- tions; 523 Elkind; E.; 57 Endriss; U.; 143 Evaluating Temporal GraphsBuilt from Texts via Transitive Reduction; 375 Exploiting Structure in Weighted Model CountingApproaches to Probabilistic Inference; 729 … False-Name Manipulations in Weighted …,*,*,*
Data Engineering,Philip A Bernstein; Nishant Dani; Badriddine Khessib; Ramesh Manne; David Shutt; Jayant Madhavan; Alon Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu; Varun Bhagwan; Mike Ching; Alex Cozzi; Raj Desai; Daniel Gruhl; Kevin Haas; Linda Kato; Jeff Kusnitz; Bryan Langston; Ferdy Nagy; Linda Nguyen; Jan Pieper; Savitha Srinivasan; Anthony Stuart; Renjie Tang,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily reflect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems. There are two DataEngineering Bulletin web sites: http://www. research. microsoft. com/research/db/debull …,*,*,*
