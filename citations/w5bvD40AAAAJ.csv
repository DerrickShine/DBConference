HaLoop: Efficient iterative data processing on large clusters,Yingyi Bu; Bill Howe; Magdalena Balazinska; Michael D Ernst,Abstract The growing demand for large-scale data mining and data analysis applicationshas led both industry and academia to design new types of highly scalable data-intensivecomputing platforms. MapReduce and Dryad are two popular platforms in which thedataflow takes the form of a directed acyclic graph of operators. These platforms lack built-insupport for iterative programs; which arise naturally in many applications including datamining; web ranking; graph analysis; model fitting; and so on. This paper presents HaLoop;a modified version of the Hadoop MapReduce framework that is designed to serve theseapplications. HaLoop not only extends MapReduce with programming support for iterativeapplications; it also dramatically improves their efficiency by making the task scheduler loop-aware and by adding various caching mechanisms. We evaluated HaLoop on real …,Proceedings of the VLDB Endowment,2010,864
The HaLoop approach to large-scale iterative data analysis,Yingyi Bu; Bill Howe; Magdalena Balazinska; Michael D Ernst,Abstract The growing demand for large-scale data mining and data analysis applicationshas led both industry and academia to design new types of highly scalable data-intensivecomputing platforms. MapReduce has enjoyed particular success. However; MapReducelacks built-in support for iterative programs; which arise naturally in many applicationsincluding data mining; web ranking; graph analysis; and model fitting. This paper (This is anextended version of the VLDB 2010 paper" HaLoop: Efficient Iterative Data Processing onLarge Clusters" PVLDB 3 (1): 285---296; 2010.) presents HaLoop; a modified version of theHadoop MapReduce framework; that is designed to serve these applications. HaLoopallows iterative applications to be assembled from existing Hadoop programs withoutmodification; and significantly improves their efficiency by providing inter-iteration caching …,The VLDB Journal—The International Journal on Very Large Data Bases,2012,131
Efficient anomaly monitoring over moving object trajectory streams,Yingyi Bu; Lei Chen; Ada Wai-Chee Fu; Dawei Liu,Abstract Lately there exist increasing demands for online abnormality monitoring overtrajectory streams; which are obtained from moving object tracking devices. This problem ischallenging due to the requirement of high speed data processing within limited space cost.In this paper; we present a novel framework for monitoring anomalies over continuoustrajectory streams. First; we illustrate the importance of distance-based anomaly monitoringover moving object trajectories. Then; we utilize the local continuity characteristics oftrajectories to build local clusters upon trajectory streams and monitor anomalies via efficientpruning strategies. Finally; we propose a piecewise metric index structure to reschedule thejoining order of local clusters to further reduce the time cost. Our extensive experimentsdemonstrate the effectiveness and efficiency of our methods.,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,126
Managing quality of context in pervasive computing,Yingyi Bu; Tao Gu; Xianping Tao; Jun Li; Shaxun Chen; Jian Lu,Context-awareness plays a key role in a paradigm shift from traditional desktop styledcomputing to emerging pervasive computing. Many context-aware systems have been builtto achieve the vision of pervasive computing and alleviate the human attention bottleneck;however; these systems are far from real world applications. Quality of context is critical inreducing the gap between existing systems and real-life applications. Aiming to provide thesupport of quality of context; in this paper; we propose a novel quality model for contextinformation and a context management mechanism for inconsistency resolution. We alsobuild a prototype system to validate our proposed model and mechanism; and to assist thedevelopment of context-aware applications. Through our evaluations and case study;context-aware applications can be built with the support of quality of context,Quality Software; 2006. QSIC 2006. Sixth International Conference on,2006,106
Combined static and dynamic automated test generation,Sai Zhang; David Saff; Yingyi Bu; Michael D Ernst,Abstract In an object-oriented program; a unit test often consists of a sequence of methodcalls that create and mutate objects; then use them as arguments to a method under test. It ischallenging to automatically generate sequences that are legal and behaviorally-diverse;that is; reaching as many different program states as possible. This paper proposes acombined static and dynamic automated test generation approach to address theseproblems; for code without a formal specification. Our approach first uses dynamic analysisto infer a call sequence model from a sample execution; then uses static analysis to identifymethod dependence relations based on the fields they may read or write. Finally; both thedynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test …,Proceedings of the 2011 International Symposium on Software Testing and Analysis,2011,97
AsterixDB: A scalable; open source BDMS,Sattam Alsubaiee; Yasser Altowim; Hotham Altwaijry; Alexander Behm; Vinayak Borkar; Yingyi Bu; Michael Carey; Inci Cetindil; Madhusudan Cheelangi; Khurram Faraaz; Eugenia Gabrielova; Raman Grover; Zachary Heilbron; Young-Seok Kim; Chen Li; Guangqiang Li; Ji Mahn Ok; Nicola Onose; Pouria Pirzadeh; Vassilis Tsotras; Rares Vernica; Jian Wen; Till Westmann,Abstract AsterixDB is a new; full-function BDMS (Big Data Management System) with afeature set that distinguishes it from other platforms in today's open source Big Dataecosystem. Its features make it well-suited to applications like web data warehousing; socialdata storage and analysis; and other use cases related to Big Data. AsterixDB has a flexibleNoSQL style data model; a query language that supports a wide range of queries; a scalableruntime; partitioned; LSM-based data storage and indexing (including B+-tree; R-tree; andtext indexes); support for external as well as natively stored data; a rich set of built-in types;support for fuzzy; spatial; and temporal types and queries; a built-in notion of data feeds foringestion of data; and transaction support akin to that of a NoSQL store. Development ofAsterixDB began in 2009 and led to a mid-2013 initial open source release. This paper is …,Proceedings of the VLDB Endowment,2014,85
Privacy preserving serial data publishing by role composition,Yingyi Bu; Ada Wai Chee Fu; Raymond Chi Wing Wong; Lei Chen; Jiuyong Li,Abstract Previous works about privacy preserving serial data publishing on dynamicdatabases have relied on unrealistic assumptions of the nature of dynamic databases. Inmany applications; some sensitive values changes freely while others never change. Forexample; in medical applications; the disease attribute changes with time when patientsrecover from one disease and develop another disease. However; patients do not recoverfrom some diseases such as HIV. We call such diseases permanent sensitive values. To thebest of our knowledge; none of the existing solutions handle these realistic issues. Wepropose a novel anonymization approach called HD-composition to solve the aboveproblems. Extensive experiments with real data confirm our theoretical results.,Proceedings of the VLDB Endowment,2008,82
Pregelix: Big (ger) graph analytics on a dataflow engine,Yingyi Bu; Vinayak Borkar; Jianfeng Jia; Michael J Carey; Tyson Condie,Abstract There is a growing need for distributed graph processing systems that are capableof gracefully scaling to very large graph datasets. Unfortunately; this challenge has not beeneasily met due to the intense memory pressure imposed by process-centric; messagepassing designs that many graph processing systems follow. Pregelix is a new open sourcedistributed graph processing system that is based on an iterative dataflow design that isbetter tuned to handle both in-memory and out-of-core workloads. As such; Pregelix offersimproved performance characteristics and scaling properties over current open sourcesystems (eg; we have seen up to 15X speedup compared to Apache Giraph and up to 35Xspeedup compared to distributed GraphLab); and more effective use of available machineresources to support Big (ger) Graph Analytics.,Proceedings of the VLDB Endowment,2014,71
Wat: Finding top-k discords in time series database,Yingyi Bu; Tat-Wing Leung; Ada Wai-Chee Fu; Eamonn Keogh; Jian Pei; Sam Meshkin,Abstract Finding discords in time series database is an important problem in a great varietyof applications; such as space shuttle telemetry; mechanical industry; biomedicine; andfinancial data analysis. However; most previous methods for this problem suffer from toomany parameter settings which are difficult for users. The best known approach to ourknowledge that has comparatively fewer parameters still requires users to choose a wordsize for the compression of subsequences. In this paper; we propose a Haar wavelet andaugmented trie based algorithm to mine the top-K discords from a time series database;which can dynamically determine the word size for compression. Due to the characteristicsof Haar wavelet transform; our algorithm has greater pruning power than previousapproaches. Through experiments with some annotated datasets; the effectiveness and …,*,2007,70
FollowMe: on research of pluggable infrastructure for context-awareness,Jun Li; Yingyi Bu; Shaxun Chen; Xianping Tao; Jian Lu,Pervasive computing is to enhance the environment by embedding many computers that aregracefully integrated with human users. To achieve this; the key research thrust is to create asmart context-awareness environment which should enclose various users and satisfydifferent needs of the users. Building such smart environments is still difficult and complexdue to lacking a uniform infrastructure that can adapt to diverse smart domains. To addressthis problem; we propose a context-aware computing infrastructure; called FollowMe. Ourinfrastructure integrates an ontology based context model and a workflow based applicationmodel with the OSGi framework. By plugging different domain contexts and applications;FollowMe can be customized to various domains.,Advanced Information Networking and Applications; 2006. AINA 2006. 20th International Conference on,2006,70
Astronomy in the cloud: using mapreduce for image co-addition,Keith Wiley; Andrew Connolly; Jeff Gardner; S Krughoff; Magdalena Balazinska; Bill Howe; Y Kwon; Yingyi Bu,Abstract In the coming decade; astronomical surveys of the sky will generate tens ofterabytes of images and detect hundreds of millions of sources every night. The study ofthese sources will involve computation challenges such as anomaly detection andclassification and moving-object tracking. Since such studies benefit from the highest-qualitydata; methods such as image co-addition; ie; astrometric registration followed by per-pixelsummation; will be a critical preprocessing step prior to scientific investigation. With arequirement that these images be analyzed on a nightly basis to identify moving sourcessuch as potentially hazardous asteroids or transient objects such as supernovae; these datastreams present many computational challenges. Given the quantity of data involved; thecomputational load of these problems can only be addressed by distributing the workload …,Publications of the Astronomical Society of the Pacific,2011,57
Scaling datalog for machine learning on big data,Yingyi Bu; Vinayak Borkar; Michael J Carey; Joshua Rosen; Neoklis Polyzotis; Tyson Condie; Markus Weimer; Raghu Ramakrishnan,Abstract: In this paper; we present the case for a declarative foundation for data-intensivemachine learning systems. Instead of creating a new system for each specific flavor ofmachine learning task; or hardcoding new optimizations; we argue for the use of recursivequeries to program a variety of machine learning systems. By taking this approach; databasequery optimization techniques can be utilized to identify effective execution plans; and theresulting runtime plans can be executed on a single unified data-parallel query processingengine. As a proof of concept; we consider two programming models--Pregel and IterativeMap-Reduce-Update---from the machine learning domain; and show how they can becaptured in Datalog; tuned for a specific task; and then compiled into an optimized physicalplan. Experiments performed on a large computing cluster with real data demonstrate that …,arXiv preprint arXiv:1203.0160,2012,47
Facade: A compiler and runtime for (almost) object-bounded big data applications,Khanh Nguyen; Kai Wang; Yingyi Bu; Lu Fang; Jianfei Hu; Guoqing Xu,Abstract The past decade has witnessed the increasing demands on data-driven businessintelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice forimplementing such applications; due to its quick development cycle and rich communityresource. While the use of such languages makes programming easier; their automatedmemory management comes at a cost. When the managed runtime meets Big Data; this costis significantly magnified and becomes a scalability-prohibiting bottleneck. This paperpresents a novel compiler framework; called Facade; that can generate highly-efficient datamanipulation code by automatically transforming the data path of an existing Big Dataapplication. The key treatment is that in the generated code; the number of runtime heap …,ACM SIGARCH Computer Architecture News,2015,46
A bloat-aware design for big data applications,Yingyi Bu; Vinayak Borkar; Guoqing Xu; Michael J Carey,Abstract Over the past decade; the increasing demands on data-driven business intelligencehave led to the proliferation of large-scale; data-intensive applications that often have hugeamounts of data (often at terabyte or petabyte scale) to process. An object-orientedprogramming language such as Java is often the developer's choice for implementing suchapplications; primarily due to its quick development cycle and rich community resource.While the use of such languages makes programming easier; significant performanceproblems can often be seen---the combination of the inefficiencies inherent in a managedrun-time system and the impact of the huge amount of data to be processed in the limitedmemory space often leads to memory bloat and performance degradation at a surprisinglyearly stage. This paper proposes a bloat-aware design paradigm towards the …,ACM SIGPLAN Notices,2013,45
Pregel algorithms for graph connectivity problems with performance guarantees,Da Yan; James Cheng; Kai Xing; Yi Lu; Wilfred Ng; Yingyi Bu,Abstract Graphs in real life applications are often huge; such as the Web graph and varioussocial networks. These massive graphs are often stored and processed in distributed sites.In this paper; we study graph algorithms that adopt Google's Pregel; an iterative vertex-centric framework for graph processing in the Cloud. We first identify a set of desirableproperties of an efficient Pregel algorithm; such as linear space; communication andcomputation cost per iteration; and logarithmic number of iterations. We define such analgorithm as a practical Pregel algorithm (PPA). We then propose PPAs for computingconnected components (CCs); biconnected components (BCCs) and strongly connectedcomponents (SCCs). The PPAs for computing BCCs and SCCs use the PPAs of manyfundamental graph problems as building blocks; which are of interest by themselves …,Proceedings of the VLDB Endowment,2014,39
Iterative mapreduce for large scale machine learning,Joshua Rosen; Neoklis Polyzotis; Vinayak Borkar; Yingyi Bu; Michael J Carey; Markus Weimer; Tyson Condie; Raghu Ramakrishnan,Abstract: Large datasets (" Big Data") are becoming ubiquitous because the potential valuein deriving insights from data; across a wide range of business and scientific applications; isincreasingly recognized. In particular; machine learning-one of the foundational disciplinesfor data analysis; summarization and inference-on Big Data has become routine at mostorganizations that operate large clouds; usually based on systems such as Hadoop thatsupport the MapReduce programming paradigm. It is now widely recognized that whileMapReduce is highly scalable; it suffers from a critical weakness for machine learning: itdoes not support iteration. Consequently; one has to program around this limitation; leadingto fragile; inefficient code. Further; reliance on the programmer is inherently flawed in a multi-tenanted cloud environment; since the programmer does not have visibility into the state …,arXiv preprint arXiv:1303.3517,2013,37
Optimal proactive caching in peer-to-peer network: analysis and application,Weixiong Rao; Lei Chen; Ada Wai-Chee Fu; YingYi Bu,Abstract As a promising new technology with the unique properties like high efficiency;scalability and fault tolerance; Peer-to-Peer (P2P) technology is used as the underlyingnetwork to build new Internet-scale applications. However; one of the well known issues insuch an application (for example WWW) is that the distribution of data popularities is heavilytailed with a Zipf-like distribution. With consideration of the skewed popularity we adopt aproactive caching approach to handle the challenge; and focus on two key problems: where(ie the placement strategy: where to place the replicas) and how (ie the degree problem:how many replicas are assigned to one specific content)? For the where problem; wepropose a novel approach which can be generally applied to structured P2P networks. Next;we solve two optimization objectives related to the how problem: MAX_PERF and …,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,36
Declarative Systems for Large-Scale Machine Learning.,Vinayak R Borkar; Yingyi Bu; Michael J Carey; Joshua Rosen; Neoklis Polyzotis; Tyson Condie; Markus Weimer; Raghu Ramakrishnan; G Dror; N Koenigstein,Abstract In this article; we make the case for a declarative foundation for data-intensivemachine learning systems. Instead of creating a new system for each specific flavor ofmachine learning task; or hardcoding new optimizations; we argue for the use of recursivequeries to program a variety of machine learning algorithms. By taking this approach;database query optimization techniques can be utilized to identify effective execution plans;and the resulting runtime plans can be executed on a single unified data-parallel queryprocessing engine.,IEEE Data Eng. Bull.,2012,32
Context consistency management using ontology based model,Yingyi Bu; Shaxun Chen; Jun Li; Xianping Tao; Jian Lu,Abstract Inconsistent contexts are death-wounds which usually result in context-awareapplications' incongruous behaviors and users' perplexed feelings; therefore the benefits ofcontext-aware computing will become less believed. This problem occurs in most sensorbased applications due to the intrinsic drawbacks of fallible physical sensors which can onlydetect some evidence of real world's situations rather than global views of them. In thispaper; we extend ontology based context modeling approach with some descriptiveinformation added to contexts; modify reasoners to support time information; bring in acontext lifecycle management strategy; establish a context exploitation mechanism; andpropose an inconsistency resolution algorithm; fostering timely; exact and conflict-freecontexts. Besides; evaluations and a case study are carried out to attest our design …,International conference on extending database technology,2006,32
ASTERIX: an open source system for big data management and analysis,Sattam Alsubaiee; Yasser Altowim; Hotham Altwaijry; Alexander Behm; Vinayak Borkar; Yingyi Bu; Michael Carey; Raman Grover; Zachary Heilbron; Young-Seok Kim; Chen Li; Nicola Onose; Pouria Pirzadeh; Rares Vernica; Jian Wen,Abstract At UC Irvine; we are building a next generation parallel database system; calledASTERIX; as our approach to addressing today's" Big Data" management challenges.ASTERIX aims to combine time-tested principles from parallel database systems with thoseof the Web-scale computing community; such as fault tolerance for long running jobs. In thisdemo; we present a whirlwind tour of ASTERIX; highlighting a few of its key features. We willdemonstrate examples of our data definition language to model semi-structured data; andexamples of interesting queries using our declarative query language. In particular; we willshow the capabilities of ASTERIX for answering geo-spatial queries and fuzzy queries; aswell as ASTERIX'data feed construct for continuously ingesting data.,Proceedings of the VLDB Endowment,2012,30
Astronomical image processing with hadoop,Keith Wiley; Andrew Connolly; Simon Krughoff; Jeff Gardner; Magdalena Balazinska; Bill Howe; Y Kwon; Yingyi Bu,5. Results Fig. 1 shows an example of image coaddition. A single r-band frame is shown onthe left,Astronomical Data Analysis Software and Systems XX,2011,20
Toward context-awareness: a workflow embedded middleware,Shaxun Chen; Yingyi Bu; Jun Li; Xianping Tao; Jian Lu,Abstract Context-aware computing is widely researched in recent years; but we lack for apowerful context-aware middleware which supports a uniform programming model. Sodeveloping context-aware applications is still complex and time-consuming. We introduce aworkflow embedded middleware called FollowMe. It supports pluggable context-awareapplications. FollowMe includes a workflow engine and sustains applications described withpvPDL; which is a workflow definition language proposed specially for context-awareness.The employment of workflow makes the development of applications simplified and themaintenance much easier. We testify the improvement by realizing an example and therelated evaluation.,International Conference on Ubiquitous Intelligence and Computing,2006,20
Big graph analytics systems,Da Yan; Yingyi Bu; Yuanyuan Tian; Amol Deshpande; James Cheng,Abstract In recent years we have witnessed a surging interest in developing Big Graphprocessing systems. To date; tens of Big Graph systems have been proposed. This tutorialprovides a timely and comprehensive review of existing Big Graph systems; andsummarizes their pros and cons from various perspectives. We start from the existing vertex-centric systems; which which a programmer thinks intuitively like a vertex when developingparallel graph algorithms. We then introduce systems that adopt other computationparadigms and execution settings. The topics covered in this tutorial include programmingmodels and algorithm design; computation models; communication mechanisms; out-of-coresupport; fault tolerance; dynamic graph support; and so on. We also highlight future researchopportunities on Big Graph analytics.,Proceedings of the 2016 International Conference on Management of Data,2016,11
Distributed maximal clique computation,Yanyan Xu; James Cheng; Ada Wai-Chee Fu; Yingyi Bu,Maximal cliques are important substructures in graph analysis. Many algorithms forcomputing maximal cliques have been proposed in the literature; however; most of them aresequential algorithms that cannot scale due to the high complexity of the problem; whileexisting parallel algorithms for computing maximal cliques are mostly immature andespecially suffer from skewed workload. In this paper; we first propose a distributedalgorithm built on a share-nothing architecture for computing the set of maximal cliques. Weeffectively address the problem of skewed workload distribution due to high-degree vertices;which also leads to drastically reduced worst-case time complexity for computing maximalcliques in common real-world graphs. Then; we also devise algorithms to support efficientupdate maintenance of the set of maximal cliques when the underlying graph is updated …,Big Data (BigData Congress); 2014 IEEE International Congress on,2014,11
An enhanced ontology based context model and fusion mechanism,Yingyi Bu; Jun Li; Shaxun Chen; Xianping Tao; Jian Lv,Abstract With diverse sensors; context-aware applications which aim at decreasing people'sattentions to computational devices are becoming more and more popular. But it isinadequate just with sensors because what applications really need is high-level contextknowledge rather than low-level raw sensor data. So a software layer which delivers highquality contexts to applications with an easy application programming model is needed. Inthis paper; we establish a formal context model using semantic web languages and design acontext fusion mechanism which not only generates high-level contexts by reasoning; butalso brings in context lifecycle management and periodically time based conflict resolutionto improve the quality of contexts. Using the context fusion mechanism; a programmable andservice oriented middleware is built upon OSGi framework to support context-aware …,International Conference on Embedded and Ubiquitous Computing,2005,10
Algebricks: A data model-agnostic compiler backend for big data languages,Vinayak Borkar; Yingyi Bu; E Preston Carman Jr; Nicola Onose; Till Westmann; Pouria Pirzadeh; Michael J Carey; Vassilis J Tsotras,Abstract A number of high-level query languages; such as Hive; Pig; Flume; and Jaql; havebeen developed in recent years to increase analyst productivity when processing andanalyzing very large datasets. The implementation of each of these languages includes acomplete; data model-dependent query compiler; yet each involves a number of similaroptimizations. In this work; we describe a new query compiler architecture that separateslanguage-specific and data model-dependent aspects from a more general query compilerbackend that can generate executable data-parallel programs for shared-nothing clustersand can be used to develop multiple languages with different data models. We have builtsuch a data model-agnostic query compiler substrate; called Algebricks; and have used it toimplement three different query languages---HiveQL; AQL; and XQuery---to validate the …,Proceedings of the Sixth ACM Symposium on Cloud Computing,2015,7
Big graph analytics platforms,Da Yan; Yingyi Bu; Yuanyuan Tian; Amol Deshpande,Abstract Due to the growing need to process large graph and network datasets created bymodern applications; recent years have witnessed a surging interest in developing biggraph platforms. Tens of such big graph systems have already been developed; but therelacks a systematic categorization and comparison of these systems. This article provides atimely and comprehensive survey of existing big graph systems; and summarizes their keyideas and technical contributions from various aspects. In addition to the popular vertex-centric systems which espouse a think-like-a-vertex paradigm for developing parallel graphapplications; this survey also covers other programming and computation models; contraststhose against each other; and provides a vision for the future research on big graphanalytics platforms. This survey aims to help readers get a systematic picture of the …,Foundations and Trends® in Databases,2017,5
Pregelix: dataflow-based big graph analytics,Yingyi Bu,Abstract Recently; Google has proposed the Pregel programming model [2] for Big Graphanalytics; where application programmers need no knowledge of parallel or distributedsystems. Instead; they just need to" think like a vertex" and write a few functions thatencapsulate the logic for what one graph vertex does. The vertex-oriented programmingmodel has been found to ease the implementation of distributed graph algorithms to a greatextent.,Proceedings of the 4th annual Symposium on Cloud Computing,2013,5
Cleanix: A parallel big data cleaning system,Hongzhi Wang; Mingda Li; Yingyi Bu; Jianzhong Li; Hong Gao; Jiacheng Zhang,Abstract For big data; data quality problem is more serious. Big data cleaning systemrequires scalability and the abilityof handling mixed errors. Motivated by this; we developCleanix; a prototype system for cleaning relational Big Data. Cleanix takes data integratedfrom multiple data sources and cleans them on a shared-nothing machine cluster. Thebackend system is built on-top-of an extensible and flexible data-parallel substrate theHyracks framework. Cleanix supports various data cleaning tasks such as abnormal valuedetection and correction; incomplete data filling; de-duplication; and conflict resolution. Inthis paper; we show the organization; data cleaning algorithms as well as the design ofCleanix.,ACM SIGMOD Record,2016,4
Cleanix: A big data cleaning parfait,Hongzhi Wang; Mingda Li; Yingyi Bu; Jianzhong Li; Hong Gao; Jiacheng Zhang,Abstract In this demo; we present Cleanix; a prototype system for cleaning relational BigData. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexibledata-parallel substrate-the Hyracks framework. Cleanix supports various data cleaning taskssuch as abnormal value detection and correction; incomplete data filling; de-duplication; andconflict resolution. We demonstrate that Cleanix is a practical tool that supports effective andefficient data cleaning at the large scale.,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,4
Large-scale complex analytics on semi-structured datasets using asterixdb and spark,Wail Y Alkowaileet; Sattam Alsubaiee; Michael J Carey; Till Westmann; Yingyi Bu,Abstract Large quantities of raw data are being generated by many different sources indifferent formats. Private and public sectors alike acclaim the valuable information andinsights that can be mined from such data to better understand the dynamics of everyday life;such as traffic; worldwide logistics; and social behavior. For this reason; storing; managing;and analyzing" Big Data" at scale is getting a tremendous amount of attention; both inacademia and industry. In this paper; we demonstrate the power of a parallel connection thatwe have built between Apache Spark and Apache AsterixDB (Incubating) to enable complexanalytics such as machine learning and graph analysis on data drawn from large semi-structured data collections. The integration of these two systems allows researchers anddata scientists to leverage AsterixDB capabilities; including fast ingestion and indexing of …,Proceedings of the VLDB Endowment,2016,2
Comparing SSD-placement Strategies to scale a Database-in-the-Cloud,Yingyi Bu; Hongrae Lee; Jayant Madhavan,Abstract Flash memory solid state drives (SSDs) have increasingly been advocated andadopted as a means of speeding up and scaling up data-driven applications. However;given the layered software architecture of cloud-based services; there are a number ofoptions available for placing SSDs. In this work; we studied the trade-offs involved indifferent SSD placement strategies; their impact of response time and throughput; andultimately the potential in achieving scalability in Google Fusion Tables (GFT); a cloud-based service for data management and visualization [1].,Proceedings of the 4th annual Symposium on Cloud Computing,2013,2
On Software Infrastructure for Scalable Graph Analytics,Yingyi Bu,Recently; there is a growing need for distributed graph processing systems that are capableof gracefully scaling to very large datasets. In the mean time; in real-world applications; it ishighly desirable to reduce the tedious; inefficient ETL (extract; transform; load) gap betweentabular data processing systems and graph processing systems. Unfortunately; thosechallenges have not been easily met due to the intense memory pressure imposed byprocess-centric; message passing designs that many graph processing systems follow; aswell as the separation of tabular data processing runtimes and graph processing runtimes.In this thesis; we explore the application of programming techniques and algorithms from thedatabase systems world to the problem of scalable graph analysis. We first propose a bloat-aware design paradigm towards the development of efficient and scalable Big Data …,*,2015,1
Understanding and Combating Memory Bloat in Managed Data-Intensive Systems,Khanh Nguyen; Kai Wang; Yingyi Bu; Lu Fang; Guoqing Xu,Abstract The past decade has witnessed increasing demands on data-driven businessintelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice forimplementing such applications; due to its quick development cycle and rich suite of librariesand frameworks. While the use of such languages makes programming easier; theirautomated memory management comes at a cost. When the managed runtime meets largevolumes of input data; memory bloat is significantly magnified and becomes a scalability-prohibiting bottleneck. This article first studies; analytically and empirically; the impact ofbloat on the performance and scalability of large-scale; real-world data-intensive systems.To combat bloat; we design a novel compiler framework; called F acade; that can …,ACM Transactions on Software Engineering and Methodology (TOSEM),2018,*
Using SSDs to scale up Google Fusion Tables; a database-in-the-cloud,Yingyi Bu; Felix Halim; Changkyu Kim; Hongrae Lee; Jayant Madhavan,Flash memory solid state drives (SSDs) have increasingly been advocated and adopted asa means of speeding up and scaling up data-driven applications. SSDs are becoming morewidely available as an option in the cloud. However; when an application considers SSDs inthe cloud; the best option for the application may not be immediate; among a number ofchoices for placing SSDs in the layers of the cloud. Although there have been many studieson SSDs; they often concern a specific setting; and how different SSD options in the cloudcompare with each other is less well understood. In this paper; we describe how GoogleFusion Tables (GFT) used SSDs and what optimizations were implemented to scale up its in-memory processing; clearly showing opportunities and limitations of SSDs in the cloud withquantitative analyses. We first discuss various SSD placement strategies and compare …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
Toward an agent-based pluggable infrastructure for context-awareness,Jun Li; Yingyi Bu; Shaxun Chen; Xianping Tao; Jian Lu,Purpose–Pervasive computing enhances the environment by embedding many computersthat are gracefully integrated with human users. The purpose of this paper is to describe thecreation of a smart context-aware environment in which computation follows people andserves them everywhere. Building such smart environments is still difficult and complex dueto lacking a uniform infrastructure that can adapt to diverse smart domains.Design/methodology/approach–To address this problem; the paper proposes an agent-based pluggable infrastructure which integrates a mobile agent system named pvMogent;establishes an ontology-based context model and introduces a workflow-based applicationmodel with the open services gateway initiative (OSGi) framework. By pluggingcorresponding domain context in ontology model and different applications; the …,International Journal of Pervasive Computing and Communications,2010,*
Query by Humming,Yingyi Bu; Raymond Chi-Wing Wong; Ada Wai-Chee Fu,In general; the term Quadtree refers to a class of representations of geometric entities (suchas points; line segments; polygons; regions) in a space of two (or more) dimensions; thatrecursively decompose the space containing these entities into blocks until the data in eachblock satisfy some condition (with respect; for example; to the block size; the number of blockentities; the characteristics of the block entities; etc.). In a more restricted sense; the termQuadtree (Octree) refers to a tree data-structure in which each internal node has four (eight)children and is used for the representation of geometric entities in a two (three) dimensionalspace. The root of the tree represents the whole space/region. Each child of a noderepresents a subregion of the subregion of its parent. The subregions of the siblingsconstitute a partition of the parent's regions. Several variations of quadtrees are possible …,*,2009,*
Master Thesis: A Study of Two Problems in Data Mining: Anomaly Monitoring and Privacy Preservation,Yingyi Bu,*,*,2008,*
