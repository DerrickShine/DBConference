Object-level ranking: bringing order to web objects,Zaiqing Nie; Yuanzhi Zhang; Ji-Rong Wen; Wei-Ying Ma,Abstract In contrast with the current Web search methods that essentially do document-levelranking and retrieval; we are exploring a new paradigm to enable Web search at the objectlevel. We collect Web information for objects relevant for a specific application domain andrank these objects in terms of their relevance and popularity to answer user queries.Traditional PageRank model is no longer valid for object popularity calculation because ofthe existence of heterogeneous relationships between objects. This paper introducesPopRank; a domain-independent object-level link analysis model to rank the objects withina specific domain. Specifically we assign a popularity propagation factor to each type ofobject relationship; study how different popularity propagation factors for theseheterogeneous relationships could affect the popularity ranking; and propose efficient …,Proceedings of the 14th international conference on World Wide Web,2005,360
StatSnowball: a statistical approach to extracting entity relationships,Jun Zhu; Zaiqing Nie; Xiaojiang Liu; Bo Zhang; Ji-Rong Wen,Abstract Traditional relation extraction methods require pre-specified relations and relation-specific human-tagged examples. Bootstrapping systems significantly reduce the number oftraining examples; but they usually apply heuristic-based methods to combine a set of stricthard rules; which limit the ability to generalize and thus generate a low recall. Furthermore;existing bootstrapping methods do not perform open information extraction (Open IE); whichcan identify various types of relations without requiring pre-specifications. In this paper; wepropose a statistical extraction framework called Statistical Snowball (StatSnowball); whichis a bootstrapping system and can perform both traditional relation extraction and Open IE.StatSnowball uses the discriminative Markov logic networks (MLNs) and softens hard rulesby learning their weights in a maximum likelihood estimate sense. MLN is a general …,Proceedings of the 18th international conference on World wide web,2009,287
Simultaneous record detection and attribute labeling in web data extraction,Jun Zhu; Zaiqing Nie; Ji-Rong Wen; Bo Zhang; Wei-Ying Ma,Abstract Recent work has shown the feasibility and promise of template-independent Webdata extraction. However; existing approaches use decoupled strategies-attempting to dodata record detection and attribute labeling in two separate phases. In this paper; we showthat separately extracting data records and attributes is highly ineffective and propose aprobabilistic model to perform these two tasks simultaneously. In our approach; recorddetection can benefit from the availability of semantics required in attribute labeling and; atthe same time; the accuracy of attribute labeling can be improved when data records arelabeled in a collective manner. The proposed model is called Hierarchical ConditionalRandom Fields. It can efficiently integrate all useful features by learning their importance;and it can also incorporate hierarchical interactions which are very important for Web data …,Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,2006,209
2d conditional random fields for web information extraction,Jun Zhu; Zaiqing Nie; Ji-Rong Wen; Bo Zhang; Wei-Ying Ma,Abstract The Web contains an abundance of useful semistructured information about realworld objects; and our empirical study shows that strong sequence characteristics exist forWeb information about objects of the same type across different Web sites. ConditionalRandom Fields (CRFs) are the state of the art approaches taking the sequencecharacteristics to do better labeling. However; as the information on a Web page is two-dimensionally laid out; previous linear-chain CRFs have their limitations for Web informationextraction. To better incorporate the two-dimensional neighborhood interactions; this paperpresents a two-dimensional CRF model to automatically extract object information from theWeb. We empirically compare the proposed model with existing linear-chain CRF models forproduct information extraction; and the results show the effectiveness of our model.,Proceedings of the 22nd international conference on Machine learning,2005,157
Web object retrieval.,Zaiqing Nie; Yunxiao Ma; Shuming Shi; Ji-Rong Wen; Wei-Ying Ma,ABSTRACT The primary function of current Web search engines is essentially relevanceranking at the document level. However; myriad structured information about real-worldobjects embedded in static Web pages and online Web databases. In this paper; wepropose a paradigm shift to enable searching at the object level. In traditional informationretrieval models; documents are taken as the retrieval units and the content of a document isconsidered reliable. However; this reliability assumption is no longer valid in the objectretrieval context when multiple copies of information about the same object typically exist.These copies may be inconsistent because of diversity of Web site qualities and the limitedperformance of current information extraction techniques. In this paper; we propose severallanguage models for Web object retrieval. We test these models on our academic search …,WWW,2007,156
Object-level Vertical Search.,Zaiqing Nie; Ji-Rong Wen; Wei-Ying Ma,ABSTRACT Current web search engines essentially conduct document-level ranking andretrieval. However; structured information about realworld objects embedded in staticwebpages and online databases exists in huge amounts. We explore a new paradigm toenable web search at the object level in this paper; extracting and integrating webinformation for objects relevant to a specific application domain. We then rank these objectsin terms of their relevance and popularity in answering user queries. In this paper; weintroduce the overview and core technologies of object-level vertical search engines thathave been implemented in two working systems: Libra Academic Search (http://libra. msra.cn) and Windows Live Product Search (http://products. live. com).,CIDR,2007,137
Detecting online commercial intention (OCI),Honghua Kathy Dai; Lingzhi Zhao; Zaiqing Nie; Ji-Rong Wen; Lee Wang; Ying Li,Abstract Understanding goals and preferences behind a user's online activities can greatlyhelp information providers; such as search engine and E-Commerce web sites; topersonalize contents and thus improve user satisfaction. Understanding a user's intentioncould also provide other business advantages to information providers. For example;information providers can decide whether to display commercial content based on user'sintent to purchase. Previous work on Web search defines three major types of user searchgoals for search queries: navigational; informational and transactional or resource [1][7]. Inthis paper; we focus our attention on capturing commercial intention from search queriesand Web pages; ie; when a user submits the query or browse a Web page; whether he/sheis about to commit or in the middle of a commercial activity; such as purchase; auction …,Proceedings of the 15th international conference on World Wide Web,2006,135
Information classification paradigm,*,A mechanism to classify source documents into one of two categories; either likely to containdesired information or unlikely to contain desired information. Generally some form of rulesbased classification in conjunction with deeper analysis using advanced techniques ondifficult cases is utilized. The rules based classification is generally good for eliminatingcases from further consideration and for identifying documents of interest based ongenerally discernable relationships between data or based on the presence or absence ofdata. The deeper analysis is used to uncover more complex relationships between data thatmay identify documents of interest. Portions of the process may use the entire documentwhile other portions of the process may use only a portion of the document.,*,2009,122
Automatic detection of online commercial intention,*,Features extracted from network browser pages and/or network search queries areleveraged to facilitate in detecting a user's browsing and/or searching intent. Machinelearning classifiers constructed from these features automatically detect a user's onlinecommercial intention (OCI). A user's intention can be commercial or non-commercial; withcommercial intentions being informational or transactional. In one instance; an OCI rankingmechanism is employed with a search engine to facilitate in providing search results that areranked according to a user's intention. This also provides a means to match purchasingadvertisements with potential customers who are more than likely ready to make a purchase(transactional stage). Additionally; informational advertisements can be matched to userswho are researching a potential purchase (informational stage).,*,2010,56
Scalable community discovery on textual data with relations,Huajing Li; Zaiqing Nie; Wang-Chien Lee; Lee Giles; Ji-Rong Wen,Abstract Every piece of textual data is generated as a method to convey its authors' opinionregarding specific topics. Authors deliberately organize their writings and create links; ie;references; acknowledgments; for better expression. Thereafter; it is of interest to study textsas well as their relations to understand the underlying topics and communities. Althoughmany efforts exist in the literature in data clustering and topic mining; they are not applicableto community discovery on large document corpus for several reasons. First; few of themconsider both textual attributes as well as relations. Second; scalability remains a significantissue for large-scale datasets. Additionally; most algorithms rely on a set of initial parametersthat are hard to be captured and tuned. Motivated by the aforementioned observations; ahierarchical community model is proposed in the paper which distinguishes community …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,54
Joint entity recognition and disambiguation,Gang Luo; Xiaojiang Huang; Chin-Yew Lin; Zaiqing Nie,Abstract Extracting named entities in text and linking extracted names to a given knowledgebase are fundamental tasks in applications for text understanding. Existing systems typicallyrun a named entity recognition (NER) model to extract entity names first; then run an entitylinking model to link extracted names to a knowledge base. NER and linking models areusually trained separately; and the mutual dependency between the two tasks is ignored.We propose JERL; Joint Entity Recognition and Linking; to jointly model NER and linkingtasks and capture the mutual dependency between them. It allows the information from eachtask to improve the performance of the other. To the best of our knowledge; JERL is the firstmodel to jointly optimize NER and linking tasks together completely. In experiments on theCoNLL'03/AIDA data set; JERL outperforms state-of-art NER and linking systems; and we …,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,51
Closing the loop in webpage understanding,Chunyu Yang; Yong Cao; Zaiqing Nie; Jie Zhou; Ji-Rong Wen,The two most important tasks in information extraction from the Web are webpage structureunderstanding and natural language sentences processing. However; little work has beendone toward an integrated statistical model for understanding webpage structures andprocessing natural language sentences within the HTML elements. Our recent work onwebpage understanding introduces a joint model of Hierarchical Conditional Random Fields(HCRFs) and extended Semi-Markov Conditional Random Fields (Semi-CRFs) to leveragethe page structure understanding results in free text segmentation and labeling. In this top-down integration model; the decision of the HCRF model could guide the decision making ofthe Semi-CRF model. However; the drawback of the top-down integration strategy is alsoapparent; ie; the decision of the Semi-CRF model could not be used by the HCRF model …,IEEE Transactions on Knowledge and Data Engineering,2010,50
Webpage understanding: an integrated approach,Jun Zhu; Bo Zhang; Zaiqing Nie; Ji-Rong Wen; Hsiao-Wuen Hon,Abstract Recent work has shown the effectiveness of leveraging layout and tag-tree structurefor segmenting webpages and labeling HTML elements. However; how to effectivelysegment and label the text contents inside HTML elements is still an open problem. Sincemany text contents on a webpage are often text fragments and not strictly grammatical;traditional natural language processing techniques; that typically expect grammaticalsentences; are no longer directly applicable. In this paper; we examine how to use layoutand tag-tree structure in a principled way to help understand text contents on webpages. Wepropose to segment and label the page structure and the text content of a webpage in a jointdiscriminative probabilistic model. In this model; semantic labels of page structure can beleveraged to help text content understanding; and semantic labels ofthe text phrases can …,Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,2007,47
A frequency-based approach for mining coverage statistics in data integration,Zaiqing Nie; Subbarao Kambhampati,Query optimization in data integration requires source coverage and overlap statistics.Gathering and storing the required statistics presents many challenges; not the least ofwhich is controlling the amount of statistics learned. We introduce StatMiner; a novelstatistics mining approach which automatically generates attribute value hierarchies;efficiently discovers frequently accessed query classes based on the learned attribute valuehierarchies; and learns statistics only with respect to these classes. We describe the detailsof our method; and present experimental results demonstrating the efficiency andeffectiveness of our approach. Our experiments are done in the context of BibFinder; apublicly fielded bibliography mediator.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,45
Joint optimization of cost and coverage of query plans in data integration,Zaiqing Nie; Subbarao Kambhampati,Abstract Existing approaches for optimizing queries in data integration use decoupledstrategies--attempting to optimize coverage and cost in two separate phases. Since sourcestend to have a variety of access limitations; such phased optimization of cost and coveragecan unfortunately lead to expensive planning as well as highly inefficient plans. In this paperwe present techniques for joint optimization of cost and coverage of the query plans. Ouralgorithms search in the space of parallel query plans that support multiple sources for eachsubgoal conjunct. The refinement of the partial plans takes into account the potentialparallelism between source calls; and the binding compatibilities between the sourcesincluded in the plan. We start by introducing and motivating our query plan representation.We then briefly review how to compute the cost and coverage of a parallel plan. Next; we …,Proceedings of the tenth international conference on Information and knowledge management,2001,44
Learning Word Representation Considering Proximity and Ambiguity.,Lin Qiu; Yong Cao; Zaiqing Nie; Yong Yu; Yong Rui,Abstract Distributed representations of words (aka word embedding) have proven helpful insolving natural language processing (NLP) tasks. Training distributed representations ofwords with neural networks has lately been a major focus of researchers in the field. Recentwork on word embedding; the Continuous Bag-of-Words (CBOW) model and the ContinuousSkip-gram (Skip-gram) model; have produced particularly impressive results; significantlyspeeding up the training process to enable word representation learning from largescaledata. However; both CBOW and Skip-gram do not pay enough attention to word proximity interms of model or word ambiguity in terms of linguistics. In this paper; we propose Proximity-Ambiguity Sensitive (PAS) models (ie PAS CBOW and PAS Skip-gram) to produce highquality distributed representations of words considering both word proximity and …,AAAI,2014,36
Hierarchical conditional random fields for web extraction,*,A method and system for labeling object information of an information page is provided. Alabeling system identifies an object record of an information page based on the labeling ofobject elements within an object record and labels object elements based on theidentification of an object record that contains the object elements. To identify the recordsand label the elements; the labeling system generates a hierarchical representation ofblocks of an information page. The labeling system identifies records and elements withinthe records by propagating probability-related information of record labels and elementlabels through the hierarchy of the blocks. The labeling system generates a feature vector foreach block to represent the block and calculates a probability of a label for a block beingcorrect based on a score derived from the feature vectors associated with related blocks …,*,2010,32
Method and system for ranking objects of different object types,*,A method and system for ranking objects of different object types based on their popularity isprovided. A ranking system calculates the popularity of objects based on relationshipsbetween the objects. A relationship indicates how one object is related to another object.Thus; objects of one object type may have one or more relationships with objects of anotherobject type. One goal of the ranking system is to rank the objects of the different object typesbased on their popularity. The objects and their relationships can be represented using agraph with nodes representing objects and links representing relationships between objects.The ranking system assigns a popularity propagation factor to each relationship to representits contribution to the popularity of objects of that type.,*,2009,32
Method and system for identifying object information,*,A method and system for identifying object information of an information page is provided.An information extraction system identifies the object blocks of an information page. Theextraction system classifies the object blocks into object types. Each object type hasassociated attributes that define a schema for the information of the object type. Theextraction system identifies object elements within an object block that may represent anattribute value for the object. After the object elements are identified; the extraction systemattempts to identify which object elements correspond to which attributes of the object type ina process referred to as “labeling.” The extraction system uses an algorithm to determine theconfidence that a certain object element corresponds to a certain attribute. The extractionsystem then selects the set of labels with the highest confidence as being the labels for …,*,2008,29
Dynamic hierarchical Markov random fields for integrated web data extraction,Jun Zhu; Zaiqing Nie; Bo Zhang; Ji-Rong Wen,Abstract Existing template-independent web data extraction approaches adopt highlyineffective decoupled strategies---attempting to do data record detection and attributelabeling in two separate phases. In this paper; we propose an integrated web data extractionparadigm with hierarchical models. The proposed model is called Dynamic HierarchicalMarkov Random Fields (DHMRFs). DHMRFs take structural uncertainty into considerationand define a joint distribution of both model structure and class labels. The joint distributionis an exponential family distribution. As a conditional model; DHMRFs relax theindependence assumption as made in directed models. Since exact inference is intractable;a variational method is developed to learn the model's parameters and to find the MAPmodel structure and label assignments. We apply DHMRFs to a real-world web data …,Journal of Machine Learning Research,2008,29
Extracting objects from the web,Zaiqing Nie; Fei Wu; Ji-Rong Wen; Wei-Ying Ma,Extracting and integrating object information from the Web is of great significance for Webdata management. The existing Web information extraction techniques cannot providesatisfactory solution to the Web object extraction task since objects of the same type aredistributed in diverse Web sources; whose structures are highly heterogeneous. In thispaper; we propose a novel approach called Object-Level Information Extraction (OLIE) toextract Web objects. This approach extends a classic information extraction algorithm;Conditional Random Fields (CRF); by adding Web-specific information. The experimentalresults show OLIE can significantly improve the Web object extraction accuracy.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,29
-BibFinder/StatMiner: Effectively Mining and Using Coverage and Overlap Statistics in Data Integration** This research is supported in part by the NSF grant IRI-980...,Zaiqing Nie; Subbarao Kambhampati; Thomas Hernandez,This chapter presents StatMiner; a system for estimating the coverage and overlap statisticswhile keeping the needed statistics tightly under control. StatMiner uses a hierarchicalclassification of the queries; and threshold based variants of familiar data mining techniquesto dynamically decide the level of resolution at which to learn the statistics. The chapterdemonstrates the major functionalities of StatMiner and the effectiveness of the learnedstatistics in BibFinder; a publicly available computer science bibliography mediator. Thesources that BibFinder integrates are autonomous and can have uncontrolled coverage andoverlap. An important focus in BibFinder was thus to mine coverage and overlap statisticsabout these sources and to exploit them to improve query processing.need for thesestatistics with an example. BibFinder Example: We have been developing BibFinder (http …,*,2003,28
Socialsearch: enhancing entity search with social network matching,Gae-won You; Seung-won Hwang; Zaiqing Nie; Ji-Rong Wen,Abstract This paper introduces the problem of matching people names to theircorresponding social network identities such as their Twitter accounts. Existing tools for thispurpose build upon naive textual matching and inevitably suffer low precision; due to falsepositives (eg; fake impersonator accounts) and false negatives (eg; accounts usingnicknames). To overcome these limitations; we leverage" relational" evidences extractedfrom the Web corpus. In particular; as such an example; weadopt Web document co-occurrences; which can be interpreted as an" implicit" counterpart of Twitter followerrelationships. Using both textual and relational features; we learn a ranking functionaggregating these features for the accurate ordering of candidate matches. Another keycontribution of this paper is to formulate confidence scoring as a separate problem from …,Proceedings of the 14th International Conference on Extending Database Technology,2011,26
Navigation system for product search,Jongwuk Lee; Seung-won Hwang; Zaiqing Nie; Ji-Rong Wen,We demonstrate Product EntityCube; a product recommendation and navigation system.While the unprecedented scale of a product search portal enables to satisfy users withdiverse needs; this scale also complicates product recommendation. Specifically; our targetapplication poses a unique challenge of overcoming insufficient user profiles andfeedbacks. To address this problem; we organize query results into clusters representingdifferent user perceptions of similarity; and provide a navigational UI to handle personalinterests. Specifically; we first discuss hybrid object clustering capturing diverse userinterests from millions of Web pages and disambiguating different perceptions using feature-based similarity. We then discuss skyline object ranking to highlight interesting items at eachcluster. Our demonstration illustrates how Product EntityCube can enrich user product …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,25
Effectively mining and using coverage and overlap statistics for data integration,Zaiqing Nie; Subbarao Kambhampati; Ullas Nambiar,Recent work in data integration has shown the importance of statistical information about thecoverage and overlap of sources for efficient query processing. Despite this recognition;there are no effective approaches for learning the needed statistics. The key challenge inlearning such statistics is keeping the number of needed statistics low enough to have thestorage and learning costs manageable. In this paper; we present a set of connectedtechniques that estimate the coverage and overlap statistics; while keeping the neededstatistics tightly under control. Our approach uses a hierarchical classification of the queriesand threshold-based variants of familiar data mining techniques to dynamically decide thelevel of resolution at which to learn the statistics. We describe the details of our method; and;present experimental results demonstrating the efficiency of the learning algorithms and …,IEEE Transactions on Knowledge and Data Engineering,2005,25
Group-by attribute value in search results,*,Search results are ranked by applying sub-relevancies within a search result list. Thispermits search result lists to be further refined into more manageable relevant groupingsfrom a user's perspective. The sub-relevancies or 'group-by'parameters are derived fromsearch result attributes. Attribute values from the attributes are employed in a rankingscheme to group the search results based on attribute value relevancy. The grouped searchresults can then be displayed to users via a search result page. In one instance users canselect which attribute value is used to group the search result list. Ranking processes arebased on object ranking algorithms that consider each attribute value as an object type.Some instances provide for search result list group condensing based on relevancy of theattribute values as well. A top-k instance can be employed to limit the search results to …,*,2011,23
Automated social networking graph mining and visualization,*,The automated social networking graph mining and visualization technique describedherein mines social connections and allows creation of a social networking graph fromgeneral (not necessarily social-application specific) Web pages. The technique uses thedistances between a person's/entity's name and related people's/entities names on one ormore Web pages to determine connections between people/entities and the strengths of theconnections. In one embodiment; the technique lays out these connections; and thenclusters them; in a 2-D layout of a social networking graph that represents the Webconnection strengths among the related people's or entities' names; by using a force-directed model.,*,2011,22
Combining machine learning and human judgment in author disambiguation,Yanan Qian; Yunhua Hu; Jianling Cui; Qinghua Zheng; Zaiqing Nie,Abstract Author disambiguation in digital libraries becomes increasingly difficult as thenumber of publications and consequently the number of ambiguous author names keepgrowing. The fully automatic author disambiguation approach could not give satisfactoryresults due to the lack of signals in many cases. Furthermore; human judgment on the basisof automatic algorithms is also not suitable because the automatically disambiguated resultsare often mixed and not understandable for humans. In this paper; we propose a LabelingOriented Author Disambiguation approach; called LOAD; to combine machine learning andhuman judgment together in author disambiguation. LOAD exploits a framework whichconsists of high precision clustering; high recall clustering; and top dissimilar clustersselection and ranking. In the framework; supervised learning algorithms are used to train …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,22
Query result clustering for object-level search,Jongwuk Lee; Seung-won Hwang; Zaiqing Nie; Ji-Rong Wen,Abstract Query result clustering has recently attracted a lot of attention to provide users witha succinct overview of relevant results. However; little work has been done on organizing thequery results for object-level search. Object-level search result clustering is challengingbecause we need to support diverse similarity notions over object-specific features (such asthe price and weight of a product) of heterogeneous domains. To address this challenge; wepropose a hybrid subspace clustering algorithm called Hydra. Algorithm Hydra captures theuser perception of diverse similarity notions from millions of Web pages and disambiguatesdifferent senses using feature-based subspace locality measures. Our proposed solution; bycombining wisdom of crowds and wisdom of data; achieves robustness and efficiency overexisting approaches. We extensively evaluate our proposed framework and demonstrate …,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,22
Optimizing recursive information gathering plans in emerac,Subbarao Kambhampati; Eric Lambrecht; Ullas Nambiar; Zaiqing Nie; Gnanaprakasam Senthil,Abstract In this paper we describe two optimization techniques that are specially tailored forinformation gathering. The first is a greedy minimization algorithm that minimizes aninformation gathering plan by removing redundant and overlapping information sourceswithout loss of completeness. We then discuss a set of heuristics that guide the greedyminimization algorithm so as to remove costlier information sources first. In contrast toprevious work; our approach can handle recursive query plans that arise commonly in thepresence of constrained sources. Second; we present a method for ordering the access tosources to reduce the execution cost. This problem differs significantly from the traditionaldatabase query optimization problem as sources on the Internet have a variety of accesslimitations and the execution cost in information gathering is affected both by network …,Journal of Intelligent Information Systems,2004,22
Web object retrieval based on a language model,*,A method and system is provided for determining relevance of an object to a term based ona language model. The relevance system provides records extracted from web pages thatrelate to the object. To determine the relevance of the object to a term; the relevance systemfirst determines; for each record of the object; a probability of generating that term using alanguage model of the record of that object. The relevance system then calculates therelevance of the object to the term by combining the probabilities. The relevance system mayalso weight the probabilities based on the accuracy or reliability of the extracted informationfor each data source.,*,2011,21
Mining name translations from entity graph mapping,Gae-won You; Seung-won Hwang; Young-In Song; Long Jiang; Zaiqing Nie,Abstract This paper studies the problem of mining entity translation; specifically; miningEnglish and Chinese name pairs. Existing efforts can be categorized into (a) a transliteration-based approach leveraging phonetic similarity and (b) a corpus-based approach exploitingbilingual co-occurrences; each of which suffers from inaccuracy and scarcity respectively. Inclear contrast; we use unleveraged resources of monolingual entity co-occurrences; crawledfrom entity search engines; represented as two entity-relationship graphs extracted from twolanguage corpora respectively. Our problem is then abstracted as finding correct mappingsacross two graphs. To achieve this goal; we propose a holistic approach; of exploiting bothtransliteration similarity and monolingual co-occurrences. This approach; building uponmonolingual corpora; complements existing corpus-based work; requiring scarce …,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,2010,21
Dynamic hierarchical Markov random fields and their application to web data extraction,Jun Zhu; Zaiqing Nie; Bo Zhang; Ji-Rong Wen,Abstract Hierarchical models have been extensively studied in various domains. However;existing models assume fixed model structures or incorporate structural uncertaintygeneratively. In this paper; we propose Dynamic Hierarchical Markov Random Fields(DHMRFs) to incorporate structural uncertainty in a discriminative manner. DHMRFs consistof two parts--structure model and class label model. Both are defined as exponential familydistributions. Conditioned on observations; DHMRFs relax the independence assumption asmade in directed models. As exact inference is intractable; a variational method isdeveloped to learn parameters and to find the MAP model structure and label assignment.We apply the model to a real-world web data extraction task; which automatically extractsproduct items for sale on the Web. The results show promise.,Proceedings of the 24th international conference on Machine learning,2007,20
Mining coverage statistics for websource selection in a mediator,Zaiqing Nie; Ullas Nambiar; Sreelakshmi Vaddi; Subbarao Kambhampati,Abstract Recent work in data integration has shown the importance of statistical informationabout the coverage and overlap of sources for efficient query processing. Despite thisrecognition there are no effective approaches for learning the needed statistics. The keychallenge in learning such statistics is keeping the number of needed statistics low enoughto have the storage and learning costs manageable. Naive approaches can becomeinfeasible very quickly. In this paper we present a set of connected techniques that estimatethe coverage and overlap statistics while keeping the needed statistics tightly under control.Our approach uses a hierarchical classification of the queries; and threshold based variantsof familiar data mining techniques to dynamically decide the level of resolution at which tolearn the statistics. We describe the details of our method; and present experimental …,Proceedings of the eleventh international conference on Information and knowledge management,2002,18
Statistical entity extraction from the web,Zaiqing Nie; Ji-Rong Wen; Wei-Ying Ma,There are various kinds of valuable semantic information about real-world entitiesembedded in webpages and databases. Extracting and integrating these entity informationfrom the Web is of great significance. Comparing to traditional information extractionproblems; web entity extraction needs to solve several new challenges to fully takeadvantage of the unique characteristic of the Web. In this paper; we introduce our recentwork on statistical extraction of structured entities; named entities; entity facts and relationsfrom Web. We also briefly introduce iKnoweb; an interactive knowledge mining frameworkfor entity information integration. We will use two novel web applications; MicrosoftAcademic Search (aka Libra) and EntityCube; as working examples.,Proceedings of the IEEE,2012,17
Havasu: A multi-objective; adaptive query processing framework for web data integration,Subbarao Kambhampati; Ullas Nambiar; Zaiqing Nie; Sreelakshmi Vaddi,Abstract Mediators for web-based data integration need the ability to handle multiple; oftenconflicting objectives; including cost; coverage and execution flexibility. This requires thedevelopment of query planning algorithms that are capable of multi-objective queryoptimization; as well as techniques for automatically gathering the requisite cost/coveragestatistics from the autonomous data sources. We are designing a query processingframework called Havasu to handle these challenges. We will present the architecture ofHavasu and describe the implementation and evaluation of its query planning and statisticsgathering modules.,ASU CSE,2002,17
Hybrid entity clustering using crowds and data,Jongwuk Lee; Hyunsouk Cho; Jin-Woo Park; Young-rok Cha; Seung-won Hwang; Zaiqing Nie; Ji-Rong Wen,Abstract Query result clustering has attracted considerable attention as a means of providingusers with a concise overview of results. However; little research effort has been devoted toorganizing the query results for entities which refer to real-world concepts; eg; people;products; and locations. Entity-level result clustering is more challenging because diversesimilarity notions between entities need to be supported in heterogeneous domains; eg;image resolution is an important feature for cameras; but not for fruits. To address thischallenge; we propose a hybrid relationship clustering algorithm; called Hydra; using co-occurrence and numeric features. Algorithm Hydra captures diverse user perceptions fromco-occurrence and disambiguates different senses using feature-based similarity. Inaddition; we extend Hydra into Hydra _ gData Hydra gData with different sources; ie …,The VLDB Journal,2013,16
BioSnowball: automated population of Wikis,Xiaojiang Liu; Zaiqing Nie; Nenghai Yu; Ji-Rong Wen,Abstract Internet users regularly have the need to find biographies and facts of people ofinterest. Wikipedia has become the first stop for celebrity biographies and facts. However;Wikipedia can only provide information for celebrities because of its neutral point of view(NPOV) editorial policy. In this paper we propose an integrated bootstrapping frameworknamed BioSnowball to automatically summarize the Web to generate Wikipedia-style pagesfor any person with a modest web presence. In BioSnowball; biography ranking and factextraction are performed together in a single integrated training and inference process usingMarkov Logic Networks (MLNs) as its underlying statistical model. The bootstrappingframework starts with only a small number of seeds and iteratively finds new facts andbiographies. As biography paragraphs on the Web are composed of the most important …,Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,2010,16
Pseudo-anchor text extraction for vertical search,*,A search method uses pseudo-anchor text associated with search objects to improve searchperformance. The pseudo-anchor text may be extracted in combination with an identifier ofthe search objects (such as a pseudo-URL) from a digital corpus such as a collection ofdocuments. Pseudo-anchor texts for each object are preferably extracted from candidateanchor blocks using a machine learning based approach. The pseudo-anchor texts aremade available for searching and used to help ranking the objects in a search result toimprove search performance. Method may be used in vertical search of objects such aspublished articles; products and images that lack explicit URL and anchor text information.,*,2010,15
Webpage understanding: beyond page-level search,Zaiqing Nie; Ji-Rong Wen; Wei-Ying Ma,Abstract In this paper we introduce the webpage understanding problem which consists ofthree subtasks: webpage segmentation; webpage structure labeling; and webpage textsegmentation and labeling. The problem is motivated by the search applications we havebeen working on including Microsoft Academic Search; Windows Live Product Search andRenlifang Entity Relationship Search. We believe that integrated webpage understandingwill be an important direction for future research in Web mining.,ACM SIGMOD Record,2009,14
Progress and prospect of the research of environmental friendly bast fiber mulch film,CY Wang; Yi YJ LV JN,Basing on the analysis of the application of agricultural mulch film in China and the currentstatus of research on degradable mulch film; this paper introduced the research anddevelopment of environmental friendly bast fiber mulch film in China; and analyzed theprospect and significance of industrialization and extension of the environmental friendlybast fiber mulch film. Moreover; we brought forward some ideas on further research anddevelopment of environmental friendly bast fiber mulch film.,Plant Fiber Sci China,2007,13
Mining source coverage statistics for data integration,Zaiqing Nie; Subbarao Kambhampati; Ullas Nambiar; Sreelakshmi Vaddi,Abstract Recent work in data integration has shown the importance of statistical informationabout the coverage and overlap of sources for efficient query processing. Despite thisrecognition there are no effective approaches for learning the needed statistics. The keychallenge in learning such statistics is keeping the number of needed statistics low enoughto have the storage and learning costs manageable. Naive approaches can becomeinfeasible very quickly. In this paper we present a set of connected techniques that estimatethe coverage and overlap statistics while keeping the needed statistics tightly under control.Our approach uses a hierarchical classification of the queries; and threshold based variantsof familiar data mining techniques to dynamically decide the level of resolution at which tolearn the statistics. We describe the details of our method; and present preliminary …,Proceedings of the 3rd international workshop on Web information and data management,2001,13
AltAlt: Combining graphplan and heuristic state search,Biplav Srivastava; XuanLong Nguyen; Subbarao Kambhampati; Minh B Do; Ullas Nambiar; Zaiqing Nie; Romeo Nigenda; Terry Zimmerman,Abstract We briefly describe the implementation and evaluation of a novel plan synthesissystem; called AltAlt. AltAlt is designed to exploit the complementary strengths of two of thecurrently popular competing approaches for plan generation:(1) graphplan and (2) heuristicstate search. It uses the planning graph to derive effective heuristics that are then used toguide heuristic state search. The heuristics derived from the planning graph do a better jobof taking the subgoal interactions into account and; as such; are significantly more effectivethan existing heuristics. AltAlt was implemented on top of two state-of-the-art planningsystems:(1) stan3. 0; a graphplan-style planner; and (2) hsp-r; a heuristic search planner.,AI Magazine,2001,12
Content object indexing using domain knowledge,*,A content object indexing process including creating a content object knowledge index;calculating a description vector of a target content object; and indexing the target contentobject by searching for the description vector in the content object knowledge database. Itmay be difficult to search for an exact content object such as a music file or academicresearcher as a conventional search index may not include related hierarchical information.A content object indexing process may add hierarchical information taken from a contentobject knowledge index and incorporate the hierarchical information to the index entry for aspecific content object. An application of such a content object indexing process may be aworld wide web search engine.,*,2010,10
SocialSearch+: enriching social network with web evidences,Gae-won You; Jin-woo Park; Seung-won Hwang; Zaiqing Nie; Ji-Rong Wen,Abstract This paper introduces the problem of searching for social network accounts; eg;Twitter accounts; with the rich information available on the Web; eg; people names;attributes; and relationships to other people. For this purpose; we need to map Twitteraccounts with Web entities. However; existing solutions building upon naive textualmatching inevitably suffer low precision due to false positives (eg; fake impersonatoraccounts) and false negatives (eg; accounts using nicknames). To overcome theselimitations; we leverage “relational” evidences extracted from the Web corpus. We considertwo types of evidence resources—First; web-scale entity relationship graphs; extracted fromname co-occurrences crawled from the Web. This co-occurrence relationship can beinterpreted as an “implicit” counterpart of Twitter follower relationships. Second; web …,World Wide Web,2013,9
Ab initio studies on n-type and p-type Li4Ti5O12,Zhong Zhi-Yong; Nie Zheng-Xin; Du Yan-Lan; Ouyang Chu-Ying; Shi Si-Qi; Lei Min-Sheng,Abstract This paper studies the structure and electronic properties of Li 4 Ti 5 O 12; as anodematerial for lithium ion batteries; from first principles calculations. The results suggest thatthere are two kinds of unit cell of Li 4 Ti 5 O 12: n-type and p-type. The two unit cells havedifferent structures and electronic properties: the n-type with two 16d site Li ions is metallicby electron; while the p-type with three 16d Li ions is metallic by hole. However; the Li 4 Ti 5O 12 is an insulator. It is very interesting that one n-type cell and two p-type cells constituteone Li 4 Ti 5 O 12 supercell which is insulating. The results show that the intercalationpotential obtained with a p-type unit cell with one additional electron is quite close to theexperimental value of 1.5 V.,Chinese Physics B,2009,9
Two-dimensional conditional random fields for web extraction,*,A labeling system uses a two-dimensional conditional random fields technique to label theobject elements. The labeling system represents transition features and state features thatdepend on object elements that are adjacent in two dimensions. The labeling systemrepresents the grid as a graph of vertices and edges with a vertex representing an objectelement and an edge representing a relationship between the object elements. The labelingsystem represents each diagonal of the graph as a sequence of states. The labeling systemselects a labeling for the vertices of the diagonals that has the highest probability based ontransition probabilities between vertices of adjacent diagonals and on the state probabilitiesof a position within a diagonal.,*,2009,8
Name disambiguation using web connection,Yiming Lu; Zaiqing Nie; Taoyuan Cheng; Ying Gao; Ji-Rong Wen,*,Proc. of AAAI,2007,8
Grounding Topic Models with Knowledge Bases.,Zhiting Hu; Gang Luo; Mrinmaya Sachan; Eric P Xing; Zaiqing Nie,Abstract Topic models represent latent topics as probability distributions over words whichcan be hard to interpret due to the lack of grounded semantics. In this paper; we propose astructured topic representation based on an entity taxonomy from a knowledge base. Aprobabilistic model is developed to infer both hidden topics and entities from text corpora.Each topic is equipped with a random walk over the entity hierarchy to extract semanticallygrounded and coherent themes. Accurate entity modeling is achieved by leveraging richtextual features from the knowledge base. Experiments show significant superiority of ourapproach in topic perplexity and key entity identification; indicating potentials of thegrounded modeling for semantic extraction and language understanding applications.,IJCAI,2016,7
Interactive framework for name disambiguation,*,A “Name Disambiguator” provides various techniques for implementing an interactiveframework for resolving or disambiguating entity names (associated with objects such aspublications) for entity searches where two or more same or similar names may refer todifferent entities. More specifically; the Name Disambiguator uses a combination of userinput and automatic models to address the disambiguation problem. In variousembodiments; the Name Disambiguator uses a two part process; including: 1) a global SVMtrained from large sets of documents or objects in a simulated interactive mode; and 2)further personalization of local SVM models (associated with individual names or groups ofnames such as; for example; a group of coauthors) derived from the global SVM model. Theresult of this process is that large sets of documents or objects are rapidly and accurately …,*,2013,7
Object-level web information retrieval,Zaiqing Nie; Yunxiao Ma; Ji-Rong Wen; Wei-Ying Ma,ABSTRACT The primary function of current Web search engines is essentially relevanceranking at the document level. However; there is lots of structured information about real-world objects embedded in static Web pages and online Web databases. Document-levelinformation retrieval will unfortunately lead to highly inaccurate relevance ranking inanswering object-oriented queries. In this paper; we consider a new paradigm shift toenable searching at the object level. In traditional information retrieval models; document istaken as the retrieval unit and the content of a document is reliable. However the reliabilityassumption is no longer valid in the object retrieval context where usually exist multiplecopies of information about the same object. These copies may be inconsistent because ofthe diverse Web site qualities and the limited performance of current information …,*,2005,7
Webpage entity extraction through joint understanding of page structures and sentences,*,Described is a technology for understanding entities of a webpage; eg; to label the entitieson the webpage. An iterative and bidirectional framework processes a webpage; including atext understanding component (eg; extended Semi-CRF model) that provides textsegmentation features to a structure understanding component (eg; extended HCRF model).The structure understanding component uses the text segmentation features and visuallayout features of the webpage to identify a structure (eg; labeled block). The textunderstanding component in turn uses the labeled block to further understand the text. Theprocess continues iteratively until a similarity criterion is met; at which time the entities maybe labeled. Also described is the use of multiple mentions of a set of text in the webpage tohelp in labeling an entity.,*,2015,6
Augmenting and presenting captured data,*,Captured data can be transformed and augmented for a particular presentation in adocument; such as a note of a notebook application; based on an identified entity for thecaptured data. The particular presentation of captured data can be provided based on entitydetection; extraction; and knowledge base resolution and retrieval. Methods; systems; andservices are provided that identify a primary entity of an item input to a notebook applicationand create an entity object for the primary entity of the item at least from one or morestructured representations for content associated with the item. A template for presenting theentity object can be determined according to the primary entity; where the template isselected from a set of templates corresponding to different primary entities such that anarrangement and presentation for one primary entity is different than that of another …,*,2015,6
Efficient entity translation mining: A parallelized graph alignment approach,Gae-Won You; Seung-Won Hwang; Young-In Song; Long Jiang; Zaiqing Nie,Abstract This article studies the problem of mining entity translation; specifically; miningEnglish and Chinese name pairs. Existing efforts can be categorized into (a) transliteration-based approaches that leverage phonetic similarity and (b) corpus-based approaches thatexploit bilingual cooccurrences. These approaches suffer from inaccuracy and scarcity;respectively. In clear contrast; we use under-leveraged resources of monolingual entitycooccurrences crawled from entity search engines; which are represented as two entity-relationship graphs extracted from two language corpora; respectively. Our problem is thenabstracted as finding correct mappings across two graphs. To achieve this goal; we proposea holistic approach to exploiting both transliteration similarity and monolingualcooccurrences. This approach; which builds upon monolingual corpora; complements …,ACM Transactions on Information Systems (TOIS),2012,6
Web-scale entity summarization,*,Described is a summarizing a web entity (eg; a person; place; product or so forth) basedupon the entity's appearance in web documents (eg; on the order of hundreds of millions orbillions of webpages). Webpages are separated into blocks; which are then processedaccording to various features to filter the number of blocks to further process; and rank themost relevant blocks with respect to the entity that remain. A redundancy removalmechanism removes redundant blocks; leaving a set of remaining blocks that are used toprovide a summary of information that is relevant to the entity.,*,2012,6
Pseudo-anchor text extraction,*,A search method uses pseudo-anchor text associated with search objects to improve searchperformance. The pseudo-anchor text may be extracted in combination with an identifier ofthe search objects (such as a pseudo-URL) from a digital corpus such as a collection ofdocuments. Pseudo-anchor texts for each object are preferably extracted from candidateanchor blocks using a machine learning based approach. The pseudo-anchor texts aremade available for searching and used to help rank the objects in a search result to improvesearch performance. The method may be used in vertical search of objects such aspublished articles; products and images that lack explicit URLs and anchor text information.,*,2011,6
富营养化水体中光照对沉水植物的影响研究进展,邹丽莎， 聂泽宇， 姚笑颜， 施积炎,摘摇要摇恢复沉水植物是修复富营养化水体和维持水体生态系统健康的关键;其中光照是主要的限制因子. 本文系统总结了影响水下光强衰减变化的因素和光强对沉水植物生理的影响机制; 重点阐述了常见沉水植物在低光胁迫下体内碳; 氮; 磷的生理代谢机理;抗氧化酶系统的响应机制以及色素组成和浓度的变化反馈. 此外; 本文还列举了常用的恢复沉水植物的工程措施; 提出了恢复富营养化水体中沉水植物群落的思路. 并针对目前研究中存在的不足; 就今后该领域的研究方向和侧重点提出建议; 为理论研究与工程实践提供参考.关键词摇沉水植物摇光照摇生理影响摇工程实践文章编号摇1001-9332 (2013) 07-2073-08摇中图分类号摇X171 摇文献标识码摇A Effects of light on submerged macrophytes ineutrophic water: Research progress. ZOU Li 鄄sha; NIE Ze 鄄yu; YAO Xiao 鄄yan; SHI Ji鄄yan (College of Environmental and Resource Sciences; Zhe 鄄jiang University …,应用生态学报,2013,5
Notice of Violation of IEEE Publication Principles Language Models for Web Object Retrieval,Jianfeng Zheng; Zaiqing Nie,Notice of Violation of IEEE Publication Principles" Language Models for Web ObjectRetrieval;" by Jianfeng Zheng; Zaiqing Nie in the Proceedings of the InternationalConference on New Trends in Information and Service Science;(NISS); June 2009; pp. 282-287 After careful and considered review of the content and authorship of this paper by a dulyconstituted expert committee; this paper has been found to be in violation of IEEE'sPublication Principles. This paper is a verbatim copy of the paper cited below. The leadauthor; Jianfeng Zheng; submitted the copied paper without the knowledge or permission ofthe coauthor; Zaiqing Nie. Due to the nature of this violation; reasonable effort should bemade to remove all past references to this paper; and future references should be made tothe following article:" Web Object Retrieval" by Zaiqing Nie; Yunxiao Ma; Shuming Shi; Ji …,New Trends in Information and Service Science; 2009. NISS'09. International Conference on,2009,5
Anchor text extraction for academic search,Shuming Shi; Fei Xing; Mingjie Zhu; Zaiqing Nie; Ji-Rong Wen,Abstract Anchor text plays a special important role in improving the performance of generalWeb search; due to the fact that it is relatively objective description for a Web page bypotentially a large number of other Web pages. Academic Search provides indexing andsearch functionality for academic articles. It may be desirable to utilize anchor text inacademic search as well to improve the search results quality. The main challenge here isthat no explicit URLs and anchor text is available for academic articles. In this paper wedefine and automatically assign a pseudo-URL for each academic article. And a machinelearning approach is adopted to extract pseudo-anchor text for academic articles; byexploiting the citation relationship between them. The extracted pseudo-anchor text is thenindexed and involved in the relevance score computation of academic articles …,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,2009,4
Notice of Violation of IEEE Publication Principles Architecture and Implementation of Object-Level Vertical Search,Jianfeng Zheng; Zaiqing Nie,Notice of Violation of IEEE Publication Principles" Architecture and Implementation of anObject-level Vertical Search" by Jianfeng Zheng; Zaiqing Nie in the Proceedings of theInternational Conference on New Trends in Information and Service Science;(NISS); June2009; pp. 264-268 After careful and considered review of the content and authorship of thispaper by a duly constituted expert committee; this paper has been found to be in violation ofIEEE's Publication Principles. This paper is a verbatim copy of the paper cited below. Thelead author; Jianfeng Zheng; submitted the copied paper without the knowledge orpermission of the coauthor; Zaiqing Nie. Due to the nature of this violation; reasonable effortshould be made to remove all past references to this paper; and future references should bemade to the following article:" Object-level Vertical Search" by Zaiqing Nie; Ji-Rong Wen …,New Trends in Information and Service Science; 2009. NISS'09. International Conference on,2009,4
Joint optimization of cost and coverage of information gathering plans,Zaiqing Nie; Subbarao Kambhampati,Abstract Existing approaches for optimizing queries in information integration use decoupledstrategies–attempting to optimize coverage and cost in two separate phases. Since sourcestend to have a variety of access limitations; this type of phased optimization of cost andcoverage can unfortunately lead to expensive planning as well as highly inefficient plans. Inthis paper we present techniques for joint optimization of cost and coverage of the queryplans. Our algorithms search in the space of parallel query plans that support multiplesources for each subgoal conjunct. The refinement of the partial plans takes into account thepotential parallelism between source calls; and the binding compatibilities between thesources included in the plan. We start by introducing and motivating our query planrepresentation; and arguing that our way of searching in the space of parallel plans can …,submitted to publication,2001,4
Template-IndependentWeb Object Extraction,Wu Nie; Ma Wen,Abstract There are various kinds of objects embedded in static Web pages and online Webdatabases. Extracting and integrating these objects from the Web is of great significance forWeb data management. The existing Web information extraction (IE) techniques cannotprovide satisfactory solution to the Web object extraction task since objects of the same typeare distributed in diverse Web sources; whose structures are highly heterogeneous. Theclassic information extraction (IE) methods; which are designed for processing plain textdocuments; also fail to meet our requirements. In this paper; we propose a novel approachcalled Object-Level Information Extraction (OLIE) to extract Web objects. This approachextends a classic IE algorithm; Conditional Random Fields (CRF); by adding Web-specificinformation. It is essentially a combination of Web IE and classic IE. Specifically; visual …,retrieved on Feb,2010,2
Notice of Violation of IEEE Publication Principles Architecture of an Object-Level Vertical Search,Jianfeng Zheng; Zaiqing Nie,Notice of Violation of IEEE Publication Principles" Architecture of an Object-level VerticalSearch" By Jianfeng Zheng and Zaiqing Nie in the Proceedings of the 2009 InternationalConference on Web Information Systems and Mining (WISM); November 2009; pp. 51-55After careful and considered review of the content and authorship of this paper by a dulyconstituted expert committee; this paper has been found to be in violation of IEEE'sPublication Principles. This paper is a verbatim copy of the paper cited below. The leadauthor; Jianfeng Zheng; submitted the copied paper without the knowledge or permission ofthe coauthor; Zaiqing Nie. Due to the nature of this violation; reasonable effort should bemade to remove all past references to this paper; and future references should be made tothe following article:" Object-level Vertical Search" by Zaiqing Nie; Ji-Rong Wen; and Wei …,Web Information Systems and Mining; 2009. WISM 2009. International Conference on,2009,2
Information Integration on the Web,Nambiar Ullas; Nie Zaiqing,*,AIMagazine,2007,2
Optimizing Recursive Information Gathering Plans in EMERAC,ERIC LAMBRECHT; ULLAS NAMBIAR; ZAIQING NIE; GNANAPRAKASAM SENTHIL,Abstract. In this paper we describe two optimization techniques that are specially tailored forinformation gathering. The first is a greedy minimization algorithm that minimizes aninformation gathering plan by removing redundant and overlapping information sourceswithout loss of completeness. We then discuss a set of heuristics that guide the greedyminimization algorithm so as to remove costlier information sources first. In contrast toprevious work; our approach can handle recursive query plans that arise commonly in thepresence of constrained sources. Second; we present a method for ordering the access tosources to reduce the execution cost. This problem differs significantly from the traditionaldatabase query optimization problem as sources on the Internet have a variety of accesslimitations and the execution cost in information gathering is affected both by network …,Journal of Intelligent Information Systems,2004,2
Displaying search results with edges/entity relationships in regions/quadrants on a display device,*,Methods and systems for Web-scale entity relationship extraction are usable to build large-scale entity relationship graphs from any data corpora stored on a computer-readablemedium or accessible through a network. Such entity relationship graphs may be used tonavigate previously undiscoverable relationships among entities within data corpora.Additionally; the entity relationship extraction may be configured to utilize discriminativemodels to jointly model correlated data found within the selected corpora.,*,2016,1
Author disambiguation,*,The techniques described herein automatically generate high precision clusters and highrecall clusters for a set of documents having an author with a same or similar name. Thehigh precision clusters and the high recall clusters can then be used in a labeling process sothat efficient and accurate author disambiguation is realized.,*,2016,1
Web object retrieval based on a language model,*,A method and system is provided for determining relevance of an object to a term based ona language model. The relevance system provides records extracted from web pages thatrelate to the object. To determine the relevance of the object to a term; the relevance systemfirst determines; for each record of the object; a probability of generating that term using alanguage model of the record of that object. The relevance system then calculates therelevance of the object to the term by combining the probabilities. The relevance system mayalso weight the probabilities based on the accuracy or reliability of the extracted informationfor each data source.,*,2011,1
Statistical Web Object Extraction,Jun Zhu; Zaiqing Nie; Bo Zhang,Abstract The World Wide Web is a vast and rapidly growing repository of information. Thereare various kinds of objects; such as products; people; conferences; and so on; embeddedin both statically and dynamically generated Web pages. Extracting the information aboutreal-world objects is a key technique for Web mining systems. For example; the object-levelsearch engines; such as Libra (http://libra. msra. cn) and Rexa (http://rexa. info); which helpresearchers find academic information like papers; conferences and researcher's personalinformation; completely rely on structured Web object information. However; how to extractthe object information from diverse Web pages is a challenging problem. Traditionalmethods are mainly template-dependent and thus not scalable to the huge number of Webpages. Furthermore; many methods are based on heuristic rules. So they are not robust …,*,2009,1
with Poisson Jump,应用,Abstract: This paper studies a ClaSS of stochastic delay different equations with Poissonjump (SDDEwPJs). In general SDDEwPJs do not have explleit solutions. Appropriatenumerical approximations; such as the Euler scheme; are therefore a vital tool in exploringtheir properties. In this paper; it is proved that the Euler approximate solutions will convergeto the exact solutions for SDDEwPJs under the local Lipschitz condition.,MATHEMATICA APPLICATA,2007,1
Pseudo-anchor text extraction for searching vertical objects,Shuming Shi; Fei Xing; Mingjie Zhu; Zaiqing Nie; Ji-Rong Wen,Abstract This paper examines the problem of utilizing pseudo-anchor text to help rankingWeb objects in vertical search. We adopt a machine learning based approach to extractpseudo-anchor text for a vertical object from its candidate anchor blocks. Experiments inacademic search domain indicate that our approach is able to dramatically improve searchperformance.,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,1
Mining and using coverage and overlap statistics for data integration,Zaiqing Nie,ABSTRACT Query processing in the context of integrating autonomous data sources on theInternet has received significant attention of late. In contrast to traditional query processingscenarios; in which each relation is stored in the same primary database and in whichcompleteness of answers is expected by users; data integration scenarios involve handlingrelations that are stored across multiple and potentially overlapping sources and dealingwith conflicting objectives in terms of what coverage of answers users want and how muchexecution cost they are willing to bear for achieving the desired coverage. Hence; queryprocessing in data integration requires coverage and overlap statistics about theseautonomous sources to generate optimal query plans. This dissertation first presentsStatMiner; an effective statistics mining approach which automatically generates attribute …,*,2004,1
Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields,Jingwei Zhuo; Yong Cao; Jun Zhu; Bo Zhang; Zaiqing Nie,Abstract Most of the sequence tagging tasks in natural language processing require torecognize segments with certain syntactic role or semantic meaning in a sentence. They areusually tackled with Conditional Random Fields (CRFs); which do indirect word-levelmodeling over word-level features and thus cannot make full use of segment-levelinformation. Semi-Markov Conditional Random Fields (Semi-CRFs) model segments directlybut extracting segment-level features for Semi-CRFs is still a very challenging problem. Thispaper presents Gated Recursive Semi-CRFs (grSemi-CRFs); which model segments directlyand automatically learn segmentlevel features through a gated recursive convolutionalneural network. Our experiments on text chunking and named entity recognition (NER)demonstrate that grSemi-CRFs generally outperform other neural models.,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2016,*
Device for determining disappearing direction and method thereof; apparatus for video camera calibration and method thereof,*,A disappearing direction determination device and method; a video camera calibrationapparatus and method; a video camera and a computer program product are provided. Thedevice comprises: a moving target detecting unit for detecting in the video image a movingtarget area where a moving object locates; a feature point extracting unit for extracting atleast one feature point on the moving object in the detected moving target area; a movingtrajectory obtaining unit for tracking a movement of the feature point in a predeterminednumber of video image frames to obtain a movement trajectory of the feature point; and adisappearing direction determining unit for determining; according to the movementtrajectories of one or more moving objects in the video image; a disappearing directionpointed by a major moving direction of the moving objects. Thus; a disappearing direction …,*,2015,*
Web-Scale Entity Relationship Extraction,*,Methods and systems for Web-scale entity relationship extraction are usable to build large-scale entity relationship graphs from any data corpora stored on a computer-readablemedium or accessible through a network. Such entity relationship graphs may be used tonavigate previously undiscoverable relationships among entities within data corpora.Additionally; the entity relationship extraction may be configured to utilize discriminativemodels to jointly model correlated data found within the selected corpora.,*,2015,*
Web-scale entity relationship extraction that extracts pattern (s) based on an extracted tuple,*,Methods and systems for Web-scale entity relationship extraction are usable to build large-scale entity relationship graphs from any data corpora stored on a computer-readablemedium or accessible through a network. Such entity relationship graphs may be used tonavigate previously undiscoverable relationships among entities within data corpora.Additionally; the entity relationship extraction may be configured to utilize discriminativemodels to jointly model correlated data found within the selected corpora.,*,2013,*
Finding web appearances of social network users via latent factor model,Kailong Chen; Zhengdong Lu; Xiaoshi Yin; Yong Yu; Zaiqing Nie,Abstract With the rapid growing of Web 2.0; people spend more time on social networkssuch as Facebook and Twitter. In order to know the people they are interacting with; findingthe web appearances of them will help the social network users to a great extent. Wepropose a novel and effective latent factor model to find web appearances of target socialnetwork users. Our method solves the name ambiguity problem by simultaneously exploringthe link structure of social networks and the web. Experiments on real-world data show thesuperiority of our method over several baselines.,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,*
A conversation with MSRA researchers,Wei-Ying Ma; Tie-Yan Liu; Ji-Rong Wen; Zheng Chen; Zaiqing Nie; Xing Xie; Hang Li; Haixun Wang; Yu Zheng,Ten years ago; KDD research was still in its infancy in China. Things have changedsignificantly. With the push from the technological advancement in academia and the pullfrom the explosive growth of application needs in industry; KDD research is flourishing. AtMicrosoft Research Asia; we have been conducting research in many areas related to KDDresearch; including web search; data mining; information retrieval; multimedia mining;natural language processing; and visualization. In addition to publishing papers in KDD anddeveloping technologies for commercial products; we have also contributed to the talentdevelopment in China by supervising students and growing young researchers who laterbecome well known in the field related to KDD in universities and industries.,ACM SIGKDD Explorations Newsletter,2012,*
Welcome to the Sixth International Workshop on Information Integration on the Web (IIWeb’07); organized in conjunction with AAAI-07; at Vancouver; British Columbi...,Ullas Nambiar; Zaiqing Nie,Information Integration on the Web Information integration techniques enable the interactionbetween users and data sources through a centralized access point and uniform queryinterfaces that give users the illusion of querying a homogeneous system. Most integrationsolutions have assumed structured sources with the heterogeneity being introduced by thevariety in source schemas and data models. But a large portion of the web consists of pagesthat contain information presented as unstructured text; such as blogs; wikis; reviews; and soon. Therefore; integration systems that can match entities/objects across both structured andunstructured sources are the need of the hour. Recent research in web object extraction;record linkage and named entity recognition have generated some initial solutions.However; many challenges remain in developing such a system. Aim of the Workshop …,*,2007,*
Mining; Using and Maintaining Source Statistics for Adaptive Data Integration,Jianchun Fan; Subbarao Kambhampati; Zaiqing Nie,The availability of structured information sources on the web has recently led to significantinterest in query processing frameworks that can integrate information sources available onthe Internet. Data integration systems [Duschka et al. 2000; Parts of this paper–dealing withthe learning of coverage and overlap statistics–have been presented at ICDE 2004 [Nie andKambhampati 2004]. This paper significantly expands the ICDE paper by (1) Consideringincremental maintenance of statistics (2) learning source latency statistics and (3)considering multi-objective source selection using coverage as well as latency statistics.This paper is also differs in content and approach from a related journal publication in IEEETKDE [Nie et al. 2004]. Please see the related work section for details. Authors'Addresses:(Authors' names listed in alphabetical order) Jianchun Fan and Subbarao …,*,2005,*
Design of multitasking and windows software for beam diagnostic system on HIRFL,Zhenpeng Nie; Zhiqing Shen; Xiangyang Xu; Jianping Zheng; Jingyu Tang; Jinmei Dong,An introduction is given to the design idea and method of multitasking and Windowssoftware for beam diagnostic system on HIRFL. The testing result is presented in the end.The software has many advantages such as powerful function; visual display; high reliabilityand friendly interface; etc,Nuclear Electronics and Detection Technology,2002,*
Design of beam position monitor system in HIRFL-CSRm,Zhenpeng Nie; Yong Zheng; Zhiqing Shen,The basic principle for the beam position monitor system in HIRFL-CSRm is described.The parameters of the PICKUP monitor are chosen and the correlative calculationis performed. A desktop experiment is completed; too.,Atomic Energy Science and Technology,2000,*
The application of image acquisition and processing technology in measurement of beam profile on particle accelerator,Zhenpeng Nie; Yong Zheng; Zhiqing Shen; Shaoming Wang,An introduction is given to the real-time measuring method which can measure the intensityand profile of the beam by a scintillator screen on HIRFL (Heavy Ion Research Facility ofLanzhou). Hardware structure is described briefly; methods of the software design aremainly presented. The system can make a dynamic analysis on the faculae image and hasmany advantages; such as good reliability; high precision; intuitional measurement; friendlyinterface of the application software etc. Finally some results of measurement are given,Nuclear Electronics and Detection Technology,2000,*
HIRFL beam emittance measurement by gradient method,Hongwei Zhao; Zhiqing Shen; Baowen Wei,HIRFL beam 12 C 5+ emittance is measured by means of quadrupole and secondaryemission multi-wire profile monitor. A series of measurements are performed and a weightedleast squares fit is used in the data processing. The method of emittance calculation and itserror calculation are described. Some of the measurement results are also presented,Atomic Energy Science and Technology,1994,*
SFC beam central phase measurement,Zhaohui Yang; Feng Ye; Zhiqing Shen; Zhengrong Huang; Hui Wang; Weiqing Yang,Beam central phase measurement is important for isochronous cyclotron in improvement ofbeam quality; beam intensity and extracting efficiency. The procedure of beam central phasemeasurement in SFC; including brief introductions about principle; preparations and results;have been described,*,1994,*
HIRFL beam emittance measurements with'three-profile method',Hongwei Zhao; Zhiqing Shen; Baowen Wei,HIRFL beam (12 C 6+) emittance has been measured with'Three-Profile Method'; in whichthe beam sizes at three different locations in a field-free drift space are measured by threesecondary emission multi-wire profile monitors. Measurement results are presented here.Particular emphasis has been laid on the calculations and discussions relating theoreticalerror of emittance measurements with the separation of the three profile monitors; thelocation and size of the beam waist so that the optimum conditions of emittancemeasurements could be determined,High Power Laser and Particle Beams,1993,*
Beam diagnostic system of HIRFL,Zhi-qing Shen; Yu-de Wang,1.Injector SFC: Four radial probes are set to measure the beam density;orbit location and beamvertical position. Seven central phase probes are used to measure the beam center phase alongradial direction. 2.The Beam Line for SFC to SSC: Fig.l shows the diagnostic element locationsalong the beam line from SFC to SSC.S sets of Farady cups;20 sets of secondary emi- ssionmulti-wire profile monitors and 3 sets of emmittance measurement system are mounted alongthe beam line. The beam emergy and energy dispersion are measured by using 74° bendingmagnet;two slits (set respectively in the front and behind of the magnet) and a Faraday cup. Anothermethod for measuring the beam energy and energy dispersion is by means of nuclearreaction;the target chamber is located behind the 50° bending magnet. The results of both methodscan be corrected each other. 3.Main Accelerator SSC: Two kinds of diag- nostic devices …,*,1989,*
A Machine Learning Framework for Combined Information Extraction and Integration,Fei Wu; Zaiqing Nie; Ji-Rong Wen; Wei-Ying Ma,ABSTRACT There are various kinds of objects embedded in static Web pages and onlineWeb databases. Extracting and integrating these objects from the Web is of greatsignificance for Web data management. The existing Web information extraction (IE)techniques cannot provide satisfactory solution to the Web object extraction task sinceobjects of the same type are distributed in diverse Web sources; whose structures are highlyheterogenous. The classic information extraction (IE) methods; which are designed forprocessing plain text documents; also fail to meet our requirements. In this paper; wepropose a novel approach called Object-Level Information Extraction (OLIE) to extract Webobjects. This approach extends a classic IE algorithm; Conditional Random Fields (CRF); byadding Web-specific information. It is essentially a combination of Web IE and classic IE …,*,*,*
Supplementary Materials for Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields,Jingwei Zhuo; Yong Cao; Jun Zhu; Bo Zhang; Zaiqing Nie,Given training data; all the parameters of grSemi-CRFs can be learnt by maximizing loglikelihood; ie; L= log p (s| x). To simplify representations; we introduce some auxiliarynotations; including g (hj; dj; yj− 1; yj)= F (sj; x)+ A (yj− 1; yj) and G (s; x)=∑| s| j= 1 g (hj; dj;yj− 1; yj). Then the likelihood can be rewritten as p (s| x)=,*,*,*
The Control System of HIRFL,Jiao Tianshu; Li Tianyou; Ma Siwen; Chu Zhensheng; Huang Tuanhua; Zhou Xun; Wang Zhen; Shen Zhiqing,The Heavy Ion Research Facility in Lanzhou (HIRFL) is a multi-purpose and variable energymachine designed to accelerate wide range of ions.'" In order to obtain a designed beam(paru'cale and energy) and to transport it to a proper experimental areas in a short time; it requires to modify a great number of parameters; this cannot be easily achieved without thehelp of a computer.,*,*,*
BibFinder/StatMiner,Effectively Mining; Zaiqing Nie; Subbarao Kambhampati; Thomas Hernandez,Page 1. BibFinder/StatMiner Architecture Query Processing in Data Integration Using the LearnedStatistics Effects of Learned Statistics on BibFinder 0 200;000 400;000 600;000 800;000 1;000;0001;200;000 1;400;000 1;600;000 0.03 0.13 0.23 0.33 0.43 0.53 0.63 0.73 minfreq(%) M e m o ryC ons um ption (b y tes ) minoverlap=0 minoverlap=0.1 minoverlap=0.2 minoverlap=0.3 0.4 0.50.6 0.7 0.8 0.9 1 0.03 0.13 0.23 0.33 0.43 0.53 0.63 0.73 minfreq(%) p re c isio n RS SG0 GS0SG0.3 GS0.3 28 33 38 43 48 53 0.03 0.13 0.23 0.33 0.43 0.53 0.63 0.73 minfreq(%) Nu mber ofd istin ct a n sw e rs RS SG0 GS0 SG0.3 GS0.3 BibFinder/StatMiner : Effectively Mining and UsingCoverage and Overlap Statistics in Data Integration Zaiqing Nie Subbarao Kambhampati ThomasHernandez Arizona State University; USA Project Havasu StatMiner ∑ − = i i i QSP QSP QQd 2)]2|ˆ( )1|ˆ([ )2;1( );( ) 1 )( CQd CP QP C tightness CQ …,*,*,*
Joint Use of Multiple Learned Statistics for Improving Online Source Selection,Thomas Hernandez; Zaiqing Nie; Subbarao Kambhampati,ABSTRACT The autonomous and decentralized nature of available online sources preventsmost existing integration systems from supporting flexible query processing that takes intoaccount conflicting user objectives such as coverage; cost-related; or data-quality objectives.To achieve multi-objective query processing; a data integration system must be able todetermine which sources are most relevant for a particular query; given the desiredobjectives. To do so; it must gather and use source-specific statistics. In this paper wepresent an approach which automatically gathers coverage and overlap statistics as well asresponse time statistics; and jointly uses these statistics to select relevant sources. Wedescribe our approach and present experimental results done in the context of BibFinderthat demonstrate the efficiency and effectiveness of our approach.,*,*,*
A 2D Conditional Random Fields Model for Web Information Extraction,Jun Zhu; Zaiqing Nie; Ji-Rong Wen; Bo Zhang; Wei-Ying Ma,Abstract The Web contains an abundance of useful semi-structured information about realworld objects; and our empirical study shows that strong sequence characteristics exist forthe Web information about the objects of the same type across different Web sites. Thispaper introduces a two dimensional Conditional Random Fields model; incorporating thesequence characteristics and the 2D neighborhood dependencies; to automatically extractobject information from the Web. We also present the experimental results comparing ourmodel with the linear-chain CRF model in the domain of product information extraction. Theexperimental results show that our model significantly outperforms existing CRF models.,*,*,*
Aggregation-Aware Top-k Computation for Full-Text Search,Mingjie Zhu; Shuming Shi; Zaiqing Nie; Ji-Rong Wen,ABSTRACT A typical scenario in information retrieval and web search is to index a giventype of items (eg; web pages; images) and provide search functionality for them. In such ascenario; the basic units of indexing and retrieval are the same. Extensive study has beendone for efficient top-k computation in such settings. This paper studies top-k processing formany emerging scenarios: efficiently retrieving top-k items of one type based on the invertedindex of another type of items. It would be very inefficient by directly utilizing traditional top-kapproaches. Here we follow TA (the Threshold Algorithm) in this scenario. We present anaggregationaware top-k computation framework with three pruning principles upon theconventional inverted index and a novel inverted index type HybridRank; which employs theitem information of both types. Experimental results show that our proposed new index …,*,*,*
