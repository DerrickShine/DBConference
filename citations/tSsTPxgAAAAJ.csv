Efficient entity resolution for large heterogeneous information spaces,George Papadakis; Ekaterini Ioannou; Claudia Niederée; Peter Fankhauser,Abstract We have recently witnessed an enormous growth in the volume of structured andsemi-structured data sets available on the Web. An important prerequisite for using andcombining such data sets is the detection and merge of information that describes the samereal-world entities; a task known as Entity Resolution. To make this quadratic task efficient;blocking techniques are typically employed. However; the high dynamics; loose schemabinding; and heterogeneity of (semi-) structured data; impose new challenges to entityresolution. Existing blocking approaches become inapplicable because they rely on thehomogeneity of the considered data and a-priory known schemata. In this paper; weintroduce a novel approach for entity resolution; scaling it up for large; noisy; andheterogeneous information spaces. It combines an attribute-agnostic mechanism for …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,64
A blocking framework for entity resolution in highly heterogeneous information spaces,George Papadakis; Ekaterini Ioannou; Themis Palpanas; Claudia Niederee; Wolfgang Nejdl,In the context of entity resolution (ER) in highly heterogeneous; noisy; user-generated entitycollections; practically all block building methods employ redundancy to achieve higheffectiveness. This practice; however; results in a high number of pairwise comparisons; witha negative impact on efficiency. Existing block processing strategies aim at discardingunnecessary comparisons at no cost in effectiveness. In this paper; we systemize blockingmethods for clean-clean ER (an inherently quadratic task) over highly heterogeneousinformation spaces (HHIS) through a novel framework that consists of two orthogonal layers:the effectiveness layer encompasses methods for building overlapping blocks with smalllikelihood of missed matches; the efficiency layer comprises a rich variety of techniques thatsignificantly restrict the required number of pairwise comparisons; having a controllable …,IEEE Transactions on Knowledge and Data Engineering,2013,56
Content vs. context for sentiment analysis: a comparative analysis over microblogs,Fotis Aisopos; George Papadakis; Konstantinos Tserpes; Theodora Varvarigou,Abstract Microblog content poses serious challenges to the applicability of traditionalsentiment analysis and classification methods; due to its inherent characteristics. To tacklethem; we introduce a method that relies on two orthogonal; but complementary sources ofevidence: content-based features captured by n-gram graphs and context-based onescaptured by polarity ratio. Both are language-neutral and noise-tolerant; guaranteeing higheffectiveness and robustness in the settings we are considering. To ensure our approachcan be integrated into practical applications with large volumes of data; we also aim atenhancing its time efficiency: we propose alternative sets of features with low extraction cost;explore dimensionality reduction and discretization techniques and experiment with multipleclassification algorithms. We then evaluate our methods over a large; real-world data set …,Proceedings of the 23rd ACM conference on Hypertext and social media,2012,48
Meta-blocking: Taking entity resolutionto the next level,George Papadakis; Georgia Koutrika; Themis Palpanas; Wolfgang Nejdl,Entity Resolution is an inherently quadratic task that typically scales to large data collectionsthrough blocking. In the context of highly heterogeneous information spaces; blockingmethods rely on redundancy in order to ensure high effectiveness at the cost of lowerefficiency (ie; more comparisons). This effect is partially ameliorated by coarse-grained blockprocessing techniques that discard entire blocks either a-priori or during the resolutionprocess. In this paper; we introduce meta-blocking as a generic procedure that intervenesbetween the creation and the processing of blocks; transforming an initial set of blocks into anew one with substantially fewer comparisons and equally high effectiveness. In essence;meta-blocking aims at extracting the most similar pairs of entities by leveraging theinformation that is encapsulated in the block-to-entity relationships. To this end; it first …,IEEE Transactions on Knowledge and Data Engineering,2014,43
Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data,George Papadakis; Ekaterini Ioannou; Claudia Niederée; Themis Palpanas; Wolfgang Nejdl,Abstract A prerequisite for leveraging the vast amount of data available on the Web is EntityResolution; ie; the process of identifying and linking data that describe the same real-worldobjects. To make this inherently quadratic process applicable to large data sets; blocking istypically employed: entities (records) are grouped into clusters-the blocks-of matchingcandidates and only entities of the same block are compared. However; novel blockingtechniques are required for dealing with the noisy; heterogeneous; semi-structured; user-generateddata in the Web; as traditional blocking techniques are inapplicable due to theirreliance on schema information. The introduction of redundancy; improves the robustness ofblocking methods but comes at the price of additional computational cost. In this paper; wepresent methods for enhancing the efficiency of redundancy-bearing blocking methods …,Proceedings of the fifth ACM international conference on Web search and data mining,2012,43
Sentiment analysis of social media content using N-Gram graphs,Fotis Aisopos; George Papadakis; Theodora Varvarigou,Abstract Sentiment Analysis over Social Media facilitates the extraction of useful conclusionsabout the average public opinion on a variety of topics; but poses serious technicalchallenges. This is because of the sparse; noisy; multilingual content that is posted on-lineby Social Media users. In this paper; we introduce a novel method for capturing textualpatterns that inherently supports this challenging type of content. In essence; it creates agraph whose nodes correspond to the character n-grams of a document; while its weightededges denote the average distance between them. Multiple documents of the same polaritycan be aggregated into a polarity class graph; which can be compared with individualdocuments in order to identify the category of their sentiment. To evaluate our approach; weconducted large scale experiments on a real-world data set stemming from a snapshot of …,Proceedings of the 3rd ACM SIGMM international workshop on Social media,2011,42
Eliminating the redundancy in blocking-based entity resolution methods,George Papadakis; Ekaterini Ioannou; Claudia Niederée; Themis Palpanas; Wolfgang Nejdl,Abstract Entity resolution is the task of identifying entities that refer to the same real-worldobject. It has important applications in the context of digital libraries; such as citationmatching and author disambiguation. Blocking is an established methodology for efficientlyaddressing this problem; it clusters similar entities together; and compares solely entitiesinside each cluster. In order to effectively deal with the current large; noisy andheterogeneous data collections; novel blocking methods that rely on redundancy have beenintroduced: they associate each entity with multiple blocks in order to increase recall; thusincreasing the computational cost; as well. In this paper; we introduce novel techniques thatremove the superfluous comparisons from any redundancy-based blocking method. Theyimprove the time-efficiency of the latter without any impact on the end result. We present …,Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries,2011,35
Representation models for text classification: a comparative analysis over three web document types,George Giannakopoulos; Petra Mavridi; Georgios Paliouras; George Papadakis; Konstantinos Tserpes,Abstract Text classification constitutes a popular task in Web research with variousapplications that range from spam filtering to sentiment analysis. To address it; patterns of co-occurring words or characters are typically extracted from the textual content of Webdocuments. However; not all documents are of the same quality; for example; the curatedcontent of news articles usually entails lower levels of noise than the user-generated contentof the blog posts and the other Social Media. In this paper; we provide some insight and apreliminary study on a tripartite categorization of Web documents; based on inherentdocument characteristics. We claim and support that each category calls for differentclassification settings with respect to the representation model. We verify this claimexperimentally; by showing that topic classification on these different document types …,Proceedings of the 2nd international conference on web intelligence; mining and semantics,2012,27
Comparative analysis of a-priori and a-posteriori dietary patterns using state-of-the-art classification algorithms: a case/case-control study,Christina-Maria Kastorini; George Papadakis; Haralampos J Milionis; Kallirroi Kalantzi; Paolo-Emilio Puddu; Vassilios Nikolaou; Konstantinos N Vemmos; John A Goudevenos; Demosthenes B Panagiotakos,Abstract Objective To compare the accuracy of a-priori and a-posteriori dietary patterns inthe prediction of acute coronary syndrome (ACS) and ischemic stroke. This is actually thefirst study to employ state-of-the-art classification methods for this purpose. Methods andmaterials During 2009–2010; 1000 participants were enrolled; 250 consecutive patients witha first ACS and 250 controls (60±12 years; 83% males); as well as 250 consecutive patientswith a first stroke and 250 controls (75±9 years; 56% males). The controls were population-based and age-sex matched to the patients. The a-priori dietary patterns were derived fromthe validated MedDietScore; whereas the a-posteriori ones were extracted from principalcomponents analysis. Both approaches were modeled using six classification algorithms:multiple logistic regression (MLR); naïve Bayes; decision trees; repeated incremental …,Artificial intelligence in medicine,2013,23
Beyond the usual suspects: context-aware revisitation support,Ricardo Kawase; George Papadakis; Eelco Herder; Wolfgang Nejdl,Abstract A considerable amount of our activities on the Web involves revisits to pages orsites. Reasons for revisiting include active monitoring of content; verification of information;regular use of online services; and reoccurring tasks. Browsers support for revisitation ismainly focused on frequently and recently visited pages. In this paper we present a dynamicbrowser toolbar that provides recommendations beyond these usual suspects; balancingdiversity and relevance. The recommendation method used is a combination of ranking andpropagation methods. Experimental outcomes show that this algorithm performs significantlybetter than the baseline method. Further experiments address the question whether it ismore appropriate to recommend specific pages or rather (portal pages of) Web sites. Weconducted two user studies with a dynamic toolbar that relies on our recommendation …,Proceedings of the 22nd ACM conference on Hypertext and hypermedia,2011,22
Parallel meta-blocking: Realizing scalable entity resolution over large; heterogeneous data,Vasilis Efthymiou; George Papadakis; George Papastefanatos; Kostas Stefanidis; Themis Palpanas,Entity resolution constitutes a crucial task for many applications; but has an inherentlyquadratic complexity. Typically; it scales to large volumes of data through blocking: similarentities are clustered into blocks so that it suffices to perform comparisons only within eachblock. Meta-blocking further increases efficiency by cleaning the overlapping blocks fromunnecessary comparisons. However; even Meta-blocking can be time-consuming: applyingit to blocks with 7.4 million entities and 2.21011 comparisons takes almost 8 days on amodern high-end server. In this paper; we parallelize Meta-blocking based on MapReduce.We propose a simple strategy that explicitly creates the core concept of Meta-blocking; theblocking graph. We then describe an advanced strategy that creates the blocking graphimplicitly; reducing the overhead of data exchange. We also introduce a load balancing …,Big Data (Big Data); 2015 IEEE International Conference on,2015,21
Comparative analysis of approximate blocking techniques for entity resolution,George Papadakis; Jonathan Svirsky; Avigdor Gal; Themis Palpanas,Abstract Entity Resolution is a core task for merging data collections. Due to its quadraticcomplexity; it typically scales to large volumes of data through blocking: similar entities areclustered into blocks and pair-wise comparisons are executed only between co-occurringentities; at the cost of some missed matches. There are numerous blocking methods; and theaim of this work is to offer a comprehensive empirical survey; extending the dimensions ofcomparison beyond what is commonly available in the literature. We consider 17 state-of-the-art blocking methods and use 6 popular real datasets to examine the robustness of theirinternal configurations and their relative balance between effectiveness and time efficiency.We also investigate their scalability over a corpus of 7 established synthetic datasets thatrange from 10;000 to 2 million entities.,Proceedings of the VLDB Endowment,2016,19
Schema-agnostic vs schema-based configurations for blocking methods on homogeneous data,George Papadakis; George Alexiou; George Papastefanatos; Georgia Koutrika,Abstract Entity Resolution constitutes a core task for data integration that; due to its quadraticcomplexity; typically scales to large datasets through blocking methods. These can beconfigured in two ways. The schema-based configuration relies on schema information inorder to select signatures of high distinctiveness and low noise; while the schema-agnosticone treats every token from all attribute values as a signature. The latter approach hassignificant potential; as it requires no fine-tuning by human experts and it applies toheterogeneous data. Yet; there is no systematic study on its relative performance withrespect to the schema-based configuration. This work covers this gap by comparinganalytically the two configurations in terms of effectiveness; time efficiency and scalability.We apply them to 9 established blocking methods and to 11 benchmarks of structured …,Proceedings of the VLDB Endowment,2015,16
Supervised meta-blocking,George Papadakis; George Papastefanatos; Georgia Koutrika,Abstract Entity Resolution matches mentions of the same entity. Being an expensive task forlarge data; its performance can be improved by blocking; ie; grouping similar entities andcomparing only entities in the same group. Blocking improves the run-time of EntityResolution; but it still involves unnecessary comparisons that limit its performance. Meta-blocking is the process of restructuring a block collection in order to prune suchcomparisons. Existing unsupervised meta-blocking methods use simple pruning rules;which offer a rather coarse-grained filtering technique that can be conservative (ie; keepingtoo many unnecessary comparisons) or aggressive (ie; pruning good comparisons). In thiswork; we introduce supervised meta-blocking techniques that learn classification models fordistinguishing promising comparisons. For this task; we propose a small set of generic …,Proceedings of the VLDB Endowment,2014,16
Decay-based Ranking for Social Application Content.,George Papadakis; Claudia Niederée; Wolfgang Nejdl,Abstract: Social applications are prone to information explosion; due to the proliferation ofuser generated content. Locating and retrieving information in their context poses; therefore;a great challenge. Classical information retrieval methods are; however; inadequate in thisenvironment and users inevitably drown in an information flood. In this paper; we present anovel method that facilitates users' information quests by identifying and improving theaccessibility of the most important resources. This is achieved through an informationvaluation method that estimates how likely it is for each information item to be accessed inthe near future. The experiments verify that our method performs significantly better thanothers typically used in social applications; while being more versatile; too.,WEBIST (1),2010,16
Textual and contextual patterns for sentiment analysis over microblogs,Fotis Aisopos; George Papadakis; Konstantinos Tserpes; Theodora Varvarigou,Abstract Microblog content poses serious challenges to the applicability of sentimentanalysis; due to its inherent characteristics. We introduce a novel method relying on content-based and context-based features; guaranteeing high effectiveness and robustness in thesettings we are considering. The evaluation of our methods over a large Twitter data setindicates significant improvements over the traditional techniques.,Proceedings of the 21st International Conference on World Wide Web,2012,15
The missing links: Discovering hidden same-as links among a billion of triples,George Papadakis; Gianluca Demartini; Peter Fankhauser; Philipp Kärger,Abstract The Semantic Web is constantly gaining momentum; as more and more Web sitesand content providers adopt its principles. At the core of these principles lies the Linked Datamovement; which demands that data on the Web shall be annotated and linked amongdifferent sources; instead of being isolated in data silos. In order to materialize this vision ofa web of semantics; existing resource identifiers should be reused and shared betweendifferent Web sites. This is not always the case with the current state of the Semantic Web;since multiple identifiers are; more often than not; redundantly introduced for the sameresources. In this paper we introduce a novel approach to automatically detect redundantidentifiers solely by matching the URIs of information resources. The approach; based on acommon pattern among Semantic Web URIs; provides a simple and practical method for …,Proceedings of the 12th International Conference on Information Integration and Web-based Applications & Services,2010,15
Efficient entity resolution methods for heterogeneous information spaces,George Papadakis; Wolfgang Nejdl,The Web of Data encompasses a voluminous; yet constantly expanding collection ofstructured and semi-structured data sets. An important prerequisite for leveraging on them isthe detection (and merge) of information that describe the same real-world entities; a taskknown as Entity Resolution. To enhance the efficiency of this quadratic task; blockingtechniques are typically employed. They are; however; inapplicable to the Web of Data; dueto the noise; the loose schema binding as well as the unprecedented heterogeneity inherentin it. In the context of my thesis; I focus on developing novel blocking methods that scale upEntity Resolution within such large; noisy; and heterogeneous information spaces. At theircore lies an attribute-agnostic mechanism that relies exclusively on the values of entityprofiles in order to build blocks effectively. The resulting set of blocks is processed …,Data Engineering Workshops (ICDEW); 2011 IEEE 27th International Conference on,2011,13
The impact of bookmarks and annotations on refinding information,Ricardo Kawase; George Papadakis; Eelco Herder; Wolfgang Nejdl,Abstract Refinding information has been interwoven with web activity since its earlybeginning. Even though all common web browsers were equipped with a history list andbookmarks early enough to facilitate this need; most users typically use search engines torefind information. However; both bookmarks and search based tools have significantlimitations that impact their usability: the former are known to be hard to manage over thecourse of time; whereas the latter require the user to recall a specific combination ofkeywords or context. Most importantly; though; both are particularly inappropriate in caseswhere a piece of information is contained within an unstructured web page. In this paper; wepresent in-context annotation as a more efficient alternative to these methodologies. Toverify this claim; we conducted a study in which we compare the performance of …,Proceedings of the 21st ACM conference on Hypertext and hypermedia,2010,13
To compare or not to compare: making entity resolution more efficient,George Papadakis; Ekaterini Ioannou; Claudia Niederée; Themis Palpanas; Wolfgang Nejdl,Abstract Blocking methods are crucial for making the inherently quadratic task of EntityResolution more efficient. The blocking methods proposed in the literature rely on thehomogeneity of data and the availability of binding schema information; thus; they areinapplicable to the voluminous; noisy; and highly heterogeneous data of the Web 2.0 user-generated content. To deal with such data; attribute-agnostic blocking has been recentlyintroduced; following a two-fold strategy: the first layer places entities into overlapping blocksin order to achieve high effectiveness; while the second layer reduces the number ofunnecessary comparisons in order to enhance efficiency. In this paper; we present a set oftechniques that can be plugged into the second strategy layer of attribute-agnostic blockingto further improve its efficiency. We introduce a technique that eliminates redundant …,Proceedings of the international workshop on semantic web information management,2011,12
Scaling Entity Resolution to Large; Heterogeneous Data with Enhanced Meta-blocking.,George Papadakis; George Papastefanatos; Themis Palpanas; Manolis Koubarakis,ABSTRACT Entity Resolution constitutes a quadratic task that typically scales to large entitycollections through blocking. The resulting blocks can be restructured by Meta-blocking inorder to significantly increase precision at a limited cost in recall. Yet; its processing can betime-consuming; while its precision remains poor for configurations with high recall. In thiswork; we propose new meta-blocking methods that improve precision by up to an order ofmagnitude at a negligible cost to recall. We also introduce two efficiency techniques that;when combined; reduce the overhead time of Metablocking by more than an order ofmagnitude. We evaluate our approaches through an extensive experimental study over 6realworld; heterogeneous datasets. The outcomes indicate that our new algorithmsoutperform all meta-blocking techniques as well as the state-of-the-art methods for block …,EDBT,2016,11
Large-scale evaluation framework for local influence theories in Twitter,Magdalini Kardara; George Papadakis; Athanasios Papaoikonomou; Konstantinos Tserpes; Theodora Varvarigou,Abstract Influence theories constitute formal models that identify those individuals that areable to affect and guide their peers through their activity. There is a large body of work ondeveloping such theories; as they have important applications in viral marketing;recommendations; as well as information retrieval. Influence theories are typically evaluatedthrough a manual process that cannot scale to data voluminous enough to draw safe;representative conclusions. To overcome this issue; we introduce in this paper a formalizedframework for large-scale; automatic evaluation of topic-specific influence theories that arespecialized in Twitter. Basically; it consists of five conjunctive conditions that are indicative ofreal influence exertion: the first three determine which influence theories are compatible withour framework; while the other two estimate their relative effectiveness. At the core of …,Information Processing & Management,2015,11
Influence patterns in topic communities of social media,Magdalini Kardara; George Papadakis; Thanos Papaoikonomou; Konstantinos Tserpes; Theodora Varvarigou,Abstract Users of Social Media typically gather into communities on the basis of somecommon interest. Their interactions inside these on-line communities follow several;interesting patterns. For example; they differ in the level of influence they exert to the rest ofthe group: some community members are actively involved; affecting a large part of thecommunity with their actions; while the majority comprises plain participants (eg; informationconsumers). Identifying users of the former category lies on the focus of interest of manyrecent works; as they can be employed in a variety of applications; like targeted marketing.In this paper; we build on previous research that examined influencers in the context of apopular Social Media web site; namely Twitter. Unlike existing works that consider its userbase as a whole; we focus on communities that are created on-the-fly by people that post …,Proceedings of the 2nd International Conference on Web Intelligence; Mining and Semantics,2012,11
Detecting and exploiting stability in evolving heterogeneous information spaces,George Papadakis; George Giannakopoulos; Claudia Niederée; Themis Palpanas; Wolfgang Nejdl,Abstract Individuals contribute content on the Web at an unprecedented rate; accumulatingimmense quantities of (semi-) structured data. Wisdom of the Crowds theory advocates thatsuch information (or parts of it) is constantly overwritten; updated; or even deleted by otherusers; with the goal of rendering it more accurate; or up-to-date. This is particularly true forthe collaboratively edited; semi-structured data of entity repositories; whose entity profilesare consistently kept fresh. Therefore; their core information that remain stable with thepassage of time; despite being reviewed by numerous users; are particularly useful for thedescription of an entity. Based on the above hypothesis; we introduce a classificationscheme that predicts; on the basis of statistical and content patterns; whether an attribute (ie;name-value pair) is going to be modified in the future. We apply our scheme on a large …,Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries,2011,7
An Ontology for Social Networking Sites Interoperability.,Konstantinos Tserpes; George Papadakis; Magdalini Kardara; Athanasios Papaoikonomou; Fotis Aisopos; Emmanuel Sardis; Theodora A Varvarigou,Abstract: The Social Networking Sites (SNS) comprise a pool from which developers canpump functionality and data. Usable REST APIs are providing access to two valuablebusiness assets: the users' generated content and social graph. The lack of standards andthe antagonistic nature of the SNSs have resulted in the use of proprietary API specificationsand-in turn-data models. Each SNS uses a different method to access and a different way todescribe notions which are largely similar; eg “friends” or shared “multimedia items”. Theconceptual similarity between entities “living” in the SNS; creates a remarkable opportunity:The aggregation of the social functionality and data can provide the basis for a uniqueplatform on top of which third parties can deploy new added value services; seamlesslyusing the underlying SNSs APIs. This paper presents an attempt to implement this …,KEOD,2012,6
In-context annotations for refinding and sharing,Ricardo Kawase; Eelco Herder; George Papadakis; Wolfgang Nejdl,Abstract Annotations support understanding; interpretation; sensemaking and scannability.As valuable as in paper-based contexts; digital online annotations provide several benefitsfor annotators and collaborators. To further explore the real benefits of online annotations;we implemented a simple Web Annotation tool; SpreadCrumbs; to support our studies. Thetool provides a simple annotation mechanism; simulating real-world paper-basedannotations. In addition; the tool supports search; sharing capabilities and social navigation.We conducted a series of user studies that empirically demonstrates the benefits of “in-context” annotations for refinding and sharing.,International Conference on Web Information Systems and Technologies,2010,6
The BigDataEurope platform–supporting the variety dimension of big data,Sören Auer; Simon Scerri; Aad Versteden; Erika Pauwels; Angelos Charalambidis; Stasinos Konstantopoulos; Jens Lehmann; Hajira Jabeen; Ivan Ermilov; Gezim Sejdiu; Andreas Ikonomopoulos; Spyros Andronopoulos; Mandy Vlachogiannis; Charalambos Pappas; Athanasios Davettas; Iraklis A Klampanos; Efstathios Grigoropoulos; Vangelis Karkaletsis; Victor de Boer; Ronald Siebes; Mohamed Nadjib Mami; Sergio Albani; Michele Lazzarini; Paulo Nunes; Emanuele Angiuli; Nikiforos Pittaras; George Giannakopoulos; Giorgos Argyriou; George Stamoulis; George Papadakis; Manolis Koubarakis; Pythagoras Karampiperis; Axel-Cyrille Ngonga Ngomo; Maria-Esther Vidal,Abstract The management and analysis of large-scale datasets–described with the term BigData–involves the three classic dimensions volume; velocity and variety. While the formertwo are well supported by a plethora of software components; the variety dimension is stillrather neglected. We present the BDE platform–an easy-to-deploy; easy-to-use andadaptable (cluster-based and standalone) platform for the execution of big data componentsand tools like Hadoop; Spark; Flink; Flume and Cassandra. The BDE platform was designedbased upon the requirements gathered from seven of the societal challenges put forward bythe European Commission in the Horizon 2020 programme and targeted by theBigDataEurope pilots. As a result; the BDE platform allows to perform a variety of Big Dataflow tasks like message passing; storage; analysis or publishing. To facilitate the …,International Conference on Web Engineering,2017,5
Supporting revisitation with contextual suggestions,Ricardo Kawase; George Papadakis; Eelco Herder,Abstract Web browsers provide only little support for users to revisit pages that they do notvisit very often. We developed a browser toolbar that reminds users of visited pages relatedto the page that they currently viewing. The recommendation method combines ranking withpropagation methods. A user evaluation shows that on average 22.7% of the revisits weretriggered by the toolbar; a considerable change on the participants' revisitation routines. Inthis paper we discuss the value of the recommendations and the implications derived fromthe evaluation.,Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries,2011,5
Boosting the efficiency of large-scale entity resolution with enhanced meta-blocking,George Papadakis; George Papastefanatos; Themis Palpanas; Manolis Koubarakis,Abstract Entity Resolution constitutes a quadratic task that typically scales to large entitycollections through blocking. The resulting blocks can be restructured by Meta-blocking toraise precision at a limited cost in recall. At the core of this procedure lies the blocking graph;where the nodes correspond to entities and the edges connect the comparable pairs. Thereare several configurations for Meta-blocking; but no hints on best practices. In general; thenode-centric approaches are more robust and suitable for a series of applications; but sufferfrom low precision; due to the large number of unnecessary comparisons they retain. In thiswork; we present three novel methods for node-centric Meta-blocking that significantlyimprove precision. We also introduce a pre-processing method that restricts the size of theblocking graph by removing a large number of noisy edges. As a result; it reduces the …,Big Data Research,2016,4
Methods for web revisitation prediction: survey and experimentation,George Papadakis; Ricardo Kawase; Eelco Herder; Wolfgang Nejdl,Abstract More than 45% of the pages that we visit on the Web are pages that we have visitedbefore. Browsers support revisits with various tools; including bookmarks; history views andURL auto-completion. However; these tools only support revisits to a small number offrequently and recently visited pages. Several browser plugins and extensions have beenproposed to better support the long tail of less frequently visited pages; usingrecommendation and prediction techniques. In this article; we present a systematic overviewof revisitation prediction techniques; distinguishing them into two main types and severalsubtypes. We also explain how the individual prediction techniques can be combined intocomprehensive revisitation workflows that achieve higher accuracy. We investigate theperformance of the most important workflows and provide a statistical analysis of the …,User Modeling and User-Adapted Interaction,2015,4
Experiences in building the public web history repository,Eelco Herder; Ricardo Kawase; George Papadakis,Learning has become an integral part of many people's everyday working life. A moreknowledge-based society and rapid changes in technology requires practically everyone;but in particular knowledge workers; to search for and read information in order to keep up-to-date. The character of learning at the workplace has shifted from a solitary; paperbasedactivity to a Web-based activity; making use of various resources; including discussionforums and social networking sites [2]. At the same time; we use the Web for our other dailyactivities. Search engines; travel planners; dictionaries and other online services havebecome essential for dealing with everyday tasks. News sites; portals; online games andstreaming video are popular resources for information and entertainment. We communicatewith our friends via email; social networking; forums; blogs and chat. As a result; one ends …,Proc. of Datatel Workshop,2011,4
Efficient term cloud generation for streaming web content,Odysseas Papapetrou; George Papadakis; Ekaterini Ioannou; Dimitrios Skoutas,Abstract Large amounts of information are posted daily on the Web; such as articlespublished online by traditional news agencies or blog posts referring to and commenting onvarious events. Although the users sometimes rely on a small set of trusted sources fromwhich to get their information; they often also want to get a wider overview and glimpse ofwhat is being reported and discussed in the news and the blogosphere. In this paper; wepresent an approach for supporting this discovery and exploration process by exploitingterm clouds. In particular; we provide an efficient method for dynamically computing the mostfrequently appearing terms in the posts of monitored online sources; for time intervalsspecified at query time; without the need to archive the actual published content. Anexperimental evaluation on a large-scale real-world set of blogs demonstrates the …,International Conference on Web Engineering,2010,4
How Predictable Are You? A Comparison of Prediction Algorithms for Web Page Revisitation.,Ricardo Kawase; George Papadakis; Eelco Herder,Abstract Users return to Web pages for various reasons. Apart from pages visited due tobacktracking; users typically monitor a number of favorite pages; while dealing with tasksthat reoccur on an infrequent basis. In this paper; we introduce a novel method for predictingthe next revisited page in a certain user context that; unlike existing methods; doesn't rely onmachine learning algorithms. We evaluate it over a large data set comprising thenavigational activity of 25 users over a period of 6 months. The outcomes suggest asignificant improvement over methods typically used in this context; thus paving the way forexploring new means of improving user's navigational support.,LWA,2010,4
Social media meta-API: leveraging the content of social networks,George Papadakis; Konstantinos Tserpes; Emmanuel Sardis; Magdalini Kardara; Athanasios Papaoikonomou; Fotis Aisopos,Abstract Social Network (SN) environments are the ideal future service marketplaces. It iswell known and documented that SN users are increasing at a tremendous pace. Takingadvantage of these social dynamics as well as the vast volumes; of amateur contentgenerated every second; is a major step towards creating a potentially huge market ofservices. In this paper; we describe the external web services that SocIoS project isresearching and developing; and will support with the Social Media community. Aiming tosupport the end users of SNs; to enhance their transactions with more automated ways; andwith the advantage for better production and performance in their workflows over SNs inputsand content; this work presents the main architecture; functionality; and benefits per externalservice. Finally; introduces the end user; into the new era of SNs with business …,Proceedings of the 21st International Conference on World Wide Web,2012,3
Generating resource profiles by exploiting the context of social annotations,Ricardo Kawase; George Papadakis; Fabian Abel,Abstract Typical tagging systems merely capture that part of the tagging interactions thatenrich the semantics of tag assignments according to the system's purposes. The commonpractice is to build tag-based resource or user profiles on the basis of statistics about tags;disregarding the additional evidence that pertain to the resource; the user or the tagassignment itself. Thus; the main bulk of this valuable information is ignored whengenerating user or resource profiles. In this work; we formalize the notion of tag-based andcontext-based resource profiles and introduce a generic strategy for building such profilesby incorporating available context information from all parts involved in a tag assignment.Our method takes into account not only the contextual information attached to the tag; theuser and the resource; but also the metadata attached to the tag assignment itself. We …,International Semantic Web Conference,2011,3
A layered approach to revisitation prediction,George Papadakis; Ricardo Kawase; Eelco Herder; Claudia Niederée,Abstract Web browser users return to Web pages for various reasons. Apart from pagesvisited due to backtracking; they typically have a number of favorite/important pages thatthey monitor or tasks that reoccur on an infrequent basis. In this paper; we introduce thearchitecture of a system that facilitates revisitations through the effective prediction of thenext page request. It consists of three layers; each dealing with a specific aspect ofrevisitation patterns: the first one estimates the value of each page by balancing the recencyand the frequency of its requests; the second one captures the contextual regularities inusers' navigational activity in order to promote related pages; and the third one dynamicallyadapts the page associations of the second layer to the constant drift in the interests ofusers. For each layer; we introduce several methods; and evaluate them over a large; real …,International Conference on Web Engineering,2011,3
Big; Linked Geospatial Data and Its Applications in Earth Observation,Manolis Koubarakis; Konstantina Bereta; George Papadakis; Dimitrianos Savva; George Stamoulis,Editor: Carole Goble • cag@cs.man.ac.uk … JULY/AUGUST 2017 1089-7801/17/$33.00 © 2017IEEE Published by the IEEE Computer Society 87 … Manolis Koubarakis; KonstantinaBereta; George Papadakis; Dimitrianos Savva; and George Stamoulis • National and KapodistrianUniversity of Athens … If the terabytes of Earth observation data currently stored in archivesare published on the web using the linked data paradigm; data discovery; integration with otherdata sources; and the development of applications will become much easier … Big; Linked;and Open EO Data Lifecycle The life of Earth observation (EO) data starts with the data's generationin the ground segment of a satellite mission; where the management of this so-called payloaddata is an important activity. Figure 1 gives a high-level view of the lifecycle of big; linked EOdata as we envisioned it in our work. Each phase of the lifecycle and its associ- ated …,IEEE Internet Computing,2017,1
Blocking for large-scale Entity Resolution: Challenges; algorithms; and practical examples,George Papadakis; Themis Palpanas,Entity Resolution constitutes one of the cornerstone tasks for the integration of overlappinginformation sources. Due to its quadratic complexity; a large amount of research has focusedon improving its efficiency so that it scales to Web Data collections; which are inherentlyvoluminous and highly heterogeneous. The most common approach for this purpose isblocking; which clusters similar entities into blocks so that the pair-wise comparisons arerestricted to the entities contained within each block. In this tutorial; we take a close look onblocking-based Entity Resolution; starting from the early blocking methods that were craftedfor database integration. We highlight the challenges posed by contemporaryheterogeneous; noisy; voluminous Web Data and explain why they render inapplicablethese schema-based techniques. We continue with the presentation of blocking methods …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,1
Blocking Techniques for efficient Entity Resolution over large; highly heterogeneous Information Spaces,Georgios Papadakis,Abstract Web Data have boomed during the last decade; due to their largely distributed wayof production: corporations of any size; individual users as well as automatic extraction toolshave contributed a constantly increasing volume of diverse; but also very heterogeneousand noisy information. Entity Resolution (ER) helps to reduce the entropy; leveraging thevalue of the fragmented Web Data by identifying those pieces of information that refer to thesame real-world objects. To scale ER to the large and very large data sets; such as WebData; data blocking techniques are typically employed. However; most of them rely onschema information and; thus; are inapplicable to the highly heterogeneous settings of WebData—a situation that calls for novel approaches. This dissertation goes beyond existingblocking techniques; by introducing a novel methodology that is inherently crafted for the …,*,2013,1
SocIoS: A Social Media Application Ontology,Konstantinos Tserpes; George Papadakis; Magdalini Kardara; Athanasios Papaoikonomou; Fotis Aisopos; Emmanuel Sardis; Theodora Varvarigou,Abstract The value that social web is adding is undeniable. However; social web is comingat a cost: the so-called" closed" web. Slowly but steadily; a growing portion of internet activityis confined within the spaces of social networking sites (SNS) or platforms that encompasssocial networking capabilities by default. Furthermore; due to their competitive stancebetween one another; conceptually common content and functionality are accessed throughdifferent mechanisms in SNSs. This work deals with the issue of semantic equivalencebetween the prevalent notions used in SNSs in order to device a common object model thatwill enable the integration of social platform APIs. The result is the proposal for an ontologyfor the implementation of cross-platform social applications. Two particular applications areconsidered as a guide for this reference model.,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2012,1
Client-and server-side revisitation prediction with SUPRA,George Papadakis; Ricardo Kawase; Eelco Herder,Abstract Users of collaborative applications as well as individual users in their privateenvironment return to previously visited Web pages for various reasons; apart from pagesvisited due to backtracking; they typically have a number of favorite or important pages thatthey monitor or tasks that reoccur on an infrequent basis. In this paper; we introduce a libraryof methods that facilitate revisitation through the effective prediction of the next page request.It is based on a generic framework that inherently incorporates contextual information;handling uniformly both server-and the client-side applications. Unlike other existingapproaches; the methods it encompasses are real-time; since they do not rely on trainingdata or machine learning algorithms. We evaluate them over two large; real-world datasets;with the outcomes suggesting a significant improvement over methods typically used in …,Proceedings of the 2nd International Conference on Web Intelligence; Mining and Semantics,2012,1
Creation and Use of Annotations in Different Usage Contexts,Ricardo Kawase; George Papadakis; Eelco Herder; Wolfgang Nejdl,Abstract. It is virtually impossible to think of a world without annotations. People writecomments in paper margins and highlight text passages; they write reminders on post-itsand put short messages on colleagues' desks. Online annotations serve different goals thantheir pen-and-paper counterparts; they are mainly limited to social bookmarking and socialnetworking. In this paper we present a series of independent yet related user studies onvarious aspects of paper-based and online annotations. We show how shared annotationsare created and used and how annotations may support refinding. We provide designrecommendations on how to better support these differences in user contexts and goals.,*,*,1
Graph vs. bag representation models for the topic classification of web documents,George Papadakis; George Giannakopoulos; Georgios Paliouras,Abstract Text classification constitutes a popular task in Web research with variousapplications that range from spam filtering to sentiment analysis. In this paper; we argue thatits performance depends on the quality of Web documents; which varies significantly. Forexample; the curated content of news articles involves different challenges than the user-generated content of blog posts and Social Media messages. We experimentally verify ourclaim; quantifying the main factors that affect the performance of text classification. We alsoargue that the established bag-of-words representation models are inadequate for handlingall document types; as they merely extract frequent; yet distinguishing terms from the textualcontent of the training set. Thus; they suffer from low robustness in the context of noisy orunseen content; unless they are enriched with contextual; application-specific information …,World Wide Web,2016,*
Integrating Remote and Social Sensing Data for a Scenario on Secure Societies in Big Data Platform,Sergio Albani; Michele Lazzarini; Manolis Koubarakis; Efi Karra Taniskidou; George Papadakis; Vangelis Karkaletsis; George Giannakopoulos,Abstract In the framework of the Horizon 2020 project BigDataEurope (Integrating Big Data;Software & Communities for Addressing Europe's Societal Challenges); a pilot for theSecure Societies Societal Challenge was designed considering the requirements comingfrom relevant stakeholders. The pilot is focusing on the integration in a Big Data platform ofdata coming from remote and social sensing. The information on land changes coming fromthe Copernicus Sentinel 1A sensor (Change Detection workflow) is integrated withinformation coming from selected Twitter and news agencies accounts (Event Detectionworkflow) in order to provide the user with multiple sources of information. The ChangeDetection workflow implements a processing chain in a distributed parallel manner;exploiting the Big Data capabilities in place; the Event Detection workflow implements …,Living Planet Symposium,2016,*
MyCites: An Intelligent Information System for Maintaining Citations,George Papadakis; Georgios Paliouras,Abstract The evaluation of their research work and its effect has always been one ofscholars' greatest concerns. The use of citations for that purpose; as proposed by EugeneGarfield; is nowadays widely accepted as the most reliable method. However; gathering ascholar's citations constitutes a particularly laborious task; even in the current Internet era; asone needs to correctly combine information from miscellaneous sources. There existstherefore a need for automating this process. Numerous academic search engines try tocover this need; but none of them addresses successfully all related problems. In this paperwe present an approach that facilitates to a great extent citation analysis by takingadvantage of new algorithms to deal with these problems.,Hellenic Conference on Artificial Intelligence,2008,*
A PILOT FOR BIG DATA EXPLOITATION IN THE SPACE AND SECURITY DOMAIN,Sergio Albani; Michele Lazzarini; Manolis Koubarakis; Efi Karra Taniskidou; George Papadakis; Vangelis Karkaletsis; George Giannakopoulos,ABSTRACT In the framework of the Horizon 2020 project BigDataEurope (Integrating BigData; Software & Communities for Addressing Europe's Societal Challenges); a platformcomprising key open-source technologies has been set up in order to meet the Big Datarequirements of seven communities representing the Horizon 2020 Societal Challenges(Health; Food and Agriculture; Energy; Transport; Climate; Social Sciences and SecureSocieties).,*,*,*
