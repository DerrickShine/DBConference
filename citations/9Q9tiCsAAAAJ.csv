DBToaster: Higher-Order Delta Processing for Dynamic; Frequently Fresh Views,Yanif Ahmad; Oliver Kennedy; Christoph Koch; Milos Nikolic,Abstract Applications ranging from algorithmic trading to scientific data analysis requirerealtime analytics based on views over databases that change at very high rates. Suchviews have to be kept fresh at low maintenance cost and latencies. At the same time; theseviews have to support classical SQL; rather than window semantics; to enable applicationsthat combine current with aged or historical data. In this paper; we present viewlettransforms; a recursive finite differencing technique applied to queries. The viewlet transformmaterializes a query and a set of its higher-order deltas as views. These views support eachother's incremental maintenance; leading to a reduced overall view maintenance cost. Theviewlet transform of a query admits efficient evaluation; the elimination of certain expensivequery operations; and aggressive parallelization. We develop viewlet transforms into a …,Proceedings of the VLDB Endowment (pVLDB),2012,65
PIP: A database system for great and small expectations,Oliver Kennedy; Christoph Koch,Estimation via sampling out of highly selective join queries is well known to be problematic;most notably in online aggregation. Without goal-directed sampling strategies; samplesfalling outside of the selection constraints lower estimation efficiency at best; and causeinaccurate estimates at worst. This problem appears in general probabilistic databasesystems; where query processing is tightly coupled with sampling. By committing to a set ofsamples before evaluating the query; the engine wastes effort on samples that will bediscarded; query processing that may need to be repeated; or unnecessarily large numbersof samples.,2010 International Conference on Data Engineering (ICDE),2010,28
DBToaster: higher-order delta processing for dynamic; frequently fresh views,Christoph Koch; Yanif Ahmad; Oliver Kennedy; Milos Nikolic; Andres Nötzli; Daniel Lupei; Amir Shaikhha,Abstract Applications ranging from algorithmic trading to scientific data analysis require real-time analytics based on views over databases receiving thousands of updates each second.Such views have to be kept fresh at millisecond latencies. At the same time; these viewshave to support classical SQL; rather than window semantics; to enable applications thatcombine current with aged or historical data. In this article; we present the DBToastersystem; which keeps materialized views of standard SQL queries continuously fresh as datachanges very rapidly. This is achieved by a combination of aggressive compilationtechniques and DBToaster's original recursive finite differencing technique whichmaterializes a query and a set of its higher-order deltas as views. These views support eachother's incremental maintenance; leading to a reduced overall view maintenance cost …,The VLDB Journal,2014,27
Inventory allocation for online graphical display advertising,Jian Yang; Erik Vee; Sergei Vassilvitskii; John Tomlin; Jayavel Shanmugasundaram; Tasos Anastasakos; Oliver Kennedy,Abstract: We discuss a multi-objective/goal programming model for the allocation ofinventory of graphical advertisements. The model considers two types of campaigns:guaranteed delivery (GD); which are sold months in advance; and non-guaranteed delivery(NGD); which are sold using real-time auctions. We investigate various advertiser andpublisher objectives such as (a) revenue from the sale of impressions; clicks andconversions;(b) future revenue from the sale of NGD inventory; and (c)" fairness" ofallocation. While the first two objectives are monetary; the third is not. This combination ofdemand types and objectives leads to potentially many variations of our model; which wedelineate and evaluate. Our experimental results; which are based on optimization runsusing real data sets; demonstrate the effectiveness and flexibility of the proposed model …,Arxiv preprint arXiv:1008.3551,2010,25
Dynamic approaches to in-network aggregation,Oliver Kennedy; Christoph Koch; Al Demers,Collaboration between small-scale wireless devices depends on their ability to inferaggregate properties of all nearby nodes. The highly dynamic environment created bymobile devices introduces a silent failure mode that is disruptive to this kind of inference. Weaddress this problem by presenting techniques for extending existing unstructuredaggregation protocols to cope with failure modes introduced by mobile environments. Themodified protocols allow devices with limited connectivity to maintain estimates ofaggregates; despite\textit {unexpected} peer departures and arrivals.,2009 International Conference on Data Engineering (ICDE),2009,20
DBToaster: Agile Views in a Dynamic Data Management System,Oliver Kennedy; Yanif Ahmad; Christoph Koch,ABSTRACT This paper calls for a new breed of lightweight systems–dynamic datamanagement systems (DDMS). In a nutshell; a DDMS manages large dynamic datastructures with agile; frequently fresh views; and provides a facility for monitoring theseviews and triggering application-level events. We motivate DDMS with applications in large-scale data analytics; database monitoring; and high-frequency algorithmic trading. Wecompare DDMS to more traditional data management systems architectures. We present theDBToaster project; which is an ongoing effort to develop a prototype DDMS system. Wedescribe its architecture design; techniques for high-frequency incremental viewmaintenance; storage; scaling up by parallelization; and the various key challenges toovercome to make DDMS a reality.,2011 Conference on Innovative Data Research (CIDR),2011,19
Securing BGP using external security monitors,Patrick Reynolds; Oliver Kennedy; Emin Gun Sirer; Fred Schneider,Abstract: Security modifications to legacy network protocols are expensive and disruptive.This paper outlines an approach based on external security monitors; for securing legacyprotocols by deploying additional hosts that locally monitor the inputs and outputs of eachhost executing the protocol check the behavior of the host against a safety specification andcommunicate using an overlay to alert other hosts about invalid behavior and to initiateremedial actions. Trusted computing hardware provides the basis for trust in externalsecurity monitors. This paper applies this approach to secure the Border Gateway Protocol;yielding an external security monitor called N-BGP. N-BGP can accurately monitor a BGProuter using commodity trusted computing hardware. Deploying N-BGP at a random 10% ofBGP routers is sufficient to guarantee the security of 80% of Internet routes where both …,*,2006,11
Lenses: an on-demand approach to ETL,Ying Yang; Niccolo Meneghetti; Ronny Fehling; Zhen Hua Liu; Oliver Kennedy,Abstract Three mentalities have emerged in analytics. One view holds that reliable analyticsis impossible without high-quality data; and relies on heavy-duty ETL processes and upfrontdata curation to provide it. The second view takes a more ad-hoc approach; collecting datainto a data lake; and placing responsibility for data quality on the analyst querying it. A third;on-demand approach has emerged over the past decade in the form of numerous systemslike Paygo or HLog; which allow for incremental curation of the data and help analysts tomake principled trade-offs between data quality and effort. Though quite useful in isolation;these systems target only specific quality problems (eg; Paygo targets only schemamatching and entity resolution). In this paper; we explore the design of a general; extensibleinfrastructure for on-demand curation that is based on probabilistic query processing. We …,Proceedings of the VLDB Endowment,2015,9
Jigsaw: Efficient optimization over uncertain enterprise data,Oliver Kennedy; Suman Nath,Abstract Probabilistic databases; in particular ones that allow users to externally definemodels or probability distributions--so called VG-Functions--are an ideal tool forconstructing; simulating and analyzing hypothetical business scenarios. Enterprises oftenuse such tools with parameterized models and need to explore a large parameter space inorder to discover parameter values that optimize for a given goal. Parameter space isusually very large; making such exploration extremely expensive. We present Jigsaw; aprobabilistic database-based simulation framework that addresses this performanceproblem. In Jigsaw; users define what-if style scenarios as parameterized probabilisticdatabase queries and identify parameter values that achieve desired properties. Jigsawuses a novel" fingerprinting" technique that efficiently identifies correlations between a …,2011 International Conference on Management of Data (SIGMOD),2011,9
Ettu: Analyzing Query Intents in Corporate Databases,Gokhan Kul; Duc Luong; Ting Xie; Patrick Coonan; Varun Chandola; Oliver Kennedy; Shambhu Upadhyaya,Abstract Insider threats to databases in the financial sector have become a very serious andpervasive security problem. This paper proposes a framework to analyze access patterns todatabases by clustering SQL queries issued to the database. Our system Ettu works bygrouping queries with other similarly structured queries. The small number of intent groupsthat result can then be efficiently labeled by human operators. We show how our system isdesigned and how the components of the system work. Our preliminary results show that oursystem accurately models user intent.,Proceedings of the 25th International Conference Companion on World Wide Web,2016,8
Adaptive Schema Databases.,William Spoth; Bahareh Sadat Arab; Eric S Chan; Dieter Gawlick; Adel Ghoneimy; Boris Glavic; Beda Christoph Hammerschmidt; Oliver Kennedy; Seokki Lee; Zhen Hua Liu; Xing Niu; Ying Yang,ABSTRACT The rigid schemas of classical relational databases help users in specifyingqueries and inform the storage organization of data. However; the advantages of schemascome at a high upfront cost through schema and ETL process design. In this work; wepropose a new paradigm where the database system takes a more active role in schemadevelopment and data integration. We refer to this approach as adaptive schema databases(ASDs). An ASD ingests semi-structured or unstructured data directly using a pluggablecombination of extraction and data integration techniques. Over time it discovers and adaptsschemas for the ingested data using information provided by data integration andinformation extraction techniques; as well as from queries and user-feedback. In contrast torelational databases; ASDs maintain multiple schema workspaces that represent …,CIDR,2017,6
Pocket data: The need for TPC-MOBILE,Oliver Kennedy; Jerry Ajay; Geoffrey Challen; Lukasz Ziarek,Abstract Embedded database engines such as SQLite provide a convenient datapersistence layer and have spread along with the applications using them to many types ofsystems; including interactive devices such as smartphones. Android; the most widely-distributed smartphone platform; both uses SQLite internally and provides interfacesencouraging apps to use SQLite to store their own private structured data. As similarfunctionality appears in all major mobile operating systems; embedded databaseperformance affects the response times and resource consumption of billions ofsmartphones and the millions of apps that run on them—making it more important than everto characterize smartphone embedded database workloads. To do so; we present resultsfrom an experiment which recorded SQLite activity on 11 Android smartphones during …,Technology Conference on Performance Evaluation and Benchmarking,2015,6
SYSTEM FOR DISPLAY ADVERTISING OPTIMIZATION USING CLICK OR CONVERSION PERFORMANCE,*,An advertisement impression distribution system includes a data processing systemoperable to generate an allocation plan for serving advertisement impressions. Theallocation plan allocates a first portion of advertisement impressions to satisfy guaranteeddemand and a second portion of advertisement impressions to satisfy nonguaranteeddemand. The data processing system includes an optimizer configured to establish arelationship between the first portion of advertisement impressions and the second portion ofadvertisement impressions. The relationship defines a range of proportions of allocation ofthe first portion and the second portion. The optimizer generates a solution maximizingguaranteed demand fairness; non-guaranteed demand revenue; and click value. Thesolution identifies a determined proportion of the first portion of advertisement …,*,2009,6
Detecting the Temporal Context of Queries,Oliver Kennedy; Ying Yang; Jan Chomicki; Ronny Fehling; Zhen Hua Liu; Dieter Gawlick,Abstract Business intelligence and reporting tools rely on a database that accurately mirrorsthe state of the world. Yet; even if the schema and queries are constructed in exacting detail;assumptions about the data made during extraction; transformation; and schema and querycreation of the reporting database may be (accidentally) ignored by end users; or maychange as the database evolves over time. As these assumptions are typically implicit (eg;assuming that a sales record relation is append-only); it can be hard to even detect that amistaken assumption has been made. In this paper; we argue that such errors areconsequences of unintended contextual dependence; ie; query outputs dependent on avariable characteristic of the database. We characterize contextual dependence; andexplore several strategies for efficiently detecting and quantifying the effects of contextual …,International Workshop on Business Intelligence for the Real-Time Enterprise (BIRTE),2014,5
Mimir: Bringing CTables into Practice,Arindam Nandi; Ying Yang; Oliver Kennedy; Boris Glavic; Ronny Fehling; Zhen Hua Liu; Dieter Gawlick,Abstract: The present state of the art in analytics requires high upfront investment of humaneffort and computational resources to curate datasets; even before the first query is posed.So-called pay-as-you-go data curation techniques allow these high costs to be spread out;first by enabling queries over uncertain and incomplete data; and then by assessing thequality of the query results. We describe the design of a system; called Mimir; around arecently introduced class of probabilistic pay-as-you-go data cleaning operators calledLenses. Mimir wraps around any deterministic database engine using JDBC; extending itwith support for probabilistic query processing. Queries processed through Mimir produceuncertainty-annotated result cursors that allow client applications to quickly assess resultquality and provenance. We also present a GUI that provides analysts with an interactive …,arXiv preprint arXiv:1601.00073,2016,4
Just-In-Time Data Structures,Oliver Kennedy; Lukasz Ziarek,ABSTRACT Adaptive indexing is a promising alternative to classical offline indexoptimization. Under adaptive indexing; index creation and re-organization take placeautomatically and incrementally as a side-effect of query execution. Adaptive indexingimplementations optimize the index's structure by progressively rewriting it until it convergesto a single idealized form such as a sorted array or B-Tree. However; the idealrepresentation changes over time: An adaptive index that is initially optimal for one workloadbecomes suboptimal as the workload's characteristics change. In this paper we generalizeadaptive indexing; adding the ability to adjust the layout and behavior of the index toworkload changes even after convergence. This radical justin-time data structure approachto index construction and maintenance allows for indexes that dynamically adapt to …,Conference on Innovative Data Systems Research (CIDR),2015,4
Monadic Logs for Collaborative Web Applications,Sumit Agarwal; Daniel Bellinger; Oliver Kennedy; Ankur Upadhyay; Lukasz Ziarek,Abstract Cloud based web-applications are quickly becoming common in modern society. Anew class of such applications; collaborative cloud applications; are gaining in popularity asthey greatly improve remote collaboration. Most of these applications use a log structure asa coordination mechanism for shared application state. Such structures typically store theentire application state as well as deltas (changes sets) while the application runs. In thispaper we propose a monadic; dependency-aware; self-cleaning log structure forcollaborative cloud applications; which we refer to as a monadic log. This structure providesa rich set of analytical tools to support a variety of log transformations and rewrites. Forexample; the garbage collection mechanisms already present in any managed languagewill automatically bound the memory footprint of a monadic log. Moreover; a monadic log …,WebDB,2013,4
Inventory allocation for online graphical display advertising using multi-objective optimization,Jian Yang; Erik Vee; Sergei Vassilvitskii; John Tomlin; Jayavel Shanmugasundaram; Tasos Anastasakos; OA Kennedy,*,*,2012,4
PigOut: Making multiple Hadoop clusters work together,Kyungho Jeon; Sharath Chandrashekhara; Feng Shen; Shikhar Mehra; Oliver Kennedy; Steven Y Ko,This paper presents PigOut; a system that enables federated data processing over multipleHadoop clusters. Using PigOut; a user (such as a data analyst) can write a single script in ahigh-level language to efficiently use multiple Hadoop clusters. There is no need tomanually write multiple scripts and coordinate the execution for different clusters. PigOutaccomplishes this by automatically partitioning a single; user-supplied script into multiplescripts that run on different clusters. Additionally; PigOut generates workflow descriptions tocoordinate execution across clusters. In doing so; PigOut leverages existing tools builtaround Hadoop; avoiding extra effort required from users or administrators. For example;PigOut uses Pig Latin; a popular query language for Hadoop MapReduce; in a (virtually)unmodified form. Through our evaluation with PigMix; the standard benchmark for Pig; we …,Big Data (Big Data); 2014 IEEE International Conference on,2014,3
Communicating Data Quality in On-Demand Curation,Poonam Kumari; Said Achmiz; Oliver Kennedy,Abstract: On-demand curation (ODC) tools like Paygo; KATARA; and Mimir allow users todefer expensive curation effort until it is necessary. In contrast to classical databases that donot respond to queries over potentially erroneous data; ODC systems instead answer withguesses or approximations. The quality and scope of these guesses may vary and it iscritical that an ODC system be able to communicate this information to an end-user. Thecentral contribution of this paper is a preliminary user study evaluating the cognitive burdenand expressiveness of four representations of" attribute-level" uncertainty. The study shows(1) insignificant differences in time taken for users to interpret the four types of uncertaintytested; and (2) that different presentations of uncertainty change the way people interpretand react to data. Ultimately; we show that a set of UI design guidelines and best …,arXiv preprint arXiv:1606.02250,2016,2
The Exception that Improves the Rule,Juliana Freire; Boris Glavic; Oliver Kennedy; Heiko Mueller,Abstract The database community has developed numerous tools and techniques for datacuration and exploration; from declarative languages; to specialized techniques for datarepair; and more. Yet; there is currently no consensus on how to best expose these powerfultools to an analyst in a simple; intuitive; and above all; flexible way. Thus; analysts continueto rely on tools such as spreadsheets; imperative languages; and notebook styleprogramming environments like Jupyter for data curation. In this work; we explore theintegration of spreadsheets; notebooks; and relational databases. We focus on a keyadvantage that both spreadsheets and imperative notebook environments have overclassical relational databases: ease of exception. By relying on set-at-a-time operations;relational databases sacrifice the ability to easily define singleton operations; exceptions …,arXiv preprint arXiv:1606.00046,2016,2
Provenance-aware Versioned Dataworkspaces,Xing Niu; Bahareh Sadat Arab; Dieter Gawlick; Zhen Hua Liu; Vasudha Krishnaswamy; Oliver Kennedy; Boris Glavic,Abstract Data preparation; curation; and analysis tasks are often exploratory in nature; withanalysts incrementally designing workflows that transform; validate; and visualize their inputsources. This requires frequent adjustments to data and workflows. Unfortunately; in currentdata management systems; even small changes can require time-and resource-heavyoperations like materialization; manual version management; and re-execution. This addedoverhead discourages exploration. We present Provenance-aware VersionedDataworkspaces (PVDs); our vision of a sandboxed environment in which users can apply—and more importantly; easily undo—changes to their data and workflows. A PVD keeps a logof the user's operations in a light-weight version graph structure. We describe a model forPVDs that admits efficient automatic refresh; merging of histories; reenactment; and …,*,2016,2
BarQL: Collaborating through Change,Oliver Kennedy; Lukasz Ziarek,*,*,*,2
Fuzzy prophet: parameter exploration in uncertain enterprise scenarios,Oliver Kennedy; Steve Lee; Charles Loboz; Slawek Smyl; Suman Nath,Abstract We present Fuzzy Prophet; a probabilistic database tool for constructing; simulatingand analyzing business scenarios with uncertain data. Fuzzy Prophet takes externallydefined probability distribution (so called VG-Functions) and a declarative description of atarget scenario; and performs Monte Carlo simulation to compute probability distribution ofthe scenario's outcomes. In addition; Fuzzy Prophet supports parameter optimization; whereprobabilistic models are parameterized and a large parameter space must be explored tofind parameters that optimize or achieve a desired goal. Fuzzy Prophet's key innovation is touse'fingerprints' that can identify parameter values producing correlated outputs of a user-provided stochastic function and to reuse computations across such values. Fingerprintssignificantly expedite the process of parameter exploration in offline optimization and …,2011 International Conference on Management of Data (SIGMOD),2011,1
Beta probabilistic databases: A scalable approach to belief updating and parameter learning,Niccolo' Meneghetti; Oliver Kennedy; Wolfgang Gatterbauer,Abstract Tuple-independent probabilistic databases (TI-PDBs) handle uncertainty byannotating each tuple with a probability parameter; when the user submits a query; thedatabase derives the marginal probabilities of each output-tuple; assuming input-tuples arestatistically independent. While query processing in TI-PDBs has been studied extensively;limited research has been dedicated to the problems of updating or deriving the parametersfrom observations of query results. Addressing this problem is the main focus of this paper.We introduce Beta Probabilistic Databases (B-PDBs); a generalization of TI-PDBs designedto support both (i) belief updating and (ii) parameter learning in a principled and scalableway. The key idea of B-PDBs is to treat each parameter as a latent; Beta-distributed randomvariable. We show how this simple expedient enables both belief updating and parameter …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Small Data,Oliver Kennedy; D Richard Hipp; Stratos Idreos; Amélie Marian; Arnab Nandi; Carmela Troncoso; Eugene Wu,Data is becoming increasingly personal. Individuals regularly interact with a wide variety ofstructured data; from SQLite databases on phones; to HR spreadsheets; to personalsensors; to open government data appearing in news articles. Although these workloads areimportant; many of the classical challenges associated with scale and Big Data do not apply.This panel brings together experts in a variety of fields to explore the new opportunities andchallenges presented by" Small Data".,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*
Convergent Interactive Inference with Leaky Joins.,Ying Yang; Oliver Kennedy,ABSTRACT One of the primary challenges in graphical models is inference; or re-constructing a marginal probability from the graphical model's factorized representation.While tractable for some graphs; the cost of inference grows exponentially with the graphicalmodel's complexity; necessitating approximation for more complex graphs. For interactiveapplications; latency is the dominant concern; making approximate inference the onlyfeasible option. Unfortunately; approximate inference can be wasteful for interactiveapplications; as exact inference can still converge faster; even for moderately complexinference problems. In this paper; we propose a new family of convergent inferencealgorithms (CIAs) that bridge the gap between approximations and exact solutions;providing early; incrementally improving approximations that become exact after a finite …,EDBT,2017,*
Stop the Truthiness and Just Be Wrong.,Oliver Kennedy,Since their earliest days; databases have held themselves to a strict invariant: Never givethe user a wrong answer. So ingrained is it in the psyche of the database community; thatthose violate have only done so through cumbersome data models [4; 11]; huge warningsigns [5]; or annoying language constructs that break classical SQL [1; 6]. Sadly; by trying toenforce perfection in the database itself; database systems fail to acknowledge that the databeing stored is rarely precise; correct; valid; or unambiguous. Emphasizing on certain;deterministic data forces the use of complex; hard-to-manage extract-transform-loadpipelines that emit deceptively certain;“truthy” data rather than acknowledging ambiguity orerror. As more decisions are automated; even small truthiness errors can drastically impactpeoples' lives; from denying a person credit 1; to deciding that an 8-year old is a terrorist 2 …,CIDR,2017,*
Summarizing Large Query Logs in Ettu,Gokhan Kul; Duc Luong; Ting Xie; Patrick Coonan; Varun Chandola; Oliver Kennedy; Shambhu Upadhyaya,Abstract: Database access logs are large; unwieldy; and hard for humans to inspect andsummarize. In spite of this; they remain the canonical go-to resource for tasks ranging fromperformance tuning to security auditing. In this paper; we address the challenge ofcompactly encoding large sequences of SQL queries for presentation to a human user. Ourapproach is based on the Weisfeiler-Lehman (WL) approximate graph isomorphismalgorithm; which identifies salient features of a graph or in our case of an abstract syntaxtree. Our generalization of WL allows us to define a distance metric for SQL queries; which inturn permits automated clustering of queries. We also present two techniques for visualizingquery clusters; and an algorithm that allows these visualizations to be constructed atinteractive speeds. Finally; we evaluate our algorithms in the context of a motivating …,arXiv preprint arXiv:1608.01013,2016,*
maybe We Should Enable More Uncertain Mobile App Programming,Geoffrey Challen; Jerry Antony Ajay; Nick DiRienzo; Oliver Kennedy; Anudipa Maiti; Anandatirtha Nandugudi; Guru Prasad; Sriram Shantharam; Jinghao Shi; Lukasz Ziarek,Abstract One of the reasons programming mobile systems is so hard is the wide variety ofenvironments a typical app encounters at runtime. As a result; in many cases only post-deployment user testing can determine the right algorithm to use; the rate at whichsomething should happen; or when an app should attempt to conserve energy.Programmers should not be forced to make these choices at development time.Unfortunately; languages leave no way for programmers to express and structureuncertainty about runtime conditions; forcing them to adopt ineffective or fragile ad-hocsolutions. We introduce a new approach based on structured uncertainty through a newlanguage construct: the maybe statement. maybe statements allow programmers to deferchoices about app behavior that cannot be made at development time; while providing …,HotMobile,2015,*
Efficient optimization over uncertain data,*,The subject disclosure is directed towards using fingerprints; comprising lists of simulationresults corresponding to partial (random sampled) simulation results; to determine whether afull simulation may be avoided by reusing simulation results from a previous full simulation.Before running a full simulation; a current fingerprint is obtained via a partial simulation. If aprevious fingerprint matches (is identical or similar to) the current fingerprint; the associatedprevious results are reused. Also described is indexing fingerprint data to facilitate efficientlookup-based fingerprint matching.,*,2014,*
Watch Out For... What?: Monitoring And Uncertainty In Scientific Computing,Oliver Kennedy,As the amount of data involved in scientific research continues to grow; the need forpowerful tools for organizing and analyzing this data grows with it. Despite considerableprogress in this area by the database research community; the uptake of databasetechnologies within the scientific community has been slow. Contributing to this limitedadoption is a tendency to try to build complex; monolithic; total solution-systems; for acommunity that can rarely afford the resources to tie their existing infrastructures into such asystem. This thesis explores two different directions for creating simpler; smaller; moregeneralpurpose tools for doing data-processing in a scientific computing environment. Grey-Box Probabilistic Databases are an attempt to create a general purpose tool for efficientlyintegrating database systems with an organization's existing model-building pipelines. By …,*,2011,*
TECHNICAL REPORT YL-2010-004,Jian Yang; Erik Vee; Sergei Vassilvitskii John Tomlin; Jayavel Shanmugasundaram; Tasos Anastasakos; Oliver Kennedy,ABSTRACT: We discuss a multi-objective/goal programming model for the allocation ofinventory of graphical advertisements. The model considers two types of campaigns:guaranteed delivery (GD); which are sold months in advance; and non-guaranteed delivery(NGD); which are sold using real-time auctions. We investigate various advertiser andpublisher objectives such as (a) revenue from the sale of impressions; clicks andconversions;(b) future revenue from the sale of NGD inventory; and (c)“fairness” ofallocation. While the first two objectives are monetary; the third is not. This combination ofdemand types and objectives leads to potentially many variations of our model; which wedelineate and evaluate. Our experimental results; which are based on optimization runsusing real data sets; demonstrate the effectiveness and flexibility of the proposed model.,arXiv preprint arXiv:1008.3551,2010,*
NetQuery: A General-Purpose Channel for Reasoning about Network,Alan Shieh; Oliver Kennedy; Emin Gun Sirer; Fred Schneider,Although the configuration of modern networks has a significant impact on the performance;robustness; and security of applications; networks lack support for reporting thesedifferences. This paper presents the design and implementation of NetQuery; a novel;general-purpose channel for disseminating the properties of networks and their participants.Net-Query implements a distributed; decentralized; tuple-based attribute store that recordsinformation about network entities. Operators can add new tuples into this store and can alsoannotate existing tuples with new; custom attributes; thus allowing the system to supportnetwork entities and properties not anticipated at the time of deployment. Net-Query clientscan query this attribute store for the current network state and install event triggers to detectfuture state transitions; thus establishing long-running guarantees over the behavior of …,*,2009,*
Convergent Inference with Leaky Joins,Ying Yang; Oliver Kennedy,ABSTRACT Over the past decade; a class of model database engines like BayesStore andMauveDB have emerged; allowing users to interact with probabilistic graphical modelsthrough queries. The main bottleneck in model databases; computing marginal probabilities;grows exponentially with the complexity of the graph. Although exact solutions are feasiblefor simple graphs; approximation techniques are often required for more complex graphs.Model databases are expected to handle a range of model types and sizes; and as a resultthey often resort to approximations; even when exact solutions are possible. In this paper;we propose a new family of inference algorithms called convergent inference algorithms(CIAs) that bridge the gap between approximations and exact solutions. Drawing ontechniques from Online Aggregation; CIAs provide approximate inference results while …,*,*,*
Representation of Meaning: a Graphical and Interactive Approach,Gary Shawver; Oliver Kennedy; Patricia Baer,T his paper will consist a discussion of some of the theoretical assumptions underlying thedesign of a piece of software and a demonstration of its function. This software; the FixedPhrase Tool is part of the first release of TAPoR (text analysis portal for research). I first sawthe desirability of such software while doing my doctoral dissertation in Toronto; a computer-aided text analysis of the semantics of story and tale in Chaucer. It was clear that the listsgenerated by traditional texts analysis tools such as TACT were both tedious to examine andpresented information in a serial fashion that was not suitable to data that were; in somesense; relational. It was also clear that attempts to reformat the output of such'listware'intomore relational forms is exceedingly laborious. Of course; this practical need presupposescertain assumptions about meaning; which follow this rough outline.,*,*,*
