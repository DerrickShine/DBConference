BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data,Sameer Agarwal; Barzan Mozafari; Aurojit Panda; Henry Milner; Samuel Madden; Ion Stoica,Abstract In this paper; we present BlinkDB; a massively parallel; approximate query enginefor running interactive SQL queries on large volumes of data. BlinkDB allows users to trade-off query accuracy for response time; enabling interactive queries over massive data byrunning queries on data samples and presenting results annotated with meaningful errorbars. To achieve this; BlinkDB uses two key ideas:(1) an adaptive optimization frameworkthat builds and maintains a set of multi-dimensional stratified samples from original dataover time; and (2) a dynamic sample selection strategy that selects an appropriately sizedsample based on a query's accuracy or response time requirements. We evaluate BlinkDBagainst the well-known TPC-H benchmarks and a real-world analytic workload derived fromConviva Inc.; a company that manages video distribution over the Internet. Our …,EuroSys,2013,400
Scarlett: coping with skewed content popularity in mapreduce clusters,Ganesh Ananthanarayanan; Sameer Agarwal; Srikanth Kandula; Albert Greenberg; Ion Stoica; Duke Harlan; Ed Harris,Abstract To improve data availability and resilience MapReduce frameworks use filesystems that replicate data uniformly. However; analysis of job logs from a large productioncluster shows wide disparity in data popularity. Machines and racks storing popular contentbecome bottlenecks; thereby increasing the completion times of jobs accessing this dataeven when there are machines with spare cycles in the cluster. To address this problem; wepresent Scarlett; a system that replicates blocks based on their popularity. By accuratelypredicting file popularity and working within hard bounds on additional storage; Scarlettcauses minimal interference to running jobs. Trace driven simulations and experiments intwo popular MapReduce frameworks (Hadoop; Dryad) show that Scarlett effectivelyalleviates hotspots and can speed up jobs by 20.2%.,EuroSys,2011,238
Re-optimizing Data-Parallel Computing,Sameer Agarwal; Srikanth Kandula; Nicolas Bruno; Ming-Chuan Wu; Ion Stoica; Jingren Zhou,Abstract Performant execution of data-parallel jobs needs good execution plans. Certainproperties of the code; the data; and the interaction between them are crucial to generatethese plans. Yet; these properties are difficult to estimate due to the highly distributed natureof these frameworks; the freedom that allows users to specify arbitrary code as operations onthe data; and since jobs in modern clusters have evolved beyond single map and reducephases to logical graphs of operations. Using fixed apriori estimates of these properties tochoose execution plans; as modern systems do; leads to poor performance in severalinstances. We present RoPE; a first step towards re-optimizing data-parallel jobs. RoPEcollects certain code and data properties by piggybacking on job execution. It adaptsexecution plans by feeding these properties to a query optimizer. We show how this …,*,2012,135
Knowing When You're Wrong: Building Fast and Reliable Approximate Query Processing Systems,Sameer Agarwal; Henry Milner; Ariel Kleiner; Ameet Talwalkar; Michael I. Jordan; Samuel Madden; Barzan Mozafari; Ion Stoica,Abstract Modern data analytics applications typically process massive amounts of data onclusters of tens; hundreds; or thousands of machines to support near-real-time decisions.The quantity of data and limitations of disk and memory bandwidth often make it infeasible todeliver answers at interactive speeds. However; it has been widely observed that manyapplications can tolerate some degree of inaccuracy. This is especially true for exploratoryqueries on data; where users are satisfied with" close-enough" answers if they can comequickly. A popular technique for speeding up queries at the cost of accuracy is to executeeach query on a sample of data; rather than the whole dataset. To ensure that the returnedresult is not too inaccurate; past work on approximate query processing has used statisticaltechniques to estimate" error bars" on returned results. However; existing work in the …,SIGMOD,2014,98
Blink and it's done: interactive queries on very large data,Sameer Agarwal; Anand P Iyer; Aurojit Panda; Samuel Madden; Barzan Mozafari; Ion Stoica,Abstract In this demonstration; we present BlinkDB; a massively parallel; sampling-basedapproximate query processing framework for running interactive queries on large volumes ofdata. The key observation in BlinkDB is that one can make reasonable decisions in theabsence of perfect answers. BlinkDB extends the Hive/HDFS stack and can handle thesame set of SPJA (selection; projection; join and aggregate) queries as supported by thesesystems. BlinkDB provides real-time answers along with statistical error guarantees; and canscale to petabytes of data and thousands of machines in a fault-tolerant manner. Ourexperiments using the TPC-H benchmark and on an anonymized real-world video contentdistribution workload from Conviva Inc. show that BlinkDB can execute a wide range ofqueries up to 150x faster than Hive on MapReduce and 10--150x faster than Shark (Hive …,Proceedings of the VLDB Endowment,2012,81
G-ola: Generalized on-line aggregation for interactive analysis on big data,Kai Zeng; Sameer Agarwal; Ankur Dave; Michael Armbrust; Ion Stoica,Abstract Nearly 15 years ago; Hellerstein; Haas and Wang proposed online aggregation(OLA); a technique that allows users to (1) observe the progress of a query by showingiteratively refined approximate answers; and (2) stop the query execution once its resultachieves the desired accuracy. In this demonstration; we present G-OLA; a novel mini-batchexecution model that generalizes OLA to support general OLAP queries with arbitrarilynested aggregates using efficient delta maintenance techniques. We have implemented G-OLA in FluoDB; a parallel online query execution framework that is built on top of the Sparkcluster computing framework that can scale to massive data sets. We will demonstrateFluoDB on a cluster of 100 machines processing roughly 10TB of real-world session logsfrom a video-sharing website. Using an ad optimization and an A/B testing based …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,27
Parallel computing execution plan optimization,*,The use of statistics collected during the parallel distributed execution of the tasks of a jobmay be used to optimize the performance of the task or similar recurring tasks. An executionplan for a job is initially generated; in which the execution plan includes tasks. Statisticsregarding operations performed in the tasks are collected while the tasks are executed viaparallel distributed execution. Another execution plan is then generated for anotherrecurring job; in which the additional execution plan has at least one task in common withthe execution plan for the job. The additional execution plan is subsequently optimizedbased at least on the statistics to produce an optimized execution plan.,*,2016,25
Parallel data computing optimization,*,*,*,2013,25
Recurring job optimization in scope,Nicolas Bruno; Sameer Agarwal; Srikanth Kandula; Bing Shi; Ming-Chuan Wu; Jingren Zhou,An increasing number of applications require distributed data storage and processinginfrastructure over large clusters of commodity hardware for critical business decisions. TheMapReduce programming model [2] helps programmers write distributed applications onlarge clusters; but requires dealing with complex implementation details (eg; reasoning withdata distribution and overall system configuration). Recent proposals; such as SCoPE [1];raise the level of abstraction by providing a declarative language that not only increasesprogramming productivity but is also amenable to sophisticated optimization. Like intraditional database systems; such optimization relies on detailed data statistics to choosethe best execution plan in a cost-based fashion. However; in contrast to database systems; itis very difficult to obtain and maintain good quality statistics in a highly distributed …,Proceedings of the 2012 international conference on Management of Data,2012,19
Snapshots in hadoop distributed file system,Sameer Agarwal; Dhruba Borthakur; Ion Stoica,Abstract The ability to take snapshots is an essential functionality of any file system; assnapshots enable system administrators to perform data backup and recovery in case offailure. We present a low-overhead snapshot solution for HDFS; a popular distributed filesystem for large clusters of commodity servers. Our solution obviates the need for complexdistributed snapshot algorithms; by taking advantage of the centralized architecture of theHDFS control plane which stores all file metadata on a single node; and alleviates the needfor expensive copy-on write operations by taking advantage of the HDFS limited interfacethat restricts the write operations to append and truncate only. Furthermore; our solutionemploys new snapshot data structures to address the inherent challenges related to datareplication and distribution in HDFS. In this paper; we have designed; implemented and …,Tech. rep.,2011,16
A general bootstrap performance diagnostic,Ariel Kleiner; Ameet Talwalkar; Sameer Agarwal; Ion Stoica; Michael I Jordan,Abstract As datasets become larger; more complex; and more available to diverse groups ofanalysts; it would be quite useful to be able to automatically and generically assess thequality of estimates; much as we are able to automatically train and evaluate predictivemodels such as classifiers. However; despite the fundamental importance of estimatorquality assessment in data analysis; this task has eluded highly automatic solutions. Whilethe bootstrap provides perhaps the most promising step in this direction; its level ofautomation is limited by the difficulty of evaluating its finite sample performance and even itsasymptotic consistency. Thus; we present here a general diagnostic procedure whichdirectly and automatically evaluates the accuracy of the bootstrap's outputs; determiningwhether or not the bootstrap is performing satisfactorily when applied to a given dataset …,Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,2013,11
IOLAP: Managing uncertainty for efficient incremental OLAP,Kai Zeng; Sameer Agarwal; Ion Stoica,Abstract The size of data and the complexity of analytics continue to grow along with theneed for timely and cost-effective analysis. However; the growth of computation powercannot keep up with the growth of data. This calls for a paradigm shift from traditional batchOLAP processing model to an incremental OLAP processing model. In this paper; wepropose iOLAP; an incremental OLAP query engine that provides a smooth trade-offbetween query accuracy and latency; and fulfills a full spectrum of user requirements fromapproximate but timely query execution to a more traditional accurate query execution.iOLAP enables interactive incremental query processing using a novel mini-batch executionmodel---given an OLAP query; iOLAP first randomly partitions the input dataset into smallersets (mini-batches) and then incrementally processes through these mini-batches by …,Proceedings of the 2016 International Conference on Management of Data,2016,6
Chronos: A Predictive Task Scheduler for MapReduce,Sameer Agarwal; Ion Stoica,*,UC Berkeley Technical Report,2010,3
Think Global; Act Local: Analyzing the trade-off between Queue Delays and Locality in MapReduce jobs,Sameer Agarwal; Ganesh Ananthanarayanan,*,UC Berkeley Technical Report,2010,2
Record Placement Based on Data Skew Using Solid State Drives,Jun Suzuki; Shivaram Venkataraman; Sameer Agarwal; Michael Franklin; Ion Stoica,Abstract Integrating a solid state drive (SSD) into a data store is expected to improve its I/Operformance. However; there is still a large difference between the price of an SSD and ahard-disk drive (HDD). One of the methods to offset the increase in cost of consisting devicesis to configure a hybrid system using both devices. In such a system; a common method todecide the placement of data records is based on reference locality; ie; placing thefrequently accessed records in a faster SSD. In this paper; we propose an alternative thatfocuses on data skew by storing records with values that appear less often in an SSD whilethose that do more in an HDD. As we will show; this enhances the performance of fetchingrecords using multi-dimensional indices. When records are fetched using one of the indicestargeted for optimization; records stored in an SSD are likely be retrieved using random …,Workshop on Big Data Benchmarks; Performance Optimization; and Emerging Hardware,2014,1
Queries with Bounded Errors & Bounded Response Times on Very Large Data,Sameer Agarwal,Abstract Modern data analytics applications typically process massive amounts of data onclusters of tens; hundreds; or thousands of machines to support near-real-time decisions.The quantity of data and limitations of disk and memory bandwidth often make it infeasible todeliver answers at human-interactive speeds. However; it has been widely observed thatmany applications can tolerate some degree of inaccuracy. This is especially true forexploratory queries on data; where users are satisfied with" close-enough" answers if theycan be provided quickly to the end user. A popular technique for speeding up queries at thecost of accuracy is to execute each query on a sample of data; rather than the whole dataset.In this thesis; we present BlinkDB; a massively parallel; approximate query engine forrunning interactive SQL queries on large volumes of data. BlinkDB allows users to trade …,*,2014,*
Lattice: A Scalable Layer-Agnostic Packet Classification Framework,Sameer Agarwal; Mosharaf Chowdhury; Dilip Joseph; Ion Stoica,Abstract—Despite widespread application; packet classification is implemented anddeployed in an ad-hoc manner at different layers of the protocol stack. Moreover; high speedpacket classification; in presence of a large number of classification rules; is both resourceand computation intensive. We propose a scalable layer-agnostic packet classificationframework (Lattice) that generalizes classifier design and enables offloading part ofcomputation and memory requirements to collaborators (eg; end hosts). Lattice eliminatesper-packet classification and perflow states in classifiers to increase scalability anddecreases vulnerability to state-based DoS attacks. Furthermore; Lattice is incentivecompatible in that collaborators cannot get better service by lying; and it incentivizesdeployment by giving preferential treatment to packets carrying Lattice-related information …,*,2011,*
Packet Classification with Explicit Coordination,Mosharaf Chowdhury; Sameer Agarwal,ABSTRACT Packet classification is a key building block of many network services andfunctionalities (eg; switching; filtering; load balancing). Despite its prevalence; packetclassification is implemented and deployed in an ad-hoc manner at different layers of theprotocol stack. Moreover; high speed packet classification; in presence of arbitrarily largenumber of classification rules; is resource and computation intensive. We argue that insteadof developing classification solutions in isolation; packet classification should be consideredas a fundamental primitive in the protocol stack. We propose a classification layer or CLayeras part of the protocol stack and a generic mechanism to explicitly configure and implementcapability-driven classification offloading. Our approach allows classifiers (eg; routers;firewalls) to offload part of their classification related computation and memory …,*,*,*
