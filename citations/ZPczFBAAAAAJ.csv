Scicumulus: A lightweight cloud middleware to explore many task computing paradigm in scientific workflows,Daniel de Oliveira; Eduardo Ogasawara; Fernanda Baião; Marta Mattoso,Most of the large-scale scientific experiments modeled as scientific workflows produce alarge amount of data and require workflow parallelism to reduce workflow execution time.Some of the existing Scientific Workflow Management Systems (SWfMS) explore parallelismtechniques-such as parameter sweep and data fragmentation. In those systems; severalcomputing resources are used to accomplish many computational tasks in homogeneousenvironments; such as multiprocessor machines or cluster systems. Cloud computing hasbecome a popular high performance computing model in which (virtualized) resources areprovided as services over the Web. Some scientists are starting to adopt the cloud model inscientific domains and are moving their scientific workflows (programs and data) from localenvironments to the cloud. Nevertheless; it is still difficult for the scientist to express a …,Cloud Computing (CLOUD); 2010 IEEE 3rd International Conference on,2010,158
Building reliable web services compositions,Paulo F Pires; Mario RF Benevides; Marta Mattoso,Abstract The recent evolution of internet technologies; mainly guided by the ExtensibleMarkup Language (XML) and its related technologies; are extending the role of the WorldWide Web from information interaction to service interaction. This next wave of the internetera is being driven by a concept named Web services. The Web services technologyprovides the underpinning to a new business opportunity; ie; the possibility of providingvalue-added Web services. However; the building of value-added services on this newenvironment is not a trivial task. Due to the many singularities of the Web serviceenvironment; such as the inherent structural and behavioral heterogeneity of Web services;as well as their strict autonomy; it is not possible to rely on the current models and solutionsto build and coordinate compositions of Web services. In this paper; we present a …,Net. ObjectDays: International Conference on Object-Oriented and Internet-Based Technologies; Concepts; and Applications for a Networked World,2002,151
An algebraic approach for data-centric scientific workflows,Eduardo Ogasawara; Jonas Dias; Daniel Oliveira; Fábio Porto; Patrick Valduriez; Marta Mattoso,ABSTRACT Scientific workflows have emerged as a basic abstraction for structuring andexecuting scientific experiments in computational environments. In many situations; theseworkflows are computationally and data intensive; thus requiring execution in large-scaleparallel computers. However; parallelization of scientific workflows remains low-level; ad-hoc and laborintensive; which makes it hard to exploit optimization opportunities. To addressthis problem; we propose an algebraic approach (inspired by relational algebra) and aparallel execution model that enable automatic optimization of scientific workflows. Weconducted a thorough validation of our approach using both a real oil exploitationapplication and synthetic data scenarios. The experiments were run in Chiron; a data-centricscientific workflow engine implemented to support our algebraic approach. Our …,Proc. of VLDB Endowment,2011,122
Towards supporting the life cycle of large scale scientific experiments,Marta Mattoso; Claudia Werner; Guilherme Horta Travassos; Vanessa Braganholo; Eduardo Ogasawara; Daniel Oliveira; Sergio Cruz; Wallace Martinho; Leonardo Murta,One of the main challenges of scientific experiments is to allow scientists to manage andexchange their scientific computational resources (data; programs; models; etc.). Theeffective management of such experiments requires a specific set of cardinal facilities; suchas experiment specification techniques; workflow derivation heuristics and provenancemechanisms. These facilities may characterise the experiment life cycle into three phases:composition; execution; and analysis. Works concerned with supporting scientific workflowsare mainly concerned with the execution and analysis phase. Therefore; they fail to supportthe scientific experiment throughout its life cycle as a set of integrated experimentationtechnologies. In large scale experiments this represents a research challenge. We proposean approach for managing large scale experiments based on provenance gathering …,International Journal of Business Process Integration and Management,2010,112
Towards a taxonomy of provenance in scientific workflow management systems,Sérgio Manuel Serra da Cruz; Maria Luiza M Campos; Marta Mattoso,Scientific Workflow Management Systems (SWfMS) have been helping scientists toprototype and execute in silico experiments. They can systematically collect provenanceinformation for the derived data products to be later queried. Despite the efforts on building astandard Open Provenance Model (OPM); provenance is tightly coupled to SWfMS. Thusscientific workflow provenance concepts; representation and mechanisms are veryheterogeneous; difficult to integrate and dependent on the SWfMS. To help comparing;integrating and analyzing scientific workflow provenance; this paper presents a taxonomyabout provenance characteristics. Its classification enables computer scientists to distinguishbetween different perspectives of provenance and guide to a better understanding ofprovenance data in general. The analysis of existing approaches will assist us in …,Services-I; 2009 World Conference on,2009,87
A survey of data-intensive scientific workflow management,Ji Liu; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract Nowadays; more and more computer-based scientific experiments need to handlemassive amounts of data. Their data processing consists of multiple computational stepsand dependencies within them. A data-intensive scientific workflow is useful for modelingsuch process. Since the sequential execution of data-intensive scientific workflows may takemuch time; Scientific Workflow Management Systems (SWfMSs) should enable the parallelexecution of data-intensive scientific workflows and exploit the resources distributed indifferent infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques.Based on a SWfMS functional architecture; we give a comparative analysis of the existingsolutions. Finally; we identify research issues for improving the execution of data …,Journal of Grid Computing,2015,86
A provenance-based adaptive scheduling heuristic for parallel scientific workflows in clouds,Daniel de Oliveira; Kary ACS Ocaña; Fernanda Baião; Marta Mattoso,Abstract In the last years; scientific workflows have emerged as a fundamental abstractionfor structuring and executing scientific experiments in computational environments. Scientificworkflows are becoming increasingly complex and more demanding in terms ofcomputational resources; thus requiring the usage of parallel techniques and highperformance computing (HPC) environments. Meanwhile; clouds have emerged as a newparadigm where resources are virtualized and provided on demand. By using clouds;scientists have expanded beyond single parallel computers to hundreds or even thousandsof virtual machines. Although the initial focus of clouds was to provide high throughputcomputing; clouds are already being used to provide an HPC environment where elasticresources can be instantiated on demand during the course of a scientific workflow …,Journal of Grid Computing,2012,86
Odyssey: A reuse environment based on domain models,Regina MM Braga; Cláudia ML Werner; Marta Mattoso,This paper presents a reuse based software development environment that provides supportto component-based software development (CBD) within certain domains; named Odyssey.Object-oriented frameworks; software architectures; artificial intelligence techniques; domainengineering; and mediators are some of the technologies used by Odyssey.,Application-Specific Systems and Software Engineering and Technology; 1999. ASSET'99. Proceedings. 1999 IEEE Symposium on,1999,75
SciPhy: a cloud-based workflow for phylogenetic analysis of drug targets in protozoan genomes,Kary ACS Ocaña; Daniel de Oliveira; Eduardo Ogasawara; Alberto MR Dávila; Alexandre AB Lima; Marta Mattoso,Abstract Bioinformatics experiments are rapidly evolving with genomic projects that analyzelarge amounts of data. This fact demands high performance computation and opens up forexploring new approaches to provide better control and performance when runningexperiments; including Phylogeny/Phylogenomics. We designed a phylogenetic scientificworkflow; named SciPhy; to construct phylogenetic trees from a set of drug target enzymesfound in protozoan genomes. Our contribution is the development; implementation and testof SciPhy in public cloud computing environments. SciPhy can be used in otherBioinformatics experiments to control a systematic execution with high performance whileproducing provenance data.,Brazilian Symposium on Bioinformatics,2011,74
Towards a taxonomy for cloud computing from an e-science perspective,Daniel de Oliveira; Fernanda Araujo Baião; Marta Mattoso,Abstract In the last few years; cloud computing has emerged as a computational paradigmthat enables scientists to build more complex scientific applications to manage large datasets or high-performance applications; based on distributed resources. By following thisparadigm; scientists may use distributed resources (infrastructure; storage; databases; andapplications) without having to deal with implementation or configuration details. In fact;there are many cloud computing environments already available for use. Despite its fastgrowth and adoption; the definition of cloud computing is not a consensus. This makes itvery difficult to comprehend the cloud computing field as a whole; correlate; classify; andcompare the various existing proposals. Over the years; taxonomy techniques have beenused to create models that allow for the classification of concepts within a domain. The …,*,2010,68
Grid data management: Open problems and new issues,Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract Initially developed for the scientific community; Grid computing is now gaining muchinterest in important areas such as enterprise information systems. This makes datamanagement critical since the techniques must scale up while addressing the autonomy;dynamicity and heterogeneity of the data sources. In this paper; we discuss the main openproblems and new issues related to Grid data management. We first recall the mainprinciples behind data management in distributed systems and the basic techniques. Thenwe make precise the requirements for Grid data management. Finally; we introduce the maintechniques needed to address these requirements. This implies revisiting distributeddatabase techniques in major ways; in particular; using P2P techniques.,Journal of Grid Computing,2007,66
Chiron: a parallel engine for algebraic scientific workflows,Eduardo Ogasawara; Jonas Dias; Vitor Silva; Fernando Chirigati; Daniel Oliveira; Fabio Porto; Patrick Valduriez; Marta Mattoso,SUMMARY Large-scale scientific experiments based on computer simulations are typicallymodeled as scientific workflows; which eases the chaining of different programs. Thesescientific workflows are defined; executed; and monitored by scientific workflowmanagement systems (SWfMS). As these experiments manage large amounts of data; itbecomes critical to execute them in high-performance computing environments; such asclusters; grids; and clouds. However; few SWfMS provide parallel support. The ones that doso are usually labor-intensive for workflow developers and have limited primitives tooptimize workflow execution. To address these issues; we developed workflow algebra tospecify and enable the optimization of parallel execution of scientific workflows. In thispaper; we show how the workflow algebra is efficiently implemented in Chiron; an …,Concurrency and Computation: Practice and Experience,2013,62
The use of mediation and ontology technologies for software component information retrieval,Regina MM Braga; Marta Mattoso; Cláudia ML Werner,Abstract Component Based Developed aims at constructing software through the inter-relationship between pre-existing components. However; these components should bebound to a specific application domain in order to be effectively reused. Reusable domaincomponents and Their related documentation are usually stored in a great variety of datasources. Thus; a possible solution for accessing this information is to use a software layerthat integrates different component information sources. We present a componentinformation integration data layer; based on mediators. Through mediators; domain ontologyacts as a technique/formalism for specifying ontological commitments or agreementsbetween component users and providers; enabling more accurate software componentinformation search.,ACM SIGSOFT Software Engineering Notes,2001,62
Managing structural genomic workflows using web services,Maria Cláudia Cavalcanti; Rafael Targino; Fernanda Baião; Shaila C Rössle; Paulo M Bisch; Paulo F Pires; Maria Luiza M Campos; Marta Mattoso,Abstract In silico scientific experiments encompass multiple combinations of program anddata resources. Each resource combination in an execution flow is called a scientificworkflow. In bioinformatics environments; program composition is a frequent operation;requiring complex management. A scientist faces many challenges when building anexperiment: finding the right program to use; the adequate parameters to tune; managinginput/output data; building and reusing workflows. Typically; these workflows areimplemented using script languages because of their simplicity; despite their specificity anddifficulty of reuse. In contrast; Web service technology was specially conceived toencapsulate and combine programs and data; providing interoperation betweenapplications from different platforms. The Web services approach is superior to scripts …,Data & Knowledge Engineering,2005,58
Exploring many task computing in scientific workflows,Eduardo Ogasawara; Daniel de Oliveira; Fernando Chirigati; Carlos Eduardo Barbosa; Renato Elias; Vanessa Braganholo; Alvaro Coutinho; Marta Mattoso,Abstract One of the main advantages of using a scientific workflow management system(SWfMS) to orchestrate data flows among scientific activities is to control and register thewhole workflow execution. The execution of activities within a workflow with highperformance computing (HPC) presents challenges in SWfMS execution control. Currentsolutions leave the scheduling to the HPC queue system. Since the workflow executionengine does not run on remote clusters; SWfMS are not aware of the parallel strategy of theworkflow execution. Consequently; remote execution control and provenance registry of theparallel activities is very limited from the SWfMS side. This work presents a set ofcomponents to be included on the workflow specification of any SWMfS to controlparallelization of activities as MTC. In addition; these components can gather provenance …,Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers,2009,55
Capturing and querying workflow runtime provenance with PROV: a practical approach,Flavio Costa; Vítor Silva; Daniel De Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Abstract Scientific workflows are commonly used to model and execute large-scale scientificexperiments. They represent key resources for scientists and are enacted and managed byScientific Workflow Management Systems (SWfMS). Each SWfMS has its particularapproach to execute workflows and to capture and manage their provenance data. Due tothe large scale of experiments; it may be unviable to analyze provenance data only after theend of the execution. A single experiment may demand weeks to run; even in highperformance computing environments. Thus scientists need to monitor the experimentduring its execution; and this can be done through provenance data. Runtime provenanceanalysis allows for scientists to monitor workflow execution and to take actions before theend of it (ie. workflow steering). This provenance data can also be used to fine-tune the …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,52
Adaptive Virtual Partitioning for OLAP Query Processing in a Database Cluster.,Alexandre AB Lima; Marta Mattoso; Patrick Valduriez,Abstract OLAP queries are typically heavy-weight and ad-hoc thus requiring high storagecapacity and processing power. In this paper; we address this problem using a databasecluster which we see as a cost-effective alternative to a tightly-coupled multiprocessor. Wepropose a solution to efficient OLAP query processing using a simple data parallelprocessing technique called adaptive virtual partitioning which dynamically tunes partitionsizes; without requiring any knowledge about the database and the DBMS. To validate oursolution; we implemented a Java prototype on a 32 node cluster system and ranexperiments with typical queries of the TPC-H benchmark. The results show that our solutionyields linear; and sometimes superlinear; speedup. In many cases; it outperforms traditionalvirtual partitioning by factors superior to 10.,SBBD,2004,50
A distribution design methodology for object DBMS,Fernanda Baião; Marta Mattoso; Gerson Zaverucha,Abstract The design of distributed databases involves making decisions on thefragmentation and placement of data and programs across the sites of a computer network.The first phase of the distribution design in a top-down approach is the fragmentation phase;which clusters in fragments the information accessed simultaneously by applications. Mostdistribution design algorithms propose a horizontal or vertical class fragmentation. However;the user has no assistance in the choice between these techniques. In this work we presenta detailed methodology for the design of distributed object databases that includes:(i) ananalysis phase; to indicate the most adequate fragmentation technique to be applied in eachclass of the database schema;(ii) a horizontal class fragmentation algorithm; and (iii) avertical class fragmentation algorithm. Basically; the analysis phase is responsible for …,Distributed and Parallel Databases,2004,49
Using ontologies for domain information retrieval,Regina MM Braga; Cláudia Maria Lima Werner; Marta Mattoso,The main objective of domain engineering is to provide domain information that helps thespecification of domain applications. Some applications need to reuse information frommultiple domains. Currently; there are several Domain Engineering methods that providedomain information using different representations that are stored in various formats. Due tothe costs involved in a domain engineering initiative; it is important to be able to access allavailable domain information. This paper describes a retrieval agent system that providesaccess to information from multiple domains; regardless of its heterogeneity or distribution.Domain ontologies and an evolutionary model of the user's interests are some of the basicconcepts used by the system to help users identify and retrieve relevant domain information.,Database and Expert Systems Applications; 2000. Proceedings. 11th International Workshop on,2000,49
Parallel OLAP query processing in database clusters with data replication,Alexandre AB Lima; Camille Furtado; Patrick Valduriez; Marta Mattoso,Abstract We consider the problem of improving the performance of OLAP applications in adatabase cluster (DBC); which is a low cost and effective parallel solution for queryprocessing. Current DBC solutions for OLAP query processing provide for intra-queryparallelism only; at the cost of full replication of the database. In this paper; we propose moreefficient distributed database design alternatives which combine physical/virtual partitioningwith partial replication. We also propose a new load balancing strategy that takes advantageof an adaptive virtual partitioning to redistribute the load to the replicas. Our experimentalvalidation is based on the implementation of our solution on the SmaQSS DBC middlewareprototype. Our experimental results using the TPC-H benchmark and a 32-node clustershow very good speedup.,Distributed and Parallel Databases,2009,46
Experiment line: Software reuse in scientific workflows,Eduardo Ogasawara; Carlos Paulino; Leonardo Murta; Cláudia Werner; Marta Mattoso,Abstract Over the last years; scientists have been using scientific workflows to buildcomputer simulations to support the development of new theories. Due to the increasing useof scientific workflows in production environments; the composition of workflows and theirexecutions can no longer be performed in an ad-hoc manner. Although current scientificworkflow management systems support the execution of workflows; they present limitationsregarding the composition of workflows when it comes to using different levels ofabstractions. This paper introduces the concept of experiment line which is a systematicapproach for the composition of scientific workflows that represents an in-silico experiment.An experiment line is inspired on the software engineering reuse discipline and allows thecomposition of scientific workflows at different levels of abstractions; which characterizes …,International Conference on Scientific and Statistical Database Management,2009,45
An adaptive parallel execution strategy for cloud‐based scientific workflows,Daniel Oliveira; Eduardo Ogasawara; Kary Ocaña; Fernanda Baião; Marta Mattoso,SUMMARY Many of the existing large-scale scientific experiments modeled as scientificworkflows are compute-intensive. Some scientific workflow management systems alreadyexplore parallel techniques; such as parameter sweep and data fragmentation; to improveperformance. In those systems; computing resources are used to accomplish manycomputational tasks in high performance environments; such as multiprocessor machines orclusters. Meanwhile; cloud computing provides scalable and elastic resources that can beinstantiated on demand during the course of a scientific experiment; without requiring itsusers to acquire expensive infrastructure or to configure many pieces of software. In fact;because of these advantages some scientists have already adopted the cloud model in theirscientific experiments. However; this model also raises many challenges. When scientists …,Concurrency and Computation: Practice and Experience,2012,44
Efficiently processing XML queries over fragmented repositories with PartiX,Alexandre Andrade; Gabriela Ruberg; Fernanda Baião; Vanessa P Braganholo; Marta Mattoso,Abstract The data volume of XML repositories and the response time of query processinghave become critical issues for many applications; especially for those in the Web. Aninteresting alternative to improve query processing performance consists in reducing thesize of XML databases through fragmentation techniques. However; traditionalfragmentation definitions do not directly apply to collections of XML documents. This workformalizes the fragmentation definition for collections of XML documents; and shows theperformance of query processing over fragmented XML data. Our prototype; PartiX; exploitsintra-query parallelism on top of XQuery-enabled sequential DBMS modules. We haveanalyzed several experimental settings; and our results showed a performanceimprovement of up to a 72 scale up factor against centralized databases.,International Conference on Extending Database Technology,2006,44
A mixed fragmentation algorithm for distributed object oriented databases,Fernanda Baião; Marta Mattoso,Abstract The performance of applications on Object Oriented Database ManagementSystems (OODBMSs) is strongly affected by Distributed Design; which reduces irrelevantdata accessed by applications and data exchange among sites. This work proposes analgorithm to the fragmentation phase of the distributed design of object oriented databases;according to a set of heuristics obtained from experimental results. The proposed algorithmaddresses specific characteristics of OODBMSs such as management of class extensionsand object relationships; and its major contributions are:(i) it observes performance issues;by allowing a class which has a very small extension not to be fragmented;(ii) it proposeshorizontal; vertical and mixed fragmentation (horizontal and vertical) of a class; and (iii) itpermits specific OO characteristics to drive the primary and derived fragmentation based …,Proc Int’l Conf Computing and Information (ICCI'98); Winnipeg,1998,40
ProvManager: a provenance management system for scientific workflows,Anderson Marinho; Leonardo Murta; Cláudia Werner; Vanessa Braganholo; Sérgio Manuel Serra da Cruz; Eduardo Ogasawara; Marta Mattoso,SUMMARY Running scientific workflows in distributed and heterogeneous environmentshas been a motivating approach for provenance management; which is loosely coupled tothe workflow execution engine. This kind of approach is interesting because it allows bothstorage and access to provenance data in a homogeneous way; even in an environmentwhere different workflow management systems work together. However; current approachesoverload scientists with many ad hoc tasks; such as script adaptations and implementationsof extra functionalities to provide provenance independence. This paper proposesProvManager; a provenance management approach that eases the gathering; storage; andanalysis of provenance information in a distributed and heterogeneous environmentscenario; without putting the burden of adaptations on the scientist. ProvManager …,Concurrency and Computation: Practice and Experience,2012,39
Supporting dynamic parameter sweep in adaptive and user-steered workflow,Jonas Dias; Eduardo Ogasawara; Daniel de Oliveira; Fabio Porto; Alvaro LGA Coutinho; Marta Mattoso,Abstract Large-scale experiments in computational science are complex to manage. Due toits exploratory nature; several iterations evaluate a large space of parameter combinations.Scientists analyze partial results and dynamically interfere on the next steps of thesimulation. Scientific workflow management systems can execute those experiments byproviding process management; distributed execution and provenance data. However;supporting scientists in complex exploratory processes involving dynamic workflows is still achallenge. Features; such as user steering on workflows to track; evaluate and adapt theexecution need to be designed to support iterative methods. We provide an approach tosupport dynamic parameter sweep; in which scientists can use the results obtained in a sliceof the parameter space to improve the remainder of the execution. We propose new …,Proceedings of the 6th workshop on Workflows in support of large-scale science,2011,38
Automatic composition of web services with contingency plans,Luiz AG da Costa; Paulo F Pires; Marta Mattoso,The semantic Web technology and the Web services description language extensibility maybe combined to describe services in an unambiguous and machine interpretable way;automating Web services discovery; selection and invocation. In this paper; we present analgorithm and a prototype for the automatic composition of Web services that implementworkflows described in a high level language. Our approach has many advantagescomparing to the manual creation of a simple program composition; such as smallerimplementation time and cost; reliability with the generation of contingency plans; greatercapacity to evolve with the dynamic service discovery; and faster execution time with the useof heuristics. We use the OWLS ontology to semantically describe Web services metadataand indexes to help selecting them. The proposed algorithm considers that equivalent …,Web Services; 2004. Proceedings. IEEE International Conference on,2004,38
Optimizing phylogenetic analysis using scihmm cloud-based scientific workflow,Kary ACS Ocana; Daniel de Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Phylogenetic analysis and multiple sequence alignment (MSA) are closely relatedbioinformatics fields. Phylogenetic analysis makes extensive use of MSA in the constructionof phylogenetic trees; which are used to infer the evolutionary relationships betweenhomologous genes. These bioinformatics experiments are usually modeled as scientificworkflows. There are many alternative workflows that use different MSA methods to conductphylogenetic analysis and each one can produce MSA with different quality. Scientists haveto explore which MSA method is the most suitable for their experiments. However; workflowsfor phylogenetic analysis are both computational and data intensive and they may runsequentially during weeks. Although there any many approaches that parallelize theseworkflows; exploring all MSA methods many become a burden and expensive task. If …,E-Science (e-Science); 2011 IEEE 7th International Conference on,2011,37
Physical and virtual partitioning in OLAP database clusters,Camille Furtado; Alexandre AB Lima; Esther Pacitti; Patrick Valduriez; Marta Mattoso,On-line analytical processing (OLAP) applications require high performance databasesupport to achieve good response time (crucial for decision making). Database clustersprovide a cost-effective alternative to parallel database systems. For OLAP applications; thattypically use heavy weight queries; intra-query parallelism yields better performance as itreduces the execution time of individual queries. Intra-query parallelism is based onprocessing the same query on different subsets of the query table. Combining physical andvirtual partitioning to define table subsets provides flexibility in intra-query parallelism whileoptimizing disk space usage and data availability. Experiments with our partitioningtechnique using TPC-H benchmark queries on a 32-dual node cluster gave linear and super-linear speedup; thereby reducing significantly the time of typical OLAP heavy weight …,Computer Architecture and High Performance Computing; 2005. SBAC-PAD 2005. 17th International Symposium on,2005,37
Dynamic steering of HPC scientific workflows: A survey,Marta Mattoso; Jonas Dias; Kary ACS Ocaña; Eduardo Ogasawara; Flavio Costa; Felipe Horta; Vítor Silva; Daniel de Oliveira,Abstract The field of scientific workflow management systems has grown significantly asapplications start using them successfully. In 2007; several active researchers in scientificworkflow developments presented the challenges for the state of the art in workflowtechnologies at that time. Many issues have been addressed; but one of them named'dynamic workflows and user steering'remains with many open problems despite thecontributions presented in the recent years. This article surveys the early and current effortsin this topic and proposes a taxonomy to identify the main concepts related to addressingissues in dynamic steering of high performance computing (HPC) in scientific workflows. Themain concepts are related to putting the human-in-the-loop of the workflow lifecycle;involving user support in real-time monitoring; notification; analysis and interference by …,Future Generation Computer Systems,2015,34
Using provenance to improve workflow design,Frederico T De Oliveira; Leonardo Murta; Claudia Werner; Marta Mattoso,Abstract With the popularity of scientific workflow management systems (WfMS); workflowspecifications are becoming available. Provenance support in WfMS can help reusing thirdparty code. Browsing can be done through queries instead of ad-hoc search on the Web.Finding dependencies among programs or services through provenance queries; withouttool support; is not a trivial task. Due to the huge number of program versions available andtheir configuration parameters; this task may be heavily error prone and counterproductive.In this work we propose a recommendation service that aims at suggesting frequentcombinations of scientific programs for reuse. Our recommendation service is designed towork over WfMS that provide provenance on workflow specification and execution logs. Wehave based our service on software components reuse and data mining techniques; and …,International Provenance and Annotation Workshop,2008,34
Odyssey-Search: A multi-agent system for component information search and retrieval,Regina MM Braga; Cláudia ML Werner; Marta Mattoso,Abstract Component Based Development aims at constructing software through theintegration of components by using interfaces and contracts among them. However; thesecomponents should be bound to a specific application domain in order to be effectivelyreused. Domain Engineering and Component Based Engineering are adequate techniquesto develop components related to specific domains. A solution for accessing domaininformation; including components; is to use a software layer that integrates differentcomponent information sources. This paper presents OSE; a search and retrieval systemthat provides heterogeneous/distributed access and storage to domain componentinformation. Ideas drawn from the field of agents; user modeling; hypermedia; and mediationwere combined to develop the OSE system. An evolutionary model of the user interests …,Journal of Systems and Software,2006,34
A performance evaluation of x-ray crystallography scientific workflow using scicumulus,Daniel de Oliveira; Kary Ocana; Eduardo Ogasawara; Jonas Dias; Fernanda Baiao; Marta Mattoso,X-ray crystallography is an important field due to its role in drug discovery and its relevancein bioinformatics experiments of comparative genomics; phylogenomics; evolutionaryanalysis; ortholog detection; and three-dimensional structure determination. Managing theseexperiments is a challenging task due to the orchestration of legacy tools and themanagement of several variations of the same experiment. Workflows can model a coherentflow of activities that are managed by scientific workflow management systems (SWfMS).Due to the huge amount of variations of the workflow to be explored (parameters; input data)it is often necessary to execute X-ray crystallography experiments in High PerformanceComputing (HPC) environments. Cloud computing is well known for its scalable and elasticHPC model. In this paper; we present a performance evaluation for the X-ray …,Cloud Computing (CLOUD); 2011 IEEE International Conference on,2011,33
Gerenciando experimentos científicos em larga escala,Marta Mattoso; Cláudia Werner; G Travassos; Vanessa Braganholo; Leonardo Murta,Abstract. Several scientific areas; such as bioinformatics and oil engineering; need means ofexecuting simulation-based experiments. The state of the practice for this; in most of thecases; consists in the execution of a set of programs. This; however; is not enough to dealwith the complexity imposed by the problems that need to be analyzed. This issue getsworse with largescale experiments. In this case; we need a system to manage thecomposition of processes and data in a coherent flux. Also; this system must be capable ofregistering the steps and parameters used in the well-succeeded executions of theexperiment. The main motivation of this paper is in identifying and analyzing the challengesthat need to be addressed to provide computational support to the development of large-scale scientific experiments. The challenges we identify here deal with the general …,SBC-SEMISH,2008,33
Performance evaluation of parallel strategies in public clouds: A study with phylogenomic workflows,Daniel De Oliveira; Kary ACS OcañA; Eduardo Ogasawara; Jonas Dias; JoãO GonçAlves; Fernanda BaiãO; Marta Mattoso,Abstract Data analysis is an exploratory process that demands high performance computing(HPC). SciPhylomics; for example; is a data-intensive workflow that aims at producingphylogenomic trees based on an input set of protein sequences of genomes to inferevolutionary relationships among living organisms. SciPhylomics can benefit from parallelprocessing techniques provided by existing approaches such as SciCumulus cloudworkflow engine and MapReduce implementations such as Hadoop. Despite someperformance fluctuations; computing clouds provide a new dimension for HPC due to itselasticity and availability features. In this paper; we present a performance evaluation forSciPhylomics executions in a real cloud environment. The workflow was executed using twoparallel execution approaches (SciCumulus and Hadoop) at the Amazon EC2 cloud. Our …,Future Generation Computer Systems,2013,32
OdysseyShare: An environment for collaborative component-based development,Claudia Werner; Marco Mangan; Leonardo Murta; Robson Pinheiro; Marta Mattoso; Regina Braga; Marcos Borges,Automated support such as the one provided by software development environments(SDEs) is a key requirement for the systematization of large-scale component-basedsoftware development. However; to provide a component-based SDE; adequate softwaredevelopment process; methods and tools that consider component-based development(CBD) activities must be previously defined. Moreover; CBD can be a highly distributed andcollaborative activity that needs group interaction support. In this paper; we describeOdysseyShare environment; a collaborative component-based SDE under development atthe Computer Science Department of COPPE/UFRJ. It supports activities involved inmodeling; construction; reuse and group interaction by providing an integrated set of toolsand a repository of reusable components.,Information Reuse and Integration; 2003. IRI 2003. IEEE International Conference on,2003,32
Webtransact: A framework for specifying and coordinating reliable web services compositions,Paulo F Pires; Mario Benevides; Marta Mattoso,Abstract The recent evolution of internet technologies; mainly guided by the ExtensibleMarkup Language (XML) and its related technologies; are extending the role of the WorldWide Web from information interaction to service interaction. This next wave of the internetera is being driven by a concept named Web services. The Web services technologyprovides the underpinning to a new business opportunity; ie; the possibility of providingvalue-added services through the composition of basic Web services. However; at itspresent stage; the Web services technology does not provide the necessary tools to buildWeb service compositions. The current Web services technology provides the necessarycommunication model for enabling message exchanges in the Web services environment.However; communication interoperability is only part of the problem when considering the …,*,2002,32
Uncertainty quantification in computational predictive models for fluid dynamics using a workflow management engine,Gabriel Guerra; Fernando A Rochinha; Renato Elias; Daniel De Oliveira; Eduardo Ogasawara; Jonas Furtado Dias; Marta Mattoso; Alvaro LGA Coutinho,ABSTRACT Computational simulation of complex engineered systems requires intensivecomputation and a significant amount of data management. Today; this management is oftencarried out on a case-by-case basis and requires great effort to track it. This is due to thecomplexity of controlling a large amount of data flowing along a chain of simulations.Moreover; many times there is a need to explore parameter variability for the same set ofdata. On a case-by-case basis; there is no register of data involved in the simulation; makingthis process prone to errors. In addition; if the user wants to analyze the behavior of asimulation sample; then he/she must wait until the end of the whole simulation. In thiscontext; techniques and methodologies of scientific workflows can improve the managementof simulations. Parameter variability can be put in the general context of uncertainty …,International Journal for Uncertainty Quantification,2012,31
An opportunistic algorithm for scheduling workflows on grids,Luiz Meyer; Doug Scheftner; Jens Voeckler; Marta Mattoso; Mike Wilde; Ian Foster,Abstract The execution of scientific workflows in Grid environments imposes manychallenges due to the dynamic nature of such environments and the characteristics ofscientific applications. This work presents an algorithm that dynamically schedules tasks ofworkflows to Grid sites based on the performance of these sites when running previous jobsfrom the same workflow. The algorithm captures the dynamic characteristics of Gridenvironments without the need to probe the remote sites. We evaluated the algorithmrunning a workflow in the Open Science Grid using twelve sites. The results showedimprovements up to 150% relative to other four usual scheduling strategies.,International Conference on High Performance Computing for Computational Science,2006,31
Provenance management in Swift,Luiz MR Gadelha Jr; Ben Clifford; Marta Mattoso; Michael Wilde; Ian Foster,Abstract The Swift parallel scripting language allows for the specification; execution andanalysis of large-scale computations in parallel and distributed environments. It incorporatesa data model for recording and querying provenance information. In this article we describethese capabilities and evaluate the interoperability with other systems through the use of theOpen Provenance Model. We describe Swift's provenance data model and compare it to theOpen Provenance Model. We also describe and evaluate activities performed within theThird Provenance Challenge; which consisted of implementing a specific scientific workflow;capturing and recording provenance information of its execution; performing provenancequeries; and exchanging provenance information with other systems. Finally; we proposeimprovements to both the Open Provenance Model and Swift's provenance system.,Future Generation Computer Systems,2011,30
Provenance services for distributed workflows,Sérgio Manuel Serra da Cruz; Patrícia M Barros; Paulo M Bisch; Maria Luiza Machado Campos; Marta Mattoso,Scientific experiments using workflows benefit from mechanisms to trace the generation ofresults. As workflows start to scale it is fundamental to have access to their underlyingprocesses; parameters and data. Particularly in molecular dynamics (MD) simulations; astudy of the interatomic interactions in proteins must use distributed high performancecomputing environments to produce timely results. Scientist's trust in experiments producedby gathering distributed partial results may be limited without provenance information. Thispaper presents a service architecture that captures and stores provenance data fromdistributed; autonomous; replicated and heterogeneous resources. Such provenance datacan be used to trace the history of the distributed execution process. These services can becoupled to workflow management systems. The Kepler system was used as a basis to …,Cluster Computing and the Grid; 2008. CCGRID'08. 8th IEEE International Symposium on,2008,30
Data parallelism in bioinformatics workflows using Hydra,Fábio Coutinho; Eduardo Ogasawara; Daniel De Oliveira; Vanessa Braganholo; Alexandre AB Lima; Alberto MR Dávila; Marta Mattoso,Abstract Large scale bioinformatics experiments are usually composed by a set of data flowsgenerated by a chain of activities (programs or services) that may be modeled as scientificworkflows. Current Scientific Workflow Management Systems (SWfMS) are used toorchestrate these workflows to control and monitor the whole execution. It is very common inbioinformatics experiments to process very large datasets. In this way; data parallelism is acommon approach used to increase performance and reduce overall execution time.However; most of current SWfMS still lack on supporting parallel executions in highperformance computing (HPC) environments. Additionally keeping track of provenance datain distributed environments is still an open; yet important problem. Recently; Hydramiddleware was proposed to bridge the gap between the SWfMS and the HPC …,Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing,2010,29
Parallel query processing for OLAP in grids,Nelson Kotowski; Alexandre AB Lima; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract OLAP query processing is critical for enterprise grids. Capitalizing on ourexperience with the ParGRES database cluster; we propose a middleware solution;GParGRES; which exploits database replication and inter-and intra-query parallelism toefficiently support OLAP queries in a grid. GParGRES is designed as a wrapper that enablesthe use of ParGRES in PC clusters of a grid (in our case; Grid5000). Our approach has twolevels of query splitting: grid-level splitting; implemented by GParGRES; and node-levelsplitting; implemented by ParGRES. GParGRES has been partially implemented asdatabase grid services compatible with existing grid solutions such as the open grid servicearchitecture and the Web services resource framework. We give preliminary experimentalresults obtained with two clusters of Grid5000 using queries of the TPC-H Benchmark …,Concurrency and Computation: Practice and Experience,2008,29
Kairos: an architecture for securing authorship and temporal information of provenance data in grid-enabled workflow management systems,Luiz MR Gadelha Jr; Marta Mattoso,Secure provenance techniques are essential in generating trustworthy provenance records;where one is interested in protecting their integrity; confidentiality; and availability. In thiswork; we suggest an architecture to provide protection of authorship and temporalinformation in grid-enabled provenance systems. It can be used in the resolution ofconflicting intellectual property claims; and in the reliable chronological reconstitution ofscientific experiments. We observe that some techniques from public key infrastructures canbe readily applied for this purpose. We discuss the issues involved in the implementation ofsuch architecture and describe some experiments realized with the proposed techniques.,eScience; 2008. eScience'08. IEEE Fourth International Conference on,2008,29
ProtozoaDB: dynamic visualization and exploration of protozoan genomes,Alberto MR Davila; Pablo N Mendes; Glauber Wagner; Diogo A Tschoeke; Rafael RC Cuadrat; Felipe Liberman; Luciana Matos; Thiago Satake; Kary ACS Ocana; Omar Triana; Sergio MS Cruz; Henrique CL Juca; Juliano C Cury; Fabricio N Silva; Guilherme A Geronimo; Margarita Ruiz; Eduardo Ruback; Floriano P Silva; Christian M Probst; Edmundo C Grisard; Marco A Krieger; Samuel Goldenberg; Maria CR Cavalcanti; Milton O Moraes; Maria LM Campos; Marta Mattoso,Abstract ProtozoaDB (http://www. biowebdb. org/protozoadb) is being developed to initiallyhost both genomics and post-genomics data from Plasmodium falciparum; Entamoebahistolytica; Trypanosoma brucei; T. cruzi and Leishmania major; but will hopefully host otherprotozoan species as more genomes are sequenced. It is based on the Genomics UnifiedSchema and offers a modern Web-based interface for user-friendly data visualization andexploration. This database is not intended to duplicate other similar efforts such as GeneDB;PlasmoDB; TcruziDB or even TDRtargets; but to be complementary by providing furtheranalyses with emphasis on distant similarities (HMM-based) and phylogeny-basedannotations including orthology analysis. ProtozoaDB will be progressively linked to theabove-mentioned databases; focusing in performing a multi-source dynamic combination …,Nucleic acids research,2007,29
Desafios no apoio à composição de experimentos científicos em larga escala,Marta Mattoso; Cláudia Werner; G Travassos; Vanessa Braganholo; Leonardo Murta; Eduardo Ogasawara; F Oliveira; Wallace Martinho,Abstract. Management of scientific experiments requires a set of specific functionalities. Oneof such functionalities is the support to experiment composition; which includes scientificworkflows conception. However; little support is given to: conception and instantiation forexecution in a Workflow Management System; reuse of workflows; control about theevolution of different workflow; and gathering of information for provenance of data. In thispaper we present solutions to some of these problems. Such solutions were obtained withSoftware Engineering and Databases Techniques. Preliminary results with real experimentspoint to the feasibility of this approach. Resumo. Para que experimentos científicos em largaescala possam ser gerenciados; é necessário que um conjunto de funcionalidades estejapresente. Dentre essas funcionalidades está o apoio à composição dos experimentos …,Seminário Integrado de Software e Hardware; SEMISH,2009,28
Planning spatial workflows to optimize grid performance,Luiz Meyer; James Annis; Mike Wilde; Marta Mattoso; Ian Foster,Abstract In many scientific workflows; particularly those that operate on spatially orienteddata; jobs that process adjacent regions of space often reference large numbers of files incommon. Such workflows; when processed using workflow planning algorithms that areunaware of the application's file reference pattern; result in a huge number of redundant filetransfers between grid sites and consequently perform poorly. This work presents ageneralized approach to planning spatial workflow schedules for Grid execution based onthe spatial proximity of files and the spatial range of jobs. We evaluate our solution to thisproblem using the file access pattern of an astronomy application that performs co-additionof images from the Sloan Digital Sky Survey. We show that; in initial tests on Grids of 5 to 25sites; our spatial clustering approach eliminates 50% to 90% of the file transfers between …,Proceedings of the 2006 ACM symposium on Applied computing,2006,26
OLAP query processing in a database cluster,Alexandre AB Lima; Marta Mattoso; Patrick Valduriez,Abstract The efficient execution of OLAP queries; which are typically read-only and heavy-weight; is a hard problem which has been traditionally solved using tightly-coupledmultiprocessors. Considering a database cluster as a cost-effective alternative; we proposean efficient; yet simple; solution; called fined-grained virtual partitioning to OLAP parallelquery processing. We designed this solution for a shared-nothing database clusterarchitecture that can scale up to very large configurations and support black-box DBMSusing non intrusive; simple techniques. To validate our solution; we implemented a Javaprototype on a 16 node cluster system and ran experiments with typical queries of the TPC-H benchmark. The results show that our solution yields linear; and sometimes super-linear;speedup. With 16 nodes; it outperforms traditional virtual partitioning by a factor of 6.,European Conference on Parallel Processing,2004,26
Sharing scientific models in environmental applications,Maria Cláudia Cavalcanti; Marta Mattoso; Maria Luiza Campos; François Llirbat; Eric Simon,Abstract Environmental applications have been stimulating the cooperation among scientistsfrom different disciplines. There are many examples where this cooperation takes placethrough exchanging scientific resources; such as data; programs and mathematical models.Finding the right model to apply in an environmental problem is a difficult task. Usually; thisdecision is based on previous experience. To facilitate the exchange and dissemination ofinformation we propose a scientific resources architecture; where scientists may share theirdata; programs and models. We also present a scientific publishing metamodel for scientificresources description. The main goal of the proposed architecture is to provide scientificmetadata support to effective model sharing; representing an innovative contribution toenvironmental applications. Scientific experiments and workflows are also considered as …,Proceedings of the 2002 ACM symposium on Applied computing,2002,26
Infra-estrutura Odyssey: estágio atual,CML Werner; RMM Braga; Marta Mattoso; Leonardo Murta; M Costa; R Pinheiro; A Oliveira,Resumo This paper presents the current version of the Odyssey infrastructure; underdevelopment at COPPE/UFRJ. The infrastructure provides support to Component-basedSoftware Development within specific application domains. It is built in Java and uses theobject manager GOA++.,XIV Simpósio Brasileiro de Engenharia de Software; Sessão de Ferramentas,2000,24
User-steering of HPC workflows: state-of-the-art and future directions,Marta Mattoso; Kary Ocaña; Felipe Horta; Jonas Dias; Eduardo Ogasawara; Vitor Silva; Daniel de Oliveira; Flavio Costa; Igor Araújo,Abstract In 2006 a group of leading researchers was gathered to discuss several challengesto scientific workflow supporting technologies and many of which still remain openchallenges; such as the steering of workflows by users. Due to big data and long lastingworkflows; many users demand steering features such as real-time monitoring; analysis andspecially execution interference. The workflow execution should respond dynamically tosuch interference in the execution; to support the experimentation process in highperformance computing. This paper revisits the issues in the user steering and dynamicworkflows; presenting the state-of-the-art in it; and the open challenges. Our goal is todiscuss research issues related to scientists' steering and present some ideas on how thesedemands may be supported in current scientific workflow technologies.,Proceedings of the 2nd ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies,2013,23
Structural genomic workflows supported by web services,Maria Cláudia Cavalcanti; Fernanda Baião; SC Rossle; Paulo Mascarello Bisch; Rafael Targino; Paulo F Pires; Maria Luiza Campos; Marta Mattoso,In silico experiments encompass multiple combinations of program and data resources;which are complex to be managed. Typically; script languages are used due to their ease ofuse; despite their specifity and difficulty of reuse. In contrast; Web service technology wasspecially conceived to encapsulate and combine programs and data; providinginteroperability; scalability and flexibility issues. We have combined metadata support withWeb services within a framework that supports scientific workflows. We have experimentedthis framework with a real structural genomic workflow; showing its viability and evidencingits advantages.,Database and Expert Systems Applications; 2003. Proceedings. 14th International Workshop on,2003,23
Exploring molecular evolution reconstruction using a parallel cloud based scientific workflow,Kary ACS Ocaña; Daniel de Oliveira; Felipe Horta; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Abstract Recent studies of evolution at molecular level address two important issues:reconstruction of the evolutionary relationships between species and investigation of theforces of the evolutionary process. Both issues experienced an explosive growth in the lasttwo decades due to massive generation of genomic data; novel statistical methods andcomputational approaches to process and analyze this large volume of data. Mostexperiments in molecular evolution are based on computing intensive simulations precededby other computation tools and post-processed by computing validators. All these tools canbe modeled as scientific workflows to improve the experiment management while capturingprovenance data. However; these evolutionary analyses experiments are very complex andmay execute for weeks. These workflows need to be executed in parallel in High …,Brazilian Symposium on Bioinformatics,2012,22
Comparison and versioning of scientific workflows,Eduardo Ogasawara; Pablo Rangel; Leonardo Murta; Claudia Werner; Marta Mattoso,Scientific Workflow Management Systems have become a widely used tool to orchestrate asequence of programs; defined by scientific workflows; to build complex computersimulations. With the aid of these workflows; scientists develop their models for in-silicoexperiments. Although these workflows are very dynamic and need a large number ofchanges in their definition; very little effort has been done to support configurationmanagement of these definitions. This work presents a configuration management modeldeveloped for scientific workflows; which includes both version control and diff/mergealgorithms.,Comparison and Versioning of Software Models; 2009. CVSM'09. ICSE Workshop on,2009,22
High-performance query processing of a real-world OLAP database with ParGRES,Melissa Paes; Alexandre AB Lima; Patrick Valduriez; Marta Mattoso,Abstract Typical OLAP queries take a long time to be processed so speeding up theexecution of each single query is imperative to decision making. ParGRES is an open-source database cluster middleware for high performance OLAP query processing. Byexploiting intra-query parallelism on PC clusters; ParGRES has shown excellentperformance using the TPC-H benchmark. In this paper; we evaluate ParGRES on a real-world OLAP database. Through adaptive virtual partitioning of the database; ParGRESyields linear and very often super-linear speedup for frequent queries. This shows thatParGRES is a very cost-effective solution for OLAP query processing in real settings.,International Conference on High Performance Computing for Computational Science,2008,22
Parallelism in bioinformatics workflows,Luiz AVC Meyer; Shaila C Rössle; Paulo M Bisch; Marta Mattoso,Abstract Parallel processing is frequently used in bioinformatics programs and in DatabaseManagement Systems to improve their performance. Parallelism can be also used toimprove performance of a combination of programs in bioinformatics workflows. This workpresents a characterization of parallel processing in scientific workflows and shows realexperimental results with different configurations for data and programs distribution withinbioinformatics workflow execution. The implementation was done with real structuralgenomic and automatic comparative annotation workflows and the experiments run on acluster of PCs.,International Conference on High Performance Computing for Computational Science,2004,22
Algebraic dataflows for big data analysis,Jonas Dias; Eduardo Ogasawara; Daniel De Oliveira; Fabio Porto; Patrick Valduriez; Marta Mattoso,Analyzing big data requires the support of dataflows with many activities to extract andexplore relevant information from the data. Recent approaches such as Pig Latin propose ahigh-level language to model such dataflows. However; the dataflow execution is typicallydelegated to a MapRe-duce implementation such as Hadoop; which does not follow analgebraic approach; thus it cannot take advantage of the optimization opportunities ofPigLatin algebra. In this paper; we propose an approach for big data analysis based onalgebraic workflows; which yields optimization and parallel execution of activities andsupports user steering using provenance queries. We illustrate how a big data processingdataflow can be modeled using the algebra. Through an experimental evaluation using realdatasets and the execution of the dataflow with Chiron; an engine that supports our …,Big Data; 2013 IEEE International Conference on,2013,21
Similarity-based workflow clustering,Vıtor Silva; Fernando Chirigati; Kely Maia; Eduardo Ogasawara; D Oliveira; Vanessa Braganholo; Leonardo Murta; Marta Mattoso,ABSTRACT Scientists have been using scientific workflow management systems (SWfMS) tosupport scientific experiments. However; SWfMS expect a modeled workflow to berepresented on its workflow language to be executed. The scientist does not have anassistance or guidance to obtain a modeled workflow. Experiment lines; which are a novelapproach to deal with these limitations; allow for the abstract representation and systematiccomposition of experiments. Since there are many scientific workflows already modeled andsuccessfully executed; they can be used to leverage the construction of new abstractrepresentations. These previous experiments can be helpful by identifying scientificworkflow clusters that are generated according to similarity criteria. This paper proposesSimiFlow; which is an architecture for similarity-based comparison and clustering to build …,JCIS,2011,21
Processing queries over distributed XML databases,Guilherme Figueiredo; Vanessa Braganholo; Marta Mattoso,Abstract The increasing volume of data stored as XML documents makes fragmentationtechniques an alternative to the performance issues in query processing. Fragmenteddatabases are feasible only if there is a transparent way to query the distributed database.Fragments allow for intra-query parallel processing and data reduction. This paper presentsour methodology for XQuery query processing over distributed XML databases. Themethodology comprises the steps of query decomposition; data localization; and globaloptimization. This methodology can be used in an XML database or in a system thatpublishes homogeneous views of semi-autonomous databases. An implementation hasbeen done and experimental results can achieve performance improvements of up to 95%when compared to the centralized environment.,Journal of Information and Data Management,2010,21
A strategy for provenance gathering in distributed scientific workflows,Anderson Marinho; Cláudia Werner; Sérgio Manuel Serra da Cruz; Marta Mattoso; Vanessa Braganholo; Leonardo Murta,Running scientific workflows in distributed environments is motivating the definition ofprovenance approaches that are loosely coupled to the workflow system. This kind ofapproach is interesting because it allows both storage and access to provenance data in anintegrated way; even in an environment where different workflow management systems worktogether. In order to provide provenance functionalities; the existing approaches overloadscientists with many manually computing tasks; such as script adaptations andimplementations of extra functionalities. However; when we are dealing with users who donot have such expertise (the majority of scientists do not have it); this is not a good solution.Hence; the objective of this paper is to define a provenance strategy that facilitates thegathering of provenance information in a distributed environment scenario.,Services-I; 2009 World Conference on,2009,21
Towards an inductive design of distributed object oriented databases,F Baido; Marta Mattoso; Gerson Zaverucha,Cooperative information systems (CIS) often consist of applications that access sharedresources such as databases. Since centralized systems may have a great impact on thesystem performance; parallel and distribution techniques are needed for attaining scalability.Distributed databases are; then; crucial for the development of cooperative applications.However; in order to improve performance; it is very important to design informationdistribution properly; which is the goal of distribution design. Considering the variousdifficulties embedded in the design of distributed object oriented databases; this workpresents an algorithm to assist distribution designers in their task. The analysis algorithmindicates the most adequate fragmentation technique (vertical; horizontal or mixed) for eachclass in the database schema; and we propose the use of a machine learning method …,Cooperative Information Systems; 1998. Proceedings. 3rd IFCIS International Conference on,1998,21
Horizontal fragmentation in object dbms: New issues and performance evaluation,Fernanda Baião; Marta Mattoso; Gerson Zaverucha,Horizontal fragmentation may improve the performance of database systems. Definingprimary and derived horizontal fragmentation along the classes in a database schema is animportant and complex issue; yet not discussed in the literature; which must be consideredwhen horizontally fragmenting a database. In this paper; we focus on an analysis to help thedesigner upon the decision for primary or derived horizontal fragmentation. This analysisconsiders performance results of distributed databases using horizontal fragmentation toevaluate the potential benefits and drawbacks of primary and derived techniques. Therefore;this paper presents a horizontal fragmentation algorithm that chooses the most adequatestrategy (primary or derived) based on class relationships; single class access and queryaccess frequencies.,Performance; Computing; and Communications Conference; 2000. IPCCC'00. Conference Proceeding of the IEEE International,2000,20
MTCProv: a practical provenance query framework for many-task scientific computing,Luiz MR Gadelha; Michael Wilde; Marta Mattoso; Ian Foster,Abstract Scientific research is increasingly assisted by computer-based experiments. Suchexperiments are often composed of a vast number of loosely-coupled computational tasksthat are specified and automated as scientific workflows. This large scale is alsocharacteristic of the data that flows within such “many-task” computations (MTC).Provenance information can record the behavior of such computational experiments via thelineage of process and data artifacts. However; work to date has focused on lineage datamodels; leaving unsolved issues of recording and querying other aspects; such as domain-specific information about the experiments; MTC behavior given by resource consumptionand failure information; or the impact of environment on performance and accuracy. In thiswork we contribute with MTCProv; a provenance query framework for many-task scientific …,Distributed and Parallel Databases,2012,19
An architecture for managing distributed scientific resources,Maria Cláudia Cavalcanti; Marta Mattoso; Maria Luiza Campos; Eric Simon; François Llirbat,There are many examples where cooperation among scientists takes place by exchangingscientific resources; such as data; programs and mathematical models. This is particularlytrue for environmental applications. Finding the right resource to apply in an environmentalproblem is a difficult task. Usually; this decision is based on previous experience. Scientistshave to cooperate in order to solve such problems. To facilitate the exchange; reuse anddissemination of information we propose an architecture for managing distributed scientificresources. Our proposal combines a mediation-based heterogeneous distributed databasesystem and an enhanced metadata support system for effective management of distributedscientific models and data.,Scientific and Statistical Database Management; 2002. Proceedings. 14th International Conference on,2002,19
Using ontologies to support deep water oil exploration scientific workflows,Daniel de Oliveira; Luiz Cunha; Luiz Tomaz; Vinicius Pereira; Marta Mattoso,Scientific experiments generate a large amount of data to be processed and analyzed. Asthe amount of data increases; the way engineers define their own experiments; analyze theoutput and share them is becoming complex to manage. Scientific Workflow ManagementSystems (WfMS) are being used to orchestrate a sequence of programs; services andresources; defined by scientific workflows. However; current WfMS are focused on theworkflow execution and present limitations on the semantic support to design theexperiment. These tools lack on semantic descriptions of available resources to designscientific workflow. This paper presents an ontology for deep water oil exploration workflow.This ontology has been used to present some semantic concepts to help defining a workflowto be further executed by a WfMS. We evaluate this semantic support on a real workflow …,Services-I; 2009 World Conference on,2009,18
XCraft: boosting the performance of active XML materialization,Gabriela Ruberg; Marta Mattoso,Abstract An active XML (AXML) document contains tags representing calls to Web services.Therefore; retrieving its contents consists in materializing its data elements by invoking theembedded service calls in a P2P network. In this process; the result of some service callscan be used as input of other calls. Also; usually several peers provide each requested Webservice; and peers can collaborate to invoke these services. This often implies a hugesearch space of many equivalent materialization alternatives; each with differentperformance. In this paper; we model AXML documents from a workflow perspective andpropose a dynamic cost-based optimization strategy to efficiently materialize them;considering the volatility of a typical P2P scenario. Our strategy enables the optimizer; calledXCraft; to get more up-to-date information on the status of the peers; and to deliver partial …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,18
Mediating heterogeneous web services,Paulo F Pires; Mário RF Benevides; Marta Mattoso,Web services technology provides the foundation for a new business opportunity; ie; thepossibility of providing value-added Web services. However; building value-added servicesin this new environment is not a trivial task. Due to many singularities of the Web serviceenvironment; such as the inherent structural and behavioral heterogeneity of Web services;in addition to their strict autonomy; it is not possible to rely on current models and solutionsto build and coordinate compositions of Web services. We present a mediation service layerwithin an architecture; named WebTransact; which provides the necessary mechanisms forhomogenizing heterogeneous Web services. Such architecture separates the task ofaggregating and homogenizing heterogeneous Web services from the task of specifyingtransaction interaction patterns; thus providing a new general mechanism to deal with the …,Applications and the Internet; 2003. Proceedings. 2003 Symposium on,2003,18
Odyssey: Infra-estrutura de Reutilização baseada em Modelos de Domínio,Cláudia Werner; Marta Mattoso; Regina Braga; Márcio Barros; Leonardo Murta; Alexandre Dantas,Resumo Este trabalho apresenta o estágio atual do projeto Odyssey; em desenvolvimentona COPPE/UFRJ. O projeto tem como objetivo a construção de uma infra-estrutura dereutilização baseada em modelos de domínio. A infra-estrutura Odyssey provê suporte aoDesenvolvimento Baseado em Componentes (DBC) em domínios específicos. Frameworksorientados a objetos; arquiteturas de software; técnicas de inteligência artificial; engenhariade domínio; hipermídia; e mediadores são algumas das tecnologias adotadas peloOdyssey. A infra-estrutura está sendo construída em Java e utiliza o servidor de objetosGOA++.,XIII Simpósio Brasileiro de Engenharia de Software; Caderno de Ferramentas; Florianópolis; Brasil,1999,18
Designing a parallel cloud based comparative genomics workflow to improve phylogenetic analyses,Kary ACS Ocaña; Daniel De Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Abstract Over the last years; comparative genomics analyses have become more compute-intensive due to the explosive number of available genome sequences. Comparativegenomics analysis is an important a prioristep for experiments in various bioinformaticsdomains. This analysis can be used to enhance the performance and quality of experimentsin areas such as evolution and phylogeny. A common phylogenetic analysis makesextensive use of Multiple Sequence Alignment (MSA) in the construction of phylogenetictrees; which are used to infer evolutionary relationships between homologous genes. Eachphylogenetic analysis aims at exploring several different MSA methods to verify whichexecution produces trees with the best quality. This phylogenetic exploration may run duringweeks; even when executed in High Performance Computing (HPC) environments …,Future Generation Computer Systems,2013,17
Towards a cost model for scheduling scientific workflows activities in cloud environments,Vitor Viana; Daniel De Oliveira; Marta Mattoso,Cloud computing has emerged as a new paradigm that enables scientists to benefit fromseveral distributed resources such as hardware and software. Clouds poses as anopportunity for scientists that need high performance computing infrastructure to executetheir scientific experiments. Most of the experiments modeled as scientific workflowsmanage the execution of several activities and work with large amounts of data. In this wayparallel techniques are often a key factor. Parallelizing a scientific workflow in the cloudenvironment is not trivial. One of the complex tasks is to define the number and types ofvirtual machines and to design the parallel execution strategy. Due to the number of optionsfor configuring an environment it is a hard task to do it manually and it may produce negativeimpact on performance. This paper initially proposes a cost model based on concepts of …,Services (SERVICES); 2011 IEEE World Congress on,2011,17
GExpLine: a tool for supporting experiment composition,Daniel de Oliveira; Eduardo Ogasawara; Fernando Seabra; Vítor Silva; Leonardo Murta; Marta Mattoso,Abstract Scientific experiments present several advantages when modeled at highabstraction levels; independent from Scientific Workflow Management System (SWfMS)specification languages. For example; the scientist can define the scientific hypothesis interms of algorithms and methods. Then; this high level experiment can be mapped intodifferent scientific workflow instances. These instances can be executed by a SWfMS andtake advantage of its provenance records. However; each workflow execution is oftentreated by the SWfMS as independent instances. There are no tools that allow modeling theconceptual experiment and linking it to the diverse workflow execution instances. This workpresents GExpLine; a tool for supporting experiment composition through provenance. In ananalogy to software development; it can be seen as a CASE tool while a SWfMS can be …,International Provenance and Annotation Workshop,2010,17
OrthoSearch: a scientific workflow approach to detect distant homologies on protozoans,Sergio Manuel Serra da Cruz; Vanessa Batista; Alberto MR Dávila; Edno Silva; Frederico Tosta; Clarissa Vilela; Maria Luiza M Campos; Rafael Cuadrat; Diogo Tschoeke; Marta Mattoso,Abstract Managing bioinformatics experiments is challenging due to the orchestration andinteroperation of tools with semantics. An effective approach for managing thoseexperiments is through workflow management systems (WfMS). We present several WfMSfeatures for supporting genome homology workflows and discuss relevant issues for typicalgenomic experiments. In our evaluation we used OrthoSearch; a real genomic pipelineoriginally defined as a Perl script. We modeled it as a scientific workflow and implemented iton Kepler WfMS. We show a case study detecting distant homologies on trypanomatidsmetabolic pathways. Our results reinforce the benefits of WfMS over script languages andpoint out challenges to WfMS in distributed environments.,Proceedings of the 2008 ACM symposium on Applied computing,2008,17
The Ecobase project: database and web technologies for environmental information systems,Luc Bouganim; Maria Claudia Cavalcanti; Françoise Fabret; Maria Luiza Campos; François Llirbat; Marta Mattoso; Rubens Melo; Ana Maria Moura; Esther Pacitti; Fabio Porto; Margareth Simoes; Eric Simon; Asterio Tanaka; Patrick Valduriez,Abstract A very large number of data sources on environment; energy; and natural resourcesare available worldwide. Unfortunately; users usually face several problems when they wantto search and use relevant information. In the Ecobase project; we address these problemsin the context of several environmental applications in Brazil and Europe. We propose adistributed architecture for environmental information systems (EIS) based on the Le Selectmiddleware developed at INRIA. In this paper; we present this architecture and itscapabilities; and discuss the lessons learned and open issues.,ACM Sigmod Record,2001,17
Dimensioning the virtual cluster for parallel scientific workflows in clouds,Daniel de Oliveira; Vitor Viana; Eduardo Ogasawara; Kary Ocana; Marta Mattoso,Abstract Cloud computing has established itself as a solid computational model that allowsfor scientists to use a series of distributed virtual resources to execute a wide range ofscientific experiments. In several cases; there is a demand for high performance in executingthese experiments since many activities are data and computing intensive. Parallelismtechniques are a key issue in this experimentation process. There are approaches thatprovide parallelism capabilities for scientific workflows in clouds. However; most of them relyon the scientist to dimension the virtual cluster to be instantiated. Dimensioning the virtualcluster to execute the workflow in parallel may be a hard task to accomplish; ie it is hard todefine and adapt the optimal number of virtual machines to be used. Most systems followthis manual configuration of the scientist for the whole workflow execution; using adaptive …,Proceedings of the 4th ACM workshop on Scientific cloud computing,2013,16
Enabling re-executions of parallel scientific workflows using runtime provenance data,Flávio Costa; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract Capturing provenance data in scientific workflows is a key issue since it allows forreproducibility and evaluation of results. Many of these workflows generate around 100;000tasks that execute in parallel in High Performance Computing environments; such as largeclusters and clouds. SciCumulus is a workflow engine for parallel execution in clouds.Activity failure is almost inevitable in clouds where virtual machine failures are a realityrather than a possibility. We present SciMultaneous; a service architecture that manages re-executions of failed scientific workflow tasks using runtime provenance. Experimental resultson clouds showed that SciMultaneous considerably increases the workflow completion andreduces the total execution time of the workflow (considering executions and re-executions)up to 11.5%; when compared to ad-hoc approaches.,International Provenance and Annotation Workshop,2012,15
Provenance Query Patterns for Many-Task Scientific Computing.,Luiz MR Gadelha Jr; Marta Mattoso; Michael Wilde; Ian T Foster,Page 1. Provenance Query Patterns for Many-Task Scientific Computing Luiz Gadelha; MartaMattoso Federal University of Rio de Janeiro Michael Wilde; Ian Foster University ofChicago/Argonne National Laboratory 3rd USENIX Workshop on the Theory and Practiceof Provenance (TaPP '11) Heraklion; Crete; Greece June 20; 2011 Page 2. Introduction ▶Problem definition: provenance modeling; gathering and querying for many-task computing(MTC). ► Data model for MTC provenance (OPM specialization). ► Identification of querypatterns for MTC provenance. ► Creating support for the identified query patterns. ►Implemented in the Swift parallel scripting system. 2 / 14 Page 3. Provenance Model ▶Requirements: 1. Gather consumption and production relationships between datasets andprocesses. 2. Gather hierarchical relationships between datasets …,TaPP,2011,15
Adaptive virtual partitioning for OLAP query processing in a database cluster,Alexandre AB Lima; Marta Mattoso; Patrick Valduriez,Abstract. OLAP queries are typically heavy-weight and ad-hoc thus requiring high storage capacityand processing power. In this paper; we address this problem using a database cluster whichwe see as a cost-effective alternative to a tightly-coupled multiprocessor. We propose a solutionto efficient OLAP query processing using a simple data parallel processing technique calledadaptive virtual partitioning which dynamically tunes partition sizes; without requiring any knowledgeabout the database and the DBMS. To validate our solution; we implemented a Java prototypeon a 32 node cluster system and ran experiments with typical queries of the TPC-Hbenchmark. The results show that our solution yields linear; and sometimes super-linear;speedup. In many cases; it outperforms traditional virtual partitioning by factors superior to10 … Categories and Subject Descriptors: Information Systems [Miscellaneous] …,Journal of Information and Data Management,2010,15
A lightweight middleware monitor for distributed scientific workflows,Sergio Manuel Serra da Cruz; Fabricio Nogueira da Silva; Luiz MR Gadelha Jr; Maria Claudia Reis Cavalcanti; Maria Luiza M Campos; Marta Mattoso,Monitoring the execution of distributed tasks within the workflow execution is not easy and isfrequently controlled manually. This work presents a lightweight middleware monitor todesign and control the parallel execution of tasks from a distributed scientific workflow. Thismiddleware can be connected into a workflow management system. This middlewareimplementation is evaluated with the Kepler workflow management system; by includingnew modules to control and monitor the distributed execution of the tasks. Thesemiddleware modules were added to a bioinformatics workflow to monitor parallel BLASTexecutions. Results show potential to high performance process execution while preservingthe original features of the workflow.,Cluster Computing and the Grid; 2008. CCGRID'08. 8th IEEE International Symposium on,2008,15
Applying theory revision to the design of distributed databases,Fernanda Baião; Marta Mattoso; Jude Shavlik; Gerson Zaverucha,Abstract This work presents the application of theory revision to the design of distributeddatabases to automatically revise a heuristic-based algorithm (called analysis algorithm)through the use of the FORTE system. The analysis algorithm decides the fragmentationtechnique to be used in each class of the database and its Prolog implementation isprovided as the initial domain theory. Fragmentation schemas with previously knownperformance; obtained from experimental results on top of an object database benchmark;are provided as the set of examples. We show the effectiveness of our approach in findingbetter fragmentation schemas with improved performance.,International Conference on Inductive Logic Programming,2003,15
Mining a large database with a parallel database server,Mauro Sergio R De Sousa; Marta Mattoso; Nelson FF Ebecken,Abstract Data mining is a data-intensive computation activity. Parallel processing has oftenbeen used in data mining algorithms. However; when data do not fit in memory; somesolutions do not apply and a database system may be required rather than flat files. Most ofthe implementations use the database system loosely coupled with the data miningtechniques. Hence; the database system only issues queries to be processed on the clientmachine. In this work; we address the data consuming activities through parallel processingon a database server providing a tight integration with data mining techniques. Experimentalresults showing the potential benefits of this integration were obtained. Despite thedifficulties in processing a complex application; we extracted rules and obtained highperformance on all the data-intensive activities such as the construction of the decision …,Intelligent Data Analysis,1999,15
Data-centric iteration in dynamic workflows,Jonas Dias; Gabriel Guerra; Fernando Rochinha; Alvaro LGA Coutinho; Patrick Valduriez; Marta Mattoso,Abstract Dynamic workflows are scientific workflows to support computational sciencesimulations; typically using dynamic processes based on runtime scientific data analyses.They require the ability of adapting the workflow; at runtime; based on user input anddynamic steering. Supporting data-centric iteration is an important step towards dynamicworkflows because user interaction with workflows is iterative. However; current support foriteration in scientific workflows is static and does not allow for changing data at runtime. Inthis paper; we propose a solution based on algebraic operators and a dynamic executionmodel to enable workflow adaptation based on user input and dynamic steering. Weintroduce the concept of iteration lineage that makes provenance data managementconsistent with dynamic iterative workflow changes. Lineage enables scientists to interact …,Future Generation Computer Systems,2015,14
Scientific workflow partitioning in multisite cloud,Ji Liu; Vítor Silva; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract Scientific workflows allow scientists to conduct experiments that manipulate datawith multiple computational activities using Scientific Workflow Management Systems(SWfMSs). As the scale of the data increases; SWfMSs need to support workflow executionin High Performance Computing (HPC) environments. Because of various benefits; cloudemerges as an appropriate infrastructure for workflow execution. However; it is difficult toexecute some scientific workflows in one cloud site because of geographical distribution ofscientists; data and computing resources. Therefore; a scientific workflow often needs to bepartitioned and executed in a multisite environment. Also; SWfMSs generally execute ascientific workflow in parallel within one site. This paper proposes a non-intrusive approachto execute scientific workflows in a multisite cloud with three workflow partitioning …,European Conference on Parallel Processing,2014,14
Using domain-specific data to enhance scientific workflow steering queries,João Carlos de AR Gonçalves; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract In scientific workflows; provenance data helps scientists in understanding;evaluating and reproducing their results. Provenance data generated at runtime can alsosupport workflow steering mechanisms. Steering facilities for workflows is considered achallenge due to its dynamic demands during execution. To steer; for example; scientistsshould be able to suspend (or stop) a workflow execution when the approximate solutionmeets (or deviates) preset criteria. These criteria are commonly evaluated based onprovenance data (execution data) and domain-specific data. We claim that the final decisionon whether to interfere on the workflow execution may only become feasible whenworkflows can be steered by scientists using provenance data enriched with domain-specificdata. In this paper we propose an approach based on specialized software components …,International Provenance and Annotation Workshop,2012,14
Evaluating parameter sweep workflows in high performance computing,Fernando Chirigati; Vítor Silva; Eduardo Ogasawara; Daniel De Oliveira; Jonas Dias; Fábio Porto; Patrick Valduriez; Marta Mattoso,Abstract Scientific experiments based on computer simulations can be defined; executedand monitored using Scientific Workflow Management Systems (SWfMS). Several SWfMSare available; each with a different goal and a different engine. Due to the exploratoryanalysis; scientists need to run parameter sweep (PS) workflows; which are workflows thatare invoked repeatedly using different input data. These workflows generate a large amountof tasks that are submitted to High Performance Computing (HPC) environments. Differentexecution models for a workflow may have significant differences in performance in HPC.However; selecting the best execution model for a given workflow is difficult due to theexistence of many characteristics of the workflow that may affect the parallel execution. Wedeveloped a study to show performance impacts of using different execution models in …,Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies,2012,14
Capturing distributed provenance metadata from cloud-based scientific workflows,Sergio Manuel Serra da Cruz; Carlos Eduardo Paulino; Daniel de Oliveira; Maria Luiza Machado Campos; Marta Mattoso,Abstract Workflows are scientific abstractions used in the modeling of scientific experiments.High performance computing environments such as clusters and grids are often required torun the experiments. Cloud computing is starting to be adopted by the scientific community.However; the cloud environment is still incipient in collecting and recording retrospectiveworkflow provenance. This paper presents an approach to capturing distributed provenancemetadata from cloud-based scientific workflows. The approach was implemented through anevolution of the Matrioshka architecture that was refactored for cloud environments.Preliminary results show that provenance metadata captured from the virtual componentsrunning at the cloud can aid scientists to manage and reproduce their large scale in silicoexperiments.,Journal of Information and Data Management,2011,14
Detecting distant homologies on protozoans metabolic pathways using scientific workflows,Sergio Manuel Serra Da Cruz; Vanessa Batista; Edno Silva; Frederico Tosta; Clarissa Vilela; Rafael Cuadrat; Diogo Tschoeke; Alberto Davila; Maria Campos; Marta Mattoso,Bioinformatics experiments are typically composed of programs in pipelines manipulating anenormous quantity of data. An interesting approach for managing those experiments isthrough workflow management systems (WfMS). In this work we discuss WfMS features tosupport genome homology workflows and present some relevant issues for typical genomicexperiments. Our evaluation used Kepler WfMS to manage a real genomic pipeline; namedOrthoSearch; originally defined as a Perl script. We show a case study detecting distanthomologies on trypanomatids metabolic pathways. Our results reinforce the benefits ofWfMS over script languages and point out challenges to WfMS in distributed environments.,International journal of data mining and bioinformatics,2010,14
Using explicit control processes in distributed workflows to gather provenance,Sérgio Manuel Serra da Cruz; Fernando Seabra Chirigati; Rafael Dahis; Maria Luiza M Campos; Marta Mattoso,Abstract Distributing workflow tasks among high performance environments involves localprocessing and remote execution on clusters and grids. This dis-tribution often needsinteroperation between heterogeneous workflow definition languages and theircorresponding execution machines. A centralized Workflow Management System (WfMS)can be locally controlling the execution of a workflow that needs a grid WfMS to execute asub-workflow that requires high performance. Workflow specification languages oftenprovide different control-flow execution structures. Moving from one environment to anotherrequires mappings between these languages. Due to heterogeneity; control-flow structures;available in one system; may not be supported in another. In these heterogeneousdistributed environments; provenance gathering becomes also heterogeneous. This work …,International Provenance and Annotation Workshop,2008,14
Ariane: An awareness mechanism for shared databases,Vaninha Vieira; Marco AS Mangan; Cláudia Werner; Marta Mattoso,Abstract Awareness is an essential requirement in collaborative activities. This paperpresents Ariane; a generic and reusable awareness infrastructure; independent of a specificapplication or DBMS. Ariane improves the availability of awareness information to differentcooperative applications by monitoring the application persistence mechanism. A prototypeof Ariane was developed using the Java Data Objects (JDO) persistence mechanism andaspect-oriented programming techniques; which were employed in order to increase thepotential reusability of the solution. A preliminary evaluation of the prototype; applied in anenvironment for cooperative software development based on components; confirmed that noadditional code is necessary to monitor JDO complaint applications. Besides; Arianeproposes a multidimensional data structure for awareness information; the awareness …,International Conference on Collaboration and Technology,2004,14
Gerência de Workflows Científicos: uma análise crítica no contexto da bioinformática,Amanda Mattos; F Silva; Nicolaas Ruberg; M Cruz,RESUMO: Esse relatório técnico tem por objetivo apresentar as principais características desistemas de gerência de workflows (SGWf) no apoio à experimentos científicos. SGWf vêmsendo utilizados há algum tempo no contexto de aplicações comerciais. Mais recentemente;experimentos científicos desenvolvidos em larga escala vêm necessitando de um controlesistemático; reprodutível e documentado. Assim; SGWf vem surgindo como uma soluçãoimportante; desde que devidamente adaptados ao contexto científico. Assim; esse relatóriodescreve os principais conceitos e requisitos dos workflows científicos; apresenta algunssistemas disponíveis e aplicações reais em bioinformática e discute os desafios face àstecnologias existentes.,COPPE/UFRJ,2008,13
Software Components Reuse Through Web Search and Retrieval.,Robson P de Souza; Marcelo N Costa; Regina MM Braga; Marta Mattoso; Cláudia Maria Lima Werner,Abstract Component Based Development (CBD) aims at constructing software through theinter-relationship between preexisting components. The main goal of this work is to provideaccess to components that can be reused in all phases of an application development withina given domain. We present an architecture for software components reuse by using amediation layer that integrates the semantics of web components with registered knowncomponents from a virtual library of components. In our architecture; components aredescribed through XML and published by local repositories or remote servers. Theinnovative aspect of our proposal is the use of domain ontologies; for reusable componentretrieval. OQL queries can be issued to the mediation layer and are processed by the GOAobject server; which can present the query results as a list of suggested components …,Workshop on Information Integration On the Web,2001,13
Data mining on parallel database systems,Mauro Sousa; Marta Mattoso; NFF Ebecken,Abstract Recent years have shown the need of an automated process to discover interestingand hidden patterns in real-world databases; handling large volumes of data. This sort ofprocess implies a lot of computational power; memory and disk IO; which can only beprovided by parallel computers. Our work contributes with a solution that integrates amachine learning algorithm; parallelism and a tightly-coupled use of a DBMS system;addressing performance problems with parallel processing and data fragmentation.,Proc. Intl. Conf. on PDPTA: Special Session on Parallel Data Warehousing; CSREA Press; Las Vegas,1998,13
Analyzing related raw data files through dataflows,Vítor Silva; Daniel Oliveira; Patrick Valduriez; Marta Mattoso,Summary Computer simulations may ingest and generate high numbers of raw data files.Most of these files follow a de facto standard format established by the application domain;for example; Flexible Image Transport System for astronomy. Although these formats aresupported by a variety of programming languages; libraries; and programs; analyzingthousands or millions of files requires developing specific programs. Database managementsystems (DBMS) are not suited for this; because they require loading the raw data andstructuring it; which becomes heavy at large scale. Systems like NoDB; RAW; and FastBithave been proposed to index and query raw data files without the overhead of using adatabase management system. However; these solutions are focused on analyzing onesingle large file instead of several related files. In this case; when related files are …,Concurrency and Computation: Practice and Experience,2016,12
Athena: text mining based discovery of scientific workflows in disperse repositories,Flavio Costa; Daniel de Oliveira; Eduardo Ogasawara; Alexandre AB Lima; Marta Mattoso,Abstract Scientific workflows are abstractions used to model and execute in silico scientificexperiments. They represent key resources for scientists and are enacted and managed byengines called Scientific Workflow Management Systems (SWfMS). Each SWfMS has aparticular workflow language. This heterogeneity of languages and formats poses ascomplex scenario for scientists to search or discover workflows in distributed repositories forreuse. The existing workflows in these repositories can be used to leverage the identificationand construction of families of workflows (clusters) that aim at a particular goal. However it ishard to compare the structure of these workflows since they are modeled in different formats.One alternative way is to compare workflow metadata such as natural language descriptions(usually found in workflow repositories) instead of comparing workflow structure. In this …,International Workshop on Resource Discovery,2010,12
PartiX: processing XQuery queries over fragmented XML repositories,Alexandre Andrade; Gabriela Ruberg; Fernanda A Baiao; Vanessa P Braganholo; Marta Mattoso,Abstract. The data volume of XML repositories and the response time of query processinghave become critical issues for many applications; especially for those in the Web. Aninteresting alternative to improve query processing performance consists in reducing thesize of XML databases through fragmentation techniques. However; traditionalfragmentation definitions do not directly apply to collections of XML documents. This workformalizes the fragmentation definition for collections of XML documents; and proposes anarchitecture for XQuery processing on top of fragmented XML data. This architecture wasimplemented in a system prototype named PartiX; which exploits intra-query parallelism ontop of XQueryenabled sequential DBMS modules. We have analyzed several experimentalsettings; and our results showed a performance improvement of up to a 72 scale up factor …,Universidade Federal do Rio de Janeiro; Tech. Rep,2005,12
A CORBA based architecture for heterogeneous information source interoperability,Paulo de Figueiredo Pires; ML Queiros Mattoso,Computational environments are characterized by islands of information. Data is spreadthroughout the computational system nodes and is controlled by several different datamanagement systems. However; application programs require an integrated view of suchdata. Thus; it is necessary to build bridges to connect these information islands.Heterogeneous database systems; especially those object-oriented based have beenreferred to as one of the most viable solutions to the problem of system integration inheterogeneous distributed environments (HDDS). The implementation of such systems isstill challenging. This work presents an experience of implementing one of such systemsusing the CORBA standard as the communication basis in an architecture that uses theconcept of HDDS.,Technology of Object-Oriented Languages and Systems; 1997. TOOLS 25; Proceedings,1997,12
Improving Many-Task computing in scientific workflows using P2P techniques,Jonas Dias; Eduardo Ogasawara; Daniel De Oliveira; Esther Pacitti; Marta Mattoso,Large-scale scientific experiments are usually supported by scientific workflows that maydemand high performance computing infrastructure. Within a given experiment; the sameworkflow may be explored with different sets of parameters. However; the parallelization ofthe workflow instances is hard to be accomplished mainly due to the heterogeneity of itsactivities. Many-Task computing paradigm seems to be a candidate approach to supportworkflow activity parallelism. However; scheduling a huge amount of workflow activities onlarge clusters may be susceptible to resource failures and overloading. In this paper; wepropose Heracles; an approach to apply consolidated P2P techniques to improve Many-Task computing of workflow activities on large clusters. We present a fault tolerancemechanism; a dynamic resource management and a hierarchical organization of …,Many-Task Computing on Grids and Supercomputers (MTAGS); 2010 IEEE Workshop on,2010,11
Integrating provenance data from distributed workflow systems with ProvManager,Anderson Marinho; Leonardo Murta; Cláudia Werner; Vanessa Braganholo; Eduardo Ogasawara; Sérgio Manuel Serra da Cruz; Marta Mattoso,Abstract Running scientific workflows in distributed environments is motivating the definitionof provenance gathering approaches that are loosely coupled to the workflow executionengine. This kind of approach is interesting because it allows both storage and access toprovenance data in an integrated way; even in an environment where different workflowsystems work together. Therefore; we have proposed a provenance gathering strategy thatis independent from the workflow system technology. This strategy has evolved into aprovenance management system named ProvManager. In this paper we show howprovenance data is captured along in a distributed execution environment withProvManager and we show its web interface; in which scientists can register experiments;monitor workflow execution; and query provenance data.,International Provenance and Annotation Workshop,2010,11
Parallel Processing Evaluation of Path Expressions.,Flavio Tavares; André O Victor; Marta Mattoso,Abstract Parallel and distributed processing are alternatives to optimize queries in DatabaseSystems. In this work different alternatives for parallel query processing were implementedand evaluated. This evaluation aims at analyzing the potential for parallel processing ofthese query strategies and providing heuristics to query optimizers. The experiments weremade with an IBM SP/2 parallel machine. Performance evaluation used the datasets andqueries specified by the OO7 benchmark. The results indicated the best query executionstrategy for different path expressions analyzed. The tests also showed a significant parallelpotential for the backward; also known as pointer-based join; execution strategy.Nevertheless; the forward execution strategy; also known as naive pointer chasing; hasproven its effectiveness when objects from a small collection point to objects of a large …,SBBD,2000,11
Multi-objective scheduling of scientific workflows in multisite clouds,Ji Liu; Esther Pacitti; Patrick Valduriez; Daniel De Oliveira; Marta Mattoso,Abstract Clouds appear as appropriate infrastructures for executing Scientific Workflows(SWfs). A cloud is typically made of several sites (or data centers); each with its ownresources and data. Thus; it becomes important to be able to execute some SWfs at morethan one cloud site because of the geographical distribution of data or available resourcesamong different cloud sites. Therefore; a major problem is how to execute a SWf in amultisite cloud; while reducing execution time and monetary costs. In this paper; we proposea general solution based on multi-objective scheduling in order to execute SWfs in amultisite cloud. The solution consists of a multi-objective cost model including execution timeand monetary costs; a Single Site Virtual Machine (VM) Provisioning approach (SSVP) andActGreedy; a multisite scheduling approach. We present an experimental evaluation …,Future Generation Computer Systems,2016,10
Many task computing for orthologous genes identification in protozoan genomes using Hydra,Fábio Coutinho; Eduardo Ogasawara; Daniel Oliveira; Vanessa Braganholo; Alexandre AB Lima; Alberto MR Dávila; Marta Mattoso,SUMMARY One of the main advantages of using a scientific workflow management system(SWfMS) is to orchestrate data flows among scientific activities and register provenance ofthe whole workflow execution. Nevertheless; the execution control of distributed activities inhigh performance computing environments by SWfMS presents challenges such as steeringcontrol and provenance gathering. Such challenges may become a complex task to beaccomplished in bioinformatics experiments; particularly in Many Task Computingscenarios. This paper presents a data parallelism solution for a bioinformatics experimentsupported by Hydra; a middleware that bridges SWfMS and high performance computing toenable workflow parallelization with provenance gathering. Hydra Many Task Computingparallelization strategies can be registered and reused. Using Hydra; provenance may …,Concurrency and Computation: Practice and Experience,2011,10
ARAXA: Storing and managing Active XML documents,Cláudio Ananias Ferraz; Vanessa Braganholo; Marta Mattoso,Abstract Active XML (AXML) documents combine extensional XML data with intentional datadefined through Web service calls. The dynamic properties of these documents posechallenges to both storage and data materialization techniques. In this paper; we presentARAXA; a non-intrusive approach to store and manage AXML documents. We also define amethodology to materialize AXML documents at query time. The storage approach ofARAXA is based on plain relational tables and user-defined functions of Object-RelationalDBMS to trigger the service calls. By using a DBMS we benefit from efficient storage toolsand query optimization. Approaches without DBMS support have to process XML in mainmemory or provide for virtual memory solutions. One of the main advantages of ARAXA isthat AXML documents do not need to be loaded into main memory at query processing …,Web Semantics: Science; Services and Agents on the World Wide Web,2010,10
A P2P approach to many tasks computing for scientific workflows,Eduardo Ogasawara; Jonas Dias; Daniel Oliveira; Carla Rodrigues; Carlos Pivotto; Rafael Antas; Vanessa Braganholo; Patrick Valduriez; Marta Mattoso,Abstract Scientific Workflow Management Systems (SWfMS) are being used intensively tosupport large scale in silico experiments. In order to reduce execution time; current SWfMShave exploited workflow parallelization under the arising Many Tasks Computing (MTC)paradigm in homogeneous computing environments; such as multiprocessors; clusters andgrids with centralized control. Although successful; this solution no longer applies toheterogeneous computing environments; such as hybrid clouds; which may combine users'own computing resources with multiple edge clouds. A promising approach to address thischallenge is Peer-to-Peer (P2P) which relies on decentralized control to deal with scalabilityand dynamic behavior of resources. In this paper; we propose a new P2P approach to applyMTC in scientific workflows. Through the results of simulation experiments; we show that …,International Conference on High Performance Computing for Computational Science,2010,10
Scientific workflow management system applied to uncertainty quantification in large eddy simulation,Gabriel Guerra; Fernando Rochinha; Renato Elias; Alvaro Coutinho; Vanessa Braganholo; D de Oliveira; Eduardo Ogasawara; Fernando Chirigati; Marta Mattoso,Abstract. Currently Large Eddy Simulation (LES) requires intensive computation and a lot ofdata management. Today this management is often carried out in a case by case basis andrequires great effort to track it. This is due to the large amount of data involved; thus makingthis process prone to errors. Moreover; there is a need to explore parameter variability (eg;eddy viscosities) for the same set of data. In this context; techniques and methodologies ofscientific workflows can improve the management of simulations. This variability can be putin the general context of Uncertainty Quantification (UQ); which provides a rationalperspective for analysts and decision makers. The objective of this work is to provide asystematic approach in:(i) modeling of LES numerical experiments;(ii) managing the UQanalysis (iii) running each variation in parallel under the control of the Scientific Workflow …,Congresso Ibero Americano de Métodos Computacionais em Engenharia,2009,10
WebComposer: a tool for the composition and execution of Web Service-based Workflows,Luiz AG da Costa; Paulo F Pires; Marta Mattoso,We present the WebComposer tool for the automatic composition and execution of Webservice-based workflows. We use ontologies to describe and browse workflows. Weassociate messages and operations with workflow domain concepts using WSDLextensibility. The automatic workflow implementation through WebComposer enables thefull separation of the workflow logic and the implementation technology. WebComposerprovides the execution of ad-hoc programs by users and the automatic maintenance ofthese programs; as the available Web services are altered.,WebMedia and LA-Web; 2004. Proceedings,2004,10
Persistência de Componentes num Ambiente de Reuso,Marta Mattoso; Cláudia Werner; Regina Braga; Robson Pinheiro; Leonardo Murta; Victor Almeida; Marcelo Costa; Eduardo Bezerra; Jorge Soares; Nicolaas Ruberg,Ambientes de apoio ao reuso de software; necessitam de recursos de armazenamento emanipulação de diversos tipos de componentes ligados ao Desenvolvimento Baseado emComponentes (DBC). Uma das características de um componente é a complexidade da suarepresentação (podendo ser objeto longo); em particular; dos relacionamentos entre eles.Aliado à necessidade de um modelo capaz de representar e armazenar componentes; estáa manipulação de grandes coleções de componentes e a navegação entre instâncias decomponentes. Tais requisitos praticamente inviabilizam o uso de sistemas de arquivo paraa persistência; devido à falta de flexibilidade do armazenamento e manipulação além deimpor uma gerência desses componentes em memória para manipular coleções grandes.Sendo assim; um SGBD (Sistema de Gerência de Bases de Dados) surge como uma …,XIV Simpósio Brasileiro de Engenharia de Software,2000,10
An analysis of the integration between data mining applications and database systems,E Bezerra; M Mattoso; G Xexeo,Abstract In this paper we present a classification of integration frameworks found in theliterature for the coupling of a data mining application and a DBMS. We classify thedatabase coupling in several categories. Along these categories; we analyse several issuesin the integration process such as degree of coupling; flexibility; portability; communicationoverhead; and use of parallelism. We also present the trade-off of using each one of theintegration frameworks and describe the situations where one framework is better than theother. We also describe how to implement DBMS integration in several Data Miningmethods and we discuss implementation aspects including parallelism issues for each one.We analyse these solutions and show their advantages and disadvantages. 1 Introd,WIT Transactions on Information and Communication Technologies,2000,10
Data mining: a tightly-coupled implementation on a parallel database server,Mauro Sousa; Marta Mattoso; NFF Ebrecken,Due to the increasingly difficulty of discovering patterns in real-world databases using onlyconventional OLAP tools; an automated process such as data mining is currently essential.As data mining over large data sets can take a prohibitive amount of time related to thecomputational complexity of the algorithms; parallel processing has often been used as asolution. However; when data does not fit in memory; some solutions do not apply and adatabase system may be required rather than flat files. Most implementations use adatabase system loosely-coupled with the data mining algorithms. We address the dataconsuming activities through parallel processing and data fragmentation on the databaseserver; providing a tight integration with data mining techniques. Experimental results showthat the potential benefits of this integration were obtained; despite the difficulties of …,Database and Expert Systems Applications; 1998. Proceedings. Ninth International Workshop on,1998,10
Parallel query processing in a shared-nothing object database server,LAVC Meyer; M Mattoso,Abstract. Parallel processing on OODBMS (Object Oriented Database ManagementSystems) may improve performance for non conventional applications that manipulate largevolumes of data. This work analyses and develops techniques which contribute for theimprovement of query processing with shared-nothing (SN) parallel OODBMS. A solution forinter-node communication is developed where the effects of communication are minimisedthrough the use of message queues; reduction of the message size and the creation ofspecific processes in each node.,Proc 3rd Int’l Meeting on Vector and Parallel Processing (VECPAR'98); Porto; Portugal,1998,10
Exploring large scale receptor-ligand pairs in molecular docking workflows in HPC clouds,Kary Ocaña; Silvia Benza; Daniel de Oliveira; Jonas Dias; Marta Mattoso,Computer-aided drug design techniques are important assets in pharmaceutical industrybecause of their support for research and development of new drugs. Molecular docking(MD) predicts specific compound's binding modes within the active site of target proteins.Since MD is a time-consuming process; existing approaches reduce the number of receptorsor ligands in docking by evaluating only small sets of compounds. This restriction in thesearch space reduces the chances to uniformly cover the diverse space of compounds andmisses opportunities to recognize whether new drugs can be identified. Another difficultywith large-scale is analyzing the results; eg browsing all directories manually to find whichpairs were docked successfully. To address these issues we explored the potential of dataprovenance analysis and parallel processing of SciCumulus; a cloud Scientific Workflow …,Parallel & Distributed Processing Symposium Workshops (IPDPSW); 2014 IEEE International,2014,9
Runtime dynamic structural changes of scientific workflows in clouds,Igor Dos Santos; Jonas Dias; Daniel De Oliveira; Eduardo Ogasawara; Kary Ocaña; Marta Mattoso,Abstract Existing Scientific Workflow Management Systems (ie SWfMS) effectively supportworkflows that do not need dynamic changes at runtime. SWfMS execute workflows byproviding process management; provenance data and distributed execution on clusters andclouds. However; the support for dynamic changes in workflows is still an open; yetimportant; problem. For example; when the program associated to an activity of the workflowis taking more time than expected to produce results or if the results do not comply withsome quality criteria; the scientist may want to try an alternative algorithm implementation.However; scientists may not want to re-execute the entire workflow for each change theymake in the workflow structure. Alternatively; changing the structure of the workflowdynamically (ie change the programs associated with workflow activities) can improve the …,Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing,2013,9
Discovering drug targets for neglected diseases using a pharmacophylogenomic cloud workflow,Kary ACS Ocana; Daniel de Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Illnesses caused by parasitic protozoan are a research priority. A representative group ofthese illnesses is the commonly known as Neglected Tropical Diseases (NTD). NTDspecially attack low socioeconomic population around the world and new anti-protozoaninhibitors are needed and several drug discovery projects focus on researching new drugtargets. Pharmacophylogenomics is a novel bioinformatics field that aims at reducing thetime and the financial cost of the drug discovery process. Pharmacophylogenomic analysesare applied mainly in the early stages of the research phase in drug discovery.Pharmacophylogenomic analysis executes several bioinformatics programs in a coherentflow to identify homologues sequences; construct phylogenetic trees and executeevolutionary and structural experiments. This way; it can be modeled as scientific …,E-Science (e-Science); 2012 IEEE 8th International Conference on,2012,9
Virtual partitioning ad-hoc queries over distributed XML databases,Carla Rodrigues; Vanessa Braganholo; Marta Mattoso,Abstract XML query processing on large repositories suffers from performance issues.Despite many efficient indexing techniques; oftentimes only physical XML datafragmentation techniques can improve query processing performance. In such approaches;the database is physically partitioned based on the attributes and selection criteria used inthe most frequent queries of the system. Distributed query processing can then takeadvantage of pruning irrelevant fragments and processing the relevant ones in parallel.However; in many applications; such as Decision Support Systems; input queries are ad-hoc. In such cases; there is no frequent attribute access and physical partitioning is not anoption. In relational settings; virtual partitioning has been successfully used to improveperformance in such scenario with parallel query processing. Inspired by those solutions …,Journal of Information and Data Management,2011,9
OLAP query processing in grids,Nelson Kotowski; Alexandre AB Lima; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract. OLAP query processing is critical for enterprise grids. Capitalizing on ourexperience with the ParGRES database cluster; we propose a middleware solution;GParGRES; which exploits database replication and inter-and intra-query parallelism toefficiently support OLAP queries in a grid. GParGRES has been partially implemented asdatabase grid services on Grid5000. We give preliminary experimental results obtained withtwo clusters of Grid5000 using queries of the TPC-H Benchmark. The results show linear oralmost linear speedup in query execution; as more nodes are added in all testedconfigurations.,3rd Workshop on Data Management in Grids (DMG 2007),2007,9
Apuama: combining intra-query and inter-query parallelism in a database cluster,Bernardo Miranda; Alexandre AB Lima; Patrick Valduriez; Marta Mattoso,Abstract Database clusters provide a cost-effective solutionn for high performance queryprocessing. By using either inter-or intra-query parallelism on replicated data; they canaccelerate individual queries and increase throughput. However; there is no databasecluster that combines inter-and intra-query parallelism while supporting intensive updatetransactions. C-JDBC is a successful database cluster that offers inter-query parallelism andcontrols database replica consistency but cannot accelerate individual heavy-weightqueries; typical of OLAP. In this paper; we propose the Apuama Engine; which adds intra-query parallelism to C-JDBC. The result is an open-source package that supports both OLTPand OLAP applications. We validated Apuama on a 32-node cluster running OLAP queriesof the TPC-H benchmark on top of PostgreSQL. Our tests show that the Apuama Engine …,International Conference on Extending Database Technology,2006,9
A Framework for the Design of Distributed Databases.,Fernanda Araujo Baião; Marta Mattoso; Gerson Zaverucha,This work presents a framework to handle the class fragmentation problem during thedesign of distributed object databases. The framework works in the conceptual level; andthus uses the object data model to capture the application semantics represented by theuser. The proposed framework integrates three modules. The heuristic module defines a setof heuristics to drive the fragmentation of object databases and incorporates them in amethodology that includes an analysis algorithm; horizontal and vertical class fragmentationalgorithms. The theory revision module automatically improves the analysis algorithmthrough the use of an artificial intelligence technique named theory revision; usingfragmentation schemas with previously known performance presented as examples. Finally;the branchand-bound module uses optimization techniques to perform an intelligent …,WDAS,2002,9
A methodology for GIS database integration,J Strauch; J Souza; Marta Mattoso,Abstract The effective development of a Multidatabase (MDB) environment for integration ofindependent Geographic Information System (GIS) databases; requires a methodology toguide the users and Database Administrators; as well as the provision of a foundation for thedevelopment of tools to help the integration. The existing methodologies do not considercharacteristics of the cartographic representation in the schemas nor do they take intoaccount the semantic richness of geographic data. They are also limited to a givenapplication domain or to specific information communities. This work presents MMultiGIS; amethodology for the integration of heterogeneous and distributed GIS databases. It hasbeen developed for the MultiGIS System; which aims at providing semantic interoperabilityamong GIS databases. MMultiGIS disciplines the activities during schema integration …,Proceedings of the IEEE Workshop on Knowledge and Data Engineering Exchange (KDEX’98),1998,9
An orthology-based analysis of pathogenic protozoa impacting global health: An improved comparative genomics approach with prokaryotes and model eukaryote o...,Rafael RC Cuadrat; Sérgio Manuel da Serra Cruz; Diogo Antônio Tschoeke; Edno Silva; Frederico Tosta; Henrique Jucá; Rodrigo Jardim; Maria Luiza M Campos; Marta Mattoso; Alberto MR Dávila,Abstract A key focus in 21st century integrative biology and drug discovery for neglectedtropical and other diseases has been the use of BLAST-based computational methods foridentification of orthologous groups in pathogenic organisms to discern orthologs; with aview to evaluate similarities and differences among species; and thus allow the transfer ofannotation from known/curated proteins to new/non-annotated ones. We used here a profile-based sensitive methodology to identify distant homologs; coupled to the NCBI's COG(Unicellular orthologs) and KOG (Eukaryote orthologs); permitting us to perform comparativegenomics analyses on five protozoan genomes. OrthoSearch was used in five protozoanproteomes showing that 3901 and 7473 orthologs can be identified by comparison withCOG and KOG proteomes; respectively. The core protozoa proteome inferred was 418 …,Omics: a journal of integrative biology,2014,8
SciLightning: a cloud provenance-based event notification for parallel workflows,Julliano Trindade Pintas; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract Conducting scientific experiments modeled as workflows is a challenging task dueto the complex management of several (often inter-related) computer-based simulations.Many of these scientific workflows are compute intensive and demand High PerformanceComputing environments to run; such as virtual parallel machines in a cloud computingenvironment. These workflows commonly present long-term" black-box" executions (ieseveral days or weeks); thus making it very difficult for scientists to monitor its executioncourse. We present a workflow event notification mechanism based on runtime monitoring ofprovenance data produced by parallel scientific workflow systems in clouds. Thismechanism queries provenance data generated at runtime for identifying preconfiguredevents and notifying scientists using technologies such as Android devices and message …,International Conference on Service-Oriented Computing,2013,8
Managing provenance in scientific workflows with provmanager,Anderson Marinho; Leonardo Murta; Cláudia Werner; Vanessa Braganholo; SMS Cruz; Eduardo Ogasawara; Marta Mattoso,Abstract. Running scientific workflows in distributed environments is motivating the definitionof provenance gathering approaches that are loosely coupled to the workflow systems. Wehave proposed a provenance gathering strategy that is independent from workflow systemtechnology. This strategy has evolved into a provenance management system namedProvManager. The main principle is that each workflow activity should collect its ownprovenance data and publish them in a repository which scientists can access to make theirqueries. In this paper we show how provenance is captured along distributedheterogeneous systems. Two main strategies are used to capture provenance: using Prologpredicates to register provenance; and using an API for the communication between thewrapped activity and the ProvManager.,International Workshop on Challenges in e-Science-SBAC,2010,8
Um mediador para o processamento de consultas sobre bases xml distribuídas,Guilherme Figueiredo; Vanessa Braganholo; Marta Mattoso,Abstract. This paper describes a tool that implements an architecture to the query processingof XQueries over distributed and fragmented XML databases. This architecture; based on aMediator with Adapters attached to the remote databases; implements a query processingmethodology where the Mediator publishes a global XML view of the distributed data whichcan be queried transparently. Resumo. Este artigo descreve uma ferramenta queimplementa uma arquitetura para o processamento de consultas XQuery sobre bases dedados XML distribuídas e fragmentadas. Esta arquitetura; baseada em um Mediador comAdaptadores acoplados aos bancos de dados remotos; implementa uma metodologia deprocessamento de consultas distribuídas; onde o Mediador fornece uma visão XML globaldos dados distribuídos que pode ser consultada de forma transparente.,Proceedings of the Demos Session of the Brazilian Symposium on Databases. João Pessoa; Brazil,2007,8
XVerter: querying XML data with OR-DBMS,Humberto Vieira; Gabriela Ruberg; Marta Mattoso,Abstract Storage techniques and queries over XML databases are being widely studied.Most works store XML documents in traditional DBMSs in order to take advantage of a wellestablished technology and also to store both structured data and XML data within a singlesystem. This work proposes a translation mechanism to execute queries expressed onXQuery on top of XML documents that are stored in an object DBMS using the DOMimplementation in disk. Rules for automatic translation from XQuery to SQL3 are presented;where an object-based representation of XML documents is exploited.,Proceedings of the 5th ACM international workshop on Web information and data management,2003,8
Estimating costs of path expression evaluation in distributed object databases,Gabriela Ruberg; Fernanda Baião; Marta Mattoso,Abstract Efficient evaluation of path expressions in distributed object databases involveschoosing among several query processing strategies; due to the rich semantics involved inobject-based data models and to the complexity added by the distribution. This workpresents a new cost model for object-based query processors and addresses relevantissues; which are ignored or relaxed in other works in the literature; such as the selectivity ofthe path expression; the sharing degree of the referenced objects; the partial participation ofthe collections in the relationships; and the distribution of the database objects across thenodes of a network. These issues allowed us to present more realistic estimates for thequery optimizer. Our cost model has been validated against experimental results obtainedwith an object DBMS prototype running in a distributed architecture; using the OO7 …,International Conference on Database and Expert Systems Applications,2002,8
The use of mediators for component retrieval in a reuse environment,R Braga; M Mattoso; C Werner,*,Workshop on Component-Based Software Engineering Process; Technology of Object-Oriented Languages and Systems Conference; Santa Bárbara,1999,8
Uncertainty quantification in numerical simulation of particle-laden flows,Gabriel M Guerra; Souleymane Zio; Jose J Camata; Jonas Dias; Renato N Elias; Marta Mattoso; Paulo LB Paraizo; Alvaro LGA Coutinho; Fernando A Rochinha,Abstract Numerical models can help to push forward the knowledge about complex dynamicphysical systems. Modern approaches employ detailed mathematical models; taking intoconsideration inherent uncertainties on input parameters (phenomenological parameters orboundary and initial conditions; among others). Particle-laden flows are complex physicalsystems found in nature; generated due to the (possible small) spatial variation on the fluiddensity promoted by the carried particles. They are one of the main mechanisms responsiblefor the deposition of sediments on the seabed. A detailed understanding of particle-ladenflows; often referred to as turbidity currents; helps geologists to understand the mechanismsthat give rise to reservoirs; strategic in oil exploration. Uncertainty quantification (UQ)provides a rational framework to assist in this task; by combining sophisticated …,Computational Geosciences,2016,7
STINGRAY: system for integrated genomic resources and analysis,Glauber Wagner; Rodrigo Jardim; Diogo A Tschoeke; Daniel R Loureiro; Kary ACS Ocaña; Antonio CB Ribeiro; Vanessa E Emmel; Christian M Probst; André N Pitaluga; Edmundo C Grisard; Maria C Cavalcanti; Maria LM Campos; Marta Mattoso; Alberto MR Dávila,The STINGRAY system has been conceived to ease the tasks of integrating; analyzing;annotating and presenting genomic and expression data from Sanger and Next GenerationSequencing (NGS) platforms. STINGRAY includes:(a) a complete and integrated workflow(more than 20 bioinformatics tools) ranging from functional annotation to phylogeny;(b) aMySQL database schema; suitable for data integration and user access control; and (c) auser-friendly graphical web-based interface that makes the system intuitive; facilitating thetasks of data analysis and annotation. STINGRAY showed to be an easy to use andcomplete system for analyzing sequencing data. While both Sanger and NGS platforms aresupported; the system could be faster using Sanger data; since the large NGS datasetscould potentially slow down the MySQL database usage. STINGRAY is available at http …,BMC research notes,2014,7
Debugging Scientific Workflows with Provenance: Achievements and Lessons Learned.,Daniel de Oliveira; Flavio Costa; Vítor Silva Sousa; Kary ACS Ocaña; Marta Mattoso,Abstract. 1Scientific Workflow Management Systems manage experiments in large-scaleand deliver provenance data. Provenance data represents the workflow execution behavior;allowing for tracing the data-flow generation. When provenance is extended withperformance execution data; it becomes an important asset to identify and analyze errorsthat occurred during the workflow execution (ie debugging). Debugging is essential forworkflows that execute in parallel in large-scale distributed environments since theincidence of errors in this type of execution is high and difficult to track. By debugging atruntime; scientists can identify errors and take the necessary actions; while the workflow isstill running. We present provenance based debugging; in real use cases; running inparallel; with virtual machines in clouds. In these experiences scientists use provenance …,SBBD,2014,7
Provenance traces from chiron parallel workflow engine,Felipe Horta; Vítor Silva; Flavio Costa; Daniel de Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Abstract Scientific workflows are commonly used to model and execute large-scale scientificexperiments. They represent key resources for scientists and are managed by ScientificWorkflow Management Systems (SWfMS). The different languages used by SWfMS mayimpact in the way the workflow engine executes the workflow; sometimes limitingoptimization opportunities. To tackle this issue; we recently proposed a scientific workflowalgebra [1]. This algebra is inspired by database relational algebra and it enables automaticoptimization of scientific workflows to be executed in parallel in high performance computing(HPC) environments. This way; the experiments presented in this paper were executed inChiron; a parallel scientific workflow engine implemented to support the scientific workflowalgebra. Before executing the workflow; Chiron stores the prospective provenance [2] of …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,7
Using Provenance to Visualize Data from Large-Scale Experiments,Felipe Horta; Jonas Dias; Kary ACS Ocana; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Large-scale scientific computations are often organized as a composition of manycomputational tasks linked through data flow. The data that flows along this manytaskcomputing often moves from a desktop to a high-performance environment and to avisualization environment. Keeping track of this data flow is a challenge to provenancesupport in high-performance Scientific Workflow Management Systems. After the completionof a computational scientific experiment; a scientist has to manually select and analyze itsstaged-out data; for instance; by checking inputs and outputs along computational tasks thatwere part of the experiment. In this paper; we present a provenance management systemthat describes the production and consumption relationships between data artifacts; such asfiles; and the computational tasks that compose the experiment. We propose a query …,High Performance Computing; Networking; Storage and Analysis (SCC); 2012 SC Companion:,2012,7
Handling failures in parallel scientific workflows using clouds,Flavio Costa; Daniel de Oliveira; Kary Ocana; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Failures are common in High Performance Computing (HPC) environments and cansignificantly impact the performance of scientific workflows executing on top of these largescale computing environments. Computing clouds are being used as promising HPCenvironments. Although clouds offer several advantages such as elasticity and availability;failures are very frequent in this type of environment; where virtualization; instabilities andproviders' actions directly impact on workflow execution. In this way; activity failures arealmost inevitable in clouds where virtual machine failures are a reality rather than apossibility. In this paper we present a set of failure handling heuristics based on cloudcharacteristics; which are implemented within SciMultaneous; a service-orientedarchitecture that manages re-executions of failed scientific workflow activities using …,High Performance Computing; Networking; Storage and Analysis (SCC); 2012 SC Companion:,2012,7
Exploring provenance in high performance scientific computing,Luiz Manoel Rocha Gadelha Junior; Michael Wilde; Marta Mattoso; Ian Foster,Abstract Large-scale scientific computations are often organized as a composition of manycomputational tasks linked through data flow. After the completion of a computationalscientific experiment; a scientist has to analyze its outcome; for instance; by checking inputsand outputs of computational tasks that are part of the experiment. This analysis can beautomated using provenance management systems that describe; for instance; theproduction and consumption relationships between data artifacts; such as files; and thecomputational tasks that compose the scientific application. In this article; we explore therelationship between high performance computing and provenance management systems;observing that storing provenance as structured data enriched with information about theruntime behavior of computational tasks in high performance computing environments …,Proceedings of the first annual workshop on High performance computing meets databases,2011,7
SimiFlow: Uma Arquitetura para Agrupamento de Workflows por Similaridade,Vítor Silva; Fernando Chirigati; Kely Maia; Eduardo Ogasawara; D Oliveira; Vanessa Braganholo; Leonardo Murta; Marta Mattoso,Abstract. Scientists have been using scientific workflows to support scientific experiments.However; the Scientific Workflow Management Systems present some limitation on workflowcomposition. Experiment Lines; which are a novel approach to deal with these limitations;allow the representation and systematic composition of the experiment. Nevertheless; thereare many scientific workflows already modeled that can leverage the construction ofexperiment lines via the identification of scientific workflows clusters that are createdaccording to similarity. This paper proposes SimiFlow; an architecture for similarity-basedcomparison and clustering to build experiment lines following a bottom-up approach.Resumo. Workflows científicos vêm sendo utilizados no apoio aos experimentos científicos.Workflows em um mesmo experimento normalmente apresentam pequenas variações …,IV e-Science,2010,7
A methodology for query processing over distributed XML databases,Guilherme Figueiredo; Vanessa Braganholo; Marta Mattoso,Abstract. The constant increase in the volume of data stored as native XML documentsmakes fragmentation techniques an important alternative to the performance issues in queryprocessing over these data. Fragmented databases are feasible only if there is a transparentway to query the distributed database; without the need of knowing the fragmentation detailsand where each fragment is located. This paper presents our methodology for XQuery queryprocessing over distributed XML databases; which consists on the steps of querydecomposition; including the query's TLC algebra representation; data localization; globaloptimization; global query execution and final result assembly. This methodology can beused in an XML database that allows fragmentation and also in a system that publishes anintegrated view of semi-autonomous and homogeneous XML databases. We propose an …,*,2007,7
An environment to define and execute in-silico workflows using web services,Rafael Targino; Maria Claudia Cavalcanti; Marta Mattoso,Abstract Scientific workflows represent an attractive alternative to describe bioinformaticsexperiments. They give an adequate support to the “Execution and Analysis” cycle; relevantto the process of knowledge discovery. Workflows can create an independent andinteroperable environment between the scientific applications and databases; whencombined with the Web Services technology. Despite the successful use of thesetechnologies in the business scenario; its use in bioinformatics is still incipient. This workpresents an integrated environment that aims at the definition and execution of in silicoexperiments through scientific workflows using Web services. A real bioinformaticsexperiment was implemented in this environment.,International Workshop on Data Integration in the Life Sciences,2005,7
Alocação de Dados em Bancos de Dados Distribuídos.,Matheus Wildemberg; Melise M Veiga de Paula; Fernanda Araujo Baião; Marta Mattoso,Abstract The problem of data allocation directly impacts on the execution cost of theapplication over a distributed database in environments that work with data distribution andreplication; such as web servers; heterogeneous database integration systems anddistributed databases. Allocation algorithms are typically used to find a data distributionamong the sites of the network such as to minimize the execution cost of the application. Inthis work; a heuristic algorithm is proposed for fragment allocation in distributed databasesystems. The proposed algorithm is based on a heuristic algorithm presented in theliterature; and its goal is to find out an allocation schema with a minimal execution cost;maintaining the complexity of the original algorithm. Through simulations performed on topof the TPC-C benchmark; it was possible to identify scenarios where the proposed …,SBBD,2003,7
Software components retrieval through mediators and web search,Robson P de Souza; Marcelo N Costa; Regina MM Braga; Marta Mattoso; Cláudia ML Werner,Abstract Component Based Development (CBD) aims at con- structing software through theintegration; using inter- faces and contracts; between pre-existing components. The main goalof this work is to provide access to component that can be published at the Web; retrieved; andreused in all phases of an application development within a given domain. We present an architecturefor soft- ware components reuse by using a mediation layer that integrates the semantics of Webcomponents with previ- ously registered components from a virtual library of com- ponents. Inour architecture; components are described through XML documents and published by localreposito- ries or remote servers. The innovative aspect of our pro- posal is the combination ofmediators and software agents for reusable component retrieval within a Domain Engi- neeringcontext. Mediators can represent application do- main as well as integrate the description …,Journal of the Brazilian Computer Society,2002,7
Raw data queries during data-intensive parallel workflow execution,Vítor Silva; José Leite; José J Camata; Daniel De Oliveira; Alvaro LGA Coutinho; Patrick Valduriez; Marta Mattoso,Abstract Computer simulations consume and produce huge amounts of raw data filespresented in different formats; eg; HDF5 in computational fluid dynamics simulations. Usersoften need to analyze domain-specific data based on related data elements from multiplefiles during the execution of computer simulations. In a raw data analysis; one shouldidentify regions of interest in the data space and retrieve the content of specific related rawdata files. Existing solutions; such as FastBit and RAW; are limited to a single raw data fileanalysis and can only be used after the execution of computer simulations. ScientificWorkflow Management Systems (SWMS) can manage the dataflow of computer simulationsand register related raw data files at a provenance database. This paper aims to combinethe advantages of a dataflow-aware SWMS and the raw data file analysis techniques to …,Future Generation Computer Systems,2017,6
Scientific workflow scheduling with provenance support in multisite cloud,Ji Liu; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract Recently; some Scientific Workflow Management Systems (SWfMSs) withprovenance support (eg Chiron) have been deployed in the cloud. However; they typicallyuse a single cloud site. In this paper; we consider a multisite cloud; where the data andcomputing resources are distributed at different sites (possibly in different regions). Basedon a multisite architecture of SWfMS; ie multisite Chiron; we propose a multisite taskscheduling algorithm that considers the time to generate provenance data. We performed anextensive experimental evaluation of our algorithm using Microsoft Azure multisite cloud andtwo real-life scientific workflows (Buzz and Montage). The results show that our schedulingalgorithm is up to 49; 6% better than baseline algorithms in terms of total execution time.,International Conference on Vector and Parallel Processing,2016,6
Linked open data publication strategies: Application in networking performance measurement data,Renan Souza; Les Cottrell; Bebo White; Maria L Campos; Marta Mattoso,Abstract Most of the data published on the web is unstructured or does not follow a standard.This makes it harder to retrieve and interchange information between different data sources.This work uses Linked Open Data (LOD) technologies and applies them in a scenario thatdeals with a large amount of computer network measurement data. The goal is to make thedata more structured; hence easier to be retrieved; analyzed; and more interoperable. Wediscuss the challenges of processing large amount of data to: transform it into a standardformat (RDF); link it to other data sources; and analyze and visualize the transformed data.Moreover; an ontology that aims to minimize the number of triples is proposed and adiscussion of how ontologies may impact performance is presented. In addition; both theadvantages of having the data in RDF format and the obstacles that the LOD community …,*,2014,6
SciCumulus 2.0: Um Sistema de Gerência de Workflows Científicos para Nuvens Orientado a Fluxo de Dados,Vítor Silva; D Oliveira; Marta Mattoso,Resumo. Ao contrário dos workflows de negócio; os workflows científicos são centrados nogrande fluxo de transformação de dados. Entretanto; as abordagens dos sistemas deworkflows em larga escala ainda são voltadas à gerência da execução paralela de tarefasao invés de gerenciar as relações entre os dados ao longo fluxo de geração dos dados doworkflow. Este artigo apresenta a execução do SciCumulus 2.0 e sua nova camada desubmissão de execução paralela que oferece diferentes níveis para a modelagem deworkflows; assim como a configuração do ambiente de paralelismo e consultas aos dadosde domínio e de proveniência em tempo de execução.,Sessão de Demos do XXIX Simpósio Brasileiro de Banco de Dados,2014,6
Tabor Network,Daniel de Oliveira; Fernanda Araújo Baião; Marta Mattoso,The most important advantage behind the concept of cloud computing for scientificexperiments is that the average scientist is capable of accessing many types of resourceswithout having to buy or configure the whole infrastructure.This is a fundamental need forscientists and scientific applications. It is preferable that scientists be isolated from thecomplexity of configuring and instantiating the whole environment; focusing only on thedevelopment of the in silico experiment.,People,2011,6
SciMulator: Um Ambiente de Simulação de Workflows Científicos em Redes P2P,Jonas Dias; Carla Rodrigues; Eduardo Ogasawara; Daniel De Oliveira; Vanessa Braganholo; Esther Pacitti; Marta Mattoso,The growth of large-scale scientific experiments motivates the search for computingenvironments that support the parallelization of computing activities; particularly those thatcomplies to the Many Task Computing (MTC) paradigm. Peer-to-Peer (P2P) environmentscan meet this demand due to the easy access and distributed control. However; building areal P2P infrastructure to evaluate this solution is very costly. Within this context; we presentthe simulator SciMulator developed to evaluate P2P architectures. We present the modelingof the simulator and an initial assessment of its performance when using the SciMulearchitecture for submission of scientific workflows activities on P2P networks.,VI Workshop de Redes Dinâmicas e Sistemas Peer-to-Peer 2010,2010,6
An adaptive approach for workflow activity execution in clouds,Daniel de Oliveira; Eduardo Ogasawara; Fernanda Baião; Marta Mattoso,Abstract. Many scientific workflows in various domains of knowledge are data-intensive. Thistype of workflow requires high performance environments to achieve the results at anacceptable response time (crucial for many critical experiments). Cloud environmentsrepresent an opportunity to provide the necessary high performance infra-structure to runthese experiments. However; this opportunity also raises many challenges. It is hard todecide a priori which resources to use and how long they are needed since the resourcesare elastic and instantiated on demand. This paper proposes an adaptive mechanism thatallows workflow activities to execute according to the environmental conditions in a cloudenvironment (eg; workload and resource availability) with respect to given performanceobjectives such as total execution time. We have evaluated the adaptive mechanism in a …,*,2010,6
A provenance-based approach to resource discovery in distributed molecular dynamics workflows,Sérgio Manuel Serra Da Cruz; Patricia M Barros; Paulo M Bisch; Maria Luiza M Campos; Marta Mattoso,Abstract The major challenge of in silico experiments consists in exploiting the amount ofdata generated by scientific apparatus. Scientific data; programs and workflows areresources to be exchanged among scientists but difficult to be efficiently used due to theirheterogeneous and distributed nature. Provenance metadata can ease the discovery ofthese resources. However; keeping track of the execution of experiments and capturingprovenance among distributed resources are not simple tasks. Thus; discovering scientificresources in distributed environments is still a challenge. This work presents an architectureto help the execution of scientific experiments in distributed environments. Additionally; itcaptures and stores provenance of the workflow execution in a repository. To validate theproposed architecture; a bioinformatics workflow has been defined for the execution of a …,International Workshop on Resource Discovery,2009,6
Adaptive hybrid partitioning for OLAP query processing in a database cluster,Camille Furtado; Alexandre AB Lima; Esther Pacitti; Patrick Valduriez; Marta Mattoso,We consider the use of a database cluster for high-performance support of Online AnalyticalProcessing (OLAP) applications. OLAP intra-query parallelism can be obtained bypartitioning the database tables across cluster nodes. We propose to combine physical andvirtual partitioning into a partitioning scheme called Adaptive Hybrid Partitioning (AHP). AHPrequires less disk space while allowing for load balancing. We developed a prototype forOLAP parallel query processing in database clusters using AHP. Our experiments on a 32-node database cluster using the TPC-H benchmark demonstrate linear and super-linearspeedup. Thus; AHP can reduce significantly the execution time of typical OLAP queries.,International journal of high performance computing and networking,2008,6
Semi-Supervised Clustering of XML Documents: Getting the Most from Structural Information,Eduardo Bezerra; Marta Mattoso; Geraldo Xexeo,As document providers can express more contextualized and complex information; semi-structured documents are becoming a major source of information in many areas; eg; indigital libraries; e-commerce or Web applications. A particular characteristic of suchdocument collections is the existence of some structure or metadata along with the data. Inthis scenario; clustering methods that can take advantage of such structural information tobetter organize such collections are highly relevant. Semi-structured documents pose newchallenges to document clustering methods; however; since it is not clear how this structuralinformation can be used to improve the quality of the generated clustering models. On theother hand; recently there has a growing interest in the semi-supervised clustering task; inwhich a little amount of prior knowledge is provided to guide the algorithm to a better …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,6
Handling dissimilarities of autonomous and equivalent web services,Valdino Azevedo; Marta Mattoso; Paulo Pires,Abstract: The web services technology provides an essential building block for dynamic e-business; facilitating program-to-program interaction and the composition of new services.However; the need to specify service ports (or interfaces) in a service composition mayconstraint the usage of such technology. Given the existence of several semanticallyequivalent services on the Internet; we need a more flexible and loosely coupled way toinclude services in compositions. Instead of including specific services; a compositionshould deal with service classes; which group services with the same semantic functionality.During runtime; one or more services inside a service class can be scheduled to run;offering a dynamic mechanism for service execution. However; aggregating autonomousservices that provide the same semantic functionality involves handling their …,Proc. of Caise-WES,2003,6
How Much Domain Data Should Be in Provenance Databases?,Daniel de Oliveira; Vítor Silva; Marta Mattoso,ABSTRACT Provenance databases are an important asset in data analytics of large-scalescientific data. The data derivation path allows for identifying parameters; files and domaindata values of interest. In scientific workflows; provenance data is automatically captured byworkflow systems. However; the power of provenance data analyses depends on theexpressiveness of domain-specific data along the provenance traces. While much has beendone through the W3C PROV initiative and its PROV-DM to represent generic provenancedata; representing domain-specific data in provenance traces has received little attention;yet it accounts for a large number of provenance analytical queries. Such queries are basedon selections on data values from input/output artifacts along workflow activities. There areseveral problems in modeling and capturing values from domain-specific attributes; some …,7th Usenix Workshop on Theory and Practice of Provenance (TaPP'15),2015,5
A survey on xml fragmentation,Vanessa Braganholo; Marta Mattoso,Abstract Efficient document processing is a must when large volumes of XML data areinvolved. In such critical scenarios; a well-known solution to this problem is to distribute(map) the data among several processing nodes; and then distribute the processingaccordingly; taking advantage of parallelism. This is the approach taken by distributeddatabases and MapReduce environments. Fragmentation techniques play an important rolein these scenarios. They provide a way to" cut" the database into pieces and distribute thepieces over a network. This way; queries can also be" cut" into sub-queries that run inparallel; thus achieving better performance when compared to the centralized environment.However; there is no consensus in the database community as to what an XML fragment is.In fact; several approaches in literature present definitions of XML fragments. In addition …,ACM SIGMOD Record,2014,5
Towards an adaptive and distributed architecture for managing workflow provenance data,Flavio Costa; Daniel de Oliveira; Marta Mattoso,Workflow provenance data represents the workflow execution behavior; allowing for tracingthe generation of the scientific data-flow. Provenance is an important asset to analyze data;identify and handle errors that occurred during the workflow execution through runtimemonitoring. The workflow execution engine can also use provenance data to set the initialamount of resources and plan adaptive task scheduling. However; efficiently managingprovenance data from distributed workflow execution has several challenges. As the size ofworkflows increases (in terms of number of activity executions or volume of data to process);the amount of provenance data to be managed also grows; especially in fine grain. Thus;centralized approaches become unviable. In this work we propose an architecture thatcombines distributed workflow management techniques with distributed provenance data …,e-Science (e-Science); 2014 IEEE 10th International Conference on,2014,5
Uma Comparação entre os Modelos de Proveniência OPM e PROV,Bárbara Bivar; Lucas Santos; Troy C Kohwalter; Anderson Marinho; Marta Mattoso; Vanessa Braganholo,Abstract. The large scale production of digital scientific objects involves a variety ofprocesses. The analysis of these processes and its results demands a rigorousmanagement of the changes that occurs throughout its execution. The OPM model hasemerged as a reference to represent the provenance of various systems such as databases;web and scientific workflows and was widely adopted. However; an initiative from W3C todefine a provenance model named PROV has recently emerged and so many systemsbased on OPM will probably migrate to PROV. The contribution of this work is a comparativestudy that aims to ease the migration of the current systems based on OPM to the PROVmodel. Resumo. A produção em grande escala de objetos digitais científicos envolve umadiversidade de processos. A análise desses processos e seus resultados exige um …,Proocedings of BRESCi,2013,5
WGL-a workflow generator language and utility,Luiz Meyer; Marta Mattoso; Mike Wilde; Ian Foster,*,University of Chicago; Tech. Rep,2013,5
Prov-Vis: large-scale scientific data visualization using provenance,Felipe Horta; Jonas Dias; Renato Elias; Daniel Oliveira; ALGA Coutinho; Marta Mattoso,Abstract—Large-scale scientific computing often rely on in-tensive tasks chained through aworkflow. Scientists need to check the status of the execution at particular points; to discoverif anything odd has happened and take actions. To achieve that; they need to track partialresult files; which is usually complex and laborious. When using a scientific workflow system;provenance data keeps track of every step of the execution. If traversing provenance data isallowed at runtime; it is easier to monitor and analyze partial results. However; visualizationof partial results is necessary to be done in sync to the workflow provenance. Prov-Vis is ascientific data visualization tool for large-scale workflows that is based on runtimeprovenance queries to organize and aggregate data for visualization. Prov-Vis helpsscientists to follow the steps of the running workflow and visualize the produced partial …,Proceedings of the International Conference on High Performance Computing; Networking; Storage and Analysis,2013,5
Challenges in Managing Implicit and Abstract Provenance Data: Experiences with ProvManager.,Anderson Marinho; Marta Mattoso; Claudia Werner; Vanessa Braganholo; Leonardo Murta,Abstract Running scientific workflows in distributed and heterogeneous environments hasbeen motivating the definition of provenance gathering approaches that are loosely coupledto workflow management systems. We have developed a provenance management systemnamed ProvManager to manage provenance data in distributed and heterogeneousenvironments independent of a specific Scientific Workflow Management System. Theexperience of using ProvManager in real workflow applications has shown manyprovenance management issues that are not addressed in current related work. We havefaced challenges such as the necessity of dealing with implicit provenance data and the lackof higher provenance abstraction levels. This paper discusses and points to directionstowards these challenges; contextualizing them according to our experience in …,TaPP,2011,5
Especificação Formal e Verificação de Workflows Científicos,E Silva; Eduardo Ogasawara; D Oliveira; M Benevides; M Mattoso,Abstract. Workflows are used in several domains for scientific purposes. In the last yearsthese workflows are becoming more complex and scientists need methods to verify itscorrectness. Most of the available systems assume that a workflow is correct if it respectscontrol and dependencies specified by the scientist. In addition; many scientific workflowsmust be completely reliable; that is why they must be correctly specified. This paperproposes an approach that supports workflow verification based on process algebraspecifications (CCS) and model checking tools. We have evaluated our approach using theGExpLine workflow tool. Resumo. Workflows são utilizados em diversos domínios compropósitos científicos. Nos últimos anos estes workflows tornaram-se mais complexos e oscientistas necessitam de métodos para verificar a sua correção. A maioria dos sistemas …,IV e-Science,2010,5
Uma Estratégia de Versionamento de Workflows Científicos em Granularidade Fina,Bruno Costa; Eduardo Ogasawara; Leonardo Murta; Marta Mattoso,Abstract. The use of scientific workflow management systems (SWfMS) has become a realityin various scientific experiments. They support the controlled enactment of sequences ofscientific activities; called scientific workflows; responsible for shaping the flow of datathrough programs. However; workflows are evolving entities and SWfMS should providesupport for controlling this evolution. This paper presents a strategy for fine grainedversioning of scientific workflows; which is based on solid configuration management (CM)principles such as separation of versioning and product spaces and bubble-up method; alsoused by popular CM systems such as Subversion. Resumo. O uso de sistemas degerenciamento de workflows científicos (SGWfC) se tornou uma realidade em diversosexperimentos científicos. Eles apóiam a execução controlada de seqüências de …,III e-Science,2009,5
Linhas de experimento: Reutilização e gerência de configuração em workflows científicos,Eduardo Ogasawara; Leonardo Murta; Cláudia Werner; Marta Mattoso,Abstract. Over the last years; scientists have been using scientific workflows to buildcomputer simulations to support the development of new theories. Nevertheless; due to theirincreasing usage; the scientific workflows conception and usage activities can no longer beperformed in an ad-hoc manner. These activities can probably be improved if we applysome existing software engineering techniques; adapted from the software context to thescientific workflow context. Thus; we introduce the concept of experiment lines; whichapplies two traditional software engineering techniques to support the conception andusage of scientific workflows: reuse and configuration management. Resumo. Nos últimosanos; cientistas vêm usando workflows científicos para apoiar o desenvolvimento de novasteorias via simulações computacionais. Entretanto; devido aos seus crescentes usos; as …,2 Workshop E-Science,2008,5
Digging database statistics and costs parameters for distributed query processing,Nicolaas Ruberg; Gabriela Ruberg; Marta Mattoso,Abstract Cost parameters and database statistics are the basis of query optimizationtechniques. However; in distributed and heterogeneous database systems; acquiring andtreating information in order to help the optimization process are often tasks of a globalquery processor; which adapts its functionalities to a specific system architecture. Moreover;this acquisition process involves a large number of parameters and requires customizedmethods to retrieve data from specific sources. DIG (Distributed Information Gatherer) is aprovider of data statistics and query costs that; through an independent and flexible service;aims to support global query optimization processing in distributed and heterogeneousdatabase systems over autonomous data sources. We have developed a DIG prototype andexperimented it with specific wrappers for a query middleware on both semi-structured …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2003,5
OdysseyShare: Desenvolvimento colaborativo de componentes,Cláudia Werner; Marcos RS Borges; Marta Mattoso; Regina Braga; Fernanda Campos; Marco Mangan; Vaninha Vieira,Abstract: Component-based software development is a collaborative intensive activity.Knowledge necessary to identify; specify; develop; and reuse a component is usuallydispersed among stakeholders. This means that stakeholders need to collaborate to sharetheir knowledge assets. This paper presents the OdysseyShare Project; whose aim is toexplore the collaborative aspects of software component development. Resumo: Odesenvolvimento de componentes de software é uma atividade colaborativa. Oconhecimento necessário para a identificação; especificação; construção e reutilização docomponente encontra-se distribuído entre os integrantes da equipe de desenvolvimento.Por esse motivo; os integrantes precisam colaborar para compartilhar seu conhecimento.Este artigo apresenta o projeto OdysseyShare; cujo objetivo é explorar aspectos de …,IX Simpósio Brasileiro de Sistemas Multimídia e Web,2003,5
Gerência de Documentos XML no GOA,Marta Mattoso; Maria Cláudia Cavalcanti; Robson Pinheiro; Humberto Vieira; Leonardo Guerreiro Azevedo; Carlete Ferreira Marques; Rodrigo Salvador Monteiro; Fátima Cristina Gonçalves; Cláudia Werner,Abstract The emerging use of XML to describe and represent software components hasmotivated the development of XML documents management in the GOA object server. In thiswork; we present the GOA XML Client API; which was developed to store and manipulateXML documents in the GOA server.,SBES; Caderno de Ferramentas,2002,5
Parallel execution of workflows driven by a distributed database management system,Renan Souza; Vítor Silva; Daniel Oliveira; Patrick Valduriez; Alexandre AB Lima; Marta Mattoso,ABSTRACT Scientific Workflow Management Systems (SWfMS) that execute large-scalesimulations need to manage many tasks computing in high performance environments. Withthe scale of tasks and processing cores to be managed; SWfMS require efficient distributeddata structures to manage data related to scheduling; data movement and provenance datagathering. Although related systems store these data in multiple specific files; some existingapproaches store them using a Database Management System (DBMS); which providespowerful analytical capabilities; including execution monitoring; anticipated result analyses;and user steering; when available at runtime. Despite these advantages; approaches relyingon a centralized DBMS introduce a point of contention; jeopardizing performance in large-scale executions. In this paper; we propose an architecture relying on a distributed DBMS …,ACM/IEEE Conference on Supercomputing; Poster,2015,4
BaMBa: towards the integrated management of Brazilian marine environmental data,Pedro Milet Meirelles; Luiz MR Gadelha; Ronaldo Bastos Francini-Filho; Rodrigo Leao de Moura; Gilberto Menezes Amado-Filho; Alex Cardoso Bastos; Rodolfo Pinheiro da Rocha Paranhos; Carlos Eduardo Rezende; Jean Swings; Eduardo Siegle; Nils Edvin Asp Neto; Sigrid Neumann Leitão; Ricardo Coutinho; Marta Mattoso; Paulo S Salomon; Rogério AB Valle; Renato Crespo Pereira; Ricardo Henrique Kruger; Cristiane Thompson; Fabiano L Thompson,Abstract A new open access database; Brazilian Marine Biodiversity (BaMBa)(https://marinebiodiversity. lncc. br); was developed in order to maintain large datasets from theBrazilian marine environment. Essentially; any environmental information can be added toBaMBa. Certified datasets obtained from integrated holistic studies; comprising physical–chemical parameters;-omics; microbiology; benthic and fish surveys can be deposited in thenew database; enabling scientific; industrial and governmental policies and actions to beundertaken on marine resources. There is a significant number of databases; howeverBaMBa is the only integrated database resource both supported by a government initiativeand exclusive for marine data. BaMBa is linked to the Information System on BrazilianBiodiversity (SiBBr; http://www. sibbr. gov. br/) and will offer opportunities for improved …,Database,2015,4
Exploratory analysis of raw data files through dataflows,Vitor Silva; Daniel de Oliveira; Marta Mattoso,Scientific applications generate raw data files in very large scale. Most of these files follow astandard format established by the domain area application; like HDF5; Net CDF and FITS.These formats are supported by a variety of programming languages; libraries andprograms. Since they are in large scale; analyzing these files require writing a specificprogram. Generic data analysis systems like database management systems (DBMS) arenot suited because of data loading and data transformation in large scale. Recently therehave been several proposals for indexing and querying raw data files without the overheadof using a DBMS; such as noDB; RAW and Fast Bit. Their goal is to offer query support to theraw data file after a scientific program has generated it. However; these solutions arefocused on the analysis of one single large file. When a large number of files are all …,Computer Architecture and High Performance Computing Workshop (SBAC-PADW); 2014 International Symposium on,2014,4
Experiences in using provenance to optimize the parallel execution of scientific workflows steered by users,Marta Mattoso; Jonas Dias; Flavio Costa; Daniel de Oliveira; Eduardo Ogasawara,Abstract. The main advantages from using Scientific Workflow Management Systems tomanage a large-scale scientific experiment are their automatic parallel execution and theimprovement of result analysis through provenance data. Provenance data becomesespecially useful for scientists when it is clearly associated to their domain data. In ourexperience; provenance data also reveals important optimizations opportunities in parallelexecution and allows for user steering of workflows at run-time. The algebraic parallelexecution engine is fine tuned by provenance statistics and users have exploredprovenance through steering support to visualize partial results from computational fluiddynamics simulations; to improve iterative uncertainty quantification applications ingeophysics and to evaluate parameter setting and algorithms in several bioinformatics …,Workshop of Provenance Analytics,2014,4
Performance analysis of data filtering in scientific workflows,João Gonçalves; Daniel de Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,*,Journal of Information and Data Management,2013,4
SGProv: Mecanismo de Sumarização para Múltiplos Grafos de Proveniência.,Daniele El-Jaick; Marta Mattoso; Alexandre AB Lima,Resumo. Os Sistemas de Gerência de Workflows Científicos (SGWfC) têm o objetivo deautomatizar a construção e execução de experimentos científicos. Várias execuções deworkflows são necessárias para realizar um experimento. O rastro de proveniência;coletado pelos SGWfC durante estas execuções; é importante para que os cientistaspossam compreender; reproduzir e analisar seus experimentos. Um rastro de proveniênciacontém o histórico da derivação dos dados; assim; pode ser representado sob a forma deum grafo direcionado e acíclico. Cada execução de um workflow gera um grafo deproveniência. Após várias execuções; por exemplo; explorando parâmetros; inúmerosgrafos são gerados. A base de proveniência; portanto; requer um espaço dearmazenamento considerável e consultá-la envolve a manipulação de um grande …,SBBD (Short Papers),2013,4
A Foundational Ontology to Support Scientific Experiments.,Sergio Manuel Serra da Cruz; Maria Luiza Machado Campos; Marta Mattoso,Abstract. Provenance is a term used to describe the history; lineage or origins of a piece ofdata. In scientific experiments that are computationally intensive the data resources areproduced in large-scale. Thus; as more scientific data are produced the importance oftracking and sharing its metadata grows. Therefore; it is desirable to make it easy to access;share; reuse; integrate and reason. To address these requirements ontologies can be of useto encode expectations and agreements concerning provenance metadata reuse andintegration. In this paper; we present a well-founded provenance ontology named OpenproVenance Ontology (OvO) which takes inspiration on three theories: the lifecycle of insilico scientific experiments; the Open Provenance Model (OPM) and the UnifiedFoundational Ontology (UFO). OvO may act as a reference conceptual model that can be …,ONTOBRAS-MOST,2012,4
Provenance management in Swift with implementation details.,LM Gadelha; B Clifford; M Mattoso; M Wilde; I Foster,Abstract The Swift parallel scripting language allows for the specification; execution andanalysis of large-scale computations in parallel and distributed environments. It incorporatesa data model for recording and querying provenance information. In this article we describethese capabilities and evaluate interoperability with other systems through the use of theOpen Provenance Model. We describe Swift's provenance data model and compare it to theOpen Provenance Model. We also describe and evaluate activities performed within theThird Provenance Challenge; which consisted of implementing a specific scientific workflow;capturing and recording provenance information of its execution; performing provenancequeries; and exchanging provenance information with other systems. Finally; we proposeimprovements to both the Open Provenance Model and Swift's provenance system.,*,2011,4
Paralelismo de dados científicos em workflows usando técnicas P2P,Jonas Dias; E Ogasawara; M Mattoso,Resumo. O crescimento dos experimentos científicos em larga escala motiva a busca porambientes computacionais que apoiem a paralelização de atividades computacionais;particularmente; as que atendem o paradigma Many Task Computing (MTC). Aparalelização em ambientes MTC envolve o modelo de bagof-tasks; porém é maiscomplexo devido ao número bem mais elevado de processos e de tarefas a seremescalonados e executados; além da heterogeneidade. À medida que máquinas paralelasutilizam centenas de milhares processadores; este ambiente se aproxima dascaracterísticas de redes P2P; onde há heterogeneidade entre os nós processadores emaior índice de queda de nós. Gerenciar a programação paralela usando apenas MessagePassing Interface (MPI) não é mais possível. Nesta dissertação propomos o uso de …,IX Workshop de Teses e Dissertações em Banco de Dados,2010,4
Using XML with Large Parallel Datasets: Is There Any Hope?,RN Elias; Vanessa Braganholo; Jerry Clarke; IHF SANTOS; M MATTOSO; ALGA COUTINHO,ABSTRACT The e-Science area has been calling attention of computer scientists to severalchallenges in supporting large scale scientific experiments. One of them is datarepresentation. Providing scientists with efficient data access starts by choosing the bestalternative to represent such data. An alternative concerned with data interoperability isnecessary in such scenario. This paper evaluates XDMF as an XML-based alternative torepresent data in large scale parallel CFD applications. In this format; data considered as“heavy” is stored in bi-nary files; while all information required to data access is stored inXML files; inheriting XML's flexibility; interoperability between different applications; self-description and easyof-access. These advantages are demonstrated in a real case study;the interaction of a parallel coupled viscous flow and temperature solver with a parallel …,Parallel Computational Fluid Dynamics: Recent Advances and Future Directions,2010,4
Database clusters,Marta Mattoso,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,4
Storing AXML documents with ARAXA.,Cláudio Ananias Ferraz; Vanessa P Braganholo; Marta Mattoso,Abstract. Active XML (AXML) documents combine extensional XML data with intentionaldata defined through Web service calls. The dynamic properties of these documents posechallenges to both storage and data materialization techniques. We present ARAXA; a non-intrusive approach to store AXML documents. It takes advantage of complex objects fromobject-relational DBMS to represent both extensional and intentional data. By using a DBMSwe benefit from efficient storage tools and query engine. We have defined a storagemechanism with a methodology to materialize AXML documents at query time. We have alsoimplemented a prototype of ARAXA. Our experimental results show that our approach isscalable and extensible.,SBBD,2007,4
Geração Eficiente de Planos de Materialização para Documentos XML Ativos.,Daniela Pereira; Gabriela Ruberg; Marta Mattoso,Resumo. Um documento AXML possui dados XML representando chamadas de serviçosWeb. Para materializar o conteúdo de um documento AXML; é preciso invocar todas assuas chamadas de serviço Web. Gerar bons planos de materialização para documentosAXML em ambientes P2P é um problema difícil; cujo espaço de soluções crescerapidamente. Este trabalho propõe a SLS-MC; uma estratégia de otimização baseada embusca local estocástica com múltiplas condições de parada; para a geração eficiente deplanos de materialização. A SLS-MC foi implementada em um ambiente de simulaçãochamado SiMAX. Foram realizados vários testes que apontam o potencial de ganho dedesempenho da SLS-MC.,SBBD,2006,4
High Performance Computing for Computational Science-VECPAR 2002: 5th International Conference; Porto; Portugal; June 26-28; 2002. Selected Papers and Invit...,José MLM Palma; Jack Dongarra; Vicente Hernández; A Augusto Sousa; Marina Waldén,The 5th edition of the VECPAR series of conferences marked a change of the conferencetitle. The full conference title now reads VECPAR 2002—5th Int-national Conference onHigh Performance Computing for Computational S-ence. This re? ects more accurately whathas been the main emphasis of the conference since its early days in 1993–the use ofcomputers for solving pr-lems in science and engineering. The present postconference bookincludes the best papers and invited talks presented during the three days of the conference;held at the Faculty of Engineering of the University of Porto (Portugal); June 26–28 2002.The book is organized into 8 chapters; which as a whole appeal to a wide researchcommunity; from those involved in the engineering applications to those interested in theactual details of the hardware or software implementation; in line with what; in these days …,*,2003,4
Odysseysearch: An agent system for component,RMM Braga; CML Werner; Marta Mattoso,*,The 2nd International Workshop on Software Engineering for Large-Scale Multi-Agent Systems; Portland; Oregon-USA,2003,4
Managing Scientific Models in Structural Genomic Projects,Maria Cláudia Cavalcanti; Maria Luiza Campos; Marta Mattoso,Scientific applications usually require combining multiple models originating from differentdisciplines. The choice of a model is usually guided by the experience of the scientistprevious case studies. Once models are chosen; scientists can run the correspondingprograms. However; choosing the right models and running the adequate programs aredone empirically. In addition; previous successful case studies from other scientists are hardto reuse. Model characteristics are described in several ways; and the experience from asuccessful model application is not always registered in paper reports. Consequently;scientists have difficulties in model management activities; such as; comparing differentmodels; finding associations between models and programs; and more importantly; takingadvantage from a large number of previous experiences. Moreover; in a multidisciplinary …,Workshop on Data Lineage and Provenance; Chicago; IL,2002,4
Processamento de consultas orientadas a objetos,Marta Mattoso; Gabriela Ruberg; André Victor; Fernanda Baião,*,Relatório Técnico ES-547/01; COPPE/UFRJ; Brasil,2001,4
A Reuse Infrastructure based on Domain Models,Regina Braga; Claudia Werner; Marta–A Mattoso,Abstract Reuse is a key component in the software development process; specifically whenit is applied in the early phases of the process. One of the most encouraging reusetechniques available is the component-based software development. Based on thisassumption; this work presents a reuse infrastructure to help the component-baseddevelopment of applications in a given domain; named RIDOM. Object-oriented frameworks;software architectures; artificial intelligence techniques; domain analysis and OODBMS aretechnologies that RIDOM1 uses to make component-based reuse feasible in a specificdomain. The innovative approach of RIDOM reduces the semantic gap between thespecification and the software development.,Proceedings of ICCI,1996,4
A Knowledge-Based Perspective of the Distributed Design of Object Oriented Databases,Fernanda Baiao; Marta Mattoso; Gerson Zaverucha,Abstract The performance of applications on Object Oriented Database ManagementSystems (OODBMSs) is strongly affected by Distributed Design; which reduces irrelevantdata accessed by applications and data exchange among sites. In an OO environment; theDistributed Design is a very complex task; and an open research problem. In this work wepropose a knowledge based approach to the fragmentation phase of the distributed designof object oriented databases. In this approach; we will show a rule-based implementation ofan analysis algorithm from our previous work and propose some ideas towards the use ofInductive Logic Programming (ILP) to perform a knowledge discovery/revisi,WIT Transactions on Information and Communication Technologies,1970,4
Improving workflow design by mining reusable tasks,Frederico E Tosta; Vanessa Braganholo; Leonardo Murta; Marta Mattoso,With the increasing popularity of scientific workflow management systems (SWfMS); moreand more workflow specifications are becoming available. Such specifications containprecious knowledge that can be reused to produce new workflows. It is a fact thatprovenance data can help reusing third party code. However; finding the dependenciesamong programs without the support of a tool is not a trivial activity and; in many cases;becomes a barrier to build more sophisticated models and analysis. Due to the hugenumber of task versions available and their configuration parameters; this activity is highlyerror prone and counterproductive. In this work; we propose workflow recommender (WR); arecommendation service that aims at suggesting frequent combinations of workflow tasks forreuse. It works similarly to an e-commerce application that applies data mining …,Journal of the Brazilian Computer Society,2015,3
Monitoramento de Desempenho usando Dados de Proveniência e de Domínio durante a Execução de Aplicações Científicas,Renan Souza; Vítor Silva; Leonardo Neves; Daniel de Oliveira; Marta Mattoso,Resumo. Simulações computacionais; em geral; são compostas pelo encadeamento deaplicações científicas e executadas em ambientes de processamento de alto desempenho.Tais execuções comumente apresentam gargalos associados ao fluxo de dados entre asaplicações. Diversas ferramentas de perfilagem de código têm apoiado a análise de dadosde desempenho; como a Tuning and Analysis Utilities (TAU). Entretanto; essas ferramentasnão favorecem as análises do fluxo de dados. Essas análises podem ser realizadas com acaptura de dados de proveniência enriquecidos com dados de domínio extraídos ao longoda execução das simulações. Neste artigo; propomos o monitoramento de dados dedesempenho por meio de consultas a uma base de dados de proveniência que integradados sobre a execução; o fluxo de dados das simulações e os dados de domínio …,Anais do XIV Workshop em Desempenho de Sistemas Computacionais e de Comunicação (WPerformance),2015,3
Sgprov: Summarization mechanism for multiple provenance graphs,Daniele El-Jaick; Marta Mattoso; Alexandre AB Lima,Abstract Scientific workflow management systems (SWfMS) are powerful tools in theautomation of scientific experiments. Several workflow executions are necessary toaccomplish one scientific experiment. Data provenance; typically collected by SWfMS duringworkflow execution; is important to understand; reproduce and analyze scientificexperiments. Provenance is about data derivation; thus it is typically represented in the formof a directed acyclic graph. For each workflow execution; a provenance graph is generated.Numerous graphs are generated after several workflow runs; exploring different parameters.The resulting provenance database requires considerable storage space and querying itinvolves handling a large volume of graphs. Typical provenance queries process manygraphs to get data derivation paths (lineage). This article proposes SGProv; a …,Journal of Information and Data Management,2014,3
Applying provenance to protect attribution in distributed computational scientific experiments,Luiz MR Gadelha; Marta Mattoso,Abstract The automation of large scale computational scientific experiments can beaccomplished with the use of scientific workflow management systems; which allow for thedefinition of their activities and data dependencies. The manual analysis of the dataresulting from their execution is burdensome; due to the usually large amounts ofinformation. Provenance systems can be used to support this task since they gather detailsabout the design and execution of these experiments. However; provenance informationdisclosure can also be seen as a threat to correct attribution; if the proper securitymechanisms are not in place to protect it. In this article; we address the problem of providingadequate security controls for protecting provenance information taking into accountrequirements that are specific to e-Science. Kairos; a provenance security architecture; is …,International Provenance and Annotation Workshop,2014,3
Provenance traces of the swift parallel scripting system,Luiz MR Gadelha Jr; Michael Wilde; Marta Mattoso; Ian Foster,Abstract In this abstract; we describe provenance traces generated from executions ofscientific workflows managed by the Swift parallel scripting system. They follow aprovenance data model; used by MTCProv; the provenance management component ofSwift. It is similar to PROV; representing most of its core concepts and including additionalinformation about the scientific domain; computational resource consumption; andprospective provenance. We describe provenance queries that follow patterns commonlyfound in high performance computing and that are straightforward to support with MTCProv'sbuilt-in procedures. These queries often involve costly relational join operations andrecursion; providing a relevant case for benchmarking.,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,3
Ontology-based semi-automatic workflow composition,Daniel de Oliveira; Eduardo Ogasawara; Jonas Dias; Fernanda Baião; Marta Mattoso,*,Journal of Information and Data Management,2012,3
Adding Ontologies to Scientific Workflow Composition.,Daniel de Oliveira; Eduardo S Ogasawara; Fernanda Araujo Baião; Marta Mattoso,Abstract. Scientific workflows are being used as an abstraction for the composition of largescale scientific experiments. As scientific workflows become more complex; theseabstractions isolate scientists from infrastructure issues. Although representing a workflow inan abstract level is a first step; there are many open issues; such as the ones related tosemantics. Adding semantics to abstract workflows enables the explicit representation ofwhich activities can be linked to each other; or which activities are equivalent to each other.However; representing an abstract workflow with semantics is an open problem. Existingapproaches address either the representation of abstract workflows or the use of domainontologies to add semantics to activities; but not both. In the latter case; these approachesfocus only on adding semantics to executable workflows; instead of working in different …,SBBD (Short Papers),2011,3
Uma avaliação da Distribuição de Atividades Estática e Dinâmica em Ambientes Paralelos usando o Hydra,Vítor Silva; Fernando Chirigati; Eduardo Ogasawara; Jonas Dias; D Oliveira; Fábio Porto; Patrick Valduriez; Marta Mattoso,Abstract. Scientific Workflows are used as a basic tool to design and execute scientificexperiments on different computational environments. These workflows can becomputational and data intensive; requiring high performance computing. The way in whichworkflow activities are parallelized and distributed over these environments affects theoverall performance of the workflow. This work evaluates two different strategies for activitydistribution (static and dynamic) using Hydra middleware integrated with VisTrails. Ourexperiments show that using the right strategy decreases elapsed time for activitydistribution in 30%. Resumo. Workflows Científicos são usados como uma abstração básicapara estruturação e execução de experimentos científicos em diferentes ambientescomputacionais. Estes workflows podem ser intensivos tanto computacionalmente …,V e-Science,2011,3
Captura de metadados de proveniencia para workflows científicos em nuvens computacionais,Carlos Paulino; D Oliveira; S Cruz; Maria Luiza Machado Campos; Marta Mattoso,Abstract. Workflows are scientific abstractions used in the modeling of scientific experiments.High performance capabilities such as clusters and grids are often required to run theexperiments. Cloud computing is starting to be adopted by the scientific community.However; the cloud environment is still incipient in collecting and recording workflowprovenance. This paper presents an approach to support collecting metadata provenance ofscientific experiments; based on an evolution of the Matrioshka architecture for the cloudenvironment. Preliminary results show that provenance metadata captured from the virtualcomponents of the cloud can aid scientists to manage and reproduce their in silicoexperiments. Resumo. Workflows científicos são abstrações utilizadas na modelagem deexperimentos científicos. Eles muitas vezes demandam recursos de alto desempenho …,Anais Do XXV Simpósio Brasileiro de Banco de Dados,2010,3
Virtual partitioning,Marta Mattoso,The valid time of a fact is the time when the fact is true in the modeled reality. Any subset ofthe time domain may be associated with a fact. Thus; valid timestamps may be sets of timeinstants and time intervals; with single instants and intervals being important special cases.Valid times are usually supplied by the user.,*,2009,3
Using ontologies to provide different levels of abstraction in scientific workflows,Daniel de Oliveira; Eduardo Ogasawara; Fernanda Araujo Baião; Marta Mattoso,Abstract: Scientific experiments are usually modeled as scientific workflows; and may beenacted using Scientific Workflow Management Systems (SWfMS). There is a huge variety ofSWfMS available; such as Taverna and VisTrails; which enables the specification andexecution of a chain of activities usually represented by programs or services. However;while using SWfMS the workflow has to be designed; revised and tuned on top of anexecution format. Such format has many data conversion components and other auxiliarytools that can pollute the workflow definition. Working on a higher level of representation letsthe scientist closer to the experiment definition rather than implementation details. There is alack of semantic help for workflow analysis; ie to query on algorithms in a group ofexecutable scientific workflows; a scientist should know a priori which program is related …,*,2009,3
Applying reinforcement learning to scheduling strategies in an actual grid environment,Bernardo Fortunato Costa; Marta Mattoso; Ines Dutra,Grid environments are dynamic and heterogeneous by nature; therefore requiring adaptivescheduling strategies. Reinforcement learning is an interesting and simple adaptiveapproach that may work well in actual grid environments. In this work; we employreinforcement learning to classify available resources in a grid environment; giving supportto two scheduling algorithms; AG and MQD. We study the makespan optimisation and loadbalancing. An algorithm known as RR is used for normalising purposes.,International Journal of High Performance Systems Architecture,2009,3
Uma Abordagem Semântica para Linhas de Experimentos Científicos Usando Ontologias,Daniel de Oliveira; Eduardo Ogasawara; Fernando Chirigati; Vítor Sousa; Leonardo Murta; Cláudia Werner; Marta Mattoso,Abstract. Scientific workflows have been used as an abstraction to compose scientificexperiments. However; the composition of these workflows is a complex task. Currently thereis no guidance or process to follow to reach a workflow specification. As workflows becomemore complex; composition needs abstraction and semantic support. Experiment lines arean innovative and promising solution to model and reuse scientific workflows in differentlevels of abstraction. This paper proposes an ontology coupled to experiment lines toprovide semantics and flexibility. Resumo. Workflows científicos vêm sendo usados comouma abstração para compor experimentos científicos. Entretanto; a composição destesworkflows é uma tarefa complexa. Atualmente não há um guia ou processo a ser seguidopara alcançar uma especificação de um workflow. Conforme os workflows se tornam …,*,2009,3
Paralelização de Tarefas de Mineração de Dados Utilizando Workflows Científicos,Carlos Eduardo Barbosa; Eduardo Ogasawara; D de Oliveira; Marta Mattoso,Abstract. Classical data mining (DM) process is usually composed by a chain of programsand data that are modeled by a specialist. This chain of programs may be modeled as aworkflow and takes advantage of scientific workflow management systems (SWfMS). Thispaper proposes an approach for parallelizing tasks within a data mining workflow.Experimental results using timeseries forecast with neural networks reinforce theperformance gains and additional benefits provided by SWfMS; such as provenancerecording of the workflow.. Resumo. Um processo de mineração de dados (MD) clássicopode ser visto como o encadeamento de programas e dados modelados por umespecialista. Esta cadeia pode ser modelada como um workflow e usufruir das vantagensde sistemas de gerência de workflows científicos (SGWfC). Este artigo prop  e uma …,V Workshop em Algoritmos e Aplicações de Mineração de Dados,2009,3
Gerência de workflows científicos: oportunidades de pesquisa em bancos de dados,Marta Mattoso; Sergio Manuel Serra da Cruz,Abstract O encadeamento de processos por meio de workflows vem sendo usado namodelagem de processos comerciais nos últimos vinte anos. Mais recentemente; novossistemas de gerência de workflows surgiram para apoiar o encadeamento de processoscientíficos na realização de experimentos. O uso de workflows na área científica tem comprincipal diferença a orientação ao fluxo de dados em contra-posição ao fluxo de controlesno ambiente comercial. A gerência de dados científicos associada a processos complexosapresenta uma série de desafios para a concepção; representação e execução dessesworkflows. Esses desafios fazem parte do documento da SBC dos grandes desafios dacomputação nos próximos dez anos; mais especificamente do desafio dois; onde é ditoque:" O objetivo deste desafio é criar; avaliar; modificar; compor; gerenciar e explorar …,Proceedings of the 23rd Brazilian symposium on Databases,2008,3
MF-Ontology; uma ontologia para o processo de mineração de textos,Daniel de Oliveira; Fernanda Baião; Marta Mattoso,Este artigo apresenta uma ontologia voltada para o processo de mineração de textos(também chamado de Knowledge Discovery on Texts). Esta ontologia foi desenvolvida eposteriormente acoplada a um sistema de gerência de workflows científicos (SGWf) a fim decriar um vocabulário comum entre os usuários do SGWf e incorporar mais semântica àsetapas de definição e seleção dos serviços computacionais do workflow. A MF-Ontologyestende uma ontologia de mineração de dados; incluindo conceitos específicos para amineração de textos. O artigo apresenta a forma como a MF-Ontology foi validada em duasfases: a primeira fase concentrou-se nos aspectos estruturais independentes de domínio;utilizando a abordagem OntoClean; e a segunda fase concentrou-se nos aspectosespecíficos do domínio; através de entrevistas com especialistas.,*,2008,3
On the Usage of Structural Information in Constrained Semi-Supervised Clustering of XML Documents,Eduardo Bezerra; Geraldo Xexéo; Marta Mattoso,AbstrAct In this chapter; we consider the problem of constrained clustering of documents. Wefocus on documents that present some form of structural information; in which priorknowledge is provided. Such structured data can guide the algorithm to a better clusteringmodel. We consider the existence of a particular form of information to be clustered: textualdocuments that present a logical structure represented in XML format. Based on thisconsideration; we present algorithms that take advantage of XML metadata (structuralinformation); thus improving the quality of the generated clustering models. This chapter alsoaddresses the problem of inconsistent constraints and defines algorithms that eliminateinconsistencies; also based on the existence of structural information associated to the XMLdocument collection.,Successes and New Directions in Data Mining,2007,3
Towards a theory revision approach for the vertical fragmentation of object oriented databases,Flavia Cruz; Fernanda Baião; Marta Mattoso; Gerson Zaverucha,Abstract The performance of applications on Object Oriented Database Ma-nagementSystems (OODBs) is strongly affected by Distribution Design; which reduces irrelevant dataaccessed by applications and data exchange among sites. In an OO environment; theDistributed Design is a complex task; and an open research problem. In this work; wepresent a knowledge-based approach for the vertical fragmentation phase of the distributeddesign of object-oriented databases. In this approach; we show a Prolog implementation ofa vertical fragmentation algorithm; and describe how it can be used as backgroundknowledge for a knowledge discovery/revision process through In-ductive LogicProgramming (ILP). The objective of the work is to extend our framework proposed to handlethe class fragmentation problem; showing the viability of automatically improving the …,Brazilian Symposium on Artificial Intelligence,2002,3
XVerter: Armazenamento e Consulta de Dados XML em SGBDs.,Humberto Vieira; Gabriela Ruberg; Marta Mattoso,Resumo Técnicas de armazenamento e consulta em bases de dados XML vêm sendoamplamente estudadas na literatura. A maioria dos trabalhos é favorável aoarmazenamento de documentos XML em SGBDs para aproveitar o potencial de umatecnologia bem estabelecida; além da vantagem de armazenar dados estruturados e dadosXML em um mesmo sistema. O objetivo deste trabalho é propor uma solução para arealização de consultas XQuery sobre uma base de dados XML armazenada em umSGBDOR através do formato de representação DOM. São propostas regras de traduçãoautomática da XQuery para a SQL3; onde é explorada a representação de documentosXML baseada em objetos.,SBBD,2002,3
A parallel spatial join framework using PMR-quadtrees,Alexandre AB Lima; Claudio Esperança; Marta Mattoso,Spatial join is the most important and complex operation in spatial databases. Therefore;there is a lot of ongoing research presenting algorithms and data structures to efficientlyprocess spatial joins. In this work we present a parallel solution for spatial join processingwith a dynamic load balance strategy. We have implemented PMR-quadtrees (Samet; 1990)on top of persistent object structures using an object oriented database server.,Database and Expert Systems Applications; 2000. Proceedings. 11th International Workshop on,2000,3
Deriving scientific workflows from algebraic experiment lines: A practical approach,Anderson Marinho; Daniel de Oliveira; Eduardo Ogasawara; Vitor Silva; Kary Ocaña; Leonardo Murta; Vanessa Braganholo; Marta Mattoso,Abstract The exploratory nature of a scientific computational experiment involves executingvariations of the same workflow with different approaches; programs; and parameters.However; current approaches do not systematize the derivation process from the experimentdefinition to the concrete workflows and do not track the experiment provenance down to theworkflow executions. Therefore; the composition; execution; and analysis for the entireexperiment become a complex task. To address this issue; we propose the AlgebraicExperiment Line (AEL). AEL uses a data-centric workflow algebra; which enriches theexperiment representation by introducing a uniform data model and its correspondingoperators. This representation and the AEL provenance model map concepts from theworkflow execution data to the AEL derived workflows with their corresponding …,Future Generation Computer Systems,2017,2
Managing hot metadata for scientific workflows on multisite clouds,Luis Pineda-Morales; Ji Liu; Alexandru Costan; Esther Pacitti; Gabriel Antoniu; Patrick Valduriez; Marta Mattoso,Large-scale scientific applications are often expressed as workflows that help defining datadependencies between their different components. Several such workflows have hugestorage and computation requirements; and so they need to be processed in multiple (cloud-federated) datacenters. It has been shown that efficient metadata handling plays a key rolein the performance of computing systems. However; most of this evidence concern onlysingle-site; HPC systems to date. In this paper; we present a hybrid decentralized/distributedmodel for handling hot metadata (frequently accessed metadata) in multisite architectures.We couple our model with a scientific workflow management system (SWfMS) to validateand tune its applicability to different real-life scientific scenarios. We show that efficientmanagement of hot metadata improves the performance of SWfMS; reducing the workflow …,Big Data (Big Data); 2016 IEEE International Conference on,2016,2
Online input data reduction in scientific workflows,Renan Souza; Vítor Silva; Alvaro Coutinho; Patrick Valduriez; Marta Mattoso,Many scientific workflows are data-intensive and need be iteratively executed for large inputsets of data elements. Reducing input data is a powerful way to reduce overall executiontime in such workflows. When this is accomplished online (ie; without requiring users to stopexecution to reduce the data and resume execution); it can save much time and userinteractions can integrate within workflow execution. Then; a major problem is to determinewhich subset of the input data should be removed. Other related problems includeguaranteeing that the workflow system will maintain execution and data consistent afterreduction; and keeping track of how users interacted with execution. In this paper; we adoptthe approach" human-in-the-loop" for scientific workflows by enabling users to steer theworkflow execution and reduce input elements from datasets at runtime. We propose an …,WORKS: Workflows in Support of Large-scale Science,2016,2
In Situ Data Steering on Sedimentation Simulation with Provenance Data,Vítor Silva; José Camata; Daniel De Oliveira; Alvaro Coutinho; Patrick Valduriez; Marta Mattoso,Parallel adaptive mesh refinement and coarsening (AMR) are optimal strategies for tacklinglarge-scale simulations. libMesh is an open-source finite-element library that supportsparallel AMR and is used in multiphysics applications. In complex simulation runs; usershave to track quantities of interest (residuals; errors estimates; etc.) to control as much aspossible the execution. However; this tracking is typically done only after the simulationends. This paper presents DfAnalyzer; a solution based on provenance data to extract andrelate strategic simulation data for online queries. We integrate DfAnalyzer to libMesh andParaView Catalyst; so that queries on quantities of interest are enhanced by in situvisualization.,SC: High Performance Computing; Networking; Storage and Analysis,2016,2
Integrating domain-data steering with code-profiling tools to debug data-intensive workflows,Vítor Silva; Leonardo Neves; Renan Souza; Alvaro Coutinho; Daniel de Oliveira; Marta Mattoso,ABSTRACT Computer simulations may be composed of scientific programs chained in acoherent flow and executed in High Performance Computing environments. Theseexecutions may present anomalies associated to the data that flows in parallel amongprograms. Several parallel code-profiling tools already support performance analysis; suchas Tuning and Analysis Utilities (TAU) or provide fine-grained performance statistics such asthe System Activity Report (SAR). However; these tools do not associate their results to theircorresponding dataflows. Such analysis is fundamental to trace back the data origins of anerror. In this paper; we propose to couple a workflow monitoring data approach to parallelcode-profiling tools for workflow executions. The goal is to profile and debug parallelworkflow executions by querying a database that is able to integrate performance …,Proc. of the 11th Workshop on Workflows in Support of Large-Scale Science,2016,2
Provenance and Annotation of Data and Processes,Marta Mattoso; Boris Glavic,This volume contains the proceedings of the 6th International Provenance and AnnotationWorkshop (IPAW); held June 7–8; 2016; at The MITRE Corporation in McLean; Virginia;USA. Following the successful inception of ProvenanceWeek in 2014; this year's installmentagain co-located the biennial IPAW workshop and the annual Workshop on the Theory andPractice of Provenance (TaPP). Together the two leading provenance workshops anchoredProvenanceWeek 2016; a full week of provenancerelated activities that included a sharedposter and demonstration session; and the PROV: Three Years Later and Provenance-based Security and Transparent Computing workshops. This year's installment of IPAW wasable to honor the extraordinary achievements of IPAW's authors through a best paper awardsponsored by Springer. We would like to use this forum to again congratulate Wellington …,6th International Provenance and Annotation Workshop (IPAW),2016,2
Data Analytics in Bioinformatics: Data Science in Practice for Genomics Analysis Workflows,Kary ACS Ocaña; Vitor Silva; Daniel De Oliveira; Marta Mattoso,Workflow systems manage large-scale experiments and deliver a large volume ofprovenance data traces. The provenance repository of these systems contains informationabout the workflow execution; which allows for tracking and analyzing data transformations.However; provenance data may still be considered a black-box; when it comes to analyzethe contents of resulting data files. Current solutions are focused on data transformation atcoarse grain; they point to input and output files; but do not allow for exploring domain-specific data. Data analytics is essential for managing large-scale workflows executed inparallel; especially when tracking anomalous executions. In this paper; we present a dataanalytics approach; which is based on the use of provenance data enriched with domain-specific data coupled to a data mining tool. A real bioinformatics workflow was modeled …,e-Science (e-Science); 2015 IEEE 11th International Conference on,2015,2
Parallelization of scientific workflows in the cloud,Ji Liu; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Nowadays; more and more scientific experiments need to handle massive amounts of data.Their data processing consists of multiple computational steps and dependencies withinthem. A data-intensive scientific workflow is an appropriate tool for modeling such process.Since the execution of data-intensive scientific workflows requires large-scale computingand storage resources; a cloud environment; which provides virtually infinite resources isappealing. However; because of the general geographical distribution of scientific groupscollaborating in the experiments; multisite management of data-intensive scientific workflowsin the cloud is becoming an important problem. This paper presents a general study of thecurrent state of the art of data-intensive scientific workflow execution in the cloud andcorresponding multisite management techniques.,*,2014,2
Towards recommendations for horizontal XML fragmentation,Tatiane Lima da Silva; Fernanda Baião; Jonice de Oliveira Sampaio; Marta Mattoso; Vanessa Braganholo,Abstract The large amount of XML data available on the web and inside organizationsmakes the performance of query processing a big concern. Several techniques can beapplied to improve query processing performance; including indexing and data distribution.The increasing popularity of clouds; clusters and grids makes data distribution a feasiblealternative. In these approaches; data is fragmented and distributed to several nodes; andqueries submitted by users are processed in parallel; thus improving performance. However;the problem of how to fragment an XML database has not been adequately addressed.There are lots of definitions for XML fragments in the literature; but few proposals focus onhow to use those definitions to actually fragment the database–a process calledfragmentation design. Inspired by the relational and object-oriented models; which both …,Journal of Information and Data Management,2013,2
Monitoramento em Tempo Real de Workflows Científicos Executados em Paralelo em Ambientes Distribuídos,Julliano Pintas; D Oliveira; Kary Ocaña; Jonas Dias; Marta Mattoso,DISSERTAÇÃO SUBMETIDA AO CORPO DOCENTE DO INSTITUTO ALBERTO LUIZCOIMBRA DE PÓS-GRADUAÇÃO E PESQUISA DE ENGENHARIA (COPPE) DAUNIVERSIDADE FEDERAL DO RIO DE JANEIRO COMO PARTE DOS REQUISITOSNECESSÁRIOS PARA A OBTENÇÃO DO GRAU DE MESTRE EM CIÊNCIAS EMENGENHARIA DE SISTEMAS E COMPUTAÇÃO.,VI e-Science workshop,2012,2
Reprodução de Experimentos Científicos Usando Nuvens.,Ary Henrique M de Oliveira; Murilo de Souza Martins; Igor Modesto; Daniel de Oliveira; Marta Mattoso,Resumo. Workflows científicos são utilizados para modelar experimentos computacionais.Os resultados desses experimentos são publicados e compartilhados na forma de artigospublicados em veículos científicos. Entretanto; para que tais resultados sejamcientificamente válidos eles devem ser passíveis de reprodução. Pesquisadores da área dee-Science têm a necessidade de compartilhar os artefatos utilizados para a geração dosresultados; dentre eles; os dados de entrada do workflow e os parâmetros utilizados noexperimento. Entretanto; reproduzir um experimento baseado nestes artefatos não é umatarefa trivial. Apesar de o workflow especificar o protocolo de execução; com dados eparâmetros de entrada disponíveis; nem sempre o ambiente de execução está acessível.Programas que foram originalmente utilizados podem estar obsoletos; versões de …,SBBD (Short Papers),2012,2
Towards a threat model for provenance in e-Science,Luiz MR Gadelha; Marta Mattoso; Michael Wilde; Ian Foster,Abstract Scientists increasingly rely on workflow management systems to perform large-scale computational scientific experiments. These systems often collect provenanceinformation that is useful in the analysis and reproduction of such experiments. On the otherhand; this provenance data may be exposed to security threats which can result; forinstance; in compromising the analysis of these experiments; or in illegitimate claims ofattribution. In this work; we describe our ongoing work to trace security requirements forprovenance systems in the context of e-Science; and propose some security controls to fulfillthem.,International Provenance and Annotation Workshop,2010,2
XCraft: A Dynamic Optimizer for the Materialization of Active XML Documents,Gabriela Ruberg; Marta Mattoso,Abstract An active XML (AXML) document contains special tags that represent calls to Webservices. Retrieving its contents consists in materializing its data elements by invoking all itsembedded service calls in a P2P network. In this process; the results of some service callsare often used as inputs to other calls. Also; usually several peers provide each requestedWeb service; and peers can collaborate to invoke these services. This implies manyequivalent materialization alternatives; with different performance. Optimizing the AXMLmaterialization process is a hard problem; which often involves searching a huge space ofsolutions. Current techniques for workflow scheduling and distributed query processing areinsufficient for this problem; since in AXML materialization:(i) the set of participating peers isnot known in advance;(ii) service calls in the result of other calls forbid a simple “optimize …,Relatório Técnico; COPPE/UFRJ,2006,2
Uma Abordagem para o Armazenamento de Documentos XML Ativos,C Ferraz; V Braganholo; M Mattoso,A busca por soluções que possibilitem tanto a integração de aplicações de forma simplesquanto o intercâmbio de informações de maneira padronizada fez com que surgissempadrões para publicação e acesso a informações e serviços na Web; tais como XML eServiços Web. O sucesso destas tecnologias proporcionou o desenvolvimento de uma novaclasse de documentos XML; os documentos XML Ativos (AXML)[1]. Documentos XML Ativospossuem chamadas de Serviços Web embutidas em seu conteúdo. A partir da execuçãodestas chamadas de serviços; o resultado das mesmas é materializado dentro dodocumento XML; enriquecendo este documento com o novo conteúdo dinâmico deinformação. A persistência de documentos AXML apresenta particularidades referentes aoaspecto ativo desta classe de documento XML. A detecção das chamadas de serviços …,XXISIMPÓSIO BRASILEIRO DE BANCO DE DADOS,2006,2
GOS: Especificação de um Mecanismo de Busca e Recuperação de Componentes,AM Oliveira; R Braga; Fernanda Campos; Marta Mattoso,*,Proceedings 7º Workshop Iberoamericano de Ingeniería de Requisitos y Ambientes Software Ideas 2004,2004,2
Evaluating the DSMIO cache-coherence algorithm in cluster-based parallel ODBMS,Carla Osthoff; Cristiana Bentes; Daniel Ariosto; Marta Mattoso; Claudio L Amorim,Abstract In this paper; we assess the performance of DSMIO cachecoherence algorithmimplemented in a parallel object-based database management system (ODBMS). Thedistinguishing feature of DSMIO is its use of the lazy release memory consistency model andmultiplewriter protocol to reduce both the number and size of coherence messages requiredto keep coherent a distributed ODBMS across a cluster of PC servers. Using a largedistributed database and several application workloads we evaluate DSMIO performanceand also compare it against that of the well-known Call-Back Locking (CBL) algorithm. Ourresults showt hat both algorithms perform very well for read operations whereas DSMIOoutperforms signi. cantly CBL for write operations with DSMIO speed-ups attaining as muchas 5.4 while CBL speed-ups reach at most 1.4 for an 8-node cluster. Overall; these results …,International Conference on Object-Oriented Information Systems,2002,2
Managing Scientific Models in Bio-phenomena Interpretation,MC Cavalcanti; M Campos; M Mattoso; M Santos; P Barr,Abstract Bio-phenomena interpretation case studies are based on the application ofscientific models. Usually; these studies are based on previous experiences. This papershows how to provide for a better support to bio-phenomena scientists and specialists; onexchanging scientific programs; models and data. We specifically describe the Cabiunascase study. Introduction Corrosion monitoring on oil platforms over the Brazilian coastalzone is one of the main concerns of scientists from CENPES-Petrobrás. Some of thesescientists are chemists; biologists and engineers that study corrosion caused by bacteria. Toidentify the main cause of bio-corrosion events such as oil spills; teams of specialists have tocollect heterogeneous distributed data and apply an adequate scientific model. Forexample; they collect water or pipe samples from the region under investigation. Then …,17th World Petroleum Congress,2002,2
Otimização de Processamento de Expressões de Caminho,A Victor; Marta Mattoso,*,*,2001,2
Mechanisms for specifying communication behavior in object oriented database systems,Paulo F Pires; Mário Roberto F Benevides; Marta Mattoso,ABSTRACT Most object oriented concurrency control is dedicated to increase methodexecution concurrency; however they do not consider the inter-object behavior. In this paperwe present mechanisms to deal with the communication behavior in OODBMS. We definelinguistic constructors that describe both the internal and the external object communicationbehavior. We also present a decentralized transaction model and a protocol that guaranteethe correct method execution for a single object (intra-object) and for a group of objects (inter-object) that cooperate to accomplish an activity.,Proceedings of the 2000 ACM symposium on Applied computing-Volume 1,2000,2
Avaliação do Algoritmo de Coerência de Cache de Disco DSMIO.,Carla Osthoff; Marta Mattoso; Cristiana Bentes Seidel; Ricardo Bianchini; Claudio Luis de Amorim,In a previous paper we proposed a disk cache coherence algorithm; called DSMIO; formulticomputers or clusters of workstations. The algorithm keeps caches coherent usingconcepts and mechanisms from software distributed shared-memory systems; such asmultiple concurrent writers and the Lazy Release Consistency model for reducing thenegative impact of false sharing. In this paper we present a detailed analysis of theperformance of DSMIO in the context of a parallel object-based database system where onenode acts as a server; accessing stable storage and keeping caches coherent. We performexperiments with the 007 Benchmarks on an SP2 multicomputer. The most interesting part ofthe paper compares the DSMIO results against those of the state-of-the-art CallBack Locking(CBL) cache coherence algorithm. The results of this comparison show that our system …,SBBD,2000,2
Evaluating Cache Coherence in the DSMIO System,Carla Osthoff; C Bentes; Ricardo Bianchini; Marta Mattoso; C Amorim,Abstract—There has been a significant amount of research on object-based databasemanagement systems (ODBMS). In contrast to traditional relational systems; ODBMS use thesame data model for the clients and the server. Thus; parallel ODBMS typically adopt a data-shipping architecture; which allows data request processing to be performed at the clients.Although such a strategy can improve performance by moving data closer to clients and byalleviating the load on the server; it raises the issue of cache coherence. Our work is basedon the observation that coherence maintenance techniques developed to improvesoftwarebased distributed shared-memory systems can be exploited to improve parallelODBMS. In previous papers we proposed a coherence algorithm; called DSMIO; that usesthe Lazy Release Consistency model and the diffing mechanism to limit the number and …,Proceedings of the 12th Symposium on Computer Architecture and High Performance Computing; Sao Pedro; Brazil,2000,2
Mediators Metadata Management Services: An Implementation Using GOA++ System,Thaís Saldunbides Brügger; Paulo de Figueiredo Pires; Marta Mattoso,Abstract The main contribution of this work is the development of a Metadata Manager tointerconnect heterogeneous and autonomous information sources in a flexible; expandableand transparent way. The interoperability at the semantic level is reached using anintegration layer; structured in a hierarchical way; based on the concept of Mediators.Services of a Mediator Metadata Manager (MMM) are specified and implemented usingfunctions based on the Outlines of GOA++. The MMM services e are available in the form ofa GOA++ API and they can be accessed remotely via CORBA or through local API calls.,Electronic Journal of SADIO; http://www. dc. uba. ar/sadio/ejs,1999,2
A Theory Refinement Approach to the Design of Distributed Object Oriented Databases,F Baião; Marta Mattoso; Gerson Zaverucha,Abstract Distributed Design involves making decisions on the fragmentation and placementof data across the sites of a computer network. The first phase of the Distributed Design in atop-down approach is the fragmentation phase; which clusters in fragments the informationaccessed simultaneously by applications. Since Distributed Design is a very complex task inthe context of the OO data model; we have presented in our previous works a strategy toassist distributed designers in the fragmentation phase of OO databases; which was dividedin three phases: Analysis Phase; Vertical Fragmentation and Horizontal Fragmentation. TheAnalysis Phase defined the most adequate fragmentation technique (horizontal; vertical ormixed) to be applied in each class of the database schema; based on some proposedheuristics. Initial experiments using our proposed approach have resulted in …,Relatório Técnico ES-493/99; COPPE/UFRJ; Brasil,1999,2
Scientific workflow scheduling with provenance data in a multisite cloud,Ji Liu; Esther Pacitti; Patrick Valduriez; Marta Mattoso,Abstract Recently; some Scientific Workflow Management Systems (SWfMSs) withprovenance support (eg Chiron) have been deployed in the cloud. However; they typicallyuse a single cloud site. In this paper; we consider a multisite cloud; where the data andcomputing resources are distributed at different sites (possibly in different regions). Basedon a multisite architecture of SWfMS; ie multisite Chiron; and its provenance model; wepropose a multisite task scheduling algorithm that considers the time to generateprovenance data. We performed an extensive experimental evaluation of our algorithmusing Microsoft Azure multisite cloud and two real-life scientific workflows (Buzz andMontage). The results show that our scheduling algorithm is up to 49.6% better thanbaseline algorithms in terms of total execution time.,*,2017,1
Scientific Workflow Execution with Multiple Objectives in Multisite Clouds,Ji Liu; Esther Pacitti; Patrick Valduriez; Daniel De Oliveira; Marta Mattoso,In this short paper (see [4] for the extended version); we propose a general solution basedon multi-objective scheduling to execute SWfs in a multisite cloud with the following maincontributions: the design of a multi-objective cost model; SSVP VM provisioning approach;ActGreedy scheduling algorithm and an extensive experimental evaluation.,BDA: Bases de Données Avancées,2016,1
Integrating scientific workflows with scientific gateways: a bioinformatics experiment in the brazilian national high-performance computing network,Maria Luiza Mondelli; M Galheigo; Vıvian Medeiros; Bruno F Bastos; Antônio Tadeu Azevedo Gomes; ATR Vasconcelos; LMR Gadelha Jr,Abstract. Bioinformatics experiments are rapidly and constantly evolving due improvementsin sequencing technologies. These experiments usually demand high performancecomputation and produce huge quantities of data. They also require different programs to beexecuted in a certain order; allowing the experiments to be modeled as workflows. However;users do not always have the infrastructure needed to perform these experiments. Ourcontribution is the integration of scientific workflow management systems and grid-enabledscientific gateways; providing the user with a transparent way to run these workflows ingeographically distributed computing resources. The availability of the workflow through thegateway allows for a better usability of these experiments.,X Brazilian e-Science Workshop. Anais do XXXVI Congresso da Sociedade Brasileira de Computaçao; SBC,2016,1
Exploiting the parallel execution of homology workflow alternatives in HPC compute clouds,Kary ACS Ocaña; Daniel de Oliveira; Vítor Silva; Silvia Benza; Marta Mattoso,Abstract Homology modeling (HM) plays an important role in drug discovery. HM analysisaims at predicting a 3D model from a biological sequence in order to discover new drugs.There are several problems in executing an HM analysis in large-scale; such as multiplesoftware to be evaluated; the management of the parallel execution; and results analysis; egbrowsing manually all results to find which structure was derived from which program withgood quality. Scientific Workflow Management System (SWfMS) with parallelism andprovenance support can aid the large-scale HM executions by addressing the resultanalysis. However; before submitting the HM workflow for execution; it has to be specifiedalong with its several alternatives (also called variants); as considered in this paper.Managing HM workflow variations is a complex task to be accomplished even with the …,*,2015,1
Towards supporting provenance gathering and querying in different database approaches,Flavio Costa; Vítor Silva; Daniel de Oliveira; Kary ACS Ocaña; Marta Mattoso,Abstract The amount of provenance data gathered from Scientific Workflow ManagementSystems (SWfMS) and stored in databases has been growing considerably. Some difficultiesare related to representation; access and query provenance databases. Despite the effort ofPROV W3C group; data analyses may require different strategies of query specificationbecause of the volume of data to be analyzed and the nature of queries. Another importantpoint is the new approaches to store and retrieve provenance; some technologies are moreappropriate than others. However; when applications are tightly coupled to specifictechnologies; it is difficult to take advantage of innovation. Based on these issues; we havebuilt WfP-API; an API to store and perform queries in different provenance databases.,International Provenance and Annotation Workshop,2014,1
On the performance of the position () XPath function,Luiz Augusto Matos da Silva; Laerte N da Silva Jr; Marta Mattoso; Vanessa Braganholo,Abstract In very large XML documents or collections; the query response times are notalways satisfactory. To overcome this limitation; parallel processing can be applied. Datacan be replicated in several processors and queries can be partitioned to run over differentvirtual data partitions on each processor; on an approach called virtual partitioning. PartiX-VP is a simple XML virtual partitioning approach that generates virtual data partitions bydividing the cardinality of the partitioning attribute by the number of allocated processors;resulting in intervals of equal size for each processor. In this approach; the XML query isrewritten and selection predicates are added to define the virtual partitions. These selectionpredicates use the position () XPath function that addresses a set of elements on a givenposition in the document. In this paper; we present an experimental evaluation of the …,Proceedings of the 2013 ACM symposium on Document engineering,2013,1
Uma Arquitetura P2P de Distribuição de Atividades para Execução Paralela de Workflows Científicos,Vítor Silva; Jonas Dias; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Abstract. Scientific workflows are composed of activities that model scientific experiments.Many Scientific Workflow Management Systems use High Performance Computingenvironments to parallelize the execution of these activities in large-scale workflows. Datadistribution; control; and optimizing the parallel execution of these activities can be acomplex task due to scalability of involved resources. This paper presents DEW; a data andactivity distribution mechanism for a parallel workflow execution engine. DEW is based on ahierarchical P2P network that enables distributed control in workflow execution usingdistributed disk and in the presence of high occurrence of churn events. Resumo. Workflowscientíficos são compostos de atividades que modelam experimentos científicos. VáriosSistemas de Gerência de Workflows Científicos fazem uso de ambientes de …,VII e-Science,2013,1
Recomendações para fragmentação horizontal de bases de dados XML.,Tatiane Lima da Silva; Fernanda Araujo Baião; Jonice de Oliveira Sampaio; Marta Mattoso; Vanessa Braganholo,Resumo. A grande quantidade de dados XML disponíveis na Web e dentro dasorganizações traz consigo um grande desafio no processamento de consultas sobreambientes distribuídos. Surge então a necessidade da aplicação de técnicas que permitamum processamento de consultas mais eficiente. Neste sentido; técnicas de fragmentação dedados e processamento paralelo de consultas sobre bases de dados distribuídas têm sidoadotadas. No entanto; a forma adequada para a geração de fragmentos XML não está bemdefinida na literatura. Há muitas definições de fragmentos XML; mas poucas propostas sãoconcentradas em como usar essas definições para realmente fragmentar uma base dedados (isso é chamado de projeto de fragmentação). Inspirado pelos modelos relacionais eorientado a objetos; que têm metodologias sólidas para o projeto de fragmentação de …,SBBD (Short Papers),2012,1
Heurísticas para Controle de Execução de Atividades de Workflows Científicos na Nuvem,Flavio Costa; D Oliveira; M Mattoso,*,Anais do Workshop de Teses e Dissertações em bancos de Dados-SBBD 2011,2011,1
Adaptive Virtual Partitioning: Further Developments,Alexandre AB Lima; Marta Mattoso; Patrick Valduriez,*,Journal of Information and Data Management,2010,1
Captura de Metadados de Proveniência para Workflows Científicos em Nuvens Computacionais.,Carlos Eduardo Paulino Silva; Daniel de Oliveira; Sérgio Manuel Serra da Cruz; Maria Luiza Machado Campos; Marta Mattoso,*,SBBD (Posters),2010,1
Modeling parallel bioinformatics workflows using MapReduce,Daniel Vega; D Oliveira; Eduardo Ogasawara; Alexandre AB Lima; Marta Mattoso,Abstract. Scientific workflows are used to model scientific experiments. Due to theexploratory characteristic of scientific experiments; it may be necessary to try many largeinput datasets to execute the workflow and produce results. These results may be composedby a large volume of data to be analyzed and validated. This exploratory process usuallydemands high performance computing environments and parallel techniques. Manyscientific workflows are candidates to benefit from MapReduce; a framework for large-scaleparallel applications. However; existing Scientific Workflow Management Systems (SWfMS)do not provide support to run scientific workflows based on MapReduce. This paperpresents an integration between Hadoop and VisTrails SWfMS to run in parallel abioinformatics experiment using Amazon EC2 cloud infrastructure.,International Workshop on Challenges in e-Science-SBAC; Petrópolis; RJ-Brazil,2010,1
MF-Ontology: an Ontology for the Text Mining Domain,Daniel de Oliveira; Fernanda Baião; Marta Mattoso,Text mining (TM) has emerged as a definitive technique for knowledge acquisition from text.The TM process is based on several phases that prepare the text for mining; process thetext; and analyze the results. Effective and efficient use of the combination of TM algorithmsand techniques is a challenge. Most of the research is focused on developing new datastructures; algorithms and methods to achieve that. However; the TM process is still lackingof modeling support. The TM analyst faces many options when modeling a TM process. Forinstance; the analyst needs to choose the most effective solution to extract the desiredknowledge. This is a complex decision involving choices for each one of the TM processphases where many algorithms and implementations are available for composition andseveral parameters must be tuned. This scenario tends to be chaotic and each time a new …,*,2009,1
A conception process for abstract workflows: an example on deep water oil exploitation domain,Wallace Pereira; Eduardo Ogasawara; Daniel de Oliveira; Fernando Chirigati; Fabrício Correa; Breno Jacob; Ismael Santos; Guilherme H Travassos; Marta Mattoso,Abstract: Experimentation is one of the ways used to support theories based on a scientificmethod. In-silico experiments are highly dependent of massive use of computationalresources to execute their simulations. One way to use in-silico experiments is through theuse of scientific work-flows. It is a model that represents the flow of programs; services anddata usually orchestrated to support a simulation. Scientific workflows are executed inengines called Scientific Workflow Management Systems (SWfMS); which are responsiblefor enacting; controlling and monitoring the workflow. Each one of the scientific workflowswithin an experiment follows specific phases regarding composition; execution and analysis.Usually; when conducting a scientific experiment; the first phase to be considered is calledComposition. One important sub-phase is the Conception; which is responsible for setting …,*,2009,1
Processamento de Alto Desempenho em Consultas sobre Bases de Dados Geoestatísticas Usando Replicação Parcial.,Melissa Paes; Alexandre AB Lima; Marta Mattoso,Abstract. ParGRES is an opensource database cluster middleware for high performanceOLAP query processing. Using query parallelism; it significantly speeds up the execution ofheavyweight queries; typical from OLAP applications. We evaluate ParGRES  performanceon BME; a geostatistical OLAP database developed by IBGE. These experiments wereperformed using partially replicated database and have focused on queries in isolation; onconcurrent queries and on streams of queries. We obtained almost always superlinearspeedup on queries and processing time reducing. These results make ParGRES a low costalternative solution for OLAP applications particularly the ones designed by governmentalinstitutions. Resumo. O ParGRES é uma camada intermediária de software (de códigoaberto) entre um banco de dados e uma aplicação cliente; provendo paralelismo inter e …,SBBD,2009,1
High Performance Computing for Computational Science-VECPAR 2008: 8th International Conference; Toulouse; France; June 24-27; 2008. Revised Selected Pap...,José M Laginha M Palma; Patrick Amestoy; Michel Daydé; Marta Mattoso; Joao Correira Lopes,This book constitutes the thoroughly refereed post-conference proceedings of the 8thInternational Conference on High Performance Computing for Computational Science;VECPAR 2008; held in Toulouse; France; in June 2008. The 51 revised full paperspresented together with the abstract of a surveying and look-ahead talk were carefullyreviewed and selected from 73 submissions. The papers are organized in topical sectionson parallel and distributed computing; cluster and grid computing; problem solvingenvironment and data centric; numerical methods; linear algebra; computing in geosciencesand biosciences; imaging and graphics; computing for aerospace and engineering; andhigh-performance data management in grid environments.,*,2008,1
RL-based Scheduling Strategies in Actual Grid Environments,Bernardo Costa; Inês Dutra; Marta Mattoso,In this work; we study the behaviour of different resource scheduling strategies when doingjob orchestration in grid environments. We empirically demonstrate that schedulingstrategies based on Reinforcement Learning are a good choice to improve the overallperformance of grid applications and resource utilization.,Parallel and Distributed Processing with Applications; 2008. ISPA'08. International Symposium on,2008,1
DWMiner: A tool for mining frequent item sets efficiently in data warehouses,Bruno Kinder Almentero; Alexandre Gonçalves Evsukoff; Marta Mattoso,Abstract This work presents DWMiner; an association rules efficient mining tool to processdata directly over a relational DBMS data warehouse. DWMiner executes the Apriorialgorithm as SQL queries in parallel; using a database PC Cluster middleware developedfor SQL query optimization in OLAP applications. DWMiner combines intra-and inter-queryparallelism in order to reduce the total time needed to find frequent item sets directly from adata warehouse. DWMiner was tested using the BMS-Web-View1 database from KDD-Cup2000 and obtained linear and super-linear speedups.,International Conference on High Performance Computing for Computational Science,2006,1
A comparison of Free Software Web Portals♦,Vanessa P Braganholo; Marta Mattoso,Abstract. One of the requirements for a given software system to be considered FreeSoftware is to make its source code widely available to the user community. An efficient wayof doing so is by hosting the system into a web portal. There are several portals that addressthis issue; offering free hosting to free software (and/or open source) projects. In this paper;we analyze some of the existing portals and evaluate them according to a set of priorestablished items. The goal of this analysis is to help users to choose the portal that best fitshis projects' needs.,6th International Workshop on Free Software,2005,1
XAloc–Uma ferramenta para avaliar algoritmos de alocação de dados,Matheus Wildemberg; M PAULA; Fernanda Baião; Marta Mattoso,*,Simpósio Brasileiro de Banco de Dados; SBBD-Demos,2004,1
A Cost Model for the Evaluation of Path Expressions in Distributed Object Databases,Gabriela Ruberg; Fernanda Baião; Marta Mattoso,Abstract. Path expression processing optimization is a central and difficult issue in currentquery languages. Efficient evaluation of path expressions in a distributed context involveschoosing among several query-processing strategies. The great variety of strategies comesfrom the rich semantics involved in object-based data models and the complexity added bythe distribution. This work presents a new cost model for object query processors andaddresses relevant issues; which are ignored or relaxed in other works in the literature; suchas the selectivity of the path expression; the sharing degree of the referenced objects; thepartial participation of the referenced collection in the relationship; object clustering in thedisk and the distribution of the database objects among the nodes of a network. Theseissues allowed us to reduce the error rate between our simulation results and the …,submitted for publication,2001,1
Uma Ferramenta para Gerência de Metadados em Arquiteturas Baseadas em Mediadores.,Yolanda Larraona; Ana Maria de Carvalho Moura; Marta Mattoso,Abstract The need to integrate different Database Management Systems (DBMSs) motivatedthe specification of many architectures to enable the interoperability betweenheterogeneous DBMSs; among which we can include those based on mediators. In thispaper we present a Meta-Schema Manager; specified in the context of an architecture basedon mediators; which can be integrated to other architectures following this same approach.Its main goal is to manage metadata that contain the description of the architecturecomponents. This description is based on an object model; allowing the mediator to know inadvance all the DBMSs participating of the architecture; as well as the available wrappersassociated to each DBMS. This model is used to integrate available data; hence solvingsemantic conflicts. The main innovation of this model is the use of metadata in the …,SBBD,1999,1
DynAdapt: Alterações na Definição de Atividades de Workflows Científicos em Tempo de Execução,Igor de Araújo dos Santos; Jonas Dias; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Abstract. Scientific workflows can represent experiments based on computer simulations.Generally; workflows executed in parallel are time consuming and manage a large amountof data. Such characteristics may make the exploratory process of the experiment moredifficult or very expensive. In this scenario; it is necessary to handle workflows with dynamicaspects; which allow for changes in workflow definitions during runtime. This way; this articleproposes an approach to allow for changes in the definition of the activities of the workflowduring runtime; according to criteria defined by scientists. Resumo. Os workflows científicossão capazes de representar experimentos baseados em simulações computacionais. Emgeral; workflows executados em paralelo manipulam uma grande massa de dados edemandam um elevado tempo de execução. Tais características podem dificultar ou …,*,*,1
BioWorkbench: A High-Performance Framework for Managing and Analyzing Bioinformatics Experiments,Maria Luiza Mondelli; Thiago Magalhães; Guilherme Loss; Michael Wilde; Ian Foster; Marta Mattoso; Daniel S Katz; Helio JC Barbosa; Ana Tereza R Vasconcelos; Kary Ocaña; Luiz MR Gadelha Jr,Abstract: Advances in sequencing techniques have led to exponential growth in biologicaldata; demanding the development of large-scale bioinformatics experiments. Because theseexperiments are computation-and data-intensive; they require high-performance computing(HPC) techniques and can benefit from specialized technologies such as Scientific WorkflowManagement Systems (SWfMS) and databases. In this work; we present BioWorkbench; aframework for managing and analyzing bioinformatics experiments. This frameworkautomatically collects provenance data; including both performance data from workflowexecution and data from the scientific domain of the workflow application. Provenance datacan be analyzed through a web application that abstracts a set of queries to the provenancedatabase; simplifying access to provenance information. We evaluate BioWorkbench …,arXiv preprint arXiv:1801.03915,2018,*
In situ visualization and data analysis for turbidity currents simulation,Jose J Camata; Vitor Silva; Patrick Valduriez; Marta Mattoso; Alvaro LGA Coutinho,Abstract Turbidity currents are underflows responsible for sediment deposits that generategeological formations of interest for the oil and gas industry. LibMesh-sedimentation is anapplication built upon the libMesh library to simulate turbidity currents. In this work; wepresent the integration of libMesh-sedimentation with in situ visualization and in transit dataanalysis tools. DfAnalyzer is a solution based on provenance data to extract and relatestrategic simulation data in transit from multiple data for online queries. We integrate libMesh-sedimentation and ParaView Catalyst to perform in situ data analysis and visualization. Wepresent a parallel performance analysis for two turbidity currents simulations showing thatthe overhead for both in situ visualization and in transit data analysis is negligible. We showthat our tools enable monitoring the sediments appearance at runtime and steer the …,Computers & Geosciences,2018,*
Data reduction in scientific workflows using provenance monitoring and user steering,Renan Souza; Vitor Silva; Alvaro LGA Coutinho; Patrick Valduriez; Marta Mattoso,Abstract Scientific workflows need to be iteratively; and often interactively; executed for largeinput datasets. Reducing data from input datasets is a powerful way to reduce overallexecution time in such workflows. When this is accomplished online (ie; without requiring theuser to stop execution to reduce the data; and then resume); it can save much time.However; determining which subsets of the input data should be removed becomes a majorproblem. A related problem is to guarantee that the workflow system will maintain executionand data consistent with the reduction. Keeping track of how users interact with the workflowis essential for data provenance purposes. In this paper; we adopt the “human-in-the-loop”approach; which enables users to steer the running workflow and reduce subsets fromdatasets online. We propose an adaptive workflow monitoring approach that combines …,Future Generation Computer Systems,2017,*
Efficient Scheduling of Scientific Workflows using Hot Metadata in a Multisite Cloud,Ji Liu; Luis Pineda-Morales; Esther Pacitti; Alexandru Costan; Patrick Valduriez; Gabriel Antoniu; Marta Mattoso,Large-scale scientific applications are often expressed as scientific workflows (SWfs) thathelp defining data processing jobs and dependencies between jobs' activities. Several SWfshave huge storage and computation requirements; and so they need to be processed inmultiple (cloud-federated) datacenters. It has been shown that efficient metadata handlingplays a key role in the performance of computing systems. However; most of this evidenceconcern only single-site; HPC systems to date. In addition; the efficient scheduling of tasksamong different data centers is critical to the SWf execution. In this paper; we present ahybrid distributed model and architecture; using hot metadata (frequently accessedmetadata) for efficient SWf scheduling in a multisite cloud. We couple our model with ascientific workflow management system (SWfMS) to validate and tune its applicability to …,BDA: Conférence sur la Gestion de Données—Principes; Technologies et Applications,2017,*
Tracking of Online Parameter Fine-tuning in Scientific Workflows,Renan Souza; Vítor Silva; José Camata; Alvaro Coutinho; Patrick Valduriez; Marta Mattoso,EXTENDED ABSTRACT In typical large-scale scientific applications; several parameters ofcomplex computational models have to be predefined in a simulation; each with a widerange of possible values. Listing all possible combinations of parameters and exhaustivelytrying them all is nearly impossible even in extreme-scale High Performance Computing(HPC). There may be a huge number of possible combinations and processing each onemay take several hours or days; making the whole computation last for weeks or months.Typically; after the initial set ups; the scientist starts the computation and occasionally fine-tunes specific parameters based on intermediate result analysis. The term" human-in-the-loop" is used when computational scientists can actively participate in the computationalprocess. Specific adaptations can generate an important improvement on performance …,Workflows in Support of Large-Scale Science (WORKS); in conjunction with ACM/IEEE Supercomputing.,2017,*
Spark Scalability Analysis in a Scientific Workflow,Renan Souza; Vitor Silva; Pedro Miranda; Alexandre Lima; Patrick Valduriez; Marta Mattoso,Spark is being successfully used for big data parallel processing in many business domains(social media; finance; retail). Spark's scalability; usability; and large user community havemotivated developers from scientific domains (bioinformatics; oil and gas; astronomy) to tryit. However; scientific applications' profile; eg; black-box programs and intense file writes;differs from traditional business workflows; which may affect its scalability. We present ascalability analysis of Spark in a real case-study in Oil and Gas domain. We exploreworkloads on a 936-cores HPC cluster processing 330 GB of scientific data. We show that itscales very well when running long-lasting scientific tasks; but its performance is lower forshort-duration tasks.,SBBD 2017: 32th Brazilian Symposium on Databases,2017,*
Clouds and Reproducibility: A Way to Go to Scientific Experiments?,Ary HM de Oliveira; Daniel de Oliveira; Marta Mattoso,Abstract Scientific research is supported by computing techniques and tools that allow forgathering; management; analysis; visualization; sharing; and reproduction of scientific dataand its experiments. The simulations performed in this type of research are called in silicoexperiments; and they are commonly composed of several applications that executetraditional algorithms and methods. Reproducibility plays a key role and gives the ability tomake changes in the data and test environment of a scientific experiment to evaluate therobustness of the proposed scientific method. By verifying and validating generated resultsof these experiments; there is an increase in productivity and quality of scientific dataanalysis processes resulting in the improvement of science development and production ofcomplex data in various scientific domains. There are many challenges to enable …,*,2017,*
Applying future Exascale HPC methodologies in the energy sector,José J Camata; José M Cela; Danilo Costa; Alvaro LGA Coutinho; Daniel Fernández-Galisteo; Carmen Jiménez; Vadim Kourdioumov; Marta Mattoso; Rafael Mayo-García; Thomas Miras; José A Moríñigo; Jose Navarro; Daniel de Oliveira; Manuel Rodríguez-Pascual; Vítor Silva; Renan Souza; Patrick Valduriez,Abstract The appliance of new exascale HPC techniques to energy industry simulations isabsolutely needed nowadays. In this sense; the common procedure is to customize thesetechniques to the specific energy sector they are of interest in order to go beyond the state-of-the-art in the required HPC exascale simulations. With this aim; the HPC4E project isdeveloping new exascale methodologies to three different energy sources that are thepresent and the future of energy: wind energy production and design; efficient combustionsystems for biomass-derived fuels (biogas); and exploration geophysics for hydrocarbonreservoirs. In this work; the general exascale advances proposed as part of HPC4E and itsoutcome to specific results in different domains are presented.,*,2016,*
Enhancing Energy Production with Exascale HPC Methods,Rafael Mayo-García; José J Camata; José M Cela; Danilo Costa; Alvaro LGA Coutinho; Daniel Fernández-Galisteo; Carmen Jiménez; Vadim Kourdioumov; Marta Mattoso; Thomas Miras; José A Moríñigo; Jorge Navarro; Philippe OA Navaux; Daniel De Oliveira; Manuel Rodríguez-Pascual; Vítor Silva; Renan Souza; Patrick Valduriez,Abstract High Performance Computing (HPC) resources have become the key actor forachieving more ambitious challenges in many disciplines. In this step beyond; an explosionon the available parallelism and the use of special purpose processors are crucial. Withsuch a goal; the HPC4E project applies new exascale HPC techniques to energy industrysimulations; customizing them if necessary; and going beyond the state-of-the-art in therequired HPC exascale simulations for different energy sources. In this paper; a generaloverview of these methods is presented as well as some specific preliminary results.,Latin American High Performance Computing Conference,2016,*
MONITORANDO EXPERIMENTOS CIENTÃ FICOS EXECUTADOS EM AMBIENTES DE NUVEM DE COMPUTADORES,Julliano Pintas; Daniel de Oliveira; Kary OcaÃ±a; Jonas Dias; Marta Mattoso,Resumo A maioria dos workflows científicos de larga escala apresenta execução de longaduração; tornando inviável para o cientista monitorar o estado da execução durante todo otempo em um terminal. Neste artigo; apresentamos uma nova abordagem paramonitoramento em tempo real de workflows científicos executados em paralelo; baseadoem consultas aos dados de proveniência gerados em tempo real; que identifica eventos pré-configurados e notifica o cientista através de tecnologias de dispositivos móveis e redessociais. A avaliação da solução proposta; chamada SciLightning; foi realizada através domonitoramento da execução em paralelo do workflow de análise filogenética chamadoSciPhy no ambiente de nuvem Amazon EC2 usando a máquina de execução de workflowsem nuvem chamada SciCumulus. A avaliação mostrou que esta nova abordagem é …,REVISTA DE TRABALHOS ACADÊMICOS,2014,*
2014 IEEE 28th International Parallel & Distributed Processing Symposium Workshops Exploring Large Scale Receptor-Ligand Pairs in Molecular Docking Workflow...,Kary Ocaña; Silvia Benza; Daniel De Oliveira; Jonas Dias; Marta Mattoso,Abstract—Computer-aided drug design techniques are important assets in pharmaceuticalindustry because of their support for research and development of new drugs. Moleculardocking (MD) predicts specific compound's binding modes within the active site of targetproteins. Since MD is a timeconsuming process; existing approaches reduce the number ofreceptors or ligands in docking by evaluating only small sets of compounds. This restrictionin the search space reduces the chances to uniformly cover the diverse space of compoundsand misses opportunities to recognize whether new drugs can be identified. Anotherdifficulty with large-scale is analyzing the results; eg browsing all directories manually to findwhich pairs were docked successfully. To address these issues we explored the potential ofdata provenance analysis and parallel processing of SciCumulus; a cloud Scientific …,*,2014,*
Algebraic Dataflows for Big Data Analysis,Dias Jonas; Eduardo Ogasawara; Oliveira Daniel De; Fabio Porto; Patrick Valduriez; Marta Mattoso,Analyzing big data requires the support of dataflows with many activities to extract andexplore relevant information from the data. Recent approaches such as Pig Latin propose ahigh-level language to model such dataflows. However; the dataflow execution is typicallydelegated to a MapReduce implementation such as Hadoop; which does not follow analgebraic approach; thus it cannot take advantage of the optimization opportunities ofPigLatin algebra. In this paper; we propose an approach for big data analysis based onalgebraic workflows; which yields optimization and parallel execution of activities andsupports user steering using provenance queries. We illustrate how a big data processingdataflow can be modeled using the algebra. Through an experimental evaluation using realdatasets and the execution of the dataflow with Chiron; an engine that supports our …,BigData'2013: International Conference on Big Data,2013,*
Distribuição de Bases de Dados de Proveniência na Nuvem.,Edimar Santos; Vanessa Assis; Flavio Costa; Daniel de Oliveira; Marta Mattoso,Resumo. Dados de proveniência no contexto de workflows científicos são peçasfundamentais; pois; por meio deles; os experimentos são passíveis de reprodução evalidação. O histórico da execução dos workflows é fundamental também para a gerênciada execução de novos workflows uma vez que possibilitam às máquinas de workflowrealizar predições sobre desempenho ou custo financeiro de nuvens de computadores.Workflows; com dados em larga escala; executados em nuvens; são com frequênciaalocados em máquinas virtuais distribuídas fisicamente. As soluções existentes coletam osdados de proveniência de forma distribuída e os armazenam de modo centralizado emúnico repositório; após o término da execução do workflow. Além da capacidade dereprodução; dados de proveniência permitem um acompanhamento refinado por parte …,SBBD (Short Papers),2013,*
Using Provenance Analyzers to Improve the Performance of Scientific Workflows in Cloud Environments.,João Carlos de AR Gonçalves; Daniel de Oliveira; Kary ACS Ocaña; Eduardo S Ogasawara; Jonas Dias; Marta Mattoso,Abstract. A major issue during scientific workflow execution is how to manage the largevolume of data to be processed. This issue is even more complex in cloud computing whereall resources are configurable in a pay per use model. A possible solution is to takeadvantage of the exploratory nature of the experiment and adopt filters to reduce data flowbetween activities. During a data exploration evaluation; the scientist may discardsuperfluous data (which is producing results that do not comply with a given quality criteria)produced during the workflow execution; avoiding unnecessary computations in the future.These quality criteria can be evaluated based on provenance and domain-specific data. Weclaim that the final decision on whether to discard superfluous data may become feasibleonly when workflows can be steered by scientists at runtime using provenance data …,SBBD (Short Papers),2012,*
Poster: scientific data parallelism using P2P technique,Jonas Dias; Eduardo Ogasawara; Daniel de Oliveira; Marta Mattoso,Abstract The complexity and the processing time in scientific experiments based oncomputational simulation models bring challenges on the conduction of these experiments.Scientific workflows have being adopted on large-scale science. The intense utilization oflarge volumes of data on these workflows demands parallelism techniques. However;parallelize a workflow requires specific tools and programming skills; which may become ablunder for scientists. To address this issue; this paper proposes Heracles; which is anapproach that makes the workflow parallelization into a more transparent task for scientists.Our approach proposes a fault tolerance and dynamic resource management mechanisminspired on P2P techniques. The purpose of Heracles is to execute activities in parallelwithout asking the scientists to specify the number of nodes involved in the execution and …,Proceedings of the 2011 companion on High Performance Computing Networking; Storage and Analysis Companion,2011,*
Distributed Database Research at COPPE/UFRJ,Marta Mattoso; Vanessa Braganholo; Alexandre AB Lima; Leonardo Murta,Abstract Our group has been working with different aspects of distributed and parallelprocessing of databases in the relational; object-oriented; and XML data models. Classictechniques for distributed design and query processing in relational database systems havebeen revisited to address dynamic issues in high performance computing and flexibilitychallenges of XML documents. More recently; large-scale scientific data combined withprocess activities management have introduced challenges to the database and softwareengineering communities; among several other computer science research areas.Regarding scientific data; challenges are the heterogeneous data formats that encompassrelational; XML; binary; and flat files. Our group has been addressing these challenges bycapitalizing on our extensive experience in distributed data management. Since each …,Journal of Information and Data Management,2011,*
SciCumulus-ECM: Um Serviço de Custos para a Execução de Workflows Científicos em Nuvens Computacionais.,Vitor Viana; Daniel de Oliveira; Eduardo S Ogasawara; Marta Mattoso,Resumo O conceito de computação em nuvem vem se firmando como um novo modelocomputacional que proporciona aos cientistas uma oportunidade de se utilizar diversosrecursos distribuídos para a execução de experimentos científicos. Muitos dos experimentoscientíficos existentes; modelados como workflows científicos; devem controlar a execuçãode atividades que consomem e produzem grandes volumes de dados. Há uma demandapor alto desempenho na execução destes experimentos uma vez que muitas destasatividades são computacionalmente intensivas. As técnicas de paralelismo sãoconsideradas um ponto chave em todo o processo de experimentação. Entretanto;paralelizar um workflow científico em um ambiente de nuvem não é uma tarefa trivial. Umadas tarefas mais complexas é definir a melhor configuração possível do ambiente de …,SBBD (Short Papers),2011,*
SARAVÁ: data sharing for online communities in P2P,Marta Mattoso; Esther Pacitti; Patrick Valduriez; Reza Akbarinia; Vanessa Braganholo; Alexandre AB Lima,This paper describes SARAVÁ; a research project that aims at investigating new challengesin P2P data sharing for online communities. The major advantage of P2P is a completelydecentralized approach to data sharing which does not require centralized administration.Users may be in high numbers and interested in different kinds of collaboration and sharingtheir knowledge; ideas; experiences; etc. Data sources can be in high numbers; fairlyautonomous; ie locally owned and controlled; and highly heterogeneous with differentsemantics and structures. Our project deals with new; decentralized data managementtechniques that scale up while addressing the autonomy; dynamic behavior andheterogeneity of both users and data sources. In this context; we focus on two majorproblems: query processing with uncertain data and management of scientific workflows.,Colloquium of Computation: Brazil/INRIA; Cooperations; Advances and Challenges,2009,*
Due to an explosive increase of XML documents; it is imperative to manage XML data in an XML data warehouse. XML warehousing imposes challenges; which are...,Alexandre AB Lima; Camille Furtado; Patrick Valduriez; Marta Mattoso,Private data sometimes must be made public. A corporation may keep its customer salesdata secret; but reveals totals by sector for marketing reasons. A hospital keeps individualpatient data secret; but might reveal outcome information about the treatment of particularillnesses over time to support epidemiological studies. In these and many other situations;aggregate data or partial data is revealed;...,Distributed and Parallel Databases,2009,*
High Performance Computing for Computational Science-VECPAR 2008 00: 8th International Conference; Toulouse; France; June 24-27; 2008. Revised Selected P...,Patrick R Amestoy; Marta Mattoso,*,*,2008,*
ClusterMiner: High Performance for Data; Text and Web Mining,Marta Mattoso; Nelson Ebecken; Gerson Zaverucha; Alexandre Evsukoff; Fernanda Baião; Myriam Costa; Guilherme Terra,Abstract. Our work addresses a variety of inter-related issues with a focus on providing toolsfor efficiently processing data in mining tasks. We investigate mechanisms to efficientlyaccess high volumes of data; as well as issues on parallel processing of data miningalgorithms. We focus on the development of ClusterMiner; an environment with highperformance tools for improving data; text and web mining using PC clusters. The goal is toimprove performance metrics on data analysis; data sorting and summarization.ClusterMiner explores parallel data access by enhancing data distribution techniques andparallel execution of heavy-weight queries using database clusters. In addition; weinvestigate parallel techniques to improve typical rule-based algorithms under a variety ofscenarios. In ClusterMiner; performance analysis plays an important role in evaluating …,iSys-Revista Brasileira de Sistemas de Informação,2008,*
Preface to the special issue on grid data management,Esther Pacitti; Marta Mattoso; Patrick Valduriez,Initially developed for the scientific community; Grid computing is now breaking into otherimportant fields such as enterprise information systems; thus making data managementmore critical than ever. New Grid applications are characterized with high heterogeneity;high autonomy and large-scale distribution of computing and data resources. Managing andtransparently accessing large numbers of autonomous; heterogeneous data resourcesefficiently is an open problem. Furthermore; different Grids may have different requirementswith respect to autonomy; query expressiveness; efficiency; quality of service; fault-tolerance; security; etc. Thus; different solutions need be investigated; ranging fromextensions of distributed and parallel computing techniques to more decentralized; self-adaptive techniques such as Peer-to-peer (P2P). This special issue on Grid data …,Journal of Grid Computing,2007,*
Experiencing GARSA as a scientific workflow on grids,Sérgio Manuel Serra da Cruz; Fábio José Coutinho da Silva; Alberto MR Dávila; Maria Luiza Machado Campos; Marta Mattoso,Abstract. Bioinformatics experiments are typically composed of programs in pipelinesmanipulating an enormous quantity of data. Managing those experiments brings a set ofchallenges; such as: how to provide interoperability among tools to achieve better and fasterexperimental results; in addition to providing data and semantics to experiments. The bestway of managing those experiments is through workflow management systems (WfMS). Infact; several such systems are found as products; open-source software and prototypes.Even though BPEL is becoming a de facto standard as a workflow definition language withits execution engine; most WfMS for scientific computing provide its own workflow language.Due to the lack of standards; the e-scientist is faced with the challenge of building its ownWfMS or choosing between all available independent systems. In this work we discuss …,BRAZILIAN SYMPOSIUM ON BIOINFORMATICS,2007,*
Open Source Web Portals,Vanessa P Braganholo; Bernardo Miranda; Marta Mattoso,ABSTRACT Open source software is required to be widely available to the user community.To help developers fulfill this requirement; Web portals provide a way to make open sourceprojects public so that the user community has access to their source code; can contribute totheir development; and can interact with the developer team. However; choosing a Webportal is not an easy task. There are several options available; each of them offering a set oftools and features to its users. The goal of this chapter is to analyze a set of existing Webportals (SourceForge. net; Apache; Tigris; ObjectWeb; and Savannah) in the hopes that thiswill help users to choose a hosting site for their projects.,Handbook of Research on Open Source Software: Technological; Economic; and,2007,*
Experiencing data grids,Nicolaas Ruberg; Nelson Kotowski; Amanda Mattos; Luciana Matos; Melissa Machado; Daniel Oliveira; Rafael Monclar; Cláudio Ferraz; Talitta Sanchotene; Vanessa Braganholo,Abstract Many scientific experiments deal with data-intensive applications and theorchestration of computational workflow activities. These can benefit from data parallelismexploited in parallel systems to minimize execution time. Due to its complexity; robustnessand efficiency to exploit data parallelism; grid infrastructures are widely used in some e-Science areas like bioinformatics. Workflow techniques are very important to in-silicobioinformatics experiments; allowing the e-scientist to describe and enact experimentalprocess in a structured; repeatable and verifiable way. The main purpose of this paper is todescribe our experience with Tavena Workbench and PeDRo; which are part of my Gridproject. Taverna is provided with a workflow toolset and enactor; allowing the specification ofprocessing units; data transfer and execution constraints. As a data entry tool; PeDRo …,International Conference on High Performance Computing for Computational Science,2006,*
Second International Workshop on Database Technologies for Handling XML Information on the Web (DataX'06)-Efficiently Processing XML Queries over Fragment...,Alexandre Andrade; Gabriela Ruberg; Fernanda Baiao; Vanessa P Braganholo; Marta Mattoso,*,Lecture Notes in Computer Science,2006,*
On the integration of Text Mining and Database Systems,Eduardo Bezerra; Marta LQ Mattoso; GB Xexéo,Abstract The volume of data in semistructured format; particularly in XML; is growingtremendously in the last few years; resulting in huge databases of news; patent documents;etc. This work proposes enhancements to the interaction between Text Mining applicationsand database systems; by defining a primitive for the classification task in semistructureddata using the XQuery language. This primitive provides the improvement of the integrationbetween algorithms for classification and the database system; and the adaptation of thesealgorithms to handle large volumes of data. For validation purposes; will implement and testthe primitive. We also intend to implement real world applications in order to validate theutility and generality of such primitive.,*,2004,*
Cooperative Information Systems (CoopIS) 2003 International Conference-Processing; Availability; and Archival for Cooperative Systems-Digging Database Statistic...,Nicolaas Rubery; Gabriela Ruberg; Marta Mattoso,*,Lecture Notes in Computer Science,2003,*
Avaliação de Estratégias para o Processamento de Expressões de Caminho.,André O Victor; Marta Mattoso,Resumo Neste trabalho analisamos o comportamento do processamento de expressões decaminho e exploramos a direção ascendente nas estratégias para o seu processamento.Propomos a estratégia—Reverse Partition Merge “(PM-R); que é baseada no algoritmo—Partition Merge “(P (PM)* M). Esta nova estratégia melhora o desempenho no custo de E/Sdo P (PM)* M; mantendo o mesmo custo de CPU da versão original. P (PM)* M éapresentado como o melhor algoritmo de junção proposto na literatura. Foram realizadosexperimentos sobre o benchmark OO7 para mostrar a importância da direção ascendente.Os resultados da estratégia PM-R são comparados aos das estratégias formadas pelo P(PM)* M; pelo algoritmo—Naïve Pointer “e por um algoritmo de junção baseado em valor.Os experimentos mostram novos resultados que não foram avaliados em trabalhos …,SBBD,2002,*
Avaliaç ao de Sistemas de Coerência de Cache em Arquiteturas ODBMS Cliente-Servidor,Carla Osthoff; Cristiana Bentes; Daniel Ariosto; Marta Mattoso; Claudio L Amorim,Resumo Neste artigo; nós avaliamos o desempenho do algoritmo de coerência de cache dedisco baseado em sistema de software de memória distribuıda (DSMIO) nas arquiteturas deSistemas Gerenciadores de Banco de Dados Orientados a Objeto (SGBDOO) cliente-servidor tradicional e cliente-servidor paralelo. A caracterıstica principal do algoritmo deDSMIO é a utilizaçao de um modelo de consistência de memória Lazy Release e umprotocolo de múltiplos escritores para reduzir a quantidade e o tamanho das mensagens decoerência; necessárias para manter um sistema distribuıdo SGBDOO coerente em umcluster de servidores PCs. Avaliamos o desempenho do algoritmo DSMIO utilizando umabase de dados grande e aplicaçoes com diferentes cargas de trabalho e o comparamos emrelaçao ao algoritmo Callback Locking (CBL). Nossos resultados mostram que o …,*,2002,*
Sharing scientif experiments and workflows in environmental applications,Eric Simon; Maria Luiza Campos; François Llirbat; Maria Cláudia Cavalcanti; Marta Mattoso,Environmental applications have been stimulating the cooperation among scientists fromdifferent disciplines. There are many examples where this cooperation takes place throughexchanging scientific resources; such as data; programs and mathematical models. TheLeSelect architecture supports environmental applications; where scientists may share theirdata and programs. We believe that besides programs and data; models; as well asexperiments and workflows are scientific resources that need to be shared in environmentalapplications. Therefore; in this paper we propose an extension to LeSelect architecture thatallows sharing of models; experiments and workflows.,Relatório Técnico NCE,2000,*
Mediators metadata management services: An Implementation using GOA system.,Thaís Saldunbides Brügger; Paulo de Figueiredo Pires; Marta Mattoso,Saldunbides Brügger; Thaís; de Figueiredo Pires; Paulo; and Mattoso; Marta. "Mediators metadatamanagement services: An Implementation using GOA system.." Electronic Journal of SADIO[electronic only] 2.1 (1999): 30-49. <http://eudml.org/doc/230348> … You must be logged into post comments … To embed these notes on your page include the following JavaScript codeon your page where you want the notes to appear … Only the controls for the widget will beshown in your chosen language. Notes will be shown in their authored language … Tells thewidget how many notes to show per page. You can cycle through additional notes using thenext and previous controls … Note: Best practice suggests putting the JavaScript code just beforethe closing </body> tag … You must be logged in to add subjects … You must be logged into use personal lists … To add items to a personal list choose the desired list from the …,Electronic Journal of SADIO [electronic only],1999,*
Análise de Dados Cientıficos: uma Análise Comparativa de Dados de Simulaç oes Computacionais,Thaylon Guedes; Vıtor Silva; José Camata; Marta Mattoso; Daniel de Oliveira,Resumo. Os avanços nas simulaçoes computacionais têm permitido o processamento devolumes de dados cada vez maiores. Para representar as estruturas de dados complexasinerentes de tais simulaç oes; elas sao armazenadas em arquivos de formatosheterogêneos. Carregar tais dados em um SGBD; como o SciDB; para apoiar as análisesdeles se torna uma tarefa complexa; ou mesmo inviável; devido ao seu volume e/ouestrutura. Para evitar esse carregamento; existem abordagens que realizam consultasadaptativas e/ou que indexam os arquivos. Escolher a mais adequada pode nao ser trivial.Neste artigo realizamos uma análise comparativa em termos de desempenho dasabordagens de consulta de dados produzidos por uma simulaç ao em dinâmica de fluıdoscomputacional.,*,*,*
2014 IEEE 10th International Conference on e-Science (e-Science)(2014),Flavio Costa; Daniel de Oliveira; Marta Mattoso,*,*,*,*
A Provenance-based Approach to Resource Discovery in Distributed Workflows,Patricia M Barros; Sérgio Manuel Serra da Cruz; Paulo M Bisch; Maria Luiza M Campos; Marta Mattoso,Abstract. One of the major challenges of in silico experiments consists in exploiting the vastamount of biological data generated by scientific apparatus. Scientific data; programs andworkflows are valuable resources to be exchanged among scientists but difficult to beefficiently managed due to their heterogeneous and distributed nature. Provenance data canease the discovery of these resources. However; keeping track of the execution ofexperiments and capturing provenance data among distributed resources are not simpletasks. Thus; discovering resources in distributed environments is still a challenge. This workpresents an architecture to help the execution of in silico scientific experiments in distributedenvironments. In addition; it captures and stores provenance data of the workflow executionin a repository. The goal of this architecture is to lower the complexity of parallel execution …,*,*,*
MODELING OF SUB-SEA SEDIMENTATION PROCESSES USING A STOCHASTIC MODEL OF GRAVITY CURRENTS,Gabriel M Guerra; Fernando Rochinha; Paulo Paraizo; Jonas Dias; Felipe Horta; Marta Mattoso; Renato Elias; Alvaro LGA Coutinho,A gravity current consists of the flow of one fluid within another due small differences indensity between the fluids. This is a very complex phenomenon and extremely difficult to bestudied in nature [1]. Experimental models are used successfully to explore only someaspects of this kind of currents where their use is limited to reproduce only some effects [2]. Abetter understanding and detailing is one of the goals of geologists. In this sense; numericalmodels can assist to go beyond the current knowledge providing us greater knowledgeabout the dynamics of this currents as example; see [3] and [4]. This does not just involvedetailed mathematical models but also models that take into account the inherentuncertainties of the model parameters. In that sense; the Uncertainty Quantification (UQ)proposes a methodology to assist in this task both to to obtain new knowledge about the …,*,*,*
USER INTERACTION IN UNCERTAINTY QUANTIFICATION ANALYSIS WORKFLOWS,Jonas Dias; Gabriel Guerra; Fernando Rochinha; Alvaro LGA Coutinho; Patrick Valduriez; Marta Mattoso,1 Computer Science; COPPE/Federal University of Rio de Janeiro; PO Box 68511; Rio deJaneiro; RJ Brazil;{jonas; marta@ cos. ufrj. br}; www. cos. ufrj. br 2 Mechanical Engineering;COPPE/Federal University of Rio de Janeiro; PO Box 68503; Rio de Janeiro; RJBrazil;{gguerra; faro@ mecanica. ufrj. br}; www. mecanica. ufrj. br 3 High PerformanceComputing Center and Civil Engineering; COPPE/Federal University of Rio de Janeiro; POBox 68506; Rio de Janeiro; RJ Brazil; alvaro@ nacad. ufrj. br}; www. nacad. ufrj. br4Antenne INRIA; LIRMM; 95 rue de la Galera; 34095 Montpellier Cedex 5; France; patrick.valduriez@ inria. fr; http://www-sop. inria. fr/members/Patrick. Valduriez/,*,*,*
BioSciCumulus: um portal para análise de dados de proveniência em workflows de biologia computacional,Débora Pina; Vinícius Campos; Vítor Silva; Kary Ocaña; Daniel de Oliveira; Marta Mattoso,Resumo. A gerência de experimentos científicos tem sido facilitada por meio de sistemas deworkflows científicos (SWC). No entanto; a análise dos resultados ainda encontradificuldades devido ao volume e a heterogeneidade dos dados gerados. Para auxiliar aanálise dos experimentos; os SWC capturam dados de proveniência que rastreiam osdados da execução do workflow. Ainda assim; a análise por parte do usuário esbarra nadificuldade de conhecimento da linguagem de consultas e da modelagem dos dados deproveniência para realizar a análise. Para apoiar essas questões; este artigo propõe oPortal BioSciCumulus para facilitar a submissão de workflows científicos no domínio dabioinformática em ambientes de Processamento de Alto Desempenho (PAD) e a análise dedados; sem a necessidade de o usuário configurar o ambiente de PAD ou especificar as …,*,*,*
Orientações para orientandos-Uma experiência em BD,Marta Mattoso,Page 1. Orientações para orientandos - Uma experiência em BD Marta Mattoso COPPE –Sistemas Universidade Federal do Rio de Janeiro III Workshop de Teses e Dissertações emBanco de Dados Page 2. Marta Mattoso – WTDBD 2004 COPPE/UFRJ 2 Sumário □ Motivação □Histórico no Brasil □ Pesquisas em Banco de Dados □ Redação da tese □ Exposição orale defesa Page 3. Marta Mattoso – WTDBD 2004 COPPE/UFRJ 3 Aprovação da tese: □ Avaliaçãodo texto da tese □ Avaliação da defesa oral ∎ Ato formal □ Apresentação da solução □Exposição oral Motivação Page 4. Marta Mattoso – WTDBD 2004 COPPE/UFRJ 4 Sumário ✓Motivação • Histórico no Brasil □ Pesquisas em Banco de Dados □ Redação da tese □Exposição oral e defesa Page 5. Marta Mattoso – WTDBD 2004 COPPE/UFRJ 5 Histórico noBrasil - mestrado □ Passado - tese ∎ Trabalho de 3 anos (1 em curso e 2 em tese) …,*,*,*
CONTROLLING PARALLEL ADAPTIVE SPARSE GRID STOCHASTIC COLLOCATION SIMULATIONS WITH WORKFLOWS,Thibault Lavril; Marta Mattoso; Vıtor Silva; Danilo Costa; Fernando A Rochinha; Alvaro LGA Coutinho; Thomas Miras,Abstract. Non-intrusive Uncertainty Quantification (UQ) methods; use a certain number ofruns of a deterministic computational model. For reduction methods; each calculationcorresponds to chosen points of the stochastic input space. The Oil and Gas companies areexamples of the needs in the industry sector where UQ in seismic imaging helps to improvethe decision process. Reverse-time migration (RTM) is a standard algorithm when it comesto draw accurate images of a subsurface. However UQ in RTM is challenging due to thecomputational cost and high dimensional uncertain inputs. To tackle this issue; a frameworkcoupling dimension reduction with an optimized deterministic model and sparse gridstochastic collocation is designed. Statistics of the outputs are estimated from thedeterministic computations; generating a large amount of data and thus requiring careful …,*,*,*
Workflows Científicos com Apoio de Bases de Conhecimento em Tempo Real,Victor S Bursztyn; Jonas Dias; Marta Mattoso,Abstract. One major challenge in large-scale experiments is the analytical capacity tocontrast ongoing results with domain knowledge. We approach this challenge byconstructing a domain-specific knowledge base; which is queried during workflowexecution. We introduce K-Chiron; an integrated solution that combines a state-of-the-artautomatic knowledge base construction (KBC) system to Chiron; a well-establishedworkflow engine. In this work we experiment in the context of Political Sciences to show howKBC may be used to improve human-in-the-loop (HIL) support in scientific experiments.While HIL in traditional domain expert supervision is done offline; in K-Chiron it is doneonline; ie at runtime. We achieve results in less laborious ways; to the point of enabling abreed of experiments that could be unfeasible with traditional HIL. Finally; we show how …,*,*,*
Workshop Organization,Alberto HF Laender; Juliana Freire; Dan Suciu; Mirella M Moro; Vanessa Braganholo; Clodoveu Davis Jr; Marcos André Gonçalves; Francesco Bonchi; Angela Bonifati; Andrea Calì; Sara Cohen; Isabel Cruz; Wolfgang Gatterbauer; Boris Glavic; Claudio Gutierrez; Solmaz Kolahi; Dongwon Lee; Domenico Lembo; Marta Mattoso; Regina Motz; Frank Neven; Rachel Pottinger; Vibhor Rastogi; Altigran S da Silva; Cristina Sirangelo; Divesh Srivastava; Julia Stoyanovich; David Toman; Alejandro Vaisman; Stijn Vansummeren; Ke Yi; Daniel Oliveira,The Alberto Mendelzon International Workshop on Foundations of Data Management (AMW2012) held in Ouro Preto; Brazil; on June 27-30; 2012; is the sixth workshop of a serieswhich started in 2006; as part on an initiative of the Latin American community ofresearchers in data management to honor the memory of our friend; colleague and mentorAlberto Mendelzon. The AMW series has been a venue for high-quality research onfoundational aspects of data management and it has helped foster and solidify the researchin this area throughout Latin America. This event; as the previous ones; has encouraged theparticipation of Latin American graduate students and includes activities specially designedfor them. In addition; with sponsorship from the VLDB Endowment; travel grants have beenprovided for students to attend the event. The proceedings of the workshop consist of 14 …,*,*,*
Execução de Workflows Científicos de Bioinformática na Nuvem: Experiências e Desafios1,Silvia Benza; Kary ACS Ocaña; Marta Mattoso,Resumo. O uso de workflows em nuvens de computadores para experimentos debioinformática em larga escala permite execuções em paralelo e dar apoio à gerência dagrande quantidade de dados biológicos. Entretanto; configurar o experimento na nuvemrequer um conhecimento do comportamento dos componentes do experimento. Nesteartigo apresentam-se as características do perfil de execução de vários workflows debioinformática para servir de guia sobre o uso; acoplamento e permitir a avalição debenefícios do desenvolvimento desta metodologia de execução na nuvem.,*,*,*
ANALYSIS OF FLOW-INDUCED VIBRATION MODEL UNDER UNCERTAINTIES USING AN ITERATIVE WORKFLOW,Gabriel M Guerra; Jonas F Dias; Daniel Olivera; Eduardo Ogasawara; Marta Mattoso; Alvaro Coutinho; Fernando A Rochinha,Abstract. Numerical simulations play a key role to understand complex Engineeringproblems. Increasingly sophisticated and realistic systems are analyzed as result of theevolution of computing capacity. This not only increased the expectations of solving practicalproblems; but has created a need to deal with errors and uncertainties inherent in themodels. The Uncertainty Quantification (UQ) proposes a methodology to assist the analystsin the task of determining the validity of numerical simulations systematically; aiming atgiving greater credibility to the results. On other hand; this kind of analysis requires intensivecomputation and a lot of data management. In this context; the techniques andmethodologies of Scientific Workflow (SC) can improve the management of thesesimulations. The objective of this work is to use a SC engine to provide an approach for …,*,*,*
Instructions for Authors of SBC Conferences Papers and Abstracts,Marta Mattoso,Resumo. Este meta-relatório descreve o estilo a ser usado na confecção do relatório finalda disciplina COS111; adotando o estilo da SBC (Sociedade Brasileira de Computação). Ésolicitada a escrita de resumo. O autor deve tomar cuidado para que o resumo nãoultrapasse 10 linhas; sendo que devem estar na primeira página do relatório; com margensmenores que o relatório.,*,*,*
Computação em Nuvem para Ciência: o Papel Fundamental da Área de Bancos de Dados,Daniel de Oliveira; Marta Mattoso,*,*,*,*
Aprendizado por Reforço aplicado a escalonamento em Grids,Bernardo Fortunato Costa; Inês Dutra; Marta Mattoso,Resumo Aprendizado por reforço é uma técnica simples que possui aplicaçao em váriasáreas. Um ambiente real de grid; em geral dinâmico e heterogêneo; oferece um ambienteinteressante para sua aplicaçao. Neste trabalho; utilizamos esta técnica para classificar osnós disponıveis em um grid; dando suporte assim a dois algoritmos de escalonamento; AGe MQD. Um ambiente de grid real foi montado e experimentos foram realizados com estesdois algoritmos; de maneira a verificar seu impacto em um ambiente real; com e sem apresença de reescalonamento.,*,*,*
Explorando Conceitos e Mecanismos de Memória Compartilhada Distribuída em Entrada/Saída Paralela,Carla Osthoff; Ricardo Bianchini; Cristiana Seidel; Marta Mattoso; Claudio L Amorim,Parallel applications from scveral arcas; such as scientific computing and commercialdatabascs; require high-performance input/output (110) systems. This papcr proposcs theexploitation of software-bascd distributed shared-memory (software DSM) concepts andmechanisms to optimizc disk caching and; as a rcsult; substantially improvc the 110performance of parallel systems. More specifically; the main contribution of the paper is a setof mechanisms that allow us:(a) to move the coherence of disk data to the main mcmorylevei;(b) to utilize a relaxed consistency model for the disk data acccsscs; and (c) to savedisk cache space. In order to evaluate our ideas; we are currently implementing lhe DSMIOsystem for a prototype parallel database manager using the IBM-SP multicomputer system.Our preliminary rcsults show that the database benchmarks that benefit the most from …,*,*,*
Heurísticas para Controle de Execução de Atividades de Workflows Científicos na Nuvem,Flavio da Silva Costa; Marta Mattoso; Daniel de Oliveira,Resumo. Neste artigo apresentamos estratégias que visam a melhora da eficácia daexecução de workflows científicos em ambientes de nuvens computacionais. Em função dacomplexidade do escalonamento de atividades computacionais nesses ambientes etambém da volatilidade do ambiente; onde máquinas virtuais podem falhar a qualquermomento; buscamos inspiração nas estratégias já consolidadas dos Sistemas de Gerênciade Bancos de Dados (SGBDs); para a criação de um mecanismo baseado em heurísticaspara o monitoramento e re-execução das atividades de um workflow científico. Estasheurísticas vêm sendo desenvolvidas em uma ferramenta capaz de tomar ações no sentidode não permitir que a execução do workflow científico falhe; independente das falhas queocorram no ambiente. Para construção das heurísticas tomamos como fonte de …,*,*,*
Definição de diretrizes para fragmentação horizontal de bases de dados XML,Tatiane Lima da Silva; Fernanda Baião; Jonice de Oliveira Sampaio; Marta Mattoso; Vanessa Braganholo,Resumo. Em contrapartida ao cenário do ambiente relacional; documentos XML deixaramde ser utilizados apenas para troca de dados; e se tornaram um importante formato derepresentação de dados; permitindo o desenvolvimento de aplicações web flexíveis;manipulação de dados de múltiplas aplicações; entre outros. Este fato faz surgir anecessidade de desenvolvimento de metodologias para processamento eficiente deconsultas sobre dados XML. Aproveitando as ideias de fragmentação e distribuiçãopropostas para o modelo relacional e orientado a objetos; vários trabalhos na literatura têmfocado em processamento de consultas XML em ambientes distribuídos e também nacriação de técnicas de fragmentação no que diz respeito ao formato dos fragmentos e osalgoritmos que os formam. Sendo assim; o objetivo desse trabalho é apresentar …,*,*,*
Consulta a bases XML distribuídas em P2P,Carla Amaral de S Rodrigues; Júlia Ferreira de Almeida; Vanessa Braganholo; Marta Mattoso,Abstract. The large volume of XML data available on the Web and organizations bringsattention to the problem of distributed query processing. This work presents an approach toXML distributed query processing over P2P networks. The main goal is to decrease thelimitations of structured distributed environments; in order to minimize query processing andcommunication costs; while keeping the P2P flexibility. Resumo. O grande volume de dadosXML disponível na Web e nas organizações faz com que o problema de processamento deconsultas distribuídas venha recebendo bastante atenção. Neste trabalho apresentamosuma abordagem para o processamento de consultas XML distribuídas sobre redes P2P. Oprincipal objetivo é mitigar as limitações de ambientes distribuídos estruturados; a fim deminimizar os custos com processamento das consultas e comunicação e ao mesmo …,*,*,*
Controles de Fluxo Explícitos em Workflows Científicos,Sérgio Manuel Serra da Cruz; Fernando Seabra Chirigati; Rafael Dahis; Maria Luiza M Campos; Marta Mattoso,Abstract. Scientific experiments often involve cooperation between large scale computingand data resources. Workflow management systems (WfMS) are emerging as a key elementto help scientists to prototype and execute experiments to accelerate the scientificdiscoveries. However; even though scientific workflows have been widely labeled as data-centered; they do require some control-flow to design the steps of the experiment–but; thesemodules are not available in the majority of WfMS. When available they are veryheterogeneous. We propose a package of generic control-flow modules independent of theWfMS execution-machine language. Our goal is to provide a meta-workflow specificationwhere control can be designed and executed or mapped to different workflow engines. Wepresent the incorporation of control-flow modules based on workflow patterns to the …,*,*,*
WebTransact: A Framework for Building Reliable Web Services Compositions,Paulo F Pires; Marta Mattoso; Mário RF Benevides,*,*,*,*
Parallel Processing Evaluation of Path Expressions,André Victor; Flávio Tavares; Marta Mattoso,Parallel and distributed processing are alternatives to optimize queries in DatabaseSystems. In this work different alternatives for parallel query processing were implementedand evaluated. This evaluation aims at analyzing the potential for parallel processing ofthese query strategies and providing heuristics to query optimizers. The experiments weremade with an IBM SP/2 parallel machine. Performance evaluation used the datasets andqueries specified by the OO7 benchmark. The results indicated the best query executionstrategy for different path expressions analyzed. The tests also showed a significant parallelpotential for the backward; also known as pointer-based join; execution strategy.Nevertheless; the forward execution strategy; also known as naive pointer chasing; hasproven its effectiveness when objects from a small collection point to objects of a large …,*,*,*
VTPortal: a Scientific Community Web Portal for Reusable Workflows,Sérgio Manuel Serra da Cruz; Alexandre Ribeiro; Marta Mattoso,Managing large scientific experiments is a complex research challenge due to the amount ofscientific resources to be managed. Science portals are a way to simplify this task byaggregating data from different sources and by providing a set of pre-designed analyses.However; such portals are often built manually; and are not flexible enough to supportsharing; reusing and executing workflows enacted by SWfMS like VisTrails. In this paper wedescribe VTPortal; a science portal that combines a set of tools and an infrastructure forproviding a collaborative environment for scientists. VTPortal takes into account therequirements of computational scientists; such as accessing high-performance computersand manipulating large amounts of data. We describe our efforts on implementing such asystem for projects with different needs at COPPE/UFRJ.,*,*,*
BioProvenance: Um framework para a proveniência de dados aplicado à Bioinformática,Ricardo Balbi; Paulo Pires; Marta Mattoso,*,*,*,*
Ariane: Um Mecanismo de Percepção em Bases de Dados Compartilhadas,Vaninha Vieira; Marco AS Mangan; Cláudia Werner; Marta Mattoso,Abstract. Awareness is an essential requirement in collaborative activities. In general;mechanisms that provide awareness information in groupware system are implemented in atightly coupled way; which is prejudicial to reuse of awareness solutions. People interactiongenerally occurs through shared artifacts manipulation. Database systems are commonlyused to persist these artifacts along many interactions. Therefore; monitoring changes inpersistent artifacts aids awareness about group activities. This paper presents Ariane; anawareness mechanism that manipulates changes in artifacts persisted in shared databases.The main contribution of Ariane is its independence of a particular groupware; DBMS or datamodel. This generality is achieved through the use of technologies that provide transparentpersistence; and aspect programming that allows the service to be coupled to any system …,*,*,*
Uma Estratégia para a Gerência de Dados de Workflows Científicos no Contexto da Bioinformática,Amanda Mattos Marta Mattoso,Resumo. A gerência dos dados envolvidos nos experimentos científicos é uma tarefa árduapara os Sistemas de Gerência de Workflows Científicos; pois cada domínio de aplicaçãocientífica; como bioinformática ou geologia; por exemplo; possui esquemas; metadados ouontologias; mais adequados ao seu contexto. Os esquemas de dados propostos pelosSistemas de Gerência de Workflows Científicos (SGWfCs) são; em geral; simples e nãosubstituem os esquemas de dados dos domínios de aplicação; que por sua vez; não sãofacilmente incorporados aos SGWfCs. A estratégia proposta considera as soluçõesdesenvolvidas para definição; execução e proveniência de dados de workflows pelosprincipais SGWfCs e visa a propor um serviço a ser acoplado a um desses SGWfCspossibilitando a definição do que de fato deve ser armazenado e em qual esquema de …,*,*,*
Components search and retrieval in software reusable environments (in Portuguese),RMM Braga; RMM Braga; C Werner; M Mattoso,*,details: Ph. D.; UFRJ–Federal University of Rio de Janeiro; Brazil,*,*
ARAXA: an object-relational approach to store active XML documents,Cláudio Ananias Ferraz; Vanessa P Braganholo; Marta Mattoso,Abstract. Active XML (AXML) documents combine extensional XML data with intentionaldata defined through Web service calls. The dynamic properties of these documents posechallenges to both storage and data materialization techniques. We present ARAXA; a non-intrusive approach to store AXML documents. It takes advantage of complex objects fromobject-relational DBMS to represent both extensional and intentional data. By using a DBMSwe benefit from efficient storage tools and query engine. We have defined a storagemechanism with a methodology to materialize AXML documents at query time. We have alsoimplemented a prototype of ARAXA. Our experimental results show that our approach isscalable and extensible.,*,*,*
Uma infra-estrutura de Reutilização baseada em Modelos de Domínio,Regina Maria Maciel Braga; Claudia Maria Lima Werner; Marta Mattoso,*,*,*,*
