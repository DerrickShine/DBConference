Advances in smartphone-based point-of-care diagnostics,Xiayu Xu; Altug Akay; Huilin Wei; ShuQi Wang; Belinda Pingguan-Murphy; Björn-Erik Erlandsson; XiuJun Li; WonGu Lee; Jie Hu; Lin Wang; Feng Xu,Point-of-care (POC) diagnostics is playing an increasingly important role in public health;environmental monitoring; and food safety analysis. Smartphones; alone or in conjunctionwith add-on devices; have shown great capability of data collection; analysis; display; andtransmission; making them popular in POC diagnostics. In this article; the state-of-the-artadvances in smartphone-based POC diagnostic technologies and their applications in thepast few years are outlined; ranging from in vivo tests that use smartphone's built-in/externalsensors to detect biological signals to in vitro tests that involves complicated biochemicalreactions. Novel techniques are illustrated by a number of attractive examples; followed by abrief discussion of the smartphone's role in telemedicine. The challenges and perspectivesof smartphone-based POC diagnostics are also provided.,Proceedings of the IEEE,2015,59
End-to-end reinforcement learning of dialogue agents for information access,Bhuwan Dhingra; Lihong Li; Xiujun Li; Jianfeng Gao; Yun-Nung Chen; Faisal Ahmed; Li Deng,Abstract: This paper proposes\emph {KB-InfoBot}---a dialogue agent that provides users withan entity from a knowledge base (KB) by interactively asking for its attributes. Allcomponents of the KB-InfoBot are trained in an end-to-end fashion using reinforcementlearning. Goal-oriented dialogue systems typically need to interact with an externaldatabase to access real-world knowledge (eg movies playing in a city). Previous systemsachieved this by issuing a symbolic query to the database and adding retrieved results to thedialogue state. However; such symbolic operations break the differentiability of the systemand prevent end-to-end training of neural dialogue agents. In this paper; we address thislimitation by replacing symbolic queries with an induced" soft" posterior distribution over theKB that indicates which entities the user is interested in. We also provide a modified …,arXiv preprint arXiv:1609.00777,2016,26
End-to-end reinforcement learning of dialogue agents for information access,Bhuwan Dhingra; Lihong Li; Xiujun Li; Jianfeng Gao; Yun-Nung Chen; Faisal Ahmed; Li Deng,Abstract: This paper proposes\emph {KB-InfoBot}---a dialogue agent that provides users withan entity from a knowledge base (KB) by interactively asking for its attributes. Allcomponents of the KB-InfoBot are trained in an end-to-end fashion using reinforcementlearning. Goal-oriented dialogue systems typically need to interact with an externaldatabase to access real-world knowledge (eg movies playing in a city). Previous systemsachieved this by issuing a symbolic query to the database and adding retrieved results to thedialogue state. However; such symbolic operations break the differentiability of the systemand prevent end-to-end training of neural dialogue agents. In this paper; we address thislimitation by replacing symbolic queries with an induced" soft" posterior distribution over theKB that indicates which entities the user is interested in. We also provide a modified …,arXiv preprint arXiv:1609.00777,2016,26
End-to-End Task-Completion Neural Dialogue Systems,Xiujun Li; Yun-Nung Chen; Lihong Li; Jianfeng Gao; Asli Celikyilmaz,Abstract: This paper presents an end-to-end learning framework for task-completion neuraldialogue systems; which leverages supervised and reinforcement learning with variousdeep-learning models. The system is able to interface with a structured database; andinteract with users for assisting them to access information and complete tasks such asbooking movie tickets. Our experiments in a movie-ticket booking domain show theproposed system outperforms a modular-based dialogue system and is more robust to noiseproduced by other components in the system. Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs. AI) Cite as: arXiv: 1703.01008 [cs. CL](or arXiv: 1703.01008 v1[cs. CL] for this version) Submission history From: Yun-Nung Chen [view email][v1] Fri; 3 Mar2017 01: 29: 11 GMT (1081kb; D),The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017),2017,18
Efficient exploration for dialogue policy learning with BBQ networks & replay buffer spiking,Zachary C Lipton; Jianfeng Gao; Lihong Li; Xiujun Li; Faisal Ahmed; Li Deng,Abstract: When rewards are sparse and action spaces large; Q-learning with $\epsilon $-greedy exploration can be inefficient. This poses problems for otherwise promisingapplications such as task-oriented dialogue systems; where the primary reward signal;indicating successful completion of a task; requires a complex sequence of appropriateactions. Under these circumstances; a randomly exploring agent might never stumble upona successful outcome in reasonable time. We present two techniques that significantlyimprove the efficiency of exploration for deep Q-learning agents in dialogue systems. First;we introduce an exploration technique based on Thompson sampling; drawing Monte Carlosamples from a Bayes-by-backprop neural network; demonstrating marked improvementover common approaches such as $\epsilon $-greedy and Boltzmann exploration …,arXiv preprint arXiv:1608.05081,2016,15
Sparse and low-rank coupling image segmentation model via nonconvex regularization,Xiujun Zhang; Chen Xu; Min Li; Xiaoli Sun,This paper investigates how to boost region-based image segmentation by inheriting theadvantages of sparse representation and low-rank representation. A novel imagesegmentation model; called nonconvex regularization based sparse and low-rank couplingmodel; is presented for such a purpose. We aim at finding the optimal solution which isprovided with sparse and low-rank simultaneously. This is achieved by relaxing sparserepresentation problem as L1/2 norm minimization other than the L1 norm minimization;while relaxing low-rank representation problem as the S1/2 norm minimization other thanthe nuclear norm minimization. This coupled model can be solved efficiently through theAugmented Lagrange Multiplier (ALM) method and half-threshold operator. Compared to theother state-of-the-art methods; the new method is better at capturing the global structure …,International Journal of Pattern Recognition and Artificial Intelligence,2015,14
End-to-end joint learning of natural language understanding and dialogue manager,Xuesong Yang; Yun-Nung Chen; Dilek Hakkani-Tür; Paul Crook; Xiujun Li; Jianfeng Gao; Li Deng,Natural language understanding and dialogue policy learning are both essential inconversational systems that predict the next system actions in response to a current userutterance. Conventional approaches aggregate separate models of natural languageunderstanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive tonoisy outputs of error-prone NLU. To address the issues; we propose an end-to-end deeprecurrent neural network with limited contextual dialogue memory by jointly training NLUand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that ourproposed model significantly outperforms the state-of-the-art pipeline models for both NLUand SAP; which indicates that our joint model is capable of mitigating the affects of noisyNLU outputs; and NLU model can be refined by error flows backpropagating from the …,Acoustics; Speech and Signal Processing (ICASSP); 2017 IEEE International Conference on,2017,11
A user simulator for task-completion dialogues,Xiujun Li; Zachary C Lipton; Bhuwan Dhingra; Lihong Li; Jianfeng Gao; Yun-Nung Chen,Abstract: Despite widespread interests in reinforcement-learning for task-oriented dialoguesystems; several obstacles can frustrate research and development progress. First;reinforcement learners typically require interaction with the environment; so conventionaldialogue corpora cannot be used directly. Second; each task presents specific challenges;requiring separate corpus of task-specific annotated data. Third; collecting and annotatinghuman-machine or human-human conversations for task-oriented dialogues requiresextensive domain knowledge. Because building an appropriate dataset can be bothfinancially costly and time-consuming; one popular approach is to build a user simulatorbased upon a corpus of example dialogues. Then; one can train reinforcement learningagents in an online fashion as they interact with the simulator. Dialogue agents trained on …,arXiv preprint arXiv:1612.05688,2016,11
Management plane analytics,Aaron Gember-Jacobson; Wenfei Wu; Xiujun Li; Aditya Akella; Ratul Mahajan,Abstract While it is generally held that network management is tedious and error-prone; it isnot well understood which specific management practices increase the risk of failures.Indeed; our survey of 51 network operators reveals a significant diversity of opinions; andour characterization of the management practices in the 850+ networks of a large onlineservice provider shows significant diversity in prevalent practices. Motivated by theseobservations; we develop a management plane analytics (MPA) framework that anorganization can use to:(i) infer which management practices impact network health; and (ii)develop a predictive model of health; based on observed practices; to improve networkmanagement. We overcome the challenges of sparse and skewed data by aggregating datafrom many networks; reducing data dimensionality; and oversampling minority cases. Our …,Proceedings of the 2015 Internet Measurement Conference,2015,10
Recurrent reinforcement learning: a hybrid approach,Xiujun Li; Lihong Li; Jianfeng Gao; Xiaodong He; Jianshu Chen; Li Deng; Ji He,Abstract: Successful applications of reinforcement learning in real-world problems oftenrequire dealing with partially observable states. It is in general very challenging to constructand infer hidden states as they often depend on the agent's entire interaction history andmay require substantial domain knowledge. In this work; we investigate a deep-learningapproach to learning the representation of states in partially observable tasks; with minimalprior knowledge of the domain. In particular; we propose a new family of hybrid models thatcombines the strength of both supervised learning (SL) and reinforcement learning (RL);trained in a joint fashion: The SL component can be a recurrent neural networks (RNN) or itslong short-term memory (LSTM) version; which is equipped with the desired property ofbeing able to capture long-term dependency on history; thus providing an effective way of …,arXiv preprint arXiv:1509.03044,2015,10
Deep learning powered in-session contextual ranking using clickthrough data,Xiujun Li; Chenlei Guo; Wei Chu; Ye-Yi Wang; Jude Shavlik,Abstract User interactions with search engines provide many cues that can be leveraged toimprove the relevance of search results through personalization. The context information(history of queries; clicked documents; etc.) provides strong signals about users' searchintent; which can be used to personalize the search experience and improve a web searchengine. We demonstrate how to generate the semantic features from in-session contextualinformation with deep learning models; and incorporate these semantic features into thecurrent ranking model to re-rank the results. We evaluate our approach using a large; real-world search log data from a major commercial web search engine; and the experimentalresults show our approach can significantly improve the performance of the search engine.Furthermore; we also find that the domain-specific; click-based features can effectively …,In Proc. of NIPS,2014,10
Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning,Baolin Peng; Xiujun Li; Lihong Li; Jianfeng Gao; Asli Celikyilmaz; Sungjin Lee; Kam-Fai Wong,Abstract Building a dialogue agent to fulfill complex tasks; such as travel planning; ischallenging because the agent has to learn to collectively complete multiple subtasks. Forexample; the agent needs to reserve a hotel and book a flight so that there leaves enoughtime for commute between arrival and hotel check-in. This paper addresses this challengeby formulating the task in the mathematical framework of options over Markov DecisionProcesses (MDPs); and proposing a hierarchical deep reinforcement learning approach tolearning a dialogue manager that operates at different temporal scales. The dialoguemanager consists of:(1) a top-level dialogue policy that selects among subtasks oroptions;(2) a low-level dialogue policy that selects primitive actions to complete the subtaskgiven by the top-level policy; and (3) a global state tracker that helps ensure all cross …,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017,5
Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems,Xiujun Li; Yun-Nung Chen; Lihong Li; Jianfeng Gao; Asli Celikyilmaz,Abstract: Language understanding is a key component in a spoken dialogue system. In thispaper; we investigate how the language understanding module influences the dialoguesystem performance by conducting a series of systematic experiments on a task-orientedneural dialogue system in a reinforcement learning based setting. The empirical studyshows that among different types of language understanding errors; slot-level errors canhave more impact on the overall performance of a dialogue system compared to intent-levelerrors. In addition; our experiments demonstrate that the reinforcement learning baseddialogue system is able to learn when and what to confirm in order to achieve betterperformance and greater robustness.,arXiv preprint arXiv:1703.07055,2017,3
Multi-model controller,*,A processing unit can operate a first recurrent computational model (RCM) to provide firststate information and a predicted result value. The processing unit can operating a firstnetwork computational model (NCM) to provide respective expectation values of a pluralityof actions based at least in part on the first state information. The processing unit can providean indication of at least one of the plurality of actions; and receive a reference result value;eg; via a communications interface. The processing unit can train the first RCM based atleast in part on the predicted result value and the reference result value to provide a secondRCM; and can train the first NCM based at least in part on the first state information and theat least one of the plurality of actions to provide a second NCM.,*,2017,1
Composite Task-Completion Dialogue System via Hierarchical Deep Reinforcement Learning,Baolin Peng; Xiujun Li; Lihong Li; Jianfeng Gao; Asli Celikyilmaz; Sungjin Lee; Kam-Fai Wong,Abstract: In a composite-domain task-completion dialogue system; a conversation agentoften switches among multiple sub-domains before it successfully completes the task. Givensuch a scenario; a standard deep reinforcement learning based dialogue agent may sufferto find a good policy due to the issues such as: increased state and action spaces; highsample complexity demands; sparse reward and long horizon; etc. In this paper; we proposeto use hierarchical deep reinforcement learning approach which can operate at differenttemporal scales and is intrinsically motivated to attack these problems. Our hierarchicalnetwork consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by meta-controller andintrinsic rewards can guide the controller to effectively explore in the state-action space …,arXiv preprint arXiv:1704.03084,2017,1
Efficient dialogue policy learning with bbq-networks,Zachary C Lipton; Xiujun Li; Jianfeng Gao; Lihong Li; Faisal Ahmed; Li Deng; Tobias Birnbaum; Yonina C Eldar; Deanna Needell; Adnan Memon; Idris Mercer; Nicola Prezza; João Paulo Ferreira Guimarães; Nasim Souly; Mubarak Shah; Roberto Santana; Zheng Zhu; Helmut G Katzgraber; Ankur Sinha; Janne Rämö; Pekka Malo; Markku Kallio; Olli Tahvonen; Saeed Mohajeryami; Giovanni Interdonato; Hien Quoc Ngo; Erik G Larsson; Pål Frenger; Vikas Chawla; Hsiang Sing Naik; Adedotun Akintayo; Dermot Hayes; Patrick Schnable; Baskar Ganapathysubramanian; Soumik Sarkar; Liang Wu; Fred Morstatter; Huan Liu; Suhang Jiang; Katerina Schenke; Jacquelynne Sue Eccles; Di Xu; Mark Warschauer; Keyu Xia; Fedor Jelezko; Jason Twamley; Hamid Izadinia; Qi Shan; Steven M Seitz; Ryan A Rossi; Rong Zhou; Stephen Mallon; Vincent Gramoli; Guillaume Jourjon; Xiaoshui Huang; Jian Zhang; Lixin Fan; Qiang Wu; Chun Yuan; George Toderici; Damien Vincent; Nick Johnston; Sung Jin Hwang; David Minnen; Joel Shor; Michele Covell; Binhuang Song; Chen Zhu; Bill Corcoran; Qibing Wang; Leimeng Zhuang; Arthur J Lowery; Harm van Seijen; Brendan Juba; Jianan Li; Xiaodan Liang; Jianshu Li; Tingfa Xu; Jiashi Feng; Shuicheng Yan; Gui-Song Xia; Jingwen Hu; Fan Hu; Baoguang Shi; Xiang Bai; Yanfei Zhong; Liangpei Zhang; Cory J Kleinheksel; Arun K Somani; Wing-Kai Hon; Ton Kloks; Fu-Hong Liu; Hsiang-Hsuan Liu; Tao-Ming Wang,Comments: arXiv admin note: text overlap with arXiv: 1607.06660 Comment: new style(lipics); using Heath-Brown theorem for number of primes in Z; improved bounds for LCParray computation and sparse suffix sorting; added construction of the LCE structure usingradix sort; added reference to lower bound for LCE query times,arXiv preprint ArXiv:1608.05081,2016,1
Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks & Replay Buffer Spiking,Zachary C Lipton; Jianfeng Gao; Lihong Li; Xiujun Li; Faisal Ahmed; Li Deng,Abstract When rewards are sparse and efficient exploration essential; deep Q-learning withe-greedy exploration tends to fail. This poses problems for otherwise promising domainssuch as task-oriented dialog systems; where the primary reward signal; indicating successfulcompletion; typically occurs only at the end of each episode but depends on the entiresequence of utterances. A poor agent encounters such successful dialogs rarely; and arandom agent may never stumble upon a successful outcome in reasonable time. Wepresent two techniques that significantly improve the efficiency of exploration for deep Q-learning agents in dialog systems. First; we demonstrate that exploration by Thompsonsampling; using Monte Carlo samples from a Bayes-by-Backprop neural network; yieldsmarked improvement over standard DQNs with Boltzmann or e-greedy exploration …,arXiv preprint arXiv:1608.05081,2016,1
Detecting semantic uncertainty by learning hedge cues in sentences using an HMM,Xiujun Li; Wei Gao; Jude W Shavlik,Speculative language refers to expressions of uncertainty over statements; which indicatesthat speakers do not back up their opinions with facts. In information retrieval and naturallanguage processing; many applications seek to extract this kind of information and try todistinguish them from the factual information since they convey a different attitude thatspeakers hold. For example; in question answering (QA); it is of paramount importance toensure that the candidate answers gathered from various sources are of high certainty orbear sufficient supporting evidence; and those less certain should be automatically pusheddownward in the answer list; or to retain users' trust on the QA system; it would be desirablefor the system providing the level of uncertainty associated with the output answers [13]. Inrecent years; with the increasing popularity of social media; the quality of information in …,Workshop on Semantic Matching in Information Retrieval (SMIR,2014,1
Mobile positioning system based on the wireless sensor network in buildings,Xiujun Li; Gang Sun; Xu Wang,Using 802.11 wireless network protocols as the communication medium; combining withradio frequency-communication and ultrasonic ranging; implement a mobile terminal systemin an intellectualized building. It can provide its holder such functions: 1) accuratepositioning 2) intelligent navigation 3) video monitoring 4) wireless communication. Theinnovative point for this paper is to apply this system into the indoor environment for thepurpose of emergent event or rescuing.,Wireless Communications; Networking and Mobile Computing; 2009. WiCom'09. 5th International Conference on,2009,1
Mobile Positioning System Based on the Wireless Sensor Network in Buildings (Extended),Xiujun Li; Gang Sun; Xu Wang,*,Journal of Communications and Network,2009,1
Integrating planning for task-completion dialogue policy learning,Baolin Peng; Xiujun Li; Jianfeng Gao; Jingjing Liu; Kam-Fai Wong,Abstract: Training a task-completion dialogue agent with real users via reinforcementlearning (RL) could be prohibitively expensive; because it requires many interactions withusers. One alternative is to resort to a user simulator; while the discrepancy of betweensimulated and real users makes the learned policy unreliable in practice. This paperaddresses these challenges by integrating planning into the dialogue policy learning basedon Dyna-Q framework; and provides a more sample-efficient approach to learn the dialoguepolices. The proposed agent consists of a planner trained on-line with limited real userexperience that can generate large amounts of simulated experience to supplement withlimited real user experience; and a policy model trained on these hybrid experiences. Theeffectiveness of our approach is validated on a movie-booking task in both a simulation …,arXiv preprint arXiv:1801.06176,2018,*
BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems,Zachary Lipton; Xiujun Li; Jianfeng Gao; Lihong Li; Faisal Ahmed; Li Deng,Abstract: We present a new algorithm that significantly improves the efficiency of explorationfor deep Q-learning agents in dialogue systems. Our agents explore via Thompsonsampling; drawing Monte Carlo samples from a Bayes-by-Backprop neural network. Ouralgorithm learns much faster than common exploration strategies such as\epsilon-greedy;Boltzmann; bootstrapping; and intrinsic-reward-based ones. Additionally; we show thatspiking the replay buffer with experiences from just a few successful episodes can make Q-learning feasible when it might otherwise fail.,arXiv preprint arXiv:1711.05715,2017,*
Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue Policy Learning,Baolin Peng; Xiujun Li; Jianfeng Gao; Jingjing Liu; Yun-Nung Chen; Kam-Fai Wong,Abstract: This paper presents a new method---adversarial advantage actor-critic (AdversarialA2C); which significantly improves the efficiency of dialogue policy learning in task-completion dialogue systems. Inspired by generative adversarial networks (GAN); we train adiscriminator to differentiate responses/actions generated by dialogue agents fromresponses/actions by experts. Then; we incorporate the discriminator as another critic intothe advantage actor-critic (A2C) framework; to encourage the dialogue agent to explorestate-action within the regions where the agent takes actions similar to those of the experts.Experimental results in a movie-ticket booking domain show that the proposed AdversarialA2C can accelerate policy exploration efficiently.,arXiv preprint arXiv:1710.11277,2017,*
