Database system concepts,Abraham Silberschatz; Henry F Korth; Shashank Sudarshan,This volume is an instructor's manual for the 4th edition of Database System Concepts byAbraham Silberschatz; Henry F. Korth and S. Sudarshan. It contains answers to theexercises at the end of each chapter of the book. Before providing answers to the exercisesfor each chapter; we include a few remarks about the chapter. The nature of these remarksvary. They include explanations of the inclusion or omission of certain material; and remarkson how we teach the chapter in our own courses. The remarks also include suggestions onmaterial to skip if time is at a premium; and tips on software and supplementary material thatcan be used for programming exercises. Beginning with this edition; solutions for someproblems have been made available on the Web. These problems have been marked with a“*” in the instructor's manual. The Web home page of the book; at http://www. db-book …,*,1997,5410,0
Keyword searching and browsing in databases using BANKS,Gaurav Bhalotia; Arvind Hulgeri; Charuta Nakhe; Soumen Chakrabarti; S. Sudarshan,With the growth of the Web; there has been a rapid increase in the number of users whoneed to access online databases without having a detailed knowledge of the schema or ofquery languages; even relatively simple query languages designed for non-experts are toocomplicated for them. We describe BANKS; a system which enables keyword-based searchon relational databases; together with data and schema browsing. BANKS enables users toextract information in a simple manner without any knowledge of the schema or any need forwriting complex queries. A user can get information by typing a few keywords; followinghyperlinks; and interacting with controls on the displayed results. BANKS models tuples asnodes in a graph; connected by links induced by foreign key and other relationships.Answers to a query are modeled as rooted trees connecting tuples that match individual …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,1106,15
Bidirectional expansion for keyword search on graph databases,Varun Kacholia; Shashank Pandit; Soumen Chakrabarti; S Sudarshan; Rushi Desai; Hrishikesh Karambelkar,Abstract Relational; XML and HTML data can be represented as graphs with entities asnodes and relationships as edges. Text is associated with nodes and possibly edges.Keyword search on such graphs has received much attention lately. A central problem in thisscenario is to efficiently extract from the data graph a small number of the" best" answertrees. A Backward Expanding search; starting at nodes matching keywords and working uptoward confluent roots; is commonly used for predominantly text-driven queries. But it canperform poorly if some keywords match many nodes; or some node has very large degree.In this paper we propose a new search algorithm; Bidirectional Search; which improves onBackward Expanding search by allowing forward search from potential roots towards leaves.To exploit this flexibility; we devise a novel search frontier prioritization technique based …,Proceedings of the 31st international conference on Very large data bases,2005,557,20
Efficient and extensible algorithms for multi query optimization,Prasan Roy; Srinivasan Seshadri; S Sudarshan; Siddhesh Bhobe,Abstract Complex queries are becoming commonplace; with the growing use of decisionsupport systems. These complex queries often have a lot of common sub-expressions; eitherwithin a single query; or across multiple such queries run as a batch. Multiquery optimizationaims at exploiting common sub-expressions to reduce evaluation cost. Multi-queryoptimization has hither-to been viewed as impractical; since earlier algorithms wereexhaustive; and explore a doubly exponential search space. In this paper we demonstratethat multi-query optimization using heuristics is practical; and provides significant benefits.We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU; which arebased on simple modifications to the Volcano search strategy; and a greedy heuristic. Ourgreedy heuristic incorporates novel optimizations that improve efficiency greatly. Our …,ACM SIGMOD Record,2000,508,17
Extending query rewriting techniques for fine-grained access control,Shariq Rizvi; Alberto Mendelzon; Sundararajarao Sudarshan; Prasan Roy,Abstract Current day database applications; with large numbers of users; require fine-grained access control mechanisms; at the level of individual tuples; not just entirerelations/views; to control which parts of the data can be accessed by each user. Fine-grained access control is often enforced in the application code; which has numerousdrawbacks; these can be avoided by specifying/enforcing access control at the databaselevel. We present a novel fine-grained access control model based on authorization viewsthat allows" authorization-transparent" querying; that is; user queries can be phrased interms of the database relations; and are valid if they can be answered using only theinformation contained in these authorization views. We extend earlier work on authorization-transparent querying by introducing a new notion of validity; conditional validity. We give …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,400,19
Turbo-charging vertical mining of large databases,Pradeep Shenoy; Jayant R Haritsa; S Sudarshan; Gaurav Bhalotia; Mayank Bawa; Devavrat Shah,Abstract In a vertical representation of a market-basket database; each item is associatedwith a column of values representing the transactions in which it is present. The association-rule mining algorithms that have been recently proposed for this representation showperformance improvements over their classical horizontal counterparts; but are eitherefficient only for certain database sizes; or assume particular characteristics of the databasecontents; or are applicable only to specific kinds of database schemas. We present here anew vertical mining algorithm called VIPER; which is general-purpose; making no specialrequirements of the underlying database. VIPER stores data in compressed bit-vectorscalled “snakes” and integrates a number of novel optimizations for efficient snakegeneration; intersection; counting and storage. We analyze the performance of VIPER for …,Acm Sigmod Record,2000,341,10
Materialized view maintenance and integrity constraint checking: Trading space for time,Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We investigate the problem of incremental maintenance of an SQL view in the faceof database updates; and show that it is possible to reduce the total time cost of viewmaintenance by materializing (and maintaining) additional views. We formulate the problemof determining the optimal set of additional views to materialize as an optimization problemover the space of possible view sets (which includes the empty set). The optimizationproblem is harder than query optimization since it has to deal with multiple view sets;updates of multiple relations; and multiple ways of maintaining each view set for eachupdated relation. We develop a memoing solution for the problem; the solution can beimplemented using the expression DAG representation used in rule-based optimizers suchas Volcano. We demonstrate that global optimization cannot; in general; be achieved by …,ACM SIGMOD Record,1996,291,20
CORAL: Control; relations and logic,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract CORAL is a modular declarative query language/programming language thatsupports general Horn clauses with complex terms; set-grouping; aggregation; negation;and relations with tuples that contain (universally quanti ed) variables. Support for persistentrelations is provided by using the EXODUS storage manager. A unique feature of CORAL isthat it provides a wide range of evaluation strategies and allows users to| optionally| tailorexecution of a program through high-level annotations. A CORAL program is organized as acollection of modules; and this structure is used as the basis for expressing control choices.CORAL has an interface to C++; and uses the class structure of C++ to provide extensibility.Finally; CORAL supports a command sublanguage; in which statements are evaluated in auser-speci ed order. The statements can be queries; updates; production-system style …,VLDB,1992,253,10
Materialized view selection and maintenance using multi-query optimization,Hoshi Mistry; Prasan Roy; S Sudarshan; Krithi Ramamritham,Abstract Materialized views have been found to be very effective at speeding up queries;and are increasingly being supported by commercial databases and data warehousesystems. However; whereas the amount of data entering a warehouse and the number ofmaterialized views are rapidly increasing; the time window available for maintainingmaterialized views is shrinking. These trends necessitate efficient techniques for themaintenance of materialized views. In this paper; we show how to find an efficient plan forthe maintenance of a set of materialized views; by exploiting common subexpressionsbetween different view maintenance expressions. In particular; we show how to efficientlyselect (a) expressions and indices that can be effectively shared; by transientmaterialization;(b) additional expressions and indices for permanent materialization; and …,ACM SIGMOD Record,2001,243,17
BANKS: Browsing and Keyword Searching in Relational Databases** Partly supported by an IBM Faculty Fellowship grant and an Infosys Ph. D. Fellowship.,B Aditya; Gaurav Bhalotia; Soumen Chakrabarti; Arvind Hulgeri; Charuta Nakhe; S Sudarshanxe,Browsing ANd Keyword Searching (BANKS) enables almost effortless Web publishing ofrelational and eXtensible Markup Language (XML) data that would otherwise remain (atleast partially) invisible to the Web. Relational databases store large amounts of data thatare queried using structured query languages. A user needs to know the underlying schemaand the query language in order to make meaningful ad hoc queries on the data. This is asubstantial barrier for casual users; such as users of Web-based information systems. HTMLforms can be provided for predefined queries. A university Website may provide a forminterface to search for faculty and students. Searching for departments would require yetanother form; as would search for courses offered. However; creating an interface for eachsuch task is laborious; and is also confusing to users since they must first expend effort …,*,2002,190,6
Dali: A high performance main memory storage manager,Hosagrahar V Jagadish; Daniel Lieuwen; Rajeev Rastogi; Abraham Silberschatz; S Sudarshan,Abstract Performance needs of many database applications dictate that the entire databasebe stored in main memory. The Dali system is a main memory storage manager designed toprovide the persistence; availability and safety guarantees one typically expects from adiskresident database; while at the same time providing very high performance by virtue ofbeing tuned to support in-memory data. Dali follows the philosophy of treating all data;including system data; uniformly as database files that can be memory mapped and directlyaccessed/updated by user processes. Direct access provides high performance; slower; butmore secure; access is also provided through the use of a server process. Various featuresof Dali can be tailored to the needs of an application to achieve high performance-forexample; concurrency control and logging can be turned off if not desired; which enables …,VLDB,1994,154,19
Keyword search on external memory data graphs,Bhavana Bharat Dalvi; Meghana Kshirsagar; S Sudarshan,Abstract Keyword search on graph structured data has attracted a lot of attention in recentyears. Graphs are a natural" lowest common denominator" representation which cancombine relational; XML and HTML data. Responses to keyword queries are usuallymodeled as trees that connect nodes matching the keywords. In this paper we address theproblem of keyword search on graphs that may be significantly larger than memory. Wepropose a graph representation technique that combines a condensed version of the graph(the" supernode graph") which is always memory resident; along with whatever parts of thedetailed graph are in a cache; to form a multi-granular graph representation. We proposetwo alternative approaches which extend existing search algorithms to exploit multigranulargraphs; both approaches attempt to minimize IO by directing search towards areas of the …,Proceedings of the VLDB Endowment,2008,147,15
Pipelining in multi-query optimization,Nilesh N Dalvi; Sumit K Sanghai; Prasan Roy; S Sudarshan,Abstract Database systems frequently have to execute a set of related queries; which shareseveral common subexpressions. Multi-query optimization exploits this; by findingevaluation plans that share common results. Current approaches to multi-query optimizationassume that common subexpressions are materialized. Significant performance benefits canbe had if common subexpressions are pipelined to their uses; without being materialized.However; plans with pipelining may not always be realizable with limited buffer space; as weshow. We present a general model for schedules with pipelining; and present a necessaryand sufficient condition for determining validity of a schedule under our model. We show thatfinding a valid schedule with minimum cost is NP-hard. We present a greedy heuristic forfinding good schedules. Finally; we present a performance study that shows the benefit of …,Journal of Computer and System Sciences,2003,126,19
Database systems: Achievements and opportunities,Avi Silberschatz; Michael Stonebraker; Jeffrey D Ullman,Abstract The history of database system research in the US is one of exceptional productivityand startling economic impact. Barely twenty years old as a basic science research field;database research conducted with Federal support in the nation's universities and in itsindustrial research laboratories has fueled an information services industry estimated at $10billion per year in the US alone. This industry has grown at an average rate of 20 percent peryear since 1965 and is continuing to expand at this rate. Achievements in database researchunderpin fundamental advances in communications systems; transportation and logistics;financial management; knowledge-based systems; accessibility to scientific literature; and ahost of other civilian and defense applications. They also serve as the foundation forconsiderable progress in basic science in various fields ranging from computing to …,ACM Sigmod Record,1990,116,19
The CORAL deductive system,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan; Praveen Seshadri,Abstract CORAL is a deductive system that supports a rich declarative language; and aninterface to C++; which allows for a combination of declarative and imperative programming.A CORAL declarative program can be organized as a collection of interacting modules.CORAL supports a wide range of evaluation strategies; and automatically chooses anefficient strategy for each module in the program. Users can guide query optimization byselecting from a wide range of control choices. The CORAL system provides imperativeconstructs to update; insert; and delete facts. Users can program in a combination ofdeclarative CORAL and C++ extended with CORAL primitives. A high degree of extensibilityis provided by allowing C++ programmers to use the class structure of C++ to enhance theCORAL implementation. CORAL provides support for main-memory data and; using the …,The VLDB Journal—The International Journal on Very Large Data Bases,1994,108,15
Implementation of the CORAL deductive database system,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan; Praveen Seshadri,Abstract CORAL is a deductive database system that supports a rich declarative language;provides a wide range of evaluation methods; and allows a combination of declarative andimperative programming. The data can be persistent on disk or can reside in main-memory.We describe the architecture and implementation of CORAL. There were two importantgoals in the design of the CORAL architecture:(1) to integrate the different evaluationstrategies in a reasonable fashion; and (2) to allow users to influence the optimizationtechniques used so as to exploit the full power of the CORAL implementation. A CORALdeclarative program can be organized as a collection of interacting modules and thismodular structure is the key to satisfying both these goals. The high level module interfaceallows modules with different evaluation techniques to interact in a transparent fashion …,ACM SIGMOD Record,1993,97,15
The architecture of the Dali main-memory storage manager,Philip Bohannon; Daniel Lieuwen; Rajeev Rastogi; Avi Silberschatz; S Seshadri; S Sudarshan,Abstract Performance needs of many database applications dictate that the entire databasebe stored in main memory. The Dalí system is a main memory storage manager designed toprovide the persistence; availability and safety guarantees one typically expects from a disk-resident database; while at the same time providing very high performance by virtue of beingtuned to support in-memory data. User processes map the entire database into their addressspace and access data directly; thus avoiding expensive remote procedure calls and buffermanager interactions typical of accesses in disk-resident commercial systems availabletoday.,*,1997,95,17
Cost-based optimization for magic: Algebra and implementation,Praveen Seshadri; Joseph M Hellerstein; Hamid Pirahesh; TY Leung; Raghu Ramakrishnan; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query; whichdiffer greatly in execution performance. We propose cost-based techniques for selecting anefficient variant from the many choices. Our first contribution is a practical scheme thatmodels magic sets rewriting as a special join method that can be added to any cost-basedquery optimizer. We derive cost formulas that allow an optimizer to choose the best variant ofthe rewriting and to decide whether it is beneficial. The order of complexity of theoptimization process is preserved by limiting the search space in a reasonable manner. Wehave implemented this technique in IBM's DB2 C/S V2 database system. Our performancemeasurements demonstrate that the cost-based magic optimization technique performs …,ACM SIGMOD Record,1996,94,10
System and method for aging versions of data in a main memory database,*,For use with a database of data records stored in a memory; a system and method forincreasing a memory capacity and a memory database employing the system or the method.The system includes:(1) a time stamping controller that assigns a time stamp to transactionsto be performed on the database; the time stamp operates to preserve an order of thetransactions;(2) a versioning controller that creates multiple versions of ones of the datarecords affected by the transactions that are update transactions and (3) an aging controller;which is associated with each of the time stamping and versioning controllers; that monitorsa measurable characteristic of the memory and deletes ones of the multiple versions of theones of the data records in response to the time stamp and the measurable characteristicthereby to increase memory capacity.,*,2000,90,20
Optimization of queries using relational algebraic theta-semijoin operator,*,A collection of equivalence rules involving the multiset version of the relational algebraictheta-semijoin operator is used to generate relational algebraic expressions equivalent to acomputer programming language query. These expressions may be employed as a searchspace which is utilized by; for example; optimizing software or software that determines theequivalency of queries. Cost formulas for the multiset version of the theta-semijoin operatormay be used in computing cost estimates for the generated expressions. Based on thesecomputed cost estimates; the least costly implementation of a complex query is determined.Thus; queries are cost-based optimized on both a local and global basis by use of therelational algebraic theta-semijoin operator.,*,2000,90,15
System and method for restoring a multiple checkpointed database in view of loss of volatile memory,*,For use with an active database stored in volatile memory for direct revision thereof; theactive database having multiple checkpoints and a stable log; having a tail stored in thevolatile memory; for tracking revisions to the active database to allow correspondingrevisions to be made to the multiple checkpoints; the active database subject to corruption; asystem for; and method of; restoring the active database and a computer system containingthe same. The system includes:(1) a checkpoint determination controller that determineswhich of the multiple checkpoints is a most recently completed checkpoint and copies themost recently completed checkpoint to the volatile memory to serve as an unreviseddatabase for reconstructing the active database and (2) a revision application controller thatretrieves selected ones of the revisions from the stable log and the tail and applies the …,*,1999,89,20
Fine grained authorization through predicated grants,Surajit Chaudhuri; Tanmoy Dutta; S Sudarshan,Authorization in SQL is currently at the level of tables or columns. Many applications need afiner level of control. We propose a model for fine-grained authorization based on addingpredicates to authorization grants. Our model supports predicated authorization to specificcolumns; cell-level authorization with nullification; authorization for function/procedureexecution; and grants with grant option. Our model also incorporates other novel features;such as query defined user groups; and authorization groups; which are designed to simplifyadministration of authorizations. Our model is designed to be a strict generalization of thecurrent SQL authorization mechanism.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,88,15
Coral++: Adding object-orientation to a logic database language,Divesh Srivastava; Raghu Ramakrishnan; Praveen Seshadri; S Sudarshan,*,VLDB,1993,88
Aggregation and Relevance in Deductive Databases.,S Sudarshan; Raghu Ramakrishnan,Abstract In this paper we present a technique to optimize queries on deductive databasesthat use aggregate operations such as min; max; and\largest k values." Our approach isbased on an extended notion of relevance of facts to queries that takes aggregateoperations into account. The approach has two parts: a rewriting part that labels predicateswith\aggregate selections;" and an evaluation part that makes use of\aggregate selections"to detect that facts are irrelevant and discards them. The rewriting complements standardrewriting algorithms like Magic sets; and the evaluation essentially re nes Semi-Naiveevaluation.,VLDB,1991,86,14
Recovering from Main-Memory Lapses.,HV Jagadish; Abraham Silberschatz; S Sudarshan,Abstract Recovery activities; like logging; checkpointing and restart; are used to restore adatabase to a consistent state after a system crash has occurred. Recovery related overheadis particularly troublesome in a mainmemory database where I/O activities are performed forthe sole purpose of ensuring data durability. In this paper we present a recovery techniquefor main-memory databases; whose bene ts are as follows. First; disk I/O is reduced bylogging to disk only redo records during normal execution. The undo log is normally residentonly in main memory; and is garbage collected after transaction commit. Second; ourtechnique reduces lock contention on account of the checkpointer by allowing actionconsistent checkpointing| to do so; the checkpointer writes to disk relevant parts of the undolog. Third; the recovery algorithm makes only a single pass over the log. Fourth; our …,VLDB,1993,85,11
Automating the detection of snapshot isolation anomalies,Sudhir Jorwekar; Alan Fekete; Krithi Ramamritham; S Sudarshan,Abstract Snapshot isolation (SI) provides significantly improved concurrency over 2PL;allowing reads to be non-blocking. Unfortunately; it can also lead to non-serializableexecutions in general. Despite this; it is widely used; supported in many commercialdatabases; and is in fact the highest available level of consistency in Oracle and Post-greSQL. Sufficient conditions for detecting whether SI anomalies could occur in a given setof transactions were presented recently; and extended to necessary conditions fortransactions without predicate reads. In this paper we address several issues in extendingthe earlier theory to practical detection/correction of anomalies. We first show how tomechanically find a set of programs which is large enough so that we ensure that allexecutions will be free of SI anomalies; by modifying these programs appropriately. We …,Proceedings of the 33rd international conference on Very large data bases,2007,84,10
Incremental organization for data recording and warehousing,HV Jagadish; PPS Narayan; Sridhar Seshadri; S Sudarshan; Rama Kanneganti,Abstract Data warehouses and recording systems typically have a large continuous streamof incoming data; that must be stored in a manner suitable for future access. Access to storedrecords is usually based on a key. Organizing the data on disk as the data arrives usingstandard techniques would result in either (a) one or more I/Os to store each incomingrecord (to keep the data clustered by the key); which is too expensive when data arrival ratesare very high; or (b) many I/Os to locate records for a particular customer (if data is storedclustered by arrival order). We study two techniques; inspired by external sorting algorithms;to store data incrementally as it arrives; simultaneously providing good performance forrecording and querying. We present concurrency control and recovery schemes for bothtechniques. We show the bene ts of our techniques both analytically and experimentally.,VLDB,1997,84,15
Method and apparatus for detecting and recovering from data corruption of a database via read logging,*,A method of detecting and recovering from data corruption of a database is characterized bythe step of logging information about reads of a database in memory to detect errors in dataof the database; wherein said errors in data of said database arise from one of bad writes ofdata to the database; of erroneous input of data to the database by users and of logicalerrors in code of a transaction. The read logging method may be implemented in a pluralityof database recovery models including a cache-recovery model; a prior state model a redo-transaction model and a delete transaction model. In the delete transaction model; it isassumed that logical information is not available to allow a redo of transactions after apossible error and the effects of transactions that read corrupted data are deleted fromhistory and any data written by a transaction reading Ararat data is treated as corrupted.,*,2002,83,0
Parametric query optimization for linear and piecewise linear cost functions,Arvind Hulgeri; S Sudarshan,This chapter presents a parametric query optimization algorithm for linear cost functions thatis nonintrusive in that it uses a conventional query optimizer without modifying it. The cost ofa query plan depends on various database and system parameters. The databaseparameters include selectivity of the predicates and sizes of the relations. The systemparameters include available memory; disk bandwidth; and latency. The exact values ofthese parameters may not be known at compile time. The cost of a query plan depends onmany parameters; such as predicate selectivities and available memory; whose values maynot be known at optimization time. Parametric query optimization (PQO) optimizes a queryinto a number of candidate plans; each optimal for some region of the parameter space. Thechapter proposes a solution for the PQO problem for the case when the cost functions are …,*,2002,81,15
Ordering the attributes of query results,Gautam Das; Vagelis Hristidis; Nishant Kapoor; Shashank Sudarshan,Abstract There has been a great deal of interest in the past few years on ranking of results ofqueries on structured databases; including work on probabilistic information retrieval; rankaggregation; and algorithms for merging of ordered lists. In many applications; for examplesales of homes; used cars or electronic goods; data items have a very large number ofattributes. When displaying a (ranked) list of items to users; only a few attributes can beshown. Traditionally; these are selected manually. We argue that automatic selection ofattributes is required to deal with different requirements of different users. We formulate theproblem as an optimization problem of choosing the most" useful" set of attributes; that is; theattributes that are most influential in the ranking of the items. We discuss different variants ofour notion of attribute usefulness; and propose a hybrid Split-Pane approach that returns …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,79,15
Rule ordering in bottom-up fixpoint evaluation of logic programs,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract Logic programs can be evaluated bottom-up by repeatedly applying all rules; in“iterations”; until the fixpoint is reached. However; it is often desirable—and in some cases;eg programs with stratified negation; even necessary to guarantee the semantics-—to applythe rules in some order. A desirable property of a ﬁxpoint evaluation algorithm is that it doesnot repeat inferences. We say that such an algorithm has the non-repetition property; andthe “semi-naive” algorithms in the literature have this property. However; these algorithms donot address the issue of how to apply rules in a speciﬁed order while retaining the non-repetition property. We present two algorithms that address this issue. One of them (GSN) iscapable of dealing with a wide range of rule orderings but with a little more overhead thanthe usual semi-naive algorithm (which we call BSN). The other (PSN) handles a smaller …,*,1991,74,20
Cost-based maintenance of materialized views,*,A method of incrementally maintaining a first materialized view of data in a database; bymeans of an additional materialized view; first determines whether a cost in time ofincrementally maintaining the first materialized view with the additional materialized view isless than the cost of incrementally maintaining the first materialized view without theadditional materialized view. The method creates the additional materialized view only if thecost in time is less therewith. Determining whether the cost of employing an additionalmaterialized view is less includes using an expression directed acyclic graph thatcorresponds to the first materialized view. Another method of determining whether the cost isless includes pruning an expression directed acyclic graph to produce a single expressiontree; and using the single expression tree to determine whether the cost is less. Both the …,*,2000,73,20
Method and apparatus for detecting and recovering from data corruption of a database via read prechecking and deferred maintenance of codewords,*,A method of detecting and recovering from data corruption of a database is characterized bythe step of protecting data of the database with codewords; one codeword for each region ofthe database; and verifying that a codeword matches associated data before the data is readfrom the database to prevent transaction-carried corruption. A deferred maintenancescheme is recommended for the codewords protecting the database such that the method ofdetecting and recovering from data corruption of a database may comprise the steps ofprotecting data of the database with codewords; one codeword for each region of thedatabase; and asynchronously maintaining the codewords to improve concurrency of thedatabase. Moreover; the database may be audited by using the codewords and noting themin a table and protecting regions of the database with latches. Once codeword values are …,*,2002,71,20
Controlling the search in bottom-up evaluation.,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract Bottom-up evaluation of queries on deductive databases has many advantagesover an evaluation scheme such as Prolog. It is sound and complete with respect to thedeclarative semantics of least Herbrand models for positive Horn clause programs. Inparticular; it is able to avoid in nite loops by detecting repeated (possibly cyclic) subgoals.Further; in many database applications; it is more e cient than Prolog due to its set-orientedness. However; the completely set-oriented; breadth-rst search strategy of bottomupevaluation has certain disadvantages. For example; to evaluate several classes of programswith negation (or aggregation); it is necessary to order the inferences; in essence; we mustevaluate all answers to a negative subgoal before making an inference that depends uponthe negative subgoal. A completely breadth-rst search strategy (14]) would have to …,JICSLP,1992,70,4
Effecting constraint magic rewriting on a query with the multiset version of the relational algebric theta-semijoin operator,*,An equivalence rule having the multiset version of the relational algebraic theta-semijoinoperator is used in effectuating Constraint Magic rewriting on a computer programminglanguage query having non-equality; as well as equality; predicates. In particular; the ruleeffectuates Constraint Magic rewriting for a single join. When applied repeatedly on asequence of joins; Constraint Magic rewriting is effectuated for a single block query. The rulemay be used to generate relational algebraic expressions to optimize or determine theequivalency of queries. Cost estimates for alternative ways of evaluating a complex querycan be efficiently computed. Based on these computed cost estimates; the least costlyimplementation of a complex query is determined.,*,2000,67,20
-AniPQO: Almost Non-intrusive Parametric Query Optimization for Nonlinear Cost Functions,Arvind Hulgeri; S Sudarshan,This chapter proposes a heuristic solution for the parametric query optimization (PQO)problem for the case when the cost functions may be nonlinear in the given parameters. Thissolution is minimally intrusive in the sense that an existing query optimizer can be used withminor modifications. The chapter implements the heuristic and the results of the tests on theTPCD benchmark indicate that the heuristic is very effective. The minimal intrusiveness;generality in terms of cost functions and number of parameters and good performanceindicates that the solution is of significant practical importance. The cost of a query plandepends on many parameters; such as predicate selectivities and available memory; whosevalues may not be known at optimization time. PQO optimizes a query into a number ofcandidate plans; each optimal for some region of the parameter space.AniPQO: Almost …,*,2003,63,15
Interestingness and Pruning of Mined Patterns.,Devavrat Shah; Laks VS Lakshmanan; Krithi Ramamritham; S Sudarshan,Abstract We study the following question: when can a mined pattern; which may be anassociation; a correlation; ratio rule; or any other; be regarded as interesting? Previousapproaches to answering this question have been largely numeric. Speci cally; we show thatthe presence of some rules may make others redundant; and therefore uninteresting. Wearticulate these principles and formalize them in the form of pruning rules. Pruning rules;when applied to a collection of mined patterns; can be used to eliminate redundant ones. Asa concrete instance; we applied our pruning rules on association rules/positive associationrules derived from a census database; and demonstrate that signi cant pruning results.,1999 ACM SIGMOD workshop on research issues in data mining and knowledge discovery,1999,63,0
Method and apparatus for crash safe enforcement of mutually exclusive access to shared resources in a multitasking computer system,*,A fast crash safe method and apparatus for enforcing mutually exclusive access to sharedresources in a computer system through the use of semaphores. The acquisition andrelease of the semaphores is implemented at the user process level. An overestimate andunderestimate of semaphore ownership are maintained in memory by library providedsemaphore acquisition and release code. A cleanup routine reconciles the overestimate andunderestimate to determine the ownership status of the semaphores.,*,1997,59,22
On-line reorganization in object-oriented databases,*,An on-line reorganization method of an object-oriented database with physical referencesinvolves a novel fuzzy traversal of the database; or a partition thereof; to identify theapproximate parents of all migrating objects. Where the entire database is traversed theprocess begins from its persistent root. For traversals of a partition the process begins fromeach object with a reference pointing to it from outside the partition. To facilitate theidentification of these inter-partitional objects an External Reference Table (“ERT”) ismaintained. During the fuzzy traversal all new inserted and deleted references are tracked ina Temporary Reference Table (“TRT”). After the fuzzy traversal is completed; for eachmigrating object; a lock is obtained on the identified approximate parents and on all newparents in which references to the object were inserted; as indicated by the TRT. Based …,*,2002,54,20
Garbage collection in object oriented databases using transactional cyclic reference counting,*,A reference counting a garbage collection process employs a reference counting techniquein which only the “last” detected strong pointer is followed to an object. Moreover; noassumptions are made about the phase locking used in the transaction and strict WAL is notfollowed. Indeed; a relatively restricted local traversal of the object graph is employed whichleads to higher efficiency. Furthermore; in a client-server arrangement; updates made by atransaction running at the client may be reflected at the server after the transaction ends; andthe updates are not required to be forced to the server before the end of the transaction.,*,2002,52,10
Foundations of aggregation constraints,Kenneth A Ross; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; that is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We formally study the fundamental problem of determining if aconjunction of aggregation constraints is satisfiable; and show that; for many classes ofaggregation constraints; the problem is undecidable. We describe a complete and minimalaxiomatization of aggregation constraints; for the SQL aggregate functions min; max; sum;count and average; over a non-empty; finite multiset on several domains. Thisaxiomatization helps identify classes of aggregation constraints for which the satisfiabilitycheck is efficient. We present a polynomial-time algorithm that directly checks for satisfiabilityof a conjunction of aggregation range constraints over a single multiset; this is a …,Theoretical Computer Science,1998,52,15
Query scheduling in multi query optimization,Amit Gupta; S Sudarshan; Sundar Vishwanathan,Complex queries are becoming commonplace; with the growing use of decision supportsystems. Decision support queries often have a lot of common sub-expressions within eachquery; and queries are often run as a batch. Multi query optimization aims at exploitingcommon sub-expressions; to reduce the evaluation cost of queries; by computing them onceand then caching them for future use; both within individual queries and across queries in abatch. In case cache space is limited; the total size of sub-expressions that are worthcaching may exceed available cache space. Prior work in multi query optimization involveschoosing a set of common sub-expressions that fit in available cache space; and oncecomputed; retaining their results across the execution of all queries in a batch. Suchoptimization algorithms do not consider the possibility of dynamically changing the cache …,Database Engineering and Applications; 2001 International Symposium on.,2001,51,19
Clustering techniques for minimizing external path length,Ajit A Diwan; Sanjeeva Rane; S Seshadri; S Sudarshan,Abstract There are a variety of main-memory access structures; such as segment trees; andquad trees; whose properties; such as good worstcase behaviour; make them attractive fordatabase applicdions. Unfortunately; the structures are typically 'long and skinny'; whereasdisk data structuies must be 'shortand-fat (that is; have a high fanout and low height) in orderto minimize I/O. We consider how to cluster the nodes (that is; map the nodes to disk pages)of mainmemory access structures such that although a path may traverse many nodes; itonly traverses a few disk pages. The number of disk pages traversed in a path is called theexternal path length. We address several versions of the clustering problem. We present aclustering algorithm for tree structures that generates optimal worst-case external pathlength mappings; we also show how to make it dynamic; to support updates. We extend …,VLDB,1996,50,20
Efficient bottom-up evaluation of logic programs,Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract In recent years; much work has been directed towards evaluating logic programsand queries on deductive databases by using an iterative bottom-up fixpoint computation.The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation inseveral situations. They are sound and complete for positive Horn clause programs; are well-suited to applications with large volumes of data (facts); and can support a variety ofextensions to the standard logic programming paradigm. We present the basics of databasequery evaluation and logic programming evaluation; and then discuss bottom-up fixpointevaluation. We discuss an approach based upon using a program transformation (“MagicTemplates”) to restrict search; followed by fixpoint computation using a technique (“Semi-naive evaluation”) that avoids repeated inferences. The program transformation …,*,1992,49,14
Method and system for compressing a data stream in a database log so as to permit recovery of only selected portions of the data stream,*,The invention relates to a system for maintaining a log of incoming records for a databasesystem. Seek points are inserted into the compressed data log in a manner that allowsrecovery to start from a specified point without a need for decompressing earlier portions ofthe log. The initial block of data is used as the compression dictionary. A new compressionsequence using the same initial compression dictionary is started at each seek point.,*,1999,47,10
Keeyword search in databases,Arvind Hulgeri; Gaurav Bhalotia; Charuta Nakhe; Soumen Chakrabarti; S Sudarshan,Abstract Querying using keywords is easily the most widely used form of querying today.While keyword searching is widely used to search documents on the Web; querying ofdatabases currently relies on complex query languages that are inappropriate for casualend-users; since they are complex and hard to learn. Given the popularity of keywordsearch; and the increasing use of databases as the back end for data published on the Web;the need for querying databases using keywords is being increasingly felt. One key problemin applying document or web keyword search techniques to databases is that informationrelated to a single answer to a keyword query may be split across multiple tuples in differentrelations. In this paper; we first present a survey of work on keyword querying in databases.We then report on the BANKS system which we have developed. BANKS integrates …,IEEE Data Eng. Bull.,2001,45,15
Logical and Physical Versioning in Main Memory Databases.,Rajeev Rastogi; S Seshadri; Philip Bohannon; Dennis W Leinbaugh; Abraham Silberschatz; So Sudarshan,Abstract We present a design for multi-version concurrency control and recovery in a mainmemory database; and describe logical and physical versioning schemes that allow read-only transactions to execute without obtaining data item locks or system latches. Theseschemes enable a system to guarantee that updaters will never interfere with read-onlytransactions; and that read-only transactions will not be delayed as long as the operatingsystem provides them with su cient cycles. Our contributions include several space savingtechniques for the main memory implementation. We extend the T-tree index structure(designed for mainmemory databases) to support concurrent access and latch-freetraversals; and demonstrate the performance bene ts of our extensions. Some of theseschemes have been implemented on a widely-used software platform within Bell Labs …,VLDB,1997,44,10
Top-down vs. bottom-up revisited,Raghu Ramakrishnan; S Sudarshan,Abstract Ullman (Ull89a; Ull89b]) has shown that for the evaluation of safe Datalogprograms; bottomup evaluation using Magic Sets optimization has time complexity less thanor equal to a particular top-down strategy; Queue-based Rule Goal Tree (QRGT) evaluation.This result has sometimes been incorrectly interpreted to mean that bottom-up evaluationbeats top-down evaluation for evaluating Datalog programs| top-down strategies such asProlog (which does no memoing; and uses last call optimization) can beat both QRGT andbottom-up evaluation on some Datalog programs. In this paper we compare a Prologevaluation based on the WAM model (using last call optimization) with a bottom-upexecution based on Magic Templates with Tail Recursion optimization (Ros91]); and showthe following:(1) Bottom-up evaluation makes no more inferences than Prolog for range …,Proceedings of the International Logic Programming Symposium,1991,44,20
Redundancy and information leakage in fine-grained access control,Govind Kabra; Ravishankar Ramamurthy; S Sudarshan,Abstract The current SQL standard for access control is coarse grained; in that it grantsaccess to all rows of a table or none. Fine-grained access control; which allows control ofaccess at the granularity of individual rows; and to specific columns within those rows; isrequired in practically all database applications. There are several models for fine grainedaccess control; but the majority of them follow a view replacement strategy. There are twosignificant problems with most implementations of the view replacement model; namely (a)the unnecessary overhead of the access control predicates when they are redundant and (b)the potential of information leakage through channels such as user-defined functions; andoperations that cause exceptions and error messages. We first propose techniques forredundancy removal. We then define when a query plan is safe with respect to UDFs and …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,42,15
Data models,Avi Silberschatz; Henry F Korth; S Sudarshan,Underlying the structure of a database is a data model; a collection of conceptual tools fordescribing the real-world entities to be modeled in the database and the relationshipsamong these entities. Data models differ in the primitives available for describing data and inthe amount of semantic detail that can be expressed. The various data models proposed fallinto three different groups: object-based logical models; record-based logical models; andphysical data models. Physical data models; used to describe data at the lowest level;capture aspects of database system implementation not covered in this article. Thus ourfocus here is on the objectbased and record-based logical models. Recently a new model;the object-relational model; has been developed. It merges the object-oriented data modelwith the dominant record-based model; the relational model. We discuss this model …,ACM Computing Surveys (CSUR),1996,39,15
E cient incremental evaluation of queries with aggregation,Raghu Ramakrishnan; Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We present a technique for e ciently evaluating queries on programs withmonotonic aggregation; a class of programs de ned by Ross and Sagiv. Our techniqueconsists of the following components: incremental computation of aggregate functions;incremental xpoint evaluation of monotonic programs and Magic Sets transformation ofmonotonic programs. We also present a formalization of the notion of incrementalcomputation of aggregate functions on a multiset; and upper and lower bounds forincremental computation of a variety of aggregate functions. We describe a proof-theoreticreformulation of the monotonic semantics in terms of computations; following the approachof Beeri et al.; this reformulation greatly simpli es the task of proving the correctness of ouroptimizations.,Proceedings of the International Logic Programming Symposium,1994,39,10
Prediction of mortality in an Indian intensive care unit,Ashish Nimgaonkar; Dilip R Karnad; S Sudarshan; Lucila Ohno-Machado; Isaac Kohane,Abstract Objective To compare hospital outcome prediction using an artificial neural networkmodel; built on an Indian data set; with the APACHE II (Acute Physiology and Chronic HealthEvaluation II) logistic regression model. Design Analysis of a database containingprospectively collected data. Setting Medical-neurological ICU of a university hospital inMumbai; India. Subjects Two thousand sixty-two consecutive admissions between 1996and1998. Interventions None. Measurements and results The 22 variables used to obtainday-1 APACHE II score and risk of death were recorded. Data from 1;962 patients were usedto train the neural network using a back-propagation algorithm. Data from the remaining1;000 patients were used for testing this model and comparing it with APACHE II. Therewere 337 deaths in these 1;000 patients; APACHE II predicted 246 deaths while the …,Intensive care medicine,2004,38,20
Extending the well-founded and valid semantics for aggregation.,S Sudarshan; Divesh Srivastava; Raghu Ramakrishnan; Catriel Beeri,Abstract We present a very general technique for de ning semantics for programs that useaggregation. We use the technique to extend the well-founded semantics and the validsemantics; both of which were designed to provide semantics for programs with negation; tohandle programs that contain possibly recursive use of aggregation. The generalization isbased on a simple but powerful idea of aggregation on three-valued multisets. The use ofthree-valued multisets makes our extended well-founded semantics; which we callaggregate-well-founded semantics; more intuitive than the extension of well-foundedmodels by Van Gelder Van92]. Our semantics and Van Gelder's semantics agree on manyprograms; and on others our semantics provides results that Van Gelder says are intuitiveand desirable; but that his semantics does not provide. The extended valid semantics …,ILPS,1993,38,10
Fine-grained access control in a database by preventing information leakage and removing redundancy,*,Fine-grained access control for querying a database with low overhead and safety plans. Ina view placement approach; base relations of the query expression are replaced by viewsthat are appropriate for the particular user. Information leakage via UDF pushing; exceptionprocessing; and error messages can be prevented by generating safe plans. An existingquery optimizer can be modified to perform redundancy removal and safe planimplementation. Subsumption testing available in materialized view matching can beexploited by the query optimizer. Authorization inference is exploited to generate safe plans.,*,2010,37,19
Foundations of aggregation constraints,Kenneth A Ross; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; which is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We study the fundamental problem of checking if a conjunction ofaggregation constraints is solvable; and present undecidability results for many differentclasses of aggregation constraints. We describe a complete and minimal axiomatization ofthe class of aggregation constraints over finite multisets of reals; which permits a naturalreduction from the class of aggregation constraints to the class of mixed integer/real; non-linear arithmetic constraints. We then present a polynomial-time algorithm that directlychecks for solvability of a useful class of aggregation constraints; where the reduction-basedapproach does not lead to efficient checks for solvability.,*,1994,37,15
Rewriting procedures for batched bindings,Ravindra Guravannavar; S Sudarshan,Abstract Queries; or calls to stored procedures/user-defined functions are often invokedmultiple times; either from within a loop in an application program; or from the where/selectclause of an outer query. When the invoked query/procedure/function involves databaseaccess; a naive implementation can result in very poor performance; due to random I/O.Query decorrelation addresses this problem in the special case of nested sub-queries; but isnot applicable otherwise. This problem is traditionally addressed by manually rewriting theapplication to make it set-oriented; by creating a batch of parameters; and by rewriting thequery/procedure to work on the batch instead of one parameter at a time. Such manualrewriting is time-consuming and error prone. In this paper; we propose techniques that canbe used to do the following;(a) Automatically rewrite programs to replace multiple calls to …,Proceedings of the VLDB Endowment,2008,36,1
Compiling query constraints,Peter J Stuckey; S Sudarshan,Abstract We present a general technique to push query constraints (such as length≤ 1000)into database views and (constraint) logic programs. We introduce the notion ofparametrized constraints; which help us push constraints with argument values that areknown only at run time; and develop techniques for pushing parametrized constraints intopredicate/view definitions. Our technique provides a way of compiling programs withconstraint queries into programs with parametrized constraints compiled in; and which canbe executed on systems; such as database query evaluation systems; that do not handle fullconstraint solving. Thereby our technique can push constraint selections that earlierconstraint query rewriting techniques could not. Our technique is independent of the actualconstraint domain; and we illustrate its use with equality constraints on structures (which …,Proceedings of the thirteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1994,36,20
Optimizations of bottom-up evaluation with non-ground terms,S Sudarshan; Raghu Ramakrishnan,*,ILPS,1993,36
Generating test data for killing SQL mutants: A constraint-based approach,Shetal Shah; S Sudarshan; Suhas Kajbaje; Sandeep Patidar; Bhanu Pratap Gupta; Devang Vira,Complex SQL queries are widely used today; but it is rather difficult to check if a complexquery has been written correctly. Formal verification based on comparing a specification withan implementation is not applicable; since SQL queries are essentially a specificationwithout any implementation. Queries are usually checked by running them on sampledatasets and checking that the correct result is returned; there is no guarantee that allpossible errors are detected. In this paper; we address the problem of test data generationfor checking correctness of SQL queries; based on the query mutation approach formodeling errors. Our presentation focuses in particular on a class of join/outer-joinmutations; comparison operator mutations; and aggregation operation mutations; which area common cause of error. To minimize human effort in testing; our techniques generate a …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,30,0
Scheduling and caching in multi-query optimization,AA Diwan; S Sudarshan; Dilys Thomas,Abstract Database systems frequently have to execute a batch of related queries. Multi-query optimization exploits evaluation plans that share common results. Current approachesto multi-query optimization assume there is infinite disk space; and very limited memoryspace. Pipelining was the only option considered for avoiding expensive disk writes. Theavailability of fairly large and inexpensive main memory motivates the need to make bestuse of available main memory for caching shared results; and scheduling queries in amanner that facilitates caching. Pipelining needs to be exploited at the same time. We lookat the problem of multi-query optimization taking into account query scheduling; caching andpipelining. We first prove that MQO with either just query scheduling or just caching is NP-complete. We then provide the first known algorithms for the most general MQO problem …,Proceedings of 13th International Conference Management of Data,2006,30,20
System for compression and buffering of a data stream with data extraction requirements,*,A data base system buffers incoming records according to destination in the disk or non-volatile memory. The data is compressed and transferred to disk when sufficient data hasbeen accumulated for a particular disk destination. Techniques for compressing thecompression dictionary as well as the data stream are described.,*,2000,29,1
Holistic optimization by prefetching query results,Karthik Ramachandra; S Sudarshan,Abstract In this paper we address the problem of optimizing performance of database/web-service backed applications by means of automatically prefetching query results. Prefetchinghas been performed in earlier work based on predicting query access patterns; howeversuch prediction is often of limited value; and can perform unnecessary prefetches. There hasbeen some earlier work on program analysis and rewriting to automatically insert prefetchrequests; however; such work has been restricted to rewriting of single procedures. In manycases; the query is in a procedure which does not offer much scope for prefetching within theprocedure; in contrast; our approach can perform prefetching in a calling procedure; evenwhen the actual query is in a called procedure; thereby greatly improving the benefits due toprefetching. Our approach does not perform any intrusive changes to the source code …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,28,10
CORAL-A Database Programming Language.,Raghu Ramakrishnan; Per Bothner; Divesh Srivastava; S Sudarshan,CORAL1 is a database programming language being developed at University of Wisconsin-Madison. We present an outline of the language design; which is currently far from complete.However; this outline should give the reader a feel for the language and the goals of theproject.,Workshop on Deductive Databases,1990,28,0
Don't trash your intermediate results; cache'em,Prasan Roy; Krithi Ramamritham; S Seshadri; Pradeep Shenoy; S Sudarshan,Abstract: In data warehouse and data mart systems; queries often take a long time toexecute due to their complex nature. Query response times can be greatly improved bycaching final/intermediate results of previous queries; and using them to answer laterqueries. In this paper we describe a caching system called Exchequer which incorporatesseveral novel features including optimization aware cache maintenance and the use of acache aware optimizer. In contrast; in existing work; the module that makes cost-benefitdecisions is part of the cache manager and works independent of the optimizer whichessentially reconsiders these decisions while finding the best plan for a query. In our work;the optimizer takes the decisions for the cache manager. Furthermore; existing approachesare either restricted to cube (slice/point) queries; or cache just the query results. On the …,arXiv preprint cs/0003005,2000,26,10
Distributed multi-level recovery in main-memory databases,Philip Bohannon; James Parker; Rajeev Rastogi; S Seshadri; Abraham Silberschatz; S Sudarshan,The authors present two schemes for concurrency control and recovery in distributed main-memory databases. In the client-server scheme; clients ship log records to the server; whichapplies the updates to its database copy. In the shared disk scheme; each site broadcasts itsupdates to other sites. The above enable the schemes to support concurrent updates to thesame page at different sites. Both schemes support an explicit multi-level recoveryabstraction for high concurrency; reduced disk I/O by writing only redo log records to diskduring normal processing; and use of per-transaction redo and undo logs to reducecontention. Further; they use a fuzzy checkpointing scheme that writes only dirty pages todisk; yet minimally interferes with normal processing; not requiring updaters to even acquirea latch before updating a page.,Parallel and Distributed Information Systems; 1996.; Fourth International Conference on,1996,26,20
Program transformations for asynchronous query submission,Mahendra Chavan; Ravindra Guravannavar; Karthik Ramachandra; S Sudarshan,Synchronous execution of queries or Web service requests forces the calling application toblock until the query/request is satisfied. The performance of applications can besignificantly improved by asynchronous submission of queries; which allows the applicationto perform other processing instead of blocking while the query is executed; and toconcurrently issue multiple queries. Concurrent submission of multiple queries can allow thequery execution engine to better utilize multiple processors and disks; and to reorder disk IOrequests to minimize seeks. Concurrent submission also reduces the impact of networkround-trip latency and delays at the database; when processing multiple queries. However;manually writing applications to exploit asynchronous query submission is tedious. In thispaper we address the issue of automatically transforming a program written assuming …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,25,0
Well-founded ordered search,Peter J Stuckey; S Sudarshan,Abstract There have been several evaluation mechanisms proposed for computing queryanswers based on the well-founded semantics; for programs with negation. However; thesetechniques are costly; in particular; for the special case of modularly stratified programsOrdered Search is more efficient than the general purpose techniques. However; OrderedSearch is applicable only to modularly stratified programs. In this paper; we extend OrderedSearch to compute the well-founded semantics for all (non-floundering) programs withnegation. Our extension behaves exactly like Ordered Search on programs that aremodularly stratified; and hence pays no extra cost for such programs.,International Conference on Foundations of Software Technology and Theoretical Computer Science,1993,25,17
Optimizing nested queries with parameter sort orders,Ravindra Guravannavar; HS Ramanujam; S Sudarshan,Abstract Nested iteration is an important technique for query evaluation. It is the default wayof executing nested subqueries in SQL. Although decorrelation often results in cheaper non-nested plans; decorrelation is not always applicable for nested subqueries. Nested iteration;if implemented properly; can also win over decorrelation for several classes of queries.Decorrelation is also hard to apply to nested iteration in user-defined SQL procedures andfunctions. Recent research has proposed evaluation techniques to speed up execution ofnested iteration; but does not address the optimization issue. In this paper; we address theissue of exploiting the ordering of nested iteration/procedure calls to speed up nestediteration. We propose state retention of operators as an important technique to exploit thesort order of parameters/correlation variables. We then show how to efficiently extend an …,Proceedings of the 31st international conference on Very large data bases,2005,24,10
The valid model semantics for logic programs,Catriel Beeri; Raghu Ramakrishnan; Divesh Srivastava; S Sudarshan,Abstract We present the valid model semantics; a new approach to providing semantics forlogic programs with negation; set-terms and grouping. The valid model semantics is a three-valued semantics; and is defined in terms of a 'normal form'computation. The valid modelsemantics also gives meaning to the generation and use of non-ground facts (ie; facts withvariables) in a computation. The formulation of the semantics in terms of a normal formcomputation offers important insight not only into the valid model semantics; but also intoother semantics proposed earlier. We show that the valid model semantics extends the well-founded semantics in a natural manner; and has several advantages over it. The well-founded semantics can also be undertood using a variant of the normal form computationsthat we use; the normal form computations used for valid semantics seem more natural …,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,23,10
Enhancing Search with Structure.,Soumen Chakrabarti; Sunita Sarawagi; S Sudarshan,Keyword search has traditionally focussed on retrieving documents in ranked order; given simplekey- word queries. Similarly; work on keyword queries on structured data has focussed on retrievingclosely connected pieces of data that together contain given query keywords. In recentyears; there has been a good deal of work that attempts to go beyond the above paradigms;to improve search experience on unstructured textual data as well as on structured or semi-structureddata. In this paper; we survey recent work on adding structure to keyword search; which canbe categorized on three axes: (a) adding structure to unstructured data; (b) adding structure toanswers; and (c) adding structure to queries al- lowing more power than simple keywordqueries; but while avoiding the complexity of elaborate query languages that demand extensiveschema knowledge … Web search and information retrieval (IR) have traditionally …,IEEE Data Eng. Bull.,2010,22,20
System and method for physically versioning data in a main memory database,*,For use with a database of data records organized into components; the database stored ina memory; a processing system for; and method of; physically versioning the database. Inone embodiment; the processing system includes:(1) a component copier that creates aphysical copy of an original component to be affected by an update transaction to be appliedto the database; and that causes pointers in nodes of the physical copy to point to othernodes in the physical copy;(2) a data updater; associated with the component copier; thatapplies the update transaction to the physical copy to create therefrom a new physicalversion; the original component remaining unaffected by the update transaction and (3) apointer updater; associated with the data updated; that employs an atomic word write torevise a component pointer; associated with the database; to cause the pointer to point to …,*,2000,22,20
DBridge: A program rewrite tool for set-oriented query execution,Mahendra Chavan; Ravindra Guravannavar; Karthik Ramachandra; S Sudarshan,We present DBridge; a novel static analysis and program transformation tool to optimizedatabase access. Traditionally; rewrite of queries and programs are done independently; bythe database query optimzier and the language compiler respectively; leaving out manyoptimization opportunities. Our tool aims to bridge this gap by performing holistictransformations; which include both program and query rewrite. Many applications invokedatabase queries multiple times with different parameter values. Such query invocationsmade using imperative loops are often the cause of poor performance due to random I/Oand round trip delays. In practice; such performance issues are addressed by manuallyrewriting the application to make it set oriented. Such manual rewriting of programs is oftentime consuming and error prone. Guravannavar et. al. propose program analysis and …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,20,3
Detection and recovery techniques for database corruption,Philip Bohannon; Rajeev Rastogi; S Seshadri; Abraham Silberschatz; S Sudarshan,Increasingly; for extensibility and performance; special purpose application code is beingintegrated with database system code. Such application code has direct access to databasesystem buffers; and as a result; the danger of data being corrupted due to inadvertentapplication writes is increased. Previously proposed hardware techniques to protect fromcorruption require system calls; and their performance depends on details of the hardwarearchitecture. We investigate an alternative approach which uses codewords associated withregions of data to detect corruption and to prevent corrupted data from being used bysubsequent transactions. We develop several such techniques which vary in the level ofprotection; space overhead; performance; and impact on concurrency. These techniques areimplemented in the Dali main-memory storage manager; and the performance impact of …,IEEE Transactions on Knowledge and Data Engineering,2003,20,20
Recoverable user-level mutual exclusion,Philip Bohannon; Daniel Lieuwen; Abraham Silberschatz; S Sudarshan; Jacques Gava,Mutual exclusion primitives based on user-level atomic instructions (often called spin locks)have proven to be much more efficient than operating-system semaphores in situationswhere the contention on the semaphore is low. However; many of these spin lock schemesdo not permit registration of ownership to be carried aut atomically with acquisition;potentially leaving the ownership undetermined if a process dies (or makes very slowprogress) at a critical point in the registration code. We present an algorithm which canensure the successful registration of ownership of a spin lock; regardless of whereprocesses fail. Thus; our spin lock implementation is' recoverable'. The determination of aspin lock's ownership can potentially be used to restore resources protected by the spin lockto consistency and then release the spin lock. Other processes using the lock can then …,Parallel and Distributed Processing; 1995. Proceedings. Seventh IEEE Symposium on,1995,18,0
THE CORAL USER MANUAL A Tutorial Introduction to CORAL,Raghu Ramakrishnan; Praveen Seshadri; Divesh Srivastava; S Sudarshan,CORAL1 is a database programming language based on Horn clause logic developed atthe University of Wisconsin {Madison. Source code for CORAL (written in C++) is availableby anonymous ftp over the internet. The CORAL project was initiated in 1988-89; under thename Conlog; and a preliminary report was presented at a workshop in the NACLP 89conference. Preliminary versions of the system have been used by a few groups; but this isthe rst widely available release. We would welcome any feedback on the system.Comments; bug reports and questions should be mailed to coral@ cs. wisc. edu. We wouldlike to acknowledge the contributions of the following people to the CORAL system. PerBothner; who was largely responsible for the initial implementation of CORAL that served asthe basis for subsequent development; was a major early contributor. Joseph Albert …,Computer Science Department; University of Wisconsin-Madison; available via anonymous ftp from ftp. cs. wisc. edu in the directory coral doc,1993,18,10
DataBlitz: A High Performance Main-Memory Storage Manager.,Jerry Baulier; Philip Bohannon; S Gogate; S Joshi; C Gupta; A Khivesera; Henry F Korth; Peter McIlroy; J Miller; PPS Narayan; M Nemeth; Rajeev Rastogi; Abraham Silberschatz; S Sudarshan,DataBlitz' is a main-memory storage management toolkit that supports the development ofhighperformance and fault-resilient applications requiring concurrent access to shared data.By combining transactions; data organization; and fault-resilience with direct access to data;DataBlitz bridges the gap between the speed of data structures in shared memory and thesafety and convenience of a traditional database system. The architecture of Datablitz differssignificantly from that of commercially available database management systems as it isoptimized for high performance in the case when sufficient main-memory is available to holdthe entire database. Our benchmark results indicate that DataBlitz can deliver as high as 50times the performance of commercial disk-based systems. Most commercial databasesystems have a clientserver architecture; assume data is primarily diskresident; and have …,VLDB,1998,15,10
Distributed multi-level recovery in main-memory databases,Rajeev Rastogi; Philip Bohannon; James Parker; Avi Silberschatz; S Seshadri; S Sudarshan,Abstract In this paper we present recovery techniques for distributed main-memorydatabases; specifically for client-server and shared-disk architectures. We present arecovery scheme for client-server architectures which is based on shipping log records tothe server; and two recovery schemes for shared-disk architectures—one based on pageshipping; and the other based on broadcasting of the log of updates. The schemes offerdifferent tradeoffs; based on factors such as update rates. Our techniques are extensions toa distributed-memory setting of a centralized recovery scheme for main-memory databases;which has been implemented in the Dalì main-memory database system. Our centralized aswell as distributed-memory recovery schemes have several attractive features—they supportan explicit multi-level recovery abstraction for high concurrency; reduce disk I/O by writing …,Distributed and Parallel Databases,1998,15,0
Garbage collection in object oriented databases using transactional cyclic reference counting,Srinivas Ashwin; Prasan Roy; S Seshadri; Abraham Silberschatz; S Sudarshan,Abstract Garbage collection is important in objectoriented databases to free the programmerfrom explicitly deallocating memory. In this paper; we present a garbage collectionalgorithm; called Transactional Cyclic Reference Counting (TCRC); for object orienteddatabases. The algorithm is based on a variant of a reference counting algorithm proposedfor functional programming languages The algorithm keeps track of auxiliary reference countinformation to detect and collect cyclic garbage. The algorithm works correctly in thepresence of concurrently running transactions; and system failures. It does not obtain anylong term locks; thereby minimizing interference with transaction processing. It usesrecovery subsystem logs to detect pointer updates; thus; existing code need not be rewritten.Finally; it exploits schema information; if available; to reduce costs. We have implemented …,VLDB,1997,14,0
Reducing order enforcement cost in complex query plans,Ravindra Guravannavar; S Sudarshan,Algorithms that exploit sort orders are widely used to implement joins; grouping; duplicateelimination and other set operations. Query optimizers traditionally deal with sort orders byusing the notion of interesting orders. The number of interesting orders is unfortunatelyfactorial in the number of participating attributes. Optimizer implementations use heuristics toprune the number of interesting orders; but the quality of the heuristics is unclear.Increasingly complex decision support queries and increasing use of covering indices;which provide multiple alternative sort orders for relations; motivate us to better address theproblem of optimization with interesting orders. We show that even a simplified version of theproblem is NP-hard and give principled heuristics for choosing interesting orders. We haveimplemented the proposed techniques in a Volcano-style optimizer; and our performance …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,13,10
On-line reorganization in object databases,Mohana K Lakhamraju; Rajeev Rastogi; S Seshadri; S Sudarshan,Abstract Reorganization of objects in an object databases is an important component ofseveral operations like compaction; clustering; and schema evolution. The high availabilityrequirements (24× 7 operation) of certain application domains requires reorganization to beperformed on-line with minimal interference to concurrently executing transactions. In thispaper; we address the problem of on-line reorganization in object databases; where a set ofobjects have to be migrated from one location to another. Specifically; we consider the casewhere objects in the database may contain physical references to other objects. Relocatingan object in this case involves finding the set of objects (parents) that refer to it; andmodifying the references in each parent. We propose an algorithm called the IncrementalReorganization Algorithm (IRA) that achieves the above task with minimal interference to …,ACM SIGMOD Record,2000,13,8
Program analysis and transformation for holistic optimization of database applications,Karthik Ramachandra; Ravindra Guravannavar; S Sudarshan,Abstract We describe DBridge; a novel program analysis and transformation tool to optimizedatabase and web service access. Traditionally; rewrite of queries and programs are doneindependently; by the database query optimizer and the language compiler respectively;leaving out many optimization opportunities. Our tool aims to bridge this gap by performingholistic transformations; which include both program and query rewrite. There has beenearlier research in this area involving program analysis and transformation for automaticallyrewriting database applications to perform optimizations; for example; our earlier work hasaddressed batching or asynchronous submission of iterative queries; and prefetching queryresults. DBridge implements these techniques for Java programs and internally uses Soot; aJava optimization framework; for static analysis and transformation. DBridge can perform …,Proceedings of the ACM SIGPLAN International Workshop on State of the Art in Java Program analysis,2012,12,20
Database systems: Design; implementation,Peter Rob; Carlos Coronel; A Silberschatz; H Korth; S Sudarshan,*,Management. Seventh Edition. Course Technology,2006,12
Memory cognizant query optimization,Arvind Hulgeri; S Seshadri; S Sudarshan,ABSTRACT гбда м ей ж з б кн йз г г вИ ж и гв в згжи в гд ж и гвз в и з гд ж и гвз ж б бгжн вивЙ з к К Ьнд а гди б о жз ззйб аа и б бгжн иг к аЙ а иг гд ж игж в и ей жн иж К йи л а м йи вд д а в з б бгжн л аа и к бгв зи аа и гд ж Й игжз жйвв в з бйаи в гйзан в д д а в К Ь гзи г в гдж игж в ж аан д в з гв и к а а б бгжнК С и б бгжн ааг и иг в гд ж игж з а зз и в л и в гдЙ и б ож ззйб зИ гзи зи б и н и гди б о ж лгйа лжгв К Ь йз и ей жн гди б о и гв в б бгжн зиж йЙ и гвж ви ж д в ви в гв з д ж и ан б н вги н а зи ж зйаизК Ь ей жн гди б о ж з гйа вги гван гвз ж ииги а б бгжн к а а йи з гйа азг гл иг к и гди б аан бгв и гд ж игжз г и да вК Я з гл гл иг гди б оей жн к в и гзи к жзйз б бгжн ааг и гв йв и гв гж гд ж игжК Я к мЙ и в и Юга вг гди б о ж иг би б бгжн г в о виК Ш жи г и г г и гди б о ж з иг л иг д д а в в л иг аг К д д а в а в жг в Д К К гвкжи Е виг аг в К йи и Й з гв иг ж д д а в а д в з йдгв л и ж и миж б бгжн к а а иг в к й а д д а …,Proc. of COMAD,2000,12,10
Well-founded ordered search: Goal-directed bottom-up evaluation of well-founded models,Peter J Stuckey; S Sudarshan,Abstract There have been several evaluation mechanisms proposed for computing queryanswers based on the well-founded semantics; for programs with negation. However; thesetechniques are costly; in particular; for the special case of modularly stratified programs;Ordered Search is more efficient than the general-purpose techniques. However; OrderedSearch is applicable only to modularly stratified programs. In this paper; we extend OrderedSearch to compute the well-founded semantics for all (non-floundering) programs withnegation. Our extension behaves exactly like Ordered Search on programs that aremodularly stratified; and hence pays no extra cost for such programs.,The journal of logic programming,1997,12,20
Using codewords to protect database data from a class of software errors,Philip Bohannon; Rajeev Rastogi; S Seshadri; Abraham Silberschatz; S Sudarshan,Increasingly; for extensibility and performance; special-purpose application code is beingintegrated with database system code. Such application code has direct access to databasesystem buffers and; as a result; the danger of data being corrupted due to inadvertentapplication writes is increased. Previously proposed hardware techniques to protect datafrom corruption required system calls; and their performance depended on the details of thehardware architecture. We investigate an alternative approach which uses codewordsassociated with regions of data to detect corruption and to prevent corrupted data from beingused by subsequent transactions. We develop several such techniques which vary in thelevel of protection; space overhead; performance and impact on concurrency. Thesetechniques are implemented in the Dali/spl acute/main-memory storage manager; and …,Data Engineering; 1999. Proceedings.; 15th International Conference on,1999,11,19
Data generation for testing and grading SQL queries,Bikash Chandra; Bhupesh Chawda; Biplab Kar; KV Maheshwara Reddy; Shetal Shah; S Sudarshan,Abstract Correctness of SQL queries is usually tested by executing the queries on one ormore datasets. Erroneous queries are often the results of small changes or mutations of thecorrect query. A mutation Q'′ of a query Q is killed by a dataset D if Q (D) ≠≠ Q'′(D).Earlier work on the XData system showed how to generate datasets that kill all mutations ina class of mutations that included join type and comparison operation mutations. In thispaper; we extend the XData data generation techniques to handle a wider variety of SQLqueries and a much larger class of mutations. We have also built a system for grading SQLqueries using the datasets generated by XData. We present a study of the effectiveness ofthe datasets generated by the extended XData approach; using a variety of queriesincluding queries submitted by students as part of a database course. We show that the …,The VLDB Journal,2015,10,15
Program transformations for asynchronous and batched query submission,Karthik Ramachandra; Mahendra Chavan; Ravindra Guravannavar; S Sudarshan,The performance of database/web-service backed applications can be significantlyimproved by asynchronous submission of queries/requests well ahead of the point wherethe results are needed; so that results are likely to have been fetched already when they areactually needed. However; manually writing applications to exploit asynchronous querysubmission is tedious and error-prone. In this paper; we address the issue of automaticallytransforming a program written assuming synchronous query submission; to one thatexploits asynchronous query submission. Our program transformation method is based ondata flow analysis and is framed as a set of transformation examples. Our examples canhandle query executions within loops; unlike some of the earlier work in this area. We alsopresent a novel approach that; at runtime; can combine multiple asynchronous requests …,IEEE Transactions on Knowledge and Data Engineering,2015,10,3
The CORAL deductive database system,Raghu Ramakrishnan; William G Roth; Praveen Seshadri; Divesh Srivastava; S Sudarshan,CORAL[4; 5] is a deductive database system that supports a powerful declarative querylanguage. The language supports general Horn clauselogic programs; extended with SQL-style groupiug; set-generation; aud negation. Programs can be organized intoindependently optimiied modules; and users can provide optimization hints in the form ofhigh-level annotations. The system supports a wide variety of optimizw tion techniques.There is art interface to C++ that enables programs to be written in a combination ofimperative and declarative styles; C++ code can be called from declarative programs; andvice versa. A notable feature of the CORAL system is that it is extensible. In particular; newdata types can be defined; and new relation and index implementations can be added. Aninterface to the EXODUS storage manager [2] provides support for disk-resident data …,ACM SIGMOD Record,1993,10,0
X-data: Generating test data for killing SQL mutants,Bhanu Pratap Gupta; Devang Vira; S Sudarshan,Checking if an SQL query has been written correctly is not an easy task. Formal verificationis not applicable; since it is based on comparing a specification with an implementation;whereas SQL queries are essentially a specification without any implementation. Thus; thestandard approach for testing queries is to manually check query results on test datasets.Intuitively; a mutant is a query variant that could have been the correct query if the query wasin error; a mutant is killed by a dataset if the original query and the mutant return differentresults on the dataset. In this paper; we address the problem of generation of test data for anSQL query; to kill mutants. Our work focuses in particular on a class of join/outer-joinmutants; which are a common cause of error. To minimize human effort in testing; ourtechniques generate a test suite containing small and intuitive test datasets; combining …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,9,3
A fast algorithm for computing sparse visibility graphs,S Sudarshan; C Pandu Rangan,Abstract An O (¦ E¦ log 2 n) algorithm is presented to construct the visibility graph for acollection of n nonintersecting line segments; where¦ E¦ is the number of edges in thevisibility graph. This algorithm is much faster than the O (n 2)-time and O (n 2)-spacealgorithms by Asano et al.; and by Welzl; on sparse visibility graphs. Thus we partiallyresolve an open problem raised by Welzl. Further; our algorithm uses only O (n) workingstorage.,Algorithmica,1990,9,20
Improving predictability of transaction execution times in real-time databases,Rajeev Rastogi; S Seshadri; Philip Bohannon; Dennis Leinbaugh; Avi Silberschatz; S Sudarshan,Abstract We present a design for multi-versionconcurrency control and recovery in a mainmemory database; anddescribe logical and physical versioning schemes that allowread-only transactions to execute without obtaining data itemlocks or system latches. Ourschemes enable a system to providethe guarantee that updaters will never interfere withread-onlytransactions; and read-only transactions will not be delayeddue to data contention.Consequently; transaction executionsbecome more predictable—this partially alleviates amajorproblem in real-time database system (RTDBS) scheduling; namely; significantunpredictability in transaction execution times. As a result; in addition to a transaction'sdeadline; a moreaccurate estimate of its execution time can also be taken intoaccount; thusfacilitating better scheduling decisions. Our contributionsinclude several space saving …,Real-Time Systems,2000,8,0
Space optimization in the bottom-up evaluation of logic programs,S Sudarshan; Divesh Srivastava; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of a logic program; all generated facts are usuallyassumed to be stored until the end of the evaluation. Considerable gains can be achievedby instead discarding facts that are no longer required: the space needed to evaluate theprogram is reduced; 1/0 costs may be reduced; and the costs of maintaining and accessingindices; eliminating duplicates etc. are reduced. Thus; discarding facts early could achievetime as well as space improvements. Given an evaluation method that is sound; completeand does not repeat derivation steps; we consider how facts can be discarded during theevaluation without compromising these properties. Our first contribution is to show that sucha space optimization technique has three distinct components. Informally; we must make allderivations that we can with each fact; detect all duplicate derivations of facts and try to …,ACM SIGMOD Record,1991,8,15
Extracting equivalent sql from imperative code in database applications,K Venkatesh Emani; Karthik Ramachandra; Subhro Bhattacharya; S Sudarshan,Abstract Optimizing the performance of database applications is an area of practicalimportance; and has received significant attention in recent years. In this paper we presentan approach to this problem which is based on extracting a concise algebraic representationof (parts of) an application; which may include imperative code as well as SQL queries. Thealgebraic representation can then be translated into SQL to improve applicationperformance; by reducing the volume of data transferred; as well as reducing latency byminimizing the number of network round trips. Our techniques can be used for performingoptimizations of database applications that techniques proposed earlier cannot perform. Thealgebraic representations can also be used for other purposes such as extracting equivalentqueries for keyword search on form results. Our experiments indicate that the techniques …,Proceedings of the 2016 International Conference on Management of Data,2016,7,19
Decorrelation of user defined function invocations in queries,Varun Simhadri; Karthik Ramachandra; Arun Chaitanya; Ravindra Guravannavar; S Sudarshan,Queries containing user-defined functions (UDFs) are widely used; since they allow queriesto be written using a mix of imperative language constructs and SQL; thereby increasing theexpressive power of SQL; further; they encourage modularity; and make queries easier tounderstand. However; not much attention has been paid to their optimization; except forsimple UDFs without imperative constructs. Queries invoking UDFs with imperativeconstructs are executed using iterative invocation of the UDFs; leading to poor performance;especially if the UDF contains queries. Such poor execution has been a major deterrent tothe wider usage of complex UDFs. In this paper we present a novel technique to decorrelateUDFs containing imperative constructs; allowing set-oriented execution of queries thatinvoke UDFs. Our technique allows imperative execution to be modeled using the Apply …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,7,19
Keyword search on form results,Aditya Ramesh; S Sudarshan; Purva Joshi; Manisha Naik Gaonkar,Abstract In recent years there has been a good deal of research in the area of keywordsearch on structured and semistructured data. Most of this body of work has a significantlimitation in the context of enterprise data; since it ignores the application code that has oftenbeen carefully designed to present data in a meaningful fashion to users. In this work; weconsider how to perform keyword search on enterprise applications; which provide anumber of forms that can take parameters; parameters may be explicit; or implicit such as theidentifier of the user. In the context of such applications; the goal of keyword search is; givena set of keywords; to retrieve forms along with corresponding parameter values; such thatresult of each retrieved form executed on the corresponding retrieved parameter values willcontain the specified keywords. Some earlier work in this area was based on creating …,The VLDB Journal,2013,7,0
User interaction in the BANKS system: a demonstration,B Aditya; Soumen Chakrabarti; Rushi Desai; Arvind Hulgeri; Hrishikesh Karambelkar; Rupesh Nasre; S Sudarshan,The BANKS system supports keyword search on databases storing structured/semi-structured data. Answers to keyword queries are ranked. As in Information Retrieval (IR)systems; the top answers may not be exactly what a user is looking for. Further interactionwith the system is required to narrow in on desired answers. We describe some of the newfeatures that we have added to the BANKS system to improve user interaction. Theseinclude an extended query model; richer support for user feedback and better display ofanswers.,*,2003,7,19
Space optimization in deductive databases,Divesh Srivastava; S Sudarshan; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of logic programs and recursively defined views ondatabases; all generated facts are usually assumed to be stored until the end of theevaluation. Discarding facts during the evaluation; however; can considerably improve theefficiency of the evaluation: the space needed to evaluate the program; the I/O costs; thecosts of maintaining and accessing indices; and the cost of eliminating duplicates may all bereduced. Given an evaluation method that is sound; complete; and does not repeatderivation steps; we consider how facts can be discarded during the evaluation withoutcompromising these properties. We show that every such space optimization method hascertain components; the first to ensure soundness and completeness; the second to avoidredundancy (ie; repetition of derivations); and the third to reduce “fact lifetimes”(ie; the …,ACM Transactions on Database Systems (TODS),1995,7,1
Method for optimizing performance of database/web-service backed applications by automatically prefetching query results,*,The present disclosure proposes the method for optimizing the performance of database/web-service backed applications by automatically prefetching query results. Theproposed system and methods automatically insert prefetch instructions at the earliestpossible points across procedure calls in application source code; in presence of conditionalbranching and loops. A data flow analysis technique called anticipable expressions analysisis extended; to analyze anticipability of queries. The benefit of prefetching is limited due tothe presence of assignment statements and conditional branches that precede the queryexecution statement. Enhancements such as code motion; chaining and rewriting prefetchrequests are devised to increase benefits of prefetching. These techniques performequivalence preserving program and query transformations.,*,2016,6,4
Ubersleep: An innovative mechanism to save energy in IEEE 802.11 based WLANs,S Sudarshan; Rajan Prasad; Abhishek Kumar; Rahul Bhatia; Bheemarjuna Reddy Tamma,Perpetually rising energy costs are forcing research communities to focus their efforts on areduction of the global CO 2 footprint; and since Information and CommunicationTechnologies (ICT) account for a significant percentage of the overall global energyconsumption; energy efficiency is becoming increasingly important in the operation of ICTinfrastructure; especially in enterprise and data center networks. Simultaneously; theproliferation of devices such as smartphones and tablets; which have to use battery poweredwireless radio adapters; and whose battery life is dependent on the power consumption ofthe radio adapters used for wireless communication indicates that efficient radio powersaving strategies are needed to increase the battery life of such devices. This paper extendsan earlier proposal by [1]; and examines the performance of the same on IEEE 802.11 n …,Electronics; Computing and Communication Technologies (IEEE CONECCT); 2014 IEEE International Conference on,2014,6,20
STAR: A System for Tuple and Attribute Ranking of Query Answers.,Nishant Kapoor; Gautam Das; Vagelis Hristidis; S Sudarshan; Gerhard Weikum,Abstract In recent years there has been a great deal of interest in developing effectivetechniques for ad-hoc search and retrieval in structured repositories such as relationaldatabases-eg; searching online databases of homes; used cars; and electronic goods. Inmany of these applications; the user often experiences “information overload”; which occurswhen the system responds to an under-specified user query by returning an overwhelmingnumber of tuples; each displayed with a huge number of features (or attributes). We havedeveloped a search and retrieval system that tackles this information overload problem fromtwo angles. First; we show how to automatically rank and display the top-n most relevanttuples. Second; our system offers techniques for ordering the attributes of the returned tuplesin decreasing order of “usefulness” and selects only a few of the most useful attributes to …,ICDE,2007,6,20
Souvenir purchase patterns of domestic tourists: Case study of Takayama city; Japan,M Nomura,*,Menomonie; Wisconsin: University of Wisconsin-Stout,2002,6
Exploiting Asynchronous IO using the Asynchronous Iterator Model.,Suresh Iyengar; S Sudarshan; Santosh Kumar; Raja Agrawal,Abstract Asynchronous IO (AIO) allows a process to continue to do other work while an IOoperation initiated earlier completes. AIO allows a large number of random IO operations tobe issued at once; allowing the disk subsystem to order access to data on disk; reducingaverage seek times considerably; as well as allowing much better utilization of disks in amulti-disk RAID environments where reads can be done in parallel across disks. In thispaper we address the issue of how to extend a database query execution engine to exploitasynchronous IO. To best exploit AIO; we propose a new iterator model called theAsynchronous Iterator Model; where a getnext () call on an operator can return a statusLATER instead of blocking on an IO; permitting other actions to be initiated while an IO ispending. We show how to modify the implementation of Index Nested Loop (INL) join by …,COMAD,2008,5,10
Viper: A vertical approach to mining association rules,P Shenoy; J Haritsa; S Sudarshan; G Bhalotia; M Bawa; D Shah,The classical association rule mining algorithms assume a horizontal data layout; whereineach row in the database records a transcation; and the items present in the transaction. Oflate there has been considerable interest in alternative vertical data representations;wherein each item is associated with a column of values representing the transactions inwhich it is present. The vertical mining algorithms that have been proposed showperformance improvements over their horizontal counterparts; but suffer from somelimitations--they are either efficient only for certain database sizes; or assume specificcharacteristics of the database contents; or are applicable only to special kinds of databaseschemas. To address the above limitations; we present a new vertical mining algorithmcalled VIPER (Vertical Itemset Partitioning for Efficient Rule-extraction). VIPER is a" …,*,2000,5,20
The DataBlitz Main-Memory Storage Manager: Architecture; Performance; and Experience,Jerry D Baulier; Philip Bohannon; Amit Khivesara; Henry F Korth; Rajeev Rastogi; Avi Silberschatz; S Sudarshan; Brian Sayrs,ABSTRACT General-purpose commercial database systems; though widely used; fail tomeet the performance requirements of applications requiring short; predictable responsetimes; and extremely high throughput rates. As a result; most high performance applicationsare custom designed and lack the flexibility needed to adapt to unforeseen; evolvingrequirements. In the military domain; command centers often contain numerous “stovepipe”systems unable to share data easily. The need for improved data management is apparentwith the rapid growth of communication networks and the increasing demand by end usersfor network-centric solutions that require flexibility and high performance. These applicationsshare the need for real-time response to a dynamically changing external environment; theneed to store a substantial amount of data; and the need to process transactions that …,The VLDB Journal,1998,5,0
Dbridge: Translating imperative code to sql,K Venkatesh Emani; Tejas Deshpande; Karthik Ramachandra; S Sudarshan,Abstract Application programs that access data located remotely (such as in a database)often perform poorly due to multiple network round trips and transfer of unused data. Thissituation is exacerbated in applications that use object-relational mapping (ORM)frameworks such as Hibernate; as developers tend to express complex query logic usingimperative code; resulting in poor performance. DBridge is a system for optimizing dataaccess in database applications by using static program analysis and programtransformations. Recently; we incorporated a new suite of optimization techniques intoDBridge. These techniques optimize database application programs by identifying relationaloperations expressed in imperative code; and translating them into SQL. In thisdemonstration; we showcase these techniques using a plugin for the IntelliJ IDEA Java …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,4,3
The XDa-TA system for automated grading of SQL query assignments,Amol Bhangdiya; Bikash Chandra; Biplab Kar; Bharath Radhakrishnan; KV Maheshwara Reddy; Shetal Shah; S Sudarshan,Grading of student SQL queries is usually done by executing the query on sample datasets(which may be unable to catch many errors) and/or by manually comparing/checking astudent query with the correct query (which can be tedious and error prone). In thisdemonstration we present the XDa-TA system which can be used by instructors and TAs forgrading SQL query assignments automatically. Given one or more correct queries for anSQL assignment; the tool uses the XData system to automatically generate datasets that aredesigned specifically to catch common errors. The grading is then done by comparing theresults of student queries with those of the correct queries against these generated datasets;instructors can optionally provide additional datasets for testing. The tool can also be used ina learning mode by students; where it can provide immediate feedback with hints …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,4,1
Optimizing join enumeration in transformation-based query optimizers,Anil Shanbhag; S Sudarshan,Abstract Query optimizers built on the Volcano/Cascades framework; which is based ontransformation rules; are used in many commercial databases. Transformation rulesetsproposed earlier for join order enumeration in such a framework either allow enumeration ofjoins with cross-products (which can significantly increase the cost of optimization); orgenerate a large number of duplicate derivations. In this paper we propose two new rulesetsfor generating cross-product free trees. One of the rulesets is a minor extension of a simplebut inefficient ruleset; which we prove is complete (we also show that a naive extension ofan efficient ruleset leads to incompleteness). We then propose an efficient new ruleset;which is based on techniques proposed recently for top-down join order enumeration; butunlike earlier work it is cleanly integrated into the Volcano/Cascades framework; and can …,Proceedings of the VLDB Endowment,2014,4,0
Extending XData to kill SQL query mutants in the wild,Bikash Chandra; Bhupesh Chawda; Shetal Shah; S Sudarshan; Ankit Shah,Abstract SQL queries are usually tested for correctness by executing them on one or moredatasets; to see if they give the desired results on each dataset. Erroneous queries are oftenthe result of small changes; or mutations; of the correct query. Earlier work on the XDatasystem showed how to generate datasets that kill all mutations in a class of mutations thatincluded join type and comparison operation mutations. However; the system could nothandle a number of commonly used SQL features. In this paper we extend the XData datageneration techniques to handle features such as null values; string constraints; aggregationwith constraints on aggregation results; and a class of subqueries; amongst others. Wepresent a study of the effectiveness of our data generation approach for correcting studentSQL assignments that were part of a database course. The datasets generated by XData …,Proceedings of the Sixth International Workshop on Testing Database Systems,2013,4,10
Entity ranking and relationship queries using an extended graph model,Ankur Agrawal; S Sudarshan; Ajitav Sahoo; Adil Anis Sandalwala; Prashant Jaiswal,Abstract There is a large amount of textual data on the Web and in Wikipedia; wherementions of entities (such as Gandhi) are annotated with a link to the disambiguated entity(such as MK Gandhi). Such annotation may have been done manually (as in Wikipedia) orcan be done using named entity recognition/disambiguation techniques. Such an annotatedcorpus allows queries to return entities; instead of documents. Entity ranking queries retrieveentities that are related to keywords in the query and belong to a given type/categoryspecified in the query; entity ranking has been an active area of research in the past fewyears. More recently; there have been extensions to allow entity-relationship queries; whichallow specification of multiple sets of entities as well as relationships between them. In thispaper we address the problem of entity ranking (" near") queries and entity-relationship …,Proceedings of the 18th International Conference on Management of Data,2012,4,0
Multi-query optimization,Prasan Roy; S Sudarshan,Advances in high throughput sequencing and ''omics''technologies and the resultingexponential growth in the amount of macromolecular sequence; structure; gene expressionmeasurements; have unleashed a transformation of biology from a data-poor science into anincreasingly data-rich science. Despite these advances; biology today; much like physicswas before Newton and Leibnitz; has remained a largely descriptive science. Machinelearning [6] currently offers some of the most cost-effective tools for building predictivemodels from biological data; eg; for annotating new genomic sequences; for predictingmacromolecular function; for identifying functionally important sites in proteins; for identifyinggenetic markers of diseases; and for discovering the networks of genetic interactions thatorchestrate important biological processes [3]. Advances in machine learning eg …,*,2009,4,1
Adaptive query processing,Kamlesh Laddhad; S Sudarshan,Abstract Researchers are attempting to architect and implement a continuously adaptivequery engine suitable for global-area systems and sensor networks. As query engines arescaled and federated; they must cope with highly unpredictable and changeableenvironments. Numerous limitations such as poor cost models; data correlations; changingsystem resources; changing data distributions; etc have surfaced in traditional cost-basedquery optimization. One promising technique to tackle these limitations is to abandon theoptimize-then-execute model of query processing; but instead interleave optimization andexecution in an adaptive fashion. There has been a lot of research on query processing inadaptive environment in recent years. Some propose evolutionary solutions such aschanging query plans mid-flight; while others propose to do away with query plans …,*,2006,3,0
Database management systems,Abraham Silberschatz; Henry F Korth; S Sudarshan,*,*,1998,3
Multi-level recovery in the dali main-memory storage manager,Philip Bohannon; Rajeev Rastogi; Avi Silberschatz; S Sudarshan,*,From Dali Homepage; AT&T,1996,3
Introduction to Data base Management System,Abraham Silberschatz; Henry F Korth; S Sudarshan,Similar to types and variables in programming languages Schema–the logical structure ofthe database eg; the database consists of information about a set of customers and accountsand the relationship between them) Analogous to type information of a variable in a programPhysical schema: database design at the physical level Logical schema: database design atthe logical level Instance–the actual content of the database at a particular point in timeAnalogous to the value of a variable Physical Data Independence–the ability to modify thephysical schema without changing the logical schema Applications depend on the logicalschema In general; the interfaces between the various levels and components should bewell defined so that changes in some parts do not seriously influence others.,*,2016,2,3
A study of locus of control among distance learning pursuing professional in Bangalore,Nita Choudhary; S Sudarshan; Niranjan Kumar Singh,Locus of control developed by Rotter (1954) is considered to be an important aspect ofpersonality. One's 'locus' can either be internal (the person believes that they control theirlife) or external (the person believe that their environment; some higher power; or otherpeople control their decisions and their life). This paper aims to study the locus of control ofmen and women employed in various sectors (mainly IT) at executive level and studyingMBA in distance learning unit of Sikkim Manipal University of Health; Medical andTechnological Sciences. For conducting this study; Rotter's scale was used having 23 itemswith two options 'a'and 'b'. This study consisted of total of 74 respondents; consisting of 54male and 20 female employed in Bangalore. The results obtained after analysis support theconclusion that men are more internal than women and believe that events result …,Middle East Journal of Management,2014,2,20
Graph clustering for keyword search,K Rose Catherine; S Sudarshan,Abstract Clustering is the process of finding out a grouping of the given set of objects; suchthat those in the same collection are similar to each other. This is important because itreveals the high level organization of the data. It is also important from the point of view ofkeyword searching in graph representation of data. Identifying graph nodes that are highlyrelated to each other; and clustering them together; can localize the computing required toanswer a particular keyword query; to a single or a few clusters. In the case that the graph isstored in external memory; it is possible to achieve good recall by exploring only a smallportion of the graph which corresponds to these few relevant clusters. In the case that thesearch is distributed; splitting the data in accordance with the clustering will reduce theamount of inter-processor communication. Thus; creating good quality clustering of the …,Proceedings of the 15th International Conference on Management of Data,2009,2,0
Distributed Databases,Y Breitbart; HF Korth; A Silberschatz; S Sudarshan,overview of transaction processing. This follows with a description of concurrency controland distributed commit pro-The importance of information in most organizations has led tothe development of a large body of concepts and techniques cessing. The section entitled''Replication of Data''describes replication issues in a distributed database system. Thisartifor the efficient management of data. Distributing data across sites or departments in anorganization allows those data to cle concludes with an annotated list of key material forfurther study. reside where they are generated or are most needed; but still to be accessiblefrom other sites and from other departments. A distributed database system (DDS) is asoftware system that Distributed Database System Overview gives users transparent accessto data; along with the ability To illustrate a distributed database system; let us consider a …,Wiley Encyclopedia of Electrical and Electronics Engineering,1999,2,20
Runtime optimization of join location in parallel data management systems,Bikash Chandra; S Sudarshan,Abstract Applications running on parallel systems often need to join a streaming relation or astored relation with data indexed in a parallel data storage system. Some applications alsocompute UDFs on the joined tuples. The join can be done at the data storage nodes;corresponding to reduce side joins; or by fetching data from the storage system to computenodes; corresponding to map side join. Both may be suboptimal: reduce side joins maycause skew; while map side joins may lead to a lot of data being transferred and replicated.In this paper; we present techniques to make runtime decisions between the two options ona per key basis; in order to improve the throughput of the join; accounting for UDFcomputation if any. Our techniques are based on an extended ski-rental algorithm andprovide worst-case performance guarantees with respect to the optimal point in the space …,Proceedings of the VLDB Endowment,2017,1,10
Efficient and Provable Multi-Query Optimization,Tarun Kathuria; S Sudarshan,Abstract Complex queries for massive data analysis jobs have become increasinglycommonplace. Many such queries contain common subexpressions; either within a singlequery or among multiple queries submitted as a batch. Conventional query optimizers do notexploit these subexpressions and produce sub-optimal plans. The problem of multi-queryoptimization (MQO) is to generate an optimal combined evaluation plan by computingcommon subexpressions once and reusing them. Exhaustive algorithms for MQO explore anO (nn) search space. Thus; this problem has primarily been tackled using various heuristicalgorithms; without providing any theoretical guarantees on the quality of their solution. Inthis paper; instead of the conventional cost minimization problem; we treat the problem asmaximizing a linear transformation of the cost function. We propose a greedy algorithm for …,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,1,20
Partial marking for automated grading of SQL queries,Bikash Chandra; Mathew Joseph; Bharath Radhakrishnan; Shreevidhya Acharya; S Sudarshan,Abstract The XData system; currently being developed at IIT Bombay; provides anautomated and interactive platform for grading student SQL queries; as well as for learningSQL. Prior work on the XData system focused on generating query specific test cases tocatch common errors in queries. These test cases are used to check whether the studentqueries are correct or not. For grading student assignments; it is usually not sufficient to justcheck if a query is correct: if the query is incorrect; partial marks may need to be given;depending on how close the query is to being correct. In this paper; we extend the XDatasystem by adding features that enable awarding of partial marks to incorrect student queries.Our system is able to go beyond numerous syntactic features when comparing a studentquery with a correct query. These features of our grading system allow the grading of SQL …,Proceedings of the VLDB Endowment,2016,1,10
Data-based research at IIT Bombay,Soumen Chakrabarti; Ganesh Ramakrishnan; Krithi Ramamritham; Sunita Sarawagi; S Sudarshan,The Indian Institute of Technology (IIT) Bombay has a history of research and developmentin the area of databases; dating back to the early 1980s. DB Phatak and NL Sarda wereamong the first faculty members at IIT Bombay to work in the area of database systems. Thiswas a period when the financial sector of India; headquartered primarily in Bombay (nowrenamed Mumbai) saw a spurt in computerization; and IIT Bombay faculty played a leadingrole as consultants for database implementations in these companies. Research in the areaof databases began in the early 1980s; but increased greatly from the early 1990s; with thehiring of several faculty including S. Seshadri; S. Sudarshan; and later Krithi Ramamritham;who moved to IIT Bombay from U. Mass. Amherst in the early to mid 1990s. With the hiring ofSunita Sarawagi and Soumen Chakrabarti in the late 1990s; there was a significant …,ACM SIGMOD Record,2013,1,22
Which sort orders are interesting?,Ravindra Guravannavar; S Sudarshan; Ajit A Diwan; Ch Sobhan Babu,Abstract Sort orders play an important role in query evaluation. Algorithms that rely onsorting are widely used to implement joins; grouping; duplicate elimination and other setoperations. The notion of interesting orders has allowed query optimizers to consider plansthat could be locally sub-optimal; but produce ordered output beneficial for other operators;and thus be part of a globally optimal plan. However; the number of interesting orders formost operators is factorial in the number of attributes involved. Optimizer implementationsuse heuristics to prune the number of interesting orders; but the quality of the heuristics isunclear. Increasingly complex decision support queries and increasing use of query-covering indices; which provide multiple alternative sort orders for relations; motivate us tobetter address the problem of choosing interesting orders. We show that even a simplified …,The VLDB Journal,2012,1,20
Distributed event delivery model for collaborative virtual simulations,Neha Singh; S Sudarshan,Abstract Networked Virtual Environments (NVEs) are computer generated; synthetic worldsthat allow simultaneous interactions of multiple participants. IP multicast and applicationlayer multicasting has been used for supporting collaborative virtual simulations. Recentlymany research work has focused on ways to partition this virtual space onto peer-to-peeroverlay. The characters in these virtual environment; either human controlled or computergenerated; need the state of the surrounding environment to decide their next move.However in doing so; they are interested in the state of virtual environment in a small areaaround them; called their Area of Interest (AoI). Thus when the virtual space gets partitionedacross multiple sites; we need an efficient peer-to-peer communication model to get thesurrounding state information that has been split across various sites. In this paper; we …,Proceedings of the 2008 ACM CoNEXT Conference,2008,1,10
Bidirectional proximity search in graph databases,Rushi Desai; Varun Kacholiya; Arvind Hulgeri; Hrishikesh Karambelkar; Shashank Pandit; Soumen Chakrabarti; S Sudarshan,*,Proceedings of 30th VLDB Conference,2004,1
Cobra: A Framework for Cost Based Rewriting of Database Applications,K Venkatesh Emani; S Sudarshan,Abstract: Database applications are typically written using a mixture of imperative languagesand declarative frameworks for data processing. Application logic gets distributed across thedeclarative and imperative parts of a program. Often; there is more than one way toimplement the same program; whose efficiency may depend on a number of parameters. Inthis paper; we propose a framework that automatically generates all equivalent alternativesof a given program using a given set of program transformations; and chooses the least costalternative. We use the concept of program regions as an algebraic abstraction of a programand extend the Volcano/Cascades framework for optimization of algebraic expressions; tooptimize programs. We illustrate the use of our framework for optimizing databaseapplications. We show through experimental results; that our framework has wide …,arXiv preprint arXiv:1801.04891,2018,*,0
Decorrelation of user-defined function invocations in queries,*,Systems; methods; and computer-readable medium; are disclosed for transforming user-defined-function invocations in a query-based environment. A user-defined-function (UDF)and a query invoking the UDF are received. The UDF is parsed into a plurality of statements.A first expression tree corresponding to the UDF and a second expression treecorresponding the query are constructed; and merged using an operator to generate atransformed expression. The transformed expression is simplified; using transformationrules; if it is determined that is can be simplified.,*,2015,*,20
Subject: Application for financial support,S Sudarshan,*,*,2015,*
Greedy Awakens: Efficient and Provable Multi-Query Optimization.,Tarun Kathuria; S Sudarshan,ABSTRACT Complex queries for massive data analysis jobs have become increasinglycommonplace. Many such queries contain common sub-expressions; either within a singlequery or among multiple queries submitted as a batch. Conventional query optimizers do notexploit these common sub-expressions and produce sub-optimal plans. The problem ofmulti-query optimization (MQO) is to generate an optimal combined evaluation plan bycomputing common sub-expressions once and reusing them. Exhaustive algorithms forMQO explore an O (nn) search space. Thus; this problem has primarily been tackled usingvarious heuristic algorithms; without providing any theoretical guarantees on the quality ofthe solution obtained. In this paper; instead of the conventional cost minimization problem;we treat the problem as maximizing a linear transformation of the cost function. We …,CoRR,2015,*,0
Big data: from querying to transaction processing,Karthik Ramachandra; S Sudarshan,Abstract The term Big Data has been used and abused extensively in the past few years;and means different things to different people. A commonly used notion says Big Data isabout" volume"(of data);" velocity"(rate at which data is inserted/updated) and" variety"(ofdata types). In this tutorial; we use the term Big Data to refer to any data processing needthat requires a high degree of parallelism. In other words; we focus primarily on the" volume"and" velocity" aspects.,Proceedings of the 19th International Conference on Management of Data,2013,*,6
Letter from the Special Issue Editor.,S Sudarshan,Extremely large amounts of data are generated routinely today; by a number of sources suchas Web sites; genome sequence and microarray data; network monitoring data; sensor data;and many other kinds of sources; such data is commonly referred to as “Big Data”. The map-reduce paradigm has proven particularly successful for processing such Big Data. Theavailability of open-source map-reduce implementations such as Hadoop has lead towidespread use of the map-reduce paradigm. However; many data processing needs thatcan be expressed very concisely in SQL; need an inordinate amount of coding effort in amap-reduce system; since the programmer has to express the logic of the computationimperatively; and further has to make choices about the implementation. The aboveproblems with processing of Big Data have been addressed using two complementary …,IEEE Data Eng. Bull.,2013,*,10
Madison WI 53706; USA,RAGHU RAMAKRISHNAN; DIVESH SRIVASTAVA; S SUDARSHAN,Abstract. In recent years; much work has been directed towards evaluating logic programsand queries on deductive databases by using an iterative bottom-up fixpoint computation.The resulting techniques offer an attractive alternative to Prolog-style top-down evaluation inseveral situations. They are sound and complete for positive Horn clause programs; are well-suited to applications with large volumes of data (facts); and can support a variety ofextensions to the standard logic programming paradigm. We present the basics of databasequery evaluation and logic programming evaluation; and then discuss bottom-up fixpointevaluation. We discuss an approach based upon using a program transformation (“MagicTemplates”) to restrict search; followed by fixpoint computation using a technique (“Semi-naive evaluation”) that avoids repeated inferences. The program transformation …,Computer Systems and Software Engineering: State-of-the-art,2012,*,10
DBridge: A Program Rewrite Tool for Holistic Optimization of Database Applications,Karthik Ramachandra; Ravindra Guravannavar; S Sudarshan,*,*,2012,*
Holistic Approaches for Robustness and Optimization of database applications,Karthik S Ramachandra; S Sudarshan,Abstract Traditional query optimization and compiler optimization techniques have evolvedindependently over a long time thanks to the extensive research that has been happening inthese areas. Though there are still many hard unsolved problems in those areas; we cansafely claim that the techniques that have evolved are mature and robust. However; inpractice; we see that there is a large class of applications that fall under the category of'database applications' ie; the applications which operate and maintain the data using arelational database. Now; the optimization of such applications is typically done using theabove independent techniques. Clearly; these techniques do not guarantee the globaloptimal execution of the application. Finding such a global optimum requires a holisticviewpoint of the database application as a single entity. Achieving this goal involves the …,*,2009,*,14
Keyword Search on Graph-Structured Data,S Sudarshan; Soumen Chakrabarti; Gaurav Bhalotia; Charuta Nakhe; Arvind Hulgeri,Page 1. Keyword Search on Graph-Structured Data S. Sudarshan CSE Dept; IIT Bombay Jointwork with Prof. Soumen Chakrabarti; Gaurav Bhalotia; Charuta Nakhe; Arvind Hulgeri; VarunKacholia; Shashank Pandit; Rushi Desai; Hrishi K.; Bhavana Dalvi and Meghana Kshirsagar;et al. Jan 2009 Page 2. 2 Keyword Search on Semi-Structured Data ∎ Keyword search ofdocuments on the Web has been enormously successful ∎ Much data is resident in databases ∎Organizational; government; scientific; medical data ∎ Deep web ∎ Goal (circa 2002/3): keywordquerying of data from ∎ relational databases ∎ multiple data sources; with different data models ∎Often with no schema or partially defined schema ∎ Extra goals (circa 2009): ∎ Web search:from documents to entities ∎ eg Google squared Page 3. 3 Keyword Search on Structured/Semi-Structured Data ∎ Key differences from IR/Web Search …,*,2009,*,0
Concurrency Control in Distributed MRA Index Structure.,Neha Singh; S Sudarshan,Abstract Answering aggregate queries like sum; count; min; max over regions containingmoving objects is often needed for virtual world applications; real-time monitoring systems;etc. Since the data set is usually very large and some queries require significant processingresources; quite often such data is stored in a distributed system wherein each systemhandles a partition of the whole space and manages all objects in that partition. Objectskeep switching from one system to another as they change their location. Currently there areno known efficient techniques for getting aggregates over moving objects while ensuringthat their position updates remains atomic to the read. We introduce an efficient techniquefor finding aggregates over mobile objects with data stored in a distributed system byextending the multi-resolution aggregate trees to work in a distributed system and over …,COMAD,2008,*,15
Query result caching in data warehouses and data marts,Prasan Roy; Jinesh Vora; Krithi Ramamritham; S Seshadri; S Sudarshan,Abstract In data warehouse and data mart systems; queries often take a long time to executedue to their complex nature. A single user may submit a sequence of related queries; andqueries submitted by dierent users may also have commonalities. Query response times canbe greatly improved by caching nal/intermediate results of previous queries; and using themto answer later queries. Further; in a distributed environment; caching query results locallycan also reduce communication time. In this paper we describe a caching middlewaresystem; which provides both the above functionalities. It can be used in centralizedwarehouse settings; as well as in distributed data mart settings. The system incorporatesseveral novel features; revolving around a tight coupling of the query optimizer and thecache manager. In particular; the tight coupling allows us to implement better cache …,*,1999,*,20
Database Research at the Indian Institute of Technology; Bombay,DB Phatak; Nandlal L.  Sarda; S Seshadri; S Sudarshan,The Indian Institute of Technology; Bombay is one of the leading universities in India.Located in Powai; a suburb of the vibrant city of Bombay (which is soon to revert to itsoriginal name; Mumbai); it is a scenic campus extending over 500 acres on the shores ofLake Powai. The institute has a faculty strength of about 400; and has about 2500 students.The Department of Computer Science has a faculty strength of 25; and around 150undergraduate and 70 postgraduate students. The Database Group in the Department ofComputer Science and Engineering is the largest database group in India. The groupcurrently has four faculty members; DB Phatak; NL Sarda; S. Seshadri and S. Sudarshan.The group also currently has three research scholars; ten Masters students; tenundergraduate students and nine project engineers. Apart from engaging in active basic …,ACM SIGMOD Record,1996,*,17
Clustering Techniques for Minimizing External Path Length,AA Diwan Sanjeeva Rane S Seshadri; S Sudarshan,*,Proceedings of the... International Conference on Very Large Data Bases,1996,*
BOTTOM-UP QUERY EVALUATION FOR DEDUCTIVE DATABASES,SUNDARARAJARAO SUDARSHAN,Abstract Deductive databases extend the power of traditional database query languagessuch as SQL by allowing recursive deﬁnitions of predicates. Bottom-up query evaluation isan important query evaluation mechanism for deductive databases and logic programs. Inrecent years; deductive databases have been extended by allowing facts to contain complexterms that can possibly include variables; and by allowing the use of aggregate operationson sets of answers. This thesis addresses optimization issues related to these extensions. Inthe ﬁrst part of the thesis we compare bottom-up and Prolog query evaluation. We show thatusing existing techniques; bottom—up evaluation performs no more “actions” than (a modelof) Prolog for a re-stricted class of programs; but this does not hold for all programs. Wedevelop rewrite—based optimization techniques that help us extend the above results to …,*,1992,*,0
Keyword Searching and Browsing in ey od Sea cgadosg Databases using BANKS,Gaurav Bhalotia; Arvind Hulgeri; Charuta Nakhe; Soumen Chakrabarti; S Sudarshan; Ben Vandervalk,Page 1. Keyword Searching and Browsing in ey od Sea cgadosg Databases using BANKS GauravBhalotia; Arvind Hulgeri; Charuta Nakhe; Soumen Chakrabarti; S. Sudarshan Presenter: MonirHajiaghayi Discussion Leader : Ben Vandervalk Page 2. Motivation • Web search engines arevery successful g y – Simple and intuitive keyword query interface • Database querying usingkeywords is desirable l / f l – Query languages; eg;SQL/QBE; are not appropriate for casual users –Form interfaces cumbersome; give limited views • Examples of keyword queries on databasesp y q – e-store database: “camcorder panasonic” – Book store: “sudarshan databases” •Differences from IR/Web Search – Normalization splits related data across multiple tuples –Answer to a query is a set of (closely) connected tuples that match all given keywords Aug 2002VLDB 2002 DEMO 2 Page 3. Basic Model • Database: modeled as a graph …,*,*,*,0
Query and Answer Models for Keyword Search,K Rose Catherine; S Sudarshan,*,*,*,*
Kenneth A. Rossy,Peter J Stuckeyx; S Sudarshan,*,*,*,*
Improving Predictability of Transaction Execution Times in Real-time,Rajeev Rastogi; S Seshadri; Philip Bohannon; Dennis Leinbaugh; Avi Silberschatz; S Sudarshan,*,*,*,*
Incremental Organization for Data Recording and Warehousing,PPS Narayan; S Seshadri; S Sudarshan; HV Jagadish; Rama Kanneganti,*,*,*,*
Versioning Algorithms for Improving Transaction Predictability in Real-time Main-memory Databases,Rajeev Rastogi; S Seshadri; Philip Bohannon; Dennis Leinbaugh; Avi Silberschatz; S Sudarshan,Abstract We present a design for multi-version concurrency control and recovery in a mainmemory database; and describe logical and physical versioning schemes that allow read-only transactions to execute without obtaining data item locks or system latches. Ourschemes enable a system to provide the guarantee that updaters will never interfere withread-only transactions; and read-only transactions will not be delayed (for the purpose ofensuring data consistency) as long as the operating system provides them with su cientcycles. Consequently; transaction executions become more predictable {this partiallyalleviates a major problem in real-time database system (RTDBS) scheduling; namely; signicant unpredictability in transaction execution times. As a result; in addition to a transaction'sdeadline; a more accurate estimate of its execution time can also be taken into account …,*,*,*,14
