Why and where: A characterization of data provenance,Peter Buneman; Sanjeev Khanna; Tan Wang-Chiew,Abstract With the proliferation of database views and curated data-bases; the issue of dataprovenance-where a piece of data came from and the process by which it arrived in thedatabase-is becoming increasingly important; especially in scientific databases whereunderstanding provenance is crucial to the accuracy and currency of data. In this paper wedescribe an approach to computing provenance when the data of interest has been createdby a database query. We adopt a syntactic approach and present results for a general datamodel that applies to relational databases as well as to hierarchical data such as XML. Anovel aspect of our work is a distinction between “why” provenance (refers to the sourcedata that had some influence on the existence of the data) and “where” provenance (refersto the location (s) in the source databases from which the data was extracted). Supported …,International conference on database theory,2001,1229,7
Keys for XML,Peter Buneman; Susan Davidson; Wenfei Fan; Carmem Hara; Wang-Chiew Tan,Keys are an essential part of database design [2;19]: they are fundamental to data models andconceptual design; they provide the means by which one tuple in a relational database mayrefer to another tuple; and they are important in update; for they enable us to guarantee that anupdate will affect precisely one tuple. More philosophically; if we think of a tuple as representingsome real-world entity; the key provides an invariant connection between the tuple andentity … If XML documents are to do double duty as databases; then we shall need keys forthem. In fact; a cursory examination 3 of existing document type definitions (DTDs) reveals anumber of cases in which some element or attribute is specified––in comments––as a “uniqueidentifier”. Moreover a number of scientific databases; which are typically stored in somespecial-purpose hierarchical data format ripe for conversion to XML; have a …,Computer networks,2002,467,7
SilkRoute: trading between relations and XML,Mary Fernández; Wang-Chiew Tan; Dan Suciu,Abstract XML is the standard format for data exchange between inter-enterprise applicationson the Internet. To facilitate data exchange; industry groups define public document typedefinitions (DTDs) that specify the format of the XML data to be exchanged between theirapplications. In this paper; we address the problem of automating the conversion ofrelational data into XML. We describe SilkRoute; a general; dynamic; and efficient tool forviewing and querying relational data in XML. SilkRoute is general; because it can expressmappings of relational data into XML that conforms to arbitrary DTDs. We call thesemappings views. Applications express the data they need as an XML-QL query over theview. SilkRoute is dynamic; because it only materializes the fragment of an XML viewneeded by an application; and it is efficient; because it fully exploits the underlying …,Computer Networks,2000,462,15
Provenance in databases: Why; how; and where,James Cheney; Laura Chiticariu; Wang-Chiew Tan,Abstract Different notions of provenance for database queries have been proposed andstudied in the past few years. In this article; we detail three main notions of databaseprovenance; some of their applications; and compare and contrast amongst them.Specifically; we review why; how; and where provenance; describe the relationships amongthese notions of provenance; and describe some of their applications in confidencecomputation; view maintenance and update; debugging; and annotation propagation.,Foundations and Trends® in Databases,2009,445,7
Composing schema mappings: Second-order dependencies to the rescue,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract A schema mapping is a specification that describes how data structured under oneschema (the source schema) is to be transformed into data structured under a differentschema (the target schema). A fundamental problem is composing schema mappings: giventwo successive schema mappings; derive a schema mapping between the source schemaof the first and the target schema of the second that has the same effect as applyingsuccessively the two schema mappings. In this article; we give a rigorous semantics to thecomposition of schema mappings and investigate the definability and computationalcomplexity of the composition of two schema mappings. We first study the important case ofschema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite …,ACM Transactions on Database Systems (TODS),2005,352,14
An annotation management system for relational databases,Deepavali Bhagwat; Laura Chiticariu; Wang-Chiew Tan; Gaurav Vijayvargiya,Abstract We present an annotation management system for relational databases. In thissystem; every piece of data in a relation is assumed to have zero or more annotationsassociated with it and annotations are propagated along; from the source to the output; asdata is being transformed through a query. Such an annotation management system couldbe used for understanding the provenance (aka lineage) of data; who has seen or edited apiece of data or the quality of data; which are useful functionalities for applications that dealwith integration of scientific and biological data. We present an extension; pSQL; of afragment of SQL that has three different types of annotation propagation schemes; eachuseful for different purposes. The default scheme propagates annotations according towhere data is copied from. The default-all scheme propagates annotations according to …,The VLDB Journal,2005,336,7
Archiving scientific data,Peter Buneman; Sanjeev Khanna; Keishi Tajima; Wang-Chiew Tan,Abstract Archiving is important for scientific data; where it is necessary to record all pastversions of a database in order to verify findings based upon a specific version. Muchscientific data is held in a hierachical format and has a key structure that provides acanonical identification for each element of the hierarchy. In this article; we exploit theseproperties to develop an archiving technique that is both efficient in its use of space andpreserves the continuity of elements through versions of the database; something that is notprovided by traditional minimum-edit-distance diff approaches. The approach also usestimestamps. All versions of the data are merged into one hierarchy where an elementappearing in multiple versions is stored only once along with a timestamp. By identifying thesemantic continuity of elements and merging them into one data structure; our technique …,ACM Transactions on Database Systems (TODS),2004,296,7
Reasoning about keys for XML,Peter Buneman; Susan Davidson; Wenfei Fan; Carmem Hara; Wang-Chiew Tan,Abstract We study absolute and relative keys for XML; and investigate their associateddecision problems. We argue that these keys are important to many forms of hierarchicallystructured data including XML documents. In contrast to other proposals of keys for XML; weshow that these keys are always (finitely) satisfiable; and their (finite) implication problem isfinitely axiomatizable. Furthermore; we provide a polynomial time algorithm for determining(finite) implication in the size of keys. Our results also demonstrate; among other things; thatthe analysis of XML keys is far more intricate than its relational counterpart.,Information Systems,2003,286,15
Data provenance: Some basic issues,Peter Buneman; Sanjeev Khanna; Wang-Chiew Tan,Abstract The ease with which one can copy and transform data on the Web; has made itincreasingly difficult to determine the origins of a piece of data. We use the term dataprovenance to refer to the process of tracing and recording the origins of data and itsmovement between databases. Provenance is now an acute issue in scientific databaseswhere it is central to the validation of data. In this paper we discuss some of the technicalissues that have emerged in an initial exploration of thetopic.,International Conference on Foundations of Software Technology and Theoretical Computer Science,2000,286,10
SilkRoute: A framework for publishing relational data in XML,Mary Fernández; Yana Kadiyska; Dan Suciu; Atsuyuki Morishima; Wang-Chiew Tan,Abstract XML is the" lingua franca" for data exchange between interenterprise applications.In this work; we describe SilkRoute; a framework for publishing relational data in XML. InSilkRoute; relational data is published in three steps: the relational tables are presented tothe database administrator in a canonical XML view; the database administrator defines inthe XQuery query language a public; virtual XML view over the canonical XML view; and anapplication formulates an XQuery query over the public view. SilkRoute composes theapplication query with the public-view query; translates the result into SQL; executes this onthe relational engine; and assembles the resulting tuple streams into an XML document.This work makes some key contributions to XML query processing. First; it describes analgorithm that translates an XQuery expression into SQL. The translation depends on a …,ACM Transactions on Database Systems (TODS),2002,253,7
Provenance in databases,Peter Buneman; Wang-Chiew Tan,Abstract The provenance of data has recently been recognized as central tothe trust oneplaces in data. It is also important to annotation; todata integration and to probabilisticdatabases. Three workshops havebeen held on the topic; and it has been the focus ofseveral researchprojects and prototype systems. This tutorial will attempt to provideanoverview of research in provenance in databases with a focus onrecent database researchand technology in this area. This tutorialis aimed at a general database research audienceand at people whowork with scientific data.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,235,7
Method for converting relational data into xml,*,A method for converting relational data to XML (eXtensible Markup Language) is provided.The method can use a greedy algorithm to efficiently construct materialized XML views ofrelational databases. A greedy algorithm designed for XML view definition queries isprovided for decomposing a large query into smaller queries and determining which querywill run faster without actually running the query.,*,2004,227,1
On propagation of deletions and annotations through views,Peter Buneman; Sanjeev Khanna; Wang-Chiew Tan,Abstract We study two classes of view update problems in relational databases. We aregiven a source database S; a monotone query Q; and the view Q (S) generated by the query.The first problem that we consider is the classical view deletion problem where we wish toidentify a minimal set T of tuples in S whose deletion will eliminate a given tuple t from theview. We study the complexity of optimizing two natural objectives in this setting; namely;find T to minimize the side-effects on the view; and the source; respectively. For bothobjective functions; we show a dichotomy in the complexity. Interestingly; the problem iseither in P or is NP-hard; for queries in the same class in either objective function. Thesecond problem in our study is the annotation placement problem. Suppose we annotate anattribute of a tuple in S. The rules for carrying the annotation forward through a query are …,Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2002,216,15
Provenance in databases: past; current; and future.,Wang Chiew Tan,Abstract The need to understand and manage provenance arises in almost every scientificapplication. In many cases; information about provenance constitutes the proof ofcorrectness of results that are generated by scientific applications. It also determines thequality and amount of trust one places on the results. For these reasons; the knowledge ofprovenance of a scientific result is typically regarded to be as important as the result itself. Inthis paper; we provide an overview of research in provenance in databases and discusssome future research directions. The content of this paper is largely based on the tutorialpresented at SIGMOD 2007 [11].,IEEE Data Eng. Bull.,2007,191,7
Method for converting relational data into a structured document,*,A method for converting relational data to XML (Extensible Markup Language) is provided.The method; sometimes referred to as SilkRoute; provides a general; dynamic and efficienttool for viewing and querying relational data in XML. SilkRoute can express mappings ofrelational data in XML that conforms to arbitrary public document type definitions. Also;SilkRoute can materialize the fragment of an XML view needed by an application and it canfully exploit the query engine of a relational database management system whenever dataitems in an XML view need to be materialized.,*,2003,173,10
STBenchmark: towards a benchmark for mapping systems,Bogdan Alexe; Wang-Chiew Tan; Yannis Velegrakis,Abstract A fundamental problem in information integration is to precisely specify therelationships; called mappings; between schemas. Designing mappings is a time-consuming process. To alleviate this problem; many mapping systems have beendeveloped to assist the design of mappings. However; a benchmark for comparing andevaluating these systems has not yet been developed. We present STBenchmark; a solutiontowards a much needed benchmark for mapping systems. We first describe the challengesthat are unique to the development of benchmarks for mapping systems. After this; wedescribe the three components of STBenchmark:(1) a basic suite of mapping scenarios thatwe believe represents a minimum set of transformations that should be readily supported byany mapping system;(2) a mapping scenario generator as well as an instance generator …,Proceedings of the VLDB Endowment,2008,142,10
Curated databases,Peter Buneman; James Cheney; Wang-Chiew Tan; Stijn Vansummeren,Abstract Curated databases are databases that are populated and updated with a great dealof human effort. Most reference works that one traditionally found on the reference shelves oflibraries--dictionaries; encyclopedias; gazetteers etc.--are now curated databases. Since it isnow easy to publish databases on the web; there has been an explosion in the number ofnew curated databases used in scientific research. The value of curated databases lies inthe organization and the quality of the data they contain. Like the paper reference works theyhave replaced; they usually represent the efforts of a dedicated group of people to produce adefinitive description of some subject area. Curated databases present a number ofchallenges for database research. The topics of annotation; provenance; and citation arecentral; because curated databases are heavily cross-referenced with; and include data …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,142,10
Peer data exchange,Ariel Fuxman; Phokion G Kolaitis; Renée J Miller; Wang-Chiew Tan,Abstract In this article; we introduce and study a framework; called peer data exchange; forsharing and exchanging data between peers. This framework is a special case of a full-fledged peer data management system and a generalization of data exchange between asource schema and a target schema. The motivation behind peer data exchange is to modelauthority relationships between peers; where a source peer may contribute data to a targetpeer; specified using source-to-target constraints; and a target peer may use target-to-source constraints to restrict the data it is willing to receive; but cannot modify the data of thesource peer. A fundamental algorithmic problem in this framework is that of deciding theexistence of a solution: given a source instance and a target instance for a fixed peer dataexchange setting; can the target instance be augmented in such a way that the source …,ACM Transactions on Database Systems (TODS),2006,140,7
Debugging schema mappings with routes,Laura Chiticariu; Wang-Chiew Tan,Abstract A schema mapping is a high-level declarative specification of the relationshipbetween two schemas; it specifies how data structured under one schema; called the sourceschema; is to be converted into data structured under a possibly different schema; called thetarget schema. Schema mappings are fundamental components for both data exchange anddata integration. To date; a language for specifying (or programming) schema mappingsexists. However; developmental support for programming schema mappings is still lacking.In particular; a tool for debugging schema mappings has not yet been developed. In thispaper; we propose to build a debugger for understanding and exploring schema mappings.We present a primary feature of our debugger; called routes; that describes the relationshipbetween source and target data with the schema mapping. We present two algorithms for …,Proceedings of the 32nd international conference on Very large data bases,2006,139,7
DBNotes: a post-it system for relational databases based on provenance,Laura Chiticariu; Wang-Chiew Tan; Gaurav Vijayvargiya,Abstract We demonstrate DBNotes; a Post-It note system for relational databases whereevery piece of data may be associated with zero or more notes (or annotations). Theseannotations are transparently propagated along as data is being transformed. The methodby which annotations are propagated is based on provenance (aka lineage): theannotations associated with a piece of data d in the result of a transformation consist of theannotations associated with each piece of data in the source where d is copied from. Oneimmediate application of this system is to use annotations to systematically trace theprovenance and flow of data. If every piece of source data is attached with an annotation thatdescribes its address (ie; origins); then the annotations of a piece of data in the result of atransformation describe its provenance. Hence; one can easily determine the provenance …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,132,10
Quasi-inverses of schema mappings,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract Schema mappings are high-level specifications that describe the relationshipbetween two database schemas. Two operators on schema mappings; namely thecomposition operator and the inverse operator; are regarded as especially important.Progress on the study of the inverse operator was not made until very recently; as evenfinding the exact semantics of this operator turned out to be a fairly delicate task.Furthermore; this notion is rather restrictive; since it is rare that a schema mappingpossesses an inverse. In this article; we introduce and study the notion of a quasi-inverse ofa schema mapping. This notion is a principled relaxation of the notion of an inverse of aschema mapping; intuitively; it is obtained from the notion of an inverse by not differentiatingbetween instances that are equivalent for data-exchange purposes. For schema …,ACM Transactions on Database Systems (TODS),2008,102,15
Research problems in data provenance.,Wang Chiew Tan,Abstract The problem of tracing the provenance (also known as lineage) of data is anubiquitous problem that is frequently encountered in databases that are the result of manytransformation steps. Scientific databases and data warehouses are some examples of suchdatabases. However; contributions from the database research community towards thisproblem have been somewhat limited. In this paper; we motivate the problem of supportingdata provenance in scientific database applications and provide some background onprevious research. We also briefly describe the DBNotes prototype developed at UC SantaCruz that can be used to “eagerly” trace the provenance and flow of relational data anddescribe some directions for further research.,IEEE Data Eng. Bull.,2004,93,7
Publishing relational data in xml: the silkroute approach,Mary F.  Fernandez; Atsuyuki Morishima; Dan Suciu; Wang Chiew  Tan,For exchange on the Internet; relational data needs to be mapped to XML; a process that wecall XML publishing. The mapping is complex; because the two data models differsignificantly. Relational data is flat; normalized into many relations; and its schema is oftenproprietary. By contrast; XML data is nested; unnormalized; and its schema is public; usuallycreated by agreement between members of a community; after lengthy negotiations.Publishing XML data involves joining tables; selecting and projecting the data that needs tobe exported; mapping the relational table and attribute names into XML element andattribute names; creating XML hierarchies; and processing values in an application specificmanner. The XML data is a view over the relational database and; as such; can be virtual ormaterialized. In a virtual view; applications consuming XML data do so by applying XML …,IEEE Data Eng. Bull.,2001,92,15
Muse: Mapping understanding and design by example,Bogdan Alexe; Laura Chiticariu; Renée J Miller; Wang-Chiew Tan,A fundamental problem in information integration is that of designing the relationships;called schema mappings; between two schemas. The specification of a semantically correctschema mapping is typically a complex task. Automated tools can suggest potentialmappings; but few tools are available for helping a designer understand mappings anddesign alternative mappings. We describe Muse; a mapping design wizard that uses dataexamples to assist designers in understanding and refining a schema mapping towards thedesired specification. We present novel algorithms behind Muse and show how Musesystematically guides the designer on two important components of a mapping design: thespecification of the desired grouping semantics for sets of data and the choice amongalternative interpretations for semantically ambiguous mappings. In every component …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,86,4
Towards a query language for annotation graphs,Steven Bird; Peter Buneman; Wang-Chiew Tan,Abstract: The multidimensional; heterogeneous; and temporal nature of speech databasesraises interesting challenges for representation and query. Recently; annotation graphshave been proposed as a general-purpose representational framework for speechdatabases. Typical queries on annotation graphs require path expressions similar to thoseused in semistructured query languages. However; the underlying model is rather differentfrom the customary graph models for semistructured data: the graph is acyclic and unrooted;and both temporal and inclusion relationships are important. We develop a query languageand describe optimization techniques for an underlying relational representation.,arXiv preprint cs/0007023,2000,76,7
Designing and refining schema mappings via data examples,Bogdan Alexe; Balder Ten Cate; Phokion G Kolaitis; Wang-Chiew Tan,Abstract A schema mapping is a specification of the relationship between a source schemaand a target schema. Schema mappings are fundamental building blocks in data integrationand data exchange and; as such; obtaining the right schema mapping constitutes a majorstep towards the integration or exchange of data. Up to now; schema mappings havetypically been specified manually or have been derived using mapping-design systems thatautomatically generate a schema mapping from a visual specification of the relationshipbetween two schemas. We present a novel paradigm and develop a system for theinteractive design of schema mappings via data examples. Each data example represents apartial specification of the semantics of the desired schema mapping. At the core of oursystem lies a sound and complete algorithm that; given a finite set of data examples …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,68,15
A hybrid machine-crowdsourcing system for matching web tables,Ju Fan; Meiyu Lu; Beng Chin Ooi; Wang-Chiew Tan; Meihui Zhang,The Web is teeming with rich structured information in the form of HTML tables; whichprovides us with the opportunity to build a knowledge repository by integrating these tables.An essential problem of web data integration is to discover semantic correspondencesbetween web table columns; and schema matching is a popular means to determine thesemantic correspondences. However; conventional schema matching techniques are notalways effective for web table matching due to the incompleteness in web tables. In thispaper; we propose a two-pronged approach for web table matching that effectivelyaddresses the above difficulties. First; we propose a concept-based approach that mapseach column of a web table to the best concept; in a well-developed knowledge base; thatrepresents it. This approach overcomes the problem that sometimes values of two web …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,67,15
The complexity of data exchange,Phokion G Kolaitis; Jonathan Panttaja; Wang-Chiew Tan,Abstract Data exchange is the problem of transforming data structured under a sourceschema into data structured under a target schema in such a way that all constraints of aschema mapping are satisfied. At the heart of data exchange; lies a basic decision problem;called the existence-of-solutions problem: given a source instance; is there a target instancethat satisfies the constraints of the schema mapping at hand? Earlier work showed that forschema mappings specified by embedded implicational dependencies; this problem issolvable in polynomial time; assuming that (1) the schema mapping is kept fixed and (2) theconstraints of the schema mapping satisfy a certain structural condition; called weakacyclicity. We investigate the effect of these assumptions on the complexity of the existence-of-solutions problem; and show that each one is indispensable in deriving polynomial …,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,66,15
Reverse data exchange: coping with nulls,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract An inverse of a schema mapping M is intended to undo what M does; thus providinga way to perform reverse data exchange. In recent years; three different formalizations of thisconcept have been introduced and studied; namely the notions of an inverse of a schemamapping; a quasi-inverse of a schema mapping; and a maximum recovery of a schemamapping. The study of these notions has been carried out in the context in which sourceinstances are restricted to consist entirely of constants; while target instances may containboth constants and labeled nulls. This restriction on source instances is crucial for obtainingsome of the main technical results about these three notions; but; at the same time; limitstheir usefulness; since reverse data exchange naturally leads to source instances that maycontain both constants and labeled nulls. We develop a new framework for reverse data …,ACM Transactions on Database Systems (TODS),2011,61,7
A deterministic model for semistructured data,Peter Buneman; Alin Deutsch; Wang-Chiew Tan,This is a preliminary report on a new model for semistructured data. The idea of semi-structured data evolved; in part; from various syntactic representations of data such asAceDB 11]; OEM 10] and has more recently been used e ectively to design query languagesfor XML 4]. Insofar as there is an agreed model; it is simply an edgelabeled graph. Howeverthis description begs a number of important questions: What can constitute an edge label?Are there values associated with the vertices? Is there a separate labeling system forvertices to provide them with independent identity? We describe here a new model for semi-structured data. It is more restrictive than the models described in 10; 6; 5] in that it isdeterministic. The edges emanating from any node in the graph have distinct labels. It is lessrestrictive in that the edges can carry data and may have structure. In fact they may …,Workshop on Query Processing for Semistructured Data and Non-Standard Data Formats,1999,60,10
Containment of relational queries with annotation propagation,Wang-Chiew Tan,Abstract We study the problem of determining whether a query is contained in another whenqueries can carry along annotations from source data. We say that a query is annotation-contained in another if the annotated output of the former is contained in the latter on everypossible annotated input databases. We study the relationship between query containmentand annotation-containment and show that annotation-containment is a more refined notionin general. As a consequence; the usual equivalences used by a typical query optimizermay no longer hold when queries can carry along annotations from the source to the output.Despite this; we show that the same annotated result is obtained whether intermediateconstructs of a query are evaluated with set or bag semantics. We also give a necessary andsufficient condition; via homomorphisms; that checks whether a query is annotation …,International Workshop on Database Programming Languages,2003,57,2
On computing functions with uncertainty,Sanjeev Khanna; Wang-Chiew Tan,Abstract We study the problem of computing a function f (x 1;…; xn) given that the actualvalues of the variables xi's are known only with some uncertainty. For each variable xi; aninterval I i is known such that the value of xi is guaranteed to fall within this interval. Any suchinterval can be probed to obtain the actual value of the underlying variable; however; thereis a cost associated with each such probe. The goal is to adaptively identify a minimum costsequence of probes such that regardless of the actual values taken by the unprobed x i's; thevalue of the function f can be computed to within a specified precision. We design onlinealgorithms for this problem when f is either the selection function or an aggregation functionsuch as sum or average. We consider three natural models of precision and give algorithmsfor each model. We analyze our algorithms in the framework of competitive analysis and …,Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2001,57,10
Asking the right questions in crowd data sourcing,Rubi Boim; Ohad Greenshpan; Tova Milo; Slava Novgorodov; Neoklis Polyzotis; Wang-Chiew Tan,Crowd-based data sourcing is a new and powerful data procurement paradigm thatengages Web users to collectively contribute information. In this work; we target the problemof gathering data from the crowd in an economical and principled fashion. We present AskIt!; a system that allows interactive data sourcing applications to effectively determine whichquestions should be directed to which users for reducing the uncertainty about the collecteddata. Ask It! uses a set of novel algorithms for minimizing the number of probing (questions)required from the different users. We demonstrate the challenge and our solution in thecontext of a multiple-choice question game played by the ICDE'12 attendees; targeted togather information on the conference's publications; authors and colleagues.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,56,7
Data is Dead... Without What-If Models.,Peter J Haas; Paul P Maglio; Patricia G Selinger; Wang Chiew Tan,Page 1. © 2011 IBM Corporation IBM Research DATA IS DEAD … WITHOUT “WHAT-IF” MODELSPeter J. Haas; Paul P. Maglio; Patricia G. Selinger; and Wang-Chiew Tan IBM Almaden ResearchCenter Page 2. © 2011 IBM Corporation IBM Research Congratulations; Database Community! ©2011 IBM Corporation OLAP Transactions & Reports; IMS Semi-structured & Unstructured textData mining Web data Relational model & SQL Text analytics Semantic data Massive Data/CloudDB Statistical analysis Machine learning Streaming data Uncertain data Page 3. © 2011 IBMCorporation IBM Research Congratulations; Database Community! © 2011 IBM Corporation OLAPTransactions & Reports; IMS Semi-structured & Unstructured text Data mining Web data Relationalmodel & SQL Text analytics Semantic data Massive Data/Cloud DB Statistical analysis Machinelearning Streaming data Uncertain data …,PVLDB,2011,54,7
Characterizing schema mappings via data examples,Bogdan Alexe; Balder TEN Cate; Phokion G Kolaitis; Wang-Chiew Tan,Abstract Schema mappings are high-level specifications that describe the relationshipbetween two database schemas; they are considered to be the essential building blocks indata exchange and data integration; and have been the object of extensive researchinvestigations. Since in real-life applications schema mappings can be quite complex; it isimportant to develop methods and tools for understanding; explaining; and refining schemamappings. A promising approach to this effect is to use “good” data examples that illustratethe schema mapping at hand. We develop a foundation for the systematic investigation ofdata examples and obtain a number of results on both the capabilities and the limitations ofdata examples in explaining and understanding schema mappings. We focus on schemamappings specified by source-to-target tuple generating dependencies (st tgds) and …,ACM Transactions on Database Systems (TODS),2011,51,15
Schema mapping evolution through composition and inversion,Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract Mappings between different representations of data are the essential buildingblocks for many information integration tasks. A schema mapping is a high-levelspecification of the relationship between two schemas; and represents a useful abstractionthat specifies how the data from a source format can be transformed into a target format. Thedevelopment of schema mappings is laborious and time consuming; even in the presence oftools that facilitate this development. At the same time; schema evolution inevitably causesthe invalidation of the existing schema mappings (since their schemas change). Providingtools and methods that can facilitate the adaptation and reuse of the existing schemamappings in the context of the new schemas is an important research problem. In thischapter; we show how two fundamental operators on schema mappings; namely …,*,2011,48,7
Laconic schema mappings: Computing the core with sql queries,Balder Ten Cate; Laura Chiticariu; Phokion Kolaitis; Wang-Chiew Tan,Abstract A schema mapping is a declarative specification of the relationship betweeninstances of a source schema and a target schema. The data exchange (or data translation)problem asks: given an instance over the source schema; materialize an instance (orsolution) over the target schema that satisfies the schema mapping. In general; a givensource instance may have numerous different solutions. Among all the solutions; universalsolutions and core universal solutions have been singled out and extensively studied. Auniversal solution is a most general one and also represents the entire space of solutions;while a core universal solution is the smallest universal solution and is unique up toisomorphism (hence; we can talk about the core). The problem of designing efficientalgorithms for computing the core has attracted considerable attention in recent years. In …,Proceedings of the VLDB Endowment,2009,47,15
Artemis: A system for analyzing missing answers,Melanie Herschel; Mauricio A Hernández; Wang-Chiew Tan,Abstract A central feature of relational database management systems is the ability to definemultiple different views over an underlying database schema. Views provide a method ofdefining access control to the underlying database; since a view exposes a part of thedatabase and hides the rest. Views also provide logical data independence to applicationprograms that access the database. For most cases; the process of specifying the desiredviews in SQL is typically tedious and error-prone. While numerous tools exist to supportdevelopers in debugging program code; we are not aware of any tool that supportsdevelopers in verifying the correctness of their views defined in SQL.,Proceedings of the VLDB Endowment,2009,46,10
Spider: a schema mapping debugger,Bogdan Alexe; Laura Chiticariu; Wang-Chiew Tan,Abstract A schema mapping is a high-level declarative specification of how data structuredunder one schema; called the source schema; is to be transformed into data structuredunder a possibly different schema; called the target schema. We demonstrate SPIDER; aprototype tool for debugging schema mappings; where the language for specifying schemamappings is based on a widely adopted formalism. We have built SPIDER on top of a dataexchange system; Clio; from IBM Almaden Research Center. At the heart of SPIDER is adata-driven facility for understanding a schema mapping through the display of routes. Aroute essentially describes the relationship between source and target data with the schemamapping. In this demonstration; we showcase our route engine; where we can display oneor all routes starting from either source or target data; as well as the intermediary data and …,Proceedings of the 32nd international conference on Very large data bases,2006,37,10
Data exchange with data-metadata translations,Mauricio A Hernández; Paolo Papotti; Wang-Chiew Tan,Abstract Data exchange is the process of converting an instance of one schema into aninstance of a different schema according to a given specification. Recent data exchangesystems have largely dealt with the case where the schemas are given a priori andtransformations can only migrate data from the first schema to an instance of the secondschema. In particular; the ability to perform data-metadata translations; transformation inwhich data is converted into metadata or metadata is converted into data; is largely ignored.This paper provides a systematic study of the data exchange problem with data-metadatatranslation capabilities. We describe the problem; our solution; implementation andexperiments. Our solution is a principled and systematic extension of the existing dataexchange framework; all the way from the constructs required in the visual interface to …,Proceedings of the VLDB Endowment,2008,36,10
Comparing and evaluating mapping systems with STBenchmark,Bogdan Alexe; Wang-Chiew Tan; Yannis Velegrakis,Abstract Schema mappings are fundamental building blocks in many information integrationapplications. Designing mappings is a time-consuming process and for that reason manymapping systems have been developed to assist in the task of designing mappings.However; to the best of our knowledge; a benchmark for comparing and evaluating thesesystems has not yet been developed. We demonstrate STBenchmark; a benchmark that wehave developed for evaluating mapping systems. Our demonstration will showcase thedifferent aspects of mapping systems that STBenchmark evaluates; highlight the results ofour comparison and evaluation of four mapping systems; as well as make a case for theneed for a standard specification input mechanism to mapping systems in order to makeprogress towards the development of a uniform testbed or repository for schema …,Proceedings of the VLDB Endowment,2008,31,7
Splash: a platform for analysis and simulation of health,Wang-Chiew Tan; Peter J Haas; Ronald L Mak; Cheryl A Kieliszewski; Patricia G Selinger; Paul P Maglio; Susanne Glissman; Melissa Cefkin; Yinan Li,Abstract As asserted by the Institute of Medicine; sound health policy and investmentdecisions require use of" what if" simulation models to analyze the potential impacts ofalternative decisions on health outcomes. The challenge is that high-level health decisionsrequire understanding complex interactions of diverse systems across many disciplines bothinside and outside of healthcare; creating a need for experts across widely different domainsto combine their data and models. Splash-the Smarter Planet Platform for Analysis andSimulation of Health-is a novel decision support framework that facilitates combiningheterogeneous; pre-existing simulation models and data from different domains anddisciplines. Splash leverages and extends data integration; search; and scientific-workflowtechnologies to permit loose coupling of models via data exchange. This approach avoids …,Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium,2012,28,4
EIRENE: Interactive design and refinement of schema mappings via data examples,Bogdan Alexe; Balder Ten Cate; Phokion G Kolaitis; Wang-Chiew Tan,ABSTRACT One of the first steps in the process of integrating information from multiplesources into a desired target format is to specify the relationships; called schema mappings;between the source schemas and the target schema. In this demonstration; we showcase anew methodology for designing schema mappings. Our system Eirene interactively solicitsdata examples from the mapping designer in order to design a schema mapping between asource schema and a target schema. A data example; in this context; is a pair consisting of asource instance and a target instance showing the desired outcome of performing dataexchange using the schema mapping being designed. One of the central parts of the systemis a module that; given a set of data examples; either returns a “best” fitting schemamapping; or reports that no fitting schema mapping exists.,Proceedings of the VLDB Endowment,2011,28,7
Efficient querying of inconsistent databases with binary integer programming,Phokion G Kolaitis; Enela Pema; Wang-Chiew Tan,Abstract An inconsistent database is a database that violates one or more integrityconstraints. A typical approach for answering a query over an inconsistent database is to firstclean the inconsistent database by transforming it to a consistent one and then apply thequery to the consistent database. An alternative and more principled approach; known asconsistent query answering; derives the answers to a query over an inconsistent databasewithout changing the database; but by taking into account all possible repairs of thedatabase. In this paper; we study the problem of consistent query answering overinconsistent databases for the class for conjunctive queries under primary key constraints.We develop a system; called EQUIP; that represents a fundamental departure from existingapproaches for computing the consistent answers to queries in this class. At the heart of …,Proceedings of the VLDB Endowment,2013,25,7
Muse: a system for understanding and designing mappings,Bogdan Alexe; Laura Chiticariu; Renée J Miller; Daniel Pepper; Wang-Chiew Tan,Abstract Schema mappings are logical assertions that specify the relationships between asource and a target schema in a declarative way. The specification of such mappings is afundamental problem in information integration. Mappings can be generated by existingmapping systems (semi-) automatically from a visual specification between two schemas. Ingeneral; the well-known 80-20 rule applies for mapping generation tools. They canautomate 80% of the work; covering common cases and creating a mapping that is close tocorrect. However; ensuring complete correctness can still require intricate manual work toperfect portions of the mapping. Previous research on mapping understanding andrefinement and anecdotal evidence from mapping designers suggest that the mappingdesign process can be perfected by using data examples to explain the mapping and …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,22,2
High-level why-not explanations using ontologies,Balder ten Cate; Cristina Civili; Evgeny Sherkhonov; Wang-Chiew Tan,Abstract We propose a novel foundational framework for why-not explanations; that is;explanations for why a tuple is missing from a query result. Our why-not explanationsleverage concepts from an ontology to provide high-level and meaningful reasons for why atuple is missing from the result of a query. A key algorithmic problem in our framework is thatof computing a most-general explanation for a why-not question; relative to an ontology;which can either be provided by the user; or it may be automatically derived from the dataand/or schema. We study the complexity of this problem and associated problems; andpresent concrete algorithms for computing why-not explanations. In the case where anexternal ontology is provided; we first show that the problem of deciding the existence of anexplanation to a why-not question is NP-complete in general. However; the problem is …,Proceedings of the 34th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2015,21,17
Federation in cloud data management: Challenges and opportunities,Gang Chen; HV Jagadish; Dawei Jiang; David Maier; Beng Chin Ooi; Kian-Lee Tan; Wang-Chiew Tan,Companies are increasingly moving their data processing to the cloud; for reasons of cost;scalability; and convenience; among others. However; hosting multiple applications andstorage systems on the same cloud introduces resource sharing and heterogeneous dataprocessing challenges due to the variety of resource usage patterns employed; the variety ofdata types stored; and the variety of query interfaces presented by those systems.Furthermore; real clouds are never perfectly symmetric-there often are differences betweenindividual processors in their capabilities and connectivity. In this paper; we introduce afederation framework to manage such heterogeneous clouds. We then use this framework todiscuss several challenges and their potential solutions.,IEEE Transactions on Knowledge and Data Engineering,2014,18,20
Linking temporal records for profiling entities,Furong Li; Mong Li Lee; Wynne Hsu; Wang-Chiew Tan,Abstract To harness the rich amount of information available on the Web today; manyorganizations start to aggregate public (and private) data to derive new knowledge bases. Afundamental challenge in constructing an accurate integrated knowledge repository fromdifferent data sources is to understand how facts across different sources are related to oneanother over time. This challenge; referred to as the temporal record linkage problem; goesfar beyond the traditional record linkage problem as it requires a fine-grained analysis ofhow two facts are temporally related if they both refer to the same entity. In this paper; wepresent a new solution for understanding how two facts may be temporally related andexploit the knowledge to profile how entities evolve over time. Our solution makes use of anovel transition model which captures sophisticated patterns of value transitions …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,17,15
Query-oriented data cleaning with oracles,Moria Bergman; Tova Milo; Slava Novgorodov; Wang-Chiew Tan,Abstract As key decisions are often made based on information contained in a database; it isimportant for the database to be as complete and correct as possible. For this reason; manydata cleaning tools have been developed to automatically resolve inconsistencies indatabases. However; data cleaning tools provide only best-effort results and usually cannoteradicate all errors that may exist in a database. Even more importantly; existing datacleaning tools do not typically address the problem of determining what information ismissing from a database. To overcome the limitations of existing data cleaning techniques;we present QOCO; a novel query-oriented system for cleaning data with oracles. Under thisframework; incorrect (resp. missing) tuples are removed from (added to) the result of a querythrough edits that are applied to the underlying database; where the edits are derived by …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,15,5
MapMerge: correlating independent schema mappings,Bogdan Alexe; Mauricio Hernández; Lucian Popa; Wang-Chiew Tan,Abstract One of the main steps toward integration or exchange of data is to design themappings that describe the (often complex) relationships between the source schemas orformats and the desired target schema. In this paper; we introduce a new operator; calledMapMerge; that can be used to correlate multiple; independently designed schemamappings of smaller scope into larger schema mappings. This allows a more modularconstruction of complex mappings from various types of smaller mappings such as schemacorrespondences produced by a schema matcher or pre-existing mappings that weredesigned by either a human user or via mapping tools. In particular; the new operator alsoenables a new" divide-and-merge" paradigm for mapping creation; where the design isdivided (on purpose) into smaller components that are easier to create and understand …,The VLDB Journal—The International Journal on Very Large Data Bases,2012,15,7
MapMerge: Correlating independent schema mappings,Bogdan Alexe; Mauricio Hernández; Lucian Popa; Wang-Chiew Tan,Abstract One of the main steps towards integration or exchange of data is to design themappings that describe the (often complex) relationships between the source schemas orformats and the desired target schema. In this paper; we introduce a new operator; calledMapMerge; that can be used to correlate multiple; independently designed schemamappings of smaller scope into larger schema mappings. This allows a more modularconstruction of complex mappings from various types of smaller mappings such as schemacorrespondences produced by a schema matcher or pre-existing mappings that weredesigned by either a human user or via mapping tools. In particular; the new operator alsoenables a new" divide-and-merge" paradigm for mapping creation; where the design isdivided (on purpose) into smaller components that are easier to create and understand …,Proceedings of the VLDB Endowment,2010,15,7
Data Integration and Data Exchange: It's Really About Time.,Mary Roth; Wang-Chiew Tan,ABSTRACT With the deluge in the amount and variety of data in the world; it is rare for datathat describes an entity to be completely contained and managed by a single data source.As a consequence; there is often great value in combining data about an entity from multiplesources; and also from versions of data reported by the same source over time. Dataintegration in which multiple dimensions of time may be expressed explicitly (eg; as part ofthe data itself) or implicitly (eg; the publication date of a data source); must be performedwith great care. This is because each data source contains only partial (time-specific)knowledge about an entity; and thus their collective knowledge about the entity may containconflicts that need to be resolved. In this paper; we call for a formal framework for dataintegration and data exchange across time that would facilitate the creation of consistent …,CIDR,2013,14,15
Preference-aware integration of temporal data,Bogdan Alexe; Mary Roth; Wang-Chiew Tan,Abstract A complete description of an entity is rarely contained in a single data source; butrather; it is often distributed across different data sources. Applications based on personalelectronic health records; sentiment analysis; and financial records all illustrate thatsignificant value can be derived from integrated; consistent; and queryable profiles ofentities from different sources. Even more so; such integrated profiles are considerablyenhanced if temporal information from different sources is carefully accounted for. Wedevelop a simple and yet versatile operator; called prawn; that is typically called as a finalstep of an entity integration workflow. Prawn is capable of consistently integrating andresolving temporal conflicts in data that may contain multiple dimensions of time based on aset of preference rules specified by a user (hence the name prawn for preference-aware …,Proceedings of the VLDB Endowment,2014,13,1
Database constraints and homomorphism dualities,Balder Ten Cate; Phokion G Kolaitis; Wang-Chiew Tan,Abstract Global-as-view (GAV) constraints form a class of database constraints that hasbeen widely used in the study of data exchange and data integration. Specifically;relationships between different database schemas are commonly described by a schemamapping consisting of a finite set of GAV constraints. Such schema mappings can be viewedas representations of an infinite set of data examples. We study the following problem: whenis finite set of GAV constraints uniquely characterizable via a finite set of data examples? Byestablishing a tight connection between this problem and homomorphism dualities; weobtain a simple criterion for unique characterizability. We also pinpoint the computationalcomplexity of the corresponding decision problem.,International Conference on Principles and Practice of Constraint Programming,2010,13,7
Fusing data management services with file systems,Scott Brandt; Carlos Maltzahn; Neoklis Polyzotis; Wang-Chiew Tan,Abstract File systems are the backbone of large-scale data processing for scientificapplications. Motivated by the need to provide an extensible and flexible framework beyondthe abstractions provided by API libraries for files to manage and analyze large-scale data;we are developing Damasc; an enhanced file system where rich data management servicesfor scientific computing are provided as a native part of the file system. This paper presentsour vision for Damasc; a performant file system that would allow scientists or even casualusers to pose declarative queries and updates over views of underlying files that are storedin their native bytestream format. In Damasc; a configurable layer is added on top of the filesystem to expose the contents of files in a logical data model through which views can bedefined and used for queries and updates. The logical data model and views are …,Proceedings of the 4th Annual Workshop on Petascale Data Storage,2009,13,10
Beyond XML query languages,Peter Buneman; Alin Deutsch; Wenfei Fan; Hartmut Liefke; Arnaud Sahuguet; Wang-Chiew Tan,Abstract A query language is essential; if XML is to serve effectively as an exchange mediumfor large data sets. The design of query languages for XML is in its infancy; and the choice ofa standard may be governed more by user acceptance than by any understanding ofunderlying principles. One would hope that expressive power; performance; andcompatibility with other languages will be considered in choosing among alternatives; but itis likely that several contenders will co-exist for some time. It is worth observing that; duringthe 20-year development of relational query languages; several competing languages weredeveloped; and even today there are several relational query language standards. In spiteof this; a great deal of technology was developed that was independent of the surface syntaxof a query language. This included technology" below" the language such as efficient …,Database Research Group (CIS),1998,13,15
Computing provenance and annotations for views,Peter Buneman; Sanjeev Khanna; Wang-Chiew Tan,Many scientific databases are views derived from a variety of source databases; many ofwhich are in turn views derived from other databases. For instance; there are roughly 500public databases [1] in the field of molecular biology. However; only a few of thesedatabases are “true” source databases—in the sense; they receive direct experimental data.In such a scenario; a piece of source data may have moved through several databases andmight have been transformed and edited on its journey from its source. Given a piece of datain one of these databases; how can one tell its provenance–why it is there and where itcame from? Scientists are often interested in the provenance of data in order to assess thequality and reliability of a piece of data in a derived view. Scientists are also interested inannotating data because annotations provide additional valueadded information about …,Workshop Paper: Workshop on Data Derivation and Provenance (Oct.). Chicago IL. Available at: http://people. cs. uchicago. edu/∼ yongzh/position papers. html,2002,12,15
Schema mappings and data examples,Balder Ten Cate; Phokion G Kolaitis; Wang-Chiew Tan,Abstract A fundamental task in data integration and data exchange is the design of schemamappings; that is; high-level declarative specifications of the relationship between twodatabase schemas. Several research prototypes and commercial systems have beendeveloped to facilitate schema-mapping design; a common characteristic of these systems isthat they produce a schema mapping based on attribute correspondences across schemassolicited from the user via a visual interface. This methodology; however; suffers from certainshortcomings. In the past few years; a fundamentally different methodology to designing andunderstanding schema mappings has emerged. This new methodology is based on thesystematic use of data examples to derive; illustrate; and refine schema mappings. Example-driven schema-mapping design is currently an active area of research in which several …,Proceedings of the 16th International Conference on Extending Database Technology,2013,10,7
Information technology for healthcare transformation,Joseph P Bigus; Murray Campbell; Boaz Carmeli; Melissa Cefkin; Henry Chang; C-H Chen-Ritzo; William F Cody; Shahram Ebadollahi; A Evfimievski; Ariel Farkash; Susanne Glissmann; David Gotz; TWA Grandison; Daniel Gruhl; Peter J Haas; Mark JH Hsiao; P-YS Hsueh; Jianying Hu; Joseph M Jasinski; James H Kaufman; Cheryl A Kieliszewski; Martin S Kohn; Sarah E Knoop; Paul P Maglio; Ronald L Mak; Haim Nelken; Chalapathy Neti; Hani Neuvirth; Yue Pan; Yardena Peres; Sreeram Ramakrishnan; Michal Rosen-Zvi; S Renly; Pat Selinger; Amnon Shabo; RK Sorrentino; Jimeng Sun; T Syeda-Mahmood; W-C Tan; Ying YY Tao; Reza Yaesoubi; Xinxin Zhu,Rising costs; decreasing quality of care; diminishing productivity; and increasing complexityhave all contributed to the present state of the healthcare industry. The interactions betweenpayers (eg; insurance companies and health plans) and providers (eg; hospitals andlaboratories) are growing and are becoming more complicated. The constant upsurge in andenhanced complexity of diagnostic and treatment information has made the clinical decision-making process more difficult. Medical transaction charges are greater than ever. Population-specific financial requirements are increasing the economic burden on the entire system.Medical insurance and identity theft frauds are on the rise. The current lack of comparativecost analytics hampers systematic efficiency. Redundant and unnecessary interventions addto medical expenditures that add no value. Contemporary payment models are antithetic …,IBM Journal of Research and Development,2011,10,3
A declarative framework for linking entities,Douglas Burdick; Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract We introduce and develop a declarative framework for entity linking and; inparticular; for entity resolution. As in some earlier approaches; our framework is based on asystematic use of constraints. However; the constraints we adopt are link-to-sourceconstraints; unlike in earlier approaches where source-to-link constraints were used todictate how to generate links. Our approach makes it possible to focus entirely on theintended properties of the outcome of entity linking; thus separating the constraints from anyprocedure of how to achieve that outcome. The core language consists of link-to-sourceconstraints that specify the desired properties of a link relation in terms of source relationsand built-in predicates such as similarity measures. A key feature of the link-to-sourceconstraints is that they employ disjunction; which enables the declarative listing of all the …,ACM Transactions on Database Systems (TODS),2016,9,20
Where was your data yesterday; and where will it go tomorrow? Data Annotation and Provenance for Scientific Applications,Peter Buneman; David Maier; Jennifer Widom,*,Proc. 2000 NSF Workshop on Information and Data Management,2000,9
QOCO: A query oriented data cleaning system with oracles,Moria Bergman; Tova Milo; Slava Novgorodov; Wang-Chiew Tan,Abstract As key decisions are often made based on information contained in a database; it isimportant for the database to be as complete and correct as possible. For this reason; manydata cleaning tools have been developed to automatically resolve inconsistencies indatabases. However; data cleaning tools provide only best-effort results and usually cannoteradicate all errors that may exist in a database. Even more importantly; existing datacleaning tools do not typically address the problem of determining what information ismissing from a database. To tackle these problems; we present QOCO; a novel queryoriented cleaning system that leverages materialized views that are defined by user queriesas a trigger for identifying the remaining incorrect/missing information. Given a user query;QOCO interacts with domain experts (which we model as oracle crowds) to identify …,Proceedings of the VLDB Endowment,2015,8,7
A time machine for information: Looking back to look forward,Xin Luna Dong; Wang-Chiew Tan,Abstract With the abundant availability of information one can mine from the Web today;there is increasing interest to develop a complete understanding of the history of an entity(ie; a person; a company; a music genre; a country; etc.)(see; for example;[7; 9; 10; 11]) andto depict trends over time [5; 12; 13]. This; however; remains a largely difficult and manualtask despite more than a couple of decades of research in the areas of temporal databasesand data integration.,Proceedings of the VLDB Endowment,2015,7,7
Provenance in scientific databases,Sarah Cohen-Boulakia; Wang-Chiew Tan,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,7,1
A graphical interface to genome multidatabases,Wang Chiew Tan; Ke Wang; Limsoon Wong,Abstract Formulating queries to access multiple databases can be a formidable taskespecially when many terms from various databases and complex constraints are involved.To specify a multidatabase query; the user usually has to search through documents forexact database terms and learn the multidatabase language. This report presents QUICK(QUery Interface to CPL-Kleisli); a graphical user interface to multiple databases. CPL(Collection Programming Language) is a high-level multidatabase language built on top ofan open query system Kleisli. QUICK allows users to handle overwhelming information fromdifferent data sources in an intuitive and uniform manner. The query specification is reducedto specifying user's terms in his/her own world; selecting paths and specifying constraints ina graph. QUICK is able to automatically generate a CPL query that corresponds to the …,Journal of Database Management (JDM),1998,6,15
Sequential composition of schema mappings,*,A method for generating a schema mapping. A provided mapping M12 relates schema S1 toschema S2. A provided mapping M23 relates schema S2 to schema S3. A mapping M13 isgenerated from schema S1 to schema S3 as a composition of mappings M12 and M23.Mappings M12; M23; and M13 are each expressed in terms of at least one second-ordernested tuple-generating dependency (SO nested tgd). Mapping M13 does not expresslyrecite any element of schema S2. At least one schema of the schemas S1 and S2 maycomprise at least one complex type expression nested inside another complex typeexpression. Mapping M13 may define the composition of the mappings M12 and M23 withrespect to a relationship semantics or a transformation semantics.,*,2010,5,20
QUICK: graphical user interface to multiple databases,Wang Chiew Tan; Ke Wang; Limsoon Wong,Formulating queries to access multiple databases can be a formidable task; especially whenmany terms from various databases and complex constraints are involved. To specify amultidatabase query; the user usually has to search through documents for exact databaseterms and learn the multidatabase language. The report presents QUICK (QUery Interface toCPL-Kleisli); a graphical user interface to multiple databases; CPL is a high-levelmultidatabase language built on top of an open query system Kleisli. QUICK allows users tohandle overwhelming information from different data sources in an intuitive and uniformmanner. The query specification is reduced to specifying user's terms in his/her own world;selecting paths and specifying constraints in a graph. QUICK is able to automaticallygenerate a CPL query that corresponds to the user's intent.,Database and Expert Systems Applications; 1996. Proceedings.; Seventh International Workshop on,1996,5,7
Splash: Simulation optimization in complex systems of systems,Peter J Haas; Nicole C Barberis; Piyaphol Phoungphol; Ignacio G Terrizzano; Wang-Chiew Tan; Patricia G Selinger; Paul P Maglio,Decision-makers increasingly need to bring together multiple models across a broad rangeof disciplines to guide investment and policy decisions around highly complex issues suchas population health and safety. We discuss the use of the Smarter Planet Platform forAnalysis Simulation of Health (Splash) for cross-disciplinary modeling; simulation; sensitivityanalysis; and optimization in the setting of complex systems of systems. Splash is aprototype system that allows combination of existing heterogeneous simulation models anddatasets to create composite simulation models of complex systems. Splash; built on acombination of data-integration; workflow management; and simulation technologies;facilitates loose coupling of models via data exchange. We describe the various componentsof Splash; with an emphasis on the experiment-management component. This latter …,Communication; Control; and Computing (Allerton); 2012 50th Annual Allerton Conference on,2012,4,7
On the tractability and intractability of consistent conjunctive query answering,Enela Pema; Phokion G Kolaitis; Wang-Chiew Tan,Abstract The consistent query answering framework has received considerable attentionsince it was first introduced as an alternative to coping with inconsistent databases. Theframework was defined based on two notions: repairs and consistent query answers.Informally; a repair is a consistent database that minimally differs from the inconsistentdatabase. The consistent answers to a query are those tuples that appear in the intersectionof the answer sets of the query when evaluated over all possible repairs. Here we study thecomplexity of the problem of consistent query answering for the class of acyclic conjunctivequeries without self-joins; under primary key constraints. The problem is known to be coNP-complete in general for this class. Our goal is to determine the boundary between tractabilityand intractability; by establishing a dichotomy to the effect that; every conjunctive query in …,Proceedings of the 2011 Joint EDBT/ICDT Ph. D. Workshop,2011,4,20
Approximation algorithms for schema-mapping discovery from data examples,Balder Ten Cate; Phokion G Kolaitis; Kun Qian; Wang-Chiew Tan,Abstract In recent years; data examples have been at the core of several differentapproaches to schema-mapping design. In particular; Gottlob and Senellart introduced aframework for schema-mapping discovery from a single data example; in which thederivation of a schema mapping is cast as an optimization problem. Our goal is to refine andstudy this framework in more depth. Among other results; we design a polynomial-time log(n)-approximation algorithm for computing optimal schema mappings from a given set ofdata examples (where n is the combined size of the given data examples) for a restrictedclass of schema mappings; moreover; we show that this approximation ratio cannot beimproved. In addition to the complexity-theoretic results; we implemented theaforementioned log (n)-approximation algorithm and carried out an experimental …,ACM Transactions on Database Systems (TODS),2017,3,7
Rudolf: interactive rule refinement system for fraud detection,Tova Milo; Slava Novgorodov; Wang-Chiew Tan,Abstract Credit card frauds are unauthorized transactions that are made or attempted by aperson or an organization that is not authorized by the card holders. In addition to machinelearning-based techniques; credit card companies often employ domain experts to manuallyspecify rules that exploit domain knowledge for improving the detection process. Over time;however; as new (fraudulent and legitimate) transaction arrive; these rules need to beupdated and refined to capture the evolving (fraud and legitimate) activity patterns. The goalof the RUDOLF system that is demonstrated here is to guide and assist domain experts inthis challenging task. RUDOLF automatically determines a best set of candidate adaptationsto existing rules to capture all fraudulent transactions and; respectively; omit all legitimatetransactions. The proposed modifications can then be further refined by domain experts …,Proceedings of the VLDB Endowment,2016,3,7
A New Framework for Designing Schema Mappings,Bogdan Alexe; Wang-Chiew Tan,Abstract One of the fundamental tasks in information integration is to specify therelationships; called schema mappings; between database schemas. Schema mappingsspecify how data structured under a source schema is to be transformed into data structuredunder a target schema. The design of schema mappings is usually a non-trivial and time-intensive process and the task of designing schema mappings is exacerbated by the factthat schemas that occur in real life tend to be large and heterogeneous. Traditionalapproaches for designing schema mappings are either manual or performed through a userinterface from which a schema mapping is interpreted from correspondences betweenattributes of the source and target schemas. These correspondences are either specified bythe user or automatically derived by applying schema matching on the two schemas. In …,*,2013,3,7
Data integration: After the teenage years,Behzad Golshan; Alon Halevy; George Mihaila; Wang-Chiew Tan,Abstract The field of data integration has expanded significantly over the years; fromproviding a uniform query and update interface to structured databases within an enterpriseto the ability to search; ex-change; and even update; structured or unstructured data that arewithin or external to the enterprise. This paper describes the evolution in the landscape ofdata integration since the work on rewriting queries using views in the mid-1990's. Inaddition; we describe two important challenges for the field going forward. The firstchallenge is to develop good open-source tools for different components of data integrationpipelines. The second challenge is to provide practitioners with viable solutions for the long-standing problem of systematically combining structured and unstructured data.,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,2,7
Synchronization of time between different simulation models,*,Embodiments relate to a method and computer program product for generating a compositesimulated model. A method includes receiving a specification request for generating a set oftarget time-series data from a set of source time-series data and obtaining specificationinformation relating to the set of source time-series data; obtaining specification informationrelating to the set of target time-series data; and obtaining the source time-series data. Themethod also includes comparing the source and target specification information todetermine if the set of source time-series data are time-aligned with the set of target time-series data and converting the set of source time-series data to the set of target time-seriesdata upon determination that time alignment is needed.,*,2017,2,7
Synchronization of time between different simulation models,*,Embodiments relate to a method and computer program product for generating a compositesimulated model. A method includes receiving a specification request for generating a set oftarget time-series data from a set of source time-series data and obtaining specificationinformation relating to the set of source time-series data; obtaining specification informationrelating to the set of target time-series data; and obtaining the source time-series data. Themethod also includes comparing the source and target specification information todetermine if the set of source time-series data are time-aligned with the set of target time-series data and converting the set of source time-series data to the set of target time-seriesdata upon determination that time alignment is needed.,*,2017,2,1
A time machine for information: Looking back to look forward,Xin Luna Dong; Anastasios Kementsietsidis; Wang-Chiew Tan,Abstract Historical data (also called long data) holds the key to understanding when factsare true. It is through long data that one can understand the trends that have developed inthe past; form the audit trails needed for justification; and make predictions about the future.For searching; there is also increasing interest to develop search capabilities over long data.In this article; we first motivate the need to develop a time machine for information that willhelp people" look back" so as to" look forward". We will overview key ideas on threecomponents (extraction; linking; and cleaning) that we believe are central to thedevelopment of any time machine for information. Finally; we conclude with our thoughts onwhat we believe are some interesting open research problems. This article is based on thematerial presented in a tutorial at VLDB 2015.,ACM SIGMOD Record,2016,2,7
System for Design and Execution of Numerical Experiments on a Composite Simulation Model,*,An experiment manager is discussed for the design and execution of numerical experimentsin composite simulation models; such as those created using the Smarter Planet Platform forAnalysis Simulation of Health (Splash). The experiment manager independently elicitsexperiment-related information from each contributor of a component model; and uses thisinformation to subsequently assist the creator of a composite model in selectingexperimental factors; creating experimental designs based on these factors; and executingthe experiments. This functionality permits cross-disciplinary modeling; simulation; sensitivityanalysis and optimization in the setting of complex systems.,*,2015,2,7
Correlating independent schema mappings,*,Embodiments of the invention relate to correlating schema mappings. In one embodiment; aset of schema mappings over a source schema and a target schema are received. Each ofthe schema mappings is decomposed into a basic schema mapping. A first set and secondset of relations re determined for the source schema and the target schema; respectively.Each relation in the first set of relations is paired to at least one relation in the second set ofrelations. The pairing forms multiple relation pairs between the first set and second ofrelations in the form of (T; T′); where T is a source portion of a relation pair and T′ is atarget portion of the relation pair. A set of basic schema mappings is identified that matchesthe relation pair. Each basic schema mapping is merged into a single schema mapping.,*,2013,2,4
Data annotations; provenance; and archiving,Wang-Chiew Tan,Abstract This dissertation examines the problem of data provenance and two main issuesrelated to provenance: Annotation and archiving. The provenance of data is the descriptionof the origins of that piece of data. Our contribution is the distinction between two kinds ofprovenance: Why-provenance and where-provenance. The why-provenance of a piece ofoutput data is the set of all witnesses to why that piece of data exists in the output. Where-provenance describes which pieces of source data contribute to a piece of output data. Weshowed that why-provenance and where-provenance can be computed by generating anew query from the original query and applying the new query on the same database.^Provenance is related to the view updates. In particular; where-provenance is related to theannotation placement problem; and why-provenance is related to the view deletion …,*,2002,2,0
Composite simulation modeling and analysis,*,An aspect of combining simulation models includes a processor configured to processinformation relating to a plurality of simulated models. The processor collects the informationto be processed related to the simulated models in at least one memory having metadatarelating to the simulated models. The processor has a transformation component generatedusing a schema mapping tool. The transformation component detects and corrects anyincompatibility between a first and a second simulation model by obtaining metadatarelating to the first simulation model and the second simulation model from the memory.,*,2017,1,7
Technical perspective: Attacking the problem of consistent query answering,Wang-Chiew Tan,Inconsistent data refers to data that do not adhere to one or more constraints. The termconstraints refers to conditions that need to be imposed on the data. Constraints often arisefrom organizational requirements or business logic; such as the requirement that everyemployee in the database must be uniquely identified by the employee id; or everyemployee must work on some project; or the expenses cannot exceed the credit limit; oreven a desired designated format for storing phone numbers. The need to manageinconsistent data arises in many settings. Quite typically; when one integrates data fromdifferent sources; the integrated data can be inconsistent data even when the data sourcesmay be individually consistent. Another scenario where inconsistency in data can arise iswhen data and/or schema evolves; for example; through the addition or removal of data …,ACM SIGMOD Record,2016,1,7
Query answering over incomplete and uncertain RDF,Enela Pema; Wang-Chiew Tan,ABSTRACT While incompleteness and uncertainty naturally arises in real-world RDF data;the RDF model itself provides little support for incomplete and uncertain data. In this paper;we introduce practical extensions of the RDF model to represent incompleteness and/oruncertainty. We adopt the semantics of certain answers as the meaningful query answers inthe presence of incompleteness and uncertainty; and we investigate the computationalcomplexity of computing the certain answers to several fragments of SPARQL under ourmodels. We determine that the problem is hard in general; and motivated by this intractabilityresult; we develop a heuristic algorithm for query answering in the presence of uncertaindata. Our algorithm can be implemented over any RDF query evaluation engine; and it canpotentially be extended to handle both incompleteness and uncertainty together.,International Workshop on the Web and Databases,2014,1,7
In Search of Elegance in the Theory and Practice of Computation: Essays Dedicated to Peter Buneman,Val Tannen; Limsoon Wong; Leonid Libkin; Wenfei Fan; Wang-Chiew Tan; Michael Fourman,This Festschrift volume; published in honour of Peter Buneman; contains contributionswritten by some of his colleagues; former students; and friends. In celebration of hisdistinguished career a colloquium was held in Edinburgh; Scotland; 27-29 October; 2013.The articles presented herein belong to some of the many areas of Peter's researchinterests.,*,2013,1,7
In Search of Elegance in the Theory and Practice of Computation,Val Tannen; Limsoon Wong; Leonid Libkin; Wenfei F An; Wang-Chiew Tan; Mich Ael Fourm An,Models for Data-Centric Workflows -- Relational Databases and Bell's Theorem -- High-LevelRules for Integration and Analysis of Data: New Challenges -- A New Framework for DesigningSchema Mappings -- User Trust and Judgments in a Curated Database with Explicit Provenance-- An Abstract; Reusable; and Extensible Programming Language Design Architecture -- A Discussionon Pricing Relational Data -- Tractable Reasoning in Description Logics with Functionality Constraints-- Toward a Theory of Self-explaining Computation -- To Show or Not to Show in Workflow Provenance-- Provenance-Directed Chase and Backchase -- Data Quality Problems beyond Consistencyand Deduplication -- Hitting Buneman Circles -- Looking at the World Thru Colored Glasses-- Static Analysis and Query Answering for Incomplete Data Trees with Constraints -- Using SQLfor Efficient Generation and Querying of Provenance Information -- Bounds and …,*,2013,1,15
Letter from the Special Issue Editors.,Xin Luna Dong; Wang Chiew Tan,The quality of data has always been important to businesses and intelligence; as policiesand business decisions are often made based on analysis performed on data. This issuecontains articles that explore different aspects of data quality that frequently arise in thecontext of multiple (Web) data sources providing overlapping; complementary; andsometimes contradictory information about the same concept or real world entity. It istherefore important to provide techniques for understanding the truthfulness andtrustworthiness of data sources to the extent possible; and reconcile their differences inorder to create a clean integrated view of the underlying data sources. Techniques for entityresolution; mapping; fusion; and data cleaning are important “ingredients” towards achievingsuch a clean unified view. The first three articles in this issue describe different …,IEEE Data Eng. Bull.,2011,1,7
Data is Dead… Without What-If Models,Peter J Haas Paul P Maglio; Patricia G Selinger; Wang-Chiew Tan,ABSTRACT Current database technology has raised the art of scalable descriptive analyticsto a very high level. Unfortunately; what enterprises really need is prescriptive analytics toidentify optimal business; policy; investment; and engineering decisions in the face ofuncertainty. Such analytics; in turn; rest on deep predictive analytics that go beyond merestatistical forecasting and are imbued with an understanding of the fundamentalmechanisms that govern a system's behavior; allowing what-if analyses. The databasecommunity needs to put what-if models and data on equal footing; developing systems thatuse both data and models to make sense of rich; real-world complexity and to supportrealworld decision-making. This model-and-data orientation requires significant extensionsof many database technologies; such as data integration; query optimization and …,Proceedings of the VLDB Endowment,2011,1,7
Splash: Smarter Planet Platform for Analysis and Simulation of Health,Melissa Cefkin; Myron Flickner; Susanne Glissmann; Peter J Haas; Leila Jalali; Paul P Maglio; Pat Selinger; Wang-Chiew Tan,Splash is a research project aimed at building a framework that supports the integration ofmultiple existing models; simulations; and data that represent parts of the broader healthecosystem. Specifically; our goal is to create a platform that takes expert models ofconstituent real-world systems related to health; synthesizing and integrating those models;resulting in an interoperating complex composite system model with which policy-makerscan try out alternatives in a low-cost; highly responsive way. The key research question iswhether such integration of independently created; deep domain models can be madefeasible; practical; flexible; cost-effective; attractive; and usable.,*,2010,1,7
Research on XML Security Technology in Electronic Commerce,WANG Tan; LIU Wei,XML was applied to each computer field aspect; with the development of XML technology;Electronic Commerce is a commerce pattern that demands to system safety is very strict alsohave begun to change. The text make contrast between Electronic Commerce securitysystem and Electronic Commerce safety frame based on XML Technology; and then makeconclusion that the safe technology of XML is in advantage in Electronic Commerce andapplies a prospect. At last we lead the Safe tier to a concrete electron sells system byauction that to resolve Electronic Commerce safety problem.,Journal of Wuhan University of Technology,2009,1,7
System and method of integrating time-aware data from multiple sources,*,Abstract A time-aware union operator is disclosed for consistent integration of time-awaredata; wherein the time-aware union produces a time-aware consistent integrated view ofunderlying sources according to specified key constraints and policies. The implementationof time-aware union is idempotent; commutative; and associative; thus making it suitable fordata integration; and it produces the same integrated outcome; modulo representation oftime; regardless of the order in which sources are integrated.,*,2018,*,7
HappyDB: A Corpus of 100;000 Crowdsourced Happy Moments,Akari Asai; Sara Evensen; Behzad Golshan; Alon Halevy; Vivian Li; Andrei Lopatenko; Daniela Stepanov; Yoshihiko Suhara; Wang-Chiew Tan; Yinzhan Xu,Abstract: The science of happiness is an area of positive psychology concerned withunderstanding what behaviors make people happy in a sustainable fashion. Recently; therehas been interest in developing technologies that help incorporate the findings of thescience of happiness into users' daily lives by steering them towards behaviors that increasehappiness. With the goal of building technology that can understand how people expresstheir happy moments in text; we crowd-sourced HappyDB; a corpus of 100;000 happymoments that we make publicly available. This paper describes HappyDB and its properties;and outlines several important NLP problems that can be studied with the help of the corpus.We also apply several state-of-the-art analysis techniques to analyze HappyDB. Our resultsdemonstrate the need for deeper NLP techniques to be developed which makes …,arXiv preprint arXiv:1801.07746,2018,*,11
Expressive Power of Entity-Linking Frameworks,Douglas Burdick; Ronald Fagin; Phokion G Kolaitis; Lucian Popa; Wang-Chiew Tan,Abstract We develop a unifying approach to declarative entity linking by introducing thenotion of an entity linking framework and an accompanying notion of the certain links in sucha framework. In an entity linking framework; logic-based constraints are used to expressproperties of the desired link relations in terms of source relations and; possibly; in terms ofother link relations. The definition of the certain links in such a framework makes use ofweighted repairs and consistent answers in inconsistent databases. We demonstrate themodeling capabilities of this approach by showing that numerous concrete entity linkingscenarios can be cast as such entity linking frameworks for suitable choices of constraintsand weights. By using the certain links as a measure of expressive power; we investigate therelative expressive power of several entity linking frameworks and obtain sharp …,LIPIcs-Leibniz International Proceedings in Informatics,2017,*,15
ACM SIGMOD Record Volume 45 Issue 3,Yanlei Diao; Vanessa Braganholo; Marco Brambilla; Chee Yong Chan; Rada Chirkova; Zackary Ives; Anastasios Kementsietsidis; Jeffrey Naughton; Frank Neven; Olga Papaemmanoui; Aditya Parameswaran; Anish Das Sarma; Alkis Simitsis; Wang-Chiew Tan; Nesime Tatbul; Marianne Winslett; Jun Yang,Google; Inc. (search). SIGN IN SIGN UP. ACM SIGMOD Record. Volume 45 Issue 3; September2016 table of contents. Editors: Yanlei Diao; University of Massachusetts Amherst. VanessaBraganholo; Universidade Federal Fluminense. Marco Brambilla; Politecnico di Milano.,*,2016,*,7
System and method of integrating time-aware data from multiple sources,*,A time-aware union operator is disclosed for consistent integration of time-aware data;wherein the time-aware union produces a time-aware consistent integrated view ofunderlying sources according to specified key constraints and policies. The implementationof time-aware union is idempotent; commutative; and associative; thus making it suitable fordata integration; and it produces the same integrated outcome; modulo representation oftime; regardless of the order in which sources are integrated.,*,2016,*,4
connect with us,Zhichao Yan; Val Tannen; Zachary G Ives,Abstract: Provenance is well-understood for relational query operators. Increasingly;however; data analytics is incorporating operations expressed through linear algebra:machine learning operations; network centrality measures; and so on. In this paper; westudy provenance information for matrix data and linear algebra operations. Our coretechnique builds upon provenance for aggregate queries and constructs a K semialgebra.This approach tracks provenance by annotating matrix data and propagating theseannotations through linear algebra operations. We investigate applications in matrixinversion and graph analysis.,*,2016,*,4
Foreword: Special Issue on Database Theory,Balder Ten Cate; Wang-Chiew Tan,This Theory of Computing Systems special issue contains five invited; extended journalarticles of papers presented at the International Conference on Database Theory (ICDT2013); which was held jointly with the International Conference on Extending DatabaseTechnology (EDBT 2013) on March 18-22; 2013 in Genoa; Italy. ICDT is one of the majorconferences for database theory. The ICDT conference started in 1986 and was a biennialconference until 2009; when it became an annual conference and is jointly held with EDBT.,Theory of Computing Systems,2015,*,7
Symposium on Principles of Database Systems (PODS'10)-Article 23 (48 pages)-Characterizing Schema Mappings via Data Examples,B Alexe; B ten Cate; PG Kolaitis; WC Tan,*,ACM Transactions on Database Systems-TODS,2011,*
Letter from the Special Issue Editor.,Wang Chiew Tan,The importance of data provenance has been increasingly recognized by both users andpublishers of data. For users of data; the scientific basis of their analysis relies largely on thecredibility and trusthworthiness of their input data. For publishers of data; the provision ofprovenance as part of their published data is important for scholarship and reproducibility. Intoday's Internet era; where complex ecosystems of data are even more prevalent; it is nowonder that data provenance has become a major research topic in many conferences andworkshops. Data provenance was already the topic of the December 2007 issue of IEEEData Engineering Bulletin. This issue attempts to complement the December 2007 issue byfocusing on how provenance has been captured and exploited by systems that either havebeen developed or are in the process of being developed in the industry and the …,IEEE Data Eng. Bull.,2010,*,14
Laconic schema mappings: computing core universal solutions by means of SQL queries,Balder ten Cate; Laura Chiticariu; Phokion Kolaitis; Wang-Chiew Tan,Abstract: We present a new method for computing core universal solutions in data exchangesettings specified by source-to-target dependencies; by means of SQL queries. Unlikepreviously known algorithms; which are recursive in nature; our method can be implementeddirectly on top of any DBMS. Our method is based on the new notion of a laconic schemamapping. A laconic schema mapping is a schema mapping for which the canonicaluniversal solution is the core universal solution. We give a procedure by which everyschema mapping specified by FO st tgds can be turned into a laconic schema mappingspecified by FO st tgds that may refer to a linear order on the domain of the source instance.We show that our results are optimal; in the sense that the linear order is necessary and themethod cannot be extended to schema mapping involving target constraints. Subjects …,arXiv preprint arXiv:0903.1953,2009,*,1
Schema Mapping Composition,Wang-Chiew Tan,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,*,20
STMark: Towards a Benchmark for Mapping Systems,Bogdan Alexe; W Tan; Yannis Velegrakis,*,*,2008,*
REASONING ABOUT KEYS FOR XML,PEtEr BunEman½; Susan Davidson¾; WEnFEi Fan; CarmEm Hara; Wang-ChiEw Tan,Abstract—We study absolute and relative keys for XML; and investigate their associateddecision problems. We argue that these keys are important to many forms of hierarchicallystructured data including XML documents. In contrast to other proposals of keys for XML; weshow that these keys are always (finitely) satisfiable; and their (finite) implication problem isfinitely axiomatizable. Furthermore; we provide a polynomial time algorithm for determining(finite) implication in the size of keys. Our results also demonstrate; among other things; thatthe analysis of XML keys is far more intricate than its relational counterpart.,*,2003,*,8
SilkRoute,Mary Fernández; Wang Chiew Tan; Dan Suciu,Abstract XML is the standard format for data exchange between inter-enterprise applicationson the Internet. To facilitate data exchange; industry groups define public document typedefinitions (DTDs) that specify the format of the XML data to be exchanged between theirapplications. In this paper; we address the problem of automating the conversion ofrelational data into XML. We describe SilkRoute; a general; dynamic; and efficient tool forviewing and querying relational data in XML. SilkRoute is general; because it can expressmappings of relational data into XML that conforms to arbitrary DTDs. We call thesemappings views. Applications express the data they need as an XML-QL query over theview. SilkRoute is dynamic; because it only materializes the fragment of an XML viewneeded by an application; and it is efficient; because it fully exploits the underlying …,Computer Networks,2000,*,22
Schema Mappings and Data Examples,Bogdan Alexe; Phokion Kolaitis; Wang-Chiew Tan; Balder ten Cate,Theorem (FKMP 2003) M=(S; T; Σ) a GLAV schema mapping.∎ Every source instance I hasa universal solution J wrt M; ie; a solution J for I such that if J'is another solution for I; thenthere is a homomorphism h: J→ J'that is constant on adom (I)(h (c)= c; for c∈ adom (I)).∎Moreover; the chase procedure can be used to construct; given a source instance I; acanonical universal solution chase,*,*,*,0
Interactive Rule Refinement for Fraud Detection,Tova Milo; Slava Novgorodov; Wang-Chiew Tan,ABSTRACT Credit card frauds are unauthorized transactions that are made or attempted bya person or an organization that is not authorized by the card holders. Fraud with general-purpose cards (credit; debit cards etc.) is a billion dollar industry and companies aretherefore investing significant efforts in identifying and preventing them. In addition tomachine learning-based techniques; credit card companies often employ domain experts tomanually specify rules that exploit general or domain knowledge for improving the detectionprocess. Over time; however; as new (fraudulent and legitimate) transaction arrive; theserules need to be updated and refined to capture the evolving (fraud and legitimate) activitypatterns. The goal of the RUDOLF system described in this paper is to guide and assistdomain experts in this challenging task. RUDOLF automatically determines the “best” …,*,*,*,7
Peter Buneman University of Edinburgh,Wang-Chiew Tan,*,*,*,*
Minimizing Uncertainty in Crowd Data Sourcing,Rubi Boim; Ohad Greenshpan; Tova Milo; Slava Novgorodov; Neoklis Polyzotis; Wang-Chiew Tan,Abstract—Crowd data sourcing engages human users to contribute information to adatabase by answering specific questions. A typical use case is in e-commerce sites; wherecustomers are asked to rate the products they purchase or the corresponding sellers; withthe ultimate goal of creating a database of product or seller ratings. In this work; weinvestigate the problem of selecting which questions to ask and to which users in order tominimize uncertainty in the resulting database with respect to an application-specifiedcriterion (eg; the shape of the rating distribution in the previous example). We formalize theoptimization problem and analyze its hardness for different practical scenarios where thereare constraints on the total number of questions; the number of questions per user; or thequestions that users are likely to answer. For the tractable cases; we develop optimal …,*,*,*,10
Workshop Officers,Laura Haas; Zachary Ives; Mukesh Mohania; Manish Bhide; Divy Agrawal; Phil Bernstein; Kevin Chang; Yi Chen; Alin Deutsch; AnHai Doan; Alon Halevy; Mizuho Iwaihara; Masaru Kitsuregawa; Craig Knoblock; Sergey Melnik; Ullas Nambiar; Felix Naumann; Evaggelia Pitoura; Prasan Roy; Michael Schrefl; Kohichi Takeda; Wang-Chiew Tan; Millist Vincent; Ji-Rong Wen,*,*,*,*
Bulletin of the Technical Committee on,Mary Fernandez; Atsuyuki Morishima; Dan Suciu; Wang-Chiew Tan,Bulletin of the Technical Committee on Ø Ò Ò Ö Ò June 2001 Vol. 24 No. 2 IEEE Computer SocietyLetters Letter from the Editor-in-Chief...................................................... David Lomet 1 Nominationsfor Chair of TCDE.................... Paul Larson; Masaru Kitsuregawa; and Betty Salzberg 1 Letterfrom the Special Issue Editor.................................................. Alon Y. Halevy 2 Special Issue on XMLData Management State-of-the-art XML Support in … Editorial Board Editor-in-Chief DavidB. Lomet Microsoft Research One Microsoft Way; Bldg. 9 Redmond WA 98052-6399 lomet@microsoft. com Associate Editors Luis Gravano Computer Science Department Columbia University1214 Amsterdam Avenue New York; NY 10027 Alon Levy University of Washington ComputerScience and Engineering Dept. Sieg Hall; Room 310 Seattle; WA 98195 Sunita Sarawagi Schoolof Information Technology Indian Institute of Technology; Bombay Powai Street Mumbai …,*,*,*,20
Tools and Techniques for Understanding Data Exchange Systems,Wang-Chiew Tan,Abstract It is common knowledge that distinct but often content-related data sources exist indifferent formats. Consequently; the ability to restructure (or exchange) data from thesesources into a format desired by consumers of such data sources is extremely valuable. It isalso common knowledge that alot of time and effort are needed to set up and tune dataexchange and data integration systems. Therefore; it is important to devise techniques andtools that can reduce the amount of effort needed to set up; and subsequently; understandand maintain such systems. Our position is that more tools and techniques are needed tofacilitate the installation; understanding and maintenance of data exchange systems.,*,*,*,15
Deep Citation and E cient Archiving in Digital Libraries,Peter Buneman; Keishi Tajima; Wang-Chiew Tan,Many digital libraries are developed in support of scienti c and scholarly work. As such they arerepositories of data whose value lies in its selection; accuracy and organization. In this respectthey perform the same function as conventional libraries. However digital libraries are capableof supporting much more complex structures; such as databases; and they can capture moreaccurately and rapidly the evolution of knowledge in an area. Given these di erences; how dowe cite some component of a digital library; and how do we e ectively archive the informationgiven that it is in continually changing? It turns out that these two issues are intimatelyconnected … In conventional scholarship there is a standard unit of citation { the document.These are the (physically) indivisible units from which a conventional library is composed; andstandards for bibliographic citation have been well developed 8; 11; 2]. However; in a …,*,*,*,10
Peter Buneman Susan Davidson Wenfei Fanz Carmem Hara,Wang-Chiew Tan,*,*,*,*
QUICK-A Graphical User Interface to Genome Multidatabases,Wang Chiew Tan; Ke Wang; Limsoon Wong; Heng Mui Keng Terrace,Abstract Formulating queries to access multiple databases can be a formidable task;especially when many terms from various databases and complex constraints are involved.To specify a multidatabase query; the user usually has to search through documents forexact database terms and learn the multidatabase language. This report presents QUICK(QUery Interface to CPL-Kleisli); a graphical user interface to multiple databases. CPL(Collection Programming Language) is a high-level multidatabase language built on top ofan open query system Kleisli. QUICK allows users to handle overwhelming information fromdifferent data sources in an intuitive and uniform manner. The query specification is reducedto specifying user's terms in his/her own world; selecting paths and specifying constraints ina graph. Through the terms entered by the user; QUICK is able to reduce the scope of the …,*,*,*,17
