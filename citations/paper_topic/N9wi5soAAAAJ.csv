A survey of approaches to automatic schema matching,Erhard Rahm; Philip A Bernstein,Abstract. Schema matching is a basic problem in many database application domains; suchas data integration; E-business; data warehousing; and semantic query processing. Incurrent implementations; schema matching is typically performed manually; which hassignificant limitations. On the other hand; previous research papers have proposed manytechniques to achieve a partial automation of the match operation for specific applicationdomains. We present a taxonomy that covers many of these existing approaches; and wedescribe the approaches in some detail. In particular; we distinguish between schema-leveland instance-level; element-level and structure-level; and language-based and constraint-based matchers. Based on our classification we review some previous matchimplementations thereby indicating which part of the solution space they cover. We intend …,VLDB Journal,2001,4084,7
Generic schema matching with Cupid,Jayant Madhavan; Philip A Bernstein; Erhard Rahm,Abstract Schema matching is a critical step in many applications; such as XML messagemapping; data warehouse loading; and schema integration. In this paper; we investigatealgorithms for generic schema matching; outside of any particular data model or application.We first present a taxonomy for past solutions; showing that a rich range of techniques isavailable. We then propose a new algorithm; Cupid; that discovers mappings betweenschema elements based on their names; data types; constraints; and schema structure;using a broader set of techniques than past approaches. Some of our innovations are theintegrated use of linguistic and structural matching; context-dependent matching of sharedtypes; and a bias toward leaf structure where much of the schema content resides. Afterdescribing our algorithm; we present experimental results that compare Cupid to two …,Proc. 27th  Int. Conf. on Very Large Data Bases; 2001,2001,1891,15
Similarity flooding: A versatile graph matching algorithm and its application to schema matching,Sergey Melnik; Hector Garcia-Molina; Erhard Rahm,Matching elements of two data schemas or two data instances plays a key role in datawarehousing; e-business; or even biochemical applications. In this paper we present amatching algorithm based on a fixpoint computation that is usable across different scenarios.The algorithm takes two graphs (schemas; catalogs; or other data structures) as input; andproduces as output a mapping between corresponding nodes of the graphs. Depending onthe matching goal; a subset of the mapping is chosen using filters. After our algorithm runs;we expect a human to check and if necessary adjust the results. As a matter of fact; weevaluate the'accuracy'of the algorithm by counting the number of needed adjustments. Weconducted a user study; in which our accuracy metric was used to estimate the labor savingsthat the users could obtain by utilizing our algorithm to obtain an initial matching. Finally …,Proc.18th Int. Conf. on Data Engineering; 2002,2002,1811,20
Data cleaning: Problems and current approaches,Erhard Rahm; Hong Hai  Do,Abstract We classify data quality problems that are addressed by data cleaning and providean overview of the main solution approaches. Data cleaning is especially required whenintegrating heterogeneous data sources and should be addressed together with schema-related data transformations. In data warehouses; data cleaning is a major part of the so-called ETL process. We also discuss current tool support for data cleaning.,IEEE Data Eng. Bull.,2000,1615,1
COMA: a system for flexible combination of schema matching approaches,Hong-Hai Do; Erhard Rahm,This chapter presents the generic schema match system; COmbination MAtch (COMA);which provides an extensible library of simple and hybrid match algorithms and supports apowerful framework for combining match results. The user can tailor match strategies byselecting the marchers and their combination for a given match problem. Hybrid matcherscan also be configured easily by combining existing matchers using the providedcombination strategies. Schema matching is the task of finding semantic correspondencesbetween elements of two schemas. It is needed in many database applications; such asintegration of web data sources; data warehouse loading; and XML message mapping. Toreduce the amount of user effort as much as possible; automatic approaches combiningseveral match techniques are required. While such match approaches have found …,Proc. 28th Int. Conf. on Very Large Data Bases,2002,1414,10
Schema and ontology matching with COMA++,David Aumueller; Hong-Hai Do; Sabine Massmann; Erhard Rahm,Abstract We demonstrate the schema and ontology matching tool COMA++. It extends ourprevious prototype COMA utilizing a composite approach to combine different matchalgorithms [3]. COMA++ implements significant improvements and offers a comprehensiveinfrastructure to solve large real-world match problems. It comes with a graphical interfaceenabling a variety of user interactions. Using a generic data representation; COMA++uniformly supports schemas and ontologies; eg the powerful standard languages W3C XMLSchema and OWL. COMA++ includes new approaches for ontology matching; in particularthe utilization of shared taxonomies. Furthermore; different match strategies can be appliedincluding various forms of reusing previously determined match results and a so-calledfragment-based match approach which decomposes a large match problem into smaller …,Proc. 2005 ACM SIGMOD Int. Conf. on Management of Data,2005,748,20
Comparison of schema matching evaluations,Hong-Hai Do; Sergey Melnik; Erhard Rahm,Abstract Recently; schema matching has found considerable interest in both research andpractice. Determining matching components of database or XML schemas is needed inmany applications; eg for E-business and data integration. Various schema matchingsystems have been developed to solve the problem semi-automatically. While there havebeen some evaluations; the overall effectiveness of currently available automatic schemamatching systems is largely unclear. This is because the evaluations were conducted indiverse ways making it difficult to assess the effectiveness of each single system; let alone tocompare their effectiveness. In this paper we survey recently published schema matchingevaluations. For this purpose; we introduce the major criteria that influence the effectivenessof a schema matching approach and use these criteria to compare the various systems …,Net. ObjectDays: International Conference on Object-Oriented and Internet-Based Technologies; Concepts; and Applications for a Networked World,2002,566,1
Rondo: A programming platform for generic model management,Sergey Melnik; Erhard Rahm; Philip A Bernstein,Abstract Model management aims at reducing the amount of programming needed for thedevelopment of metadata-intensive applications. We present a first complete prototype of ageneric model management system; in which high-level operators are used to manipulatemodels and mappings between models. We define the key conceptual structures: models;morphisms; and selectors; and describe their use and implementation. We specify thesemantics of the known model-management operators applied to these structures; suggestnew ones; and develop new algorithms for implementing the individual operators. Weexamine the solutions for two model-management tasks that involve manipulations ofrelational schemas; XML schemas; and SQL views.,Proc. 2003 ACM SIGMOD Int. Conf. on Management of Data,2003,427,4
AgentWork: a workflow system supporting rule-based workflow adaptation,Robert Müller; Ulrike Greiner; Erhard Rahm,Abstract Current workflow management systems still lack support for dynamic and automaticworkflow adaptations. However; this functionality is a major requirement for next–generationworkflow systems to provide sufficient flexibility to cope with unexpected failure events. Wepresent the concepts and implementation of A gent W ork; a workflow management systemsupporting automated workflow adaptations in a comprehensive way. A rule-basedapproach is followed to specify exceptions and necessary workflow adaptations. A gent Work uses temporal estimates to determine which remaining parts of running workflows areaffected by an exception and is able to predictively perform suitable adaptations. This helpsto ensure that necessary adaptations are performed in time with minimal user interactionwhich is especially valuable in complex applications such as for medical treatments.,Data & Knowledge Engineering,2004,370,1
Schema matching and mapping,Zohra Bellahsene; Angela Bonifati; Erhard Rahm,*,*,2011,343
Datenbanksysteme: Konzepte und Techniken der Implementierung,Theo Härder; Erhard Rahm,*,*,2001,306
Frameworks for entity matching: A comparison,Hanna Köpcke; Erhard Rahm,Abstract Entity matching is a crucial and difficult task for data integration. Entity matchingframeworks provide several methods and their combination to effectively solve differentmatch tasks. In this paper; we comparatively analyze 11 proposed frameworks for entitymatching. Our study considers both frameworks which do or do not utilize training data tosemi-automatically find an entity matching strategy to solve a given match task. Moreover;we consider support for blocking and the combination of different match algorithms. Wefurther study how the different frameworks have been evaluated. The study aims at exploringthe current state of the art in research prototypes of entity matching frameworks and theirevaluations. The proposed criteria should be helpful to identify promising frameworkapproaches and enable categorizing and comparatively assessing additional entity …,Data & Knowledge Engineering,2010,294,15
Matching large schemas: Approaches and evaluation,Hong-Hai Do; Erhard Rahm,Abstract Current schema matching approaches still have to improve for large and complexSchemas. The large search space increases the likelihood for false matches as well asexecution times. Further difficulties for Schema matching are posed by the high expressivepower and versatility of modern schema languages; in particular user-defined types andclasses; component reuse capabilities; and support for distributed schemas andnamespaces. To better assist the user in matching complex schemas; we have developed anew generic schema matching tool; COMA++; providing a library of individual matchers anda flexible infrastructure to combine the matchers and refine their results. Different matchstrategies can be applied including a new scalable approach to identify context-dependentcorrespondences between schemas with shared elements and a fragment-based match …,Information Systems,2007,269,2
Evaluation of entity resolution approaches on real-world match problems,Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract Despite the huge amount of recent research efforts on entity resolution (matching)there has not yet been a comparative evaluation on the relative effectiveness and efficiencyof alternate approaches. We therefore present such an evaluation of existingimplementations on challenging real-world match tasks. We consider approaches both withand without using machine learning to find suitable parameterization and combination ofsimilarity functions. In addition to approaches from the research community we also considera state-of-the-art commercial entity resolution implementation. Our results indicate significantquality and efficiency differences between different approaches. We also find that somechallenging resolution tasks such as matching product entities from online shops are notsufficiently solved with conventional approaches based on the similarity of attribute …,Proceedings of the VLDB Endowment,2010,226,6
Generic schema matching; ten years later,Philip A Bernstein; Jayant Madhavan; Erhard Rahm,ABSTRACT In a paper published in the 2001 VLDB Conference; we proposed treatinggeneric schema matching as an independent problem. We developed a taxonomy ofexisting techniques; a new schema matching algorithm; and an approach to comparativeevaluation. Since then; the field has grown into a major research topic. We briefly summarizethe new techniques that have been developed and applications of the techniques in thecommercial world. We conclude by discussing future trends and recommendations forfurther work.,Proceedings of the VLDB Endowment,2011,215,7
On matching schemas automatically,Erhard Rahm; Philip A Bernstein,Abstract Schema matching is a basic problem in many database application domains; suchas data integration; E-business; data warehousing; and semantic query processing. Incurrent implementations; schema matching is typically performed manually; which hassignificant limitations. On the other hand; in previous research many techniques have beenproposed to achieve a partial automation of the Match operation for specific applicationdomains. We present a taxonomy that covers many of the existing approaches; and wedescribe these approaches in some detail. In particular; we distinguish between schema-and instance-level; element-and structure-level; and language-and constraint-basedmatchers. Based on our classification we review some previous match implementationsthereby indicating which part of the solution space they cover. We intend our taxonomy …,Technical Report,2001,203,7
Multi-dimensional database allocation for parallel data warehouses,Thomas Stöhr; Holger Märtens; Erhard Rahm,Abstract Data allocation is a key performance factor for parallel database systems (PDBS).This holds especially for data warehousing environments where huge amounts of data andcomplex analytical queries have to be dealt with. While there are several studies on dataallocation for relational PDBS; the specific requirements of data warehouses have not yetbeen sufficiently addressed. In this study; we consider the allocation of relational datawarehouses based on a star schema and utilizing bitmap index structures. We investigatehow a multi-dimensional hierarchical data fragmentation of the fact table supports queriesreferencing different subsets of the schema dimensions. Our analysis is based on realisticparameters derived from a decision support benchmark. The performance implications ofdifferent allocation choices are evaluated by means of a detailed simulation model.,Proc. 26th Int. Conf. on Very Large Data Bases,2000,199,17
XMach-1: A benchmark for XML data management,Timo Böhme; Erhard Rahm,Abstract We propose a scaleable multi-user benchmark called XMach-1 (AML DataManagement benchmark) for evaluating the performance of XML data managementsystems. It is based on a web application and considers different types of XML data; inparticular text documents; schema-less data and structured data. We specify the structure ofthe benchmark database and the generation of its contents. Furthermore; we define a mix ofXML queries and update operations for which system performance is determined. Theprimary performance metric; Xqps; measures the query throughput of a system underresponse time constraints. We will use XMach-1 to evaluate both native XML datamanagement systems and XML-enabled relational DBMS.,*,2001,188,7
Matching Large XML Schemas,Erhard Rahm; H Do; S. Maßmann,Abstract Current schema matching approaches still have to improve for very large andcomplex schemas. Such schemas are increasingly written in the standard language W3CXML schema; especially in E-business applications. The high expressive power andversatility of this schema language; in particular its type system and support for distributedschemas and name-spaces; introduce new issues. In this paper; we study some of theimportant problems in matching such large XML schemas. We propose a fragment-orientedmatch approach to decompose a large match problem into several smaller ones and toreuse previous match results at the level of schema fragments.,SIGMOD Record,2004,172,15
Mehrrechner-Datenbanksysteme: Grundlagen der verteilten und parallelen Datenbankverarbeitung,Erhard Rahm,Traditionellerweise erfolgt die Datenbankverarbeitung zentralisiert; wobei die Datenbank aneinem Rechner gehalten und verwaltet wird. Zudem arbeiten üblicherweise Anwendungenund Transaktionen nur mit einer Datenbank. Dieser einfache Ansatz verliert jedochzunehmend an Bedeutung; da viele Anwendungsbereiche eine verteilte und paralleleDatenbankverarbeitung verlangen. Treibende Kräfte hierfür sind Forderungen nach hoherLeistungsfähigkeit und Verfügbarkeit; Verbesserung der Kosteneffektivität sowieUnterstützung dezentraler Organisationsstrukturen. Weiterhin muß Anwendungen häufig derintegrierte Zugriff auf mehrere unabhängige und heterogene Datenbanken ermöglichtwerden. Zur Erfüllung dieser Anforderungen eignen sich verschiedene Klassensogenannter Mehrrechner-Datenbanksysteme; bei denen Datenbanksysteme (DBS) auf …,*,1994,166,22
Towards large-scale schema and ontology matching,Erhard Rahm,Abstract The purely manual specification of semantic correspondences between schemas isalmost infeasible for very large schemas or when many different schemas have to bematched. Hence; solving such large-scale match tasks asks for automatic or semiautomaticschema matching approaches. Large-scale matching needs especially to be supported forXML schemas and different kinds of ontologies due to their increasing use and size; eg; in e-business and web and life science applications. Unfortunately; correctly and efficientlymatching large schemas and ontologies are very challenging; and most previous matchsystems have only addressed small match tasks. We provide an overview about recentlyproposed approaches to achieve high match quality or/and high efficiency for large-scalematching. In addition to describing some recent matchers utilizing instance and usage …,*,2011,164,20
Data warehouse scenarios for model management,Philip Bernstein; Erhard Rahm,Abstract Model management is a framework for supporting meta-data related applicationswhere models and mappings are manipulated as first class objects using operations such asMatch; Merge; ApplyFunction; and Compose. To demonstrate the approach; we show how touse model management in two scenarios related to loading data warehouses. The casestudy illustrates the value of model management as a methodology for approaching meta-data related problems. It also helps clarify the required semantics of key operations. Thesedetailed scenarios provide evidence that generic model management is useful and; verylikely; implementable.,Conceptual Modeling—ER 2000,2000,161,15
Load Balancing for MapReduce-based Entity Resolution,Lars Kolb; Andreas Thor; Erhard Rahm,The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. Inthe presence of skewed data; sophisticated redistribution approaches thus becomenecessary to achieve load balancing among all reduce tasks to be executed in parallel. Forthe complex problem of entity resolution; we propose and evaluate two approaches for suchskew handling and load balancing. The approaches support blocking techniques to reducethe search space of entity resolution; utilize a preprocessing MapReduce job to analyze thedata distribution; and distribute the entities of large blocks among multiple reduce tasks. Theevaluation on a real cloud infrastructure shows the value and effectiveness of the proposedload balancing approaches.,Proc. ICDE,2012,145,11
FUNC: a package for detecting significant associations between gene sets and ontological annotations,Kay Prüfer; Bjoern Muetzel; Hong-Hai Do; Gunter Weiss; Philipp Khaitovich; Erhard Rahm; Svante Pääbo; Michael Lachmann; Wolfgang Enard,Genome-wide expression; sequence and association studies typically yield large sets ofgene candidates; which must then be further analysed and interpreted. Information aboutthese genes is increasingly being captured and organized in ontologies; such as the GeneOntology. Relationships between the gene sets identified by experimental methods andbiological knowledge can be made explicit and used in the interpretation of results.However; it is often difficult to assess the statistical significance of such analyses since manyinter-dependent categories are tested simultaneously. We developed the program packageFUNC that includes and expands on currently available methods to identify significantassociations between gene sets and ontological annotations. Implemented are several testsin particular well suited for genome wide sequence comparisons; estimates of the family …,BMC Bioinformatics,2007,127,10
MediGRID: Towards a user friendly secured grid infrastructure,Dagmar Krefting; Julian Bart; Kamen Beronov; Olga Dzhimova; Jürgen Falkner; Michael Hartung; Andreas Hoheisel; Tobias A Knoch; Thomas Lingner; Yassene Mohammed; Kathrin Peter; Erhard Rahm; Ulrich Sax; Dietmar Sommerfeld; Thomas Steinke; Thomas Tolxdorff; Michal Vossberg; Fred Viezens; Anette Weisbecker,Abstract Many scenarios in medical research are predestined for grid computing. Largeamounts of data in complex medical image; biosignal and genome processing demandlarge computing power and data storage. Integration of distributed; heterogeneous data; egcorrelation between phenotype and genotype data are playing an essential part in lifesciences. Sharing of specialized software; data and processing results for collaborative workare further tasks which would strongly benefit from the use of grid infrastructures. However;two major barriers are identified in existing grid environments that prevent extensive usewithin the life sciences community: Extended security requirements and appropriateusability. To meet these requirements; the MediGRID project is enhancing the basic D-Gridinfrastructure along with the implementation of prototype applications from different fields …,Future Generation Computer Systems,2009,114,15
Supporting executable mappings in model management,Sergey Melnik; Philip A Bernstein; Alon Halevy; Erhard Rahm,Abstract Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators; such as Compose; Diff; andMerge; that are applied to models; such as database schemas or interface specifications;and to mappings between models. Prior model management solutions focused on a simpleclass of mappings that do not have executable semantics. Yet many metadata applicationsrequire that mappings be executable; expressed in SQL; XSLT; or other data transformationlanguages. In this paper; we develop a semantics for model-management operators thatallows applying the operators to executable mappings. Our semantics captures previously-proposed desiderata and is language-independent: the effect of the operators is expressedin terms of what they do to the instances of models and mappings. We describe an …,Proc.  2005 ACM SIGMOD Int. Conf. on Management of Data,2005,114,4
Dedoop: Efficient Deduplication with Hadoop,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract We demonstrate a powerful and easy-to-use tool called Dedoop (< u> De</u>duplication with Ha< u> doop</u>) for MapReduce-based entity resolution (ER) of largedatasets. Dedoop supports a browser-based specification of complex ER workflowsincluding blocking and matching steps as well as the optional use of machine learning forthe automatic generation of match classifiers. Specified workflows are automaticallytranslated into MapReduce jobs for parallel execution on different Hadoop clusters. Toachieve high performance Dedoop supports several advanced load balancing strategies.,PVLDB,2012,111,7
Multi-pass sorted neighborhood blocking with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract Cloud infrastructures enable the efficient parallel execution of data-intensive taskssuch as entity resolution on large datasets. We investigate challenges and possiblesolutions of using the MapReduce programming model for parallel entity resolution usingSorting Neighborhood blocking (SN). We propose and evaluate two efficient MapReduce-based implementations for single-and multi-pass SN that either use multiple MapReducejobs or apply a tailored data replication. We also propose an automatic data partitioningapproach for multi-pass SN to achieve load balancing. Our evaluation based on real-worlddatasets shows the high efficiency and effectiveness of the proposed approaches.,Computer Science-Research and Development,2012,105,5
Supporting Efficient Streaming and Insertion of XML Data in RDBMS.,Timo Böhme; Erhard Rahm,Abstract. Relational database systems are increasingly used to manage XML documents;especially for data-centric XML. In this paper we present a new approach to efficientlymanage document-centric XML data based on a generic relational mapping. Such a genericXML storage is especially useful in data integration systems to manage highly diverse XMLdocuments. We focus on efficient insert operations; support of streamed data and fastretrieval of document fragments. Therefore we introduce a new numbering scheme calledDLN (Dynamic Level Numbering) and several variants of it. A performance evaluation basedon a prototypical implementation demonstrates the high efficiency of DLN.,DIWeb,2004,102,4
Citation analysis of database publications,Erhard Rahm; Andreas Thor,Abstract We analyze citation frequencies for two main database conferences (SIGMOD;VLDB) and three database journals (TODS; VLDB Journal; Sigmod Record) over 10 years.The citation data is obtained by integrating and cleaning data from DBLP and GoogleScholar. Our analysis considers different comparative metrics per publication venue; inparticular the total and average number of citations as well as the impact factor which has sofar only been considered for journals. We also determine the most cited papers; authors;author institutions and their countries.,ACM Sigmod Record,2005,99,20
Dynamic multi-resource load balancing in parallel database systems,Erhard Rahm; Robert Marek,Abstract Parallel database systems have to support the effective parallelization of complexqueries in multi-user mode; ie in combination with inter-query/inter-transaction parallelism.For this purpose; dynamic scheduling and load balancing strategies are necessary thatconsider the current system state for determining the degree of intra-query parallelism andfor selecting the processors for executing subqueries. We study these issues for parallelhash join processing and show that the two subproblems should be addressed in anintegrated way. Even more importantly; however; is the use of a multiresource loadbalancing approach that considers all potential bottleneck resources; in particular memory;disk and CPU. We discuss basic performance tradeoffs to consider and evaluate theperformance of several load balancing strategies by means of a detailed simulation …,Proc. 22th Int. Conf. on Very Large Data Bases,1995,96,15
Quickmig: automatic schema matching for data migration projects,Christian Drumm; Matthias Schmitt; Hong-Hai Do; Erhard Rahm,Abstract A common task in many database applications is the migration of legacy data frommultiple sources into a new one. This requires identifying semantically related elements ofthe source and target systems and the creation of mapping expressions to transforminstances of those elements from the source format to the target format. Currently; datamigration is typically done manually; a tedious and timeconsuming process; which is difficultto scale to a high number of data sources. In this paper; we describe QuickMig; a new semi-automatic approach to determining semantic correspondences between schema elementsfor data migration applications. QuickMig advances the state of the art with a set of newtechniques exploiting sample instances; domain ontologies; and reuse of existing mappingsto detect not only element correspondences but also their mapping expressions …,Proc. 16th ACM CIKM (Conf. on Information and Knowledge Management),2007,95,1
Convergent validity of bibliometric Google Scholar data in the field of chemistry—Citation counts for papers that were accepted by Angewandte Chemie International...,Lutz Bornmann; Werner Marx; Hermann Schier; Erhard Rahm; Andreas Thor; Hans-Dieter Daniel,Abstract Examining a comprehensive set of papers (n= 1837) that were accepted forpublication by the journal Angewandte Chemie International Edition (one of the primechemistry journals in the world) or rejected by the journal but then published elsewhere; thisstudy tested the extent to which the use of the freely available database Google Scholar(GS) can be expected to yield valid citation counts in the field of chemistry. Analyses ofcitations for the set of papers returned by three fee-based databases–Science CitationIndex; Scopus; and Chemical Abstracts–were compared to the analysis of citations foundusing GS data. Whereas the analyses using citations returned by the three fee-baseddatabases show very similar results; the results of the analysis using GS citation datadiffered greatly from the findings using citations from the fee-based databases. Our study …,Journal of informetrics,2009,93,20
GOMMA: a component-based infrastructure for managing and analyzing life science ontologies and their evolution,Toralf Kirsten; Anika Gross; Michael Hartung; Erhard Rahm,Ontologies are increasingly used to structure and semantically describe entities of domains;such as genes and proteins in life sciences. Their increasing size and the high frequency ofupdates resulting in a large set of ontology versions necessitates efficient management andanalysis of this data. We present GOMMA; a generic infrastructure for managing andanalyzing life science ontologies and their evolution. GOMMA utilizes a generic repository touniformly and efficiently manage ontology versions and different kinds of mappings.Furthermore; it provides components for ontology matching; and determining evolutionaryontology changes. These components are used by analysis tools; such as the OntologyEvolution Explorer (OnEX) and the detection of unstable ontology regions. We introduce thecomponent-based infrastructure and show analysis results for selected components and …,Journal of Biomedical Semantics,2011,89,8
An Integrative and Uniform Model for Metadata Management in Data Warehousing Environments,T Stöhr; R Müller; E Rahm,Abstract Due to the increasing complexity of data warehouses; a centralized and declarativemanagement of metadata is essential for data warehouse administration; maintenance andusage. Metadata are usually divided into technical and semantic metadata. Typically; currentapproaches only support subsets of these metadata types; such as data movementmetadata or multidimensional metadata for OLAP. In particular; the interdependenciesbetween technical and semantic metadata have not yet been investigated sufficiently. Therepresentation of these interdependencies form an important prerequisite for the translationof queries formulated at the business concept level to executable queries on physical data.Therefore; we suggest a uniform and integrative model for data warehouse metadata. Thismodel uses a uniform representation approach based on the Uniform Modeling …,Proc. Int. Workshop on Design an Management of Fata Warehouses (DMDW’99). http://sunsite. informatik. rwth-aachen. de/Publications/CEUR-WS,1999,85,7
Instance Matching with COMA++.,Daniel Engmann; Sabine Massmann,Abstract: Schema matching is the process of identifying semantic correspondences betweenschemas. COMA++ is a matching prototype which uses several characteristics of schemas todetermine similarities between them; for example the names and data types of the schemaelements and structural information. In this paper we propose two instance-based matchersfor COMA++ to gain a further quality improvement. The features of the matchers and firstresults are described.,BTW workshops,2007,83,4
Empirical performance evaluation of concurrency and coherency control protocols for database sharing systems,Erhard Rahm,Abstract Database Sharing (DB-sharing) refers to a general approach for building adistributed high performance transaction system. The nodes of a DB-sharing system arelocally coupled via a high-speed interconnect and share a common database at the disklevel. This is also known as a “shared disk” approach. We compare database sharing withthe database partitioning (shared nothing) approach and discuss the functional DBMScomponents that require new and coordinated solutions for DB-sharing. The performance ofDB-sharing systems critically depends on the protocols used for concurrency and coherencycontrol. The frequency of communication required for these functions has to be kept as lowas possible in order to achieve high transation rates and short response times. A trace-driven simulation system for DB-sharing complexes has been developed that allows a …,ACM Transactions on Database Systems (TODS),1993,79,5
COnto–Diff: generation of complex evolution mappings for life science ontologies,Michael Hartung; Anika Groß; Erhard Rahm,Abstract Life science ontologies evolve frequently to meet new requirements or to betterreflect the current domain knowledge. The development and adaptation of large andcomplex ontologies is typically performed collaboratively by several curators. To effectivelymanage the evolution of ontologies it is essential to identify the difference (Diff) betweenontology versions. Such a Diff supports the synchronization of changes in collaborativecuration; the adaptation of dependent data such as annotations; and ontology versionmanagement. We propose a novel approach COnto–Diff to determine an expressive andinvertible diff evolution mapping between given versions of an ontology. Our approach firstmatches the ontology versions and determines an initial evolution mapping consisting ofbasic change operations (insert/update/delete). To semantically enrich the evolution …,Journal of biomedical informatics,2013,77,15
Dynamic load balancing in parallel database systems,Erhard Rahm,Abstract Dynamic load balancing is a prerequisite for effectively utilizing large paralleldatabase systems. Load balancing at different levels is required in particular for assigningtransactions and queries as well as subqueries to nodes. Special problems are posed by theneed to support both inter-transaction/query as well as intra-transaction/query parallelismdue to conflicting performance requirements. We compare the major architectures forparallel database systems; Shared Nothing and Shared Disk; with respect to their loadbalancing potential. For this purpose; we focus on parallel scan and join processing in multi-user mode. It turns out that both the degree of query parallelism as well as the processorallocation should be determined in a coordinated way and based on the current utilization ofcritical resource types; in particular CPU and memory.,European Conference on Parallel Processing,1996,76,8
An online bibliography on schema evolution,Erhard Rahm; Philip A Bernstein,Abstract We briefly motivate and present a new online bibliog- raphy on schema evolution; anarea which has recently gained much interest in both research and practice. 1 Introduction Schemaevolution is the ability to change deployed schemas; ie; metadata structures formally describingcomplex artifacts such as databases; messages; application programs or workflows. Typicalschemas thus include relational or object-oriented (OO) database schemas; conceptual ER orUML models; ontologies; XML schemas; software interfaces and workflow specifications.Obviously; the need for schema evolution occurs very often in order to deal with new or changedrequirements; to correct deficiencies in the current schemas or to migrate to a new platform. Effectivesupport for schema evolution is challeng- ing since schema changes may have to bepropagated; correctly and efficiently; to instance data; views; applica- tions and other …,ACM Sigmod Record,2006,75,1
MOMA-a mapping-based object matching system,Andreas Thor; Erhard Rahm,Object matching or object consolidation is a crucial task for data integration and datacleaning. It addresses the problem of identifying object instances in data sources referring tothe same real world entity. We propose a flexible framework called MOMA for mapping-based object matching. It allows the construction of match workflows combining the results ofseveral matcher algorithms on both attribute values and contextual information. The outputof a match task is an instance-level mapping that supports information fusion in P2P dataintegration systems and can be re-used for other match tasks. MOMA utilizes furthersemantic mappings of different cardinalities and provides merge and compose operators formapping combination. We propose and evaluate several strategies for both object matchingbetween different sources as well as for duplicate identification within a single data …,Proc. CIDR,2007,74,7
Evolution of the COMA Match System,Sabine Massmann; Salvatore Raunich; David Aumüller; Patrick Arnold; Erhard Rahm,Abstract. The schema and ontology matching systems COMA and COMA++ are widely usedin the community as a basis for comparison of new match approaches. We give an overviewof the evolution of COMA during the last decade. In particular we discuss lessons learned onstrong points and remaining weaknesses. Furthermore; we outline the design andfunctionality of the upcoming COMA 3.0.,Proc. ISWC workshop on Ontology Matching,2011,70,15
Analyzing the evolution of life science ontologies and mappings,Michael Hartung; Toralf Kirsten; Erhard Rahm,Abstract Ontologies are heavily developed and used in life sciences and undergocontinuous changes. However; the evolution of life science ontologies and references tothem (eg; annotations) is not well understood and has received little attention so far. Wetherefore propose a generic framework for analyzing both the evolution of ontologies andthe evolution of ontology-related mappings; in particular annotations referring to ontologiesand similarity (match) mappings between ontologies. We use our framework for an extensivecomparative evaluation of evolution measures for 16 life science ontologies. Moreover; weanalyze the evolution of annotation mappings and ontology mappings for the GeneOntology.,International Workshop on Data Integration in the Life Sciences,2008,70,7
Analysis of dynamic load balancing strategies for parallel shared nothing database systems,Erhard Rahm; Robert Marek,Abstract Parallel database systems have to support both inter-transaction as well as intra-transaction parallelism. Inter-transaction parallelism (multi-user mode) is required to achievehigh throughput; in particular for OL TP transactions; and sufficient cost-effectiveness. Intra-transaction parallelism is a prerequisite for reducing the response time of complex and data-intensive transactions (queries). In order to achieve both goals dynamic strategies for loadbalancing and scheduling are necessary which take the current system state into account forallocating transactions and subqueries to processors and for determining the degree of intra-transaction parallelism. We study the load balancing problem for parallel join processing inShared Nothing database systems. In these systems; join processing is typically based on adynamic redistribution of relations to join processors thus making dynamic load balancing …,Proc. 19th Int. Conf. on Very Large Data Bases,1993,69,4
Recent advances in schema and ontology evolution,Michael Hartung; James Terwilliger; Erhard Rahm,Abstract Schema evolution is the increasingly important ability to adapt deployed schemasto changing requirements. Effective support for schema evolution is challenging sinceschema changes may have to be propagated; correctly and efficiently; to instance data anddependent schemas; mappings; or applications. We introduce the major requirements foreffective schema and ontology evolution; including support for a rich set of changeoperations; simplicity of change specification; evolution transparency (eg; by providing andmaintaining views or schema versions); automated generation of evolution mappings; andpredictable instance migration that minimizes data loss and manual intervention. We thengive an overview about the current state of the art and recent research results for theevolution of relational schemas; XML schemas; and ontologies. For numerous …,*,2011,67,7
On metadata interoperability in data warehouses,Hong Hai Do; Erhard Rahm,Abstract. In current data warehouse environments there is either no or only insufficientsupport for a consistent and comprehensive metadata management. Typically; a multitude oflargely autonomous and heterogeneously organized repositories coexist. We categorize themajor metadata types and their interdependencies within a three-dimensional classificationapproach. We then investigate how interoperability and integration of metadata can beachieved based on a federated metadata architecture and standardization efforts such asOIM and CWM. In particular; we examine synchronization alternatives to keep replicatedmetadata consistent. We also give an overview of currently available commercialrepositories and discuss interoperability issues to couple data warehouses with informationportals.,Technischer Report,2000,64,15
Parallel query processing in shared disk database systems,Erhard Rahm,Abstract System developments and research on parallel query processing haveconcentrated either on “Shared Everything” or “Shared Nothing” architectures so far. Whilethere are several commercial DBMS based on the “Shared Disk” alternative; this architecturehas received very little attention with respect to parallel query processing. A comparisonbetween Shared Disk and Shared Nothing reveals many potential benefits for Shared Diskwith respect to parallel query processing. In particular; Shared Disk supports more flexiblecontrol over the communication overhead for intra-transaction parallelism; and a higherpotential for dynamic load balancing and efficient processing of mixed OLTP/queryworkloads. We also sketch necessary extensions for transaction management (concurrency/coherency control; logging/recovery) to support intra-transaction parallelism in the Shared …,ACM SIGMOD Record,1993,63,7
AMC - a framework for modelling and comparing matching systems as matching processes,Eric Peukert; Julian Eberius; Erhard Rahm,We present the Auto Mapping Core (AMC); a new framework that supports fast constructionand tuning of schema matching approaches for specific domains such as ontologyalignment; model matching or database-schema matching. Distinctive features of ourframework are new visualisation techniques for modelling matching processes; stepwisetuning of parameters; intermediate result analysis and performance-oriented rewrites.Furthermore; existing matchers can be plugged into the framework to comparativelyevaluate them in a common environment. This allows deeper analysis of behaviour andshortcomings in existing complex matching systems.,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,62,7
ATOM: Automatic target-driven ontology merging,Salvatore Raunich; Erhard Rahm,The proliferation of ontologies and taxonomies in many domains increasingly demands theintegration of multiple such ontologies to provide a unified view on them. We demonstrate anew automatic approach to merge large taxonomies such as product catalogs or webdirectories. Our approach is based on an equivalence matching between a source andtarget taxonomy to merge them. It is target-driven; ie it preserves the structure of the targettaxonomy as much as possible. Further; we show how the approach can utilize additionalrelationships between source and target concepts to semantically improve the merge result.,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,60,7
Multi-user evaluation of XML data management systems with XMach-1,Timo Böhme; Erhard Rahm,Abstract XMach-1 was the first XML data management benchmark designed for generalapplicability [1]. It is still the only benchmark supporting a multi-user performance evaluationof XML database systems. After a brief review of XMach-1 we summarize three additionallyproposed benchmarks (XMark; XOO7; Mbench) and provide a comparison between thesebenchmarks. We then present experiences and performance results from evaluating XMLdatabase systems with XMach-1.,*,2003,59,11
A Survey of Current Link Discovery Frameworks,Markus Nentwig; Michael Hartung; Axel-Cyrille Ngonga Ngomo; Erhard Rahm,Abstract Links build the backbone of the Linked Data Cloud. With the steady growth in sizeof datasets comes an increased need for end users to know which frameworks to use forderiving links between datasets. In this survey; we comparatively evaluate current LinkDiscovery tools and frameworks. For this purpose; we outline general requirements andderive a generic architecture of Link Discovery frameworks. Based on this genericarchitecture; we study and compare the features of state-of-the-art linking frameworks. Wealso analyze reported performance evaluations for the different frameworks. Finally; wederive insights pertaining to possible future developments in the domain of Link Discovery.,*,2015,57,4
Rule-based dynamic modification of workflows in a medical domain,Robert Müller; Erhard Rahm,Abstract A major limitation of current workflow systems is their lack of supporting dynamicworkflow modifications. However; this functionality is a major requirement for next-generation systems in order to provide sufficient flexibility to cope with unexpected situationsand failures. For example; our experience with data intensive medical domains such ascancer therapy shows that the large number of medical exceptions is hard to manage fordomain experts. We therefore have developed a rule-based approach for partiallyautomated management of semantic exceptions during workflow instance execution. Whenan exception occurs; we automatically determine which running workflow instances wrtwhich workflow regions are affected; and adjust the control flow. Rules are being used todetect semantic exceptions and to decide which activities have to be dropped or added …,Proc. BTW,1999,57,17
A framework for workload allocation in distributed transaction processing systems,Erhard Rahm,Abstract Ever-increasing demands for high transaction rates; limitations of high-endprocessors; high availability; and modular growth considerations are all driving forcestoward distributed architectures for transaction processing. However; a prerequisite to takingadvantage of the capacity of a distributed transaction processing system is an effectivestrategy for workload allocation. The distribution of the workload should not only achieveload balancing; but also support an efficient transaction processing with a minimum ofintersystem communication. To this end; adaptive schemes for transaction routing have to beemployed that are highly responsive to workload fluctuations and configuration changes.Adaptive allocation schemes are also important for simplifying system administration; whichis a major problem in distributed transaction processing systems. In this article we …,Journal of Systems and Software,1992,56,7
Concurrency and coherency control in database sharing systems,Erhard Rahm,Abstract: Database sharing refers to a general architecture for distributed transaction anddatabase processing. The nodes of a database sharing system are locally coupled via ahigh-speed interconnect and share the common database at the disk level (" shared disk").We discuss system functions requiring new and coordinated solutions for database sharing.In particular; the most relevant alternatives for concurrency and coherency control areclassified and surveyed. We consider the techniques used in existing database sharingsystems as well as algorithms proposed in the literature. Furthermore; we summarizeprevious performance studies on database sharing. Related concurrency and coherencycontrol schemes for workstation/server database systems; network file systems; anddistributed shared memory systems are also discussed.,*,1991,56,15
Instance-based matching of large life science ontologies,Toralf Kirsten; Andreas Thor; Erhard Rahm,Abstract Ontologies are heavily used in life sciences so that there is increasing value tomatch different ontologies in order to determine related conceptual categories. We proposea simple yet powerful methodology for instance-based ontology matching which utilizes theassociations between molecular-biological objects and ontologies. The approach can buildon many existing ontology associations for instance objects like sequences and proteinsand thus makes heavy use of available domain knowledge. Furthermore; the approach isflexible and extensible since each instance source with associations to the ontologies ofinterest can contribute to the ontology mapping. We study several approaches to determinethe instance-based similarity of ontology categories. We perform an extensive experimentalevaluation to use protein associations for different species to match between …,Proc.  4th Int. Conf. Data Integration in the Life Sciences,2007,55,12
Developing metadata-intensive applications with Rondo,Sergey Melnik; Erhard Rahm; Philip A Bernstein,Abstract The future of the Semantic Web depends on whether or not we succeed to integratereliably thousands of online applications; services; and databases. These systems are tiedtogether using mediators; mappings; database views; and transformation scripts. Model-management aims at reducing the amount of programming needed for the development ofsuch integrated applications. We present a first complete prototype of a generic model-management system; in which high-level operators are used to manipulate models andmappings between models. We define the key operators and conceptual structures anddescribe their use and implementation. We examine the solutions for three model-management tasks: change propagation; view reuse; and reintegration.,Web Semantics: Science; Services and Agents on the World Wide Web,2003,55,20
Flexible integration of molecular-biological annotation data: The GenMapper approach,Hong-Hai Do; Erhard Rahm,Abstract Molecular-biological annotation data is continuously being collected; curated andmade accessible in numerous public data sources. Integration of this data is a majorchallenge in bioinformatics. We present the GenMapper system that physically integratesheterogeneous annotation data in a flexible way and supports large-scale analysis on theintegrated data. It uses a generic data model to uniformly represent different kinds ofannotations originating from different data sources. Existing associations between objects;which represent valuable biological knowledge; are explicitly utilized to drive dataintegration and combine annotation knowledge from different sources. To serve specificanalysis needs; powerful operators are provided to derive tailored annotation views from thegeneric data representation. GenMapper is operational and has been successfully used …,Advances in Database Technology-EDBT 2004,2004,53,15
Reinforcement learning architecture for web recommendations,Nick Golovin; Erhard Rahm,A large number of Web sites use online recommendations to make Web users interested intheir products or content. Since no single recommendation approach is always best it isnecessary to effectively combine different recommendation algorithms. This paper describesthe architecture of a rule-based recommendation system which combines recommendationsfrom different algorithms in a single recommendation database. Reinforcement learning isapplied to continuously evaluate the users' acceptance of presented recommendations andto adapt the recommendations to reflect the users' interests. We describe the generalarchitecture of the system; the database structure; the learning algorithm and the test settingfor assessing the quality of the approach.,Information Technology: Coding and Computing; 2004. Proceedings. ITCC 2004. International Conference on,2004,52,22
COMA++: Results for the ontology alignment contest OAEI 2006,Sabine Massmann; Daniel Engmann; Erhard Rahm,COMA++: Results for the Ontology Alignment Contest OAEI 2006 Sabine Massmann1; DanielEngmann; Erhard Rahm University of Leipzig 1 massmann@ informatik. uni-leipzig. deAbstract. This paper summarizes the OAEI Contest 2006 results for the matching toolCOMA++. The study shows that a generic schema matching system can also effectively solvecomplex ontology matching tasks. 1 Presentation of the system COMA++ is an extension of ourprevious COMA prototype [1]. It is a customizable and generic tool for matching both schemasand ontologies specified in languages such as SQL; XML Schema or OWL [2]. COMA++ offersa GUI and supports the combined use of several match algorithms as well as the reuse of previouslyconfirmed match results [6]. The COMA++ architecture is shown in figure 1. The Repository persistentlystores all match-related data; the Model and Mapping Pools manage all schemas …,Proceedings of the 1st International Conference on Ontology Matching-Volume 225,2006,50,7
Dealing with logical failures for collaborating workflows,Robert Müller; Erhard Rahm,Abstract Logical failures occurring during workflow execution require the dynamicadaptation of affected workflows. The consequences such a dynamic adaptation may havefor collaborating workflows have not yet been investigated sufficiently. We propose a rule-based approach for dynamic workflow adaptation to deal with logical failures. In ourapproach; workflow collaboration is based on agreements specifying the delivery time andquality of objects a workflow expects from its collaboration partners. Our mechanisms decidewhich collaborating workflows have to be informed when a dynamic adaptation isperformed. In particular; we estimate the temporal and qualitative implications a dynamicadaptation has for collaboration partners. Because of the automated handling of logicalfailures; we expect that our approach significantly improves the robustness and …,International Conference on Cooperative Information Systems,2000,50,1
Data partitioning for parallel entity matching,Toralf Kirsten; Lars Kolb; Michael Hartung; Anika Groß; Hanna Köpcke; Erhard Rahm,Abstract: Entity matching is an important and difficult step for integrating web data. To reducethe typically high execution time for matching we investigate how we can perform entitymatching in parallel on a distributed infrastructure. We propose different strategies topartition the input data and generate multiple match tasks that can be independentlyexecuted. One of our strategies supports both; blocking to reduce the search space formatching and parallel matching to improve efficiency. Special attention is given to thenumber and size of data partitions as they impact the overall communication overhead andmemory requirements of individual match tasks. We have developed a service-baseddistributed infrastructure for the parallel execution of match workflows. We evaluate ourapproach in detail for different match strategies for matching real-world product data of …,arXiv preprint arXiv:1006.5309,2010,49,7
Recovery concepts for data sharing systems,Erhard Rahm,Abstract Data sharing refers to a general distributed architecture for high performancetransaction processing. The nodes of a data sharing system are locally coupled via a high-speed interconnect and can directly access all disks and thus the entire database. Whileconcurrency and coherence control protocols for data sharing have been discussed inprevious work; the important area of recovery has mostly been ignored. This paperdiscusses the new problems for crash and media recovery that have to be addressed in datasharing systems. Recovery is complicated by dependencies on other functions such asbuffer management and concurrency control. Furthermore; a global log file is to beconstructed where the modifications of committed transactions are reflected in chronologicalorder. New logging and recovery protocols are proposed for loosely coupled data sharing …,Proc. Fault-Tolerant Computing; 1991. FTCS-21. Digest of Papers.; 21th  Int. Symposium,1991,49,2
A Self-Configuring Schema Matching System,Eric Peukert; Julian Eberius; Erhard Rahm,Mapping complex metadata structures is crucial in a number of domains such as dataintegration; ontology alignment or model management. To speed up the generation of suchmappings; automatic matching systems were developed to compute mapping suggestionsthat can be corrected by a user. However; constructing and tuning match strategies stillrequires a high manual effort by matching experts as well as correct mappings to evaluategenerated mappings. We therefore propose a self-configuring schema matching system thatis able to automatically adapt to the given mapping problem at hand. Our approach is basedon analyzing the input schemas as well as intermediate matching results. A variety ofmatching rules use the analysis results to automatically construct and adapt an underlyingmatching process for a given match task. We comprehensively evaluate our approach on …,Int. Conf. on Data Engineering,2012,48,15
A clustering-based approach for large-scale ontology matching,Alsayed Algergawy; Sabine Massmann; Erhard Rahm,Abstract Schema and ontology matching have attracted a great deal of interest amongresearchers. Despite the advances achieved; the large matching problem still presents areal challenge; such as it is a time-consuming and memory-intensive process. We thereforepropose a scalable; clustering-based matching approach that breaks up the large matchingproblem into smaller matching problems. In particular; we first introduce a structure-basedclustering approach to partition each schema graph into a set of disjoint subgraphs(clusters). Then; we propose a new measure that efficiently determines similar clustersbetween every two sets of clusters to obtain a set of small matching tasks. Finally; we adoptthe matching prototype COMA++ to solve individual matching tasks and combine theirresults. The experimental analysis reveals that the proposed method permits encouraging …,East European Conference on Advances in Databases and Information Systems,2011,48,7
On matching large life science ontologies in parallel,Anika Gross; Michael Hartung; Toralf Kirsten; Erhard Rahm,Abstract Matching life science ontologies to determine ontology mappings has recentlybecome an active field of research. The large size of existing ontologies and the applicationof complex match strategies for obtaining high quality mappings makes ontology matching aresource-and time-intensive process. To improve performance we investigate differentapproaches for parallel matching on multiple compute nodes. In particular; we consider inter-matcher and intra-matcher parallelism as well as the parallel execution of element-andstructure-level matching. We implemented a distributed infrastructure for parallel ontologymatching and evaluate different approaches for parallel matching of large life scienceontologies in the field of anatomy and molecular biology.,International Conference on Data Integration in the Life Sciences,2010,47,7
Performance evaluation of extended storage architectures for transaction processing,Erhard Rahm,Abstract The use of non-volatile semiconductor memory within an extended storagehierarchy promises significant performance improvements for transaction processing.Although page-addressable semiconductor memories like extended memory; solid-statedisks and disk caches are commercially available since several years; no detailedinvestigation of their use for transaction processing has been performed so far. We present acomprehensive simulation study that compares the performance of these storage types andof different usage forms. The following usage forms are considered: allocation of entire logand database files in non-volatile semiconductor memory; using a so-called write buffer toperform disk writes asynchronously; and caching of database pages at intermediate storagelevels (in addition to main memory caching). Simulation results will be presented for the …,Proc. ACM SIGMOD Conf.,1992,47,0
Data integration support for mashups,Andreas Thor; David Aumueller; Erhard Rahm,*,Proc. Information Integration on the Web (IIWeb),2007,45
Training selection for tuning entity matching,Hanna Köpcke; Erhard Rahm,ABSTRACT Entity matching is a crucial and difficult task for data integration. An effectivesolution strategy typically has to combine several techniques and to find suitable settings forcritical configuration parameters such as similarity thresholds. Supervised (trainingbased)approaches promise to reduce the manual work for determining (learning) effectivestrategies for entity matching. However; they critically depend on training data selectionwhich is a difficult problem that has so far mostly been addressed manually by humanexperts. In this paper we propose a trainingbased framework called STEM for entitymatching and present different generic methods for automatically selecting training data tocombine and configure several matching techniques. We evaluate the proposed methods fordifferent match tasks and small-and medium-sized training sets.,Proc. 6th Int. Workshop Quality in Databases (QDB) and Management of Uncertain Data (MUD); 2008,2008,42,8
Rewrite techniques for performance optimization of schema matching processes,Eric Peukert; Henrike Berthold; Erhard Rahm,Abstract A recurring manual task in data integration; ontology alignment or modelmanagement is finding mappings between complex meta data structures. In order to reducethe manual effort; many matching algorithms for semi-automatically computing mappingswere introduced.,Proc.  13th Int.Conf. on Extending Database Technology (EDBT),2010,41,1
Panel: Is Generic Metadata Management Feasible?,Philip A Bernstein; L Haas; Matthias Jarke; Erhard Rahm; Gio Wiederhold,The database field has worked on metadata-related problems for 30 years. Examplesinclude data translation and migration; schema evolution; database design;schema/ontology integration; XML wrapper generation; data scrubbing and transformationfor data warehouses; message mapping for e-business; and schema-driven web site design.Tools that address these problems are strikingly similar in their design. Arguably; we aremaking very little progress; since we keep reapplying the same old 1970's techniques ofdata translation [9] and views to one new problem after another; without getting muchleverage from each succeeding step. Despite all the research on the above tools; we haveso far been unable to offer generalpurpose database technology that factors out the similaraspects of these tools into generic database infrastructure.,Proc. 26th Int.  Conf.  on Very Large Data Bases,2000,41,0
Primary copy synchronization for DB-sharing,Erhard Rahm,Abstract In a database sharing (DB-Sharing) system multiple loosely or closely coupledprocessors share access to a single set of databases. Such systems promise betteravailability and linear growth of transaction throughput at equivalent response timecompared to single processor database systems. The efficiency of a DB-Sharing systemheavily depends on the synchronization technique used for maintaining consistency of theshared data. A promising algorithm is the primary copy approach which will be presented inthis paper. We describe the actions of the lock manager in a basic and in a more advancedversion. Furthermore; it is shown how the lock managers can be enabled to deal with the so-called buffer invalidation problem that results from the existence of a database buffer in eachprocessor.,Information Systems,1986,40,7
A new distributed optimistic concurrency control method and a comparison of its performance with two-phase locking,Alexander Thomasian; Erhard Rahm,A distributed optimistic concurrency control (OCC) method followed by locking; such thatlocking is an integral part of distributed validation and two-phase commit is presented. ThisOCC method ensures that a transaction failing its validation will not be reexecuted morethan once; in general. Furthermore; deadlocks; which are difficult to handle in a distributedenvironment; are avoided by serializing lock requests. Implementation details are outlined;and the performance of the schemes is compared with distributed two-phase locking (2PL)through a detailed simulation; which incorporates queueing effects at the devices of thecomputer systems; buffer management; concurrency control; and commit processing. It isshown that in the case of higher data contention levels; the hybrid OCC method allows amuch higher maximum transaction throughput than distributed 2PL. The performance of …,Proc. 10th Int. Conf. on Distributed Computing Systems; ICDCS 1990.,1990,36,15
Tailoring entity resolution for matching product offers,Hanna Köpcke; Andreas Thor; Stefan Thomas; Erhard Rahm,Abstract Product matching is a challenging variation of entity resolution to identifyrepresentations and offers referring to the same product. Product matching is highly difficultdue to the broad spectrum of products; many similar but different products; frequentlymissing or wrong values; and the textual nature of product titles and descriptions. Wepropose the use of tailored approaches for product matching based on a preprocessing ofproduct offers to extract and clean new attributes usable for matching. In particular; wepropose a new approach to extract and use so-called product codes to identify products anddistinguish them from similar product variations. We evaluate the effectiveness of theproposed approaches with challenging real-life datasets with product offers from onlineshops. We also show that the UPC information in product offers is often error-prone and …,Proc. 15th Int.Conf. on Extending Database Technology (EDBT),2012,35,1
Mapping Composition for Matching Large Life Science Ontologies.,Anika Gross; Michael Hartung; Toralf Kirsten; Erhard Rahm,Abstract. There is an increasing need to interrelate different life science ontologies in orderto facilitate data integration or semantic data analysis. Ontology matching aims at a largelyautomatic generation of mappings between ontologies mostly by calculating the linguisticand structural similarity of their concepts. In this paper we investigate an indirectcomputation of ontology mappings that composes and thus reuses previously determinedontology mappings that involve intermediate ontologies. The composition approachpromises a fast computation of new mappings with reduced manual effort. Our evaluation forlarge anatomy ontologies shows that composing mappings via intermediate hub ontologiesis not only highly efficient but can also achieve better match quality than with a directmatching of ontologies.,ICBO,2011,35,7
OnEX: Exploring changes in life science ontologies,Michael Hartung; Toralf Kirsten; Anika Gross; Erhard Rahm,Numerous ontologies have recently been developed in life sciences to support a consistentannotation of biological objects; such as genes or proteins. These ontologies underliecontinuous changes which can impact existing annotations. Therefore; it is valuable forusers of ontologies to study the stability of ontologies and to see how many and what kind ofontology changes occurred. We present OnEX (Ontology Evolution EXplorer) a system forexploring ontology changes. Currently; OnEX provides access to about 560 versions of 16well-known life science ontologies. The system is based on a three-tier architectureincluding an ontology version repository; a middleware component and the OnEX webapplication. Interactive workflows allow a systematic and explorative change analysis ofontologies and their concepts as well as the semi-automatic migration of out-dated …,BMC Bioinformatics,2009,35,7
Instance-based matching of hierarchical ontologies.,Andreas Thor; Toralf Kirsten; Erhard Rahm,Abstract: We study an instance-based approach for matching hierarchical ontologies; suchas product catalogs. The motivation for utilizing instances is that metadata-based matchapproaches often suffer from semantic heterogeneity; eg ambiguous concept names;different concept granularities or incomparable categorizations. Our instance-based matchapproach matches categories based on the instances (eg products) assigned to them. Thisway we partly translate the ontology match problem into an instance match problem which isoften easier to solve; especially when instances carry globally unique object ids. Sinceconcepts of different ontologies rarely match 1: 1 we propose to determine correspondencesbetween sets of concepts. We experimentally evaluate the match approaches for realproduct catalogs.,BTW,2007,34,7
GOMMA Results for OAEI 2012,Anika Groß; Michael Hartung; Toralf Kirsten; Erhard Rahm,Abstract. We present the OAEI 2012 evaluation results for the matching system GOMMAdeveloped at the University of Leipzig. The original application focus of GOMMA has beenthe life science domain but as a generic tool it can also match ontologies from other areas. Itcould thus participate in all OAEI tracks running on the SEALS platform. GOMMA supportsseveral methods for efficiently matching large ontologies in particular parallel matching onmultiple cores or machines; reducing the search space as well as reusing and composingprevious mappings to related ontologies.,Proc. ISWC Workshop on Ontology matching (OM),2012,33,7
Warlock: A data allocation tool for parallel warehouses,T Stohr; Erhard Rahm,We present the WARLOCK tool to automatically determine a parallel data warehouse'sallocation to disk. This GUI-equipped tool is implemented in Java and utilizes an internalcost model and heuristics to determine a disk allocation minimizing both I/O work and queryresponse times. WAR-LOCK recommends a ranked list of fragmentation candidates; adetailed query performance analysis and a tailored physical allocation scheme for relationalstar schemas and bitmap indexes. It supports multi-dimensional fragmentations and candeal with data skew for parallel data warehouses based on a Shared Everything or SharedDisk architecture.,Proc. 27th Int. Conf. on Very Large Data Bases,2001,33,7
Adaptive guideline-based treatment workflows with Adaptflow,Ulrike Greiner; Jan Ramsch; Barbara Heller; Markus Löffler; Robert Müller; Erhard Rahm,Abstract. One goal in modern medicine is to increase the treatment quality. A major steptowards this aim is to support the execution of standardized; guideline-based clinicalprotocols; which are used in many medical domains; eg; for oncological chemotherapies.Standardized chemotherapy protocols contain detailed and structured therapy plansdescribing the single therapy steps (eg; examinations or drug applications). Therefore;workflow management systems offer good support for these processes. However; thetreatment of a particular patient often requires modifications due to unexpected infections;toxicities; or social factors. The modifications are described in the treatment protocol but notas part of the standard process. To be able to further execute the therapy workflows in caseof exceptions running workflows have to be adapted dynamically. Furthermore; the …,Studies in health technology and informatics,2004,32,20
Cache management method and apparatus for shared; sequentially-accessed; data,*,Page management mechanisms provide candidates for page stealing and prefetching froma main storage data cache of shared data when the jobs sharing the data are accessing it ina sequential manner. Pages are stolen behind the first reader in the cache; and thereafter atlocations least likely to be soon re-referenced by trailing readers. A" clustering" of readersmay be promoted to reduce I/O contention. Prefetching is carried out so that the pages mostlikely to be soon referenced by one of the readers are brought into the cache.,*,1992,32,7
AdaptFlow: protocol-based medical treatment using adaptive workflows,U Greiner; R Mueller; E Rahm; J Ramsch; B Heller; M Loeffler,Summary Objectives: In many medical domains investigatorinitiated clinical trials are used tointroduce new treatments and hence act as implementations of guidelinebased therapies.Trial protocols contain detailed instructions to conduct the therapy and additionally specifyreactions to exceptional situations (for instance an infection or a toxicity). To increase qualityin health care and raise the number of patients treated according to trial protocols; aconsultation system is needed that supports the handling of the complex trial therapyprocesses efficiently. Our objective was to design and evaluate a consultation system thatshould 1) observe the status of the therapies currently being applied; 2) offer automaticrecognition of exceptional situations and appropriate decision support and 3) provide anautomatic adaptation of affected therapy processes to handle exceptional situations …,Methods of Information in Medicine-Methodik der Information in der Medizin,2005,31,7
Semi-Automatic Adaptation of Mappings between Life Science Ontologies,Anika Groß; Julio Cesar Dos Reis; Michael Hartung; Cédric Pruski; Erhard Rahm,Abstract The continuous evolution of life science ontologies requires the adaptation of theirassociated mappings. We propose two approaches for tackling this problem in a largelyautomatic way:(1) a composition-based adaptation relying on the principle of mappingcomposition and (2) a diff-based adaptation algorithm individually handling changeoperations to update the mapping. Both techniques reuse unaffected correspondences; andadapt only the affected mapping part. We experimentally assess and confirm theeffectiveness of our approaches for evolving mappings between large life scienceontologies.,Proc. DILS (Data Integration in the Life Sciences),2013,30,15
Impact of ontology evolution on functional analyses,Anika Groß; Michael Hartung; Kay Prüfer; Janet Kelso; Erhard Rahm,Abstract Motivation: Ontologies are used in the annotation and analysis of biological data.As knowledge accumulates; ontologies and annotation undergo constant modifications toreflect this new knowledge. These modifications may influence the results of statisticalapplications such as functional enrichment analyses that describe experimental data interms of ontological groupings. Here; we investigate to what degree modifications of theGene Ontology (GO) impact these statistical analyses for both experimental and simulateddata. The analysis is based on new measures for the stability of result sets and considersdifferent ontology and annotation changes. Results: Our results show that past changes inthe GO are non-uniformly distributed over different branches of the ontology. Consideringthe semantic relatedness of significant categories in analysis results allows a more …,Bioinformatics,2012,30,11
Learning-based approaches for matching web data entities,Hanna Köpcke; Andreas Thor; Erhard Rahm,Entity matching is a key task for data integration and especially challenging for Web data.Effective entity matching typically requires combining several match techniques and findingsuitable configuration parameters; such as similarity thresholds. The authors investigate towhat degree machine learning helps semi-automatically determine suitable match strategieswith a limited amount of manual training effort. They use a new framework; Fever; toevaluate several learning-based approaches for matching different sets of Web data entities.In particular; they study different approaches for training-data selection and how muchtraining is needed to find effective combined match strategies and configurations.,Internet Computing; IEEE,2010,29,14
iFuice-Information Fusion utilizing Instance Correspondences and Peer Mappings.,Erhard Rahm; Andreas Thor; David Aumueller; Hong Hai Do; Nick Golovin; Toralf Kirsten,ABSTRACT We present a new approach to information fusion of web data sources. It isbased on peer-to-peer mappings between sources and utilizes correspondences betweentheir instances. Such correspondences are already available between many sources; eg inthe form of web links; and help combine the information about specific objects and support ahigh quality data fusion. Sources and mappings relate to a domain model to support asemantically focused information fusion. The iFuice architecture incorporates a mappingmediator offering both an interactive and a script-driven; workflow-like access to the sourcesand their mappings. The script programmer can use powerful generic operators to executeand manipulate mappings and their results. The paper motivates the new approach andoutlines the architecture and its main components; in particular the domain model; source …,WebDB,2005,29,7
Enriching Ontology Mappings with Semantic Relations,Patrick Arnold; Erhard Rahm,Abstract There is a large number of tools to match or align corresponding concepts betweenontologies. Most tools are restricted to equality correspondences; although many conceptsmay be related differently; eg according to an is-a or part-of relationship. Supporting suchadditional semantic correspondences can greatly improve the expressiveness of ontologymappings and their usefulness for tasks such as ontology merging and ontology evolution.We present a new approach called STROMA (SemanTic Refinement of Ontology MAppings)to determine semantic ontology mappings. In contrast to previous approaches; it follows a so-called enrichment strategy that refines the mappings determined with a state-of-the-artmatch tool. The enrichment strategy employs several techniques including the use ofbackground knowledge and linguistic approaches to identify the additional kinds of …,Data and Knowledge Engineering,2014,28,7
Towards a Benchmark for Ontology Merging,Salvatore Raunich; Erhard Rahm,Abstract Benchmarking approaches for ontology merging is challenging and has receivedlittle attention so far. A key problem is that there is in general no single best solution for amerge task and that merging may either be performed symmetrically or asymmetrically. As afirst step to evaluate the quality of ontology merging solutions we propose the use of generalmetrics such as the relative coverage of the input ontologies; the compactness of the mergeresult as well as the degree of introduced redundancy. We use these metrics to evaluatethree merge approaches for different merge scenarios.,Proc. 7th OTM Workshop on Enterprise Integration; Interoperability and Networking (EI2N'2012),2012,28,15
Comparative evaluation of entity resolution approaches with FEVER,Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract We present FEVER; a new evaluation platform for entity resolution approaches. Themodular structure of the FEVER framework supports the incorporation or reconstruction ofmany previously proposed approaches for entity resolution. A distinctive feature of FEVER isthat it not only evaluates traditional measures such as precision and recall but also the effortfor configuring (eg; parameter tuning; training) a good entity resolution approach. FEVERthus strives for a fair comparative evaluation of different approaches by considering both theeffectiveness and configuration effort.,Proceedings of the VLDB Endowment,2009,28,15
Quality-oriented handling of exceptions in web-service-based cooperative processes,Ulrike Greiner; Erhard Rahm,Abstract: Web services are increasingly used to integrate heterogeneous and autonomousapplications in cross-organizational cooperations. A key problem is to support a highexecution quality of complex cooperative processes; eg in e-business or health care. Oneimportant aspect that has received little attention so far is the dynamic handling ofexceptions during process execution. To address this problem; we propose a rule-basedapproach to automatically control and enforce quality constraints for web-service-basedcooperative processes.,Proc. EAI-Workshop 2004-Enterprise Application Integration,2004,28,15
Performance evaluation of parallel transaction processing in shared nothing database systems,Robert Marek; Erhard Rahm,Abstract Complex and data-intensive database queries mandate parallel processingstrategies to achieve sufficiently short response times. In praxis; parallel databaseprocessing is mostly based on so-called “shared nothing” architectures entailing a physicalpartitioning and allocation of the database among multiple processing nodes. We examinethe performance of such architectures by using a detailed simulation system. We analyseresponse time performance of transactions and individual database queries in single-useras well as in multi-user mode. Furthermore; we study the throughput behavior for on-linetransactions. Three workload types covering a wide range of commercial applications areused for performance evaluation: the debit-credit benchmark load; synthetically generatedrelational queries as well as real-life workloads represented by database traces.,International Conference on Parallel Architectures and Languages Europe,1992,28,7
Target-driven merging of taxonomies with Atom,Salvatore Raunich; Erhard Rahm,Abstract The proliferation of ontologies and taxonomies in many domains increasinglydemands the integration of multiple such ontologies. We propose a new taxonomy mergingalgorithm called A tom that; given as input two taxonomies and a match mapping betweenthem; can generate an integrated taxonomy in a largely automatic manner. The approach istarget-driven; ie we merge a source taxonomy into the target taxonomy and preserve thetarget ontology as much as possible. In contrast to previous approaches; A tom does not aimat fully preserving all input concepts and relationships but strives to reduce the semanticheterogeneity of the merge results for improved understandability. A tom can also exploitadvanced match mappings containing is-a relationships in addition to equivalencerelationships between concepts of the input taxonomies. We evaluate A tom for synthetic …,Information Systems,2014,27,7
Parallel Entity Resolution with Dedoop,Lars Kolb; Erhard Rahm,Abstract We provide an overview of Dedoop (De duplication with Ha doop); a new tool forparallel entity resolution (ER) on cloud infrastructures. Dedoop supports a browser-basedspecification of complex ER strategies and provides a large library of blocking and matchingapproaches. To simplify the configuration of ER strategies with several similarity metrics;training-based machine learning approaches can be employed with Dedoop. Specified ERstrategies are automatically translated into MapReduce jobs for parallel execution ondifferent Hadoop clusters. For improved performance; Dedoop supports redundancy-freemulti-pass blocking as well as advanced load balancing approaches. To illustrate theusefulness of Dedoop; we present the results of a comparative evaluation of different ERstrategies on a challenging real-world dataset.,Datenbank-Spektrum,2013,27,7
Graph-based data integration and business intelligence with BIIIG,André Petermann; Martin Junghanns; Robert Müller; Erhard Rahm,Abstract We demonstrate BIIIG (Business Intelligence with Integrated Instance Graphs); anew system for graph-based data integration and analysis. It aims at improving businessanalytics compared to traditional OLAP approaches by comprehensively trackingrelationships between entities and making them available for analysis. BIIIG supports alargely automatic data integration pipeline for metadata and instance data. Metadata fromheterogeneous sources are integrated in a so-called Unified Metadata Graph (UMG) whileinstance data is combined in a single integrated instance graph (IIG). A unique feature ofBIIIG is the concept of business transaction graphs; which are derived from the IIG and whichreflect all steps involved in a specific business process. Queries and analysis tasks can referto the entire instance graph or sets of business transaction graphs. In the demonstration …,Proceedings of the VLDB Endowment,2014,26,4
Web & Datenbanken,Erhard Rahm; Gottfried Vossen,*,*,2003,26
Use of global extended memory for distributed transaction processing,Erhard Rahm,Abstract In current mainframe architectures; extended memory is used as apageaddressable main memory extension to improve I/O performance. If made non-volatile;extended memory can significantly increase transaction processing performance. In thispaper; we study how locally distributed transaction systems may use shared extendedmemory to support high transaction rates and short response times. We outline how aspecific store called GEM (Global Extended Memory) can be utilized to improve I/Operformance and inter-system communication and cooperation. It turns out that distributedtransaction systems of the “shared disk” type (data sharing systems) can benefit most fromsuch a shared store. We describe simple yet efficient schemes using GEM for globalconcurrency control and constructing a global log file for such systems.,Proceedings of the 4th Int. Workshop on High Performance Transaction Systems; Asilomar; CA,1991,26,15
Mehrrechner-Datenbanksysteme für Transaktionssysteme hoher Leistungsfähigkeit,T Härder; E Rahm,A requirement analysis of transaction systems reveals a number of key properties to beoffered by future high performance database systems: high transaction rates; highavailability; modular growth as well as simple administration and maintenance. Theseproperties determine characteristic architectural features leading to our classificationscheme. Two architectural approaches of multiprocessor database systems-DB-sharing andDB-distribution-seem to be adequate to implement transaction systems satisfying the mostimportant requirements. By comparing and evaluating the general system properties we tryto determine the practical usefulness of both architectural approaches.,Informationstechnik,1986,26,7
BIIIG: Enabling Business Intelligence with Integrated Instance Graphs,André Petermann; Martin Junghanns; Robert Müller; Erhard Rahm,We propose a new graph-based framework for business intelligence called BIIIG supportingthe flexible evaluation of relationships between data instances. It builds on the broadavailability of interconnected objects in existing business information systems. Our approachextracts such interconnected data from multiple sources and integrates them into anintegrated instance graph. To support specific analytic goals; we extract subgraphs from thisintegrated instance graph representing executed business activities with all their data tracesand involved master data. We provide an overview of the BIIIG approach and describe itsmain steps. We also present initial results from an evaluation with real ERP data.,Proc. ICDE workshop on Graph Data Management (GDM),2014,25,7
Don’t Match Twice: Redundancy-free Similarity Computation with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract To improve the effectiveness of pair-wise similarity computation; state-of-the-artapproaches assign objects to multiple overlapping clusters. This introduces redundant paircomparisons when similar objects share more than one cluster. We propose an approachthat eliminates such redundant comparisons and that can be easily integrated into existingMapReduce implementations. We evaluate the approach on a real cloud infrastructure andshow its effectiveness for all degrees of redundancy.,*,2012,25,10
AWESOME: A Data Warehouse–based System for Adaptive Website Recommendations,Andreas Thor; Erhard Rahm,Abstract Recommendations are crucial for the success of large websites. While there aremany ways to determine recommendations; the relative quality of these recommendersdepends on many factors and is largely unknown. We propose a new classification ofrecommenders and comparatively evaluate their relative quality for a sample web-site. Theevaluation is performed with AWESOME (Adaptive website recommendations); a new datawarehouse-based recommendation system capturing and evaluating user feedback onpresented recommendations. Moreover; we show how AWESOME performs an automaticand adaptive closed-loop website optimization by dynamically selecting the most promisingrecommenders based on continuously measured recommendation feedback. We proposeand evaluate several alternatives for dynamic recommender selection including a …,Proceedings of the 30th VLDB Conference. Toronto; Canada,2004,25,1
TID hash joins,Robert Marek; Erhard Rahm,Abstract TID hash joins are a simple and memory-efficient method for processing large joinqueries. They are based on standard hash join algorithms but only store TID/key pairs in thehash table instead of entire ttrples. This typically reduces memory requirements by morethan art order of magnitude bringing substantial benefits. In particular; performance for joinson Gigs-Byte relations can substantially be improved by reducing the amount of disk f/O to alarge extent. Furthermore; efficient processing of mixed multi-user workloads consisting ofboth join queries and OLTP transactions is supported. We present a detailed simulationstudy to analyze the performance of TID hash joins. In particular; we identify the conditionsunder which TID hash joins are most beneficial. Furthermore; we compare TID hash join withadaptive hash join algorithms that have been proposed to deal with mixed workloads.,Proc. 3rd ACM CIKM (Conf. on Information and Knowledge Management),1994,25,7
Comparing the Scientific Impact of p g p Conference and Journal Publications in Computer Science,Erhard Rahm,Page 1. Comparing the Scientific Impact of p g p Conference and Journal Publications inComputer Science Erhard Rahm http://dbs.uni-leipzig.de A di P bli hi i E (APE) C f 2008 Bli Academic Publishing in Europe (APE) Conf.; 2008; Berlin Jan. 23; 2008 Citation analysis ►Citation analysis is increasingly used to measure scientific i f impact of ► Journals (impactfactor) ► Authors ► Authors ► Institutions ► JCR impact factors limited to journals ► JCRimpact factors limited to journals ► Much computer science research is published only inconferences ► Need to consider citations from / to (refereed) conference publications ►Citation analysis is a huge data integration problem ► Need to automate as much as possiblewith good data quality 2 Page 2. MS Libra statistics (Dec 2007) MS Libra statistics (Dec. 2007)http://libra msra cn http://libra.msra.cn # # # it d # # it d …,*,2008,24,15
BioFuice: mapping-based data integration in bioinformatics,Toralf Kirsten; Erhard Rahm,Abstract We introduce the BioFuice approach for integrating data from different private andpublic data sources and ontologies. BioFuice follows a peer-to-peer-like data integrationbased on bidirectional mappings. Sources and mappings are associated with a domainmodel to support a semantically meaningful interoperability. BioFuice extends the genericiFuice integration platform which utilizes specific operators for data fusion and workflow-likescript programs. BioFuice supports explorative data analysis and query and searchcapabilities. We outline the integration approach by an illustrating scenario; the architectureof BioFuice and its query interface.,International Workshop on Data Integration in the Life Sciences,2006,24,7
Analysis of parallel scan processing in shared disk database systems,Erhard Rahm; Thomas Stöhr,Abstract Shared Disk database systems offer a high flexibility for parallel transaction andquery processing. This is because each node can process any transaction; query orsubquery because it has access to the entire database. Compared to Shared Nothingdatabase systems; this is particularly advantageous for scan queries for which the degree ofintra-query parallelism as well as the scan processors themselves can dynamically bechosen. On the other hand; there is the danger of disk contention between subqueries; inparticular for index scans. We present a detailed simulation study to analyze theeffectiveness of parallel scan processing in Shared Disk database systems. In particular; weinvestigate the relationship between the degree of declustering and the degree of scanparallelism for relation scans; clustered index scans; and non-clustered index scans …,European Conference on Parallel Processing,1995,23,10
How do computed ontology mappings evolve?  A case study for life science ontologies,Anika Gross; Michael Hartung; Andreas Thor; Erhard Rahm,Abstract. Mappings between related ontologies are increasingly used to support dataintegration and analysis tasks. Changes in the ontologies also require the adaptation ofontology mappings. So far the evolution of ontology mappings has received little attentionalbeit ontologies change continuously especially in the life sciences. We therefore analyzehow mappings between popular life science ontologies evolve for different matchalgorithms. We also evaluate which semantic ontology changes primarily affect themappings. Our results can be valuable for users working with ontology mappings; eg; onecan learn from past ontology/mapping changes and their correlation to estimate possiblemapping changes if new ontology versions become available.,Proc. ISWC workshop on Knowledge Evolution and Ontology Dynamics,2012,22,10
Design of optimistic methods for concurrency control in database sharing systems,Erhard Rahm,Abstract Database Sharing refers to a local multiprocessor architecture where all processorsshare a common database at the disk level. Transaction systems running in such anenvironment intend to support high transaction rates with short response times as well ashigh availability and modular growth. In order to achieve these goals; Database Sharing (DB-sharing) requires an efficient syn chronization component to control the processors'accesses to the shared database. In this paper; we concentrate ourselves on optimisticmethods for concurrency control in DB-sharing environ ments that promise lesssynchronization messages per transaction than locking algorithms. We describe a newdistributed protocol called broadcast validation where the validations for a transaction aresimultaneously started at multiple processors by a broadcast message. Such a parallel …,Proc. 7th Int. Conf. on Distributed Computing Systems; ICDCS 1987; Berlin,1987,22,4
Semantic Enrichment of Ontology Mappings: A Linguistic-based Approach,Patrick Arnold; Erhard Rahm,Abstract There are numerous approaches to match or align ontologies resulting intomappings specifying semantically corresponding ontology concepts. Most approaches focuson finding equality correspondences between concepts; although many concepts may nothave a strict equality match in other ontologies. We present a new approach to determinemore expressive ontology mappings supporting different kinds of correspondences such asequality; is-a and part-of relationships between ontologies. In contrast to previousapproaches; we follow a so-called enrichment strategy that semantically refines themappings determined with a state-of-the art match tool. The enrichment strategy employsseveral linguistic approaches to identify the additional kinds of correspondences. An initialevaluation shows promising results and confirms the viability of the proposed enrichment …,Proc. ADBIS Conf; LNCS,2013,21,17
Evaluating Instance-based Matching of Web Directories.,Sabine Massmann; Erhard Rahm,ABSTRACT Web directories such as Yahoo or Google Directory semantically categorizemany websites and are heavily used to find relevant websites in a particular domain ofinterest. Mappings between different web directories can be useful to integrate theinformation of different directories and to improve query and search results. The creation ofsuch mappings is a challenging match task due to the large size and heterogeneity of webdirectories. Our study evaluates to what degree current match technology can be used toautomatically determine directory mappings. We further propose specific instance-basedmatch techniques utilizing the URL; name and description of the categorized websites. Weevaluate the instance-based approaches for different similarity measures and study theircombination with metadata-based approaches.,WebDB,2008,21,7
Dynamic fusion of web data,Erhard Rahm; Andreas Thor; David Aumüller,Abstract Mashups exemplify a workflow-like approach to dynamically integrate data andservices from multiple web sources. Such integration workflows can build on existingservices for web search; entity search; database querying; and information extraction andthus complement other data integration approaches. A key challenge is the efficientexecution of integration workflows and their query and matching steps at runtime. We relatemashup data integration with other approaches; list major challenges; and outline featuresof a first prototype design.,Database and XMLTechnologies,2007,21,22
Analyzing extended property graphs with apache flink,Martin Junghanns; André Petermann; Niklas Teichmann; Kevin Gómez; Erhard Rahm,Abstract Graphs are an intuitive way to model complex relationships between real-worlddata objects. Thus; graph analytics plays an important role in research and industry. Asgraphs often reflect heterogeneous domain data; their representation requires an expressivedata model including the abstraction of graph collections; for example; to analyzecommunities inside a social network. Further on; answering complex analytical questionsabout such graphs entails combining multiple analytical operations. To satisfy theserequirements; we propose the Extended Property Graph Model; which is semantically rich;schema-free and supports multiple distinct graphs. Based on this representation; it providesdeclarative and combinable operators to analyze both single graphs and graph collections.Our current implementation is based on the distributed dataflow framework Apache Flink …,Proceedings of the 1st ACM SIGMOD Workshop on Network Data Analytics,2016,20,7
Dynamic query scheduling in parallel data warehouses,Holger Märtens; Erhard Rahm; Thomas Stöhr,Abstract Data warehouse queries pose challenging performance problems that oftennecessitate the use of parallel database systems (PDBS). Although dynamic load balancingis of key importance in PDBS; to our knowledge it has not yet been investigated thoroughlyfor parallel data warehouses. In this study; we propose a scheduling strategy thatsimultaneously considers both processors and disks while utilizing the load balancingpotential of a Shared Disk architecture. We compare the performance of this new method toseveral other approaches in a comprehensive simulation study; incorporating skew aspectsand typical data warehouse features such as star schemas.,*,2002,20,8
Estimating the quality of ontology-based annotations by considering evolutionary changes,Anika Gross; Michael Hartung; Toralf Kirsten; Erhard Rahm,Abstract Ontology-based annotations associate objects; such as genes and proteins; withwell-defined ontology concepts to semantically and uniformly describe object properties.Such annotation mappings are utilized in different applications and analysis studies whoseresults strongly depend on the quality of the used annotations. To study the quality ofannotations we propose a generic evaluation approach considering the annotationgeneration methods (provenance) as well as the evolution of ontologies; object sources; andannotations. Thus; it facilitates the identification of reliable annotations; eg; for use inanalysis applications. We evaluate our approach for functional protein annotations inEnsembl and Swiss-Prot using the Gene Ontology.,International Workshop on Data Integration in the Life Sciences,2009,19,22
Caravela: Semantic Content Management with Automatic Information Integration and Categorization,David Aumüller; Erhard Rahm,*,Proc. ECSW; The Semantic Web: Research and Applications,2007,19
A data warehouse for multidimensional gene expression analysis,Toralf Kirsten; Hong-Hai Do; Erhard Rahm,Abstract We introduce the GeWare data warehouse system for microarraybased geneexpression analysis. GeWare centrally stores expression data together with a variety ofannotations to support different analysis forms. Compared to previous work; our approach isunique in several aspects. First; GeWare offers high flexibility with a multidimensional datamodel where expression data is stored in several fact tables which are associated withmultiple hierarchical dimensions holding describing annotations on genes; samples;experiments; and processing methods. Second; all annotations are integrated and managedin a generic way; thus supporting easy evolution and extensibility. Especially; consistentexperiment annotation is achieved by means of pre-defined annotation templates andcontrolled vocabularies. Finally; various analysis methods have been integrated using a …,*,2004,19,7
Dynamic query scheduling in parallel data warehouses,Holger Märtens; Erhard Rahm; Thomas Stöhr,Abstract Parallel processing is a key to high performance in very large data warehouseapplications that execute complex analytical queries on huge amounts of data. Althoughparallel database systems (PDBSs) have been studied extensively in the past decades; thespecifics of load balancing in parallel data warehouses have not been addressed in detail.,Concurrency and Computation: Practice and Experience,2003,19,2
The scholarly impact of CLEF (2000–2009),Theodora Tsikrika; Birger Larsen; Henning Müller; Stefan Endrullis; Erhard Rahm,Abstract This paper assesses the scholarly impact of the CLEF evaluation campaign byperforming a bibliometric analysis of the citations of the CLEF 2000–2009 proceedingspublications collected through Scopus and Google Scholar. Our analysis indicates asignificant impact of CLEF; particularly for its well-established Adhoc; ImageCLEF; and QAlabs; and for the lab/task overview publications that attract considerable interest. Moreover;initial analysis indicates that the scholarly impact of ImageCLEF is comparable to that ofTRECVid.,International Conference of the Cross-Language Evaluation Forum for European Languages,2013,18,7
When to Reach for the Cloud: Using Parallel Hardware for Link Discovery,Axel-Cyrille Ngonga Ngomo; Lars Kolb; Norman Heino; Michael Hartung; Sören Auer; Erhard Rahm,Abstract With the ever-growing amount of RDF data available across the Web; the discoveryof links between datasets and deduplication of resources within knowledge bases havebecome tasks of crucial importance. Over the last years; several link discovery approacheshave been developed to tackle the runtime and complexity problems that are intrinsic to linkdiscovery. Yet; so far; little attention has been paid to the management of hardwareresources for the execution of link discovery tasks. This paper addresses this research gapby investigating the efficient use of hardware resources for link discovery. We implement theHR^3 approach for three different parallel processing paradigms including the use of GPUsand MapReduce platforms. We also perform a thorough performance comparison for theseimplementations. Our results show that certain tasks that appear to require cloud …,Proc. ESW C,2013,18,15
CODEX: exploration of semantic changes between ontology versions,Michael Hartung; Anika Gross; Erhard Rahm,Abstract Summary: Life science ontologies substantially change over time to meet therequirements of their users and to include the newest domain knowledge. Thus; animportant task is to know what has been modified between two versions of an ontology (diff).This diff should contain all performed changes as compact and understandable as possible.We present CODEX (Co mplex O ntology D iff Ex plorer); a tool that allows determiningsemantic changes between two versions of an ontology; which users can interactivelyanalyze in multiple ways. Availability and implementation: CODEX is available underhttp://www. izbi. de/codex and is supported by all major browsers. It is implemented in Javabased on Google Web Toolkit technology. Additionally; users can access a web serviceinterface to use the diff functionality in their applications and analyses. Contact: hartung …,Bioinformatics,2012,18,7
db-dissertations; Universität Leipzig,Erhard Rahm,*,*,2012,18
Block-based load balancing for entity resolution with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract The effectiveness and scalability of MapReduce-based implementations of complexdata-intensive tasks depend on an even redistribution of data between map and reducetasks. In the presence of skewed data; sophisticated redistribution approaches thus becomenecessary to achieve load balancing among all reduce tasks to be executed in parallel. Forthe complex problem of entity resolution with blocking; we propose BlockSplit; a loadbalancing approach that supports blocking techniques to reduce the search space of entityresolution. The evaluation on a real cloud infrastructure shows the value and effectivenessof the proposed approach.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,18,7
Synchronisation in Mehrrechner-Datenbanksystemen: Konzepte; Realisierungsformen und quantitative Bewertung,Erhard Rahm,*,*,1988,18
Gradoop: Scalable graph data management and analytics with hadoop,Martin Junghanns; André Petermann; Kevin Gómez; Erhard Rahm,Abstract: Many Big Data applications in business and science require the management andanalysis of huge amounts of graph data. Previous approaches for graph analytics such asgraph databases and parallel graph processing systems (eg; Pregel) either lack sufficientscalability or flexibility and expressiveness. We are therefore developing a new end-to-endapproach for graph data management and analysis based on the Hadoop ecosystem; calledGradoop (Graph analytics on Hadoop). Gradoop is designed around the so-called ExtendedProperty Graph Data Model (EPGM) supporting semantically rich; schema-free graph datawithin many distinct graphs. A set of high-level operators is provided for analyzing bothsingle graphs and collections of graphs. Based on these operators; we propose a domain-specific language to define analytical workflows. The Gradoop graph store is currently …,arXiv preprint arXiv:1506.00548,2015,17,1
LinkLion: A Link Repository for the Web of Data,Markus Nentwig; Tommaso Soru; Axel-Cyrille Ngonga Ngomo; Erhard Rahm,Abstract Links between knowledge bases build the backbone of the Web of Data.Consequently; numerous applications have been developed to compute; evaluate and inferlinks. Still; the results of many of these applications remain inaccessible to the tools andframeworks that rely upon it. We address this problem by presenting LinkLion; a repositoryfor links between knowledge bases. Our repository is designed as an open-access andopen-source portal for the management and distribution of link discovery results. Users areempowered to upload links and specify how these were created. Moreover; users andapplications can select and download sets of links via dumps or SPARQL queries. Currently;our portal contains 12.6 million links of 10 different types distributed across 3184 mappingsthat link 449 datasets. In this demo; we will present the repository as well as different …,Proc ESWC,2014,17,4
The GeWare data warehouse platform for the analysis of molecular-biological and clinical data,Erhard Rahm; Toralf Kirsten; Jörg Lange,Abstract We introduce the GeWare data warehouse platform for the integrated analysis ofclinical information; microarray data and annotations within large biomedical researchstudies. Clinical data is obtained from a commercial study management system whilepublicly available data is integrated using a mediator approach. The platform utilizes ageneric approach to manage different types of annotations. We outline the overallarchitecture of the platform; its implementation as well as the main processing and analysisworkflows.,Journal of Integrative Bioinformatics,2007,17,4
Similarity flooding: A versatile graph matching algorithm (extended technical report),Sergey Melnik; Hector Garcia-Molina; Erhard Rahm,Matching elements of two data schemas or two data instances plays a key role in datawarehousing; e-business; or even biochemical applications. In this paper we present amatching algorithm based on a fixpoint computation that is usable across different scenarios.The algorithm takes two graphs (schemas; catalogs; or other data structures) as input; andproduces as output a mapping between corresponding nodes of the graphs. Depending onthe matching goal; a subset of the mapping is chosen using filters. After our algorithm runs;we expect a human to check and if necessary adjust the results. As a matter of fact; weevaluate theaccuracy'of the algorithm by counting the number of needed adjustments. Weconducted a user study; in which our accuracy metric was used to estimate the labor savingsthat the users could obtain by utilizing our algorithm to obtain an initial matching. Finally …,*,2001,17,7
Evaluation of closely coupled systems for high performance database processing,Erhard Rahm,Closely coupled systems aim at a more efficient communication and cooperation betweenprocessing nodes compared to loosely coupled systems. This can be achieved by usingglobally shared semiconductor memory to speed up the exchange of messages or to storeglobal data structures. For distributed database processing; the database sharing (shareddisk) architecture can benefit most from such a close coupling. The author presents adetailed simulation study of closely coupled database sharing systems. A shared storecalled global extended memory (GEM) was used for system-wide concurrency andcoherency control; and to improve input/output (I/O) performance. The performance of suchan architecture is evaluated and compared with loosely coupled database sharing systemsemploying the primary copy approach for concurrency and coherency control. In …,Proc. 13th Int. Conf. on Distributed Computing Systems; ICDCS 1993.,1993,17,10
A clustering-based framework to control block sizes for entity resolution,Jeffrey Fisher; Peter Christen; Qing Wang; Erhard Rahm,Abstract Entity resolution (ER) is a common data cleaning task that involves determiningwhich records from one or more data sets refer to the same real-world entities. Because apairwise comparison of all records scales quadratically with the number of records in thedata sets to be matched; it is common to use blocking or indexing techniques to reduce thenumber of comparisons required. These techniques split the data sets into blocks and onlyrecords within blocks are compared with each other. Most existing blocking techniques donot provide control over the size of the generated blocks; despite this control being importantin many practical applications of ER; such as privacy-preserving record linkage and real-time ER. We propose two novel hierarchical clustering approaches which can generateblocks within a specified size range; and we present a penalty function which allows …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,16,7
Learning-based Entity Resolution with MapReduce,Lars Kolb; Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract Entity resolution is a crucial step for data quality and data integration. Learning-based approaches show high effectiveness at the expense of poor efficiency. To reduce thetypically high execution times; we investigate how learning-based entity resolution can berealized in a cloud infrastructure using MapReduce. We propose and evaluate two efficientMapReduce-based strategies for pair-wise similarity computation and classifier applicationon the Cartesian product of two input sources. Our evaluation is based on real-worlddatasets and shows the high efficiency and effectiveness of the proposed approaches.,Proc. 3rd Int. Workshop on Cloud Data Management,2011,16,20
Comparative evaluation of microarray-based gene expression databases,Hong-Hai Do; Toralf Kirsten; Erhard Rahm,Abstract. Microarrays make it possible to monitor the expression of thousands of genes inparallel thus generating huge amounts of data. So far; several databases have beendeveloped for managing and analyzing this kind of data but the current state of the art in thisfield is still early stage. In this paper; we comprehensively analyze the requirements formicroarray data management. We consider the various kinds of data involved as well asdata preparation; integration and analysis needs. The identified requirements are then usedto comparatively evaluate eight existing microarray databases described in the literature. Inaddition to providing an overview of the current state of the art we identify problems thatshould be addressed in the future to obtain better solutions for managing and analyzingmicroarray data.,Proc. 10th Conf. Database Systems for Business; Technology and Web (BTW),2003,16,17
The Case for Holistic Data Integration,Erhard Rahm,Abstract Current data integration approaches are mostly limited to few data sources; partlydue to the use of binary match approaches between pairs of sources. We thus advocate forthe development of more holistic; clustering-based data integration approaches that scale tomany data sources. We outline different use cases and provide an overview of initialapproaches for holistic schema/ontology integration and entity clustering. The discussionalso considers open data repositories and so-called knowledge graphs.,Proc. ADBIS,2016,15,7
FoodBroker-generating synthetic datasets for graph-based business analytics,André Petermann; Martin Junghanns; Robert Müller; Erhard Rahm,Abstract We present FoodBroker; a new data generator for benchmarking graph-basedbusiness intelligence systems and approaches. It covers two realistic business processesand their involved master and transactional data objects. The interactions are correlated incontrolled ways to enable non-uniform distributions for data and relationships. Forbenchmarking data integration; the generated data is stored in two interrelated databases.The dataset can be arbitrarily scaled and allows comprehensive graph-and pattern-basedanalysis.,Workshop on Big Data Benchmarks,2014,15,15
Rule-based generation of diff evolution mappings between ontology versions,Michael Hartung; Anika Groß; Erhard Rahm,Abstract: Ontologies such as taxonomies; product catalogs or web directories are heavilyused and hence evolve frequently to meet new requirements or to better reflect the currentinstance data of a domain. To effectively manage the evolution of ontologies it is essential toidentify the difference (Diff) between two ontology versions. We propose a novel approach todetermine an expressive and invertible diff evolution mapping between given versions of anontology. Our approach utilizes the result of a match operation to determine an evolutionmapping consisting of a set of basic change operations (insert/update/delete). Tosemantically enrich the evolution mapping we adopt a rule-based approach to transform thebasic change operations into a smaller set of more complex change operations; such asmerge; split; or changes of entire subgraphs. The proposed algorithm is customizable in …,arXiv preprint arXiv:1010.0122,2010,15,15
Discovering evolving regions in life science ontologies,Michael Hartung; Anika Gross; Toralf Kirsten; Erhard Rahm,Abstract Ontologies are heavily used in life sciences and evolve continuously to incorporatenew or changed insights. Often ontology changes affect only specific parts (regions) ofontologies making it valuable for ontology users and applications to know the heavilychanged regions on the one hand and stable regions on the other hand. However; the sizeand complexity of life science ontologies renders manual approaches to localize changingor stable regions impossible. We therefore propose an approach to automatically discoverevolving or stable ontology regions. We evaluate the approach by studying evolving regionsin the Gene Ontology and the NCI Thesaurus.,International Conference on Data Integration in the Life Sciences,2010,15,4
Management and Analysis of Big Graph Data: Current Systems and Open Challenges,Martin Junghanns; André Petermann; Martin Neumann; Erhard Rahm,Abstract Many big data applications in business and science require the management andanalysis of huge amounts of graph data. Suitable systems to manage and to analyze suchgraph data should meet a number of challenging requirements including support for anexpressive graph data model with heterogeneous vertices and edges; powerful query andgraph mining capabilities; ease of use as well as high performance and scalability. In thischapter; we survey current system approaches for management and analysis of “big graphdata”. We discuss graph database systems; distributed graph processing systems such asGoogle Pregel and its variations; and graph dataflow approaches based on Apache Sparkand Flink. We further outline a recent research framework called Gradoop that is build on theso-called Extended Property Graph Data Model with dedicated support for analyzing not …,*,2016,14,7
Privacy Preserving Record Linkage with PPJoin,Ziad Sehili; Lars Kolb; Christian Borgs; Rainer Schnell; Erhard Rahm,Privacy-preserving record linkage (PPRL) becomes increasingly important to match andintegrate records with sensitive data. PPRL not only has to preserve the anonymity of thepersons or entities involved but should also be highly efficient and scalable to largedatasets. We therefore investigate how to adapt PPJoin; one of the fastest approaches forregular record linkage; to PPRL resulting in a new approach called P4Join. The use of bitvectors for PPRL also allows us to devise a parallel execution of P4Join on GPUs. Weevaluate the new approaches and compare their efficiency with a PPRL approach based onmultibit trees.,Proc. BTW Conf,2015,14,7
Extracting Semantic Concept Relations from Wikipedia,Patrick Arnold; Erhard Rahm,Abstract Background knowledge as provided by repositories such as WordNet is of criticalimportance for linking or mapping ontologies and related tasks. Since current repositoriesare quite limited in their scope and currentness; we investigate how to automatically build upimproved repositories by extracting semantic relations (eg; is-a and part-of relations) fromWikipedia articles. Our approach uses a comprehensive set of semantic patterns; finite statemachines and NLP-techniques to process Wikipedia definitions and to identify semanticrelations between concepts. Our approach is able to extract multiple relations from a singleWikipedia article. An evaluation for different domains shows the high quality andeffectiveness of the proposed approach.,Proc. 4th Int. Conf. Web Intelligence; Mining and Semantics (WIMS),2014,14,22
Entity Search Strategies for Mashup Applications,Stefan Endrullis; Andreas Thor; Erhard Rahm,Programmatic data integration approaches such as mashups have become a viableapproach to dynamically integrate web data at runtime. Key data sources for mashupsinclude entity search engines and hidden databases that need to be queried via source-specific search interfaces or web forms. Current mashups are typically restricted to simplequery approaches such as using keyword search. Such approaches may need a highnumber of queries if many objects have to be found. Furthermore; the effectiveness of thequeries may be limited; ie; they may miss relevant results. We therefore propose moreadvanced search strategies that aim at finding a set of entities with high efficiency and higheffectiveness. Our strategies use different kinds of queries that are determined by source-specific query generators. Furthermore; the queries are selected based on the …,Proc ICDE,2012,14,1
An Evolutionbased Approach for Assessing Ontology Mappings-A Case Study in the Life Sciences.,Andreas Thor; Michael Hartung; Anika Gross; Toralf Kirsten; Erhard Rahm,Abstract: Ontology matching has been widely studied. However; the resulting ontologymappings can be rather unstable when the participating ontologies or utilized secondarysources (eg; instance sources; thesauri) evolve. We propose an evolution-based approachfor assessing ontology mappings by annotating their correspondences by information aboutsimilarity values for past ontology versions. These annotations allow us to assess thestability of correspondences over time and they can thus be used to determine better andmore robust ontology mappings. The approach is generic in that it can be appliedindependently from the utilized match technique. We define different stability measures andshow results of a first evaluation for the life science domain.,BTW,2009,14,7
Efficient Management of Biomedical Ontology Versions,Toralf Kirsten; Michael Hartung; Anika Groß; Erhard Rahm,Abstract Ontologies have become very popular in life sciences and other domains. Theymostly undergo continuous changes and new ontology versions are frequently released.However; current analysis studies do not consider the ontology changes reflected in differentversions but typically limit themselves to a specific ontology version which may quicklybecome obsolete. To allow applications easy access to different ontology versions wepropose a central and uniform management of the versions of different biomedicalontologies. The proposed database approach takes concept and structural changes ofsucceeding ontology versions into account thereby supporting different kinds of changeanalysis. Furthermore; it is very space-efficient by avoiding redundant storage of ontologycomponents which remain unchanged in different versions. We evaluate the storage …,On the Move to Meaningful Internet Systems: OTM 2009 Workshops,2009,14,15
Hybrid integration of molecular-biological annotation data,Toralf Kirsten; Hong-Hai Do; Christine Körner; Erhard Rahm,Abstract We present a new approach to integrate annotation data from public sources for theexpression analysis of genes and proteins. Expression data is materialized in a datawarehouse supporting high performance for data-intensive analysis tasks. On the otherhand; annotation data is integrated virtually according to analysis needs. Our virtualintegration utilizes the commercial product SRS (Sequence Retrieval System) of LIONbioscience. To couple the data warehouse and SRS; we implemented a query mediatorexploiting correspondences between molecular-biological objects explicitly captured frompublic data sources. This hybrid integration approach has been implemented for a largegene expression warehouse and supports functional analysis using annotation data fromGeneOntology; Locuslink and Ensembl. The paper motivates the chosen approach …,International Workshop on Data Integration in the Life Sciences,2005,14,7
Effective Mapping Composition for Biomedical Ontologies,Michael Hartung; Anika Gross; Toralf Kirsten; Erhard Rahm,*,Proc. ESWC workshop on  Semantic Interoperability in Medical Informatics  (SIMI),2012,13
Adaptive website recommendations with Awesome,Andreas Thor; Nick Golovin; Erhard Rahm,Abstract Recommendations are crucial for the success of large websites. While there aremany ways to determine recommendations; the relative quality of these recommendersdepends on many factors and is largely unknown. We present the architecture andimplementation of AWESOME (Adaptive website recommendations); a data warehouse-based recommendation system. It allows the coordinated use of a large number ofrecommenders to automatically generate website recommendations. Recommendations aredynamically selected by efficient rule-based approaches utilizing continuously measureduser feedback on presented recommendations. AWESOME supports a completely automaticgeneration and optimization of selection rules to minimize website administration overheadand quickly adapt to changing situations. We propose a classification of recommenders …,The VLDB journal,2005,12,20
Optimistische Synchronisationskonzepte für zentralisierte und verteilte Datenbanksysteme,E Rahm,*,IT Informationstechnik,1988,12
Nah gekoppelte Rechnerarchitekturen für ein DB-Sharing-System,Erhard Rahm,*,Proc. 9. NTG/GI-Fachtagung über Architektur und Betrieb von Rechensystemen; VDE-Verlag; NTG-Fachberichte,1986,12
Verteiltes und paralleles datenmanagement: von verteilten datenbanken zu big data und cloud,Erhard Rahm; Gunter Saake; Kai-Uwe Sattler,Verteilte und parallele Verarbeitung in Datenbanken erlebte eine erste Blüte in den 80er-Jahren; als parallele Verarbeitung und geografisch verteilte Speicherung von relationalenDaten erstmals intensiv untersucht wurden. In dieser Ära erschienen ein ganze Reihe vonfundamentalen Veröffentlichungen und Lehrbüchern; die die Grundlagen für derartigeVerfahren legten. In den darauffolgenden Jahren wurden diese Verfahren genutzt; ohnedass dabei ein erneuter Hype zu beobachten war. Dies änderte sich in den letzten Jahrenmit Konzepten wie der Cloud-Speicherung und der Analyse in Big-Data-Szenarien; die nunmoderne Anforderungen an die Skalierbarkeit mit 20 Jahre altem Lehrbuchwissenkonfrontierten. Auch wenn viele der damaligen Entwicklungen auch nach dieser Zeitunverändert gültig sind; fügen diese neuen Szenarien neue Aspekte hinzu; die damals …,*,2015,11,7
Evaluation of query generators for entity search engines,Stefan Endrullis; Andreas Thor; Erhard Rahm,Abstract: Dynamic web applications such as mashups need efficient access to web data thatis only accessible via entity search engines (eg product or publication search engines).However; most current mashup systems and applications only support simple keywordsearches for retrieving data from search engines. We propose the use of more powerfulsearch strategies building on so-called query generators. For a given set of entities querygenerators are able to automatically determine a set of search queries to retrieve theseentities from an entity search engine. We demonstrate the usefulness of query generators foron-demand web data integration and evaluate the effectiveness and efficiency of querygenerators for a challenging real-world integration scenario. Subjects: Databases (cs. DB);Information Retrieval (cs. IR) ACM classes: H. 3.3; H. 3.4 Cite as: arXiv: 1003.4418 [cs. DB …,arXiv preprint arXiv:1003.4418,2010,11,7
A semantics for model management operators,Sergey Melnik; Philip A Bernstein; Alon Halevy; Erhard Rahm,Abstract Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators; such as Compose; Extract;and Merge; that are applied to models; such as database schemas or interfacespecifications; and to mappings between models. To be used in practice; these operatorsneed to be implemented for particular schema definition languages and mappinglanguages. To guide that implementation; we need a language-independent semantics thattells what the operators should do. In this paper we develop a state-based semantics of theoperators. That is; we express the effect of applying the operators to models in terms of whatthe operators do to instances of these models. We show that our semantics capturespreviouslyproposed desiderata for the operators. We study formal properties of the …,Microsoft Technical Report,2004,11,7
Hochleistungs-Transaktionssysteme,Erhard Rahm,Transaktionssysteme sind in der kommerziellen Datenverarbeitung weitverbreitet und vonenormer tikonomischer Bedeutung. Die Leistungsanforderungen vieler Anwendungensteigen sehr stark und ktinnen nur von sogenannten Hochleistungs-Transaktionssystemenerfiillt werden. Dies betriffi insbesondere die zu bewaltigenden Transaktionsraten fUrvorgeplante Anwendungsfunktionen. Daneben sind auch zunehmend ungeplante Ad-Hoc-Anfragen auf derselben Datenbank auszuftihren; welche oft den Zugriff auf groBeDatenmengen verlangen und ftir die die Gewahrleistung kurzer Antwortzeiten einHauptproblem darstellt. Weitere Anforderungen an Hochleistungs-Transaktionssystemebetreffen die Gewahrleistung einer hohen Verfiigbarkeit; die Mtiglichkeit eines modularenWachstums sowie die Untersttitzung einer hohen Kosteneffektivitat. Ftir die Erzielung …,Konzepte und Entwicklungen moderner Datenbankarchitekturen. Vieweg,1993,11,7
Hochleistungsdatenbanksysteme - Vergleich und Bewertung aktueller Architekturen und ihrer Implementierung,T Härder; E Rahm,*,Informationstechnik it,1987,11
Quantitative Analyse eines Synchronisationsalgorithmus für DB-Sharing,Theo Härder; Erhard Rahm,Abstract Als DB-Sharing bezeichnet man die gemeinsame Benutzung einer Datenbankdurch Datenbank-Verwaltungssysteme auf einem System lose gekoppelter Prozessoren.Das wesentliche Entwurfsziel solcher Mehrrechner-Datenbanksysteme ist die annäherndlineare Erhöhung des Durchsatzes im Transaktionsbetrieb bei nur wenig verändertenAntwortzeiten im Vergleich zum 1-Rechner-Fall. Weiterhin wird eine deutliche Verbesserungder Verfügbarkeit des Systems für die DB-Verarbeitung angestrebt. Nach einerBeschreibung des Aufbaus und der funktionellen Komponenten eines DBSharing-Systemswerden seine speziellen Charakteristika für den Transaktionsbetrieb diskutiert. Darausergeben sich die wesentlichen Aspekte; die bei der Modellbildung eines Mehrrechner-Datenbanksystems zu berücksichtigen sind. Für ein DB-Sharing mit zwei Rechnern wird …,*,1985,11,7
Benchmarking XML Database Systems–First Experiences,Timo Böhme; Erhard Rahm,We recently developed and published a scaleable multi-user benchmark called XMach-1(XML Data Management benchmark) for evaluating the performance of XML datamanagement systems [1]. To our knowledge it is the first such benchmark. It aims atrealistically evaluating the performance of individual systems as well as to allow for aperformance comparison between different systems and architectures ranging from nativeXML data management systems to XML-enabled relational DBMS. Specifying andimplementing the benchmark revealed a number of problems which are partly due to thelack of a standardized XML query language; the complexity of the XML format and therelative immaturity of current XML database software. After a brief review of XMach-1 we willdiscuss our experiences made so far.,Ninth International Workshop on High Performance Transaction Systems (HPTS); Pacific Grove; California,2001,10,7
collected student  theses / seminar write-ups,Database group leipzig,*,*,2000,10
Algorithmen zur effizienten Lastkontrolle in Mehrrechner-Datenbanksystemen,Erhard Rahm,Zusammenfassung: ln einem Mehrrechner-Datenbanksystem ist die Lastkontrolle für dieVerteilung der aktuellen Transaktionslast auf die verfügbaren Rechner verantwortlich. Essind dabei kurze Antwortzeiten einerseits sawie eine gleichmäßige Systemaus/astung undhoher Durchsatz andererseits anzustreben. Nach einer Einführung wird die Funktion undRealisierung der Lastkontrolle in einem Mehrrechner-Datenbanksystem diskutiert. ImHauptteil der Arbeit werden mehrere Verfahren zur Erstellung einer sogenannten Routing-Tabelle vorgestellt; mit deren Hilfe die Transaktionslast dynamisch verteilt wird.,Angewandte Informatik,1986,10,15
Optimizing Similarity Computations for Ontology Matching-Experiences from GOMMA,Michael Hartung; Lars Kolb; Anika Groß; Erhard Rahm,Abstract An efficient computation of ontology mappings requires optimized algorithms andsignificant computing resources especially for large life science ontologies. We describehow we optimized n-gram matching for computing the similarity of concept names andsynonyms in our match system GOMMA. Furthermore; we outline how to enable a highlyparallel string matching on Graphical Processing Units (GPU). The evaluation on the OAEILargeBio match task demonstrates the high effectiveness of the proposed optimizations andthat the use of GPUs in addition to standard processors enables significant performanceimprovements.,Proc. DILS (Data Integration in the Life Sciences),2013,9,7
Affiliation Analysis of Database Publications,David Aumueller; Erhard Rahm,Abstract We analyze the author affiliations of database publications to determine the maininstitutions contributing research results in our field. We consider the publications of the lastdecade (2000'2009) that appeared in the top conferences SIGMOD and VLDB and in theVLDBJ and TODS journals. We determine the top affiliations in terms of number of papersand aggregate the numbers at the levels of entire countries and continents. Further; weanalyze to which degree authors from different affiliations and countries cooperate on jointlyauthored papers; and study the development over time. We also consider the number andsize of affiliations of different countries.,SIGMOD Record-Special Interest Group on Management Data,2011,9,7
Database and XML Technologies: Proc. First International XML Database Symposium; XSym 2003,Zohra Bellahsène; A. Chaudri; Erhard Rahm; Michael Rys; Rainer Unland,*,*,2003,9
Sysplex-Cluster-Technologien für Hochleistungs-Datenbanken,Wilhelm G.  Spruth; Erhard Rahm,Transaktions-und Datenbankanwendungen in modernen Großunternehmen erfordernRechenleistungen; die von einem einzelnen Rechner nicht mehr erbracht werden können.Kommerzielle Großrechner werden deshalb seit einiger Zeit im Rahmen von Clusterneingesetzt. Internetanwendungen und Web-Services verstärken diesen Trend; weil derAbstand zwischen Durchschnitts-und Spitzenbelastung ständig größer wird. Cluster leidenallgemein unter Skalierungsschwierigkeiten; insbesondere aufgrund vonSynchronisationsproblemen beim Zugriff auf gemeinsam genutzte Daten. Der IBM OS/390-bzw. z/OS-Sysplex verfügt über eine Reihe neuartiger technologischer Ansätze; die eineproblemlose Skalierung bis zu über einhundert CPUs ermöglichen. In dem vorliegendenBeitrag werden einige dieser Technologien erläutert. Großrechner; besonders auch …,Datenbank-Spektrum,2002,9,15
Cache management for shared sequential data access,Erhard Rahm; Donald Ferguson,Abstract This paper presents a new set of cache management algorithms for shared dataobjects that are accessed sequentially. I/O delays on sequentially accessed data is adominant performance factor in many application domains; in particular for batchprocessing. Our algorithms fall into three classes: replacement; prefetching and schedulingstrategies. Our replacement algorithms empirically estimate the rate at which the jobs areproceeding through the data. These velocity estimates are used to project the next referencetimes for cached data objects and our algorithms replace data with the longest time to re-use. The second type of algorithm performs asynchronous prefetching. This algorithm usesthe velocity estimations to predict future cache misses and attempts to preload data to avoidthese misses. Finally; we present a simple job scheduling strategy that increases locality …,Information Systems,1993,9,3
Extended memory support for high performance transaction systems,Volker Bohn; Theo Härder; Erhard Rahm,Abstract To achieve high performance transaction processing vertical as well as horizontalsystem growth is considered. A prime obstacle for vertical growth is the unfavorable ratio ofI/O time vs. CPU time making it increasingly difficult to utilize fast CPUs and multiprocessors.Prerequisites for horizontal growth are a low communication overhead and effective loadbalancing; both subgoals are the more difficult to meet the more systems to be utilized. Wepropose the use of a fast and non-volatile extended memory which provides synchronousaccess for closely coupled systems. We discuss its properties supporting high volumetransaction processing. Subsequently; we investigate its performance behavior forcentralized and distributed computing environments. Simulation results are presented forsynthetic Debit-Credit transactions as well as for real-life workloads represented by …,Proc. MMB,1990,9,7
Der Database-Sharing-Ansatz zur Realisierung von Hochleistungs-Transaktionssystemen,Erhard Rahm,Zusammenfassung. Database Sharing (DB-Sharing) bezeichnet eine allgemeine Klassevon Mehrrechner-Datenbanksystemen; bei der jeder Rechner direkten Zugriff auf diegemeinsamen Datenbestände hat. Hinsichtlich der Realisierung von Hochleistungs-Transaktionssystemen bieten DB-Sharing~ Systeme signifikante Vorteile gegenüberMehrrechner-Datenbanksystemen; bei denen eine statische Datenpartitionierungvorzunehmen ist. Dieser Aufsatz diskutiert diejenigen Systemkomponenten; für die bei DB-Sharing zur Nutzung des hohen Leistungspotentials neue und koordinierteLösungskonzepte erforderlich sind. Insbesondere hinsichtlich der Synchronisation sowieder Behandlung von Pufferinvalidierungen werden die vielversprechendstenRealisierungsformen überblickartig vorgestellt und bewertet. Außer Aspekten der …,Informatik Spektrum,1989,9,1
Empirical performance evaluation of concurrency and coherency control protocols for data sharing,Erhard Rahm,*,*,1988,9
Distributed Grouping of Property Graphs with GRADOOP,Martin Junghanns; André Petermann; Erhard Rahm,Property graphs are an intuitive way to model; analyze and visualize complex relationshipsamong heterogeneous data objects; for example; as they occur in social; biological andinformation networks. These graphs typically contain thousands or millions of vertices andedges and their entire representation can easily overwhelm an analyst. One way to reducecomplexity is the grouping of vertices and edges to summary graphs. In this paper; wepresent an algorithm for graph grouping with support for attribute aggregation and structuralsummarization by user-defined vertex and edge properties. The algorithm is part of G; anopen-source system for graph analytics. G is implemented on top of Apache Flink; a state-of-the-art distributed dataflow framework; and thus allows us to scale graph analyticalprograms across multiple machines. Our evaluation demonstrates the scalability of the …,Proc. BTW,2017,8,22
Privacy-Preserving Record Linkage for Big Data: Current Approaches and Research Challenges,Dinusha Vatsalan; Ziad Sehili; Peter Christen; Erhard Rahm,Abstract The growth of Big Data; especially personal data dispersed in multiple datasources; presents enormous opportunities and insights for businesses to explore andleverage the value of linked and integrated data. However; privacy concerns impede sharingor exchanging data for linkage across different organizations. Privacy-preserving recordlinkage (PPRL) aims to address this problem by identifying and linking records thatcorrespond to the same real-world entity across several data sources held by differentparties without revealing any sensitive information about these entities. PPRL is increasinglybeing required in many real-world application areas. Examples range from public healthsurveillance to crime and fraud detection; and national security. PPRL for Big Data posesseveral challenges; with the three major ones being (1) scalability to multiple large …,*,2016,8,0
Iterative Computation of Connected Graph Components with MapReduce,Lars Kolb; Ziad Sehili; Erhard Rahm,Abstract The use of the MapReduce framework for iterative graph algorithms is challenging.To achieve high performance it is critical to limit the amount of intermediate results as well asthe number of necessary iterations. We address these issues for the important problem offinding connected components in large graphs. We analyze an existing MapReducealgorithm; CC-MR; and present techniques to improve its performance including a memory-based connection of subgraphs in the map phase. Our evaluation with several large graphdatasets shows that the improvements can substantially reduce the amount of generateddata by up to a factor of 8.8 and runtime by up to factor of 3.5.,Datenbank-Spektrum,2014,8,1
Introduction to the special issue on data quality,Mourad Ouzzani; Paolo Papotti; Erhard Rahm,Abstract Poor data quality in databases; data warehouses; and information systems affectsevery application domain. Many data processing tasks; such as information integration; datasharing; information retrieval; information extraction; and knowledge discovery requirevarious forms of data preparation and consolidation with complex data processingtechniques. These tasks usually assume that the data input contains no missing;inconsistent or incorrect values. This leaves a large gap between the available “dirty” dataand the machinery to effectively process the data for the application purposes. In addition;tasks such as data integration and information extraction may themselves introduce errors inthe data.,Information Systems,2013,8,22
Composition Methods for Link Discovery,Michael Hartung; Anika Groß; Erhard Rahm,Abstract: The Linked Open Data community publishes an increasing number of data sourceson the so-called Data Web and interlinks them to support data integration applications. Weinvestigate how the composition of existing links and mappings can help discovering newlinks and mappings between LOD sources. Often there will be many alternatives forcomposition so that the problem arises which paths can provide the best linking results withthe least computation effort. We therefore investigate different methods to select andcombine the most suitable mapping paths. We also propose an approach for selecting andcomposing individual links instead of entire mappings. We comparatively evaluate themethods on several real-world linking problems from the LOD cloud. The results show thehigh value of reusing and composing existing links as well as the high effectiveness of …,Proc. Databases for Business; Technology and Web (BTW),2013,8,4
StudentischeArbeiten-Abteilung Datenbanken,E. Rahm; et al (eds),*,*,2009,8
Web usage mining,Erhart Rahm,*,*,2002,8
Goal-oriented Performance Control for Transaction Processing.,Erhard Rahm,Abstract The performance of current transaction processing systems largely depends onhuman experts for administration and tuning. These experts have to specify a multitude ofinternal control parameters in different subsystems for which finding appropriate settings isvery difficult. Another shortcoming of manual system administration lies in its inability toquickly react to changing system and workload conditions. In order to overcome theselimitations we advocate for an automatic and adaptive performance control. To simplify theadministration we pursue a" goaloriented" approach that aims at automatically enforcingexternal performance objectives; in particular response time goals. The effectiveimplementation of such a scheme poses a multitude of largely unsolved challenges. Ourapproach is based on a feedback loop for automatic detection and correction of …,MMB,1997,8,7
Graph Mining for Complex Data Analytics,André Petermann; Martin Junghanns; Stephan Kemper; Kevin Gómez; Niklas Teichmann; Erhard Rahm,Complex data analytics that involve data mining often comprise not only a single algorithmbut also further data processing steps; for example; to restrict the search space or to filter theresult. We demonstrate graph mining with Gradoop; the first scalable system supportingdeclarative analytical programs composed from multiple graph operations. We use abusiness intelligence example including frequent subgraph mining to highlight the analyticalcapabilities enabled by such programs. The results can be visualized and; to show its easeof use; the program can be modified on visitors request. Gradoop is built on top of state-of-the-art big data technology and out-of-the-box horizontally scalable. Its source code ispublicly available and designed for easy extensibility. We offer to the graph miningcommunity; to apply Gradoop in large scale use cases and to contribute further algorithms.,Proc ICDM,2016,7,8
Automatic extraction of semantic relations from wikipedia,Patrick Arnold; Erhard Rahm,We introduce a novel approach to extract semantic relations (eg; is-a and part-of relations)from Wikipedia articles. These relations are used to build up a large and up-to-datethesaurus providing background knowledge for tasks such as determining semanticontology mappings. Our automatic approach uses a comprehensive set of semanticpatterns; finite state machines and NLP techniques to extract millions of relations betweenconcepts. An evaluation for different domains shows the high quality and effectiveness of theproposed approach. We also illustrate the value of the newly found relations for improvingexisting ontology mappings.,International Journal on Artificial Intelligence Tools,2015,7,12
WETSUIT: An efficient mashup tool for searching and fusing web entities,Stefan Endrullis; Andreas Thor; Erhard Rahm,Abstract We demonstrate a new powerful mashup tool called WETSUIT (Web EnTity Searchand fUsIon Tool) to search and integrate web data from diverse sources and domain-specificentity search engines. WETSUIT supports adaptive search strategies to query sets ofrelevant entities with a minimum of communication overhead. Mashups can be composedusing a set of high-level operators based on the Java-compatible language Scala. Theoperator implementation supports a high degree of parallel processing; in particular astreaming of entities between all data transformation operations facilitating a fastpresentation of intermediate results. WETSUIT has already been applied to solvechallenging integration tasks from different domains.,PVLDB,2012,7,11
An integrated platform for analyzing molecular-biological data within clinical studies,Toralf Kirsten; Jörg Lange; Erhard Rahm,Abstract To investigate molecular-biological causes and effects of diseases and theirtherapies it becomes increasingly important to combine data from clinical trials with highvolumes of experimental genetic data and annotations. We present our approach tointegrate such data for two large collaborative cancer research studies in Germany. Ourplatform interconnects a commercial study management system (eRN) with a datawarehouse-based gene expression analysis system (GeWare). We utilize a genericapproach to import different anonymized pathological and patient-related annotations intothe warehouse. The platform also integrates different forms of experimental data and publicmolecular-biological annotation data and thus supports a wide range of genetic analyses forboth clinical and non-clinical parameters.,International Conference on Extending Database Technology,2006,7,15
WEBFLOW: Ein System zur flexiblen Ausführung webbasierter; kooperativer Workflows,Ulrike Greiner; Erhard Rahm,Page 1. WebFlow: Ein System zur flexiblen Ausführung webbasierter; kooperativer WorkflowsUlrike Greiner; Erhard Rahm Universität Leipzig http://dbs.uni-leipzig.de gefördert durch: Page2. 2 Motivation • Geschäftsprozesse sind vermehrt kooperativ zwischen Organisationseinheitenabzuwickeln • Web-Anbindung und Web Services stehen zur Verfügung Realisierung kooperativerWorkflows wird erleichtert • Beispiel: Bearbeitung eines Bauantrags • weitere Beispiele:Patientenbehandlung; Konstruktion von Fahrzeugen / Produkten Erfassung Stammdaten Planungs-und bauord- nungsrechtliche Prüfung Bearbeitung durch interne und externe FachinstanzenGenehmigung Behörden / Dienststellen Regierungs- präsidium Sachverständige für Statik;Umweltschutz; Brandschutz o.ä. . . . . Page 3. 3 Herausforderungen Kennzeichen kooperativerWorkflows: Aktivitäten werden durch Dienste unterschiedlicher …,Proc. BTW Conf.,2003,7,7
Datenbanksysteme,Erhard Rahm,*,Vorlesungsskript,1999,7
On the performance of parallel join processing in shared nothing database systems,Robert Marek; Erhard Rahm,Abstract Parallel database systems aim at providing high throughput for OLTP transactionsas well as short response times for complex and data-intensive queries. Shared nothingsystems represent the major architecture for parallel database processing. While theperformance of such systems has been extensively analyzed in the past; the correspondingstudies have made a number of best-case assumptions. In particular; almost all performancestudies on parallel query processing assumed single-user mode; ie; that the entire system isexclusively reserved for processing a single query. We study the performance of parallel joinprocessing under more realistic conditions; in particular for multi-user mode. Experimentsconducted with a detailed simulation model of shared nothing systems demonstrate theneed for dynamic load balancing strategies for efficient join processing in multi-user …,International Conference on Parallel Architectures and Languages Europe,1993,7,7
Distributed optimistic concurrency control for high performance transaction processing,Erhard Rahm; Alexander Thomasian,A novel optimistic concurrency control (OCC) protocol for distributed high-performancetransaction systems is presented. Unlike other proposals for OCC in distributed systems; thisscheme limits the number of restarts by acquiring locks to guarantee a failed transaction asuccessful second execution. Lock acquisition and validation are imbedded in the commitprotocol in order to avoid any extra messages. Deadlocks are avoided by requesting alllocks at once before performing validation. The protocol is fully distributed and employsparallel validation and lock acquisition. A main advantage compared with distributed lockingschemes is that locks are held only during commit processing in general; thus considerablyreducing the degree of lock contention. As first simulation results have confirmed; this is ofparticular benefit for high-performance transaction-processing complexes with fast …,Databases; Parallel Architectures and Their Applications;. PARBASE-90; International Conference on,1990,7,15
Goal-oriented workload management in locally distributed transaction systems,Erhard Rahm; D Ferguson; L Georgiadis; C Nikolaou; GW Su; M Swanson; G Wang,*,IBM Research Report RC,1989,7
Cypher-based graph pattern matching in Gradoop,Martin Junghanns; Max Kießling; Alex Averbuch; André Petermann; Erhard Rahm,Abstract Graph pattern matching is an important and challenging operation on graph data.Typical use cases are related to graph analytics. Since analysts are often non-programmers;a graph system will only gain acceptance; if there is a comprehensible way to declarepattern matching queries. However; respective query languages are currently onlysupported by graph databases but not by distributed graph processing systems. To enablepattern matching on a large scale; we implemented the declarative graph query languageCypher within the distributed graph analysis platform Gradoop. Using LDBC graph data; weshow that our query engine is scalable for operational as well as analytical workloads. Theimplementation is open-source and easy to extend for further research.,Proceedings of the Fifth International Workshop on Graph Data-management Experiences & Systems,2017,6,23
Holistic entity clustering for linked data,Markus Nentwig; Anika Groß; Erhard Rahm,Pairwise link discovery approaches for the Web of Data do not scale to many sourcesthereby limiting the potential for data integration. We thus propose a holistic approach forlinking many data sources based on a clustering of entities representing the same real-worldobject. Our clustering approach utilizes existing links and can deal with entities of differentsemantic types. The approach is able to identify errors in existing links and can findnumerous additional links. An initial evaluation on real-world linked data shows theeffectiveness of the proposed holistic entity matching.,Data Mining Workshops (ICDMW); 2016 IEEE 16th International Conference on,2016,6,8
Annotating medical forms using UMLS,Victor Christen; Anika Groß; Julian Varghese; Martin Dugas; Erhard Rahm,Abstract Medical forms are frequently used to document patient data or to collect relevantdata for clinical trials. It is crucial to harmonize medical forms in order to improveinteroperability and data integration between medical applications. Here we propose a(semi-) automatic annotation of medical forms with concepts of the Unified MedicalLanguage System (UMLS). Our annotation workflow encompasses a novel semanticblocking; sophisticated match techniques and post-processing steps to select reasonableannotations. We evaluate our methods based on reference mappings between medicalforms and UMLS; and further manually validate the recommended annotations.,International Conference on Data Integration in the Life Sciences,2015,6,7
A multi-part matching strategy for mapping LOINC with laboratory terminologies,Li-Hui Lee; Anika Groß; Michael Hartung; Der-Ming Liou; Erhard Rahm,Abstract Objective To address the problem of mapping local laboratory terminologies toLogical Observation Identifiers Names and Codes (LOINC). To study different ontologymatching algorithms and investigate how the probability of term combinations in LOINChelps to increase match quality and reduce manual effort. Materials and methods Weproposed two matching strategies: full name and multi-part. The multi-part approach alsoconsiders the occurrence probability of combined concept parts. It can further recommendpossible combinations of concept parts to allow more local terms to be mapped. Three real-world laboratory databases from Taiwanese hospitals were used to validate the proposedstrategies with respect to different quality measures and execution run time. A comparisonwith the commonly used tool; Regenstrief LOINC Mapping Assistant (RELMA) Lab Auto …,Journal of the American Medical Informatics Association,2013,6,20
Cloudfuice: A flexible cloud-based data integration system,Andreas Thor; Erhard Rahm,Abstract The advent of cloud computing technologies shows great promise for webengineering and facilitates the development of flexible; distributed; and scalable webapplications. Data integration can notably benefit from cloud computing because integratingweb data is usually an expensive task. This paper introduces CloudFuice; a data integrationsystem that follows a mashup-like specification of advanced dataflows for data integration.CloudFuice's task-based execution approach allows for an efficient; asynchronous; andparallel execution of dataflows in the cloud and utilizes recent cloud-based web engineeringinstruments. We demonstrate and evaluate CloudFuice's applicability for mashup-baseddata integration in the cloud with the help of a first prototype implementation.,International Conference on Web Engineering,2011,6,7
Parallel Sorted Neighborhood Blocking with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract: Cloud infrastructures enable the efficient parallel execution of data-intensive taskssuch as entity resolution on large datasets. We investigate challenges and possiblesolutions of using the MapReduce programming model for parallel entity resolution. Inparticular; we propose and evaluate two MapReduce-based implementations for SortedNeighborhood blocking that either use multiple MapReduce jobs or apply a tailored datareplication. Subjects: Distributed; Parallel; and Cluster Computing (cs. DC) Cite as: arXiv:1010.3053 [cs. DC](or arXiv: 1010.3053 v1 [cs. DC] for this version) Submission historyFrom: Lars Kolb [view email][v1] Fri; 15 Oct 2010 00: 28: 44 GMT (268kb; D),arXiv preprint arXiv:1010.3053,2010,6,10
A platform for collaborative management of semantic grid metadata,Michael Hartung; Frank Loebe; Heinrich Herre; Erhard Rahm,Summary Grid environments; providing distributed infrastructures; computing resources anddata storage; usually show a high degree of heterogeneity in their metadata. We propose aplatform for collaborative management and maintenance of common metadata for grids. Asthe conceptual foundation of this platform; a meta model is presented which distinguishesstructured descriptions and classification structures. On this basis; the system allows for theuser-friendly creation and editing of grid relevant metadata and provides various search andnavigation facilities for grid participants. We applied the platform to the German D-Gridinitiative by establishing the D-Grid Ontology (DGO).,*,2008,6,11
Data integration in the life sciences,Erhard Rahm,*,*,2004,6
Web; Web-services; and database systems: NODe 2002 Web-and database-related workshops; Erfurt; Germany; October 7-10; 2002: revised papers,Akmal B Chaudhri; M. Jeckle; Erhard Rahm; Rainer Unland,This book constitutes the thoroughly refereed post-proceedings of the Web-and Database-Related Workshops held during the NetObjectDays international conference NODe 2002; inErfurt; Germany; in October 2002. The 19 revised full papers presented together with 3keynote papers were carefully selected during 2 rounds of reviewing and improvement. Thepapers are organized in topical sections on advanced Web-services; UDDI extensions;description and classification of Web services; applications based on Web-services;indexing and accessing; Web and XML databases; mobile devices and the Internet; andXML query languages.,*,2003,6,11
On flexible allocation of index and temporary data in parallel database systems,Erhard Rahm; Holger Märtens; Thomas Stöhr,Data placement is a key factor for high performance database systems. This is particularlytrue for parallel database systems where data allocation must support both I/O parallelismand processing parallelism within complex queries and between independent queries andtransactions. Determining an effective data placement is a complex administration problemdepending on many parameters including system architecture; database and workloadcharacteristics; hardware configuration; etc. Research and tool support has so farconcentrated on data placement for base tables; especially for Shared Nothing (SN); eg[MD97]. On the other hand; to our knowledge; data placement issues for architectures wheremultiple DBMS instances share access to the same disks (Shared Disk; Shared Everything;specific hybrid architectures) have not yet been investigated in a systematic way …,Proc. 8th HPTS Workshop; Asilomar,1999,6,7
HematoWork: A Knowledge-based Workflow System for Distributed Cancer Therapy,R Müller; B Heller; M Löffler; E Rahm; A Winter; L Mantovani; M Klöss; H Berger; F Brümmer; F Fiebig; E Jödecke; U Neubert; R Speer,1 Objectives The domain of hemato-oncology is characterized by a complex and data-intensive treatment and the involvement of geographically distributed institutions (egoncological ward; central commission; external panels) in the context of protocol-directedtrials. Current research efforts in this domain (eg [1-3]) focus on specialized subtasks suchas chemotherapy calculation and toxicity monitoring; but fail to support inter-application dataflow and coordination aspects which have been identified as essential for integration inheterogeneous and distributed clinical environments (eg [4; 5]). Therefore; at LeipzigUniversity; the distributed workflow system HEMATOWORK; which has explicit knowledgeabout the oncological treatment and the associated communication paths between theinvolved institutions; is currently developed. In particular; HEMATOWORK intends to …,Proc. GMDS98,1998,6,15
A Reliable and Efficient Synchronization Protocol for Database Sharing Systems,Erhard Rahm,Abstract Database sharing (DB-sharing) refers to a loosely or closely coupledmultiprocessor architecture where all processors share a common database at the disklevel. Such systems primarily aim at high availability and high performance demanded bylarge applications in online transaction processing. To achieve these goals asynchronization technique is required that efficiently coordinates the processors' databaseaccesses and that also works properly after a processor crash. The described primary copyalgorithm seems to be a good candidate to meet these requirements since it permits flexibleadaption to changing working conditions. Besides of the basic protocol we specify therecovery actions after a processor crash. Mechanisms are proposed that allow to continuesynchronization after a crash with little interference to transaction processing …,Proc. 3rd Conf.  Fehlertolerierende Systeme,1987,6,10
Concurrency Control in DB-Sharing Systems,Erhard Rahm,Abstract In a database sharing (DB-Sharing) system multiple loosely or closely coupledprocessors share access to a single set of databases. Such systems primarily aim at highavailability and high performance demanded by large transaction processing systems. Toachieve high transaction rates with short response times an efficient concurrency control isrequired for synchronizing accesses to the shared database. This paper gives an overviewof conceivable concurrency control algorithms for DB-Sharing. We distinguish betweenlocking and optimistic methods and between centralized and distributed solutions. Fivesynchronization protocols are described in some detail and compared with each other.,Proc. 16. GI annual conf.; Berlin; Springer-Verlag,1986,6,15
Comparative Evaluation of Distributed Clustering Schemes for Multi-source Entity Resolution,Alieh Saeedi; Eric Peukert; Erhard Rahm,Abstract Entity resolution identifies semantically equivalent entities; eg; describing the sameproduct or customer. It is especially challenging for big data applications where largevolumes of data from many sources have to be matched and integrated. Entity resolution formultiple data sources is best addressed by clustering schemes that group all matchingentities within clusters. While there are many possible clustering schemes for entityresolution; their relative suitability and scalability is still unclear. We therefore implementedand comparatively evaluate distributed versions of six clustering schemes based on ApacheFlink within a new entity resolution framework called Famer. Our evaluation for different real-life and synthetically generated datasets considers both the match quality as well as thescalability for different number of machines and data sizes.,Proc. ADBIS,2017,5,22
SemRep–A Repository for Semantic Mapping,Patrick Arnold; Erhard Rahm,In schema and ontology matching; background knowledge such as dictionaries and thesaurican considerably improve the mapping quality. Such knowledge resources are especiallyvaluable to determine the semantic relation type (eg; equal; is-a or part-of) that holdsbetween related concepts. Previous match tools mostly use WordNet as their primaryresource for background knowledge; although WordNet provides only a limited coverageand currentness. We present the design and use of a new comprehensive repository calledSemRep that combines concepts and semantic relationships from different resources. Itintegrates both manually developed resources (including WordNet) and semi-automaticallyextracted relations from Wikipedia. To determine the semantic relationship between twoconcepts of interest; SemRep also considers indirect relationships of possibly different …,Proc. BTW. Conf.,2015,5,7
PDFMeat: Managing publications on the semantic desktop.,David Aumüller; Erhard Rahm,Abstract Researchers maintain bibliographies and extensive sets of PDF files of scholarlypublications on their desktop. The lack of proper metadata of downloaded PDFs makes thistask a tedious one. With PDFMeat we present a solution to automatically determinepublication metadata for scholarly papers within the user's desktop environment and link themetadata to the files. PDFMeat effectively matches local full texts to an online repository. Inan evaluation for more than 2.000 diverse PDF files it worked highly reliable and showedexcellent accuracy of up to 98 percent. We demonstrate PDFMeat for different sets of papers;highlighting the semantic integration and use of the retrieved metadata within the filebrowser of the desktop environment.,Proc 20th  CIKM,2011,5,7
Management of evolving semantic grid metadata within a collaborative platform,Michael Hartung; Frank Loebe; Heinrich Herre; Erhard Rahm,Abstract Grid environments; providing distributed infrastructures; computing resources anddata storage; usually show a high degree of heterogeneity and change in their metadata.We propose a platform for collaborative management and maintenance of commonmetadata for grids. As the conceptual foundation of this platform; a meta model is presentedwhich distinguishes structured descriptions and classification structures that both aremodifiable. On this basis; the system allows for the creation and editing of grid relevantmetadata and provides various search and navigation facilities for grid participants. Weapplied the platform to the German D-Grid initiative by establishing the D-Grid Ontology(DGO).,Information Sciences,2010,5,1
Web-based Affiliation Matching.,David Aumueller; Erhard Rahm,Abstract: Authors of scholarly publications state their affiliation in various forms. This kind ofheterogeneity makes bibliographic analysis tasks on institutions impossible unless acomprehensive cleaning and consolidation of affiliation data is performed. We investigateautomatic approaches to consolidate affiliation data to reduce manual work and supportscalability of affiliation analysis. In particular; we propose to set up a reference database ofaffiliation strings found in publications. A key step in this task is the matching of differentaffiliation strings to determine whether or not they match. For affiliation matching weinvestigate web based similarity measures utilizing the cognitive power of current searchengines. They determine the similarity of affiliations based on how the URLs in the resultsets of affiliation web searches overlap. We evaluate the effectiveness of affiliation …,ICIQ,2009,5,5
A grid middleware for ontology access,Michael Hartung; Erhard Rahm,Zusammenfassung Many advanced grid applications need access to ontologies represent-ing knowledge about a certain application domain. To deal with the high heterogeneity ofavailable ontologies; we propose a general ser-vice-oriented middleware for makingontologies accessible to grid ap-plications. Our implementation is integrated in the GermanD-Grid in-frastructure and provides several applications a uniform access to biomedicalontologies such as Gene Ontology; NCI Thesaurus and several OBO ontologies.,German e-Science Conference,2007,5,7
Automatic optimization of web recommendations using feedback and ontology graphs,Nick Golovin; Erhard Rahm,Abstract Web recommendation systems have become a popular means to improve theusability of web sites. This paper describes the architecture of a rule-based recommendationsystem and presents its evaluation on two real-life applications. The architecture combinesrecommendations from different algorithms in a recommendation database and appliesfeedback-based machine learning to optimize the selection of the presentedrecommendations. The recommendations database also stores ontology graphs; which areused to semantically enrich the recommendations. We describe the general architecture ofthe system and the test setting; illustrate the application of several optimization approachesand present comparative results.,International Conference on Web Engineering,2005,5,1
DBMS-Leistungsvergleich mit TPC-Benchmarks,E Rahm,*,IX-Magazin; Mai,1993,5
Zugriffspfad-Unterstützung zur Sicherung der Relationalen Invarianten,Theo Härder; Erhard Rahm,Das Relationenmodell garantiert seinen Benutzern Integritätszusicherungen fürEntitätsintegrität und Referentielle Integrität. Datenbanksysteme; die dem SQL2-Standardgenügen wollen; müssen die Einhaltung dieser fundamentalen Integritätsregeln; dersogenannten Relationalen Invarianten; gewährleisten. Das bedeutet; daß bei denentsprechenden Modifikationsoperationen Primärschlüssel-und Fremdschlüsselbedingungexplizit zu überprüfen sind. In diesem Aufsatz werden für Schlüssel; die aus einfachenAttributen bestehen; Verfahren untersucht; um diese Integritätsprüfungen effizient abwickelnzu können. Dabei werden Zugriffspfade betrachtet; die sich durch B*-Bäume und ihreVarianten implementieren lassen. Von besonderer Wichtigkeit wurde die Synchronisationauf solchen Indexstrukturen angesehen; da sich dort Behinderungen von parallelen …,*,1992,5,7
Trends in distributed and cooperative database management,Klaus Küspert; Erhard Rahm,In the past; database management systems (DBMS) coutd roughly be subdivided into twomajor classes which one might call PC/workstation DBMS and (centralized) mainframeDBMS. PC/workstation DBMS provide a set of data management functions in a stand-alonefashion as a single-user system on a personal computer or workstation. There is no real dataexchange between a PC/workstation DBMS and other data management systems; remoteusers; or DBMS at another place. Some PC/workstation DBMS provide limited functions toextract data as a" snapshot" from a remote database to process these data locally on thePC/workstation; there is; however; usually no way to propagate changed data back from thePC/workstation to the remote database at the end. Moreover; PC/workstation DBMS are notable to provide an integrated view of the data in a large organization.,*,1990,5,7
Evolution of biomedical ontologies and mappings: Overview of recent approaches,Anika Groß; Cédric Pruski; Erhard Rahm,Abstract Biomedical ontologies are heavily used to annotate data; and different ontologiesare often interlinked by ontology mappings. These ontology-based mappings andannotations are used in many applications and analysis tasks. Since biomedical ontologiesare continuously updated dependent artifacts can become outdated and need to undergoevolution as well. Hence there is a need for largely automated approaches to keep ontology-based mappings up-to-date in the presence of evolving ontologies. In this article; we surveycurrent approaches and novel directions in the context of ontology and mapping evolution.We will discuss requirements for mapping adaptation and provide a comprehensiveoverview on existing approaches. We will further identify open challenges and outline ideasfor future developments.,Computational and structural biotechnology journal,2016,4,15
Speeding up privacy preserving record linkage for metric space similarity measures,Ziad Sehili; Erhard Rahm,Abstract The analysis of person-related data in Big Data applications faces the tradeoff offinding useful results while preserving a high degree of privacy. This is especiallychallenging when person-related data from multiple sources need to be integrated andanalyzed. Privacy-preserving record linkage (PPRL) addresses this problem by encodingsensitive attribute values such that the identification of persons is prevented but records canstill be matched. In this paper we study how to improve the efficiency and scalability of PPRLby restricting the search space for matching encoded records. We focus on similaritymeasures for metric spaces and investigate the use of M‑trees as well as pivot-basedsolutions. Our evaluation shows that the new schemes outperform previous filter approachesby an order of magnitude.,Datenbank-Spektrum,2016,4,7
Leveraging the Impact of Ontology Evolution on Semantic Annotations,Silvio Domingos Cardoso; Cédric Pruski; Marcos Da Silveira; Ying-Chi Lin; Anika Groß; Erhard Rahm; Chantal Reynaud-Delaître,Abstract This paper deals with the problem of maintenance of semantic annotationsproduced based on domain ontologies. Many annotated texts have been produced andmade available to end-users. If not reviewed regularly; the quality of these annotations tendsto decrease over time due to the evolution of the domain ontologies. The quality of theseannotations is critical for tools that exploit them (eg; search engines and decision supportsystems) and need to ensure an acceptable level of performance. Although the recentadvances for ontology-based annotation systems to annotate new documents; themaintenance of existing annotations remains under studied. In this work we present ananalysis of the impact of ontology evolution on existing annotations. To do so; we used twowell-known annotators to generate more than 66 million annotations from a pre-selected …,Proc. EKAW,2016,4,1
Scalable privacy-preserving linking of multiple databases using counting Bloom filters,Dinusha Vatsalan; Peter Christen; Erhard Rahm,The integration; mining; and analysis of person-specific data can provide enormousopportunities for organizations; governments; and researchers to leverage today's massivedata collections. However; the use of personal or otherwise sensitive data also raisesconcerns about the privacy; confidentiality; and potential discrimination of people. Privacy-preserving record linkage (PPRL) is a growing research area that aims at integratingsensitive information from multiple disparate databases held by different organizations whilepreserving the privacy of the individuals in these databases by not revealing their identitiesand thereby preventing re-identification and discrimination. PPRL approaches areincreasingly required in many real-world application areas ranging from healthcare tonational security. Previous approaches to PPRL have mostly focused on linking only two …,Proc. ICDM Workshops,2016,4,20
Discovering product counterfeits in online shops: a big data integration challenge,ERHARD RAHM,Counterfeit products are illegal imitations or replicas of products offered for sale. TheInternational Chamber of Commerce (www. icc-ccs. org/icc/cib) estimated in 2011 thatcounterfeiting accounted for about 5–7% of world trade and an estimated $600 billion ayear. In parallel to the increasing volume of online sales; the volume of counterfeiting in Websales has also increased. Fake products are offered and sold in numerous online shops andauction sites as well as on B2B marketplaces for wholesale trading. Almost all kinds ofproducts are subject to counterfeiting; ranging from; say; electronic devices and apparel tofood and drugs [Heinonen et al. 2012]. Counterfeits not only cause enormous economic lossbut can also damage the reputation of a brand. Buyers of fake products not only receive low-quality product in many cases; but may even be exposed to serious safety and health …,ACM Journal on Data and Information Quality,2014,4,7
Rule-based construction of matching processes,Eric Peukert; Julian Eberius; Erhard Rahm,Abstract Semi-automatic schema matching systems have been developed to computemapping suggestions that can be corrected by a user. However; constructing and tuningmatch strategies still requires a high manual effort. We therefore propose a self-configuringschema matching system that is able to automatically adapt to the given mapping problem athand. Our approach is based on analyzing the input schemas as well as intermediate matchresults. A variety of matching rules use the analysis results to automatically construct andadapt an underlying matching process for a given match task. The evaluation shows that oursystem is able to robustly return good quality mappings across different mapping problemsand domains.,Proc.  20th ACM CIKM (Conf. on Information and Knowledge Management),2011,4,23
Web-basiertes SQL-Training im Bildungsportal Sachsen.,D Sosna; E Rahm,Kurzfassung: Der Betrag skizziert die Konzeption und Realisierung eines interaktiven; web-basierten SQL-Trainers. Der SQL-Trainer wurde im Rahmen des Verbundprojekts„Bildungsportal Sachsen “entwickelt und ist unter der URL http://sql-trainer. uni-leipzig. demit einem Gast-Login zum Testen erreichbar.,BTW-Workshop Datenbanken und E-Learning,2003,4,1
Evaluierung von Data Warehouse-Werkzeugen,Hong Hai Do; Thomas Stöhr; Erhard Rahm; Robert Müller; Gernot Dern,Zusammenfassung Die wachsende Bedeutung von Data Warehouse-Lösungen zurEntscheidungsunterstützung in großen Unternehmen hat zu einer unüberschaubarenVielfalt von Software-Produkten geführt. Aktuelle Data Warehouse-Projekte zeigen; daß derErfolg auch von der Wahl der passenden Werkzeuge für diese komplexe undkostenintensive Umgebung abhängt. Wir präsentieren eine Methode zur Evaluierung vonData Warehouse Tools; die eine Kombination aus Bewertung per Kriterienkatalog unddetaillierten praktischen Tests umfaßt. Die Vorgehensweise ist im Rahmen von Projekten mitIndustriepartnern erprobt und wird am Beispiel einer Evaluierung führender ETL-Werkzeugedemonstriert.,*,2000,4,4
Evaluation of object-relational database systems for fulltext retrieval,Erhard Rahm,Abstract Object-relational database systems add object-oriented features to relational DBMSand allow the DBMS's functionality to be extended to new application domains. For theimportant domain of fulltext retrieval and document management; we analyze whethercurrent object-relational DBMS are already able to compete with specialized informationretrieval (IR) systems. After discussing the main requirements; we present a comparison ofthe ORDBMS Informix and several text data blades with the information retrieval systemFulcrum and a relational DBMS extended by IR functionality (Oracle ConText). A qualitativeand quantitative evaluation is presented considering the degree of IR functionality;performance; and retrieval quality.,Proc. WITS,1998,4,7
Architekturansätze zur Unterstützung heterogener Datenbanken,Karl-Ludwig Butsch; Erhard Rahm,Zusammenfassung Die ständig zunehmende Verbreitung von Datenbanken verlangt einegeeignete Unterstützung eines koordinierten Zugriffs auf heterogen strukturierteDatenbestände. Trotz der Notwendigkeit einer Kooperation beim Zugriff auf mehrereDatenbanken soll die Autonomie der beteiligten Datenbanken weitgehend erhalten bleiben.Verteilte Datenbanksysteme bieten hierfür keinen geeigneten Ansatz; da sie Homogenitätund eine enge Zusammenarbeit der Datenbankverwaltungssysteme verlangen. Wirklassifizieren für den Zugriff auf heterogene Datenbanken besser geeigneteSystemarchitekturen und stellen ihre kennzeichnenden Eigenschaften heraus.Insbesondere vergleichen wir verschiedene Arten von verteilten DC-Systemen sowieföderativen Mehrrechner-Datenbanksystemen. Als Vetreter zweier wichtiger …,*,1992,4,4
A reuse-based annotation approach for medical documents,Victor Christen; Anika Groß; Erhard Rahm,Abstract Annotations are useful to semantically enrich documents and other datasets withconcepts of standardized vocabularies and ontologies. In the medical domain; manydocuments are not annotated at all and manual annotation is a difficult process makingautomatic annotation methods highly desirable to support human annotators. We propose areuse-based annotation approach that utilizes previous annotations to annotate similarmedical documents. The approach clusters items in documents such as medical formsaccording to previous ontology-based annotations and uses these clusters to determinecandidate annotations for new items. The final annotations are selected according to a newcontext-based strategy that considers the co-occurrence and semantic relatedness ofannotating concepts. The evaluation based on previous UMLS annotations of medical …,International Semantic Web Conference,2016,3,15
ScaDS Dresden/Leipzig: Ein serviceorientiertes Kompetenzzentrum für Big Data.,Erhard Rahm; Wolfgang E Nagel,Das BMBF hat im Rahmen seiner Förderinitiative zu Big Data die Einrichtung von zwei Kompetenzzentrenin Deutschland angekündigt; ScaDS Dresden/Leipzig und BBDC (Berlin Big Data Center). Das"Competence Center for Scalable Data Services and Solu- tions Dresden/Leipzig" (ScaDSDresden/Leipzig) (www.scads.de) bündelt die Metho- denkompetenz der TU Dresden und derUniv. Leipzig zur ganzheitlichen Adressierung von Big Data Herausforderungen in unterschiedlichenwissenschaftlichen und wirt- schaftlichen Anwendungsbereichen. Profilbestimmende Forschungsschwerpunkteliegen in den Gebieten der Datenintegration; der Wissensextraktion sowie der visuellenAnaly- se. Anforderungen der Datensicherheit und des Datenschutzes werden umfassendbe- rücksichtigt und adressiert … In einem serviceorientierten; modularen Ansatz werden inScaDS Dresden/Leipzig An- wendungen nach Gemeinsamkeiten und Unterschieden im …,GI-Jahrestagung,2014,3,2
Restricting the overlap of Top-N sets in schema matching,Eric Peukert; Erhard Rahm,Abstract Computing similarities between metadata elements is an essential process inschema and ontology matching systems. Such systems aim at reducing the manual effort offinding mappings for data integration or ontology alignment. Similarity measures computesyntactic; semantic or structural similarities of metadata elements. Typically; differentsimilarities are combined and the most similar element pairs are selected to produce a best-1 mapping suggestion. Unfortunately automatic schema matching systems are only rarelycommercially adopted since correcting the best-1mapping suggestion is often harder thanfinding the mapping manually. To alleviate this; schema matching must be usedincrementally by computing Top-N mapping suggestions that the user can select from.However; current similarity measures and selection operators suggest the same target …,Proc.  1st Workshop on New Trends in Similarity Search,2011,3,10
Data-Warehouse-Einsatz zur Web-Zugriffsanalyse,Erhard Rahm; Thomas Stöhr,Kurzfassung Die Analyse des Nutzungsverhaltens von Websites ermöglicht wichtigeHinweise zur Optimierung und Weiterentwicklung eines Web-Auftritts. Skalierbarkeit undFlexibilität der Auswertungen verlangen oft eine datenbankbasierte Realisierung. Wirdiskutieren hierzu verschiedene Varianten; insbesondere den Einsatz eines» Web DataWarehouse «; in dem neben den Web-Log-Daten Informationen zu Nutzern/Kunden;Inhalten/Produkten und Anwendungsfunktionen integriert werden. Weiterhin geben wireinen Überblick zu derzeit verfügbaren Werkzeugen für die Web-Zugriffsanalyse.,*,2003,3,7
OLAP-Auswertung von Web-Zugriffen.,Thomas Stöhr; Erhard Rahm; Stephan Quitzsch,• Die besuchten Web-Adressen werden üblicherweise in Form ihrer Directory-/Dateinamenabgespeichert. Die Herstellung eines inhaltlichen Bezuges und damit die semantischeAussagekraft und Auswertbarkeit solcher Dateizugriffspfade ist naturgemäß starkeingeschränkt. Erschwert wird die Analyse zusätzlich durch die üblicherweise sehr großeAnzahl von zu verwaltenden Web-Seiten.• In der Regel ist keine oder nur sehr umständlicheVerknüpfung mit weiteren Datenquellen; wie zB demoskopischen Daten; möglich.,GI-Workshop Internet-Datenbanken,2000,3,15
Kohärenzkontrolle in verteilten Systemen,Erhard Rahm,Im nächsten Kapitel präzisieren wir zunächst die Aufgabenstellung der Kohärenzkontrolleund geben einen Überblick über die wichtigsten Lösungsansätze. Die angesprochenenLösungsmöglichkeiten sind dabei in mehreren (bzw. allen) der angesprochenen Bereicheeinsetzbar; wenngleich sie teilweise unabhängig voneinander (unter verschiedenenBezeichnungen) vorgeschlagen wurden. In Kap. 3 und 4 gehen wir dann näher auf dieeinzelnen DBS-bzw. BS-Bereiche ein; in denen Kohärenzprotokolle untersucht werden; umBesonderheiten bzw. Unterschiede aufzuzeigen.,Datenbank Rundbrief,1993,3,4
Klassifikation und Vergleich verteilter Transaktionssysteme,Erhard Rahm,Transaktionssysteme [11] befinden sich in weit verbreitetem Einsatz in der kommerziellenDatenverarbeitung; insbesondere im Rahmen von Auskunfts-und Buchungssystemen sowiezur aktenlosen Sachbearbeitung; zB bei Behörden oder Versicherungen. KennzeichnendeEigenschaften solcher Systeme sind ua die dialogorientierte Verarbeitung von meisteinfachen Transaktionen (Bsp.: Kontenbuchung) sowie die Bereitstellung einer hohen(maskenorientierten) Endbenutzer-Schnittstelle. Hauptkomponenten einesTransaktionssystemes sind das Datenbanksystem (DBS); bestehend aus Datenbank undDatenbankverwaltungssystem (DBVS); sowie ein DC-System; bestehend aus einem TP-Monitor sowie der Menge der Transaktions-oder Anwendungsprogramme. Der TP-Monitorkontrolliert die Ausführung der Programme und realisiert die Kommunikation von …,Datenbank Rundbrief,1991,3,4
Utilization of Extended Storage Architectures for High-Volume Transaction Processing,Erhard Rahm; Kaiserslautern Univ.(Germany). Zentrum Ingenieursysteme (ZRI),*,ZRI-Bericht,1990,3
Performance Analysis of Primary Copy Synchronization in Database Sharing Systems,Erhard Rahm,*,*,1987,3
Integrated solutions to concurrency control and buffer lnvalidation in database sharing systems,Erhard Rahm,*,Proc.  2nd IEEE Int.  Conf. on Computers and Applications,1987,3
Buffer invalidation problem in DB-sharing systems,Erhard Rahm,*,*,1986,3
Klassifikation von Mehrrechner-Datenbanksystemen: Anforderungen; Entwurfsprinzipien; Realisierungskonzepte.,T Härder; E Rahm,*,*,1985,3
Big Data Analytics,Erhard Rahm,Big Data has become a core topic in different industries and research disciplines as well asfor society as a whole. This is because the ability to generate; collect; distribute; process andanalyze unprecedented amounts of diverse data has almost universal utility and helps tofundamentally change the way industries operate; how research can be done and howpeople live and use modern technology. Different industries such as automotive; finance;healthcare or manufacturing; can dramatically benefit from improved and faster dataanalysis; eg; as illustrated by current industry trends like “Industry 4.0” and “Internet ofThings”. Data-driven research approaches utilizing Big Data technology and analysisbecome increasingly commonplace; eg; in the life sciences; geo sciences or in astronomy.Users utilizing smartphones; social media; and web resources spend increasing amounts …,it-Information Technology,2016,2,15
How do Ontology Mappings Change in the Life Sciences?,Anika Gross; Michael Hartung; Andreas Thor; Erhard Rahm,Abstract: Mappings between related ontologies are increasingly used to support dataintegration and analysis tasks. Changes in the ontologies also require the adaptation ofontology mappings. So far the evolution of ontology mappings has received little attentionalbeit ontologies change continuously especially in the life sciences. We therefore analyzehow mappings between popular life science ontologies evolve for different matchalgorithms. We also evaluate which semantic ontology changes primarily affect themappings. We further investigate alternatives to predict or estimate the degree of futuremapping changes based on previous ontology and mapping transitions.,arXiv preprint arXiv:1204.2731,2012,2,2
Analyse von Zitierungshäufigkeiten für die Datenbankkonferenz BTW,Hanna Koepcke; Erhard Rahm,*,Datenbank-Spektrum,2007,2
Functional Profiling of Genes Differently Expressed in the Brains of Humans and Chimpanzees,B Mützel; HH Do; P Khaitovich; G. Weiß; E. Rahm; S. Pääbo,*,Proc 2nd Biotechnology Day,2003,2
On parallel join processing in object-relational database systems,Holger Märtens; Erhard Rahm,Abstract So far only few performance studies on parallel object-relational database systemsare available. In particular; the relative performance of relational vs. reference-based joinprocessing in a parallel environment has not been inves-tigated sufficiently. We present aperformance study based on the BUCKY bench-mark to compare parallel join processingusing reference attributes with relational hash-and merge-join algorithms. In addition; wepropose a data allo-cation scheme especially suited for object hierarchies and set-valuedattributes.,Datenbanksysteme in Büro; Technik und Wissenschaft: 9. GI-Fachtagung Oldenburg; 7.-9. März 2001,2001,2,7
Evaluierung und Produktvorschlag für ein technisches Metadaten-Management im R+ V-Data Warehouse,R Müller; T Stöhr; E Rahm; S Quitzsch,*,Projektbericht,1998,2
Parallele Datenbanksysteme,Erhard Rahm,*,Informix Vision,1995,2
Quantitative Analyse eines Synchronisationsprotokolls für Mehrrechner-Datenbanksysteme,Erhard Rahm,*,*,1984,2
Using Link Features for Entity Clustering in Knowledge Graphs,Alieh Saeedi; Eric Peukert; Erhard Rahm,Abstract. Knowledge graphs holistically integrate information about entities from multiplesources. A key step in the construction and maintenance of knowledge graphs is theclustering of equivalent entities from different sources. Previous approaches for such anentity clustering suffer from several problems; eg; the creation of overlapping clusters or theinclusion of several entities from the same source within clusters. We therefore propose anew entity clustering algorithm CLIP that can be applied both to create entity clusters and torepair entity clusters determined with another clustering scheme. In contrast to previousapproaches; CLIP not only uses the similarity between entities for clustering but also furtherfeatures of entity links such as the so-called link strength. To achieve a good scalability weprovide a parallel implementation of CLIP based on Apache Flink. Our evaluation for …,submitted for publication,2018,1,15
Distributed holistic clustering on linked data,Markus Nentwig; Anika Groß; Maximilian Möller; Erhard Rahm,Abstract Link discovery is an active field of research to support data integration in the Web ofData. Due to the huge size and number of available data sources; efficient and effective linkdiscovery is a very challenging task. Common pairwise link discovery approaches do notscale to many sources with very large entity sets. We propose a distributed holistic approachto link many data sources based on a clustering of entities that represent the same real-world object. Our approach provides a compact and fused representation of entities; and canidentify errors in existing links as well as many new links. We support distributed execution;show scalability for large real-world data sets and evaluate our methods with respect toeffectiveness and efficiency for two domains.,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2017,1,22
The Big Picture: Understanding large-scale graphs using Graph Grouping with GRADOOP,Martin Junghanns; André Petermann; Niklas Teichmann; Erhard Rahm,Graph grouping supports data analysts in decision making based on the characteristics oflarge-scale; heterogeneous networks containing millions or even billions of vertices andedges. We demonstrate graph grouping with G; a scalable system supporting declarativeprograms composed from multiple graph operations. Using social network data; we highlightthe analytical capabilities enabled by graph grouping in combination with other graphoperators. The resulting graphs are visualized and visitors are invited to either modifyexisting or write new analytical programs. G is implemented on top of Apache Flink; a state-of-the-art distributed dataflow framework; and thus allows us to scale graph analyticalprograms across multiple machines. In the demonstration; programs can either be executedlocally or remotely on our research cluster.,Proc. BTW,2017,1,7
A Smart Link Infrastructure for Integrating and Analyzing Process Data,Eric Peukert; Christian Wartner; Erhard Rahm,An often forgotten asset of many companies is internal process data. From the everydayprocesses that run within companies; huge amounts of such data is collected within differentsoftware systems. However; the value of analyzing this data holistically is often not exploited.This is mainly due to the inherent heterogeneity of the different data sources and the missingflexibility of existing approaches to integrate additional sources in an ad-hoc fashion. In thispaper the Smart Link Infrastructure is introduced. It offers tools that enable data integrationand linking to support a holistic analysis of process data. The infrastructure consists of easyto use components for matching schemas; linking entities; storing entities as a graph andexecuting pattern queries on top of the integrated data. We showcase the value of thepresented integration approach with two real world usecases; one based on knowledge …,Proc. BTW,2015,1,1
Data Integration in the Life Sciences,Helena Galhardas; Erhard Rahm,*,*,2014,1
Der Lehrstuhl Datenbanken an der Universität Leipzig,Erhard Rahm,Zusammenfassung Der Lehrstuhl Datenbanken an der Universität Leipzig befasst sichschwerpunktmäßig mit automatisierten Verfahren zur Integration und Analyse großerMengen heterogener Daten; va aus dem Web. Im Zusammenhang mit “Big Data” werdenunterschiedlichste Hochleistungsstrategien verfolgt; ua Skew-resistenteLastbalancierungsmethoden für MapReduce sowie die Nutzung modernerGrafikprozessoren (GPUs). Zum Matching von Modellen (Schemas; Ontologien) und vonInstanzdaten wurden leistungsfähige Verfahren und mehrere Prototypen entwickelt.Untersucht werden ferner Methoden zur Evolution von Ontologien und Mappings; um dieAuswirkungen von Ontologieänderungen zu minimieren. Der Bericht gibt nach einerEinleitung zur Entwicklung des Lehrstuhls einen Überblick zu den aktuellen …,Datenbank-Spektrum,2013,1,4
Parameterized XPath Views,Timo Böhme; Erhard Rahm,Abstract We present a new approach for accelerating the execution of XPath expressionsusing parameterized materialized XPath views (PXV). While the approach is generic weshow how it can be utilized in an XML extension for relational database systems.Furthermore we discuss an algorithm for automatically determining the best PXV candidatesto materialize based on a given workload. We evaluate our approach and show thesuperiority of our cost based algorithm for determining PXV candidates over frequent patternbased algorithms.,British National Conference on Databases,2007,1,4
LOTS - Online-Training an der Universität Leipzig,Timo Böhme; Erhard Rahm; Dieter Sosna,*,*,2006,1
Hybride Integration von molekularbiologischen Annotationsdaten,Christine Körner; Toralf Kirsten; Hong-Hai Do; Erhard Rahm,Abstract: Wir präsentieren einen Ansatz; um Annotationsdaten von molekularbiologischenObjekten wie Genen; Proteinen und Pathways aus öffentlichen Datenquellen fürdatenintensive Expressionsanalysen verwendbar zu machen. Die Expressionsdaten sindmit Experimentbeschreibungen physisch in einem Data Warehouse integriert; um schnelleAuswertungen zu unterstützen. Die öffentlichen Annotationsdaten werden virtuell über einenMediatoransatz integriert und bedarfsgesteuert für Analysen abgerufen. Für die einheitlicheAnbindung der Datenquellen wird das verbreitete Tool SRS (Sequence Retrieval System)der Fa. LION bioscience genutzt. Die Kopplung zwischen dem Warehouse und SRS erfolgtüber einen Query-Mediator unter Nutzung explizit gespeicherter Beziehungen (Mappings)zwischen den Instanzen der öffentlichen Datenquellen. Dieser hybride Integrationsansatz …,Proc. BTW Conf.,2005,1,10
XML-Datenbanksysteme,Timo Böhme; Erhard Rahm,*,IAO Forum,2002,1
DOL: An Interoperable Document Server,Sergey Melnik; Erhard Rahm; Dieter Sosna,Abstract We describe the design and experiences gained mth the database-and web-baseddocument server DOL; which we developed at the University of Leipzig (http://dolMni-leipzig.def The sener provides a centrai repository far a variety of fülltest docilmente. In Leipzig; ithas been used since 199? as a university-w ide digitai library for dùciiiiiéiìts by local authors;iti pò rtictdar Ph. D. these?* waster theses* research papers* fectftre notes etc; offering acentrai access point te the university s research results and educational material.Decetitratized administration and different frûrkflows are supported to meet organizationaland legai requirements of specific document types (e* g.; Ph. D. theses). Ali docmuenfs areconverted info several'formats* and can be don-nloaded or viewed online in a pagewisefashion The documents are searchable in a flexi-ble H-ary using fulltexf and bibliographic …,Proceedings Gl-Jahrestagung,2001,1,20
Metadaten-Verwaltung fuer heterogene Informationssysteme (Eingeladener Vortrag - Zusammenfassung),Erhard Rahm,*,Grundlagen von Datenbanken,2001,1
Automatisches; zielorientiertes Performance Tuning von Transaktionssystemen,Erhard Rahm,Die Überwachung und Steuerung des Leistungsverhaltens derzeitiger Transaktionssystemeund Datenbanksysteme weist eine Reihe gravierender Schwächen auf:-weitgehendmanuelles Performance Tuning durch Systemverwalter (zB DBA)-komplizierteSystemverwaltung durch Vielzahl von internen Parametern-mangelnde Abstimmungzwischen Resource-Managern; insbesondere zwischen DBS; TP-Monitor undBetriebssystem (zB verwenden TP-Monitore Transaktionsprioritäten; Betriebssystemedagegen Prozeß-Prioritäten; das DBS kennt meist gar keine Prioritäten)*-unzureichendeUnterscheidung verschiedener Lastgruppen im DBS bei der Zuteilung von Betriebsmitteln(Sperren; Pufferplatz; etc.). Diese Probleme stellen sich bereits im Falle zentralisierterTransaktionssysteme; welche ua aus einem DBS; einem TP-Monitor sowie den …,Workshop" Methoden und Werkzeuge zur Datenbankadministration"; Darmstadt,1996,1,6
Nutzung neuer Speicherarchitekturen in Hochleistungs-Transaktionssystemen,Theo Härder; Erhard Rahm,*,Architektur von Rechensystemen; Tagungsband; 11. ITG/GI-Fachtagung,1990,1
Interactive Visualization of Large Similarity Graphs and Entity Resolution Clusters,M Ali Rostami; Alieh Saeedi; Eric Peukert; Erhard Rahm,ABSTRACT Entity Resolution (ER) identifies semantically equivalent entities; eg describingthe same product or customer. It is a crucial and challenging step when integratingheterogeneous (big) data sources. ER approaches typically compute a similarity graphwhere vertices represent entities and edges (links) connect similar entities. Differentclustering algorithms can be applied on such similarity graphs to finally determine groups ofmatching entities. In this demonstration paper; we introduce a new interactive tool tovisualize and thus help to analyze large similarity graphs and large sets of ER clusters.Users can intuitively investigate the link and cluster structure to identify potential problemssuch as overly large clusters; cluster overlaps or singletons that might indicate the need forrepair activities on the ER result. To support large graphs; computation-intensive tasks …,*,2018,*,10
Evaluating and improving annotation tools for medical forms,Ying-Chi Lin; Victor Christen; Anika Groß; Silvio Domingos Cardoso; Cédric Pruski; Marcos Da Silveira; Erhard Rahm,Abstract The annotation of entities with concepts from standardized terminologies andontologies is of high importance in the life sciences to enhance semantic interoperability;information retrieval and meta-analysis. Unfortunately; medical documents such as clinicalforms or electronic health records are still rarely annotated despite the availability of sometools to automatically determine possible annotations. In this study; we comparativelyevaluate the quality of two such tools; cTAKES and MetaMap; as well as of a recentlyproposed annotation approach from our group for annotating medical forms. We alsoinvestigate how to improve the match quality of the tools by post-filtering computedannotations as well as by combining several annotation approaches.,International Conference on Data Integration in the Life Sciences,2017,*,7
Mining and ranking of generalized multi-dimensional frequent subgraphs,André Petermann; Giovanni Micale; Giacomo Bergami; Alfredo Pulvirenti; Erhard Rahm,Frequent pattern mining is an important research field and can be applied to differentlabeled data structures ranging from itemsets to graphs. There are scenarios where a labelcan be assigned to a taxonomy and generalized patterns can be mined by replacing labelsby their ancestors. In this work; we propose a novel approach to generalized frequentsubgraph mining. In contrast to existing work; our approach considers new requirementsfrom use cases beyond molecular databases. In particular; we support directed multigraphsas well as multiple taxonomies to deal with the different semantic meaning of vertices. Sinceresults of generalized frequent subgraph mining can be very large; we use a fast analyticalmethod of p-value estimation to rank results by significance. We propose two extensions ofthe popular gSpan algorithm that mine frequent subgraphs across all taxonomy levels …,Digital Information Management (ICDIM); 2017 Twelfth International Conference on,2017,*,15
DIMSpan-Transactional Frequent Subgraph Mining with Distributed In-Memory Dataflow Systems,André Petermann; Martin Junghanns; Erhard Rahm,Abstract: Transactional frequent subgraph mining identifies frequent subgraphs in acollection of graphs. This research problem has wide applicability and increasingly requireshigher scalability over single machine solutions to address the needs of Big Data use cases.We introduce DIMSpan; an advanced approach to frequent subgraph mining that utilizes thefeatures provided by distributed in-memory dataflow systems such as Apache Spark orApache Flink. It determines the complete set of frequent subgraphs from arbitrary string-labeled directed multigraphs as they occur in social; business and knowledge networks.DIMSpan is optimized to runtime and minimal network traffic but memory-aware. Anextensive performance evaluation on large graph collections shows the scalability ofDIMSpan and the effectiveness of its pruning and optimization techniques. Subjects …,arXiv preprint arXiv:1703.01910,2017,*,3
Towards a Multi-level Approach for the Maintenance of Semantic Annotations.,Silvio Domingos Cardoso; Chantal Reynaud-Delaître; Marcos Da Silveira; Ying-Chi Lin; Anika Groß; Erhard Rahm; Cédric Pruski,Abstract: Semantic annotations are often used to enrich documents as clinical trials andelectronic health records. However; the usability of these annotations tends to decrease overtime due to the evolution of the domain ontologies. The maintenance of these annotations iscritical for tools that exploit them (eg; search engines and decision support systems) in orderto assure an acceptable level of performance. Despite the recent advances in ontologyevolution systems; the maintenance of semantic annotations remains an open problem. Inthis paper; we introduce; based on previous experiments; the main components of a multi-level approach towards the automatic maintenance of semantic annotations. We furtherprovide examples for strengthening our proposal.,HEALTHINF,2017,*,2
Evaluation of Metadata Representations in RDF stores,Johannes Frey; Kay Müller; Sebastian Hellmann; Erhard Rahm; Maria-Esther Vidal,AbstractThe maintenance and use of metadata such as provenance and time-relatedinformation (when was a data entity created or retrieved) is of increasing importance in theSemantic Web; especially for Big Data applications that work on heterogeneous data frommultiple sources and which require high data quality. In an RDF dataset; it is possible tostore metadata alongside the actual RDF data and several possible metadatarepresentation models have been proposed. However; there is still no indepth comparativeevaluation of the main representation alternatives on both the conceptual level and theimplementation level using different graph backends. In order to help to close this gap; weintroduce major use cases and requirements for storing and using diverse kinds ofmetadata. Based on these requirements; we perform a detailed comparison and …,*,2017,*,15
Temporal group linkage and evolution analysis for census data,Victor Christen; Anika Groß; Jeffrey Fisher; Qing Wang; Peter Christen; Erhard Rahm,ABSTRACT The temporal linkage of census data allows the detailed analysis of population-related changes in an area of interest. It should not only link records about the same personbut also support the linkage of groups of related persons such as households. In this paper;we thus propose a new approach to both temporal record and group (household) linkage forcensus data and study its application for change analysis. The approach utilizes therelationships between individuals to determine the similarity of groups and their memberswithin a graph-based method. The approach is also iterative by first identifying high qualitymatches that are subsequently extended by matches found with less restrictive similaritycriteria. A comprehensive evaluation using historical census data from the UK indicates ahigh effectiveness of the proposed approach. Furthermore; the linkage enables an …,EDBT,2017,*,15
Semi-Automatic Identification of Counterfeit Offers in Online Shopping Platforms,Patrick Arnold; Christian Wartner; Erhard Rahm,ABSTRACT Product counterfeiting in online platforms is an increasingly serious problemcausing estimated losses of billions of dollars every year. The huge number of online shopsand offered products call for largely automated approaches to identify likely counterfeits;although identifying counterfeits is very difficult even for humans. The authors propose theadoption of a semi-automatic workflow to inspect product offers in online platforms and todetermine likely counterfeit offers based on different criteria. Such suspicious offers are to bepresented to a domain expert for manual verification. The workflow includes steps to matchand cluster similar product offers; and to assess the counterfeit suspiciousness based ondifferent criteria. The goal is to support the periodic identification of many counterfeit offerswith a limited amount of manual effort. The authors also present a preliminary evaluation …,Journal of Internet Commerce,2016,*,14
Joint workshop on data management for science (DMS),Sebastian Dorok; Birgitta König-Ries; Matthias Lange; Erhard Rahm; Gunter Saake; Bernhard Seeger,The Workshop on Data Management for Science (DMS) is a joint workshop consisting of thetwo workshops Data Management for Life Sciences (DMforLS) and Big Data in Science(BigDS). BigDS focuses on addressing big data challenges in various scientific disciplines.In this context; DMforLS focuses especially on life sciences. In the following; we give shortexcerpts of the call for papers of both workshops: Data Management for Life Sciences In lifesciences; scientists collect an increasing amount of data that must be stored; integrated;processed; and analyzed efficiently to make effective use of them. Thereby; not only thehuge volume of available data raises challenges regarding storage space and analysisthroughput; but also data quality issues; incomplete semantic annotation; long termpreservation; data access; and compliance issues; such as data provenance; make it …,Datenbanksysteme für Business; Technologie und Web (BTW 2015)-Workshopband,2015,*,7
30 Jahre „Datenbanksysteme für Business; Technologie und Web “,T Härder; E Rahm,Die Datenbankgemeinde im deutschsprachigen Raum brauchte recht lange; um sich mitihren Arbeitsgruppen zu organisieren und mit einer allgemein akzeptierten Fachtagung ihrwissenschaftliches Forum zu finden. Wollten DB-Forscher ihre Ergebnisse in Deutschlandeinem Fachpublikum vorstellen und sich dabei mit Fachkollegen austauschen; konnten sieab 1970 zunächst nur die GI-Jahrestagung wählen; wo aber kaum mehr als eine Sitzung mitDB-Themen gefüllt werden konnte. Mitte der 1970-er Jahre ergaben sich einige weitereMöglichkeiten und zwar schon mit speziellerer DB-Orientierung:„Praxis der Realisierungvon Informationssystemen “(Tagung des German Chapter of the ACM; Jülich; Nov.1975);„Datenbanken in Rechnernetzen mit Kleinrechnern “(GI-Fachtagung; Karlsruhe; April1978);„Datenbanktechnologie; Einsatz großer; verteilter und intelligenter Datenbanken “ …,Datenbank-Spektrum,2015,*,7
Informationsintegration in Service-Architekturen,Erhard Rahm; Harald Schöning,*,Proc. GI-Jahrestagung; LNI,2010,*
Logging and Recovery,Erhard Rahm,A language model assigns a probability to a piece of unseen text; based on some trainingdata. For example; a language model based on a big English newspaper archive isexpected to assign a higher probability to ''a bit of text''than to ''aw pit tov tags;''because thewords in the former phrase (or word pairs or word triples if so-called N-Gram Models areused) occur more frequently in the data than the words in the latter phrase. For informationretrieval; typical usage is to build a language model for each document. At search time; thetop ranked document is the one whose language model assigns the highest probability tothe query.,*,2009,*,5
Model Management,Erhard Rahm,*,Datenbank-Spektrum,2007,*
Datenbanksysteme für Business; Technologie und Web (Proc. BTW’03),Gerhard Weikum; Harald Schöning; Erhard Rahm,*,*,2003,*
14 Benchmarking von XML-Datenbanksystemen,Timo Böhme; Erhard Rahm,Kurzfassung Zum Vergleich der Leistungsfähigkeit von XML-Datenbanksystemen sindBenchmarks erforderlich; welche den Spezifika der XML-Datenverarbeitung Rechnungtragen. Das Kapitel beschreibt die wesentlichen Anforderungen an geeignete Benchmarks.Ferner werden drei konkrete Benchmarks vorgestellt und miteinander verglichen: XMach-1;Xmark und XOO7.,*,2003,*,7
Implementierung von Datenbanksystemen,Erhard Rahm,*,UNIVERSITÄT LEIPZIG; Vorlesungsskript,2002,*
Topics to be covered,E Rahm; PA Bernstein,∎ Schema integration □ Developing global view over set of independently developed schemas∎ Data warehousing □ Transforming data from source format to warehouse format ∎ E-commerce/ B2B integration □ Transforming between message types and trading partner formats ∎ Semanticquery processing □ Mapping user-specified query concepts to database schema elements …∎ IN: 2 schemas S1 and S2 ∎ OUT: match result – mapping of elements in S1 and S2 … ∎Schema: set of elements connected by some structure … ∎ Mapping element: specification ofelements in S1 that map to elements in S2 and a mapping expression specifying how they arerelated... □ {elements in S1} ~= {elements in S2} : (mapping expression) … ∎ Schema-levelmatchers □ Consider schema information; not instance data □ Mapping expressions on schemaelement name; description; type; constraints; structure; etc. ∎ Instance-level matchers □ …,The VLDB Journal,2001,*,7
Informatikforschung an der Universität Leipzig,Erhard Rahm,Eine interaktive WWW-Galerie entsteht als Kooperationsprojekt mit dem FachbereichMedienkunst der Hochschule fur Grafik und Buchkunst Leipzig (http://www. hgb-leipzig.de/projekt/wwwgal/wwwgal. htm). Die ju ngste Entwicklung im Bereich Multimedia;Electronic Publishing und Internet zeigt; daß die Erstellung leistungsfä higer Anwendungenweder von technologischer Seite noch von der Seite der Gestaltung her allein gelo stwerden kann. Im Zentrum des Interesses steht daher fu r die Galerie die interdisziplinäreZusammenarbeit zwischen Medienkunst und Informatik. Aus der Sicht der Informatik dientdas Projekt vornehmlich dem Aufbau einer WWW-Referenzanwendung unter Einbeziehungund Weiterentwicklung geeigneter Werkzeuge fu r komplexe multimediale Anwendungen imWWW. Die Verwaltung der Bilder soll datenbankbasiert erfolgen.,Informatik Forschung und Entwicklung,1997,*,4
Datenföderalismus: Methoden und Probleme verteilter Datenbanksysteme,Erhard Rahm,*,C'T,1993,*
