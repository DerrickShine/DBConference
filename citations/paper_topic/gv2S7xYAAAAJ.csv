The well-founded semantics for general logic programs,Allen Van Gelder; Kenneth A Ross; John S Schlipf,Abstract. A general logic program (abbreviated to “program” hereafter) is a set of roles thathave both positive and negative subgoals. It is common to view a deductive database as ageneral logic program consisting of rules (IDB) slttmg above elementary relations (EDB;facts). It is desirable to associate one Herbrand model with a program and think of thatmodel as the “meaning of the program;” or Its “declarative semantics.” Ideally; queriesdirected to the program would be answered in accordance with this model. Recent researchindicates that some programs do not have a “satisfactory” total model; for such programs; thequestion of an appropriate partial model arises. Unfounded sets and well-founded partialmodels are introduced and the well-founded semantics of a program are defined to be itswell-founded partial model. If the well-founded partial model is m fact a total model. it is …,Journal of the ACM (JACM),1991,2032,15
Filtering algorithms and implementation for very fast publish/subscribe systems,Françoise Fabret; H Arno Jacobsen; François Llirbat; Joăo Pereira; Kenneth A Ross; Dennis Shasha,Abstract Publish/Subscribe is the paradigm in which users express long-term interests(“subscriptions”) and some agent “publishes” events (eg; offers). The job ofPublish/Subscribe software is to send events to the owners of subscriptions satisfied bythose events. For example; a user subscription may consist of an interest in an airplane of acertain type; not to exceed a certain price. A published event may consist of an offer of anairplane with certain properties including price. Each subscription consists of a conjunctionof (attribute; comparison operator; value) predicates. A subscription closely resembles atrigger in that it is a long-lived conditional query associated with an action (usually; informingthe subscriber). However; it is less general than a trigger so novel data structures andimplementations may enable the creation of more scalable; high performance publish …,ACM Sigmod Record,2001,681,10
Unfounded sets and well-founded semantics for general logic programs,Allen Van Gelder; Kenneth Ross; John S Schlipf,Abstract A general logic program (abbreviated to “program” hereafter) is a set of rules thathave both positive and negative subgoals. It is common to view a deductive database as ageneral logic program consisting of rules (IDB) sitting above elementary relations (EDB;facts). It is desirable to associate one Herbrand model with a program and think of thatmodel as the “meaning of the program;” or its “declarative semantics.” Ideally; queriesdirected to the program would be answered in accordance with this model. We introduceunfounded sets and well-founded partial models; and define the well-founded semantics ofa program to be its well-founded partial model. If the well-founded partial model is in fact amodel; we call it the well-founded model; and say the program is “well-behaved”. We showthat the class of well-behaved programs properly includes previously studied classes of “ …,Proceedings of the seventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1988,588,7
Making B+-trees cache conscious in main memory,Jun Rao; Kenneth A Ross,Abstract Previous research has shown that cache behavior is important for main memoryindex structures. Cache conscious index structures such as Cache Sensitive Search Trees(CSS-Trees) perform lookups much faster than binary search and T-Trees. However; CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees; their utilization of a cacheline is low since half of the space is used to store child pointers. Nevertheless; forapplications that require incremental updates; traditional B+-Trees perform well. Our goal isto make B+-Trees as cache conscious as CSS-Trees without increasing their update cost toomuch. We propose a new indexing technique called “Cache Sensitive B+-Trees”(CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node …,ACM SIGMOD Record,2000,469,7
Cache conscious indexing for decision-support in main memory,Jun Rao; Kenneth A Ross,Abstract We study indexing techniques for main memory; including hash indexes; binarysearch trees; T-trees; B+-trees; interpolation search; and binary search on arrays. In adecision-support context; our primary concerns are the lookup time; and the space occupiedby the index structure. Our goal is to provide faster lookup times than binary search bypaying attention to reference locality and cache behavior; without using substantial extraspace. We propose a new indexing technique called “Cache-Sensitive Search Trees”(CSS-trees). Our technique stores a directory structure on top of a sorted array. Nodes in thisdirectory have size matching the cache-line size of the machine. We store the directory in anarray and do not store internal-node pointers; child nodes can be found by performingarithmetic on array offsets.,VLDB,1999,338,20
Fast computation of sparse datacubes,Kenneth A Ross; Divesh Srivastava,Abstract Datacube queries compute aggregates over database relations at a variety ofgranularities; and they constitute an important class of decision support queries. Real-worlddata is frequently sparse; and hence e ciently computing datacubes over large sparserelations is important. We show that current techniques for computing datacubes over sparserelations do not scale well with the number of CUBE BY attributes; especially when therelation is much larger than main memory. We propose a novel algorithm for the fastcomputation of datacubes over sparse relations; and demonstrate the e ciency of ouralgorithm using synthetic; benchmark and real-world data sets. When the relation ts inmemory; our technique performs multiple in-memory sorts; and does not incur any I/Obeyond the input of the relation and the output of the datacube itself. When the relation …,VLDB,1997,324,20
The New Jersey data reduction report,Daniel Barbar'a; William DuMouchel; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,Abstract this paper we describe and evaluate several popular techniques for data reduction.Historically; the primary need for data reduction has been internal to a database system; in acost-based query optimizer. The need is for the query optimizer to estimate the cost ofalternative query plans cheaply--clearly the effort required to do so must be much smallerthan the effort of actually executing the query; and yet the cost of executing any query plandepends strongly upon the numerosity of specified attribute values and the selectivities ofspecified predicates. To address these query optimizer needs; many databases keepsummary statistics. Sampling techniques have also been proposed. More recently; there hasbeen an explosion of interest in the analysis of data in warehouses. Data warehouses canbe extremely large; yet obtaining answers quickly is important. Often; it is quite acceptable …,IEEE Data Engineering Bulletin,1997,321,20
Materialized view maintenance and integrity constraint checking: Trading space for time,Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We investigate the problem of incremental maintenance of an SQL view in the faceof database updates; and show that it is possible to reduce the total time cost of viewmaintenance by materializing (and maintaining) additional views. We formulate the problemof determining the optimal set of additional views to materialize as an optimization problemover the space of possible view sets (which includes the empty set). The optimizationproblem is harder than query optimization since it has to deal with multiple view sets;updates of multiple relations; and multiple ways of maintaining each view set for eachupdated relation. We develop a memoing solution for the problem; the solution can beimplemented using the expression DAG representation used in rule-based optimizers suchas Volcano. We demonstrate that global optimization cannot; in general; be achieved by …,ACM SIGMOD Record,1996,290,10
Implementing database operations using SIMD instructions,Jingren Zhou; Kenneth A Ross,Abstract Modern CPUs have instructions that allow basic operations to be performed onseveral data elements in parallel. These instructions are called SIMD instructions; since theyapply a single instruction to multiple data elements. SIMD technology was initially built intocommodity processors in order to accelerate the performance of multimedia applications.SIMD instructions provide new opportunities for database engine design andimplementation. We study various kinds of operations in a database context; and show howthe inner loop of the operations can be accelerated using SIMD instructions. The use ofSIMD instructions has two immediate performance benefits: It allows a degree of parallelism;so that many operands can be processed at once. It also often leads to the elimination ofconditional branch instructions; reducing branch mispredictions. We consider the most …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,199,2
Adapting materialized views after redefinitions,Ashish Gupta; Inderpal S Mumick; Kenneth A Ross,Abstract We consider a variant of the view maintenance problem: How does one keep amaterialized view up-to-date when the view definition itself changes? Can one do betterthan recomputing the view from the base relations? Traditional view maintenance tries tomaintain the materialized view in response to modifications to the base relations; we try to"adapt" the view in response to changes in the view definition. Such techniques are neededfor applications where the user can change queries dynamically and see the changes in theresults fast. Data archaeology; data visualization; and dynamic queries are examples ofsuch applications. We consider all possible redefinitions of SQL SELECT-FROM-WHERE-GROUPBY; UNION; and EXCEPT views; and show how these views can be adapted usingthe old materialization for the cases where it is possible to do so. We identify extra …,ACM SIGMOD Record,1995,153,7
A procedural semantics for well-founded negation in logic programs,Kenneth A Ross,Abstract We introduce global SLS-resolution; a procedural semantics for well-foundednegation as defined by Van Gelder; Ross; and Schlipf. Global SLS-resolution extendsPrzymusinski's SLS-resolution and may be applied to all programs; whether locally stratifiedor not. Global SLS-resolution is defined in terms of global trees; a new data structurerepresenting the dependence of goals on derived negative subgoals. We prove that globalSLS-resolution is sound with respect to the well-founded semantics and complete fornonfloundering queries. Although not effective in general; global SLS-resolution is effectivefor classes of “acrylic” programs and can be augmented with a memoing device to beeffective for all function-free programs.,The Journal of Logic Programming,1992,152,22
Supporting multiple view maintenance policies,Latha S Colby; Akira Kawaguchi; Daniel F Lieuwen; Inderpal Singh Mumick; Kenneth A Ross,Abstract Materialized views and view maintenance are becoming increasingly important inpractice. In order to satisfy different data currency and performance requirements; a numberof view maintenance policies have been proposed. Immediate maintenance involves apotential refresh of the view after every update to the deriving tables. When staleness ofviews can be tolerated; a view may be refreshed periodically or (on-demand) when it isqueried. The maintenance policies that are chosen for views have implications on thevalidity of the results of queries and affect the performance of queries and updates. In thispaper; we investigate a number of issues related to supporting multiple views with differentmaintenance policies. We develop formal notions of consistency for views with differentmaintenance policies. We then introduce a model based on view groupings for view …,ACM SIGMOD Record,1997,150,4
Inferring negative information from disjunctive databases,Kenneth A Ross; Rodney W Topor,Abstract We propose criteria that any rule for inferring negative information from disjunctivedatabases should satisfy; and examine existing rules from this viewpoint. We then present anew inference rule; the 'disjunctive database rule'(DDR); and compare it to the existing ruleswith respect to the criteria. In particular; the DDR is equivalent to the CWA for definitedatabases; it infers no more negative information than the GCWA; and it interpretsdisjunction inclusively rather than exclusively. We generalize the DDR to a class of layereddatabases; describe an implementation of the DDR;'negation as positive failure'; and studyits soundness and completeness properties.,Journal of automated reasoning,1988,135,1
SSD bufferpool extensions for database systems,Mustafa Canim; George A Mihaila; Bishwaranjan Bhattacharjee; Kenneth A Ross; Christian A Lang,Abstract High-end solid state disks (SSDs) provide much faster access to data compared toconventional hard disk drives. We present a technique for using solid-state storage as acaching layer between RAM and hard disks in database management systems. By cachingdata that is accessed frequently; disk I/O is reduced. For random I/O; the potentialperformance gains are particularly significant. Our system continuously monitors the diskaccess patterns to identify hot regions of the disk. Temperature statistics are maintained atthe granularity of an extent; ie; 32 pages; and are kept current through an aging mechanism.Unlike prior caching methods; once the SSD is populated with pages from warm regionscold pages are not admitted into the cache; leading to low levels of cache pollution.Simulations based on DB2 I/O traces; and a prototype implementation within DB2 both …,Proceedings of the VLDB Endowment,2010,130,7
Querying multiple features of groups in relational databases,Damianos Chatziantoniou; Kenneth A Ross,Abstract Some aggregate and grouping queries are conceptually simple; but difficult toexpress in SQL. This difficulty causes both conceptual and implementation problems for theSQL-based database system. Complicated queries and views are hard to understand andmaintain. Further; the code produced is sometimes unnecessarily inefficient; as wedemonstrate experimentally using a commercial database system. In this paper; we examinea class of queries involving (potentially repeated) selection; grouping and aggregation overthe same groups; and propose an extension of SQL syntax that allows the succinctrepresentation of these queries. We propose a new relational algebra operation thatrepresents several levels of aggregation over the same groups in an operand relation. Wedemonstrate that the extended relational operator can be evaluated using efficient …,VLDB,1996,128,4
Modular stratification and magic sets for DATALOG programs with negation,Kenneth A Ross,Abstract We propose a class of programs; called modularly stratified programs that haveseveral attractive properties. Modular stratification generalizes stratification and localstratification; while allowing programs that are not expressible by stratified programs. Formodularly stratified programs the well-founded semantics coincides with the stable modelsemantics; and makes every ground literal true or false. Modularly stratified programs are allweakly stratified; but the converse is false. Unlike some weakly stratified programs;modularly stratified programs can be evaluated in a subgoal-at-a-time fashion. Wedemonstrate a technique for rewriting a modularly stratified program for bottom-upevaluation and extend this rewriting to include magic-set techniques. The rewritten program;when evaluated bottom-up; gives the same answers as the well-founded semantics. We …,Proceedings of the ninth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1990,126,22
Glue-Nail: A deductive database system,Geoffrey Phipps; Marcia A Derr; Kenneth A Ross,Abstract Glue is a procedural language for deductive databases. It is designed tocomplement the purely declarative NAIL! language; firstly by performing system functionsimpossible to write in NAIL!; and secondly by allowing the procedural specification ofalgorithms for critical code sections. The two languages together are sufficient to write acomplete application. Glue was designed to be as close to NAIL! as possible; henceminimizing the impedance mismatch problem. In this paper we concentrate on Glue. Pseudo-higher order programming is used in both languages; in the style of HiLog[1]. In particularGlue-Nail can handle set valued attributes(non-1NF schemas) in a clean and efficientmanner. NAIL! code is compiled into Glue code; simplifying the system design. Anexperimental implementation has been written; a more efficient version is under design.,ACM SIGMOD Record,1991,117,8
Challenges and Opportunities with big data 2011-1,Divyakant Agrawal; Philip Bernstein; Elisa Bertino; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jagadish; Alexandros Labrinidis; Sam Madden; Yannis Papakonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of``Big Data.''While the promise of Big Data is real--for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009--there is currently a wide gap between its potential and its realization.,*,2011,107,7
Adaptive aggregation on chip multiprocessors,John Cieslewicz; Kenneth A Ross,Abstract The recent introduction of commodity chip multiprocessors requires that the designof core database operations be carefully examined to take full advantage of on-chipparallelism. In this paper we examine aggregation in a multi-core environment; the SunUltraSPARC T1; a chip multiprocessor with eight cores and a shared L2 cache. Aggregationis an important aspect of query processing that is seemingly easy to understand andimplement. Our research; however; demonstrates that a chip multiprocessor adds newdimensions to understanding hash-based aggregation performance---concurrent sharing ofaggregation data structures and contentious accesses to frequently used values. We alsoidentify a trade off between private data structures assigned to each thread versus shareddata structures for aggregation. Depending on input characteristics; different aggregation …,Proceedings of the 33rd international conference on Very large data bases,2007,107,4
Q100: The architecture and design of a database processing unit,Lisa Wu; Andrea Lottarini; Timothy K Paine; Martha A Kim; Kenneth A Ross,Abstract In this paper; we propose Database Processing Units; or DPUs; a class of domain-specific database processors that can efficiently handle database applications. As a proof ofconcept; we present the instruction set architecture; microarchitecture; and hardwareimplementation of one DPU; called Q100. The Q100 has a collection of heterogeneous ASICtiles that process relational tables and columns quickly and energy-efficiently. Thearchitecture uses coarse grained in-structions that manipulate streams of data; therebymaximizing pipeline and data parallelism; and minimizing the need to time multiplex theaccelerator tiles and spill inter-mediate results to memory. This work explores a Q100 de-sign space of 150 configurations; selecting three for further analysis: a small; power-conscious implementation; a high-performance implementation; and a balanced design …,Acm Sigplan Notices,2014,100,0
Improving database performance on simultaneous multithreading processors,Jingren Zhou; John Cieslewicz; Kenneth A Ross; Mihir Shah,Abstract Simultaneous multithreading (SMT) allows multiple threads to supply instructions tothe instruction pipeline of a superscalar processor. Because threads share processorresources; an SMT system is inherently different from a multiprocessor system and;therefore; utilizing multiple threads on an SMT processor creates new challenges fordatabase implementers. We investigate three thread-based techniques to exploit SMTarchitectures on memory-resident data. First; we consider running independent operationsin separate threads; a technique applied to conventional multi-processor systems. Second;we describe a novel implementation strategy in which individual operators are implementedin a multi-threaded fashion. Finally; we introduce a new data-structure called a work-aheadset that allows us to use one of the threads to aggressively preload data into the cache …,Proceedings of the 31st international conference on Very large data bases,2005,100,15
The well founded semantics for disjunctive logic programs,Kenneth A Ross,Abstract We extend the well-founded semantics of Van Gelder; Ross and Schlipf todisjunctive logic programs. We propose a “strong” semantics; in which disjunction is treatedexclusively; and a “weak” semantics in which disjunction is treated inclusively. The strongsemantics infers a subset of the information inferred under the perfect model approach ofPrzymusinski for locally stratified programs; but is well defined for all programs. The weaksemantics generalizes the disjunctive database rule of Ross and Topor. We then define an“optimal” semantics which treats each rule either inclusively or exclusively according to theprogrammer's specification. We compare our proposed semantics to previous approaches;and examine some of its properties.,*,1990,97,4
Modular stratification and magic sets for Datalog programs with negation,Kenneth A Ross,Abstract A class of “modularly stratified” logic programs is defined. Modular stratificationgeneralizes stratification and local stratification; while allowing programs that are notexpressible as stratified programs. For modularly stratified programs; the well-foundedsemantics coincides with the stable model semantics and makes every ground literal true orfalse. Modularly stratified programs are weakly stratified; but the converse is false. Unlikesome weakly stratified programs; modularly stratified programs can be evaluated in asubgoal-at-a time fashion. An extension of top-down methods with memoing that handlesthis broader class of programs is presented. A technique for rewriting a modularly stratifiedprogram for bottom-up evaluation is demonstrated and extended to include magic-settechniques. The rewritten program; when evaluated bottom-up; gives correct answers …,Journal of the ACM (JACM),1994,95,7
Monotonic aggregation in deductive databases,Kenneth A Ross; Yehoshua Sagiv,Abstract We propose a semantics for aggregates in deductive databases based on a notionof minimality. Unlike some previous approaches; we form a minimal model of a programcomponent including aggregate operators; rather than insisting that the aggregate apply toatoms that have been fully determined; or that aggregate functions are rewritten in terms ofnegation. In order to guarantee the existence of such a minimal model we need to insist thatthe domains over which we are aggregating are complete lattices; and that the program is ina sense monotonic. Our approach generalizes previous approaches based on the well-founded semantics and various forms of stratification. We are also able to handle a largevariety of monotonic (or pseudo-monotonic) aggregate functions.,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,88,14
Buffering databse operations for enhanced instruction cache performance,Jingren Zhou; Kenneth A Ross,Abstract As more and more query processing work can be done in main memory access isbecoming a significant cost component of database operations. Recent database researchhas shown that most of the memory stalls are due to second-level cache data misses andfirst-level instruction cache misses. While a lot of research has focused on reducing the datacache misses; relatively little research has been done on improving the instruction cacheperformance of database systems. We first answer the question" Why does a databasesystem incur so many instruction cache misses?" We demonstrate that current demand-pullpipelined query execution engines suffer from significant instruction cache thrashingbetween different operators. We propose techniques to buffer database operations duringquery execution to avoid instruction cache thrashing. We implement a new light-weight" …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,82,7
Concurrency control theory for deferred materialized views,Akira Kawaguchi; Daniel Lieuwen; Inderpal Singh Mumick; Dallan Quass; Kenneth A Ross,Abstract We consider concurrency control problems that arise in the presence ofmaterialized views. Consider a database system supporting materialized views to speed upqueries. For a range of important applications (eg banking; billing; network management);transactions that access materialized views would like to get some consistency guarantees—if a transaction reads a base relation after an update; and then reads a materialized viewderived from the base relation; it expects to see the effect of the base update on thematerialized view. If a transaction reads two views; it expects that the two views reflect asingle consistent database state. Such guarantees are not easy to obtain; as materializedviews become inconsistent upon updates to base relations. Immediate maintenancereestablishes consistency within the transaction that updates the base relation; but this …,International Conference on Database Theory,1997,82,20
Buffering accesses to memory-resident index structures,Jingren Zhou; Kenneth A Ross,Abstract Recent studies have shown that cache-conscious indexes outperform conventionalmain memory indexes. Cache-conscious indexes focus on better utilization of each cacheline for improving search performance of a single lookup. None has exploited cache spatialand temporal locality between consecutive lookups. We show that conventional indexes;even" cache-conscious" ones; suffer from significant cache thrashing between accesses.Such thrashing can impact the performance of applications such as stream processing andquery operations such as index-nested-loops join. We propose techniques to bufferaccesses to memory-resident tree-structured indexes to avoid cache thrashing. We studyseveral alternative designs of the buffering technique; including whether to use fixed-size orvariable-sized buffers; whether to buffer at each tree level or only at some of the levels …,Proceedings of the 29th international conference on Very large data bases-Volume 29,2003,79,7
Navigating big data with high-throughput; energy-efficient data partitioning,Lisa Wu; Raymond J Barker; Martha A Kim; Kenneth A Ross,Abstract The global pool of data is growing at 2.5 quintillion bytes per day; with 90% of itproduced in the last two years alone [24]. There is no doubt the era of big data has arrived.This paper explores targeted deployment of hardware accelerators to improve thethroughput and energy efficiency of large-scale data processing. In particular; datapartitioning is a critical operation for manipulating large data sets. It is often the limiting factorin database performance and represents a significant fraction of the overall runtime of largedata queries. To accelerate partitioning; this paper describes a hardware accelerator forrange partitioning; or HARP; and a hardware-software data streaming framework. Thestreaming framework offers a seamless execution environment for streaming acceleratorssuch as HARP. Together; HARP and the streaming framework provide an order of …,ACM SIGARCH Computer Architecture News,2013,77,7
Fast joins using join indices,Zhe Li; Kenneth A Ross,Abstract Two new algorithms;“Jive join” and “Slam join;” are proposed for computing the joinof two relations using a join index. The algorithms are duals: Jive join range-partitions inputrelation tuple ids and then processes each partition; while Slam join forms ordered runs ofinput relation tuple ids and then merges the results. Both algorithms make a singlesequential pass through each input relation; in addition to one pass through the join indexand two passes through a temporary file; whose size is half that of the join index. Bothalgorithms require only that the number of blocks in main memory is of the order of thesquare root of the number of blocks in the smaller relation. By storing intermediate and finaljoin results in a vertically partitioned fashion; our algorithms need to manipulate less data inmemory at a given time than other algorithms. The algorithms are resistant to data skew …,The VLDB Journal—The International Journal on Very Large Data Bases,1999,74,7
Cost-based maintenance of materialized views,*,A method of incrementally maintaining a first materialized view of data in a database; bymeans of an additional materialized view; first determines whether a cost in time ofincrementally maintaining the first materialized view with the additional materialized view isless than the cost of incrementally maintaining the first materialized view without theadditional materialized view. The method creates the additional materialized view only if thecost in time is less therewith. Determining whether the cost of employing an additionalmaterialized view is less includes using an expression directed acyclic graph thatcorresponds to the first materialized view. Another method of determining whether the cost isless includes pruning an expression directed acyclic graph to produce a single expressiontree; and using the single expression tree to determine whether the cost is less. Both the …,*,2000,73,1
Publish/subscribe on the web at extreme speed,João Pereira; Françoise Fabret; François Llirbat; Radu Preotiuc-Pietro; Kenneth A Ross; Dennis Shasha,1 Introduction This demonstration presents Le Subscribe an event notification system for theWeb. It is widely accepted that the majority of human information will be on the Web in tenyears. As pointed out in [6]; besides systems for searching; querying and retrievinginformation from the Web; there is a need for systems being able to capture the dynamicaspect of the web information by notifying users of interesting events. This functionality iscrucial for web users (or applications) who want to exploit highly dynamic web informationsuch as stock markets updates or auctions. A tool that implements this functionality must bescalable and efficient. Indeed; it should manage millions of user demands for notifications (iesubscriptions); It should handle high rates of events (several millions per day) and notify theinterested users in a short delay. In addition; it should provide a simple and expressive …,VLDB,2000,71,2
Efficient hash probes on modern processors,Kenneth A Ross,Bucketized versions of Cuckoo hashing can achieve 95-99% occupancy; without any spaceoverhead for pointers or other structures. However; such methods typically need to consultmultiple hash buckets per probe; and have therefore been seen as having worse probeperformance than conventional techniques for large tables. We consider workloads typical ofdatabase and stream processing; in which keys and payloads are small; and in which alarge number of probes are processed in bulk. We show how to improve probe performanceby (a) eliminating branch instructions from the probe code; enabling better scheduling andlatency-hiding by modern processors; and (b) using SIMD instructions to process multiplekeys/payloads in parallel. We show that on modern architectures; probes to a bucketizedCuckoo hash table can be processed much faster than conventional hash table probes …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,68,20
Method of calculating tuples for data cubes,*,A method and apparatus of calculating data cubes is shown in which a data set is partitionedinto memory sized data fragments and cuboid tuples are calculated from the data fragments.A search lattice of the data cube is used as a basis for ordering calculations of lowerdimensional cuboids in the data cube. Identification of a minimum number of paths throughthe lattice that is sufficient to traverse all nodes in the lattice is achieved by iterativelyduplicating twice all paths in a lower dimensional space; distributing a new attribute to thefirst duplicate; moving end points from paths of the second duplicate to a corresponding pathin the first duplicate and merging the first and second duplicates.,*,1999,68,4
An object placement advisor for DB2 using solid state storage,Mustafa Canim; George A Mihaila; Bishwaranjan Bhattacharjee; Kenneth A Ross; Christian A Lang,Abstract Solid state disks (SSDs) provide much faster random access to data compared toconventional hard disk drives. Therefore; the response time of a database engine could beimproved by moving the objects that are frequently accessed in a random fashion to theSSD. Considering the price and limited storage capacity of solid state disks; the databaseadministrator needs to determine which objects (tables; indexes; materialized views; etc.); ifplaced on the SSD; would most improve the performance of the system. In this paper wepropose a tool called" Object Placement Advisor" for making a wise decision for the objectplacement problem. By collecting profile inputs from workload runs; the advisor utilityprovides a list of objects to be placed on the SSD by applying heuristics like the greedyknapsack technique or dynamic programming. To show that the proposed approach is …,Proceedings of the VLDB Endowment,2009,67,4
Scalable aggregation on multicore processors,Yang Ye; Kenneth A Ross; Norases Vesdapunt,Abstract In data-intensive and multi-threaded programming; the performance bottleneck hasshifted from I/O bandwidth to main memory bandwidth. The availability; size; and otherproperties of on-chip cache strongly influence performance. A key question is whether toallow different threads to work independently; or whether to coordinate the shared workloadamong the threads. The independent approach avoids synchronization overhead; butrequires resources proportional to the number of threads and thus is not scalable. On theother hand; the shared method suffers from coordination overhead and potential contention.In this paper; we aim to provide a solution to performing in-memory parallel aggregation onthe Intel Nehalem architecture. We consider several previously proposed techniques thatwere evaluated on other architectures; including a hybrid independent/shared method …,Proceedings of the Seventh International Workshop on Data Management on New Hardware,2011,64,15
System and method for performing joins and self-joins in a database system,*,A technique for efficiently joining multiple large tables in a database system which utilizes ajoin index. The technique uses a join index and minimizes the number of input/outputoperations while maximizing the use of the small main memory through a buffer allocationprocess based on the join index entries. The technique uses multi-dimensional partitioningand assigns partition identifiers to each buffer which are used to coordinate the resultantoutput files when the technique is complete. The output is vertically fragmented with onefragment for each input table which further allows the individual processing of each inputtable. The technique performs self-joins in a very efficient manner by requiring the records ofthe input table to be read only once.,*,1999,63,7
Complex aggregation at multiple granularities,Kenneth A Ross; Divesh Srivastava; Damianos Chatziantoniou,Abstract Datacube queries compute simple aggregates at multiple granularities. In thispaper we examine the more general and useful problem of computing a complex subqueryinvolving multiple dependent aggregates at multiple granularities. We call such queries“multi-feature cubes.” An example is “Broken down by all combinations of month andcustomer; find the fraction of the total sales in 1996 of a particular item due to supplierssupplying within 10% of the minimum price (within the group); showing all subtotals acrosseach dimension.” We classify multi-feature cubes based on the extent to which finegranularity results can be used to compute coarse granularity results; this classificationincludes distributive; algebraic and holistic multi-feature cubes. We provide syntacticsufficient conditions to determine when a multi-feature cube is either distributive or …,International Conference on Extending Database Technology,1998,60,10
Monotonic aggregation in deductive databases,Kenneth A Ross; Yehoshua Sagiv,Abstract We propose a semantics for aggregates in deductive databases based on a notionof minimality. Unlike some previous approaches; we form a minimal model of a programcomponent including aggregate operators; rather than insisting that the aggregate apply toatoms that have been fully determined or that aggregate functions are rewritten in terms ofnegation. In order to guarantee the existence of such a minimal model we need to insist thatthe domains over which we are aggregating are complete lattices and that the program is ina sense mono-tonic. Our approach generalizes previous approaches based on the well-founded semantics and various forms of stratification. We are also able to handle a largevariety of monotonic (or pseudo-monotonic) aggregate functions.,Journal of Computer and System Sciences,1997,59,7
Perf join: An alternative to two-way semijoin and bloomjoin,Zhe Li; Kenneth A Ross,Abstract This paper presents“Positionally Encoded Record Filters”(PERFs) and describestheir use in a distributed query processing technique called PERF join. A PERF is a noveltwo-way join reduction implementation primitive. While having the same storage andtransmission efficiency as a haah filter(eg; Bloom Filter); a PERF is based on the relationtuple scan order instead of hashing. Hence it doesn't suffer arty loss of join informationincurred by hash collisions. Using the query response time measured in terms of networkcost as a comparison criterion; we demonstrate through analytical studies that PERF joinperforms significantly better than two-way Bloomjoin and two-way semijoin variants under awide range of relevant cost parameter values. For the large number of distributed queryprocessing algorithms relying on Bloomjoin or semijoin variants to reduce their network …,Proceedings of the fourth international conference on Information and knowledge management,1995,59,20
Conjunctive selection conditions in main memory,Kenneth A Ross,Abstract We consider the fundamental operation of applying a conjunction of selectionconditions to a set of records. With large main memories available cheaply; systems maychoose to keep the data entirely in main memory; in order to improve query and/or updateperformance. The design of a data-intensive algorithm in main memory needs to take intoaccount the architectural characteristics of modern processors; just as a disk-based methodneeds to consider the physical characteristics of disk devices. An important architecturalfeature that influences the performance of main memory algorithms is the branchmisprediction penalty. We demonstrate that branch misprediction has a substantial impacton the performance of an algorithm for applying selection conditions. We describe a spaceof" query plans" that are logically equivalent; but differ in terms of performance due to …,Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2002,58,22
Groupwise processing of relational queries,Damianos Chatziantoniou; Kenneth A Ross,Abstract In this paper; we define and examine a particular class of queries called groupqueries. Group queries are natural queries in many decisionsupport applications. The maincharacteristic of a group query is that it can be executed in a groupby-group fashion. In otherwords; the underlying relation (s) can be partitioned (based on some set of attributes) intodisjoint groups; and each group can be processed separately. We give a syntactic criterionto identify these queries and prove its sufficiency. We also prove the strong result that everygroup query has an equivalent formulation that satisfies our syntactic criterion. We describea general evaluation technique for group queries; and demonstrate how an optimizer candetermine this plan. We then consider more complex queries whose components are groupqueries with potentially different partitioning attributes. We give two methods to identify …,VLDB,1997,58,6
Adapting materialized views after redefinitions: Techniques and a performance study,Ashish Gupta; Inderpal S Mumick; Jun Rao; Kenneth A Ross,Abstract We consider a variant of the view maintenance problem: How does one keep amaterialized view up-to-date when the view definition itself changes? Can one do betterthan recomputing the view from the base relations? Traditional view maintenance tries tomaintain the materialized view in response to modifications to the base relations; we try to“adapt” the view in response to changes in the view definition. Such techniques are neededfor applications where the user can change queries dynamically and wants to see thechanges in the results fast. Data archaeology; data visualization; and dynamic queries areexamples of such applications. Views defined over the Internet tend to evolve and ourtechnique can be useful for adapting such views. We consider all possible redefinitions ofSQL SELECT-FROM-WHERE-GROUP-BY-HAVING; UNION; and EXCEPT views; and …,Information systems,2001,55,15
Relations with relation names as arguments: Algebra and calculus,Kenneth A Ross,Abstract We consider a version of the relational model in which relation names may appearas arguments of other relations. Allowing relation names as arguments provides enhancedmodelling capabilities; allowing some object-oriented features to be expressed within therelational model. We extend relational algebra with operators for accessing relations; andalso define a relational calculus based on the logic HiLog. We prove two equivalence resultsbetween extensions of relational algebra provide higher expressive power than relationalalgebra on any given database. Finally; we argue that the extensions proposed here arerelatively easy to provide in practice; and should be expressible within modern querylanguages.,Proceedings of the eleventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1992,55,0
Implementing incremental view maintenance in nested data models,Akira Kawaguchi; Daniel Lieuwen; Inderpal Mumick; Kenneth Ross,Abstract Previous research on materialized views has primarily been in the context of flatrelational databases—materialized views defined in terms of one or more flat relations. Thispaper discusses a broader class of view definitions-materialized views defined over anested data model such as the nested relational model or an object-oriented data model. Anattribute of a tuple deriving the view can be a reference (ie; a pointer) to a nested relation;with arbitrary levels of nesting possible. The extended capability of this nested data model;together with materialized views; simplifies data modeling and gives more flexibility. Simpleextensions of standard view maintenance techniques to the nested model would do toomuch work for maintenance: a change in a nested set would re-process the entire nestedset; not just the changed parts. We show how existing incremental maintenance …,International Workshop on Database Programming Languages,1997,53,20
Foundations of aggregation constraints,Kenneth A Ross; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; that is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We formally study the fundamental problem of determining if aconjunction of aggregation constraints is satisfiable; and show that; for many classes ofaggregation constraints; the problem is undecidable. We describe a complete and minimalaxiomatization of aggregation constraints; for the SQL aggregate functions min; max; sum;count and average; over a non-empty; finite multiset on several domains. Thisaxiomatization helps identify classes of aggregation constraints for which the satisfiabilitycheck is efficient. We present a polynomial-time algorithm that directly checks for satisfiabilityof a conjunction of aggregation range constraints over a single multiset; this is a …,Theoretical Computer Science,1998,52,0
Rethinking SIMD vectorization for in-memory databases,Orestis Polychroniou; Arun Raghavan; Kenneth A Ross,Abstract Analytical databases are continuously adapting to the underlying hardware in orderto saturate all sources of parallelism. At the same time; hardware evolves in multipledirections to explore different trade-offs. The MIC architecture; one such example; straysfrom the mainstream CPU design by packing a larger number of simpler cores per chip;relying on SIMD instructions to fill the performance gap. Databases have been attempting toutilize the SIMD capabilities of CPUs. However; mainstream CPUs have only recentlyadopted wider SIMD registers and more advanced instructions; since they do not relyprimarily on SIMD for efficiency. In this paper; we present novel vectorized designs andimplementations of database operators; based on advanced SIMD operations; such asgathers and scatters. We study selections; hash tables; and partitioning; and combine …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,50,7
Reusing invariants: A new strategy for correlated queries,Jun Rao; Kenneth A Ross,Abstract Correlated queries are very common and important in decision support systems.Traditional nested iteration evaluation methods for such queries can be very timeconsuming. When they apply; query rewriting techniques have been shown to be muchmore efficient. But query rewriting is not always possible. When query rewriting does notapply; can we do something better than the traditional nested iteration methods? In thispaper; we propose a new invariant technique to evaluate correlated queries efficiently. Thebasic idea is to recognize the part of the subquery that is not related to the outer referencesand cache the result of that part after its first execution. Later; we can reuse the result andcombine it with the result of the rest of the subquery that is changing for each iteration. Ourtechnique applies to arbitrary correlated subqueries. This paper introduces algorithms to …,ACM SIGMOD Record,1998,47,2
Noodle: A language for declarative querying in an object-oriented database,Inderpal Singh Mumick; Kenneth A Ross,Abstract We present a language Noodle in which to specify declarative queries in an object-oriented database system. The language models object-identity; classes; relations; views;inheritance; complex objects; and methods; in addition to logical rules; and permits powerfulschema querying. Noodle is intended to be used as a query language in an object-orienteddatabase system; with the benefits of declarativeness; namely increased efficiency andreduced programming time. Our work bridges the gap between relational; deductive andobject-oriented databases. Noodle is being implemented in the SWORD database system: adeclarative object-oriented database being built at AT&T Bell Laboratories.,International Conference on Deductive and Object-Oriented Databases,1993,46,7
Cache sensitive search (CSS) tree indexing system and method,*,Cache sensitive search tree (CSS-tree) index structures for providing improved search andlookup performance compared with conventional searching schemes. The CSS-tree indexstructures include a directory tree structure which is stored in an array (216) and serves asan index for a sorted array of elements. The nodes (215) in the directory tree structure maybe of sizes selected to correspond to the cache line size in the computer system utilizing theCSS-tree index structures. Child nodes (213) within the directory tree structure are locatedby performing arithmetic operations on array offsets. Thus; it is not necessary to storeinternal child node pointers; thereby reducing memory storage requirements. In addition; theCSS-tree index structures are organized so that traversing each level in the tree yields gooddata reference locality; and therefore relatively few cache misses. Thus; the CSS-tree …,*,2004,45,1
Data partitioning on chip multiprocessors,John Cieslewicz; Kenneth A Ross,Abstract Partitioning is a key database task. In this paper we explore partitioningperformance on a chip multiprocessor (CMP) that provides a relatively high degree of on-chip thread-level parallelism. It is therefore important to implement the partitioning algorithmto take advantage of the CMP's parallel execution resources. We identify the coordination ofwriting partition output as the main challenge in a parallel partitioning implementation andevaluate four techniques for enabling parallel partitioning. We confirm previous work insingle threaded partitioning that finds L2 cache misses and translation lookaside buffermisses to be important performance issues; but we now add the management of concurrentthreads to this analysis.,Proceedings of the 4th international workshop on Data management on new hardware,2008,44,11
A comprehensive study of main-memory partitioning and its application to large-scale comparison-and radix-sort,Orestis Polychroniou; Kenneth A Ross,Abstract Analytical database systems can achieve high throughput main-memory queryexecution by being aware of the dynamics of highly-parallel modern hardware. Suchsystems rely on partitioning to cluster or divide data into smaller pieces and thus achievebetter parallelism and memory locality. This paper considers a comprehensive collection ofvariants of main-memory partitioning tuned for various layers of the memory hierarchy. Werevisit the pitfalls of in-cache partitioning; and utilizing the crucial performance factors; weintroduce new variants for partitioning out-of-cache. Besides non-in-place variants wherelinear extra space is used; we introduce large-scale in-place variants; and propose NUMA-aware partitioning that guarantees locality on multiple processors. Also; we make rangepartitioning comparably fast with hash or radix; by designing a novel cache-resident index …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,43,15
Modular acyclicity and tail recursion in logic programs,Kenneth A Ross,In this paper we consider two problems. The first is related to the question of acyclicity; ie;the absence of positive and negative loops. In the absence of loops; some desirablesemantic properties hold; and certain program optimizations become possible. The secondproblem is related to the question of tail-recursion elimination for logic programs. By notrepresenting intermediate results of a bottomup computation; substantial savings over magicsets can be realized.,Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1991,43,15
Track join: distributed joins with minimal network traffic,Orestis Polychroniou; Rajkumar Sen; Kenneth A Ross,Abstract Network communication is the slowest component of many operators in distributedparallel databases deployed for large-scale analytics. Whereas considerable work hasfocused on speeding up databases on modern hardware; communication reduction hasreceived less attention. Existing parallel DBMSs rely on algorithms designed for disks withminor modifications for networks. A more complicated algorithm may burden the CPUs; butcould avoid redundant transfers of tuples across the network. We introduce track join; anovel distributed join algorithm that minimizes network traffic by generating an optimaltransfer schedule for each distinct join key. Track join extends the trade-off options betweenCPU and network. Our evaluation based on real and synthetic data shows that track joinadapts to diverse cases and degrees of locality. Considering both network traffic and …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,42,7
Data integration and access,JosÚ Luis Ambite; Yigal Arens; Walter Bourne; Steve Feiner; Luis Gravano; Vasileios Hatzivassiloglou; Eduard Hovy; Judith Klavans; Andrew Philpot; Usha Ramachandran; Kenneth A Ross; Jay Sandhaus; Deniz Sarioz; Rolfe R Schmidt; Cyrus Shahabi; Anurag Singla; Surabhan Temiyabutr; Brian Whitman; Kazi Zaman,Abstract This chapter describes the progress of the Digital Government Research Center intackling the challenges of integrating and accessing the massive amount of statistical andtext data available from government agencies. In particular; we address the issues ofdatabase heterogeneity; size; distribution; and control of terminology. In this chapter weprovide an overview of our results in addressing problems such as (1) ontological mappingsfor terminology standardization;(2) data integration across data bases with high speed queryprocessing; and (3) interfaces for query input and presentation of results. The DGRC is acollaboration between researchers from Columbia University and the Information SciencesInstitute of the University of Southern California employing technology developed at bothlocations; in particular the SENSUS ontology; the SIMS multi-database access planner …,*,2002,42,2
A multi-resolution block storage model for database design,Jingren Zhou; Kenneth A Ross,We propose a new storage model called MBSM (multiresolution block storage model) forlaying out tables on disks. MBSM is intended to speed up operations such as scans that aretypical of data warehouse workloads. Disk blocks are grouped into" super-blocks;" with asingle record stored in a partitioned fashion among the blocks in a superblock. The intentionis that a scan operation that needs to consult only a small number of attributes can accessjust those blocks of each super-block that contain the desired attributes. To achieve goodperformance given the physical characteristics of modern disks; we organize super-blockson the disk into fixed-size" mega-blocks." Within a megablock; blocks of the same type (fromvarious super-blocks) are stored contiguously. We describe the changes needed in aconventional database system to manage tables using such a disk organization. We …,Database Engineering and Applications Symposium; 2003. Proceedings. Seventh International,2003,41,14
Selection conditions in main memory,Kenneth A Ross,Abstract We consider the fundamental operation of applying a compound filtering conditionto a set of records. With large main memories available cheaply; systems may choose tokeep the data entirely in main memory; in order to improve query and/or updateperformance. The design of a data-intensive algorithm in main memory needs to take intoaccount the architectural characteristics of modern processors; just as a disk-based methodneeds to consider the physical characteristics of disk devices. An important architecturalfeature that influences the performance of main memory algorithms is the branchmisprediction penalty. We demonstrate that branch misprediction has a substantial impacton the performance of an algorithm for applying selection conditions. We describe a spaceof" query plans" that are logically equivalent; but differ in terms of performance due to …,ACM Transactions on Database Systems (TODS),2004,40,7
E cient incremental evaluation of queries with aggregation,Raghu Ramakrishnan; Kenneth A Ross; Divesh Srivastava; S Sudarshan,Abstract We present a technique for e ciently evaluating queries on programs withmonotonic aggregation; a class of programs de ned by Ross and Sagiv. Our techniqueconsists of the following components: incremental computation of aggregate functions;incremental xpoint evaluation of monotonic programs and Magic Sets transformation ofmonotonic programs. We also present a formalization of the notion of incrementalcomputation of aggregate functions on a multiset; and upper and lower bounds forincremental computation of a variety of aggregate functions. We describe a proof-theoreticreformulation of the monotonic semantics in terms of computations; following the approachof Beeri et al.; this reformulation greatly simpli es the task of proving the correctness of ouroptimizations.,Proceedings of the International Logic Programming Symposium,1994,39,20
Independence Diagrams: A Technique for Visual Data Mining.,Stefan Berchtold; HV Jagadish; Kenneth A Ross,Abstract An important issue in data mining is the recognition of complex dependenciesbetween attributes. Past techniques for identifying attribute dependence include correlationcoefficients; scatterplots; and equiwidth histograms. These techniques are sensitive tooutliers; and often are not sufficiently informative to identify the kind of attribute dependencepresent. We propose a new approach; which we call independence diagrams. We divideeach attribute into ranges; for each pair of attributes; the combination of these rangesdefines a two-dimensional grid. For each cell of this grid; we store the number of data itemsin it. We display the grid; scaling each attribute axis so that the displayed width of a range isproportional to the total number of data items within that range. The brightness of a cell isproportional to the density of data items in it. As a result; both attributes are independently …,KDD,1998,38,8
Foundations of aggregation constraints,Kenneth A Ross; Divesh Srivastava; Peter J Stuckey; S Sudarshan,Abstract We introduce a new constraint domain; aggregation constraints; which is useful indatabase query languages; and in constraint logic programming languages that incorporateaggregate functions. We study the fundamental problem of checking if a conjunction ofaggregation constraints is solvable; and present undecidability results for many differentclasses of aggregation constraints. We describe a complete and minimal axiomatization ofthe class of aggregation constraints over finite multisets of reals; which permits a naturalreduction from the class of aggregation constraints to the class of mixed integer/real; non-linear arithmetic constraints. We then present a polynomial-time algorithm that directlychecks for solvability of a useful class of aggregation constraints; where the reduction-basedapproach does not lead to efficient checks for solvability.,*,1994,37,4
Parallel buffers for chip multiprocessors,John Cieslewicz; Kenneth A Ross; Ioannis Giannakakis,Abstract Chip multiprocessors (CMPs) present new opportunities for improving databaseperformance on large queries. Because CMPs often share execution; cache; or bandwidthresources among many hardware threads; implementing parallel database operators thatefficiently share these resources is key to maximizing performance. A crucial aspect of thisparallelism is managing concurrent; shared input and output to the parallel operators. In thispaper we propose and evaluate a parallel buffer that enables intra-operator parallelism onCMPs by avoiding contention between hardware threads that need to concurrently read orwrite to the same buffer. The parallel buffer handles parallel input and output coordination aswell as load balancing so individual operators do not need to reimplement that functionality.,Proceedings of the 3rd international workshop on Data management on new hardware,2007,36,10
Thread-level parallel indexing of update intensive moving-object workloads,Darius Šidlauskas; Kenneth A Ross; Christian S Jensen; Simonas Šaltenis,Abstract Modern processors consist of multiple cores that each support parallel processingby multiple physical threads; and they offer ample main-memory storage. This paper studiesthe use of such processors for the processing of update-intensive moving-object workloadsthat contain very frequent updates as well as contain queries. The non-trivial challengeaddressed is that of avoiding contention between long-running queries and frequentupdates. Specifically; the paper proposes a grid-based indexing technique. A static gridindexes a near up-to-date snapshot of the data to support queries; while a live grid supportsupdates. An efficient cloning technique that exploits the memcpy system call is used tomaintain the static grid. An empirical study conducted with three modern processors findsthat very frequent cloning; on the order of tens of milliseconds; is feasible; that the …,International Symposium on Spatial and Temporal Databases,2011,35,15
Evidence for somatic gene conversion and deletion in bipolar disorder; Crohn's disease; coronary artery disease; hypertension; rheumatoid arthritis; type-1 diabetes;...,Kenneth Andrew Ross,During gene conversion; genetic information is transferred unidirectionally between highlyhomologous but non-allelic regions of DNA. While germ-line gene conversion has beenimplicated in the pathogenesis of some diseases; somatic gene conversion has remainedtechnically difficult to investigate on a large scale. A novel analysis technique is proposed fordetecting the signature of somatic gene conversion from SNP microarray data. TheWellcome Trust Case Control Consortium has gathered SNP microarray data for two controlpopulations and cohorts for bipolar disorder (BD); cardiovascular disease (CAD); Crohn'sdisease (CD); hypertension (HT); rheumatoid arthritis (RA); type-1 diabetes (T1D) and type-2diabetes (T2D). Using the new analysis technique; the seven disease cohorts are analyzedto identify cohort-specific SNPs at which conversion is predicted. The quality of the …,BMC medicine,2011,34,7
Academic dishonesty and the Internet,Kenneth A Ross,30 October 2005/Vol. 48; No. 10 COMMUNICATIONS OF THE ACM identifying informationabout the poster; namely that his address was in Miami; FL. However; I wasn't ready to jumpto conclusions. Many of our students in New York might originally be from Florida. Also;several students were taking the class remotely via the Web. Moreover; the poster may nothave been honest in listing Miami as his address. The listing was part of the www.rentacoder. com trust-establishment process in which a contractor might gain someconfidence that a poster is legitimate and the contractor will be paid. My guess was that theMiami location was automatically generated from the address on the credit card the posterused for payment. At this point; all student projects had been submitted; so I tried to find theproblematic one by inspecting the submissions. While I wasn't sure what kind of …,Communications of the ACM,2005,34,7
Refreshing materialized views of a database to maintain consistency with underlying data,*,In a database; a database manager can generate a view; which can be considered as asubset of the database; and which is placed outside the database for use without disturbingthe database. However; if the database changes; the views will not reflect those changes;because the views are separate from the database. To solve this problem; a process called“refreshing” keeps the views consistent with the data within the database. But differentrefreshing approaches are used: some views require immediate refreshing when thedatabase changes; other types can be refreshed at later times; and still other types can berefreshed at different times and intervals. The invention presents a system which keeps dataconsistent among the views and the database; despite the different times of refreshingundertaken.,*,2001,34,22
System and method for performing an efficient join operation on large tables with a small main memory,*,A technique for efficiently joining multiple large tables in a database system with a processorusing a small main memory. The technique utilizes a join index and minimizes the number ofInput/Output operations while maximizing the use of the small main memory through a bufferallocation process. The technique partitions available main memory into buffers and assignsconditions to the buffers to ensure that each buffer will receive a substantially equal amountof data in the join result. The technique then processes each input table separately based onthe assigned conditions and sequentially reads and processes each input table. The outputis vertically fragmented with one fragment for each input table which further allows theindividual processing of each input table. Also described is a method for creating a joinindex if one is not present.,*,1997,34,7
Realizing parallelism in database operations: insights from a massively multithreaded architecture,John Cieslewicz; Jonathan Berry; Bruce Hendrickson; Kenneth A Ross,Abstract A new trend in processor design is increased on-chip support for multithreading inthe form of both chip multiprocessors and simultaneous multithreading. Recent research indatabase systems has begun to explore increased thread-level parallelism made possibleby these new multicore and multithreaded processors. The question of how best to use thisnew technology remains open; particularly as the number of cores per chip and threads percore increase. In this paper we use an existing massively multithreaded architecture; theCray MTA-2; to explore the implications that a larger degree of on-chip multithreading mayhave for parallelism in database operations. We find that parallelism in database operationsis easy to achieve on the MTA-2 and that; with little effort; parallelism can be made to scalelinearly with the number of available processor cores.,Proceedings of the 2nd international workshop on Data management on new hardware,2006,33,20
System and method for performing an efficient join operation,*,A technique for efficiently joining multiple large tables in a database system with a processorusing a small main memory. The technique utilizes a join index and minimizes the number ofInput/Output operations while maximizing the use of the small main memory through a bufferallocation process. Three embodiments of the technique are described all of which use theparallel-merge operation. The first technique; slam-join; is for joining two tables and doesnot require any pre-allocation of buffers to perform the join operation. The second technique;multi-slam-join; is for joining three or more tables and adds the parallel-merge technique toa join technique which partitions memory to be used for an efficient join operation. The thirdtechnique; called parallel-join; processes each input table completely independently usingthe parallel-merge technique. The parallel-merge technique identifies the lowest value …,*,1998,33,15
Automatic contention detection and amelioration for data-intensive operations,John Cieslewicz; Kenneth A Ross; Kyoho Satsumi; Yang Ye,Abstract To take full advantage of the parallelism offered by a multi-core machine; one mustwrite parallel code. Writing parallel code is difficult. Even when one writes correct code;there are numerous performance pitfalls. For example; an unrecognized data hotspot couldmean that all threads effectively serialize their access to the hotspot; and throughput isdramatically reduced. Previous work has demonstrated that database operations suffer fromsuch hotspots when naively implemented to run in parallel on a multi-core processor. In thispaper; we aim to provide a generic framework for performing certain kinds of concurrentdatabase operations in parallel. The formalism is similar to user-defined aggregates andGoogle's MapReduce in that users specify certain functions for parts of the computation thatneed to be performed over large volumes of data. We provide infrastructure that allows …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,31,5
On negation in HiLog,Kenneth A Ross,Abstract The logic HiLog of Chen; Kifer; and Warren has a second order syntax; while itssemantics is first order. HiLog programs with negative literals in the body are considered. Astable-model semantics and a well-founded semantics for this class of program are defined;and it is shown that these semantics generalize the stable-model semantics and the well-founded semantics; respectively; for range-restricted normal programs. A second orderproperty called preservation under extensions is proposed and investigated. Preservationunder extensions ensures that the semantics of a program is not changed when rules havingno symbols in common with the program are appended to the program. It is shown that fornormal programs; domain independence and preservation under extensions are equivalent;while for HiLog programs; preservation under extensions is strictly stronger. Range …,The Journal of Logic Programming,1994,30,11
Ameliorating memory contention of OLAP operators on GPU processors,Evangelia A Sitaridi; Kenneth A Ross,Abstract Implementations of database operators on GPU processors have shown dramaticperformance improvement compared to multicore-CPU implementations. GPU threads cancooperate using shared memory; which is organized in interleaved banks and is fast onlywhen threads read and modify addresses belonging to distinct memory banks. Therefore;data processing operators implemented on a GPU; in addition to contention caused bypopular values; have to deal with a new performance limiting factor: thread serializationwhen accessing values belonging to the same bank. Here; we define the problem of bankand value conflict optimization for data processing operators using the CUDA platform. Toanalyze the impact of these two factors on operator performance we use two databaseoperations: foreignkey join and grouped aggregation. We suggest and evaluate …,Proceedings of the Eighth International Workshop on Data Management on New Hardware,2012,29,7
Querying faceted databases,Kenneth A Ross; Angel Janevski,Abstract Faceted classification allows one to model applications with complex classificationhierarchies using orthogonal dimensions. Recent work has examined the use of facetedclassification for browsing and search. In this paper; we go further by developing a generalquery language; called the entity algebra; for hierarchically classified data. The entityalgebra is compositional; with query inputs and outputs being sets of entities. Our languagehas linear data complexity in terms of space and quadratic data complexity in terms of time.We compare the expressive power of the entity algebra with relational algebra. We alsodescribe an end-to-end query system based on the language in the context of anarcheological database.,International Workshop on Semantic Web and Databases,2004,29,7
Temperature-aware buffered caching for solid state storage,*,A system and method for managing a cache includes monitoring a temperature of regionson a secondary storage based on a cumulative cost to access pages from each region of thesecondary storage. Similar temperature pages are grouped in logical blocks. Data is writtento a cache in a logical block granularity by overwriting cooler blocks with hotter blocks.,*,2012,28,7
Efficient index compression in DB2 LUW,Bishwaranjan Bhattacharjee; Lipyeow Lim; Timothy Malkemus; George Mihaila; Kenneth Ross; Sherman Lau; Cathy McArthur; Zoltan Toth; Reza Sherkat,Abstract In database systems; the cost of data storage and retrieval are importantcomponents of the total cost and response time of the system. A popular mechanism toreduce the storage footprint is by compressing the data residing in tables and indexes.Compressing indexes efficiently; while maintaining response time requirements; is known tobe challenging. This is especially true when designing for a workload spectrum coveringboth data warehousing and transaction processing environments. DB2 Linux; UNIX;Windows (LUW) recently introduced index compression for use in both environments. Thisuses techniques that are able to compress index data efficiently while incurring virtually noperformance penalty for query processing. On the contrary; for certain operations; theperformance is actually better. In this paper; we detail the design of index compression in …,Proceedings of the VLDB Endowment,2009,27,7
Cache-conscious buffering for database operators with state,John Cieslewicz; William Mee; Kenneth A Ross,Abstract Database processes must be cache-efficient to effectively utilize modern hardware.In this paper; we analyze the importance of temporal locality and the resultant cachebehavior in scheduling database operators for in-memory; block oriented query processing.We demonstrate how the overall performance of a workload of multiple database operatorsis strongly dependent on how they are interleaved with each other. Longer time slicescombined with temporal locality within an operator amortize the effects of the initialcompulsory cache misses needed to load the operator's state; such as a hash table; into thecache. Though running an operator to completion over all of its input results in the greatestamortization of cache misses; this is typically infeasible because of the large intermediatestorage requirement to materialize all input tuples to an operator. We show …,Proceedings of the Fifth International Workshop on Data Management on New Hardware,2009,25,15
Database optimizations for modern hardware,John Cieslewicz; Kenneth A Ross,Databases are an important workload for modern commodity microarchitectures. Achievingthe best performance requires that careful attention be paid to the underlying architecture;including instruction and data cache usage; data layout; branch prediction; andmultithreading. Specialized commodity microarchitectures; such as graphics cards andnetwork processors; have also been investigated as effective query coprocessors. Thispaper presents a survey of recent architecture-sensitive database research. The insightsgained from optimizing database performance on modern microarchitectures are alsoapplicable to other domains; particularly those that are similarly data intensive.,Proceedings of the IEEE,2008,25,10
Modeling the performance of algorithms on flash memory devices,Kenneth A Ross,Abstract NAND flash memory is fast becoming popular as a component of large scalestorage devices. For workloads requiring many random I/Os; flash devices can provide twoorders of magnitude increased performance relative to magnetic disks. Flash memory hassome unusual characteristics. In particular; general updates require a page write; whileupdates of 1 bits to 0 bits can be done in-place. In order to measure how well algorithmsperform on such a device; we propose the" EWOM" model for analyzing algorithms on flashmemory devices. We introduce flash-aware algorithms for counting; listmanagement; and B-trees; and analyze them using the EWOM model. This analysis shows that one can use theincremental 1-to-0 update properties of flash memory in interesting ways to reduce therequired number of page-write operations.,Proceedings of the 4th international workshop on Data management on new hardware,2008,24,1
Partitioned optimization of complex queries,Damianos Chatziantoniou; Kenneth A Ross,Abstract Performing complex analysis on top of massive data stores is essential to mostmodern enterprises and organizations and requires significant aggregation over differentattribute sets (dimensions) of the participating relations. Such queries may take hours ordays; a time period unacceptable in most cases. As a result; it is important to study thesequeries and identify special frequent cases that can be evaluated with specializedalgorithms. Understanding complex aggregate queries leads to better execution plans and;consequently; performance. The idea of partitioning is fundamental and central in aggregatequeries. This concept can be used to define a class of queries called group queries. Themain characteristic of a group query is that it can be evaluated in a partitioned (orgroupwise) fashion; ie the underlying relation (s) can be partitioned (based on a set of …,Information systems,2007,24,7
Buffered Bloom Filters on Solid State Storage.,Mustafa Canim; George A Mihaila; Bishwaranjan Bhattacharjee; Christian A Lang; Kenneth A Ross,ABSTRACT Bloom Filters are widely used in many applications including databasemanagement systems. With a certain allowable error rate; this data structure provides anefficient solution for membership queries. The error rate is inversely proportional to the sizeof the Bloom filter. Currently; Bloom filters are stored in main memory because the lowlocality of operations makes them impractical on secondary storage. In multi-user databasemanagement systems; where there is a high contention for the shared memory heap; thelimited memory available for allocating a Bloom filter may cause a high rate of falsepositives. In this paper we are proposing a technique to reduce the memory requirement forBloom filters with the help of solid state storage devices (SSD). By using a limited memoryspace for buffering the read/write requests; we can afford a larger SSD space for the …,ADMS@ VLDB,2010,23,20
Using associative memory to perform database operations,*,A system and method for employing associative memory for the storing the data of arelational database. The system and method of the present invention optionally includeadditional hardware components in order for the Associative memory to be usable for therelational database; as CAM (content associated memory).,*,2004,22,7
High throughput heavy hitter aggregation for modern SIMD processors,Orestis Polychroniou; Kenneth A Ross,Abstract Heavy hitters are data items that occur at high frequency in a data set. They areamong the most important items for an organization to summarize and understand duringanalytical processing. In data sets with sufficient skew; the number of heavy hitters can berelatively small. We take advantage of this small footprint to compute aggregate functions forthe heavy hitters in fast cache memory in a single pass. We design cache-resident; shared-nothing structures that hold only the most frequent elements. Our algorithm works in threephases. It first samples and picks heavy hitter candidates. It then builds a hash table andcomputes the exact aggregates of these elements. Finally; a validation step identifies thetrue heavy hitters from among the candidates.,Proceedings of the Ninth International Workshop on Data Management on New Hardware,2013,21,7
Using asymmetric memory,*,In one illustrative embodiment; a computer implemented method using asymmetric memorymanagement is provided. The computer implemented method receives a request; containinga search key; to access an array of records in the asymmetric memory; wherein the array hasa sorted prefix portion and an unsorted append portion; the append portion alternativelycomprising a linked-list; and responsive to a determination that the request is an insertrequest; inserts the record in the request in arrival order in the unsorted append portion toform a newly inserted record. Responsive to a determination that the newly inserted recordcompletes the group of records; stores an index; in sorted order; for the group of records.,*,2011,21,22
Challenges and Opportunities with Big Data,Elisa Bertino; Philip Bernstein; Divyakant Agrawal; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jadadish; Alexandros Labrinidis; Sam Madden; Yannis Papokonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of" Big Data". While the promise of Bid Data isreal-for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009-there is currently a wide gap between its potential and its realization.,*,2011,21,1
Splash tables: an efficient hash scheme for processors,*,A computer implemented method; data processing system; and computer usable programcode are provided for storing data items in a computer. A plurality of hash functions of datavalues in a data item are computed. A corresponding memory location is determined for oneof the plurality of hash functions. The data item and a key portion and a payload portion of alldata items are stored contiguously within the memory location. Also provided for areretrieving data items in a computer. A plurality of hash functions of data values in a probekey are computed. A corresponding memory location is determined for each of the pluralityof hash functions. Data items in each memory location are examined to determine a matchwith the probe key. Responsive to a match; a payload of the matching stored data item isreturned. All of the steps are performed free of conditional branch instructions.,*,2010,21,7
Optimal splitters for database partitioning with size bounds,Kenneth A Ross; John Cieslewicz,Abstract Partitioning is an important step in several database algorithms; including sorting;aggregation; and joins. Partitioning is also fundamental for dividing work into equal-sized (orbalanced) parallel subtasks. In this paper; we aim to find; materialize and maintain a set ofpartitioning elements (splitters) for a data set. Unlike traditional partitioning elements; oursplitters define both inequality and equality partitions; which allows us to bound the size ofthe inequality partitions. We provide an algorithm for determining an optimal set of splittersfrom a sorted data set and show that it has time complexity O (k lg 2 N); where k is thenumber of splitters requested and N is the size of the data set. We show how the algorithmcan be extended to pairs of tables; so that joins can be partitioned into work units that havebalanced cost. We demonstrate experimentally (a) that finding the optimal set of splitters …,Proceedings of the 12th International Conference on Database Theory,2009,21,1
Serving datacube tuples from main memory,Kenneth A Ross; Kazi A Zaman,Existing datacube precomputation schemes materialize selected datacube tuples on disk;choosing the most beneficial cuboids (ie; combinations of dimensions) to materialize given aspace limit. However in the context of a data-warehouse receiving frequent" append"updates to the database; the cost of keeping these disk-resident cuboids up-to-date can behigh. In this paper we propose a main memory based framework which provides rapidresponse to queries and requires considerably less maintenance cost than a disk basedscheme in an append-only environment. For a given datacube query; we first look among aset of previously materialized tuples for a direct answer. If not found; we use a hash basedscheme reminiscent of partial match retrieval to rapidly compute the answer to the queryfrom the finest-level data stored in a special in-memory data structure. Our approach is …,Scientific and Statistical Database Management; 2000. Proceedings. 12th International Conference on,2000,21,11
Making updates disk-I/O friendly using SSDs,Mohammad Sadoghi; Kenneth A Ross; Mustafa Canim; Bishwaranjan Bhattacharjee,Abstract Multiversion databases store both current and historical data. Rows are typicallyannotated with timestamps representing the period when the row is/was valid. We developnovel techniques for reducing index maintenance in multiversion databases; so that indexescan be used effectively for analytical queries over current data without being a heavy burdenon transaction throughput. To achieve this end; we re-design persistent index data structuresin the storage hierarchy to employ an extra level of indirection. The indirection level is storedon solid state disks that can support very fast random I/Os; so that traversing the extra levelof indirection incurs a relatively small overhead. The extra level of indirection dramaticallyreduces the number of magnetic disk I/Os that are needed for index updates; and localizesmaintenance to indexes on updated attributes. Further; we batch insertions within the …,Proceedings of the VLDB Endowment,2013,20,15
Cost-based unbalanced R-trees,Kenneth A Ross; Inga Sitzmann; Peter J Stuckey,Cost-based unbalanced R-trees (CUR-trees) are a cost-function-based data structure forspatial data. CUR-trees are constructed specifically to improve the evaluation of intersectionqueries; the most basic selection query in an R-tree. A CUR-tree is built taking into account agiven query distribution for the queries and a cost model for their execution. Depending onthe expected frequency of access; objects or subtrees are stored higher up in the tree. Aftereach insertion in the tree; local reorganizations of a node and its children have theirexpected query cost evaluated; and a reorganization is performed if this is beneficial. Nostrict balancing of the trees applies; allowing the tree to unfold solely based on the result ofthe cost evaluation. We present our cost-based approach and describe the evaluation andreorganization operations based on the cost function. We present a cost model for in …,Scientific and Statistical Database Management; 2001. SSDBM 2001. Proceedings. Thirteenth International Conference on,2001,19,7
Optimizing selections over datacubes,Kenneth A Ross; Kazi A Zaman,Datacube queries compute aggregates over database relations at a variety of granularities.Often one wants only datacube output tuples whose aggregate value satisfies a certaincondition; such as exceeding a given threshold. We develop algorithms for processing adatacube query using the selection condition internally during the computation. Thus; wecan safely prune parts of the computation and end up with a more efficient computation ofthe answer Our first technique; called" specialization"; uses the fact that a tuple in thedatacube does not meet the given threshold to infer that all finer level aggregates cannotmeet the threshold. Our second technique is called" generalization"; and applies in the casewhere the actual value of the aggregate is not needed in the output; but used just tocompare with the threshold. We demonstrate the efficiency of these techniques by …,Scientific and Statistical Database Management; 2000. Proceedings. 12th International Conference on,2000,19,7
Vectorized Bloom filters for advanced SIMD processors,Orestis Polychroniou; Kenneth A Ross,Abstract Analytics are at the core of many business intelligence tasks. Efficient queryexecution is facilitated by advanced hardware features; such as multi-core parallelism;shared-nothing low-latency caches; and SIMD vector instructions. Only recently; the SIMDcapabilities of mainstream hardware have been augmented with wider vectors and non-contiguous loads termed gathers. While analytical DBMSs minimize the use of indexes infavor of scans based on sequential memory accesses; some data structures remain crucial.The Bloom filter; one such example; is the most efficient structure for filtering tuples based ontheir existence in a set and its performance is critical when joining tables with vastly differentcardinalities. We introduce a vectorized implementation for probing Bloom filters based ongathers that eliminates conditional control flow and is independent of the SIMD length …,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,17,15
Enhancing recovery using an SSD buffer pool extension,Bishwaranjan Bhattacharjee; Kenneth A Ross; Christian Lang; George A Mihaila; Mohammad Banikazemi,Abstract Recent advances in solid state technology have led to the introduction of solid statedrives (SSDs). Today's SSDs store data persistently using NAND flash memory and supportgood random IO performance. Current work in exploiting flash in database systems hasprimarily focused on using its random IO capability for second level bufferpools below mainmemory. There has not been much emphasis on exploiting its persistence. In this paper; wedescribe a mechanism extending our previous work on a SSD Bufferpool on a DB2 LUWprototype; to exploit the SSD persistence for recovery and normal restart. We demonstratesignificantly shorter recovery times; and improved performance immediately after recoverycompletes. We quantify the overhead of supporting recovery and show that the overhead isminimal.,Proceedings of the Seventh International Workshop on Data Management on New Hardware,2011,17,7
A faceted query engine applied to archaeology,Kenneth A Ross; Angel Janevski; Julia Stoyanovich,Abstract In this demonstration; we describe a system for storing and querying facetedhierarchies. We have developed a general faceted domain model and a query language forhierarchically classified data. We present here the use of our system on two realarchaeological datasets containing thousands of artifacts. Our system is a sharable;evolvable resource that can provide global access to sizeable datasets in queriable format;and can serve as a valuable tool for data analysis and research in many applicationdomains.,Proceedings of the 31st international conference on Very large data bases,2005,17,15
Faster joins; self-joins and multi-way joins using join indices,Hui Lei; Kenneth A Ross,Abstract We propose a new algorithm; called Stripe-join; for performing a join given a joinindex. Stripe-join is inspired by an algorithm called 'Jive-join'developed by Li and Ross.Stripe-join makes a single sequential pass through each input relation; in addition to onepass through the join index and two passes through a set of temporary files that containtuple identifiers but no input tuples. Stripe-join performs this efficiently even when the inputrelations are much larger than main memory; as long as the number of blocks in mainmemory is of the order of the square root of the number of blocks in the participatingrelations. Stripe-join is particularly efficient for self-joins. To our knowledge; Stripe-join is thefirst algorithm that; given a join index and a relation significantly larger than main memory;can perform a self-join with just a single pass over the input relation and without storing …,Data & knowledge engineering,1998,17,8
The semantics of deductive databases,Kenneth Andrew Ross,Abstract The major advantage of a deductive database is the ability to write queries andprograms declaratively; using both facts and simple logical rules to represent knowledge.Declarativeness makes queries easier to write; and thus reduces the time taken and theprogramming skills needed to specify a query. Two important questions arise: What shouldthe semantics of such a deductive database language be; and how should declarativequeries be answered e ciently? We address both of these questions. The question ofsemantics becomes complicated when one needs some sort of negation by default. Forexample; the absence of a fact stating that a student is in a class should allow us to concludethat the student is not in that class. Several researchers have previously looked at thisproblem and have tried to come up with formal de nitions of an intuitively satisfying …,*,1991,17,7
Symmetric relations and cardinality-bounded multisets in database systems,Kenneth A Ross; Julia Stoyanovich,Abstract In a binary symmetric relationship; A is related to B if and only if B is related to A.Symmetric relationships between k participating entities can be represented as multisets ofcardinality k. Cardinality-bounded multisets are natural in several real-world applications.Conventional representations in relational databases suffer from several consistency andperformance problems. We argue that the database system itself should provide nativesupport for cardinality-bounded multisets. We provide techniques to be implemented by thedatabase engine that avoid the drawbacks; and allow a schema designer to simply declarea table to be symmetric in certain attributes. We describe a compact data structure; andupdate methods for the structure. We describe an algebraic symmetric closure operator; andshow how it can be moved around in a query plan during query optimization in order to …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,15,7
Storage Class Memory Aware Data Management.,Bishwaranjan Bhattacharjee; Mustafa Canim; Christian A Lang; George A Mihaila; Kenneth A Ross,Abstract Storage Class Memory (SCM) is here to stay. It has characteristics that place it in aclass apart both from main memory and hard disk drives. Software and systems;architectures and algorithms need to be revisited to extract the maximum benefit from SCM.In this paper; we describe work that is being done in the area of Storage Class Memoryaware Data Management at IBM. We specifically cover the challenges in placement ofobjects in storage and memory systems which have NAND flash (one kind of SCM) in ahierarchy or in the same level with other storage devices. We also focus on the challenges ofadapting data structures which are inherently main memory based to work out of a memoryhierarchy consisting of DRAM and flash. We describe how these could be addressed for apopular main memory data structure; namely the Bloom filter.,IEEE Data Eng. Bull.,2010,14,1
Digital media indexing on the Cell processor,Lurng-Kuo Liu; Qiang Liu; Apostol Natsev; Kenneth A Ross; John R Smith; Ana Lucia Varbanescu,We present a case study of developing a digital media indexing application; code-namedMARVEL; on the STI cell broadband engine (CBE) processor. There are two aspects of thetarget application that require significant computing power: image analysis for featureextraction; and support vector machine (SVM) based pattern classification for conceptdetection. We discuss the mapping of a large application like MARVEL onto a multicoreprocessor; and show how feature extraction and concept detection can be implemented onthe CBE. We discuss how the synergistic processing units of a CBE can be used to gaindramatic performance improvements. The empirical results of our experiments; conductedon a Cell blade running at 3.2 GHz; show that the CBE provides a significant performancespeed-up in our digital media indexing application.,Multimedia and Expo; 2007 IEEE International Conference on,2007,14,4
A Syntactic Stratification Condition Using Constraints.,Kenneth A Ross,Abstract Strati cation conditions for logic programs aim to ensure a two-valued semantics byrestricting the class of allowable programs. Previous strati cation conditions su er from one oftwo problems. Some (such as modular strati cation and weak strati cation) are semantic; andcannot be recognized without examining the facts in addition to the rules of the program.Others (such as strati cation and local stratication) are syntactic; but do not allow a number ofuseful examples. A nonsemantic version of modular strati cation; ie; whether a program ismodularly strati ed for all extensional databases; is shown to be undecidable. We propose acondition that generalizes local strati cation; that ensures a two-valued well-founded model;and that can be syntactically determined from the rules and some constraints on the facts inthe program. We call this condition\Universal Constraint Strati cation." While not every …,ILPS,1994,14,7
Better semijoins using tuple bit-vectors,Zhe Li; Kenneth A Ross,Abstract This paper presents the idea of\tuple bit-vectors" for distributed query processing.Using tuple bit-vectors; a new two-way semijoin operator called 2SJ++ that enhances thesemijoin with an essentially\free" backward reduction capability is proposed. We explore indetail the bene ts and costs of 2SJ++ compared with other semijoin variants; and its e ect ondistributed query processing performance. We then focus on one particular distributed queryprocessing algorithm; called the\one-shot" algorithm. We modify the one-shot algorithm byusing 2SJ++ and demonstrate the improvements achieved in network transmission costcompared with the original one-shot technique. We use this improvement to demonstratethat equipped with the 2SJ++ technique; one can improve the performance of distributedquery processing algorithms signi cantly without adding much complexity to the …,*,1994,14,7
Optimizing select conditions on GPUs,Evangelia A Sitaridi; Kenneth A Ross,Abstract Implementations of data processing operators on GPU processors have achievedsignificant performance improvements over their multicore CPU counterparts. To achievemaximum performance; database operator implementations must take into considerationspecial features of GPU architectures. A crucial difference is that the unit of execution is agroup (" warp") of threads; 32 threads in our target architecture; as opposed to a singlethread for CPUs. In the presence of branches; threads in a warp have to follow the sameexecution path; if some threads diverge then different paths are serialized. Additionally;similarly to CPUs; branches degrade the efficiency of instruction scheduling. Here; we studyconjunctive selection queries where branching hurts performance. We compute the optimalexecution plan for a conjunctive query; taking branch penalties into account and consider …,Proceedings of the Ninth International Workshop on Data Management on New Hardware,2013,13,2
Path processing using solid state storage,Manos Athanassoulis; Bishwaranjan Bhattacharjee; Mustafa Canim; Kenneth A Ross,ABSTRACT Recent advances in solid state technology have led to the introduction of SolidState Drives (SSDs). Todays SSDs store data persistently using NAND flash memory. WhileSSDs are more expensive than hard disks when measured in dollars per gigabyte; they aresignificantly cheaper when measured in dollars per random I/O per second. Additionalstorage technologies are under development; Phase Change Memory (PCM) being the nextone to enter the marketplace. PCM is nonvolatile; it can be byte-addressable; and in futureMulti Level Cell (MLC) devices; PCM is expected to be denser than DRAM. PCM has lowerread and write latency compared to NAND flash memory; and it can endure orders ofmagnitude more write cycles before wearing out. Recent research has shown that solid statedevices can be particularly beneficial for latency-bound applications involving dependent …,Proceedings of the 3rd International Workshop on Accelerating Data Management Systems Using Modern Processor and Storage Architectures (ADMS 2012),2012,13,7
GPU-accelerated string matching for database applications,Evangelia A Sitaridi; Kenneth A Ross,Abstract Implementations of relational operators on GPU processors have resulted in orderof magnitude speedups compared to their multicore CPU counterparts. Here we focus on theefficient implementation of string matching operators common in SQL queries. Due todifferent architectural features the optimal algorithm for CPUs might be suboptimal for GPUs.GPUs achieve high memory bandwidth by running thousands of threads; so it is not feasibleto keep the working set of all threads in the cache in a naive implementation. In GPUs theunit of execution is a group of threads and in the presence of loops and branches; threads ina group have to follow the same execution path; if some threads diverge; then different pathsare serialized. We study the cache memory efficiency of single-and multi-pattern stringmatching algorithms for conventional and pivoted string layouts in the GPU memory. We …,The VLDB Journal,2016,12,7
Reducing database locking contention through multi-version concurrency,Mohammad Sadoghi; Mustafa Canim; Bishwaranjan Bhattacharjee; Fabian Nagel; Kenneth A Ross,Abstract In multi-version databases; updates and deletions of records by transactionsrequire appending a new record to tables rather than performing in-place updates. Thismechanism incurs non-negligible performance overhead in the presence of multiple indexeson a table; where changes need to be propagated to all indexes. Additionally; anuncommitted record update will block other active transactions from using the index to fetchthe most recently committed values for the updated record. In general; in order to supportsnapshot isolation and/or multi-version concurrency; either each active transaction is forcedto search a database temporary area (eg; roll-back segments) to fetch old values of desiredrecords; or each transaction is forced to scan the entire table to find the older versions of therecord in a multi-version database (in the absence of specialized temporal indexes). In …,Proceedings of the VLDB Endowment,2014,12,10
Column-oriented query processing for row stores,Amr El-Helw; Kenneth A Ross; Bishwaranjan Bhattacharjee; Christian A Lang; George A Mihaila,Abstract Column-oriented DBMSs have gained increasing interest due to their superiorperformance for analytical workloads. Prior efforts tried to determine the possibility ofsimulating the query processing techniques of column-oriented systems in row-orienteddatabases; in a hope to improve their performance; especially for OLAP and datawarehousing applications. In this paper; we show that column-oriented query processingcan significantly improve the performance of row-oriented DBMSs. We introduce newoperators that take into account the unique characteristics of data obtained from indexes;and exploit new technologies such as flash SSDs and multi-core processors to boost theperformance. We demonstrate our approach with an experimental study using a prototypebuilt on a commercial row-oriented DBMS.,Proceedings of the ACM 14th international workshop on Data Warehousing and OLAP,2011,12,3
Semantic ranking and result visualization for life sciences publications,Julia Stoyanovich; William Mee; Kenneth A Ross,An ever-increasing amount of data and semantic knowledge in the domain of life sciences isbringing about new data management challenges. In this paper we focus on adding thesemantic dimension to literature search; a central task in scientific research. We focus ourattention on PubMed; the most significant bibliographic source in life sciences; and exploreways to use high-quality semantic annotations from the MeSH vocabulary to rank searchresults. We start by developing several families of ranking functions that relate a searchquery to a document's annotations. We then propose an efficient adaptive rankingmechanism for each of the families. We also describe a two-dimensional Skyline-basedvisualization that can be used in conjunction with the ranking to further improve the user'sinteraction with the system; and demonstrate how such Skylines can be computed …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,12,11
An effective strategy for porting c++ applications on cell,Ana Lucia Varbanescu; Henk Sips; Kenneth A Ross; Qiang Liu; Lurng-Kuo Liu; Apostol Natsev; John R Smith,In this paper we present a solution for efficient porting of sequential C++ applications on theCell BE processor. We present our step-by-step approach; focusing on its generality; weprovide a set of code templates and optimization guidelines to support the porting; and weinclude a set of equations to estimate the performance gain of the new application. As acase-study; we show the use of our solution on a multimedia content analysis application;named MARVEL. The results of our experiments with MARVEL prove the significantperformance increase in favor of the application running on Cell when compared with thereference implementation.,Parallel Processing; 2007. ICPP 2007. International Conference on,2007,12,7
Coherent somatic mutation in autoimmune disease,Kenneth Andrew Ross,Background Many aspects of autoimmune disease are not well understood; including thespecificities of autoimmune targets; and patterns of co-morbidity and cross-heritability acrossdiseases. Prior work has provided evidence that somatic mutation caused by geneconversion and deletion at segmentally duplicated loci is relevant to several diseases.Simple tandem repeat (STR) sequence is highly mutable; both somatically and in the germ-line; and somatic STR mutations are observed under inflammation. Results Protein-codinggenes spanning STRs having markers of mutability; including germ-line variability; high totallength; repeat count and/or repeat similarity; are evaluated in the context of autoimmunity.For the initiation of autoimmune disease; antigens whose autoantibodies are the firstobserved in a disease; termed primary autoantigens; are informative. Three primary …,PLoS One,2014,10,7
QueryScope: visualizing queries for repeatable database tuning,Ling Hu; Kenneth A Ross; Yuan-Chi Chang; Christian A Lang; Donghui Zhang,Abstract Reading and perceiving complex SQL queries has been a time consuming task intraditional database applications for decades. When it comes to decision support systemswith automatically generated and sometimes highly nested SQL queries; humanunderstanding or tuning of these workloads becomes even more challenging. Thisdemonstration explores visualization methods to represent queries as graphs. Wedeveloped the QueryScope tool to help visualize and understand critical elements of aquery; thereby cutting down the learning curve. We show how the tool allows the user to drilldown on particular queries or to find similarly structured queries that may exhibit similartuning opportunities. The queries shown in the demonstration are taken from real tuningengagements.,Proceedings of the VLDB Endowment,2008,10,8
Reducing database locking contention using multi-version data record concurrency control,*,Managing a multi-version data record database is provided. A mapping is maintainedbetween a logical record identifier and committed and uncommitted physical recordidentifiers corresponding to data records using an indirection mapping table. Entries areupdated within an index to point to the logical record identifier instead of the committed anduncommitted physical record identifiers. The committed physical record identifiercorresponding to a data record is read from the indirection mapping table to access acommitted version of the data record while a writer is modifying the data record to preventthe writer from blocking a reader. An uncommitted physical record identifier corresponding tothe data record is written in the indirection mapping table to insert a new uncommittedversion of the data record within a data table while the reader is reading the committed …,*,2016,9,15
Compressing a multi-version database,*,Managing a multi-version database is provided. A logical record identifier to physical recordrow identifier indirection mapping table on a solid-state storage device is extended toinclude a plurality of delta blocks. A delta block within the plurality of delta blocks ismaintained for each primary key in a plurality of primary keys associated with a data table ona magnetic hard disk storage device.,*,2016,9,15
Implementing latency-insensitive dataflow blocks,Bingyi Cao; Kenneth A Ross; Martha A Kim; Stephen A Edwards,To simplify the implementation of dataflow systems in hardware; we present a technique fordesigning latency-insensitive dataflow blocks. We provide buffering with backpressure;resulting in blocks that compose into deep; high-speed pipelines without introducing longcombinational paths. Our input and output buffers are easy to assemble into simple unit-ratedataflow blocks; arbiters; and blocks for Kahn networks. We prove the correctness of ourbuffers; illustrate how they can be used to assemble arbitrary dataflow blocks; discusspitfalls; and present experimental results that suggest our pipelines can operate at a highclock rate independent of length.,Formal Methods and Models for Codesign (MEMOCODE); 2015 ACM/IEEE International Conference on,2015,8,7
Optimizing read convoys in main-memory query processing,Kenneth A Ross,Abstract Concurrent read-only scans of memory-resident fact tables can form convoys; whichgenerally help performance because cache misses are amortized over several members ofthe convoy. Nevertheless; we identify two performance hazards for such convoys. Onehazard is underutilization of the memory bandwidth because all members of the convoy hitthe same cache lines at the same time; rather than reading several different linesconcurrently. The other hazard is a form of interference that occurs on the Sun Niagara T1and T2 machines under certain workloads. We propose solutions to these hazards;including a local shuffle method that reduces interference; preserves the beneficial aspectsof convoy behavior; and increases the effective bandwidth by allowing different members ofa convoy to concurrently access different cache lines. We provide experimental validation …,Proceedings of the Sixth International Workshop on Data Management on New Hardware,2010,8,7
Tail recursion elimination in deductive databases,Kenneth A Ross,Abstract We consider an optimization technique for deductive and relational databases. Theoptimization technique is an extension of the magic templates rewriting; and it can improvethe performance of query evaluation by not materializing the extension of intermediateviews. Standard relational techniques; such as unfolding embedded view definitions; do notapply to recursively defined views; and so alternative techniques are necessary. Wedemonstrate the correctness of our rewriting. We define a class of “nonrepeating” viewdefinitions; and show that for certain queries our rewriting performs at least as well as magictemplates on nonrepeating views; and often much better. A syntactically recognizableproperty; called “weak right-linearity”; is proposed. Weak right-linearity is a sufficientcondition for nonrepetition and is more general than right-linearity. Our technique gives …,ACM Transactions on Database Systems (TODS),1996,8,7
Exploiting SSDs in operational multiversion databases,Mohammad Sadoghi; Kenneth A Ross; Mustafa Canim; Bishwaranjan Bhattacharjee,Abstract Multiversion databases store both current and historical data. Rows are typicallyannotated with timestamps representing the period when the row is/was valid. We developnovel techniques to reduce index maintenance in multiversion databases; so that indexescan be used effectively for analytical queries over current data without being a heavy burdenon transaction throughput. To achieve this end; we re-design persistent index data structuresin the storage hierarchy to employ an extra level of indirection. The indirection level is storedon solid-state disks that can support very fast random I/Os; so that traversing the extra levelof indirection incurs a relatively small overhead. The extra level of indirection dramaticallyreduces the number of magnetic disk I/Os that are needed for index updates and localizesmaintenance to indexes on updated attributes. Additionally; we batch insertions within the …,The VLDB Journal,2016,7,7
Hardware partitioning for big data analytics,Lisa Wu; Raymond J Barker; Martha A Kim; Kenneth A Ross,Targeted deployment of hardware accelerators can improve the throughput and energyefficiency of large-scale data processing. Data partitioning is a critical operation formanipulating large datasets and is often the limiting factor in database performance. Ahardware-software streaming framework offers a seamless execution environment forstreaming accelerators such as the Hardware-Accelerated Range Partitioner (HARP).Together; the streaming framework and HARP provide an order of magnitude improvementin partitioning and energy performance.,IEEE Micro,2014,7,7
Cache-conscious query processing,Kenneth A Ross,Query processing algorithms are designed to efficiently exploit the available cache units inthe memory hierarchy. Cache-conscious algorithms typically employ knowledge ofarchitectural parameters such as cache size and latency. This knowledge can be used toensure that the algorithms have good temporal and/or spatial locality on the target platform.,*,2009,7,7
Glue-Nail: A Deductive Database System,MA Derr; G Phipps; KA Ross,*,Proc. Int'l Conf. ACM on Management of Data,1991,7
Architecture Sensitive Database Design: Examples from the Columbia Group.,Kenneth A Ross; John Cieslewicz; Jun Rao; Jingren Zhou,In this article; we discuss how different aspects of modern CPU architecture can beeffectively used by database systems. While we shall try to cover most important aspects ofmodern architectures; we shall not aim to provide an exhaustive survey of the literature.Rather; we will use examples from our own recent work to illustrate the principles involved.While there is related work on using other components of modern machines such asgraphics cards and intelligent disk controllers for database work; we limit our discussionhere to just a single CPU and the memory hierarchy (excluding disks). In recent years theprimary focus of database performance research has fundamentally shifted from disks tomain memory. Systems with a very large main memory allow a database's working set to fitin RAM. At the same time processors have continued to double in performance roughly …,IEEE Data Eng. Bull.,2005,6,12
Adapting Materialized Views after Redefinitions: Techniques and a Performance Study,Ashish Gupta; Inderpal S Mumick; Jun Rao; Kenneth A Ross,Abstract We consider a variant of the view maintenance problem: How does one keep amaterialized view up-to-date when the view definition itself changes? Can one do betterthan recomputing the view from the base relations? Traditional view maintenance tries tomaintain the materialized view in response to modifications to the base relations we tryto\adapt" the view in response to changes in the view definition.,*,1997,6,7
Energy analysis of hardware and software range partitioning,Lisa Wu; Orestis Polychroniou; Raymond J Barker; Martha A Kim; Kenneth A Ross,Abstract Data partitioning is a critical operation for manipulating large datasets because itsubdivides tasks into pieces that are more amenable to efficient processing. It is often thelimiting factor in database performance and represents a significant fraction of the overallruntime of large data queries. This article measures the performance and energy of state-of-the-art software partitioners; and describes and evaluates a hardware range partitioner thatfurther improves efficiency. The software implementation is broken into two phases; allowingseparate analysis of the partition function computation and data shuffling costs. Althoughrange partitioning is commonly thought to be more expensive than simpler strategies suchas hash partitioning; our measurements indicate that careful data movement andoptimization of the partition function can allow it to approach the throughput and energy …,ACM Transactions on Computer Systems (TOCS),2014,5,7
Efficient lightweight compression alongside fast scans,Orestis Polychroniou; Kenneth A Ross,Abstract The increasing main-memory capacity has allowed query execution to occurprimarily in main memory. Database systems employ compression; not only to fit the data inmain memory; but also to address the memory bandwidth bottleneck. Lightweightcompression schemes focus on efficiency over compression rate and allow query operatorsto process the data in compressed form. For instance; dictionary compression keeps thedistinct column values in a sorted dictionary and stores the values as index codes with theminimum number of bits. Packing the bits of each code contiguously; namely horizontal bitpacking; has been optimized by using SIMD instructions for unpacking and by evaluatingpredicates in parallel per processor word for selection scans. Interleaving the bits of codes;namely vertical bit packing; provides faster scans; but incurs prohibitive costs for packing …,Proceedings of the 11th International Workshop on Data Management on New Hardware,2015,4,4
Evaluating application mapping scenarios on the Cell/BE,Ana Lucia Varbanescu; Henk Sips; Kenneth A Ross; Qiang Liu; John R Smith; Lurng‐Kuo Liu,Abstract Applications running on multicore platforms are difficult to program; and even moredifficult to optimize; mainly due to (1) the several layers where the optimizations occur and(2) the multitude of available resources to be exploited in parallel. Although low-leveloptimizations only target code running on individual cores; high-level optimizations (eg data-and task-parallelism) target the overall application performance. In this paper; we focus onthe latter; by evaluating possible mapping scenarios of a real application on aheterogeneous multicore processor. Specifically; we focus on analyzing the impact ofcombining data-and task-parallelism for a multimedia analysis application running on theCell Broadband Engine (Cell/BE). We find that both low-level and high-level optimizationsare important for the overall application speed-up. However; we show that a speed-up …,Concurrency and Computation: Practice and Experience,2009,4,7
Practical preference relations for large data sets,Kenneth A Ross; Peter J Stuckey; Amélie Marian,User-defined preferences allow personalized ranking of query results. A user provides adeclarative specification of his/lier preferences; and the system is expected to use thatspecification to give more prominence to preferred answers. We study constraint formalismsfor expressing user preferences as base facts in a partial order. We consider a language thatallows comparison and a limited form of arithmetic; and show that the transitive closurecomputation required to complete the partial order terminates. We consider various ways ofcomposing partial orders from smaller pieces; and provide results on the size of the resultingtransitive closures. Finally; we show how preference queries within our language can besupported by suitable index structures for efficient evaluation over large data sets. Ourresults provide guidance about when complex preferences can be efficiently evaluated …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,4,7
On the adequacy of partial orders for preference composition,Kenneth A Ross,Abstract We identify several anomalies in the behavior of conventional notions ofcomposition for preferences defined by strict partial orders. These anomalies can beavoided by defining a preorder that extends the given partial order; and using the pair oforders to define order composition.,DBRank Workshop,2007,4,2
Concurrency control in materialized views of a database,*,In a database; a database manager can generate a view; which; in concept; is a subset ofthe database; which is placed outside the database for use without disturbing the database;and without disturbance by others using the database. The subset; or view; can beunderstood as a collection of rows; or tuples; of data copied from the database. With viewsexisting; multiple copies of data within the database now exist: the original in the database;and copies in the views. If one of these is changed; without corresponding changes made inthe others; then inconsistencies occur; which cannot be tolerated. Under the invention; whena user seeks a lock on a view; indicating that a change may be imminent; the invention locksa superset of the tuples in the database from which the view is derived. A superset is a setwhich contains the set of tuples of the view; plus possibly others. Thus; more tuples are …,*,2005,4,20
Efficiently following object references for large object collections and small main memory,Kenneth A Ross,Abstract We consider queries over large object-oriented databases in which one class ofobjects contains references to another class of objects. In order to answer the queryefficiently; the database system needs to be able to follow object pointers from a largecollection of objects in a way that minimizes the I/O cost. Traditional techniques requiresignificant redundant I/O when both the referencing class and the referenced class aresubstantially larger than main memory. We propose a new technique for processing a classof object-oriented queries that is an adaptation of the Jive-Join algorithm of Ross and Li. Ouralgorithm applies as long as the number of disk blocks in the referenced relation is roughlyof the order of the square of the number of blocks that fit in main memory. The cost of thealgorithm is at most one pass through each input class extension; one pass through an …,International Conference on Deductive and Object-Oriented Databases,1995,4,7
On the Cost of Transitive Closures in Relational Databases,Zhe Li; Kenneth A Ross,Abstract We consider the question of taking transitive closures on top of pure relationalsystems (Sybase and Ingres in this case). We developed three kinds of transitive closureprograms; one using a stored procedure to simulate a built-in transitive closure operator;one using the C language embedded with SQL statements to simulate the iterated executionof the transitive closure operation; and one using Floyd's matrix algorithm to compute thetransitive closure of an input graph. By comparing and analyzing the respectiveperformances of their di erent versions in terms of elapsed time spent on taking the transitiveclosure; we identify some of the bottlenecks that arise when de ning the transitive closureoperator on top of existing relational systems. The main purpose of the work is to estimatethe costs of taking transitive closures on top of relational systems; isolate the di erent cost …,*,1993,4,10
Massively-parallel lossless data decompression,Evangelia Sitaridi; Rene Mueller; Tim Kaldewey; Guy Lohman; Kenneth A Ross,Today's exponentially increasing data volumes and the high cost of storage makecompression essential for the Big Data industry. Although research has concentrated onefficient compression; fast decompression is critical for analytics queries that repeatedlyread compressed data. While decompression can be parallelized somewhat by assigningeach data block to a different process; break-through speed-ups require exploiting themassive parallelism of modern multi-core processors and GPUs for data decompressionwithin a block. We propose two new techniques to increase the degree of parallelism duringdecompression. The first technique exploits the massive parallelism of GPU and SIMDarchitectures. The second sacrifices some compression efficiency to eliminate datadependencies that limit parallelism during decompression. We evaluate these techniques …,Parallel Processing (ICPP); 2016 45th International Conference on,2016,3,7
A course on programming and problem solving,Swapneel Sheth; Christian Murphy; Kenneth A Ross; Dennis Shasha,Abstract At its core; Computer Science is the study of algorithmic problem solving. Althoughit is necessary to teach programming; data structures; computer organization; etc.; studentsshould ultimately learn to use these things to solve problems; understand what is good andbad about their solutions; and share their solutions with others.,Proceedings of the 47th ACM Technical Symposium on Computing Science Education,2016,3,4
The Q100 database processing unit,Lisa Wu; Andrea Lottarini; Timothy K Paine; Martha A Kim; Kenneth A Ross,The Q100 uses hardware specialization to improve the energy efficiency of analyticdatabase applications. The proposed accelerators are called database processing units.DPUs are analogous to GPUs; but where GPUs target graphics applications; DPUs targetanalytic database workloads. This article demonstrates a proof of concept design; called theQ100; which provides one to two orders of magnitude improvement in efficiency over single-and multithreaded software database management systems. The Q100 exploits the innatestructure of the workload; viewing the data in terms of tables and columns rather than as anunstructured array of bytes; to more efficiently move and manipulate database content.Because a large proportion of a chip's energy is spent delivering data to the computationengines; this approach both improves overall energy efficiency and complements other …,IEEE Micro,2015,3,7
Partitioned blockmap indexes for multidimensional data access,Kenneth A Ross; Evangelia Sitaridi,ABSTRACT Given recent increases in the size of main memory in modern machines; it isnow common to to store large data sets in RAM for faster processing. Multidimensionalaccess methods aim to provide efficient access to large data sets when queries applypredicates to some of the data dimensions. We examine multidimensional access methodsin the context of an in-memory column store tuned for on-line analytical processing orscientific data analysis. We propose a multidimensional data structure that contains a novelcombination of a grid array and several bitmaps. The base data is clustered in an ordermatching that of the index structure. The bitmaps contain one bit per block of data; motivatingthe term “blockmap.” The proposed data structures are compact; typically taking less thanone bit of space per row of data. Partition boundaries can be chosen in a way that reflects …,Technical Report; Department of Computer Science,2012,3,7
SkylineSearch: semantic ranking and result visualization for pubmed,Julia Stoyanovich; Mayur Lodha; William Mee; Kenneth A Ross,Abstract Life sciences researchers perform scientific literature search as part of their dailyactivities. Many such searches are executed against PubMed; a central repository of lifesciences articles; and often return hundreds; or even thousands; of results; pointing to theneed for data exploration tools. In this demonstration we present SkylineSearch; a semanticranking and result visualization system designed specifically for PubMed; and available tothe scientific community at skyline. cs. columbia. edu. Our system leverages semanticannotations of articles with terms from the MeSH controlled vocabulary; and presents resultsas a two-dimensional skyline; plotting relevance against publication date. We demonstratethat SkylineSearch supports a richer data exploration experience than does the searchfunctionality of PubMed; allowing users to find relevant references more easily. We also …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,3,4
Structural totality and constraint stratification,Kenneth A Ross,Abstract Kenneth A. Ross* Department of Computer Science; Columbia University NewYork; NY 10027 kar@ cs. columbia. edu In previous work we proposed a condition thatgeneralizes local stratification; that ensures a two-valued well-founded model; and that canbe syntactically determined from the rules and some monotonicity constraints on the facts inthe program. This condition was called“universal constraint stratification.” In this paper wegeneralize the class of constraints from monotonicity constraints to an arbitrary constraintdomain. We generalize universal constraint stratification to the more general case; andprove that the condition always ensures a two-valued wellfounded model for finitedatabases. We also provide an alternative characterization of local stratification using aversion of universal constraint stratification with equality constraints. We extend …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,3,10
Constraint Stratification in Deductive Databases.,Kenneth A Ross,Abstract We propose a syntactic condition on deductive database programs that ensures atwovalued well-founded model. This condition; called constraint strati cation; is signi cantlymore general than previous syntactic conditions such as strati cation and local strati cation.Modular strati cation has been proposed as a semantic (ie; nonsyntactic) condition forensuring a twovalued well-founded model. While not every modularly strati ed program isconstraint strati ed; all of the well-known practical examples of modularly strati ed programsare constraint strati ed under appropriate natural constraints. In addition; there existconstraint strati ed programs that are not modularly strati ed. We also show how the magicsets optimization technique can be applied for constraint strati ed programs.,Workshop on Deductive Databases and Logic Programming,1994,3,7
SIMD-accelerated regular expression matching,Evangelia Sitaridi; Orestis Polychroniou; Kenneth A Ross,Abstract String processing tasks are common in analytical queries powering businessintelligence. Besides substring matching; provided in SQL by the like operator; popularDBMSs also support regular expressions as selective filters. Substring matching can beoptimized by using specialized SIMD instructions on mainstream CPUs; reaching theperformance of numeric column scans. However; generic regular expressions are harder toevaluate; being dependent on both the DFA size and the irregularity of the input. Here; weoptimize matching string columns against regular expressions using SIMD-vectorized code.Our approach avoids accessing the strings in lockstep without branching; to exploit caseswhen some strings are accepted or rejected early by looking at the first few characters. Oncommon string lengths; our implementation is up to 2X faster than scalar code on a …,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,2,15
Supporting transient snapshot with coordinated/uncoordinated commit protocol,*,Methods and a system are provided. A method includes maintaining an in-page log forrecords in each of a plurality of data pages of a multi-version database. The method furtherincludes adding record update information to the in-page log when a corresponding one ofthe records is deleted or updated. The method also includes consulting the in-page log for arecently updated one of the records or a recently deleted one of the records to determine arecord status thereof. The method additionally includes spilling; by a processor-basedoverflow manager; to overflow pages when the in-page log is full. The data pages includeany of row-oriented data pages and column-oriented data pages.,*,2016,2,1
Hardware-accelerated range partitioning,Lisa Wu; Raymond J Barker; Martha A Kim; Kenneth A Ross,Abstract With global pool of data growing at over 2.5 quinitillion bytes per day and over 90%of all data in existence created in the last two years alone [23]; there can be little doubt thatwe have entered the big data era. This trend has brought database performance to theforefront of high throughput; low energy system design. This paper explores targeteddeployment of hardware accelerators to improve the throughput and efficiency of databaseprocessing. Partitioning; a critical operation when manipulating large data sets; is often thelimiting factor in database performance; and represents a significant amount of the overallruntime of database processing workloads. This paper describes a hardware-softwarestreaming framework and a hardware accelerator for range partitioning; or HARP. Thestreaming framework offers seamless execution environment for database processing …,Columbia University Computer Science Technical Reports,2012,2,7
ReoptSMART: A Learning Query Plan Cache,Julia Stoyanovich; Kenneth A Ross; Jun Rao; Wei Fan; Volker Markl; Guy Lohman,Abstract: The task of query optimization in modern relational database systems is importantbut can be computationally expensive. Parametric query optimization (PQO) has as its goalthe prediction of optimal query execution plans based on historical results; withoutconsulting the query optimizer. We develop machine learning techniques that can accuratelymodel the output of a query optimizer. Our algorithms handle non-linear boundaries in planspace and achieve high prediction accuracy even when a limited amount of data is availablefor training. We use both predicted and actual query execution times for learning; and arethe first to demonstrate a total net win of a PQO method over a state-of-the-art queryoptimizer for some workloads. ReoptSMART realizes savings not only in optimization time;but also in query execution time; for an over-all improvement by more than an order of …,*,2008,2,1
Alpha radiation is a major germ-line mutagen over evolutionary timescales,Kenneth Andrew Ross,Natural selection operates on the genetic variation provided by mutation. If the spectrum of mutationalchange is different in two lineages; one might expect to see different patterns of genomic evolutionin those lineages. Alpha radiation of cell cultures at small doses [up to 1.7 mGy (Nagasawa andLittle; 2002) or 5 mGy (Huo et al.; 2001)] generates a mutational spectrum in which almost allmutations induced by the radiation are small local DNA changes; such as point mutations. Largerdoses generate chromosomal mutations such as partial and total gene deletions (Huo et al.;2001; Nagasawa and Little; 2002). Similar changes in spectra are observed for gamma radiation(Schwartz et al.; 2000). The general pattern of chromosomal mutations occurring more oftenwith higher exposure is robust; at least in vitro (Jostes; 1996; Hei et al.; 2004). Chromosomalmutations are associated with 'direct hits' of nuclei by alpha particles; while point …,Evolutionary Ecology Research,2006,2,7
Sword: A declarative object-oriented database architecture,Inderpal Singh Mumick; Kenneth A Ross,Abstract We present a language Noodle in which to specify declarative queries in an object-oriented database system. The language models object-identity; classes; relations; views;inheritance; complex objects; and methods; in addition to logical rules. Noodle is intended tobe used as a query language in an object-oriented database system; with the bene ts ofdeclarativeness; namely increased e ciency and reduced programming time. We addressimplementation and optimization issues. We describe how a system; such as the one we aredesigning at AT&T Bell Labs; would compile and optimize the language Noodle in an object-oriented system where encapsulation makes it di cult to do global query optimization. Ourwork bridges the gap between deductive and object-oriented databases. We believe that ourlanguage is su ciently general to allow a wide range of object-oriented queries to be …,*,1992,2,4
Managing a multi-version database,*,Managing different versions of a data record is provided. A mapping is maintained betweena version-independent logical record identifier and a version-dependent physical record rowidentifier that correspond to each data record within a plurality of data records of a data tableusing a logical record identifier to physical record row identifier indirection mapping table.Entries within leaf pages of an index associated with the data table are updated to point tothe version-independent logical record identifier corresponding to a data record instead ofpointing to the version-dependent physical record row identifier corresponding to the datarecord. The logical record identifier to physical record row identifier indirection mappingtable is updated in response to performing an operation on the data record instead ofupdating the entries within the leaf pages of the index associated with the data table.,*,2016,1,7
Architecture-conscious database system,John Cieslewicz; Kenneth A Ross,1. Software non-determinism: A software system is non-deterministic if; when re-executed; itresults in a different execution path than a prior execution. Non-determinism can arise when;for example; paths are determined by relative processor speed or the sequence of externalevents. Such software bugs have been called ''Heisenbugs''(hard failures being ''Bohrbugs'').2. Soft hardware failures: Hardware can also suffer from ''Heisenbugs.''For example; atransient hardware failure may be triggered by an environmental cause; such as a cosmicray changing a memory bit; etc. 3. Operator failures: Systems occasionally require operatorintervention. Operators; being human; make mistakes. An operator is unlikely to make thesame mistake at the same point in a subsequent execution.,*,2009,1,4
View adaptation,Kenneth A Ross,The valid time of a fact is the time when the fact is true in the modeled reality. Any subset ofthe time domain may be associated with a fact. Thus; valid timestamps may be sets of timeinstants and time intervals; with single instants and intervals being important special cases.Valid times are usually supplied by the user.,*,2009,1,15
FlowPuter: A Cluster Architecture Unifying Switch; Server and Storage Processing,Alfred V Aho; Angelos D Keromytis; Vishal Misra; Jason Nieh; Kenneth A Ross; Yechiam Yemini,Abstract—We present a novel cluster architecture that unifies switch; server and storageprocessing to achieve a level of price-performance and simplicity of applicationdevelopment not achievable with current architectures. Our architecture takes advantage ofthe increasing disparity between storage capacity; network switching on the one hand; andprocessing power of modern processors and architectures on the other. We propose the useof Network Processors (NPUs); which can apply simple classify/act/forward operations ondata packets at wire speeds; to split processing of operations such as complex databasequeries across a network. We quantify the theoretical benefits of such an architecture overtraditional server-cluster approaches using warehouse database queries as a motivatingapplication. We also discuss the challenges such an architecture presents to …,Proceedings of the 1st International Workshop on Data Processing and Storage Distributed: Towards Grid Computing (DPSN'04); Athens; GR,2004,1,7
Reminiscences on influential papers,Kenneth A Ross,First; together with Agrawal; Imielinski; and Swami's SIGMOD'93 paper:" Mining AssociationRules between Sets of Items in Large Databases;" it identifies a new and important task indata mining: association rule mining; ie; finding frequent patterns or itemsets (sets of items)that occur frequently together in large databases. This has proven truly useful for frequentpattern or association mining; dependency or correlation analysis; etc.; with manyapplications. Some following studies have shown that it is also usefifl for associatiombasedclassification; sequential or structured pattern analysis; constraint-based mining; clusteranalysis; semantic data compression; data cube computation; and so on. Identification of acrucial research problem itself makes the paper distinct from many others. Second; itdiscovers a nice and elegant property in association mining; namely Apriori; which states …,ACM SIGMOD Record,2001,1,7
Independence diagrams: A technique for data visualization,Stefan Berchtold; HV Jagadish; Kenneth A Ross,An important issue in data visualization is the recognition of complex dependenciesbetween attributes. Past techniques for identifying attribute dependence include correlationcoefficients; scatterplots; and equi-width histograms. These techniques are sensitive tooutliers; and often are not sufficiently informative to identify the kind of attribute dependencepresent. We propose a new approach; which we call independence diagrams. We divideeach attribute into ranges; for each pair of attributes; the combination of these rangesdefines a two-dimensional grid. For each cell of this grid; we store the number of data itemsin it. We display the grid; scaling each attribute axis so that the displayed width of a range isproportional to the total number of data items within that range. The brightness of a cell isproportional to the density of data items in it. As a result; both attributes are independently …,Journal of Electronic Imaging,2000,1,7
Power-Pipelining for Enhanced Query Performance,Jun Rao; Kenneth A Ross,Abstract As random access memory gets cheaper; it becomes increasingly affordable tobuild computers with large main memories. In this paper; we consider processing querieswithin the context of a main memory database system and try to enhance the queryexecution engine of such a system. An execution plan is usually represented as an operatortree. Traditional query execution engines evaluate a query by recursively iterating eachoperator and returning exactly one tuple result for each iteration. This generates a largenumber of function calls. In a main-memory database system; the cost of a function call isrelatively high. We propose a new evaluation method called power-pipelining. Eachoperator processes and passes many tuples instead of one tuple each time. We keep thenumber of matches generated in a join in a local counter array. We reconstitute the run …,*,2000,1,4
Programming and Problem Solving: A Transcript of the Spring 1999 Class,Kenneth A Ross; Simon R Shamoun,This report contains edited transcripts of the discussion held in Columbia's Programming andProblem- Solving course; taught as W4995-01 during Spring 1999. The class notes were takenby the teaching assistant so that students could focus on the class material. As a service to boththe students and to others who would like to get some insight into the class experience; we havedrawn all of the class handouts; discussion; and some of the results into this technical report.… 1.1 The First Handout: Course Description and Problem Statements . . . . . . . . . . . . . . . . 41.1.1 Project 1: Toetjes III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.1.2 Project 2: Where canwe go from here? . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.1.3 Project 3: Battlesplat . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . 8 1.1.4 Project 4: Egalitarian Island. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 9 1.1 … 2.1 Monday January 25; 1999 …,*,1999,1,4
Faster joins; self-joins and multi-way joins using join indices,Hui Lei; Kenneth A Ross,Abstract We propose a new algorithm; called Stripe-join; for performing a join given a joinindex. Stripe-join is inspired by an algorithm called 'Jive-join'developed by Li and Ross.Stripe-join makes a single sequential pass through each input relation; in addition to onepass through the join index and two passes through a set of temporary files that containtuple identifiers but no input tuples. Stripe-join performs this efficiently even when the inputrelations are much larger than main memory; as long as the number of blocks in mainmemory is of the order of the square root of the number of blocks in the participatingrelations. Stripe-join is particularly efficient for self-joins. To our knowledge; Stripe-join is thefirst algorithm that; given a join index and a relation significantly larger than main memory;can perform a self-join with just a single pass over the input relation and without storing …,Data & knowledge engineering,1999,1,4
The New Jersey Data Reduction Report,Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,There is often a need to get quick approximate answers from large databases. This leads toa need for data reduction. There are many di erent approaches to this problem; some ofthem not traditionally posed as solutions to a data reduction problem. In this paper wedescribe and evaluate several popular techniques for data reduction. Historically; theprimary need for data reduction has been internal to a database system; in a cost-basedquery optimizer. The need is for the query optimizer to estimate the cost of alternative queryplans cheaply {clearly the e ort required to do so must be much smaller than the e ort ofactually executing the query; and yet the cost of executing any query plan depends stronglyupon the numerosity of speci ed attribute values and the selectivities of speci ed predicates.To address these query optimizer needs; many databases keep summary statistics …,Bulletin of the IEEE Computer Society Technical Committee on Data Engineering,1997,1,4
A new client-server architecture for distributed query processing,Zhe Li; Kenneth A Ross,Abstract This paper presents the idea of\tuple bit-vectors" for distributed query processing.Using tuple bit-vectors; a new two-way semijoin operator called 2SJ++ that enhances thesemijoin with an essentially\free" backward reduction capability is proposed. We explore indetail the bene ts and costs of 2SJ++ compared with other semijoin variants; and its e ect ondistributed query processing performance. We then focus on one particular distributed queryprocessing algorithm; called the\one-shot" algorithm. We modify the one-shot algorithm byusing 2SJ++ and demonstrate the improvements achieved in network transmission costcompared with the original one-shot technique. We use this improvement to demonstratethat equipped with the 2SJ++ technique; one can improve the performance of distributedquery processing algorithms signi cantly without adding much complexity to the …,*,1994,1,1
Design and implementation of the sword declarative object-oriented database system,Inderpal Singh Mumick; K Ross; S Sudershan,Abstract SWORD is a declarative object-oriented database being built at AT&T BellLaboratories. SWORD provides both procedural and declarative data manipulationlanguages. SWORD is integrated with the O++ language (an extension of C++) of the ODEdatabase system. The data de nition and procedural data manipulation in SWORD are donein the O++ sublanguage. In addition; SWORD provides a declarative sublanguage toexpress queries. Declarativeness yields bene ts in reduced programming time andautomatic optimization in response to changing database characteristics. The declarativesublanguage is based on the Noodle language of MR93]; modi ed for integration withC++/O++ to avoid the impedance mismatch problem. The declarative sublanguage modelsobject-identity; classes; relations; views; inheritance; complex objects; and methods; in …,*,1993,1,4
A programming and problem solving seminar,Kenneth A Ross; Donald Ervin Knuth,This report contains edited transcripts of the discussions held in Stanford's ComputerScience problem solving course; CS304; during winter quarter 1989. Since the topics span alarge range of ideas in computer science; and since most of the important researchparadigms and programming paradigms were touched on during the discussions; thesenotes may be of interest to graduate students of computer science at other universities; aswell as to their professors and to professional people in the “real world.” The present reportis the eighth in a series of such transcripts; continuing the tradition established in STAN-CS-77-606 (Michael J. Clancy; 1977); STAN-CS-79-707 (Chris Van Wyk; 1979); STAN-CS-81-863 (Allan A. MI11 er; 1981); STAN-CS-83-989 (Joseph S. Weening; 1983); STAN-CS-83-990 (John D. Hobby; 1983); STAN-CS-85-1055 (Ramsey W. Haddad; 1985) and STAN …,*,1989,1,1
Network synthesis for database processing units,Andrea Lottarini; Stephen A Edwards; Kenneth A Ross; Martha A Kim,We explore on-chip network topologies for the Q100; an analytic query accelerator forrelational databases. In such data-centric accelerators; interconnects play a critical role bymoving large volumes of data. In this paper we show that various interconnect topologiescan trade a factor of 2.5× in performance for 3.3× area. Moreover; standard topologies (eg;ring or mesh) are not optimal. Significant prior work on network topology specializationaugments generic topologies with additional dedicated links. In this paper; we present anetwork specialization algorithm that builds a specialized network first then introduces ageneric network as a fallback. We find our algorithm produces networks that are 1.24×slower than the highest-performance generic topology considered (a fat tree); and 18%smaller than the least expensive (a double ring). Moreover; our method produces …,Design Automation Conference (DAC); 2017 54th ACM/EDAC/IEEE,2017,*,20
Adaptive concurrency control using hardware transactional memory and locking mechanism,*,A method includes the following steps. Runtime statistics related to data transactionprocessing in a concurrent system are collected. A given request to access shared data inthe concurrent system is receive. Based on the collected runtime statistics; the number ofreattempts the given request can make to access the shared data prior to access controlbeing switched from a hardware transactional memory to a locking mechanism is adaptivelydetermined.,*,2017,*,10
Deadlock-free joins in DB-mesh; an asynchronous systolic array accelerator,Bingyi Cao; Kenneth A Ross; Stephen A Edwards; Martha A Kim,Abstract Previous database accelerator proposals such as the Q100 provide a fixed set ofdatabase operators; chosen to support a target query workload. Some queries may not bewell-supported by a fixed accelerator; typically because they need more resources/operatorsof a particular kind than the accelerator provides. By Amdahl's law; these queries becomerelatively more expensive as they are not fully accelerated. We propose a second-levelaccelerator; DB-Mesh; to take up some of this workload. DB-Mesh is an asynchronoussystolic array that is more generic than the Q100; and can be configured to run a variety ofoperators with configurable parameters such as record widths. We demonstrate DB-Meshapplied to nested loops joins; an operator that is not directly supported on the Q100. Weshow that a naïve implementation has the potential for deadlock; and show how to avoid …,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,*,20
Access frequency approximation for remote direct memory access,*,A method includes the following steps. One or more records are accessed from a databasememory bypassing a database access mechanism of a database system. Data representingaccess frequency of the one or more records are collected. The collected access frequencydata for the one or more records are maintained. The access frequency data for the one ormore records are aggregated until the access frequency reaches a threshold value. Theaggregated access frequency data are asynchronously reported for the one or more recordsto the database system.,*,2017,*,4
Supporting transient snapshot with coordinated/uncoordinated commit protocol,*,Methods and a system are provided. A method includes maintaining an in-page log forrecords in each of a plurality of data pages of a multi-version database. The method furtherincludes adding record update information to the in-page log when a corresponding one ofthe records is deleted or updated. The method also includes consulting the in-page log for arecently updated one of the records or a recently deleted one of the records to determine arecord status thereof. The method additionally includes spilling; by a processor-basedoverflow manager; to overflow pages when the in-page log is full. The data pages includeany of row-oriented data pages and column-oriented data pages.,*,2016,*,7
Method and system for automatic space organization in tier2 solid state drive (SSD) cache in databases for multi page support,*,A system and method for adjusting space allocated for different page sizes on a recordingmedium includes dividing the recording medium into multiple blocks such that a block size ofthe multiple blocks supports a largest page size; and such that each of the multiple blocks isused for a single page size; and assigning an incoming page to a block based on atemperature of the incoming page.,*,2016,*,7
Preplaying transactions that mix hot and cold data,*,Methods and systems for performing database transactions include executing a firsttransaction request in a preplay mode that locks the requested data with a prefetch-lock andreads one or more requested data items from storage into a main memory buffer; locking therequested data items with a read/write lock after said data items are read into the mainmemory buffer; and performing the requested transaction on the data items in the mainmemory buffer using a processor.,*,2015,*,7
Deferring data record changes using query rewriting,*,Staging data record changes from a faster storage medium to a slower storage mediumusing data query rewriting is provided. In response to receiving a data query correspondingto a particular data record; it is determined whether the data query is one of a transactionaldata query or an analytical data query. In response to determining that the data query is atransactional data query; the transactional data query is rewritten to apply transactional deltachanges to the particular data record on a storage-class memory of a computer. In responseto determining that the data query is an analytical data query; the analytical data query isrewritten to select and reconcile each data record corresponding to the particular datarecord stored on the storage-class memory with the particular data record stored on apersistent data storage device of the computer.,*,2015,*,4
Deferring data record changes using query rewriting,*,Staging data record changes from a faster storage medium to a slower storage mediumusing data query rewriting is provided. In response to receiving a data query correspondingto a particular data record; it is determined whether the data query is one of a transactionaldata query or an analytical data query. In response to determining that the data query is atransactional data query; the transactional data query is rewritten to apply transactional deltachanges to the particular data record on a storage-class memory of a computer. In responseto determining that the data query is an analytical data query; the analytical data query isrewritten to select and reconcile each data record corresponding to the particular datarecord stored on the storage-class memory with the particular data record stored on apersistent data storage device of the computer.,*,2015,*,7
Multicore Processors and Database Systems: The multicore transformation (Ubiquity symposium),Kenneth A Ross,Abstract Database management systems are necessary for transaction processing andquery processing. Today; parallel database systems can be run on multicore platforms.Presented within is an overview of how multicore machines have impacted the design andimplementation of database management systems.,Ubiquity,2014,*,7
Querying Persistent Graphs using Solid State Storage,Manos Athanassoulis; Bishwaranjan Bhattacharjee; Mustafa Canim; Kenneth A Ross,ABSTRACT Solid State Drives (SSDs) are an important component of secondary storagesystems. While Hard Disk Drives (HDDs) are cheaper per gigabyte; SSDs are cheaper perrandom I/O per second. A variety of of solid-state technologies are being developed withNAND flash being the most mature; and Phase Change Memory (PCM) beginning to enterthe marketplace. Compared with flash; PCM has finer-grained addressability and higherwrite endurance. PCM is also expected to offer lower read and write response times. In thiswork we study the use of solid-state storage in latencybound applications; a type of workloadthat can benefit from the characteristics of flash and PCM technologies. We identify graphprocessing and Resource Description Framework (RDF) query processing as candidateapplications. Using an early PCM prototype device; we demonstrate the benefits of PCM …,Proceedings of the Annual Non-Volatile Memories Workshop (NVMW),2013,*,15
High Throughput Heavy Hitter Aggregation,Orestis Polychroniou; Kenneth A Ross,ABSTRACT Heavy hitters are data items that occur at high frequency in a data set. Heavyhitters are among the most important items for an organization to summarize and understandduring analytical processing. In data sets with sufficient skew; the number of heavy hitterscan be relatively small. We take advantage of this small footprint to compute aggregatefunctions for the heavy hitters in fast cache memory. We design cache-resident; shared-nothing structures that hold only the most frequent elements from the table. Our approachworks in three phases. It first samples and picks heavy hitter candidates. It then builds ahash table and computes the exact aggregates of these candidates. Finally; if necessary; avalidation step identifies the true heavy hitters from among the candidates based on thequery specification. We identify trade-offs between the hash table capacity and …,*,2012,*,15
Special issue: best papers of VLDB 2008,Peter Buneman; Volker Markl; Beng Chin Ooi; Kenneth Ross,This issue of the VLDB Journal is dedicated to the best papers presented at the 34thInternational Conference on Very Large Databases; which took place in Auckland; NewZealand on 23–28 August; 2008. This conference marked two significant departures fromtradition. First; an Experiments and Analyses track; devoted to analytical or empirical studiesand of existing techniques; was introduced. A measure of the success of this track is that oneof the papers was selected as co-recipient of the best paper award; and an extended versionof it appears in this issue. Second; this conference saw the first moves towards thetransformation of the VLDB proceedings into a fully refereed journal; the Proceedings of theVLDB (PVLDB). At the time of writing this editorial; the transformation is well underway. Itshould be noted that PVLDB is in no sense a competitor to this journal (VLDBJ). The …,The VLDB Journal,2010,*,7
Proceedings of the VLDB Endowment Volume 1 Issue 1,Peter Buneman; Beng Chin Ooi; Kenneth Ross; Gerald Weber,*,*,2008,*
Fourth International Workshop on Data Management on New Hardware (DaMoN 2008),Qiong Luo; Kenneth A Ross,FOREWARD Objective The aim of this one-day workshop is to bring together researchers whoare interested in optimizing database performance on modern computing infrastructure by designingnew data management techniques and tools. Topics of Interest The continued evolution of computinghardware and infrastructure imposes new challenges and bottlenecks to programperformance. As a result; traditional database architectures that focus solely on I/O optimizationincreasingly fail to utilize hardware resources efficiently. CPUs with superscalar out-of-orderexecution; simultaneous multi-threading; multi-level memory hierarchies; and future storagehardware (such as flash drives) impose a great challenge to optimizing databaseperformance. Consequently; exploiting the characteristics of modern hardware has become animportant topic of database systems research. The goal is to make database systems …,*,2008,*,23
Schema polynomials and applications,Kenneth A Ross; Julia Stoyanovich,Abstract Conceptual complexity is emerging as a new bottleneck as data-base developers;application developers; and database administrators struggle to design and comprehendlarge; complex schemas. The simplicity and conciseness of a schema depends critically onthe idioms available to express the schema. We propose a formal conceptual schemarepresentation language that combines different design formalisms; and allows schemamanipulation that exposes the strengths of each of these formalisms. We demonstrate howthe schema factorization framework can be used to generate relational; object-oriented; andfaceted physical schemas; allowing a wider exploration of physical schema alternatives thantraditional methodologies. We illustrate the potential practical benefits of schemafactorization by showing that simple heuristics can significantly reduce the size of a real …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,*,22
Finding shapes in a set of points,Kenneth A Ross; David Vespe; David Hessing; Pranay Jain,Abstract We present a tool for querying a set of points for geometric shapes. This tool wasdeveloped as part of a larger project studying the architecture of 13th century Frenchchurches. We present a query language for specifying shapes. We describe animplementation of the tool that optimizes and executes shape queries. We describe theperformance of the tool; and show examples of how it can be used to analyze buildingfloorplans. The tool is available for download over the internet.,ACM SIGMOD Record,2007,*,22
Reminiscences on influential papers.,Kenneth A Ross; James R.  Hamilton; Laks V. S.  Lakshmanan; Limsoon Wong,This paper recently won the theory community's prestigious Goedel award; yet it has alsobeen tremendously influential in the the database community in general and at a personallevel on my research. In one short paper; the authors established many founding principlesof the data stream model; gave ingenious algorithms for computing the second frequencymoment; a general method for finding all frequency moments; and lower bounds on thespace needed. While these problems initially appear abstract; the paper has had wideinfluence on algorithms; database and network research. Reading this paper as a graduatestudent was a revelation: having been thinking about some related questions before readingthe paper; I found some of the results almost unbelievable. The crisp presentation inspiredme to learn the relevant tools used to give such initially surprising proofs. For the …,SIGMOD Record,2005,*,1
Reminiscences on influential papers,Kenneth A Ross,This paper abstracts from the particular features of PRISMA/DB; and evaluates and analyzesthe performance trade-offs for a wide range of parallel query processing strategies. Its clearstyle of presentation; along with careful attention to previous work both in its discussion aswell as in the experiments and analysis; make this paper into a concise introductory or“refereshment” text for researchers interested in parallel query execution.,ACM SIGMOD Record,2004,*,20
Reminiscences on influential papers,Kenneth A Ross,Sometimes the simplest ideas are the best. Hellerstein et al. introduced the idea ofextending the relational database engine to compute basic SQL aggregates in an onlinefashion; recognizing that OLAP queries are often posed to get rough estimates (eg; decisionsupport) and thus do not require" painstaking precision". While there had been previouswork on approximate query answering as well as so-called" fast-first" query processing; thisis the first paper to consider online; interactive query processing in an RDB. The keyimplementation ideas include random access to data; non-blocking strategies; and round-robin fetching; all based on leveraging standard database tools. However; more importantthan these specifics is the paradigm shift away from traditional batch processing. Althoughthe techniques in this paper are inherently for data stored in a DBMS; this general …,ACM SIGMOD Record,2002,*,22
Reminiscences on Influential Papers,S Ceri; L Gravano; P Larson; L Libkin; T Milo; K Ross,Segnalazioni con codici 20501/20503/20504: Alcuni dati obbligatori per il sito CINECA nonsono presenti o invalidi; oppure il sito CINECA non è riuscito ad individuare una rivista con idati forniti; è necessario controllare la correttezza dell'ISSN e/o EISSN dove applicabili e iltitolo della rivista.Segnalazioni con codici 20201/20202: La pubblicazione non è statatrasferita SOLO per i docenti segnalati nel messaggio a causa di problemi nell'anagraficaCINECA e/o di Ateneo (pe i codici fiscali non sono gli stessi) oppure perché si tratta didocenti che; pur essendo abilitati all'inserimento nel sistema di Ateneo; non hanno facoltà disincronizzare le proprie pubblicazioni in CINECA. Tutti gli altri eventuali coautori senzasegnalazione troveranno la pubblicazione correttamente nel proprio spazio personaleCINECA.,SIGMOD RECORD,2001,*,15
Reminiscences on Influential Papers,Kenneth A Ross,My personal interest in database estimation and approximation problems did not start until I readthis paper a few years after it was written. I believe there are two reasons why it had such a significantinfluence on my work: its contents and its elegance. It took a comprehensive look at all the basicassumptions that database optimizers used at the time (and some still do) and studied theirimpact; showing that they generate estimates that are extreme while reality is often betterbehaved. Several other papers had approached similar subjects; but they either addressed specialcases or were rather informal and empirical. I felt that this paper cleared the field by giving consiseanswers to several critical problems; and set the stage for much of the subsequent work in theestimation/approximation area. In addition to its technical merits from a databaseperspective; however; I was also attracted to its really elegant mathematics. This paper …,ACM SIGMOD Record,2000,*,10
Efficient matching algorithms for publish and subscribe systems,Françoise Fabret; François Llirbat; Arno Jacobsen; Joâo Pereira; Kenneth Ross; Dennis Shasha,Publish/Subscribe is the paradigm in which users express long-term interests(«subscriptions») and some external agent (perhaps other users)«publishes» events (eg;offers). The job of Publish/Subscribe software is to send events to the owners ofsubscriptions satisfied by those events. For example; a user subscription may consist of aninterest in an airplane of a certain type; not to exceed a certain price. A published event mayconsist of an offer of an airplane with certain properties including price. A subscriptio-nclosely resembles a trigger in that it is a long-lived conditional query associated with anaction (usually; informing the subscriber). However; it is less general than a trigger so noveldata structures and implementations may enable the creation of scalable; high performancepublish-subscribe systems. This paper describes an attempt at the construction of such …,*,2000,*,12
Database Research at Columbia University,Shih-Fu Chang; Luis Gravano; Gail E Kaiser; Kenneth A Ross; Salvatore J Stolfo,Columbia University has a number of projects that touch on database systems issues. In thisreport; we describe the Columbia Fast Query Project (Section 2); the JAM project (Section 3);the CARDGIS project (Section 4); the Columbia Internet Information Searching Project(Section 5); the Columbia Content-Based Visual Query project (Section 6); and projectsassociated with Columbia's Programming Systems Laboratory (Section 7).,SIGMOD Record,1998,*,7
Attribute-oriented view definitions in relational and deductive databases,Inderpal Singh Mumick; Kenneth A Ross,Abstract In some database applications it is natural to define individual attributes of arelation as derived attributes; separately from the definition of other attributes. This approachminimizes the complexity of the schema. We propose a formalism in which rules are dividedinto two classes:“Defining rules” specify the existence of tuples in the relation; together withsome of their attributes.“Refining rules” specify derived attributes of tuples that already existaccording to the defining rules. A potential problem with this approach is that view definitionsmay appear to be recursive; as some attributes of a relation are defined in terms of others.We show how this apparent recursion can be dealt with by identifying the attributes throughwhich the recursion occurs. We present semantics based on these notions; and show howthey allow the natural formulation of a motivating example. We discuss evaluation of rules …,International Conference on Deductive and Object-Oriented Databases,1997,*,1
We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Datalo...,Kenneth A Ross; Yehoshua Sagiv,We propose a semantics for aggregates in deductive databases based on a notion ofminimality. Unlike some previous approaches; we form a minimal model of a programcomponent including aggregate operators; rather than insisting that the aggregate apply toatoms that have been fully determined or that aggregate functions are rewritten in terms ofnegation. In order to guarantee the existence of such...,Journal of Computer and System Sciences,1997,*,8
Reminiscences on influential papers,Kenneth A Ross,Rarely; a paper comes along that summarizes the current literature and brings it all togetherin a single place; concisely written; and clearly argued and these papers form the basis pointfor both future product development and further research. The ARIES paper featured hereachieved that goal with unusual clarity yet goes a step further by not just summarizing thecurrent research but in also documenting in detail how leading commercial relationaldatabases had implemented logging; concurrency control; buffer management; and loggingat a time when there was almost no information available describing the internals of thesesystems. The paper is massive in scope covering logging and recovery methods; providing ataxonomy of different approaches to buffer management; concurrency control and lockhierarchies with intent locking; fined grained (row) locking techniques and the impact on …,ACM TODS,1992,*,7
Incorporating DATABASE TECHNOLOGY,Nicole Bidoit-Tollu; Felipe Carino Jr; Stavros Christodoulakis; Klaus R Dittrich; UniversitÌt Zˇrich; TYPA Panepistimioupolis; B Kemme; N Koudas; Maurizio Lenzerini; P Loucopoulos; Patrick O’Neil; KA Ross; Gottfried Vossen; M Yoshikawa; Matthias Jarke; InformatikV RWTHAachen; Dennis Shasha,*,*,*,*
Data Engineering,G Graefe; S Harizopoulos; H Kuno; MA Shah; D Tsirogiannis; JL Wiener; B Bhattacharjee; M Canim; CA Lang; GA Mihaila; KA Ross; M Bjørling; P Bonnet; L Bouganim; B Jònsson,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*,7
ACM SIGMOD Conference; June 1996; pp. 447-458,Kenneth A Ross; Divesh Srivastava; S Sudarshan,*,*,*,*
Data Engineering,William DuMouchel Barbará; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,There is often a need to get quick approximate answers from large databases. This leads toa need for data reduction. There are many different approaches to this problem; some ofthem not traditionally posed as solutions to a data reduction problem. In this paper wedescribe and evaluate several popular techniques for data reduction. Historically; theprimary need for data reduction has been internal to a database system; in a cost-basedquery optimizer. The need is for the query optimizer to estimate the cost of alternative queryplans cheaply–clearly the effort required to do so must be much smaller than the effort ofactually executing the query; and yet the cost of executing any query plan depends stronglyupon the numerosity of specified attribute values and the selectivities of specified predicates.To address these query optimizer needs; many databases keep summary statistics …,*,*,*,14
Data Engineering,Kenneth A Ross; John Cieslewicz; Jun Rao; Jingren Zhou,The Bulletin of the Technical Committee on Data Engineering is published quarterly and isdistributed to all TC members. Its scope includes the design; implementation; modelling;theory and application of database systems and their technology. Letters; conferenceinformation; and news should be sent to the Editor-in-Chief. Papers for each issue aresolicited by and should be sent to the Associate Editor responsible for the issue. Opinionsexpressed in contributions are those of the authors and do not necessarily reflect thepositions of the TC on Data Engineering; the IEEE Computer Society; or the authors'organizations. Membership in the TC on Data Engineering is open to all current members ofthe IEEE Computer Society who are interested in database systems. There are two DataEngineering Bulletin web sites: http://www. research. microsoft. com/research/db/debull …,Ann Arbor,*,*,4
Minker; J. 1982. On indefinite data bases and the closed world assumption. In Proc. 6-th Conference on Automated Deduction; 292 308. New York: Springer Verlag....,A Van Gelder; KA Ross; JS Schlipf,Przymusinski; TC 1991b. Semantics of disjunctive logic programs and deductive databases.In Delobel; C.; Kifer; M.; and Masunaga; Y.; eds.; Proceedings of the Second InternationalConference on Deductive and Object-Oriented Databases DOOD'91; 85 {107. Munich;Germany: Springer Verlag. Przymusinski; TC 1991c. Stable semantics for disjunctiveprograms. New Generation Computing Journal 9: 401 {424.(Extended abstract appeared in:Extended stable semantics for normal and disjunctive logic programs. Proceedings of the 7-th International Logic Programming Conference; Jerusalem; pages 459 {477; 1990. MITPress.). Przymusinski; TC 1994. Static semantics for normal and disjunctive logic programs.Annals of Mathematics and Arti cial Intelligence.(in print). Reiter; R. 1978. On closed-worlddata bases. In Gallaire; H.; and Minker; J.; eds.; Logic and Data Bases. New York: Plenum …,*,*,*,1
Department of Computer Science; Columbia University; New York; NY 10027 fli; karg@ cs. columbia. edu,Zhe Li; Kenneth A Ross,Abstract This paper presents\Positionally Encoded Record Filters"(PERFs) and describestheir use in a distributed query processing technique called PERF join. A PERF is a noveltwo-way join reduction implementation primitive. While having the same storage andtransmission e ciency as a hash lter (eg; Bloom Filter); a PERF is based on the relation tuplescan order instead of hashing. Hence it doesn't su er any loss of join information incurred byhash collisions. Using the query response time measured in terms of network cost as acomparison criterion; we demonstrate through analytical studies that PERF join performssigni cantly better than two-way Bloomjoin and two-way semijoin variants under a widerange of relevant cost parameter values. For the large number of distributed queryprocessing algorithms relying on Bloomjoin or semijoin variants to reduce their network …,*,*,*,4
Conference Organisers,Alexandros Biliris; Inderpal Singh Mumick; Advisory Council; Stavros Christodoulakis; Ron Sacks-Davis; David Dewitt; Peter Lockemann; Sham Navathe; Klaus Dittrich; Yannis Ioannidis; HV Jagadish; Nelson M Mattos; Andy Witkowski; Kenneth A Ross; Alex Delis; Euthimios Panagos; Vibby Gottemukkala; Ashish Gupta; Tamer Ozsu; Area Coordinators; Francois Bancilhon; Hector Garcia-Molina,General Conference Chairs Alexandros Biliris (AT&T Labs; USA) Inderpal Singh Mumick (SaveraSystems; USA) Advisory Council Stavros Christodoulakis (University of Crete; Greece) RonSacks-Davis (CITRI) David Dewitt (University of Wisconsin; USA) Peter Lockemann (Universityof Karlsruhe; Germany) Sham Navathe (Georgia Tech; USA) Research Program ChairsAmericas/Australia: Jennifer Widom (Stanford University; USA) Africa/Asia/Europe: Oded Shmueli(Technion; Israel) Industrial Program Chairs Americas/Australia: Dennis Shasha (CourantInstitute; USA) Africa/Asia/Europe: Patrick Valduriez (INRIA; France) Panel Program Chairs KlausDittrich (Universitat Zurich; Switzerland) Yannis Ioannidis (University of Athens; Greece) TutorialProgram Chair HV Jagadish (AT&T Labs; USA) Exhibits Program Chairs Nelson M. Mattos(IBM; USA) Andy Witkowski (Oracle Corporation; USA) Treasurer Kenneth A. Ross …,*,*,*,7
Main Memory Performance for Database Systems,Kenneth A Ross,As random access memory gets cheaper; computers with large main memories becomeincreasingly affordable. Thus for both main memory databases and disk-resident databases;the efficient use of a large main memory has become an important performance goal. Fordisk-based databases; CPU and main-memory related performance has begun to dominateI/O performance as the critical bottleneck [GL01; ADHW99]. It is thus imperative thatdatabase architects switch from optimizing their systems for I/O to optimizing them formemory performance. It is not even clear that monolithic commercial systems can beadequately tuned for in-memory performance; given the design decisions embedded in theirimplementations. systems such as Monet that are designed for in-memory performance yieldorders of magnitude improvements in query performance over commercial systems for …,*,*,*,7
Incorporating DATABASE TECHNOLOGY,Ricardo Baeza-Yates; Jan Van den Bussche; Felipe Carino Jr; TYPA Panepistimioupolis; B Kemme; N Koudas; Maurizio Lenzerini; P Loucopoulos; ID Melamed; Patrick O’Neil; Philippe Pucheral; KA Ross; Mathias Weske; Business ProcessTechnology; Masatoshi Yoshikawa; Gottfried Vossen; Dennis Shasha,*,*,*,*
Incorporating DATABASE TECHNOLOGY,Ricardo Baeza-Yates; Nicole Bidoit-Tollu; Jan Van den Bussche; Felipe Carino Jr; TYPA Panepistimioupolis; B Kemme; N Koudas; Maurizio Lenzerini; P Loucopoulos; ID Melamed; Patrick O’Neil; KA Ross; Mathias Weske; Business ProcessTechnology; Masatoshi Yoshikawa; Gottfried Vossen; Dennis Shasha,*,*,*,*
Programme Committee Members,Michele Adiba; Divyakant Agrawal; Peter Apers; Ricardo Baeza-Yates; Jose A Blakeley; Mokrane Bouzeghoub; Alex Buchmann; Peter Buneman; Michael Carey; Marco Casanova; Nick J Cercone; Sharma Chakravarthy; Jan Chomicki; Stavros Christodoulakis; Sophie Cluet; Armin B Cremers; Walter Cunto; Sergio Delgado; Klaus D&rich; Christos Faloutsos; Raymundo Forradellas; Antonio Furtado; Sumit Ganguly; Georges Gardarin; Narain Gehani; Goetz Graefe; Jim Gray; Ehud Gudes; Theo Haerder; Paula Hawthorn; Richard Hull; Sushi Jajodia; Christian Jensen; Manfred Jeusfeld; Yahiko Kambayayshi; Alfons Kemper; Ramamohanarao Kotagiri; Masaru Kitsuregawa; Michel Kuntz; Rosana Lanzelotte; Claudia Medeii; Alberto Mendelzon; Michele Missikoff; C Mohan; Ami Mono; Richard R Muntz; Erich Neuhold; Jack Orenstein; Maria Orlowska; Gultekin Ozsoyoglu; Alain Pirotte; Kenneth Ross; Ron Sacks-Davis; Felix Saltor; Betty Sal&erg; Joachim W Schmidt; Michael Schrefl; Amit P Sheth; Avi Silberschatz; Richard Snodgrass; Stefano Spaccapietra; VS Subrahmanian; TC Tay; Patrick Valduriez; Yannis Vassiliou; P Venkat Rangan; Victor Vianu; Gerhard Weikum; Jennifer Widom; Kam Fai Wong,Michele Adiba (France) Divyakant Agrawal (USA) Peter Apers (Netherlands) RicardoBaeza-Yates (Chile) David Bell (UK) Jose A. Blakeley (Mexico/USA) Mokrane Bouzeghoub(Prance) Alex Buchmann (Mexico/Germany) Peter Buneman (USA) Michael Carey (USA) MarcoCasanova (Brazil) Nick J. Cercone (Canada) Sharma Chakravarthy (USA) Jan Chomicki(USA) Stavros Christodoulakis (Greece) Sophie Cluet (France) Armin B. Cremers (Germany)Walter Cunto (Venezuela) S.Misbah Deen (UK) Sergio Delgado (Mexico) Klaus D&rich(Switzerland) Christos Faloutsos (USA) Raymundo Forradellas (Argentina) Antonio Furtado(Brazil) Sumit Ganguly (USA) Georges Gardarin (France) Narain Gehani (USA) Shahram Ghandeharizadeh(US A) Goetz Graefe (USA) Jim Gray (USA) Ehud Gudes (Israel) Theo Haerder (Germany) PaulaHawthorn (USA) Richard Hull (USA) Sushi1 Jajodia (USA) Christian Jensen (Denmark) …,*,*,*,1
On te Adequacy of Partial Orders for Preference omposition,Kenneth A Ross,Abstract We identify several anomalies in the behavior of conventional notions ofcomposition for preferences defined by strict partial orders. These anomalies can beavoided by defining a preorder that extends the given partial order; and using the pair oforders to define order composition.,*,*,*,20
Bulletin of the Technical Committee on,William DuMouchel Barbará; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,*,*,*,*
