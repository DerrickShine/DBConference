High-performance complex event processing over streams,Eugene Wu; Yanlei Diao; Shariq Rizvi,Abstract In this paper; we present the design; implementation; and evaluation of a systemthat executes complex event queries over real-time streams of RFID readings encoded asevents. These complex event queries filter and correlate events to match specific patterns;and transform the relevant events into new composite events for the use of externalmonitoring applications. Stream-based execution of these queries enables time-criticalactions to be taken in environments such as supply chain management; surveillance andfacility management; healthcare; etc. We first propose a complex event language thatsignificantly extends existing event languages to meet the needs of a range of RFID-enabledmonitoring applications. We then describe a query plan-based approach to efficientlyimplementing this language. Our approach uses native operators to efficiently handle …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,919,13
Webtables: exploring the power of tables on the web,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang,Abstract The World-Wide Web consists of a huge number of unstructured documents; but italso contains structured data in the form of HTML tables. We extracted 14.1 billion HTMLtables from Google's general-purpose web crawl; and used statistical classificationtechniques to find the estimated 154M that contain high-quality relational data. Becauseeach relational table has its own" schema" of labeled and typed columns; each such tablecan be considered a small structured database. The resulting corpus of databases is largerthan any other corpus we are aware of; by at least five orders of magnitude.,Proceedings of the VLDB Endowment,2008,523,12
Relational cloud: A database-as-a-service for the cloud,Carlo Curino; Evan PC Jones; Raluca Ada Popa; Nirmesh Malviya; Eugene Wu; Sam Madden; Hari Balakrishnan; Nickolai Zeldovich,This paper introduces a new transactional “database-as-a-service”(DBaaS) calledRelational Cloud. A DBaaS promises to move much of the operational burden ofprovisioning; configuration; scaling; performance tuning; backup; privacy; and access controlfrom the database users to the service operator; offering lower overall costs to users. EarlyDBaaS efforts include Amazon RDS and Microsoft SQL Azure; which are promising in termsof establishing the market need for such a service; but which do not address three importantchallenges: efficient multi-tenancy; elastic scalability; and database privacy. We argue thatthese three challenges must be overcome before outsourcing database software andmanagement becomes attractive to many users; and cost-effective for service providers. Thekey technical features of Relational Cloud include:(1) a workload-aware approach to multi …,*,2011,347,19
Human-powered sorts and joins,Adam Marcus; Eugene Wu; David Karger; Samuel Madden; Robert Miller,Abstract Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible totask people with small jobs; such as labeling images or looking up phone numbers; via aprogrammatic interface. MTurk tasks for processing datasets with humans are currentlydesigned with significant reimplementation of common workflows and ad-hoc selection ofparameters such as price to pay per task. We describe how we have integrated crowds intoa declarative workflow engine called Qurk to reduce the burden on workflow designers. Inthis paper; we focus on how to use humans to compare items for sorting and joining data;two of the most common operations in DBMSs. We describe our basic query interface andthe user interface of the tasks we post to MTurk. We also propose a number of optimizations;including task batching; replacing pairwise comparisons with numerical ratings; and pre …,Proceedings of the VLDB Endowment,2011,256,12
Design considerations for high fan-in systems: The HiFi approach,Michael J Franklin; Shawn R Jeffery; Sailesh Krishnamurthy; Frederick Reiss; Shariq Rizvi; Eugene Wu; Owen Cooper; Anil Edakkunni; Wei Hong,Abstract Advances in data acquisition and sensor technologies are leading towards thedevelopment of “high fan-in” architectures: widely distributed systems whose edges consistof numerous receptors such as sensor networks; RFID readers; or probes; and whoseinterior nodes are traditional host computers organized using the principles of cascadingstreams and successive aggregation. Examples include RFID-enabled supply chainmanagement; largescale environmental monitoring; and various types of network andcomputing infrastructure monitoring. In this paper; we identify the key characteristics anddata management challenges presented by high fan-in systems; and argue for a uniform;query-based approach towards addressing them. We then present our initial designconcepts behind HiFi; the system we are building to embody these ideas; and describe a …,*,2005,246,15
SASE: Complex event processing over streams,Daniel Gyllstrom; Eugene Wu; Hee-Jin Chae; Yanlei Diao; Patrick Stahlberg; Gordon Anderson,Abstract: RFID technology is gaining adoption on an increasing scale for tracking andmonitoring purposes. Wide deployments of RFID devices will soon generate anunprecedented volume of data. Emerging applications require the RFID data to be filteredand correlated for complex pattern detection and transformed to events that providemeaningful; actionable information to end applications. In this work; we design and developSASE; a com-plex event processing system that performs such data-informationtransformation over real-time streams. We design a complex event language for specifyingapplication logic for such transformation; devise new query processing techniques to effi-ciently implement the language; and develop a comprehensive system that collects; cleans;and processes RFID data for deliv-ery of relevant; timely information as well as storing …,arXiv preprint cs/0612128,2006,218,19
Crowdsourced databases: Query processing with people,Adam Marcus; Eugene Wu; David R Karger; Samuel R Madden; Robert C Miller,Amazon's Mechanical Turk (\MTurk") service allows users to post short tasks (\HITs") thatother users can receive a small amount of money for completing. Common tasks on thesystem include labelling a collection of images; com-bining two sets of images to identifypeople which appear in both; or extracting sentiment from a corpus of text snippets.Designing a work ow of various kinds of HITs for ltering; aggregating; sorting; and joiningdata sources together is common; and comes with a set of challenges in optimizing the costper HIT; the overall time to task completion; and the accuracy of MTurk results. We proposeQurk; a novel query system for managing these work ows; allowing crowd-poweredprocessing of relational databases. We describe a number of query execution andoptimization challenges; and discuss some potential solutions.,*,2011,192,5
Uncovering the relational web,Michael J Cafarella; Alon Y Halevy; Yang Zhang; Daisy Zhe Wang; Eugene Wu,ABSTRACT The World-Wide Web consists of a huge number of unstructured hypertextdocuments; but it also contains structured data in the form of HTML tables. Many of thesetables contain both relational-style data and a small “schema” of labeled and typed columns;making each such table a small structured database. The WebTables project is an effort toextract and make use of the huge number of these structured tables on the Web. A cleancollection of relational-style tables could be useful for improving web search; schemadesign; and many other applications. This paper describes the first stage of the WebTablesproject. First; we give an in-depth study of the Web's HTML table corpus. For example; weextracted 14.1 billion HTML tables from a several-billion-page portion of Google'sgeneralpurpose web crawl; and estimate that 154 million of these tables contain high …,under review,2008,134,12
Trajstore: An adaptive storage system for very large trajectory data sets,Philippe Cudre-Mauroux; Eugene Wu; Samuel Madden,The rise of GPS and broadband-speed wireless devices has led to tremendous excitementabout a range of applications broadly characterized as “location based services”. Currentdatabase storage systems; however; are inadequate for manipulating the very large anddynamic spatio-temporal data sets required to support such services. Proposals in theliterature either present new indices without discussing how to cluster data; potentiallyresulting in many disk seeks for lookups of densely packed objects; or use static quadtreesor other partitioning structures; which become rapidly suboptimal as the data or queriesevolve. As a result of these performance limitations; we built TrajStore; a dynamic storagesystem optimized for efficiently retrieving all data in a particular spatiotemporal region.TrajStore maintains an optimal index on the data and dynamically co-locates and …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,130,5
Scorpion: Explaining Away Outliers in Aggregate Queries,Eugene Wu; Samuel Madden,Abstract Database users commonly explore large data sets by running aggregate queriesthat project the data down to a smaller number of points and dimensions; and visualizing theresults. Often; such visualizations will reveal outliers that correspond to errors or surprisingfeatures of the input data set. Unfortunately; databases and visualization systems do notprovide a way to work backwards from an outlier point to the common properties of the(possibly many) unaggregated input tuples that correspond to that outlier. We proposeScorpion; a system that takes a set of user-specified outlier points in an aggregate queryresult as input and finds predicates that explain the outliers in terms of properties of the inputtuples that are used to compute the selected outlier results. Specifically; this explanationidentifies predicates that; when applied to the input data; cause the outliers to disappear …,VLDB,2013,71,12
Relational Cloud: The Case for a Database Service,Sam Madden; Eugene Wu; Samuel Madden; Yang Zhang; Evan Jones; Carlo Curino,In this paper; we make the case for â databases as a serviceâ (DaaS); with two targetscenarios in mind:(i) consolidation of data management functionality for large organizationsand (ii) outsourcing data management to a cloud-based service provider for small/mediumorganizations. We analyze the many challenges to be faced; and discuss the design of adatabase service we are building; called Relational Cloud. The system has been designedfrom scratch and combines many recent advances and novel solutions. The prototype wepresent exploits multiple dedicated storage engines; provides high-availability viatransparent replication; supports automatic workload partitioning and live data migration;and provides serializable distributed transactions. While the system is still under activedevelopment; we are able to present promising initial results that showcase the key …,*,2010,52,12
Probabilistic Data Management for Pervasive Computing: The Data Furnace Project.,Minos N Garofalakis; Kurt P Brown; Michael J Franklin; Joseph M Hellerstein; Daisy Zhe Wang; Eirinaios Michelakis; Liviu Tancau; Eugene Wu; Shawn R Jeffery; Ryan Aipperspach,Abstract The wide deployment of wireless sensor and RFID (Radio Frequency IDentification)devices is one of the key enablers for next-generation pervasive computing applications;including large-scale environmental monitoring and control; context-aware computing; and“smart digital homes”. Sensory readings are inherently unreliable and typically exhibit strongtemporal and spatial correlations (within and across different sensing devices); effectivereasoning over such unreliable streams introduces a host of new data managementchallenges. The Data Furnace project at Intel Research and UC-Berkeley aims to build aprobabilistic data management infrastructure for pervasive computing environments thathandles the uncertain nature of such data as a first-class citizen through a principledframework grounded in probabilistic models and inference techniques.,IEEE Data Eng. Bull.,2006,50,18
The case for data visualization management systems: vision paper,Eugene Wu; Leilani Battle; Samuel R Madden,Abstract Most visualizations today are produced by retrieving data from a database andusing a specialized visualization tool to render it. This decoupled approach results insignificant duplication of functionality; such as aggregation and filters; and missestremendous opportunities for cross-layer optimizations. In this paper; we present the case foran integrated Data Visualization Management System (DVMS) based on a declarativevisualization language that fully compiles the end-to-end visualization pipeline into a set ofrelational algebra queries. Thus the DVMS can be both expressive via the visualizationlanguage; and performant by lever-aging traditional and visualization-specific optimizationsto scale interactive visualizations to massive datasets.,Proceedings of the VLDB Endowment,2014,49,18
Demonstration of qurk: a query processor for humanoperators,Adam Marcus; Eugene Wu; David R Karger; Samuel Madden; Robert C Miller,Abstract Crowdsourcing technologies such as Amazon's Mechanical Turk (" MTurk") servicehave exploded in popularity in recent years. These services are increasingly used forcomplex human-reliant data processing tasks; such as labelling a collection of images;combining two sets of images to identify people that appear in both; or extracting sentimentfrom a corpus of text snippets. There are several challenges in designing a workflow thatfilters; aggregates; sorts and joins human-generated data sources. Currently; crowdsourcing-based workflows are hand-built; resulting in increasingly complex programs. Additionally;developers must hand-optimize tradeoffs among monetary cost; accuracy; and time tocompletion of results. These challenges are well-suited to a declarative query interface thatallows developers to describe their worflow at a high level and automatically optimizes …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,41,12
Vertexica: your relational friend for graph analytics!,Alekh Jindal; Praynaa Rawlani; Eugene Wu; Samuel Madden; Amol Deshpande; Mike Stonebraker,Abstract In this paper; we present Vertexica; a graph analytics tools on top of a relationaldatabase; which is user friendly and yet highly efficient. Instead of constraining programmersto SQL; Vertexica offers a popular vertex-centric query interface; which is more natural foranalysts to express many graph queries. The programmers simply provide their vertex-compute functions and Vertexica takes care of efficiently executing them in the standard SQLengine. The advantage of using Vertexica is its ability to leverage the relational features andenable much more sophisticated graph analysis. These include expressing graphalgorithms which are difficult in vertex-centric but straightforward in SQL and the ability tocompose end-to-end data processing pipelines; including pre-and post-processing ofgraphs as well as combining multiple algorithms for deeper insights. Vertexica has a …,Proceedings of the VLDB Endowment,2014,35,21
Hifi: A unified architecture for high fan-in systems,Owen Cooper; Anil Edakkunni; Michael J Franklin; Wei Hong; Shawn R Jeffery; Sailesh Krishnamurthy; Fredrick Reiss; Shariq Rizvi; Eugene Wu,Abstract Advances in data acquisition and sensor technologies are leading towards thedevelopment of" High Fan-in" architectures: widely distributed systems whose edges consistof numerous receptors such as sensor networks and RFID readers and whose interior nodesconsist of traditional host computers organized using the principle of successiveaggregation. Such architectures pose significant new data management challenges. TheHiFi system; under development at UC Berkeley; is aimed at addressing these challenges.We demonstrate an initial prototype of HiFi that uses data stream query processing toacquire; filter; and aggregate data from multiple devices including sensor motes; RFIDreaders; and low power gateways organized as a High Fan-in system.,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,2004,34,9
Clamshell: Speeding up crowds for low-latency data labeling,Daniel Haas; Jiannan Wang; Eugene Wu; Michael J Franklin,Abstract Data labeling is a necessary but often slow process that impedes the developmentof interactive systems for modern data analysis. Despite rising demand for manual datalabeling; there is a surprising lack of work addressing its high and unpredictable latency. Inthis paper; we introduce CLAMShell; a system that speeds up crowds in order to achieveconsistently low-latency data labeling. We offer a taxonomy of the sources of labelinglatency and study several large crowd-sourced labeling deployments to understand theirempirical latency profiles. Driven by these insights; we comprehensively tackle each sourceof latency; both by developing novel techniques such as straggler mitigation and poolmaintenance and by optimizing existing methods such as crowd retainer pools and activelearning. We evaluate CLAMShell in simulation and on live workers on Amazon's …,Proceedings of the VLDB Endowment,2015,27,10
Subzero: a fine-grained lineage system for scientific databases,Eugene Wu; Samuel Madden; Michael Stonebraker,Data lineage is a key component of provenance that helps scientists track and queryrelationships between input and output data. While current systems readily support lineagerelationships at the file or data array level; finer-grained support at an array-cell level isimpractical due to the lack of support for user defined operators and the high runtime andstorage overhead to store such lineage. We interviewed scientists in several domains toidentify a set of common semantics that can be leveraged to efficiently store fine-grainedlineage. We use the insights to define lineage representations that efficiently capturecommon locality properties in the lineage data; and a set of APIs so operator developers caneasily export lineage information from user defined operators. Finally; we introduce twobenchmarks derived from astronomy and genomics; and show that our techniques can …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,26,5
The case for rodentstore; an adaptive; declarative storage system,Philippe Cudre-Mauroux; Eugene Wu; Sam Madden,Abstract: Recent excitement in the database community surrounding new applications?analytic; scientific; graph; geospatial; etc.? has led to an explosion in research on databasestorage systems. New storage systems are vital to the database community; as they are atthe heart of making database systems perform well in new application domains.Unfortunately; each such system also represents a substantial engineering effort including agreat deal of duplication of mechanisms for features such as transactions and caching. Inthis paper; we make the case for RodentStore; an adaptive and declarative storage systemproviding a high-level interface for describing the physical representation of data.Specifically; RodentStore uses a declarative storage algebra whereby administrators (ordatabase design tools) specify how a logical schema should be grouped into collections …,arXiv preprint arXiv:0909.1779,2009,26,12
Partitioning techniques for fine-grained indexing,Eugene Wu; Samuel Madden,Many data-intensive websites use databases that grow much faster than the rate that usersaccess the data. Such growing datasets lead to ever-increasing space and performanceoverheads for maintaining and accessing indexes. Furthermore; there is often considerableskew with popular users and recent data accessed much more frequently. Theseobservations led us to design Shinobi; a system which uses horizontal partitioning as amechanism for improving query performance to cluster the physical data; and increasinginsert performance by only indexing data that is frequently accessed. We present databasedesign algorithms that optimally partition tables; drop indexes from partitions that areinfrequently queried; and maintain these partitions as workloads change. We show a 60×performance improvement over traditionally indexed tables using a real-world query …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,25,19
Wisteria: Nurturing scalable data cleaning infrastructure,Daniel Haas; Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Eugene Wu,Abstract Analysts report spending upwards of 80% of their time on problems in datacleaning. The data cleaning process is inherently iterative; with evolving cleaning workflowsthat start with basic exploratory data analysis on small samples of dirty data; then refineanalysis with more sophisticated/expensive cleaning operators (eg; crowdsourcing); andfinally apply the insights to a full dataset. While an analyst often knows at a logical level whatoperations need to be done; they often have to manage a large search space of physicaloperators and parameters. We present Wisteria; a system designed to support the iterativedevelopment and optimization of data cleaning workflows; especially ones that utilize thecrowd. Wisteria separates logical operations from physical implementations; and driven byanalyst feedback; suggests optimizations and/or replacements to the analyst's choice of …,Proceedings of the VLDB Endowment,2015,24,19
Automated metadata construction to support portable building applications,Arka A Bhattacharya; Dezhi Hong; David Culler; Jorge Ortiz; Kamin Whitehouse; Eugene Wu,Abstract Commercial buildings consume nearly 19\% of delivered energy in the US; nearlyhalf (42%) of which is consumed in buildings with digital control systems comprised of wiredsensor networks. These sensors have scant metadata; and are represented by``tags''whichare obscure; building-specific and not machine parseable. We develop a human-in-the-loopsynthesis technique which uses syntactic and data-driven steps to parse these sensor tagsinto a common namespace; which can enable portable building applications. We show thatour technique allows an expert to fully parse a large fraction (~ 70%) of the tags with 24; 15and 43 examples for three large commercial buildings comprising 1586; 2522 and 1865sensors respectively; and deploy three portable applications on two buildings with less than30 examples.,Proceedings of the 2nd ACM International Conference on Embedded Systems for Energy-Efficient Built Environments,2015,22,12
ActiveClean: interactive data cleaning for statistical modeling,Sanjay Krishnan; Jiannan Wang; Eugene Wu; Michael J Franklin; Ken Goldberg,Abstract Analysts often clean dirty data iteratively--cleaning some data; executing theanalysis; and then cleaning more data based on the results. We explore the iterativecleaning process in the context of statistical model training; which is an increasingly popularform of data analytics. We propose ActiveClean; which allows for progressive and iterativecleaning in statistical modeling problems while preserving convergence guarantees.ActiveClean supports an important class of models called convex loss models (eg; linearregression and SVMs); and prioritizes cleaning those records likely to affect the results. Weevaluate ActiveClean on five real-world datasets UCI Adult; UCI EEG; MNIST; IMDB; andDollars For Docs with both real and synthetic errors. The results show that our proposedoptimizations can improve model accuracy by up-to 2.5 x for the same amount of data …,Proceedings of the VLDB Endowment,2016,17,12
Collaborative data analytics with DataHub,Anant Bhardwaj; Amol Deshpande; Aaron J Elmore; David Karger; Sam Madden; Aditya Parameswaran; Harihar Subramanyam; Eugene Wu; Rebecca Zhang,Abstract While there have been many solutions proposed for storing and analyzing largevolumes of data; all of these solutions have limited support for collaborative data analytics;especially given the many individuals and teams are simultaneously analyzing; modifyingand exchanging datasets; employing a number of heterogeneous tools or languages fordata analysis; and writing scripts to clean; preprocess; or query data. We demonstrateDataHub; a unified platform with the ability to load; store; query; collaboratively analyze;interactively visualize; interface with external applications; and share datasets. We willdemonstrate the following aspects of the DataHub platform:(a) flexible data storage; sharing;and native versioning capabilities: multiple conference attendees can concurrently updatethe database and browse the different versions and inspect conflicts;(b) an app …,Proceedings of the VLDB Endowment,2015,17,10
Towards reliable interactive data cleaning: A user survey and recommendations,Sanjay Krishnan; Daniel Haas; Michael J Franklin; Eugene Wu,Abstract Data cleaning is frequently an iterative process tailored to the requirements of aspecific analysis task. The design and implementation of iterative data cleaning toolspresents novel challenges; both technical and organizational; to the community. In thispaper; we present results from a user survey (N= 29) of data analysts and infrastructureengineers from industry and academia. We highlight three important themes:(1) the iterativenature of data cleaning;(2) the lack of rigor in evaluating the correctness of data cleaning;and (3) the disconnect between the analysts who query the data and the infrastructureengineers who design the cleaning pipelines. We conclude by presenting a number ofrecommendations for future work in which we envision an interactive data cleaning systemthat accounts for the observed challenges.,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,11,12
SampleClean: Fast and Reliable Analytics on Dirty Data.,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu,Abstract An important obstacle to accurate data analytics is dirty data in the form of missing;duplicate; incorrect; or inconsistent values. In the SampleClean project; we have developeda new suite of techniques to estimate the results of queries when only a sample of data canbe cleaned. Some forms of data corruption; such as duplication; can affect samplingprobabilities; and thus; new techniques have to be designed to ensure correctness of theapproximate query results. We first describe our initial project on computing statisticallybounded estimates of sum; count; and avg queries from samples of cleaned data. Wesubsequently explored how the same techniques could apply to other problems in databaseresearch; namely; materialized view maintenance. To avoid expensive incrementalmaintenance; we maintain only a sample of rows in a view; and then leverage …,IEEE Data Eng. Bull.,2015,11,12
Activeclean: Interactive data cleaning while learning convex loss models,Sanjay Krishnan; Jiannan Wang; Eugene Wu; Michael J Franklin; Ken Goldberg,Abstract: Data cleaning is often an important step to ensure that predictive models; such asregression and classification; are not affected by systematic errors such as inconsistent; out-of-date; or outlier data. Identifying dirty data is often a manual and iterative process; and canbe challenging on large datasets. However; many data cleaning workflows can introducesubtle biases into the training processes due to violation of independence assumptions. Wepropose ActiveClean; a progressive cleaning approach where the model is updatedincrementally instead of re-training and can guarantee accuracy on partially cleaned data.ActiveClean supports a popular class of models called convex loss models (eg; linearregression and SVMs). ActiveClean also leverages the structure of a user's model toprioritize cleaning those records likely to affect the results. We evaluate ActiveClean on …,arXiv preprint arXiv:1601.03797,2016,9,19
A demonstration of DBWipes: clean as you query,Eugene Wu; Samuel Madden; Michael Stonebraker,Abstract As data analytics becomes mainstream; and the complexity of the underlying dataand computation grows; it will be increasingly important to provide tools that help analystsunderstand the underlying reasons when they encounter errors in the result. While dataprovenance has been a large step in providing tools to help debug complex workflows; itscurrent form has limited utility when debugging aggregation operators that compute a singleoutput from a large collection of inputs. Traditional provenance will return the entire inputcollection; which has very low precision. In contrast; users are seeking precise descriptionsof the inputs that caused the errors. We propose a Ranked Provenance System; whichidentifies subsets of inputs that influenced the output error; describes each subset withhuman readable predicates and orders them by contribution to the error. In this …,Proceedings of the VLDB Endowment,2012,8,10
Combining Design and Performance in a Data Visualization Management System.,Eugene Wu; Fotis Psallidas; Zhengjie Miao; Haoci Zhang; Laura Rettig; Yifan Wu; Thibault Sellam,ABSTRACT Interactive data visualizations have emerged as a prominent way to bring dataexploration and analysis capabilities to both technical and non-technical users. Despite theirubiquity and importance across applications; multiple design-and performance-relatedchallenges lurk beneath the visualization creation process. To meet these challenges;application designers either use visualization systems (eg; Endeca; Tableau; and Splunk)that are tailored to domainspecific analyses; or manually design; implement; and optimizetheir own solutions. Unfortunately; both approaches typically slow down the creationprocess. In this paper; we describe the status of our progress towards an end-to-endrelational approach in our data visualization management system (DVMS). We introduceDeVIL; a SQL-like language to express static as well as interactive visualizations as …,CIDR,2017,6,0
QFix: Diagnosing errors through query histories,Xiaolan Wang; Alexandra Meliou; Eugene Wu,Abstract Data-driven applications rely on the correctness of their data to function properlyand effectively. Errors in data can be incredibly costly and disruptive; leading to loss ofrevenue; incorrect conclusions; and misguided policy decisions. While data cleaning toolscan purge datasets of many errors before the data is used; applications and usersinteracting with the data can introduce new errors. Subsequent valid updates can obscurethese errors and propagate them through the dataset causing more discrepancies. Evenwhen some of these discrepancies are discovered; they are often corrected superficially; ona case-by-case basis; further obscuring the true underlying cause; and making detection ofthe remaining errors harder. In this paper; we propose QFix; a framework that derivesexplanations and repairs for discrepancies in relational data; by analyzing the effect of …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,5,12
Towards perception-aware interactive data visualization systems,Eugene Wu; Arnab Nandi,Page 1. Towards Perception-aware Interactive Data Visualization Systems Eugene Wu / ArnabNandi Columbia University / The Ohio State University Page 2. + (faster) VIZ Page 3. InteractionPage 4. Interaction Page 5. Interactive Visualization Page 6. Output Awareness Don't showmore data than # pixels in output M4 (database community) Immens (viz community) Readless data & render approximate results Error bars; uncertainty Sampling/Online AggApproximation Page 7. time am ount Page 8. Create Vis Resolution Awareness PerceptualAwareness Page 9. Graphical Perception Cleveland et al. > true value true value logerr Page10. Just Noticeable Difference how much change before you notice? JND ~ k * MagnitudeWeber's Law Steven's Law Page 11. Perceptual Functions as Abstractions Univariate (Cleveland)penc (true value) = err of perceived value Bivariate (JND) …,DSIA Workshop; IEEE VIS,2015,5,10
Pfunk-h: Approximate query processing using perceptual models,Daniel Alabi; Eugene Wu,Abstract Interactive visualization tools (eg; crossfilter) are critical to many data analysts bymaking the discovery and verification of hypotheses quick and seamless. Increasing datasizes has made the scalability of these tools a necessity. To bridge the gap between datasizes and interactivity; many visualization systems have turned to sampling-basedapproximate query processing frameworks. However; these systems are currently obliviousto human perceptual visual accuracy. This could either lead to overly aggressive samplingwhen the approximation accuracy is higher than needed or an incorrect visual renderingwhen the accuracy is too lax. Thus; for both correctness and efficiency; we propose to useempirical knowledge of human perceptual limitations to automatically bound the error ofapproximate answers meant for visualization. This paper explores a preliminary model of …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,4,3
Activeclean: An interactive data cleaning framework for modern machine learning,Sanjay Krishnan; Michael J Franklin; Ken Goldberg; Jiannan Wang; Eugene Wu,Abstract Databases can be corrupted with various errors such as missing; incorrect; orinconsistent values. Increasingly; modern data analysis pipelines involve Machine Learning;and the effects of dirty data can be difficult to debug. Dirty data is often sparse; and naivesampling solutions are not suited for high-dimensional models. We propose ActiveClean; aprogressive framework for training Machine Learning models with data cleaning. Ourframework updates a model iteratively as the analyst cleans small batches of data; andincludes numerous optimizations such as importance weighting and dirty data detection. Wedesigned a visual interface to wrap around this framework and demonstrate ActiveClean fora video classification problem and a topic modeling problem.,Proceedings of the 2016 International Conference on Management of Data,2016,4,5
Demonstration of the trajstore system,Eugene Wu; Philippe Cudre-Mauroux; Samuel Madden,Abstract The proliferation of GPS devices has led to a substantial interest in location basedservices. In particular; modern vehicles can generate an incredible amount of drive data.However; current storage systems are not optimized for storing and querying such largespatial-temporal data sets. In this demonstration; we show the performance of the TrajStoresystem; a dynamic storage system optimized for quickly accessing data in a particular spatial-temporal region. In particular; TrajStore uses a novel adaptive indexing technique thatdynamically adjusts itself to co-locate spatially close trajectories on disk; as well as anumber of compression techniques in the storage layer that significantly reduce access timefor a given index cell. In this demonstration; we will store a set of real world taxi cab drivetraces in TrajStore; and users will be able to query the data through a map based …,Proceedings of the VLDB Endowment,2009,4,12
No bits left behind,Eugene Wu; Carlo Curino; Samuel Madden,One of the key tenets of database system design is making efficient use of storage andmemory resources. However; existing database system implementations are actuallyextremely wasteful of such resources; for example; most systems leave a great deal of emptyspace in tuples; index pages; and data pages; and spend many CPU cycles reading coldrecords from disk that are never used. In this paper; we identify a number of such sources ofwaste; and present a series of techniques that limit this waste (eg; forcing better memorylocality for hot data and using empty space in index pages to cache popular tuples) withoutsubstantially complicating interfaces or system design. We show that these techniqueseffectively reduce memory requirements for real scenarios from the Wikipedia database (byup to 17.8×) while increasing query performance (by up to 8×).,*,2011,3,3
PALM: Machine Learning Explanations For Iterative Debugging,Sanjay Krishnan; Eugene Wu,Abstract When a Deep Neural Network makes a misprediction; it can be challenging for adeveloper to understand why. While there are many models for interpretability in terms ofpredictive features; it may be more natural to isolate a small set of training examples thathave the greatest influence on the prediction. However; it is often the case that every trainingexample contributes to a prediction in some way but with varying degrees of responsibility.We present Partition Aware Local Model (PALM); which is a tool that learns and summarizesthis responsibility structure to aide machine learning debugging. PALM approximates acomplex model (eg; a deep neural network) using a two-part surrogate model: a meta-modelthat partitions the training data; and a set of sub-models that approximate the patterns withineach partition. These sub-models can be arbitrarily complex to capture intricate local …,Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,2017,2,12
Skipping-oriented partitioning for columnar layouts,Liwen Sun; Michael J Franklin; Jiannan Wang; Eugene Wu,Abstract As data volumes continue to grow; modern database systems increasingly rely ondata skipping mechanisms to improve performance by avoiding access to irrelevant data.Recent work [39] proposed a fine-grained partitioning scheme that was shown to improvethe opportunities for data skipping in row-oriented systems. Modern analytics and big datasystems increasingly adopt columnar storage schemes; and in such systems; a row-basedapproach misses important opportunities for further improving data skipping. The flexibility ofcolumn-oriented organizations; however; comes with the additional cost of tuplereconstruction. In this paper; we develop Generalized Skipping-Oriented Partitioning(GSOP); a novel hybrid data skipping framework that takes into account these row-basedand column-based tradeoffs. In contrast to previous column-oriented physical design …,Proceedings of the VLDB Endowment,2016,2,12
Graphical Perception in Animated Data Visualizations,Eugene Wu; Lilong Jiang; Larry Xu; Arnab Nandi,*,*,2015,2
Scorpion,Eugene Wu; Samuel Madden,Q: Why the average temperature at 12PM and 1PM are unexpectedly high? A: The hightemperature caused by sensors near windows that heat up under the sun around noon andanother sensor running out of energy that starts producing erroneous readings.• MedicalCost Analysis: Amongst a population of cancer patients; the top 15% of patients by costrepresented more than 50% of the total dollars spent. Q: Were these patients significantlysicker? Did they have significantly better or worse outcomes than the median-cost patient. A:Small number of doctors were over-prescribing these procedures; which were presumablynot necessary because the outcomes didn't improve.,*,2005,2,16
A DeVIL-ish approach to inconsistency in interactive visualizations,Yifan Wu; Joseph M Hellerstein; Eugene Wu,Abstract Declarative languages have a long tradition in both the database systems and datavisualization communities; separating specifications from implementations. In databases;declarative languages like SQL shield application programmers from changes to physicaland logical properties like disk layouts; indexes and schema changes. In data visualization;declarative languages like Polaris; ggplot2 and Vega shield visualization programmers fromvariations in rendering; including screen layout; resolution; and color schemes. Declarativelanguages have been considered a foundational step forward in both communities.,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,1,21
QFix: Demonstrating error diagnosis in query histories,Xiaolan Wang; Alexandra Meliou; Eugene Wu,Abstract An increasing number of applications in all aspects of society rely on data. Despitethe long line of research in data cleaning and repairs; data correctness has been an elusivegoal. Errors in the data can be extremely disruptive; and are detrimental to the effectivenessand proper function of data-driven applications. Even when data is cleaned; new errors canbe introduced by applications and users who interact with the data. Subsequent validupdates can obscure these errors and propagate them through the dataset causing morediscrepancies. Any discovered errors tend to be corrected superficially; on a case-by-casebasis; further obscuring the true underlying cause; and making detection of the remainingerrors harder. In this demo proposal; we outline the design of QFix; a query-centricframework that derives explanations and repairs for discrepancies in relational data …,Proceedings of the 2016 International Conference on Management of Data,2016,1,19
Data Visualization Management Systems.,Eugene Wu,The holy grail of visualization systems makes exploring different data facets so intuitive; andrecommends views that are so relevant; that users rapidly converge onto valuable insights–irrespective of dataset size. Unfortunately; existing systems fall far short of this goal. Mostvisualizations are produced by retrieving raw data from a database and using a specializedvisualization tool to process and render it. Although the database can sometimes be used tofilter the raw data (eg; return data within a visible bounding box); visualization tools try toavoid roundtrips to the database by managing their own results cache and executing datatransformations directly. This approach is plagued with high communication costs; a loss ofvisualization-level semantic information; and over-engineering to ensure low latency (clientsoften re-implement data-processing operations).,CIDR,2015,1,3
Mobile applications need targeted micro-updates,Alvin Cheung; Lenin Ravindranath; Eugene Wu; Samuel Madden; Hari Balakrishnan,Abstract Smart-phone applications (" apps") run across a wide range of environmentalconditions; locations; and hardware platforms. They are often subject to an array ofinteractions that are hard or impossible for developers to emulate or even anticipate duringtesting. Once an application is released; feedback obtained from users and from analyticsover usage and performance data result in further modifications. Many of these changes arerelatively small; and can often be parameterized.,Proceedings of the 4th Asia-Pacific Workshop on Systems,2013,1,12
Data In Context: Aiding News Consumers while Taming Dataspaces,Adam Marcus; Eugene Wu; Sam Madden,ABSTRACT We present MuckRaker; a tool that provides news consumers with datasets andvisualizations that contextualize facts and figures in the articles they read. MuckRaker takesadvantage of data integration techniques to identify matching datasets; and makes use ofdata and schema extraction algorithms to identify data points of interest in articles. Itpresents the output of these algorithms to users requesting additional context; and allowsusers to further refine these outputs. In doing so; MuckRaker creates a synergisticrelationship between news consumers and the database research community; providingtraining data to improve existing algorithms; and a grand challenge for the next generation ofdataspace management research.,DBCrowd 2013,2013,1,12
Effect of nanoscale fillers on the viscoelasticity of polymer nanocomposites,Mohammad Bonakdar; Gary Seidel; Daniel Inman,For viscoelastic materials the stiffness and loss properties directly depend on not only strainbut also strain rate and implicitly depend on temperature via time temperature superposition;which in case of harmonic loading leads to frequency dependent response. For viscoelasticcomposites in which at least one of the constituent materials is viscoelastic; there is greatutility in the ability to predict the effective dynamic mechanical properties as a function of theconstituent phase properties and geometry. The presence of different concentrations ofnanofillers not only directly impacts the nanocomposite effective properties; but alsocontributes to the formation of an interphase layer; which also has a very strong influence onthe effective properties. The interphase layer usually has properties which are distinct fromthat of the particle and the matrix phases. In this paper micromechanical methods …,*,2012,1,10
Platform considerations in human computation,Adam Marcus; Eugene Wu; David R Karger; Samuel Madden; Robert C Miller,ABSTRACT With the recent growth of interest in human computation; the number ofcrowdsourcing platforms and corresponding workflows has been growing rapidly. Thispresents problems for both platform developers; who reimplement the same building blockswith each new platform; and human computation workflow developers; who must cope withthe increasing complexity of tasks that may span multiple platforms. In this paper; wedescribe two systems designed to alleviate these pain points. Qurk is a system that letsdevelopers describe workflows in a high-level declarative language; and whichautomatically optimizes the workflow across multiple platforms. Djurk is an open sourcehuman computation platform that is both usable out-of-the-box; and provides a feature-richstarting point for building new crowdsourcing platforms.,Workshop on crowdsourcing and human computation,2011,1,21
Shinobi: Insert-aware partitioning and indexing techniques for skewed database workloads,Eugene Wu,Many data-intensive websites are characterized by a dataset that grows much faster than therate that users access the data and possibly high insertion rates. In such systems; thegrowing size of the dataset leads to a larger overhead for maintaining and accessingindexes even while the query workload becomes increasingly skewed. Additionally; thedatabase index update costs can be a non-trivial proportion of the overall system cost.Shinobi introduces a cost model that takes index update costs account; and proposesdatabase design algorithms that optimally partition tables and drop indexes from partitionsthat are not queried often; and that maintain these partitions as workloads change. We showa 60x performance improvement over traditionally indexed tables using a real-world queryworkload derived from a traffic monitoring application and over 8x improvement for a …,*,2010,1,5
SmartCrawl: Deep Web Crawling Driven By Data Enrichment,Pei Wang; Ryan Shea; Jiannan Wang; Eugene Wu,ABSTRACT Entity resolution is defined as finding different records that refer to the same real-world entity. In this paper; we study deep entity resolution (DeepER) which aims to find pairsof records that describe the same entity between a local database and a hidden database.The local database can be accessed freely but the hidden database can only be accessedby a keyword-search query interface. To the best of our knowledge; we are the first to studythis problem. We first show that straightforward solutions are inefficient because they fail toexploit the ideas of query sharing and local-database-aware crawling. In response; wepropose SMARTCRAWL; a novel framework to overcome the limitations. Given a budget of bqueries; SMARTCRAWL first constructs a query pool based on the local database and theniteratively issues b queries to the hidden database such that the union of the query results …,*,*,1,18
Smoke: Fine-grained Lineage at Interactive Speed,Fotis Psallidas; Eugene Wu,Abstract: Data lineage describes the relationship between individual input and output dataitems of a workflow; and has served as an integral ingredient for both traditional (eg;debugging; auditing; data integration; and security) and emergent (eg; interactivevisualizations; iterative analytics; explanations; and cleaning) applications. The core; long-standing problem that lineage systems need to address---and the main focus of this paper---is to capture the relationships between input and output data items across a workflow withthe goal to streamline queries over lineage. Unfortunately; current lineage systems eitherincur high lineage capture overheads; or lineage query processing costs; or both. As aresult; applications; that in principle can express their logic declaratively in lineage terms;resort to hand-tuned implementations. To this end; we introduce Smoke; an in-memory …,arXiv preprint arXiv:1801.07237,2018,*,12
Mining Precision Interfaces From Query Logs,Haoci Zhang; Thibault Sellam; Eugene Wu,Abstract: Interactive tools make data analysis both more efficient and more accessible to abroad population. Simple interfaces such as Google Finance as well as complex visualexploration interfaces such as Tableau are effective because they are tailored to the desireduser tasks. Yet; designing interactive interfaces requires technical expertise and domainknowledge. Experts are scarce and expensive; and therefore it is currently infeasible toprovide tailored (or precise) interfaces for every user and every task. We envision a data-driven approach to generate tailored interactive interfaces. We observe that interactiveinterfaces are designed to express sets of programs; thus; samples of programs-increasinglycollected by data systems-may help us build interactive interfaces. Based on this idea;Precision Interfaces is a language-agnostic system that examines an input query log …,arXiv preprint arXiv:1712.00078,2017,*,12
BL-ECD: Broad Learning based Enterprise Community Detection via Hierarchical Structure Fusion,Jiawei Zhang; Limeng Cui; Philip S Yu; Yuanhua Lv,Abstract Employees in companies can be divided into different social communities; andthose who frequently socialize with each other will be treated as close friends and aregrouped in the same community. In the enterprise context; a large amount of informationabout the employees is available in both (1) offline company internal sources and (2) onlineenterprise social networks (ESNs). Each of the information sources also contain multiplecategories of employees' socialization activities at the same time. In this paper; we proposeto detect the social communities of the employees in companies based on the broadlearning setting with both these online and offline information sources simultaneously; andthe problem is formally called the" Broad Learning based Enterprise CommunityDetection"(BL-ECD) problem. To address the problem; a novel broad learning based …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*,18
BoostClean: Automated Error Detection and Repair for Machine Learning,Sanjay Krishnan; Michael J Franklin; Ken Goldberg; Eugene Wu,Abstract: Predictive models based on machine learning can be highly sensitive to data error.Training data are often combined with a variety of different sources; each susceptible todifferent types of inconsistencies; and new data streams during prediction time; the modelmay encounter previously unseen inconsistencies. An important class of suchinconsistencies is domain value violations that occur when an attribute value is outside of anallowed domain. We explore automatically detecting and repairing such violations byleveraging the often available clean test labels to determine whether a given detection andrepair combination will improve model accuracy. We present BoostClean whichautomatically selects an ensemble of error detection and repair combinations usingstatistical boosting. BoostClean selects this ensemble from an extensible library that is …,arXiv preprint arXiv:1711.01299,2017,*,14
Small Data,Oliver Kennedy; D Richard Hipp; Stratos Idreos; Amélie Marian; Arnab Nandi; Carmela Troncoso; Eugene Wu,Data is becoming increasingly personal. Individuals regularly interact with a wide variety ofstructured data; from SQLite databases on phones; to HR spreadsheets; to personalsensors; to open government data appearing in news articles. Although these workloads areimportant; many of the classical challenges associated with scale and Big Data do not apply.This panel brings together experts in a variety of fields to explore the new opportunities andchallenges presented by" Small Data".,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*,10
PreCog: Improving Crowdsourced Data Quality Before Acquisition,Hamed Nilforoshan; Jiannan Wang; Eugene Wu,Abstract: Quality control in crowdsourcing systems is crucial. It is typically done after datacollection; often using additional crowdsourced tasks to assess and improve the quality.These post-hoc methods can easily add cost and latency to the acquisition process--particularly if collecting high-quality data is important. In this paper; we argue for pre-hocinterface optimizations based on feedback that helps workers improve data quality before itis submitted and is well suited to complement post-hoc techniques. We propose the Precogsystem that explicitly supports such interface optimizations for common integrity constraintsas well as more ambiguous text acquisition tasks where quality is ill-defined. We thendevelop the Segment-Predict-Explain pattern for detecting low-quality text segments andgenerating prescriptive explanations to help the worker improve their text input. Our …,arXiv preprint arXiv:1704.02384,2017,*,21
Dialectic: Enhancing Text Input Fields with Automatic Feedback to Improve Social Content Writing Quality,Hamed Nilforoshan; James Sands; Kevin Lin; Rahul Khanna; Eugene Wu,Abstract: Modern social media relies on high quality user generated writing such as reviews;explanations; and answers. In contrast to standard validation to provide feedback forstructured inputs (eg; dates; email addresses); it is difficult to provide timely; high quality;customized feedback for free-form text input. While existing solutions based oncrowdsourced feedback (eg; up-votes and comments) can eventually produce high qualityfeedback; they suffer from high latency and costs; whereas fully automated approaches arelimited to syntactic feedback that does not address text content. We introduce Dialectic; anend-to-end extensible system that simplifies the process of creating; customizing; anddeploying content-specific feedback for free-text inputs. Our main observation is that manyservices already have a corpus of crowdsourced feedback that can be used to bootstrap a …,arXiv preprint arXiv:1701.06718,2017,*,10
Load-n-Go: Fast Approximate Join Visualizations That Improve Over Time,Marianne Procopio; Carlos Scheidegger; Eugene Wu; Remco Chang,Abstract—Visual exploratory analysis of large-scale databases often relies on precomputedquery results in order to guarantee interactivity with the visualization system. This isespecially true when the query requires joining tables across the database since join is oneof the most computationally expensive operations and in the worst case requires joiningevery row of one table to every row of the other table. Advances in approximate queryprocessing enable quick look summary statistics; but with limitations to the types andcomplexities of the queries. A recent advancement in approximate query processing is atechnique called Wander Join; that performs random walks across these joins; resulting infaster convergence on aggregation values. However; this online aggregation technique isnot tailored for visualization tasks that often involve filtering on one or more conditions or …,*,2017,*,11
Towards a Bayesian Model of Data Visualization Cognition,Yifan Wu; Larry Xu; Remco Chang; Eugene Wu,*,DECISIVE,2017,*
Searching for Meaning in RNNs using Deep Neural Inspection,Kevin Lin; Eugene Wu,Abstract Recent variants of Recurrent Neural Networks (RNNs)—in particular; Long Short-Term Memory (LSTM) networks—have established RNNs as a deep learning staple inmodeling sequential data in a variety of machine learning tasks. However; RNNs are stilloften used as a black box with limited understanding of the hidden representation that theylearn. Existing approaches such as visualization are limited by the manual effort to examinethe visualizations and require considerable expertise; while neural attention models change;rather than interpret; the model. We propose Deep Neural Inspection to search for neuronsbased on existing interpretable models; features; or programs.,*,2017,*,21
CIDR: Chat-oriented Innovations in Database Research.,Eugene Wu,The number of large innovations per year has been increasing rapidly since the start of thenew millennia (Figure??). From Hadoop and big data; to internet of things; to millennialstartups; to visualization; to deep learning; each new innovation has left an undeniable markon society; and ultimately; the world. Although the database community may not have led thecharge in these innovations; we have been successful at staking our claim to theinfrastructure and core tenants in each of these innovations. But must we continue to play areactive role? Should we not be the drivers in new innovative areas? The answer is clearly aresounding yes; but towards what nascent area? Well; do I have a bridge to sell you; and it ischat bots.,CIDR,2017,*,12
TrendQuery: a system for interactive exploration of trends,Niranjan Kamat; Eugene Wu; Arnab Nandi,Abstract The surfacing of trends from data collections such as user-generated contentstreams and news articles is a popular and important data analysis activity; used inapplications such as business intelligence; quantitative stock trading and; social mediaexploration. Unlike traditional content analysis; trend analysis includes an additional vitaltime dimension: a trend can be defined as a temporal pattern over a group of semanticallyrelated items. The unsupervised discovery of trends is often not sufficient; either due toinadequacies in the trend analysis algorithm; or because the data collection itself does notpossess all of the information to identify the trend. Thus; it is necessary for an expert human-in-the-loop to be involved in the process of trend analysis. To this end; we introduceTrendQuery; a system designed towards iterative and interactive surfacing of trends. Our …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,*,12
Indexing for Time-Sensitive Prediction,Eugene Wu; Yulun Du; Aditya Parameswaran; Leilani Battle; Edward Benson,ABSTRACT Online services such as ad serving or fraud detection increasingly depend onmaking fast predictions that meet hard latency bounds. Unfortunately; most machinelearning systems select a set of features that maximize prediction accuracy; and use all ofthem when making predictions; while ignoring the cost of extracting those features. Since thefeature extraction costs can easily dominate the total prediction latency; this approach is ill-suited to online prediction tasks. For example; a spike in user traffic that overloads CPUresources; or varying input sizes (eg; user profile histories or uploaded image size); cancause the cost (ie; time taken) of feature extraction to exceed hard response constraints. Inthese cases; it is desirable to pick a subset of the available features that maximizes theprediction accuracy while meeting the time constraint. We develop algorithms and …,*,2015,*,12
Explaining data in visual analytic systems,Eugene Wu,Data-driven decision making and data analysis has grown in both importance andavailability in the past decade; and has seen increasing acceptance in the broaderpopulation. Visual tools are needed to help non-technical users explore and make sense oftheir datasets. However even with existing tools; many common data analysis tasks are stillperformed using manual; error-prone methods; or simply inaccessible due to non-intuitiveinterfaces. In this thesis; we addressed a common data analysis task that is ill-served byexisting visual analytical tools. Specifically; although visualization tools are well suited toidentify patterns in datasets; they do not help users characterize surprising trends or outliersin the visualization and leave that task to the user. We explored the necessary techniques sousers can visually explore datasets; specify outliers in the resulting visualizations; and …,*,2015,*,6
Indexing Cost Sensitive Prediction,Leilani Battle; Edward Benson; Aditya Parameswaran; Eugene Wu,Abstract: Predictive models are often used for real-time decision making. However; typicalmachine learning techniques ignore feature evaluation cost; and focus solely on theaccuracy of the machine learning models obtained utilizing all the features available. Wedevelop algorithms and indexes to support cost-sensitive prediction; ie; making decisionsusing machine learning models taking feature evaluation cost into account. Given an itemand a online computation cost (ie; time) budget; we present two approaches to return anappropriately chosen machine learning model that will run within the specified time on thegiven item. The first approach returns the optimal machine learning model; ie; one with thehighest accuracy; that runs within the specified time; but requires significant up-frontprecomputation time. The second approach returns a possibly sub-optimal machine …,arXiv preprint arXiv:1408.4072,2014,*,12
Progressive Failure Analysis Method of a Pi Joint with Uncertainties in Fracture Properties,Wooseok Ji; Anthony Waas; Ravi Raveendra,I. Introduction dhesively bonded joint technology is now widely used in aircraft structuraldesigns because of its advantage over conventional fastening systems. Stressconcentrations that are unavoidable at fastener areas can be reduced with adhesivelybonded joints; and thus fatigue resistance can be significantly improved. Structural weightcan be reduced by replacement of the fastener hardware with the adhesive joints. Variousadhesive joint profiles are available for joining structural parts in a complex configurationthat may be difficult and costly to achieve with a mechanical fastening system. Especially; anemerging and promising concept in joining laminated structures is the “Pi joint”. The Pi-shaped joint improves performance by increasing the bonding area between adherends.However; the bonded interface is still the weakest link due to the large amount of load …,*,2012,*,21
Intelligent Conversational Systems,Emilie Kim; Alex Pesterev; Eugene Wu,If language is what truly separates humans from other animals; then good communicationwith language is what keeps humans together. All relationships are based on goodcommunication; whether it be familial; friendly; romantic; or professional. Without the abilityto share ideas or thoughts effectively; we as humans surely would not be able to function.However; when it comes to our relationships with computers; we seem to make exceptions.According to a study conducted by independent research firm Kelton Research; 65% ofconsumers report spending more time with their computer than with their significant other [5].Additionally; the report stated:,*,2008,*,18
Deeper: A Data Enrichment System Powered by Deep Web,Pei Wang; Yongjun He; Ryan Shea; Jiannan Wang; Eugene Wu,ABSTRACT Data scientists often spend more than 80% of their time on data preparation.Data enrichment; the act of extending a local database with new attributes from external datasources; is among the most time-consuming tasks. Existing data enrichment works areresource intensive: data-intensive by relying on web tables or knowledge bases; monetarily-intensive by purchasing entire datasets; or timeintensive by fully crawling a web-based datasource. In this work; we explore a more targeted alternative that uses resources (in terms ofweb API calls) proportional to the size of the local database of interest. We build Deeper; adata enrichment system powered by deep web. The goal of Deeper is to help data scientiststo link a local database to a hidden database so that they can easily enrich the localdatabase with the attributes from the hidden database. We find that a challenging …,*,*,*,10
“I Like the Way You Think!” Inspecting the Internal Logic of Recurrent Neural Networks,Thibault Sellam; Kevin Lin; Ian Yiran Huang; Carl Vondrick; Eugene Wu,Recurrent neural networks (RNNs) are revolutionizing many domains. Their expressivity andthe wide availability of training data enables them to tackle a wide range of problems;including language understanding [8]; image generation [9]; and program synthesis [5]; andthe proliferation of libraries and programming frameworks is significantly reducing the effortto construct and deploy them [2; 4; 12]. How do we ensure that learned models behavereliably and as intended? In traditional; non-learned software systems; we ensure reliabilitythrough software engineering principles [22] such as abstractions; modularity; testing; andlogical requirements. Modern engineers do not write software systems as a single codeblock; nor do they deploy them without understanding the system logic and extensivelytesting the system.,Hypothesis,*,*,5
ICDE 2017 Reviewers,Yannis Papakonstantinou; Lei Chen; Reynold Cheng; Wolfgang Gatterbauer; Bingsheng He; Stratos Idreos; Christopher Jermaine; Chen Li; Gerome Miklau; Tamer Özsu; Olga Papaemmanouil; Evimaria Terzi; Eugene Wu; Ashraf Aboulnaga; Alex Alves; Amazon Gabriel Antoniu; INRIA Arvind Arasu; Andrey Balmin; Workday Zhifeng Bao; Sumita Barahmand; Srikanta Bedathur; Carsten Binnig; Spyros Blanas; Marco Brambilla; Stephane Bressan; K Selcuk Candan; Zhao Cao; James Cheng; Fei Chiang; Panos K Chrysanthis; Philippe Cudre-Mauroux,ICDE 2017 Program Committee Chairs Yannis Papakonstantinou; University of California; SanDiego Yanlei Diao; Ecole Polytechnique; France; and University of Massachusetts; Amherst …ICDE 2017 Area Chairs Lei Chen; Hong Kong University of Science and Technology ReynoldCheng; University of Hong Kong Wolfgang Gatterbauer; Carnegie Mellon University BingshengHe; National University of Singapore Stratos Idreos; Harvard University ChristopherJermaine; Rice University Chen Li; University of California Irvine Gerome Miklau; University ofMassachusetts Tamer Özsu; University of Waterloo Olga Papaemmanouil; Brandeis UniversityEvimaria Terzi; Boston University Eugene Wu; Columbia University … ICDE 2017 Program CommitteeAshraf Aboulnaga; Qatar Computing Research Institute Alex Alves; Amazon Gabriel Antoniu;INRIA Arvind Arasu; Microsoft Research Andrey Balmin; Workday Zhifeng Bao; RMIT …,*,*,*,21
Web Data,Michael J Cafarella; Alon Halevy; Daisy Zhe Wang; Eugene Wu; Yang Zhang WebTables,Since the previous edition of this collection; the World Wide Web has unequivocally laid anylingering questions regarding its longevity and global impact to rest. Several multi-Billion-user services including Google and Facebook have become central to modern life in the firstworld; while Internet-and Web-related technology has permeated both business andpersonal interactions. The Web is undoubtedly here to stayat least for the foreseeable future.Web data systems bring a new set of challenges; including high scale; data heterogeneity;and a complex and evolving set of user interaction modes. Classical relational databasesystem designs did not have the Web workload in mind; and are not the technology ofchoice in this context. Rather; Web data management requires a melange of techniquesspanning information retrieval; database internals; data integration; and distributed …,*,*,*,12
Data Engineering,Sanjay Krishnan; Jiannan Wang; Michael J Franklin; Ken Goldberg; Tim Kraska; Tova Milo; Eugene Wu; Yachao Lu; Saravanan Thirumuruganathan; Nan Zhang; Gautam Das,Abstract There has been much research on various aspects of Approximate QueryProcessing (AQP); such as different sampling strategies; error estimation mechanisms; andvarious types of data synopses. However; many subtle challenges arise when building anactual AQP engine that can be deployed and used by real world applications. Thesesubtleties are often ignored (or at least not elaborated) by the theoretical literature andacademic prototypes alike. For the first time to the best of our knowledge; in this article; wefocus on these subtle challenges that one must address when designing an AQP system.Our intention for this article is to serve as a handbook listing critical design choices thatdatabase practitioners must be aware of when building or using an AQP system; not toprescribe a specific solution to each challenge.,*,*,*,3
Research Statement and Agenda,Eugene Wu,The past decade has seen tremendous growth in both the volume of data and the complexityof analyses that organizations want to perform. Increasingly; these analyses are notperformed by experienced database administrators but by domain experts that need simpleways to process their data without worrying about scalability and management limitations.To be useful for these users; new systems must simplify analyses and address performancebottlenecks at every step in the analysis pipeline. My work has focused on many of thesebottlenecks—robust data ingestion [7]; improving core query performance [3; 8; 11; 4; 2];supporting new forms of data and new domains [9; 5; 6; 1]; summarizing and debuggingquery results [12; 13]; and scaling up visualizations [10]. More broadly; my goal is to buildscalable and useful data management systems for real people. This is reflected in my …,*,*,*,10
Explanatory Lineage,Eugene Wu; Samuel Madden,ABSTRACT As data analytics becomes mainstream; and the complexity of the underlyingdata and computation grows; end-users will increasingly rely on visualizations to presentsimplified statistics that summarizes interesting properties in the data. It is even moreimportant to provide tools that help analysts understand the underlying reasons when theysee anomalous results. We envision adding a new explanatory dimension into visualizationlibraries and creation tools that automatically give end-users the capability to mine andunderstand the reasons for outliers and trends that they see in the visualizations. In thispaper; we propose an initial problem formulation targeted towards business dashboardapplications; and propose a set of modifications to existing declarative visualization librariesthat enables this ability in existing visualizations with minimal changes by the developer.,*,*,*,21
YFilter++–Efficient Declarative Querying Over Event Streams,Andy Carle; Eugene Wu,Abstract Event processing systems are growing in importance as more and more sequentialdata is becoming available from sensors and other real-time monitoring equipment. Much ofthis data must be processed quickly and efficiently in order to be useful. However; manycurrent systems are drastically slowed by dependence on SQL over streams which is veryinefficient for sequential data. We propose a new solution; an extension to the University ofCalifornia–Berkeley's YFilter system to handle event processing. Our new system; YFilter++;has shown much better performance than contemporary streaming database systems whenrun over event streams in initial testing. In addition; the event description language that wepropose is more expressive than SQL over streams. There are still many optimizations thatcould be made to YFilter++; but overall we believe we have created a very viable system …,*,*,*,2
