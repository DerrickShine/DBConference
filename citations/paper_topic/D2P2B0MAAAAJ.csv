MCDB: a monte carlo approach to managing uncertain data,Ravi Jampani; Fei Xu; Mingxi Wu; Luis Leopoldo Perez; Christopher Jermaine; Peter J Haas,Abstract To deal with data uncertainty; existing probabilistic database systems augmenttuples with attribute-level or tuple-level probability values; which are loaded into thedatabase along with the data itself. This approach can severely limit the system's ability togracefully handle complex or unforeseen types of uncertainty; and does not permit theuncertainty model to be dynamically parameterized according to the current state of thedatabase. We introduce MCDB; a system for managing uncertain data that is based on aMonte Carlo approach. MCDB represents uncertainty via" VG functions;" which are used topseudorandomly generate realized values for uncertain attributes. VG functions can beparameterized on the results of SQL queries over" parameter tables" that are stored in thedatabase; facilitating what-if analyses. By storing parameters; and not probabilities; and …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,273,10
Conditional anomaly detection,Xiuyao Song; Mingxi Wu; Christopher Jermaine; Sanjay Ranka,When anomaly detection software is used as a data analysis tool; finding the hardest-to-detect anomalies is not the most critical task. Rather; it is often more important to make surethat those anomalies that are reported to the user are in fact interesting. If too manyunremarkable data points are returned to the user labeled as candidate anomalies; thesoftware can soon fall into disuse. One way to ensure that returned anomalies are useful isto make use of domain knowledge provided by the user. Often; the data in question includesa set of environmental attributes whose values a user would never consider to be directlyindicative of an anomaly. However; such attributes cannot be ignored because they have adirect effect on the expected distribution of the result attributes whose values can indicate ananomalous observation. This paper describes a general purpose method called …,IEEE Transactions on Knowledge and Data Engineering,2007,213,12
Synopses for massive data: Samples; histograms; wavelets; sketches,Graham Cormode; Minos Garofalakis; Peter J Haas; Chris Jermaine,Abstract Methods for Approximate Query Processing (AQP) are essential for dealing withmassive data. They are often the only means of providing interactive response times whenexploring massive datasets; and are also needed to handle high speed data streams. Thesemethods proceed by computing a lossy; compact synopsis of the data; and then executingthe query of interest against the synopsis rather than the entire dataset. We describe basicprinciples and recent developments in AQP. We focus on four key synopses: randomsamples; histograms; wavelets; and sketches. We consider issues such as accuracy; spaceand time efficiency; optimality; practicality; range of applicability; error bounds on queryanswers; and incremental maintenance. We also discuss the trade-offs between the differentsynopsis types.,Foundations and Trends in Databases,2012,196,15
Efficient data allocation over multiple channels at broadcast servers,Wai Gen Yee; Shamkant B Navathe; Edward Omiecinski; Chris Jermaine,Broadcast is a scalable way of disseminating data because broadcasting an item satisfies alloutstanding client requests for it. However; because the transmission medium is shared;individual requests may have high response times. In this paper; we show how to minimizethe average response time given multiple broadcast channels by optimally partitioning dataamong them. We also offer an approximation algorithm that is less complex than the optimaland show that its performance is near-optimal for a wide range of parameters. Finally; webriefly discuss the extensibility of our work with two simple; yet seldom researchedextensions; namely; handling varying sized items and generating single channel schedules.,IEEE Transactions on Computers,2002,145,12
Online aggregation for large mapreduce jobs,Niketan Pansare; Vinayak R Borkar; Chris Jermaine; Tyson Condie,ABSTRACT In online aggregation; a database system processes a user's aggregation queryin an online fashion. At all times during processing; the system gives the user an estimate ofthe final query result; with the confidence bounds that become tighter over time. In thispaper; we consider how online aggregation can be built into a MapReduce system for large-scale data processing. Given the MapReduce paradigm's close relationship with cloudcomputing (in that one might expect a large fraction of MapReduce jobs to be run in thecloud); online aggregation is a very attractive technology. Since large-scale cloudcomputations are typically pay-as-you-go; a user can monitor the accuracy obtained in anonline fashion; and then save money by killing the computation early once sufficientaccuracy has been obtained.,Proc. VLDB Endow,2011,141,12
Scalable approximate query processing with the DBO engine,Chris Jermaine; Subramanian Arumugam; Abhijit Pol; Alin Dobra,Abstract This article describes query processing in the DBO database system. Like otherdatabase systems designed for ad hoc analytic processing; DBO is able to compute theexact answers to queries over a large relational database in a scalable fashion. Unlike anyother system designed for analytic processing; DBO can constantly maintain a guess as tothe final answer to an aggregate query throughout execution; along with statisticallymeaningful bounds for the guess's accuracy. As DBO gathers more and more information;the guess gets more and more accurate; until it is 100&percnt; accurate as the query iscompleted. This allows users to stop the execution as soon as they are happy with the queryaccuracy; and thus encourages exploratory data analysis.,ACM Transactions on Database Systems (TODS),2008,132,10
The DataPath system: a data-centric analytic processing engine for large data warehouses,Subi Arumugam; Alin Dobra; Christopher M Jermaine; Niketan Pansare; Luis Perez,Abstract Since the 1970's; database systems have been" compute-centric". When acomputation needs the data; it requests the data; and the data are pulled through thesystem. We believe that this is problematic for two reasons. First; requests for data naturallyincur high latency as the data are pulled through the memory hierarchy; and second; itmakes it difficult or impossible for multiple queries or operations that are interested in thesame data to amortize the bandwidth and latency costs associated with their data access. Inthis paper; we describe a purely-push based; research prototype database system calledDataPath. DataPath is" data-centric". In DataPath; queries do not request data. Instead; dataare automatically pushed onto processors; where they are then processed by any interestedcomputation. We show experimentally on a multi-terabyte benchmark that this basic …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,91,0
Statistical change detection for multi-dimensional data,Xiuyao Song; Mingxi Wu; Christopher Jermaine; Sanjay Ranka,Abstract This paper deals with detecting change of distribution in multi-dimensional datasets. For a given baseline data set and a set of newly observed data points; we define astatistical test called the density test for deciding if the observed data points are sampledfrom the underlying distribution that produced the baseline data set. We define a test statisticthat is strictly distribution-free under the null hypothesis. Our experimental results show thatthe density test has substantially more power than the two existing methods for multi-dimensional change detection.,Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,2007,83,12
Outlier detection by sampling with accuracy guarantees,Mingxi Wu; Christopher Jermaine,Abstract An effective approach to detecting anomalous points in a data set is distance-basedoutlier detection. This paper describes a simple sampling algorithm to effciently detectdistance-based outliers in domains where each and every distance computation is veryexpensive. Unlike any existing algorithms; the sampling algorithm requires a xed number ofdistance computations and can return good results with accuracy guarantees. The mostcomputationally expensive aspect of estimating the accuracy of the result is sorting all of thedistances computed by the sampling algorithm. The experimental study on two expensivedomains as well as ten additional real-life datasets demonstrates both the effciency andeffectiveness of the sampling algorithm in comparison with the state-of-the-art algorithm andthere liability of the accuracy guarantees.,Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,2006,69,18
Closest-point-of-approach join for moving object histories,Subramanian Arumugam; Chris Jermaine,In applications that produce a large amount of data describing the paths of moving objects;there is a need to ask questions about the interaction of objects over a long recorded history.In this paper; we consider the problem of computing joins over massive moving objecthistories. The particular join that we study is the" Closest-Point-Of-Approach" join; whichasks: Given a massive moving object history; which objects approached within a distance'd'of one another? We carefully consider several relatively obvious strategies for computingthe answer to such a join; and then propose a novel; adaptive join algorithm which naturallyalters the way in which it computes the join in response to the characteristics of theunderlying data.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,59,12
The partitioned exponential file for database storage management,Christopher Jermaine; Edward Omiecinski; Wai Gen Yee,Abstract The rate of increase in hard disk storage capacity continues to outpace the rate ofdecrease in hard disk seek time. This trend implies that the value of a seek is increasingexponentially relative to the value of storage. With this trend in mind; we introduce thepartitioned exponential file (PE file) which is a generic storage manager that can becustomized for many different types of data (eg; numerical; spatial; or temporal). The PE fileis intended for use in environments with intense update loads and concurrent; analyticqueries. Such an environment may be found; for example; in long-running scientificapplications which can produce petabytes of data. For example; the proposed LargeSynoptic Survey Telescope [36] will produce 50---100 petabytes of observational; scientificdata over its multi-year lifetime. This database will never be taken off-line; so bursty …,The VLDB Journal—The International Journal on Very Large Data Bases,2007,57,19
A novel index supporting high volume data warehouse insertion,Chris Jermaine; Anindya Datta; Edward Omiecinski,Abstract While the desire to support fast; ad hoc query processing for large data warehouseshas motivated the recent introduction of many new indexing structures; with a few notableexceptions (namely; the LSM-Tree [4] and the Stepped Merge Method [1]) little attention hasbeen given to developing new indexing schemes that allow fast insertions. Since additionsto a large warehouse may number in the millions per day; indices that require a disk seek (oreven a significant fraction of a seek) per insertion are not acceptable. In this paper; we offeran alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequentinsertions. The Y-tree is a new indexing structure supporting both point and range queriesover a single attribute; with retrieval performance comparable to the B+-tree. For processinginsertions; however; the Y-tree may exhibit a speedup of 100 times over batched …,VLDB,1999,55,18
Reference-based indexing of sequence databases,Jayendra Venkateswaran; Deepak Lachwani; Tamer Kahveci; Christopher Jermaine,Abstract We consider the problem of similarity search in a very large sequence databasewith edit distance as the similarity measure. Given limited main memory; our goal is todevelop a reference-based index that reduces the number of costly edit distancecomputations in order to answer a query. The idea in reference-based indexing is to select asmall set of reference sequences that serve as a surrogate for the other sequences in thedatabase. We consider two novel strategies for selecting references as well as a newstrategy for assigning references to database sequences. Our experimental results showthat our selection and assignment methods far outperform competitive methods. Forexample; our methods prune up to 20 times as many sequences as the Omni method; andas many as 30 times as many sequences as frequency vectors. Our methods also scale …,Proceedings of the 32nd international conference on Very large data bases,2006,50,12
Online maintenance of very large random samples,Christopher Jermaine; Abhijit Pol; Subramanian Arumugam,Abstract Random sampling is one of the most fundamental data management toolsavailable. However; most current research involving sampling considers the problem of howto use a sample; and not how to compute one. The implicit assumption is that a" sample" is asmall data structure that is easily maintained as new data are encountered; even thoughsimple statistical arguments demonstrate that very large samples of gigabytes or terabytes insize can be necessary to provide high accuracy. No existing work tackles the problem ofmaintaining very large; disk-based samples from a data management perspective; and notechniques now exist for maintaining very large samples in an online manner from streamingdata. In this paper; we present online algorithms for maintaining on-disk samples that aregigabytes or terabytes in size. The algorithms are designed for streaming data; or for any …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,50,19
Simulation of database-valued Markov chains using SimSQL,Zhuhua Cai; Zografoula Vagena; Luis Perez; Subramanian Arumugam; Peter J Haas; Christopher Jermaine,Abstract This paper describes the SimSQL system; which allows for SQLbased specification;simulation; and querying of database-valued Markov chains; ie; chains whose value at anytime step comprises the contents of an entire database. SimSQL extends the earlier MonteCarlo database system (MCDB); which permitted Monte Carlo simulation of static database-valued random variables. Like MCDB; SimSQL uses user-specified" VG functions" togenerate the simulated data values that are the building blocks of a simulated database. Theenhanced functionality of SimSQL is enabled by the ability to parametrize VG functionsusing stochastic tables; so that one stochastic database can be used to parametrize thegeneration of another stochastic database; which can parametrize another; and so on. Otherkey extensions include the ability to explicitly define recursive versions of a stochastic …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,49,17
A comparison of platforms for implementing and running very large scale machine learning algorithms,Zhuhua Cai; Zekai J Gao; Shangyu Luo; Luis L Perez; Zografoula Vagena; Christopher Jermaine,Abstract We describe an extensive benchmark of platforms available to a user who wants torun a machine learning (ML) inference algorithm over a very large data set; but cannot findan existing implementation and thus must" roll her own" ML code. We have carefully chosena set of five ML implementation tasks that involve learning relatively complex; hierarchicalmodels. We completed those tasks on four different computational platforms; and using70;000 hours of Amazon EC2 compute time; we carefully compared running times; tuningrequirements; and ease-of-programming of each.,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,38,21
The sort-merge-shrink join,Christopher Jermaine; Alin Dobra; Subramanian Arumugam; Shantanu Joshi; Abhijit Pol,Abstract One of the most common operations in analytic query processing is the applicationof an aggregate function to the result of a relational join. We describe an algorithm called theSort-Merge-Shrink (SMS) Join for computing the answer to such a query over large; disk-based input tables. The key innovation of the SMS join is that if the input data are clusteredin a statistically random fashion on disk; then at all times; the join provides an online;statistical estimator for the eventual answer to the query as well as probabilistic confidencebounds. Thus; a user can monitor the progress of the join throughout its execution and stopthe join when satisfied with the estimate's accuracy or run the algorithm to completion with atotal time requirement that is not much longer than that of other common join algorithms. Thiscontrasts with other online join algorithms; which either do not offer such statistical …,ACM Transactions on Database Systems (TODS),2006,37,19
New sampling-based estimators for OLAP queries,Ruoming Jin; Leonid Glimcher; Chris Jermaine; Gagan Agrawal,One important way in which sampling for approximate query processing in a databaseenvironment differs from traditional applications of sampling is that in a database; it isfeasible to collect accurate summary statistics from the data in addition to the sample. Thispaper describes a set of sampling-based estimators for approximate query processing thatmake use of simple summary statistics to to greatly increase the accuracy of sampling-basedestimators. Our estimators are able to give tight probabilistic guarantees on estimationaccuracy. They are suitable for low or high dimensional data; and work with categorical ornumerical attributes. Furthermore; the information used by our estimators can easily begathered in a single pass; making them suitable for use in a streaming environment.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,36,12
Materialized sample views for database approximation,Shantanu Joshi; Christopher Jermaine,We consider the problem of creating a sample view of a database table. A sample view is anindexed materialized view that permits efficient sampling from an arbitrary range query overthe view. Such" sample views" are very useful in applications that require random samplesfrom a database: approximate query processing; online aggregation; data mining; andrandomized algorithms are a few examples. Our core technical contribution is a new fileorganization called the appendability; combinability; and exponentiality (ACE) tree that issuitable for organizing and indexing a sample view. One of the most important aspects of theACE tree is that it supports online random sampling from the view. That is; at all times; theset of records returned by the ACE tree constitutes a statistically random sample of thedatabase records satisfying the relational selection predicate over the view. Our paper …,IEEE Transactions on Knowledge and Data Engineering,2008,35,21
A disk-based join with probabilistic guarantees,Christopher Jermaine; Alin Dobra; Subramanian Arumugam; Shantanu Joshi; Abhijit Pol,Abstract One of the most common operations in analytic query processing is the applicationof an aggregate function to the result of a relational join. We describe an algorithm forcomputing the answer to such a query over large; disk-based input tables. The keyinnovation of our algorithm is that at all times; it provides an online; statistical estimator forthe eventual answer to the query; as well as probabilistic confidence bounds. Thus; a usercan monitor the progress of the join throughout its execution and stop the join when satisfiedwith the estimate's accuracy; or run the algorithm to completion with a total time requirementthat is not much longer than other common join algorithms. This contrasts with other onlinejoin algorithms; which either do not offer such statistical guarantees or can only offerguarantees so long as the input data can fit into core memory.,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,35,21
The latent community model for detecting sybil attacks in social networks,Zhuhua Cai; Christopher Jermaine,Abstract Collaborative and recommendation-based computer systems are plagued byattackers who create fake or malicious identities to gain more influence in the system—suchattacks are often referred to as “Sybil attacks”. We propose a new statistical model andassociated learning algorithms for detecting Sybil attacks in a collaborative network; calledthe latent community (LC) model. The LC model is hierarchical; and groups the nodes in anetwork into closely linked communities that are linked relatively loosely with the rest of thegraph. Since the author of a Sybil attack will typically create many false identities and linkthem together in an attempt to gain influence in the network; a Sybil attack will oftencorrespond to a learned community in the LC model. Evaluation of the LC model using real-world networks validates the model and shows that it can be superior to competitive …,Proc. NDSS,2012,34,12
A LRT framework for fast spatial anomaly detection,Mingxi Wu; Xiuyao Song; Chris Jermaine; Sanjay Ranka; John Gums,Abstract Given a spatial data set placed on an nxn grid; our goal is to find the rectangularregions within which subsets of the data set exhibit anomalous behavior. We developalgorithms that; given any user-supplied arbitrary likelihood function; conduct a likelihoodratio hypothesis test (LRT) over each rectangular region in the grid; rank all of the rectanglesbased on the computed LRT statistics; and return the top few most interesting rectangles. Tospeed this process; we develop methods to prune rectangles without computing theirassociated LRT statistics.,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,32,10
A lrt framework for fast spatial anomaly detection,Mingxi Wu; Xiuyao Song; Chris Jermaine; Sanjay Ranka; John Gums,Abstract Given a spatial data set placed on an nxn grid; our goal is to find the rectangularregions within which subsets of the data set exhibit anomalous behavior. We developalgorithms that; given any user-supplied arbitrary likelihood function; conduct a likelihoodratio hypothesis test (LRT) over each rectangular region in the grid; rank all of the rectanglesbased on the computed LRT statistics; and return the top few most interesting rectangles. Tospeed this process; we develop methods to prune rectangles without computing theirassociated LRT statistics.,Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,2009,32,16
The monte carlo database system: Stochastic analysis close to the data,Ravi Jampani; Fei Xu; Mingxi Wu; Luis Perez; Chris Jermaine; Peter J Haas,Abstract The application of stochastic models and analysis techniques to large datasets isnow commonplace. Unfortunately; in practice this usually means extracting data from adatabase system into an external tool (such as SAS; R; Arena; or Matlab); and then runningthe analysis there. This extract-and-model paradigm is typically error-prone; slow; does notsupport fine-grained modeling; and discourages what-if and sensitivity analyses. In thisarticle we describe MCDB; a database system that permits a wide spectrum of stochasticmodels to be used in conjunction with the data stored in a large database; without everextracting the data. MCDB facilitates in-database execution of tasks such as riskassessment; prediction; and imputation of missing data; as well as management of errorsdue to data integration; information extraction; and privacy-preserving data …,ACM Transactions on Database Systems (TODS),2011,29,12
Turbo-charging estimate convergence in dbo,Alin Dobra; Chris Jermaine; Florin Rusu; Fei Xu,Abstract DBO is a database system that utilizes randomized algorithms to give statisticallymeaningful estimates for the final answer to a multi-table; disk-based query from start tofinish during query execution. However; DBO's" time'til utility"(or" TTU"; that is; the time untilDBO can give a useful estimate) can be overly large; particularly in the case that manydatabase tables are joined in a query; or in the case that a join query includes a veryselective predicate on one or more of the tables; or when the data are skewed. In this paper;we describe Turbo DBO; which is a prototype database system that can answer multi-tablejoin queries in a scalable fashion; just like DBO. However; Turbo DBO often has a muchlower TTU than DBO. The key innovation of Turbo DBO is that it makes use of novelalgorithms that look for and remember" partial match" tuples in a randomized fashion …,Proceedings of the VLDB Endowment,2009,27,4
A bayesian method for guessing the extreme values in a data set?,Mingxi Wu; Christopher Jermaine,Abstract For a large number of data management problems; it would be very useful to beable to obtain a few samples from a data set; and to use the samples to guess the largest (orsmallest) value in the entire data set. Min/max online aggregation; top-k query processing;outlier detection; and distance join are just a few possible applications. This paper details astatistically rigorous; Bayesian approach to attacking this problem. Just as importantly; wedemonstrate the utility of our approach by showing how it can be applied to two specificproblems that arise in the context of data management.,Proceedings of the 33rd international conference on Very large data bases,2007,26,21
Bridging the gap between response time and energy-efficiency in broadcast schedule design,Wai Gen Yee; Shamkant B Navathe; Edward Omiecinski; Christopher Jermaine,Abstract In this paper; we propose techniques for scheduling data broadcasts that arefavorable in terms of both response and tuning time. In other words; these techniquesensure that a typical data request will be quickly satisfied and its reception will require a lowclient-side energy expenditure. By generating broadcast schedules based on Acharya etal.'s broadcast disk paradigm; we bridge the gap between these two mutually exclusivebodies of work—response time and energy expenditure. We prove the utility of our approachanalytically and via experiments. Our analysis of optimal scheduling is presented under avariety of assumptions about size and popularity of data items; making our resultsgeneralizable to a range of applications.,International Conference on Extending Database Technology,2002,25,12
A sampling algebra for aggregate estimation,Supriya Nirkhiwale; Alin Dobra; Christopher Jermaine,Abstract As of 2005; sampling has been incorporated in all major database systems. Whileefficient sampling techniques are realizable; determining the accuracy of an estimateobtained from the sample is still an unresolved problem. In this paper; we present atheoretical framework that allows an elegant treatment of the problem. We base our work ongeneralized uniform sampling (GUS); a class of sampling methods that subsumes a widevariety of sampling techniques. We introduce a key notion of equivalence that allows GUSsampling operators to commute with selection and join; and derivation of confidenceintervals. We illustrate the theory through extensive examples and give indications on how touse it to provide meaningful estimates in database systems.,Proceedings of the VLDB Endowment,2013,24,12
Relational confidence bounds are easy with the bootstrap,Abhijit Pol; Christopher Jermaine,Abstract Statistical estimation and approximate query processing have become increasinglyprevalent applications for database systems. However; approximation is usually of little usewithout some sort of guarantee on estimation accuracy; or" confidence bound." Analyticallyderiving probabilistic guarantees for database queries over sampled data is a daunting task;not suitable for the faint of heart; and certainly beyond the expertise of the typical databasesystem end-user. This paper considers the problem of incorporating into a database systema powerful" plug-in" method for computing confidence bounds on the answer to relationaldatabase queries over sampled or incomplete data. This statistical tool; called the bootstrap;is simple enough that it can be used by a data-base programmer with a rudimentarymathematical background; but general enough that it can be applied to almost any …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,24,18
-Robust Estimation With Sampling and Approximate Pre-Aggregation,Christopher Jermaine,This chapter considers the problem of approximation of aggregate functions over categoricaldata; or mixed categorical/numerical data. It proposes a method based upon randomsampling; called approximate pre-aggregation (APA); a framework for using simplesummary statistics to greatly increase the accuracy of random sampling for estimation ofaggregate queries over categorical or mixed categorical/numerical data. This is importantbecause many previous estimation techniques have largely ignored categorical data. APA isbased upon sound; statistical techniques such as maximum likelihood estimation andconstrained quadratic programming. It is also suitable for estimation in a streamingenvironment; since the information used by APA can be collected in a single database scan.The biggest drawback of sampling for aggregate function estimating is the sensitivity of …,*,2003,24,20
Finding the most interesting correlations in a database: How hard can it be?,Christopher Jermaine,Abstract This paper addresses some of the foundational issues associated with discoveringthe best few correlations from a database. Specifically; we consider the computationalcomplexity of various definitions of the “top-k correlation problem;” where the goal is todiscover the few sets of events whose co-occurrence exhibits the smallest degree ofindependence. Our results show that many rigorous definitions of correlation lead tointractable and strongly inapproximable problems. Proof of this inapproximability issignificant; since similar problems studied by the computer science theory community haveresisted such analysis. One goal of the paper (and for future research) is to developalternative correlation metrics whose use will both allow efficient search and produce resultsthat are satisfactory for users.,Information Systems,2005,23,12
Robust stratified sampling plans for low selectivity queries,Shantanu Joshi; Christopher Jermaine,We consider the problem of estimating the result of an aggregate query with a very lowselectivity. Traditional sampling techniques can be ineffective for such a problem since asmall random sample is likely to miss most or even all of the records satisfying the restrictiveselection predicate. Stratfied sampling is useful in this situation; but a key problem inapplying stratified sampling effectively is identifying which strata are important anddeveloping a sampling plan that favors those strata in a robust fashion. We develop asolution to this problem that combines any prior knowledge or expectation about thestratification with information obtained from pilot sampling in a principled Bayesianframework.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,22,20
Sampling-based estimators for subset-based queries,Shantanu Joshi; Christopher Jermaine,Abstract We consider the problem of using sampling to estimate the result of an aggregationoperation over a subset-based SQL query; where a subquery is correlated to an outer queryby a NOT EXISTS; NOT IN; EXISTS or IN clause. We design an unbiased estimator for ourquery and prove that it is indeed unbiased. We then provide a second; biased estimator thatmakes use of the superpopulation concept from statistics to minimize the mean squarederror of the resulting estimate. The two estimators are tested over an extensive set ofexperiments.,The VLDB Journal—The International Journal on Very Large Data Bases,2009,21,10
Reference-based indexing for metric spaces with costly distance measures,Jayendra Venkateswaran; Tamer Kahveci; Christopher Jermaine; Deepak Lachwani,Abstract We consider the problem of similarity search in databases with costly metricdistance measures. Given limited main memory; our goal is to develop a reference-basedindex that reduces the number of comparisons in order to answer a query. The idea inreference-based indexing is to select a small set of reference objects that serve as asurrogate for the other objects in the database. We consider novel strategies for selection ofreferences and assigning references to database objects. For dynamic databases withfrequent updates; we propose two incremental versions of the selection algorithm. Ourexperimental results show that our selection and assignment methods far outperformcompeting methods.,The VLDB Journal—The International Journal on Very Large Data Bases,2008,21,10
Online estimation for subset-based SQL queries,Christopher Jermaine; Alin Dobra; Abhijit Pol; Shantanu Joshi,Abstract The largest databases in use today are so large that answering a query exactly cantake minutes; hours; or even days. One way to address this problem is to make use ofapproximation algorithms. Previous work on online aggregation has considered how to giveonline estimates with ever-increasing accuracy for aggregate functions over relational joinand selection queries. However; no existing work is applicable to online estimation oversubset-based SQL queries-those queries with a correlated subquery linked to an outerquery via a NOT EXISTS; NOT IN; EXISTS; or IN clause (other queries such as EXCEPT andINTERSECT can also be seen as subset-based queries). In this paper we developalgorithms for online estimation over such queries; and consider the difficult problem ofproviding probabilistic accuracy guarantees at all times during query execution.,Proceedings of the 31st international conference on Very large data bases,2005,19,0
MCDB-R: Risk analysis in the database,Subi Arumugam; Fei Xu; Ravi Jampani; Christopher Jermaine; Luis L Perez; Peter J Haas,Abstract Enterprises often need to assess and manage the risk arising from uncertainty intheir data. Such uncertainty is typically modeled as a probability distribution over theuncertain data values; specified by means of a complex (often predictive) stochastic model.The probability distribution over data values leads to a probability distribution over databasequery results; and risk assessment amounts to exploration of the upper or lower tail of aquery-result distribution. In this paper; we extend the Monte Carlo Database System toefficiently obtain a set of samples from the tail of a query-result distribution by adaptingrecent" Gibbs cloning" ideas from the simulation literature to a database setting.,Proceedings of the VLDB Endowment,2010,17,12
The dbo database system,Florin Rusu; Fei Xu; Luis Leopoldo Perez; Mingxi Wu; Ravi Jampani; Chris Jermaine; Alin Dobra,Abstract We demonstrate our prototype of the DBO database system. DBO is designed tofacilitate scalable analytic processing over large data archives. DBO's analytic processingperformance is competitive with other database systems; however; unlike any other existingresearch or industrial system; DBO maintains a statistically meaningful guess to the finalanswer to a query from start to finish during query processing. This guess may be quiteaccurate after only a few seconds or minutes; while answering a query exactly may takehours. This can result in significant savings in both user and computer time; since a user canabort a query as soon as he or she is happy with the guess' accuracy.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,17,12
The computational complexity of high-dimensional correlation search,Chris Jermaine,There is a growing awareness that the popular support metric (often used to guide search inmarket-basket analysis) is not appropriate for use in every association mining application.Support measures only the co-occurrence frequency of a set of events when determiningwhich patterns to report back to the user. It incorporates no rigorous statistical notion ofsurprise or interest; and many of the patterns deemed interesting by the support metric areuninteresting to the user. However; a positive aspect of support is that search using supportis very efficient. The question addresses in the paper is: can we retain this efficiency if wemove beyond support; and to other more rigorous metrics? We consider the computationalimplications of incorporating simple expectation into the data mining task. It turns out thatmany variations on the problem which incorporate more rigorous tests of dependence (or …,Data Mining; 2001. ICDM 2001; Proceedings IEEE International Conference on,2001,17,12
Evaluation of probabilistic threshold queries in MCDB,Luis L Perez; Subi Arumugam; Christopher M Jermaine,Abstract MCDB is a prototype database system for managing stochastic models for uncertaindata. In this paper; we study the problem of how to use MCDB to answer statistical queriesthat search for database objects which satisfy some filter condition with greater (or less than)a user-specified probability. For example:" Which packages will arrive late with> 5%probability?"" Which regions will see more than a 2% decline in sales with> 50%probability?"" What items will be out of stock by Friday with> 20% probability?" We considerboth the systems aspects and the statistical aspects of the problem.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,14,12
Confidence bounds for sampling-based group by estimates,Fei Xu; Christopher Jermaine; Alin Dobra,Abstract Sampling is now a very important data management tool; to such an extent that aninterface for database sampling is included in the latest SQL standard. In this article wereconsider in depth what at first may seem like a very simple problem—computing the errorof a sampling-based guess for the answer to a GROUP BY query over a multitable join. Thedifficulty when sampling for the answer to such a query is that the same sample will be usedto guess the result of the query for each group; which induces correlations among theestimates. Thus; from a statistical point-of-view it is very problematic and even dangerous touse traditional methods such as confidence intervals for communicating estimate accuracyto the user. We explore ways to address this problem; and pay particular attention to thecomputational aspects of computing “safe” confidence intervals.,ACM Transactions on Database Systems (TODS),2008,13,8
Maintaining very large random samples using the geometric file,Abhijit Pol; Christopher Jermaine; Subramanian Arumugam,Abstract Random sampling is one of the most fundamental data management toolsavailable. However; most current research involving sampling considers the problem of howto use a sample; and not how to compute one. The implicit assumption is that a" sample" is asmall data structure that is easily maintained as new data are encountered; even thoughsimple statistical arguments demonstrate that very large samples of gigabytes or terabytes insize can be necessary to provide high accuracy. No existing work tackles the problem ofmaintaining very large; disk-based samples from a data management perspective; and notechniques now exist for maintaining very large samples in an online manner from streamingdata. In this paper; we present online algorithms for maintaining on-disk samples that aregigabytes or terabytes in size. The algorithms are designed for streaming data; or for any …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,13,12
Playing hide-and-seek with correlations,Christopher Jermaine,Abstract We present a method for very high-dimensional correlation analysis. The methodrelies equally on rigorous search strategies and on human interaction. At each step; themethod conservatively" shaves off" a fraction of the database tuples and attributes; so thatmost of the correlations present in the data are not affected by the decomposition. Instead;the correlations become more obvious to the user; because they are hidden in a muchsmaller portion of the database. This process can be repeated iteratively and interactively;until only the most important correlations remain. The main technical difficulty of theapproach is figuring out how to" shave off" part of the database so as to preserve mostcorrelations. We develop an algorithm for this problem that has a polynomial running timeand guarantees result quality.,Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,2003,12,12
A model-agnostic framework for fast spatial anomaly detection,Mingxi Wu; Chris Jermaine; Sanjay Ranka; Xiuyao Song; John Gums,Abstract Given a spatial dataset placed on an n× n grid; our goal is to find the rectangularregions within which subsets of the dataset exhibit anomalous behavior. We developalgorithms that; given any user-supplied arbitrary likelihood function; conduct a likelihoodratio hypothesis test (LRT) over each rectangular region in the grid; rank all of the rectanglesbased on the computed LRT statistics; and return the top few most interesting rectangles. Tospeed this process; we develop methods to prune rectangles without computing theirassociated LRT statistics.,ACM Transactions on Knowledge Discovery from Data (TKDD),2010,11,6
Computing program modularizations using the k-cut method,Chris Jermaine,The problem of producing modularizations of huge legacy software systems which lack anobvious modular structure is an important one. Such modularizations allow maintenanceand reuse of smaller; more manageable pieces of program source code; and also mayprovide insights into the overall structure of the software system. We consider the applicationof an algorithm from graph theory known as the k-cut method to the domain of computingprogram modularizations. We describe the k-cut method; its associated algorithms; and theproblem's application to reverse engineering of legacy systems. We also describe ourexperience in applying the method.,Reverse Engineering; 1999. Proceedings. Sixth Working Conference on,1999,11,12
Mixture models for learning low-dimensional roles in high-dimensional data,Manas Somaiya; Christopher Jermaine; Sanjay Ranka,Abstract Archived data often describe entities that participate in multiple roles. Each of theseroles may influence various aspects of the data. For example; a register transaction collectedat a retail store may have been initiated by a person who is a woman; a mother; an avidreader; and an action movie fan. Each of these roles can influence various aspects of thecustomer's purchase: the fact that the customer is a mother may greatly influence thepurchase of a toddler-sized pair of pants; but have no influence on the purchase of an action-adventure novel. The fact that the customer is an action move fan and an avid reader mayinfluence the purchase of the novel; but will have no effect on the purchase of a shirt. In thispaper; we present a generic; Bayesian framework for capturing exactly this situation. In ourframework; it is assumed that multiple roles exist; and each data point corresponds to an …,Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,2010,10,21
Online random shuffling of large database tables,Christopher Jermaine,Many applications require a randomized ordering of input data. Examples includealgorithms for online aggregation; data mining; and various randomized algorithms. Mostexisting work seems to assume that accessing the records from a large database in arandomized order is not a difficult problem. However; it turns out to be extremely difficult inpractice. Using existing methods; randomization is either extremely expensive at the frontend (as data are loaded); or at the back end (as data are queried). This paper presents asimple file structure which supports both efficient; online random shuffling of a largedatabase; as well as efficient online sampling or randomization of the database when it isqueried. The key innovation of our method is the introduction of a small degree of carefullycontrolled; rigorously monitored nonrandomness into the file,IEEE transactions on knowledge and data engineering,2007,10,21
Grading the graders: Motivating peer graders in a MOOC,Yanxin Lu; Joe Warren; Christopher Jermaine; Swarat Chaudhuri; Scott Rixner,Abstract In this paper; we detail our efforts at creating and running a controlled studydesigned to examine how students in a MOOC might be motivated to do a better job duringpeer grading. This study involves more than one thousand students of a popular MOOC. Weask two specific questions:(1) When a student knows that his or her own peer grading effortsare being examined by peers; does this knowledge alone tend to motivate the student to doa better job when grading assignments? And (2) when a student not only knows that his orher own peer grading efforts are being examined by peers; but he or she is also given anumber of other peer grading efforts to evaluate (so the peer graders see how other peergraders evaluate assignments); do both of these together tend to motivate the student to doa better job when grading assignments? We find strong statistical evidence that``grading …,Proceedings of the 24th International Conference on World Wide Web,2015,9,6
History-aware query optimization with materialized intermediate views,Luis L Perez; Christopher M Jermaine,The use of materialized views derived from the intermediate results of frequently executedqueries is a popular strategy for improving performance in query workloads. Optimizerscapable of matching such views with inbound queries can generate alternative executionplans that read the materialized contents directly instead of re-computing the correspondingsubqueries; which tends to result in reduced query execution times. In this paper; weintroduce an architecture called Hawc that extends a cost-based logical optimizer with thecapability to use history information to identify query plans that; if executed; produceintermediate result sets that can be used to create materialized views with the potential toreduce the execution time of future queries. We present techniques for using knowledge ofpast queries to assist the query optimizer and match; generate and select useful …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,9,21
Guessing the extreme values in a data set: a Bayesian method and its applications,Mingxi Wu; Chris Jermaine,Abstract For a large number of data management problems; it would be very useful to beable to obtain a few samples from a data set; and to use the samples to guess the largest (orsmallest) value in the entire data set. Min/max online aggregation; Top-k query processing;outlier detection; and distance join are just a few possible applications. This paper details astatistically rigorous; Bayesian approach to attacking this problem. Just as importantly; wedemonstrate the utility of our approach by showing how it can be applied to four specificproblems that arise in the context of data management.,The VLDB Journal—The International Journal on Very Large Data Bases,2009,9,12
Learning to grade student programs in a massive open online course,Anna Drummond; Yanxin Lu; Swarat Chaudhuri; Christopher Jermaine; Joe Warren; Scott Rixner,We study the problem of automatically evaluating the quality of computer programsproduced by students in a very large; online; interactive programming course (or" MOOC").Automatically evaluating interactive programs (such as computer games) is not easybecause such programs lack any sort of well-defined logical specification. As an alternative;we devise some simple statistical approaches to assigning a score to a student-producedcode.,Data Mining (ICDM); 2014 IEEE International Conference on,2014,8,14
Predicting intracranial pressure and brain tissue oxygen crises in patients with severe traumatic brain injury,Risa B Myers; Christos Lazaridis; Christopher M Jermaine; Claudia S Robertson; Craig G Rusin,Objectives: To develop computer algorithms that can recognize physiologic patterns intraumatic brain injury patients that occur in advance of intracranial pressure and partial braintissue oxygenation crises. The automated early detection of crisis precursors can provideclinicians with time to intervene in order to prevent or mitigate secondary brain injury.Design: A retrospective study was conducted from prospectively collected physiologic data.intracranial pressure; and partial brain tissue oxygenation crisis events were defined asintracranial pressure of greater than or equal to 20 mm Hg lasting at least 15 minutes andpartial brain tissue oxygenation value of less than 10 mm Hg for at least 10 minutes;respectively. The physiologic data preceding each crisis event were used to identifyprecursors associated with crisis onset. Multivariate classification models were applied to …,Critical care medicine,2016,7,12
A bayesian mixture model with linear regression mixing proportions,Xiuyao Song; Chris Jermaine; Sanjay Ranka; John Gums,Abstract Classic mixture models assume that the prevalence of the various mixturecomponents is fixed and does not vary over time. This presents problems for applicationswhere the goal is to learn how complex data distributions evolve. We develop models andBayesian learning algorithms for inferring the temporal trends of the components in amixture model as a function of time. We show the utility of our models by applying them tothe real-life problem of tracking changes in the rates of antibiotic resistance in Escherichiacoli and Staphylococcus aureus. The results show that our methods can derive meaningfultemporal antibiotic resistance patterns.,Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,2008,7,12
Bayesian Sketch Learning for Program Synthesis,Vijayaraghavan Murali; Swarat Chaudhuri; Chris Jermaine,Abstract: We present a data-driven approach to the problem of inductive computer programsynthesis. Our method learns a probabilistic model for real-world programs from a corpus ofexisting code. It uses this model during synthesis to automatically infer a posteriordistribution over sketches; or syntactic models of the problem to be synthesized. Sketchessampled from this posterior are then used to drive combinatorial synthesis of a program in ahigh-level programming language. The key technical innovation of our approach---embodied in a system called Bayou---is utilizing user-supplied evidence as to the program'sdesired behavior; along with a Bayesian update; to obtain a posterior distribution over theprogram's true; latent specification (indicating user intent); which in turn produces a posteriorover possible sketches. As we show experimentally; explicitly modeling uncertainty in …,arXiv preprint arXiv:1703.05698,2017,6,12
Risk analysis for data-intensive stochastic models,*,A risk analysis system and method are provided. The system includes an analyzer foranalyzing database instances by executing a query on each database instance andselecting a cutoff value. The analyzer also discards the sets of uncertainty data that yieldquery-result values below the cutoff value and retains the database instances that yieldquery-result values above the cutoff value as elite sets. The system also includes a cloner toreplicate the elite sets; and a sampler to modify the elite sets so that each elite set is mutuallystatistically independent while still yielding query-result values above the cutoff value.,*,2012,6,12
Scalable linear algebra on a relational database system,Shangyu Luo; Zekai J Gao; Michael Gubanov; Luis L Perez; Christopher Jermaine,As data analytics has become an important application for modern data managementsystems; a new category of data management system has appeared recently: the scalablelinear algebra system. In this paper; we argue that a parallel or distributed database systemis actually an excellent platform upon which to build such functionality. Most relationalsystems already have support for cost-based optimization—which is vital to scaling linearalgebra computations—and it is well-known how to make relational systems scale. We showthat by making just a few changes to a parallel/distributed relational database system; sucha system can be a competitive platform for scalable linear algebra. Taken together; ourresults should at least raise the possibility that brand new systems designed from the groundup to support scalable linear algebra are not absolutely necessary; and that such systems …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,5,3
The pairwise gaussian random field for high-dimensional data imputation,Zhuhua Cai; Christopher Jermaine; Zografoula Vagena; Dionysios Logothetis; Luis L Perez,In this paper; we consider the problem of imputation (recovering missing values) in very high-dimensional data with an arbitrary covariance structure. The modern solution to this problemis the Gaussian Markov random field (GMRF). The problem with applying a GMRF to veryhigh-dimensional data imputation is that while the GMRF model itself can be useful even fordata having tens of thousands of dimensions; utilizing a GMRF requires access to asparsified; inverse covariance matrix for the data. Computing this matrix using even state-of-the-art methods is very costly; as it typically requires first estimating the covariance matrixfrom the data (at a O (nm2) cost for m dimensions and n data points) and then performing aregularized inversion of the estimated covariance matrix; which is also very expensive. Thisis impractical for even moderately-sized; high-dimensional data sets. In this paper; we …,Data Mining (ICDM); 2013 IEEE 13th International Conference on,2013,5,12
TRIAL: A Tool for Finding Distant Structural Similarities,Jayendra Venkateswaran; Bin Song; Tamer Kahveci; Chris Jermaine,Abstract Finding structural similarities in distantly related proteins can reveal functionalrelationships that can not be identified using sequence comparison. Given two proteins Aand B and threshold\epsilon Å; we develop an algorithm; TRiplet-based Iterative ALignment(TRIAL) for computing the transformation of B that maximizes the number of aligned residuessuch that the root mean square deviation (RMSD) of the alignment is at most\epsilon Å. Ouralgorithm is designed with the specific goal of effectively handling proteins with low similarityin primary structure; where existing algorithms perform particularly poorly. Experiments showthat our method outperforms existing methods. TRIAL alignment brings the secondarystructures of distantly related proteins to similar orientations. It also finds larger number ofsecondary structure matches at lower RMSD values and increased overall alignment …,IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),2011,5,12
Learning correlations using the mixture-of-subsets model,Manas Somaiya; Christopher Jermaine; Sanjay Ranka,Abstract Using a mixture of random variables to model data is a tried-and-tested methodcommon in data mining; machine learning; and statistics. By using mixture modeling it isoften possible to accurately model even complex; multimodal data via very simplecomponents. However; the classical mixture model assumes that a data point is generatedby a single component in the model. A lot of datasets can be modeled closer to theunderlying reality if we drop this restriction. We propose a probabilistic framework; themixture-of-subsets (MOS) model; by making two fundamental changes to the classicalmixture model. First; we allow a data point to be generated by a set of components; ratherthan just a single component. Next; we limit the number of data attributes that eachcomponent can influence. We also propose an EM framework to learn the MOS model …,ACM Transactions on Knowledge Discovery from Data (TKDD),2008,5,19
Lossy reduction for very high dimensional data,Chris Jermaine; Edward Omiecinski,We consider the use of data reduction techniques for the problem of approximate queryanswering. We focus on applications for which accurate answers to selective queries arerequired; and for which the data are very high dimensional (having hundreds of attributes).We present a new data reduction method for this type of application; called the RS kernel.We demonstrate the effectiveness of this method for answering difficult; highly selectivequeries over high dimensional data using several real datasets.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,4,12
Topic models for feature selection in document clustering,Anna Drummond; Zografoula Vagena; Chris Jermaine,Abstract We investigate the idea of using a topic model such as the popular Latent DirichletAllocation model as a feature selection step for unsupervised document clustering; wheredocuments are clustered using the proportion of the various topics that are present in eachdocument. One concern with using “vanilla” LDA as a feature selection method for input to aclustering algorithm is that the Dirichlet prior on the topic mixing proportions is too smoothand well-behaved. It does not encourage a “bumpy” distribution of topic mixing proportionvectors; which is what one would desire as input to a clustering algorithm. As such; wepropose two variant topic models that are designed to do a better job of producing topicmixing proportions that have a good clustering structure.,*,2013,3,12
Topic models over spoken language,Niketan Pansare; Chris Jermaine; Peter Haas; Nitendra Rajput,Virtually all work on topic modeling has assumed that the topics are to be learned over a text-based document corpus. However; there exist important applications where topic modelsmust be learned over an audio corpus of spoken language. Unfortunately; speech-to-textprograms can have very low accuracy. We therefore propose a novel topic model for spokenlanguage that incorporates a statistical model of speech-to-text software behavior. Crucially;our model exploits the uncertainty numbers returned by the software. Our ideas apply to anydomain in which it would be useful to build a topic model over data in which uncertaintiesare explicitly represented.,Data Mining (ICDM); 2012 IEEE 12th International Conference on,2012,3,0
Out from under the trees [linear file template],Chris Jermaine; Edward Omiecinski; Wai Gen Yee,Hard disk storage capacity continues to increase expo- nentially; at the same time that thereis little change in hard disk seek time. This means that per byte of storage; performing a seekbecomes exponentially more expen- sive over time. We believe that this trend will make populartree- based structures (such as the B-Tree and the R-Tree) less and less useful in the longrun. Hierarchical structures typically require two seeks per insertion: one to read the leaf pagewhich will receive the insert; and one to write it back again to disk. Since pages are often locatedran- domly on disk; multiple seeks can be required to evaluate large range queries. In environmentswith intense update/ analytic query loads (such as a modern data warehouse) those seeks maybe debilitating. However; while seeks have become more expensive; sequential disk I/O capabilityhas done a much better job keeping up with ever-increasing hard disk capacities. In this …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,3,12
Data-Driven Program Completion,Yanxin Lu; Swarat Chaudhuri; Chris Jermaine; David Melski,Abstract: We introduce program splicing; a programming methodology that aims to automatethe commonly used workflow of copying; pasting; and modifying code available online.Here; the programmer starts by writing a" draft" that mixes unfinished code; natural languagecomments; and correctness requirements in the form of test cases or API call sequenceconstraints. A program synthesizer that interacts with a large; searchable database ofprogram snippets is used to automatically complete the draft into a program that meets therequirements. The synthesis process happens in two stages. First; the synthesizer identifiesa small number of programs in the database that are relevant to the synthesis task. Next ituses an enumerative search to systematically fill the draft with expressions and statementsfrom these relevant programs. The resulting program is returned to the programmer; who …,arXiv preprint arXiv:1705.09042,2017,2,10
Hybrid: A large-scale linear-relational database management system,Michael Gubanov; Christopher Jermaine; Zekai Gao; Shangyu Luo,Aspects of Relational Database Management Systems (RDBMSs) make them attractiveplatforms for implementing and executing large-scale statistical data processing andmachine learning tasks. Much of the world's data is stored in RDBMSs; and it is desirable torun statistical algorithms using the same system where the data are stored; withoutextracting them and re-loading elsewhere. Further; relational databases are fundamentallybased upon the declarative programming paradigm: the programmer specifies what he orshe wants; and not how to compute it. This should be especially compelling formathematicians and statisticians; who are rarely experts in implementation strategies fordistributed computations. In contrast to a code written directly on top of a system such asHadoop or Shark [4]; a declarative system automatically chooses the best execution …,MIT Annual DB Conference,2016,2,21
Correlating Surgical Vital Sign Quality with 30-Day Outcomes using Regression on Time Series Segment Features,Risa B Myers; John C Frenzel; Joseph R Ruiz; Christopher M Jermaine,Abstract Anesthesiologists are taught to carefully manage patient vital signs during surgery.Unfortunately; there is little empirical evidence that vital sign management; as currentlypracticed; is correlated with patient outcomes. We seek to validate or repudiate currentclinical practice. Using a database of over 90;000 cases; we attempt to determine whetherthose cases that an anesthesiologist would subjectively decide are “low quality” are morelikely to result in negative outcomes. The problem reduces to one of multidimensional timeseries classification. Our approach is to have an expert anesthesiologist label a smallnumber of training cases; from which we can train a classifier to use to label all 90;000cases. We then use the labeling to search for correlation with outcomes. We considerseveral standard classification methods; such as dynamic time warping in conjunction …,*,2015,2,12
Very large scale bayesian inference using mcdb,Zhuhua Cai; Zografoula Vagena; Christopher Jermaine; P Haas,Abstract This extended abstract describes how the Monte Carlo database system (MCDB)can be used to easily implement Bayesian inference via Markov chain Monte Carlo (MCMC)over very large data sets. To implement an MCMC simulation in MCDB; a programmerspecifies dependencies among variables and how they parameterize one another using theSQL language. A user can write MCDB SQL without worrying about how the code will beparallelized across many machines; MCDB automatically takes care of the optimization andparallelization.,Big Learn Workshop; Advances in Neural Information Processing Systems,2011,2,12
Maintaining a large spatial database with T2SM,Christopher Jermaine; Edward Omiecinksi; Wai Gen Yee,Abstract Large; spatial databases may be required to prosses both intense query andintense update loads. For example; a data warehouse which contains spatial data may berequired to answer difficult; analytical queries at the same time as it accepts massiveamounts of new data; and any downtime to merge new data into existing data organizationsmay be unacceptable. Traditional; incremental spatial access methods (like the popular R-tree and its variants) may be unacceptable for use in such an environment since theysupport relatively slow update rates; and may lead to a relatively poor global dataorganization resulting in slow query evaluation. In this paper; we present the details T2SM;which is an instantiation of the linear file template; for use with spatial data. in T2SM; dataare organized as a set of ongoing; external memory sorts based on the STR algorithm …,Proceedings of the 9th ACM international symposium on Advances in geographic information systems,2001,2,12
PlinyCompute: A Platform for High-Performance; Distributed; Data-Intesive Tool Development,Jia Zou; R Matthew Barnett; Tania Lorido-Botran; Shangyu Luo; Carlos Monroy; Sourav Sikdar; Kia Teymourian; Binhang Yuan; Chris Jermaine,Abstract: This paper describes PlinyCompute; a system for development of high-performance; data-intensive; distributed computing tools and libraries. In the large;PlinyCompute presents the programmer with a very high-level; declarative interface; relyingon automatic; relational-database style optimization to figure out how to stage distributedcomputations. However; in the small; PlinyCompute presents the capable systemsprogrammer with a persistent object data model and API (the" PC object model") andassociated memory management system that has been designed from the ground-up forhigh performance; distributed; data-intensive computing. This contrasts with most other BigData systems; which are constructed on top of the Java Virtual Machine (JVM); and hencemust at least partially cede performance-critical concerns such as memory management …,arXiv preprint arXiv:1711.05573,2017,1,12
An experimental comparison of complex object implementations for big data systems,Sourav Sikdar; Kia Teymourian; Chris Jermaine,Abstract Many cloud-based data management and analytics systems support complexobjects. Dataflow platforms such as Spark and Flink allow programmers to manipulate setsconsisting of objects from a host programming language (often Java). Document databasessuch as MongoDB make use of hierarchical interchange formats---most popularly JSON---which embody a data model where individual records can themselves contain sets ofrecords. Systems such as Dremel and AsterixDB allow complex nesting of data structures.Clearly; no system designer would expect a system that stores JSON objects as text toperform at the same level as a system based upon a custom-built physical data model. Thequestion we ask is: How significant is the performance hit associated with choosing aparticular physical implementation? Is the choice going to result in a negligible …,Proceedings of the 2017 Symposium on Cloud Computing,2017,1,7
Bayesian specification learning for finding API usage errors,Vijayaraghavan Murali; Swarat Chaudhuri; Chris Jermaine,Abstract We present a Bayesian framework for learning probabilistic specifications fromlarge; unstructured code corpora; and then using these specifications to statically detectanomalous; hence likely buggy; program behavior. Our key insight is to build a statisticalmodel that correlates all specifications hidden inside a corpus with the syntax and observedbehavior of programs that implement these specifications. During the analysis of a particularprogram; this model is conditioned into a posterior distribution that prioritizes specificationsthat are relevant to the program. The problem of finding anomalies is now framedquantitatively; as a problem of computing a distance between a" reference distribution" overprogram behaviors that our model expects from the program; and the distribution overbehaviors that the program actually produces. We implement our ideas in a system …,Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,2017,1,12
Real-time High Performance Anomaly Detection over Data Streams: Grand Challenge,Dimitrije Jankov; Sourav Sikdar; Rohan Mukherjee; Kia Teymourian; Chris Jermaine,Abstract Real-time analytics over data streams are crucial for a wide range of use cases inindustry and research. Today's sensor systems can produce high throughput data streamsthat have to be analyzed in real-time. One important analytic task is anomaly or outlierdetection from the streaming data. In many industry applications; sensing devices produce adata stream that can be monitored to know the correct operation of industry devices andconsequently avoid damages by triggering reactions in real-time. While anomaly detection isa well-studied topic in data mining; the real-time high-performance anomaly detection frombig data streams require special studies and well-optimized implementation. This paperpresents our implementation of a real-time anomaly detection system over data streams. Weoutline details of our two separate implementations using the Java and C++ programming …,Proceedings of the 11th ACM International Conference on Distributed and Event-based Systems,2017,1,12
The BUDS Language for Distributed Bayesian Machine Learning,Zekai J Gao; Shangyu Luo; Luis L Perez; Chris Jermaine,Abstract We describe BUDS; a declarative language for succinctly and simply specifying theimplementation of large-scale machine learning algorithms on a distributed computingplatform. The types supported in BUDS--vectors; arrays; etc.--are simply logical abstractionsuseful for programming; and do not correspond to the actual implementation. In fact; BUDSautomatically chooses the physical realization of these abstractions in a distributed system;by taking into account the characteristics of the data. Likewise; there are many availableimplementations of the abstract operations offered by BUDS (matrix multiplies; transposes;Hadamard products; etc.). These are tightly coupled with the physical representation. InBUDS; these implementations are co-optimized along with the representation. All of thisallows for the BUDS compiler to automatically perform deep optimizations of the user's …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1,5
Finding Likely Errors with Bayesian Specifications,Vijayaraghavan Murali; Swarat Chaudhuri; Chris Jermaine,Abstract: We present a Bayesian framework for learning probabilistic specifications fromlarge; unstructured code corpora; and a method to use this framework to statically detectanomalous; hence likely buggy; program behavior. The distinctive insight here is to build astatistical model that correlates all specifications hidden inside a corpus with the syntax andobserved behavior of programs that implement these specifications. During the analysis of aparticular program; this model is conditioned into a posterior distribution that prioritizesspecifications that are relevant to this program. This allows accurate program analysis evenif the corpus is highly heterogeneous. The problem of finding anomalies is now framedquantitatively; as a problem of computing a distance between a" reference distribution" overprogram behaviors that our model expects from the program; and the distribution over …,arXiv preprint arXiv:1703.01370,2017,1,12
ESICM LIVES 2016: part two,S Sivakumar; FS Taccone; KA Desai; C Lazaridis; M Skarzynski; M Sekhon; W Henderson; D Griesdale; L Chapple; A Deane; L Williams; R Strickland; K Lange; D Heyland; M Chapman; MJ Rowland; P Garry; J Westbrook; R Corkill; CA Antoniades; KT Pattinson; G Fatania; AJ Strong; RB Myers; CM Jermaine; CS Robertson; CG Rusin; J Hofmeijer; L Sondag; MC Tjepkema-Cloostermans; A Beishuizen; FH Bosch; MJAM van Putten; L Carteron; C Patet; D Solari; M Oddo; MA Ali; C Dias; R Almeida; A Vaz-Ferreira; J Silva; E Monteiro; A Cerejo; AP Rocha; AA Elsayed; AM Abougabal; BN Beshey; KM Alzahaby; S Pozzebon; A Blandino Ortiz; S Cristallini; O Lheureux; A Brasseur; JL Vincent; J Creteur; M Hravnak; K Yousef; Y Chang; E Crago; RM Friedlander; SA Abdelmonem; SA Tahon; TA Helmy; HS Meligy; F Puig; I Dunn-Siegrist; J Pugin; S Gupta; D Govil; S Srinivasan; SJ Patel; A Gupta; DS Tomar; M Shafi; R Harne; DP Arora; N Talwar; S Mazumdar; EE Papakrivou; D Makris; E Manoulakas; B Tsolaki; B Karadodas; E Zakynthinos; I Palacios Garcia; A Diaz Martin; V Sanchez Encinares; M Pachón Ibañez; J Garnacho Montero; G Labrador; T Cebrero Cangueiro; V Poulose; J Koh; JW Kam; H Yeter; A Kara; O Aktepe; A Topeli; I Tsolakoglou; G Intas; P Stergiannis; AA Kolaros; E Chalari; E Athanasiadou; A Martika; G Fildisis; V Faivre; C Mengelle; B Favier; D Payen; A Poppe; MS Winkler; E Mudersbach; J Schreiber; ML Wruck; E Schwedhelm; S Kluge; C Zöllner; T Tavladaki; AM Spanaki; H Dimitriou; E Kondili; C Choulaki; E Meleti; D Kafetzopoulos; D Georgopoulos; G Briassoulis; A García-de La Torre; MV de la Torre-Prados; T Tsvetanova-Spasova; P Nuevo-Ortega; C Rueda-Molina; A Fernández-Porcel; E Camara-Sola; L Salido-Díaz; A García-Alcántara; DE Meleti; B Suberviola; J Riera; L Rellan; M Sanchez; JC Robles; E Lopez; R Vicente; E Miñambres; M Santibañez; M Le Guen; J Moore; N Mason,Objectives To investigate practices in bedside monitoring for ABI patients. Particularlyinterested in differences among “neurointensivists”(NIs; defined here as intensivists whoseclinical practice is comprised> 1/3 by neurocritical care) and other intensivists (OIs). Also; toexplore patterns specific to traumatic brain injury (TBI) and subarachnoid hemorrhage(SAH); as well as preferences and availability of particular technologies/devices. MethodsElectronic survey of 22 items including two case-based scenarios; endorsed by SCCM(9;000 recipients) and ESICM (on-line newsletter) in 2013. A sample size of 370 wascalculated based on a population of 10;000 physician members; a 5% margin error; and95% confidence interval. We summarized results using descriptive statistics (proportionswith 95% confidence intervals). A chi-square test was used to compare proportions of …,Intensive care medicine experimental,2016,1,2
Do Anesthesiologists Know What They Are Doing? Mining a Surgical Time-Series Database to Correlate Expert Assessment with Outcomes,Risa B Myers; John C Frenzel MD; Joseph R Ruiz Md; Christopher M Jermaine,Abstract Anesthesiologists are taught to carefully manage patient vital signs during surgery.Unfortunately; there is little empirical evidence that vital sign management; as currentlypracticed; is correlated with patient outcomes. We seek to validate or repudiate currentclinical practice and determine whether or not clinician evaluation of surgical vital signscorrelate with outcomes. Using a database of over 90;000 cases; we attempt to determinewhether those cases that anesthesiologists would subjectively decide are “low quality” aremore likely to result in negative outcomes. The problem reduces to one of multi-dimensionaltime-series classification. Our approach is to have a set of expert anesthesiologistsindependently label a small number of training cases; from which we build classifiers andlabel all 90;000 cases. We then use the labeling to search for correlation with outcomes …,ACM Transactions on Knowledge Discovery from Data (TKDD),2016,1,12
Workload-Driven Antijoin Cardinality Estimation,Florin Rusu; Zixuan Zhuang; Mingxi Wu; Chris Jermaine,Abstract Antijoin cardinality estimation is among a handful of problems that has eludedaccurate efficient solutions amenable to implementation in relational query optimizers. Giventhe widespread use of antijoin and subset-based queries in analytical workloads and theextensive research targeted at join cardinality estimation—a seemingly related problem—the lack of adequate solutions for antijoin cardinality estimation is intriguing. In this article;we introduce a novel sampling-based estimator for antijoin cardinality that (unlike existentestimators) provides sufficient accuracy and efficiency to be implemented in a queryoptimizer. The proposed estimator incorporates three novel ideas. First; we use priorworkload information when learning a mixture superpopulation model of the data offline.Second; we design a Bayesian statistics framework that updates the superpopulation …,ACM Transactions on Database Systems (TODS),2015,1,12
Randomized algorithms for data reconciliation in wide area aggregate query processing,Fei Xu; Christopher Jermaine,Abstract Many aspects of the data integration problem have been considered in theliterature: how to match schemas across different data sources; how to decide when differentrecords refer to the same entity; how to efficiently perform the required entity resolution in abatch fashion; and so on. However; what has largely been ignored is a way to efficientlydeploy these existing methods in a realistic; distributed enterprise integration environment.The straightforward use of existing methods often requires that all data be shipped to acoordinator for cleaning; which is often unacceptable. We develop a set of randomizedalgorithms that allow efficient application of existing entity resolution methods to theanswering of aggregate queries over data that have been distributed across multiple sites.Using our methods; it is possible to efficiently generate aggregate query results that …,Proceedings of the 33rd international conference on Very large data bases,2007,1,12
Abridging source code,Binhang Yuan; Vijayaraghavan Murali; Christopher Jermaine,Abstract In this paper; we consider the problem of source code abridgment; where the goalis to remove statements from a source code in order to display the source code in a smallspace; while at the same time leaving the``important''parts of the source code intact; so thatan engineer can read the code and quickly understand purpose of the code. To this end; wedevelop an algorithm that looks at a number of examples; human-created source codeabridgments; and learns how to remove lines from the code in order to mimic the humanabridger. The learning algorithm takes into account syntactic features of the code; as well assemantic features such as control flow and data dependencies. Through a comprehensiveuser study; we show that the abridgments that our system produces can decrease the timethat a user must look at code in order to understand its functionality; as well as increase …,Proceedings of the ACM on Programming Languages,2017,*,12
Distributed Algorithms for Computing Very Large Thresholded Covariance Matrices,Zekai J Gao; Chris Jermaine,Abstract Computation of covariance matrices from observed data is an important problem; assuch matrices are used in applications such as principal component analysis (PCA); lineardiscriminant analysis (LDA); and increasingly in the learning and application of probabilisticgraphical models. However; computing an empirical covariance matrix is not always an easyproblem. There are two key difficulties associated with computing such a matrix from a veryhigh-dimensional dataset. The first problem is over-fitting. For a p-dimensional covariancematrix; there are p (p− 1)/2 unique; off-diagonal entries in the empirical covariance matrix Ŝfor large p (say; p> 10 5); the size n of the dataset is often much smaller than the number ofcovariances to compute. Over-fitting is a concern in any situation in which the number ofparameters learned can greatly exceed the size of the dataset. Thus; there are strong …,ACM Transactions on Knowledge Discovery from Data (TKDD),2016,*,10
Exploring phylogenetic hypotheses via Gibbs sampling on evolutionary networks,Yun Yu; Christopher Jermaine; Luay Nakhleh,Phylogenetic networks are leaf-labeled graphs used to model and display complexevolutionary relationships that do not fit a single tree. There are two classes of phylogeneticnetworks: Data-display networks and evolutionary networks. While data-display networksare very commonly used to explore data; they are not amenable to incorporatingprobabilistic models of gene and genome evolution. Evolutionary networks; on the otherhand; can accommodate such probabilistic models; but they are not commonly used forexploration. In this work; we show how to turn evolutionary networks into a tool for statisticalexploration of phylogenetic hypotheses via a novel application of Gibbs sampling. Wedemonstrate the utility of our work on two recently available genomic data sets; one from agroup of mosquitos and the other from a group of modern birds. We demonstrate that our …,BMC genomics,2016,*,12
A COMPARISON OF MORTALITY PREDICTORS IN CANCER SURGERY PATIENTS,R Myers; JR Ruiz; CM Jermaine; JC Frenzel,*,ANESTHESIA AND ANALGESIA,2016,*
Abstract PR622: A Comparison of Mortality Predictors in Cancer Surgery Patients,R Myers; JR Ruiz; CM Jermaine; JC Frenzel,Background & Objectives: A patient's health status often determines whether surgery isrecommended for cancer therapy. The risk of complications may outweigh the perceivedbenefit. Consequently; measures that predict surgical outcomes may be helpful indetermining whether or not to operate. Materials & Methods: We retrospectively apply fourmortality predictors to 62;763 adult surgical cancer patients from the University of Texas MDAnderson Cancer Center during January 2007-March 2014. We use the first surgery foreach patient that is over 60 minutes in duration and compare the following indexes:Charlson Comorbidity Index 1 as implemented by Deyo et al. 2; Dalton's Risk QuantificationIndex (RQI) 3; Sessler's Risk Stratification Index (RSI) 4 and the Surgical Apgar Score 5.,Anesthesia & Analgesia,2016,*,12
Guest editorial: Special section on the international conference on data engineering,Christian S Jensen; Christopher Jermaine; Xiaofang Zhou,THE 29th International Conference on Data Engineering was held in Brisbane; QLD;Australia; on April 8-11; 2013. ICDE 2013 attracted 443 submissions in the research track; 20submissions in the industrial track; and 69 demo proposals. Each submission was assignedto three reviewers. The evaluation process had several phases: assignment of papers toreviewers; reviewing; discussions among reviewers; decision making by area chairs;consoli- dation of decisions; and handling of papers assigned for shepherding. As a result ofthese efforts; 95 research papers; eight industrial papers; and 27 demos were selected for inclusionin the conference program. This special section consists of journal versions of seven outstandingpapers selected among the 95 accepted research papers. All papers were revised and substantiallyextended over their conference versions and went through a rigorous review process to …,IEEE Transactions on Knowledge and Data Engineering,2015,*,5
Managing uncertain data using Monte Carlo techniques,*,According to one embodiment of the present invention; a method for managing uncertaindata is provided. The method includes specifying data uncertainty using at least onevariable generation (VG) function. The VG function generates pseudorandom samples ofuncertain data values. A random database based on the VG function is specified andmultiple Monte Carlo instantiations of the random database are generated. Using a MonteCarlo method; a query is repeatedly executed over the multiple Monte Carlo instantiations tooutput a Monte Carlo method result and associated query-results. The Monte Carlo methodresult may then be used to estimate statistical properties of a probability distribution of thequery-result.,*,2015,*,19
Senders; Receivers and Authors in Document Classification,Anna Drummond; Christopher Jermaine,In many document classification problems; sets of people will be associated with thedocument. These sets might include document authors; or people who have read thedocument; or the sender of an electronic message; or the recipients of the message; or thosecarbon copied; or those blind carbon copied. It is obvious that these sets of people canconstitute important information that can help to classify the document. In this paper; wepropose a simple method for mapping the set of people in a sender or receiver category to asingle; low dimensional vector in a latent space. There are many ways that this vector can beused to help with the document classification task; and in the paper we consider threedistinct possibilities in detail. We find that mapping a set of senders or receivers to a latentspace in this way and incorporating this mapping into a classifier can greatly boost …,Data Mining (ICDM); 2014 IEEE International Conference on,2014,*,10
Letter from the Special Issue Editor.,Chris Jermaine,Approximate query processing (AQP) has a long history in databases—for at least 30 years;people have been interested in trading accuracy for better performance; with “betterperformance” equating to faster response time or lower memory utilization. Over the years;various approximation methodologies have been considered; including sampling;histograms; wavelets; and sketches. Curiously; those decades of high-quality academic andindustrial research on approximation have had surprisingly little commercial impact; at leastuntil recently. While databases are often sampled; direct support for AQP in datamanagement systems is not widespread. There are plenty of explanations for this: one canargue that few people had sufficient data that performance was enough of a concern toaccept approximation; end users were suspicious of approximations and statistical …,IEEE Data Eng. Bull.,2014,*,11
The MCDB System for Management and Analysis of Petabyte-Scale Uncertain Data,Chris Jermaine,Analysts working with very large data sets often use statistical models to “guess” atunknown; inaccurate; or missing information associated with the data. For example; a distantobject viewed through an optical lens will have its position slightly shifted by imperfections inthe lens. Thus; rather than considering the object's observed position to be absolutelycorrect; it makes sense to take into account the lens's imperfections to obtain a probabilisticguess as to the object's true position. For another example; it might be important to associatesome sort of error distribution with each of the individual sensors in an array ofmagnetometers. This error distribution may be complex and include spatially-drivencovariances; because errors in nearby sensors are likely to be correlated (caused; forexample; by the presence of some nearby; fixed metal object). This project is concerned …,*,2013,*,5
Multiclass domain adaptation with iterative manifold alignment,Brian D Bue; Chris Jermaine,We propose a novel approach for multiclass domain adaptation using an iterative manifoldalignment technique inspired by the TRiplet-based Iterative ALignment (TRIAL) proteinstructure alignment algorithm. Our technique learns a rigid transformation for each classusing a set of automatically-selected pivot samples that characterize the relativerelationships between classes in two similar; but not identical; feature spaces. Wedemonstrate that our technique robustly reconciles domain-specific differences betweensimilar classes in hyperspectral images captured under different conditions; and yields moreaccurate results than recently-proposed manifold alignment techniques. We evaluate ourmethod on a pair of real-world hyperspectral images of Cuprite; NV; and provide a MATLABimplementation of our algorithm; available online.,Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS); 2013 5th Workshop on,2013,*,21
Message from the ICDE 2013 Program Committee and General,Chris Jermaine,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2232200823 …,International Conference on Data Engineering,2013,*,12
Message from the ICDE 2013 program committee and general chairs,Christian S Jensen; Chris Jermaine; Xiaofang Zhou; Rao Kotagiri; Beng Chin Ooi,Skip navigation NUS. IVLE; Email; Library; Map; Calendar. NUS. Home; Research Outputs: Viewresearch outputs; Deposit publication / dataset. Researchers; Help: FAQs; Contact us. Guidelines.Sign on to: My ScholarBank; Receive email updates; Edit Account details. ScholarBank@NUS;1. Staff; Staff Publications. Please use this identifier to cite or link to this item: https://doi.org/10.1109/ICDE.2013.6544799. Title: Message from the ICDE 2013 program committee and general chairs.Authors: Jensen; CS Jermaine; C. Zhou; X. Kotagiri; R. Ooi; BC. Issue Date: 2013. Source Title:Proceedings - International Conference on Data Engineering. URI: http://scholarbank.nus.edu.sg/handle/10635/78441. ISBN: 9781467349086. ISSN: 10844627. DOI: 10.1109/ICDE.2013.6544799. Appears in Collections: Staff Publications. Show full item record Files in This Item:There are no files associated with this item. Page view(s) …,*,2013,*,3
Surrogate ranking for very expensive similarity queries,Fei Xu; Ravi Jampani; Mingxi Wu; Chris Jermaine; Tamer Kahveci,We consider the problem of similarity search in applications where the cost of computing thesimilarity between two records is very expensive; and the similarity measure is not a metric.In such applications; comparing even a tiny fraction of the database records to a singlequery record can be orders of magnitude slower than reading the entire database from disk;and indexing is often not possible. We develop a general-purpose; statistical framework foranswering top-k queries in such databases; when the database administrator is able tosupply an inexpensive surrogate ranking function that substitutes for the actual similaritymeasure. We develop a robust method that learns the relationship between the surrogatefunction and the similarity measure. Given a query; we use Bayesian statistics to update themodel by taking into account the observed partial results. Using the updated model; we …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,*,0
Welcome message from conference chairs,PYK Chau; K Lyytinen; CP Wei,Skip navigation …,ACM International Conference Proceeding Series,2009,*,10
This paper addresses some of the foundational issues associated with discovering the best few correlations from a database. Specifically; we consider the computati...,Christopher Jermaine,Many large organizations have multiple databases distributed in different branches; andtherefore multi-database mining is an important task for data mining. To reduce the searchcost in the data from all databases; we need to identify which databases are most likelyrelevant to a data mining application. This is referred to as database selection. For real-world applications; database selection has...,Information Systems,2005,*,21
Core Database Technology Program Committee,Anastassia Ailamaki; Gustavo Alonso; Walid Aref; Lars Arge; Brian Babcock; Mikael Berndtsson; Elisa Bertino; Claudio Bettini; Michael Boehlen; Anthony Bonner; Philippe Bonnet; Alex Buchmann; Tiziana Catarci; Surajit Chaudhuri; Peter Dadam; Amol Deshpande; Asuman Dogac; Christos Faloutsos; Elena Ferrari; Johann-Christoph Freytag; Dieter Gawlick; Johannes Gehrke; Torsten Grust; Ralf Hartmut Güting; Jayant Haritsa; Chris Jermaine; Christoph Koch; George Kollios; Mong Li Lee; Wolfgang Lindner; David Lomet; Hongjun Lu; Samuel Madden; Giansalvatore Mecca; Alberto Mendelzon; Rosa Meo; Tova Milo; Michele Missikoff; C Mohan; Mario Nascimento; Shojiro Nishio; Ed Omiecinski; Norman Paton; Torben Bach Pedersen; Calton Pu; Philippe Pucheral; Raghu Ramakrishnan; Thomas Rölleke; Ken Ross; Gunther Saake; Albrecht Schmidt; Marc Scholl; Bernhard Seeger,Committee Chair: Martin Kersten; CWI; The Netherlands … Serge Abiteboul; INRIA; France AnastassiaAilamaki; Carnegie Mellon University; USA Gustavo Alonso; ETH Zurich; Switzerland WalidAref; Purdue University; USA Lars Arge; Aarhus University; Denmark Brian Babcock; StanfordUniversity; USA Mikael Berndtsson; University of Skövde; Sweden Elisa Bertino; PurdueUniversity; USA Claudio Bettini; University of Milan; Italy Michael Boehlen; Free University ofBolzano/Bozen; Italy Peter Boncz; CWI; The Netherlands Anthony Bonner; University ofToronto; Canada Philippe Bonnet; University of Copenhagen; Denmark Alex Buchmann; Universityof Darmstadt; Germany Tiziana Catarci; University of Rome 'La Sapienza'; Italy SurajitChaudhuri; Microsoft; USA Vassilis Christophides; FORTH; Greece Peter Dadam; Universityof Ulm; Germany Amol Deshpande; University of California; Berkeley; USA Asuman …,VLDB 2005: 31st International Conference on Very Large Data Bases: Proceedings of the 31st International Conference on Very Large Data Bases; Trondheim; Norway; August 30-September 2; 2005,2005,*,18
ICDE 2017 Reviewers,Yannis Papakonstantinou; Lei Chen; Reynold Cheng; Wolfgang Gatterbauer; Bingsheng He; Stratos Idreos; Christopher Jermaine; Chen Li; Gerome Miklau; Tamer Özsu; Olga Papaemmanouil; Evimaria Terzi; Eugene Wu; Ashraf Aboulnaga; Alex Alves; Amazon Gabriel Antoniu; INRIA Arvind Arasu; Andrey Balmin; Workday Zhifeng Bao; Sumita Barahmand; Srikanta Bedathur; Carsten Binnig; Spyros Blanas; Marco Brambilla; Stephane Bressan; K Selcuk Candan; Zhao Cao; James Cheng; Fei Chiang; Panos K Chrysanthis; Philippe Cudre-Mauroux,ICDE 2017 Program Committee Chairs Yannis Papakonstantinou; University of California; SanDiego Yanlei Diao; Ecole Polytechnique; France; and University of Massachusetts; Amherst …ICDE 2017 Area Chairs Lei Chen; Hong Kong University of Science and Technology ReynoldCheng; University of Hong Kong Wolfgang Gatterbauer; Carnegie Mellon University BingshengHe; National University of Singapore Stratos Idreos; Harvard University ChristopherJermaine; Rice University Chen Li; University of California Irvine Gerome Miklau; University ofMassachusetts Tamer Özsu; University of Waterloo Olga Papaemmanouil; Brandeis UniversityEvimaria Terzi; Boston University Eugene Wu; Columbia University … ICDE 2017 Program CommitteeAshraf Aboulnaga; Qatar Computing Research Institute Alex Alves; Amazon Gabriel Antoniu;INRIA Arvind Arasu; Microsoft Research Andrey Balmin; Workday Zhifeng Bao; RMIT …,*,*,*,12
Subi Arumugam,Chris Jermaine,◆ DBO (Database Online): Implemented a prototype relational query engine to supportonline aggregation of a select-project-join query over an arbitrary number of input relations.Benchmarked with 10 GB TPC-H data sets (C/C++/Pthreads LOC:~ 17K)◆ ProbabilisticSelection: Designed and implemented novel algorithms for the evaluation of relationalselection predicates over probabilistic databases (C LOC:~ 8K)◆ CPA-Join: Designed andimplemented algorithms to compute the distance join between two spatiotemporal relationsstoring moving object histories. Benchmarked over two fifty gigabyte scientific data sets with100;000 objects (C/C++/STL LOC:~ 6K)◆ SMS (Sort Merge Shrink) Join: Implemented avariant of the Sort Merge Join to support online execution of OLAP queries over two largedisk-resident relations. Benchmarked using two 20 gigabyte relations containing 200 …,*,*,*,6
The Editors-in-Chief and the Editorial Board would like to acknowledge the following people for their expert and continuing assistance in evaluating manuscript sub...,Gustavo Alonso; Marcelo Arenas; Vijay Atluri; Sudarshan Chawathe; Yatin Chawathe; Edgar Chavez; Panos Chountas; Vassilis Christophides; Joaquin Delgado; Koji Eguchi; Richard Furuta; Nicholas Gold; Paul Grefen; Terry Halpin; Holger Hansmann; Carlos Hurtado; Sergio Ilarri; Alfons Juan; Christopher Jermaine; Paul Johannesson; Leonid Kalinichenko; Panagiotis Kardasis; George Kollios; Kecheng Liu; Jinyan Li; Akira Maeda; Sebastian Maneth; Alistair Moffat; Rebecca Montanari; Atsushi Morishima; Olfa Nasraoui; Andreas Oberweis; Barbara Pernici; Ilias Petrounias; Michael Rabinovich; Krithi Ramamritham; Manfred Reichert; John T Robinson; Daniela Rosca; Kai-Uwe Sattler; Heiko Schuldt; Hilmar Schuschel; Fabrizio Sebastiani; Timos Sellis; Edleno Silva de Moura; Richard T Snodgrass; Janis Stirna; Dan Suciu; Keishi Tajima; Frank Tompa; Panayiotis Tsaparas; George Tzanetakis; Millist W Vincent; Michael Vlachos; X Sean Wang; Benkt Wangler; Mathias Weske; Duminda Wijesekara; Hugo Zaragoza; Donghui Zhang,*,*,*,*
MCDB and SimSQL: Scalable Stochastic Analytics within the Database,Peter J Haas; Chris Jermaine,ABSTRACT This short paper highlights published work on the MCDB database system aswell as its successor; SimSQL. These database systems are designed from the ground up tosupport stochastic analytics; that is; data analysis performed with the aid of stochasticsimulations.,*,*,*,20
