A survey of top-k query processing techniques in relational database systems,Ihab F Ilyas; George Beskales; Mohamed A Soliman,Abstract Efficient processing of top-k queries is a crucial requirement in many interactiveenvironments that involve massive amounts of data. In particular; efficient top-k processingin domains such as the Web; multimedia search; and distributed systems has shown a greatimpact on performance. In this survey; we describe and classify top-k processing techniquesin relational databases. We discuss different design dimensions in the current techniquesincluding query models; data access methods; implementation levels; data and querycertainty; and supported scoring functions. We show the implications of each dimension onthe design of the underlying techniques. We also discuss top-k queries in XML domain; andshow their connections to relational approaches.,ACM Computing Surveys (CSUR),2008,730,10
Top-k query processing in uncertain databases,Mohamed A Soliman; Ihab F Ilyas; Kevin Chen-Chuan Chang,Top-k processing in uncertain databases is semantically and computationally different fromtraditional top-k processing. The interplay between score and uncertainty makes traditionaltechniques inapplicable. We introduce new probabilistic formulations for top-k queries. Ourformulations are based on" marriage" of traditional top-k semantics and possible worldssemantics. In the light of these formulations; we construct a framework that encapsulates astate space model and efficient query processing techniques to tackle the challenges ofuncertain data settings. We prove that our techniques are optimal in terms of the number ofaccessed tuples and materialized search states. Our experiments show the efficiency of ourtechniques under different data distributions with orders of magnitude improvement overnaive materialization of possible worlds.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,484,10
Supporting top-k join queries in relational databases,Ihab F Ilyas; Walid G Aref; Ahmed K Elmagarmid,Abstract Ranking queries; also known as top-k queries; produce results that are ordered onsome computed score. Typically; these queries involve joins; where users are usuallyinterested only in the top-k join results. Top-k queries are dominant in many emergingapplications; eg; multimedia retrieval by content; Web databases; data mining; middlewares;and most information retrieval applications. Current relational query processors do nothandle ranking queries efficiently; especially when joins are involved. In this paper; weaddress supporting top-k join queries in relational query processors. We introduce a newrank-join algorithm that makes use of the individual orders of its inputs to produce joinresults ordered on a user-specified scoring function. The idea is to rank the join resultsprogressively during the join operation. We introduce two physical query operators based …,The VLDB Journal—The International Journal on Very Large Data Bases,2004,453,10
RankSQL: query algebra and optimization for relational top-k queries,Chengkai Li; Kevin Chen-Chuan Chang; Ihab F Ilyas; Sumin Song,Abstract This paper introduces RankSQL; a system that provides a systematic and principledframework to support efficient evaluations of ranking (top-k) queries in relational databasesystems (RDBMS); by extending relational algebra and query optimization. Previously; top-kquery processing is studied in the middleware scenario or in RDBMS in a" piecemeal"fashion; ie; focusing on specific operator or sitting outside the core of query engines. Incontrast; we aim to support ranking as a first-class database construct. As a key insight; thenew ranking relationship can be viewed as another logical property of data; parallel to the"membership" property of relational data model. While membership is essentially supportedin RDBMS; the same support for ranking is clearly lacking. We address the fundamentalintegration of ranking in RDBMS in a way similar to how membership; ie; Boolean filtering …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,307,4
CORDS: automatic discovery of correlations and soft functional dependencies,Ihab F Ilyas; Volker Markl; Peter Haas; Paul Brown; Ashraf Aboulnaga,Abstract The rich dependency structure found in the columns of real-world relationaldatabases can be exploited to great advantage; but can also cause query optimizers---whichusually assume that columns are statistically independent---to underestimate theselectivities of conjunctive predicates by orders of magnitude. We introduce CORDS; anefficient and scalable tool for automatic discovery of correlations and soft functionaldependencies between columns. CORDS searches for column pairs that might haveinteresting and useful dependency relations by systematically enumerating candidate pairsand simultaneously pruning unpromising candidates using a flexible set of heuristics. Arobust chi-squared analysis is applied to a sample of column values in order to identifycorrelations; and the number of distinct values in the sampled columns is analyzed to …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,275,4
Efficient search for the top-k probable nearest neighbors in uncertain databases,George Beskales; Mohamed A Soliman; Ihab F Ilyas,Abstract Uncertainty pervades many domains in our lives. Current real-life applications; eg;location tracking using GPS devices or cell phones; multimedia feature extraction; andsensor data management; deal with different kinds of uncertainty. Finding the nearestneighbor objects to a given query point is an important query type in these applications. Inthis paper; we study the problem of finding objects with the highest marginal probability ofbeing the nearest neighbors to a query object. We adopt a general uncertainty modelallowing for data and query uncertainty. Under this model; we define new query semantics;and provide several efficient evaluation algorithms. We analyze the cost factors involved inquery evaluation; and present novel techniques to address the trade-offs among thesefactors. We give multiple extensions to our techniques including handling dependencies …,Proceedings of the VLDB Endowment,2008,167,10
Rank-aware query optimization,Ihab F Ilyas; Rahul Shah; Walid G Aref; Jeffrey Scott Vitter; Ahmed K Elmagarmid,Abstract Ranking is an important property that needs to be fully supported by currentrelational query engines. Recently; several rank-join query operators have been proposedbased on rank aggregation algorithms. Rank-join operators progressively rank the joinresults while performing the join operation. The new operators have a direct impact ontraditional query processing and optimization. We introduce a rank-aware query optimizationframework that fully integrates rank-join operators into relational query engines. Theframework is based on extending the System R dynamic programming algorithm in bothenumeration and pruning. We define ranking as an interesting property that triggers thegeneration of rank-aware query plans. Unlike traditional join operators; optimizing for rank-join operators depends on estimating the input cardinality of these operators. We …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,144,4
Nile: A query processing engine for data streams,Moustafa A Hammad; Mohamed F Mokbel; Mohamed H Ali; Walid G Aref; Ann Christine Catlin; Ahmed K Elmagarmid; Mohamed Eltabakh; Mohamed G Elfeky; Thanaa M Ghanem; Robert Gwadera; Ihab F Ilyas; Mirette Marzouk; Xiaopeng Xiong,We present the demonstration of the design of" STEAM"; Purdue Boiler Makers' streamdatabase system that allows for the processing of continuous and snap-shot queries overdata streams. Specifically; the demonstration focuses on the query processing engine;"Nile". Nile extends the query processor engine of an object-relational databasemanagement system; PREDATOR; to process continuous queries over data streams. Nilesupports extended SQL operators that handle sliding-window execution as an approach torestrict the size of the stored state in operators such as join.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,140,10
NADEEF: a commodity data cleaning system,Michele Dallachiesa; Amr Ebaid; Ahmed Eldawy; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Nan Tang,Abstract Despite the increasing importance of data quality and the rich theoretical andpractical contributions in all aspects of data cleaning; there is no single end-to-end off-the-shelf solution to (semi-) automate the detection and the repairing of violations wrt a set ofheterogeneous and ad-hoc quality constraints. In short; there is no commodity platformsimilar to general purpose DBMSs that can be easily customized and deployed to solveapplication-specific data quality problems. In this paper; we present NADEEF; an extensible;generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between aprogramming interface and a core to achieve generality and extensibility. The programminginterface allows the users to specify multiple types of data quality rules; which uniformlydefine what is wrong with the data and (possibly) how to repair it through writing code that …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,136,4
Guided data repair,Mohamed Yakout; Ahmed K Elmagarmid; Jennifer Neville; Mourad Ouzzani; Ihab F Ilyas,Abstract In this paper we present GDR; a Guided Data Repair framework that incorporatesuser feedback in the cleaning process to enhance and accelerate existing automatic repairtechniques while minimizing user involvement. GDR consults the user on the updates thatare most likely to be beneficial in improving data quality. GDR also uses machine learningmethods to identify and apply the correct updates directly to the database without the actualinvolvement of the user on these specific updates. To rank potential updates for consultationby the user; we first group these repairs and quantify the utility of each group using thedecision-theory concept of value of information (VOI). We then apply active learning to orderupdates within a group based on their ability to improve the learned model. User feedback isused to repair the database and to adaptively refine the training set for the model. We …,Proceedings of the VLDB Endowment,2011,124,22
Xseed: Accurate and fast cardinality estimation for xpath queries,Ning Zhang; M Tamer Ozsu; Ashraf Aboulnaga; Ihab F Ilyas,We propose XSEED; a synopsis of path queries for cardinality estimation that is accurate;robust; efficient; and adaptive to memory budgets. XSEED starts from a very small kernel;and then incrementally updates information of the synopsis. With such an incrementalconstruction; a synopsis structure can be dynamically configured to accommodate differentmemory budgets. Cardinality estimation based on XSEED can be performed very efficientlyand accurately. Extensive experiments on both synthetic and real data sets show that evenwith less memory; XSEED could achieve accuracy that is an order of magnitude better thanthat of other synopsis structures. The cardinality estimation time is under 2% of the actualquerying time for a wide range of queries in all test cases.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,106,22
Data Curation at Scale: The Data Tamer System.,Michael Stonebraker; Daniel Bruckner; Ihab F Ilyas; George Beskales; Mitch Cherniack; Stanley B Zdonik; Alexander Pagan; Shan Xu,ABSTRACT Data curation is the act of discovering a data source (s) of interest; cleaning andtransforming the new data; semantically integrating it with other local data sources; anddeduplicating the resulting composite. There has been much research on the variouscomponents of curation (especially data integration and deduplication). However; there hasbeen little work on collecting all of the curation components into an integrated end-to-endsystem. In addition; most of the previous work will not scale to the sizes of problems that weare finding in the field. For example; one web aggregator requires the curation of 80;000URLs and a second biotech company has the problem of curating 8000 spreadsheets. Atthis scale; data curation cannot be a manual (human) effort; but must entail machinelearning approaches with a human assist only when necessary.,CIDR,2013,105,4
Joining ranked inputs in practice,Ihab F Ilyas; Walid G Aref; Ahmed K Elmagarmid,This chapter carries out an extensive performance study to evaluate two recent algorithmsfor obtaining a global rank from multiple ranked inputs. A growing number of databaseapplications require the processing of ranking queries based on multiple attributes. In thecontext of multimedia retrieval; predicates often involve image similarity matching withrespect to several features. Users may present an example image; and query the databasefor images “most similar” to the example based on color and texture. Although eachdatabase image object can easily be ranked for color and texture separately; results must bepresented to the user in a combined similarity order. Another example from informationretrieval is the search for documents containing search topics from multiple sources. Whileeach source provides retrieved documents sorted by relevance; the collection of retrieved …,*,2002,102,22
Holistic data cleaning: Putting violations into context,Xu Chu; Ihab F Ilyas; Paolo Papotti,Data cleaning is an important problem and data quality rules are the most promising way toface it with a declarative approach. Previous work has focused on specific formalisms; suchas functional dependencies (FDs); conditional functional dependencies (CFDs); andmatching dependencies (MDs); and those have always been studied in isolation. Moreover;such techniques are usually applied in a pipeline or interleaved. In this work we tackle theproblem in a novel; unified framework. First; we let users specify quality rules using denialconstraints with ad-hoc predicates. This language subsumes existing formalisms and canexpress rules involving numerical values; with predicates such as “greater than” and “lessthan”. More importantly; we exploit the interaction of the heterogeneous constraints byencoding them in a conflict hypergraph. Such holistic view of the conflicts is the starting …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,95,13
Sampling the repairs of functional dependency violations under hard constraints,George Beskales; Ihab F Ilyas; Lukasz Golab,Abstract Violations of functional dependencies (FDs) are common in practice; often arising inthe context of data integration or Web data extraction. Resolving these violations is known tobe challenging for a variety of reasons; one of them being the exponential number ofpossible" repairs". Previous work has tackled this problem either by producing a singlerepair that is (nearly) optimal with respect to some metric; or by computing consistentanswers to selected classes of queries without explicitly generating the repairs. In this paper;we propose a novel data cleaning approach that is not limited to finding a single repair or toa particular class of queries; namely; sampling from the space of possible repairs. We giveseveral motivating scenarios where sampling from the space of FD repairs is desirable;propose a new class of useful repairs; and present an algorithm that randomly samples …,Proceedings of the VLDB Endowment,2010,91,4
Ranking with uncertain scores,Mohamed A Soliman; Ihab F Ilyas,Large databases with uncertain information are becoming more common in manyapplications including data integration; location tracking; and Web search. In theseapplications; ranking records with uncertain attributes needs to handle new problems thatare fundamentally different from conventional ranking. Specifically; uncertainty in records'scores induces a partial order over records; as opposed to the total order that is assumed inthe conventional ranking settings. In this paper; we present a new probabilistic model; basedon partial orders; to encapsulate the space of possible rankings originating from scoreuncertainty. Under this model; we formulate several ranking query types with differentsemantics. We describe and analyze a set of efficient query evaluation algorithms. We showthat our techniques can be used to solve the problem of rank aggregation in partial orders …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,90,18
Probabilistic top-k and ranking-aggregate queries,Mohamed A Soliman; Ihab F Ilyas; Kevin Chen--Chuan Chang,Abstract Ranking and aggregation queries are widely used in data exploration; dataanalysis; and decision-making scenarios. While most of the currently proposed ranking andaggregation techniques focus on deterministic data; several emerging applications involvedata that is unclean or uncertain. Ranking and aggregating uncertain (probabilistic) dataraises new challenges in query semantics and processing; making conventional methodsinapplicable. Furthermore; uncertainty imposes probability as a new ranking dimension thatdoes not exist in the traditional settings. In this article we introduce new probabilisticformulations for top-k and ranking-aggregate queries in probabilistic databases. Ourformulations are based on marriage of traditional top-k semantics with possible worldssemantics. In the light of these formulations; we construct a generic processing framework …,ACM Transactions on Database Systems (TODS),2008,83,10
Supporting ad-hoc ranking aggregates,Chengkai Li; Kevin Chen-Chuan Chang; Ihab F Ilyas,Abstract This paper presents a principled framework for efficient processing of ad-hoc top-k(ranking) aggregate queries; which provide the k groups with the highest aggregates asresults. Essential support of such queries is lacking in current systems; which process thequeries in a naïve materialize-group-sort scheme that can be prohibitively inefficient. Ourframework is based on three fundamental principles. The Upper-Bound Principle dictatesthe requirements of early pruning; and the Group-Ranking and Tuple-Ranking Principlesdictate group-ordering and tuple-ordering requirements. They together guide the queryprocessor toward a provably optimal tuple schedule for aggregate query processing. Wepropose a new execution framework to apply the principles and requirements. We addressthe challenges in realizing the framework and implementing new query operators …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,82,4
Expressive and flexible access to web-extracted data: a keyword-based structured query language,Jeffrey Pound; Ihab F Ilyas; Grant Weddell,Abstract Automated extraction of structured data from Web sources often leads to largeheterogeneous knowledge bases (KB); with data and schema items numbering in thehundreds of thousands or millions. Formulating information needs with conventionalstructured query languages is difficult due to the sheer size of schema information availableto the user. We address this challenge by proposing a new query language that blendskeyword search with structured query processing over large information graphs with richsemantics. Our formalism for structured queries based on keywords combines the flexibilityof keyword search with the expressiveness of structures queries. We propose a solution tothe resulting disambiguation problem caused by introducing keywords as primitives in astructured query language. We show how expressions in our proposed language can be …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,73,13
Sp-gist: An extensible database index for supporting space partitioning trees,Walid G Aref; Ihab F Ilyas,Abstract Emerging database applications require the use of new indexing structures beyondB-trees and R-trees. Examples are the kD tree; the trie; the quadtree; and their variants. Theyare often proposed as supporting structures in data mining; GIS; and CAD/CAM applications.A common feature of all these indexes is that they recursively divide the space intopartitions. A new extensible index structure; termed SP-GiST is presented that supports thisclass of data structures; mainly the class of space partitioning unbalanced trees. Simplemethod implementations are provided that demonstrate how SP-GiST can behave as a kDtree; a trie; a quadtree; or any of their variants. Issues related to clustering tree nodes intopages as well as concurrency control for SP-GiST are addressed. A dynamic minimum-height clustering technique is applied to minimize disk accesses and to make using such …,Journal of Intelligent Information Systems,2001,68,12
Katara: A data cleaning system powered by knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Classical approaches to clean data have relied on using integrity constraints;statistics; or machine learning. These approaches are known to be limited in the cleaningaccuracy; which can usually be improved by consulting master data and involving experts toresolve ambiguity. The advent of knowledge bases KBs both general-purpose and withinenterprises; and crowdsourcing marketplaces are providing yet more opportunities toachieve higher accuracy at a larger scale. We propose KATARA; a knowledge base andcrowd powered data cleaning system that; given a table; a KB; and a crowd; interprets tablesemantics to align it with the KB; identifies correct and incorrect data; and generates top-kpossible repairs for incorrect data. Experiments show that KATARA can be applied tovarious datasets and KBs; and can efficiently annotate data and suggest possible repairs.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,63,22
Adaptive rank-aware query optimization in relational databases,Ihab F Ilyas; Walid G Aref; Ahmed K Elmagarmid; Hicham G Elmongui; Rahul Shah; Jeffrey Scott Vitter,Abstract Rank-aware query processing has emerged as a key requirement in modernapplications. In these applications; efficient and adaptive evaluation of top-k queries is anintegral part of the application semantics. In this article; we introduce a rank-aware queryoptimization framework that fully integrates rank-join operators into relational query engines.The framework is based on extending the System R dynamic programming algorithm in bothenumeration and pruning. We define ranking as an interesting physical property that triggersthe generation of rank-aware query plans. Unlike traditional join operators; optimizing forrank-join operators depends on estimating the input cardinality of these operators. Weintroduce a probabilistic model for estimating the input cardinality; and hence the cost of arank-join operator. To our knowledge; this is the first effort in estimating the needed input …,ACM Transactions on Database Systems (TODS),2006,63,4
Creating competitive products,Qian Wan; Raymond Chi-Wing Wong; Ihab F Ilyas; M Tamer Özsu; Yu Peng,Abstract The importance of dominance and skyline analysis has been well recognized inmulti-criteria decision making applications. Most previous works study how to helpcustomers find a set of" best" possible products from a pool of given products. In this paper;we identify an interesting problem; creating competitive products; which has not beenstudied before. Given a set of products in the existing market; we want to study how to createa set of" best" possible products such that the newly created products are not dominated bythe products in the existing market. We refer such products as competitive products. Astraightforward solution is to generate a set of all possible products and check fordominance relationships. However; the whole set is quite large. In this paper; we propose asolution to generate a subset of this set effectively. An extensive performance study using …,Proceedings of the VLDB Endowment,2009,58,10
Supporting ranking queries on uncertain and incomplete data,Mohamed A Soliman; Ihab F Ilyas; Shalev Ben-David,Abstract Large databases with uncertain information are becoming more common in manyapplications including data integration; location tracking; and Web search. In theseapplications; ranking records with uncertain attributes introduces new problems that arefundamentally different from conventional ranking. Specifically; uncertainty in records' scoresinduces a partial order over records; as opposed to the total order that is assumed in theconventional ranking settings. In this paper; we present a new probabilistic model; based onpartial orders; to encapsulate the space of possible rankings originating from scoreuncertainty. Under this model; we formulate several ranking query types with differentsemantics. We describe and analyze a set of efficient query evaluation algorithms. We showthat our techniques can be used to solve the problem of rank aggregation in partial orders …,The VLDB Journal,2010,56,14
Fix: Feature-based indexing technique for xml documents,Ning Zhang; M Tamer Özsu; Ihab F Ilyas; Ashraf Aboulnaga,Abstract Indexing large XML databases is crucial for efficient evaluation of XML twig queries.In this paper; we propose a feature-based indexing technique; called FIX; based on spectralgraph theory. The basic idea is that for each twig pattern in a collection of XML documents;we calculate a vector of features based on its structural properties. These features are usedas keys for the patterns and stored in a B+ tree. Given an XPath query; its feature vector isfirst calculated and looked up in the index. Then a further refinement phase is performed tofetch the final results. We experimentally study the indexing technique over both syntheticand real data sets. Our experiments show that FIX provides great pruning power and couldgain an order of magnitude performance improvement for many XPath queries over existingevaluation techniques.,Proceedings of the 32nd international conference on Very large data bases,2006,53,10
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,52,7
Ranking with uncertain scoring functions: semantics and sensitivity measures,Mohamed A Soliman; Ihab F Ilyas; Davide Martinenghi; Marco Tagliasacchi,Abstract Ranking queries report the top-K results according to a user-defined scoringfunction. A widely used scoring function is the weighted summation of multiple scores. Oftentimes; users cannot precisely specify the weights in such functions in order to produce thepreferred order of results. Adopting uncertain/incomplete scoring functions (eg; using weightranges and partially-specified weight preferences) can better capture user's preferences inthis scenario. In this paper; we study two aspects in uncertain scoring functions. The firstaspect is the semantics of ranking queries; and the second aspect is the sensitivity ofcomputed results to refinements made by the user. We formalize and solve multipleproblems under both aspects; and present novel techniques that compute query resultsefficiently to comply with the interactive nature of these problems.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,51,12
Interpreting keyword queries over web knowledge bases,Jeffrey Pound; Alexander K Hudek; Ihab F Ilyas; Grant Weddell,Abstract Many keyword queries issued to Web search engines target information about realworld entities; and interpreting these queries over Web knowledge bases can often enablethe search system to provide exact answers to queries. Equally important is the problem ofdetecting when the reference knowledge base is not capable of answering the keywordquery; due to lack of domain coverage. In this work we present an approach to computingstructured representations of keyword queries over a reference knowledge base. We minefrequent query structures from a Web query log and map these structures into a referenceknowledge base. Our approach exploits coarse linguistic structure in keyword queries; andcombines it with rich structured query representations of information needs.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,50,4
Discovering denial constraints,Xu Chu; Ihab F Ilyas; Paolo Papotti,Abstract Integrity constraints (ICs) provide a valuable tool for enforcing correct applicationsemantics. However; designing ICs requires experts and time. Proposals for automaticdiscovery have been made for some formalisms; such as functional dependencies and theirextension conditional functional dependencies. Unfortunately; these dependencies cannotexpress many common business rules. For example; an American citizen cannot have lowersalary and higher tax rate than another citizen in the same state. In this paper; we tackle thechallenges of discovering dependencies in a more expressive integrity constraint language;namely Denial Constraints (DCs). DCs are expressive enough to overcome the limits ofprevious languages and; at the same time; have enough structure to allow efficient discoveryand application in several scenarios. We lay out theoretical and practical foundations for …,Proceedings of the VLDB Endowment,2013,46,3
Estimating the compilation time of a query optimizer,*,A compilation time estimator provides a quantified estimate of the optimizer compilation timefor a given query optimizer. The estimator automates the optimizer to choose the right levelof optimization in commercial database systems. The estimator reuses an optimizer's joinenumerator to obtain actual number of joins; but bypasses plan generation to saveestimation overhead; and maintains a small number of interesting physical properties toestimate the number of plans by using a linear regression model. The estimator uses thenumber of generated plans to estimate query compilation time.,*,2008,46,23
Knowledge representation systems and methods incorporating customization,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including an elemental data structure andknowledge processing rules that are machine-readable. The elemental data structure mayinclude a universal kernel and customized modules; which may represent knowledge that isgenerally applicable to a population and knowledge that is specifically applicable toindividual data consumers; respectively. A method of constructing an elemental datastructure may include analyzing first information to identify a first elemental componentassociated with a data consumer; and adding the first elemental component to a customizedmodule corresponding to the data consumer. The method may also include analyzingsecond information to identify a second elemental component associated with a …,*,2013,45,22
Detecting correlation from data,*,A system and method of discovering dependencies between relational database columnpairs and application of discoveries to query optimization is provided. For each candidatecolumn pair remaining after simultaneously generating column pairs; pruning pairs notsatisfying specified heuristic constraints; and eliminating pairs with trivial instances ofcorrelation; a random sample of data values is collected. A candidate column pair is testedfor the existence of a soft functional dependency (FD); and if a dependency is not found;statistically tested for correlation using a robust chi-squared statistic. Column pairs for whicheither a soft FD or a statistical correlation exists are prioritized for recommendation to aquery optimizer; based on any of: strength of dependency; degree of correlation; oradjustment factor; statistics for recommended columns pairs are tracked to improve …,*,2010,43,10
Knowledge representation systems and methods incorporating inference rules,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including both an elemental datastructure and knowledge processing rules stored as machine-readable data and/orprogramming instructions. One or more of the knowledge processing rules may be appliedto analyze an input complex KR to deconstruct its complex concepts and/or conceptrelationships to elemental concepts and/or concept relationships to be included in theelemental data structure. One or more of the knowledge processing rules may be applied tosynthesize an output complex KR from the stored elemental data structure in accordancewith context information. Methods of populating an elemental data structure and methods ofsynthesizing complex KRs from the elemental data structure may rely on linguistic …,*,2012,42,10
Modeling and querying possible repairs in duplicate detection,George Beskales; Mohamed A Soliman; Ihab F Ilyas; Shai Ben-David,Abstract One of the most prominent data quality problems is the existence of duplicaterecords. Current duplicate elimination procedures usually produce one clean instance(repair) of the input data; by carefully choosing the parameters of the duplicate detectionalgorithms. Finding the right parameter settings can be hard; and in many cases; perfectsettings do not exist. Furthermore; replacing the input dirty data with one possible cleaninstance may result in unrecoverable errors; for example; identification and merging ofpossible duplicate records in health care systems. In this paper; we treat duplicate detectionprocedures as data processing tasks with uncertain outcomes. We concentrate on a family ofduplicate detection algorithms that are based on parameterized clustering. We propose anovel uncertainty model that compactly encodes the space of possible repairs …,Proceedings of the VLDB Endowment,2009,42,13
Methods and apparatus for providing information of interest to one or more users,*,Methods and system for providing information selected from a large set of digital content to auser. Techniques include receiving user context information associated with the user andidentifying or generating a first concept in a semantic network; the first concept representingat least a portion of the user context information. The method further comprises obtaining atleast one concept; including a second concept; semantically relevant to the first concept atleast in part; by synthesizing the second concept based on the first concept and at least oneother concept in the semantic network; and providing information to the user; wherein theinformation is selected by using the first concept and the at least one obtained conceptsemantically relevant to the first concept; wherein the first concept in a semantic network isrepresented by a data structure storing any data associated with a node in the semantic …,*,2016,38,22
On the relative trust between inconsistent data and inaccurate constraints,George Beskales; Ihab F Ilyas; Lukasz Golab; Artur Galiullin,Functional dependencies (FDs) specify the intended data semantics while violations of FDsindicate deviation from these semantics. In this paper; we study a data cleaning problem inwhich the FDs may not be completely correct; eg; due to data evolution or incompleteknowledge of the data semantics. We argue that the notion of relative trust is a crucial aspectof this problem: if the FDs are outdated; we should modify them to fit the data; but if wesuspect that there are problems with the data; we should modify the data to fit the FDs. Inpractice; it is usually unclear how much to trust the data versus the FDs. To address thisproblem; we propose an algorithm for generating non-redundant solutions (ie; simultaneousmodifications of the data and the FDs) corresponding to various levels of relative trust. Thiscan help users determine the best way to modify their data and/or FDs to achieve …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,33,4
Descriptive and prescriptive data cleaning,Anup Chalamalla; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti,Abstract Data cleaning techniques usually rely on some quality rules to identify violatingtuples; and then fix these violations using some repair algorithms. Oftentimes; the rules;which are related to the business logic; can only be defined on some target report generatedby transformations over multiple data sources. This creates a situation where the violationsdetected in the report are decoupled in space and time from the actual source of errors. Inaddition; applying the repair on the report would need to be repeated whenever the datasources change. Finally; even if repairing the report is possible and affordable; this would beof little help towards identifying and analyzing the actual sources of errors for futureprevention of violations at the target. In this paper; we propose a system to address thisdecoupling. The system takes quality rules defined over the output of a transformation …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,32,22
Trends in cleaning relational data: Consistency and deduplication,Ihab F Ilyas; Xu Chu,Abstract Data quality is one of the most important problems in data management; since dirtydata often leads to inaccurate data analytics results and wrong business decisions. Poordata across businesses and the government cost the US economy $3.1 trillion a year;according to a report by InsightSquared in 2012. To detect data errors; data quality rules orintegrity constraints (ICs) have been proposed as a declarative way to describe legal orcorrect data instances. Any subset of data that does not conform to the defined rules isconsidered erroneous; which is also referred to as a violation. Various kinds of datarepairing techniques with different objectives have been introduced; where algorithms areused to detect subsets of the data that violate the declared integrity constraints; and even tosuggest updates to the database such that the new database instance conforms with …,Foundations and Trends® in Databases,2015,31,12
A study of ontology-based query expansion,Jiewen Wu; Ihab Ilyas; Grant Weddell,Abstract. With enormous data emerging on the Web; traditional keyword searching ischallenged by short queries posed by users to vaguely describe their information need.Query expansion has been researched for decades and a variety of expansion strategieshave improved retrieval effectiveness. At present; knowledge-based query expansionapproaches are popular as the Web becomes more semantic. This paper studies state-of-the-art in ontologybased query expansion approaches; and expands on practical strategiesto exploit the rich semantics of domain ontologies. This paper; one the one hand; focuses onfinding out the success factors for ontology-based query expansion; on the other hand; itemphasizes the tradeoff between the gained retrieval effectiveness and the incurredcomputation cost.,Technical report CS-2011–04,2011,28,13
Collecting and maintaining just-in-time statistics,Amr El-Helw; Ihab F Ilyas; Wing Lau; Volker Markl; Calisto Zuzarte,Traditional DBMSs decouple statistics collection and query optimization both in space andtime. Decoupling in time may lead to outdated statistics. Decoupling in space may causestatistics not to be available at the desired granularity needed to optimize a particular query;or some important statistics may not be available at all. Overall; this decoupling often leadsto large cardinality estimation errors and; in consequence; to the selection of suboptimalplans for query execution. In this paper; we present JITS; a system for proactively collectingquery-specific statistics during query compilation. The system employs a lightweightsensitivity analysis to choose which statistics to collect by making use of previously collectedstatistics and database activity patterns. The collected statistics are materialized andincrementally updated for later reuse. We present the basic concepts; architecture; and …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,28,10
Estimating compilation time of a query optimizer,Ihab F Ilyas; Jun Rao; Guy Lohman; Dengfeng Gao; Eileen Lin,Abstract A query optimizer compares alternative plans in its search space to find the bestplan for a given query. Depending on the search space and the enumeration algorithm;optimizers vary in their compilation time and the quality of the execution plan they cangenerate. This paper describes a compilation time estimator that provides a quantifiedestimate of the optimizer compilation time for a given query. Such an estimator is useful forautomatically choosing the right level of optimization in commercial database systems. Inaddition; compilation time estimates can be quite helpful for mid-query reoptimization; formonitoring the progress of workload analysis tools where a large number queries need to becompiled (but not executed); and for judicious design and tuning of an optimizer. Previousattempts to estimate optimizer compilation complexity used the number of possible binary …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,28,4
Top-k nearest neighbor search in uncertain data series,Michele Dallachiesa; Themis Palpanas; Ihab F Ilyas,Abstract Many real applications consume data that is intrinsically uncertain; noisy and error-prone. In this study; we investigate the problem of finding the top-k nearest neighbors inuncertain data series; which occur in several different domains. We formalize the top-knearest neighbor problem for uncertain data series; and describe a model for uncertain dataseries that captures both uncertainty and correlation. This distinguishes our approach fromprior work that compromises the accuracy of the model by assuming independence of thevalue distribution at neighboring time-stamps. We introduce the Holistic-PkNN algorithm;which uses novel metric bounds for uncertain series and an efficient refinement strategy toreduce the overall number of required probability estimates. We evaluate our proposalunder a variety of settings using a combination of synthetic and 45 real datasets from …,Proceedings of the VLDB Endowment,2014,27,22
RankSQL: supporting ranking queries in relational database management systems,Chengkai Li; Mohamed A Soliman; Kevin Chen-Chuan Chang; Ihab F Ilyas,Abstract Ranking queries (or top-k queries) are dominant in many emerging applications;eg; similarity queries in multimedia databases; searching Web databases; middleware; anddata mining. The increasing importance of top-k queries warrants an efficient support ofranking in the relational database management system (RDBMS) and has recently gainedthe attention of the research community. Top-k queries aim at providing only the top k queryresults; according to a user-specified ranking function; which in many cases is an aggregateof multiple criteria. The following is an example top-k query.,Proceedings of the 31st international conference on Very large data bases,2005,27,23
An extensible index for spatial databases,Walid G Aref; Ihab F Ilyas,Emerging database applications require the use of new indexing structures beyond B-treesand R-trees. Examples are the kD tree; the trie; the quadtree; and their variants. They areoften proposed as supporting structures in data mining; GIS; and CAD/CAM applications. Acommon feature of all these indexes is that they recursively divide the spare into partitions. Anovel extensible index structure; termed SP-GiST; is presented that supports this class ofdata structure; mainly the class of space partitioning unbalanced trees. Simple methodimplementations are provided that demonstrate how SP-GiST can behave as a kD tree; atrie; a quadtree; or any of their variants. Issues related to clustering tree nodes into pages aswell as concurrency control for SP-GiST are addressed. A dynamic minimum-heightclustering technique is applied to minimize disk accesses and to make using such trees in …,Scientific and Statistical Database Management; 2001. SSDBM 2001. Proceedings. Thirteenth International Conference on,2001,27,22
Knowledge representation systems and methods incorporating data consumer models and preferences,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including both an elemental datastructure and knowledge processing rules stored as machine-readable data and/orprogramming instructions. One or more of the knowledge processing rules may be appliedto analyze an input complex KR to deconstruct its complex concepts and/or conceptrelationships to elemental concepts and/or concept relationships to be included in theelemental data structure. One or more of the knowledge processing rules may be applied tosynthesize an output complex KR from the stored elemental data structure in accordancewith context information. Methods of populating an elemental data structure and methods ofsynthesizing complex KRs from the elemental data structure may depend on user models …,*,2012,25,4
Discovering and exploiting statistical properties for query optimization in relational databases: A survey,Peter J Haas; Ihab F Ilyas; Guy M Lohman; Volker Markl,Abstract Discovering and exploiting statistical features in relational datasets is key to queryoptimization in a relational database management system (RDBMS); and is also needed fordatabase design; cleaning; and integration. This paper surveys a variety of methods forautomatically discovering important statistical features such as correlations; functionaldependencies; keys; and algebraic constraints. We discuss proactive approaches in whichthe data is scanned or sampled (periodically; at optimization time or at query time); or inwhich exploratory queries are executed. Also discussed are reactive approaches thatmonitor the results of the query processing. Finally; we discuss methods for dealing with thepractical challenges of maintaining statistical information in the face of heavy systemutilization; and of dealing with inconsistencies that arise from incomplete cardinality …,*,2009,25,23
NADEEF: A generalized data cleaning system,Amr Ebaid; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang; Si Yin,Abstract We present NADEEF; an extensible; generic and easy-to-deploy data cleaningsystem. NADEEF distinguishes between a programming interface and a core to achievegenerality and extensibility. The programming interface allows users to specify data qualityrules by writing code that implements predefined classes. These classes uniformly definewhat is wrong with the data and (possibly) how to fix it. We will demonstrate the followingfeatures provided by NADEEF.(1) Heterogeneity: The programming interface can be used toexpress many types of data quality rules beyond the well known CFDs (FDs); MDs and ETLrules.(2) Interdependency: The core algorithms can interleave multiple types of rules todetect and repair data errors.(3) Deployment and extensibility: Users can easily customizeNADEEF by defining new types of rules; or by extending the core.(4) Metadata …,Proceedings of the VLDB Endowment,2013,24,10
URank: formulation and efficient evaluation of top-k queries in uncertain databases,Mohamed A Soliman; Ihab F Ilyas; Kevin Chen-Chuan Chang,Abstract Top-k processing in uncertain databases is semantically and computationallydifferent from traditional top-k processing. The interplay between query scores and datauncertainty makes traditional techniques inapplicable. We introduce URank; a system thatprocesses new probabilistic formulations of top-k queries inuncertain databases. The newformulations are based on marriage of traditional top-k semantics with possible worldssemantics. URank encapsulates a new processing framework that leverages existing queryprocessing capabilities; and implements efficient search strategies that integrate ranking onscores with ranking on probabilities; to obtain meaningful answers for top-k queries.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,24,12
Systems and methods for analyzing and synthesizing complex knowledge representations,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including both an elemental datastructure and knowledge processing rules stored as machine-readable data and/orprogramming instructions. One or more of the knowledge processing rules may be appliedto analyze an input complex KR to deconstruct its complex concepts and/or conceptrelationships to elemental concepts and/or concept relationships to be included in theelemental data structure. One or more of the knowledge processing rules may be applied tosynthesize an output complex KR from the stored elemental data structure in accordancewith an input context. Multiple input complex KRs of various types may be analyzed anddeconstructed to populate the elemental data structure; and input complex KRs may be …,*,2015,23,10
Detecting Data Errors: Where are we and what needs to be done?,Ziawasch Abedjan; Xu Chu; Dong Deng; Raul Castro Fernandez; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker; Nan Tang,Abstract Data cleaning has played a critical role in ensuring data quality for enterpriseapplications. Naturally; there has been extensive research in this area; and many datacleaning algorithms have been translated into tools to detect and to possibly repair certainclasses of errors such as outliers; duplicates; missing values; and violations of integrityconstraints. Since different types of errors may coexist in the same data set; we often need torun more than one kind of tool. In this paper; we investigate two pragmatic questions:(1) arethese tools robust enough to capture most errors in real-world data sets? and (2) what is thebest strategy to holistically run multiple tools to optimize the detection effort? To answerthese two questions; we obtained multiple data cleaning tools that utilize a variety of errordetection techniques. We also collected five real-world data sets; for which we could …,Proceedings of the VLDB Endowment,2016,22,10
Benchmarking Smart Meter Data Analytics.,Xiufeng Liu; Lukasz Golab; Wojciech M Golab; Ihab F Ilyas,ABSTRACT Smart electricity meters have been replacing conventional meters worldwide;enabling automated collection of fine-grained (every 15 minutes or hourly) consumptiondata. A variety of smart meter analytics algorithms and applications have been proposed;mainly in the smart grid literature; but the focus thus far has been on what can be done withthe data rather than how to do it efficiently. In this paper; we examine smart meter analyticsfrom a software performance perspective. First; we propose a performance benchmark thatincludes common data analysis tasks on smart meter data. Second; since obtaining largeamounts of smart meter data is difficult due to privacy issues; we present an algorithm forgenerating large realistic data sets from a small seed of real data. Third; we implement theproposed benchmark using five representative platforms: a traditional numeric computing …,EDBT,2015,22,10
Systems and methods for applying statistical inference techniques to knowledge representations,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including both an elemental datastructure and knowledge processing rules stored as machine-readable data and/orprogramming instructions. One or more of the knowledge processing rules may be appliedto analyze an input complex KR to deconstruct its complex concepts and/or conceptrelationships to elemental concepts and/or concept relationships to be included in theelemental data structure. One or more of the knowledge processing rules may be applied tosynthesize an output complex KR from the stored elemental data structure in accordancewith context information. Methods of populating an elemental data structure and methods ofsynthesizing a complex KR from the elemental data structure may rely on statistical …,*,2014,22,3
Management of interesting database statistics,*,A method; system; and computer program product for managing database statistics areprovided. The method; system; and computer program product provide for receiving a queryfor optimizing; collecting statistics specific to the query prior to generating any access plansfor executing the query; and generating an access plan for executing the query based on thecollected statistics.,*,2014,22,10
Sampling from repairs of conditional functional dependency violations,George Beskales; Ihab F Ilyas; Lukasz Golab; Artur Galiullin,Abstract Violations of functional dependencies (FDs) and conditional functionaldependencies (CFDs) are common in practice; often indicating deviations from the intendeddata semantics. These violations arise in many contexts such as data integration and Webdata extraction. Resolving these violations is challenging for a variety of reasons; one ofthem being the exponential number of possible repairs. Most of the previous work hastackled this problem by producing a single repair that is nearly optimal with respect to somemetric. In this paper; we propose a novel data cleaning approach that is not limited to findinga single repair; namely sampling from the space of possible repairs. We give severalmotivating scenarios where sampling from the space of CFD repairs is desirable; wepropose a new class of useful repairs; and we present an algorithm that randomly …,The VLDB Journal,2014,22,14
A Video Database Management System for Advancing Video Database Research.,Walid G Aref; Ann Christine Catlin; Jianping Fan; Ahmed K Elmagarmid; Moustafa A Hammad; Ihab F Ilyas; Mirette S Marzouk; Xingquan Zhu,Abstract The most useful environments for advancing research and development in videodatabases are those that provide complete video database management; including (1) videopreprocessing for content representation and indexing;(2) storage management for video;metadata and indices;(3) image and semantic-based query processing;(4) realtime buffermanagement; and (5) continuous media streaming. Such environments support the entireprocess of investigating; implementing; analyzing and evaluating new techniques; thusidentifying in a concrete way which techniques are truly practical and robust. In this paperwe present a video database research initiative that culminated in the successfuldevelopment of VDBMS; a video database research platform that supports comprehensiveand efficient database management for digital video. We describe key video processing …,Multimedia Information Systems,2002,22,22
System and method for performing a semantic operation on a digital social network,*,Disclosed is a system and method for performing a semantic operation on a social network.In an embodiment; the method comprises receiving a social network user context associatedwith a user of the social network; generating; through a semantic operation; an interestnetwork based on the user context information; and filtering; ranking or augmenting; using atleast one processor executing stored program instructions; a retrieval of information relatedto the social network based on the interest network; wherein the interest network comprisesconcepts represented by a data structure associated with the concepts in the interestnetwork. In another embodiment; the method further comprises representing the interestnetwork as an interest graph. In yet another embodiment; the semantic operation is asynthesis operation or retrieval operation performed on a knowledge representation.,*,2013,20,10
Data cleaning: Overview and emerging challenges,Xu Chu; Ihab F Ilyas; Sanjay Krishnan; Jiannan Wang,Abstract Detecting and repairing dirty data is one of the perennial challenges in dataanalytics; and failure to do so can result in inaccurate analytics and unreliable decisions.Over the past few years; there has been a surge of interest from both industry and academiaon data cleaning problems including new abstractions; interfaces; approaches for scalability;and statistical techniques. To better understand the new advances in the field; we will firstpresent a taxonomy of the data cleaning literature in which we highlight the recent interest intechniques that use constraints; rules; or patterns to detect errors; which we call qualitativedata cleaning. We will describe the state-of-the-art techniques and also highlight theirlimitations with a series of illustrative examples. While traditionally such approaches aredistinct from quantitative approaches such as outlier detection; we also discuss recent …,Proceedings of the 2016 International Conference on Management of Data,2016,17,16
SMAS: A smart meter data analytics system,Xiufeng Liu; Lukasz Golab; Ihab F Ilyas,Smart electricity meters are replacing conventional meters worldwide and have enabled anew application domain: smart meter data analytics. In this paper; we introduce SMAS; oursmart meter analytics system; which demonstrates the actionable insight that consumers andutilities can obtain from smart meter data. Notably; we implemented SMAS inside a relationaldatabase management system using open source tools: PostgreSQL and the MADLibmachine learning toolkit. In the proposed demonstration; conference attendees will interactwith SMAS as electricity providers; consultants and consumers; and will perform variousanalyses on real data sets.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,16,10
Video query processing in the VDBMS testbed for video database research,Walid Aref; Moustafa Hammad; Ann Christine Catlin; Ihab Ilyas; Thanaa Ghanem; Ahmed Elmagarmid; Mirette Marzouk,Abstract The increased use of video data sets for multimedia-based applications has createda demand for strong video database support; including efficient methods for handling thecontent-based query and retrieval of video data. Video query processing presents significantresearch challenges; mainly associated with the size; complexity and unstructured nature ofvideo data. A video query processor must support video operations for search by contentand streaming; new query types; and the incorporation of video methods and operators ingenerating; optimizing and executing query plans. In this paper; we address these queryprocessing issues in two contexts; first as applied to the video data type and then as appliedto the stream data type. We first present the query processing functionality of the VDBMSvideo database management system as a framework designed to support the full range of …,Proceedings of the 1st ACM international workshop on Multimedia databases,2003,15,23
Distributed data deduplication,Xu Chu; Ihab F Ilyas; Paraschos Koutris,Abstract Data deduplication refers to the process of identifying tuples in a relation that referto the same real world entity. The complexity of the problem is inherently quadratic withrespect to the number of tuples; since a similarity value must be computed for every pair oftuples. To avoid comparing tuple pairs that are obviously non-duplicates; blockingtechniques are used to divide the tuples into blocks and only tuples within the same blockare compared. However; even with the use of blocking; data deduplication remains a costlyproblem for large datasets. In this paper; we show how to further speed up datadeduplication by leveraging parallelism in a shared-nothing computing environment. Ourmain contribution is a distribution strategy; called Dis-Dedup; that minimizes the maximumworkload across all worker nodes and provides strong theoretical guarantees. We …,Proceedings of the VLDB Endowment,2016,14,4
Dataxformer: An interactive data transformation tool,John Morcos; Ziawasch Abedjan; Ihab Francis Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract While syntactic transformations require the application of a formula on the inputvalues; such as unit conversion or date format conversions; semantic transformations; suchas" zip code to city"; require a look-up in some reference data. We recently presentedDataXFormer; a system that leverages Web tables; Web forms; and expert sourcing to covera wide range of transformations. In this demonstration; we present the user-interaction withDataXFormer and show scenarios on how it can be used to transform data and explore theeffectiveness and efficiency of several approaches for transformation discovery; leveragingabout 112 million tables and online sources.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,14,22
Smart meter data analytics: systems; algorithms; and benchmarking,Xiufeng Liu; Lukasz Golab; Wojciech Golab; Ihab F Ilyas; Shichao Jin,Abstract Smart electricity meters have been replacing conventional meters worldwide;enabling automated collection of fine-grained (eg; every 15 minutes or hourly) consumptiondata. A variety of smart meter analytics algorithms and applications have been proposed;mainly in the smart grid literature. However; the focus has been on what can be done withthe data rather than how to do it efficiently. In this article; we examine smart meter analyticsfrom a software performance perspective. First; we design a performance benchmark thatincludes common smart meter analytics tasks. These include offline feature extraction andmodel building as well as a framework for online anomaly detection that we propose.Second; since obtaining real smart meter data is difficult due to privacy issues; we presentan algorithm for generating large realistic datasets from a small seed of real data. Third …,ACM Transactions on Database Systems (TODS),2017,13,16
Dataxformer: Leveraging the Web for Semantic Transformations.,Ziawasch Abedjan; John Morcos; Michael N Gubanov; Ihab F Ilyas; Michael Stonebraker; Paolo Papotti; Mourad Ouzzani,ABSTRACT Data transformation is a crucial step in data integration. While sometransformations; such as liters to gallons; can be easily performed by applying a formula or aprogram on the input values; others; such as zip code to city; require sifting through arepository containing explicit value mappings. There are already powerful systems thatprovide formulae and algorithms for transformations. However; the automated identificationof reference datasets to support value mapping remains largely unresolved. The Web ishome to millions of tables with many containing explicit value mappings. This is in additionto value mappings hidden behind Web forms. In this paper; we present DataXFormer; atransformation engine that leverages Web tables and Web forms to perform transformationtasks. In particular; we describe an inductive; filter-refine approach for identifying explicit …,CIDR,2015,13,4
Probabilistic Ranking Techniques in Relational Databases,Ihab F Ilyas; Mohamed A Soliman,Abstract Ranking queries are widely used in data exploration; data analysis and decisionmaking scenarios. While most of the currently proposed ranking techniques focus ondeterministic data; several emerging applications involve data that are imprecise oruncertain. Ranking uncertain data raises new challenges in query semantics andprocessing; making conventional methods inapplicable. Furthermore; the interplay betweenranking and uncertainty models introduces new dimensions for ordering query results thatdo not exist in the traditional settings. This lecture describes new formulations andprocessing techniques for ranking queries on uncertain data. The formulations are based onmarriage of traditional ranking semantics with possible worlds semantics under widely-adopted uncertainty models. In particular; we focus on discussing the impact of tuple-level …,Synthesis Lectures on Data Management,2011,13,22
Building ranked mashups of unstructured sources with uncertain information,Mohamed A Soliman; Ihab F Ilyas; Mina Saleeb,Abstract Mashups are situational applications that join multiple sources to better meet theinformation needs of Web users. Web sources can be huge databases behind queryinterfaces; which triggers the need of ranking mashup results based on some userpreferences. We present MashRank; a mashup authoring and processing system buildingon concepts from rank-aware processing; probabilistic databases; and information extractionto enable ranked mashups of (unstructured) sources with uncertain ranking attributes.MashRank is based on new semantics; formulations and processing techniques to handleuncertain preference scores; represented as intervals enclosing possible score values.,Proceedings of the VLDB Endowment,2010,13,10
InterJoin: Exploiting indexes and materialized views in XPath evaluation,Derek Phillips; Ning Zhang; Ihab F Ilyas; M Tamer Ozsu,XML has become the standard for data exchange for a wide variety of applications;particularly in the scientific community. In order to efficiently process queries on XMLrepresentations of scientific data; we require specialized techniques for evaluating XPathexpressions. Exploiting materialized views in query processing significantly enhances queryprocessing performance. We propose a novel view definition that allows for intermediate(structural) join results to be stored and reused in XML query evaluation. Unlike current XMLview proposals; our views do not require navigation in the original document or path-basedpattern matching. Hence; they are evaluated significantly faster and are easily costed as partof a query plan. In general; current structural joins cannot exploit views efficiently when theview definition is not a prefix (or a suffix) of the XPath query. To increase the applicability …,Scientific and Statistical Database Management; 2006. 18th International Conference on,2006,13,22
A distributed database server for continuous media,Walid G Aref; Ann Christine Catlin; Ahmed K Elmagarmid; Jianping Fan; J Guo; M Hammad; Ihab F Ilyas; Mirette S Marzouk; Sunil Prabhakar; Abdelmounaam Rezgui; S Teoh; Evimaria Terzi; Y Tu; Athena Vakali; XQ Zhu,In our project; we are adopting a new approach for handling video data. We view the videoas a well-defined data type with its own description; parameters and applicable methods.The system is based on PREDATOR; an open-source object-relational DBMS. PREDATORuses Shore as the underlying storage manager. Supporting video operations (storing;searching-by-content and streaming) and new query types (query-by-example and multi-feature similarity searching) requires major changes in many of the traditional systemcomponents. More specifically; the storage and buffer manager has to deal with hugevolumes of data with real-time constraints. Query processing has to consider the videomethods and operators in generating; optimizing and executing the query plans.,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,13,4
The Data Civilizer System.,Dong Deng; Raul Castro Fernandez; Ziawasch Abedjan; Sibo Wang; Michael Stonebraker; Ahmed K Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Nan Tang,ABSTRACT In many organizations; it is often challenging for users to find relevant data forspecific tasks; since the data is usually scattered across the enterprise and ofteninconsistent. In fact; data scientists routinely report that the majority of their effort is spentfinding; cleaning; integrating; and accessing data of interest to a task at hand. In order todecrease the “grunt work” needed to facilitate the analysis of data “in the wild”; we presentDATA CIVILIZER; an end-to-end big data management system. DATA CIVILIZER has alinkage graph computation module to build a linkage graph for the data and a data discoverymodule which utilizes the linkage graph to help identify data that is relevant to user tasks. Italso uses the linkage graph to discover possible join paths that can then be used in a query.For the actual query execution; we use a polystore DBMS; which federates query …,CIDR,2017,12,10
Learning to identify relevant studies for systematic reviews using random forest and external information,Madian Khabsa; Ahmed Elmagarmid; Ihab Ilyas; Hossam Hammady; Mourad Ouzzani,Abstract We tackle the problem of automatically filtering studies while preparing SystematicReviews (SRs) which normally entails manually inspecting thousands of studies to identifythe few to be included. The problem is modeled as an imbalanced data classification taskwhere the cost of misclassifying the minority class is higher than the cost of misclassifyingthe majority class. This work introduces a novel method for representing systematic reviewsbased not only on lexical features; but also utilizing word clustering and citation features.This novel representation is shown to outperform previously used features in representingsystematic reviews; regardless of the classifier. Our work utilizes a random forest classifierwith the novel features to accurately predict included studies with high recall. Theparameters of the random forest are automatically configured using heuristics methods …,Machine Learning,2016,11,22
Statadvisor: Recommending statistical views,Amr El-Helw; Ihab F Ilyas; Calisto Zuzarte,Abstract Database statistics are crucial to cost-based optimizers for estimating the executioncost of a query plan. Using traditional basic statistics on base tables requires adoptingunrealistic assumptions to estimate the cardinalities of intermediate results; which usuallycauses large estimation errors that can be several orders of magnitude. Modern commercialdatabase systems support statistical or sample views; which give more accurate statistics onintermediate results and query sub-expressions. While previous research focused oncreating and maintaining these advanced statistics; only little effort has been done towardsautomatically recommending the most beneficial statistical views to construct. In this paper;we present StatAdvisor; a system for recommending statistical views for a given SQLworkload. The StatAdvisor addresses the special characteristics of statistical views with …,Proceedings of the VLDB Endowment,2009,11,23
VDBMS: A testbed facility for research in video database benchmarking,Walid Aref; Ann Christine Catlin; Ahmed Elmagarmid; Jianping Fan; Moustafa Hammad; Ihab Ilyas; Mirette Marzouk; Sunil Prabhakar; Yi-Cheng Tu; Xingquan Zhu,Abstract. Real-world video-based applications require database technology that is capableof storing digital video in the form of video databases and providing content-based videosearch and retrieval. Methods for handling traditional data storage; query; search; retrieval;and presentation cannot be extended to provide this functionality. The VDBMS researchinitiative is motivated by the requirements of video-based applications to search and retrieveportions of video data based on content and by the need for testbed facilities to facilitateresearch in the area of video database management. In this paper we describe the VDBMSvideo database research platform; a system that supports comprehensive and efficientdatabase management for digital video. Our fundamental concept is to provide a full rangeof functionality for video as a well-defined abstract database data type; with its own …,Multimedia Systems,2004,11,10
CLAMS: bringing quality to Data Lakes,Mina Farid; Alexandra Roatis; Ihab F Ilyas; Hella-Franziska Hoffmann; Xu Chu,Abstract With the increasing incentive of enterprises to ingest as much data as they can inwhat is commonly referred to as" data lakes"; and with the recent development of multipletechnologies to support this" load-first" paradigm; the new environment presents seriousdata management challenges. Among them; the assessment of data quality and cleaninglarge volumes of heterogeneous data sources become essential tasks in unveiling the valueof big data. The coveted use of unstructured and semi-structured data in large volumesmakes current data cleaning tools (primarily designed for relational data) not directlyadoptable. We present CLAMS; a system to discover and enforce expressive integrityconstraints from large amounts of lake data with very limited schema information (eg;represented as RDF triples). This demonstration shows how CLAMS is able to discover …,Proceedings of the 2016 International Conference on Management of Data,2016,10,23
RuleMiner: Data quality rules discovery,Xu Chu; Ihab F Ilyas; Paolo Papotti; Yin Ye,Integrity constraints (ICs) are valuables tools for enforcing correct application semantics.However; manually designing ICs require experts and time; hence the need for automaticdiscovery. Previous automatic ICs discovery suffer from (1) limited ICs languageexpressiveness; and (2) time-consuming manual verification of discovered ICs. Weintroduce RULEMINER; a system for discovering data quality rules that addresses thelimitations of existing solutions.,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,10,12
Just-in-time information extraction using extraction views,Amr El-Helw; Mina H Farid; Ihab F Ilyas,Modern real-world applications often rely on large amounts of data that grow at rapid rates.While some of this data is stored in a structured form; unstructured text documents such asWeb pages; email messages; news articles and reports contain rich information that can bevery useful if extracted on time. The ability to answer structured SQL queries over this kind ofunstructured data allows for more complex analysis and better insights into that data.Example applications that benefit from structured queries over unstructured textual datainclude reputation management systems; which download Web pages to track the “buzz”around companies and products; comparative shopping agents; which locate ecommerceWeb sites and index the products offered; and other information extraction applications;which retrieve documents and extract structured relations from the unstructured text. For …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,10,22
Rank-join algorithms for search computing,Ihab F Ilyas; Davide Martinenghi; Marco Tagliasacchi,Abstract Joins represent the basic functional operations of complex query plans in a SearchComputing system; as discussed in the previous chapter. In this chapter we provide furtherinsight on this matter; by focusing on algorithms that deal with joining ranked resultsproduced by search services. We cast this problem as a generalization of the traditional rankaggregation problem; ie; combining several ranked lists of objects to produce a singleconsensus ranking. Rank-join algorithms; also called top-k join algorithms; aim atdetermining the best overall results without accessing all the objects. The rank-join problemhas been dealt with in the literature by extending rank aggregation algorithms to the case ofjoin in the setting of relational databases. However; previous approaches to top-k queriesdid not consider some of the distinctive features of search engines on the Web. Indeed; as …,*,2010,10,18
Holoclean: Holistic data repairs with probabilistic inference,Theodoros Rekatsinas; Xu Chu; Ihab F Ilyas; Christopher Ré,Abstract We introduce HoloClean; a framework for holistic data repairing driven byprobabilistic inference. HoloClean unifies qualitative data repairing; which relies on integrityconstraints or external data sources; with quantitative data repairing methods; whichleverage statistical properties of the input data. Given an inconsistent dataset as input;HoloClean automatically generates a probabilistic program that performs data repairing.Inspired by recent theoretical advances in probabilistic inference; we introduce a series ofoptimizations which ensure that inference over HoloClean's probabilistic model scales toinstances with millions of tuples. We show that HoloClean finds data repairs with an averageprecision of∼ 90% and an average recall of above∼ 76% across a diverse array of datasetsexhibiting different types of errors. This yields an average F1 improvement of more than 2 …,Proceedings of the VLDB Endowment,2017,9,22
Methods and devices for customizing knowledge representation systems,*,Techniques for customizing knowledge representation systems including identifying; basedon a plurality of concepts in a knowledge representation (KR); a group of one or moreconcepts relevant to user context information; and providing the identified group of one moreconcepts to a user. The KR may include a combination of modules. The modules mayinclude a kernel and a customized module customized for the user. The kernel mayaccessible via a second KR.,*,2016,9,10
NADEEF/ER: Generic and interactive entity resolution,Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,ABSTRACT Entity resolution (ER); the process of identifying and eventually merging recordsthat refer to the same real-world entities; is an important and long-standing problem. Wepresent Nadeef/Er; a generic and interactive entity resolution system; which is built as anextension over our open-source generalized data cleaning system Nadeef. Nadeef/Erprovides a rich programming interface for manipulating entities; which allows generic;efficient and extensible ER. In this demo; users will have the opportunity to experience thefollowing features:(1) Easy specification–Users can easily define ER rules with a browser-based specification; which will then be automatically transformed to various functions;treated as black-boxes by Nadeef;(2) Generality and extensibility–Users can customize theirER rules by refining and fine-tuning the above functions to achieve both effective and …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,9,12
System and method for obtaining preferences with a user interface,*,Techniques for obtaining user preferences. The techniques include receiving user contextinformation associated with at least one user; identifying; based at least in part on thereceived user context information; a plurality of attributes of items in a plurality of items;obtaining; using at least one processor; at least one first-order user preference based atleast in part on a first input provided by the at least one user; wherein the plurality of first-order user preferences comprises a preference for a first attribute in the plurality of attributes;and obtaining; using the at least one processor; at least one second-order user preferencebased at least in part on a second input provided by the at least one user; wherein the atleast one second-order user preference comprises a preference among attributes in theplurality of attributes.,*,2012,9,10
Recommending statistical views using cost/benefit metrics,*,A workload to be handled by a database system can be identified. The workload can includeat least one query that the database system is to handle. A set of at least one candidatestatistical views (statviews) to be utilized when optimizing the workload can be enumerated.A benefit value and a cost value of the each of the enumerated candidate statistical viewsrelative to the entire workload can be computed. The cost value can reflect a cost ofconstructing and collecting statistics on the associated statistical view. A set of the candidateviews most beneficial for handling the workload can be determined based upon thecomputed benefit values and computed cost values. A generalization phase that augmentsthe candidate view set with higher value candidate views for consideration during therecommendation phase. The optimum subset of views from the determined set of …,*,2010,9,4
Automatic relationship discovery in self-managing database systems,Ihab Ilyas; Volker Markl; P Haas; Paul G Brown; Ashraf Aboulnaga,In this paper; we describe CORDS; an algorithm that automatically discovers correlationsand soft functional dependencies (FDs) between pairs of columns and; based on theserelationships; determines a set of statistics to maintain. This data-driven technology is anessential complement to query-driven approaches such as LEO; helping to ensureacceptable performance during slow learning periods. CORDS focuses on column pairsbecause this greatly simplifies the algorithms; and experiments have shown that themarginal benefit of capturing n-way dependencies for n> 2 is relatively small.,Autonomic Computing; 2004. Proceedings. International Conference on,2004,8,10
DataXFormer: A robust transformation discovery system,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,In data integration; data curation; and other data analysis tasks; users spend a considerableamount of time converting data from one representation to another. For example US dates toEuropean dates or airport codes to city names. In a previous vision paper; we presented theinitial design of DataXFormer; a system that uses web resources to assist in transformationdiscovery. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. In this paper; we presentthe full fledged system along with several extensions. In particular; we present algorithms tofind (i) transformations that entail multiple columns of input data;(ii) indirect transformationsthat are compositions of other transformations;(iii) transformations that are not functions butrather relationships; and (iv) transformations from a knowledge base of public data. We …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,7,12
Preference-guided semantic processing,*,Techniques for specifying user preferences in a semantic network. The techniques includereceiving; using at least one processor; a plurality of first-order user preferences for at leastone concept in a semantic network; wherein the plurality of first-order user preferences areindicative of a user's preferences among children of attributes of the at least one concept inthe semantic network; receiving; using the at least one processor; at least one second-orderuser preference for the at least one concept in the semantic network; wherein the at leastone second-order user preference is indicative of the user's preferences among attributes ofthe at least one concept; and performing at least one semantic processing act by using thesemantic network; the plurality of first-order user preferences; and the at least one second-order user preference.,*,2015,7,10
Identifying information of interest based on user preferences,*,Techniques for calculating a ranking of at least one item in a plurality of items. Thetechniques include receiving user preferences comprising a plurality of first-order userpreferences indicative of a user's preferences for items in the plurality of items; and at leastone second-order user preference indicative of the user's preferences among first-order userpreferences in the plurality of first-order user preferences; calculating; with at least oneprocessor; a ranking of the at least one item in the plurality of items based; at least in part on;at least one data structure encoding a preference graph that represents the received userpreferences; and identifying and outputting at least a subset of the plurality of items to a user;in accordance with the ranking.,*,2015,7,10
Mashrank: Towards uncertainty-aware and rank-aware mashups,Mohamed A Soliman; Mina Saleeb; Ihab F Ilyas,Mashups are situational applications that build data flows to link the contents of multipleWeb sources. Often times; ranking the results of a mashup is handled in a materialize-then-sort fashion; since combining multiple data sources usually destroys their original rankings.Moreover; although uncertainty is ubiquitous on the Web; most mashup tools do not reasonabout or reflect such uncertainty. We introduce MashRank; a mashup tool that treats rankingas a first-class citizen in mashup construction; and allows for rank-joining Web sources withuncertain information. To the best of our knowledge; no current tools allow for similarfunctionalities. MashRank encapsulates a new probabilistic model reflecting uncertainty inranking; a set of techniques implemented as pipelined operators in mashup plans; and aprobabilistic ranking infrastructure based on Monte-Carlo sampling.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,7,4
Nile: a query processing engine for data streams,WG Aref; A Elmargamid; M Ali; M Caltin; MG Elfeky,This demonstration presents the design of" STEAM"; Purdue Boiler Makers' stream databasesystem that allows for the processing of continuous and snap-shot queries over datastreams. Specifically; the demonstration will focus on the query processing part;" Nile". Nileextends the query processor engine of an object-relational database management system tosupport data streams. Our approach is motivated by the fact that industrial strength DBMSprovides greater flexibility in defining and executing queries over static data. Furthermore;many emerging applications; particularly in pervasive computing; sensor-basedenvironments; retail transactions; and video processing continuously report up-to-the-minutereadings of sensor values; locations; status updates; etc. Therefore; extending DBMSfunctionality to support data streams could play a central role for these emerging …,Proceedings of ICDE: the 20th International Conference on Data Engineering,2004,7,12
Quick: Expressive and flexible search over knowledge bases and text collections,Jeffrey Pound; Ihab F Ilyas; Grant Weddell,Abstract Recent work on Web-extracted data sets has produced an interesting new source ofstructured Web data. These data sets can be viewed as knowledge bases (KB)--largeheterogeneous linked entity collections with millions of unique edge and node labels; oftenencoding rich semantic information over entities. For example; YAGO [5] and ExDB [2] havefact collections numbering in the tens and hundreds of millions respectfully; and WebTables[1] contains over one hundred million extracted relations. In terms of schema information; theExDB; YAGO; and WebTables data sets all have schema items numbering in the millions.,Proceedings of the VLDB Endowment,2010,6,5
KATARA: Reliable data cleaning with knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Data cleaning with guaranteed reliability is hard to achieve without accessingexternal sources; since the truth is not necessarily discoverable from the data at hand.Furthermore; even in the presence of external sources; mainly knowledge bases andhumans; effectively leveraging them still faces many challenges; such as aligningheterogeneous data sources and decomposing a complex task into simpler units that can beconsumed by humans. We present K atara; a novel end-to-end data cleaning systempowered by knowledge bases and crowdsourcing. Given a table; a kb; and a crowd; K atara(i) interprets the table semantics wrt the given kb;(ii) identifies correct and wrong data; and(iii) generates top-k possible repairs for the wrong data. Users will have the opportunity toexperience the following features of K atara:(1) Easy specification: Users can define a K …,Proceedings of the VLDB Endowment,2015,5,7
Psalm: Cardinality estimation in the presence of fine-grained access controls,Huaxin Zhang; Ihab F Ilyas; Kenneth Salem,In database systems that support fine-grained access controls; each user has access rightsthat determine which tuples are accessible and which are inaccessible. Queries areanswered as if the inaccessible tuples are not present in the database. Thus; users withdifferent access rights may get different answers to a given query. To process queriesefficiently in the presence of fine-grained access controls; the database system needsaccurate estimates of the number of tuples that are both accessible according to the accessrights of the submitting user and relevant according to the selection predicates in the query.In this paper; we present PSALM; a sampling-based cardinality estimation technique for usein the presence of fine-grained access controls. Our technique exploits the fact that accessrights are relatively static and are common to all queries that are evaluated on behalf of a …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,5,10
Finding skyline and top-k bargaining solutions,Mohamed A Soliman; Ihab F Ilyas; Nick Koudas,Page 1. Finding Skyline and Top-k Bargaining Solutions Mohamed A. Soliman Ihab F. Ilyas NickKoudas School of Computer Science School of Computer Science Department of ComputerScience University of Waterloo University of Waterloo University of Toronto m2ali@cs.uwaterloo.ca ilyas@uwaterloo.ca koudas@cs.toronto.edu Abstract tities. Profiles are matched beforeinteraction commences to form the bi-matrices in Figure 1(c) which represent two We addressskyline and top-k processing in web interac- potential interactions. The utility pair (5; 30); forexample; tion scenarios. We model the problem space based on game means that site gainsa utility of 5 if it gives a discount in theory principles andpresent new algorithms and heuristicsexchange of user's email; while a gains of 30 in utility is ob- to realize solutions efficiently. tainedif one reveals an email address as a result of obtaining a discount …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,5,22
LOT: A robust overlay for distributed range query processing,André Allavena; Qiang Wang; Ihab Ilyas; Srinivasan Keshav,Abstract Large-scale data-centric services are often handled by clusters of computers thatinclude hundreds of thousands of computing nodes. However; traditional distributed queryprocessing techniques fail to handle the large-scale distribution; peer-to-peercommunication and frequent disconnection. In this paper; we introduce LOT; a robust; fault-tolerant and highly distributed overlay network for large-scale peer-to-peer queryprocessing. LOT is based on a robust tree overlay for distributed systems. It usesvirtualization; replication; geographic-based clustering and flexible state definition as basicdesign principles. We show how we map these principles to desirable performance goals.Moreover; we provide a lightweight maintenance mechanism for updating state information.Analysis and simulations show that our approach is superior to other well-known …,*,2006,5,10
ProbClean: A probabilistic duplicate detection system,George Beskales; Mohamed A Soliman; Ihab F Ilyas; Shai Ben-David; Yubin Kim,One of the most prominent data quality problems is the existence of duplicate records.Current data cleaning systems usually produce one clean instance (repair) of the input data;by carefully choosing the parameters of the duplicate detection algorithms. Finding the rightparameter settings can be hard; and in many cases; perfect settings do not exist. Wepropose ProbClean; a system that treats duplicate detection procedures as data processingtasks with uncertain outcomes. We use a novel uncertainty model that compactly encodesthe space of possible repairs corresponding to different parameter settings. ProbCleanefficiently supports relational queries and allows new types of queries against a set ofpossible repairs.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,4,22
Rank-aware query processsing and optimization,Ihab F Ilyas; Walid G Aref,Efficient execution of ranking query is increasingly becoming a major challenge for databasetechnology. DBMSs provide efficient update; indexing; concurrency and recovery. On theother hand; IR on text and multimedia requires techniques involving uncertainty and rankingfor effective retrieval. The main goal of this paper is to give an in-depth look on supportingranking queries as an increasingly interesting area of research. We cover the state-of-the-arttechniques in research prototypes and industry-strength database engines for efficienthandling of ranking and queries. We focus primarily on how to integrate ranking as a newquery processing and optimization dimension; with the aim of supporting ranking queries asa basic and core functionality. The paper identifies several challenges that need to beaddressed towards a true support for ranking and effective retrieval in database …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,4,16
Efficient processing of ad-hoc top-k aggregate queries in OLAP,Chengkai Li; Kevin Chen-Chuan Chang; Ihab F Ilyas,In this paper; we develop a principled framework for efficient processing of ad-hoc top-k(ranking) aggregate queries in OLAP. Such queries provide the k groups with the highestaggregates to decision makers. Essential support of top-k aggregate queries is lacking incurrent RDBMSs; which process such queries in a naive and overkill materialize-group-sortscheme; therefore can be prohibitively inefficient. Our new framework is based on twofundamental properties; the Group-Ranking and Tuple-Ranking Principles. The principlesdictate group-ordering and tuple-ordering requirement that together guide the queryprocessor toward the optimal aggregate query processing. To realize the requirements; wepropose a new execution model and address the challenges of implementing new queryoperators; enabling efficient top-k aggregate query plans that are both group-aware and …,*,2005,4,22
Rank-aware query processing and optimization,Ihab F Ilyas,This dissertation studies supporting ranked retrieval in database systems. In rankedretrieval; answers to user queries are reported (and computed) in an order that respectsspecific user criteria. Example of ranked retrieval support is producing the top k answers to asimilarity query in multimedia databases. In this chapter; we begin by highlighting the biggerpicture of integrating information retrieval technology in database systems. Next; in Section1.2; we motivate (using real examples) for the need to support ranked retrieval in databasesystems through efficient handling of ranking queries. In Section 1.4; we give formaldefinitions of two types of ranking queries: the top-k selection and the top-k join queries. Wegive an overview of our solution to support ranked retrieval in Section 1.5. We summarizethe main contributions of the dissertation in Section 1.6. Section 1.7 gives an outline for …,Purdue University; West Lafayette; IN,2004,4,23
Effective Data Cleaning with Continuous Evaluation.,Ihab F Ilyas,Abstract Enterprises have been acquiring large amounts of data from a variety of sources tobuild their own “Data Lakes”; with the goal of enriching their data asset and enabling richerand more informed analytics. The pace of the acquisition and the variety of the data sourcesmake it impossible to clean this data as it arrives. This new reality has made data cleaning acontinuous process and a part of day-to-day data processing activities. The large body ofdata cleaning algorithms and techniques is strong evidence of how complex the problem is;yet; it has had little success in being adopted in real-world data cleaning applications. In thisarticle we examine how the community has been evaluating the effectiveness of datacleaning algorithms; and if current data cleaning proposals are solving the right problems toenable the development of deployable and effective solutions.,IEEE Data Eng. Bull.,2016,3,23
Semantics and Encoding of the kell-m Calculus,Rolando Blanco; Paulo Alencar,Abstract We present kell-m; an asynchronous higher-order process algebra with hierarchicallocalities. The main focus of this report is on the operational semantics and behaviouralequivalences for kell-m. The operational semantics determine how systems representedusing kell-m evolve; the behavioural equivalences determine what it means for two kell-mprocesses to behave similarly. We also present and encoding of kell-m into MMC; thevariation of the-calculus as implemented in the Mobility Model Checker (MMC).,*,2011,3,22
Jtop algorithms for top-k join queries,Reza Akbarinia; Ihab F Ilyas; M Tamer Özsu; Patrick Valduriez,Top-k queries have attracted much interest in many different areas of computing such asnetwork and system monitoring [5][21]; information retrieval [4][20][22]; sensor networks[29][31]; multimedia databases [1][10][16][27]; spatial data analysis [33][19]; probabilisticdatabases [28]; data stream management systems [23][25]; etc. The main reason is that theyavoid overwhelming the user with large numbers of uninteresting answers. The two mainforms of top-k queries are top-k selection and top-k join. The seminal work by Fagin [13] ontop-k selection queries (or top-k queries for short) proposes a general model for answeringtop-k queries as follows. Suppose we have m lists of n data items such that each data itemhas a local score in each list. The lists are sorted according to the local scores of their dataitems. Each data item has an overall score; which is computed based on its local scores …,*,2008,2,10
Modeling Uncertainty in Duplicate Elimination,George Beskales; Mohamed A Soliman; Ihab F Ilyas,Abstract Real-world databases experience various data quality problems of different causesincluding heterogeneity of consolidated data sources; imprecision of reading devices; anddata entry errors. Existence of duplicate records is a prominent data quality problem. Theprocess of duplicate elimination often involves uncertainty in deciding on the true duplicates.Current tools resolve such uncertainty either through expert intervention; which is not alwayspossible; or by taking destructive decisions that may lead to unrecoverable errors. In thispaper; we approach duplicate elimination from a new perspective treating deduplicationprocedures as data processing tasks with uncertain outcomes. We propose a completeuncertainty model that compactly encodes the space of clean instances of the input data;and introduce efficient model implementations. We extend our model to capture the …,*,2008,2,4
A Framework for Supporting the Class of Space Partitioning Trees,Walid G Aref; Ihab F Ilyas,Abstract Emerging database applications require the use of new indexing structures beyondB-trees and R-trees. Examples are the kD tree; the trie; the quadtree; and their variants. Theyare often proposed as supporting structures in data mining; GIS; and CAD/CAM applications.A common feature of all these indexes is that they recursively divide the space intopartitions. A new extensible index structure; termed SP-GiST is presented that supports thisclass of data structures; mainly the class of space partitioning unbalanced trees. Simplemethod implementations are provided that demonstrate how SP-GiST can behave as a kDtree; a trie; a quadtree; or any of their variants. Issues related to clustering tree nodes intopages as well as concurrency control for SP-GiST are addressed. A dynamic minimum-height clustering technique is applied to minimize disk accesses and to make using such …,*,2001,2,4
CORDS,Ihab Ilyas; V Markl; P Haas; P Brown; A Aboulnaga,• A simple solution: build statistics on groups … • Relation R with two columns A;B … – Comparethe actual selectivity to the estimated … • The goal is to collect useful joint … – Apply pruningrules to limit # for Phase 2 … – Across all joinable tables (PK-FK pairing rule) … IF“skewed”: FILTER S with the frequent values … Build a (skew-dependent) contingency tablefor A × B from S … If correlated; RETURN (“Correlated with degree of correlation = x”) … • Falsepositives are ok (if not many) … • Basic CGS is the number of distinct … • CGS gives a betterestimate of joint … • Error due to faulty independence assumption is eliminated … – Inpractice; most error is due to independence assumption … – Future work: exploit column groupdistribution statistics … • Rank Soft FD's by their Strength … 0.9 0.92 0.94 0.96 0.98 0.99 1 24 6 … C G S (s e c o n d s ) … 100 1000 10000 100000 1000000 10000000 Sample Size,Proceedings of the 2004 ACM SIGMOD international conference on Management of data-SIGMOD,*,2,13
LONLIES: estimating property values for long tail entities,Mina Farid; Ihab F Ilyas; Steven Euijong Whang; Cong Yu,Abstract Web search engines often retrieve answers for queries about popular entities froma growing knowledge base that is populated by a continuous information extraction process.However; less popular entities are not frequently mentioned on the web and are generallyinteresting to fewer users; these entities reside on the long tail of information. Traditionalknowledge base construction techniques that rely on the high frequency of entity mentions toextract accurate facts about these mentions have little success with entities that have lowtextual support. We present Lonlies; a system for estimating property values of long tailentities by leveraging their relationships to head topics and entities. We demonstrate (1) howLonlies builds communities of entities that are relevant to a long tail entity utilizing a textcorpus and a knowledge base;(2) how Lonlies determines which communities to use in …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,1,4
Special issue on web data quality,Christian Bizer; Luna Dong; Ihab Ilyas; Maria-Esther Vidal,Abstract: We are delighted to present this special issue of the Journal of Data andInformation Quality on web data quality. This issue includes four innovative research articlescovering the areas of web data profiling; web data quality assessment; and web datacleansing. Over the last few years; the volume and variety of data that is available on theWeb has risen sharply. In addition to traditional data sources and formats such as CSV files;HTML tables; and deep web query interfaces; new techniques such as microdata; RDFa;microformats; and linked data have found wide adoption. In parallel; techniques forextracting structured data from web text and emistructured web content have maturedresulting in the creation of large-scale knowledge bases such as NELL; YAGO; DBpedia;and the Knowledge Vault. Independent of the specific data source or format or information …,Journal of data and information quality,2016,1,22
We are drowning in a sea of least publishable units (LPUs),David J DeWitt; Ihab F Ilyas; Jeffrey Naughton; Michael Stonebraker,Abstract Our field is drowning in a sea of conference submissions. We assert that the sheernumber of papers has begun to seriously hurt the quality of the work that the field is doingand that the field is going to implode unless we take action to remedy the situation. In orderto improve the quality of the papers being published we must reduce the number beingsubmitted. This will require a change in the culture of our field where" more" is beingequated to" better" by both hiring and promotion committees. In this panel we will exploresome ideas for correcting the situation.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,1,10
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1,4
Uncertainty in rank join,Ihab F Ilyas,Abstract At the core of the query processing engine of a search computing system areoperators that retrieve; filter; join and aggregate results from these Web services. The maingoal is to deliver relevant and multi-domain answers to user queries. In these scenarios;users usually expect a ranked list of relevant answers in contrast to the full answer set.Hence; ranking query results in the presence of uncertainty is a fundamental queryprocessing challenge in search computing environments. Rank-join is a basic relationaloperator that reports the top-k join results as soon as possible; avoiding the expensivematerialize-then-sort approach. Due to the early-out and pipelined nature of rank-join; it actsas one of the major building blocks in compiling execution plans for multi-domain queries(also knows as liquid queries). In this chapter; we discuss the implication of data …,*,2011,1,13
Trends in rank join,Ihab Ilyas; Davide Martinenghi; Neoklis Polyzotis; Marco Tagliasacchi,Abstract This chapter reports the main findings of a panel that was moderated by DavideMartinenghi in which Ihab Ilyas; Neoklis Polyzotis; and Marco Tagliasacchi shared theirthoughts with the attendees of the Second SeCo Workshop regarding current and futureissues related to what was presented during the session on rank join. The topics touchedupon during the discussion regarded relevance for Search Computing; pertinence ofoptimization; multi-way joins; approximate answers; and uncertainty.,*,2011,1,3
Report on the First International Workshop on Ranking in Databases (DBRank'07),Ihab F Ilyas; Gautam Das,This report summarizes the presentations; keynotes and dis- cussions that took place duringthe first international work- shop on ranking in databases (DBRank'07). The workshop was heldon April 16; 2007; in conjunction with ICDE in Istanbul; Turkey … The International Workshopon Ranking in Databases (DBRank) focuses on the semantics; the modeling and the implementationof ranking and ordering in database systems and applications. In recent years; there has beena great deal of interest in developing effective techniques for ad-hoc search and retrieval in avariety of domains such as relational databases; document and multimedia databases; and scientificinformation systems. In particular; a large number of emerging applications require exploratoryquerying on such databases; examples include users wishing to search databases and catalogsof products such as homes; cars; cameras; restaurants; and pho- tographs. Traditional …,ACM SIGMOD Record,2007,1,13
Skyline and Top-k Processing in Web Bargaining,Mohamed A Soliman; Ihab F Ilyas; Nick Koudas,Abstract Skyline and top-k queries are gaining increasing importance in many emergingapplications. Current skyline and top-k query processing techniques work on deterministicobject attributes and known scores. However; in many practical scenarios these settings areinapplicable. In this paper we focus on web interaction scenarios where each interaction is adata object with a set of possible outcomes (scores). Obtaining exact interaction scores isexpensive as it involves complex arbitration between interacting parties. Moreover; theoutcome of each interaction might redefine other interactions scores. We demonstrate thatthe search space for solving such problems is very large. Based on this we formulate andpresent skyline and top-k processing algorithms that can efficiently reduce the search space.We present the results of a thorough experimental evaluation quantifying the relative …,*,2006,1,21
Data Quality: The Role of Empiricism,Shazia Sadiq; Tamraparni Dasu; Xin Luna Dong; Juliana Freire; Ihab F Ilyas; Sebastian Link; Miller J Miller; Felix Naumann; Xiaofang Zhou; Divesh Srivastava,Abstract We outline a call to action for promoting empiricism in data quality research. Theaction points result from an analysis of the landscape of data quality research. Thelandscape exhibits two dimensions of empiricism in data quality research relating to type ofmetrics and scope of method. Our study indicates the presence of a data continuum rangingfrom real to synthetic data; which has implications for how data quality methods areevaluated. The dimensions of empiricism and their inter-relationships provide a means ofpositioning data quality research; and help expose limitations; gaps and opportunities.,ACM SIGMOD Record,2018,*,10
A Formal Framework For Probabilistic Unclean Databases,Christopher De Sa; Ihab F Ilyas; Benny Kimelfeld; Christopher Re; Theodoros Rekatsinas,Abstract: Traditional modeling of inconsistency in database theory casts all possible"repairs" equally likely. Yet; effective data cleaning needs to incorporate statistical reasoning.For example; yearly salary of\$100 k and age of 22 are more likely than\$100 k and 122 andtwo people with same address are likely to share their last name (ie; a functionaldependency tends to hold but may occasionally be violated). We propose a formalframework for unclean databases; where two types of statistical knowledge are incorporated.The first represents a belief of how intended (clean) data is generated; and the secondrepresents a belief of how the actual database is realized through the introduction of noise.Formally; a Probabilistic Unclean Database (PUD) is a triple that consists of a probabilisticdatabase that we call the" intention"; a probabilistic data transformator that we call the" …,arXiv preprint arXiv:1801.06750,2018,*,3
Private Exploration Primitives for Data Cleaning,Chang Ge; Ihab F Ilyas; Xi He; Ashwin Machanavajjhala,Abstract: Data cleaning is the process of detecting and repairing inaccurate or corruptrecords in the data. Data cleaning is inherently human-driven and state of the art systemsassume cleaning experts can access the data to tune the cleaning process. However; insensitive datasets; like electronic medical records; privacy constraints disallow unfetteredaccess to the data. To address this challenge; we propose an utility-aware differentiallyprivate framework which allows data cleaner to query on the private data for a givencleaning task; while the data owner can track privacy loss over these queries. In this paper;we first identify a set of primitives based on counting queries for general data cleaning tasksand show that even with some errors; these cleaning tasks can be completed withreasonably good quality. We also design a privacy engine which translates the accuracy …,arXiv preprint arXiv:1712.10266,2017,*,7
Entity Consolidation: The Golden Record Problem,Dong Deng; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract: Four key subprocesses in data integration are: data preparation (ie; transformingand cleaning data); schema integration (ie; lining up like attributes); entity resolution (ie;finding clusters of records that represent the same entity) and entity consolidation (ie;merging each cluster into a" golden record" which contains the canonical values for eachattribute). In real scenarios; the output of entity resolution typically contains multiple dataformats and different abbreviations for cell values; in addition to the omnipresent problem ofmissing data. These issues make entity consolidation challenging. In this paper; we studythe entity consolidation problem. Truth discovery systems can be used to solve this problem.They usually employ simplistic heuristics such as majority consensus (MC) or sourceauthority to determine the golden record. However; these techniques are not capable of …,arXiv preprint arXiv:1709.10436,2017,*,22
Methods and devices for customizing knowledge representation systems,*,Techniques for analyzing and synthesizing complex knowledge representations (KRs) mayutilize an atomic knowledge representation model including both an elemental datastructure and knowledge processing rules stored as machine-readable data and/orprogramming instructions. One or more of the knowledge processing rules may be appliedto analyze an input complex KR to deconstruct its complex concepts and/or conceptrelationships to elemental concepts and/or concept relationships to be included in theelemental data structure. One or more of the knowledge processing rules may be applied tosynthesize an output complex KR from the stored elemental data structure in accordancewith context information. Methods of populating an elemental data structure and methods ofsynthesizing complex KRs from the elemental data structure may depend on user models …,*,2017,*,4
Techniques for presenting content to a user based on the user's preferences,*,Techniques for presenting content to users. The techniques include: obtaining user contextinformation including a first keyword; identifying; based on the first keyword; a first attributeand a second attribute among the plurality of attributes; the first attribute being acharacteristic of the first keyword and the second attribute being another characteristic of thefirst keyword; obtaining; based on the user context information; at least one second-orderuser preference among attributes in the plurality of attributes including a preference betweenthe first attribute and the second attribute; identifying a set of content items among theplurality of content items based on the first attribute and the second attribute; determining aranking of content items in the set of content items based on the at least one second-orderuser preference; and presenting content items to the user in accordance with the ranking.,*,2017,*,19
Methods and devices for customizing knowledge representation systems,*,Techniques for customizing knowledge representation systems including identifying; basedon a plurality of concepts in a knowledge representation (KR); a group of one or moreconcepts relevant to user context information; and providing the identified group of one moreconcepts to a user. The KR may include a combination of modules. The modules mayinclude a kernel and a customized module customized for the user. The kernel mayaccessible via a second KR.,*,2017,*,23
A Demo of the Data Civilizer System,Raul Castro Fernandez; Dong Deng; Essam Mansour; Abdulhakim A Qahtan; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract Finding relevant data for a specific task from the numerous data sources availablein any organization is a daunting task. This is not only because of the number of possibledata sources where the data of interest resides; but also due to the data being scattered allover the enterprise and being typically dirty and inconsistent. In practice; data scientists areroutinely reporting that the majority (more than 80%) of their effort is spent finding; cleaning;integrating; and accessing data of interest to a task at hand. We propose to demonstrateDATA CIVILIZER to ease the pain faced in analyzing data" in the wild". DATA CIVILIZER isan end-to-end big data management system with components for data discovery; dataintegration and stitching; data cleaning; and querying data from a large variety of storageengines; running in large enterprises.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*,10
Method and system for large scale data curation,*,An end-to-end data curation system and the various methods used in linking; matching; andcleaning large-scale data sources. The goal of this system is to provide scalable andefficient record deduplication. The system uses a crowd of experts to train the system. Thesystem operator can optionally provide a set of hints to reduce the number of questions sentto the experts. The system solves the problem of schema mapping and record deduplicationin a holistic way by unifying these problems into a unified linkage problem.,*,2017,*,10
Qualitative data cleaning,Xu Chu; Ihab F Ilyas,ABSTRACT Data quality is one of the most important problems in data management; since dirtydata often leads to inaccurate data analytics results and wrong business decisions. Data cleaningexercise often consist of two phases: error detection and error repairing. Error detection techniquescan either be quantitative or qualitative; and error repairing is performed by applying data transformationscripts or by involving human experts; and sometimes both. In this tutorial; we discuss the mainfacets and direc- tions in designing qualitative data cleaning techniques. We present a taxonomyof current qualitative error detection techniques; as well as a taxonomy of current data repair-ing techniques. We will also discuss proposals for tackling the challenges for cleaning “bigdata” in terms of scale and distribution … 1. INTRODUCTION Enterprises have been acquiringlarge amounts of data from a variety of sources to build their own “Data Lakes”; with the …,Proceedings of the VLDB Endowment,2016,*,16
CYBER SECURITY PART 2 AUTO-IMMUNITY,A Elmagarmid; P Cochrane; M Ouzzani; WJ Al Marri; Q Malluhi; M Tang; WG Aref; Z Abedjan; X Chu; D Deng; RC Fernandez; IF Ilyas; P Papotti; M Stonebraker; N Tang; Z Khayyat; W Lucia; J Quiane-Ruiz; T Nan; Y Yu; QM Malluhi; TK Saha; HM Hammady; AK Elmagarmid; H Hammady; Z Fedorowicz; M Yakout; H Elmeleegy; Y Qi,*,US Patent App,2016,*
Dark Data: Are we solving the right problems?,Michael Cafarella; Ihab F Ilyas; Marcel Kornacker; Tim Kraska; Christopher Ré,With the increasing urge of the enterprises to ingest as much data as they can in what'scommonly referred to as “Data Lakes”; the new environment presents serious challenges totraditional ETL models and to building analytic layers on top of well-understood globalschema. With the recent development of multiple technologies to support this “load-first”paradigm; even traditional enterprises have fairly large HDFS-based data lakes now. Theyhave even had them long enough that their first generation IT projects delivered on some;but not all; of the promise of integrating their enterprise's data assets. In short; we movedfrom no data to Dark data. Dark data is what enterprises might have in their possession;without the ability to access it or with limited awareness of what this data represents. Inparticular; business-critical information might still remain out of reach. This panel is about …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*,22
Data curation system with version control for workflow states and provenance,*,A data curation system that includes various methods to enable efficient reuse of human andmachine effort. To reuse effort; various facilities are presented that model; save; and allowthe querying of provenance and state information of a curation workflow and allow forincremental; stateful transitions of the data and the metadata.,*,2016,*,10
Probabilistic Web Data Management,Lei Chen; Ihab Ilyas; Christopher Re; Xiaofang Zhou,With the development of Web 2.0 technology; enormous data are generated every day.Among these data; there exist quite a lot uncertainty due to careless data entry; incompleteinformation; and inconsistency among different data description. Although significant efforthas been paid to find effective and efficient solutions for managing and mining generaluncertain data; little attention is paid to manage uncertain data on the Web. This specialissue is proposed to attract research attempts on handling uncertainty of the Web data. Thisspecial issue has attracted 12 submissions; after two rounds of very careful reviews bydomain experts; we accepted three excellent papers. These three papers present new ideasto address issues on Probabilistic Web Data Management. The first paper;“an efficientapproach to suggesting topically related Web queries using hidden topic model” …,World Wide Web,2013,*,4
Universal Top-k Keyword Search over Relational Databases,Ning Zhang; Ihab F Ilyas; M Tamer Ozsu,Keyword search is one of the most effective paradigms for information discovery. One of the keyadvantages of keyword search querying is its simplicity. There is an increasing need for allowingordinary users to issue keyword queries without any knowledge of the database schema. Theretrieval unit of keyword search queries over relational databases is different than in IRsystems. While the retrieval unit in those IR systems is a document; in our case; the result is asynthesized document formed by joining a number of tuples. We measure result quality usingtwo metrics: structural quality and content quality. The content quality of a JTT is an IR-style scorethat indicates how well the information nodes match the keywords; while the structural qualityof JTT is a score that evaluates the meaningfulness/semantics of connecting informationnodes; for example; the closeness of the corresponding relationship. We design a hybrid …,*,2011,*,3
Guided data repair,Ahmed Elmagarmid; Jennifer Neville; Mourad Ouzzani; Ihab Ilyas; Mohamed Yakout,Abstract In this paper we present GDR; a Guided Data Repair framework that incorporatesuser feedback in the cleaning process to enhance and accelerate existing automatic repairtechniques while minimizing user involvement. GDR consults the user on the updates thatare most likely to be beneficial in improving data quality. GDR also uses machine learningmethods to identify and apply the correct updates directly to the database without the actualinvolvement of the user on these specific updates. To rank potential updates for consultationby the user; we first group these repairs and quantify the utility of each group using thedecision-theory concept of value of information (VOI). We then apply active learning to orderupdates within a group based on their ability to improve the learned model. User feedback isused to repair the database and to adaptively refine the training set for the model. We …,Proceedings of the VLDB Endowment,2011,*,10
Working Group: Classification; Representation and Modeling.,S Das; C Koch; B König-Ries; Ander de Keijzer; V Markl; A Deshpande; M van Keulen; PJ Haas; IF Ilyas; T Neumann; D Olteanu; M Theobald; V Vassalos,KNAW Narcis. Back to search results. Publication Working Group: Classification;Representation and Modeling. (2009). Pagina-navigatie: Main …,*,2009,*,10
Guest editorial: special issue on ranking in databases,Ihab Ilyas,In recent years; there has been a great deal of interest in developing effective techniques forad-hoc search and retrieval in databases systems. In particular; a large number of emergingapplications require exploratory querying on general-purpose or domain-specific databases;examples include users wishing to search bibliographic databases or catalogs of productssuch as homes; cars; cameras; restaurants; and photographs. Current database querylanguages such as SQL follow the Boolean retrieval model; ie; tuples or elements thatexactly satisfy the selection conditions laid out in the query are returned. While extremelyuseful for the expert user; this retrieval model is inadequate for ad-hoc retrieval byexploratory users who cannot articulate the perfect query for their needs; either their queriesare very specific; resulting in no (or too few) answers; or are very broad; resulting in too …,Distributed and Parallel Databases,2009,*,18
08421 Working Group: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab F Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,Abstract The following summary tries to capture a collection of state-of-the-art techniquesand challenges for future work on lineage management in uncertain and probabilisticdatabases that we discussed in our working group. It was one half of a larger committee thatwe had initially formed; which then got split into two groups---one focusing on lineage as ameans of explanation of data; and one focusing more on lineage usage in probabilisticdatabases (see also the" Explanation" working group report for more details on the firstsubgroup).,Dagstuhl Seminar Proceedings,2009,*,12
Message from the DBRANK’08 program co-chairs,Vagelis Hristidis; Ihab F Ilyas,The Second International Workshop on Ranking in Databases (DBRank'08) was held inconjunction with the IEEE 24th International Conference on Data Engineering (ICDE 2008)in Cancun; Mexico on April 12; 2008. The workshop brought together researchers andpractitioners with the goal of discussing the semantics; the modeling and the implementationof ranking and ordering in database systems and applications. The workshop provided themwith a unique opportunity to share their experience in supporting ranking in variousdatabase systems; from relational to semi-structures and unstructured data; and on differentlevels from query formulation and preference modeling to query processing and optimizationframeworks. A distinguished program committee consisting of 20 program committeemembers put the technical program together. We received 25 submissions; and each …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,*,22
Working Group Report: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab F Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,The following summary tries to capture a collection of state-of-the-art techniques andchallenges for future work on lineage management in uncertain and probabilistic databasesthat we discussed in our working group. It was one half of a larger committee that we hadinitially formed; which then got split into two groups---one focusing on lineage as a means ofexplanation of data; and one focusing more on lineage usage in probabilistic databases(see also the" Explanation" working group report for more details on the first subgroup).,*,2008,*,4
Tom Crecelius 1480 Carlo A. Curino 761; 882 Emiran Curtmola 1408; 1448 D Harish D. 1124; 1325,Florian Daniel; David DeWitt; Amol Deshpande; AnHai Doan; Marcus Fontoura; Juliana Freire; Venkatesh Ganti; Hong Gao; Hector Garcia-Molina; Minos Garofalakis; Charles Garrod; Tingjian Ge; Lise Getoor; Phillip Gibbons; Lukasz Golab; Wojciech Golab; Yihong Gong; Albert Greenberg; Maxim Grinev; Peter Haas; Wook-Shin Han; Michael Hay; Monika Henzinger; Mauricio Hernandez; Mark Hill; Howard Ho; Allison Holloway; Mingsheng Hong; Chien-Yi Hou; Yanli Hu; Kien Hua; Jiansheng Huang; Ihab Francis Ilyas; Zachary G Ives; Marie Jacob; HV Jagadish; Magesh Jayapandian; David Jensen; Haifeng Jiang; Cheqing Jin; Ryan Johnson; Theodore Johnson; Vanja Josifovski,*,*,2008,*
08421 Working Group: Classification; Representation and Modeling.,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,*,Uncertainty Management in Information Systems,2008,*
Ph. D. Workshop In conjunction with the 22 nd IEEE International Conference on Data Engineering April 3; 2006 http://ir. iit. edu/icde06phd,Brian Cooper; Christopher Jermaine; Ying Liu; Rachel Pottinger; Arvind Arasu; Shivnath Babu; Ihab Ilyas; Stratis D Viglas,It is our great pleasure to present the Ph. D. Workshop of the 22 nd IEEE InternationalConference on Data Engineering. The goal of the Workshop is to introduce Ph. D. studentsto the research community and to foster the exchange of ideas in a constructive andcongenial atmosphere. The Workshop program includes paper presentations and paneldiscussions. We received a total of 28 papers on current and emerging topics being pursuedby Ph. D. researchers from over a dozen countries around the world. After a careful reviewby a program committee primarily composed of junior faculty; we selected the 13 best forpresentation.,*,2006,*,10
Reminiscences on influential papers,Kenneth A Ross,This paper abstracts from the particular features of PRISMA/DB; and evaluates and analyzesthe performance trade-offs for a wide range of parallel query processing strategies. Its clearstyle of presentation; along with careful attention to previous work both in its discussion aswell as in the experiments and analysis; make this paper into a concise introductory or“refereshment” text for researchers interested in parallel query execution.,ACM SIGMOD Record,2004,*,16
Query Algebra and Optimization for Relational Top-k Queries,C Lu; KCC Chang; IF Ilyas; S Song,*,*,2004,*
A Framework for Supporting the Class of Space Partitioning,Walid G Aref; Ihab F Ilyas,Abstract Emerging database applications require the use of new indexing structures beyondB-trees and R-trees. Examples are the kD tree; the trie; the quadtree; and their variants. Theyare often proposed as supporting structures in data mining; GIS; and CAD/CAM applications.A common feature of all these indexes is that they recursively divide the space intopartitions. A new extensible index structure; termed SF-GiST is presented that supports thisclass of data structures; mainly the class of space partitioning unbalanced trees. Simplemethod implementations are provided that demonstrate how SP-GiST can behave as a kDtree; a tria; a quadtree; or any of their variants. Issues related to clustering tree nodes intopages as well as concurrency control for SP-GiST are addressed. A dynamic minimum-height clustering technique is applied to minimize disk accesses and to make using such …,*,2001,*,4
Discovering Data Transformations in Web Resources,Ziawasch Abedjan; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker,Abstract. In data integration; data curation; and other data analysis tasks; users spend aconsiderable amount of time converting data from one representation to another. Forexample US dates to European dates or airport codes to city names. In practice; datascientists have to code most of the transformation tasks manually; search for the appropriatedictionaries; and involve domain experts. In a previous vision paper; we presented the initialdesign of DataXFormer; a system that uses web resources to assist in transformationdiscovery [1]. Specifically; DataXFormer discovers possible transformations from web tablesand web forms and involves human feedback where appropriate. We demonstrated thesystem at SIGMOD 2015 and deployed an open version of the system; which helped us toincrease our initial workload from 50 to 120 transformations [3]. At the same time we …,*,*,*,22
2015 IEEE 31st International Conference on Data Engineering (ICDE),Rakesh Agrawal,Asia is in the midst of a historic transformation. Asia's per capita income is projected to risesixfold and its share of global gross domestic product is expected to increase to 52 percentby 2050 [1]. Science and technology has been cited as one of the key pillars for the successof Asia's development [2].,*,*,*,10
Getting Data Right,Jerry Held; Michael Stonebraker; Thomas H Davenport; Ihab Ilyas; Michael L Brodie; Andy Palmer; James Markarian,Companies have invested an estimated $3–4 trillion in IT over the last 20-plus years; most ofit directed at developing and deploying single-vendor applications to automate and optimizekey business processes. And what has been the result of all of this disparate activ‐ity? Datasilos; schema proliferation; and radical data heterogeneity. With companies now investingheavily in big data analytics; this entropy is making the job considerably more complex. Thiscom‐plexity is best seen when companies attempt to ask “simple” ques‐tions of data that isspread across many business silos (divisions; geographies; or functions). Questions assimple as “Are we getting the best price for everything we buy?” often go unansweredbecause on their own; top-down; deterministic data unification approaches aren't preparedto scale to the variety of hundreds; thousands; or tens of thousands of data silos.,*,*,*,23
USA USA {ilyas; aref}@ cs. purdue. edu ahmed elmagarmid@ hp. com,Ihab F Ilyas,Abstract Joining ranked inputs is an essential requirement for many database applications;such as ranking search results from multiple search engines and answering multi-featurequeries for multimedia retrieval systems. We introduce a new practical binary pipelinedquery operator; termed NRA-RJ; that produces a global rank from two input ranked streamsbased on a score function. The output of NRARJ can serve as a valid input to other NRARJoperators in the query pipeline. Hence; the NRA-RJ operator can support a hierarchy of joinoperations and can be easily integrated in query processing engines of commercialdatabase systems. The NRA-RJ operator bridges Fagin's optimal aggregation algorithm intoa practical implementation and contains several optimizations that address performanceissues. We compare the performance of NRA-RJ against recent rank join algorithms …,*,*,*,22
InterJoin: Exploiting Materialized Views in XML Query Processing,Derek Phillips; Ning Zhang; Ihab F Ilyas; M Tamer Özsu,Abstract. Efficient processing of XPath expressions is an integral part of XML data queryprocessing. Exploiting materialized views in query processing can significantly enhancequery processing performance. We propose a novel view definition that allows forintermediate (structural) join results to be stored and reused in XML query evaluation. Unlikecurrent XML view proposals; our views do not require navigation in the original document orpath-based pattern matching. Hence; they can be evaluated significantly faster and can bemore easily costed as part of a query plan. In general; current structural joins cannot exploitviews efficiently when the view definition is not a prefix (or a suffix) of the XPath query. Toincrease the applicability of our proposed view definition; we propose a novel physicalstructural join operator called InterJoin. The InterJoin operator allows for joining …,*,*,*,4
Xian Xu Guizhen Yang Moustafa A. Youssef,Salman Akram; James Caverlee; Franois Charoy; Philippe Cudre-Mauroux; Adriano Di Pasquale; Alpay Erturkmen; Sarunas Girdzijauskas; Claude Godart; Jiang Haiying; Moustafa Hammad; Weiping He; Omer Horvitz; Lieming Huang; Ihab Ilyas; George-Dimitrios Kapos; Gokce Laleci; Bendick Mahleko; Zaki Malik; Brahim Medjahed; Iwaihara Mizuho; Mohamed Mokbel; Pascal Molli; Christine O’Keefe; Mourad Ouzzani; Ovgu Ozturk; Olivier Perrin; Thomais Pilioura; Lakshmish Ramaswamy; Abdelmounaam Rezgui; Chatvichienchai Somchai; Murakami Takaharu; Li Xiong,A not-for-profit organization; IEEE is the world's largest technical professional organization dedicatedto advancing technology for the benefit of humanity. © Copyright 2017 IEEE - All rightsreserved. Use of this web site signifies your agreement to the terms and conditions.,*,*,*,10
AN AGENT-BASED VIEW MAINTENANCE SCHEME FOR MOBILE ENVIRONMENTS,IHAB F ILYAS; NAGWA M EL-MAKKY; AHMED ELNAHAS,*,*,*,*
Working Group: Lineage/Provenance,Anish Das Sarma; Amol Deshpande; Thomas Hubauer; Ihab Ilyas; Birgitta König-Ries; Matthias Renz; Martin Theobald,Abstract. The following summary tries to capture a collection of state-of-the-art techniquesand challenges for future work on lineage management in uncertain and probabilisticdatabases that we discussed in our working group. It was one half of a larger committee thatwe had initially formed; which then got split into two groups—one focusing on lineage as ameans of explanation of data; and one focusing more on lineage usage in probabilisticdatabases (see also the “Explanation” working group report for more details on the firstsubgroup).,*,*,*,22
Workshop Chairs,Shin'ichi Satoh; Edward Chang; Yasuo Ariki; Masayoshi Aritsugi; Noboru Babaguchi; Nozha Boujemma; France Arbee Chen; Alberto Del Bimbo; Ajay Divakaran; Jianping Fan; Thomas Huang; Ihab F Ilyas; Alex Jaimes; Björn Þór Jónsson; Mohan Kankanhalli; Norio Katayama; Michael Lew; Rainer Lienhart; Michael Lyu; Yuichi Nakamura; Vincent Oria; NTT Yasushi Sakurai; Japan Nicu Sebe; Cyrus Shahabi; Qi Tian,Page 1. ii Workshop Chairs Shin'ichi Satoh; National Institute of Informatics; Japan Edward Chang;University of California; Santa Barbara; USA Program Committee Laurent Amsaleg; IRISA-CNRS;France Yasuo Ariki; Kobe University; Japan Masayoshi Aritsugi; Gunma University; Japan NoboruBabaguchi; Osaka University; Japan Nozha Boujemma; INRIA Rocquencourt; France Arbee Chen;Tsinghua University; Taiwan Alberto Del Bimbo; University of Florence; Italy Ajay Divakaran;Mitsubishi Electric Research Laboratories; USA Jianping Fan; The University of North Carolinaat Charlotte; USA Thomas Huang; University of Illinois at Urbana-Champaign; USA Ihab F. Ilyas;University of Waterloo; Canada Alex Jaimes; FujiXerox; Japan Björn Þór Jónsson; ReykjavíkUniversity; Iceland Mohan Kankanhalli; National University of Singapore; Singapore NorioKatayama; National Institute of Informatics ; Japan …,*,*,*,10
Program Committee Chair,Nick Koudas; Moustafa Hammad; Denilson Barbosa; Ashraf Aboulnaga; Periklis Andritsos; Peter Buneman; Gautam Das; Amr El Abbadi; Christos Faloutsos; Jarek Gryz; Dimitrios Gunopoulos; Ihab Ilyas; Chris Jermaine; Bettina Kemme; George Kollios; Manolis Koubarakis; Laks Lakshmanan; Silvia Nittel; Dimitris Papadias; Sunil Prabhakar; Ken Ross; Joerg Sander; Kyuseok Shim; Yufei Tao; Dimitri Theodoratos; Yannis Theodoridis; Anthony Tung; A Shoshani; W Grossmann,General Chair: Ken Barker University of Calgary … Program Committee Chair: Nick KoudasUniversity of Toronto … Proceedings/Local Organization Chairs: Moustafa Hammad Universityof Calgary Denilson Barbosa University of Calgary … Program Committee: Ashraf AboulnagaUniversity of Waterloo Periklis Andritsos University of Trento Peter Buneman University of EdinburghTiziana Catarci University of Roma Gautam Das University of Texas at Arlington Amr El AbbadiUC Santa Barbara Christos Faloutsos Carnegie Mellon University Jarek Gryz York UniversityDimitrios Gunopoulos UC Riverside Marios Hadjieleftheriou AT&T Research Ihab Ilyas Universityof Waterloo Chris Jermaine University of Florida Bettina Kemme McGill University George KolliosBoston University Manolis Koubarakis University of Athens Laks Lakshmanan University of BritishColumbia Silvia Nittel University of Maine Dimitris Papadias Hong Kong University Sunil …,*,*,*,4
Program Co-Chairs,Jing Peng; Xiuwen Liu; Marian Bartlett; Stefano Cagnoni; Tom Dietterich; Carlotta Domeniconi; Bruce Draper; Riad Hammoud; Delphi Doug Heisterkamp; Katsu Ikeuchi; Anil Jain; Chris Krawiec; Peter Meer; Ram Nevatia; Lucas Paletta; Gregory Power; Air Force; Mateen Rizki; Paul Sajda,Page 1. Organizing Committee General Chairs TJ Tarn Toshio E;ukuda Washington University;USA Nagoya University; Japan Program Chair Kimon Valavanis University of South Florida; USAProgram Co-chairs Bruno Siciliano; University of Naples Federico 11; Italy Nick Papanikolopoulos;University of Minnesota; USA Makoto Kaneko; Hiroshima University; Japan Awards CommitteeChair Bill Hamel; University of (Tennessee; USA Workshop & Tutorial Chair Robin Murphy;University of South Florida; USA Video Proceedings Chair Ken Goldberg; UC Berkeley; USAPublications Chair Max Meng; Chinese University of Hong Kong; China Exhibits Chair BradNelson; ETH Zurich; Switzerland Publicity Chairs Maja Matijasevic; University of Zagreb; CroatiaNing Xi; Michigan State University; USA Finance Chair Xiaoping Yun; NE; USA …,*,*,*,3
PSALM: Accurate Sampling for Cardinality Estimation in a Multi-user Environment,Huaxin Zhang; Ihab F Ilyas; Kenneth Salem,Abstract In database systems that support fine-grained access controls; each user hasaccess rights that determine which tuples are accessible and which are inaccessible.Queries are answered as if the inaccessible tuples are not present in the database. Thus;users with different access rights may get different answers to a given query. To processqueries efficiently in the presence of fine-grained access controls; the database systemneeds accurate estimates of the number of tuples that are both accessible according to theaccess rights of the submitting user and relevant according to the selection predicates in thequery. In this paper we present sampling-based cardinality estimation techniques for use inthe presence of fine-grained access controls. These techniques exploit the fact that accessrights are relatively static and are common to all queries that are evaluated on behalf of a …,*,*,*,10
