Aurora: a new model and architecture for data stream management,Daniel J Abadi; Don Carney; Ugur Çetintemel; Mitch Cherniack; Christian Convey; Sangdon Lee; Michael Stonebraker; Nesime Tatbul; Stan Zdonik,Abstract. This paper describes the basic processing model and architecture of Aurora; a newsystem to manage data streams for monitoring applications. Monitoring applications differsubstantially from conventional business data processing. The fact that a software systemmust process and react to continual inputs from many sources (eg; sensors) rather than fromhuman operators requires one to rethink the fundamental architecture of a DBMS for thisapplication area. In this paper; we present Aurora; a new DBMS currently under constructionat Brandeis University; Brown University; and MIT We first provide an overview of the basicAurora model and architecture and then describe in detail a stream-oriented set ofoperators.,The VLDB Journal,2003,1719,10
The Design of the Borealis Stream Processing Engine.,Daniel J Abadi; Yanif Ahmad; Magdalena Balazinska; Ugur Cetintemel; Mitch Cherniack; Jeong-Hyon Hwang; Wolfgang Lindner; Anurag Maskey; Alex Rasin; Esther Ryvkina; Nesime Tatbul; Ying Xing; Stanley B Zdonik,Abstract Borealis is a second-generation distributed stream processing engine that is beingdeveloped at Brandeis University; Brown University; and MIT. Borealis inherits core streamprocessing functionality from Aurora [14] and distribution functionality from Medusa [51].Borealis modifies and extends both systems in non-trivial and critical ways to provideadvanced capabilities that are commonly required by newly-emerging stream processingapplications. In this paper; we outline the basic design and functionality of Borealis. Throughsample real-world applications; we motivate the need for dynamically revising query resultsand modifying query specifications. We then describe how Borealis addresses thesechallenges through an innovative set of features; including revision records; time travel; andcontrol lines. Finally; we present a highly flexible and scalable QoS-based optimization …,Cidr,2005,1512,15
C-store: a column-oriented DBMS,Mike Stonebraker; Daniel J Abadi; Adam Batkin; Xuedong Chen; Mitch Cherniack; Miguel Ferreira; Edmond Lau; Amerson Lin; Sam Madden; Elizabeth O'Neil; Pat O'Neil; Alex Rasin; Nga Tran; Stan Zdonik,Abstract This paper presents the design of a read-optimized relational DBMS that contrastssharply with most current systems; which are write-optimized. Among the many differencesin its design are: storage of data by column rather than by row; careful coding and packing ofobjects into storage including main memory during query processing; storing an overlappingcollection of column-oriented projections; rather than the current fare of tables and indexes;a non-traditional implementation of transactions which includes high availability andsnapshot isolation for read-only transactions; and the extensive use of bitmap indexes tocomplement B-tree structures. We present preliminary performance data on a subset of TPC-H and show that the system we are building; C-Store; is substantially faster than popularcommercial products. Hence; the architecture looks very encouraging.,Proceedings of the 31st international conference on Very large data bases,2005,1248,4
A comparison of approaches to large-scale data analysis,Andrew Pavlo; Erik Paulson; Alexander Rasin; Daniel J Abadi; David J DeWitt; Samuel Madden; Michael Stonebraker,Abstract There is currently considerable enthusiasm around the MapReduce (MR) paradigmfor large-scale data analysis [17]. Although the basic control flow of this framework hasexisted in parallel SQL database management systems (DBMS) for over 20 years; somehave called MR a dramatically new computing model [8; 17]. In this paper; we describe andcompare both paradigms. Furthermore; we evaluate both kinds of systems in terms ofperformance and development complexity. To this end; we define a benchmark consisting ofa collection of tasks that we have run on an open source version of MR as well as on twoparallel DBMSs. For each task; we measure each system's performance for various degreesof parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs.Although the process to load data into and tune the execution of parallel DBMSs took …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,1209,4
HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads,Azza Abouzeid; Kamil Bajda-Pawlikowski; Daniel Abadi; Avi Silberschatz; Alexander Rasin,Abstract The production environment for analytical data management applications is rapidlychanging. Many enterprises are shifting away from deploying their analytical databases onhigh-end proprietary machines; and moving towards cheaper; lower-end; commodityhardware; typically arranged in a shared-nothing MPP architecture; often in a virtualizedenvironment inside public or private" clouds". At the same time; the amount of data thatneeds to be analyzed is exploding; requiring hundreds to thousands of machines to work inparallel to perform the analysis. There tend to be two schools of thought regarding whattechnology to use for data analysis in such an environment. Proponents of paralleldatabases argue that the strong emphasis on performance and efficiency of paralleldatabases makes them well-suited to perform such analysis. On the other hand; others …,Proceedings of the VLDB Endowment,2009,1004,7
Scalable semantic web data management using vertical partitioning,Daniel J Abadi; Adam Marcus; Samuel R Madden; Kate Hollenbach,Abstract Efficient management of RDF data is an important factor in realizing the SemanticWeb vision. Performance and scalability issues are becoming increasingly pressing asSemantic Web technology is applied to real-world applications. In this paper; we examinethe reasons why current data management solutions for RDF data scale poorly; and explorethe fundamental scalability limitations of these approaches. We review the state of the art forimproving performance for RDF databases and consider a recent suggestion;" propertytables." We then discuss practically and empirically why this solution has undesirablefeatures. As an improvement; we propose an alternative solution: vertically partitioning theRDF data. We compare the performance of vertical partitioning with prior art on queriesgenerated by a Web-based RDF browser over a large-scale (more than 50 million triples) …,Proceedings of the 33rd international conference on Very large data bases,2007,741,14
The end of an architectural era:(it's time for a complete rewrite),Michael Stonebraker; Samuel Madden; Daniel J Abadi; Stavros Harizopoulos; Nabil Hachem; Pat Helland,Abstract In previous papers [SC05; SBC+ 07]; some of us predicted the end of" one size fitsall" as a commercial relational DBMS paradigm. These papers presented reasons andexperimental evidence that showed that the major RDBMS vendors can be outperformed by1--2 orders of magnitude by specialized engines in the data warehouse; stream processing;text; and scientific database markets. Assuming that specialized engines dominate thesemarkets over time; the current relational DBMS code lines will be left with the business dataprocessing (OLTP) market and hybrid markets where more than one kind of capability isrequired. In this paper we show that current RDBMSs can be beaten by nearly two orders ofmagnitude in the OLTP market as well. The experimental evidence comes from comparing anew OLTP prototype; H-Store; which we have built at MIT to a popular RDBMS on the …,Proceedings of the 33rd international conference on Very large data bases,2007,687,1
Data management in the cloud: Limitations and opportunities.,Daniel J Abadi,Abstract Recently the cloud computing paradigm has been receiving significant excitementand attention in the media and blogosphere. To some; cloud computing seems to be littlemore than a marketing umbrella; encompassing topics such as distributed computing; gridcomputing; utility computing; and softwareas-a-service; that have already receivedsignificant research focus and commercial implementation. Nonetheless; there exist anincreasing number of large companies that are offering cloud computing infrastructureproducts and services that do not entirely resemble the visions of these individualcomponent topics. In this article we discuss the limitations and opportunities of deployingdata management issues on these emerging cloud computing platforms (eg; Amazon WebServices). We speculate that large scale data analysis tasks; decision support systems …,IEEE Data Eng. Bull.,2009,642,20
Column-stores vs. row-stores: how different are they really?,Daniel J Abadi; Samuel R Madden; Nabil Hachem,Abstract There has been a significant amount of excitement and recent work on column-oriented database systems (" column-stores"). These database systems have been shown toperform more than an order of magnitude better than traditional row-oriented databasesystems (" row-stores") on analytical workloads such as those found in data warehouses;decision support; and business intelligence applications. The elevator pitch behind thisperformance difference is straightforward: column-stores are more I/O efficient for read-onlyqueries since they only have to read from disk (or from memory) those attributes accessedby a query. This simplistic view leads to the assumption that one can obtain the performancebenefits of a column-store using a row-store: either by vertically partitioning the schema; orby indexing every column so that columns can be accessed independently. In this paper …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,580,7
Integrating compression and execution in column-oriented database systems,Daniel Abadi; Samuel Madden; Miguel Ferreira,Abstract Column-oriented database system architectures invite a re-evaluation of how andwhen data in databases is compressed. Storing data in a column-oriented fashion greatlyincreases the similarity of adjacent records on disk and thus opportunities for compression.The ability to compress many adjacent tuples at once lowers the per-tuple cost ofcompression; both in terms of CPU and space overheads. In this paper; we discuss how weextended C-Store (a column-oriented DBMS) with a compression sub-system. We show howcompression schemes not traditionally used in row-oriented DBMSs can be applied tocolumn-oriented systems. We then evaluate a set of compression schemes and show thatthe best scheme depends not only on the properties of the data but also on the nature of thequery workload.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,567,7
MapReduce and parallel DBMSs: friends or foes?,Michael Stonebraker; Daniel Abadi; David J DeWitt; Sam Madden; Erik Paulson; Andrew Pavlo; Alexander Rasin,The technology press has been focusing on the revolution of "cloud computing;" a paradigmthat entails the harnessing of large numbers of processors working in parallel to solve computingproblems. In effect; this suggests constructing a data center by lining up a large number oflow-end servers; rather than deploying a smaller set of high-end servers. Along with this interestin clusters has come a proliferation of tools for programming them. MR is one such tool; an attractiveoption to many because it provides a simple model through which users are able to expressrelatively sophisticated distributed programs … Given the interest in the MR model both commerciallyand academically; it is natural to ask whether MR systems should replace parallel databasesystems. Parallel DBMSs were first available commercially nearly two decades ago; and;today; systems (from about a dozen vendors) are available. As robust; high-performance …,Communications of the ACM,2010,548,7
H-store: a high-performance; distributed main memory transaction processing system,Robert Kallman; Hideaki Kimura; Jonathan Natkins; Andrew Pavlo; Alexander Rasin; Stanley Zdonik; Evan PC Jones; Samuel Madden; Michael Stonebraker; Yang Zhang; John Hugg; Daniel J Abadi,Abstract Our previous work has shown that architectural and application shifts have resultedin modern OLTP databases increasingly falling short of optimal performance [10]. Inparticular; the availability of multiple-cores; the abundance of main memory; the lack of userstalls; and the dominant use of stored procedures are factors that portend a clean-slateredesign of RDBMSs. This previous work showed that such a redesign has the potential tooutperform legacy OLTP databases by a significant factor. These results; however; wereobtained using a bare-bones prototype that was developed just to demonstrate the potentialof such a system. We have since set out to design a more complete execution platform; andto implement some of the ideas presented in the original paper. Our demonstrationpresented here provides insight on the development of a distributed main memory OLTP …,Proceedings of the VLDB Endowment,2008,439,7
Scalable SPARQL querying of large RDF graphs,Jiewen Huang; Daniel J Abadi; Kun Ren,ABSTRACT The generation of RDF data has accelerated to the point where many data setsneed to be partitioned across multiple machines in order to achieve reasonableperformance when querying the data. Although tremendous progress has been made in theSemantic Web community for achieving high performance data management on a singlenode; current solutions that allow the data to be partitioned across multiple machines arehighly inefficient. In this paper; we introduce a scalable RDF data management system thatis up to three orders of magnitude more efficient than popular multi-node RDF datamanagement systems. In so doing; we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner thathelps accelerate query processing through locality optimizations and (3) decomposing …,Proceedings of the VLDB Endowment,2011,385,2
Reed: Robust; efficient filtering and event detection in sensor networks,Daniel J Abadi; Samuel Madden; Wolfgang Lindner,Abstract This paper presents a set of algorithms for efficiently evaluating join queries overstatic data tables in sensor networks. We describe and evaluate three algorithms that takeadvantage of distributed join techniques. Our algorithms are capable of running in limitedamounts of RAM; can distribute the storage burden over groups of nodes; and are tolerant todropped packets and node failures. REED is thus suitable for a wide range of event-detection applications that traditional sensor network database and data collection systemscannot be used to implement.,Proceedings of the 31st international conference on Very large data bases,2005,287,15
OLTP through the looking glass; and what we found there,Stavros Harizopoulos; Daniel J Abadi; Samuel Madden; Michael Stonebraker,Abstract Online Transaction Processing (OLTP) databases include a suite of features-disk-resident B-trees and heap files; locking-based concurrency control; support for multi-threading-that were optimized for computer technology of the late 1970's. Advances inmodern processors; memories; and networks mean that today's computers are vastlydifferent from those of 30 years ago; such that many OLTP databases will now fit in mainmemory; and most OLTP transactions can be processed in milliseconds or less. Yetdatabase architecture has changed little. Based on this observation; we look at someinteresting variants of conventional database systems that one might build that exploit recenthardware trends; and speculate on their performance through a detailed instruction-levelbreakdown of the major components involved in a transaction processing database …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,282,1
Consistency tradeoffs in modern distributed database system design: CAP is only part of the story,Daniel Abadi,The CAP theorem's impact on modern distributed database system design is more limitedthan is often perceived. Another tradeoff-between consistency and latency-has had a moredirect influence on several well-known DDBSs. A proposed new formulation; PACELC;unifies this tradeoff with CAP.,Computer,2012,279,20
SW-Store: a vertically partitioned DBMS for Semantic Web data management,Daniel J Abadi; Adam Marcus; Samuel R Madden; Kate Hollenbach,Abstract Efficient management of RDF data is an important prerequisite for realizing theSemantic Web vision. Performance and scalability issues are becoming increasinglypressing as Semantic Web technology is applied to real-world applications. In this paper; weexamine the reasons why current data management solutions for RDF data scale poorly;and explore the fundamental scalability limitations of these approaches. We review the stateof the art for improving performance of RDF databases and consider a recentsuggestion;“property tables”. We then discuss practically and empirically why this solutionhas undesirable features. As an improvement; we propose an alternative solution: verticallypartitioning the RDF data. We compare the performance of vertical partitioning with prior arton queries generated by a Web-based RDF browser over a large-scale (more than 50 …,The VLDB Journal,2009,279,15
Calvin: fast distributed transactions for partitioned database systems,Alexander Thomson; Thaddeus Diamond; Shu-Chun Weng; Kun Ren; Philip Shao; Daniel J Abadi,Abstract Many distributed storage systems achieve high data access throughput viapartitioning and replication; each system with its own advantages and tradeoffs. In order toachieve high scalability; however; today's systems generally reduce transactional support;disallowing single transactions from spanning multiple partitions. Calvin is a practicaltransaction scheduling and data replication layer that uses a deterministic orderingguarantee to significantly reduce the normally prohibitive contention costs associated withdistributed transactions. Unlike previous deterministic database system prototypes; Calvinsupports disk-based storage; scales near-linearly on a cluster of commodity machines; andhas no single point of failure. By replicating transaction inputs rather than effects; Calvin isalso able to support multiple consistency levels---including Paxos-based strong …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,271,4
Materialization strategies in a column-oriented DBMS,Daniel J Abadi; Daniel S Myers; David J DeWitt; Samuel R Madden,There has been renewed interest in column-oriented database architectures in recent years.For read-mostly query workloads such as those found in data warehouse and decisionsupport applications;" column-stores" have been shown to perform particularly well relativeto" row-stores" In order for column-stores to be readily adopted as a replacement for row-stores; however; they must present the same interface to client applications as do row stores;which implies that they must output row-store-style tuples. Thus; the input columns stored ondisk must be converted to rows at some point in the query plan; but the optimal point atwhich to do the conversion is not obvious. This problem can be considered as the oppositeof the projection problem in row-store systems: while row-stores need to determine where inquery plans to place projection operators to make tuples narrower; column-stores need to …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,259,7
Column-oriented database systems,Daniel J Abadi; Peter A Boncz; Stavros Harizopoulos,Abstract Column-oriented database systems (column-stores) have attracted a lot of attentionin the past few years. Column-stores; in a nutshell; store each database table columnseparately; with attribute values belonging to the same column stored contiguously;compressed; and densely packed; as opposed to traditional database systems that storeentire records (rows) one after the other. Reading a subset of a table's columns becomesfaster; at the potential expense of excessive disk-head seeking from column to column forscattered reads or updates. After several dozens of research papers and at least a dozen ofnew column-store start-ups; several questions remain. Are these a new breed of systems orsimply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data …,Proceedings of the VLDB Endowment,2009,241,22
Aurora: a data stream management system,Daniel Abadi; Donald Carney; Ugur Cetintemel; Mitch Cherniack; Christian Convey; C Erwin; Eduardo Galvez; M Hatoun; Anurag Maskey; Alex Rasin; A Singer; Michael Stonebraker; Nesime Tatbul; Ying Xing; Rongguo Yan; S Zdonik,Streams are continuous data feeds generated by such sources as sensors; satellites; andstock feeds. Monitoring applications track data from numerous streams; filtering them forsigns of abnormal activity; and processing them for purposes of filtering; aggregation;reduction; and correlation. Aurora [1; 2; 3] is a general-purpose data stream manager that isbeing designed and implemented (at Brandeis University; Brown University; and MIT) toefficiently support a variety of real-time monitoring applications.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,222,7
Performance tradeoffs in read-optimized databases,Stavros Harizopoulos; Velen Liang; Daniel J Abadi; Samuel Madden,Abstract Database systems have traditionally optimized performance for write-intensiveworkloads. Recently; there has been renewed interest in architectures that optimize readperformance by using column-oriented data representation and light-weight compression.This previous work has shown that under certain broad classes of workloads; column-basedsystems can outperform row-based systems. Previous work; however; has not characterizedthe precise conditions under which a particular query workload can be expected to performbetter on a column-oriented database. In this paper we first identify the distinctivecomponents of a read-optimized DBMS and describe our implementation of a high-performance query engine that can operate on both row and column-oriented data. We thenuse our prototype to perform an in-depth analysis of the tradeoffs between column and …,Proceedings of the 32nd international conference on Very large data bases,2006,211,10
Low overhead concurrency control for partitioned main memory databases,Evan PC Jones; Daniel J Abadi; Samuel Madden,Abstract Database partitioning is a technique for improving the performance of distributedOLTP databases; since" single partition" transactions that access data on one partition donot need coordination with other partitions. For workloads that are amenable to partitioning;some argue that transactions should be executed serially on each partition without anyconcurrency at all. This strategy makes sense for a main memory database where there areno disk or user stalls; since the CPU can be fully utilized and the overhead of traditionalconcurrency control; such as two-phase locking; can be avoided. Unfortunately; many OLTPapplications have some transactions which access multiple partitions. This introducesnetwork stalls in order to coordinate distributed transactions; which will limit the performanceof a database that does not allow concurrency. In this paper; we compare two low …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,144,7
Query execution in column-oriented database systems,Daniel J Abadi,There are two obvious ways to map a two-dimension relational database table onto a one-dimensional storage interface: store the table row-by-row; or store the table column-by-column. Historically; database system implementations and research have focused on therow-by row data layout; since it performs best on the most common application for databasesystems: business transactional data processing. However; there are a set of emergingapplications for database systems for which the row-by-row layout performs poorly. Theseapplications are more analytical in nature; whose goal is to read through the data to gainnew insight and use it to drive decision making and planning. In this dissertation; we studythe problem of poor performance of row-by-row data layout for these emerging applications;and evaluate the column-by-column data layout opportunity as a solution to this problem …,*,2008,136,1
The design and implementation of modern column-oriented database systems,Daniel Abadi; Peter Boncz; Stavros Harizopoulos; Stratos Idreos; Samuel Madden,Abstract In this article; we survey recent research on column-oriented database systems; orcolumn-stores; where each attribute of a table is stored in a separate file or region onstorage. Such databases have seen a resurgence in recent years with a rise in interest inanalytic queries that perform scans and aggregates over large portions of a few columns of atable. The main advantage of a column-store is that it can access just the columns needed toanswer such queries. We specifically focus on three influential research prototypes;MonetDB [46]; MonetDB/X100 [18]; and C-Store [86]. These systems have formed the basisfor several well-known commercial column-store implementations. We describe theirsimilarities and differences and discuss their specific architectural features for compression;late materialization; join processing; vectorization and adaptive indexing (database …,Foundations and Trends® in Databases,2013,124,1
Column Stores for Wide and Sparse Data.,Daniel J Abadi,ABSTRACT While it is generally accepted that data warehouses and OLAP workloads areexcellent applications for column-stores; this paper speculates that column-stores may wellbe suited for additional applications. In particular we observe that column-stores do not seea performance degradation when storing extremely wide tables; and column-stores handlesparse data very well. These two properties lead us to conjecture that column-stores may begood storage layers for Semantic Web data; XML data; and data with GEM-style schemas.,CIDR,2007,103,22
The case for determinism in database systems,Alexander Thomson; Daniel J Abadi,Abstract Replication is a widely used method for achieving high availability in databasesystems. Due to the nondeterminism inherent in traditional concurrency control schemes;however; special care must be taken to ensure that replicas don't diverge. Log shipping;eager commit protocols; and lazy synchronization protocols are well-understood methods forsafely replicating databases; but each comes with its own cost in availability; performance;or consistency. In this paper; we propose a distributed database system which combines asimple deadlock avoidance technique with concurrency control schemes that guaranteeequivalence to a predetermined serial ordering of transactions. This effectively removes allnondeterminism from typical OLTP workloads; allowing active replication with nosynchronization overhead whatsoever. Further; our system eliminates the requirement for …,Proceedings of the VLDB Endowment,2010,100,10
Efficient processing of data warehousing queries in a split execution environment,Kamil Bajda-Pawlikowski; Daniel J Abadi; Avi Silberschatz; Erik Paulson,Abstract Hadapt is a start-up company currently commercializing the Yale Universityresearch project called HadoopDB. The company focuses on building a platform for BigData analytics in the cloud by introducing a storage layer optimized for structured data andby providing a framework for executing SQL queries efficiently. This work considersprocessing data warehousing queries over very large datasets. Our goal is to maximizeperfor mance while; at the same time; not giving up fault tolerance and scalability. Weanalyze the complexity of this problem in the split execution environment of HadoopDB.Here; incoming queries are examined; parts of the query are pushed down and executedinside the higher performing database layer; and the rest of the query is processed in a moregeneric MapReduce framework. In this paper; we discuss in detail performance-oriented …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,93,2
Processing of data using a database system in communication with a data processing framework,*,A system; method; and computer program product for processing data are disclosed. Thesystem includes a data processing framework configured to receive a data processing taskfor processing; a plurality of database systems coupled to the data processing framework;and a storage component in communication with the data processing framework and theplurality database systems. The database systems perform a data processing task. The dataprocessing task is partitioned into a plurality of partitions and each database systemprocesses a partition of the data processing task assigned for processing to that databasesystem. Each database system performs processing of its assigned partition of the dataprocessing task in parallel with another database system processing another partition of thedata processing task assigned to the another database system. The data processing …,*,2016,82,1
HadoopDB in action: building real world applications,Azza Abouzied; Kamil Bajda-Pawlikowski; Jiewen Huang; Daniel J Abadi; Avi Silberschatz,Abstract HadoopDB is a hybrid of MapReduce and DBMS technologies; designed to meetthe growing demand of analyzing massive datasets on very large clusters of machines. Ourprevious work has shown that HadoopDB approaches parallel databases in performanceand still yields the scalability and fault tolerance of MapReduce-based systems. In thisdemonstration; we focus on HadoopDB's flexible architecture and versatility with two realworld application scenarios: a semantic web data application for protein sequence analysisand a business data warehousing application based on TPC-H. The demonstration offers athorough walk-through of how to easily build applications on top of HadoopDB.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,79,15
An integration framework for sensor networks and data stream management systems,Daniel J Abadi; Wolfgang Lindner; Samuel Madden; Jörg Schuler,Abstract This demonstration shows an integrated query processing environment whereusers can seamlessly query both a data stream management system and a sensor networkwith one query expression. By integrating the two query processing systems; theoptimization goals of the sensor network (primarily power) and server network (primarilylatency and quality) can be unified into one quality of service metric. The demo showsvarious steps of the unified optimization process for a sample query where the effects ofeach step that the optimizer takes can be directly viewed using a quality of service monitor.Our demo includes sensors deployed in the demo area in a tiny mockup of a factoryapplication.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,78,7
Query execution systems and methods,*,System; method and computer program product for processing a query are disclosed. Queryprocessing includes partitioning the stored data into a plurality of partitions based on at leastone vertex in the plurality of vertexes; storing at least another triple in the plurality of tripleson the at least one node; assigning; based on the triple containing the at least one vertex; atleast one partition in the plurality of partitions corresponding to the triple to at least one nodein the plurality of nodes; and processing; based on the assigning; the query by processingthe plurality of partitions.,*,2014,69,15
Lightweight locking for main memory database systems,Kun Ren; Alexander Thomson; Daniel J Abadi,Abstract Locking is widely used as a concurrency control mechanism in database systems.As more OLTP databases are stored mostly or entirely in memory; transactional throughputis less and less limited by disk IO; and lock managers increasingly become performancebottlenecks. In this paper; we introduce very lightweight locking (VLL); an alternativeapproach to pessimistic concurrency control for main-memory database systems that avoidsalmost all overhead associated with traditional lock manager operations. We also propose aprotocol called selective contention analysis (SCA); which enables systems implementingVLL to achieve high transactional throughput under high contention workloads. Weimplement these protocols both in a traditional single-machine multi-core database serversetting and in a distributed database where data is partitioned across many commodity …,Proceedings of the VLDB Endowment,2012,68,1
CalvinFS: Consistent WAN Replication and Scalable Metadata Management for Distributed File Systems.,Alexander Thomson; Daniel J Abadi,Abstract Existing file systems; even the most scalable systems that store hundreds ofpetabytes (or more) of data across thousands of machines; store file metadata on a singleserver or via a shared-disk architecture in order to ensure consistency and validity of themetadata. This paper describes a completely different approach for the design of replicated;scalable file systems; which leverages a high-throughput distributed database system formetadata management. This results in improved scalability of the metadata layer of the filesystem; as file metadata can be partitioned (and replicated) across a (shared-nothing)cluster of independent servers; and operations on file metadata transformed into distributedtransactions. In addition; our file system is able to support standard file system semantics—including fully linearizable random writes by concurrent users to arbitrary byte offsets …,FAST,2015,51,15
Rethinking serializable multiversion concurrency control,Jose M Faleiro; Daniel J Abadi,Abstract Multi-versioned database systems have the potential to significantly increase theamount of concurrency in transaction processing because they can avoid read-writeconflicts. Unfortunately; the increase in concurrency usually comes at the cost of transactionserializability. If a database user requests full serializability; modern multi-versioned systemssignificantly constrain read-write concurrency among conflicting transactions and employexpensive synchronization patterns in their design. In main-memory multi-core settings;these additional constraints are so burdensome that multi-versioned systems are oftensignificantly outperformed by single-version systems. We propose B ohm; a newconcurrency control protocol for main-memory multi-versioned database systems. B ohmguarantees serializable execution while ensuring that reads never block writes. In …,Proceedings of the VLDB Endowment,2015,49,7
Query execution systems and methods,*,System; method; and computer program product for processing data are disclosed. Themethod includes receiving a query for processing of data; wherein the data is stored in atable in a plurality of tables; wherein the table is stored on at least one node within thedatabase system; determining an attribute of the table and another table in the plurality oftables; partitioning one of the table and the another table in the plurality of tables using thedetermined attribute into a plurality of partitions; and performing a join of at least twopartitions of the table and the another table using the determined attribute. The join isperformed on a single node in the database system.,*,2015,47,20
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47,22
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45,4
Problems with CAP; and Yahoo’s little known NoSQL system,Daniel Abadi,*,DBMS Musings,2010,40
Invisible loading: access-driven data transfer from raw files into database systems,Azza Abouzied; Daniel J Abadi; Avi Silberschatz,Abstract Commercial analytical database systems suffer from a high" time-to-first-analysis":before data can be processed; it must be modeled and schematized (a human effort);transferred into the database's storage layer; and optionally clustered and indexed (acomputational effort). For many types of structured data; this upfront effort is unjustifiable; sothe data are processed directly over the file system using the Hadoop framework; despite thecumulative performance benefits of processing this data in an analytical database system. Inthis paper we describe a system that achieves the immediate gratification of runningMapReduce jobs directly over a file system; while still making progress towards the long-term performance benefits of database systems. The basic idea is to piggyback onMapReduce jobs; leverage their parsing and tuple extraction operations to incrementally …,Proceedings of the 16th International Conference on Extending Database Technology,2013,38,20
Using the Barton libraries dataset as an RDF benchmark,Daniel J Abadi; Adam Marcus; Samuel R Madden; Kate Hollenbach,Page 1. Computer Science and Artificial Intelligence Laboratory Technical Report massachusettsinstitute of technology; cambridge; ma 02139 usa — www.csail.mit.edu MIT-CSAIL-TR-2007-036 July 6; 2007 Using The Barton Libraries Dataset As An RDF benchmark Daniel J. Abadi;Adam Marcus; Samuel R. Madden; and Kate Hollenbach Page 2. Using The Barton LibrariesDataset As An RDF benchmark Daniel J. Abadi MIT dna@csail.mit.edu Adam Marcus MITmarcua@csail.mit.edu Samuel R. Madden MIT madden@csail.mit.edu Kate Hollenbach MITkjhollen@mit.edu ABSTRACT This report describes the Barton Libraries RDF dataset and Long-well query benchmark that we use for our recent VLDB paper on Scalable Semantic Web DataManagement Using Vertical Partition- ing [4] …,*,2007,38,2
Data loading systems and methods,*,System; method; and computer program product for processing data are disclosed. Thesystem is configured to perform transfer of data from a file system to a database system.Such transfer is accomplished through receiving a request for loading data into a databasesystem; wherein the data includes a plurality of attributes; determining at least one attributeof the data for loading into the database system; and loading the at least one attribute of thedata into the database system while continuing to process remaining attributes of the data.,*,2016,30,1
Sinew: a SQL system for multi-structured data,Daniel Tahara; Thaddeus Diamond; Daniel J Abadi,Abstract As applications are becoming increasingly dynamic; the notion that a schema canbe created in advance for an application and remain relatively stable is becomingincreasingly unrealistic. This has pushed application developers away from traditionalrelational database systems and away from the SQL interface; despite their many well-established benefits. Instead; developers often prefer self-describing data models such asJSON; and NoSQL systems designed specifically for their relaxed semantics. In this paper;we discuss the design of a system that enables developers to continue to represent theirdata using self-describing formats without moving away from SQL and traditional relationaldatabase systems. Our system stores arbitrary documents of key-value pairs inside physicaland virtual columns of a traditional relational database system; and adds a layer above …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,29,7
Lazy evaluation of transactions in database systems,Jose M Faleiro; Alexander Thomson; Daniel J Abadi,Abstract Existing database systems employ an\textit {eager} transaction processing scheme---that is; upon receiving a transaction request; the system executes all the operations entailedin running the transaction (which typically includes reading database records; executinguser-specified transaction logic; and logging updates and writes) before reporting to theclient that the transaction has completed. We introduce a\textit {lazy} transaction executionengine; in which a transaction may be considered durably completed after only partialexecution; while the bulk of its operations (notably all reads from the database and allexecution of transaction logic) may be deferred until an arbitrary future time; such as when auser attempts to read some element of the transaction's write-set---all without modifying thesemantics of the transaction or sacrificing ACID guarantees. Lazy transactions are …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,22,7
Systems and methods for fault tolerant; adaptive execution of arbitrary queries at low latency,*,A system and method for performing distributed execution of database queries includes aquery server that receives a query to be executed on a database; forms a query plan basedon the query; assigns tasks to task slots on a plurality of worker nodes in a cluster; and; uponreceipt of a notification that a task has completed on a worker node; immediately assigns anunassigned task to a free task slot on that worker node; such that the task may beginexecuting on that worker node substantially immediately thereafter. The task slots on workernodes include pools of resources that run tasks without start-up overhead.,*,2014,20,7
Query optimization of distributed pattern matching,Jiewen Huang; Kartik Venkatraman; Daniel J Abadi,Greedy algorithms for subgraph pattern matching operations are often sufficient when thegraph data set can be held in memory on a single machine. However; as graph data setsincreasingly expand and require external storage and partitioning across a cluster ofmachines; more sophisticated query optimization techniques become critical to avoidexplosions in query latency. In this paper; we introduce several query optimizationtechniques for distributed graph pattern matching. These techniques include (1) a System-Rstyle dynamic programming-based optimization algorithm that considers both linear andbushy plans;(2) a cycle detection-based algorithm that leverages cycles to reduceintermediate result set sizes; and (3) a computation reusing technique that eliminatesredundant query execution and data transfer over the network. Experimental results show …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,18,7
An evaluation of the advantages and disadvantages of deterministic database systems,Kun Ren; Alexander Thomson; Daniel J Abadi,Abstract Recent proposals for deterministic database system designs argue thatdeterministic database systems facilitate replication since the same input can beindependently sent to two different replicas without concern for replica divergence. Inaddition; they argue that determinism yields performance benefits due to (1) the introductionof deadlock avoidance techniques;(2) the reduction (or elimination) of distributed commitprotocols; and (3) light-weight locking. However; these performance benefits are notuniversally applicable; and there exist several disadvantages of determinism; including (1)the additional overhead of processing transactions for which it is not known in advance whatdata will be accessed;(2) an inability to abort transactions arbitrarily (eg; in the case ofdatabase or partition overload); and (3) the increased latency required by a …,Proceedings of the VLDB Endowment,2014,15,1
Design principles for scaling multi-core oltp under high contention,Kun Ren; Jose M Faleiro; Daniel J Abadi,Page 1. Design Principles for Scaling Multi-core OLTP Under High Contention Kun RenYale University kun.ren@yale.edu Jose M. Faleiro Yale University jose.faleiro@yale.eduDaniel J. Abadi Yale University dna@cs.yale.edu ABSTRACT Although significant recentprogress has been made in improving the multi-core scalability of high throughputtransactional database systems; modern systems still fail to achieve scalable throughputfor workloads involving frequent access to highly contended data. Most of this inability toachieve high throughput is explained by the fundamental constraints involved inguaranteeing ACID — the addition of cores results in more concurrent transactions accessingthe same contended data for which access must be serialized in or- der to guarantee isolation.Thus; linear scalability for contended workloads is impossible …,Proceedings of the 2016 International Conference on Management of Data,2016,14,7
Leopard: Lightweight edge-oriented partitioning and replication for dynamic graphs,Jiewen Huang; Daniel J Abadi,Abstract This paper introduces a dynamic graph partitioning algorithm; designed for large;constantly changing graphs. We propose a partitioning framework that adjusts on the fly asthe graph structure changes. We also introduce a replication algorithm that is tightlyintegrated with the partitioning algorithm; which further reduces the number of edges cut bythe partitioning algorithm. Even though the proposed approach is handicapped by onlytaking into consideration local parts of the graph when reassigning vertices; extensiveevaluation shows that the proposed approach maintains a quality partitioning over time;which is comparable at any point in time to performing a full partitioning from scratch using astate-the-art static graph partitioning algorithm such as METIS. Furthermore; when vertexreplication is turned on; edge-cut can improve by an order of magnitude.,Proceedings of the VLDB Endowment,2016,14,11
The Aurora and Borealis Stream Processing Engines,Uğur Çetintemel; Daniel Abadi; Yanif Ahmad; Hari Balakrishnan; Magdalena Balazinska; Mitch Cherniack; Jeong-Hyon Hwang; Samuel Madden; Anurag Maskey; Alexander Rasin; Esther Ryvkina; Mike Stonebraker; Nesime Tatbul; Ying Xing; Stan Zdonik,Abstract Over the last several years; a great deal of progress has been made in the area ofstream-processing engines (SPEs). Three basic tenets distinguish SPEs from current dataprocessing engines. First; they must support primitives for streaming applications. UnlikeOnline Transaction Processing (OLTP); which processes messages in isolation; streamingapplications entail time series operations on streams of messages. Second; streamingapplications entail a real-time component. If one is content to see an answer later; then onecan store incoming messages in a data warehouse and run a historical query on thewarehouse to find information of interest. This tactic does not work if the answer must beconstructed in real time. The need for real-time answers also dictates a fundamentallydifferent storage architecture. DBMSs universally store and index data records before …,*,2016,13,1
Are we experiencing a big data bubble?,Fatma Özcan; Nesime Tatbul; Daniel J Abadi; Marcel Kornacker; C Mohan; Karthik Ramasamy; Janet Wiener,Over the last decade; the database field has seen resurgence with the big data wave.Accelerated increase in data volumes; and modern hardware have been two major factorsthat brought in significant investment in new database technologies. Our field has benefitedfrom this increased interest and focus. There is now an abundance of NoSQL; NewSQL; andSQL-on-Hadoop systems. According to nosql-database. org; the list of NoSQL databases [6]has reached 150. Many of these systems claim horizontal scalability; and support for non-relational data. However; this high scalability usually comes at the cost of strong support forACID transactions. Most of them only provide eventual consistency; or even worse; defermanaging transactional semantics to the application layer. Another important aspect of theseNoSQL systems is the lack of declarative query interfaces. Most only support …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,13,2
Fast distributed transactions and strongly consistent replication for oltp database systems,Alexander Thomson; Thaddeus Diamond; Shu-Chun Weng; Kun Ren; Philip Shao; Daniel J Abadi,Abstract As more data management software is designed for deployment in public andprivate clouds; or on a cluster of commodity servers; new distributed storage systemsincreasingly achieve high data access throughput via partitioning and replication. In order toachieve high scalability; however; today's systems generally reduce transactional support;disallowing single transactions from spanning multiple partitions. This article describesCalvin; a practical transaction scheduling and data replication layer that uses a deterministicordering guarantee to significantly reduce the normally prohibitive contention costsassociated with distributed transactions. This allows near-linear scalability on a cluster ofcommodity machines; without eliminating traditional transactional guarantees; introducing asingle point of failure; or requiring application developers to reason about data …,ACM Transactions on Database Systems (TODS),2014,12,15
Automatic generation of normalized relational schemas from nested key-value data,Michael DiScala; Daniel J Abadi,Abstract Self-describing key-value data formats such as JSON are becoming increasinglypopular as application developers choose to avoid the rigidity imposed by the relationalmodel. Database systems designed for these self-describing formats; such as MongoDB;encourage users to use denormalized; heavily nested data models so that relationshipsacross records and other schema information need not be predefined or standardized. Suchdata models contribute to long-term development complexity; as their lack of explicit entityand relationship tracking burdens new developers unfamiliar with the dataset. Furthermore;the large amount of data repetition present in such data layouts can introduce updateanomalies and poor scan performance; which reduce both the quality and performance ofanalytics over the data. In this paper we present an algorithm that automatically …,Proceedings of the 2016 International Conference on Management of Data,2016,9,7
Modularity and Scalability in Calvin.,Alexander Thomson; Daniel J Abadi,Abstract Calvin is a transaction scheduling and replication management layer for distributedstorage systems. By first writing transaction requests to a durable; replicated log; and thenusing a concurrency control mechanism that emulates a deterministic serial execution of thelog's transaction requests; Calvin supports strongly consistent replication and fully ACIDdistributed transactions while incurring significantly lower inter-partition transactioncoordination costs than traditional distributed database systems. Furthermore; Calvin'sdeclarative specification of target concurrency-control behavior allows system componentsto avoid interacting with actual transaction scheduling mechanisms—whereas in traditionalDBMSs; the analogous components often have to explicitly observe concurrency controlmodules'(highly nondeterministic) procedural behaviors in order to function correctly.,IEEE Data Eng. Bull.,2013,9,20
Tradeoffs between parallel database systems; hadoop; and hadoopdb as platforms for petabyte-scale analysis,Daniel J Abadi,Abstract As the market demand for analyzing data sets of increasing variety and scalecontinues to explode; the software options for performing this analysis are beginning toproliferate. No fewer than a dozen companies have launched in the past few years that sellparallel database products to meet this market demand. At the same time; MapReduce-based options; such as the open source Hadoop framework are becoming increasinglypopular; and there have been a plethora of research publications in the past two years thatdemonstrate how MapReduce can be used to accelerate and scale various data analysistasks. Both parallel databases and MapReduce-based options have strengths andweaknesses that a practitioner must be aware of before selecting an analytical datamanagement platform. In this talk; I describe some experiences in using these systems …,International Conference on Scientific and Statistical Database Management,2010,9,15
CherniackM,Camey D AbadiD; U Cetintemel,*,Convey C; Lee S; Stonebraker M; Tatbul N; Zdonik S. Aurora Anewmodeland architecturefordatastreammanagement. TheVLDBJoumal,2003,9
Schema-less access to stored data,*,A system; a method; and a computer program product for accessing stored partiallystructured data using a structure-based query language. A partially-structured data is storedin a data management system using at least one key-value pair. Using the datamanagement system; a query of data is processed. The query is generated using astructured language.,*,2016,8,10
Scalable pattern matching over compressed graphs via dedensification,Antonio Maccioni; Daniel J Abadi,Abstract One of the most common operations on graph databases is graph pattern matching(eg; graph isomorphism and more general types of" subgraph pattern matching"). In fact; insome graph query languages every single query is expressed as a graph matchingoperation. Consequently; there has been a significant amount of research effort in optimizinggraph matching operations in graph database systems. As graph databases have scaled inrecent years; so too has recent work on scaling graph matching operations. However; theperformance of recent proposals for scaling graph pattern matching is limited by thepresence of high-degree nodes. These high-degree nodes result in an explosion ofintermediate result sizes during query execution; and therefore significant performancebottlenecks. In this paper we present a dedensification technique that losslessly …,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2016,8,7
Low-overhead asynchronous checkpointing in main-memory database systems,Kun Ren; Thaddeus Diamond; Daniel J Abadi; Alexander Thomson,Abstract As it becomes increasingly common for transaction processing systems to operateon datasets that fit within the main memory of a single machine or a cluster of commoditymachines; traditional mechanisms for guaranteeing transaction durability---which typicallyinvolve synchronous log flushes---incur increasingly unappealing costs to otherwiselightweight transactions. Many applications have turned to periodically checkpointing fulldatabase state. However; existing checkpointing methods---even those which avoid freezingthe storage layer---often come with significant costs to operation throughput; end-to-endlatency; and total memory usage. This paper presents Checkpointing Asynchronously usingLogical Consistency (CALC); a lightweight; asynchronous technique for capturing databasesnapshots that does not require a physical point of consistency to create a checkpoint …,Proceedings of the 2016 International Conference on Management of Data,2016,7,20
Cloud databases: what's new?,Daniel J Abadi; Michael Carey; Surajit Chaudhuri; Hector Garcia-Molina; Jignesh M Patel; Raghu Ramakrishnan,Abstract The panelists will discuss what characterizes data management in the cloud; andhow this differs from the broad range of applications that conventional databasemanagement systems have supported over the past few decades. They will examinewhether we need to develop new technologies to address demonstrably new challenges; orwhether we can largely re-position existing systems and approaches. The discussion willcover data analysis in the cloud using Map-Reduce based systems such as Hadoop; andcloud data serving (and so-called" No SQL" systems).,Proceedings of the VLDB Endowment,2010,7,7
Distributed data streams,Minos Garofalakis,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,7,2
Tutorial: SQL-on-Hadoop Systems,Daniel Abadi; Shivnath Babu; Fatma Ozcan; Ippokratis Pandis,*,Proceedings of the VLDB Endowment,2015,6
Comparing domain-specific and non-domain-specific anaphora resolution techniques,Daniel Abadi,1. Abstract A quantification is provided for the improvements made in traditional salience-based pronominal anaphora resolution precision when input text that has been parsedusing a large scale grammar to locate syntactic function of noun phrases; is used instead ofinput text where more shallow syntactic analysis techniques were used for identifyinggrammatical function. In addition; domain-specific techniques where domain knowledge canbe automatically acquired from a database backend schema representation in an emailresponse system are examined as a means for further improvements in pronoun resolution.,Cambridge University MPhil Dissertation,2003,6,4
SQL-on-hadoop systems: tutorial,Daniel Abadi; Shivnath Babu; Fatma Özcan; Ippokratis Pandis,Abstract Enterprises are increasingly using Apache Hadoop; more specifically HDFS; as acentral repository for all their data; data coming from various sources; including operationalsystems; social media and the web; sensors and smart devices; as well as their applications.At the same time many enterprise data management tools (eg from SAP ERP and SAS toTableau) rely on SQL and many enterprise users are familiar and comfortable with SQL. Asa result; SQL processing over Hadoop data has gained significant traction over the recentyears; and the number of systems that provide such capability has increased significantly. Inthis tutorial we use the term SQL-on-Hadoop to refer to systems that provide some level ofdeclarative SQL (-like) processing over HDFS and noSQL data sources; using architecturesthat include computational or storage engines compatible with Apache Hadoop.,Proceedings of the VLDB Endowment,2015,5,20
Design issues for second generation stream processing engines,Daniel J Abadi; Yanif Ahmad; Magdalena Balazinska; Ugur Cetintemel; Mitch Cherniack; Jeong-Hyon Hwang; Wolfgang Lindner; A Maskey; N Tatbul; Y Xing; S Zdonik,Abstract Borealis is a second-generation distributed stream processing engine that is beingdeveloped at Brandeis University; Brown University; and MIT. Borealis inherits core streamprocessing functionality from Aurora [13] and distribution functionality from Medusa [49].Borealis modifies and extends both systems in non-trivial and critical ways to provideadvanced capabilities that are commonly required by newly-emerging stream processingapplications. In this paper; we outline the basic design and functionality of Borealis. Throughsample real-world applications; we motivate the need for dynamically revising query resultsand modifying query specifications. We then describe how Borealis addresses thesechallenges through an innovative set of features; including revision records; time travel; andcontrol lines. Finally; we present a highly flexible and scalable QoS-based optimization …,Proc. of the Conference for Innovative Database Research (CIDR); Asilomar; CA,2005,5,7
U. etintemel; M,D Abadi; D Carney,*,Cherniack; C. Convey; S. Lee; M. Stonebraker; N. Tatbul; and S. Zdonik,*,5
High performance transactions via early write visibility,Jose M Faleiro; Daniel J Abadi; Joseph M Hellerstein,Abstract In order to guarantee recoverable transaction execution; database systems permit atransaction's writes to be observable only at the end of its execution. As a consequence;there is generally a delay between the time a transaction performs a write and the time latertransactions are permitted to read it. This delayed write visibility can significantly impact theperformance of serializable database systems by reducing concurrency among conflictingtransactions. This paper makes the observation that delayed write visibility stems from thefact that database systems can arbitrarily abort transactions at any point during theirexecution. Accordingly; we make the case for database systems which only aborttransactions under a restricted set of conditions; thereby enabling a new recoverabilitymechanism; early write visibility; which safely makes transactions' writes visible prior to …,Proceedings of the VLDB Endowment,2017,4,1
FIT: A Distributed Database Performance Tradeoff.,Jose M Faleiro; Daniel J Abadi,Abstract Designing distributed database systems is an inherently complex process; involvingmany choices and tradeoffs that must be considered. It is impossible to build a distributeddatabase that has all the features that users intuitively desire. In this article; we discuss thethree-way relationship between three such desirable features—fairness; isolation; andthroughput (FIT)—and argue that only two out of the three of them can be achievedsimultaneously.,IEEE Data Eng. Bull.,2015,4,20
Optimizing index deployment order for evolving OLAP,Hideaki Kimura; Carleton Coffrin; Alexander Rasin; Stanley B Zdonik,Abstract Many database applications deploy hundreds or thousands of indexes to speed upquery execution. Despite a plethora of prior work on index selection; designing anddeploying indexes remains a difficult task for database administrators. First; real-worldbusinesses often require online index deployment; and the traditional off-line approach toindex selection ignores intermediate workload performance during index deployment.Second; recent work on on-line index selection does not address effects of complexinteractions that manifest during index deployment. In this paper; we propose a newapproach that incorporates transitional design performance into the overall problem ofphysical database design. We call our approach Incremental Database Design. As the firststep in this direction; we study the problem of ordering index deployment. The benefits of …,Proceedings of the 15th International Conference on Extending Database Technology,2012,3,7
VLL: a lock manager redesign for main memory database systems,Kun Ren; Alexander Thomson; Daniel J Abadi,Abstract Lock managers are increasingly becoming a bottleneck in database systems thatuse pessimistic concurrency control. In this paper; we introduce very lightweight locking(VLL); an alternative approach to pessimistic concurrency control for main memory databasesystems; which avoids almost all overhead associated with traditional lock manageroperations. We also propose a protocol called selective contention analysis (SCA); whichenables systems implementing VLL to achieve high transactional throughput under high-contention workloads. We implement these protocols both in a traditional single-machinemulti-core database server setting and in a distributed database where data are partitionedacross many commodity machines in a shared-nothing cluster. Furthermore; we show howVLL and SCA can be extended to enable range locking. Our experiments show that VLL …,The VLDB Journal,2015,2,15
Deterministic database systems,*,In an embodiment; a plurality of transactions for accessing a database may be acquired. Thedatabase may be associated with a plurality of locks. The plurality of transactions mayinclude a first transaction; a second transaction; and a third transaction. A logicalserialization sequence for executing the transactions may be identified. The logicalserialization sequence may indicate that (1) the first transaction is to be executed before thesecond transaction based on all locks that are required by the first transaction beingavailable;(2) the second transaction is to be executed after the first transaction hascompleted execution based on the second transaction requiring a lock that is required by thefirst transaction; and (3) the third transaction is to be executed before or during execution ofthe first transaction based on all locks required by the third transaction being different …,*,2014,2,20
Column-Oriented Database Systems (Tutorial),D Abadi; Peter A Boncz; Stavros Harizopoulos,textabstractColumn-oriented database systems (column-stores) have attracted a lot ofattention in the past few years. Column-stores; in a nutshell; store each database tablecolumn separately; with attribute values belonging to the same column stored contiguously;compressed; and densely packed; as opposed to traditional database systems that storeentire records (rows) one after the other. Reading a subset of a table's columns becomesfaster; at the potential expense of excessive disk-head seeking from column to column forscattered reads or updates. After several dozens of research papers and at least a dozen ofnew column-store start-ups; several questions remain. Are these a new breed of systems orsimply old wine in new bottles? How easily can a major row-based system achieve column-store performance? Are column-stores the answer to effortlessly support large-scale data …,*,2009,2,7
Latch-free Synchronization in Database Systems: Silver Bullet or Fool's Gold?,Jose M Faleiro; Daniel J Abadi,ABSTRACT Recent research on multi-core database architectures has made the argumentthat; when possible; database systems should abandon the use of latches in favor of latch-free algorithms. Latch-based algorithms are thought to scale poorly due to their use ofsynchronization based on mutual exclusion. In contrast; latch-free algorithms make strongtheoretical guarantees which ensure that the progress of a thread is never impeded due tothe delay or failure of other threads. In this paper; we analyze the various factors thatinfluence the performance and scalability of latch-free and latch-based algorithms; andperform a microbenchmark evaluation of latch-free and latch-based synchronizationalgorithms. Our findings indicate that the argument for latch-free algorithms' superiorscalability is far more nuanced than the current state-of-the-art in multi-core database …,CIDR,2017,1,7
Sensor Network Integration with Streaming Database Systems,Daniel Abadi; Samuel Madden; Wolfgang Lindner,Abstract Recent advances in computing technology have led to the production of a newclass of computing device: the wireless; battery powered; smart sensor. Traditional sensorsdeployed throughout buildings; labs; and equipment are passive devices that simplymodulate a voltage based on some environmental parameter. In contrast; these newsensors are active; full fledged computers; capable not only of sampling real worldphenomena but also filtering; sharing; and combining those sensor readings with each otherand nearby Internet-equipped endpoints. Over the past several years; we have designedand implemented a query processor for such sensor networks called TinyDB. TinyDB is adistributed query processor that runs on each of the nodes in a sensor network that isexplicitly designed to simplify many of the data collection applications described above …,*,2016,1,20
SQL Beyond Structure: Text; Documents and Key-Value Pairs,Daniel Tahara; Daniel Abadi,ABSTRACT Despite; or perhaps because of; the bevy of data formats used in modernapplications; the development community has yet to settle on a standard query interface foranalyzing that data in an efficient manner. As a result; they are forced to rely on complicatedscripting and ETL in order to analyze their data; significantly increasing their overall 'time toinsight.'Meanwhile; SQL remains entrenched among the skillsets of analysts and databasemanagers; with conventional wisdom saying that its semantics are incompatibile with thenew; relaxed data formats. However; with our 'Flexible Schema and Multi-structured Tables'approach; we show that it is possible to unify structured; semi-structured; and fullyunstructured data as part of a single analytics system. Our approach defines an extendedrelational abstraction (Multistructured Tables) that maps arbitrarily structured data into a …,Proc. New England Database Day 2014,2014,1,15
How best to build web-scale data managers?,Philip A Bernstein; Daniel J Abadi; Michael J Cafarella; Joseph M Hellerstein; Donald Kossmann; Samuel Madden,1. PANEL OVERVIEW Many of the largest database-driven web sites use custom web- scaledata managers (WDMs). On the surface; these WDMs are being applied to problems that arewell-suited for relational database systems. Some examples are the following: • Map-Reduce[5]; Hadoop [7]; and Dryad [9] are used to process queries on large data sets using sequentialscan and aggregation. Hive [8] is a data warehouse built on Hadoop. • Google's Bigtable [3] isused to store a replicated table of rows of semi-structured data. • Amazon's Dynamo [6] is usedto store partitioned; replicated databases of key-value pairs. Cassandra [2] is similar. • Objectcaching systems are used instead of a persistent store; such as memcached [10]; Oracle'sCoherence; and Microsoft's Velocity project … These WDMs have challenging requirementsthat are not met by current relational database products. They need to scale out to …,Proceedings of the VLDB Endowment,2009,1,15
Foundations and Trends® in Databases,Daniel Abadi; Peter Boncz; Stavros Harizopoulos; Stratos Idreos; Samuel Madden,*,Foundations and Trends® in Databases,2013,*
1. IMPORTANT DATES,Carmen Tang,It is widely understood that China shelters one of the richest mycofloras in the world. Thevariety of ecological zones; topological relief; and geographical extent supports this fact. Theoldest and largest mycological collection in the country is that maintained at the Institute ofMicrobiology; Academia Sinica; in Beijing. The Beijing herbarium (acronym: HMAS) wasestablished in 1953 and now houses over 75;000 specimens of fungi. The rapiddevelopment of mycology in China during the last two decades has not been brought tointernational attention (but see Bartholomew; Brittonia 3 1: 1-25; 1979; Ma; Taxon 38: 617-620; 1989). It is a pleasure; therefore; to introduce the mycological herbarium at Kunming;Yunnan; and to welcome foreign visitors and loan requests.,*,2013,*,22
BIBLIOGRAPHICAL REFERENCES Fabio A. Schreiber lectures,DJ Abadi; S Alamri; D Taniar; M Safar; G Amato,Other references to bibliographical items and to interesting web sites can be directly found atthe end of the lectures slides … • AA.VV. – The Growing Impact of the CAP Theorem – IEEEComputer; February 2012 … • AA.VV. – Special Issue on Column Store Systems - IEEE DataEngineering Bulletin; Vol. 35; No. 1; March 2012 http://sites.computer.org/debull/A12mar/issue1.htm … • AA.VV. – Special Issue on Query Optimization for Big Data Systems- IEEE Data EngineeringBulletin; Vol. 36; No. 1; March 2013 http://sites.computer.org/debull/A13mar/issue1.htm … •AA.VV. – Special Issue on Main-Memory Database Systems - IEEE Data EngineeringBulletin; Vol. 36; n. 2; June 2013 http://sites.computer.org/debull/A13june/issue1.htm … • DJAbadi - Data Management in the Cloud: Limitations and Opportunities - IEEE Data EngineeringBulletin; Vol. 32 No. 1; March 2009 http://sites.computer.org/debull/A09mar/A09MAR-CD …,Bulletin; Vol,2012,*,15
Encyclopedia of Database Systems: A Springer Live Reference,Simonas Šaltenis; Ugur Cetintemel; Fatma Özcan; M Tamer Özsu; Peter Øhrstrøm; Ali Ünlü; Daniel Abadi; Alberto Abello; Serge Abiteboul; Ioannis Aekaterinidis,*,*,2011,*
Hold the Accusations That Limit Scientific Innovation Response,Michael Stonebraker; Daniel Abadi; David J DeWitt; Sam Madden; Erik Paulson; Andrew Pavlo; Alexander Rasin,*,COMMUNICATIONS OF THE ACM,2010,*
Hold the Accusations That Limit Scientific Innovation. Authors' reply,Jonathan Grier; Michael STONEBRAKER; Daniel ABADI; David J DEWITT; Sam MADDEN; Erik PAULSON; Andrew PAVLO; Alexander RASIN,*,Communications of the ACM,2010,*
Data Partitioning,Daniel Abadi,realized using such logical structures. For example; in tree based data acquisition protocols;a collection tree is built that is rooted at the data collection center such as the sink node [8].The dissemination of the data requests from the participating nodes and collection of datafrom the sensor nodes are accomplished using this tree. A cluster based data acquisitionmechanism has been proposed in [3]. As shown in Fig. 1; nodes are organized into a fixednumber of clusters; and nodes within each cluster dynamically elect a cluster head. The dataacquisition is carried out in two phases. In the first phase; cluster heads collect data fromtheir cluster nodes. In the second phase; cluster heads send collected data to the nodes thathave subscribed to the data. The cluster heads are re-elected to balance energyconsumption among the nodes in the cluster. Zhang et al.[13] have proposed an adaptive …,*,2009,*,3
H-Store: A High-Performance; Distributed Main Memory Transaction Processing System,Evan PC Jones; Samuel Madden; Michael Stonebraker; Yang Zhang; John Hugg; Daniel J Abadi,ABSTRACT Our previous work has shown that architectural and application shifts haveresulted in modern OLTP databases increasingly falling short of optimal performance [10]. Inparticular; the availability of multiple-cores; the abundance of main memory; the lack of userstalls; and the dominant use of stored procedures are factors that portend a clean-slateredesign of RDBMSs. This previous work showed that such a redesign has the potential tooutperform legacy OLTP databases by a significant factor. These results; however; wereobtained using a bare-bones prototype that was developed just to demonstrate the potentialof such a system. We have since set out to design a more complete execution platform; andto implement some of the ideas presented in the original paper. Our demonstrationpresented here provides insight on the development of a distributed main memory OLTP …,*,2008,*,7
Visual COKO: a debugger for query optimizer development,Daniel J Abadi; Mitch Cherniack,Abstract Query optimization generates plans to retrieve data requested by queries. Queryrewriting; which is the first step of this process; rewrites a query expression into anequivalent form to prepare it for plan generation. COKO-KOLA introduced a new approach toquery rewriting that enables query rewrites to be formally verified using an automatedtheorem prover [1]. KOLA is a language for expressing term rewriting rules that can be" fired"on query expressions. COKO is a language for expressing query rewriting transformationsthat are too complex to express with simple KOLA rules [2]. COKO is a programminglanguage designed for query optimizer development. Programming languages requiredebuggers; and in this demonstration; we illustrate our COKO debugger: Visual COKO.Visual COKO enables a query optimization developer to visually trace the execution of a …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,*,2
New DBMS Architectures,Mike Stonebraker; Daniel J Abadi; Adam Batkin; Xuedong Chen; Mitch Cherniack; Miguel Ferreira; Edmond Lau,Probably the most important thing that has happened in the DBMS landscape is the death of“one size fits all”. Until the early 2000's the traditional disk-based row-store architecture wasomni-present. In effect; the commercial vendors had a hammer and everything was a nail. Inthe last fifteen years; there have been several major upheavals; which we discuss in turn.First; the community realized that column stores are dramatically superior to row stores in thedata warehouse marketplace. Data warehouses found early acceptance in customer facingretail environments and quickly spread to customer facing data in general. Warehousesrecorded historical information on customer transactions. In effect; this is the who-what-why-when-where of each customer interaction. The conventional wisdom is to structure a datawarehouse around a central Fact table in which this transactional information is recorded …,*,*,*,15
13th USENIX Conference on File and Storage Technologies,Alexander Thomson; Daniel J Abadi,Erez Zadok opened the conference with the numbers: 130 submissions with 28 papersselected. They used a two round online review process; with each paper getting threereviews during the first round; and the 68 remaining papers getting two more reviews in thenext round. The final decisions were made in an all-day meeting at Stony Brook. JiriSchindler announced awards; starting with ACM Test-of-Time Fast Track awards going to“RAIDShield: Characterizing; Monitoring; and Proactively Protecting against Disk Failures;”by Ao Ma et al.; and “BetrFS: A Right-Optimized Write-Optimized File System;” by WilliamJannen et al. The Best Paper Award went to “Skylight—A Window on Shingled DiskOperation;” by Abutalib Aghayev and Peter Desnoyers. These researchers cut a window intoa shingled (SMR) drive so that they could use a high-speed camera to record disk seeks …,*,*,*,7
On Compressing Graph Databases,Antonio Maccioni; Daniel J Abadi,ABSTRACT Distributed and parallel databases are not always a scalable solution for graphsbecause of the low locality in the data matching the query. A different approach for scalabilityis to consider compressed versions of the input graph in order to reduce the cost of joinoperations at query time. These methods proved to be effective when the queries are givenin advance but are not general enough to be adopted since we cannot reconstruct theoriginal graph with decompression. This means that the results of the query areapproximations of the exact answers. We want to discuss with the audience a novelcompression technique that enables the database engine to evaluate graph queries (eg;graph pattern matching queries) in an exact way. Preliminary experiments show that ourapproach is scalable with respect to the size of the graph and that this method can be …,*,*,*,7
Building Deterministic Transaction Processing Systems without Deterministic Thread Scheduling,Alexander Thomson; Daniel J Abadi,ABSTRACT Standard implementations of transactional systems such as database systemsallow several sources of nondeterminism to introduce unpredictable behavior. The recentintroduction of an architecture and execution model that isolates sources of nondeterministicbehavior in online transaction processing systems in order to yield deterministic transactionresults makes active replication easier and mitigates major scalability barriers. We observehere that (a) this approach would nicely complement other determinism techniques in theassembly of a fully deterministic application stack and (b) the approach does not rely on anyspecial thread-scheduling machinery or deterministic concurrency primitives and evenbenefits from the nondeterminism inherent in typical OS schedulers.,*,*,*,22
HP RiR Research Plan,Daniel Abadi,*,*,*,*
An Integration Framework for Sensor Networks and DSMSs,Daniel Abadi; Samuel Madden; Michael Stonebraker,DSMSs [1; 4; 2] were developed to support an emerging class of applications-monitoringapplications-that proved problematic for traditional DBMSs. Monitoring applications areapplications that monitor continuous streams of data. Their fundamental data-active/human-passive; real-time; trigger oriented model is difficult to support in the traditional human-active/data-passive trigger-as-secondclass-citizen DBMS model. DSMSs are better suited tosupporting these applications by organizing query operators in a work-flow diagram andallowing data to actively stream through these operators; which transform the input dataaccording to a continuous query plan. The performance of a DSMS can be measured usinga Quality of Service (QoS) metric in which an application can specify the utility of observedlatency; throughput; or quality of result tuples that reach the application.Sensor networks …,*,*,*,7
Program Committees,Daniel Abadi; Ashraf ABOULNAGA; Laurent AMSALEG; Walid AREF; Sourav BHOWMICK; Angela BONIFATI; Peter BONCZ; Philippe BONNET; Luc BOUGANIM; Stéphane BRESSAN; Nicolas BRUNO; Barbara CATANIA; Chee Yong CHAN; Ugur CETINTEMEL; Lei CHEN; Shimin CHEN; Reynold CHENG; Brian COOPER; Bin CUI; Gautam DAS; Amol DESHPANDE; Alin DEUTSCH; Yanlei DIAO; Wenfei FAN; Alan FEKETE; Elena FERRARI; Shel FINKELSTEIN; Peter FISCHER; Minos GAROFALAKIS; Johannes GEHRKE; Gabriel GHINITA; Leo GIAKOUMAKIS,Page 1. VLDB 2010 xi SINGAPORE PROGRAM COMMITTEES Core Database Technology KianLee TAN; Program Chair (National University of Singapore; Singapore) Daniel ABADI (YaleUniversity; USA) Ashraf ABOULNAGA (University of Waterloo; Canada) Laurent AMSALEG(IRISA-CNRS; France) Walid AREF (Purdue University; USA) Sourav BHOWMICK (NanyangTechnological University; Singapore) Angela BONIFATI (Icar-CNR; Italy) Peter BONCZ (CWI; TheNetherlands) Philippe BONNET (University of Copenhagen; Denmark) Luc BOUGANIM (INRIA;France) Stéphane BRESSAN (National University of Singapore; Singapore) Nicolas BRUNO(Microsoft Research; USA) Barbara CATANIA (Universita di Genova; Italy ) Chee Yong CHAN(National University of Singapore; Singapore) Ugur CETINTEMEL (Brown University; USA) LeiCHEN (Hong Kong University of Science and Technology; China) …,*,*,*,4
An Analysis of the Benefits of Applying Column-Oriented Data Storage Techniques to RDF Stores,Daniel Abadi,*,*,*,*
Research Abstracts-2006,Daniel J Abadi; David J DeWitt; Stavros Harizopoulos; Samuel R Madden; Daniel S Myers; Michael R Stonebraker; Transparent Accountable Data Mining Initiative; Harold Abelson; Tim Berners-Lee; Chris Hanson; Lalana Kagal; Gerald Jay Sussman; K Krasnow Waterman; Daniel J Weitzner; Varun Aggarwal; Wesley O Jin; Una-May O'Reilly; RF Circuit Sizing; Unbounded Transactional Memory; C Scott Ananian; Krste Asanovic; Bradley C Kuszmaul; Charles E Leiserson; Shay Artzi; Michael Ernst; David Glasser; Adam Kiezun; Carlos Pacheco; Jeff Perkins; Hari Balakrishnan; Samuel Madden; Vladimir Bychkovsky; Kevin Chen; Waseem Daher; Michel Goraczko; Hongyi Hu; Bret Hull; Allen Miu; Eugene Shih; Kenneth C Barr; Michael A Bender; Martin Farach-Colton; Simai He; Robert Beverly; Steven Bauer; Karen Sollins; Michael Bolin; Greg Little; Ricarose Roque; Darris Hupp; Vikki Chou; Robert Miller; David D Clark; Peyman Faratin; Steve Bauer; W Lehr; R Sami; Charles Fine; Andrew Lippman; James Cowling; Daniel Myers; Barbara Liskov; Dorothy Curtis; Asfandyar Qureshi; Esteban Pino; Lucila Ohno-Machado; Robert Greenes; John Guttag; Marcelo d'Amorim; Darko Marinov; Tao Xie; Michael D Ernst; Nirav Dave; Michael Pellauer; Joel Emer; Brian Demsky; Philip J Guo; Stephen McCamant; Jeff H Perkins; Martin Rinard; Jack B Dennis; Matthew Drake; Rodric Rabbah; Saman Amarasinghe; Jonathan Edwards; Raimondas Lencevicius; Eric Fellheimer; Bryan Ford; Jacob Strauss; Chris Lesniewski-Laas; Sean Rhea; Frans Kaashoek; Robert Morris; Simson L Garfinkel; Robert C Miller; Steve Gerding; Lewis Girod; Kyle Jamieson; Yuan Mei; Stan Rost,Most major DBMS vendors implement record-oriented storage systems; where the attributesof a record (or tuple) are placed contiguously in storage. With this row store architecture; asingle disk write suffices to push all of the fields of a single record out to disk. Hence; highperformance writes are achieved; and a DBMS with a row store architecture is a write-optimized system. These are especially effective on OLTP-style applications.In contrast;systems oriented toward ad-hoc querying of large amounts of data should be read-optimized. Data warehouses represent one class of read-optimized system; in whichperiodically a bulk load of new data is performed; followed by a relatively long period of ad-hoc queries. Other read-mostly applications include customer relationship management(CRM) systems; electronic library card catalogs; and other ad-hoc inquiry systems. In …,*,*,*,15
How Best to Build Web-Scale Data Managers? A Panel Discussion,Philip A Bernstein; Daniel J Abadi; Michael J Cafarella; Joseph M Hellerstein; Donald Kossmann,Many of the largest database-driven web sites use custom webscale data managers(WDMs). On the surface; these WDMs are being applied to problems that are well-suited forrelational database systems. Some examples are the following:• Map-Reduce [5]; Hadoop[7]; and Dryad [9] are used to process queries on large data sets using sequential scan andaggregation. Hive [8] is a data warehouse built on Hadoop.• Google's Bigtable [3] is used tostore a replicated table of rows of semi-structured data.• Amazon's Dynamo [6] is used tostore partitioned; replicated databases of key-value pairs. Cassandra [2] is similar.• Objectcaching systems are used instead of a persistent store; such as memcached [10]; Oracle'sCoherence; and Microsoft's Velocity project. These WDMs have challenging requirementsthat are not met by current relational database products. They need to scale out to …,*,*,*,22
Q1) Map Reduce and Distributed Databases,A Abouzeid Workloads; K BajdaPawlikowski; D Abadi; A Silberschatz; A Rasin,Page 1. 1 Jozsef Patvarczki Comprehensive exam Due August 24 th ; 2010 Subject: DistributedDatabase Systems Q1) Map Reduce and Distributed Databases Map Reduce (Hadoop) is apopular framework for conducting data processing in a parallel manner. It requires us to takethe data "out of" the database in order to process it efficiently; yet promises to achievehigh-performance and scalable data processing by exploiting the power of a compute cluster(cloud) with ease. More recently; a marriage of MapReduce with DBMS Technologies has beentouted as the new approach towards achieving both performance on the one hand as well asscalability and flexibility on the other hand. One example of such a hybrid architecture isHadoopDB by Abouzeid et al. [1]. a) First review the key issues that must be tackled with designingsuch a hybrid combining map-reduce style processing with database technology …,*,*,*,7
