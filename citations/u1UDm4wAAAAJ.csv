A comparison of approaches to large-scale data analysis,Andrew Pavlo; Erik Paulson; Alexander Rasin; Daniel J Abadi; David J DeWitt; Samuel Madden; Michael Stonebraker,Abstract There is currently considerable enthusiasm around the MapReduce (MR) paradigmfor large-scale data analysis [17]. Although the basic control flow of this framework hasexisted in parallel SQL database management systems (DBMS) for over 20 years; somehave called MR a dramatically new computing model [8; 17]. In this paper; we describe andcompare both paradigms. Furthermore; we evaluate both kinds of systems in terms ofperformance and development complexity. To this end; we define a benchmark consisting ofa collection of tasks that we have run on an open source version of MR as well as on twoparallel DBMSs. For each task; we measure each system's performance for various degreesof parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs.Although the process to load data into and tune the execution of parallel DBMSs took …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,1209
MapReduce and parallel DBMSs: friends or foes?,Michael Stonebraker; Daniel Abadi; David J DeWitt; Sam Madden; Erik Paulson; Andrew Pavlo; Alexander Rasin,The technology press has been focusing on the revolution of "cloud computing;" a paradigmthat entails the harnessing of large numbers of processors working in parallel to solve computingproblems. In effect; this suggests constructing a data center by lining up a large number oflow-end servers; rather than deploying a smaller set of high-end servers. Along with this interestin clusters has come a proliferation of tools for programming them. MR is one such tool; an attractiveoption to many because it provides a simple model through which users are able to expressrelatively sophisticated distributed programs … Given the interest in the MR model both commerciallyand academically; it is natural to ask whether MR systems should replace parallel databasesystems. Parallel DBMSs were first available commercially nearly two decades ago; and;today; systems (from about a dozen vendors) are available. As robust; high-performance …,Communications of the ACM,2010,548
H-store: a high-performance; distributed main memory transaction processing system,Robert Kallman; Hideaki Kimura; Jonathan Natkins; Andrew Pavlo; Alexander Rasin; Stanley Zdonik; Evan PC Jones; Samuel Madden; Michael Stonebraker; Yang Zhang; John Hugg; Daniel J Abadi,Abstract Our previous work has shown that architectural and application shifts have resultedin modern OLTP databases increasingly falling short of optimal performance [10]. Inparticular; the availability of multiple-cores; the abundance of main memory; the lack of userstalls; and the dominant use of stored procedures are factors that portend a clean-slateredesign of RDBMSs. This previous work showed that such a redesign has the potential tooutperform legacy OLTP databases by a significant factor. These results; however; wereobtained using a bare-bones prototype that was developed just to demonstrate the potentialof such a system. We have since set out to design a more complete execution platform; andto implement some of the ideas presented in the original paper. Our demonstrationpresented here provides insight on the development of a distributed main memory OLTP …,Proceedings of the VLDB Endowment,2008,439
Skew-aware automatic database partitioning in shared-nothing; parallel OLTP systems,Andrew Pavlo; Carlo Curino; Stanley Zdonik,Abstract The advent of affordable; shared-nothing computing systems portends a new classof parallel database management systems (DBMS) for on-line transaction processing(OLTP) applications that scale without sacrificing ACID guarantees [7; 9]. The performanceof these DBMSs is predicated on the existence of an optimal database design that is tailoredfor the unique characteristics of OLTP workloads. Deriving such designs for modern DBMSsis difficult; especially for enterprise-class OLTP systems; since they impose extra challenges:the use of stored procedures; the need for load balancing in the presence of time-varyingskew; complex schemas; and deployments with larger number of partitions. To this purpose;we present a novel approach to automatically partitioning databases for enterprise-classOLTP systems that significantly extends the state of the art by:(1) minimizing the number …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,191
Oltp-bench: An extensible testbed for benchmarking relational databases,Djellel Eddine Difallah; Andrew Pavlo; Carlo Curino; Philippe Cudre-Mauroux,Abstract Benchmarking is an essential aspect of any database management system (DBMS)effort. Despite several recent advancements; such as pre-configured cloud database imagesand database-as-a-service (DBaaS) offerings; the deployment of a comprehensive testingplatform with a diverse set of datasets and workloads is still far from being trivial. In manycases; researchers and developers are limited to a small number of workloads to evaluatethe performance characteristics of their work. This is due to the lack of a universalbenchmarking infrastructure; and to the difficulty of gaining access to real data andworkloads. This results in lots of unnecessary engineering efforts and makes theperformance evaluation results difficult to compare. To remedy these problems; we presentOLTP-Bench; an extensible" batteries included" DBMS benchmarking testbed. The key …,Proceedings of the VLDB Endowment,2013,115
Staring into the abyss: An evaluation of concurrency control with one thousand cores,Xiangyao Yu; George Bezerra; Andrew Pavlo; Srinivas Devadas; Michael Stonebraker,Abstract Computer architectures are moving towards an era dominated by many-coremachines with dozens or even hundreds of cores on a single chip. This unprecedented levelof on-chip parallelism introduces a new dimension to scalability that current databasemanagement systems (DBMSs) were not designed for. In particular; as the number of coresincreases; the problem of concurrency control becomes extremely challenging. Withhundreds of threads running in parallel; the complexity of coordinating competing accessesto data will likely diminish the gains from increased core counts. To better understand justhow unprepared current DBMSs are for future CPU architectures; we performed anevaluation of concurrency control for on-line transaction processing (OLTP) workloads onmany-core chips. We implemented seven concurrency control algorithms on a main …,Proceedings of the VLDB Endowment,2014,84
Anti-caching: A new approach to database management system architecture,Justin DeBrabant; Andrew Pavlo; Stephen Tu; Michael Stonebraker; Stan Zdonik,Abstract The traditional wisdom for building disk-based relational database managementsystems (DBMS) is to organize data in heavily-encoded blocks stored on disk; with a mainmemory block cache. In order to improve performance given high disk latency; thesesystems use a multi-threaded architecture with dynamic record-level locking that allowsmultiple transactions to access the database at the same time. Previous research has shownthat this results in substantial overhead for on-line transaction processing (OLTP)applications [15]. The next generation DBMSs seek to overcome these limitations witharchitecture based on main memory resident data. To overcome the restriction that all data fitin main memory; we propose a new technique; called anti-caching; where cold data ismoved to disk in a transactionally-safe manner as the database grows in size. Because …,Proceedings of the VLDB Endowment,2013,75
Let's talk about storage & recovery methods for non-volatile memory database systems,Joy Arulraj; Andrew Pavlo; Subramanya R Dulloor,Abstract The advent of non-volatile memory (NVM) will fundamentally change the dichotomybetween memory and durable storage in database management systems (DBMSs). Thesenew NVM devices are almost as fast as DRAM; but all writes to it are potentially persistenteven after power loss. Existing DBMSs are unable to take full advantage of this technologybecause their internal architectures are predicated on the assumption that memory isvolatile. With NVM; many of the components of legacy DBMSs are unnecessary and willdegrade the performance of data intensive applications. To better understand these issues;we implemented three engines in a modular DBMS testbed that are based on differentstorage management architectures:(1) in-place updates;(2) copy-on-write updates; and (3)log-structured updates. We then present NVM-aware variants of these architectures that …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,67
E-store: Fine-grained elastic partitioning for distributed transaction processing systems,Rebecca Taft; Essam Mansour; Marco Serafini; Jennie Duggan; Aaron J Elmore; Ashraf Aboulnaga; Andrew Pavlo; Michael Stonebraker,Abstract On-line transaction processing (OLTP) database management systems (DBMSs)often serve time-varying workloads due to daily; weekly or seasonal fluctuations in demand;or because of rapid growth in demand due to a company's business success. In addition;many OLTP workloads are heavily skewed to" hot" tuples or ranges of tuples. For example;the majority of NYSE volume involves only 40 stocks. To deal with such fluctuations; anOLTP DBMS needs to be elastic; that is; it must be able to expand and contract resources inresponse to load fluctuations and dynamically balance load as hot tuples vary over time.This paper presents E-Store; an elastic partitioning framework for distributed OLTP DBMSs.It automatically scales resources in response to demand spikes; periodic events; andgradual changes in an application's workload. E-Store addresses localized bottlenecks …,Proceedings of the VLDB Endowment,2014,54
On predictive modeling for optimizing transaction execution in parallel OLTP systems,Andrew Pavlo; Evan PC Jones; Stanley Zdonik,Abstract A new emerging class of parallel database management systems (DBMS) isdesigned to take advantage of the partitionable workloads of on-line transaction processing(OLTP) applications [23; 20]. Transactions in these systems are optimized to execute tocompletion on a single node in a shared-nothing cluster without needing to coordinate withother nodes or use expensive concurrency control measures [18]. But some OLTPapplications cannot be partitioned such that all of their transactions execute within a single-partition in this manner. These distributed transactions access data not stored within theirlocal partitions and subsequently require more heavy-weight concurrency control protocols.Further difficulties arise when the transaction's execution properties; such as the number ofpartitions it may need to access or whether it will abort; are not known beforehand. The …,Proceedings of the VLDB Endowment,2011,47
S-Store: a streaming NewSQL system for big velocity applications,Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; John Meehan; Andrew Pavlo; Michael Stonebraker; Erik Sutherland; Nesime Tatbul; Kristin Tufte; Hao Wang; Stanley Zdonik,Abstract First-generation streaming systems did not pay much attention to state managementvia ACID transactions (eg;[3; 4]). S-Store is a data management system that combines OLTPtransactions with stream processing. To create S-Store; we begin with H-Store; a main-memory transaction processing engine; and add primitives to support streaming. Thisincludes triggers and transaction workflows to implement push-based processing; windowsto provide a way to bound the computation; and tables with hidden state to implementscoping for proper isolation. This demo explores the benefits of this approach by showinghow a naïve implementation of our benchmarks using only H-Store can yield incorrectresults. We also show that by exploiting push-based semantics and our implementation oftriggers; we can achieve significant improvement in transaction throughput. We demo two …,Proceedings of the VLDB Endowment,2014,44
S-Store: streaming meets transaction processing,John Meehan; Nesime Tatbul; Stan Zdonik; Cansu Aslantas; Ugur Cetintemel; Jiang Du; Tim Kraska; Samuel Madden; David Maier; Andrew Pavlo; Michael Stonebraker; Kristin Tufte; Hao Wang,Abstract Stream processing addresses the needs of real-time applications. Transactionprocessing addresses the coordination and safety of short atomic computations. Heretofore;these two modes of operation existed in separate; stove-piped systems. In this work; weattempt to fuse the two computational paradigms in a single system called S-Store. In thisway; S-Store can simultaneously accommodate OLTP and streaming applications. Wepresent a simple transaction model for streams that integrates seamlessly with a traditionalOLTP system; and provides both ACID and stream-oriented guarantees. We chose to build S-Store as an extension of H-Store-an open-source; in-memory; distributed OLTP databasesystem. By implementing S-Store in this way; we can make use of the transaction processingfacilities that H-Store already provides; and we can concentrate on the additional features …,Proceedings of the VLDB Endowment,2015,38
A prolegomenon on OLTP database systems for non-volatile memory,Justin DeBrabant; Joy Arulraj; Andrew Pavlo; Michael Stonebraker; Stan Zdonik; Subramanya Dulloor,ABSTRACT The design of a database management system's (DBMS) architecture ispredicated on the target storage hierarchy. Traditional diskoriented systems use a two-levelhierarchy; with fast volatile memory used for caching; and slower; durable device used forprimary storage. As such; these systems use a buffer pool and complex concurrency controlschemes to mask disk latencies. Compare this to main memory DBMSs that assume all datacan reside in DRAM; and thus do not need these components. But emerging non-volatilememory (NVM) technologies require us to rethink this dichotomy. Such memory devices areslightly slower than DRAM; but all writes are persistent; even after power loss. We exploretwo possible use cases of NVM for on-line transaction processing (OLTP) DBMSs. The first iswhere NVM completely replaces DRAM and the other is where NVM and DRAM coexist …,ADMS@ VLDB,2014,33
The NMI Build & Test Laboratory: Continuous Integration Framework for Distributed Computing Software.,Andrew Pavlo; Peter Couvares; Rebekah Gietzel; Anatoly Karp; Ian D Alderman; Miron Livny; Charles Bacon,Abstract We present a framework for building and testing software in a heterogeneous; multi-user; distributed computing environment. Unlike other systems for automated builds andtests; our framework is not tied to a specific developer tool; revision control system; or testingframework; and allows access to computing resources across administrative boundaries.Users define complex software building procedures for multiple platforms with simplesemantics. The system balances the need to continually integrate software changes whilestill providing on-demand access for developers. Our key contributions in this paper are:(1)the development of design principles for distributed build-and-test systems;(2) a descriptionof an implemented system that satisfies those principles; and (3) case studies on how thissystem is used in practice at two sites where large; multi-component systems are built and …,LISA,2006,33
Squall: Fine-grained live reconfiguration for partitioned main memory databases,Aaron J Elmore; Vaibhav Arora; Rebecca Taft; Andrew Pavlo; Divyakant Agrawal; Amr El Abbadi,Abstract For data-intensive applications with many concurrent users; modern distributedmain memory database management systems (DBMS) provide the necessary scale-outsupport beyond what is possible with single-node systems. These DBMSs are optimized forthe short-lived transactions that are common in on-line transaction processing (OLTP)workloads. One way that they achieve this is to partition the database into disjoint subsetsand use a single-threaded transaction manager per partition that executes transactions one-at-a-time in serial order. This minimizes the overhead of concurrency control mechanisms;but requires careful partitioning to limit distributed transactions that span multiple partitions.Previous methods used off-line analysis to determine how to partition data; but the dynamicnature of these applications means that they are prone to hotspots. In these situations; the …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,28
Bridging the archipelago between row-stores and column-stores for hybrid workloads,Joy Arulraj; Andrew Pavlo; Prashanth Menon,Abstract Data-intensive applications seek to obtain trill insights in real-time by analyzing acombination of historical data sets alongside recently collected data. This means that tosupport such hybrid workloads; database management systems (DBMSs) need to handleboth fast ACID transactions and complex analytical queries on the same database. But thecurrent trend is to use specialized systems that are optimized for only one of theseworkloads; and thus require an organization to maintain separate copies of the database.This adds additional cost to deploying a database application in terms of both storage andadministration overhead. To overcome this barrier; we present a hybrid DBMS architecturethat efficiently supports varied workloads on the same database. Our approach differs fromprevious methods in that we use a single execution engine that is oblivious to the storage …,Proceedings of the 2016 International Conference on Management of Data,2016,25
Benchmarking oltp/web databases in the cloud: The oltp-bench framework,Carlo A Curino; Djellel E Difallah; Andrew Pavlo; Philippe Cudre-Mauroux,Abstract Benchmarking is a key activity in building and tuning data management systems;but the lack of reference workloads and a common platform makes it a time consuming andpainful task. The need for such a tool is heightened with the advent of cloud computing--withits pay-per-use cost models; shared multi-tenant infrastructures; and lack of control onsystem configuration. Benchmarking is the only avenue for users to validate the quality ofservice they receive and to optimize their deployments for performance and resourceutilization. In this talk; we present our experience in building several adhoc benchmarkinginfrastructures for various research projects targeting several OLTP DBMSs; ranging fromtraditional relational databases; main-memory distributed systems; and cloud-basedscalable architectures. We also discuss our struggle to build meaningful micro …,Proceedings of the fourth international workshop on Cloud data management,2012,25
Tictoc: Time traveling optimistic concurrency control,Xiangyao Yu; Andrew Pavlo; Daniel Sanchez; Srinivas Devadas,Abstract Concurrency control for on-line transaction processing (OLTP) databasemanagement systems (DBMSs) is a nasty game. Achieving higher performance onemerging many-core systems is difficult. Previous research has shown that timestampmanagement is the key scalability bottleneck in concurrency control algorithms. Thisprevents the system from scaling to large numbers of cores. In this paper we present TicToc;a new optimistic concurrency control algorithm that avoids the scalability and concurrencybottlenecks of prior T/O schemes. TicToc relies on a novel and provably correct data-driventimestamp management protocol. Instead of assigning timestamps to transactions; thisprotocol assigns read and write timestamps to data items and uses them to lazily compute avalid commit timestamp for each transaction. TicToc removes the need for centralized …,Proceedings of the 2016 International Conference on Management of Data,2016,22
Self-Driving Database Management Systems.,Andrew Pavlo; Gustavo Angulo; Joy Arulraj; Haibin Lin; Jiexi Lin; Lin Ma; Prashanth Menon; Todd C Mowry; Matthew Perron; Ian Quah; Siddharth Santurkar; Anthony Tomasic; Skye Toor; Dana Van Aken; Ziqi Wang; Yingjun Wu; Ran Xian; Tieying Zhang,ABSTRACT In the last two decades; both researchers and vendors have built advisory toolsto assist database administrators (DBAs) in various aspects of system tuning and physicaldesign. Most of this previous work; however; is incomplete because they still require humansto make the final decisions about any changes to the database and are reactionarymeasures that fix problems after they occur. What is needed for a truly “self-driving”database management system (DBMS) is a new architecture that is designed forautonomous operation. This is different than earlier attempts because all aspects of thesystem are controlled by an integrated planning component that not only optimizes thesystem for the current workload; but also predicts future workload trends so that the systemcan prepare itself accordingly. With this; the DBMS can support all of the previous tuning …,CIDR,2017,19
Write-behind logging,Joy Arulraj; Matthew Perron; Andrew Pavlo,Abstract The design of the logging and recovery components of database managementsystems (DBMSs) has always been influenced by the difference in the performancecharacteristics of volatile (DRAM) and non-volatile storage devices (HDD/SSDs). The keyassumption has been that non-volatile storage is much slower than DRAM and onlysupports block-oriented read/writes. But the arrival of new non-volatile memory (NVM)storage that is almost as fast as DRAM with fine-grained read/writes invalidates theseprevious design choices. This paper explores the changes that are required in a DBMS toleverage the unique properties of NVM in systems that still include volatile DRAM. We makethe case for a new logging and recovery protocol; called write-behind logging; that enablesa DBMS to recover nearly instantaneously from system failures. The key idea is that the …,Proceedings of the VLDB Endowment,2016,19
Automatic database management system tuning through large-scale machine learning,Dana Van Aken; Andrew Pavlo; Geoffrey J Gordon; Bohan Zhang,Abstract Database management system (DBMS) configuration tuning is an essential aspectof any data-intensive application effort. But this is historically a difficult task because DBMSshave hundreds of configuration" knobs" that control everything in the system; such as theamount of memory to use for caches and how often data is written to storage. The problemwith these knobs is that they are not standardized (ie; two DBMSs use a different name forthe same knob); not independent (ie; changing one knob can impact others); and notuniversal (ie; what works for one application may be sub-optimal for another). Worse;information about the effects of the knobs typically comes only from (expensive) experience.To overcome these challenges; we present an automated approach that leverages pastexperience and collects new information to tune DBMS configurations: we use a …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,18
Reducing the storage overhead of main-memory OLTP databases with hybrid indexes,Huanchen Zhang; David G Andersen; Andrew Pavlo; Michael Kaminsky; Lin Ma; Rui Shen,Abstract Using indexes for query execution is crucial for achieving high performance inmodern on-line transaction processing databases. For a main-memory database; however;these indexes consume a large fraction of the total memory available and are thus a majorsource of storage overhead of in-memory databases. To reduce this overhead; we proposeusing a two-stage index: The first stage ingests all incoming entries and is kept small for fastread and write operations. The index periodically migrates entries from the first stage to thesecond; which uses a more compact; read-optimized data structure. Our first contribution ishybrid index; a dual-stage index architecture that achieves both space efficiency and highperformance. Our second contribution is Dual-Stage Transformation (DST); a set ofguidelines for converting any order-preserving index structure into a hybrid index. Our …,Proceedings of the 2016 International Conference on Management of Data,2016,16
What’s Really New with NewSQL?,Andrew Pavlo; Matthew Aslett,Abstract A new class of database management systems (DBMSs) called NewSQL tout theirability to scale modern on-line transaction processing (OLTP) workloads in a way that is notpossible with legacy systems. The term NewSQL was first used by one of the authors of thisarticle in a 2011 business analysis report discussing the rise of new database systems aschallengers to these established vendors (Oracle; IBM; Microsoft). The other author wasworking on what became one of the first examples of a NewSQL DBMS. Since then severalcompanies and research projects have used this term (rightly and wrongly) to describe theirsystems. Given that relational DBMSs have been around for over four decades; it isjustifiable to ask whether the claim of NewSQL's superiority is actually true or whether it issimply marketing. If they are indeed able to get better performance; then the next question …,SIGMOD Record,2016,15
An empirical evaluation of in-memory multi-version concurrency control,Yingjun Wu; Joy Arulraj; Jiexi Lin; Ran Xian; Andrew Pavlo,Abstract Multi-version concurrency control (MVCC) is currently the most popular transactionmanagement scheme in modern database management systems (DBMSs). Although MVCCwas discovered in the late 1970s; it is used in almost every major relational DBMS releasedin the last decade. Maintaining multiple versions of data potentially increases parallelismwithout sacrificing serializability when processing transactions. But scaling MVCC in a multi-core and in-memory setting is non-trivial: when there are a large number of threads runningin parallel; the synchronization overhead can outweigh the benefits of multi-versioning. Tounderstand how MVCC perform when processing transactions in modern hardware settings;we conduct an extensive study of the scheme's four key design decisions: concurrencycontrol protocol; version storage; garbage collection; and index management. We …,Proceedings of the VLDB Endowment,2017,14
How to build a non-volatile memory database management system,Joy Arulraj; Andrew Pavlo,Abstract The difference in the performance characteristics of volatile (DRAM) and non-volatile storage devices (HDD/SSDs) influences the design of database managementsystems (DBMSs). The key assumption has always been that the latter is much slower thanthe former. This affects all aspects of a DBMS's runtime architecture. But the arrival of newnon-volatile memory (NVM) storage that is almost as fast as DRAM with fine-grainedread/writes invalidates these previous design choices. In this tutorial; we provide an outlineon how to build a new DBMS given the changes to hardware landscape due to NVM. Wesurvey recent developments in this area; and discuss the lessons learned from priorresearch on designing NVM database systems. We highlight a set of open researchproblems; and present ideas for solving some of them.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,10
Enterprise database applications and the cloud: A difficult road ahead,Michael Stonebraker; Andrew Pavlo; Rebecca Taft; Michael L Brodie,There is considerable interest in moving DBMS applications from inside enterprise datacenters to the cloud; both to reduce cost and to increase flexibility and elasticity. Some ofthese applications are" green field" projects (ie; new applications); others are existing legacysystems that must be migrated to the cloud. In another dimension; some are decisionsupport applications while others are update-oriented. In this paper; we discuss thetechnical and political challenges that these various enterprise applications face whenconsidering cloud deployment. In addition; a requirement for quality-of-service (QoS)guarantees will generate additional disruptive issues. In some circumstances; achievinggood DBMS performance on current cloud architectures and future hardware technologieswill be non-trivial. In summary; there is a difficult road ahead for enterprise database …,Cloud Engineering (IC2E); 2014 IEEE International Conference on,2014,10
Pegasus and DAGMan From Concept to Execution: Mapping Scientific Workflows onto Today's Cyberinfrastructure.,Ewa Deelman; Miron Livny; Gaurang Mehta; Andrew Pavlo; Gurmeet Singh; Mei-Hui Su; Karan Vahi; R Kent Wenger,Abstract: In this chapter we describe an end-to-end workflow management system thatenables scientists to describe their large-scale analysis in abstract terms; then maps andexecutes the workflows in an efficient and reliable manner on distributed resources. Wedescribe Pegasus and DAGMan and various workflow restructuring and optimizations theyperform and demonstrate the scalability and reliability of the approach using applicationsfrom astronomy; gravitational-wave physics; and earthquake science.,High Performance Computing Workshop,2006,9
A parent-centered radial layout algorithm for interactive graph visualization and animation,Andrew Pavlo; Christopher Homan; Jonathan Schull,Abstract: We have developed (1) a graph visualization system that allows users to exploregraphs by viewing them as a succession of spanning trees selected interactively;(2) a radialgraph layout algorithm; and (3) an animation algorithm that generates meaningfulvisualizations and smooth transitions between graphs while minimizing edge crossingsduring transitions and in static layouts. Our system is similar to the radial layout system ofYee et al.(2001); but differs primarily in that each node is positioned on a coordinate systemcentered on its own parent rather than on a single coordinate system for all nodes. Oursystem is thus easy to define recursively and lends itself to parallelization. It also guaranteesthat layouts have many nice properties; such as: it guarantees certain edges never crossduring an animation.,arXiv preprint cs/0606007,2006,8
Larger-than-memory data management on modern storage hardware for in-memory OLTP database systems,Lin Ma; Joy Arulraj; Sam Zhao; Andrew Pavlo; Subramanya R Dulloor; Michael J Giardino; Jeff Parkhurst; Jason L Gardner; Kshitij Doshi; Stanley Zdonik,Abstract In-memory database management systems (DBMSs) outperform disk-orientedsystems for on-line transaction processing (OLTP) workloads. But this improvedperformance is only achievable when the database is smaller than the amount of physicalmemory available in the system. To overcome this limitation; some in-memory DBMSs canmove cold data out of volatile DRAM to secondary storage. Such data appears as if it residesin memory with the rest of the database even though it does not. Although there have beenseveral implementations proposed for this type of cold data storage; there has not been athorough evaluation of the design decisions in implementing this technique; such as policiesfor when to evict tuples and how to bring them back when they are needed. These choicesare further complicated by the varying performance characteristics of different storage …,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,7
Clay: fine-grained adaptive partitioning for general database schemas,Marco Serafini; Rebecca Taft; Aaron J Elmore; Andrew Pavlo; Ashraf Aboulnaga; Michael Stonebraker,Abstract Transaction processing database management systems (DBMSs) are critical fortoday's data-intensive applications because they enable an organization to quickly ingestand query new information. Many of these applications exceed the capabilities of a singleserver; and thus their database has to be deployed in a distributed DBMS. The key factoraffecting such a system's performance is how the database is partitioned. If the database ispartitioned incorrectly; the number of distributed transactions can be high. Thesetransactions have to synchronize their operations over the network; which is considerablyslower and leads to poor performance. Previous work on elastic database repartitioning hasfocused on a certain class of applications whose database schema can be represented in ahierarchical tree structure. But many applications cannot be partitioned in this manner …,Proceedings of the VLDB Endowment,2016,5
Reducing replication bandwidth for distributed document databases,Lianghong Xu; Andrew Pavlo; Sudipta Sengupta; Jin Li; Gregory R Ganger,Abstract With the rise of large-scale; Web-based applications; users are increasinglyadopting a new class of document-oriented database management systems (DBMSs) thatallow for rapid prototyping while also achieving scalable performance. Like for otherdistributed storage systems; replication is important for document DBMSs in order toguarantee availability. The network bandwidth required to keep replicas synchronized isexpensive and is often a performance bottleneck. As such; there is a strong need to reducethe replication bandwidth; especially for geo-replication scenarios where wide-area network(WAN) bandwidth is limited. This paper presents a deduplication system called sDedup thatreduces the amount of data transferred over the network for replicated document DBMSs.sDedup uses similarity-based deduplication to remove redundancy in replication data by …,Proceedings of the Sixth ACM Symposium on Cloud Computing,2015,5
On scalable transaction execution in partitioned main memory database management systems,Andrew Pavlo,*,*,2014,5
Interactive; tree-based graph visualization,Andrew Pavlo,Abstract We introduce an interactive graph visualization scheme that allows users to exploregraphs by viewing them as a sequence of spanning trees; rather than the entire graph all atonce. The user determines which spanning trees are displayed by selecting a vertex fromthe graph to be the root. Our main contributions are a graph drawing algorithm thatgenerates meaningful representations of graphs using extracted spanning trees; and agraph animation algorithm for creating smooth; continuous transitions between graphdrawings. We conduct experiments to measure how well our algorithms visualize graphsand compare them to another visualization scheme.,*,2006,5
Relaxed operator fusion for in-memory databases: making compilation; vectorization; and prefetching work together at last,Prashanth Menon; Todd C Mowry; Andrew Pavlo,Abstract In-memory database management systems (DBMSs) are a key component ofmodern on-line analytic processing (OLAP) applications; since they provide low-latencyaccess to large volumes of data. Because disk accesses are no longer the principlebottleneck in such systems; the focus in designing query execution engines has shifted tooptimizing CPU performance. Recent systems have revived an older technique of using just-in-time (JIT) compilation to execute queries as native code instead of interpreting a plan. Thestate-of-the-art in query compilation is to fuse operators together in a query plan to minimizematerialization overhead by passing tuples efficiently between operators. Our empiricalanalysis shows; however; that more tactful materialization yields better performance. Wepresent a query processing model called" relaxed operator fusion" that allows the DBMS …,Proceedings of the VLDB Endowment,2017,3
Main Memory Database Systems,Franz Faerber; Alfons Kemper; Per-Åke Larson; Justin Levandoski; Thomas Neumann; Andrew Pavlo,Abstract This article provides an overview of recent developments in mainmemory databasesystems. With growing memory sizes and memory prices dropping by a factor of 10 every 5years; data having a “primary home” in memory is now a reality. Main-memory databaseseschew many of the traditional architectural pillars of relational database systems thatoptimized for disk-resident data. The result of these memory-optimized designs are systemsthat feature several innovative approaches to fundamental issues (eg; concurrency control;query processing) that achieve orders of magnitude performance improvements overtraditional designs. Our survey covers five main issues and architectural choices that need tobe made when building a high performance main-memory optimized database: dataorganization and storage; indexing; concurrency control; durability and recovery …,Foundations and Trends® in Databases,2017,2
Emerging hardware trends in large-scale transaction processing,Andrew Pavlo,It wasn't obvious right away to some that a memory-oriented architecture was the way to go forscalable OLTP applications. In the early days of VoltDB (the commercial implementation ofH-Store); customers were uncomfortable with the idea of stor- ing your entire database in volatileDRAM. I visited PayPal with others in 2009 to talk to them about VoltDB; and I remember thattheir senior manage- ment was unnerved by the idea of a DBMS that didn't store all physicalchanges to tuples immedi- ately on disk. The prevailing conventional wisdom has obviouslychanged; and now many mission- critical applications use VoltDB. Since then; several othermemory-oriented systems are now available; including MemSQL; SAP HANA;2 and MicrosoftHekaton.3 Other notable in-memory academic sys- tems that came along after H- Store includeShore- MT;4 HyPer;5 and Silo.6 In my opinion; the big challenges in main memory …,IEEE Internet Computing,2015,2
MapReduce and parallel DBMSs,Michael Stonebraker; Daniel Abadi; David J DeWitt; Sam Madden; Erik Paulson; Andrew Pavlo; Alexander Rasin,*,Communications of the ACM,2010,2
OLTP Benchmarks,Carlo Curino; Djellel E Difallah; Andy Pavlo; Philippe Cudre-Maroux; Evan Jones; Yang Zhang; Samuel Madden,*,*,*,2
Research for practice: distributed consensus and implications of NVM on database management systems,Peter Bailis; Camille Fournier; Joy Arulraj; Andrew Pavlo,Research for Practice combines the resources of the ACM Digital Library; the largestcollection of computer science research in the world; with the expertise of the ACMmembership. In every RfP column two experts share a short; curated selection of papers ona concentrated; practically oriented topic.,Communications of the ACM,2016,1
Tastes Great; Less Filling: Low-Impact OLAP MapReduce Queries on High-Performance OLTP Systems,Xin Jia; Andrew Pavlo; Stan Zdonik,ABSTRACT The previous decade saw the rise of separate; dedicated databasemanagement systems (DBMS) for online transaction processing (OLTP) and onlineanalytical processing (OLAP) workloads [3]. The former are focused on executing short-lived; small-footprint transactions with high throughput and strong consistency guarantees.OLAP DBMSs typically target longer running and more complex queries that examine thedatabase after it is offloaded from the front-end OLTP DBMS. For many; the latencyoverhead of transferring data between these two systems; as well as their administrativecosts; is too onerous. A burgeoning alternative is to use a hybrid approach where an OLTPsystem is able execute OLAP-style queries alongside the transactional workload [1]. Thisprovides users the ability to execute business intelligence and other analytical queries in …,Tiny Transactions on Computer Science,2012,1
Graffiti networks: A subversive; internet-scale file sharing model,Andrew Pavlo; Ning Shi,Abstract: The proliferation of peer-to-peer (P2P) file sharing protocols is due to their efficientand scalable methods for data dissemination to numerous users. But many of thesenetworks have no provisions to provide users with long term access to files after the initialinterest has diminished; nor are they able to guarantee protection for users from maliciousclients that wish to implicate them in incriminating activities. As such; users may turn tosupplementary measures for storing and transferring data in P2P systems. We present anew file sharing paradigm; called a Graffiti Network; which allows peers to harness thepotentially unlimited storage of the Internet as a third-party intermediary. Our keycontributions in this paper are (1) an overview of a distributed system based on this newthreat model and (2) a measurement of its viability through a one-year deployment study …,arXiv preprint arXiv:1101.0350,2011,1
Smoother transitions between breadth-first-spanning-tree-based drawings,Christopher Homan; Andrew Pavlo; Jonathan Schull,We demonstrate a collection of techniques that seek to make the transition be- tween drawingsbased on two topologically distinct spanning trees of the same graph as clear as possible. AsHerman; Melançon; and Marshall note [HMM00]; one way to draw a large graph is to extracta spanning tree from it; use a tree layout algorithm [CK95; Ead92; RT81; II90; TM02;GADM04; LY05] to draw the spanning tree; and then add back the graph edges not includedin the spanning tree. The problem with this approach is that the drawings tend to favor the edgesthat are part of the spanning tree; even though they may be no more important in the underlyingstructure than non-spanning tree edges. One way of dealing with this problem is to facilitate explorationof multiple spanning trees. Yee et al. [YFDH01] describe a system that produces layouts basedon Eades' radial layout algorithm [Ead92] and lets users interactively select a new node …,International Symposium on Graph Drawing,2006,1
What Are We Doing With Our Lives?: Nobody Cares About Our Concurrency Control Research,Andrew Pavlo,Abstract Most of the academic papers on concurrency control published in the last five yearshave assumed the following two design decisions:(1) applications execute transactions withserializable isolation and (2) applications execute most (if not all) of their transactions usingstored procedures. But results from a recent survey of database administrators indicates thatthese assumptions are not realistic. This survey includes both legacy deployments wherethe cost of changing the application to use either serializable isolation or stored proceduresis not feasible; as well as new" greenfield" projects that not encumbered by prior constraints.As such; the research produced by our community is not helping people with their real-worldsystems and thus is essentially irrelevant. I know this because I am guilty of writing thesepapers too. In this talk/denouncement; I will descend from my ivory tower and argue that …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Online Deduplication for Databases,Lianghong Xu; Andrew Pavlo; Sudipta Sengupta; Gregory R Ganger,Abstract dbDedup is a similarity-based deduplication scheme for on-line databasemanagement systems (DBMSs). Beyond block-level compression of individual databasepages or operation log (oplog) messages; as used in today's DBMSs; dbDedup uses byte-level delta encoding of individual records within the database to achieve greater savings.dbDedup's single-pass encoding method can be integrated into the storage and loggingcomponents of a DBMS to provide two benefits:(1) reduced size of data stored on diskbeyond what traditional compression schemes provide; and (2) reduced amount of datatransmitted over the network for replication services. To evaluate our work; we implementeddbDedup in a distributed NoSQL DBMS and analyzed its properties using four real datasets.Our results show that dbDedup achieves up to 37x reduction in the storage size and …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Data Management on New Hardware,Spyros Blanas; Rajesh Bordawekar; Tirthankar Lahiri; Justin Levandoski; Andrew Pavlo,The International Workshop on Accelerating Analytics and Data Management SystemsUsing Modern Processor and Storage Architectures (ADMS) and the International Workshopon In-Memory Data Management (IMDM) were held jointly this year. The objective of thisworkshop is to investigate opportunities in accelerating analytics/data management systemsand workloads (which include traditional OLTP; data warehousing/OLAP; ETL;streaming/real-time; business analytics; and XML/RDF processing) running in memory-onlyenvironments; using processors (eg; commodity and specialized multi-core; GPUs; andFPGAs); storage systems (eg; storage-class memories like SSDs and phase-changememory); and hybrid programming models such as CUDA; OpenCL; and OpenACC. Theworkshop hopes to explore the interplay between overall system design; core algorithms …,*,2017,*
Can We Make This Easier?,Joy Arulraj; Andrew Pavlo,Finally we come to the question; have we built ourselves into unnecessary complexity bytaking it on faith that Paxos and its close cousins are the only way to implement consensus?What if there was an algorithm that we could also show to be correct but was designed to beeasier for people to comprehend and implement correctly? Raft is a consensus algorithmwritten for managing a replicated log but designed with the goal of making the algorithmitself more understandable than Paxos. This is done both by decomposing the problem intopieces that can be implemented and understood independently and by reducing the numberof states that are valid for the system to hold. Consensus is decomposed into issues ofleader election; log replication; and safety. Leader election uses randomized electiontimeouts to reduce the likelihood of two candidates for leader splitting the vote and …,COMMUNICATIONS OF THE ACM,2016,*
Similarity-based Deduplication for Databases,Lianghong Xu; Andrew Pavlo; Sudipta Sengupta; Gregory R Ganger,The rate of data growth outpaces the decline of hardware costs. Database compression isone solution to this problem. For database storage; in addition to space saving; compressionhelps reduce the number of disk I/Os and improve performance; because queried data fits infewer pages. For distributed databases replicated across geographical regions; there is alsoa strong need to reduce the amount of data transfer used to keep replicas in sync. The mostwidely used approach for data reduction in DBMSs is block-level compression [29; 36; 44;41; 3; 16]. Although this method is simple and effective; it fails to address redundancy acrossblocks and therefore leaves significant room for improvement for many applications (eg; dueto app-level versioning in wikis or partial record copying in message boards). Deduplication(dedup) has become popular in backup systems for eliminating duplicate content across …,*,2016,*
BenchPress: Dynamic Workload Control in the OLTP-Bench Testbed,Dana Van Aken; Djellel E Difallah; Andrew Pavlo; Carlo Curino; Philippe Cudré-Mauroux,Abstract Benchmarking is an essential activity when choosing database products; tuningsystems; and understanding the trade-offs of the underlying engines. But the workloadsavailable for this effort are often restrictive and non-representative of the ever changingrequirements of the modern database applications. We recently introduced OLTP-Bench; anextensible testbed for benchmarking relational databases that is bundled with 15 workloads.The key features that set this framework apart is its ability to tightly control the request rateand dynamically change the transaction mixture. This allows an administrator to composecomplex execution targets that recreate real system loads; and opens the doors to newresearch directions involving tuning for special execution patterns and multi-tenancy. In thisdemonstration; we highlight OLTP-Bench's important features through the BenchPress …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,*
In Memory Data Management and Analysis,Arun Jagatheesan; Justin Levandoski; Thomas Neumann; Andrew Pavlo,Over the last 30 years; memory prices have been dropping by a factor of 10 every 5 years.The number of I/O operations per second (IOPS) in DRAM is far greater than other storagemedia such as hard disks and SSDs. DRAM is readily available in the market at better pricepoint in comparison to DRAM-alternatives. These trends make DRAM a better storage mediafor latency-sensitive data management applications. For example; mobile applicationsrequire low-latency responses to user requests. The “hot set” of large transactionalworkloads fit comfortably in memory. Many largescale web applications such as Facebookand Amazon manage most of their active data in main memory. With the emergence of sucha diverse pool of latency-sensitive applications coupled with dropping DRAM prices; it istimely to explore main-memory optimized data management platforms. In addition; almost …,*,2015,*
The SEATS airline ticketing systems benchmark. h ttp,Michael Stonebraker; Andrew Pavlo,*,hstore. cs. brown. edu/projects/seats,2015,*
Graffiti Networks,Andrew Pavlo,Page 1. Graffiti Networks: A Subversive; Internet-Scale File Sharing Model Andrew Pavlo DC401 –Rhode Island Defcon Group October 12; 2009 Page 2. Co-Authors ▪ Ning Shi (Brown) October12; 2009 Page 3. Outline ▪ Open BitTorrent Problems ▪ A Subversive Solution ▪ ExperimentalEvaluation ▪ Aftermath ▪ Lessons Learned ▪ Concluding Remarks October 12; 2009 Page 4. LeechBitTorrent Overview October 12; 2009 Tracker Seed Leech Seed Seed Page 5. Two months later…October 12; 2009 Tracker Seed Seed Seed Leech ? Page 6. Problem Statement ▪ How to providea means for peers to retrieve data long after seeds have disappeared? ▪ Requirements: ▪ Mustprovide public access. ▪ Must not allow malicious users to destroy data. ▪ Must allow all content. ▪Must be free! October 12; 2009 Page 7. Case Studies ▪ Average time a peer remains connectedto swarm after finished downloading = 7 hours …,*,2009,*
A One-Year Study of Subversive Data Storage on Websites using the Graffiti Network System,Andrew Pavlo; Ning Shi,*,*,*,*
Taurus: A Parallel Transaction Recovery Method Based on Fine-Granularity Dependency Tracking,Xiangyao Yu; Siye Zhu; Justin Kaashoek; Andrew Pavlo; Srinivas Devadas,ABSTRACT Logging is crucial to performance in modern multicore main-memory databasemanagement systems (DBMSs). Traditional data logging (ARIES) and command loggingalgorithms enforce a sequential order among log records using a global log sequencenumber (LSN). Log flushing and recovery after a crash are both performed in the LSN order.This serialization of transaction logging and recovery can limit the system performance athigh core count. In this paper; we propose Taurus to break the LSN abstraction and enableparallel logging and recovery by tracking fine-grained dependencies among transactions.The dependency tracking lends Taurus three salient features.(1) Taurus decouples thetransaction logging order with commit order and allows transactions to be flushed topersistent storage in parallel independently. Transactions that are persistent before …,*,*,*
Intelligent Data Clustering for Cold-Data Storage in Main-Memory Databases,Atreyee Maiti; Andrew Pavlo,• Main-memory databases optimal for OLTP workloads• Restrained by the requirement thatentire dataset fit in RAM• Techniques like Anti-caching and OS paging help in overcomingthis• The idea is to identify the cold data and evict it out to secondary storage as the data sizegrows,memory,*,*
IC2E 2014,Michael Stonebraker; Andrew Pavlo; Rebecca Taft; Michael L Brodie,Enterprise Database Applications and the Cloud: A Difficult Road Ahead ......................................................................1 Michael Stonebraker; Andrew Pavlo; Rebecca Taft; and Michael L. Brodie… Agentless Cloud-Wide Streaming of Guest File System Updates ...................................................................................7 Wolfgang Richter; Canturk Isci; Benjamin Gilbert; Jan Harkes; VasanthBala; and Mahadev Satyanarayanan … Preservation of Security Configurations in the Cloud....................................................................................................17 Arash Eghtesadi; Yosr Jarraya; MouradDebbabi; and Makan Pourzandi … Benchmarking the Performance Impact of Transport LayerSecurity in Cloud Database Systems .........................................................................................................................................................................27 Steffen Müller; David Bermbach; Stefan Tai; and FrankPallas … Silver Lining: Enforcing Secure Information Flow at the Cloud Edge …,*,*,*
Graffiti Networks: A Subversive; Internet-Scale Peer-to-Peer File Sharing Model,Andrew Pavlo; Ning Shi,*,*,*,*
