Tensor decompositions for learning latent variable models,Animashree Anandkumar; Rong Ge; Daniel Hsu; Sham M Kakade; Matus Telgarsky,Abstract This work considers a computationally and statistically efficient parameterestimation method for a wide class of latent variable models—including Gaussian mixturemodels; hidden Markov models; and latent Dirichlet allocation—which exploits a certaintensor structure in their low-order observable moments (typically; of second-and third-order).Specifically; parameter estimation is reduced to the problem of extracting a certain(orthogonal) decomposition of a symmetric tensor derived from the moments; thisdecomposition can be viewed as a natural generalization of the singular valuedecomposition for matrices. Although tensor decompositions are generally intractable tocompute; the decomposition of these specially structured tensors can be efficiently obtainedby a variety of approaches; including power iterations and maximization approaches …,The Journal of Machine Learning Research,2014,464
Computing a nonnegative matrix factorization--provably,Sanjeev Arora; Rong Ge; Ravindran Kannan; Ankur Moitra,Abstract The Nonnegative Matrix Factorization (NMF) problem has a rich history spanningquantum mechanics; probability theory; data analysis; polyhedral combinatorics;communication complexity; demography; chemometrics; etc. In the past decade NMF hasbecome enormously popular in machine learning; where the factorization is computed usinga variety of local search heuristics. Vavasis recently proved that this problem is NP-complete. We initiate a study of when this problem is solvable in polynomial time. Consider anonnegative mxn matrix $ M $ and a target inner-dimension r. Our results are the following:-We give a polynomial-time algorithm for exact and approximate NMF for every constant r.Indeed NMF is most interesting in applications precisely when r is small. We complementthis with a hardness result; that if exact NMF can be solved in time (nm) o (r); 3-SAT has a …,Proceedings of the forty-fourth annual ACM symposium on Theory of computing,2012,244
Learning topic models--going beyond SVD,Sanjeev Arora; Rong Ge; Ankur Moitra,Topic Modeling is an approach used for automatic comprehension and classification of datain a variety of settings; and perhaps the canonical application is in uncovering thematicstructure in a corpus of documents. A number of foundational works both in machinelearning and in theory have suggested a probabilistic model for documents; wherebydocuments arise as a convex combination of (ie distribution on) a small number of topicvectors; each topic vector being a distribution on words (ie a vector of word-frequencies).Similar models have since been used in a variety of application areas; the Latent DirichletAllocation or LDA model of Blei et al. is especially popular. Theoretical studies of topicmodeling focus on learning the model's parameters assuming the data is actually generatedfrom it. Existing approaches for the most part rely on Singular Value Decomposition (SVD) …,Foundations of Computer Science (FOCS); 2012 IEEE 53rd Annual Symposium on,2012,239
A practical algorithm for topic modeling with provable guarantees,Sanjeev Arora; Rong Ge; Yonatan Halpern; David Mimno; Ankur Moitra; David Sontag; Yichen Wu; Michael Zhu,Abstract Topic models provide a useful method for dimensionality reduction and exploratorydata analysis in large text corpora. Most approaches to topic model learning have beenbased on a maximum likelihood objective. Efficient algorithms exist that attempt toapproximate this objective; but they have no provable guarantees. Recently; algorithmshave been introduced that provide provable bounds; but these algorithms are not practicalbecause they are inefficient and not robust to violations of model assumptions. In this paperwe present an algorithm for learning topic models that is both provable and practical. Thealgorithm produces results comparable to the best MCMC implementations while runningorders of magnitude faster.,International Conference on Machine Learning,2013,206
Escaping from saddle points—online stochastic gradient for tensor decomposition,Rong Ge; Furong Huang; Chi Jin; Yang Yuan,Abstract We analyze stochastic gradient descent for optimizing non-convex functions. Inmany cases for non-convex functions the goal is to find a reasonable local minimum; and themain concern is that gradient updates are trapped in\em saddle points. In this paper weidentify\em strict saddle property for non-convex problem that allows for efficientoptimization. Using this property we show that from an\em arbitrary starting point; stochasticgradient descent converges to a local minimum in a polynomial number of iterations. To thebest of our knowledge this is the first work that gives\em global convergence guarantees forstochastic gradient descent on non-convex functions with exponentially many local minimaand saddle points. Our analysis can be applied to orthogonal tensor decomposition; which iswidely used in learning a rich class of latent variable models. We propose a new …,Conference on Learning Theory,2015,205
A tensor spectral approach to learning mixed membership community models,Animashree Anandkumar; Rong Ge; Daniel Hsu; Sham Kakade,Abstract Modeling community formation and detecting hidden communities in networks is awell studied problem. However; theoretical analysis of community detection has been mostlylimited to models with non-overlapping communities such as the stochastic block model. Inthis paper; we remove this restriction; and consider a family of probabilistic network modelswith overlapping communities; termed as the mixed membership Dirichlet model; firstintroduced in Aioroldi et. al (2008). This model allows for nodes to have fractionalmemberships in multiple communities and assumes that the community memberships aredrawn from a Dirichlet distribution. We propose a unified approach to learning these modelsvia a tensor spectral decomposition method. Our estimator is based on low-order momenttensor of the observed network; consisting of 3-star counts. Our learning method is fast …,Conference on Learning Theory,2013,164
Provable bounds for learning some deep representations,Sanjeev Arora; Aditya Bhaskara; Rong Ge; Tengyu Ma,Abstract We give algorithms with provable guarantees that learn a class of deep nets in thegenerative model view popularized by Hinton and others. Our generative model is an nnode multilayer network that has degree at most nγ for some γ< 1 and each edge has arandom edge weight in [− 1; 1]. Our algorithm learns almost all networks in this class withpolynomial running time. The sample complexity is quadratic or cubic depending upon thedetails of the model. The algorithm uses layerwise learning. It is based upon a novel idea ofobserving correlations among features and using these to infer the underlying edgestructure via a global graph recovery procedure. The analysis of the algorithm revealsinteresting structure of neural nets with random edge weights.,International Conference on Machine Learning,2014,152
New algorithms for learning in presence of errors,Sanjeev Arora; Rong Ge,Abstract We give new algorithms for a variety of randomly-generated instances ofcomputational problems using a linearization technique that reduces to solving a system oflinear equations. These algorithms are derived in the context of learning with structurednoise; a notion introduced in this paper. This notion is best illustrated with the learningparities with noise (LPN) problem—well-studied in learning theory and cryptography. In thestandard version; we have access to an oracle that; each time we press a button; returns arandom vector a∈GF(2)^n together with a bit b∈GF(2) that was computed as a· u+ η; whereu∈GF(2)^n is a secret vector; and η∈GF(2) is a noise bit that is 1 with some probability p.Say p= 1/3. The goal is to recover u. This task is conjectured to be intractable. In thestructured noise setting we introduce a slight (?) variation of the model: upon pressing a …,Automata; Languages and Programming,2011,141
New algorithms for learning incoherent and overcomplete dictionaries,Sanjeev Arora; Rong Ge; Ankur Moitra,Abstract In\em sparse recovery we are given a matrix A∈\mathbbR^ n\times m (“thedictionary”) and a vector of the form AX where X is\em sparse; and the goal is to recover X.This is a central notion in signal processing; statistics and machine learning. But inapplications such as\em sparse coding; edge detection; compression and super resolution;the dictionary A is unknown and has to be learned from random examples of the form Y= AXwhere X is drawn from an appropriate distribution—this is the\em dictionary learningproblem. In most settings; A is\em overcomplete: it has more columns than rows. This paperpresents a polynomial-time algorithm for learning overcomplete dictionaries; the onlypreviously known algorithm with provable guarantees is the recent work of Spielman etal.(2012) who who gave an algorithm for the undercomplete case; which is rarely the …,Conference on Learning Theory,2014,118
Matrix completion has no spurious local minimum,Rong Ge; Jason D Lee; Tengyu Ma,Abstract Matrix completion is a basic machine learning problem that has wide applications;especially in collaborative filtering and recommender systems. Simple non-convexoptimization algorithms are popular and effective in practice. Despite recent progress inproving various non-convex algorithms converge from a good initial point; it remains unclearwhy random or arbitrary initialization suffices in practice. We prove that the commonly usednon-convex objective function for matrix completion has no spurious local minima\---all localminima must also be global. Therefore; many popular optimization algorithms such as(stochastic) gradient descent can provably solve matrix completion with\textit {arbitrary}initialization in polynomial time.,Advances in Neural Information Processing Systems,2016,115
Computational complexity and information asymmetry in financial products,Sanjeev Arora; Boaz Barak; Markus Brunnermeier; Rong Ge,A financial derivative is a contract entered between two parties; in which they agree toexchange payments based on events or on the performance of one or more underlyingassets—independently of whether they own or control the underlying assets (eg; the DOWJones index falling below 10;000). Securitization of cash flows using derivatives transformedthe financial industry over the last three decades. However; mispricing of these derivatives isbelieved to have contributed to the financial crash of 2008 (see eg Brunnermeier8 andCoval et al. 10). There are also suggestions that derivatives were deliberately misused. InSpring 2010 there was a famous allegation about investment bank Goldman Sachs'sAbacus derivative. The security and exchange commission alleged that Goldman Sachscollaborated with shortseller Paulson to select particularly bad mortgages as the …,Communications of the ACM,2011,90
Simple; efficient; and neural algorithms for sparse coding,Sanjeev Arora; Rong Ge; Tengyu Ma; Ankur Moitra,Abstract Sparse coding is a basic task in many fields including signal processing;neuroscience and machine learning where the goal is to learn a basis that enables a sparserepresentation of a given set of data; if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternatingminimization. Recent work has resulted in several algorithms for sparse coding withprovable guarantees; but somewhat surprisingly these are outperformed by the simplealternating minimization heuristics. Here we give a general framework for understandingalternating minimization which we leverage to analyze existing heuristics and to design newones also with provable guarantees. Some of these algorithms seem implementable onsimple neural architectures; which was the original motivation of Olshausen and Field in …,Conference on Learning Theory,2015,86
Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1 $ Updates,Animashree Anandkumar; Rong Ge; Majid Janzamin,Abstract: In this paper; we provide local and global convergence guarantees for recoveringCP (Candecomp/Parafac) tensor decomposition. The main step of the proposed algorithm isa simple alternating rank-$1 $ update which is the alternating version of the tensor poweriteration adapted for asymmetric tensors. Local convergence guarantees are established forthird order tensors of rank $ k $ in $ d $ dimensions; when $ k= o\bigl (d^{1.5}\bigr) $ and thetensor components are incoherent. Thus; we can recover overcomplete tensordecomposition. We also strengthen the results to global convergence guarantees understricter rank condition $ k\le\beta d $(for arbitrary constant $\beta> 1$) through a simpleinitialization procedure where the algorithm is initialized by top singular vectors of randomtensor slices. Furthermore; the approximate local convergence guarantees for $ p $-th …,arXiv preprint arXiv:1402.5180,2014,64
Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization,Roy Frostig; Rong Ge; Sham Kakade; Aaron Sidford,Abstract We develop a family of accelerated stochastic algorithms that optimize sums ofconvex functions. Our algorithms improve upon the fastest running time for empirical riskminimization (ERM); and in particular linear least-squares regression; across a wide rangeof problem settings. To achieve this; we establish a framework; based on the classicalproximal point algorithm; useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically; we demonstrate that the resulting algorithms exhibit notions ofstability that are advantageous in practice. Both in theory and in practice; the providedalgorithms reap the computational benefits of adding a large strongly convex regularizationterm; without incurring a corresponding bias to the original ERM problem.,International Conference on Machine Learning,2015,63
How to escape saddle points efficiently,Chi Jin; Rong Ge; Praneeth Netrapalli; Sham M Kakade; Michael I Jordan,Abstract: This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically ondimension (ie; it is almost" dimension-free"). The convergence rate of this procedurematches the well-known convergence rate of gradient descent to first-order stationary points;up to log factors. When all saddle points are non-degenerate; all second-order stationarypoints are local minima; and our result thus shows that perturbed gradient descent canescape saddle points almost for free. Our results can be directly applied to many machinelearning applications; including deep learning. As a particular concrete example of such anapplication; we show that our results can be used directly to establish sharp globalconvergence rates for matrix factorization. Our results rely on a novel characterization of …,arXiv preprint arXiv:1703.00887,2017,55
Generalization and equilibrium in generative adversarial nets (gans),Sanjeev Arora; Rong Ge; Yingyu Liang; Tengyu Ma; Yi Zhang,Abstract: This paper makes progress on several open theoretical issues related toGenerative Adversarial Networks. A definition is provided for what it means for the training togeneralize; and it is shown that generalization is not guaranteed for the popular distancesbetween distributions such as Jensen-Shannon or Wasserstein. We introduce a new metriccalled neural net distance for which generalization does occur. We also show that anapproximate pure equilibrium in the 2-player game exists for a natural training objective(Wasserstein). Showing such a result has been an open problem (for any training objective).,arXiv preprint arXiv:1703.00573,2017,55
Provable ICA with Unknown Gaussian Noise; and Implications for Gaussian Mixtures and Autoencoders,Sanjeev Arora; Rong Ge; Ankur Moitra; Sushant Sachdeva,Abstract We present a new algorithm for Independent Component Analysis (ICA) which hasprovable performance guarantees. In particular; suppose we are given samples of the form $y= Ax+\eta $ where $ A $ is an unknown $ n\times n $ matrix and $ x $ is chosen uniformly atrandom from $\{+ 1;-1\}^ n $; $\eta $ is an $ n $-dimensional Gaussian random variable withunknown covariance $\Sigma $: We give an algorithm that provable recovers $ A $ and$\Sigma $ up to an additive $\epsilon $ whose running time and sample complexity arepolynomial in $ n $ and $1/\epsilon $. To accomplish this; we introduce a novel``quasi-whitening''step that may be useful in other contexts in which the covariance of Gaussiannoise is not known in advance. We also give a general framework for finding all local optimaof a function (given an oracle for approximately finding just one) and this is a crucial step …,Arxiv preprint arXiv:1206.5349,2012,55
Finding overlapping communities in social networks: toward a rigorous approach,Sanjeev Arora; Rong Ge; Sushant Sachdeva; Grant Schoenebeck,Abstract A community in a social network is usually understood to be a group of nodes moredensely connected with each other than with the rest of the network. This is an importantconcept in most domains where networks arise: social; technological; biological; etc. Formany years algorithms for finding communities implicitly assumed communities arenonoverlapping (leading to use of clustering-based approaches) but there is increasinginterest in finding overlapping communities. A barrier to finding communities is that thesolution concept is often defined in terms of an NP-complete problem such as Clique orHierarchical Clustering. This paper seeks to initiate a rigorous approach to the problem offinding overlapping communities; where" rigorous" means that we clearly state thefollowing:(a) the object sought by our algorithm (b) the assumptions about the underlying …,Proceedings of the 13th ACM Conference on Electronic Commerce,2012,48
Competing with the empirical risk minimizer in a single pass,Roy Frostig; Rong Ge; Sham M Kakade; Aaron Sidford,Abstract In many estimation problems; eg linear and logistic regression; we wish to minimizean unknown objective given only unbiased samples of the objective function. Furthermore;we aim to achieve this using as few samples as possible. In the absence of computationalconstraints; the minimizer of a sample average of observed data–commonly referred to aseither the empirical risk minimizer (ERM) or the M-estimator–is widely regarded as theestimation strategy of choice due to its desirable statistical convergence properties. Our goalin this work is to perform as well as the ERM; on\emphevery problem; while minimizing theuse of computational resources such as running time and space usage. We provide a simplestreaming algorithm which; under standard regularity assumptions on the underlyingproblem; enjoys the following properties:\beginenumerate\item The algorithm can be …,Conference on learning theory,2015,41
No spurious local minima in nonconvex low rank problems: A unified geometric analysis,Rong Ge; Chi Jin; Yi Zheng,Abstract: In this paper we develop a new framework that captures the common landscapeunderlying the common non-convex low-rank matrix problems including matrix sensing;matrix completion and robust PCA. In particular; we show for all above problems (includingasymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddlepoints exists. These results explain why simple algorithms such as stochastic gradientdescent have global converge; and efficiently optimize these non-convex objective functionsin practice. Our framework connects and simplifies the existing analyses on optimizationlandscapes for matrix sensing and symmetric matrix completion. The framework naturallyleads to new results for asymmetric matrix completion and robust PCA. Subjects: Learning(cs. LG); Optimization and Control (math. OC); Machine Learning (stat. ML) Cite as: arXiv …,arXiv preprint arXiv:1704.00708,2017,34
Efficient approaches for escaping higher order saddle points in non-convex optimization,Animashree Anandkumar; Rong Ge,Abstract Local search heuristics for non-convex optimizations are popular in appliedmachine learning. However; in general it is hard to guarantee that such algorithms evenconverge to a local minimum; due to the existence of complicated saddle point structures inhigh dimensions. Many functions have degenerate saddle points such that the first andsecond order derivatives cannot distinguish them with local optima. In this paper we usehigher order derivatives to escape these saddle points: we design the first efficient algorithmguaranteed to converge to a third order local optimum (while existing techniques are at mostsecond order). We also show that it is NP-hard to extend this further to finding fourth orderlocal optima.,Conference on Learning Theory,2016,33
More algorithms for provable dictionary learning,Sanjeev Arora; Aditya Bhaskara; Rong Ge; Tengyu Ma,Abstract: In dictionary learning; also known as sparse coding; the algorithm is given samplesof the form $ y= Ax $ where $ x\in\mathbb {R}^ m $ is an unknown random sparse vector and$ A $ is an unknown dictionary matrix in $\mathbb {R}^{n\times m} $(usually $ m> n $; whichis the overcomplete case). The goal is to learn $ A $ and $ x $. This problem has beenstudied in neuroscience; machine learning; visions; and image processing. In practice it issolved by heuristic algorithms and provable algorithms seemed hard to find. Recently;provable algorithms were found that work if the unknown feature vector $ x $ is $\sqrt {n} $-sparse or even sparser. Spielman et al.\cite {DBLP: journals/jmlr/SpielmanWW12} did this fordictionaries where $ m= n $; Arora et al.\cite {AGM} gave an algorithm for overcomplete ($m> n $) and incoherent matrices $ A $; and Agarwal et al.\cite {DBLP: journals/corr …,arXiv preprint arXiv:1401.0579,2014,32
Learning mixtures of gaussians in high dimensions,Rong Ge; Qingqing Huang; Sham M Kakade,Abstract Efficiently learning mixture of Gaussians is a fundamental problem in statistics andlearning theory. Given samples coming from a random one out of k Gaussian distributions inRn; the learning problem asks to estimate the means and the covariance matrices of theseGaussians. This learning problem arises in many areas ranging from the natural sciences tothe social sciences; and has also found many ma-chine learning applications. Unfortunately;learning mixture of Gaussians is an information theoretically hard problem: in order to learnthe parameters up to a reasonable accuracy; the number of samples required is exponentialin the number of Gaussian components in the worst case. In this work; we show thatprovided we are in high enough dimensions; the class of Gaussian mixtures is learnable inits most general form under a smoothed analysis framework; where the parameters are …,Proceedings of the forty-seventh annual ACM symposium on Theory of computing,2015,29
Decomposing overcomplete 3rd order tensors using sum-of-squares algorithms,Rong Ge; Tengyu Ma,Abstract: Tensor rank and low-rank tensor decompositions have many applications inlearning and complexity theory. Most known algorithms use unfoldings of tensors and canonly handle rank up to $ n^{\lfloor p/2\rfloor} $ for a $ p $-th order tensor in $\mathbb {R}^{n^p} $. Previously no efficient algorithm can decompose 3rd order tensors when the rank issuper-linear in the dimension. Using ideas from sum-of-squares hierarchy; we give the firstquasi-polynomial time algorithm that can decompose a random 3rd order tensordecomposition when the rank is as large as $ n^{3/2}/\textrm {polylog} n $. We also give apolynomial time algorithm for certifying the injective norm of random low rank tensors. Ourtensor decomposition algorithm exploits the relationship between injective norm and thetensor components. The proof relies on interesting tools for decoupling random variables …,arXiv preprint arXiv:1504.05287,2015,26
Learning overcomplete latent variable models through tensor methods,Animashree Anandkumar; Rong Ge; Majid Janzamin,Abstract We provide guarantees for learning latent variable models emphasizing on theovercomplete regime; where the dimensionality of the latent space exceeds the observeddimensionality. In particular; we consider multiview mixtures; ICA; and sparse codingmodels. Our main tool is a new algorithm for tensor decomposition that works in theovercomplete regime. In the semi-supervised setting; we exploit label information to get arough estimate of the model parameters; and then refine it using the tensor method onunlabeled samples. We establish learning guarantees when the number of componentsscales as k= o (d^ p/2); where d is the observed dimension; and p is the order of theobserved moment employed in the tensor method (usually p= 3; 4). In the unsupervisedsetting; a simple initialization algorithm based on SVD of the tensor slices is proposed …,Conference on Learning Theory,2015,21
Analyzing tensor power method dynamics: Applications to learning overcomplete latent variable models,Anima Anandkumar; Rong Ge; Majid Janzamin,Abstract In this paper we provide new guarantees for unsupervised learning of overcompletelatent variable models; where the number of hidden components exceeds the dimensionalityof the observed data. In particular; we consider multi-view mixture models and sphericalGaussian mixtures with random mean vectors. Given the third order moment tensor; welearn the parameters using tensor power iterations. We prove that our algorithm can learnthe model parameters even when the number of hidden components k is significantly largerthan the dimension d up to k= o (d1. 5); and the signal-to-noise ratio we require issignificantly lower than previous results. We present a novel analysis of the dynamics oftensor power iterations; and an efficient characterization of the basin of attraction of thedesired local optima.,CoRR abs/1411.1488,2014,20
Efficient algorithms for large-scale generalized eigenvector computation and canonical correlation analysis,Rong Ge; Chi Jin; Praneeth Netrapalli; Aaron Sidford,Abstract This paper considers the problem of canonical-correlation analysis (CCA) and;more broadly; the generalized eigenvector problem for a pair of symmetric matrices. Theseare two fundamental problems in data analysis and scientific computing with numerousapplications in machine learning and statistics. We provide simple iterative algorithms; withimproved runtimes; for solving these problems that are globally linearly convergent withmoderate dependencies on the condition numbers and eigenvalue gaps of the matricesinvolved. We obtain our results by reducing CCA to the top-k generalized eigenvectorproblem. We solve this problem through a general framework that simply requires black boxaccess to an approximate linear system solver. Instantiating this framework with acceleratedgradient descent we obtain a running time of\order\fracz k\sqrtκρ\log (1/ε)\log\left (kκ/ρ …,International Conference on Machine Learning,2016,16
Sample complexity analysis for learning overcomplete latent variable models through tensor methods,Animashree Anandkumar; Rong Ge; Majid Janzamin,Abstract: We provide guarantees for learning latent variable models emphasizing on theovercomplete regime; where the dimensionality of the latent space can exceed the observeddimensionality. In particular; we consider multiview mixtures; spherical Gaussian mixtures;ICA; and sparse coding models. We provide tight concentration bounds for empiricalmoments through novel covering arguments. We analyze parameter recovery through asimple tensor power update algorithm. In the semi-supervised setting; we exploit the label orprior information to get a rough estimate of the model parameters; and then refine it usingthe tensor method on unlabeled samples. We establish that learning is possible when thenumber of components scales as $ k= o (d^{p/2}) $; where $ d $ is the observed dimension;and $ p $ is the order of the observed moment employed in the tensor method. Our …,arXiv preprint arXiv:1408.0553,2014,13
Analyzing tensor power method dynamics in overcomplete regime,Animashree Anandkumar; Rong Ge; Majid Janzamin,Abstract We present a novel analysis of the dynamics of tensor power iterations in theovercomplete regime where the tensor CP rank is larger than the input dimension. Findingthe CP decomposition of an overcomplete tensor is NP-hard in general. We consider thecase where the tensor components are randomly drawn; and show that the simple poweriteration recovers the components with bounded error under mild initialization conditions.We apply our analysis to unsupervised learning of latent variable models; such as multi-viewmixture models and spherical Gaussian mixtures. Given the third order moment tensor; welearn the parameters using tensor power iterations. We prove it can correctly learn the modelparameters when the number of hidden components k is much larger than the datadimension d; up to k= o (d1. 5). We initialize the power iterations with data samples and …,Journal of Machine Learning Research,2017,12
Learning One-hidden-layer Neural Networks with Landscape Design,Rong Ge; Jason D Lee; Tengyu Ma,Abstract: We consider the problem of learning a one-hidden-layer neural network: weassume the input $ x\in\R^ d $ is from Gaussian distribution and the label $ y= a^\top\sigma(Bx)+\xi $; where $ a $ is a nonnegative vector in $\R^ m $ with $ m\le d $; $ B\in\R^{m\timesd} $ is a full-rank weight matrix; and $\xi $ is a noise vector. We first give an analytic formulafor the population risk of the standard squared loss and demonstrate that it implicitly attemptsto decompose a sequence of low-rank tensors simultaneously.,arXiv preprint arXiv:1711.00501,2017,10
Computing a Nonnegative Matrix Factorization---Provably,Sanjeev Arora; Rong Ge; Ravi Kannan; Ankur Moitra,In the nonnegative matrix factorization (NMF) problem we are given an n*m nonnegativematrix M and an integer r>0. Our goal is to express M as AW; where A and W arenonnegative matrices of size n*r and r*m; respectively. In some applications; it makes senseto ask instead for the product AW to approximate M; ie (approximately) minimize \leftM-AW_F\right\rVert; where \left\lVert\right\rVert_F; denotes the Frobenius norm; we refer to thisas approximate NMF. This problem has a rich history spanning quantum mechanics;probability theory; data analysis; polyhedral combinatorics; communication complexity;demography; chemometrics; etc. In the past decade NMF has become enormously popularin machine learning; where A and W are computed using a variety of local search heuristics.Vavasis recently proved that this problem is NP-complete.(Without the restriction that A …,SIAM Journal on Computing,2016,9
New tools for graph coloring,Sanjeev Arora; Rong Ge,Abstract How to color 3 colorable graphs with few colors is a problem of longstandinginterest. The best polynomial-time algorithm uses n 0.2072 colors. There are no indicationsthat coloring using say O (log n) colors is hard. It has been suggested that SDP hierarchiescould be used to design algorithms that use n ε colors for arbitrarily small ε> 0. We explorethis possibility in this paper and find some cause for optimism. While the case of generalgraphs is till open; we can analyse the Lasserre relaxation for two interesting families ofgraphs. For graphs with low threshold rank (a class of graphs identified in the recent paperof Arora; Barak and Steurer on the unique games problem); Lasserre relaxations can beused to find an independent set of size Ω (n)(ie; progress towards a coloring with O (log n)colors) in n O (D) time; where D is the threshold rank of the graph. This algorithm is …,*,2011,9
On the Optimization Landscape of Tensor Decompositions,Rong Ge; Tengyu Ma,Abstract Non-convex optimization with local search heuristics has been widely used inmachine learning; achieving many state-of-art results. It becomes increasingly important tounderstand why they can work for these NP-hard problems on typical data. The landscape ofmany objective functions in learning has been conjectured to have the geometric propertythat``all local optima are (approximately) global optima''; and thus they can be solvedefficiently by local search algorithms. However; establishing such property can be verydifficult. In this paper; we analyze the optimization landscape of the random over-completetensor decomposition problem; which has many applications in unsupervised leaning;especially in learning latent variable models. In practice; it can be efficiently solved bygradient ascent on a non-convex objective. We show that for any small constant $\epsilon …,Advances in Neural Information Processing Systems,2017,8
Minimal realization problem for hidden markov models,Qingqing Huang; Rong Ge; Sham Kakade; Munther Dahleh,In this paper; we study the problem of finding a minimal order (quasi-) Hidden Markov Modelfor a random process; which is the output process of an unknown stationary HMM of finiteorder. In the main theorem; we show that excluding a measure zero set in the parameterspace of transition and observation probability matrices; both the minimal quasi-HMMrealization and the minimal HMM realization can be efficiently constructed based on the jointprobabilities of length N output strings; for N> max (4 log d (k)+ 1; 3); where d is the size ofthe output alphabet size; and k is the minimal order of the realization. We also aim toconnect the two lines of literature: realization theory of HMMs/automatas; and the recentdevelopment in learning latent variable models with tensor techniques.,Communication; Control; and Computing (Allerton); 2014 52nd Annual Allerton Conference on,2014,8
Provable learning of overcomplete latent variable models: Semi-supervised and unsupervised settings,Animashree Anandkumar; Rong Ge; Majid Janzamin,Abstract We provide guarantees for learning latent variable models emphasizing on theovercomplete regime; where the dimensionality of the latent space can exceed the observeddimensionality. In particular; we consider multiview mixtures; spherical Gaussian mixtures;ICA; and sparse coding models. We provide tight concentration bounds for empiricalmoments through novel covering arguments. We analyze parameter recovery through asimple tensor power update algorithm. In the semi-supervised setting; we exploit the label orprior information to get a rough estimate of the model parameters; and then refine it usingthe tensor method on unlabeled samples. We establish that learning is possible when thenumber of components scales as k= o (dp/2); where d is the observed dimension; and p isthe order of the observed moment employed in the tensor method. Our concentration …,arXiv preprint arXiv:1408.0553,2014,8
Tensor decompositions for learning latent variable models; 2012,Anima Anandkumar; Rong Ge; Daniel Hsu; Sham M Kakade; Matus Telgarsky,*,arXiv preprint arXiv:1210.7559,*,8
Provable learning of noisy-or networks,Sanjeev Arora; Rong Ge; Tengyu Ma; Andrej Risteski,Abstract: Many machine learning applications use latent variable models to explain structurein data; whereby visible variables (= coordinates of the given datapoint) are explained as aprobabilistic function of some hidden variables. Finding parameters with the maximumlikelihood is NP-hard even in very simple settings. In recent years; provably efficientalgorithms were nevertheless developed for models with linear structures: topic models;mixture models; hidden markov models; etc. These algorithms use matrix or tensordecomposition; and make some reasonable assumptions about the parameters of theunderlying model. But matrix or tensor decomposition seems of little use when the latentvariable model has nonlinearities. The current paper shows how to make progress: tensordecomposition is applied for learning the single-layer {\em noisy or} network; which is a …,arXiv preprint arXiv:1612.08795,2016,7
Tensor decompositions for learning latent variable models (A survey for ALT),Anima Anandkumar; Rong Ge; Daniel Hsu; Sham M Kakade; Matus Telgarsky,Abstract This note is a short version of that in [1]. It is intended as a survey for the 2015Algorithmic Learning Theory (ALT) conference. This work considers a computationally andstatistically efficient parameter estimation method for a wide class of latent variable models—including Gaussian mixture models; hidden Markov models; and latent Dirichlet allocation—which exploits a certain tensor structure in their low-order observable moments (typically; ofsecond-and third-order). Specifically; parameter estimation is reduced to the problem ofextracting a certain (orthogonal) decomposition of a symmetric tensor derived from themoments; this decomposition can be viewed as a natural generalization of the singularvalue decomposition for matrices. Although tensor decompositions are generally intractableto compute; the decomposition of these specially structured tensors can be efficiently …,International Conference on Algorithmic Learning Theory,2015,7
Provable algorithms for inference in topic models,Sanjeev Arora; Rong Ge; Frederic Koehler; Tengyu Ma; Ankur Moitra,Abstract Recently; there has been considerable progress on designing algorithms withprovable guarantees—typically using linear algebraic methods—for parameter learning inlatent variable models. But designing provable algorithms for inference has proven to bemore challenging. Here we take a first step towards provable inference in topic models. Weleverage a property of topic models that enables us to construct simple linear estimators forthe unknown topic proportions that have small variance; and consequently can work withshort documents. Our estimators also correspond to finding an estimate around which theposterior is well-concentrated. We show lower bounds that for shorter documents it can beinformation theoretically impossible to find the hidden topics. Finally; we give empiricalresults that demonstrate that our algorithm works on realistic topic models. It yields good …,International Conference on Machine Learning,2016,6
Towards a better approximation for sparsest cut?,Sanjeev Arora; Rong Ge; Ali Kemal Sinop,We give a new (1+ ε)-approximation for SPARSEST CUT problem on graphs where smallsets expand significantly more than the sparsest cut (expansion of sets of size n/r exceedsthat of the sparsest cut by a factor√ log n log r; for some small r; this condition holds formany natural graph families). We give two different algorithms. One involves Guruswami-Sinop rounding on the level-r Lasserre relaxation. The other is combinatorial and involves anew notion called Small Set Expander Flows (inspired by the expander flows of [1]) whichwe show exists in the input graph. Both algorithms run in time 2 O (r) poly (n). We also showsimilar approximation algorithms in graphs with genus g with an analogous local expansioncondition. This is the first algorithm we know of that achieves (1+ ε)-approximation on suchgeneral family of graphs.,Foundations of Computer Science (FOCS); 2013 IEEE 54th Annual Symposium on,2013,6
Intersecting faces: Non-negative matrix factorization with new guarantees,Rong Ge; James Zou,Abstract Non-negative matrix factorization (NMF) is a natural model of admixture and iswidely used in science and engineering. A plethora of algorithms have been developed totackle NMF; but due to the non-convex nature of the problem; there is little guarantee on howwell these methods work. Recently a surge of research have focused on a very restrictedclass of NMFs; called separable NMF; where provably correct algorithms have beendeveloped. In this paper; we propose the notion of subset-separable NMF; whichsubstantially generalizes the property of separability. We show that subset-separability is anatural necessary condition for the factorization to be unique or to have minimum volume.We developed the Face-Intersect algorithm which provably and efficiently solves subset-separable NMF under natural conditions; and we prove that our algorithm is robust to …,International Conference on Machine Learning,2015,5
On the ability of neural nets to express distributions,Holden Lee; Rong Ge; Andrej Risteski; Tengyu Ma; Sanjeev Arora,Abstract: Deep neural nets have caused a revolution in many classification tasks. A relatedongoing revolution---also theoretically not understood---concerns their ability to serve asgenerative models for complicated types of data such as images and texts. These modelsare trained using ideas like variational autoencoders and Generative Adversarial Networks.We take a first cut at explaining the expressivity of multilayer nets by giving a sufficientcriterion for a function to be approximable by a neural network with $ n $ hidden layers. Akey ingredient is Barron's Theorem\cite {Barron1993}; which gives a Fourier criterion forapproximability of a function by a neural network with 1 hidden layer. We show that acomposition of $ n $ functions which satisfy certain Fourier conditions (" Barron functions")can be approximated by a $ n+ 1$-layer neural network.,arXiv preprint arXiv:1702.07028,2017,4
Provable algorithms for machine learning problems,Rong Ge,Abstract Modern machine learning algorithms can extract useful information from text;images and videos. All these applications involve solving NP-hard problems in averagecase using heuristics. What properties of the input allow it to be solved efficiently?Theoretically analyzing the heuristics is very challenging. Few results were known.,*,2013,4
New results on simple stochastic games,Decheng Dai; Rong Ge,Abstract We study the problem of solving simple stochastic games; and give both aninteresting new algorithm and a hardness result. We show a reduction from fineapproximation of simple stochastic games to coarse approximation of a polynomial sizedgame; which can be viewed as an evidence showing the hardness to approximate the valueof simple stochastic games. We also present a randomized algorithm that runs in ̃O(|V_R|!)time; where |V_R| is the number of RANDOM vertices and ̃O ignores polynomial terms. Thisalgorithm is the fastest known algorithm when |V_R|=ω(\logn) and |V_R|=o(|V_min|;|V_max|)and it works for general (non-stopping) simple stochastic games.,International Symposium on Algorithms and Computation,2009,4
Homotopy Analysis for Tensor PCA,Anima Anandkumar; Yuan Deng; Rong Ge; Hossein Mobahi,Abstract: Developing efficient and guaranteed nonconvex algorithms has been an importantchallenge in modern machine learning. Algorithms with good empirical performance such asstochastic gradient descent often lack theoretical guarantees. In this paper; we analyze theclass of homotopy or continuation methods for global optimization of nonconvex functions.These methods start from an objective function that is efficient to optimize (eg convex); andprogressively modify it to obtain the required objective; and the solutions are passed alongthe homotopy path. For the challenging problem of tensor PCA; we prove globalconvergence of the homotopy method in the" high noise" regime. The signal-to-noiserequirement for our algorithm is tight in the sense that it matches the recovery guarantee forthe best degree-4 sum-of-squares algorithm. In addition; we prove a phase transition …,arXiv preprint arXiv:1610.09322,2016,3
Online service with delay,Yossi Azar; Arun Ganesh; Rong Ge; Debmalya Panigrahi,Abstract: In this paper; we introduce the online service with delay problem. In this problem;there are $ n $ points in a metric space that issue service requests over time; and a serverthat serves these requests. The goal is to minimize the sum of distance traveled by theserver and the total delay in serving the requests. This problem models the fundamentaltradeoff between batching requests to improve locality and reducing delay to improveresponse time; that has many applications in operations management; operating systems;logistics; supply chain management; and scheduling. Our main result is to show a poly-logarithmic competitive ratio for the online service with delay problem. This result is obtainedby an algorithm that we call the preemptive service algorithm. The salient feature of thisalgorithm is a process called preemptive service; which uses a novel combination of …,arXiv preprint arXiv:1708.05611,2017,1
Rich component analysis,Rong Ge; James Zou,Abstract In many settings; we have multiple data sets (also called views) that capturedifferent and overlapping aspects of the same phenomenon. We are often interested infinding patterns that are unique to one or to a subset of the views. For example; we mighthave one set of molecular observations and one set of physiological observations on thesame group of individuals; and we want to quantify molecular patterns that are uncorrelatedwith physiology. Despite being a common problem; this is highly challenging when thecorrelations come from complex distributions. In this paper; we develop the generalframework of Rich Component Analysis (RCA) to model settings where the observationsfrom different views are driven by different sets of latent components; and each componentcan be a complex; high-dimensional distribution. We introduce algorithms based on …,International Conference on Machine Learning,2016,1
Stronger generalization bounds for deep nets via a compression approach,Sanjeev Arora; Rong Ge; Behnam Neyshabur; Yi Zhang,Abstract: Deep nets generalize well despite having more parameters than the number oftraining samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses; but do not as yet result in sample complexity bounds better than naiveparameter counting. The current paper shows generalization bounds that're orders ofmagnitude better in practice. These rely upon new succinct reparametrizations of the trainednet---a compression that is explicit and efficient. These yield generalization bounds via asimple compression-based framework introduced here. Our results also provide sometheoretical justification for widespread empirical success in compressing deep nets. Analysisof correctness of our compression relies upon some newly identified\textquotedblleft noisestability\textquotedblright properties of trained deep nets; which are also experimentally …,arXiv preprint arXiv:1802.05296,2018,*
Global Convergence of Policy Gradient Methods for Linearized Control Problems,Maryam Fazel; Rong Ge; Sham M Kakade; Mehran Mesbahi,Abstract: Direct policy gradient methods for reinforcement learning and continuous controlproblems are a popular approach for a variety of reasons: 1) they are easy to implementwithout explicit knowledge of the underlying model 2) they are an" end-to-end" approach;directly optimizing the performance metric of interest 3) they inherently allow for richlyparameterized policies. A notable drawback is that even in the most basic continuous controlproblem (that of linear quadratic regulators); these methods must solve a non-convexoptimization problem; where little is understood about their efficiency from bothcomputational and statistical perspectives. In contrast; system identification and modelbased planning in optimal control theory have a much more solid theoretical footing; wheremuch is known with regards to their computational and statistical properties. This work …,arXiv preprint arXiv:1801.05039,2018,*
Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,Rong Ge; Holden Lee; Andrej Risteski,Abstract: A key task in Bayesian statistics is sampling from distributions that are onlyspecified up to a partition function (ie; constant of proportionality). However; without anyassumptions; sampling (even approximately) can be# P-hard; and few works have provided"beyond worst-case" guarantees for such settings. For log-concave distributions; classicalresults going back to Bakry and\'Emery (1985) show that natural continuous-time Markovchains called Langevin diffusions mix in polynomial time. The most salient feature of log-concavity violated in practice is uni-modality: commonly; the distributions we wish to samplefrom are multi-modal. In the presence of multiple deep and well-separated modes; Langevindiffusion suffers from torpid mixing.,arXiv preprint arXiv:1710.02736,2017,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Narasimhaiah Gorla; Yan Wah Lam,A longstanding vision in distributed systems is to build reliable systems from unreliablecomponents. An enticing formulation of this vision is Byzantine fault-tolerant (BFT) statemachine replication; in which a group of servers collectively act as a correct server even ifsome of the servers misbehave or malfunction in arbitrary (" Byzantine") ways. Despite thispromise; practitioners hesitate to deploy BFT systems at least partly because of theperception that BFT must impose high overheads.In this article; we present Zyzzyva; aprotocol that uses speculation to reduce the cost of BFT replication. In Zyzzyva; replicasreply to a client's request without first running an expensive three-phase commit protocol toagree on the order to process requests. Instead; they optimistically adopt the order proposedby a primary server; process the request; and reply immediately to the client. If the primary …,Communications of the ACM,2008,*
Learning Topic Models—Provably and Efficiently,Sanjeev Arora; Rong Ge; Yoni Halpern; David Mimno; Ankur Moitra; David Sontag; Yichen Wu; Michael Zhu,Today; we have both the blessing and the curse of being overloaded with information. Neverbefore has text been more important to how we communicate; or more easily available. Butmassive text streams far outstrip anyone's ability to read. We need automated tools that canhelp make sense of their thematic structure; and find strands of meaning that connectdocuments; all without human supervision. Such methods can also help us organize andnavigate large text corpora. Popular tools for this task range from Latent Semantic Analysis(LSA)[8] which uses standard linear algebra; to deep learning which relies on non-convexoptimization. This paper concerns topic modeling which posits a simple probabilistic modelof how a document is generated. We give a formal description of the generative model at theend of the section; but next we will outline its important features. Topic modeling …,*,2008,*
