Swoosh: a generic approach to entity resolution,Omar Benjelloun; Hector Garcia-Molina; David Menestrina; Qi Su; Steven Euijong Whang; Jennifer Widom,Abstract We consider the entity resolution (ER) problem (also known as deduplication; ormerge---purge); in which records determined to represent the same real-world entity aresuccessively located and merged. We formalize the generic ER problem; treating thefunctions for comparing and merging records as black-boxes; which permits expressive andextensible ER solutions. We identify four important properties that; if satisfied by the matchand merge functions; enable much more efficient ER algorithms. We develop three efficientER algorithms: G-Swoosh for the case where the four properties do not hold; and R-Swooshand F-Swoosh that exploit the four properties. F-Swoosh in addition assumes knowledge ofthe" features"(eg; attributes) used by the match function. We experimentally evaluate thealgorithms using comparison shopping data from Yahoo! Shopping and hotel information …,The VLDB Journal—The International Journal on Very Large Data Bases,2009,460
Entity resolution with iterative blocking,Steven Euijong Whang; David Menestrina; Georgia Koutrika; Martin Theobald; Hector Garcia-Molina,Abstract Entity Resolution (ER) is the problem of identifying which records in a databaserefer to the same real-world entity. An exhaustive ER process involves computing thesimilarities between pairs of records; which can be very expensive for large datasets.Various blocking techniques can be used to enhance the performance of ER by dividing therecords into blocks in multiple ways and only comparing records within the same block.However; most blocking techniques process blocks separately and do not exploit the resultsof other blocks. In this paper; we propose an iterative blocking framework where the ERresults of blocks are reflected to subsequently processed blocks. Blocks are now iterativelyprocessed until no block contains any more matching records. Compared to simple blocking;iterative blocking may achieve higher accuracy because reflecting the ER results of …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,207
Question selection for crowd entity resolution,Steven Euijong Whang; Peter Lofgren; Hector Garcia-Molina,Abstract We study the problem of enhancing Entity Resolution (ER) with the help ofcrowdsourcing. ER is the problem of clustering records that refer to the same real-worldentity and can be an extremely difficult process for computer algorithms alone. For example;figuring out which images refer to the same person can be a hard task for computers; but aneasy one for humans. We study the problem of resolving records with crowdsourcing wherewe ask questions to humans in order to guide ER into producing accurate results. Sincehuman work is costly; our goal is to ask as few questions as possible. We propose aprobabilistic framework for ER that can be used to estimate how much ER accuracy weobtain by asking each question and select the best question with the highest expectedaccuracy. Computing the expected accuracy is# P-hard; so we propose approximation …,Proceedings of the VLDB Endowment,2013,113
Pay-as-you-go entity resolution,Steven Euijong Whang; David Marmaros; Hector Garcia-Molina,Entity resolution (ER) is the problem of identifying which records in a database refer to thesame entity. In practice; many applications need to resolve large data sets efficiently; but donot require the ER result to be exact. For example; people data from the web may simply betoo large to completely resolve with a reasonable amount of work. As another example; real-time applications may not be able to tolerate any ER processing that takes longer than acertain amount of time. This paper investigates how we can maximize the progress of ERwith a limited amount of work using “hints;” which give information on records that are likelyto refer to the same real-world entity. A hint can be represented in various formats (eg; agrouping of records based on their likelihood of matching); and ER can use this informationas a guideline for which records to compare first. We introduce a family of techniques for …,IEEE Transactions on Knowledge and Data Engineering,2013,90
Entity resolution with evolving rules,Steven Euijong Whang; Hector Garcia-Molina,Abstract Entity resolution (ER) identifies database records that refer to the same real worldentity. In practice; ER is not a one-time process; but is constantly improved as the data;schema and application are better understood. We address the problem of keeping the ERresult up-to-date when the ER logic" evolves" frequently. A naïve approach that re-runs ERfrom scratch may not be tolerable for resolving large datasets. This paper investigates whenand how we can instead exploit previous" materialized" ER results to save redundant workwith evolved logic. We introduce algorithm properties that facilitate evolution; and wepropose efficient rule evolution techniques for two clustering ER models: match-basedclustering and distance-based clustering. Using real data sets; we illustrate the cost ofmaterializations and the potential gains over the naïve approach.,Proceedings of the VLDB Endowment,2010,79
Indexing boolean expressions,Steven Euijong Whang; Hector Garcia-Molina; Chad Brower; Jayavel Shanmugasundaram; Sergei Vassilvitskii; Erik Vee; Ramana Yerneni,Abstract We consider the problem of efficiently indexing Disjunctive Normal Form (DNF) andConjunctive Normal Form (CNF) Boolean expressions over a high-dimensional multi-valuedattribute space. The goal is to rapidly find the set of Boolean expressions that evaluate totrue for a given assignment of values to attributes. A solution to this problem has applicationsin online advertising (where a Boolean expression represents an advertiser's user targetingrequirements; and an assignment of values to attributes represents the characteristics of auser visiting an online page) and in general any publish/subscribe system (where a Booleanexpression represents a subscription; and an assignment of values to attributes representsan event). All existing solutions that we are aware of can only index a specialized sub-set ofconjunctive and/or disjunctive expressions; and cannot efficiently handle general DNF …,Proceedings of the VLDB Endowment,2009,78
Evaluating entity resolution results,David Menestrina; Steven Euijong Whang; Hector Garcia-Molina,Abstract Entity Resolution (ER) is the process of identifying groups of records that refer to thesame real-world entity. Various measures (eg; pairwise F 1; cluster F 1) have been used forevaluating ER results. However; ER measures tend to be chosen in an ad-hoc fashionwithout careful thought as to what defines a good result for the specific application at hand.In this paper; our contributions are twofold. First; we conduct an analysis on existing ERmeasures; showing that they can often conflict with each other by ranking the results of ERalgorithms differently. Second; we explore a new distance measure for ER (called"generalized merge distance" or GMD) inspired by the edit distance of strings; using clustersplits and merges as its basic operations. A significant advantage of GMD is that the costfunctions for splits and merges can be configured; enabling us to clearly understand the …,Proceedings of the VLDB Endowment,2010,60
Generic entity resolution with negative rules,Steven Euijong Whang; Omar Benjelloun; Hector Garcia-Molina,Abstract Entity resolution (ER)(also known as deduplication or merge-purge) is a process ofidentifying records that refer to the same real-world entity and merging them together. Inpractice; ER results may contain “inconsistencies;” either due to mistakes by the match andmerge function writers or changes in the application semantics. To remove theinconsistencies; we introduce “negative rules” that disallow inconsistencies in the ERsolution (ER-N). A consistent solution is then derived based on the guidance from a domainexpert. The inconsistencies can be resolved in several ways; leading to accurate solutions.We formalize ER-N; treating the match; merge; and negative rules as black boxes; whichpermits expressive and extensible ER-N solutions. We identify important properties for therules that; if satisfied; enable less costly ER-N. We develop and evaluate two algorithms …,The VLDB Journal,2009,55
Biperpedia: An ontology for search applications,Rahul Gupta; Alon Halevy; Xuezhi Wang; Steven Euijong Whang; Fei Wu,Abstract Search engines make significant efforts to recognize queries that can be answeredby structured data and invest heavily in creating and maintaining high-precision databases.While these databases have a relatively wide coverage of entities; the number of attributesthey model (eg; GDP; CAPITAL; ANTHEM) is relatively small. Extending the number ofattributes known to the search engine can enable it to more precisely answer queries fromthe long and heavy tail; extract a broader range of facts from the Web; and recover thesemantics of tables on the Web. We describe Biperpedia; an ontology with 1.6 M (class;attribute) pairs and 67K distinct attribute names. Biperpedia extracts attributes from the querystream; and then uses the best extractions to seed attribute extraction from text. For everyattribute Biperpedia saves a set of synonyms and text patterns in which it appears …,Proceedings of the VLDB Endowment,2014,51
Renoun: Fact extraction for nominal attributes,Mohamed Yahya; Steven Whang; Rahul Gupta; Alon Halevy,Abstract Search engines are increasingly relying on large knowledge bases of facts toprovide direct answers to users' queries. However; the construction of these knowledgebases is largely manual and does not scale to the long and heavy tail of facts. Openinformation extraction tries to address this challenge; but typically assumes that facts areexpressed with verb phrases; and therefore has had difficulty extracting facts for noun-basedrelations. We describe ReNoun; an open information extraction system that complementsprevious efforts by focusing on nominal attributes and on the long tail. ReNoun's approach isbased on leveraging a large ontology of noun attributes mined from a text corpus and fromuser queries. ReNoun creates a seed set of training data by using specialized patterns andrequiring that the facts mention an attribute in the ontology. ReNoun then generalizes …,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014,46
Incremental entity resolution on rules and data,Steven Euijong Whang; Hector Garcia-Molina,Abstract Entity resolution (ER) identifies database records that refer to the same real-worldentity. In practice; ER is not a one-time process; but is constantly improved as the data;schema and application are better understood. We first address the problem of keeping theER result up-to-date when the ER logic or data “evolve” frequently. A naïve approach that re-runs ER from scratch may not be tolerable for resolving large datasets. This paperinvestigates when and how we can instead exploit previous “materialized” ER results tosave redundant work with evolved logic and data. We introduce algorithm properties thatfacilitate evolution; and we propose efficient rule and data evolution techniques for three ERmodels: match-based clustering (records are clustered based on Boolean matchinginformation); distance-based clustering (records are clustered based on relative distances …,The VLDB Journal,2014,38
Joint entity resolution,Steven Euijong Whang; Hector Garcia-Molina,*,*,*,33
Goods: Organizing google's datasets,Alon Halevy; Flip Korn; Natalya F Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract Enterprises increasingly rely on structured datasets to run their businesses. Thesedatasets take a variety of forms; such as structured files; databases; spreadsheets; or evenservices that provide access to the data. The datasets often reside in different storagesystems; may vary in their formats; may change every day. In this paper; we present GOODS;a project to rethink how we organize structured datasets at scale; in a setting where teamsuse diverse and often idiosyncratic ways to produce the datasets and where there is nocentralized system for storing and querying them. GOODS extracts metadata ranging fromsalient information about each dataset (owners; timestamps; schema) to relationshipsamong datasets; such as similarity and provenance. It then exposes this metadata throughservices that allow engineers to find datasets within the company; to monitor datasets; to …,Proceedings of the 2016 International Conference on Management of Data,2016,27
Developments in generic entity resolution,Steven Euijong Whang; Hector Garcia-Molina,Entity resolution (ER) is the problem of identifying which records in a database refer to thesame entity. Although ER is a well-known problem; the rapid increase of data has made ERa challenging problem in many application areas ranging from resolving shopping items tocounter-terrorism. The SERF project at Stanford focuses on providing scalable and accurateER techniques that can be used across applications. We introduce generic ER and explainthe recent advances made in our project.,IEEE Data Engineering Bulletin,2011,13
Managing information leakage,Steven Euijong Whang; Hector Garcia-Molina,We explore the problem of managing information leakage by connecting two hithertodisconnected topics: entity resolution (ER) and data privacy (DP). As more of our sensitivedata gets exposed to a variety of merchants; health care providers; employers; social sitesand so on; there is a higher chance that an adversary can``connect the dots''and piecetogether our information; leading to even more loss of privacy. For instance; suppose thatAlice has a social networking profile with her name and photo and a web homepagecontaining her name and address. An adversary Eve may be able to link the profile andhomepage to connect the photo and address of Alice and thus glean more personalinformation. The better Eve is at linking the information; the more vulnerable is Alice'sprivacy. Thus in order to gain DP; one must try to prevent important bits of information …,*,2010,13
A model for quantifying information leakage,Steven Euijong Whang; Hector Garcia-Molina,Abstract We study data privacy in the context of information leakage. As more of oursensitive data gets exposed to merchants; health care providers; employers; social sites andso on; there is a higher chance that an adversary can “connect the dots” and piece togethera lot of our information. The more complete the integrated information; the more our privacyis compromised. We present a model that captures this privacy loss (information leakage)relative to a target person; on a continuous scale from 0 (no information about the target isknown by the adversary) to 1 (adversary knows everything about the target). The modeltakes into account the confidence the adversary has for the gathered information (leakage isless if the adversary is not confident); as well as incorrect information (leakage is less if thegathered information does not match the target's). We compare our information leakage …,Workshop on Secure Data Management,2012,12
Tfx: A tensorflow-based production-scale machine learning platform,Denis Baylor; Eric Breck; Heng-Tze Cheng; Noah Fiedel; Chuan Yu Foo; Zakaria Haque; Salem Haykal; Mustafa Ispir; Vihan Jain; Levent Koc; Chiu Yuen Koo; Lukasz Lew; Clemens Mewald; Akshay Naresh Modi; Neoklis Polyzotis; Sukriti Ramesh; Sudip Roy; Steven Euijong Whang; Martin Wicke; Jarek Wilkiewicz; Xin Zhang; Martin Zinkevich,Abstract Creating and maintaining a platform for reliably producing and deploying machinelearning models requires careful orchestration of many components---a learner forgenerating models based on training data; modules for analyzing and validating both dataas well as models; and finally infrastructure for serving models in production. This becomesparticularly challenging when data changes over time and fresh models need to beproduced continuously. Unfortunately; such orchestration is often done ad hoc using gluecode and custom scripts developed by individual teams for specific use cases; leading toduplicated effort and fragile systems with high technical debt. We present TensorFlowExtended (TFX); a TensorFlow-based general-purpose machine learning platformimplemented at Google. By integrating the aforementioned components into one platform …,Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2017,11
Discovering structure in the universe of attribute names,Alon Halevy; Natalya Noy; Sunita Sarawagi; Steven Euijong Whang; Xiao Yu,Abstract Recently; search engines have invested significant effort to answering entity--attribute queries from structured data; but have focused mostly on queries for frequentattributes. In parallel; several research efforts have demonstrated that there is a long tail ofattributes; often thousands per class of entities; that are of interest to users. Researchers arebeginning to leverage these new collections of attributes to expand the ontologies thatpower search engines and to recognize entity--attribute queries. Because of the sheernumber of potential attributes; such tasks require us to impose some structure on this longand heavy tail of attributes. This paper introduces the problem of organizing the attributes byexpressing the compositional structure of their names as a rule-based grammar. These rulesoffer a compact and rich semantic interpretation of multi-word attributes; while …,Proceedings of the 25th International Conference on World Wide Web,2016,10
Compare me maybe: Crowd entity resolution interfaces,Steven Euijong Whang; Julian McAuley; Hector Garcia-Molina,We study the problem of enhancing entity resolution (ER) with the help of crowdsourcing. ERis the problem of identifying records that refer to the same real-world entity and can be anextremely difficult process for computer algorithms alone. For example; figuring out whichimages refer to the same person can be a hard task for computers; but an easy one forhumans. An important component of crowdsourcing is the interface that is used for humanand algorithm interaction. In this paper; we explore how the interface design along with otherfactors impact the human quality of comparing records. We also propose a model forseparating good human workers from bad workers. Our analysis is based on extensiveexperiments on Amazon Mechanical Turk using real and synthetic image datasets.,*,2012,10
Data management challenges in production machine learning,Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang; Martin Zinkevich,Abstract The tutorial discusses data-management issues that arise in the context of machinelearning pipelines deployed in production. Informed by our own experience with suchlargescale pipelines; we focus on issues related to understanding; validating; cleaning; andenriching training data. The goal of the tutorial is to bring forth these issues; drawconnections to prior work in the database literature; and outline the open research questionsthat are not addressed by prior art.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,7
Joint entity resolution on multiple datasets,Steven Euijong Whang; Hector Garcia-Molina,Abstract Entity resolution (ER) is the problem of identifying which records in a databaserepresent the same entity. Often; records of different types are involved (eg; authors;publications; institutions; venues); and resolving records of one type can impact theresolution of other types of records. In this paper we propose a flexible; modular resolutionframework where existing ER algorithms developed for a given record type can be pluggedin and used in concert with other ER algorithms. Our approach also makes it possible to runER on subsets of similar records at a time; important when the full data are too large toresolve together. We study the scheduling and coordination of the individual ER algorithms;in order to resolve the full dataset; and show the scalability of our approach. We alsointroduce a “state-based” training technique where each ER algorithm is trained for the …,The VLDB Journal,2013,6
Disinformation techniques for entity resolution,Steven Euijong Whang; Hector Garcia-Molina,Abstract We study the problem of disinformation. We assume that an``agent''has somesensitive information that the``adversary''is trying to obtain. For example; a camera company(the agent) may secretly be developing its new camera model; and a user (the adversary)may want to know in advance the detailed specs of the model. The agent's goal is todisseminate false information to``dilute''what is known by the adversary. We model theadversary as an Entity Resolution (ER) process that pieces together available information.We formalize the problem of finding the disinformation with the highest benefit given alimited budget for creating the disinformation and propose efficient algorithms for solving theproblem. We then evaluate our disinformation planning algorithms on real and syntheticdata and compare the robustness of existing ER algorithms. In general; our disinformation …,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,6
A practitioner’s approach to normalizing XQuery expressions,Ki-Hoon Lee; Seo-Young Kim; Euijong Whang; Jae-Gil Lee,Abstract XQuery becomes a standard of the XML query language. Just like in SQL; XQueryallows nested expressions. To optimize XQuery processing; a lot of research has been doneon normalization; ie; transforming nested expressions to equivalent unnested ones.Previous normalization rules are classified into two categories–source-level and algebra-level–depending on whether a construct is specified by using a query language or analgebraic expression. In implementation point of view; we contend that the source-level ruleis preferable to the algebra-level rule because algebras used for normalization are hard tobe directly exploited in a typical DBMS. However; a complete set of source-level rules is yetto be developed. In this paper; we propose source-level rules for normalizing XQueryexpressions and present an implementation mechanism. We show that our rules are …,Database Systems for Advanced Applications,2006,6
Managing Google's data lake: an overview of the Goods system.,Alon Y Halevy; Flip Korn; Natalya Fridman Noy; Christopher Olston; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang,Abstract For most large enterprises today; data constitutes their core asset; along with codeand infrastructure. For most enterprises; the amount of data that they produce internally hasexploded in recent years. At the same time; in many cases; engineers and data scientists donot use centralized data-management systems and end up creating what became known asa data lake—a collection of datasets that often are not well organized or not organized at alland where one needs to “fish” for useful datasets. In this paper; we describe our experiencebuilding and deploying GOODS; a system to manage Google's internal data lake. GOODScrawls Google's infrastructure and builds a catalog of discovered datasets; includingstructured files; databases; spreadsheets; and even services that provide access to the data.GOODS extracts metadata about datasets in a post-hoc way: engineers continue to …,IEEE Data Eng. Bull.,2016,5
Discovering subsumption relationships for web-based ontologies,Dana Movshovitz-Attias; Steven Euijong Whang; Natalya Noy; Alon Halevy,Abstract As search engines are becoming smarter at interpreting user queries and providingmeaningful responses; they rely on ontologies to understand the meaning of entities.Creating ontologies manually is a laborious process; and resulting ontologies may notreflect the way users think about the world; as many concepts used in queries are noisy; andnot easily amenable to formal modeling. There has been considerable effort in generatingontologies from Web text and query streams; which may be more reflective of how usersquery and write content. In this paper; we describe the LATTE system that automaticallygenerates a subconcept--superconcept hierarchy; which is critical for using ontologies toanswer queries. LATTE combines signals based on word-vector representations of conceptsand dependency parse trees; however; LATTE derives most of its power from an ontology …,Proceedings of the 18th International Workshop on Web and Databases,2015,4
System and Method for Automatic Matching of Highest Scoring Contracts to Impression Opportunities Using Complex Predicates and an Inverted Index,*,A method for indexing advertising contracts for rapid retrieval and matching in order to matchonly the top N satisfying contracts to advertising slots. Descriptions of advertising contractsinclude logical predicates indicating weighted applicability to a particular demographic.Descriptions of advertising slots also contain logical predicates indicating weightedapplicability to particular demographics; thus matches are performed on the basis of aweighed score of intersecting demographics. Disclosed are structure and techniques forreceiving a set of contracts with weighted predicates; preparing a data structure index of theset of contracts; receiving an advertising slot with weighted predicates; and retrieving fromthe data structure only the top N weighted score contracts that satisfy a match to theadvertising slot predicates. Various disclosed cases include predicates presented in …,*,2011,4
Evaluating entity resolution results (extended version),David Menestrina; Steven Euijong Whang; Hector Garcia-Molina,Entity Resolution (ER) is the process of identifying groups of records that refer to the samereal-world entity. Various measures (eg; pairwise $ F_1 $; cluster $ F_1 $) have been usedfor evaluating ER results. However; ER measures tend to be chosen in an ad-hoc fashionwithout careful thought as to what defines a good result for the specific application at hand.In this paper; our contributions are twofold. First; we conduct an extensive survey on existingER measures; showing that they can often conflict with each other by ranking the results ofER algorithms differently. Second; we propose a new distance measure for ER(called``merge distance'') inspired by the edit distance of strings; using cluster splits andmerges as its basic operations. A significant advantage of merge distance is that the costfunctions for splits and merges can be configured to adjust two important parameters …,*,2009,3
Data Analytics: Integration and Privacy,Steven Euijong Whang,Data analytics has become an extremely important and challenging problem in disciplineslike computer science; biology; medicine; finance; and homeland security. As massiveamounts of data are available for analysis; scalable integration techniques becomeimportant. At the same time; new privacy issues arise where one's sensitive information caneasily be inferred from the large amounts of data. In this thesis; we first cover the problem of{\it entity resolution}(ER); which identifies database records that refer to the same real-worldentity. The recent explosion of data has now made ER a challenging problem in a widerange of applications. We propose scalable ER techniques and new ER functionalities thathave not been studied in the past. We also view ER as a black-box operation and providegeneral techniques that can be used across applications. Next; we introduce the problem …,*,2012,2
Extracting facts from documents,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for extracting facts from a collection of documents. One of the methodsincludes obtaining a plurality of seed facts; generating a plurality of patterns from the seedfacts; wherein each of the plurality of patterns is a dependency pattern generated from adependency parse; applying the patterns to documents in a collection of documents toextract a plurality of candidate additional facts from the collection of documents; andselecting one or more additional facts from the plurality of candidate additional facts.,*,2017,1
LONLIES: estimating property values for long tail entities,Mina Farid; Ihab F Ilyas; Steven Euijong Whang; Cong Yu,Abstract Web search engines often retrieve answers for queries about popular entities froma growing knowledge base that is populated by a continuous information extraction process.However; less popular entities are not frequently mentioned on the web and are generallyinteresting to fewer users; these entities reside on the long tail of information. Traditionalknowledge base construction techniques that rely on the high frequency of entity mentions toextract accurate facts about these mentions have little success with entities that have lowtextual support. We present Lonlies; a system for estimating property values of long tailentities by leveraging their relationships to head topics and entities. We demonstrate (1) howLonlies builds communities of entities that are relevant to a long tail entity utilizing a textcorpus and a knowledge base;(2) how Lonlies determines which communities to use in …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,1
Additional experiments on negative rules,Steven Euijong Whang; Omar Benjelloun; Hector Garcia-Molina,We implemented the General and Enhanced algorithms as described in our technical report"Generic Entity Resolution with Negative Rules" and conducted extensive experiments. Weran our experiments on a comparison shopping dataset provided by Yahoo!. In thisapplication; hundreds of thousands of records arrive on a regular basis from different onlinestores and must be resolved before they are used to answer customer queries. Because ofthe volume of data; we used blocking techniques to partition the data into independentclusters and then applied our algorithms on each cluster. In our experiments; we used apartition containing records with the sub-string" iPod" in their titles; we will call these iPod-related records from now on. The algorithms were implemented in Java; and ourexperiments were run on a 1.8 GHz AMD Opteron processor with 20.4 GB of memory …,*,2008,1
Identifying entity attributes,*,Abstract Methods; systems; and apparatus; including computer programs encoded oncomputer storage media; for generating an ontology of entity attributes. One of the methodsincludes extracting a plurality of attributes based upon a plurality of queries; andconstructing an ontology based upon the plurality of attributes and a plurality of entityclasses.,*,2018,*
TFX Frontend: A Graphical User Interface for a Production-Scale Machine Learning Platform,James Wexler Yang; Steven Euijong Whang,The widespread use of machine learning has highlighted the need for platforms that provideintegrated tools for all phases of the machine learning development process. An end-to-endmachine learning platform contains components that span from ingesting raw data to servinga model and logging its predictions. TFX [1] is one implementation of such a platform that isused for productionscale machine learning at Google. In this extended abstract we discussthe TFX frontend; which is a unified and guided interface that supports workflows that spanacross all TFX components. It is the primary interface via which our users interact with TFXand inspect the result of their interactions.,*,2018,*
Data Infrastructure for Machine Learning,Eric Breck; Neoklis Polyzotis; Sudip Roy; Steven Euijong Whang; Martin Zinkevich,ABSTRACT Data quality is critical for effective machine learning; and this makes data a first-class citizen in the context of machine learning; on par with algorithms; software; andinfrastructure. As a result; machine-learning platforms need to support data analysis andvalidation in a principled manner; throughout the lifecycle of the machine learning process.This paper reviews the data infrastructure we built at Google to address these challenges inthe context of large-scale production machine learning pipelines.,*,2018,*
Slice Finder: Automated Data Slicing for Model Interpretability,Yeounoh Chung; Tim Kraska; Steven Euijong Whang; Neoklis Polyzotis,ABSTRACT As machine learning (ML) systems become democratized; helping users easilydebug their models becomes increasingly important. Yet current data tools are still primitivewhen it comes to helping users trace model performance problems all the way to the data.We focus on the particular problem of slicing data to identify subsets of the training datawhere the model performs poorly. Unlike general techniques (eg; clustering) that can findarbitrary slices; our goal is to find interpretable slices (which are easier to take actioncompared to arbitrary subsets) that are problematic and large. We propose Slice Finder;which is an interactive framework for identifying such slices using statistical techniques. Theslices can be used for applications like diagnosing model fairness and fraud detectionwhere describing slices that are interpretable to humans is necessary.,*,2018,*
Post-hoc management of datasets,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for generating a catalog for multiple datasets; the method comprisingaccessing multiple extant data sets; the extant data sets including data sets that areindependently generated and structurally dissimilar; organizing the data sets intocollections; each data set in each collection belonging to the collection based on collectiondata associated with the data set; for each collection of data sets: determining; from a subsetof the data sets that belong to the collection; metadata that describe the data sets that belongto the collection; wherein the metadata does not include the collection data; and attributing;to other data sets in the collection; the metadata determined from the subset of data sets;and generating; from the collections of data sets and the determined metadata; a catalog …,*,2017,*
TFX: A TensorFlow-Based Production-Scale Machine Learning Platform,Akshay Naresh Modi; Chiu Yuen Koo; Chuan Yu Foo; Clemens Mewald; Denis M Baylor; Eric Breck; Heng-Tze Cheng; Jarek Wilkiewicz; Levent Koc; Lukasz Lew; Martin A Zinkevich; Martin Wicke; Mustafa Ispir; Neoklis Polyzotis; Noah Fiedel; Salem Elie Haykal; Steven Whang; Sudip Roy; Sukriti Ramesh; Vihan Jain; Xin Zhang; Zakaria Haque,Abstract Creating and maintaining a platform for reliably producing and deploying machinelearning models requires careful orchestration of many components—a learner forgenerating models based on training data; modules for analyzing and validating both dataas well as models; and finally infrastructure for serving models in production. This becomesparticularly challenging when data changes over time and fresh models need to beproduced continuously. Unfortunately; such orchestration is often done ad hoc using gluecode and custom scripts developed by individual teams for specific use cases; leading toduplicated effort and fragile systems with high technical debt. We present TensorFlowExtended (TFX); a TensorFlow-based general-purpose machine learning platformimplemented at Google. By integrating the aforementioned components into one platform …,*,2017,*
Biperpedia: An Ontology for Search Applications,Alon Halevy; Xuezhi Wang; Steven Whang; Fei Wu,*,*,2014,*
System and Method for Automatic Matching of Contracts to Impression Opportunities Using Complex Predicates and an Inverted Index,*,A method for indexing advertising contracts for rapid retrieval and matching in order to matchsatisfying contracts to advertising slots. The descriptions of the advertising contracts includelogical predicates indicating applicability to a particular demographic. Also; the descriptionsof advertising slots contain logical predicates indicating applicability to a particulardemographic; thus matches can be performed using at least matches on the basis ofintersecting demographics. The disclosure contains structure and techniques for receiving aset of contracts with predicates; preparing a data structure index of the set of contracts;receiving an advertising slot with predicates; and structure and techniques for retrieving fromthe data structure contracts that satisfy a match to the advertising slot predicates. Thedisclosure includes cases were the predicates are presented in conjoint forms and in …,*,2010,*
QuickStart: An Upfront Client-Based Design Advisor for Parallel Data Warehouses,Malu Castellanos; Ivo Jimenez; Neal Coddington; Hans Zeller; Steven Whang; Umeshwar Dayal,QuickStart is a tool to automate the physical design of data warehouses for HP's Neoviewsystem. It has been researched and prototyped at HP Labs with close interaction fromNeoview design experts. It embodies heuristics and best practices of the experts to searchfor candidate physical features and uses cost calculations to recommend the features thatresult in good designs. It has some unique characteristics that differentiate it from otherphysical design advisors. In particular; it is the only advisor that is client-based; does notrequire a DBMS server installation and can work off a laptop by simply connecting to thecustomers' flat files. Another unique characteristic of QuickStart is that it provides therationale for its recommendations.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,*
CS730R: Topics in Data and Information Management–Big Data Analytics,Steven Euijong Whang; Hector Garcia-Molina,1 Summary The paper presents two concepts: entity resolution (ER; record linkage) and dataprivacy (DP). Authors presented a sketch of a framework for managing information leakage;and studied how the framework can be used to answer a variety of questions related to ERand DP. In the paper they studied the problems of measuring the incremental leakage ofcritical information. The framework bases on definitions and usage of two functions–matchand merge. The former function allows to detect attribute values; which describe the sameentity; while the latter function merges such values into one record describing such entity.Calling these functions subsequently incrementally builds a set of data that are disclosedabout described entity. Authors used disinformation as a mechanism to minimize informationleakage. The paper presents a model of the problem; shows an idea of the framework …,*,*,*
BIPERPEDIA,Rahul Gupta; Alon Halevy; Xuezhi Wang; Steven Euijong Whang; Fei Wu,*,*,*,*
