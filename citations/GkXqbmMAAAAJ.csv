DeepDive: declarative knowledge base construction,Ce Zhang; Christopher Ré; Michael Cafarella; Christopher De Sa; Alex Ratner; Jaeho Shin; Feiran Wang; Sen Wu,Abstract The dark data extraction or knowledge base construction (KBC) problem is topopulate a relational database with information from unstructured data sources; such asemails; webpages; and PDFs. KBC is a long-standing problem in industry and research thatencompasses problems of data extraction; cleaning; and integration. We describeDeepDive; a system that combines database and machine learning ideas to help to developKBC systems. The key idea in DeepDive is to frame traditional extract-transform-load (ETL)style data management problems as a single large statistical inference task that isdeclaratively defined by the user. DeepDive leverages the effectiveness and efficiency ofstatistical inference and machine learning for difficult extraction tasks; whereas not requiringusers to directly write any probabilistic inference algorithms. Instead; domain experts …,Communications of the ACM,2017,224
The use of categorization information in language models for question retrieval,Xin Cao; Gao Cong; Bin Cui; Christian Søndergaard Jensen; Ce Zhang,Abstract Community Question Answering (CQA) has emerged as a popular type of servicemeeting a wide range of information needs. Such services enable users to ask and answerquestions and to access existing question-answer pairs. CQA archives contain very largevolumes of valuable user-generated content and have become important informationresources on the Web. To make the body of knowledge accumulated in CQA archivesaccessible; effective and efficient question search is required. Question search in a CQAarchive aims to retrieve historical questions that are relevant to new questions posed byusers. This paper proposes a category-based framework for search in CQA archives. Theframework embodies several new techniques that use language models to exploitcategories of questions for improving question-answer search. Experiments conducted on …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,99
Building an entity-centric stream filtering test collection for TREC 2012,John R Frank; Max Kleiman-Weiner; Daniel A Roberts; Feng Niu; Ce Zhang; Christopher Ré; Ian Soboroff,Abstract: The Knowledge Base Acceleration track in TREC 2012 focused on a single task:filter a time-ordered corpus for documents that are highly relevant to a predefined list ofentities. KBA differs from previous filtering evaluations in two primary ways: the streamcorpus is 100x larger than previous filtering collections; and the use of entities as topicsenables systems to incorporate structured knowledge bases (KB); such as Wikipedia; asexternal data sources. A successful KBA system must do more than resolve the meaning ofentity mentions by linking documents to the KB: it must also distinguish centrally relevantdocuments that are worth citing in the entity's WP article. This combines thinking from naturallanguage processing (NLP) and information retrieval (IR). Filtering tracks in TREC havetypically used queries based on topics described by a set of keyword queries or short …,*,2012,86
Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features,Kun-Hsing Yu; Ce Zhang; Gerald J Berry; Russ B Altman; Christopher Ré; Daniel L Rubin; Michael Snyder,Abstract Lung cancer is the most prevalent cancer worldwide; and histopathologicalassessment is indispensable for its diagnosis. However; human evaluation of pathologyslides cannot accurately predict patients' prognoses. In this study; we obtain 2;186haematoxylin and eosin stained histopathology whole-slide images of lung adenocarcinomaand squamous cell carcinoma patients from The Cancer Genome Atlas (TCGA); and 294additional images from Stanford Tissue Microarray (TMA) Database. We extract 9;879quantitative image features and use regularized machine-learning methods to select the topfeatures and to distinguish shorter-term survivors from longer-term survivors with stage Iadenocarcinoma (P< 0.003) or squamous cell carcinoma (P= 0.023) in the TCGA data set.We validate the survival prediction framework with the TMA cohort (P< 0.036 for both …,Nature communications,2016,82
Dimmwitted: A study of main-memory statistical analytics,Ce Zhang; Christopher Ré,Abstract We perform the first study of the tradeoff space of access methods and replication tosupport statistical analytics using first-order methods executed in the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical analytics systems differ fromconventional SQL-analytics in the amount and types of memory incoherence that they cantolerate. Our goal is to understand tradeoffs in accessing the data in row-or column-orderand at what granularity one should share the model and data for a statistical task. We studythis new tradeoff space and discover that there are tradeoffs between hardware andstatistical efficiency. We argue that our tradeoff study may provide valuable information fordesigners of analytics engines: for each system we consider; our prototype engine can runat least one popular task at least 100× faster. We conduct our study across five …,Proceedings of the VLDB Endowment,2014,73
Materialization optimizations for feature selection workloads,Ce Zhang; Arun Kumar; Christopher Ré,Abstract There is an arms race in the data management industry to support statisticalanalytics. Feature selection; the process of selecting a feature set that will be used to build astatistical model; is widely regarded as the most critical step of statistical analytics. Thus; weargue that managing the feature selection process is a pressing data managementchallenge. We study this challenge by describing a feature selection language and asupporting prototype system that builds on top of current industrial R-integration layers. Fromour interactions with analysts; we learned that feature selection is an interactive human-in-the-loop process; which means that feature selection workloads are rife with reuseopportunities. Thus; we study how to materialize portions of this computation using not onlyclassical database materialization optimizations but also methods that have not …,ACM Transactions on Database Systems (TODS),2016,69
Brainwash: A Data System for Feature Engineering.,Michael R Anderson; Dolan Antenucci; Victor Bittorf; Matthew Burgess; Michael J Cafarella; Arun Kumar; Feng Niu; Yongjoo Park; Christopher Ré; Ce Zhang,ABSTRACT A new generation of data processing systems; including web search; Google'sKnowledge Graph; IBM's Watson; and several different recommendation systems; combinerich databases with software driven by machine learning. The spectacular successes ofthese trained systems have been among the most notable in all of computing and havegenerated excitement in health care; finance; energy; and general business. But buildingthem can be challenging; even for computer scientists with PhD-level training. If thesesystems are to have a truly broad impact; building them must become easier. We exploreone crucial pain point in the construction of trained systems: feature engineering. Given thesheer size of modern datasets; feature developers must (1) write code with few effectiveclues about how their code will interact with the data and (2) repeatedly endure long …,CIDR,2013,67
Elementary: Large-scale knowledge-base construction via machine learning and statistical inference,Feng Niu; Ce Zhang; Christopher Ré; Jude Shavlik,Abstract Researchers have approached knowledge-base construction (KBC) with a widerange of data resources and techniques. The authors present Elementary; a prototype KBCsystem that is able to combine diverse resources and different KBC techniques via machinelearning and statistical inference to construct knowledge bases. Using Elementary; theyhave implemented a solution to the TAC-KBP challenge with quality comparable to the stateof the art; as well as an end-to-end online demonstration that automatically and continuouslyenriches Wikipedia with structured data by reading millions of webpages on a daily basis.The authors describe several challenges and their solutions in designing; implementing;and deploying Elementary. In particular; the authors first describe the conceptual frameworkand architecture of Elementary to integrate different data resources and KBC techniques …,International Journal on Semantic Web and Information Systems (IJSWIS),2012,63
Taming the wild: A unified analysis of hogwild-style algorithms,Christopher M De Sa; Ce Zhang; Kunle Olukotun; Christopher Ré,Abstract Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machinelearning problems. Researchers and industry have developed several techniques tooptimize SGD's runtime performance; including asynchronous execution and reducedprecision. Our main result is a martingale-based analysis that enables us to capture the richnoise models that may arise from such techniques. Specifically; we useour new analysis inthree ways:(1) we derive convergence rates for the convex case (Hogwild) with relaxedassumptions on the sparsity of the problem;(2) we analyze asynchronous SGD algorithmsfor non-convex matrix problems including matrix completion; and (3) we design and analyzean asynchronous SGD algorithm; called Buckwild; that uses lower-precision arithmetic. Weshow experimentally that our algorithms run efficiently for a variety of problems on …,Advances in neural information processing systems,2015,47
Towards high-throughput Gibbs sampling at scale: A study across storage managers,Ce Zhang; Christopher Ré,Abstract Factor graphs and Gibbs sampling are a popular combination for Bayesianstatistical methods that are used to solve diverse problems including insurance risk models;pricing models; and information extraction. Given a fixed sampling method and a fixedamount of time; an implementation of a sampler that achieves a higher throughput ofsamples will achieve a higher quality than a lower-throughput sampler. We study how (andwhether) traditional data processing choices about materialization; page layout; and buffer-replacement policy need to be changed to achieve high-throughput Gibbs sampling forfactor graphs that are larger than main memory. We find that both new theoretical and newalgorithmic techniques are required to understand the tradeoff space for each choice. Onboth real and synthetic data; we demonstrate that traditional baseline approaches may …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,43
A machine reading system for assembling synthetic paleontological databases,Shanan E Peters; Ce Zhang; Miron Livny; Christopher Ré,Many aspects of macroevolutionary theory and our understanding of biotic responses toglobal environmental change derive from literature-based compilations of paleontologicaldata. Existing manually assembled databases are; however; incomplete and difficult toassess and enhance with new data types. Here; we develop and validate the quality of amachine reading system; PaleoDeepDive; that automatically locates and extracts data fromheterogeneous text; tables; and figures in publications. PaleoDeepDive performscomparably to humans in several complex data extraction and inference tasks andgenerates congruent synthetic results that describe the geological history of taxonomicdiversity and genus-level rates of origination and extinction. Unlike traditional databases;PaleoDeepDive produces a probabilistic database that systematically improves as …,PLoS One,2014,36
Multiple feature fusion for social media applications,Bin Cui; Anthony KH Tung; Ce Zhang; Zhe Zhao,Abstract The emergence of social media as a crucial paradigm has posed new challenges tothe research and industry communities; where media are designed to be disseminatedthrough social interaction. Recent literature has noted the generality of multiple features inthe social media environment; such as textual; visual and user information. However; most ofthe studies employ only a relatively simple mechanism to merge the features rather than fullyexploit feature correlation for social media applications. In this paper; we propose a novelapproach to fusing multiple features and their correlations for similarity evaluation.Specifically; we first build a Feature Interaction Graph (FIG) by taking features as nodes andthe correlations between them as edges. Then; we employ a probabilistic model based onMarkov Random Field to describe the graph for similarity measure between multimedia …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,35
Evaluating stream filtering for entity profile updates for TREC 2013 (KBA Track Overview),John R Frank; Steven J Bauer; Max Kleiman-Weiner; Daniel A Roberts; Nilesh Tripuraneni; Ce Zhang; Christopher Re; Ellen Voorhees; Ian Soboroff,Abstract: The Knowledge Base Acceleration (KBA) track in TREC 2013 expanded the entity-centric filtering evaluation from TREC KBA 2012. This track evaluates systems that filter atime-ordered corpus for documents and slot fills that would change an entity profile in apredefined list of entities. We doubled the size of the KBA streamcorpus to twelve thousandcontiguous hours and a billion documents from blogs; news; and Web content. Wequadrupled the number of entities as query topics from structured knowledge bases (KB);such as Wikipedia and Twitter. We also added a second task component: identifying entityslot values that change over the course of the stream. This Streaming Slot Filling (SSF)subtask focuses on natural language understanding and is a step toward decomposing theprofile update process undertaken by humans maintaining a knowledge base. A …,*,2013,31
Big data versus the crowd: Looking for relationships in all the right places,Ce Zhang; Feng Niu; Christopher Ré; Jude Shavlik,Abstract Classically; training relation extractors relies on high-quality; manually annotatedtraining data; which can be expensive to obtain. To mitigate this cost; NLU researchers haveconsidered two newly available sources of less expensive (but potentially lower quality)labeled data from distant supervision and crowd sourcing. There is; however; no studycomparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap; we empirically study how state-of-the-art techniques areaffected by scaling these two sources. We use corpus sizes of up to 100 million documentsand tens of thousands of crowd-source labeled examples. Our experiments show thatincreasing the corpus size for distant supervision has a statistically significant; positiveimpact on quality (F1 score). In contrast; human feedback has a positive and statistically …,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,2012,28
Large-scale extraction of gene interactions from full-text literature using DeepDive,Emily K Mallory; Ce Zhang; Christopher Ré; Russ B Altman,Abstract Motivation: A complete repository of gene–gene interactions is key forunderstanding cellular processes; human disease and drug response. These gene–geneinteractions include both protein–protein interactions and transcription factor interactions.The majority of known interactions are found in the biomedical literature. Interactiondatabases; such as BioGRID and ChEA; annotate these gene–gene interactions; however;curation becomes difficult as the literature grows exponentially. DeepDive is a trainedsystem for extracting information from a variety of sources; including text. In this work; weused DeepDive to extract both protein–protein and transcription factor interactions from over100 000 full-text PLOS articles. Methods: We built an extractor for gene–gene interactionsthat identified candidate gene–gene relations within an input sentence. For each …,Bioinformatics,2016,27
Stanford’s 2014 slot filling systems,Gabor Angeli; Sonal Gupta; Melvin Jose; Christopher D Manning; Christopher Ré; Julie Tibshirani; Jean Y Wu; Sen Wu; Ce Zhang,Abstract We describe Stanford's entry in the TACKBP 2014 Slot Filling challenge. Wesubmitted two broad approaches to Slot Filling: one based on the DeepDive framework (Niuet al.; 2012); and another based on the multi-instance multi-label relation extractor ofSurdeanu et al.(2012). In addition; we evaluate the impact of learned and hard-codedpatterns on performance for slot filling; and the impact of the partial annotations described inAngeli et al.(2014).,TAC KBP,2014,24
Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit,Kevin Schawinski; Ce Zhang; Hantian Zhang; Lucas Fowler; Gokula Krishnan Santhanam,Abstract Observations of astrophysical objects such as galaxies are limited by varioussources of random and systematic noise from the sky background; the optical system of thetelescope and the detector used to record the data. Conventional deconvolution techniquesare limited in their ability to recover features in imaging data by the Shannon–Nyquistsampling theorem. Here; we train a generative adversarial network (GAN) on a sample of4550 images of nearby galaxies at 0.01< z< 0.02 from the Sloan Digital Sky Survey andconduct 10× cross-validation to evaluate the results. We present a method using a GANtrained on galaxy images that can recover features from artificially degraded images withworse seeing and higher noise than the original with a performance that far exceeds simpledeconvolution. The ability to better recover detailed features such as galaxy morphology …,Monthly Notices of the Royal Astronomical Society: Letters,2017,23
Asynchrony begets momentum; with an application to deep learning,Ioannis Mitliagkas; Ce Zhang; Stefan Hadjis; Christopher Ré,Asynchronous methods are widely used in deep learning; but have limited theoreticaljustification when applied to non-convex problems. We show that running stochasticgradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function;so it is applicable to deep learning systems. We observe that a standard queuing model ofasynchrony results in a form of momentum that is commonly used by deep learningpractitioners. This forges a link between queuing theory and asynchrony in deep learningsystems; which could be useful for systems builders. For convolutional neural networks; weexperimentally validate that the degree of asynchrony directly correlates with themomentum; confirming our main result. An important implication is that tuning the …,Communication; Control; and Computing (Allerton); 2016 54th Annual Allerton Conference on,2016,23
Feature engineering for knowledge base construction,Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang,Abstract: Knowledge base construction (KBC) is the process of populating a knowledgebase; ie; a relational database together with inference rules; with information extracted fromdocuments and structured sources. KBC blurs the distinction between two traditionaldatabase problems; information extraction and information integration. For the last severalyears; our group has been building knowledge bases with scientific collaborators. Using ourapproach; we have built knowledge bases that have comparable and sometimes betterquality than those constructed by human volunteers. In contrast to these knowledge bases;which took experts a decade or more human years to construct; many of our projects areconstructed by a single graduate student. Our approach to KBC is based on jointprobabilistic inference and learning; but we do not see inference as either a panacea or a …,arXiv preprint arXiv:1407.6439,2014,21
Parallel feature selection inspired by group testing,Yingbo Zhou; Utkarsh Porwal; Ce Zhang; Hung Q Ngo; XuanLong Nguyen; Christopher Ré; Venu Govindaraju,Abstract This paper presents a parallel feature selection method for classification that scalesup to very high dimensions and large data sizes. Our original method is inspired by grouptesting theory; under which the feature selection procedure consists of a collection ofrandomized tests to be performed in parallel. Each test corresponds to a subset of features;for which a scoring function may be applied to measure the relevance of the features in aclassification task. We develop a general theory providing sufficient conditions under whichtrue features are guaranteed to be correctly identified. Superior performance of our methodis demonstrated on a challenging relation extraction task from a very large data set that haveboth redundant features and sample size in the order of millions. We present comprehensivecomparisons with state-of-the-art feature selection methods on a range of data sets; for …,Advances in Neural Information Processing Systems,2014,21
Understanding tables in context using standard NLP toolkits,Vidhya Govindaraju; Ce Zhang; Christopher Ré,Abstract Tabular information in text documents contains a wealth of information; and sotables are a natural candidate for information extraction. There are many cues buried in botha table and its surrounding text that allow us to understand the meaning of the data in atable. We study how natural-language tools; such as part-of-speech tagging; dependencypaths; and named-entity recognition; can be used to improve the quality of relation extractionfrom tables. In three domains we show that (1) a model that performs joint probabilisticinference across tabular and natural language features achieves an F1 score that is twice ashigh as either a puretable or pure-text system; and (2) using only shallower features or non-joint inference results in lower quality.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2013,20
Caffe con Troll: Shallow ideas to speed up deep learning,Stefan Hadjis; Firas Abuzaid; Ce Zhang; Christopher Ré,Abstract We present Caffe con Troll (CcT); a fully compatible end-to-end version of thepopular framework Caffe with rebuilt internals. We built CcT to examine the performancecharacteristics of training and deploying general-purpose convolutional neural networksacross different hardware architectures. We find that; by employing standard batchingoptimizations for CPU training; we achieve a 6: 3× throughput improvement over Caffe onpopular networks like CaffeNet. Moreover; with these improvements; the end-to-end trainingtime for CNNs is directly proportional to the FLOPS delivered by the CPU; which enables usto efficiently train hybrid CPU-GPU systems for CNNs.,Proceedings of the Fourth Workshop on Data analytics in the Cloud,2015,19
Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries,Yuke Zhu; Ce Zhang; Christopher Ré; Li Fei-Fei,Abstract: The complexity of the visual world creates significant challenges forcomprehensive visual understanding. In spite of recent successes in visual recognition;today's vision systems would still struggle to deal with visual queries that require a deeperreasoning. We propose a knowledge base (KB) framework to handle an assortment of visualqueries; without the need to train new classifiers for new tasks. Building such a large-scalemultimodal KB presents a major challenge of scalability. We cast a large-scale MRF into aKB representation; incorporating visual; textual and structured data; as well as their diverserelations. We introduce a scalable knowledge base construction system that is capable ofbuilding a KB with half billion variables and millions of parameters in a few hours. Oursystem achieves competitive results compared to purpose-built models on standard …,arXiv preprint arXiv:1507.05670,2015,18
GeoDeepDive: statistical inference using familiar data-processing languages,Ce Zhang; Vidhya Govindaraju; Jackson Borchardt; Tim Foltz; Christopher Ré; Shanan Peters,Abstract We describe our proposed demonstration of GeoDeepDive; a system that helpsgeoscientists discover information and knowledge buried in the text; tables; and figures ofgeology journal articles. This requires solving a host of classical data managementchallenges including data acquisition (eg; from scanned documents); data extraction; anddata integration. SIGMOD attendees will see demonstrations of three aspects of oursystem:(1) an end-to-end system that is of a high enough quality to perform novel geologicalscience; but is written by a small enough team so that each aspect can be manageablyexplained;(2) a simple feature engineering system that allows a user to write in familiar SQLor Python; and (3) the effect of different sources of feedback on result quality including expertlabeling; distant supervision; traditional rules; and crowd-sourced data. Our prototype …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,18
Scaling inference for Markov logic via dual decomposition,Feng Niu; Ce Zhang; Christopher Re; Jude Shavlik,Markov logic is a knowledge-representation language that allows one to specify largegraphical models. However; the resulting large graphical models can make inference forMarkov logic a computationally challenging problem. Recently; dual decomposition (DD)has become a popular approach for scalable inference on graphical models. We study howto apply DD to scale up inference in Markov logic. A standard approach for DD first partitionsa graphical model into multiple tree-structured sub problems. We apply this approach toMarkov logic and show that DD can outperform prior inference approaches. Nevertheless;we observe that the standard approach for DD is suboptimal as it does not exploit the richstructure often present in the Markov logic program. Thus; we describe a noveldecomposition strategy that partitions a Markov logic program into parts based on its …,Data Mining (ICDM); 2012 IEEE 12th International Conference on,2012,18
Content-enriched classifier for web video classification,Bin Cui; Ce Zhang; Gao Cong,Abstract With the explosive growth of online videos; automatic real-time categorization ofWeb videos plays a key role for organizing; browsing and retrieving the huge amount ofvideos on the Web. Previous work shows that; in addition to text features; content features ofvideos are also useful for Web video classification. Unfortunately; extracting content featuresis computationally prohibitive for real-time video classification. In this paper we propose anovel video classification framework that is able to exploit both content and text features forvideo classification while avoiding the expensive computation of extracting content featuresat classification time. The main idea of our approach is to utilize the content featuresextracted from training data to enrich the text based semantic kernels; yielding content-enriched semantic kernels. The content-enriched semantic kernels enable to utilize both …,Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,2010,18
An approximate; efficient LP solver for LP rounding,Srikrishna Sridhar; Stephen Wright; Christopher Re; Ji Liu; Victor Bittorf; Ce Zhang,Abstract Many problems in machine learning can be solved by rounding the solution of anappropriate linear program. We propose a scheme that is based on a quadratic programrelaxation which allows us to use parallel stochastic-coordinate-descent to approximatelysolve large linear programs efficiently. Our software is an order of magnitude faster thanCplex (a commercial linear programming solver) and yields similar solution quality. Ourresults include a novel perturbation analysis of a quadratic-penalty formulation of linearprogramming and a convergence result; which we use to derive running time and qualityguarantees.,Advances in Neural Information Processing Systems,2013,16
Cyclades: Conflict-free asynchronous machine learning,Xinghao Pan; Maximilian Lam; Stephen Tu; Dimitris Papailiopoulos; Ce Zhang; Michael I Jordan; Kannan Ramchandran; Chris Re; Benjamin Recht,Abstract We present Cyclades; a general framework for parallelizing stochastic optimizationalgorithms in a shared memory setting. Cyclades is asynchronous during model updates;and requires no memory locking mechanisms; similar to Hogwild!-type algorithms. UnlikeHogwild!; Cyclades introduces no conflicts during parallel execution; and offers a black-boxanalysis for provable speedups across a large family of algorithms. Due to its inherent cachelocality and conflict-free nature; our multi-core implementation of Cyclades consistentlyoutperforms Hogwild!-type algorithms on sufficiently sparse datasets; leading to up to 40%speedup gains compared to Hogwild!; and up to 5\times gains over asynchronousimplementations of variance reduction algorithms.,NIPS,2016,15
Felix: Scaling inference for markov logic with an operator-based approach,Feng Niu; Ce Zhang; Christopher Ré; Jude Shavlik,Abstract We examine how to scale up text-processing applications that are expressed in alanguage; Markov Logic; that allows one to express both logical and statistical rules. Ouridea is to exploit the observation that to build text-processing applications one must solve ahost of common subtasks; eg; named-entity extraction; relationship discovery; coreferenceresolution. For some subtasks; there are specialized algorithms that achieve both highquality and high performance. But current general-purpose statistical inference approachesare oblivious to these subtasks and so use a single algorithm independent of the subtasksthat they are performing. The result is that general purpose approaches have either lowerquality; performance; or both compared to the specialized approaches. To combat this; wepresent Felix. In Felix programs are expressed in Markov Logic but are executed using a …,ArXiv e-prints,2011,15
ZipML: Training Linear Models with End-to-End Low Precision; and a Little Bit of Deep Learning,Hantian Zhang; Jerry Li; Kaan Kara; Dan Alistarh; Ji Liu; Ce Zhang,Abstract Recently there has been significant interest in training machine-learning models atlow precision: by reducing precision; one can reduce computation and communication byone order of magnitude. We examine training at reduced precision; both from a theoreticaland practical perspective; and ask: is it possible to train models at end-to-end low precisionwith provable guarantees? Can this lead to consistent order-of-magnitude speedups? Wemainly focus on linear models; and the answer is yes for linear models. We develop a simpleframework called ZipML based on one simple but novel strategy called double sampling.Our ZipML framework is able to execute training at low precision with no bias; guaranteeingconvergence; whereas naive quantization would introduce significant bias. We validate ourframework across a range of applications; and show that it enables an FPGA prototype …,International Conference on Machine Learning,2017,13
Omnivore: An optimizer for multi-device deep learning on cpus and gpus,Stefan Hadjis; Ce Zhang; Ioannis Mitliagkas; Dan Iter; Christopher Ré,Abstract: We study the factors affecting training time in multi-device deep learning systems.Given a specification of a convolutional neural network; our goal is to minimize the time totrain this model on a cluster of commodity CPUs and GPUs. We first focus on the single-node setting and show that by using standard batching and data-parallel techniques;throughput can be improved by at least 5.5 x over state-of-the-art systems on CPUs. Thisensures an end-to-end training speed directly proportional to the throughput of a deviceregardless of its underlying hardware; allowing each node in the cluster to be treated as ablack box. Our second contribution is a theoretical and empirical study of the tradeoffsaffecting end-to-end training time in a multiple-device setting. We identify the degree ofasynchronous parallelization as a key factor affecting both hardware and statistical …,arXiv preprint arXiv:1606.04487,2016,13
Heterogeneity-aware distributed parameter servers,Jiawei Jiang; Bin Cui; Ce Zhang; Lele Yu,Abstract We study distributed machine learning in heterogeneous environments in this work.We first conduct a systematic study of existing systems running distributed stochasticgradient descent; we find that; although these systems work well in homogeneousenvironments; they can suffer performance degradation; sometimes up to 10x; inheterogeneous environments where stragglers are common because their synchronizationprotocols cannot fit a heterogeneous setting. Our first contribution is a heterogeneity-awarealgorithm that uses a constant learning rate schedule for updates before adding them to theglobal parameter. This allows us to suppress stragglers' harm on robust convergence. As afurther improvement; our second contribution is a more sophisticated learning rate schedulethat takes into consideration the delayed information of each update. We theoretically …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,12
Stanford’s distantly supervised slot filling systems for KBP 2014,Gabor Angeli; Sonal Gupta; Melvin Johnson Premkumar; Christopher D Manning; Christopher Ré; Julie Tibshirani; Jean Y Wu; Sen Wu; Ce Zhang,Abstract We describe Stanford's entry in the TACKBP 2014 Slot Filling challenge. Wesubmitted two broad approaches to Slot Filling; both strongly based on the ideas of distantsupervision: one built on the Deep-Dive framework (Niu et al.; 2012); and another based onthe multi-instance multilabel relation extractor of Surdeanu et al.(2012). In addition; weevaluate the impact of learned and hard-coded patterns on performance for slot filling; andthe impact of the partial annotations described in Angeli et al.(2014).,Text Analysis Conference (TAC-KBP),2015,9
Rapidly mixing gibbs sampling for a class of factor graphs using hierarchy width,Christopher M De Sa; Ce Zhang; Kunle Olukotun; Christopher Ré,Abstract Gibbs sampling on factor graphs is a widely used inference technique; which oftenproduces good empirical results. Theoretical guarantees for its performance are weak: evenfor tree structured graphs; the mixing time of Gibbs may be exponential in the number ofvariables. To help understand the behavior of Gibbs sampling; we introduce a new (hyper)graph property; called hierarchy width. We show that under suitable conditions on theweights; bounded hierarchy width ensures polynomial mixing time. Our study of hierarchywidth is in part motivated by a class of factor graph templates; hierarchical templates; whichhave bounded hierarchy width—regardless of the data used to instantiate them. Wedemonstrate a rich application from natural language processing in which Gibbs samplingprovably mixes rapidly and achieves accuracy that exceeds human volunteers.,Advances in neural information processing systems,2015,9
FPGA-accelerated Dense Linear Machine Learning: A Precision-Convergence Trade-off,Kaan Kara; Dan Alistarh; Gustavo Alonso; Onur Mutlu; Ce Zhang,Stochastic gradient descent (SGD) is a commonly used algorithm for training linear machinelearning models. Based on vector algebra; it benefits from the inherent parallelism availablein an FPGA. In this paper; we first present a single-precision floating-point SGDimplementation on an FPGA that provides similar performance as a 10-core CPU. We thenadapt the design to make it capable of processing low-precision data. The low-precisiondata is obtained from a novel compression scheme-called stochastic quantization;specifically designed for machine learning applications. We test both full-precision and low-precision designs on various regression and classification data sets. We achieve up to anorder of magnitude training speedup when using low-precision data compared to a full-precision SGD on the same FPGA and a state-of-the-art multi-core solution; while …,Field-Programmable Custom Computing Machines (FCCM); 2017 IEEE 25th Annual International Symposium on,2017,6
A machine-compiled macroevolutionary history of Phanerozoic life,Shanan E Peters; Ce Zhang; Miron Livny; Christopher Ré,Abstract: Many aspects of macroevolutionary theory and our understanding of bioticresponses to global environmental change derive from literature-based compilations ofpalaeontological data. Existing manually assembled databases are; however; incompleteand difficult to assess and enhance. Here; we develop and validate the quality of a machinereading system; PaleoDeepDive; that automatically locates and extracts data fromheterogeneous text; tables; and figures in publications. PaleoDeepDive performscomparably to humans in complex data extraction and inference tasks and generatescongruent synthetic macroevolutionary results. Unlike traditional databases; PaleoDeepDiveproduces a probabilistic database that systematically improves as information is added. Wealso show that the system can readily accommodate sophisticated data types; such as …,arXiv preprint arXiv:1406.2963,2014,6
Modeling user expertise in folksonomies by fusing multi-type features,Junjie Yao; Bin Cui; Qiaosha Han; Ce Zhang; Yanhong Zhou,Abstract The folksonomy refers to the online collaborative tagging system which offers a newopen platform for content annotation with uncontrolled vocabulary. As folksonomies aregaining in popularity; the expert search and spammer detection in folksonomies attract moreand more attention. However; most of previous work are limited on some folksonomyfeatures. In this paper; we introduce a generic and flexible user expertise model for expertsearch and spammer detection. We first investigate a comprehensive set of expertiseevidences related to users; objects and tags in folksonomies. Then we discuss the richinteractions between them and propose a unified Continuous CRF model to integrate thesefeatures and interactions. This model's applications for expert recommendation andspammer detection are also exploited. Extensive experiments are conducted on a real …,International Conference on Database Systems for Advanced Applications,2011,6
Semantic similarity based on compact concept ontology,Ce Zhang; Yu-Jing Wang; Bin Cui; Gao Cong,Abstract This paper presents a new method of calculating the semantic similarity betweentwo articles based on WordNet. To further improve the performance of the proposed method;we build a new Compact Concept Ontology (CCO) from WordNet by combining the wordswith similar semantic meanings. The experimental results show that our approachsignificantly outperforms a recent proposal of computing semantic similarity; anddemonstrate the superiority of the proposed CCO method.,Proceedings of the 17th international conference on World Wide Web,2008,6
Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent,Xiangru Lian; Ce Zhang; Huan Zhang; Cho-Jio Hsieh; Wei Zhang; Ji Liu,Abstract Most distributed machine learning systems nowadays; including TensorFlow andCNTK; are built in a centralized fashion. One bottleneck of centralized algorithms lies onhigh communication cost on the central node. Motivated by this; we ask; can decentralizedalgorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community; existing analysis and theorydo not show any advantage over centralized PSGD (C-PSGD) algorithms; simply assumingthe application scenario where only the decentralized network is available. In this paper; westudy a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime inwhich decentralized algorithms might outperform centralized algorithms for distributedstochastic gradient descent. This is because D-PSGD has comparable total …,*,2017,5
An experimental evaluation of simrank-based similarity search algorithms,Zhipeng Zhang; Yingxia Shao; Bin Cui; Ce Zhang,Abstract Given a graph; SimRank is one of the most popular measures of the similaritybetween two vertices. We focus on efficiently calculating SimRank; which has been studiedintensively over the last decade. This has led to many algorithms that efficiently calculate orapproximate SimRank being proposed by researchers. Despite these abundant researchefforts; there is no systematic comparison of these algorithms. In this paper; we conduct astudy to compare these algorithms to understand their pros and cons. We first introduce ataxonomy for different algorithms that calculate SimRank and classify each algorithm intoone of the following three classes; namely; iterative-; non-iterative-; and random walk-basedmethod. We implement ten algorithms published from 2002 to 2015; and compare themusing synthetic and real-world graphs. To ensure the fairness of our study; our …,Proceedings of the VLDB Endowment,2017,5
Feature engineering for knowledge base construction,Ce Zhang; Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu,Abstract Knowledge base construction (KBC) is the process of populating a knowledgebase; ie; a relational database together with inference rules; with information extracted fromdocuments and structured sources. KBC blurs the distinction between two traditionaldatabase problems; information extraction and information integration. For the last severalyears; our group has been building knowledge bases with scientific collaborators. Using ourapproach; we have built knowledge bases that have comparable and sometimes betterquality than those constructed by human volunteers. In contrast to these knowledge bases;which took experts a decade or more human years to construct; many of our projects areconstructed by a single graduate student. Our approach to KBC is based on jointprobabilistic inference and learning; but we do not see inference as either a panacea or a …,CoRR; abs/1407.6439,2014,5
A Revisit of Query Expansion with different semantic levels,Ce Zhang; Bin Cui; Gao Cong; Yu-Jing Wang,Abstract Query expansion has received extensive attention in information retrievalcommunity. Although semantic based query expansion appears to be promising inimproving retrieval performance; previous research has shown that it cannot consistentlyimprove retrieval performance. It is a tricky problem to automatically determine whether to doquery expansion for a given query. In this paper; we introduce Compact Concept Ontology(CCO) and provide users the option of exploring different semantic levels by using differentCCOs. Experimental results show our approach is superior to previous work in many cases.Additionally; we integrate the proposed methods into a text-based video search system(iVSearcher); to improve the user's experience and retrieval performance significantly. Tothe best of our knowledge; this is the first system that integrates semantic information into …,International Conference on Database Systems for Advanced Applications,2009,3
Video annotation system based on categorizing and keyword labelling,Bin Cui; Bei Pan; Heng Tao Shen; Ying Wang; Ce Zhang,Abstract In this work; we demonstrate an automatic video annotation system which canprovide users with the representative keywords for new videos. The system explores thehierarchical concept model and multiple feature model to improve the effectiveness ofannotation; which consists of two components: a SVM classifier to ascertain the category;and a multiple feature model to label the keywords. We implement the demo system usingthe videos downloaded from YouTube. The results show the superiority of our approach.,International Conference on Database Systems for Advanced Applications,2009,3
Mlog: Towards declarative in-database machine learning,Xupeng Li; Bin Cui; Yiru Chen; Wentao Wu; Ce Zhang,Abstract We demonstrate MLog; a high-level language that integrates machine learning intodata management systems. Unlike existing machine learning frameworks (eg; TensorFlow;Theano; and Caffe); MLog is declarative; in the sense that the system manages all datamovement; data persistency; and machine-learning related optimizations (such as databatching) automatically. Our interactive demonstration will show audience how this isachieved based on the novel notion of tensoral views (TViews); which are similar torelational views but operate over tensors with linear algebra. With MLog; users cansuccinctly specify not only simple models such as SVM (in just two lines); but alsosophisticated deep learning models that are not supported by existing in-database analyticssystems (eg; MADlib; PAL; and SciDB); as a series of cascaded TViews. Given the …,Proceedings of the VLDB Endowment,2017,2
TencentBoost: A Gradient Boosting Tree System with Parameter Server,Jie Jiang; Jiawei Jiang; Bin Cui; Ce Zhang,Gradient boosting tree (GBT); a widely used machine learning algorithm; achieves state-of-the-art performance in academia; industry; and data analytics competitions. Althoughexisting scalable systems which implement GBT; such as XGBoost and MLlib; perform wellfor datasets with medium-dimensional features; they can suffer performance degradation formany industrial applications where the trained datasets contain highdimensional features.The performance degradation derives from their inefficient mechanisms for modelaggregation—either mapreduce or all-reduce. To address this high-dimensional problem;we propose a scalable execution plan using the parameter server architecture to facilitatethe model aggregation. Further; we introduce a sparse-pull method and an efficient indexstructure to increase the processing speed. We implement a GBT system; namely …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,2
An Overreaction to the Broken Machine Learning Abstraction: The ease.ml Vision,Ce Zhang; Wentao Wu; Tian Li,Abstract After hours of teaching astrophysicists TensorFlow and then see them;nevertheless; continue to struggle in the most creative way possible; we asked; What is thepoint of all of these efforts? It was a warm winter afternoon; Zurich was not gloomy at all;while Seattle was sunny as usual; and Beijing's air was crystally clear. One of the authorsstormed out of a Marathon meeting with biologists; and our journey of overreaction begins.We ask; Can we build a system that gets domain experts completely out of the machinelearning loop? Can this system have exactly the same interface as linear regression; thebare minimum requirement of a scientist?,Workshop on Human-In-the-Loop Data Analytics (HILDA),2017,2
Scaling Inference for Markov Logic with a Task-Decomposition Approach,Feng Niu; Ce Zhang; Christopher Ré; Jude Shavlik,Abstract: Motivated by applications in large-scale knowledge base construction; we studythe problem of scaling up a sophisticated statistical inference framework called MarkovLogic Networks (MLNs). Our approach; Felix; uses the idea of Lagrangian relaxation frommathematical programming to decompose a program into smaller tasks while preserving thejoint-inference property of the original MLN. The advantage is that we can use highlyscalable specialized algorithms for common tasks such as classification and coreference.We propose an architecture to support Lagrangian relaxation in an RDBMS which we showenables scalable joint inference for MLNs. We empirically validate that Felix is significantlymore scalable and efficient than prior approaches to MLN inference by constructing aknowledge base from 1.8 M documents as part of the TAC challenge. We show that Felix …,arXiv preprint arXiv:1108.0294,2011,2
User manual of tuffy 0.3,AnHai Doan; Feng Niu; Christopher Ré; Jude Shavlik; Ce Zhang,Markov logic networks (MLNs)[7; 1] have emerged as a powerful framework that combinesstatistical and logical reasoning; they have been applied to many data intensive problemsincluding information extraction; entity resolution; text mining; and natural languageprocessing. Based on principled data management techniques; Tuffy is an MLN engine thatachieves scalability and orders of magnitude speedup compared to prior artimplementations. It is written in Java and relies on PostgreSQL. For a brief introduction toMLNs and the technical details of Tuffy; please see our VLDB 2011 paper [4].,*,2011,2
Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads,Tian Li; Jie Zhong; Ji Liu; Wentao Wu; Ce Zhang,Abstract We present ease. ml; a declarative machine learning service platform. With ease.ml; a user defines the high-level schema of an ML application and submits the task via aWeb interface. The system then deals with the rest; such as model selection and datamovement. The ultimate question we hope to understand is that; as a" service provider" thatmanages a shared cluster of machines running machine learning workloads; what is theresource sharing strategy that maximizes the global satisfaction of all our users? This paperdoes not completely answer this general question; but focuses on solving the first technicalchallenge we were facing when trying to build ease. ml. We observe that resource sharing isa critical yet subtle issue in this multi-tenant scenario; as we have to balance betweenefficiency and fairness. We first formalize the problem that we call multi-tenant model …,Proceedings of the VLDB Endowment (PVLDB),2018,1
Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond,Heng Guo; Kaan Kara; Ce Zhang,Abstract: For Markov chain Monte Carlo methods; one of the greatest discrepancies betweentheory and system is the scan order-while most theoretical development on the mixing timeanalysis deals with random updates; real-world systems are implemented with systematicscans. We bridge this gap for models that exhibit a bipartite structure; including; mostnotably; the Restricted/Deep Boltzmann Machine. The de facto implementation for thesemodels scans variables in a layerwise fashion. We show that the Gibbs sampler with alayerwise alternating scan order has its relaxation time (in terms of epochs) no larger thanthat of a random-update Gibbs sampler (in terms of variable updates). We also constructexamples to show that this bound is asymptotically tight. Through standard inequalities; ourresult also implies a comparison on the mixing times.,International Conference on Artificial Intelligence and Statistics (AISTATS),2018,1
LDA*: a robust and large-scale topic modeling system,Lele Yut; Ce Zhang; Yingxia Shao; Bin Cui,Abstract We present LDA*; a system that has been deployed in one of the largest Internetcompanies to fulfil their requirements of" topic modeling as an internal service"---relying onthousands of machines; engineers in different sectors submit their data; some are as largeas 1.8 TB; to LDA* and get results back in hours. LDA* is motivated by the observation thatnone of the existing topic modeling systems is robust enough---Each of these existingsystems is designed for a specific point in the tradeoff space that can be sub-optimal;sometimes by up to 10×; across workloads. Our first contribution is a systematic study of allrecently proposed samplers: AliasLDA; F+ LDA; LightLDA; and WarpLDA. We discovered anovel system tradeoff among these samplers. Each sampler has different samplingcomplexity and performs differently; sometimes by 5×; on documents with different lengths …,Proceedings of the VLDB Endowment,2017,1
Android malware development on public malware scanning platforms: A large-scale data-driven study,Heqing Huang; Cong Zheng; Junyuan Zeng; Wu Zhou; Sencun Zhu; Peng Liu; Suresh Chari; Ce Zhang,Android malware scanning services (eg; VirusTotal) are websites that users submitsuspicious Android programs and get an array of malware detection results. With thegrowing popularity of such websites; we suspect that; these services are not only used byinnocent users; but also; malware writers for testing the evasion capability of their malwaresamples. May this hypothesis be true; it not only provides interesting insight on Androidmalware development (AMD); but also provides opportunities for important securityapplications such as zero-day sample detection. In this work; we first validate this hypothesiswith massive data; then design a system AMDHunter to hunt for AMDs on VirusTotal thatreveals new threats for Android that has never been revealed before. This is the firstsystematic study of the malware development phenomenon on VirusTotal; and the first …,Big Data (Big Data); 2016 IEEE International Conference on,2016,1
A model for AGN variability on multiple timescales,Lia F Sartori; Kevin Schawinski; Benny Trakhtenbrot; Neven Caplar; Ezequiel Treister; Michael J Koss; C Megan Urry; Ce Zhang,Abstract We present a framework to link and describe AGN variability on a wide range oftimescales; from days to billions of years. In particular; we concentrate on the AGN variabilityfeatures related to changes in black hole fuelling and accretion rate. In our framework; thevariability features observed in different AGN at different timescales may be explained asrealisations of the same underlying statistical properties. In this context; we propose a modelto simulate the evolution of AGN light curves with time based on the probability densityfunction (PDF) and power spectral density (PSD) of the Eddington ratio (L/L Edd)distribution. Motivated by general galaxy population properties; we propose that the PDFmay be inspired by the L/L Edd distribution function (ERDF); and that a single (or limitednumber of) ERDF+ PSD set may explain all observed variability features. After outlining …,Monthly Notices of the Royal Astronomical Society: Letters,2018,*
Compressive Sensing with Low Precision Data Representation: Radio Astronomy and Beyond,Nezihe Merve Gürel; Kaan Kara; Dan Alistarh; Ce Zhang,Abstract: Modern scientific instruments produce vast amounts of data; which can overwhelmthe processing ability of computer systems. Lossy compression of data is an intriguingsolution but comes with its own dangers; such as potential signal loss; and the need forcareful parameter optimization. In this work; we focus on a setting where this problem isespecially acute compressive sensing frameworks for radio astronomy and ask: Can theprecision of the data representation be lowered for all input data; with recovery guaranteesand good practical performance? Our first contribution is a theoretical analysis of theIterative Hard Thresholding (IHT) algorithm when all input data; that is; the measurementmatrix and the observation; are quantized aggressively; to as little as 2 bits per value. Underreasonable constraints; we show that there exists a variant of low precision IHT which can …,arXiv preprint arXiv:1802.04907,2018,*
DataBright: Towards a Global Exchange for Decentralized Data Ownership and Trusted Computation,David Dao; Dan Alistarh; Claudiu Musat; Ce Zhang,Abstract: It is safe to assume that; for the foreseeable future; machine learning; especiallydeep learning will remain both data-and computation-hungry. In this paper; we ask: Can webuild a global exchange where everyone can contribute computation and data to train thenext generation of machine learning applications? We present an early; but runningprototype of DataBright; a system that turns the creation of training examples and the sharingof computation into an investment mechanism. Unlike most crowdsourcing platforms; wherethe contributor gets paid when they submit their data; DataBright pays dividends whenever acontributor's data or hardware is used by someone to train a machine learning model. Thecontributor becomes a shareholder in the dataset they created. To enable the measurementof usage; a computation platform that contributors can trust is also necessary. DataBright …,arXiv preprint arXiv:1802.04780,2018,*
A Large-scale Study of Android Malware Development Phenomenon on Public Malware Submission and Scanning Platform,Heqing Huang; Cong Zheng; Junyuan Zeng; Wu Zhou; Sencun Zhu; Peng Liu; Ian Molloy; Suresh Chari; Ce Zhang; Quanlong Guan,With the steady growth of Android malware; we suspect that; during the malwaredevelopment phase; some Android malware writers use the popular public scanningservices for testing the evasion capability of their malware samples; which we name Androidmalware development cases (AMDs). In this work; we design an AMD hunter in the contextof VirusTotal to hunt for AMDs and reveal new threats for Android. First; the AMD hunter siftsthrough millions of file submissions on VirusTotal efficiently and alert more suspicioussubmission traces. Second; it performs package level analysis; static code and dynamicanalyses on the APKs of the suspicious submissions to validate the AMDs. The implementedhunter has been used in a leading security company for 4 months; which processed 153million of submissions on VirusTotal; and identified 1;623 AMDs with 13;855 samples …,IEEE Transactions on Big Data,2018,*
DimBoost: Boosting Gradient Boosting Tree to Higher Dimensions,Jiawei Jiang; Bin Cui; Ce Zhang; Fangcheng Fu,*,SIGMOD,2018,*
Generative adversarial networks as a tool to recover structural information from cryo-electron microscopy data,Min Su; Hantian Zhang; Kevin Schawinski; Ce Zhang; Michael A Cianfrocco,Cryo-electron microscopy (cryo-EM) is a powerful structural biology technique capable ofdetermining atomic-resolution structures of biological macromolecules. However; theprocess of optimizing sample preparation and data collection requires expert-level usersmany months to years in order to find amenable conditions to collect and determine high-resolution structures due to the low signal-to-noise (SNR) ratio of the raw cryo-EMmicrographs. To help address this problem; we have trained and tested generativeadversarial networks; a form of artificial intelligence; to denoise and CTF-correct individualparticles. This approach effectively recovers global structural information for both syntheticand real cryo-EM data; facilitating per-particle assessment from noisy raw images. Ourresults suggest that generative adversarial networks may be able to provide an approach …,bioRxiv,2018,*
Exploring galaxy evolution with latent space walks,Kevin Schawinski; Dennis Turp; Ce Zhang,Abstract We present a new approach using artificial intelligence to perform data-drivenforward models of astrophysical phenomena. We describe how a variational autoencodercan be used to encode galaxies to latent space; independently manipulate properties suchas the specific star formation rate; and return it to real space. Such transformations can beused for forward modeling phenomena using data as the only constraints. We demonstratethe utility of this approach using the question of the quenching of star formation in galaxies.,American Astronomical Society Meeting Abstracts,2018,*
Generative Models in Deep Learning: Constraints for Galaxy Evolution,Maximilian Dennis Turp; Kevin Schawinski; Ce Zhang; Anna K Weigel,Abstract New techniques are essential to make advances in the field of galaxy evolution.Recent developments in the field of artificial intelligence and machine learning have proventhat these tools can be applied to problems far more complex than simple image recognition.We use these purely data driven approaches to investigate the process of star formationquenching. We show that Variational Autoencoders provide a powerful method to forwardmodel the process of galaxy quenching. Our results imply that simple changes in specificstar formation rate and bulge to disk ratio cannot fully describe the properties of thequenched population.,American Astronomical Society Meeting Abstracts,2018,*
Synchronous Multi-GPU Deep Learning with Low-Precision Communication: An Experimental Study,Demjan Grubic; Leo Tam; Dan Alistarh; Ce Zhang,*,International Conference on Extending Database Technology (EDBT),2018,*
Asynchronous Decentralized Parallel Stochastic Gradient Descent,Xiangru Lian; Wei Zhang; Ce Zhang; Ji Liu,Abstract: Recent work shows that decentralized parallel stochastic gradient decent (D-PSGD) can outperform its centralized counterpart both theoretically and practically. Whileasynchronous parallelism is a powerful technology to improve the efficiency of parallelism indistributed machine learning platforms and has been widely used in many popular machinelearning softwares and solvers based on centralized parallel protocols such as Tensorflow; itstill remains unclear how to apply the asynchronous parallelism to improve the efficiency ofdecentralized parallel algorithms. This paper proposes an asynchronous decentralizeparallel stochastic gradient descent algorithm to apply the asynchronous parallelismtechnology to decentralized algorithms. Our theoretical analysis provides the convergencerate or equivalently the computational complexity; which is consistent with many special …,arXiv preprint arXiv:1710.06952,2017,*
Scalable inference of decision tree ensembles: Flexible design for CPU-FPGA platforms,Muhsen Owaida; Hantian Zhang; Ce Zhang; Gustavo Alonso,Decision tree ensembles are commonly used in a wide range of applications and becomingthe de facto algorithm for decision tree based classifiers. Different trees in an ensemble canbe processed in parallel during tree inference; making them a suitable use case for FPGAs.Large tree ensembles; however; require careful mapping of trees to on-chip memory andmanagement of memory accesses. As a result; existing FPGA solutions suffer from theinability to scale beyond tens of trees and lack the flexibility to support different treeensembles. In this paper we present an FPGA tree ensemble classifier together with asoftware driver to efficiently manage the FPGA's memory resources. The classifierarchitecture efficiently utilizes the FPGA's resources to fit half a million tree nodes in on-chipmemory; delivering up to 20× speedup over a 10-threaded CPU implementation when …,Field Programmable Logic and Applications (FPL); 2017 27th International Conference on,2017,*
How Good Are Machine Learning Clouds for Binary Classification with Good Features?,Hantian Zhang; Luyuan Zeng; Wentao Wu; Ce Zhang,Abstract: We conduct an empirical study of machine learning functionalities provided bymajor cloud service providers; which we call em machine learning clouds. Machine learningclouds hold the promise of hiding all the sophistication of running large-scale machinelearning: Instead of specifying how to run a machine learning task; users only specify whatmachine learning task to run and the cloud figures out the rest. Raising the level ofabstraction; however; rarely comes free---a performance penalty is possible. How good;then; are current machine learning clouds on real-world machine learning workloads? Westudy this question by presenting mlbench; a novel benchmark dataset constructed with thetop winning code for all available competitions on Kaggle; as well as the results we obtainedby running mlbench on machine learning clouds from both Azure and Amazon. We …,arXiv preprint arXiv:1707.09562,2017,*
GalaxyGAN: Generative Adversarial Networks for recovery of galaxy features,Kevin Schawinski; Ce Zhang; Hantian Zhang; Lucas Fowler; Gokula Krishnan Santhanam,Abstract GalaxyGAN uses Generative Adversarial Networks to reliably recover features inimages of galaxies. The package uses machine learning to train on higher quality data andlearns to recover detailed features such as galaxy morphology by effectively building priors.This method opens up the possibility of recovering more information from existing and futureimaging data.,Astrophysics Source Code Library,2017,*
Tradeoffs in Main-Memory Statistical Analytics from Impala to DimmWitted.,Victor Bittorf; Marcel Kornacker; Christopher Ré; Ce Zhang,Recent years have seen a surge in main-memory SQL-style analytic solutions to quicklydeliver business critical information over massive data sets [1; 7; 14]. At the same time; thereis an arms race to offer increasingly sophisticated statistical analytics inspired by thesuccess of web search; voice recognition; and image analysis; eg; Google Brain [8];Facebook [6]; and Microsoft's Adam [2]. This talk describes the first author's experienceporting statistical analytics to Impala via MADlib and observations about research for high-performance main-memory analytics that may be relevant for systems like Impala. A majormotivation for Impala was to enable interactive SQL-analytics queries over data stored inHadoop. Impala achieves high performance through many techniques including as co-location of computation with data in HDFS; LLVM code generation [13]; and aggressive …,IMDM@ VLDB,2014,*
USER MANUAL OF FELIX 0.2,Feng Niu; Christopher Ré; Jude Shavlik; Josh Slauson; Ce Zhang,Recent years have seen a surge of sophisticated statistical frameworks with the relationaldata model (via SQL/logic-like languages). Examples include MLNs and PRMs. While thismovement has demonstrated significant quality advantage in numerous applications onsmall datasets; efficiency and scalability have been a critical challenge to their deploymentin Enterprise settings. Felix addresses such performance challenges with an operator-basedapproach; where each operator performs a statistical algorithm with relational input/output.The key observations underlying Felix are 1. Task Decomposition: Conventionalapproaches to statistical-relational inference are monolithic in that; not only do they expresscomplex tasks in a “unified” language; they also attempt to solve the tasks by running ageneric algorithm (eg; random walk; greedy; or sampling) over the entire program. This …,*,2011,*
Felix: Scaling up Global Statistical Information Extraction Using an Operator-based Approach,Feng Niu; Ce Zhang; Christopher Ré; Jude Shavlik,Abstract To support the next generation of sophisticated information extraction (IE)applications; several researchers have proposed frameworks that integrate SQL-likelanguages with statistical reasoning. While these frameworks demonstrate impressivequality on small IE tasks; they currently do not scale to enterprise-sized tasks. To enable thenext generation of IE; a promising approach is to improve the scalability and performance ofsuch statistical frameworks. Our technical observation is that many IE subtasks; such ascoreference resolution or classification; can be solved by specialized algorithms thatachieve both high quality and high performance. In contrast; current general-purposestatistical inference approaches are oblivious to these subtasks and so use a singlealgorithm independent of the subtask that they are performing. We present Felix; in which …,*,2011,*
Faster Max-Product Message Passing using Lazy Feature Extractors,Ce Zhang,*,*,*,*
Data Engineering,Eric Gribkoff; Dan Suciu; Guy Van den Broeck; Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang; Matthias Boehm; Douglas R Burdick; Alexandre V Evfimievski; Berthold Reinwald; Frederick R Reiss; Prithviraj Sen; Shirish Tatikonda; Yuanyuan Tian; Botong Huang; Nicholas WD Jarrett; Shivnath Babu; Sayan Mukherjee; Jun Yang,Modern knowledge bases such as Yago [14]; DeepDive [19]; and Google's Knowledge Vault[6] are constructed from large corpora of text by using some form of supervised informationextraction. The extracted data usually starts as a large probabilistic database; then itsaccuracy is improved by adding domain knowledge expressed as hard or soft constraints.Finally; the knowledge base can be queried using some general-purpose query language(SQL; or Sparql). A key technical challenge during the construction; refinement; andquerying of knowledge bases is probabilistic reasoning. Because of the size of the datainvolved; probabilistic reasoning in knowledge bases becomes a central data managementproblem. The number of random variables is very large; typically one for each fact in theknowledge base. Most systems today perform inference by using Markov Chain Monte …,*,*,*
