Hybrid parallelization strategies for large-scale machine learning in SystemML,Matthias Boehm; Shirish Tatikonda; Berthold Reinwald; Prithviraj Sen; Yuanyuan Tian; Douglas R Burdick; Shivakumar Vaithyanathan,Abstract SystemML aims at declarative; large-scale machine learning (ML) on top ofMapReduce; where high-level ML scripts with R-like syntax are compiled to programs of MRjobs. The declarative specification of ML algorithms enables---in contrast to existing large-scale machine learning libraries---automatic optimization. SystemML's primary focus is ondata parallelism but many ML algorithms inherently exhibit opportunities for task parallelismas well. A major challenge is how to efficiently combine both types of parallelism for arbitraryML scripts and workloads. In this paper; we present a systematic approach for combiningtask and data parallelism for large-scale machine learning on top of MapReduce. Weemploy a generic Parallel FOR construct (ParFOR) as known from high performancecomputing (HPC). Our core contributions are (1) complementary parallelization strategies …,Proceedings of the VLDB Endowment,2014,46
Data management in the mirabel smart grid system,Matthias Boehm; Lars Dannecker; Andreas Doms; Erik Dovgan; Bogdan Filipič; Ulrike Fischer; Wolfgang Lehner; Torben Bach Pedersen; Yoann Pitarch; Laurynas Šikšnys; Tea Tušar,Abstract Nowadays; Renewable Energy Sources (RES) are attracting more and moreinterest. Thus; many countries aim to increase the share of green energy and have to facewith several challenges (eg; balancing; storage; pricing). In this paper; we address thebalancing challenge and present the MIRABEL project which aims to prototype an EnergyData Management System (EDMS) which takes benefit of flexibilities to efficiently balanceenergy demand and supply. The EDMS consists of millions of heterogeneous nodes thateach incorporates advanced components (eg; aggregation; forecasting; scheduling;negotiation). We describe each of these components and their interaction. Preliminaryexperimental results confirm the feasibility of our EDMS.,Proceedings of the 2012 Joint EDBT/ICDT Workshops,2012,39
Efficient In-Memory Indexing with Generalized Prefix Trees.,Matthias Boehm; Benjamin Schlegel; Peter Benjamin Volk; Ulrike Fischer; Dirk Habich; Wolfgang Lehner,Abstract: Efficient data structures for in-memory indexing gain in importance due to (1) theexponentially increasing amount of data;(2) the growing main-memory capacity; and (3) thegap between main-memory and CPU speed. In consequence; there are high performancedemands for in-memory data structures. Such index structures are used—with minorchanges—as primary or secondary indices in almost every DBMS. Typically; tree-based orhash-based structures are used; while structures based on prefix-trees (tries) are neglectedin this context. For tree-based and hash-based structures; the major disadvantages areinherently caused by the need for reorganization and key comparisons. In contrast; themajor disadvantage of trie-based structures in terms of high memory consumption (createdand accessed nodes) could be improved. In this paper; we argue for reconsidering prefix …,BTW,2011,37
Compressed linear algebra for large-scale machine learning,Ahmed Elgohary; Matthias Boehm; Peter J Haas; Frederick R Reiss; Berthold Reinwald,Abstract Large-scale machine learning (ML) algorithms are often iterative; using repeatedread-only data access and I/O-bound matrix-vector multiplications to converge to an optimalmodel. It is crucial for performance to fit the data into single-node or distributed mainmemory. General-purpose; heavy-and lightweight compression techniques struggle toachieve both good compression ratios and fast decompression speed to enable block-wiseuncompressed operations. Hence; we initiate work on compressed linear algebra (CLA); inwhich lightweight database compression techniques are applied to matrices and then linearalgebra operations such as matrix-vector multiplication are executed directly on thecompressed representations. We contribute effective column compression schemes; cache-conscious operations; and an efficient sampling-based compression algorithm. Our …,Proceedings of the VLDB Endowment,2016,23
Resource elasticity for large-scale machine learning,Botong Huang; Matthias Boehm; Yuanyuan Tian; Berthold Reinwald; Shirish Tatikonda; Frederick R Reiss,Abstract Declarative large-scale machine learning (ML) aims at flexible specification of MLalgorithms and automatic generation of hybrid runtime plans ranging from single node; in-memory computations to distributed computations on MapReduce (MR) or similarframeworks. State-of-the-art compilers in this context are very sensitive to memoryconstraints of the master process and MR cluster configuration. Different memoryconfigurations can lead to significant performance differences. Interestingly; resourcenegotiation frameworks like YARN allow us to explicitly request preferred resourcesincluding memory. This capability enables automatic resource elasticity; which is not justimportant for performance but also removes the need for a static cluster configuration; whichis always a compromise in multi-tenancy environments. In this paper; we introduce a …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,23
SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs.,Matthias Boehm; Douglas R Burdick; Alexandre V Evfimievski; Berthold Reinwald; Frederick R Reiss; Prithviraj Sen; Shirish Tatikonda; Yuanyuan Tian,Abstract SystemML enables declarative; large-scale machine learning (ML) via a high-levellanguage with R-like syntax. Data scientists use this language to express their MLalgorithms with full flexibility but without the need to hand-tune distributed runtime executionplans and system configurations. These ML programs are dynamically compiled andoptimized based on data and cluster characteristics using ruleand cost-based optimizationtechniques. The compiler automatically generates hybrid runtime execution plans rangingfrom in-memory; single node execution to distributed MapReduce (MR) computation anddata access. This paper describes the SystemML optimizer; its compilation chain; andselected optimization phases for generating efficient execution plans.,IEEE Data Eng. Bull.,2014,22
SystemML: Declarative machine learning on spark,Matthias Boehm; Michael W Dusenberry; Deron Eriksson; Alexandre V Evfimievski; Faraz Makari Manshadi; Niketan Pansare; Berthold Reinwald; Frederick R Reiss; Prithviraj Sen; Arvind C Surve; Shirish Tatikonda,Abstract The rising need for custom machine learning (ML) algorithms and the growing datasizes that require the exploitation of distributed; data-parallel frameworks such asMapReduce or Spark; pose significant productivity challenges to data scientists. ApacheSystemML addresses these challenges through declarative ML by (1) increasing theproductivity of data scientists as they are able to express custom algorithms in a familiardomain-specific language covering linear algebra primitives and statistical functions; and (2)transparently running these ML algorithms on distributed; data-parallel frameworks byapplying cost-based compilation techniques to generate efficient; low-level execution planswith in-memory single-node and large-scale distributed operations. This paper describesSystemML on Apache Spark; end to end; including insights into various optimizer and …,Proceedings of the VLDB Endowment,2016,20
Dipbench toolsuite: A framework for benchmarking integration systems,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,So far the optimization of integration processes between heterogeneous data sources is stillan open challenge. A first step towards sufficient techniques was the specification of auniversal benchmark for integration systems. This DIPBench allows to compare solutionsunder controlled conditions and would help generate interest in this research area.However; we see the requirement for providing a sophisticated toolsuite in order to minimizethe effort for benchmark execution. This demo illustrates the use of the DIPBench toolsuite.We show the macro-architecture as well as the micro-architecture of each tool. Furthermore;we also present the first reference benchmark implementation using a federated DBMS.Thereby; we discuss the impact of the defined benchmark scale factors. Finally; we want togive guidance on how to benchmark other integration systems and how to extend the …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,19
Context-aware parameter estimation for forecast models in the energy domain,Lars Dannecker; Robert Schulze; Matthias Böhm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Continuous balancing of energy demand and supply is a fundamental prerequisitefor the stability and efficiency of energy grids. This balancing task requires accurateforecasts of future electricity consumption and production at any point in time. For thispurpose; database systems need to be able to rapidly process forecasting queries and toprovide accurate results in short time frames. However; time series from the electricitydomain pose the challenge that measurements are constantly appended to the time series.Using a naive maintenance approach for such evolving time series would mean a re-estimation of the employed mathematical forecast model from scratch for each newmeasurement; which is very time consuming. We speed-up the forecast model maintenanceby exploiting the particularities of electricity time series to reuse previously employed …,International Conference on Scientific and Statistical Database Management,2011,18
Towards Self-Optimization of Message Transformation Processes.,Matthias Böhm; Dirk Habich; Uwe Wloka; Jürgen Bittner; Wolfgang Lehner,Abstract. The Message Transformation Model (MTM); for modeling complex messagetransformation processes in data centric application scenarios; provides strong capabilitiesfor describing the data and control flow; transactional behavior and even the interaction withexternal systems. Thus; this general model could be used for different integration platformslike EAI-Servers; Message-Brokers and Subscription-Systems; as well as for ETL-Tools. Inthis paper; we describe self-optimization strategies for MTM processes to determine anoptimal executable process. Our proposed strategies can be distinguished into rule-basedand workload-based techniques. Aside from theoretical consideration; we describeimplementation aspects within the integration platform Trans-ConnectR©. Furthermore; wepresent some first evaluation results.,ADBIS Research Communications,2007,15
Towards integrated data analytics: Time series forecasting in DBMS,Ulrike Fischer; Lars Dannecker; Laurynas Siksnys; Frank Rosenthal; Matthias Boehm; Wolfgang Lehner,Abstract Integrating sophisticated statistical methods into database management systems isgaining more and more attention in research and industry in order to be able to cope withincreasing data volume and increasing complexity of the analytical algorithms. Oneimportant statistical method is time series forecasting; which is crucial for decision makingprocesses in many domains. The deep integration of time series forecasting offers additionaladvanced functionalities within a DBMS. More importantly; however; it allows foroptimizations that improve the efficiency; consistency; and transparency of the overallforecasting process. To enable efficient integrated forecasting; we propose to enhance thetraditional 3-layer ANSI/SPARC architecture of a DBMS with forecasting functionalities. Thisarticle gives a general overview of our proposed enhancements and presents how …,Datenbank-Spektrum,2013,12
On optimizing machine learning workloads via kernel fusion,Arash Ashari; Shirish Tatikonda; Matthias Boehm; Berthold Reinwald; Keith Campbell; John Keenleyside; P Sadayappan,Abstract Exploitation of parallel architectures has become critical to scalable machinelearning (ML). Since a wide range of ML algorithms employ linear algebraic operators;GPUs with BLAS libraries are a natural choice for such an exploitation. Two approaches arecommonly pursued:(i) developing specific GPU accelerated implementations of completeML algorithms; and (ii) developing GPU kernels for primitive linear algebraic operators likematrix-vector multiplication; which are then used in developing ML algorithms. This paperextends the latter approach by developing fused kernels for a combination of primitiveoperators that are commonly found in popular ML algorithms. We identify the generic patternof computation (alpha* X^ T (v*(X* y))+ beta* z) and its various instantiations. We develop afused kernel to optimize this computation on GPUs--with specialized techniques to handle …,ACM SIGPLAN Notices,2015,11
Indexing forecast models for matching and maintenance,Ulrike Fischer; Frank Rosenthal; Matthias Boehm; Wolfgang Lehner,Abstract Forecasts are important to decision-making and risk assessment in many domains.There has been recent interest in integrating forecast queries inside a DBMS. Answering aforecast query requires the creation of forecast models. Creating a forecast model is anexpensive process and may require several scans over the base data as well as expensiveoperations to estimate model parameters. However; if forecast queries are issuedrepeatedly; answer times can be reduced significantly if forecast models are reused. Due tothe possibly high number of forecast queries; existing models need to be found quickly.Therefore; we propose a model index that efficiently stores forecast models and allows forthe efficient reuse of existing ones. Our experiments illustrate that the model index shows anegligible overhead for update transactions; but it yields significant improvements during …,Proceedings of the Fourteenth International Database Engineering & Applications Symposium,2010,11
Exploting Renewables by Request-Based Balancing of Energy Demand and Supply,Henrike Berthold; Matthias Böhm; Lars Dannecker; Frens-Jan Rumph; Torben Bach Pedersen; Christos Nychtis; Hellmuth Frey; Zoran Marinsek; Bogdan Filipic; Stathis Tselepis,{"controller"=>"catalog"; "action"=>"show"; "locale"=>"en"; "id"=>"2389358771 …,Proceedings of the the 11th Iaee European Conference: Energy Economy; Policies and Supply Security: Surviving the Global Economic Crisis,2010,11
Workload-based optimization of integration processes,Matthias Boehm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract The efficient execution of integration processes between distributed;heterogeneous data sources and applications is a challenging research area of datamanagement. These integration processes are an abstraction for workflow-based integrationtasks; used in EAI servers and WfMS. The major problem are significant workload changesduring runtime. The performance of integration processes strongly depends on thosedynamic workload characteristics; and hence workload-based optimization is important.However; existing approaches of workflow optimization only address the rule-basedoptimization and disregard changing workload characteristics. To overcome the problem ofinefficient process execution in the presence of workload shifts; here; we present anapproach for the workload-based optimization of instance-based integration processes …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,11
Model-Driven Generation and Optimization of Complex Integration Processes.,Matthias Böhm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract: The integration of heterogeneous systems is still one of the main challenges in thearea of data management. Its importance is based on the trend towards heterogeneoussystem environments; where the different levels of integration approaches result in a largenumber of different integration systems. Due to these proprietary solutions and the lack of astandard for data-intensive integration processes; the model-driven development—followingthe paradigm of the Model-Driven Architecture (MDA)—is advantageous. This papercontributes to the model-driven development of complex and data-intensive integrationprocesses. In addition; we illustrate optimization possibilities offered by this model-drivenapproach and discuss first evaluation results.,ICEIS (1),2008,11
Forcasting evolving time series of energy demand and supply,Lars Dannecker; Matthias Böhm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Real-time balancing of energy demand and supply requires accurate and efficientforecasting in order to take future consumption and production into account. Thesebalancing capabilities are reasoned by emerging energy market developments; which alsopose new challenges to forecasting in the energy domain not addressed so far: First; real-time balancing requires accurate forecasts at any point in time. Second; the hierarchicalmarket organization motivates forecasting in a distributed system environment. In this paper;we present an approach that adapts forecasting to the hierarchical organization of today'senergy markets. Furthermore; we introduce a forecasting framework; which allows efficientforecasting and forecast model maintenance of time series that evolve due to continuousstreams of measurements. This framework includes model evaluation and adaptation …,East European Conference on Advances in Databases and Information Systems,2011,10
Offline Design Tuning for Hierarchies of Forecast Models.,Ulrike Fischer; Matthias Boehm; Wolfgang Lehner,Abstract: Forecasting of time series data is crucial for decision-making processes in manydomains as it allows the prediction of future behavior. In this context; a model is fit to theobserved data points of the time series by estimating the model parameters. The computedparameters are then utilized to forecast future points in time. Existing approaches integrateforecasting into traditional relational query processing; where a forecast query requests thecreation of a forecast model. Models of continued interest should be deployed only once andused many times afterwards. This however leads to additional maintenance costs as modelsneed to be kept up-to-date. Costs can be reduced by choosing a well-defined subset ofmodels and answering queries using derivation schemes. In contrast to materialized viewselection; model selection opens a whole new problem area as results are approximate …,BTW,2011,10
Context-aware parameter estimation for forecast models,*,Methods; systems; and computer-readable storage media for providing at least oneparameter for use with a forecast model. Implementations include actions of receiving a firstcontext vector; the first context vector including a plurality of context attributes that describe afirst context; retrieving a first parameter vector from a repository based on the first contextvector; the repository electronically storing a plurality of parameter vector; each parametervector being associated with a respective context and including one or more parameters;parameterizing the forecast model based on parameters provided in the first parametervector to provide a parameterized forecast model; optimizing the parameterized forecastmodel to provide an optimized forecast model; and forecasting one or more values using theoptimized forecast model.,*,2016,9
DIPBench: An independent benchmark for data-intensive integration processes,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,The integration of heterogeneous data sources is one of the main challenges within the areaof data engineering. Due to the absence of an independent and universal benchmark fordata-intensive integration processes; we propose a scalable benchmark; called DIPBench(Data Intensive Integration Process Benchmark); for evaluating the performance ofintegration systems. This benchmark could be used for subscription systems; like replicationservers; distributed and federated DBMS or message-oriented middleware platforms likeEnterprise Application Integration (EAI) servers and Extraction Transformation Loading(ETL) tools. In order to reach the mentioned universal view for integration processes; thebenchmark is designed in a conceptual; process-driven way. The benchmark comprises 15integration process types. We specify the source and target data schemas and provide a …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,9
Data management in machine learning: Challenges; techniques; and systems,Arun Kumar; Matthias Boehm; Jun Yang,Abstract Large-scale data analytics using statistical machine learning (ML); popularly calledadvanced analytics; underpins many modern data-driven applications. The datamanagement community has been working for over a decade on tackling data management-related challenges that arise in ML workloads; and has built several systems for advancedanalytics. This tutorial provides a comprehensive review of such systems and analyzes keydata management challenges and techniques. We focus on three complementary lines ofwork:(1) integrating ML algorithms and languages with existing data systems such asRDBMSs;(2) adapting data management-inspired techniques such as query optimization;partitioning; and compression to new systems that target ML workloads; and (3) combiningdata management and ML ideas to build systems that improve ML lifecycle-related tasks …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,8
A high-throughput in-memory index; durable on flash-based SSD: insights into the winning solution of the SIGMOD programming contest 2011,Thomas Kissinger; Benjamin Schlegel; Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Growing memory capacities and the increasing number of cores on modernhardware enforces the design of new in-memory indexing structures that reduce the numberof memory transfers and minimizes the need for locking to allow massive parallel access.However; most applications depend on hard durability constraints requiring a persistentmedium like SSDs; which shorten the latency and throughput gap between main memoryand hard disks. In this paper; we present our winning solution of the SIGMOD ProgrammingContest 2011. It consists of an in-memory indexing structure that provides a balancedread/write performance as well as non-blocking reads and single-lock writes.Complementary to this index; we describe an SSD-optimized logging approach to fit harddurability requirements at a high throughput rate.,ACM SIGMOD Record,2012,8
Cost-based optimization of integration flows,Matthias Böhm,Abstract Integration flows are increasingly used to specify and execute data-intensiveintegration tasks between several heterogeneous systems and applications. There are manydifferent application areas such as (near) real-time ETL (Extraction Transformation Loading)and data synchronization between operational systems. For the reasons of (1) an increasingamount of data;(2) typically highly distributed IT infrastructures; and (3) high requirements fordata consistency and up-to-dateness; many instances of integration flows—with rather smallamounts of data per instance—are executed over time by the central integration platform.Due to this high load as well as blocking synchronous source systems or client applications;the performance of the central integration platform is crucial for an IT infrastructure. As aresult; there is a need for optimizing integration flows. Existing approaches for the …,*,2011,8
Cost-based vectorization of instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract Integration processes are workflow-based integration tasks. The inefficiency ofthese processes is often caused by low resource utilization and significant waiting times forexternal systems. With the aim to overcome these problems; we proposed the concept ofprocess vectorization. There; instance-based integration processes are transparentlyexecuted with the pipes-and-filters execution model. The term vectorization is used in thesense of processing a sequence (vector) of messages by one standing process. Although ithas been shown that process vectorization achieves a significant throughput improvement;this concept has two major drawbacks. First; the theoretical performance of a vectorizedintegration process mainly depends on the performance of the most cost-intensive operator.Second; the practical performance strongly depends on the number of used threads and …,Information Systems,2011,8
Cost-based vectorization of instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract The inefficiency of integration processes—as an abstraction of workflow-basedintegration tasks—is often reasoned by low resource utilization and significant waiting timesfor external systems. With the aim to overcome these problems; we proposed the concept ofprocess vectorization. There; instance-based integration processes are transparentlyexecuted with the pipes-and-filters execution model. Here; the term vectorization is used inthe sense of processing a sequence (vector) of messages by one standing process.Although it has been shown that process vectorization achieves a significant throughputimprovement; this concept has two major drawbacks. First; the theoretical performance of avectorized integration process mainly depends on the performance of the most cost-intensive operator. Second; the practical performance strongly depends on the number of …,East European Conference on Advances in Databases and Information Systems,2009,8
GCIP: Exploiting the generation and optimization of integration processes,Matthias Boehm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract As a result of the changing scope of data management towards the management ofhighly distributed systems and applications; integration processes have gained inimportance. Such integration processes represent an abstraction of workflow-basedintegration tasks. In practice; integration processes are pervasive and the performance ofcomplete IT infrastructures strongly depends on the performance of the central integrationplatform that executes the specified integration processes. In this area; the three majorproblems are:(1) significant development efforts;(2) low portability; and (3) inefficientexecution. To overcome those problems; we follow a model-driven generation approach forintegration processes. In this demo proposal; we want to introduce the so-called GCIPFramework (Generation of Complex Integration Processes) which allows the modeling of …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,8
Declarative Machine Learning-A Classification of Basic Properties and Types,Matthias Boehm; Alexandre V Evfimievski; Niketan Pansare; Berthold Reinwald,Abstract: Declarative machine learning (ML) aims at the high-level specification of ML tasksor algorithms; and automatic generation of optimized execution plans from thesespecifications. The fundamental goal is to simplify the usage and/or development of MLalgorithms; which is especially important in the context of large-scale computations.However; ML systems at different abstraction levels have emerged over time andaccordingly there has been a controversy about the meaning of this general definition ofdeclarative ML. Specification alternatives range from ML algorithms expressed in domain-specific languages (DSLs) with optimization for performance; to ML task (learning problem)specifications with optimization for performance and accuracy. We argue that these differenttypes of declarative ML complement each other as they address different users (data …,arXiv preprint arXiv:1605.05826,2016,7
SPOOF: Sum-Product Optimization and Operator Fusion for Large-Scale Machine Learning.,Tarek Elgamal; Shangyu Luo; Matthias Boehm; Alexandre V Evfimievski; Shirish Tatikonda; Berthold Reinwald; Prithviraj Sen,Page 1. © 2017 IBM Corporation SPOOF: Sum-Product Optimization and Operator Fusion forLarge-Scale Machine Learning Tarek Elgamal2; Shangyu Luo3; Matthias Boehm1; AlexandreV. Evfimievski1; Shirish Tatikonda4; Berthold Reinwald1; Prithviraj Sen1 1 IBM Research –Almaden 2 University of Illinois 3 Rice University 4 Target Corporation IBM Research Page 2. ©2017 IBM Corporation Motivation ▪ Declarative Large-Scale Machine Learning (ML) – Simplifydevelopment / usage of ML tasks or algorithms – SystemML: High-level language → dataindependence / plan generation – State-of-the-art compilers: rewrites; operator selection; fusedoperators ▪ Ubiquitous Optimization Opportunities – Example Rewrites: X ┬ y → (y ┬ X) ┬ ;sum(λ X) → λ sum(X); trace(XY) → sum(X ʘ Y ┬ ) – Example Fused operators: sum(X ʘ Y ʘZ); X ┬ (X v); sum(X ʘ log(UV ┬ )) 2 IBM Research X Y Z * sum * X v X 1st pass 2nd …,CIDR,2017,6
Model-driven development of complex and data-intensive integration processes,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract Due to the changing scope of data management from centrally stored data towardsthe management of distributed and heterogeneous systems; the integration takes place ondifferent levels. The lack of standards for information integration as well as applicationintegration resulted in a large number of different integration models and proprietarysolutions. With the aim of a high degree of portability and the reduction of developmentefforts; the model-driven development—following the Model-Driven Architecture (MDA)—isadvantageous in this context as well. Hence; in the GCIP project (Generation of ComplexIntegration Processes); we focus on the model-driven generation and optimization ofintegration tasks using a process-based approach. In this paper; we contribute detailedgeneration aspects and finally discuss open issues and further challenges.,*,2008,6
An Advanced Transaction Model for Recovery Processing of Integration Processes.,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. Integration processes are increasingly used in order to integrate distributed andheterogeneous systems. Although transactional behavior of workflows has been discussedextensively; recovery processing has been disregarded so far. Due to the huge number ofdifferent integration systems and models; there are also different transaction concepts withoverlapping functionalities available. However; there is the need for a discussion of problemcategories and guarantees in order to consolidate the existing transaction concepts. In thispaper; we survey possible anomalies of recovery processing in message-orientedmiddleware and—in conclusion—we define the comprehensive transaction model SIR fordata-intensive—but instance-based—integration processes. This model includes thedefinition of specific transaction levels; which are the precondition for the integration …,ADBIS (local proceedings),2008,6
Optimizing notifications of subscription-based forecast queries,Ulrike Fischer; Matthias Böhm; Wolfgang Lehner; Torben Bach Pedersen,Abstract Integrating sophisticated statistical methods into database management systems isgaining more and more attention in research and industry. One important statistical methodis time series forecasting; which is crucial for decision management in many domains. In thiscontext; previous work addressed the processing of ad-hoc and recurring forecast queries.In contrast; we focus on subscription-based forecast queries that arise when an application(subscriber) continuously requires forecast values for further processing. Forecast queriesexhibit the unique characteristic that the underlying forecast model is updated with each newactual value and better forecast values might be available. However;(re-) sending newforecast values to the subscriber for every new value is infeasible because this can causesignificant overhead at the subscriber side. The subscriber therefore wishes to be notified …,International Conference on Scientific and Statistical Database Management,2012,5
Vectorizing instance-based integration processes,Matthias Boehm; Dirk Habich; Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Abstract The inefficiency of integration processes—as an abstraction of workflow-basedintegration tasks—is often reasoned by low resource utilization and significant waiting timesfor external systems. Due to the increasing use of integration processes within ITinfrastructures; the throughput optimization has high influence on the overall performance ofsuch an infrastructure. In the area of computational engineering; low resource utilization isaddressed with vectorization techniques. In this paper; we introduce the concept ofvectorization in the context of integration processes in order to achieve a higher degree ofparallelism. Here; transactional behavior and serialized execution must be ensured. Inconclusion of our evaluation; the message throughput can be significantly increased.,International Conference on Enterprise Information Systems,2009,5
On-demand re-optimization of integration flows,Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Integration flows are used to propagate data between heterogeneous operationalsystems or to consolidate data into data warehouse infrastructures. In order to meet theincreasing need of up-to-date information; many messages are exchanged over time. Theefficiency of those integration flows is therefore crucial to handle the high load of messagesand to reduce message latency. State-of-the-art strategies to address this performancebottleneck are based on incremental statistic maintenance and periodic cost-based re-optimization. This also achieves adaptation to unknown statistics and changing workloadcharacteristics; which is important since integration flows are deployed for long timehorizons. However; the major drawbacks of periodic re-optimization are many unnecessaryre-optimization steps and missed optimization opportunities due to adaptation delays. In …,Information Systems,2014,4
Partitioning and Multi-core Parallelization of Multi-equation Forecast Models,Lars Dannecker; Matthias Boehm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Forecasting is an important analysis technique used in many application domainssuch as electricity management; sales and retail and; traffic predictions. The employedstatistical models already provide very accurate predictions; but recent developments inthese domains pose new requirements on the calculation speed of the forecast models.Especially; the often used multi-equation models tend to be very complex and theirestimation is very time consuming. To still allow the use of these highly accurate forecastmodels; it is necessary to improve the data processing capabilities of the involved datamanagement systems. For this purpose; we introduce a partitioning approach for multi-equation forecast models that considers the specific data access pattern of these models tooptimize the data storage and memory access. With the help of our approach we avoid …,SSDBM,2012,4
Multi-flow optimization via horizontal message queue partitioning,Matthias Boehm; Dirk Habich; Wolfgang Lehner,Abstract Integration flows are increasingly used to specify and execute data-intensiveintegration tasks between heterogeneous systems and applications. There are manydifferent application areas such as near real-time ETL and data synchronization betweenoperational systems. For the reasons of an increasing amount of data; highly distributed ITinfrastructures; as well as high requirements for up-to-dateness of analytical query resultsand data consistency; many instances of integration flows are executed over time. Due tothis high load; the performance of the central integration platform is crucial for an ITinfrastructure. With the aim of throughput maximization; we propose the concept of multi-flowoptimization (MFO). In this approach; messages are collected during a waiting time andexecuted in batches to optimize sequences of plan instances of a single integration flow …,International Conference on Enterprise Information Systems,2010,4
Large scale discriminative metric learning,Peter D Kirchner; Matthias Boehm; Berthold Reinwald; Daby Sow; Michael Schmidt; Deepak Turaga; Alain Biem,We consider the learning of a distance metric; using the Localized Supervised MetricLearning (LSML) scheme; that discriminates entities characterized by high dimensionalfeature attributes; with respect to labels assigned to each entity. LSML is a supervisedlearning scheme that learns a Mahalanobis distance grouping together features with thesame label and repulsing features with different labels. In this paper; we propose an efficientand scalable implementation of LSML allowing us to scale significantly and process largedata sets; both in terms of dimensions and instances. This implementation of LSML isprogrammed in SystemML with an R-like syntax; and compiled; optimized; and executed onHadoop. We also propose experimental approaches for the tuning of LSML parametersyielding significant analytical and empirical improvements in terms of discriminative …,Parallel & Distributed Processing Symposium Workshops (IPDPSW); 2014 IEEE International,2014,3
Compiling machine learning algorithms with SystemML,Matthias Boehm; Douglas Burdick; A Evfimievski; Berthold Reinwald; Prithviraj Sen; Shirish Tatikonda; Yuanyuan Tian,Abstract Analytics on big data range from passenger volume prediction in transportation tocustomer satisfaction in automotive diagnostic systems; and from correlation analysis insocial media data to log analysis in manufacturing. Expressing and running these analyticsfor varying data characteristics and at scale is challenging. To address these challenges;SystemML implements a declarative; high-level language using an R-like syntax extendedwith machine-learning-specific constructs; that is compiled to a MapReduce runtime [2]. Thelanguage is rich enough to express a wide class of statistical; predictive modeling andmachine learning algorithms (Fig. 1). We chose robust algorithms that scale to large; andpotentially sparse data with many features.,Proceedings of the 4th annual Symposium on Cloud Computing,2013,3
Next Generation Database Programming and Execution Environment.,Dirk Habich; Matthias Boehm; Maik Thiele; Benjamin Schlegel; Ulrike Fischer; Hannes Voigt; Wolfgang Lehner,ABSTRACT The database research is always on the move. In order to integrate novelconcepts; the significance of the database programmability aspect more and moreincreases. The programmability aspect focuses on internal components as well as onprinciple to push-down application logic to the database system. In this paper; we propose anovel database programming model and a corresponding database architecture frameworkenabling extensibility and a better integration of application code into DBMS. In detail; wepresent a scripting language pyDBL which is unified utilizable to implement physicaldatabase operators; query plans and even complete applications. We demonstrate theapplicability of our approach in terms of a moderate performance overhead.,DBPL,2011,3
Model-driven generation of dynamic adapters for integration platforms,Matthias Böhm; Jürgen Bittner; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. The concept of Enterprise Application Integration (EAI) is widely used forintegrating heterogeneous applications and systems via message-based communication.Typically; EAI servers provide a huge set of specific inbound and outbound adapters usedfor interacting with the external systems and for converting proprietary message formats.However; one major shortcoming in currently available products is the monolithic design ofthose adapters; resulting in performance deficits caused by the need for data independence.Further; also functional restrictions must be noticed here. In this paper; we give (1) a detailedproblem characterization; followed by (2) a discussion of alternative data representationsand adapter architectures; and (3) we introduce our model-driven DIEFOS (dataindependence; efficiency and functional flexibility using feature-oriented software …,1st International Workshop on Model Driven Interoperability for Sustainable Information Systems,2008,3
Ein Nachrichtentransformationsmodell für komplexe Transformationsprozesse in datenzentrischen Anwendungsszenarien.,Matthias Böhm; Jürgen Bittner; Uwe Wloka; Dirk Habich; Wolfgang Lehner,Abstract: Die horizontale Integration von Systemen durch eine nachrichtenbasierteKommunikation über Middleware-Produkte stellt eine; sich immer weiter verbreitende; Artder Anwendungsintegration dar; um eine hinreichend lose Kopplung der partizipierendenSysteme und Anwendungen zu gewährleisten. Für die Beschreibung derartigerIntegrationsprozesse kommen zunehmend funktional-orientierteProzessbeschreibungssprachen wie beispielsweise WSBPEL zum Einsatz; welcheallerdings Defizite bei der Beschreibung von datenzentrischen Anwendungsszenarienoffenbaren. Dieses Papier leistet einen Beitrag zur systematischen Modellbildung fürkomplexe Nachrichtentransformationen in datenzentrischen Prozessen. Die Realisierbarkeitder Ergebnisse wird dabei an der Integrationsplattform TransConnect R,BTW,2007,3
Multi-process Optimization Via Horizontal Message Queue Partitioning.,Matthias Böhm; Dirk Habich; Wolfgang Lehner,Page 1. ICEIS 2010 Databases and Information Systems Integration Multi-Process Optimizationvia Horizontal Message Queue Partitioning Matthias Böhm; Dirk Habich; Wolfgang Lehner DresdenUniversity of Technology © Prof. Dr.-Ing. Wolfgang Lehner | 10.06.2010 Page 2. > MotivationInformation S P id Systems Pyramid ▪ Horizontal Integration ▪ Data synchronization by many smallmessages → EAI servers; MOM systems ▪ Vertical Integration ▪ Trend towards Operational BI;direct data propagation to the DWH → (near) real-time ETL High Performance Requirements ▪Many independent instances of integration flows → Optimization is required Matthias Böhm |Multi-Process Optimization via Horizontal Message Queue Partitioning | 2 y p g ▪ Temporalconsistency between distributed systems Page 3. > System Architecture Reference SystemArchitecture for integration flows ▪ Inbound / outbound adapters …,ICEIS (1),2010,2
Query Processing on Prefix Trees,Matthias Boehm; Patrick Lehmann; Peter Benjamin Volk; Wolfgang Lehner,*,*,2010,2
Invisible Deployment of Integration Processes,Matthias Boehm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract Due to the changing scope of data management towards the management ofheterogeneous and distributed systems and applications; integration processes gain inimportance. This is particularly true for those processes used as abstractions of workflow-based integration tasks; these are widely applied in practice. In such scenarios; a typical ITinfrastructure comprises multiple integration systems with overlapping functionalities. Themajor problems in this area are high development effort; low portability and inefficiency.Therefore; in this paper; we introduce the vision of invisible deployment that addresses thevirtualization of multiple; heterogeneous; physical integration systems into a single logicalintegration system. This vision comprises several challenging issues in the fields ofdeployment aspects as well as runtime aspects. Here; we describe those challenges …,International Conference on Enterprise Information Systems,2009,2
Model driven development of complex and data intensive integration processes,Matthias Bohm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. Due to the changing scope of data management from centrally stored data towardsthe management of distributed and heterogeneous systems; the integration takes place ondifferent levels. The lack of standards for information integration as well as applicationintegration resulted in a large number of different integration models and proprietarysolutions. With the aim of a high degree of portability and the reduction of developmentefforts; the model-driven development—following the Model-Driven Architecture (MDA)—isadvantageous in this context as well. Hence; in the GCIP project (Generation of ComplexIntegration Processes); we focus on the model-driven generation and optimization ofintegration tasks using a process-based approach. In this paper; we contribute detailedgeneration aspects and finally discuss open issues and further challenges.,Model-Based Software and Data Integration: First International Workshop; MBSDI 2008; Berlin; Germany; April 1-3; 2008; Proceedings,2008,2
Scaling machine learning via compressed linear algebra,Ahmed Elgohary; Matthias Boehm; Peter J Haas; Frederick R Reiss; Berthold Reinwald,Abstract Large-scale machine learning (ML) algorithms are often iterative; using repeatedread-only data access and I/Obound matrix-vector multiplications to converge to an optimalmodel. It is crucial for performance to fit the data into single-node or distributed main memoryand enable very fast matrix-vector operations on in-memory data. Generalpurpose; heavy-and lightweight compression techniques struggle to achieve both good compression ratiosand fast decompression speed to enable block-wise uncompressed operations.Compressed linear algebra (CLA) avoids these problems by applying lightweight losslessdatabase compression techniques to matrices and then executing linear algebra operationssuch as matrix-vector multiplication directly on the compressed representations. The keyingredients are effective column compression schemes; cache-conscious operations; and …,ACM SIGMOD Record,2017,1
Pipelined approach to fused kernels for optimization of machine learning workloads on graphical processing units,*,A method for optimization of machine learning (ML) workloads on a graphics processor unit(GPU). The method includes identifying a computation having a generic pattern commonlyobserved in ML processes. An optimized fused GPU kernel is employed to exploit temporallocality for inherent data-flow dependencies in the identified computation. Hierarchicalaggregation spanning a memory hierarchy of the GPU for processing for the identifiedcomputation is performed. GPU kernel launch parameters are estimated following ananalytical model that maximizes thread occupancy and minimizes atomic writes to GPUglobal memory.,*,2017,1
Costing generated runtime execution plans for large-scale machine learning programs,Matthias Boehm,Abstract: Declarative large-scale machine learning (ML) aims at the specification of MLalgorithms in a high-level language and automatic generation of hybrid runtime executionplans ranging from single node; in-memory computations to distributed computations onMapReduce (MR) or similar frameworks like Spark. The compilation of large-scale MLprograms exhibits many opportunities for automatic optimization. Advanced cost-basedoptimization techniques require---as a fundamental precondition---an accurate cost modelfor evaluating the impact of optimization decisions. In this paper; we share insights into asimple and robust yet accurate technique for costing alternative runtime execution plans ofML programs. Our cost model relies on generating and costing runtime plans in order toautomatically reflect all successive optimization phases. Costing runtime plans also …,arXiv preprint arXiv:1503.06384,2015,1
Efficient integration of external information into forecast models from the energy domain,Lars Dannecker; Elena Vasilyeva; Matthias Boehm; Wolfgang Lehner; Gregor Hackenbroich,Abstract Forecasting is an important analysis technique to support decisions andfunctionalities in many application domains. While the employed statistical models oftenprovide a sufficient accuracy; recent developments pose new challenges to the forecastingprocess. Typically the available time for estimating the forecast models and providingaccurate predictions is significantly decreasing. This is especially an issue in the energydomain; where forecast models often consider external influences to provide a highaccuracy. As a result; these models exhibit a higher number of parameters; resulting inincreased estimation efforts. Also; in the energy domain new measurements are constantlyappended to the time series; requiring a continuous adaptation of the models to newdevelopments. This typically involves a parameter re-estimation; which is often almost as …,East European Conference on Advances in Databases and Information Systems,2012,1
Resiliency-Aware Data Management,Matthias Boehm; Wolfgang Lehner; Christof Fetzer,ABSTRACT Computing architectures change towards massively parallel environments withincreasing numbers of heterogeneous components. The large scale in combination withdecreasing feature sizes leads to dramatically increasing error rates. The heterogeneityfurther leads to new error types. Techniques for ensuring resiliency in terms of robustnessregarding these errors are typically applied at hardware abstraction and operating systemlevels. However; as errors become the normal case; we observe increasing costs in terms ofcomputation overhead for ensuring robustness. In this paper; we argue that ensuringresiliency on the data management level can reduce the required overhead by exploitingcontext knowledge of query processing and data storage. Apart from reacting on alreadydetected errors; this was mostly neglected in database research so far. We therefore give …,Proceedings of the VLDB Endowment,2011,1
Message Indexing for Document-Oriented Integration Processes.,Matthias Böhm; Uwe Wloka; Dirk Habich; Wolfgang Lehner,ABSTRACT The integration of heterogeneous systems is still an evolving research area.Due to the complexity of integration processes; there are challenges for the optimization ofintegration processes. Message-based integration systems; like EAI servers and workflowprocess engines; are mostly documentoriented using XML technologies in order to achievesuitable data independence from the different and particular proprietary datarepresentations of the supported external systems. However; such an approach causeslarge costs for single-value evaluations within the integration processes. At this point;message indexing; adopting extended database technologies; could be applied in order toreach better performance. In this paper; we introduce our message indexing structure MIXand discuss and evaluate immediate as well as deferred indexing concepts. Further; we …,ICEIS (1),2008,1
Improving Data Independence; Efficiency and Functional Flexibility of Integration Platforms.,Matthias Böhm; Jürgen Bittner; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Abstract. The concept of Enterprise Application Integration (EAI) is widely used forintegrating heterogeneous applications and systems via message-based communication.Typically; EAI servers provide a huge set of specific inbound and outbound adapters usedfor interacting with the external systems and for converting proprietary message formats.However; the main problems in currently available products are the monolithic design ofthese adapters and performance deficits caused by the need for data independence. First;we classify and discuss these open problems. Second; we introduce our model-drivenDIEFOS (data independence; efficiency and functional flexibility using feature-orientedsoftware engineering) approach and show how the feature-based generation of dynamicadapters can improve data independence; efficiency and functional flexibility. Finally; we …,CAiSE Forum,2008,1
A message transformation model for data-centric service integration processes,Matthias Böhm; Uwe Wloka; Jürgen Bittner; Dirk Habich; Wolfgang Lehner,ABSTRACT The horizontal integration of systems by message-based communication viamiddleware products is a widespread method of application integration to ensure anadequate loose coupling of participating systems and applications. For the description ofsuch service integration processes; the use of functionally oriented process descriptionlanguages; like WSBPEL; is gaining in importance. However; these languages revealdeficits when describing data-centric application scenarios. Due to these deficits and thelack of a model for service integration processes; this paper contributes to the systematicmodeling of complex message transformations in data-centric integration processes. Thepracticability of the model is shown with a prototypical implementation within the serviceintegration platform TransConnectR© of the SQL GmbH Dresden.,33rd International Very Large Data Bases Conference,2007,1
Deep Learning with Apache SystemML,Niketan Pansare; Michael Dusenberry; Nakul Jindal; Matthias Boehm; Berthold Reinwald; Prithviraj Sen,Abstract: Enterprises operate large data lakes using Hadoop and Spark frameworks that (1)run a plethora of tools to automate powerful data preparation/transformation pipelines;(2)run on shared; large clusters to (3) perform many different analytics tasks ranging frommodel preparation; building; evaluation; and tuning for both machine learning and deeplearning. Developing machine/deep learning models on data in such shared environmentsis challenging. Apache SystemML provides a unified framework for implementing machinelearning and deep learning algorithms in a variety of shared deployment scenarios.SystemML's novel compilation approach automatically generates runtime execution plansfor machine/deep learning algorithms that are composed of single-node and distributedruntime operations depending on data and cluster characteristics such as data size; data …,arXiv preprint arXiv:1802.04647,2018,*
On Optimizing Operator Fusion Plans for Large-Scale Machine Learning in SystemML,Matthias Boehm; Berthold Reinwald; Dylan Hutchison; Alexandre V Evfimievski; Prithviraj Sen,Abstract: Many large-scale machine learning (ML) systems allow specifying custom MLalgorithms by means of linear algebra programs; and then automatically generate efficientexecution plans. In this context; optimization opportunities for fused operators---in terms offused chains of basic operators---are ubiquitous. These opportunities include (1) fewermaterialized intermediates;(2) fewer scans of input data; and (3) the exploitation of sparsityacross chains of operators. Automatic operator fusion eliminates the need for hand-writtenfused operators and significantly improves performance for complex or previously unseenchains of operations. However; existing fusion heuristics struggle to find good fusion plansfor complex DAGs or hybrid plans of local and distributed operations. In this paper; weintroduce an optimization framework for systematically reason about fusion plans that …,arXiv preprint arXiv:1801.00829,2018,*
Dynamic recompilation techniques for machine learning programs,*,The embodiments described herein relate to recompiling an execution plan of a machine-learning program during runtime. An execution plan of a machine-learning program iscompiled. In response to identifying a directed acyclic graph of high-level operations (HOPDAG) for recompilation during runtime; the execution plan is dynamically recompiled. Thedynamic recompilation includes updating statistics and dynamically rewriting one or moreoperators of the identified HOP DAG; recomputing memory estimates of operators of therewritten HOP DAG based on the updated statistics and rewritten operators; constructing adirected acyclic graph of low-level operations (LOP DAG) corresponding to the rewrittenHOP DAG based in part on the recomputed memory estimates; and generating runtimeinstructions based on the LOP DAG.,*,2017,*
R-language integration with a declarative machine learning language,*,In a method for analyzing a large data set using a statistical computing environmentlanguage operation; a processor generates code from the statistical computing environmentlanguage operation that can be understood by a software system for processing machinelearning algorithms in a MapReduce environment. A processor transfers the code to thesoftware system for processing machine learning algorithms in a MapReduce environment.A processor invokes execution of the code with the software system for processing machinelearning algorithms in a MapReduce environment.,*,2017,*
Global data flow optimization for machine learning programs,*,A method for global data flow optimization for machine learning (ML) programs. The methodincludes receiving; by a storage device; an initial plan for an ML program. A processor buildsa nested global data flow graph representation using the initial plan. Operator directedacyclic graphs (DAGs) are connected using crossblock operators according to inter-blockdata dependencies. The initial plan for the ML program is re-written resulting in an optimizedplan for the ML program with respect to its global data flow properties. The re-writingincludes re-writes of: configuration dataflow properties; operator selection and structuralchanges.,*,2017,*
Hybrid parallelization strategies for machine learning programs on top of mapreduce,*,Parallel execution of machine learning programs is provided. Program code is received. Theprogram code contains at least one parallel for statement having a plurality of iterations. Aparallel execution plan is determined for the program code. According to the parallelexecution plan; the plurality of iterations is partitioned into a plurality of tasks. Each taskcomprises at least one iteration. The iterations of each task are independent. Data requiredby the plurality of tasks is determined. An access pattern by the plurality of tasks of the datais determined. The data is partitioned based on the access pattern.,*,2016,*
Hybrid parallelization strategies for machine learning programs on top of MapReduce,*,Hybrid parallelization strategies for machine learning programs on top of MapReduce areprovided. In one embodiment; a method of and computer program product for parallelexecution of machine learning programs are provided. Program code is received. Theprogram code contains at least one parallel for statement having a plurality of iterations. Aparallel execution plan is determined for the program code. According to the parallelexecution plan; the plurality of iterations is partitioned into a plurality of tasks. Each taskcomprises at least one iteration. The iterations of each task are independent.,*,2016,*
D3. 1 State-of-the-art report on data collection and analysis,Laurynas Siksnys; Matthias Boehm; Torben Bach Pedersen; Christian Søndergaard Jensen; Dalia Martisiuté,Abstract: Today; many countries aim to increase the share of energy consumed that comesfrom renewable sources. Unfortunately; the electrical power produced from weather-dependent renewable energy sources (RESs; eg; wind turbines; solar panels) is producedin varying quantities that do not match the varying energy needs. As more and more suchrenewable energy becomes available; it becomes an increasingly difficult challenge tomaintain an energy system that enables the effective use of all available renewable energy.Consequently; tackling this problem is one of the top goals in the energy domain. TheMIRACLE (Micro-Request-Based Aggregation; Forecasting and Scheduling of EnergyDemand; Supply and Distribution) project aims to invent and prototype key elements of anenergy system that is better able to accommodate large volume of electricity from …,*,2010,*
Anfragegetriebene Indizierung räumlicher Daten.,Hannes Voigt; Steffen Preißler; Matthias Böhm; Wolfgang Lehner,Abstract: Mit der zunehmenden Verbreitung von GPS-und internetfähigen Smartphoneswerden ortsbezogene Informationsdienste immer beliebter. Zur Sicherung einer hohenDienstqualität werden die zugrundeliegenden Ortsinformationen indiziert. BekannteIndexstrukturen für räumliche Daten teilen diese gemäß ihrer Verteilung auf; wodurch alleAnfragen gleich behandelt werden. Möchte man häufige Anfrage durch eine genauereIndizierung besonders unterstützen; so muss sich die Aufteilung der Daten nicht an derDatenverteilung; sondern an der Anfrageverteilung orientieren. In diesem Papier stellen wirdas QD-Grid vor; eine räumliche Indexstruktur; deren Indizierung sich inkrementell mit dengestellten Anfragen aufbaut. Zusätzlich präsentieren wir Evaluationsergebnisse.,GI Jahrestagung,2009,*
ADBIS 2009,Matthias Boehm; Dirk Habich; Steffen Preissler1 Matthias Boehm; Dirk Habich Steffen Preissler; Wolfgang Lehner; Uwe Wloka,Page 1. ADBIS 2009 Cost-Based Vectorization of Instance-Based Integration Processes MatthiasBoehm1;2 Dirk Habich1 Steffen Preissler1 Matthias Boehm ; Dirk Habich Steffen PreisslerWolfgang Lehner1 Uwe Wloka2 1 TU Dresden; Database Technology Group; Germany 2 HTWDresden; Database Group; Germany Page 2. Problem Description • Context – Integrationprocesses Receive o1 SAP R/3 Integration processes • Workflow-based integration tasks • Dataexchange between heterogeneous systems and applications 1 Translation o2 SAP R/3 systemsand applications – Eg; EAI servers; MOM; WfMS – Typically; instance-based process executionInvoke o3 JDBC • Problem – Efficiency of the integration systems has high impact C it fdtbt dit ib td t • Consistency of data between distributed systems • Performance of the source systems(synchronous execution) → Optimization strongly needed …,*,*,*
System ubergreifende Kostennormalisierung f ur Integrationsprozesse,Matthias Böhm; Dirk Habich; Wolfgang Lehner; Uwe Wloka,Zusammenfassung Auf Grundlage der Vielzahl proprietärer Integrationssysteme istzunehmend die Entwicklung von Ansätzen zur modellbasierten Generierung vonIntegrationsprozessen zu beobachten. Eine derartige Generierung bietet weiterhin dieMöglichkeit der Auswahl des optimalen Integrationssystems; welche ein hohesOptimierungspotenzial in sich birgt. Die Grundlage für eine solche Entscheidung ist jedocheine integrationssystemübergreifende Kostennormalisierung; um die Vergleichbarkeit vonVerarbeitungsstatistiken zu ermöglichen. Basierend auf einem plattformunabhängigenKostenmodell und der systemübergreifenden Kostennormalisierung kann einekostenbasierte Optimalitätsentscheidung hinsichtlich des effizientesten Integrationssystemsgetroffen werden. Folglich können hierbei veränderliche Workload-Charakteristika in die …,Gesellschaft für Informatik (GI) publishes this series in order to make available to a broad public recent findings in informatics (ie computer science and informa-tion systems); to document conferences that are organized in co-operation with GI and to publish the annual GI Award dissertation.,*,*
Query Processing on Prefix Trees Revisited,Thomas Kissinger; Matthias Boehm; Patrick Lehmann; Wolfgang Lehner,Abstract There is a trend towards operational or Live BI (Business Intelligence) that requiresimmediate synchronization between the operational source systems and the datawarehouse infrastructure in order to achieve high up-to-dateness for analytical query results.The high performance requirements imposed by many ad-hoc queries are typicallyaddressed with read-optimized column stores and in-memory data management. However;Live BI additionally requires transactional and update-friendly in-memory indexing due tohigh update rates of propagated data. For example; in-memory indexing with prefix treesexhibits a well-balanced read/write performance because no index reorganization isrequired. The vision of this project is to use the underlying in-memory index structures; in theform of prefix trees; for query processing as well. Prefix trees are used as intermediate …,*,*,*
Data Engineering,Eric Gribkoff; Dan Suciu; Guy Van den Broeck; Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang; Matthias Boehm; Douglas R Burdick; Alexandre V Evfimievski; Berthold Reinwald; Frederick R Reiss; Prithviraj Sen; Shirish Tatikonda; Yuanyuan Tian; Botong Huang; Nicholas WD Jarrett; Shivnath Babu; Sayan Mukherjee; Jun Yang,Modern knowledge bases such as Yago [14]; DeepDive [19]; and Google's Knowledge Vault[6] are constructed from large corpora of text by using some form of supervised informationextraction. The extracted data usually starts as a large probabilistic database; then itsaccuracy is improved by adding domain knowledge expressed as hard or soft constraints.Finally; the knowledge base can be queried using some general-purpose query language(SQL; or Sparql). A key technical challenge during the construction; refinement; andquerying of knowledge bases is probabilistic reasoning. Because of the size of the datainvolved; probabilistic reasoning in knowledge bases becomes a central data managementproblem. The number of random variables is very large; typically one for each fact in theknowledge base. Most systems today perform inference by using Markov Chain Monte …,*,*,*
