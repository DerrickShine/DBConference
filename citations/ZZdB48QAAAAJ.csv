Gestures without libraries; toolkits or training: a $1 recognizer for user interface prototypes,Jacob O Wobbrock; Andrew D Wilson; Yang Li,Abstract Although mobile; tablet; large display; and tabletop computers increasingly presentopportunities for using pen; finger; and wand gestures in user interfaces; implementinggesture recognition largely has been the privilege of pattern matching experts; not userinterface prototypers. Although some user interface libraries and toolkits offer gesturerecognizers; such infrastructure is often unavailable in design-oriented environments likeFlash; scripting environments like JavaScript; or brand new off-desktop prototypingenvironments. To enable novice programmers to incorporate gestures into their UIprototypes; we present a" $1 recognizer" that is easy; cheap; and usable almost anywhere inabout 100 lines of code. In a study comparing our $1 recognizer; Dynamic Time Warping;and the Rubine classifier on user-supplied gestures; we found that $1 obtains over 97 …,Proceedings of the 20th annual ACM symposium on User interface software and technology,2007,734
User-defined motion gestures for mobile interaction,Jaime Ruiz; Yang Li; Edward Lank,Abstract Modern smartphones contain sophisticated sensors to monitor three-dimensionalmovement of the device. These sensors permit devices to recognize motion gestures-deliberate movements of the device by end-users to invoke commands. However; little isknown about best-practices in motion gesture design for the mobile computing paradigm. Toaddress this issue; we present the results of a guessability study that elicits end-user motiongestures to invoke commands on a smartphone device. We demonstrate that consensusexists among our participants on parameters of movement and on mappings of motiongestures onto commands. We use this consensus to develop a taxonomy for motiongestures and to specify an end-user inspired motion gesture set. We highlight theimplications of this work to the design of smartphone applications and hardware. Finally …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2011,283
Experimental analysis of mode switching techniques in pen-based user interfaces,Yang Li; Ken Hinckley; Zhiwei Guan; James A Landay,Abstract Inking and gesturing are two central tasks in pen-based user interfaces. Switchingbetween modes for entry of uninterpreted ink and entry of gestures is required by many pen-based user interfaces. Without an appropriate mode switching technique; pen-basedinteractions in such situations may be inefficient and cumbersome. In this paper; weinvestigate five techniques for switching between ink and gesture modes in pen interfaces;including a pen-pressure based mode switching technique that allows implicit modetransition. A quantitative experimental study was conducted to evaluate the performance ofthese techniques. The results suggest that pressing a button with the non-preferred handoffers the fastest performance; while the technique of holding the pen still is significantlyslower and more prone to error than the other techniques. Pressure; while promising; did …,Proceedings of the SIGCHI conference on Human factors in computing systems,2005,200
Topiary: a tool for prototyping location-enhanced applications,Yang Li; Jason I Hong; James A Landay,Abstract Location-enhanced applications use the location of people; places; and things toaugment or streamline interaction. Location-enhanced applications are just starting toemerge in several different domains; and many people believe that this type of applicationwill experience tremendous growth in the near future. However; it currently requires a highlevel of technical expertise to build location-enhanced applications; making it hard to iterateon designs. To address this problem we introduce Topiary; a tool for rapidly prototypinglocation-enhanced applications. Topiary lets designers create a map that models thelocation of people; places; and things; use this active map to demonstrate scenariosdepicting location contexts; use these scenarios in creating storyboards that describeinteraction sequences; and then run these storyboards on mobile devices; with a wizard …,Proceedings of the 17th annual ACM symposium on User interface software and technology,2004,195
Protractor: a fast and accurate gesture recognizer,Yang Li,Abstract Protractor is a novel gesture recognizer that can be easily implemented and quicklycustomized for different users. Protractor uses a nearest neighbor approach; whichrecognizes an unknown gesture based on its similarity to each of the known gestures; eg;training samples or examples given by a user. In particular; it employs a novel method tomeasure the similarity between gestures; by calculating a minimum angular distancebetween them with a closed-form solution. As a result; Protractor is more accurate; naturallycovers more gesture variation; runs significantly faster and uses much less memory than itspeers. This makes Protractor suitable for mobile computing; which is limited in processingpower and memory. An evaluation on both a previously published gesture data set and anewly collected gesture data set indicates that Protractor outperforms its peers in many …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2010,151
Experimental analysis of touch-screen gesture designs in mobile environments,Andrew Bragdon; Eugene Nelson; Yang Li; Ken Hinckley,Abstract Direct-touch interaction on mobile phones revolves around screens that competefor visual attention with users' real-world tasks and activities. This paper investigates theimpact of these situational impairments on touch-screen interaction. We probe severaldesign factors for touch-screen gestures; under various levels of environmental demands onattention; in comparison to the status-quo approach of soft buttons. We find that in thepresence of environmental distractions; gestures can offer significant performance gains andreduced attentional load; while performing as well as soft buttons when the user's attentionis focused on the phone. In fact; the speed and accuracy of bezel gestures did not appear tobe significantly affected by environment; and some gestures could be articulated eyes-free;with one hand. Bezel-initiated gestures offered the fastest performance; and mark-based …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2011,144
Gesture recognition on computing device correlating input to a template,*,A computer-implemented user interface method is disclosed. The method includesdisplaying information on a touchscreen of a computing device; receiving from a user of thedevice an input drawn on the touchscreen; correlating the input to a template; where thecorrelating includes employing a closed-form solution to find a rotation that reduces angulardistance between the input and the template; and providing output based on a result of thecorrelating.,*,2012,99
FFitts law: modeling finger touch with fitts' law,Xiaojun Bi; Yang Li; Shumin Zhai,Abstract Fitts' law has proven to be a strong predictor of pointing performance under a widerange of conditions. However; it has been insufficient in modeling small-target acquisitionwith finger-touch based input on screens. We propose a dual-distribution hypothesis tointerpret the distribution of the endpoints in finger touch input. We hypothesize themovement endpoint distribution as a sum of two independent normal distributions. Onedistribution reflects the relative precision governed by the speed-accuracy tradeoff rule in thehuman motor system; and the other captures the absolute precision of finger touchindependent of the speed-accuracy tradeoff effect. Based on this hypothesis; we derived theFFitts model-an expansion of Fitts' law for finger touch input. We present three experimentsin 1D target acquisition; 2D target acquisition and touchscreen keyboard typing tasks …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2013,91
Gesture search: a tool for fast mobile data access,Yang Li,Abstract Modern mobile phones can store a large amount of data; such as contacts;applications and music. However; it is difficult to access specific data items via existingmobile user interfaces. In this paper; we present Gesture Search; a tool that allows a user toquickly access various data items on a mobile phone by drawing gestures on its touchscreen. Gesture Search contributes a unique way of combining gesture-based interactionand search for fast mobile data access. It also demonstrates a novel approach for couplinggestures with standard GUI interaction. A real world deployment with mobile phone usersshowed that Gesture Search enabled fast; easy access to mobile data in their day-to-daylives. Gesture Search has been released to public and is currently in use by hundreds ofthousands of mobile users. It was rated positively by users; with a mean of 4.5 out of 5 for …,Proceedings of the 23nd annual ACM symposium on User interface software and technology,2010,91
Activity-based prototyping of ubicomp applications for long-lived; everyday human activities,Yang Li; James A Landay,Abstract We designed an activity-based prototyping process realized in the ActivityDesignersystem that combines the theoretical framework of Activity-Centered Design with traditionaliterative design. This process allows designers to leverage human activities as first classobjects for design and is supported in ActivityDesigner by three novel features. First; this toolallows designers to model activities based on concrete scenarios collected from everydaylives. The models form a context for design and computational constructs for creatingfunctional prototypes. Second; it allows designers to prototype interaction behaviors basedon activity streams spanning time. Third; it allows designers to easily test these prototypeswith real users continuously; in situ. We have garnered positive feedback from a series oflaboratory user studies and several case studies in which ActivityDesigner was used in …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2008,87
Cascadia: a system for specifying; detecting; and managing RFID events,Evan Welbourne; Nodira Khoussainova; Julie Letchner; Yang Li; Magdalena Balazinska; Gaetano Borriello; Dan Suciu,Abstract Cascadia is a system that provides RFID-based pervasive computing applicationswith an infrastructure for specifying; extracting and managing meaningful high-level eventsfrom raw RFID data. Cascadia provides three important services. First; it allows applicationdevelopers and even users to specify events using either a declarative query language oran intuitive visual language based on direct manipulation. Second; it provides an API thatfacilitates the development of applications which rely on RFID-based events. Third; itautomatically detects the specified events; forwards them to registered applications andstores them for later use (eg; for historical queries). We present the design andimplementation of Cascadia along with an evaluation that includes both a user study andmeasurements on traces collected in a building-wide RFID deployment. To demonstrate …,Proceeding of the 6th international conference on Mobile systems; applications; and services,2008,83
Gesture coder: a tool for programming multi-touch gestures by demonstration,Hao Lü; Yang Li,Abstract Multi-touch gestures have become popular on a wide range of touchscreendevices; but the programming of these gestures remains an art. It is time-consuming anderror-prone for a developer to handle the complicated touch state transitions that result frommultiple fingers and their simultaneous movements. In this paper; we present GestureCoder; which by learning from a few examples given by the developer automaticallygenerates code that recognizes multi-touch gestures; tracks their state changes and invokescorresponding application actions. Developers can easily test the generated code inGesture Coder; refine it by adding more examples; and once they are satisfied with itsperformance integrate the code into their applications. We evaluated our learning algorithmexhaustively with various conditions over a large set of noisy data. Our results show that it …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2012,80
Deep shot: a framework for migrating tasks across devices using mobile phone cameras,Tsung-Hsiang Chang; Yang Li,Abstract A user task often spans multiple heterogeneous devices; eg; working on a PC in theoffice and continuing the work on a laptop or a mobile phone while commuting on a shuttle.However; there is a lack of support for users to easily migrate their tasks across devices. Toaddress this problem; we created Deep Shot; a framework for capturing the user's work statethat is needed for a task (eg; the specific part of a webpage being viewed) and resuming iton a different device. In particular; Deep Shot supports two novel and intuitive interactiontechniques; deep shooting and deep posting; for pulling and pushing work states;respectively; using a mobile phone camera. In addition; Deep Shot provides a concise APIfor developers to leverage its services and make their application states migratable. Wedemonstrated that Deep Shot can be used to support a range of everyday tasks migrating …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2011,74
DoubleFlip: a motion gesture delimiter for mobile interaction,Jaime Ruiz; Yang Li,Abstract To make motion gestures more widely adopted on mobile devices it is importantthat devices be able to distinguish between motion intended for mobile interaction and every-day motion. In this paper; we present DoubleFlip; a unique motion gesture designed as aninput delimiter for mobile motion-based interaction. The DoubleFlip gesture is distinct fromregular motion of a mobile device. Based on a collection of 2;100 hours of motion datacaptured from 99 users; we found that our DoubleFlip recognizer is extremely resistant tofalse positive conditions; while still achieving a high recognition rate. Since DoubleFlip iseasy to perform and unlikely to be accidentally invoked; it provides an always-active inputevent for mobile interaction.,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2011,68
External representations in ubiquitous computing design and the implications for design tools,Steven Dow; T Scott Saponas; Yang Li; James A Landay,Abstract One challenge for ubiquitous computing is providing appropriate tools forprofessional designers; thus leading to stronger user-valued applications. Unlike manyprevious tool-builders' attempts to support a specific technology; we take a designer-centered stance; asking the question: how do professional designers externalize ideas foroff-the-desktop computing and how do these inform next generation design tools? We reporton interviews with designers from various domains; including experience; interaction;industrial; and space designers. The study broadly reveals perceived challenges of movinginto a non-traditional design medium; emphasizes the practice of storytelling for relating thecontext of interaction; and through two case studies; traces the use of various externalrepresentations during the design progression of ubicomp applications. Using …,Proceedings of the 6th conference on Designing Interactive systems,2006,51
Tap; swipe; or move: attentional demands for distracted smartphone input,Matei Negulescu; Jaime Ruiz; Yang Li; Edward Lank,Abstract Smartphones are frequently used in environments where the user is distracted byanother task; for example by walking or by driving. While the typical interface forsmartphones involves hardware and software buttons and surface gestures; researchershave recently posited that; for distracted environments; benefits may exist in using motiongestures to execute commands. In this paper; we examine the relative cognitive demands ofmotion gestures and surface taps and gestures in two specific distracted scenarios: awalking scenario; and an eyes-free seated scenario. We show; first; that there is nosignificant difference in reaction time for motion gestures; taps; or surface gestures onsmartphones. We further show that motion gestures result in significantly less time looking atthe smartphone during walking than does tapping on the screen; even with interfaces …,Proceedings of the International Working Conference on Advanced Visual Interfaces,2012,45
Flipping for motion-based input,*,A computer-implemented method for identifying motion-based inputs to an electronic deviceinvolves determining that the electronic device has been rotated in a first direction of rotationpast a first threshold orientation; determining that the electronic device has been rotated in asecond direction of rotation that is substantially the opposite of the first direction of rotation;past a second threshold; and analyzing motion of the device to identify motion-based inputsto the device other than the rotation of the device in the first and second directions; based onthe two determinations.,*,2012,42
Gesture avatar: a technique for operating mobile user interfaces using gestures,Hao Lü; Yang Li,Abstract Finger-based touch input has become a major interaction modality for mobile userinterfaces. However; due to the low precision of finger input; small user interfacecomponents are often difficult to acquire and operate on a mobile device. It is even harderwhen the user is on the go and unable to pay close attention to the interface. In this paper;we present Gesture Avatar; a novel interaction technique that allows users to operateexisting arbitrary user interfaces using gestures. It leverages the visibility of graphical userinterfaces and the casual interaction of gestures. Gesture Avatar can be used to enhance arange of mobile interactions. A user study we conducted showed that compared to Shift (analternative technique for target acquisition tasks); Gesture Avatar performed at a much lowererror rate on various target sizes and significantly faster on small targets (1mm). It also …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2011,41
Bootstrapping personal gesture shortcuts with the wisdom of the crowd and handwriting recognition,Tom Ouyang; Yang Li,Abstract Personal user-defined gesture shortcuts have shown great potential for accessingthe ever-growing amount of data and computing power on touchscreen mobile devices.However; their lack of scalability is a major challenge for their wide adoption. In this paper;we present Gesture Marks; a novel approach to touch-gesture interaction that allows a userto access applications and websites using gestures without having to define them first. Itoffers two distinctive solutions to address the problem of scalability. First; it leverages the"wisdom of the crowd"; a continually evolving library of gesture shortcuts that are collectedfrom the user population; to infer the meaning of gestures that a user never defined himself.Second; it combines an extensible template-based gesture recognizer with a specializedhandwriting recognizer to even better address handwriting-based gestures; which are a …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2012,35
Glyph entry on computing device,*,A computer-implemented user interface method is disclosed. The method includesdisplaying on a touchscreen of a computing device a first sub-portion of a list of items;receiving from a user of the device an drawn input on the touchscreen; correlating the drawninput to an alphanumeric character; and automatically displaying a second sub-portion ofthe list having one or more entries whose first character correlates to the alphanumericcharacter.,*,2014,32
Sketching informal presentations,Yang Li; James A Landay; Zhiwei Guan; Xiangshi Ren; Guozhong Dai,Abstract Informal presentations are a lightweight means for fast and convenientcommunication of ideas. People communicate their ideas to others on paper andwhiteboards; which afford fluid sketching of graphs; words and other expressive symbols.Unlike existing authoring tools that are designed for formal presentations; we createdSketchPoint to help presenters design informal presentations via freeform sketching. InSketchPoint; presenters can quickly author presentations by sketching slide content; overallhierarchical structures and hyperlinks. To facilitate the transition from idea capture tocommunication; a note-taking workspace was built for accumulating ideas and sketchingpresentation outlines. Informal feedback showed that SketchPoint is a promising tool for ideacommunication.,Proceedings of the 5th international conference on Multimodal interfaces,2003,30
Weave: Scripting Cross-Device Wearable Interaction,Pei-Yu Peggy Chi; Yang Li,Abstract We present Weave; a framework for developers to create cross-device wearableinteraction by scripting. Weave provides a set of high-level APIs; based on JavaScript; fordevelopers to easily distribute UI output and combine sensing events and user input acrossmobile and wearable devices. Weave allows developers to focus on their target interactionbehaviors and manipulate devices regarding their capabilities and affordances; rather thanlow-level specifications. Weave also contributes an integrated authoring environment fordevelopers to program and test cross-device behaviors; and when ready; deploy thesebehaviors to its runtime environment on users' ad-hoc network of devices. An evaluation ofWeave with 12 participants on a range of tasks revealed that Weave significantly reducedthe effort of developers for creating and iterating on cross-device interaction.,Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,2015,29
Gesture studio: authoring multi-touch interactions through demonstration and declaration,Hao Lü; Yang Li,Abstract The prevalence of multi-touch devices opens the space for rich interactions.However; the complexity for creating multi-touch interactions hinders this potential. In thispaper; we present Gesture Studio; a tool for creating multi-touch interaction behaviors bycombining the strength of two distinct but complementary approaches: programming bydemonstration and declaration. We employ an intuitive video-authoring metaphor fordevelopers to demonstrate touch gestures; compose complicated behaviors; test thesebehaviors in the tool and export them as source code that can be integrated into thedevelopers' project.,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2013,29
Translating user interaction with a touch screen into input commands,*,Systems and techniques for translating user interaction with a touch screen into inputcommands. In one aspect; a touch screen system includes a touch screen graphical display;a command interpreter configured to identify modifier interactions with the graphical displayand; in response to identification of the modifier interaction; modify the interpretation of otherinteraction with the graphical display; one or more persistent data storage devices. The datastorage devices store a first set of rules for interpreting user interaction with the graphicaldisplay and a second set of rules for interpreting user interaction with the graphical display.The first set of rules interpret motion across a map or a proper subset of a collection ofinformation as a scrolling or panning command. The second set of rules interpret the motionas a glyph. The command interpreter is configured to modify the interpretation of the other …,*,2013,28
Moving information between computing devices,*,A computer-implemented method for moving information between computing devicesincludes capturing a digital image of a display of a first computing device using a camera ofa second computing device; transmitting; to the first computing device; data that correspondsto the digital image; analyzing the transmitted data on the first computing device todetermine whether the digital image matches a current display of the first computing device;and using the analysis to cause one of the first or second computing devices to invoke anapplication and match a state of an application that is executing on the other of the first orsecond computing devices.,*,2012,27
Collaborative gesture-based input language,*,In one example; a method includes receiving; by a server; data representative of a group ofgestures detected by the plurality of computing devices and data representative of one ormore shortcuts associated with the group of gestures from a plurality of computing devices;wherein each shortcut corresponds to an action performed by at least one of the computingdevices. The method may further include aggregating; by the server; the data representativeof the gestures and the data representative of the associated shortcuts received from theplurality of computing devices based at least in part on detected similarities between at leastone of 1) the group of gestures and 2) the associated shortcuts; and defining; by the server;a gesture-shortcut language based at least in part on the aggregated data.,*,2014,25
The adaptive hybrid cursor: A pressure-based target selection technique for pen-based user interfaces,Xiangshi Ren; Jibin Yin; Shengdong Zhao; Yang Li,Abstract We present the Adaptive Hybrid Cursor; a novel target acquisition technique for pen-based interfaces. To assist a user in a target selection task; this technique automaticallyadapts the size of the cursor and/or its contexts (the target size and the selectionbackground) based on pen pressure input. We systematically evaluated the new techniquewith various 2D target acquisition tasks. The experimental results indicated that the AdaptiveHybrid Cursor had better selection performance; and was particularly effective for small-target and high-density environments in which the regular cursor and the Bubble Cursor [13]failed to show significant advantages. The Adaptive Hybrid Cursor is a novel way to improvetarget acquisition via pressure input; and our study demonstrated its viability and potentialfor pen-based interfaces.,IFIP Conference on Human-Computer Interaction,2007,25
Structuralizing freeform notes by implicit sketch understanding,Yang Li; Zhiwei Guan; Hongan Wang; Guozhong Dai; Xiangshi Ren,People are accustomed to capture important events and ideas by sketching notes onpapers. Pen and paper offer people great freedom and naturalness to perform theseactivities. During note sketching; people generally don't pay much attention to theorganization of notes; instead they use implicit spatial relationships and concise informalorganizing symbols to structuralize notes. We conducted a study to discover how peoplesketch and organize their notes. The structuralized notes are easy to be maintained;manipulated and reused. Based on our observations; we designed algorithms tostructuralize freeform notes into a consistent hierarchical structure. Both implicit spatialparsing and gesture-based structuralizing are enabled. With the supports of thesealgorithms; we built a system SketchPoint notebook as a tool that allows a user to sketch …,AAAI Spring Symposium on Sketch Understanding,2002,25
Touch gestures for text-entry operations,*,In general; this disclosure describes techniques for providing a user of a computing devicewith the ability to perform text-entry operations (eg; using a touch screen) on a computingdevice. Specifically; the techniques of this disclosure may; in some examples; allow the userto use gestures on a mobile computing device to perform text entry and editing operations.Using a presence-sensitive user interface device (eg; a touch screen); the user may usegestures to enter text into text-based applications (eg; short message service (SMS)messages; e-mail message; uniform resource locators (URLs); and the like). Using visually-defined areas on the touch screen; the user may utilize gestures of certain patterns; relativeto the defined areas; to indicate text entry and editing operations such as; for example;deleting characters and words; indicating a space or return characters; and the like.,*,2012,24
Informal prototyping of continuous graphical interactions by demonstration,Yang Li; James A Landay,Abstract Informal prototyping tools have shown great potential in facilitating the early stagedesign of user interfaces. How-ever; continuous interactions; an important constituent ofhighly interactive interfaces; have not been well supported by previous tools. Theseinteractions give continuous visual feedback; such as geometric changes of a graphicalobject; in response to continuous user input; such as the movement of a mouse. We builtMonet; a sketch-based tool for proto-typing continuous interactions by demonstration. InMonet; designers can prototype continuous widgets and their states of interest usingexamples. They can also demonstrate com-pound behaviors involving multiple widgets bydirect ma-nipulation. Monet allows continuous interactions to be eas-ily integrated with event-based; discrete interactions. Con-tinuous widgets can be embedded into storyboards and …,Proceedings of the 18th annual ACM symposium on User interface software and technology,2005,22
Gesture-based search,*,In general; the subject matter described in this specification can be embodied in methods;systems; and program products for performing searches with gesture-based input. A searchsystem receives gesture data corresponding to one or more characters that have beendrawn on a display of a client device. The search system recognizes the one or morecharacters that correspond to the gesture data. The search system formulates a search thatincludes the one or more characters as a query term. The search system communicates tothe client device one or more search results for the search; and data identifying the one ormore characters.,*,2013,18
Beyond pinch and flick: Enriching mobile gesture interaction,Yang Li,WIMP-based interaction is particularly problematic and awkward on mobile phones; whichhave a small form factor and are primarily used on the go. Mobile phone users want to beable to continue focusing on realworld tasks at hand rather than have to carefully key incommands; tap buttons; and navigate through menus. Touchscreen gestures can provide aquicker; more intuitive way to operate a mobile phone. Gestures have always played animportant role in human communication (A. Kendon; Gesture: Visible,Computer,2009,18
Gesture detection using an array of short-range communication devices,*,In general; techniques and systems for defining a gesture with a computing device usingshort-range communication are described. In one example; a method includes obtainingposition information from an array of position devices using near-field communication (NFC)during a movement of the computing device with respect to the array; wherein the positioninformation identifies unique positions within the array for each position device from whichposition information was obtained. The method may also include determining sequenceinformation associated with the position information; wherein the sequence information isrepresentative of an order in which the position information was obtained from each positiondevice; and performing; by the computing device; an action based at least in part on theposition information and the sequence information; wherein the position information and …,*,2014,17
CrowdLearner: rapidly creating mobile recognizers using crowdsourcing,Shahriyar Amini; Yang Li,Abstract Mobile applications can offer improved user experience through the use of novelmodalities and user context. However; these new input dimensions often require recognition-based techniques; with which mobile app developers or designers may not be familiar.Furthermore; the recruiting; data collection and labeling; necessary for using thesetechniques; are usually time-consuming and expensive. We present CrowdLearner; aframework based on crowdsourcing to automatically generate recognizers using mobilesensor input such as accelerometer or touchscreen readings. CrowdLearner allows adeveloper to easily create a recognition task; distribute it to the crowd; and monitor itsprogress as more data becomes available. We deployed CrowdLearner to a crowd of 72mobile users over a period of 2.5 weeks. We evaluated the system by experimenting with …,Proceedings of the 26th annual ACM symposium on User interface software and technology,2013,17
Collaborative gesture-based input language,*,In one example; a method includes receiving; by a server and from a plurality of computingdevices; data representative of a group of gesture-shortcut pairs that each include a gesturehaving been detected by at least one computing device from the plurality of computingdevices and a shortcut associated with the detected gesture; and wherein the shortcutcorresponds to an operation to be executed by the at least one from the plurality ofcomputing devices. The method includes sorting the data representative of each respectivegesture-shortcut pair from the group of gesture-shortcut pairs into at least two subgroups; afirst subgroup including the first gesture-shortcut pair and a second including the secondgesture-shortcut pair; the sorting being based on detected similarities between at least oneof each respective gesture from the group of gesture-shortcut pairs and each respective …,*,2012,17
Gesture script: recognizing gestures and their structure using rendering scripts and interactively trained parts,Hao Lü; James A Fogarty; Yang Li,Abstract Gesture-based interactions have become an essential part of the modern userinterface. However; it remains challenging for developers to create gestures for theirapplications. This paper studies unistroke gestures; an important category of gesturesdefined by their single-stroke trajectories. We present Gesture Script; a tool for creatingunistroke gesture recognizers. Gesture Script enhances example-based learning withinteractive declarative guidance through rendering scripts and interactively trained parts.The structural information from the rendering scripts allows Gesture Script to synthesizegesture variations and generate a more accurate recognizer that also automatically extractsgesture attributes needed by applications. The results of our study with developers show thatGesture Script preserves the threshold of familiar example based gesture tools; while …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2014,16
ContextMap: modeling scenes of the real world for context-aware computing,Yang Li; Jason I Hong; James A Landay,ABSTRACT We present a scenegraph-based schema; the ContextMap; to model contextinformation. Locations with hierarchical relations are the skeleton of the ContextMap wherenodes of people; objects and activities can be attached. Context information can becollected by traversing the ContextMap. The ContextMap provides a uniform method torepresent physical and social semantics for context-aware computing. In addition; contextambiguity can be modeled as well.,5th International Conference on Ubiquitous Computing,2003,16
Character recognition for overlapping textual user input,*,Techniques described herein may recognize handwritten characters that are written at leastpartially over the top of one another that are input to a computing device. The handwrittencharacters may be formed of one or more strokes. A user may write characters or parts ofwords over approximately the same area of graphical user interface (ie; on top of each other)without having to wait for a timeout between character input and without having to select abutton or provide another input indicating the character is complete before entering input foranother character. Once a character is at least partially recognized; a graphical indicationcorresponding to the user input displayed on a screen may be altered. Such alterations mayinclude fading or changing size or location of the graphical indication.,*,2014,15
Character recognition for overlapping textual user input,*,Techniques described herein may recognize handwritten characters that are written at leastpartially over the top of one another that are input to a computing device. The handwrittencharacters may be formed of one or more strokes. A user may write characters or parts ofwords over approximately the same area of graphical user interface (ie; on top of each other)without having to wait for a timeout between character input and without having to select abutton or provide another input indicating the character is complete before entering input foranother character. Once a character is at least partially recognized; a graphical indicationcorresponding to the user input displayed on a screen may be altered. Such alterations mayinclude fading or changing size or location of the graphical indication.,*,2014,15
Detecting tapping motion on the side of mobile devices by probabilistically combining hand postures,William McGrath; Yang Li,Abstract We contribute a novel method for detecting finger taps on the different sides of asmartphone; using the built-in motion sensors of the device. In particular; we discuss newfeatures and algorithms that infer side taps by probabilistically combining estimates of taplocation and the hand pose--the hand holding the device. Based on a dataset collected from9 participants; our method achieved 97.3% precision and 98.4% recall on tap eventdetection against ambient motion. For detecting single-tap locations; our methodoutperformed an approach that uses inferred hand postures deterministically by 3% and anapproach that does not use hand posture inference by 17%. For inferring the location of twoconsecutive side taps from the same direction; our method outperformed the two baselineapproaches by 6% and 17% respectively. We discuss our insights into designing the …,Proceedings of the 27th annual ACM symposium on User interface software and technology,2014,15
Translating user interaction with a touch screen into text,*,Methods; systems; and apparatus; including computer programs encoded on a computerstorage medium; for translating user interaction with a touchscreen into text are provided. Inone embodiment; a method comprises receiving information characterizing a first userinteraction with a touchscreen; receiving information characterizing a second userinteraction with the touchscreen; assigning a probability to each of two or moreinterpretations of a combination of the first user interaction and the second user interaction;each of the interpretations construing the combination as a different pair of characters; eachof the probabilities embodying a likelihood that the first user interaction and the second userinteraction represent the respective pair of characters; using the two or more interpretationsto search a collection of data for objects that are each identifiable by one of the two or …,*,2013,15
Teaching motion gestures via recognizer feedback,Ankit Kamal; Yang Li; Edward Lank,Abstract When using motion gestures; 3D movements of a mobile phone; as an inputmodality; one significant challenge is how to teach end users the movement parametersnecessary to successfully issue a command. Is a simple video or image depicting movementof a smartphone sufficient? Or do we need three-dimensional depictions of movement onexternal screens to train users? In this paper; we explore mechanisms to teach end usersmotion gestures; examining two factors. The first factor is how to represent motion gestures:as icons that describe movement; video that depicts movement using the smartphonescreen; or a Kinect-based teaching mechanism that captures and depicts the gesture on anexternal display in three-dimensional space. The second factor we explore is recognizerfeedback; ie a simple representation of the proximity of a motion gesture to the desired …,Proceedings of the 19th international conference on Intelligent User Interfaces,2014,14
A context-aware infrastructure for supporting applications with pen-based interaction,Yang Li; ZhiWei Guan; GuoZhong Dai; XiangShi Ren; Yong Han,Abstract Pen-based user interfaces which leverage the affordances of the pen provide userswith more flexibility and natural interaction. However; it is difficult to construct usable pen-based user interfaces because of the lack of support for their development. Toolkit-levelsupport has been exploited to solve this problem; but this approach makes it hard to gainplatform independence; easy maintenance and easy extension. In this paper a context-aware infrastructure is created; called WEAVER; to provide pen interaction services for bothnovel pen-based applications and legacy GUI-based applications. WEAVER aims to supportthe pen as another standard interactive device along with the keyboard and mouse andpresent a high-level access interface to pen input. It employs application context to tailor itsservice to different applications. By modeling the application context and registering the …,Journal of Computer Science and Technology,2003,14
Hierarchical route maps for efficient navigation,Fangzhou Wang; Yang Li; Daisuke Sakamoto; Takeo Igarashi,Abstract One of the difficulties with standard route maps is accessing to multi-scale routinginformation. The user needs to display maps in both a large scale to see details and a smallscale to see an overview; but this requires tedious interaction such as zooming in and out.We propose to use a hierarchical structure for a route map; called a" Route Tree"; to addressthis problem; and describe an algorithm to automatically construct such a structure. A RouteTree is a hierarchical grouping of all small route segments to allow quick access tomeaningful large and small-scale views. We propose two Route Tree applications;"RouteZoom" for interactive map browsing and" TreePrint" for route information printing; toshow the applicability and usability of the structure. We conducted a preliminary user studyon RouteZoom; and the results showed that RouteZoom significantly lowers the …,Proceedings of the 19th international conference on Intelligent User Interfaces,2014,13
Touch gestures for remote control operations,*,In general; this disclosure describes techniques for providing a user of a first computingdevice (eg; a mobile device) with the ability to utilize the first computing device to control asecond computing device (eg; a television). Specifically; the techniques of this disclosuremay; in some examples; allow the user to use drawing gestures on a mobile computingdevice to remotely control and operate the second computing device. Using a presence-sensitive user interface device (eg; a touch screen); the user may use drawing gestures toindicate characters associated with operations and commands to control the secondcomputing device.,*,2012,13
Prediction completion gesture,*,In one example; a method includes detecting; by a computing device; at least one usercontact with a presence-sensitive screen of the computing device to input one or morecharacters of an input string. The method also includes detecting; by the computing device; asubsequent user contact with the presence-sensitive screen. The method also includesdetecting; by the computing device; a gesture at a region of the presence-sensitive screenthat is associated with a terminator symbol while the subsequent user contact is maintainedwith the presence-sensitive screen. The method also includes adding; by the computingdevice; the terminator symbol to the input string when the gesture comprises a virtual keypress gesture. The method also includes replacing; by the computing device; the input stringwith a predicted completed string for the input string when the gesture comprises a …,*,2014,12
GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard,Haimo Zhang; Yang Li,Abstract Stroke gestures are intuitive and efficient but often require gesture-capable inputhardware such as a touchscreen. In this paper; we present GestKeyboard; a novel techniquefor gesturing over an ordinary; unmodified physical keyboard that remains the major inputmodality for existing desktop and laptop computers. We discuss an exploratory study forunderstanding the design space of gesturing on a physical keyboard and our algorithms fordetecting gestures in a modeless way; without interfering with the keyboard's majorfunctionality such as text entry and shortcuts activation. We explored various features fordetecting gestures from a keyboard event stream. Our experiment based on the datacollected from 10 participants indicated it is feasible to reliably detect gestures from normalkeyboard use; 95% detection accuracy within a maximum latency of 200ms.,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2014,12
HOBS: head orientation-based selection in physical spaces,Ben Zhang; Yu-Hsiang Chen; Claire Tuna; Achal Dave; Yang Li; Edward Lee; Björn Hartmann,Abstract Emerging head-worn computing devices can enable interactions with smart objectsin physical spaces. We present the iterative design and evaluation of HOBS--a Head-Orientation Based Selection technique for interacting with these devices at a distance. Weaugment a commercial wearable device; Google Glass; with an infrared (IR) emitter to selecttargets equipped with IR receivers. Our first design shows that a naive IR implementationcan outperform list selection; but has poor performance when refinement between multipletargets is needed. A second design uses IR intensity measurement at targets to improverefinement. To address the lack of natural mapping of on-screen target lists to spatial targetlocation; our third design infers a spatial data structure of the targets enabling a natural head-motion based disambiguation. Finally; we demonstrate a universal remote control …,Proceedings of the 2nd ACM symposium on Spatial user interaction,2014,11
Creating and organizing events in an activity stream,*,A system and method for creating and organizing events includes an activity streamapplication that captures; searches and collaborates on one or more events. The eventsinclude unstructured data comprising text; digital ink; an audio clip and an image. Theactivity stream application receives user input and generates a new event and combinesrelated events into the same activity. The activity stream application receives a search queryand searches for events that are relevant to the search query. In one embodiment; thesearch query includes contextual information that includes at least one of at a similar time; ata similar location; in a similar situation and a relatedness of event attributes.,*,2014,11
Computing device interaction with visual media,*,In general; techniques and systems for retrieving supplemental information associated withvisual media are described. In one example; a method includes obtaining; by a computingdevice; position information from a position device using near-field communication (NFC);wherein the position information identifies a position of the position device in relation to aportion of visual media associated with the position device. The method may also includereceiving; by the computing device; supplemental information that is provided by anetworked device based at least in part on the position information; wherein thesupplemental information comprises additional information related to the portion of thevisual media. In some examples; the position device is included within an array of positiondevices mounted behind the visual media.,*,2013,11
Research on gesture-based human-computer interaction,Li Yang; Guan Zhiwei; Chen Youdi; Dai Guozhong,*,Journal of System Simulation,2000,11
Optimistic Programming of Touch Interaction,Yang Li; Hao Lu; Haimo Zhang,Abstract Touch-sensitive surfaces have become a predominant input medium for computingdevices. In particular; multitouch capability of these devices has given rise to developing richinteraction vocabularies for “real” direct manipulation of user interfaces. However; therichness and flexibility of touch interaction often comes with significant complexity forprogramming these behaviors. Particularly; finger touches; though intuitive; are impreciseand lead to ambiguity. Touch input often involves coordinated movements of multiple fingersas opposed to the single pointer of a traditional WIMP interface. It is challenging in not onlydetecting the intended motion carried out by these fingers but also in determining the targetobjects being manipulated due to multiple focus points. Currently; developers often need tobuild touch behaviors by dealing with raw touch events that is effort consuming and error …,ACM Transactions on Computer-Human Interaction (TOCHI),2014,10
Dynamic user interface for navigating among GUI elements,*,In one example; a computing device executes a plurality of application processes; each ofwhich has an associated graphical user interface element. The computing device renders acommon graphical user interface on a presence-sensitive screen. The common graphicaluser interface includes a currently rendered graphical user interface element associatedwith a currently selected application process from among the plurality of applicationprocesses; a tab row comprising a plurality of tabs; wherein each of the graphical userinterface elements corresponds to one of the plurality of tabs; and a slide bar positionedproximate to the tab row. The computing device renders a first tab in the tab row with a largersize than other graphical tabs in the plurality of tabs responsive to receipt by the presence-sensitive screen of a gesture input associated with a region of the slide bar that is closest …,*,2014,10
Open project: a lightweight framework for remote sharing of mobile applications,Matei Negulescu; Yang Li,Abstract The form factor of mobile devices remains small while their computing power growsat an accelerated rate. Prior work has explored expanding the output space by leveragingfree displays in the environment. However; existing solutions often do not scale. In this paperwe discuss Open Project; an end-to-end framework that allows a user to" project" a nativemobile application onto a display using a phone camera; leveraging interaction spacesranging from a PC monitor to a public wall-sized display. Any display becomes projectableinstantaneously by simply accessing the lightweight Open Project server via a web browser.By distributing computation load onto each projecting mobile device; our framework easilyscales for hosting many projection sessions and devices simultaneously. Our performanceexperiments and user studies indicated that Open Project supported a variety of useful …,Proceedings of the 26th annual ACM symposium on User interface software and technology,2013,10
Touch gestures for text-entry operations,*,In general; this disclosure describes techniques for providing a user of a computing devicewith the ability to perform text-entry operations (eg; using a touch screen) on a computingdevice. Specifically; the techniques of this disclosure may; in some examples; allow the userto use gestures on a mobile computing device to perform text entry and editing operations.Using a presence-sensitive user interface device (eg; a touch screen); the user may usegestures to enter text into text-based applications (eg; short message service (SMS)messages; e-mail message; uniform resource locators (URLs); and the like). Using visually-defined areas on the touch screen; the user may utilize gestures of certain patterns; relativeto the defined areas; to indicate text entry and editing operations such as; for example;deleting characters and words; indicating a space or return characters; and the like.,*,2012,10
Touch gestures for remote control operations,*,In general; this disclosure describes techniques for providing a user of a first computingdevice (eg; a mobile device) with the ability to utilize the first computing device to control asecond computing device (eg; a television). Specifically; the techniques of this disclosuremay; in some examples; allow the user to use drawing gestures on a mobile computingdevice to remotely control and operate the second computing device. Using a presence-sensitive user interface device (eg; a touch screen); the user may use drawing gestures toindicate characters associated with operations and commands to control the secondcomputing device.,*,2012,10
Enhancing cross-device interaction scripting with interactive illustrations,Pei-Yu Peggy Chi; Yang Li; Björn Hartmann,Abstract Cross-device interactions involve input and output on multiple computing devices.Implementing and reasoning about interactions that cover multiple devices with a diversity ofform factors and capabilities can be complex. To assist developers in programming cross-device interactions; we created DemoScript; a technique that automatically analyzes a cross-device interaction program while it is being written. DemoScript visually illustrates the step-by-step execution of a selected portion or the entire program with a novel; automaticallygenerated cross-device storyboard visualization. In addition to helping developersunderstand the behavior of the program; DemoScript also allows developers to revise theirprogram by interactively manipulating the cross-device storyboard. We evaluatedDemoScript with 8 professional programmers and found that DemoScript significantly …,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,2016,9
A Context Menu for the Real World: Controlling Physical Appliances Through Head-Worn Infrared Targeting,Yu-Hsiang Chen; Ben Zhang; Claire Tuna; Yang Li; Edward A Lee; Björn Hartmann,Abstract: We introduce a novel method for selecting and controlling smart appliances inphysical spaces through a head-worn computing device with near-eye display and wirelesscommunication. We augment a commercial wearable computing device; Google Glass; witha narrow-beam infrared emitter for this purpose. This configuration yields a usable beamwidth of 2 to 4 feet (60 to 120 cm) for targeting at room scale. We describe a disambiguationtechnique if infrared targeting hits multiple targets simultaneously. A target acquisition studywith 14 participants shows that selection using head orientation with our device outperformslist selection on a wearable device. We also report qualitative data from using our device tocontrol multiple appliances in a smart home scenario. Descriptors:* AUGMENTATION;*HEADGEAR;* INFRARED EQUIPMENT;* MOBILE COMPUTING;* REMOTE CONTROL …,*,2013,9
Controlling a target device using short-range communication,*,In general; techniques and systems for retrieving supplemental information associated withvisual media are described. In one example; a method includes obtaining; by a computingdevice; position information from a position device using near-field communication (NFC);wherein the position information identifies a position of the position device in relation to aportion of visual media associated with the position device. The method may also includereceiving; by the computing device; supplemental information that is provided by anetworked device based at least in part on the position information; wherein thesupplemental information comprises additional information related to the portion of thevisual media. In some examples; the position device is included within an array of positiondevices mounted behind the visual media.,*,2013,9
Gesture-based interaction: a new dimension for mobile user interfaces,Yang Li,Abstract Today; smart phones with touchscreens and sensors are the predominant; fastestgrowing class of consumer computing devices. However; because these devices are used indiverse situations; and have unique capabilities and form factors; they also raise new userinterface challenges; and at the same time; offer great opportunities for impactful HCIresearch.,Proceedings of the International Working Conference on Advanced Visual Interfaces,2012,9
Operation of mobile device interface using gestures,*,In general; this disclosure describes techniques for providing a user of a computing device(eg; a mobile device) with the ability to utilize drawn gestures to operate objects displayedon a user interface of the computing device. Specifically; the techniques of this disclosuremay; in some examples; include receiving user input comprising a gesture that defines anattribute associated with one or more target elements displayed in the user interface andgraphically highlighting the one or more target elements in the user interface. The user maythen utilize the drawn gesture to operate the highlighted target element by interacting withthe defined selection area.,*,2015,8
Gesture Search: Random Access to Smartphone Content,Yang Li,Gesture Search offers users an alternative to existing WYSIWYG; GUI-oriented interactionwith smartphones. It supports random access of a phone's content and functionality usinggesture shortcuts; so users no longer need to manually search and navigate through theinterface hierarchy.,IEEE Pervasive Computing,2011,8
BrickRoad: a light-weight tool for spontaneous design of location-enhanced applications,Alan L Liu; Yang Li,Abstract It is difficult to design and test location-enhancedapplications. A large part of thisdifficulty is due to the added complexity of supporting location. Wizard of Oz (WOz) hasbecome an effective technique for the early stage design of location-enhanced applicationsbecause it allows designers to test an application prototype bysimulating nonexistentcomponents such as location sensing. However; existing WOz tools 1) require nontrivialeffort from designers to specify how a prototype should behave before it can be tested withend users; and 2) support only limited control over application behavior during a test.BrickRoad is a WOz tool for spontaneousdesign of location-enhanced applications. It lowersthe threshold to acquiring user feedback and exploring a design space. With BrickRoad; adesigner does not need to specify any interaction logic and can experiment on-the-fly with …,Proceedings of the SIGCHI conference on Human factors in computing systems,2007,8
Gesture on: Enabling always-on touch gestures for fast mobile access from the device standby mode,Hao Lu; Yang Li,Abstract A significant percentage of mobile interaction involves short-period usages thatoriginate from the standby mode-users wake up a device by pressing the power button;unlock the device by authenticating themselves; and then search for a target app orfunctionality on the device. These additional steps preceding a target task imposessignificant overhead on users for each mobile device access. To address the issue; wedeveloped Gesture On; a system that enables gesture shortcuts in the standby mode bywhich a user can draw a gesture on the touchscreen before the screen is turned on. Basedon the gesture; our system directly brings up a target item onto the screen that bypasses allthese additional steps in a mobile access. This paper examines several challenges inrealizing Gesture On; including robustly rejecting accidental touches when the device is …,Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,2015,6
Systems and methods for transferring images and information from a mobile computing device to a computer monitor for display,*,Certain implementations of the disclosed technology may include systems; methods; andcomputer-readable media for transferring images and information from a mobile computingdevice to a computer monitor for display. In one example implementation; a method isprovided that includes receiving; from a remote client; an initiation request; wherein theremote client is associated with a remote display. The method further includes sending arepresentation of a unique code to the remote client; and receiving; from a mobile device; anindication that the mobile device captured the representation of the unique code. Themethod further includes receiving; from the mobile device; a display image for presentationon the remote display; and sending the display image to the remote client for presentationon the remote display.,*,2013,6
Creating a social network based on an activity,*,A system for creating an activity-based social includes receiving information from acomputing device of a participant in an activity; and determining that the information qualifiesthe participant for membership in a social network associated with the activity. The systemalso includes associating the participant with the social network; and enabling access; bythe participant; to an electronic portal that provides access to the social network.,*,2015,5
InkAnchor: enhancing informal ink-based note taking on touchscreen mobile phones,Yi Ren; Yang Li; Edward Lank,Abstract Although touchscreen mobile phones are widely used for recording informal textnotes (eg; grocery lists; reminders and directions); the lack of efficient mechanisms forcombining informal graphical content with text is a persistent challenge. In this paper; wepresent InkAnchor; a digital ink editor that allows users to easily create ink-based notes byfinger drawing and writing on a mobile phone touchscreen. InkAnchor incorporates flexibleanchoring; focus-plus-context input; content chunking; and lightweight editing mechanismsto support the capture of informal notes and annotations. We describe the design andevaluation of InkAnchor through a series of user studies; which revealed that the integratedsupport enabled by InkAnchor is a significant improvement over current mobile note takingapplications on a range of mobile note-taking tasks.,Proceedings of the 32nd annual ACM conference on Human factors in computing systems,2014,5
Design and experimental analysis of continuous location tracking techniques for Wizard of Oz testing,Yang Li; Evan Welbourne; James A Landay,Abstract Wizard of Oz (WOz) testing has shown promise as an effective way to test location-enhanced applications. However; it is challenging to conduct a location-based WOz testbecause of the dynamic nature of target settings in the field. In particular; continuous locationtracking; a major task in such a test; requires a wizard to frequently update a user's locationto simulate a location system. This imposes a heavy task load on a wizard. To ease wizards'tasks for location tracking; we designed two techniques; Directional Crossing and Steering;and conducted a field experiment to investigate the performance of the two techniques. Aquantitative analysis shows that Directional Crossing and Steering significantly lowered awizard's task load for location tracking without sacrificing accuracy.,Proceedings of the SIGCHI conference on Human Factors in computing systems,2006,5
Exploring activity-based ubiquitous computing: interaction styles; models and tool support,Yang Li; James A Landay,ABSTRACT Activity-based computing is a promising paradigm for ubiquitous computing. Byproviding a consistent framework and structural view for integrating ubicomp technologiesinto natural human activities; activity-based computing can better facilitate our day-to-daylives and allow ubicomp technologies that are sustainable in the dynamic; complex world inwhich we live. In this position paper; we surface several aspects of activity-based computing.We first briefly describe what activity-based computing is. We discuss our early ideas on anactivity framework for structuring ubicomp technologies and representing interactioncontexts. Next; we discuss the interaction styles and models of this new paradigm. We thendiscuss tool support for activity-based computing. In particular; we give an overview of ourongoing work on analysis/design tools supporting early stage prototyping of activity …,computing,2006,5
Rapid prototyping tools for context-aware applications,Yang Li; James A Landay,ABSTRACT Context-aware applications are one of the most important forms of nextgeneration interactive systems. We discuss new attributes exhibited by context-awareinteractions and introduce our experience with developing a tool for prototyping location-enhanced applications; which are a useful subset of context-aware applications. Based onthis; we discuss what challenges we face in developing a more general context-awareapplication design tool and how these challenges can be addressed. In addition; we brieflydescribe our ongoing work on building such a tool for prototyping general context-awareapplications based on human activities.,Proceedings of the CHI 2005 workshop on the Future of User Interface Design Tools,2005,5
Mogeste: A Mobile Tool for In-Situ Motion Gesture Design,Aman Parnami; Apurva Gupta; Gabriel Reyes; Ramik Sadana; Yang Li; Gregory D Abowd,Abstract Motion gestures can be expressive; fast to access and perform; and facilitated byubiquitous inertial sensors. However; implementing a gesture recognizer requiressubstantial programming and pattern recognition expertise. Although several graphicaldesktop-based tools lower the threshold of development; they do not support ad hocdevelopment in naturalistic settings. We present Mogeste; a mobile tool for in-situ motiongesture design. Mogeste allows interaction designers to within minutes envision; train; andtest motion gesture recognizers using inertial sensors in commodity devices. Furthermore; itenables rapid creative exploration by designers; at any time and within any context thatinspires them. By supporting data collection; iterative design; and evaluation of envisionedgestural interactions within the context of its end-use; Mogeste reduces the gap between …,Proceedings of the 8th Indian Conference on Human Computer Interaction,2016,4
Authoring multi-finger interactions through demonstration and composition,*,A computing device comprises a processor and an authoring tool executing on theprocessor. The processor receives demonstration data representative of at least onedemonstration of a multi-finger gesture and declaration data specifying one or moreconstraints for the multi-finger gesture. The processor generates; in accordance with thedemonstration data and the declaration data; a module to detect the multi-finger gesturewithin a computer-generated user interface.,*,2015,4
Reflection: enabling event prediction as an on-device service for mobile interaction,Yang Li,Abstract By knowing which upcoming action a user might perform; a mobile application canoptimize its user interface for accomplishing the task. However; it is technically challengingfor developers to implement event prediction in their own application. We created Reflection;an on-device service that answers queries from a mobile application regarding whichactions the user is likely to perform at a given time. Any application can register itself andcommunicate with Reflection via a simple API. Reflection continuously learns a predictionmodel for each application based on its evolving event history. It employs a novel method forprediction by 1) combining multiple well-designed predictors with an online learningmethod; and 2) capturing event patterns not only within but also across registeredapplications--only possible as an infrastructure solution. We evaluated Reflection with two …,Proceedings of the 27th annual ACM symposium on User interface software and technology,2014,4
Gesturemote: interacting with remote displays through touch gestures,Hao Lü; Matei Negulescu; Yang Li,Abstract We present Gesturemote; a technique for interacting with remote displays throughtouch gestures on a handheld touch surface. By combining a variety of different touchgestures and connecting them smoothly; Gesturemote supports a wide range of interactionbehaviors; from low pixel-level interaction such as pointing and clicking; to medium-levelinteraction such as structured navigation of a user interface; to high-level interaction such asinvoking a function directly (eg shortcuts). Gesturemote requires no visual attention to useand thus is eyes-free. We received positive initial feedback for Gesturemote from theparticipants in an interview where we walked them through the design. In addition; weinvestigated the usability of our gesture-based target acquisition technique by comparing itwith a trackpad in a target acquisition task. The results indicate that Gesturemote performs …,Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces,2014,3
Penbuilder: Platform for the development of pen-based user interface,Yang Li; Zhiwei Guan; Youdi Chen; Guozhong Dai,Abstract Pen-based user interfaces is widely used in mobile computing environment; whichprovides natural; efficient interaction. It has substantial differences from any previousinterface and it is difficult to be implemented. A welldesigned platform will improve thedevelopment of pen-based user interface. In this paper; the architecture of Pen-Book-Page-Paper is presented; which is extended from the metaphor of Pen-Paper. Penbuilder is adevelopment platform based on this architecture. Pen-based user interface can beconstructed from three layers of Penbuilder: modal-primitive layer; task-primitive layer andtask layer. Each layer provides different extent supports for flexibility and reusability. Thecomponents of the platform are discussed in detail and some properties of the platform areargued. Penbuilder is also compliant for the development of distributed user interface. In …,Advances in Multimodal Interfaces—ICMI 2000,2000,3
Improv: an input framework for improvising cross-device interaction by demonstration,Xiang‘Anthony’ Chen; Yang Li,Abstract As computing devices become increasingly ubiquitous; it is now possible tocombine the unique capabilities of different devices or Internet of Things to accomplish atask. However; there is currently a high technical barrier for creating cross-device interaction.This is especially challenging for end users who have limited technical expertise—end userswould greatly benefit from custom cross-device interaction that best suits their needs. In thisarticle; we present Improv; a cross-device input framework that allows a user to easilyleverage the capability of additional devices to create new input methods for an existing;unmodified application; eg; creating custom gestures on a smartphone to control a desktoppresentation application. Instead of requiring developers to anticipate and program thesecross-device behaviors in advance; Improv enables end users to improvise them on the …,ACM Transactions on Computer-Human Interaction (TOCHI),2017,2
Experimental evaluation of penbased Chinese word processing usability,Zhiwei Guan; Yang Li; Guozhong Dai,An evaluation experiment was conducted to compare four interface styles: Hanwang FixedWindow; Translucent Mobile Single Window; Translucent Mobile Double Window; and NoWindow. Subjects tested the four styles for typical word process task; including input; insert;exchange on a free style text edit system. The results show that the TMDW was the mostefficient and the most accurate mode; in terms of total editing time and accuracy rate. Alsoexperimental data exhibited that NW is the most nature style; in terms of the subjects'preferences. This experiment confirmed that a proper employment of interface style couldimprove the interactive efficiency of text editing systems. Our tests for the first time givestatistical support to the view that the nature gesture is useful for editing interface for wordprocess systems.,Proceedings of APCHI'00 International Conference of Asia-Pacific Computer-Human Interaction; Singapore; Elsevier Science; Nov,2000,2
Gesture recognition on computing device correlating input to a template,*,A computer-implemented user interface method and apparatus are disclosed. A user inputsignal corresponding to a drawn gesture is received and sampled. If the input signal isorientation invariant; the sampled; spaced points are rotated in accordance with anindicative angle to generate an input vector. If the input signal is orientation sensitive; thesampled; spaced points are rotated to align with a base orientation to generate the inputvector. The gesture is recognized based on a comparison of the input vector to a plurality oftemplates.,*,2017,1
Prediction completion gesture,*,In one example; a method includes detecting; by a computing device; at least one usercontact with a presence-sensitive screen of the computing device to input one or morecharacters of an input string. The method also includes detecting; by the computing device; asubsequent user contact with the presence-sensitive screen. The method also includesdetecting; by the computing device; a gesture at a region of the presence-sensitive screenthat is associated with a terminator symbol while the subsequent user contact is maintainedwith the presence-sensitive screen. The method also includes adding; by the computingdevice; the terminator symbol to the input string when the gesture comprises a virtual keypress gesture. The method also includes replacing; by the computing device; the input stringwith a predicted completed string for the input string when the gesture comprises a …,*,2017,1
Using audio cues to support motion gesture interaction on mobile devices,Sarah Morrison-Smith; Megan Hofmann; Yang Li; Jaime Ruiz,Abstract Motion gestures are an underutilized input modality for mobile interaction despitenumerous potential advantages. Negulescu et al. found that the lack of feedback onattempted motion gestures made it difficult for participants to diagnose and correct errors;resulting in poor recognition performance and user frustration. In this article; we describeand evaluate a training and feedback technique; Glissando; which uses audiocharacteristics to provide feedback on the system's interpretation of user input. Thistechnique enables feedback by verbally confirming correct gestures and notifying users oferrors in addition to providing continuous feedback by manipulating the pitch of distinctmusical notes mapped to each of three dimensional axes in order to provide both spatialand temporal information.,ACM Transactions on Applied Perception (TAP),2016,1
Creating and organizing events in an activity stream,*,A system and method for creating and organizing events includes an activity streamapplication that captures; searches and collaborates on one or more events. The eventsinclude unstructured data comprising text; digital ink; an audio clip and an image. Theactivity stream application receives user input and generates a new event and combinesrelated events into the same activity. The activity stream application receives a search queryand searches for events that are relevant to the search query. In one embodiment; thesearch query includes contextual information that includes at least one of at a similar time; ata similar location; in a similar situation and a relatedness of event attributes.,*,2015,1
Specification; Detection; and Notification of RFID Events with Cascadia,Evan Welbourne; Garret Cole; Nodira Khoussainova; Julie Letchner; Yang Li; Magdalena Balazinska; Gaetano Borriello; Dan Suciu,The Cascadia system [5] greatly simplifies the development of pervasive RFID applicationsin spite of the inherent uncertainty in RFID data. It provides applications with aninfrastructure for specifying; extracting and managing meaningful high-level events from rawRFID data. Cascadia allows end users to specify events using a web-based tool with anintuitive visual language based on direct manipulation. It also extracts specified events fromthe data in spite of the unreliability of RFID technology and the inherent ambiguity in eventextraction. We demonstrate Cascadia's workings with a map-based interface that displaysstreams of RFID events and with a digital diary application that automatically populates acalendar with events in which the user participated. We use RFID traces collected in ourbuilding-wide RFID deployment; the RFID Ecosystem [1].,*,*,1
Gesture detection using an array of short-range communication devices,*,Abstract In general; techniques and systems for defining a gesture with a computing deviceusing short-range communication are described. In one example; a method includesobtaining position information from an array of position devices using near-fieldcommunication (NFC) during a movement of the computing device with respect to the array;wherein the position information identifies unique positions within the array for each positiondevice from which position information was obtained. The method may also includedetermining sequence information associated with the position information; wherein thesequence information is representative of an order in which the position information wasobtained from each position device; and performing; by the computing device; an actionbased at least in part on the position information and the sequence information; wherein …,*,2018,*
M3 Gesture Menu: Design and Experimental Analyses of Marking Menus for Touchscreen Mobile Interaction,Jingjie Zheng; Xiaojun Bi; Kun Li; Yang Li; Shumin Zhai,ABSTRACT Despite their learning advantages in theory; marking menus have facedadoption challenges in practice; even on today's touchscreen-based mobile devices. Weaddress these challenges by designing; implementing; and evaluating multiple versions ofM3 Gesture Menu (M3); a reimagination of marking menus targeted at mobile interfaces. M3is defined on a grid rather than in a radial space; relies on gestural shapes rather thandirectional marks; and has constant and stationary space use. Our first controlled experimenton expert performance showed M3 was faster and less error-prone by a factor of two thantraditional marking menus. A second experiment on learning demonstrated for the first timethat users could successfully transition to recall-based execution of a dozen commands afterthree ten-minute practice sessions with both M3 and Multi-Stroke Marking Menu …,*,2018,*
Auto-completion for user interface design,*,Techniques for automatically completing a partially completed UI design created by a userare described. A UI query including attributes of UI components in the partially completed UIdesign is created. Example designs with similar UI components are identified. UIcomponents of one such design example are displayed to automatically complete thepartially completed UI design (also called an “auto-complete suggestion”). The user cansystematically navigate the design examples and accept auto-completed suggestions toinclude into the partially complete UI design.,*,2017,*
Authenticating user and launching an application on a single intentional user gesture,*,Methods; systems; and apparatus; including computer programs encoded on computerstorage media; for combining authentication and application shortcut. An example methodincludes detecting; by a device having a touchscreen; a gesture by a user on thetouchscreen while the device is in a sleep mode; classifying the gesture; by the device; asan intentional gesture or an accidental gesture; maintaining the device in the sleep mode ifthe gesture is classified as an accidental gesture; responsive to determining; by the device;that the gesture matches one or more confirmed gestures stored on the device based atleast in part on a set of predefined criteria; if the gesture is classified as an intentionalgesture: recognizing the user as authenticated; and without requiring additional user input;selecting an application; from a plurality of different applications; according to the gesture …,*,2017,*
Gesture morpher: video-based retargeting of multi-touch interactions,Ramik Sadana; Yang Li,Abstract We present Gesture Morpher; a tool for prototyping and testing multi-touchinteractions based on video recordings of target application behaviors; eg; a sequence ofscreenshots recorded by a screen capture tool. Gesture Morpher extracts continuousbehaviors from video recordings; such as transformations of UI content; and suggests a setof multi-touch interactions that are suitable for achieving these behaviors. Designers caneasily test different interactions on a touch device with visual response that is automaticallysynthesized from the video recording; all without any programming. We discuss a range ofmulti-touch interaction scenarios Gesture Morpher supports; our method for extractingcontinuous interaction behaviors from video recordings; and techniques for associatingtouch-input with the output effect extracted from the videos.,Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services,2016,*
Auto-completion for user interface design,*,Techniques for automatically completing a partially completed UI design created by a userare described. A UI query including attributes of UI components in the partially completed UIdesign is created. Example designs with similar UI components are identified. UIcomponents of one such design example are displayed to automatically complete thepartially completed UI design (also called an “auto-complete suggestion”). The user cansystematically navigate the design examples and accept auto-completed suggestions toinclude into the partially complete UI design.,*,2016,*
Auto-completion for user interface design,*,Techniques for automatically completing a partially completed UI design created by a userare described. A UI query including attributes of UI components in the partially completed UIdesign is created. Example designs with similar UI components are identified. UIcomponents of one such design example are displayed to automatically complete thepartially completed UI design (also called an “auto-complete suggestion”). The user cansystematically navigate the design examples and accept auto-completed suggestions toinclude into the partially complete UI design.,*,2016,*
Enabling event prediction as an on-device service for mobile interaction,*,By knowing which upcoming actions a user might perform; a mobile application can optimizea user interface or reduce the amount of user input needed for accomplishing a task. Aherein-described prediction module can answer queries from a mobile application regardingwhich actions in the application the user is likely to perform at a given time. Any applicationcan register and communicate with the prediction module via a straightforward applicationprogramming interface (API). The prediction module continuously learns a prediction modelfor each application based on the application's evolving event history. The predictionmodule generates predictions by combining multiple predictors with an online learningmethod; and capturing event patterns not only within but also across registered applications.The prediction module is evaluated using events collected from multiple types of mobile …,*,2016,*
Auto-completion for user interface design,*,Techniques for automatically completing a partially completed UI design created by a userare described. A UI query including attributes of UI components in the partially completed UIdesign is created. Design examples with similar UI components are identified. UIcomponents of one such design example are displayed to automatically complete thepartially completed UI design (also called an “auto-complete suggestion”). The user cansystematically navigate the design examples and accept auto-completed suggestions toinclude into the partially complete UI design.,*,2016,*
Novel Tools for Programming Mobile Interaction,Yang Li,The prevalence of mobile devices such as smartphones has fundamentally changed howwe use computing devices. Mobile devices are now an indispensable part of our everydayactivities; from casual activities such as physical workout or cooking to serious productiontasks such as authoring a document. However; mobile interaction suffers from two majorissues: the small form factor of mobile devices–the device constraint–and mobile usersbeing occupied by real-world tasks while operating mobile devices–the user constraint. Toaddress these issues; extensive effort has been devoted to expanding the interactionbandwidth of mobile devices by 1) enabling interaction techniques based on new sensingcapabilities such as touch; motion or camera-based input; and 2) leveraging the rich contextsuch as the time and the behavior history in which mobile interaction is situated to …,한국 HCI 학회 학술대회,2014,*
ユーザインタフェースとインタラクティブシステム,Guan Zhiwei; Ren Xiangshi; Li Yang; Dai Guozhong,抄録 This paper describes the design and evaluation of a new selection technique calledZoom Selector. Zoom Selector uses a circular zoom area to select a small target. It employsa pre-selection mechanism to activate the objects (eg icons) inside the zoom area. ZoomSelector relocates and enlarges these captured objects as sectors of a zoom pie accordingto their original locations. An empirical evaluation was performed to compare Zoom Selectorwith other four selection techniques. The evaluation results indicate that Zoom Selectorhelps a user easily select small target and reduces the cognitive burden of each targetacquisition. This evaluation also provides clear evidence that Zoom Selector has a potentialto greatly enhance the efficiency accuracy and usability of small target acquisition in mobilesystems. This paper describes the design and evaluation of a new selection technique …,情報処理学会論文誌,2004,*
A Specification Language for Post-WIMP User Interfaces Based on Hybrid Automaton,Y Li; Z Guan; G Dai; G Li,Post-WIMP (ie Windows; Icons; Menus and Pointer) interface as the next generation userinterface has substantial differences from WIMP interface; which dominates the currenthumancomputer interaction. It provides more natural and effective way for interaction. Inorder to construct post-WIMP interface effectively; it is a better way to specify it at an abstractlevel without concerning the details of the implementation before construction. In this paper;the essence of post-WIMP interface is discussed. The most distinguished attribute of post-WIMP is the property of continuous interaction. We analyze post-WIMP interaction from thepoint of view of hybrid system; which can give accurate and strict analysis to post-WIMPinterface. We employ hybrid automaton to specify the interaction in post-WIMP interface. Asemi-formal specification language LEA2F is introduced for specifying post-WIMP …,UNIT 45002 APO AP 96337-5002,2000,*
Improving Target Acquisitions through Utilizing Pen Pressure,Xiangshi Ren; Jibin Yin; Shengdong Zhao; Yang Li,*,*,*,*
