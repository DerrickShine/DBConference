Life-like characters: tools; affective functions; and applications,Helmut Prendinger; Mitsuru Ishizuka,Life-like characters is one of the most exciting technologies for human-computer interfaceapplications today. They convincingly take the roles of virtual presenters; synthetic actorsand sales personas; teammates and tutors. A common characteristic underlying their life-likeness or believability as virtual conversational partners is computational models thatprovide them with affective functions such as synthetic emotions and personalities andimplement human interactive behavior. The wide dissemination of life-like characters inmultimedia systems; however; will greatly depend on the availability of control languagesand tools that facilitate scripting of intelligent conversational behaviour. This book presentsthe first comprehensive collection of the latest developments in scripting and representationlanguages for life-like characters; rounded off with an in-depth comparison and synopsis …,*,2013,276
Life-like characters: tools; affective functions; and applications,Helmut Prendinger; Mitsuru Ishizuka,Life-like characters is one of the most exciting technologies for human-computer interfaceapplications today. They convincingly take the roles of virtual presenters; synthetic actorsand sales personas; teammates and tutors. A common characteristic underlying their life-likeness or believability as virtual conversational partners is computational models thatprovide them with affective functions such as synthetic emotions and personalities andimplement human interactive behavior. The wide dissemination of life-like characters inmultimedia systems; however; will greatly depend on the availability of control languagesand tools that facilitate scripting of intelligent conversational behaviour. This book presentsthe first comprehensive collection of the latest developments in scripting and representationlanguages for life-like characters; rounded off with an in-depth comparison and synopsis …,*,2013,276
A media conversion from speech to facial image for intelligent man-machine interface,Shigeo Morishima; Hiroshi Harashima,An automatic field motion image synthesis scheme (driven by speech) and a real-time imagesynthesis design are presented. The purpose of this research is to realize an intelligenthuman-machine interface or intelligent communication system with talking head images. Ahuman face is reconstructed on the display of a terminal using a 3-D surface model andtexture mapping technique. Facial motion images are synthesized naturally bytransformation of the lattice points on 3-D wire frames. Two driving motion methods; a text-to-image conversion scheme and a voice-to-image conversion scheme; are proposed. In thefirst method; the synthesized head image can appear to speak some given words andphrases naturally. In the second case; some mouth and jaw motions can be synthesized insynchronization with voice signals from a speaker. Facial expressions other than mouth …,IEEE Journal on selected areas in communications,1991,143
Real-time estimation of human body posture from monocular thermal images,Shoichiro Iwasawa; Kazuyuki Ebihara; Jun Ohya; Shigeo Morishima,This paper introduces a new real-time method to estimate the posture of a human fromthermal images acquired by an infrared camera regardless of the back-ground and lightingconditions. Distance transformation is performed for the human body area extracted from thethresholded thermal image for the. Calculation of the center of gravity. After the orientation ofthe upper half of the body is obtained by calculating the moment of inertia; significant pointssuch as the top of the head; the tips of the hands and foot are heuristically located. Inaddition; the elbow and foot positions are estimated from the detected (significant) pointsusing a genetic algorithm based learning procedure. The experimental results demonstratethe robustness of the proposed algorithm and real-time (faster than 20 frames per second)performance.,Computer Vision and Pattern Recognition; 1997. Proceedings.; 1997 IEEE Computer Society Conference on,1997,119
An intelligent facial image coding driven by speech and phoneme,Shigeo Morishima; Kiyohm Aizawa; Himshi Harashima,The authors propose and compare two types of model-based facial motion coding schemes;ie synthesis by rules and synthesis by parameters. In synthesis by rules; facial motionimages are synthesized on the basis of rules extracted by analysis of training imagesamples that include all of the phonemes and coarticulation. This system can be utilized asan automatic facial animation synthesizer from text input or as a man-machine interfaceusing the facial motion image. In synthesis by parameters; facial motion images aresynthesized on the basis of a code word index of speech parameters. Experimental resultsindicate good performance for both systems; which can create natural facial-motion imageswith very low transmission rate. Details of 3-D modeling; algorithm synthesis; andperformance are discussed.,Acoustics; Speech; and Signal Processing; 1989. ICASSP-89.; 1989 International Conference on,1989,117
Expression analysis/synthesis system based on emotion space constructed by multilayered neural network,Nobuo Ueki; Shigeo Morishima; Hiroshi Yamada; Hiroshi Harashima,*,Systems and Computers in Japan,1994,64
Real-time; 3D estimation of human body postures from trinocular images,Shoichiro Iwasawa; Jun Ohya; Kazuhiko Takahashi; Tatsumi Sakaguchi; Sinjiro Kawato; Kazuyuki Ebihara; Sigeo Morishima,This paper proposes a new real-time method for estimating human postures in 3D fromtrinocular images. In this method; an upper body orientation detection and a heuristiccontour analysis are performed on the human silhouettes extracted from the trinocularimages so that representative points such as the top of the head can be located. The majorjoint positions are estimated based on a genetic algorithm based learning procedure. 3Dcoordinates of the representative points and joints are then obtained from the two views byevaluating the appropriateness of the three views. The proposed method implemented on apersonal computer runs in real-time (30 frames/second). Experimental results show highestimation accuracies and the effectiveness of the view selection process.,Modelling People; 1999. Proceedings. IEEE International Workshop on,1999,63
Face analysis and synthesis,Shigeo Morishima,The author's goal is to generate a virtual space close to the real communication environmentbetween network users or between humans and machines. There should be an avatar incyberspace that projects the features of each user with a realistic texture-mapped face togenerate facial expression and action controlled by a multimodal input signal. Users canalso get a view in cyberspace through the avatar's eyes; so they can communicate with eachother by gaze crossing. The face fitting tool from multi-view camera images is introduced tomake a realistic three-dimensional (3-D) face model with texture and geometry very close tothe original. This fitting tool is a GUI-based system using easy mouse operation to pick upeach feature point on a face contour and the face parts; which can enable easy constructionof a 3-D personal face model. When an avatar is speaking; the voice signal is essential in …,IEEE Signal Processing Magazine,2001,60
Emotion space for analysis and synthesis of facial expression,Shigeo Morishima; Hiroshi Harashima,This paper presents a new emotion model which gives a criteria to decide human's emotioncondition from the face image. Our final goal is to realize very natural and user-friendlyhuman-machine communication environment by giving a face to computer terminal orcommunication system which can also understand the user's emotion condition. So it isnecessary for the emotion model to express emotional meanings of a parameterized faceexpression and its motion quantitatively. Our emotion model is based on 5-layered neuralnetwork which has generalization and nonlinear mapping performance. Both input andoutput layer has the same number of units. So identity mapping can be realized and emotionspace can be constructed in the middle-layer (3rd layer). The mapping from input layer tomiddle layer means emotion recognition and that from middle layer to output layer …,Robot and Human Communication; 1993. Proceedings.; 2nd IEEE International Workshop on,1993,57
擬人化音声対話エージェントツールキット Galatea,嵯峨山茂樹， 川本真一， 下平博， 新田恒雄， 西本卓也， 中村哲， 伊藤克亘， 森島繁生， 四倉達夫， 甲斐充彦， 李晃伸， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 峯松信明， 山田篤， 伝康晴， 宇津呂武仁,あらまし 筆者らが開発した擬人化音声対話エージェントのツールキット “Galatea”についてその概要を述べる. 主要な機能は音声認識; 音声合成; 顔画像合成であり;これらの機能を統合して; 対話制御の下で動作させるものである. 研究のプラットフォームとして利用されることを想定してカスタマイズ可能性を重視した結果; 顔画像が容易に交換可能で;音声合成が話者適応可能で; 対話制御の記述変更が容易で; 更にこれらの機能モジュール自体を別のモジュールに差し替えることが容易であり; かつ処理ハードウェアの個数に柔軟に対処できるなどの特徴を持つシステムとなった. この成果はソース公開し; 一般に無償使用許諾する予定である.,情報処理学会研究報告音声言語情報処理 (SLP),2003,42
Real-time talking head driven by voice and its application to communication and entertainment,Shigeo Morishima,ABSTRACT Recently computer can make cyberspace to walk through by an interactivevirtual reality technique. An avatar in cyberspace can bring us a virtual face-tofacecommunication environment. In this paper; we realize an avatar which has a real face incyberspace and construct a multi-user communication system by voice transmission throughnetwork. Voice from microphone is transmitted and analyzed; then mouth shape and facialexpression of avatar are synchronously estimated and synthesized on real time. And alsowe introduce an entertainment application of a real-time voice driven synthetic face. Thisproject is named" Fifteen Seconds of Fame" which is an example of interactive movie.,AVSP'98 International Conference on Auditory-Visual Speech Processing,1998,39
「顔」 の情報処理,長谷川修， 森島繁生， 金子正秀,「顔」 は人間にとって非常に身近な存在であると共に; 顔の持ち主である一人一人の人間における個人的な情報; コミュニケーションに係わる情報を始めとした; 言語的手段では表現しにくいようなさまざまな情報を担っている. 近年; 工学分野では主としてコミュニケーションメディアやヒューマンインタフェースへの応用の観点から;「顔」 の工学的取扱いに対する研究が活発に行われている.具体的には; ユーザである人間を対象とした視覚機能をコンピュータにもたせるための顔の認識技術と; コンピュータあるいはコミュニケーションメディアに表現力豊かな顔をもたせるための顔の合成技術である. これらの研究成果は; 従来個別に検討が行われていた顔関連の心理学; 人類学; 美容;歯科等さまざまな分野においても活用されつつある. 本論文では; このような観点からコンピュータによる顔情報処理に焦点を当て; まず要素技術としての顔画像合成と表情認識について最近の技術動向を概観する. 次に;「顔」 の諸特性について考察した後に; 人と人との対面コミュニケーションの …,電子情報通信学会論文誌 A,1997,39
Emotion modeling in speech production using emotion space,Jun Sato; Shigeo Morishima,This paper describes the modeling scheme of emotions appearing in a speech productionby using a neural network and the synthesizing technique of emotional condition fromneutral speech. To model emotion conditions in speech production; emotion space isintroduced. Emotion space can represent the emotion condition appearing in speechproduction in a two dimensional space and realize both mapping and inverse mappingbetween the emotion condition and the speech production. We developed the emotionalspeech synthesizer to synthesize emotional speech. The emotional speech synthesizer hasan ability to synthesize emotional speech by modifying neutral speech in its timing; pitch andintensity. This paper also describes the subjective evaluation result of synthesized speechfrom the emotion space.,Robot and Human Communication; 1996.; 5th IEEE International Workshop on,1996,35
Physics-based muscle model for mouth shape control,Hajime Sera; Shigeo Morishima; Demetri Terzopoulos,Human image synthesis by computer graphics is essential to a virtual agent in humaninterfaces and entertainment visual systems. In this paper; a muscle model is proposed tocreate a super realistic human face. There are several researches to synthesize humanexpression; however; research about mouth shape control in conversation is limited to ourgroup. Especially; we try to choose and modify muscles which are good for mouth shapegeneration to realize a natural conversation scene. Basic mouth shape is defined bymeasuring the real image captured by camera. We also try to make animation usingstandard phoneme duration to realize lip-sync.,Robot and Human Communication; 1996.; 5th IEEE International Workshop on,1996,34
Real-time facial action image synthesis system driven by speech and text,Shigeo Morishima; Kiyoharu Aizawa; Hiroshi Harashima,Automatic facial motion image synthesis schemes and a real-time system design arepresented. The purpose of this schemes is to realize an intelligent human-machine interfaceor intelligent communication system with talking head images. Human's face isreconstructed with 3D surface model and texture mapping technique on the display ofterminal. Facial motion images are synthesized naturally by transformation of the latticepoints on wire frames. Two types of motion drive methods; text to image conversion andspeech to image conversion are proposed in this paper. In the former manner; synthesizedhead can speak some given texts naturally and in the latter case; some mouth and jawmotions can be synthesized in time to speech signal of behind speaker. These schemeswere implemented to a parallel image computer and a real-time image synthesizer could …,Visual Communications and Image Processing'90: Fifth in a Series,1990,34
Galatea: Open-source software for developing anthropomorphic spoken dialog agents,Shin-ichi Kawamoto; Hiroshi Shimodaira; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Keikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro; Shigeki Sagayama,Summary Galatea is a software toolkit to develop a human-like spoken dialog agent. In orderto easily integrate the modules of different characteristics including speech recognizer;speech synthesizer; facial animation synthesizer; and dialog controller; each module ismodeled as a virtual machine having a simple common interface and connected to eachother through a broker (communication manager). Galatea employs model-based speechand facial animation synthesizers whose model parameters are adapted easily to those foran existing person if his or her training data is given. The software toolkit that runs on bothUNIX/Linux and Windows operating systems will be publicly available in the middle of 2003[7; 6].,*,2004,32
HyperMask–projecting a talking head onto a real object,Tatsuo Yotsukura; Shigeo Morishima; Frank Nielsen; Kim Binsted; Claudio Pinhanez,HyperMask is a system which projects an animated face onto a physical mask worn by anactor. As the mask moves within a prescribed area; its position and orientation are detectedby a camera and the projected image changes with respect to the viewpoint of the audience.The lips of the projected face are automatically synthesized in real time with the voice of theactor; who also controls the facial expressions. As a theatrical tool; Hyper-Mask enables anew style of storytelling. As a prototype system; we put a self-contained HyperMask systemin a trolley (disguised as a linen cart); so that it projects onto the mask worn by the actorpushing the trolley.,The Visual Computer,2002,31
Modeling of facial expression and emotion for human communication system,Shigeo Morishima,Abstract The goal of this research is to realize a face-to-face communication environmentwith machine by giving a facial expression to computer system. In this paper; modelingmethods of facial expression including 3D face model; expression model and emotionmodel are presented. Facial expression is parameterized with Facial Action Coding System(FACS) which is translated to the grid's motion of face model which is constructed from the3D range sensor data. An emotion condition is described compactly by the point in a 3Dspace generated by a 5-layered neural network and its evaluation result shows the highperformance of this model.,Displays,1996,29
An assessment of the effectiveness of high definition cameras as remote monitoring tools for dolphin ecology studies,Estênio Guimarães Paiva; Chandra Salgado-Kent; Marthe Monique Gagnon; Iain Parnum; Robert McCauley,Research involving marine mammals often requires costly field programs. This paperassessed whether the benefits of using cameras outweighs the implications of havingpersonnel performing marine mammal detection in the field. The efficacy of video and stillcameras to detect Indo-Pacific bottlenose dolphins (Tursiops aduncus) in the FremantleHarbour (Western Australia) was evaluated; with consideration on how environmentalconditions affect detectability. The cameras were set on a tower in the Fremantle Portchannel and videos were perused at 1.75 times the normal speed. Images from the cameraswere used to estimate position of dolphins at the water's surface. Dolphin detections rangedfrom 5.6 m to 463.3 m for the video camera; and from 10.8 m to 347.8 m for the still camera.Detection range showed to be satisfactory when compared to distances at which dolphins …,PloS one,2015,28
Human body postures from trinocular camera images,Shoichiro Iwasawa; Jun Ohya; Kazuhiko Takahashi; Tatsumi Sakaguchi; Kazuyuki Ebihara; Shigeo Morishima,This paper proposes a new real-time method for estimating human postures in 3D fromtrinocular images. In this method; an upper body orientation detection and a heuristiccontour analysis are performed on the human silhouettes extracted from the trinocularimages so that representative points such as the top of the head can be located. The majorjoint positions are estimated based on a genetic algorithm-based learning procedure. 3Dcoordinates of the representative points and joints are then obtained from the two views byevaluating the appropriateness of the three views. The proposed method implemented on apersonal computer runs in real-time. Experimental results show high estimation accuraciesand the effectiveness of the view selection process.,Automatic Face and Gesture Recognition; 2000. Proceedings. Fourth IEEE International Conference on,2000,28
Real-time human posture estimation using monocular thermal images,Shoichiro Iwasawa; Kazuyuki Ebihara; Jun Ohya; Shigeo Morishima,This paper introduces a new real-time method to estimate the posture of a human fromthermal images acquired by an infrared camera regardless of the background and lightingconditions. Distance transformation is performed for the human body area extracted from thethresholded thermal image for the calculation of the center of gravity. After the orientation ofthe upper half of the body is obtained by calculating the moment of inertia; significant pointssuch as the top of the head; the tips of the hands and foot are heuristically located. Inaddition; the elbow and knee positions are estimated from the detected (significant) pointsusing a genetic algorithm based learning procedure. The experimental results demonstratethe robustness of the proposed algorithm and real-time (faster than 20 frames per second)performance.,Automatic Face and Gesture Recognition; 1998. Proceedings. Third IEEE International Conference on,1998,27
Open-source software for developing anthropomorphic spoken dialog agent,Shin-ichi Kawamoto; Hiroshi Shimodaira; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Keikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro; Shigeki Sagayama,Abstract An architecture for highly-interactive human-like spoken-dialog agent is discussedin this paper. In order to easily integrate the modules of different characteristics includingspeech recognizer; speech synthesizer; facial-image synthesizer and dialog controller; eachmodule is modeled as a virtual machine that has a simple common interface and isconnected to each other through a broker (communication manager). The agent systemunder development is supported by the IPA and it will be publicly available as a softwaretoolkit this year.,Proc. of PRICAI-02; International Workshop on Lifelike Animated Agents,2002,24
Construction and psychological evaluation of 3-d emotion space,FUMIO KAWAKAMI; HIROSHI YAMADA; SHIGEO MORISHIMA; HIROSHI HARASHIMA,Tecognizing and synthesizingthefacialexpression can suTely improve the communicationenvironment between human and machine. Emotion isone of the mostirnportantfactorswhich facialexpression can descTibe. Especialiywe focused on this emotionconditionwhich appears on humanlsface.,International Journal of Biomedical Soft Computing and Human Sciences: the official journal of the Biomedical Fuzzy Systems Association,1995,24
カスタマイズ性を考慮した擬人化音声対話ソフトウェアツールキットの設計,川本真一， 下平博， 新田恒雄， 西本卓也， 中村哲， 伊藤克亘， 森島繁生， 四倉達夫， 甲斐充彦， 李晃伸， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 峯松信明， 山田篤， 伝康晴， 宇津呂武仁， 嵯峨山茂樹,川 本 真 一† 1 下 平 博† 1 新 田 恒 雄† 2 西 本 卓 也† 3 中 村 哲† 4 伊 藤 克 亘† 5 森 島 繁 生†6 四 倉 達 夫† 6 甲 斐 充 彦† 7 李 晃 伸† 8 山 下 洋 一† 9 小 林 隆 夫† 10 徳 田 恵 一† 11 広 瀬啓 吉† 14 峯 松 信 明† 14 山 田 篤† 12 伝 康 晴† 13 宇津呂 武仁† 2 嵯峨山 茂樹† 14本論文では; 擬人化音声対話エージェントを将来のヒューマンインタフェースの重要な技術要素として位置づけ; 研究開発の共通プラットフォームとなりうる高いカスタマイズ可能性を備えたソフトウェアツールキットの実現を目指し; それに必要な要素とその実現技術について論じる. 今後のヒューマンインタフェース技術において; コンピュータがあたかも一個の人間として振る舞い;人間の顔や姿を持ち; ユーザと音声言語で対話するようにすることは; 大きな目標の 1 つである.このような研究開発を進めるにあたっては; 多分野の協力が必要であり; 研究成果を集積していくための共通プラットフォームが必要である. それには; 音声認識; 音声合成; 画像合成; 対話制御 …,情報処理学会論文誌,2002,22
Processing of facial information by computer,Osamu Hasegawa; Shigeo Morishima; Masahide Kaneko,Abstract 「顔」 は人間にとって非常に身近な存在であると共に; 顔の持ち主である一人一人の人間における個人的な情報; コミュニケーションに係わる情報を始めとした; 言語的手段では表現しにくいようなさまざまな情報を担っている. 近年; 工学分野では主としてコミュニケーションメディアやヒューマンインタフェースへの応用の観点から;「顔」 の工学的取扱いに対する研究が活発に行われている. 具体的には; ユーザである人間を対象とした視覚機能をコンピュータにもたせるための顔の認識技術と; コンピュータあるいはコミュニケーションメディアに表現力豊かな顔をもたせるための顔の合成技術である. これらの研究成果は; 従来個別に検討が行われていた顔関連の心理学;人類学; 美容; 歯科等さまざまな分野においても活用されつつある. 本論文では; このような観点からコンピュータによる顔情報処理に焦点を当て; まず要素技術としての顔画像合成と表情認識について最近の技術動向を概観する. 次に;「顔」 の諸特性について考察した後に; 人と人との対面 …,The Transactions of the Institute of Electronics; Information and Communication Engineers.,1997,22
多層ニューラルネットによって構成された感情空間に基づく表情の分析・合成システムの構築,上木伸夫， 森島繁生， 山田寛， 原島博,人間とコンピュータとがあたかもフェーストゥフェースの環境で対話できるユーザフレンドリーなインタフェースを実現するためには; コンピュータが相手である人間の顔表情の認識を行って感情状態を把握し; それに対するふさわしい自然な表情を合成・表示できる必要がある.この表情分析・合成を容易に記述するためには; コンピュータが何らかの感情モデルを自らもつ必要がある. 汎化性能をもち; 非線形写像能力に優れた 5 層ニューラルネットを用いて;パラメータで記述された顔表情を恒等写像学習させ; そのとき生成された中間層出力空間を感情モデルとみなすことで感情空間を構成した. また; この感情空間に基づいて表情から感情;更に感情から表情へのマッピングを同時に実現するシステムの構築を試みた. また;生成された感情空間の主観評価を行い; このモデルの妥当性を確認した. 更に;実際に人物の顔画像から情報認識を行うため; 顔の特徴点から表情パラメータを求める手法 …,電子情報通信学会論文誌 D,1994,22
Dancereproducer: An automatic mashup music video generation system by reusing dance video clips on the web,Tomoyasu Nakano; Sora Murofushi; Masataka Goto; Shigeo Morishima,ABSTRACT We propose a dance video authoring system; DanceReProducer; that canautomatically generate a dance video clip appropriate to a given piece of music bysegmenting and concatenating existing dance video clips. In this paper; we focus on thereuse of everYincreasing userYgenerated dance video clips on a video sharing web service.In a video clip consisting of music (audio signals) and image sequences (video frames); theimage sequences are often synchronized with or related to the music. Such relationY shipsare diverse in different video clips; but were not dealt with by previous methods for automaticmusic video genY eration. Our system employs machine learning and beat tracNingtechniques to model these relationships. To genY erate new music video clips; short imagesequences that have been previously extracted from other music clips are stretched and …,Proc. of the 8th Sound and Music Computing Conference (SMC 2011),2011,21
Instant casting movie theater: the future cast system,Akinobu Maejima; Shuhei Wemler; Tamotsu Machida; Masao Takebayashi; Shigeo Morishima,Summary: We have developed a visual entertainment system called" Future Cast" whichenables anyone to easily participate in a pre-recorded or pre-created film as an instant CGmovie star. This system provides audiences with the amazing opportunity to join the cast of amovie in real-time. The Future Cast System can automatically perform all the processesrequired to make this possible; from capturing participants' facial characteristics to renderingthem into the movie. Our system can also be applied to any movie created using the sameproduction process. We conducted our first experimental trial demonstration of the FutureCast System at the Mitsui-Toshiba pavilion at the 2005 World Exposition in Aichi Japan.,IEICE transactions on information and systems,2008,21
顔画像を基にした 3 次元感情モデルの構築とその評価,坂口竜己， 山田寛， 森島繁生,人間の感情状態をコンピュータ内で擬似的に表現するために; 感情モデルを構築する研究を進めている. この感情モデルを顔表情の記述に利用することで; 顔表情画像の圧縮伝送等に応用が可能となる. 既に 5 層ニューラルネットワークを表情記述パラメータにより恒等写像学習を行うことで;その中間層に内部構造としての 2 次元感情空間 (感情モデル) を構築する手法を提案しているが;心理学的に不十分な解釈しか行われていなかった. 本論文では感情モデルを 3 次元に拡張し;作成された感情空間の心理学的な妥当性の検証; および表情認識システムの構築も行った.多数の被験者による主観評価実験により空間内の位置と心理学的評価の対応関係を明らかにし;この感情モデルによる表情の認識手法では; より人間に近い反応が得られることを示す.,電子情報通信学会論文誌 A,1997,21
Facial image reconstruction by estimated muscle parameter,Takahiro Ishikawa; Hajime Sera; Shigeo Morishima; Demetri Terzopoulos,Muscle based face image synthesis is one of the most realistic approach to realize life-likeagent in computer. Facial muscle model is composed of facial tissue elements and muscles.In this model; forces are calculated effecting facial tissue element by contraction of eachmuscle strength; so the combination of each muscle parameter decide a specific facialexpression. Now each muscle parameter is decided on trial and error procedure comparingthe sample photograph and generated image using our Muscle-Editor to generate a specificface image. In this paper; we propose the strategy of automatic estimation of facial muscleparameters from 2D marker movements using neural network. This corresponds to the non-realtime 3D facial motion tracking from 2D image under the physics based condition.,Automatic Face and Gesture Recognition; 1998. Proceedings. Third IEEE International Conference on,1998,20
Construction of 3-D emotion space using neural network,F Kawakami,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proceedings of the 3rd International Conference on Fuzzy Logic; Neural Nets and Soft Computing; Iizuka; 1994,1994,20
Speech-to-image media conversion based on VQ and neural network,Shigeo Morishima; Hiroshi Harashima,Automatic media conversion schemes from speech to a facial image and a construction of areal-time image synthesis system are presented. The purpose of this research is to realizean intelligent human-machine interface or intelligent communication system withsynthesized human face images. A human face image is reconstructed on the display of aterminal using a 3-D surface model and texture mapping technique. Facial motion imagesare synthesized by transformation of the 3-D model. In the motion driving method; based onvector quantization and the neural network; the synthesized head image can appear tospeak some given words and phrases naturally; in synchronization with voice signals from aspeaker.,Acoustics; Speech; and Signal Processing; 1991. ICASSP-91.; 1991 International Conference on,1991,20
画像の 2 次元離散コサイン変換を利用した実時間顔表情認識,坂口竜己， 森島繁生,人物顔表情の認識は; 心理学や工学などさまざまな分野での応用を期待され;多くの研究がなされている. しかしそのほとんどは; 認識の精度と計算量とのトレードオフにより実時間での処理は困難であった. 本論文では比較的低速なワークステーションや PC上での動作を前提とした実時間表情認識システムを提案する. 顔の領域抽出はフレーム間差分画像によりまばたきを検出することで行い; 動画像中の領域追跡では 1 次元の相関マッチングを利用する. この手法は単純なアルゴリズムで実現されるため高速であり; 表情の特徴を空間周波数成分から得る本手法と組み合わせる場合において十分な性能をもっている. 表情認識は離散コサイン変換(DCT) とニューラルネットワークにより行う. 特定個人の 4 表情認識では; 動画像中の領域追跡も含め; 90% 以上の精度が得られることを確認できた.,電子情報通信学会論文誌 D,1997,19
Better face communication,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Visual Proceedings of ACM SIGGRAPH'95; Interactive Communities,1995,19
Facial muscle parameter decision from 2D frontal image,Shigeo Morishima; Takahiro Ishikawa; Demetri Terzopoulos,Muscle based face image synthesis is one of the most realistic approaches to realizing life-like agents in a computer. A facial muscle model is composed of facial tissue elements andmuscles. In this model; forces are calculated effecting facial tissue elements by contraction ofeach muscle strength; so the combination of each muscle parameter decides a specificfacial expression. Each muscle parameter is decided based on a trial and error procedurecomparing the sample photograph and generated image using our Muscle-Editor togenerate a specific face image. We propose a strategy of automatic estimation of facialmuscle parameters from 2D marker movements using a neural network. We can also carryout 3D motion estimation from 2D point or flow information in a captured image underrestriction of a physics based face model.,Pattern Recognition; 1998. Proceedings. Fourteenth International Conference on,1998,17
顔の認識・合成のための標準ソフトウェアの開発,森島繁生， 八木康史， 金子正秀， 原島博， 谷内田正彦， 原文雄,抄録 顔画像の処理に関する共通ソフトウェアツール作成に向けた活動は;「感性擬人化エージェントのための顔情報処理システムの開発」 と呼ばれ; 情報処理振興技術協会 (IPA)における独創的情報技術育成事業に関わる開発テーマの一つとして; 平成 7 年度より 3年間の計画で精力的に活動を行ってきた. そしてそのプロジェクトの成果として顔の認識・合成システムのための標準ソフトウェアを構築した. 本システムでは工学のみならず心理学や医学;歯学などの分野も含めた顔関連分野における共通の実験用ツールを広く提供することも目標としている.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,1998,17
Expression and motion control of hair using fast collision detection methods,Makoto Ando; Shigeo Morishima,Abstract A trial to generate the object in the natural world by computer graphics is nowactively done. Normally; huge computing power and storage capacity are necessary to makereal and natural movement of the human's hair. In this paper; a technique to synthesizehuman's hair with short processing time and little storage capacity is discussed. A newSpace Curve Model and Rigid Segment Model are proposed. And also high speed collisiondetection with the human's body is discussed.,International Computer Science Conference,1995,17
A modeling of facial expression and emotion for recognition and synthesis,Shigeo Morishima; Fumio Kawakami; Hiroshi Yamada; Hiroshi Harashima,Publisher Summary Facial expression is essential to human communication as well asvoice. It includes several kinds of factors which can express non-verbal information as avoluntary or a spontaneous activity. So; recognizing and synthesizing facial expressions canalso improve the communication environment between humans and machines. Emotion isone of the most important factors which facial expressions can describe. This chapterpresents an approach to achieve the modeling of a human's emotion appearing on the face.This system helps in the analysis; synthesis; and coding of the face image on the emotionlevel. There are many recent researches about facial image recognition and synthesis.However; the trials for modeling of emotion conditions have only been discussed by somepsychologists since a few years ago. But it's not available as the criteria for the …,*,1995,17
Multimedia ambiance communication,Tadashi Ichikawa; Kunio Yamada; Toshifumi Kanamaru; Takeshi Naemura; Kiyoharu Aizawa; Shigeo Morishima; Takahiro Saito,The photo-realistic 3-D image space of multimedia ambiance communication offersenhanced interpersonal communication. Formed by taking the laws of perspective and thecharacteristics of human visual perception into account; the space provides a naturalenvironment that users can feel to be a part of. It is based on the concept of a three-layerstructure; with long-; mid-; and short-range views. We constructed a testbed for multimediaambiance communication consisting of a high-speed graphics computer and a curvedscreen onto which images are stereoscopically projected from the rear; and developed athree-camera system for capturing environment images. In addition; we developed the two-plane expression for processing backdrop views and highly efficient mid-range views.Finally; we constructed a photo-realistic 3-D image space using these image processing …,IEEE Signal Processing Magazine,2001,16
HYPERMASK TALKING HEAD PROJECTED ONTO REAL OBJECT,Shigeo Morishima; Tatsuo Yotsukura; Kim Binsted; Frank Nielsen; Claudio Pinhanez,Abstract HYPERMASK is a system which projects an animated face onto a physical mask;worn by an actor. As the mask moves within a prescribed area; its position and orientationare detected by a camera; and the projected image changes with respect to the viewpoint ofthe audience. The lips of the projected face are automatically synthesized in real time withthe voice of the actor; who also controls the facial expressions. As a theatrical tool;HYPERMASK enables a new style of storytelling. As a prototype system; we propose to put aself-contained HYPERMASK system in a trolley (disguised as a linen cart); so that it projectsonto the mask worn by the actor pushing the trolley.,*,2000,16
Simulation-based cartoon hair animation,Eiji Sugisaki; Yizhou Yu; Ken Anjyo; Shigeo Morishima,This paper describes a new hybrid technique for cartoon hair animation; one that allows theanimators to create attractive and controllable hair animations without having to draweverything by hand except a sparse set of key frames. We demonstrate how to give a celanimation character accentuated hair motion. The novelty of this approach is that we neithersimply interpolate the key frames nor generate the movement of the hair only using physicalsimulations. From a small number of rough sketches we prepare key frames that are used asindicators of hair motion. The hair movements are created based on a hair motion databasebuilt from physical simulations custom-designed by the animator. Hair animations withconstraints from the key frames can be generated in two stages: a matching process tosearch for the desired motion sequences from the database and then smoothly connect …,*,2005,15
Facial aging simulator considering geometry and patch-tiled texture,Yusuke Tazoe; Hiroaki Gohara; Akinobu Maejima; Shigeo Morishima,Abstract People can estimate an approximate age of others by looking at their faces. This isbecause faces have certain elements by which people can judge a person's age. Ifcomputers can extract and manipulate such information; wide variety of applications forentertainment and security purpose would be expected.,ACM SIGGRAPH 2012 Posters,2012,14
Emotion model: A criterion for recognition; synthesis and compression of face and emotion,S Morishima,*,Proc. Automatic Face and Gesture Recognition,1995,14
Model-based facial image coding controlled by the speech parameter,S Morishima; K Aizawa; H Harashima,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: handwriting-recognition; Search; Register; Log in …,*,1988,14
Optimization of cloth simulation parameters by considering static and dynamic features,Shoji Kunitomo; Shinsuke Nakamura; Shigeo Morishima,Abstract Realistic drape and motion of virtual clothing is now possible by using an up-to-datecloth simulator; but it is even difficult and time consuming to adjust and tune manyparameters to achieve an authentic looking of a real particular fabric. Bhat et al.[2003]proposed a way to estimate the parameters from the video data of real fabrics. However; thisprojects structured light patterns on the fabrics; so it might not be possible to estimate theaccurate value of the parameters if fabrics have colors and textures. In addition to thestructured light patterns; they use a motion capture system to track how the fabrics move. Inthis paper; we will introduce a new method using only a motion capture system by attachinga few markers on fabric surface without any other devices. Moreover; animators can easilyestimate the parameters of many kinds of fabrics with this method. Authentic looking and …,ACM SIGGRAPH 2010 Posters,2010,13
Perceptual similarity measurement of speech by combination of acoustic features,Yoshihiro Adachi; Shinichi Kawamoto; Shigeo Morishima; Satoshi Nakamura,Future cast system is a new entertainment system where participant's face is captured andrendered into the movie as an instant Computer Graphics (CG) movie star; which had beenfirst exhibited at the 2005 World Exposition in Aichi Japan. We are working to add newfunctionality which enables mapping not only faces but also speech individualities to thecast. Our approach is to find a speaker with the closest speech individuality and apply voiceconversion. This paper investigates acoustic features to estimate perceptual similarity ofspeech individuality. We propose a method linearly combined eight acoustic features relatedto the perception of speech individualities. The proposed method optimizes weights for theacoustic features considering perceptual similarities. We have evaluated performance of ourmethod with Spearman's rank correlation coefficients to perceptual similarities. As the …,Acoustics; Speech and Signal Processing; 2008. ICASSP 2008. IEEE International Conference on,2008,13
Emotion-based 3-d computer graphics emotion model forming system,*,A system for forming a 3D computer graphics expression model based on emotionaltransition; which is provided in a computer device comprising input means; storage means;control means; output means; and display means; comprising: storage means for storing thelast three layers of a five-layer neural network for expanding three-dimensional emotionparameters into n-dimensional expression synthesis parameters; three-dimensional emotionparameters in emotional space corresponding to basic emotions; and shape data thatserves as a source for the formation of a 3D computer graphics expression model forexpression synthesis; means for deriving emotion parameters in emotional spacecorresponding to specific emotions; and calculation means whereby; using data for the lastthree layers in a five-layer neural network having a three-unit middle layer; emotion …,*,2004,13
Model-based talking face synthesis for anthropomorphic spoken dialog agent system,Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,Abstract Towards natural human-machine communication; interface technologies by way ofspeech and image information have been intensively developed. An anthropomorphicdialog agent is an ideal system; which integrates spoken dialog and natural facialexpressions. This paper reports on our project aiming to create a general-purpose toolkit forbuilding an easily customizable anthropomorphic agent. There have been almost no toolsso far such as intuitive; easy to understand; fully interactive; and open source. Ouranthropomorphic agent is designed to fulfill these requirements. This toolkit consists fourmodules; multi modal dialog integration; speech recognition; speech synthesis; and faceimage synthesis. These modules are highly modularized and interlinked by a simplecommunication protocols. In this paper; we focus on the construction of an agent's face …,Proceedings of the eleventh ACM international conference on Multimedia,2003,13
3D face expression estimation and generation from 2D image based on a physically constraint model,Takahiro Ishikawa; Shigeo Morishima; Demetri Terzopoulos,Muscle based face image synthesis is one of the most realistic approaches to the realizationof a life-like agent in computers. A facial muscle model is composed of facial tissue elementsand simulated muscles. In this model; forces are calculated effecting a facial tissue elementby contraction of each muscle string; so the combination of each muscle contracting forcedecides a specific facial expression. This muscle parameter is determined on a trial anderror basis by comparing the sample photograph and a generated image using our Muscle-Editor to generate a specific face image. In this paper; we propose the strategy of automaticestimation of facial muscle parameters from 2D markers' movements located on a face usinga neural network. This corresponds to the non-realtime 3D facial motion capturing from 2Dcamera image under the physics based condition.,IEICE TRANSACTIONS on Information and Systems,2000,13
An evaluation of 3-d emotion space,Fumio Kawakami; Motohiro Okura; Hiroshi Yamada; Hiroshi Harashima; Shigeo Morishima,The goal of the research is to realize a very natural human-machine communication systemby giving a facial expression to a computer. The 3-D emotion space previously proposed;can represent both human and computer emotion conditions appearing on the facecompactly in 3-D space. Namely; this 3-D emotion space can also realize both mapping andinverse mapping from facial expression to this 3-D space. This paper is mainly about thesubjective evaluation using the synthesized facial expression from the 3-D emotion space.,Robot and Human Communication; 1995. RO-MAN'95 TOKYO; Proceedings.; 4th IEEE International Workshop on,1995,13
Facial expression synthesis based on natural voice for virtual face-to-face communication with machine,Shigeo Morishima; Hiroshi Harashima,Basic research to a virtual face-to-face communication environment between an operatorand a machine is presented. In this system; a human natural face appears on the display ofmachine and can talk to a operator with natural voice and natural face expressions. A faceexpression synthesis scheme driven by natural voice is presented. Voice includes not onlylinguistic information but also emotional features. An expression control scheme driven byboth features is proposed. A human head with a 3-D wire frame model is expressed. Thesurface model is generated by texture mapping with 2-D real image. All the motions andexpressions are synthesized and controlled automatically by the movement of some featurepoints on the model.,Virtual reality annual international symposium; 1993.; 1993 IEEE,1993,13
A proposal of a knowledge based isolated word recognition,Shigeo Morishima; Hiroshi Harashima; H Miyakawa,This paper describes a knowledge based isolated Japanese word recognition algorithm.The program is written with Prolog/KR [4] and has two basic inference processes; ie; aBottom-up search and a Top-down search. In the Bottom-up process; a segmentation and avowel decision are performed and some target word patterns are generated. The Top-downprocess includes a consonant decision using a score of each candidate word calculatedbased on the Fuzzy Set Theory [1]. In the vowel inference; a template matching is appliedmainly. In the segmentation; heuristic rules based on the spectrum transition and the waveform are used. But in the consonant inference; each rule has a hierarchy structure and it isdefined automatically in the form of the multi-valued threshold function from learning data.This system can treat an obscure information about the consonant classification and to …,Acoustics; Speech; and Signal Processing; IEEE International Conference on ICASSP'86.,1986,13
LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics,Shigeo Morishima Shoto Sasaki; Kazuyoshi Yoshii; Tomoyasu Nakano; MAsataka Goto,ABSTRACT This paper presents a lyrics retrieval system called LyricsRadar that enablesusers to interactively browse song lyrics by visualizing their topics. Since conventional lyricsretrieval systems are based on simple word search; those systems often fail to reflect user'sintention behind a query when a word given as a query can be used in different contexts. Forexample; the word “tears” can appear not only in sad songs (eg; feel heartrending); but alsoin happy songs (eg; weep for joy). To overcome this limitation; we propose to automaticallyanalyze and visualize topics of lyrics by using a well-known text analysis method calledlatent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topicvisualization. One is the topic radar chart that visualizes the relative weights of five latenttopics of each song on a pentagon-shaped chart. The other is radar-like arrangement of …,ISMIR 2014,2014,12
Activities of interactive speech technology consortium (ISTC) targeting open software development for MMI systems,T Nitta; S Sagayama; Y Yamashita; T Kawahara; S Morishima; S Nakamura; A Yamada; K Ito; M Kai; A Li; M Mimura; K Hirose; T Kobayashi; K Tokuda; N Minematsu; Y Den; T Utsuro; T Yotsukura; H Shimodaira; M Araki; T Nishimoto; N Kawaguchi; H Banno; K Katsurada,Interactive Speech Technology Consortium (ISTC); established on November 2003 afterthree years activity of the Galatea project supported by Information-technology PromotionAgency (IPA) of Japan; aims at supporting open-source free software development of multi-modal interaction (MMI) for human-like agents. The software named Galatea-toolkitdeveloped by 24 researchers of 16 research institutes in Japan includes a Japanese speechrecognition engine; a Japanese speech synthesis engine; and a facial image synthesisengine used for developing an anthropomorphic agent; as well as dialogue manager thatcan integrates multiple modalities; interprets them; and decides an action with differentiatingit to multiple media of voice and facial expression. ISTC provides members a one-daytechnical seminar and one-week training course to master Galatea-toolkit; as well as a …,Robot and Human Interactive Communication; 2004. ROMAN 2004. 13th IEEE International Workshop on,2004,12
Face-to-face communicative avatar driven by voice,Shigeo Morishima; Tatsuo Yotsukura,Recently computer can make cyberspace to walk through by an interactive virtual realitytechnique. An avatar in cyberspace can bring us a virtual face-to-face communicationenvironment. In this paper we realize an avatar which has a real face in cyberspace toconstruct a multi-user communication system by voice transmission through network. Voicefrom microphone is transmitted and analyzed; then mouth shape and facial expression ofavatar are synchronously estimated and synthesized on real time. And also we introduce anentertainment application of a real-time voice driven synthetic face. This project is named"Fifteen Seconds of Fame" which is an example of interactive movie.,Image Processing; 1999. ICIP 99. Proceedings. 1999 International Conference on,1999,12
知的インタフェースのための顔の表情合成法の一検討,森島繁生， 岡田信一， 原島博,知的マン・マシンインタフェースのための画像合成法について検討を行った. 知的マン・マシンインタフェースとは; 現在の文字情報を中心としたユーザインタフェースに代わって;画面に人物の顔画像が現れてこちらの問いに答えてくれる; 音声と画像を中心としたよりユーザフレンドリーなマン・マシンインタフェースのことである. 従って; 画面上に現れるのは人間の顔であり;話し方や表情などが極めて自然であれば; あたかも実際の人間と話しているような自然な機械との対話が可能である. この知的マン・マシンインタフェースの実現には; 顔の表情の自然な画像合成が不可欠であると考えられる. 本論文では; 与えられたテキスト情報; および感情情報に基づいて顔画像の表情合成を行う方法について検討を行った. この方法は分析合成画像符号化の手法を応用しており; 対象人物に整合させた 3 次元モデルに輝度値データを投影して顔画像を表現し;入力された音韻や感情に基づいて; あらかじめ定めてある変形ルールに従って 3 次元モデルを …,電子情報通信学会論文誌 D,1990,12
Learning arm motion strategies for balance recovery of humanoid robots,Masaki Nakada; Brian Allen; Shigeo Morishima; Demetri Terzopoulos,Humans are able to robustly maintain balance in the presence of disturbances by combininga variety of control strategies using posture adjustments and limb motions. Such responsescan be applied to balance control in two-armed bipedal robots. We present an upper-bodycontrol strategy for improving balance in a humanoid robot. Our method improves on lower-body balance techniques by introducing an arm rotation strategy (ARS). The ARS uses Q-learning to map sensed state to the appropriate arm control torques. We demonstratesuccessful balance in a physically-simulated humanoid robot; in response to perturbationsthat overwhelm lower-body balance strategies alone.,Emerging Security Technologies (EST); 2010 International Conference on,2010,11
Dive into the Movie,Shigeo Morishima,Summary:" Dive into the Movie (DIM)" is a name of project to aim to realize a worldinnovative entertainment system which can provide an immersion experience into the storyby giving a chance to audience to share an impression with his family or friends by watchinga movie in which all audience can participate in the story as movie casts. To realize thissystem; several techniques to model and capture the personal characteristics instantly inface; body; gesture; hair and voice by combining computer graphics; computer vision andspeech signal processing technique. Anyway; all of the modeling; casting; charactersynthesis; rendering and compositing processes have to be performed on real-time withoutany operator. In this paper; first a novel entertainment system; Future Cast System (FCS); isintroduced which can create DIM movie with audience's participation by replacing the …,IEICE TRANSACTIONS on Information and Systems,2008,11
Data-driven efficient production of cartoon character animation,Shigeo Morishima; Shigeru Kuriyama; Shinichi Kawamoto; Tadamichi Suzuki; Masaaki Taira; Tatsuo Yotsukura; Satoshi Nakamura,Abstract In this paper; we outline two new 3DCG technologies; MoCaToon and AniFace;which have been developed to improve the efficiency of the movie production process in thelabor-intensive world of Japanese Anime; where much of the work is still done by hand.MoCaToon is a technology which enables motion capture systems to be used in the animeproduction process; and AniFace enables anime characters' lips to be synchronized with pre-scored voices. To verify the feasibility of these technologies; we reproduced the popularJapanese TV anime series; The Galaxy Railways; with 3D character animation utilizing bothMoCaToon and AniFace. As a result of this experiment; it seems that the two technologiescan be effectively used by animators to customize 3DCG characters according to theirspecific requirements; especially in terms of character style and motion.,ACM SIGGRAPH 2007 sketches,2007,11
Audio-visual speech translation with automatic lip syncqronization and face tracking based on 3-d head model,Shigeo Morishima; Shin Ogata; Kazumasa Murai; Satoshi Nakamura,Speech-to-speech translation has been studied to realize natural human communicationbeyond language barriers. Toward further multi-modal natural communication; visualinformation such as face and lip movements will be necessary. In this paper; we introduce amulti-modal English-to-Japanese and Japanese-to-English translation system that alsotranslates the speaker's speech motion while synchronizing it to the translated speech. Toretain the speaker's facial expression; we substitute only the speech organ's image with thesynthesized one; which is made by a three-dimensional wire-frame model that is adaptableto any speaker. Our approach enables image synthesis and translation with an extremelysmall database. We conduct subjective evaluation by connected digit discrimination usingdata with and without audiovisual lip-synchronicity. The results confirm the sufficient …,Acoustics; Speech; and Signal Processing (ICASSP); 2002 IEEE International Conference on,2002,11
Representation of Feel and Motion of the Thread-like Objects,S Kobayashi; S Morishima; H Harashima,*,Proc. of NICOGRAPH90,1990,11
The effects of virtual characters on audiences’ movie experience,Tao Lin; Shigeo Morishima; Akinobu Maejima; Ningjiu Tang,Abstract In this paper; we first present a new audience-participating movie form in which 3Dvirtual characters of audiences are constructed by computer graphics (CG) technologies andare embedded into a in a pre-rendered movie as different roles. Then; we investigate howthe audiences respond to these virtual characters using physiological and subjectiveevaluation methods. To facilitate the investigation; we present three versions of a movie toan audience—a Traditional version; its SDIM version with the participation of the audience'svirtual character; and its SFDIM version with the co-participation of the audience and her/hisfriends' virtual characters. The subjective evaluation results show that the participation ofvirtual characters indeed causes increased subjective sense of spatial presence andengagement; and emotional reaction; moreover; SFDIM performs significantly better than …,Interacting with Computers,2009,10
Using subjective and physiological measures to evaluate audience-participating movie experience,Tao Lin; Akinobu Maejima; Shigeo Morishima,Abstract In this paper we subjectively and physiologically investigate the effects of theaudiences' 3D virtual actor in a movie on their movie experience; using the audience-participating movie DIM as the object of study. In DIM; the photo-realistic 3D virtual actors ofaudience are constructed by combining current computer graphics (CG) technologies andcan act different roles in a pre-rendered CG movie. To facilitate the investigation; wepresented three versions of a CG movie to an audience---a Traditional version; its Self-DIM(SDIM) version with the participation of the audience's virtual actor; and its Self-Friend-DIM(SFDIM) version with the co-participation of the audience and his friends' virtual actors. Theresults show that the participation of audience's 3D virtual actors indeed cause increasedsubjective sense of presence and engagement; and emotional reaction; moreover …,Proceedings of the working conference on Advanced visual interfaces,2008,10
ビデオ翻訳システム-自動翻訳合成音声とのモデルベースリップシンクの実現,緒方信， 中村哲， 森島繁生,Abstract In this paper; we introduce the multi-modal English to Japanese and Japanese toEnglish translation system; which translates the speaking motion synchronized to thetranslated speech. To retain the speaker's facial expression; we substitute only speechorgan's image with the synthesized one; which is made by a three-dimensional wire framemodel that is adaptable to any speaker. Our approach enables the image synthesis andtranslation with extremely small database.,情報処理学会インタラクション論文集,2001,10
Facial animation synthesis for human-machine communication system,Shigeo Morishima; Hiroshi Harashima,*,ADVANCES IN HUMAN FACTORS ERGONOMICS,1993,10
A facial image synthesis system for human-machine interface,S Morishima; Tatsumi Sakaguchi; Hiroshi Harashima,The authors introduce a'face'interface which is a user-friendly human-machine interface withmulti-media and can realize a face-to-face communication environment between theoperator and the machine. In this system; a human face appears on the display of themachine and it can talk to the operator with a natural voice. This paper describes the facemotion and expression synthesis schemes which can be applied to this' face'interface. Theauthors express a human head with a 3D model. The surface model is built by texturemapping with a 2D real image. All the motions and expressions are synthesized andcontrolled automatically by the movement of some feature points on the model. This'face'interface is one of the applications of the model based image coding and mediaconversion schemes.,Robot and Human Communication; 1992. Proceedings.; IEEE International Workshop on,1992,10
擬人化音声対話エージェント基本ソフトウェアの開発プロジェクト報告,嵯峨山茂樹， 伊藤克亘， 宇津呂武仁， 甲斐充彦， 小林隆夫， 下平博， 伝康晴， 徳田恵一， 中村哲， 西本卓也， 新田恒雄， 広瀬啓吉， 峯松信明， 森島繁生， 山下洋一， 山田篤， 李晃伸,Abstract This paper describes the outline of “Galatea;” a software toolkit of anthropomorphicspoken dialog agent developed by the authors. Major functions such as speech recognition;speech synthesis and face animation generation are integrated and controlled under adialog control. To emphasize custonizability as the dialog research platform; this systenfeatures easily replaceable face; speaker-adaptive speech synthesis; easily modification ofdialog control script; exchangeable function modules; and multi-processor capability. Thistoolkit is ready for download with an open-source and license-free policy.,情報処理学会研究報告音声言語情報処理 (SLP),2003,9
Model-Based Lip Synchronization with Automatic Translated Synthetic Voice Toward Multi-modal Translation System,Shin Ogata; Kazumasa Murai; Satoshi Nakamura; Shigeo Morishima,ABSTRACT In this paper; we introduce a multi-modal English-to-Japanese and Japanese-to-English translation system that also translates the speaker's speech motion whilesynchronizing it to the translated speech. To retain the speaker's facial expression; wesubstitute only the speech organ's image with the synthesized one; which is made by a three-dimensional wire-frame model that is adaptable to any speaker. Our approach enablesimage synthesis and translation with an extremely small database.,*,2001,9
Automatic face tracking and model match-move in video sequence using 3D face model,Takafumi Misawa,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,International Conference on Multi-media Exp. ICME2001,2001,9
実写画像をベースとしたマルチメディア・アンビアンスコミュニケーションの提案,市川忠嗣， 吉村哲也， 山田邦男， 金丸利文， 須賀弘道， 岩澤昭一郎， 苗村健， 相澤清晴， 森島繁生， 齊藤隆弘,実写画像 をベ ース と したマルチ メデ ィア ・ア ン ビア ンス … Multimedia Ambiance Communicationbased on Video and Image … 正会員 市 川 忠 嗣 †1; 吉 村 哲 也 †1; 山 田 邦 男 †1; 正会員 金丸 利 文 †1 … 正会員 須 賀 弘 道 †1; 岩 澤 昭 一 郎 †1; 正会員 苗 村 健 †1;†2; 正会員 相 澤 清晴 †1;†2 … 正会員 森 島 繁 生 †1;†3; 正会員 齊 藤 隆 弘 †1;†4 … Tadashi Ichikawa†1; TetsuyaYoshimura†1; Kunio Yamada†1; Toshifumi Kanamaru†1 … Hiromichi Suga†1; ShoichiroIwasawa†1; Takeshi Naemura†1;†2; Kiyoharu Aizawa†1;†2; Sigeo Morishima†1;†3 and TakahiroSaito†1;†4 … あ らま し ビデオ カメラや ディジタル カメラで撮影 された実写画像 を用 い;遠 …景;中 景;近 景の レイヤ構 造で … 表現 し;再 構成 す るフォ トリア リステ ィックな3次 元画像 空間 を提案す る と共 に;こ の画像空 間 による コ ミュニ … ケー シ ョンを提案 し;そ の実現方法 を報告す る …キーワ-ド: 空間共有通信;3次 元画像空間;実 写画像;イ メージベーストレンダリング;遠 景 ・中景 …,映像情報メディア学会誌,2000,9
3D lip expression generation by using new lip parameters,K Ito; T Misawa; J Muto; S Morishima,*,IEICE Technical Report; A-16-24,2000,9
Multiple points face-to-face communication in cyberspace using multi-model agent,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,HCI Communication; Cooperation; and Application Design,1999,9
Construction and evaluation of 3-D emotion space based on facial image analysis,Tatsumi Sakaguchi; Hiroshi Yamada; Shigeo Morishima,*,IEICE Transactions,1997,9
Construction of 3-D emotion space based on parameterized faces,Fumio Kawakami; Shigeo Morishima; F Yamada; R Harashima,If a machine can treat'kansei'information like emotion; the relation between human andmachine would become more friendly. Our goal is to realize a natural human-machinecommunication environment by giving a face to the computer terminal or communicationsystem. To realize this environment; it is necessary for the machine to recognize human'semotion condition appearing in the face; and synthesize the reasonable facial image againstit. For this purpose; the machine should have emotion model based on parameterized faceswhich can map his or her emotion condition one by one to this space and can also mapinversely in reply.,Robot and Human Communication; 1994. RO-MAN'94 Nagoya; Proceedings.; 3rd IEEE International Workshop on,1994,9
音声に含まれる感情情報抽出の一検討,平賀裕， 斎藤， 善行， 森島， 繁生， 原島博,抄録 音声に含まれる基本的感情を分析するため; 演劇経験者に感情を込めて単語音声・短文音声を発声してもらい; それぞれに関して分析を試みた. 本研究では扱う感情を 「怒り」「喜び」「悲しみ」「嫌悪」 の 4 種とし;「平静」 音声と比較を基に今まであまり行なわれていなかったピッチ周波数・振幅の変化パターンの検討を中心に分析を行った. またより豊かな感情分析のために FMラジオから感情音声を採取し; 主観評価した後同様の検討を加えた. その結果;矛盾点も皆無というわけではなかったが; 相互に多大なる共通項を見いだすことが出来た.,信学技報,1994,9
A facial motion synthesis for intelligent man‐machine interface,Shigeo Morishima; Shin'Ichi Okada; Hiroshi Harashima,Abstract A facial motion image synthesis method for intelligent man-machine interface isexamined. Here; the intelligent man-machine interface is a kind of friendly man-machineinterface with voices and pictures in which human faces appear on a screen and answerquestions; compared to the currently existing user interfaces which primarily uses letters.Thus what appears on the screen is human faces; and if speech mannerisms and facialexpressions are natural; then the interactions with the machine are similar to those withactual human beings.,Systems and Computers in Japan,1991,9
Hair motion cloning from cartoon animation sequences,Eiji Sugisaki; Yosuke Kazama; Shigeo Morishima; Natsuko Tanaka; Akiko Sato,Abstract This paper describes a new approach to create cartoon hair animation that allowsusers to use existing cel character animation sequences. We demonstrate the generation ofcartoon hair animation accentuated in 'anime-like'motions. The novelty of this method is thatusers can choose the existing cel animation of a character's hair animation and applyenvironmental elements such as wind to other characters with a three-dimensional structure.In fact; users can reuse existing cartoon sequences as input to endow another character withenvironmental elements as if both characters exist in the same scene. A three-dimensionalcharacter's hair motions are created based on hair motions from input cartoon animationsequences. First; users extract hair shapes at each frame from input sequences from whichthey then construct physical equations.'Anime-like'hair motion is created by using these …,Computer Animation and Virtual Worlds,2006,8
フユーチャーキャストシステム 『三井・東芝館』,森島繁生,映画の登場人物に扮 して宇宙を駆け巡る体験をしてみたい; あ るいは; 正 義の味方の役で物語の主人公を演 じてみたい; そ ういう希望を描いた経験が誰しも一度や二度はあると思う. こ こで述べる『フユーチャーキャス トシステム』 とは; そ のような希望を容易に叶えて くれるまったく新しいエンタテイメン トシステムである. 三井・東芝館では; こ の 『フューチ ャーキャス トシステム』を世界で初めて実現 し; シ アターが収容できる定員である 240 名 の来場者全員が; 映像の中に出演できるア トラクシ ョンとして具現化 している. 本 稿では; こ の 『フューチャーキャストシステム』 の概要について述べる.,映像情報メディア学会誌,2005,8
擬人化音声対話エージェントツールキットの基本設計,川本真一， 下平博， 新田恒雄， 西本卓也， 中村哲， 伊藤克亘， 森島繁生， 四倉達夫， 甲斐充彦， 李晃伸， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 峯松信明， 山田篤， 伝康晴， 宇津呂武仁， 嵯峨山茂樹,あらまし 筆者らは; 顔画像が容易に交換可能で; 音声合成が話者適応可能で;対話制御の記述変更が容易で; 更にこれらの機能モジュール自体を別のモジュールに差し替えることが容易であり; かつ処理ハードウェアの個数に柔軟に対処できるなどの特徴を持つ擬人化音声対話エージェントシステムを構想し; 実装した. 各モジュールのインタフェースを統一化して扱い;モジュール間の入出力は; UNIX システムで使われている標準入出力を用いる簡便な方法にてモジュール統合機構を実現した. いくつかの簡単な対話タスクについてエージェントを試作し;必要な機能に関する達成度を確認した. また; 顔画像合成モジュールを制御する新たなモジュールの追加を容易に実現することができた.,情報処理学会研究報告ヒューマンコンピュータインタラクション (HCI),2002,8
擬人化音声対話エージェントツールキットの基本設計,川本真一， 下平博， 新田恒雄， 西本卓也， 中村哲， 伊藤克亘， 森島繁生， 四倉達夫， 甲斐充彦， 李晃伸， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 峯松信明， 山田篤， 伝康晴， 宇津呂武仁， 嵯峨山茂樹,あらまし 筆者らは; 顔画像が容易に交換可能で; 音声合成が話者適応可能で;対話制御の記述変更が容易で; 更にこれらの機能モジュール自体を別のモジュールに差し替えることが容易であり; かつ処理ハードウェアの個数に柔軟に対処できるなどの特徴を持つ擬人化音声対話エージェントシステムを構想し; 実装した. 各モジュールのインタフェースを統一化して扱い;モジュール間の入出力は; UNIX システムで使われている標準入出力を用いる簡便な方法にてモジュール統合機構を実現した. いくつかの簡単な対話タスクについてエージェントを試作し;必要な機能に関する達成度を確認した. また; 顔画像合成モジュールを制御する新たなモジュールの追加を容易に実現することができた.,情報処理学会研究報告ヒューマンコンピュータインタラクション (HCI),2002,8
3D head model generation using multi-angle images and facial expression generation,K Ito; T Misawa; J Muto; S Morishima,*,IEICE Technical Report,2000,8
Multi-media ambiance communication based on actual moving pictures,Tadashi Ichikawa; Tetsuya Yoshimura; Kunio Yamada; Toshifumi Kanamaru; Hiromichi Suga; Shoichiro Iwasawa; Takeshi Naemura; Kiyoharu Aizawa; Shigeo Morishima; Takahiro Saito,Multi-media ambiance communication refers to a means of shared-space communication;that makes use of actual moving pictures captured by video camera; combined with the lawsof perspective; as used in painting and the visual characteristics of human beings; toestablish a photo-realistic quality three-dimensional image space that users can naturallyfeel part of. We aim to enable this ambiance communication by basing the shared-space onactual moving pictures rather than on an accurate three-dimensional image space; such asis used in computer graphics.,Image Processing; 1999. ICIP 99. Proceedings. 1999 International Conference on,1999,8
Life-Like; Believable Communication Agents,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Sigraph 96,1996,8
Analysis and synthesis of facial expression using high-definition wire frame model,Tatsumi Sakaguchi; Masatoshi Ueno; S Morishima; Hiroshi Harashima,We propose the method to extract facial expression parameter with quantitative analysis of 9-dimensional movement of facial surface that arises from real human expression; using high-definition model. By dividing the face surface to some regions having similar characteristicsof movement and modeling with 3-dimensional round surface in each region respectively;we can derive the muscle control parameters and the rule of movement. Considering thefindings of this analysis we propose also a synthesis method of real facial expression imagebased on this high-definition model.,Robot and Human Communication; 1993. Proceedings.; 2nd IEEE International Workshop on,1993,8
A model based shade estimation and reproduction schemes for rotational face,E Ono,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,PCS'93,1993,8
Multi‐layer Lattice Model for Real‐Time Dynamic Character Deformation,Naoya Iwamoto; Hubert PH Shum; Longzhi Yang; Shigeo Morishima,Abstract Due to the recent advancement of computer graphics hardware and softwarealgorithms; deformable characters have become more and more popular in real-timeapplications such as computer games. While there are mature techniques to generateprimary deformation from skeletal movement; simulating realistic and stable secondarydeformation such as jiggling of fats remains challenging. On one hand; traditional volumetricapproaches such as the finite element method require higher computational cost and areinfeasible for limited hardware such as game consoles. On the other hand; while shapematching based simulations can produce plausible deformation in real-time; they suffer froma stiffness problem in which particles either show unrealistic deformation due to high gains;or cannot catch up with the body movement. In this paper; we propose a unified multi …,Computer Graphics Forum,2015,7
Fast plausible 3d face generation from a single photograph,Akinobu Maejima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,ACM SIGGRAPH ASIA 2008,2008,7
動画の 3 次元周波数成分を用いた顔認証システム,山名信弘， 井辺昭人， 三浦文裕， 前島謙宣， 森島繁生,抄録 本研究は; 表情変化する顔を撮影した動画像を 3 次元フーリエ変換して得た 3次元周波数成分を特徴量とする顔認証システムを提案する. 表情変化する顔を用いることが;静止画を用いた顔認証で懸念されているなりすましに対して防止効果があることを示す.本システムの特徴として; 特徴量に周波数成分を用いることで顔領域を抽出することなく認証を行う.用いる周波数項は分散分析によって選定する. また; 認証処理は watch list 形式であり;重判別分析とマハラノビス汎距離測定の 2 つの手法を直列的に組み合わせて行う.,電子情報通信学会技術研究報告. MI; 医用画像,2006,7
モーションキャプチャによる顔表情の定量表現 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),柳澤博昭， 祖川慎治， 前島謙宣， 四倉達夫， 森島繁生,抄録 本稿では; モーションキャプチャを用いて計測された顔表情データからの顔表情合成手法;および顔表情データを主成分分析することで; 顔表情データから新たに互いに相関のない特徴量を導出し; その特徴量がどのような表情を示すかのに対する定義づけを行った. 顔表情のデータは;FACS に基づいた人間の表情の基本単位; およびその基本単位を組み合わせた表情の計 64種類を撮影することで得られる. 撮影には光学式モーションキャプチャシステムを用い;被験者の顔表面に 146 点のマーカを配置し; 表情変化時の詳細な 3 次元移動量を取得した.取得した顔表面の 3 次元移動量を; 被験者の正面画像に対して整合された 3 次元顔モデルに適用することで; 表情を合成することが可能となる. さらに; 各表情の 3 次元移動量に対して段階的に主成分分析を行い; 各表情の 3 次元移動量を直交化することで; 他の表情と相関の無い表情変化のパラメータを提案する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2005,7
レンジファインダを用いた表情変形ルールと表情編集ツールの構築,大橋俊介， 杉崎英嗣， 伊藤圭， 森島繁生,抄録 近年; アバタを介したコミュニケーションシステムやヒューマンインタフェースにおいて; CGによる表情合成の研究が注目を集めている. コンピュータと人間とのコミュニケーションを円滑に行うには; 人間同士がフェーストゥーフェースで対話しているような環境が重要であり; 表情を含めたノンバーバルな情報の表現が課題となる. 本稿では; 従来から行っている Facial Action CodingSystem (FACS) に基づく表情合成規則を; レンジファインダを用いて高精度に再現することを目標とし; 実際; 個々のアクションユニットに関して; 表情変化を実測して; 高精細な 3次元顔変形ルールを定めた. またこの表情変化ルールに基づき; 任意の表情変化を記述する表情編集ツールを実現した.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2002,7
高速度カメラで捉えた自発表情と演技表情の動的変化,山田寛， 内田英子， 四倉達夫， 森島繁生， 鉄谷信二， 赤松茂,抄録 本研究では; 人間が自然な表情を自発した時と普遍的で典型的と言われている表情を演じる時の顔の動きを高速度カメラで撮影し; 顔の特徴点の変位の測定に基づいて顔の動きの定量的な特性を分析した. 自然な表情は; Gross & Levens (1995) が標準化した情動喚起刺激を被験者に提示することによって自発させた. 典型的な表情の演技は; FACS の定義に基づいた. 自発表出条件;演技表出条件ともに顔の各部位の動き出しの差は微細であり高速度カメラを用いたことの有効性が示された. また情動ごとおよび表出条件ごとに顔の各部の動きの量や速さに特徴的な違いが認められたが; 動きの変化そのものの様相には興味深い共通性が認められた.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2001,7
高速度カメラを用いた顔面表情の動的変化に関する分析,内田英子， 四倉達夫， 森島繁生， 山田寛， 大谷淳， 赤松茂,抄録 顔面表情に焦点をあて; 意図的なコントロールを受けたものと; なんらかの情動喚起に伴い自発的に現れるものとの違い; 特に動的な変化の違いを実験的に検討した. 被験者の顔面表情の変化を次の 2 条件下で高速度カメラにより撮影した. 1 つが意図的表出 (動作教示) 条件;もう一つが自発的表出条件である. 意図的表出条件では; 顔面動作教示に従って被験者に 6つの基本表情を演じさせた. 一方; 自発的表出条件では; 情動喚起映像 (喜び; 驚き; 怒り; 悲しみ;嫌悪; 恐れ) を提示し; 被験者に自然な表情を自発させた. 高速度カメラで撮影した顔面表情の動的変化 (特徴点の変位) を; 画像解析ツールを用いて測定した.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2000,7
複数アングル画像からの 3 次元頭部モデルの作成と表情合成,伊藤圭， 三澤貴文， 武藤淳一， 森島繁生,抄録 サイバースペース上でのコミュニケーションを目的とし; 特定人物のアバタを作成する方法として; レンジスキャナーのような特別な機材を用いることなく; 複数のアングルから撮影した頭部の画像によって; 3 次元頭部モデルを簡単に作成する手法を提案した. これは; 任意のアングルから撮影した頭部画像にワイヤフレームを独自に開発した GUI を用いてフィッティングさせ; 特に正面と側面から撮影された画像のブレンディングによって頭部のテクスチャを構成するものである.さらにこのモデルに基づいて; 発話時の口形状を定義するパラメータを見なおし;任意の方向から見ても; 違和感のない口形状の表現が可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2000,7
Denger hamster 2000,Kim Binsted,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,SIGGRAPH Sketches and Application,2000,7
サイバースペース上の仮想人物による実時間対話システムの構築,四倉達夫， 藤井英史， 森島繁生,コンピュータの処理能力の急速な発展により; 複数のユーザがネットワークを介してサイバースペースを共有し; 対話や協調作業を行うインタラクション環境が整ってきている. この仮想空間への没入感覚と臨場感を向上させるためには; 実空間と同等の自然さで; 人間同士のコミュニケーションを実現する必要がある. そこで本稿では; 自分自身の姿を投影した顔を持つアバタ (Avatar)を仮想空間上に生成し; マイクから入力された自然音声に同期させて会話時の口形状の推定をリアルタイムに実施し; 同時にキー入力された感情情報によってアバタの表情合成を行うシステムを提案する. このシステムによりサイバースペース上で多人数が参加可能なフェイストウフェイスの対話環境が実現可能となった.,情報処理学会論文誌,1999,7
Virtual face-to-face communication driven by voice through network,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proceedings of Workshop on Perceptual User Interfaces (PUI'97); Oct.,1997,7
Image synthesis and editing system for a multi-media human interface with speaking head,S Morishima; H Harashima,The authors' goal is to realize a very natural human-machine communication environmentby giving a face to computer terminal or communication system. The paper presents aprototype multi-media man-machine interface with a speaking head on a workstation. Theface on the display; speaks the system messages and mail text to the user. They propose aprototype computer interface; and synchronization method between image and speech; anda scenario tool to express delicate face expressions and animation scenes.,Image Processing and its Applications; 1992.; International Conference on,1992,7
Data-driven speech animation synthesis focusing on realistic inside of the mouth,Masahide Kawai; Tomoyori Iwao; Daisuke Mima; Akinobu Maejima; Shigeo Morishima,Abstract: Speech animation synthesis is still a challenging topic in the field of computergraphics. Despite many challenges; representing detailed appearance of inner mouth suchas nipping tongue's tip with teeth and tongue's back hasn't been achieved in the resultinganimation. To solve this problem; we propose a method of data-driven speech animationsynthesis especially when focusing on the inside of the mouth. First; we classify inner mouthinto teeth labeling opening distance of the teeth and a tongue according to phonemeinformation. We then insert them into existing speech animation based on opening distanceof the teeth and phoneme information. Finally; we apply patch-based texture synthesistechnique with a 2;213 images database created from 7 subjects to the resulting animation.By using the proposed method; we can automatically generate a speech animation with …,Journal of information processing,2014,6
Facial aging simulation by patch-based texture synthesis with statistical wrinkle aging pattern model,Akinobu Maejima; Ai Mizokawa; Daiki Kuwahara; Shigeo Morishima,Abstract We propose a method for synthesizing a photorealistic human aged-face imagebased on the patch-based texture synthesis using a set of human face images of a targetage. The advantage of our method is that it synthesizes an aged-face image with fine skintexture such as spots and pigments of facial skin; as well as age-related facial wrinkleswithout blurs (such as those resulting from lack of accurate pixel-wise alignments as in thelinear combination model) while maintaining the quality of the original image.,*,2014,6
Detection of driver's drowsy facial expression,Taro Nakamura; Akinobu Maejima; Shigeo Morishima,We propose a method for the estimation of the degree of a driver's drowsiness on basis ofchanges in facial expressions captured by an IR camera. Typically; drowsiness isaccompanied by falling of eyelids. Therefore; most of the related studies have focused ontracking eyelid movement by monitoring facial feature points. However; textural changes thatarise from frowning are also very important and sensitive features in the initial stage ofdrowsiness; and it is difficult to detect such changes solely using facial feature points. In thispaper; we propose a more precise drowsiness-degree estimation method consideringwrinkles change by calculating local edge intensity on faces that expresses drowsinessmore directly in the initial stage.,Pattern Recognition (ACPR); 2013 2nd IAPR Asian Conference on,2013,6
Development of an integrated multi-modal communication robotic face,Brennand Pierce; Takaaki Kuratate; Akinobu Maejima; Shigeo Morishima; Yosuke Matsusaka; Marko Durkovic; Klaus Diepold; Gordon Cheng,Development of an integrated multi-modal communication robotic face … BrennandPierce1; Takaaki Kuratate1; Akinobu Maejima2; Shigeo Morishima2; Yosuke Matsusaka3; MarkoDurkovic4; Klaus Diepold4 and Gordon Cheng1 … 1 Institute for Cognitive Systems(http://www.ics.ei.tum.de/); 4 Institute for Data Processing(http://www.ldv.ei.tum.de/); TechnischeUniversität München; 2 Waseda University; Japan;3 AIST; Japan … Calibrated talking headanimation Audio data *.jpg … Fig. 1. Over view of the original Mask-Bot system …Abstract—This paper presents an overview of the new version of our multi-model communicationface “Mask-Bot”; a rear-projected animated robotic head; including our display system; faceanimation; speech communication and sound localization … For robots to coexist and collaboratewith humans in every day situations they need to communicate and interact in a way that …,Advanced robotics and its social impacts (arso); 2012 ieee workshop on,2012,6
Curvature-dependent reflectance function for rendering translucent materials,Hiroyuki Kubo; Yoshinori Dobashi; Shigeo Morishima,Abstract Simulating sub-surface scattering is one of the most effective ways for realisticallysynthesizing translucent materials such as marble; milk and human skin. In previous work;the method developed by Jensen et al.[2002] significantly improved the speed of thesimulation. However; the process is still not fast enough to produce realtime rendering. Thus;we have developed a curvature-dependent reflectance function (CDRF) which mimics thepresence of a subsurface scattering effect.,ACM SIGGRAPH 2010 Talks,2010,6
Storage medium storing animation image generating program,*,An animation image generating program is provided which allows animation images to bereadily generated by CG without complicated setups; and more particularly the animationimage generating program is suited to generate a plurality of types of face animation imagesby CG. The animation image generating program includes the steps of controlling selectionof specific vertices of a standard model and a user model; providing control such that firsttarget vertices are associated with second target vertices where the first target vertices arethe selected specific vertices of the standard model and the second target vertices are theselected specific vertices of the user model; providing control by arithmetic means such thatcoordinates of the first target vertices approximate to those of the second target vertices; togenerate fitting information; and animating the user model based on animation data of the …,*,2009,6
An empirical study of bringing audience into the movie,Tao Lin; Akinobu Maejima; Shigeo Morishima,Abstract In this paper we first present an audience-participating movie experience DIM; inwhich the photo-realistic 3D virtual actor of audience is constructed by computer graphictechnologies; and then evaluate the effects of DIM on audience experience usingphysiological and subjective methods. The empirical results suggest that the participation ofvirtual actors causes increased subjective sense of presence and engagement; and moreintensive emotional responses as compared to traditional movie form; interestingly; therealso significantly different physiological responses caused by the participation of virtualactors; objectively indicating the improvement of interaction between audience and movie.,International Symposium on Smart Graphics,2008,6
Tweakable shadows for cartoon animation,Hidehito Nakajima; Eiji Sugisaki; Shigeo Morishima,The role of shadow is important in cartoon animation. Shadows in hand-drawn animationreflect the expression of the animators' style; rather than mere physical phenomena.However shadows in 3DCG cannot express such an animators' touch. In this paper wepropose a novel method for editing the shadow with both advantages of hand drawnanimation and 3DCG technology. In particular; we discuss two phases that enable animatorsto transform and deform the shadow tweakably. The first phase is that a shadow projectionmatrix is applied to deform the shape of the shadow. The second one is that we manipulatevectors to transform the shadow such as scaling and translation. These vectors are used inshadow volume method. The shadows are edited directably by integration of these twophases. Our approach enables animators to edit the shadow by simple mouse operations …,*,2007,6
Future cast system,Shigeo Morishima; Akinobu Maejima; Shuhei Wemler; Tamotsu Machida; Masao Takebayashi,Most likely; everyone has gone through dreaming of acting a hero in the movie. The systemcalled “Future Case System”(FCS) is a new entertainment system that is easily able toaccomplish that dream. FCS has been implemented in MITSUI TOSHIBA pavilion at Expo2005 Aichi Japan first in the world. This attraction is to allow 240 theater attendances to havea role of the movie they are watching. Comparing FCS with Previous Attraction Systems(PASs); PASs provide the visual information to visitors one-sidedly. On the other hand; sinceattendances' faces come out on the screen; they can feel as if they are acting in the movie.This is the main idea of FCS and makes attendances' feeling of immersion increase.,ACM SIGGRAPH 2005 Sketches,2005,6
Dynamic micro aspects of facial movements in elicited and posed expressions using high-speed camera,S Morishima; T Yotsukura; H Yamada; H Uchida; N Tetsutani; S Akamatsu,The presented study investigated the dynamic aspects of facial movements inspontaneously elicited and posed facial expressions of emotion. We recorded participants'facial movements when they were shown a set of emotional eliciting films; and when theyposed typical facial expressions. Those facial movements were recorded by a high-speedcamera of 250 frames per second. We measured facial movements frame by frame in termsof displacements of facial feature points. Such micro-temporal analysis showed that;although it was very subtle; there exits the characteristic onset asynchrony of each part'smovement. Furthermore; it was found the commonality of each part's movement in temporalchange although the speed and the amount of each movement varied along withexpressional conditions and emotions.,Robot and Human Interactive Communication; 2001. Proceedings. 10th IEEE International Workshop on,2001,6
頭髪表現のスタイリングとアニメーション表現,岸啓補， 森島繁生,サイバースペースにおける仮想人物の合成やコミュニケーションシステムの実現に向け;コンピュータグラフィックスによる人物画像合成等が注目を集めている. 本論文では; 特に人物の CGの中でも合成が難しいとされる頭髪の表現方法について述べる. 人物画像において頭髪は視覚的に重要であるにもかかわらず; 簡単な曲面や背景の一部で代用されることが多い.頭髪をマッピング技術を用いて表現する手法が成果をあげているが運動の表現には不適当である.そこで頭髪の表現にテクスチャやポリゴンを用いずに空間曲線を用いる. 更に頭髪の部分的な集まりである房単位にモデル化することでヘアスタイルデザインを簡略化する. 本論文では;この新しいヘアスタイルデザインシステム; 房のモデル化手法; レンダリング手法;四分岐法による衝突判定; 運動表現について述べる. また; 実際にこのヘアスタイルデザインシステムを用いて頭髪をデザインし; 風になびかせるアニメーションを実現した.,電子情報通信学会論文誌 D,2000,6
Generation of a life-like agent in cyberspace using media conversion,Tatsuo YOTSUKURA; Eishi FUJII; Tomonori KOBAYASHI; Shigeo MORISHIMA,*,IEICE Technical Report; MVE97-103,1998,6
Face Information Processing,Osamu Hasegawa; Shigeo Morishima; Masahide Kaneko,*,Trans. IEICE,1997,6
Real-time facial expression recognition based on the 2-dimensional DCT,Tatsumi Sakaguchi; Shigeo Morishima,*,Trans. IEICE (D-II),1997,6
Face feature extraction from spatial frequency for dynamic expression recognition,Tatsumi Sakaguchi; Shigeo Morishima,A new facial feature extraction technique for expression recognition is proposed. We employthe spatial frequency domain information to obtain robust performance to the random noiseon a image or the lighting conditions. It exhibited high ability sufficiently even if combinedwith a low-performance region tracking method. As an application of this technique; we haveconstructed a dynamic facial expression recognition system. We use hidden Markov modelsto utilize temporal changes in the facial expressions. The spatial frequency information andthe temporal information make better rates of facial expression recognition. In theexperiment; we established a correct response rate of approximately 84.1% of recognitionwith six categories.,Pattern Recognition; 1996.; Proceedings of the 13th International Conference on,1996,6
MusicMixer: computer-aided DJ system based on an automatic song mixing,Tatsunori Hirai; Hironori Doi; Shigeo Morishima,Abstract In this paper; we present MusicMixer; a computer-aided DJ system that helps DJs;specifically with song mixing. MusicMixer continuously mixes and plays songs using anautomatic music mixing method that employs audio similarity calculations. By calculatingsimilarities between song sections that can be naturally mixed; MusicMixer enablesseamless song transitions. Though song mixing is the most fundamental and importantfactor in DJ performance; it is difficult for untrained people to seamlessly connect songs.MusicMixer realizes automatic song mixing using an audio signal processing approach;therefore; users can perform DJ mixing simply by selecting a song from a list of songssuggested by the system; enabling effective DJ song mixing and lowering entry barriers forthe inexperienced. We also propose personalization for song suggestions using a …,Proceedings of the 12th International Conference on Advances in Computer Entertainment Technology,2015,5
FG2015 age progression evaluation,Andreas Lanitis; Nicolas Tsapatsoulis; Kleanthis Soteriou; Daiki Kuwahara; Shigeo Morishima,The topic of face-aging received increased attention by the computer vision communityduring the recent years. This interest is motivated by important real life applications whereaccurate age progression algorithms can be used. However; age progressionmethodologies may only be used in real applications provided that they have the ability toproduce accurate age progressed images. Therefore it is of utmost importance to encouragethe development of accurate age progression algorithms through the formulation ofperformance evaluation protocols that can be used for obtaining accurate performanceevaluation results for different algorithms reported in the literature. In this paper we describethe organization of the; first ever; pilot independent age progression competition that aims toprovide the basis of a robust framework for assessing age progression methodologies …,Automatic Face and Gesture Recognition (FG); 2015 11th IEEE International Conference and Workshops on,2015,5
Automatic Depiction of Onomatopoeia in Animation Considering Physical Phenomena,Shigeo Morishima Tsukasa Fukusato,*,MIG 2014,2014,5
Efficient video viewing system for racquet sports with automatic summarization focusing on rally scenes,Shunya Kawamura; Tsukasa Fukusato; Tatsunori Hirai; Shigeo Morishima,This paper presents a system for viewing important rallies efficiently in racquet sports videos.There are several long matches in a tournament; thus; watching racquet sports can betimeconsuming. Liu et al.[2009] proposed a racquet sports video summarization systembased on a supervised audio classification that generates the summary video composed ofonly rally shots. However; summarizing a video is time-consuming because video editorsmust label audio information every second in the first 30 minutes. To solve this problem; wepropose a system to summarize racquet sports video automatically using audio and visualinformation. In addition; we propose an efficient viewing system that maintainsunderstanding of games using fast-forwarding.,ACM SIGGRAPH 2014 Posters,2014,5
Automatic generation of head models and facial animations considering personal characteristics,Akinobu Maejima; Hiroto Yarimizu; Hiroyuki Kubo; Shigeo Morishima,Abstract We propose a new automatic head modeling system to generate individualizedhead models which can express person-specific facial expressions. The head modelingsystem consists of two core processes. The head modeling process with the proposedautomatic mesh completion generates a whole head model only from facial range scan data.The key shape generation process generates key shapes for the generated head modelbased on physics-based facial muscle simulation with an individual muscle layout estimatedfrom subject's facial expression videos. Facial animations considering personalcharacteristics can be synthesized using the individualized head model and key shapes.Experimental results show that the proposed system can generate head models where 84%of subjects can identify themselves. Therefore; we conclude that our head modeling …,Proceedings of the 17th ACM Symposium on Virtual Reality Software and Technology,2010,5
来場者の声の特徴を反映する映像エンタテインメントシステムのための台詞音声生成システム,川本真一， 足立吉広， 大谷大和， 四倉達夫， 森島繁生， 中村哲,視聴者の顔を CG で再現し; CG キャラクタとして映画に登場させる Future Cast System (FCS)を改良し; 視聴者から収録した少量の音声サンプルを用いて; 視聴者に似た台詞音声を生成するため複数手法を統合し; 生成された台詞音声をシーンに合わせて同期再生することで;視聴者の声の特徴をキャラクタに反映させるシステムを提案する. 話者データベースから視聴者と声が似た話者を選択する手法 (類似話者選択技術) と; 複数話者音声を混合することで視聴者の声に似た音声を生成する手法 (音声モーフィング技術) を組み合わせたシステムを構築し;複数処理を並列化することで; 上映準備時間の要求条件を満たした. 実環境を想定して BGM/SEを重畳した音声によって; 従来手法である類似話者選択技術より得られる音声と;提案法で導入した音声モーフィング技術より得られる音声を主観評価実験により評価した結果;Preference Score で 56.5% のモーフィング音声が目標話者の音声に似ていると判断され; 音声 …,*,2010,5
Hair motion reconstruction using motion capture system,Takahito Ishikawa; Yosuke Kazama; Eiji Sugisaki; Shigeo Morishima,Abstract Processes such as modeling; rendering and simulating hair are essential forcreating virtual humans in various CG applications. CG hair motion simulations arecommonly constructed using a physics-based model and have been extensivelyresearched. While physics-based approaches have achieved effective and sophisticatedhair motion; data-driven approaches for hair motion have been scarcely researched. Thispaper therefore outlines our hair animation approach which uses Motion Capture System tocapture 3D hair motion. Our approach enables users to easily simulate complex hairmotions such as the collision between hair strands and the human body and the movementof hair strands as they are blown in a variety of directions by wind.,ACM SIGGRAPH 2007 posters,2007,5
顔情報データベース FIND,渡邊伸行， 鈴木竜太， 吉田宏之， 續木大介， 番場あやの， 時田学， 和田万紀， 森島繁生， 山田寛,抄録 This paper offers a database of facial images of Japanese; available for various kindsof studies on face and facial expressions. The Facial Information Norm Database (FIND)currently includes more than 13;000 images of 150 Japanese neutral faces; sevenprototypical facial expressions of emotion (happiness; surprise; fear; sadness; anger;disgust; and contempt); and facial behaviors of single Action Unit (AU) and AU combinationsbased on the Facial Action Coding System (FACS; Ekman et al.; 2002). FIND also containsinformation on each image such as a demographics; facial structural (shape) information byfitting a facial wireframe model onto the image; cognitive judgment data; andpsychophysiological data obtained in judgment studies. We call the images and all otherinformation mentioned above “facial information.” This paper describes the FIND and …,感情心理学研究,2007,5
Construction of audio-visual speech corpus using motion-capture system and corpus based facial animation,Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,An accurate audio-visual speech corpus is inevitable for talking-heads research. This paperpresents our audio-visual speech corpus collection and proposes a head-movementnormalization method and a facial motion generation method. The audio-visual corpuscontains speech data; movie data on faces; and positions and movements of facial organs.The corpus consists of Japanese phoneme-balanced sentences uttered by a female nativespeaker. An accurate facial capture is realized by using an optical motion-capture system.We captured high-resolution 3D data by arranging many markers on the speaker's face. Inaddition; we propose a method of acquiring the facial movements and removing headmovements by using affine transformation for computing displacements of pure facialorgans. Finally; in order to easily create facial animation from this motion data; we …,IEICE TRANSACTIONS on Information and Systems,2005,5
話者のイントネーションを模倣するインタラクティブ声質変換システムの構築,足立吉広， 森島繁生,Abstract In this paper; an interactive speech conversion system which can modify emotionalfactor in sppech; emphasize utterance and append a resional dialect is presented. Prosodyof stored sample voice is converted to a new prosody given by realtime analysis ofmicrophone captured reference voice. Only intonation is controlled by converting utterancespeed; pitch and power of speech by keeping speaker characteristic and linguisticinformation as original. Especially; an interpolation method of utterance speed is originallyproposed and a quality of synthesized speech is evaluated subjectively.,インタラクション,2005,5
擬人化音声対話エージェント基本ソフトウェアの開発プロジェクト報告,嵯峨山茂樹， 伊藤克亘， 宇津呂武仁， 甲斐充彦， 小林隆夫， 下平博， 伝康晴， 徳田恵一， 中村哲， 西本卓也， 新田恒雄， 広瀬啓吉， 峯松信明， 森島繁生， 山下洋一， 山田篤， 李晃伸,Abstract This paper describes the outline of “Galatea;” a software toolkit of anthropomorphicspoken dialog agent developed by the authors. Major functions such as speech recognition;speech synthesis and face animation generation are integrated and controlled under adialog control. To emphasize custonizability as the dialog research platform; this systenfeatures easily replaceable face; speaker-adaptive speech synthesis; easily modification ofdialog control script; exchangeable function modules; and multi-processor capability. Thistoolkit is ready for download with an open-source and license-free policy.,情報処理学会研究報告音声言語情報処理 (SLP),2003,5
エンタテインメントのための表情分析・合成技術 (< 特集> エンタテインメント VR),森島繁生,抄録 Copying human action accurately is main scheme in entertainment VR area to controland generate a virtual human or cartoon character in a screen. Motion capture system isgenerally used to generate a crone human in the scene of motion picture or interactivegame; however; huge manual operation at post processing is inevitable to generate highquality image. In this paper; especially copying facial action is focused on and high qualityand accurate method to generate and copy natural impression of face with semi-interactiveprocess using face image analysis and face image synthesis scheme. Finally; someapplication systems in entertainment area are introduced.,日本バーチャルリアリティ学会論文誌,2002,5
ズーム変化を含む動画中の顔自動トラッキングとマッチムーブによる表情合成,長田誉弘， 大室学， 緒方信， 森島繁生,抄録 映画製作においては; 手動のマッチムーブ処理によって; 主人公の顔部分を別の人物に置き換える処理がしばしば行われるが; 経験と時間を要する処理である. また; 洋画の吹き替えにおいては; 口形と音声の同期が取れず; しばしば; 口の動きからせりふが制約を受ける場合もある.本稿では; 映像中の人物の顔の位置と向きを自動的に推定し; 顔の全体もしくは一部を置き換える手法を提案し; この問題点に対応する. 顔のトラッキングには; 3 次元テンプレートを利用する手法を提案し; 精度の高い推定を実現する. また; このトラッキング結果に基づいて画像中にワイヤフレームを当てはめ; 顔を他の人物のものと置換したり; 口の部分を入れ替えて; 別の言葉を発声する映像に変換するシステムを提案する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2002,5
Multimedia ambience communication based on actual moving pictures in a steroscopic projection display environment,Kunio Yamada; Tadashi Ichikawa; Takeshi Naemura; Kiyoharu Aizawa; Shigeo Morishima; Takahiro Saito,Multi-media Ambiance Project of TAO has been researching and developing an imagespace; that can be shared by people in different locations and can lend a real sense ofpresence. The image space is mainly based on photo-realistic texture; and somedeformations which depend on human vision characteristics or pictorial expressions arebeing applied. We aim to accomplish shared-space communication by an immersiveenvironment consisting of the image space stereoscopically projected on an arched screen.We refer to this scheme as' ambiance communication.'The first half of this paper presentsglobal descriptions on basic concepts of the project; the display system and the 3-cameraimage capturing system. And the latter half focuses on two methods to create a photo-realistic image space using the captured images of a natural environment. One is the …,Stereoscopic Displays and Virtual Reality Systems VII,2000,5
Realtime face analysis and synthesis using neural network,Shigeo Morishima,Describes recent research results about how to generate an avatar's face in a real-timeprocess; exactly copying a real person's face. It is very important for the synthesis of a realavatar to precisely duplicate the emotions and impressions included in the original faceimage and voice. A face-fitting tool from multi-angle camera images is introduced in order tomake a real 3D face model with real texture and geometry that is very close to the original.When an avatar is speaking something; the voice signal is very essential for deciding themouth shape features; so a real-time mouth shape control mechanism is proposed byconversion from speech parameters to lip shape parameters using a multi-layered neuralnetwork. For dynamic modeling of facial expressions; a muscle structure constraint isintroduced to generate a facial expression naturally with just a few parameters. We also …,Neural Networks for Signal Processing X; 2000. Proceedings of the 2000 IEEE Signal Processing Society Workshop,2000,5
オプティカルフローを用いた正面顔画像からの顔面筋パラメータの自動推定,矢崎和彦， 石川貴博， 森島繁生,抄録 コンピュータ上で顔表情を表現する手法の一つとして顔面筋肉モデルが挙げられる.顔面筋肉モデルはモデル化した皮膚組織及び表情筋を持ち皮膚組織に与える影響力を物理的に計算し; 皮膚組織を変形させることによって表情表出が可能である. 表情を決定するパラメータは筋肉が収縮する力 (筋肉パラメータ) である. 筋肉パラメータからの表情への変換は力の重ねあわせによって行なわれるが; 特定の表情に対応する筋肉パラメータの決定は試行錯誤的に行なう必要があった. そこで本研究ではオプティカルフロー を用いることにより正面顔画像から自動的に顔画像に対応する筋肉パラメータを推定する試みを行なった.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,1998,5
熱画像を用いた人物全身像の実時間姿勢推定,岩澤昭一郎， 海老原一之， 大谷淳， 中津良平， 森島繁生,Page 1. 論文 論文小特集 ■ 画像 技術 に おけ る学 習 ・適応 ・進化 熱画像を用いた人物全身像の実時間姿勢推定 岩 澤 昭一 郎 †††;正 会員 海 老原 一 之 †; 大 谷 淳 †; 中 津 良 平 †;正 会員 森島 繁 生 ††; Real-Time Estimation of Human Body Postures from Monocular Thermal ImagesShoichiro Iwasawa† ††; Kazuyuki Ebihara†; Jun Ohya†; Ryohei Nakatsu† and Shigeo Morishima†† Abstract This paper proposes a new real-time method for estimating human body posturesfrom ther- mal images acquired by an infrared camera ; regardless of the background and lightingconditions. Distance transformation is performed for the human body area extracted from thethresholded thermal image ; in order to calculate the center of gravity. After the orientation …,映像情報メディア学会誌,1997,5
熱画像を用いた人物全身像の実時間姿勢推定,岩澤昭一郎， 海老原一之， 大谷淳， 中津良平， 森島繁生,Page 1. 論文 論文小特集 ■ 画像 技術 に おけ る学 習 ・適応 ・進化 熱画像を用いた人物全身像の実時間姿勢推定 岩 澤 昭一 郎 †††;正 会員 海 老原 一 之 †; 大 谷 淳 †; 中 津 良 平 †;正 会員 森島 繁 生 ††; Real-Time Estimation of Human Body Postures from Monocular Thermal ImagesShoichiro Iwasawa† ††; Kazuyuki Ebihara†; Jun Ohya†; Ryohei Nakatsu† and Shigeo Morishima†† Abstract This paper proposes a new real-time method for estimating human body posturesfrom ther- mal images acquired by an infrared camera ; regardless of the background and lightingconditions. Distance transformation is performed for the human body area extracted from thethresholded thermal image ; in order to calculate the center of gravity. After the orientation …,映像情報メディア学会誌,1997,5
音声に込められた感情の意味次元に関する検討,佐藤順， 森島繁生,抄録 音声に込められた感情の意味次元について 3 つの主観評価実験を行った. まず; 実験 1としてテレビドラマより収録した音声資料の感情カテゴリーを分類し; 実験 2 では実験 3 で用いる SD法の評価形容詞対として適当なものを選択する実験を行った. 最後に; 実験 1で分類された音声資料に対し; それらの評価語対を用いて SD 法により主観評価実験を行った.さらに; 実験 3 で得られた結果について因子分析を行った. その結果; 2 つの因子が得られ; ひとつは「活性化」 の因子で; もうひとつは 「快-不快」 の因子であることがわかった. また;各音声資料についてそれぞれの因子得点を求めたところ; 基本感情のカテゴリー毎に分離されたグループを形成することが分かった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,5
通常映像および ISDN 映像の話者映像効果の比較研究,呉俊， 田村博， 渋谷雄,本論文では複数音声を電子的に混合した単語音声を一方の話者映像と共に視聴者に提示し;単語認識の正答率を測り; 通常映像と ISDN 映像の話者映像効果を比較した.本手法では複数音声を左右耳から提示する方法に比べ; 話者映像効果がより顕著に現れるほか;正答率に異なった特性が見られることが示された. ISDN 映像の話者映像効果は映像と音声が同期している場合でも低く; 話者映像効果は通常映像の 1/2 程度であった. また ISDN映像の話者映像効果は個人差が大きく; 通常映像以上に話者映像効果を示す人がいる一方で;逆に話者映像効果が皆無か逆効果を示す視聴者も 30% に達した. このような差が見られた理由を;映像の動的効果と静的効果によって検討し; 正答率と音声対の波形の類似性について検討を行った. ディジタル画像の圧縮・伝送においては; 圧縮率; 伝送速度だけでなく映像効果に着目した研究が必要であることを示した.,電子情報通信学会論文誌 D,1996,5
Automatic rule extraction from statistical data and fuzzy tree search,Shigeo Morishima; Hiroshi Harashima,Abstract Generally speaking; it is necessary to extract knowledge from an expert in a givendiscipline and implement this knowledge into a system when constructing an expert system.However; it is not easy to extract knowledge in such fields as medical diagnosis or patternrecognition because the inference logic depends on the experience and intuition of theexpert.,Systems and computers in Japan,1988,5
Facial Aging Simulator by Data-Driven Component-Based Texture Cloning,Daiki Kuwahara; Akinobu Maejima; Shigeo Morishima,Abstract Facial aging and rejuvenation simulation is a challenging topic because keepingpersonal characteristics in every age is difficult problem. In this demonstration; we simulate afacial aging/rejuvenating only from a single photo. Our system alters an input face image toaged face by reconstructing every facial component with face database for target age. Anappropriate facial components image are selected by a special similarity measurementbetween current age and target age to keep personal characteristics as much as possible.Our system successfully generated aged/rejuvenated faces with age-related features suchas spots; wrinkles; and sagging while keeping personal characteristics throughout all ages.,International Conference on Multimedia Modeling,2015,4
VRMixer: mixing video and real world with video segmentation,Tatsunori Hirai; Satoshi Nakamura; Tsubasa Yumura; Shigeo Morishima,Abstract This paper presents VRMixer; a system that mixes real world and a video clip lettinga user enter the video clip and realize a virtual co-starring role with people appearing in theclip. Our system constructs a simple virtual space by allocating video frames and the peopleappearing in the clip within the user's 3D space. By measuring the user's 3D depth in realtime; the time space of the video clip and the user's 3D space become mixed. VRMixerautomatically extracts human images from a video clip by using a video segmentationtechnique based on 3D graph cut segmentation that employs face detection to detach thehuman area from the background. A virtual 3D space (ie; 2.5 D space) is constructed bypositioning the background in the back and the people in the front. In the video clip; the usercan stand in front of or behind the people by using a depth camera. Real objects that are …,Proceedings of the 11th Conference on Advances in Computer Entertainment Technology,2014,4
Application friendly voxelization on GPU by geometry splitting,Zhuopeng Zhang; Shigeo Morishima,Abstract In this paper; we present a novel approach that utilizes the geometry shader todynamically voxelize 3D models in real-time. In the geometry shader; the primitives are splitby their Z-order; and then rendered to tiles which compose a single 2D texture. This methodis completely based on graphic pipeline; rather than computational methods likeCUDA/OpenCL implementation. So it can be easily integrated into a rendering or simulationsystem. Another advantage of our algorithm is that while doing voxelization; it cansimultaneously record the additional mesh information like normal; material properties andeven speed of vertex displacement. Our method achieves conservative voxelization by onlytwo passes of rendering without any preprocessing and it fully runs on GPU. As a result; ouralgorithm is very useful for dynamic application.,International Symposium on Smart Graphics,2014,4
Driver drowsiness estimation from facial expression features computer vision feature investigation using a CG model,Taro Nakamura; Akinobu Maejima; Shigeo Morishima,We propose a method for estimating the degree of a driver's drowsiness on the basis ofchanges in facial expressions captured by an IR camera. Typically; drowsiness isaccompanied by drooping eyelids. Therefore; most related studies have focused on trackingeyelid movement by monitoring facial feature points. However; the drowsiness featureemerges not only in eyelid movements but also in other facial expressions. To moreprecisely estimate drowsiness; we must select other effective features. In this study; wedetected a new drowsiness feature by comparing a video image and CG model that areapplied to the existing feature point information. In addition; we propose a more precisedegree of drowsiness estimation method using wrinkle changes and calculating local edgeintensity on faces; which expresses drowsiness more directly in the initial stage.,Computer Vision Theory and Applications (VISAPP); 2014 International Conference on,2014,4
Photorealistic aged face image synthesis by wrinkles manipulation,Ai Mizokawa; Hiroki Nakai; Akinobu Maejima; Shigeo Morishima,Permission to make digital or hard copies of part or all of this work for personal or classroomuse is granted without fee provided that copies are not made or distributed for commercial advantageand that copies bear this notice and the full citation on the first page. Copyrights for third-partycomponents of this work must be honored. For all other uses; contact the Owner/Author. SIGGRAPH2013; July 21 – 25; 2013; Anaheim; California. 2013 Copyright held by the Owner/Author. ACM978-1-4503-2261-4/13/07 … Photorealistic Aged Face Image Synthesis by Wrinkles Manipulation… Ai MIZOKAWA* Hiroki NAKAI Akinobu MAEJIMA Shigeo MORISHIMA† Waseda University… Many studies on an aged face image synthesis have been reported with the purpose of securityapplication such as investigation for criminal or kidnapped child and entertainment applicationssuch as movie or video game. Tazoe et al. [2012] proposed a facial aging technique …,ACM SIGGRAPH 2013 Posters,2013,4
Instant movie casting with personality: Dive into the movie system,Shigeo Morishima; Yasushi Yagi; Satoshi Nakamura,Abstract “Dive into the Movie (DIM)” is a name of project to aim to realize a world innovativeentertainment system which can provide an immersion experience into the story by giving achance to audience to share an impression with his family or friends by watching a movie inwhich all audience can participate in the story as movie casts. To realize this system; we aretrying to model and capture the personal characteristics instantly and precisely in face; body;gait; hair and voice. All of the modeling; character synthesis; rendering and compositingprocesses have to be performed on real-time without any manual operation. In this paper; anovel entertainment system; Future Cast System (FCS); is introduced as a prototype of DIM.The first experimental trial demonstration of FCS was performed at the World Exposition2005 in which 1;630;000 people have experienced this event during 6 months. And finally …,International Conference on Virtual and Mixed Reality,2011,4
Data driven in-betweening for hand drawn rotating face,Hiroaki Gohara; Shiori Sugimoto; Shigeo Morishima,Abstract In anime production; some key-frames are drawn by artist precisely and then agreat number of in-betweening frames are drawn by assistants' hands. However; it isseriously time-consuming and skilled work to draw many characters especially includingface rotation. In this paper; we propose an automatic in-betweening technique for rotatingface of hand drawn character only from a front image and a diagonal image (Fig. 1). Baxter[2009] represented generating in-betweening using image morphing technique. However;their approach doesn't consider reflecting the artist's style and touch. Accordingly; werepresent reflecting style and touch using morphing technique trained by his own databaseand introduced especially to generate a rotational in-betweening faces. This databasecontains center of gravity of each part (right eye; left eye; nose; mouth; eyebrow) and the …,ACM SIGGRAPH 2010 Posters,2010,4
Curvature depended local illumination approximation of ambient occlusion,Tomohito Hattori; Hiroyuki Kubo; Shigeo Morishima,1. Introduction1 This paper discusses an approach for computing the ambient occlusion by curvaturedepended approximation of occlusion. Ambient occlusion is widely used to improve the realismof fast lighting simulation. The ambient occlusion is defined as follows … ∫ + Ω = ω θ ω π dxV x Labocc cos);( 1 )( )1 … V(x; ω) is the visibility function in direction ω from vertex locationx. θ is the angle between vertex normal and ω. Ω+ is a hemisphere domain of integration definedby the normal. While this method enables effective lighting simulation by considering surroundingocclusion; previous works [Miguel et al. 2008 : Janne et al. 2005]; needs great computationcost; because the integration is solved by monte-carlo collision detection. By contrast; we approximateocclusion by curvature to aim to reduce the cost … 2. Shape Geometry ApproximationPrecisely; the previous ambient occlusion needs monte-carlo colli- sion detection …,ACM SIGGRAPH 2010 Posters,2010,4
Human head modeling based on fast-automatic mesh completion,Akinobu Maejima; Shigeo Morishima,Abstract The need to rapidly create 3D human head models is still an important issue ingame and film production. Blanz et al have developed a morphable model which can semi-automatically reconstruct the facial appearance (3D shape and texture) and simulatedhairstyles of" new" faces (faces not yet scanned into an existing database) usingphotographs taken from the front or other angles [Blanz et al. 2004]. However; this methodstill requires manual marker specification and approximately 4 minutes of computationaltime. Moreover; the facial reconstruction produced by this system is not accurate unless adatabase containing a large variety of facial models is available. We have developed asystem that can rapidly generate human head models using only frontal facial range scandata. Where it is impossible to measure the 3D geometry accurately (as with hair regions) …,ACM SIGGRAPH ASIA 2009 Posters,2009,4
Muscle-based facial animation considering fat layer structure captured by MRI,Hiroto Yarimizu; Yasushi Ishibashi; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Abstract Muscle-based facial animation [Lee et al. 1995] is one of the best approaches torealize facial expressions of characters. However; this approach does not consider thepersonal variation in facial tissue model such as skin thickness. So personal character inemotional expression can not be reflected in this model.,SIGGRAPH'09: Posters,2009,4
Interactive shadowing for 2D Anime,Eiji Sugisaki; Hock Soon Seah; Feng Tian; Shigeo Morishima,Abstract In this paper; we propose an instant shadow generation technique for 2Danimation; especially Japanese Anime. In traditional 2D Anime production; the entireanimation including shadows is drawn by hand so that it takes long time to complete.Shadows play an important role in the creation of symbolic visual effects. However shadowsare not always drawn due to time constraints and lack of animators especially when theproduction schedule is tight. To solve this problem; we develop an easy shadowingapproach that enables animators to easily create a layer of shadow and its animation basedon the character's shapes. Our approach is both instant and intuitive. The only inputsrequired are character or object shapes in input animation sequence with alpha valuegenerally used in the Anime production pipeline. First; shadows are automatically …,Computer Animation and Virtual Worlds,2009,4
The online gait measurement for the audience-participant digital entertainment,Mayu Okumura; Yasushi Makihara; Shinsuke Nakamura; Shigeo Morishima; Yasushi Yagi,Abstract. This paper presents a method to measure online the gait features from the gaitsilhouette images and reflect the gait features to CG characters for an audience-participationdigital entertainment. First; both static and dynamic gait features are extracted from thesilhouette images captured by the online gait measurement system with two cameras and achroma-key background. Then; Standard Gait Models (SGMs) with various types of gaitfeatures are constructed and stored; which are composed of a pair of CG characters'rendering parameters and synthesized silhouette images. Finally; blend ratios of the SGMsare estimated to minimize gait feature errors between the blended model and the onlinemeasurement. In an experiment; a gait database with 100 subjects is used for gait featureanalysis and it is confirmed that the measured gait features are reflected to the CG …,Invited Workshop on Vision Based Human Modeling and Synthesis in Motion and Expression,2009,4
雑音環境下での音声の聞き取り実験による合成発話顔アニメーションの評価,前島謙宣， 四倉達夫， 森島繁生， 中村哲,人間のような見た目をもつ擬人化エージェントの実現は; コンピュータを介して人間同士のコミュニケーションの幅を広げるための重要な研究課題である. 筆者らは; このようなコミュニケーションを可能にするための; 自然な発話顔アニメーションの合成手法を提案している.しかし; 発話顔アニメーションに対する性能の評価方法は課題として残されていた.発話顔アニメーションの性能は;(1) 読唇をできる程度に再現されているか;(2)視覚的に自然であるか;(3) 音声と正確に同期しているかの 3 点により決定される. 本論文では;まず雑音環境下において発話顔アニメーションと音声とを被験者に提示し; 発話内容の聞き取り実験を行うことにより (1) を検証する. 次に (2) について; 発話顔アニメーションの視覚的な自然さ及び発話口形の滑らかさを 5 段階評価する. 最後に (3) について; ある一定間隔で音声と発話顔アニメーションとの同期をずらしたものを被験者に提示し; 同期のずれの主観値を調査する …,電子情報通信学会論文誌 A,2005,4
Multimodal translation system using texture-mapped lip-sync images for video mail and automatic dubbing applications,Shigeo Morishima; Satoshi Nakamura,Abstract We introduce a multimodal English-to-Japanese and Japanese-to-Englishtranslation system that also translates the speaker's speech motion by synchronizing it to thetranslated speech. This system also introduces both a face synthesis technique that cangenerate any viseme lip shape and a face tracking technique that can estimate the originalposition and rotation of a speaker's face in an image sequence. To retain the speaker's facialexpression; we substitute only the speech organ's image with the synthesized one; which ismade by a 3D wire-frame model that is adaptable to any speaker. Our approach providestranslated image synthesis with an extremely small database. The tracking motion of theface from a video image is performed by template matching. In this system; the translationand rotation of the face are detected by using a 3D personal face model whose texture is …,EURASIP Journal on Advances in Signal Processing,2004,4
感情音声と表情画像を同時に提示した場合のマルチモーダル印象の評価,比留間庸介， 足立吉広， 森島繁生,抄録 人間同士のコミュニケーションにおいて感情のやり取りを行う場合に; 発話音声から聞き取られる感情と表情から読み取れる感情は矛盾なく一致して相手に伝えられる. 擬人化エージェントにおいては; 感情表現技術の未熟さから; 必ずしもリアリティが高く感情豊かな表情合成や音声合成が実現できているわけではない. したがって; しばしば受ける印象に違和感が生じる場合がある.そこで本稿では; 発話音声に含まれる感情表現と表情動画像に含まれる感情表現に矛盾が生じた場合に; 人間の受け取る印象にどのような変化が生じるかを評価することによって;音声に強い影響を受ける感情は何か; 画像に強い影響を受ける感情は何かを明らかにすることを試みた. まず評価実験は; 自然音声と合成音声を対象として; 音声単独での感情聞き取り実験を実施した. 次にビデオで収録した感情動画像のみを無音で提示し; 評価した. 最後に; 映像と音声を同時に提示して評価した. その際; 映像に同期するように音声の発話速度を制御して; 異なる感情の …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2004,4
The second report of constructing facial information norm database: capturing environment and searching interface of images,H Yoshida; R Suzuki; N Watanabe; T Yamaguchi; Y Ogawa; M Kitamura; A Maeda; D Tsuzuki; G Tokita; M Wada; S Morishima; H Yamada,*,IEIC Tech Rep,2004,4
Galatea: an anthropomorphic spoken dialogue agent toolkit,Shigeki Sagayama; S Kawamoto; H Shimodaira; T Nitta; T Nishimoto; S Nakamura; K Itou; S Morishima; T Yotsukura; A Kai; A Lee; Y Yamashita; T Kobayashi; K Tokuda; K Hirose; N Moinematsu; A Yamada; Y Den; T Utsuro,*,IPSJ SIG-SLP,2003,4
An open source development tool for anthropomorphic dialog agent: face image synthesis and lip synchronization,Tatsuo Yotsukura; Shigeo Morishima,We describe the design and report the development of an open source ware toolkit forbuilding an easily customizable anthropomorphic dialog agent. This toolkit consists of fourmodules for multi-modal dialog integration; speech recognition; speech synthesis; and faceimage synthesis. In this paper; we focus on the construction of an agent's face imagesynthesis.,Multimedia Signal Processing; 2002 IEEE Workshop on,2002,4
Information processing method and apparatus and providing medium,*,In computer graphics; the distal end of a hair being drawn is to be prevented from beinglowered in massy feeling. To this end; a data storage unit stores three-dimensionalcoordinates of control points of a curve; and outputs these coordinate values to a curveapproximating unit under control by a controller. The curve approximating unit generates aBezier curve; using the coordinate values of the input control points; to output the generatedBezier curve to a z-coordinate comparator. The z-coordinate comparator stores the one ofthe coordinate values having the same x-components and y-components which is closest toa viewing point; that is; which has the maximum z-component. A reflected light intensitycomputing unit computes the intensity of the light reflected by the surface of the curve. Animage synthesis unit mixes luminance values of a portion of the curve from an input pre …,*,2002,4
Hypermask: Reactive talking head for storytelling,Tatsuo Yotsukura; Frank Nielsen; Kim Binsted; Nobuji Tetsutani; Ryouhei Nakatsu; Shigeo Morishima,*,IEICE Transactions on Information and Systems,2001,4
Novel 3D Image Structure; Processing and Communications Technologies Based on Hyper-Realistic Image for Multi-Media Ambiance Communication,K Mochizuki,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proc. AD/IDW'01,2001,4
仮想空間上におけるリアルな三次元口形状の作成,伊藤圭， 三澤， 貴文， 武藤， 森島繁生,コ ン ピ ュータ と入間との コ ミュニ ケ ーシ ョ ン を円 滑 に行 う には人 と人が 直接 対 話 して い るよ うな環 境を実 現 す る こ とが 理 想 である. また; そ うい っ た人との 会話の 理解に お い て;唇の 動きが特に重 要 な役割を果 た して い る こ と も研 究 に よ りわか っ て い る. つ ま り; こ のよ うな環境を実現 する た め に は; 相 手が 違和感を抱くこ との な い 自然な 口 形状合成を 行うことが 必要で あ る と考え,2000 信学総大,2000,4
Physics-model-based 3D facial image reconstruction from frontal images using optical flow,Shigeo Morishima; Takahiro Ishikawa; Demetri Terzopoulos,Physics-based face image synthesis is one of the most real- istic approaches to realize life-likeagents in computers. A facial-muscle model1 is composed of facial tissue elements andmuscles. In this model; forces affecting facial tissue ele- ments are calculated by contraction ofeach muscle; so the combination of each muscle parameter determines a specific facialexpression. Then each muscle parameter is specified in a trial-and-error procedure comparingthe sample photograph and generated image using our Muscle-Editor to generate a specificface image. In this sketch; we propose a strategy for automatic estimation of facial muscle parametersfrom opti- cal-flow using a neural network. This corresponds to 3D facial-motion tracking froma 2D image under the physics- model-based constraint. This technique is also 3D-motion estimationfrom 2D tracking in a captured image under the constraint of the physics-based face model.,ACM SIGGRAPH 98 Conference abstracts and applications,1998,4
表情認識・合成の技術課題,森島繁生,抄録 顔表情の認識・合成は; Face-to-Face の対話を実現する次世代のヒューマンインタフェースの基礎技術として重要な役割を果たす. 本稿では; 主として表情合成の立場から現在の技術レベルを概観し; その問題点と今後の技術課題を抽出する. 特に顔のモデル化手法; 動的な表情のモデル化手法; 感情のモデル化手法; 頭髪の形状および運動のモデル化手法; リップシンク手法について述べる.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,1997,4
ダイナミックスモデルに基づく頭髪の運動表現,三枝太， 森島繁生,あらましサイバースペースにおける仮想人物の合成やコミュニケーションシステムのための人物像合成等で; コンピュータグラフイックスによる人物合成が注目を集めている. 本稿では; 特に人物のCG の中でも特に合成が難しいとされる頭髪の表現手法について述べる. 既に頭髪を空間曲線により近似することで; 形状データの容量を大幅に削減し; また剛体セグメントモデルで近似することにより頭髪の運動制御を実現する方法を筆者らは提案した. 本稿では; この運動制御方法をさらに改良し実際の運動に即したモデルで記述することで; 頭髪の動きをより自然なものにした. また;新たな衝突判定アルゴリズムを提案し; 高速な衛突判定処理を行った. さらに; ディスプレイの数倍の解像度を持つイメージバッフアを用いたレンダリング手法により; より滑らかな頭髪の表現を可能にした.,情報処理学会研究報告グラフィクスと CAD (CG),1997,4
モデルフィッティングのための正面顔画像からの特徴点自動抽出,岩澤昭一郎， 森島繁生,抄録 The geometry modeling in computer graphics has been very difficult and have neededa lot of work as usual. The facial geometry has particular compleity and personality. In thispaper; a generic model is used for the facial modeling method and is deformed and fittedalong facial feature points to make the personal model. This paper also describes thealgorithm to extract facial feature points from frontal view image. This algorithm is composedof the region segmentation techniques using color information to select eye/eyebrow/lipregions; and the heuristical extraction of facial feature points within local regions. And theexperiment results using actual face image shows the error to the manual modeling is slight.,テレビジョン学会技術報告,1996,4
感情音声による感情空間の構築,佐藤順， 川上文雄， 森島繁生,抄録 人と機械との対話を可能にするインタフェースを実現するために音声に含まれる感情情報に関する研究を行っている. 音声に含まれる感情情報は主に韻律情報であることがすでに報告されている. そこで; 感情毎のモデルを簡略化するために音節毎に韻律情報を求め;これらを音声に含まれる感情のパラメータとした. これまで顔表情の分野においてニューラルネットを用いた感情空間が提案されている. これは; 表情記述パラメータをその中間層に圧縮しこれを感情空間と呼ぶことで表情の分析・合成を行おうというものである. 本稿ではこのモデルを音声の分野に適応し音声による感情空間を提案する. また; 感情空間からパラメータを再現しこれを平静音声に付加することにより感情合成音声を生成する.,電子情報通信学会総合大会講演論文集,1996,4
感情音声による感情空間の構築,佐藤順， 川上文雄， 森島繁生,抄録 人と機械との対話を可能にするインタフェースを実現するために音声に含まれる感情情報に関する研究を行っている. 音声に含まれる感情情報は主に韻律情報であることがすでに報告されている. そこで; 感情毎のモデルを簡略化するために音節毎に韻律情報を求め;これらを音声に含まれる感情のパラメータとした. これまで顔表情の分野においてニューラルネットを用いた感情空間が提案されている. これは; 表情記述パラメータをその中間層に圧縮しこれを感情空間と呼ぶことで表情の分析・合成を行おうというものである. 本稿ではこのモデルを音声の分野に適応し音声による感情空間を提案する. また; 感情空間からパラメータを再現しこれを平静音声に付加することにより感情合成音声を生成する.,電子情報通信学会総合大会講演論文集,1996,4
Communication by Facial Expressions and Gestures,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,ACM SIGGRAPH'96 (Aug.),1996,4
3-D emotion space for interactive communication,Fumio Kawakami; Motohiro Ohkura; Hiroshi Yamada; Hiroshi Harashima; Shigeo Morishima,Abstract In this paper; the methods for modeling facial expression and emotion arcproposed. This Emotion Model; called 3-D Emotion Space can represent both human andcomputer emotion conditions appearing on the face as a coordinate in the 3-D Space. Forthe construction of this 3-D Emotion Space; 5-laycr neural network which is superior in non-linear mapping performance is applied. After the network training with backpropagalion torealize Identity Mapping; both mapping from facial expression parameters to the 3-DEmotion Space and inverse mapping from the Emotion Space to the expression parameterswere realized. As a result a system which can analyze and synthesize the facial expressionwere constructed simultaneously. Moreover; this inverse mapping to the facial expression isevaluated by the subjective evaluation using the synthesized expressions as test images …,International Computer Science Conference,1995,4
表情に基づく 3 次元感情空間への工学的・心理学的アプローチ,川上文雄， 坂口竜巳， 森島繁生， 山田寛， 原島博,抄録 人と機械との関係がユーザフレンドリーなものとなるためには; 機械が相手である人間の顔に表出される感情状態を認識してそれに対する表情を合成する必要がある. これには機械が何らかの定量化された感情モデルを持つ必要がある. そこで; 筆者らは汎化性能を持ち非線形写像能力に優れた 5 層ニューラルネットに表情を AU パラメータとして記述して繰り返し学習させ;このときの中間層のユニット数をこれまでの 2 ユニットから 3 ユニットに変更することで 3次元感情モデルを構築した. これにより表情のの認識・合成を同時に実現するシステムの構築を行った. また; この 3 次元感情モデルを心理学で提案されている表情空間と比較することで興味深い相関を得た.,電子情報通信学会技術研究報告. HC; ヒューマンコミュニケーション,1994,4
Human machine interface using media conversion and model-based coding schemes,S Morishima; H Harashima,Abstract One of the most user-friendly computer interface is face-to-face communication typeinterface using multi-media. Typical example is as follows. Human natural face on thedisplay can talk to operator with natural voice and the system can recognize the motion;expression and voice of operator. This paper describes face motion and expressionsynthesis schemes applicable to this face-to-face type interface. Human head is expressedwith 3D model and the surface model is constructed by texture mapping with real image. Allof the motions and expressions are synthesized and controlled automatically by themovement of some feature points on the model. This is the one of the applications of modelbased image coding and media conversion schemes.,*,1992,4
Shigeo morishima and Hiroshi Harashima;" A Media Conversion from English Text to Face Image for Pronunciation CAI System,Manabu Sudo,*,Proceedings of the 1992 IEICE spring conference A-276,1992,4
FOCUSING PATCH: Automatic Photorealistic Deblurring for Facial Images by Patch-Based Color Transfer,Masahide Kawai; Shigeo Morishima,Abstract Facial image synthesis creates blurred facial images almost without high-frequencycomponents; resulting in flat edges. Moreover; the synthesis process results in inconsistentfacial images; such as the conditions where the white part of the eye is tinged with the colorof the iris and the nasal cavity is tinged with the skin color. Therefore; we propose a methodthat can deblur an inconsistent synthesized facial image; including strong blurs created bycommon image morphing methods; and synthesize photographic quality facial images asclear as an image captured by a camera. Our system uses two original algorithms: patchcolor transfer and patch-optimized visio-lization. Patch color transfer can normalize facialluminance values with high precision; and patch-optimized visio-lization can synthesize adeblurred; photographic quality facial image. The advantages of our method are that it …,International Conference on Multimedia Modeling,2015,3
SPOTTING A QUERY PHRASE FROM POLYPHONIC MUSIC AUDIO SIGNALS BASED ON SEMI-SUPERVISED NONNEGATIVE MATRIX FACTORIZATION,Shigeo Morishima Taro Masuda; Kazuyoshi Yoshii; Masataka Goto,ABSTRACT This paper proposes a query-by-audio system that aims to detect temporallocations where a musical phrase given as a query is played in musical pieces. The “phrase”in this paper means a short audio excerpt that is not limited to a main melody (singing part)and is usually played by a single musical instrument. A main problem of this task is that thequery is often buried in mixture signals consisting of various instruments. To solve thisproblem; we propose a method that can appropriately calculate the distance between aquery and partial components of a musical piece. More specifically; gamma processnonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of thequery into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis …,ISMIR 2014,2014,3
PatchMove: Patch-based Fast Image Interpolation with Greedy Bidirectional Correspondence,Shigeo Morishima Shunsuke Saito; Ryuuki Sakamoto,*,Pacific Graphics 2014,2014,3
Quasi 3D rotation for hand-drawn characters,Chie Furusawa; Tsukasa Fukusato; Narumi Okada; Tatsunori Hirai; Shigeo Morishima,This paper presents a novel in-betweening method of a handdrawn 2D character animationeven including 3D motion like face rotation from front view to profile view. In-betweening isone of the most inevitable processes in commercial 2D animation production to generateintermediate frames automatically and naturally between hand generated key-frames. Ingeneral; the more number of key-frames are created; the better quality of animated motion isachieved. Especially; 3D-style motion like face rotation is so difficult to maintain high qualitythat it is necessary to create many key-frames which is time consuming with skilledanimators' labor cost. Depending on these background; recently many in-betweeningmethods are proposed focusing on a character face. Gohara et. al.[2010] proposed a datadriven face rotation method for a new character face considering a non-linear rotation …,ACM SIGGRAPH 2014 Posters,2014,3
PatchMove: Patch-based fast image interpolation with greedy bidirectional correspondence,S Morishima S Saito; R Sakamoto,In this paper; we present a method for the plausible interpolation of images. This method hasseveral applications; such as for smooth view interpolation; low frame-rate videoupsampling; and animation. The central idea is to quickly form dense correspondencesusing a patch-based nearest-neighbor search method called PatchMatch. However; theconventional PatchMatch method does not always find an accurate correspondence. Thismeans that some patches do not find appropriate counterparts. Our method employs agreedy algorithm and an occlusion handling technique to correct inaccuratecorrespondences. Furthermore; our texture reconstruction method successfully reducesblurring effects. We demonstrate that our method significantly reduces the computation timerequired for interpolation; and show that the quality of reconstructed images is almost …,*,2014,3
A Visuomotor Coordination Model for Obstacle Recognition,Tomoyori Iwao; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,In this paper; we propose a novel method for animating CG characters that while walking orrunning pay heed to obstacles. Here; our primary contribution is to formulate a genericvisuomotor coordination model for obstacle recognition with whole body movements. Inaddition; our model easily generates gaze shifts; which expresses the individuality ofcharacters. Based on experimental evidence; we also incorporate the coordination of eyemovements in response to obstacle recognition behavior via simple parameters related tothe target position and individuality of the characters's gaze shifts. Our overall model cangenerate plausible visuomotor coordinated movements in various scenes by manipulatingparameters of our proposed functions.,*,2014,3
Driver drowsiness estimation using facial wrinkle feature,Taro Nakamura; Tatsuhide Matsuda; Akinobu Maejima; Shigeo Morishima,Abstract In recent years; the rate of fatal motor vehicle accidents caused by distracted drivingresulting from factors such as sleeping at the wheel has been increasing. Therefore; an alertsystem that detects driver drowsiness and prevents accidents as a result by warning driversbefore they fall asleep is urgently required. Non-contact measuring systems using computervision techniques have been studied; and in vision approach; it is important to decide whatkind of feature we should use for estimating drowsiness.,ACM SIGGRAPH 2013 Posters,2013,3
Affective music recommendation system using input images,Shoto Sasaki; Tatsunori Hirai; Hayato Ohya; Shigeo Morishima,Abstract Music that matches our current mood can create a deep impression; which weusually want to enjoy when we listen to music. However; we do not know which music bestmatches our present mood. We have to listen to each song; searching for music thatmatches our mood. As it is difficult to select music manually; we need a recommendationsystem that can operate affectively. Most recommendation methods; such as collaborativefiltering or content similarity; do not target a specific mood. In addition; there may be no wordexactly specifying the mood. Therefore; textual retrieval is not effective. In this paper; weassume that there exists a relationship between our mood and images because visualinformation affects our mood when we listen to music. We now present an affective musicrecommendation system using an input image without textual information.,ACM SIGGRAPH 2013 Posters,2013,3
Automatic face replacement for a humanoid robot with 3D face shape display,Akinobu Maejima; Takaaki Kuratate; Brennand Pierce; Shigeo Morishima; Gordon Cheng,In this paper; we propose a method to apply any new face to a retro-projected 3D facesystem; the Mask-bot; which we have developed as a human-robot interface. The robot faceusing facial animation projected onto a 3D face mask can be quickly replaced by a new facebased on a single frontal image of any person. Our contribution is to apply an automatic facereplacement technique with the modified texture morphable model fitting to the 3D facemask. Using our technique; a face model displayed on Mask-bot can be automaticallyreplaced within approximately 3 seconds; which makes Mask-bot widely suitable toapplications such as video conferencing and cognitive experiments.,Humanoid Robots (Humanoids); 2012 12th IEEE-RAS International Conference on,2012,3
Fast-accurate 3d face model generation using a single video camera,Tomoya Hara; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,In this paper; we present a new method to generate a 3D face model; based on both Data-Driven and Structure-from-Motion approach. Considering both 2D frontal face imageconstraint; 3D geometric constraint; and likelihood constraint; we are able to reconstructsubject's face model accurately; robustly; and automatically. Using our method; it is possibleto create a 3D face model in 5.8 [sec] by only shaking own head freely in front of a singlevideo camera.,Pattern Recognition (ICPR); 2012 21st International Conference on,2012,3
A waist-mounted ProCam system for remote collaboration,Shigeki Morishima; Tomohiro Mashita; Kiyoshi Kiyokawa; Haruo Takemura,We propose a waist-mounted projector-camera (ProCam) system for asymmetric remotecollaboration. A wearable camera is often used to transmit a worker's situation to a remoteinstructor; however 3D structure of the worker's environment is not always available and theinstructor has a minimal flexibility in changing the camera's viewpoint. A stationary 3Dmeasurement system is also commonly used for remote collaboration; however a narrowmeasurement area and occlusion from a worker's body can be a severe problem. Our waist-mounted ProCam system reconstructs worker's environment in real-time without occlusionfrom the worker's body. The remote instructor can give instructions simply by drawingannotations on the reconstructed environment on screen; and they are properly projected infront of the worker. Structured-light based reconstruction; vision-based localization; and …,Mixed and Augmented Reality (ISMAR); 2012 IEEE International Symposium on,2012,3
Analysis and synthesis of realistic eye movement in face-to-face communication,Tomoyori Iwao; Daisuke Mima; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Abstract During face-to-face conversation; human eyes always rotate irregularly. Therefore;it is essential to synthesize such complex eye movements for the achievement of realistichuman facial animation. In the conversation; there are two kinds of eye movements;Saccades and Fixation Eye Movements (FEMs). Saccade is a relatively large scale motion ineye movements compared with FEMs' smaller one. Gu et al [2007] mainly focused onsaccades and suggested probability models based on their measurements. In theirresearch; they treated FEMs as a part of saccades; even though saccades and FEMs aretotally different kinds of eye movements. Thus their approximation is insufficient and bringsunnatural appearance especially in conversations. Moreover; they avoided to provide anapproximation of blinks' motions.,ACM SIGGRAPH 2012 Posters,2012,3
Realistic facial animation by automatic individual head modeling and facial muscle adjustment,Akinobu Maejima; Hiroyuki Kubo; Shigeo Morishima,Abstract We propose a technique for automatically generating a realistic facial animationwith precise individual facial geometry and characteristic facial expressions. Our method isdivided into two key methods: the head modeling process automatically generates a wholehead model only from facial range scan data; the facial animation setup processautomatically generates key shapes which represent individual facial expressions based onphysics-based facial muscle simulation with an individual muscle layout estimated fromfacial expression videos. Facial animations considering individual characteristics can besynthesized using the generated head model and key shapes. Experimental results showthat the proposed method can generate facial animations where 84% of subjects can identifythemselves. Therefore; we conclude that our head modeling techniques are effective to …,International Conference on Virtual and Mixed Reality,2011,3
The online gait measurement for characteristic gait animation synthesis,Yasushi Makihara; Mayu Okumura; Yasushi Yagi; Shigeo Morishima,Abstract This paper presents a method to measure online the gait features from the gaitsilhouette images and to synthesize characteristic gait animation for an audience-participantdigital entertainment. First; both static and dynamic gait features are extracted from thesilhouette images captured by an online gait measurement system. Then; key motion datafor various gaits are captured and a new motion data is synthesized by blending key motiondata. Finally; blend ratios of the key motion data are estimated to minimize gait feature errorsbetween the blended model and the online measurement. In experiments; the effectivenessof gait feature extraction were confirmed by using 100 subjects from OU-ISIR Gait Databaseand characteristic gait animations were created based on the measured gait features.,International Conference on Virtual and Mixed Reality,2011,3
Development of a toolkit for spoken dialog systems with an anthropomorphic agent: Galatea,Kouichi Katsurada; Akinobu Lee; Tatsuya Kawahara; Tatsuo Yotsukura; Shigeo Morishima; Takuya Nishimoto; Yoichi Yamashita; Tsuneo Nitta,The Interactive Speech Technology Consortium (ISTC) has been developing a toolkit calledGalatea that comprises four fundamental modules for speech recognition; speech synthesis;face synthesis; and dialog control; that can be used to realize an interface for spoken dialogsystems with an anthropomorphic agent. This paper describes the development of theGalatea toolkit and the functions of each module; in addition; it discusses the standardizationof the description of multi-modal interactions.,Proceedings: APSIPA ASC 2009: Asia-Pacific Signal and Information Processing Association; 2009 Annual Summit and Conference,2009,3
Aging model of human face by averaging geometry and filtering texture in database,Satoko Kasai; Shigeo Morishima,Abstract The presence of CG character is essential in anime and movie contents recently.Especially; personality and aging factor are also important in character modeling. However;the modeling CG character is based on hand-made process yet; so it costs huge amount ofmoney and labor to give one character several variations. About a facial animation; musclebased process or blend shape based process is very popular in contents production;however; in case of considering aging mechanism on face skin and bone; the differentmodel of each age has to be constructed for every character.,SIGGRAPH'09: Posters,2009,3
Characteristic gait animation synthesis from single view silhouette,Shinsuke Nakamura; Masashi Shiraishi; Shigeo Morishima; Mayu Okumura; Yasushi Makihara; Yasushi Yagi,Abstract Characteristics of human motion; such as walking; running or jumping vary fromperson to person. Differences in human motion enable people to identify oneself or a friend.However; it is challenging to generate animation where individual characters exhibitcharacteristic motion using computer graphics. Our goal is to construct a system thatsynthesizes characteristic gait animation automatically. As a result; when crowd animation isgenerated for instance; the motion with the variation can be made using our system. In oursystem; we first acquire a silhouette image as input data using a video camera. Second; weextract gait feature from single view silhouette. Finally we automatically synthesize 3D gaitanimation using the method blending a small number of motion data [KOVAR; L et al 2003].This blending weight is estimated using the gait feature automatically.,SIGGRAPH'09: Posters,2009,3
最適化局所アフィン変換に基づく正面顔レンジスキャンデータからの頭部モデル自動生成,前島謙宣， 森島繁生,< あらまし> 人間の三次元頭部モデルの構築において; 三次元レンジスキャナから獲得した頭部の形状データに対して; テンプレートメッシュを変形し整合する手法が一般的である. しかし;整合に必要なマーカの手動設定が必要とされ; レンジスキャン不可能な頭髪部位のデータ欠損の影響で後頭部の構築が困難な箇所の手修正を行う必要が生じる. 本稿では; レンジスキャンで得られる正面顔のみの形状とテンプレートメッシュから; 通常では計測困難な後頭部の形状をテンプレートメッシュの形状から補完可能な自動頭部生成法を提案する. 本手法は;実測された顔形状とテンプレートメッシュ形状の境界がシームレスに接続された頭部モデルを構築することが可能である. また; 一般的な整合処理は数分の時間を要するのに対し;本手法の処理時間は平均 10.4 秒であるため; 短時間に多くの CG 頭部モデル作成を必要とするゲーム; 映像制作などエンタテインメント分野において有効であると考えられる.,画像電子学会誌,2009,3
ダンス動画コンテンツを再利用して音楽に合わせた動画を自動生成するシステム,室伏空， 中野倫靖， 後藤真孝， 森島繁生,本研究では; 既存のダンス動画コンテンツの複数の動画像を分割して連結 (切り貼り) することで;音楽に合ったダンス動画を自動生成するシステムを提案する. 従来; 切り貼りに基づいた動画の自動生成に関する研究はあったが; 音楽–映像間の多様な関係性を対応付ける研究はなかった.本システムでは; そうした多様な関係性をモデル化するために; Web 上で公開されている二次創作された大量のコンテンツを利用し; クラスタリングと複数の線形回帰モデルを用いることで音楽に合う映像の素片を選択する. その際; 音楽–映像間の関係だけでなく; 生成される動画の時間的連続性や音楽的構造もコストとして考慮することで; 動画像の生成をビタビ探索によるコスト最小化問題として解いた.,研究報告音楽情報科学 (MUS),2009,3
DanceRe-Producer: 既存のダンス動画の再利用により音楽に合った動画を作成できるシステム,室伏空,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,WISS2009 講演論文集,2009,3
Quantitative representation of face expression by motion capture,Hiroaki Yanagisawa; Shinji Sogawa; A Maejima; T Yotsukura; S Morishima,*,HCS,2005,3
Galatea: 音声対話擬人化エージェント開発キット,西本卓也， 荒木雅弘， 伊藤克亘， 宇津呂武仁， 甲斐充彦， 河口信夫， 河原達也， 桂田浩一， 小林隆夫， 嵯峨山茂樹， 下平博， 伝康晴， 徳田恵一， 中村哲， 新田恒雄， 坂野秀樹， 広瀬啓吉， 峯松信明， 三村正人， 森島繁生， 山下洋一， 山田篤， 四倉達夫， 李晃伸,(1) 東京大学大学院 情報理工学系研究科 (〒 113-8656 東京都文京区本郷 7-3-1 E-mail:nishi@ hil. tu-tokyo. ac. jp)(2) 京都工繊大 (3) 名大 (4) 京大 (5) 静岡大 (6) 豊橋技科大 (7)東工大 (8) 北陸先端大 (9) 千葉大 (10) 名工大 (11) ATR (12) 和歌山大 (13) ASTEM (14) 成蹊大(15) 立命館大 (16) 奈良先端大,インタラクション,2004,3
Face expression synthesis based on a facial motion distribution chart,Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,In recent years; 3D computer graphic techniques are used for virtual human and cartooncharacters in the entertainment industry. Their facial expressions and mouth movements arenatural and smooth. However; these successful results require a tremendous amount of timeand effort on the part of accomplished CG creators. In fact; the technique of producing facialexpressions for a face mesh object typically calls for preparing all of the transformed objectsafter changing expressions. Then; using blend shape (another way of saying “morphing”)deformers; we can change the neutral face object into the transformed objects. To solvethese problems; we propose a technique to create deformed models of the expressions fromany user-created face object in a short period of time (Figure 1). First; we create" FacialMotion Distribution Chart"(FMD Chart) which describes the 3D displacement difference of …,ACM SIGGRAPH 2004 Posters,2004,3
Cartoon hair animation based on physical simulation,Eiji Sugisaki; Yizhou Yu; Ken Anjyo; Shigeo Morishima,The hair movement in cel character animation is sometimes inconsistent. The representationof hair; therefore; is a specialized work in cel animation. It is difficult to achieve in computergraphics; since cel animation may not be consistent with all the hair's attributes from allcamera positions. In fact; what animators create by solving physics equations is not alwayswhat the animators desire even though it is physically correct. Also the motion of hair incartoon animation carries meaning. Therefore; what the animators want exists only in theirimagination; and all the hair forms (shape modeling and number of hairs) in the key framesfrom the animation sequence cannot be in complete agreement (Fig1). Quite convincingresults; however; can be obtained from physically inconsistent frames. This is the mostdifficult part of expressing cartoon hair animation in computer graphics and is the reason …,ACM SIGGRAPH 2004 Posters,2004,3
顔情報データベース構築の基礎的検討 (2): 撮影環境と検索インターフェイスについて,吉田宏之， 鈴木竜太， 渡邊伸行， 山口拓人， 小川宜子， 北村麻梨， 前田亜希， 續木大介， 時田学， 和田万紀， 森島繁生， 山田寛,抄録 顔情報データベースとは顔研究に欠かせない顔情報を収集し; 様々な研究用途に提供するものである. データベース化する顔情報は FACS に基づいて表情表出を行った静止画像;その静止画像から抽出する顔の構造情報; 表情表出時の運動情報などから構成される.本報では登録されるデータを撮影するときの環境条件; ならびにデータベースサーバとそのインターフェイスについての報告を行う. また; 顔情報というきわめてプライバシー性の高い情報を取り扱うにあたり; 個人情報保護の取り組みついて現在まで検討を進めている公開方式等についても併せて報告する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2004,3
How to capture absolute human skeletal posture,Shoichiro Iwasawa; Kiyoshi Kojima; Kenji Mase; Shigeo Morishima,Abstract Commercially available motion capture products give us fairly precise movementsof human body segments but do not measure enough information to define skeletal posturein its entirety. This sketch describes how to obtain the complete posture of skeletal structurewith the help of marker locations relative to bones that are derived from MRI data sets.,ACM SIGGRAPH 2003 Sketches & Applications,2003,3
空間共有コミュニケーションの実験システム: BEOEB (感性情報処理および一般),望月研二， 山田邦男， 岩澤昭一郎， 吉田俊介， 鳥羽美奈子， 相澤清晴， 森島繁生， 齊藤隆弘,抄録 通信・放送メディアなどの映像メディアでは; 画像情報を 2 次元から; より臨場感のある 3次元映像の様な高い次元の情報とする要求が高くなってきている. 本郷空間共有リサーチセンタでは; 人類の夢の画像コミュニケーションである 「空間共有コミュニケーション」 の実現へ向けて;視点からの距離に依存した 3 層構造の表現をベースにした実写画像等を用いてリアルな仮想空間を共有する技術について提案した. 今回は; このコミュニケーションの場の実現を目指した各要素技術について説明すると共にこれを実現する実験システム BEOEB について述べる.,映像情報メディア学会技術報告 26.70,2002,3
高速度カメラによる動的な顔面表情の分析および合成,四倉達夫， 内田英子， 山田寛， 赤松茂， 鉄谷信二， 森島繁生,本稿では; 人間が自然な表情した場合: 自発表出と典型的な表情を演じる際の顔表情:演技表出を撮影し; 顔の各部位に設定した特徴点の変位量に基づき顔の動きの定量的な測定を高速度カメラを用いて分析した. また測定結果から CG によって構築した顔モデルのアニメーション生成を行った. 自発表出条件; 演技表出条件ともに顔の各部位の動き出しの差は微細であり高速度カメラを用いたことの有効性が示された. また情動ごとおよび表出条件ごとに顔の動き量や速さに特徴的な違いが認められたが; 動きの変化そのものの様相には興味深い共通性が認められた.顔モデルのアニメーションに関しても; 線形補間によるキーフレームアニメーションと比べより自然な顔表情表出が可能となった.,*,2002,3
Multi-modal translation and evaluation of lip-synchronization using noise added voice,Shigeo Morishima; Satoshi Nakamura,ABSTRACT Speech-to-speech translation has been studied to realize natural humancommunication beyond language barriers. Toward further multi-modal naturalcommunication; visual information such as face and lip movements will be necessary. In thispaper; we introduce a multi-modal English-to-Japanese and Japanese-to-Englishtranslation system that also translates the speaker's speech motion while synchronizing it tothe translated speech. To retain the speaker's facial expression; we substitute only thespeech organ's image with the synthesized one; which is made by a three-dimensional wire-frame model that is adaptable to any speaker. Our approach enables image synthesis andtranslation with an extremely small database. We conduct subjective evaluation tests usingthe connected digit discrimination test using data with and without audio-visual lip …,Proceedings of AAMAS 2002 workshop: Embodied conversational agents-let’s specify and evaluate them,2002,3
Multi-modal translation system and its evaluation,Shigeo Morishima; Satoshi Nakamura,Speech-to-speech translation has been studied to realize natural human communicationbeyond language barriers. Toward further multi-modal natural communication; visualinformation such as face and lip movements will be necessary. We introduce a multi-modalEnglish-to-Japanese and Japanese-to-English translation system that also translates thespeaker's speech motion while synchronizing it to the translated speech. To retain thespeaker's facial expression; we substitute only the speech organ's image with thesynthesized one; which is made by a three-dimensional wire-frame model that is adaptableto any speaker. Our approach enables image synthesis and translation with an extremelysmall database. We conduct subjective evaluation tests using the connected digitdiscrimination test using data with and without audio-visual lip-synchronization. The …,Multimodal Interfaces; 2002. Proceedings. Fourth IEEE International Conference on,2002,3
A Micro-Temporal Analysis of Facial Movements in Spontaneously Elicited and Posed Expressions of Emotion with the Use of a High-Speed Camera,H Yamada; H Uchida; T Yotsukura; S Morishima; N Tetsutani; S Akamatsu,*,IEICE; HCS2000-60,2001,3
Face-to-face Communication in Cyberspace using Analysis and Synthesis of Facial Expression,Shigeo Morishima,ABSTRACT Recentlycomputer can muke cyberspuce towalk through by an interactivcvinuulreulity technique. An avatar in cybersp. iicc cun bring us u virtual face-to-fuce communicationenvironment.[nthis paper. an avatar is realized which httsa reu] faceincyberspace and amulti-user coniinunication system isconstructed by voicc transmission threugh network;Nbicel'rommicrophone istrunsmittedund analyzed; then mouth shape and faciul cxpressionof avatar are synchronous] y estimu {ed and synthesized on real time. And a] so anentertuinn] enl application of u real-time voice drivensynthetic faceis introduccdand this isanexampte ot'intcractivemovie. Finally; facemotion capture system using physicsbasedfacemode [isintroduced. or heroin; Thefacepartof movie star infamous movie filmisreplacedwith user's own faceand this faceis controlled interactive] y by user's voice and a few …,ITE Technical Report 23.3,1999,3
Real Time Face-to-Face Communication System in Cyberspace Using Voice Driven Avatar with Texture Mapped Face,T Yotsukura; E Fujii; S Morishima,*,Transactions of Information Processing Society of Japan,1999,3
空間周波数に基づく顔器官の形状認識と再合成,武藤淳一， 藤井英史， 森島繁生,空間周波数成分を用いて顔表情の認識と再合成を実時間で行うシステムを提案する.画像から自動的にトラッキングされた目・口周辺の正方領域について; 高速フーリエ変換を実時間で行い空間周波数成分を求める. 次にこの帯域パワーから顔器官の形状; ここでは FACS に基づくAU のパラメータ値をニューラルネットワークを用いて推定する. 実際にこの推定結果から;顔表情を再合成して原画像との印象を比較した結果; 学習には用いていない表情に対しても;原画像と類似した印象を再合成することが可能となった. これにより; 瞬きや口の開き;目の開き工合などが忠実にトラッキングすることができる. したがって; マーカー等を顔面に添付することなく; 非装着; 非接着で表情の印象レベルでのモーションキャプチャを実現することが可能となった.,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),1998,3
ヒューマンコンピュータインタラクションのための音声から画像へのリアルタイムメディア変換,宮下直也， 坂口竜己， 森島繁生,デルの頂点数が多ければ;. 忠実な形状モデルの再現が可能となるが; その反面;計算コストが大きくなる. そこで; 本稿では; 人物に忠実で; 変形が容易で; 変形処理にコストのかからない標準モデルを構築した. そのモデルを図 2 に示ず. このモデルには;歯のモデルが付加されており; 歯のモデルは 3 次元レーザスキャナより歯の型をスキャンすることで構築した. このモデルにテクスチャマッピングを施すことで合成画像を作成する. 1.はじめに人間同様の顔を持つエージェント (擬人化エージェント) がヒューマンインタフェース分野のホットなトピックとなっている. これは; あたかも人と人とが直接; 接してい,インタラクション'97 論文集,1997,3
3 次元感情モデルに基づく表情分析・合成システムの構築,川上文雄， 山田寛， 原島博， 森島繁生,抄録 これまで 5 層ニューラルネットの優れた恒等与像能力を用いて人間の顔に表出される感情状態を空間的に表現しこの空間を 「3 次元感情空間」(3 次元感情モデル) と呼ぶことで表情から感情空間及び; 感情空間から表情への両写像を実現するためのシステムの構築を行った.しかしながら; 感情空間から表情への写像にはその心理学的妥当性がないこと;感情空間への入力は表情パラメータであることや例え表情から感情空間への写像を実現したとしても; その心理学的意味が明解でないことなど多くの問題を抱えていた. そこで; 感情空間の心理学的妥当性を確認するために心理学実験を行い; 入力顔画像から表情パラメータの推定を行った上でユーザの顔画像から感情状態を把握可能なシステムの構築を行った.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1996,3
3 次元計測に基づく顔表情変化の分析と合成,坂口竜己， 森島繁生， 大谷淳， 岸野文郎,抄録 よりユーザフレンドリーなコンピュータとのコミュニケーション環境実現のため;顔表情動画像を用いたインタフェース構築の研究を進めている. 筆者らはすでにモデルベース手法を応用した表情動画像の作成について提案しているが; この表情変形規則は 2次元的な計測を基に作られたものであったため; 満足な性能は得られていなかった. 本稿では;顔表面の 3 次元計測により; 各表情表出時の顔面皮膚の移動量を求め; 新たな移度制御点(特徴点) の設定と移動規則の決定を行なっている. 3 次元計測では正面・側面画像を利用する手法を採用し; 誤差±1.2% 程度の精度を得ている. 更に得られた特徴点位置についての測定結果よりFACS の AU の定量化を見直し; 特徴点以外の点の補間法を検討してより自然な画像合成を行なっている.,電子情報通信学会技術研究報告. HC; ヒューマンコミュニケーション,1994,3
表情の認識-工学の立場から,森島繁生,Facial expresslons include multidirnensional factors as some psychologists suggest; so it'snot easy for us to handle them. We presented the methods to construct a facial expressionspace; vhich can make a human to handle a facial expression easily. By using a multi-layered feed―for; vard neural net; vork; vhich has an identity mapping; a non-linearrnapping and a generaliza‐tion performance; the dilnension of the expression space can bereduced to 3-D which can repre も ent all of the face categories. It's called as θ―五) 五レタタθ ″θ% 砂 α び夕. ThiS systern can also realize analysis and synthesis of facial expressionsilnultaneously. Because the mapping from input to 3-D space means an analysis processand the mapping from 3-D space to output corresponds to a synthesis process. The trainingof neural network is based on only 6 basic expressions because all of the faces are …,Medical Imaging Technology,1994,3
Extraction of Emotional States in Speech,H Hiraga; Y Saitou; Shigeo MoriShima; H Harashima,*,IEICE; HC93-66,1994,3
自然な表情合成のための頭部高精細ワイヤフレームの構成とその階層的制御について,上野雅俊， 小野英太， 森島繁生， 原島博,概要人物の頭部形状を 3 次元モデルによって近似し; これに人物から得られたテクスチャを賠り付けることによって; 人間の表情や会話シーンなどのリアルな画像を合成する研究を進めている.本論文では; 対象人物の頭部形状をより忠実に表現できる高精細 3 次元モデルを構成し;このモデルと顔画像の分析・合成の分野で広く用いられている正面顔画像用の 3次元モデルとのリンクを取ることによって; 従来まで研究されてきた表情・口形合成の成果をそのまま活かしながら階層的に高精度モデルを制御し; より自然な顔画像を合成する手法について述べる.,情報処理学会研究報告グラフィクスと CAD (CG),1992,3
A soundtrack generation system to synchronize the climax of a video clip with music,Haruki Sato; Tatsunori Hirai; Tomoyasu Nakano; Masataka Goto; Shigeo Morishima,In this paper; we present a soundtrack generation system that can automatically add asoundtrack with the length and climax points aligned to those of a video clip. Adding asoundtrack to a video clip is an important process in video editing. Editors tend to addchorus sections to the climax points of the video clip by replacing and concatenating musicalsegments. However; this process is time-consuming. Our system automatically detectsclimaxes of both the video clips and music based on feature extraction and analysis. Thisenables the system to add a soundtrack in which the climax is synchronized to the climax ofthe video clip. We evaluated the generated soundtracks through a subjective evaluation.,Multimedia and Expo (ICME); 2016 IEEE International Conference on,2016,2
Region-of-interest-based subtitle placement using eye-tracking data of multiple viewers,Wataru Akahori; Tatsunori Hirai; Shunya Kawamura; Shigeo Morishima,Abstract We present a subtitle-placement method that reduces viewer's eye movementwithout interfering with the target region of interest (ROI) in a video scene. Subtitles helpviewers understand foreign-language videos. However; subtitles tend to attract viewers' lineof sight; which cause viewers to lose focus on the video content. To address this problem;previous studies have attempted to improve viewer experience by dynamically shiftingsubtitle positions. Nevertheless; in their user studies; some participants felt that the visualappearance of such subtitles was unnatural and caused them fatigue. We propose a methodthat places subtitles below the ROI; which is calculated by eye-tracking data from multipleviewers. Two experiments were conducted to evaluate viewer impression and compare lineof sight for videos with subtitles placed by the proposed and previous methods.,Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video,2016,2
Computational cartoonist: a comic-style video summarization system for anime films,Tsukasa Fukusato; Tatsunori Hirai; Shunya Kawamura; Shigeo Morishima,Abstract This paper presents Computational Cartoonist; a comic-style anime summarizationsystem that detects key frame and generates comic layout automatically. In contract toprevious studies; we define evaluation criteria based on the correspondence betweenanime films and original comics to determine whether the result of comic-stylesummarization is relevant. To detect key frame detection for anime films; the proposedsystem segments the input video into a series of basic temporal units; and computes frameimportance using image characteristics such as motion. Subsequently; comic-style layoutsare decided on the basis of pre-defined templates stored in a database. Several resultsdemonstrate the efficiency of our key frame detection over previous methods by evaluatingthe matching accuracy between key frames and original comic panels.,International Conference on Multimedia Modeling,2016,2
RSViewer: An Efficient Video Viewer for Racquet Sports Focusing on Rally Scenes.,Shunya Kawamura; Tsukasa Fukusato; Tatsunori Hirai; Shigeo Morishima,Abstract: This paper presents RSViewer; a video browsing system specialized for racquetsports; which reflects users' interests. Methods to support users in browsing racquet sportsmatches by summarizing video composed of important rally shots have been discussed in aprevious study. However; the method is not practical enough because the auditory eventsshould be manually annotated in advance to detect such scenes. Therefore; we propose anautomatic rally shot detection based on shot clustering method using white line detection.Our system calculates the importance of rally shots based on audio features. As the result;the summarized video can facilitate users find and review the information they need. Theresult of experiments shows that our method is effective in an aspect of efficient videobrowsing experience. Furthermore; we propose a high-speed playback method …,VISIGRAPP (2: IVAPP),2016,2
Wrinkles individuality representing aging simulation,Pavel A Savkin; Daiki Kuwahara; Masahide Kawai; Takuya Kato; Shigeo Morishima,Abstract An appearance of a human face changes due to aging: sagging; spots; lusters; andwrinkles would be observed. Therefore; facial aging simulation techniques are required forlong-term criminal investigation. While the appearance of an aged face varies greatly fromperson to person; wrinkles are one of the most important features which represent thehuman individuality. An individuality of wrinkles is defined by wrinkles shape and position.,SIGGRAPH Asia 2015 Posters,2015,2
A music video authoring system synchronizing climax of video clips and music via rearrangement of musical bars,Haruki Sato; Tatsunori Hirai; Tomoyasu Nakano; Masataka Goto; Shigeo Morishima,Abstract This paper presents a system that can automatically add a soundtrack to a videoclip by replacing and concatenating an existing song's musical bars considering a user'spreference. Since a soundtrack makes a video clip attractive; adding a soundtrack to a clip isone of the most important processes in video editing. To make a video clip more attractive;an editor of the clip tends to add a soundtrack considering its timing and climax. Forexample; editors often add chorus sections to the climax of the clip by replacing andconcatenating musical bars in an existing song. However; in the process; editors should takenaturalness of rearranged soundtrack into account. Therefore; editors have to decide how toreplace musical bars in a song considering its timing; climax; and naturalness of rearrangedsoundtrack simultaneously. In this case; editors are required to optimize the soundtrack …,ACM SIGGRAPH 2015 Posters,2015,2
Dance Motion Segmentation Method based on Choreographic Primitives.,Narumi Okada; Naoya Iwamoto; Tsukasa Fukusato; Shigeo Morishima,Abstract: Data-driven animation using a large human motion database enables theprograming of various natural human motions. While the development of a motion capturesystem allows the acquisition of realistic human motion; segmenting the captured motioninto a series of primitive motions for the construction of a motion database is necessary.Although most segmentation methods have focused on periodic motion; eg; walking andjogging; segmenting non-periodic and asymmetrical motions such as dance performance;remains a challenging problem. In this paper; we present a specialized segmentationapproach for human dance motion. Our approach consists of three steps based on theassumption that human dance motion is composed of consecutive choreographic primitives.First; we perform an investigation based on dancer perception to determine segmentation …,GRAPP,2015,2
Affective Music Recommendation System Based on the Mood of Input Video,Shoto Sasaki; Tatsunori Hirai; Hayato Ohya; Shigeo Morishima,Abstract We present an affective music recommendation system just fitting to an input videowithout textual information. Music that matches our current environmental mood canenhance a deep impression. However; we cannot know easily which music best matchesour present mood from huge music database. So we often select a well-known popular songrepeatedly in spite of the present mood. In this paper; we analyze the video sequence whichrepresent current mood and recommend an appropriate music which affects the currentmood. Our system matches an input video with music using valence-arousal plane which isan emotional plane.,International Conference on Multimedia Modeling,2015,2
MusicMean: fusion-based music generation,Tatsunori Hirai; Shoto Sasaki; Shigeo Morishima,*,Proceedings of SMC,2015,2
Real-time hair simulation on mobile device,Zhuopeng Zhang; Shigeo Morishima,Abstract Hair rendering and simulation is a fundamental part in the representation of virtualcharacters. But intensive calculation for the dynamic on thousands of hair strands makes thetask much challengeable; especially on a portable device. The aim of this short paper is tosolve the problem of how to perform real-time hair simulation and rendering on mobiledevice. In this paper; the process of hair simulation and rendering is adapted according tothe property of mobile device hardware. To increase the number of hair strands ofsimulation; we adopted the Dynamic follow-the-leader (DFTL) method and altered it by ournew method of interpolation. We also pictured a rendering strategy basing on the survey ofthe limitation of mobile GPU. Lastly we present an innovational method that carried out orderindependent transparency at a relatively inexpensive cost.,Proceedings of Motion on Games,2013,2
Automatic mash up music video generation system by remixing existing video content,Hayato Ohya; Shigeo Morishima,Music video is a short film which presents a visual representation of recent music. In thesedays; there is a trend that amateur users create music video in the video sharing website.Especially; the music video which is created by cutting and pasting existing video is calledmashup music video. In this paper; we proposed the system that users can easily createmushup music video by using existing music videos. In addition; we conducted assessmentevaluation experiment for our system. The system firstly extracts music features and videofeatures from existing music videos. Then; the each feature is clustered and the relationshipbetween each feature is learned by Hidden Markov Model. At last; the system cuts learnedvideo scene which is the closest feature among learned videos and pastes it synchronizingwith input song. Experiment shows that our method can generate more synchronized …,Culture and Computing (Culture Computing); 2013 International Conference on,2013,2
Affective Music Recommendation System Reflecting the Mood of Input Image,Shoto Sasaki; Tatsunori Hirai; Hayato Ohya; Shigeo Morishima,We present an affective music recommendation system using input images without textualinformation. Music that matches our current mood can create a deep impression. However;we do not know which music best matches our present mood. As it is difficult to select musicmanually; we need a recommendation system that can operate affectively. In this paper; weassume that there exists a relationship between our mood and images because visualinformation affects our mood when we listen to music. Our system matches an input imagewith music using valence-arousal plane which is an emotional plane.,Culture and Computing (Culture Computing); 2013 International Conference on,2013,2
既存音楽動画の再利用による音楽に合った動画の自動生成システム,平井辰典， 大矢隼士， 森島繁生,概要: 本論文では; 任意の入力楽曲を基に; 既存の音楽動画コンテンツを再利用し;音楽と映像が同期した音楽動画を自動生成するシステムを提案する. 本研究では;まずシステムの土台となる音楽と映像の同期手法を主観評価実験により検討した.その結果に基づき; 音のエネルギーを表す RMS の変化に; 映像のアクセント (明滅や動きなど)を対応させるような音楽動画自動生成システムを実装した. 音楽動画の自動生成の手順は以下のとおりである. まずデータベースの構築として既存の音楽動画の各フレームにおける明滅;動きに関する映像特徴量の計算を行う. そして; 動画生成として; 入力楽曲の RMS を抽出し;その推移に最も近い推移を示す映像特徴量を持つ音楽動画の素片をデータベースから探索し;それらの映像を切り貼りすることで; 音楽に最も同期した音楽動画の生成を行う. また;本システムによる生成動画の評価も行った.,情報処理学会論文誌,2013,2
Video-Realistic Inner Mouth Reanimation,Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,We propose a novel post-effect method that can make an existing speech animation video-realistic by generating an inner mouth appearance that is tailored to the speaker. Theautomatic generation of photorealistic inner mouth appearances requires only simple inputsand small databases and is not restricted by video lighting. The approach is also applicablefor creature speech animation with human voices. Our system uses two key algorithms; oneof the algorithms models the inner mouth appearances based on physical assumptions; andthe other algorithm synthesizes the inner mouth images with care taken to maintain timecontinuity and luminance gradients.,*,2013,2
Automatic feature point detection using linear predictors with facial shape constraint,T Matsuda; T Hara; A Maejima; S Morishima,*,IEICE Trans.,2012,2
シーンの連続性と顔類似度に基づく動画コンテンツ中の同一人物登場シーンの同定,平井辰典， 中野倫靖， 後藤真孝， 森島繁生,抄録 We present a method that can automatically annotate when and who is appearing in avideo stream that is shot in an unstaged condition. Previous face recognition methods werenot robust against different shooting conditions; such as those with variable lighting; facedirections; and other factors; in a video stream and had difficulties identifying a person andthe scenes the person appears in. To overcome such difficulties; our method groupsconsecutive video frames (scenes) into clusters that each have the same person's face;which we call a “facial-temporal continuum;” and identifies a person by using many videoframes in each cluster. In our experiments; accuracy with our method was approximately twoor three times higher than a previous method that recognizes a face in each frame.,映像情報メディア学会誌,2012,2
Real time ambient occlusion by curvature dependent occlusion function,Tomohito Hattori; Hiroyuki Kubo; Shigeo Morishima,Abstract We present the novel technique to compute ambient occlusion [2008] on real-timegraphics hardware. Because current real-time ambient occlusion techniques like SSAOneed at least 16 rays sampling and too high computational cost to implement on computergames. Our method approximates occlusion as a local illumination model by introducingcurvature-dependent function.,SIGGRAPH Asia 2011 Posters,2011,2
表出過程の印象を考慮したより自然な笑顔動画像の合成 (一般;『コミュニケーションの身体を捉える』 及び一般),藤代裕紀， 前島謙宣， 森島繁生,抄録 顔の表情は; 人と人との円滑なコミュニケーションにおいて重要な役割を担っている.特に笑顔は相手に対して肯定的な印象を与えるため様々な研究が行われている.特に笑顔が相手に与える印象の研究は多く; そのほとんどが静止画での研究であった.しかし最近では; 表出後のみだけでなく; 表出過程が相手に与える印象に大きく影響するとの指摘もある. そこで本研究では; 笑いの自然さに注目し; 表出過程においてどの部位の動きが笑いの自然さの印象に寄与するかを調べた. また; 分析結果に基づく顔部位変形を適用し; オリジナルよりも自然さを強調した合成動画像を作成して; 主観評価により分析結果の正当性を示した.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2011,2
曲率に依存する反射関数を用いた半透明物体の高速レンダリング,久保尋之， 土橋宜典， 森島繁生,CG を用いて人間の肌や大理石に代表される半透明物体を表現するために; 物体内部で生じる表面下散乱現象を考慮することは有効な手段といえる. しかし表面下散乱の物理的に正確なモデル化には; 大域照明モデルを導入せざるを得ず; 実時間でのレンダリングにはほど遠いのが現状である.表面下散乱による影響は; 物体表面の起伏の激しい点において特に顕著であると考えられる.そこで本研究では物体表面の起伏の強さを表す指標として; 曲率を導入する. 曲率は物体表面上で局所的に決定されるパラメータであるため; 局所照明モデルが適用可能であり;高速なレンダリングが実現される. まず与えられた物体と同じ物質パラメータを有する半径 rの球について; 単一の白色な平行光源下で球表面の放射輝度分布を推定する. 半径 rの球面の曲率は 1/r で表されるため; 様々な半径の球面に対する放射輝度分布を調査することにより; 曲率 κ; 光源と法線とのなす角 θ i に対する反射関数 gr (θ i; κ) をルックアップテーブル …,電子情報通信学会論文誌 A,2010,2
Learning arm motion strategies to recover balance in bipedal robots,Masaki Nakada; Brian F Allen; Shigeo Morishima; Demetri Terzopoulos,*,Intl Symposium on Learning and Adaptive Behavior in Robotic Systems; Canterbury; UK,2010,2
Example based skinning with progressively optimized support joints,Kentaro Yamanaka; Akane Yano; Shigeo Morishima,Abstract Skeleton-Subspace Deformation (SSD); which is the most popular method forarticulated character animation; often causes some artifacts. Animators have to edit mesheach time; which is seriously tedious and time-consuming. So example based skinning hasbeen proposed. It employs edited mesh as target poses and generates plausible animationefficiently. In this technique; character mesh should be deformed to accurately fit targetposes. Mohr et al.[2003] introduced additional joints. They expect animators to embedskeleton precisely.,ACM SIGGRAPH ASIA 2009 Posters,2009,2
Expressive facial subspace construction from key face selection,Ryo Takamizawa; Takanori Suzuki; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Abstract MoCap-based facial expression synthesis techniques have been applied to provideCG character with expressive and accurate facial expressions [Deng et al. 2006: Lau et al.2007]. The representative performance of these techniques depends on the variety ofcaptured facial expressions. It is also difficult to guess what expressions are needed tosynthesize expressive face before capture. Therefore; much MoCap data are required toconstruct a subspace employing dimensional compression techniques; and then the spaceenables us to synthesize expressions with linear-combination of basis vectors of the space.However; it is hard work to take much facial MoCap data to obtain expressive result.,SIGGRAPH'09: Posters,2009,2
Automatic voice assignment tool for Instant Casting movie System,Yoshihiro Adachi; Shinichi Kawamoto; Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,In Instant Casting movie System; a personal CG character is automatically generated. Thecharacter resembles a participant in a face geometry and texture. However; the voice of acharacter was an alternative voice determined by the gender of the participant. Thereforesometimes it's not enough to identify the personality of a CG character. In this paper; anautomatic pre-scored voice assignment tool for a personal CG character is presented. Voiceis essential to identify a personal character as well as a face feature. Our proposed systemselects the most similar voice to the participants from voice database; and assigns it as avoice of CG character. Voice similarity criterion is presented by combination of eight acousticfeatures. After assigning voice data to a personal character; the voice track is played back insynchronization with the movement of the CG character. 60 voice variations have been …,Acoustics; Speech and Signal Processing; 2009. ICASSP 2009. IEEE International Conference on,2009,2
個人の音声を反映する映像エンタテインメントシステム,足立吉広， 大谷大和， 川本真一， 四倉達夫， 森島繁生， 中村哲,視聴者の顔を CG で再現し; CG キャラクタとして映画に登場させる Future Cast System (FCS)を改良し; 視聴者の声の特徴をそのキャラクタの台詞音声へ反映させ; キャラクタの顔と声の一致度を向上させて音声を出力するシステムを構築する. あらかじめ構築した話者データベースから視聴者の知覚的類似話者を選出し; その話者の台詞音声を視聴者のキャラクタに割り当て;短時間で台詞音声を映像と同期出力するシステムを提案する. 知覚的類似話者は;個人性の知覚と関係があると報告されている 8 つの音響特徴量による距離の線形結合を用いて推定する. 声優による 60 種類の声質の台詞音声データベースを用いた音声出力同期システムを構築し; 視聴者のキャラクタの顔と選択された音声の一致度に関して 5 段階の主観評価を行った.登場者数と話者データベースの規模; および類似話者の許容度の関係を予備実験により調査し;実験条件にあてはめたところ; 予想される許容度約 51% に対して主観実験値において 35% の …,情報処理学会論文誌,2008,2
Preliminary evaluation of the audience-driven movie,Tao Lin; Akinobu Maejima; Shigeo Morishima; Atsumi Imamiya,Abstract In this paper we introduce an audience-driven theater experience; DIM Movie; inwhich audience participates in a pre-created CG movie as its roles; and report the subjectiveand physiological evaluations for the audience experience offered by DIM movie.Specifically; we present three different experiences to an audience-a traditional movie; itsSelf-DIM (SDIM) version with the audience's participation; and its Self-Friend-DIM (SFDIM)version with co-participation of the audience and his friends. The evaluation results showthat the DIM movies (SDIM and SFDIM) elicit greater subjective sense of presence;engagement; and emotional reaction; and stronger physiological response (galvanic skinresponse; GSR) as compared with the traditional movie form; moreover; audiences show aphasic GSR increase responding to the appearance of their own or friends' CG characters …,CHI'08 Extended Abstracts on Human Factors in Computing Systems,2008,2
Directable and Stylized Hair Simulation.,Yosuke Kazama; Eiji Sugisaki; Shigeo Morishima,Abstract: Creating natural looking hair motion is considered to be one of the most difficultand time consuming challenges in CG animation. A detailed physics-based model isessential in creating convincing hair animation. However; hair animation created usingdetailed hair dynamics might not always be the result desired by creators. For this reason; ahair simulation system that is both detailed and editable is required in contemporaryComputer Graphics. In this paper we therefore; propose the use of External Force Field(EFF) to construct hair motion using a motion capture system. Furthermore; we havedeveloped a system for editing the hair motion obtained using this process. First; theenvironment around a subject is captured using a motion capture system and the EFF isdefined. Second; we apply our EFF-based hair motion editing system to produce creator …,GRAPP,2008,2
Variable rate speech animation synthesis,Akane Yano; Hiroyuki Kubo; Yoshihiro Adachi; Demetri Terzopoulos; Shigeo Morishima,Abstract Speech animation has traditionally been achieved by two main approaches: image-based methods [Ezzat et al. 2002] and key-framing methods [Cohen and Massaro 1993].Both approaches require large databases that include many images or 3D shapes withfacial expressions and speech lip shapes.,ACM SIGGRAPH 2007 posters,2007,2
Interactive shade control for cartoon animation,Yohei Shimotori; Hidehito Nakajima; Eiji Sugisaki; Akinobu Maejima; Shigeo Morishima,Abstract Shade in traditional cel animation is an essential element and plays a symbolic rolein the artistic portrayal of character and scene. The shade on a character's face can be usedto show emotion such as anger; sadness and hatred. Despite its usability; shade is relativelyquickly or roughly drawn in animation for the following reasons. Firstly; cel animators requirea large amount of time to draw shade. Secondly; because of the time requirements;producing complicated shade is expensive. On the other hand; 3D models enable users tomore easily render or recreate shade than cel animation techniques. Consequently; 3Dmodels are being gradually introduced into cartoon animation. Using Phong's 3D shadingmodel; however; shade is rendered too realistically for cartoon animation; and is thereforenot appropriate for traditional-style cel animation. To solve similar problems with …,ACM SIGGRAPH 2007 posters,2007,2
レンジデータに整合された顔モデル 3 次元座標の PCA による大規模データベース対応型顔認証システム,野中悠介， 山名信弘， 井辺昭人， 三浦文裕， 前島謙宣， 森島繁生,抄録 本稿では; 顔の 3 次元形状を利用した認証システムを提案する. 顔の 3次元形状に対して整合された 3 次元メッシュモデルの頂点座標に対して主成分分析を行い;その主成分得点を特徴量として使用する. 本システムでは; まず主成分得点に対して演算量の削減の為に各主成分納上で正規分布を仮定した不均一量子化を行い; 識別を行う.次に識別された候補に対して; ガウス関数の積分値を距離尺度として認証を行う. 2782 人の 3次元顔形状データベースを用いて全員を登録者とする認証実験を行った結果; EER は 4.53 (%)となった. また; 登録人数を 15 人に削減した場合では EER が 2.32 (%) となり;高い精度での認証を実現できた.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2007,2
Anime hair motion design from animation database,Eiji Sugisaki; Yosuke Kazama; Shigeo Morishima; Natsuko Tanaka; Akiko Sato,Abstract This paper describes a new method to animate anime-like hair motion that allowsusers to use existing cel character animation sequences. We demonstrate how to createcartoon hair animation accentuated in anime-like motions. The novelty of this approach isthat users can extract the motion from the existing cel animation such as windy scene ormotion of grass and fire etc. Then; they can apply the extracted motion to other characterswith a three-dimensional structure. In fact; users can re-use existing anime sequences asinput to animate another character as if both characters existed in the same environment.Also; creating abundant hair motion database from various animator's characteristic hairmotion enables users to create the hair motion that has user's favorite kind of the animator'scharacteristic even though a user is not an animator.,Proceedings of the 2006 international conference on Game research and development,2006,2
Facial animation by the manipulation of a few control points subject to muscle constraints,Hiroyuki Kubo; Hiroaki Yanagisawa; Akinobu Maejima; Demetri Terzopoulos; Shigeo Morishima,Muscle-based facial animation [Waters 1987; Lee et al. 1995] is one of the best approachesto realizing a realistic; lifelike character. However; the optimal control of each muscle togenerate facial animation is complicated. Appropriate muscle actions are usuallydetermined manually by trial and error. We introduce a technique for synthesizing realisticfacial animation with a variety of facial expressions by automatically estimating facial muscleparameters through the manipulation of only a few control points on the face. We specify thecontrol points in accordance with the movement of 3D markers obtained by a motion capturesystem. We also propose a facial expression cloning method that transfers an actor's muscleparameters to another character.,ACM SIGGRAPH 2006 Research posters,2006,2
フューチャーキャストシステムの舞台裏と今後の展開 (ソーシャルインタラクション及び一般),森島繁生,抄録 本稿では; 愛知万博の三井・東芝パビリオンにおいて世界に先駆けて具現化された; 全く新しいエンタテインメントシステムであるフューチャーキャストシステムの全容について述べ;実現するまでのプロセスと今後のビジョンについて述べる. このシステムでは; 観客全員 (240 名) が;映画の登場人物に扮してスクリーンに登場し; 本人そっくりのキャラクタが台詞を喋り;感情を表現しながらストーリーが展開していくものである. 顔の 3 次元形状計測から;個人のワイヤフレーム生成; さらにリアルタイムレンダリングによる映像生成までを完全自動で実行させる. 9 月来場者により; システム評価を実施したところ; 平均で 93.4% の人物が実際に映像に登場していたことが分かった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2006,2
フューチャーキャストシステムの舞台裏と今後の展開 (ソーシャルインタラクション及び一般),森島繁生,抄録 本稿では; 愛知万博の三井・東芝パビリオンにおいて世界に先駆けて具現化された; 全く新しいエンタテインメントシステムであるフューチャーキャストシステムの全容について述べ;実現するまでのプロセスと今後のビジョンについて述べる. このシステムでは; 観客全員 (240 名) が;映画の登場人物に扮してスクリーンに登場し; 本人そっくりのキャラクタが台詞を喋り;感情を表現しながらストーリーが展開していくものである. 顔の 3 次元形状計測から;個人のワイヤフレーム生成; さらにリアルタイムレンダリングによる映像生成までを完全自動で実行させる. 9 月来場者により; システム評価を実施したところ; 平均で 93.4% の人物が実際に映像に登場していたことが分かった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2006,2
Face authentication system using 3D frequency component of moving image,N Yamana; A Inbe; F Miura; A Maejima; S Morishima,*,*,2006,2
特徴点の 3 次元情報を利用した顔認証システムの構築 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),井辺昭人， 佐藤康之， 前島謙宣， 森島繁生,抄録 本稿では; 特徴点の 3 次元情報を利用した顔認証システムを提案する. レンジスキャナから得られた 3 次元データを個人認証に利用することで認証精度は向上する. しかし; 顔全体の 3次元データを個人認証に使用した場合データ量や計算量が膨大になる. そこで本研究では顔の各部位に特徴点を定義し; 特徴点の 3 次元情報を使用することでデータ量の削減を行い; 特徴点の 2次元データによる認証精度と 2 次元に奥行きを付加した 3 次元データによる認証精度を比較することによって顔の 3 次元データの優位性を検証した. また; 特徴点に対して分散分析を行い;分散比の大きい特徴点を個人認証に使用することにより認証精度を維持したまま更なるデータ量の削減に対する検討を行った.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2005,2
Bone Extraction using MRI and High Realistic Motion Capturing of Bone and Joint,Syouhei Nishimura; Kiyoshi Kojima; Syouichiro Iwasawa; Shigeo Morishima,*,Digital Contents Symposium,2005,2
Anthropomorphic Dialog Agent Development Tool Using Facial Image Synthesis and Lip Synchronization,Shigeo MORISHIMA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Human-Computer Interaction (Part2),2003,2
HAI におけるエージェントのリアリティとコミュニケーションギャップ (< 特集> HAI: ヒューマンエージェントインタラクション),森島繁生,マルチモーダルを駆使して人間と機械がフェーストウフェースで対話する環境を実現しようとする試みは; ポスト GUI に向けて特に注目を集める分野となっている. エージェントのリアリテイに関する議論もしばしば行われるが; カートウーンキャラクタのような本来の動きというものがもともと存在せず架空に仕立て上げられたものに多くの人々が愛着を覚えるのに比して; 人間そのもののクローンを実現する技術は実物が存在するがゆえに要求される条件も自ずと厳しくなり; わずかな粗も無視することはできない. したがって; 現時点では HAI の人物描写において生身の人間と同等のリアリテイを追及し; まさにビリーバブルなエージェントを実現するには; まだまだ年月を要するであろうし;音声認識・合成・対話技術とのバランスも考慮しなくてはならない. 本稿で述べる HAIのための人物描写は; あらかじめオリジナルのオンラインもしくはオフラインの映像が存有三することを前提としている. イメージベースレンダリングやビデオリライトとも関連するが; 完全にイメージ …,人工知能学会誌,2002,2
Magical face: integrated tool for muscle based facial animation,Tatsuo Yotsukura; Mitsunori Takahashi; Shigeo Morishima; Kazunori Nakamura; Hirokazu Kudoh,Abstract In recent years; tremendous advances have been achieved in the 3D computergraphics used in the entertainment industry; and in the semiconductor technologies used tofabricate graphics chips and CPUs. However; although good reproduction of facialexpressions is possible through 3D CG; the creation of realistic expressions and mouthmotion is not a simple task.,ACM SIGGRAPH 2002 conference abstracts and applications,2002,2
高速度カメラを用いた顔面動作の分析および表情合成,四倉達夫， 内田英子， 山田寛， 赤松茂， 鉄谷信二， 森島繁生,抄録 本稿では; 高速度カメラを用いて人間が自然な表情した場合 (自発表出)と普遍的かつ典型的な表情を演じる際の顔表情 (演技表出) を撮影し; 顔の各部位に設定した特徴点の変位量に基づき顔の動きの定量的な測定を分析した. また測定結果から CGによって構築した顔モデルのアニメーション生成を行った. 自発表出条件; 演技表出条件ともに顔の各部位の動き出しの差は微細であり高速度カメラを用いたことの有効性が示された.また情動ごとおよび表出条件ごとに顔の動き量や速さに特徴的な違いが認められたが;動きの変化そのものの様相には興味深い共通性が認められた. 顔モデルのアニメーションに関しても; 線形補間によるキーフレームアニメーションと比べより自然な顔表情表出が可能となった.,映像情報メディア学会技術報告 25.58,2001,2
Three-dimensional image capturing and representation for multimedia ambiance communication,Tadashi Ichikawa; Shoichiro Iwasawa; Kunio Yamada; Toshifumi Kanamaru; Takeshi Naemura; Kiyoharu Aizawa; Shigeo Morishima; Takahiro Saito,Multimedia Ambiance Communication is as a means of achieving shared-spacecommunication in an immersive environment consisting of an arch-type stereoscopicprojection display. Our goal is to enable shared-space communication by creating a photo-realistic three-dimensional (3D) image space that users can feel a part of. The concept of alayered structure defined for painting; such as long-range; mid-range; and short-rangeviews; can be applied to a 3D image space. New techniques; such as two-plane expression;high quality panorama image generation and setting representation for image processing;3D image representation and generation for photo-realistic 3D image space have beendeveloped. Also; we propose a life-like avatar within the 3D image space. To obtain thecharacteristics of user's body; a human subject is scanned using a Cyberware TM whole …,Stereoscopic Displays and Virtual Reality Systems VIII,2001,2
Hyper Mask-Projecting Virtual Face onto a Moving Real Object,S Morishima; T Yotsukura; F Nielsen; K Binsted; C Pinhanez,*,Proc. of Eurographics' 01,2001,2
空間共有コミュニケーションにおける表情入力のための人物顔追跡機能の実現,吉村哲也， 市川忠嗣， 森島繁生， 相澤清晴， 齊藤隆弘,3. 顔追 跡抽出機能 3. 1; 処理 内容顔抽 出処理 は; 次の よ うに行 う. は じめ に; 追跡すべ き人物を; 固定 カ メ ラで 撮影し; その 顔部分 をモ デル 画 像と し; 追跡 を開始する. 固定 カ メ ラ の 各フィ,電子情報通信学会総合大会講演論文集,2000,2
対話システムにおける顔面像生成,森島繁生,川本 真一; 下平 博; 新田 恒雄; 西本 卓也; 中村 哲; 伊藤 克亘; 森島 繁生; 四倉 達夫; 甲斐 充彦;李晃伸; 山下 洋一; 小林 隆夫; 徳田 恵一; 広瀬 啓吉; 峯松 信明; 山田 篤; 伝 康晴; 宇津呂 武仁;嵯峨山 茂樹,情報処理学会研究報告,2000,2
Multimedia Modeling. Modeling Multimedia Information and Systems,S Hashimoto,*,*,2000,2
Voice conversion to append emotional impression by controlling articulation information,S Ogata; T Yotsukura; S Morishima,*,*,2000,2
1-3 空間共有コミュニケーションにおける表情入力のための顔抽出,吉村哲也， 市川忠嗣， 森島繁生， 相澤清晴， 齊藤隆弘,J-STAGE: My J-STAGEとは？ ログイン; 新規登録; ショッピングカート; ヘルプ. View in English.映像情報メディア学会冬季大会講演予稿集. 一般社団法人 映像情報メディア学会. ONLINE ISSN:2424-2306 PRINT ISSN: 1343-4357. 2017年06月06日現在 収録数: 1;679記事. 記事; 巻号頁;DOI. 詳細検索. 巻 号 頁 詳細検索. 詳細検索. 閲覧する; 発行機関について. 最新巻号. 発行機関連絡先;他のJ-STAGE内発行資料. J-STAGE トップ > 資料トップ > 書誌事項. p. 35-. 記事言語: English.前の記事; |; 次の記事. http://doi.org/10.11485/itewac.1999.0_35. 主催: 会議情報 会議名:1999年映像情報メディア学会冬季大会 開催地: アクロス福岡 開催日: 1999/12/09 - 1999/12/10.本文. 1-3 空間共有コミュニケーションにおける表情入力のための顔抽出. 吉村 哲也 1) ; 市川 忠嗣1) ; 森島 繁生 2) ; 相澤 清晴 3) ; 齊藤 隆弘 4). 1) 通信・放送機構 2) 通信・放送機構:成蹊大学工学部3) 通信・放送機構:東京大学工学部 4) 通信・放送機構:神奈川大学工学部 …,映像情報メディア学会冬季大会講演予稿集 1999,1999,2
アクティブカメラによる視線追跡・自動 Lip Reading,四倉達夫， 島田直幸， 森島繁生， 大谷淳,抄録 本稿ではカメラヘッドのパン・チルトとズームのコントロール可能なアクティブカメラ 2 台を用い;常時ユーザの口・目領域を高解像度でキャプチャし自動追跡を行う手法を提案する.各々のカメラから取り込んだ画像を 2 値化することにより; 口や目領域を検出し;キャプチャされた画像における各領域の位置とその面積からカメラの回転方向と回転速度;ズーム速度を決定し; カメラ制御を行う. この追跡法によって抽出された口・目領域の 2個化画像の特徴を分析し; LipReading; 瞬きの検出や視線の追跡が可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,1999,2
HyperMask; Virtual reactive faces for story-telling,Claudio Pinhanez; Kim Binsted; Frank Nielsen; Tatsuo Yotsukura; Shigeo Morishima,*,ACM SIGGRAPH'99,1999,2
Processing of facial information by computer,Osamu Hasegawa; Shigeo Morishima; Masahide Kaneko,Abstract Faces are very familiar to everyone and convey various information; includinginformation that is specific to the individual and information that is part of mutualcommunication. Usually verbal media is not able to describe such information appropriately.Recently; detailed studies on facial information processing by computer have been carriedout in the engineering field for application to communication media and human interfaces.Two main topics are recognition of human faces and synthesis of facial images. Theobjective of the former is to enable computers to recognize human faces and that of the latteris to provide a natural and impressive interface in the form of a “face” for communicationmedia. These technologies have also been found to be useful in various fields related to theface; such as psychology; anthropology; cosmetology; and dentistry. Although most of the …,Electronics and Communications in Japan (Part III: Fundamental Electronic Science),1998,2
Dynamic modeling of human hair and GUI based hair style designing system,Keisuke Kishi; Shigeo Morishima,Generation of super-realistic faces has focused recently on computer-human interaction or virtualpeople synthesis. Though the naturalness of hair is a very important factor in the visualimpression; it is treated with a very simple model or as a texture. Because real hair has hugepieces and complex features; it is one of the hardest objects to model with com- putergraphics … In this sketch; a new hair style modeling system is presented. The system helpsdesigners use computer graphics to easily create any hair style with a tuft editor. Each pieceof hair is modeled independently; and dynamic motion can be easily simulated by solving motionequations. However; each piece of hair has a few segments; which modeled with a 3D B- splinecurve; so the calculation cost is not so large … It is necessary to create a model of each pieceof hair to generate the natural dynamic motion of hair blowing in the wind. However …,ACM SIGGRAPH 98 Conference abstracts and applications,1998,2
A physics-based talking head for interface agent,Shigeo Morishima; Hajime Sera; Demetri Terzopoulos,Abstract Computer graphics is vital for the depiction of life-like agents for computer interfacesand visual entertainment systems. This paper develops a realistic graphical model of thehuman face capable of synthesizing facial expressions in natural conversation scenarios. Inparticular; we propose a facial muscle model that includes oral musculature for controllingmouth shapes during speech production. Basic mouth shapes are defined through themeasurement of natural human facial photographs captured by two cameras. We animatespeech by realizing lip synchronization using standard phoneme durations.,Proc. of IJCAI; Workshop on Animated Interface Agents,1997,2
Speech coding based on a multi-layer neural network,Shigeo Morishima; H Harashima; Y Katayama,The authors present a speech-compression scheme based on a three-layer perceptron inwhich the number of units in the hidden layer is reduced. Input and output layers have thesame number of units in order to achieve identity mapping. Speech coding is realized byscalar or vector quantization of hidden-layer outputs. By analyzing the weighting coefficients;it can be shown that speech coding based on a three-layer neural network is speaker-independent. Transform coding is automatically based on back propagation. The relationbetween compression ratio and SNR (signal-to-noise ratio) is investigated. The bit allocationand optimum number of hidden-layer units necessary to realize a specific bit rate are given.According to the analysis of weighting coefficients; speech coding based on a neuralnetwork is transform coding similar to Karhunen-Loeve transformation. The characteristics …,Communications; 1990. ICC'90; Including Supercomm Technical Sessions. SUPERCOMM/ICC'90. Conference Record.; IEEE International Conference on,1990,2
音声情報圧縮を実現する多層ニューラルネットの特性解析,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索. 日本の論文をさがす;大学図書館の本をさがす; 日本の博士論文をさがす. 日本の論文をさがす; 大学図書館の本をさがす;日本の博士論文をさがす. 新規登録; ログイン; English. 検索. すべて. CiNiiに本文あり.CiNiiに本文あり・連携サービスへのリンクあり. すべて. CiNiiに本文あり. CiNiiに本文あり・連携サービスへのリンクあり. タイトル. 著者名. 著者ID. 著者所属. 刊行物名. ISSN. 巻号ページ. 出版者.参考文献. 出版年. 年から 年まで. 検索. 閉じる. 検索. 検索. 音声情報圧縮を実現する多層ニューラルネットの特性解析. 森島繁生; 被引用文献: 3件. 著者. 森島繁生; 収録刊行物.電子情報通信学会技術報告書 電子情報通信学会技術報告書; SP89-12; 1990. 被引用文献: 3件.被引用文献を見るにはログインが必要です。ユーザIDをお持ちでない方は新規登録してください。 Tweet;各種コード. NII論文ID(NAID) 80005045238. 資料種別 雑誌論文. データ提供元 CJP引用. 書き出し …,電子情報通信学会技術報告書,1990,2
Feature point automatic extracting from fro~ t face picture for model fitting,Shouichirou IWASAWA; Shigeo MORISHIMA,*,Television study skill information,*,2
Hypermask: Talking head projected onto real objects,Kim Binsted; Shigeo Morishima; Frank Nielsen; Claudio S Pinhanez; Tatsuo Yotsukura,*,International Conference on MultiMedia Modeling (MMM),*,2
Automatic arranging musical score for piano using important musical elements,Hirofumi Takamori; Haruki Sato; Takayuki Nakatsuka; Shigeo Morishima,ABSTRACT There is a demand for arranging music composed using multiple instruments fora solo piano because there are several pianists who wish to practice playing their favoritesongs or music. Generally; the method used for piano arrangement entails reducing originalnotes to fit on a two-line staff. However; a fundamental solution that improves originality andplayability in conjunction with score quality continues to elude approaches proposed byextant studies. Hence; the present study proposes a new approach to arranging a musicalscore for the piano by using four musical components; namely melody; chords; rhythm; andthe number of notes that can be extracted from an original score. The proposed methodinvolves inputting an original score and subsequently generating both right-and left-handplaying parts of piano scores. With respect to the right part; optional notes from a chord …,Proceedings of the 14th Sound and Music Computing Conference; Aalto; Finland,2017,1
Face texture synthesis from multiple images via sparse and dense correspondence,Shugo Yamaguchi; Shigeo Morishima,Abstract We have a desire to edit images for various purposes such as art; entertainment;and film production so texture synthesis methods have been proposed. Especially;PatchMatch algorithm [Barnes et al. 2009] enabled us to easily use many image editingtools. However; these tools are applied to one image. If we can automatically synthesizefrom various examples; we can create new and higher quality images. Visio-lization[Mohammed et al. 2009] generated average face by synthesis of face image database.However; the synthesis was applied block-wise so there were artifacts on the result and freeform features of source images such as wrinkles could not be preserved. We proposed anew synthesis method for multiple images. We applied sparse and dense nearest neighborsearch so that we can preserve both input and source database image features. Our …,SIGGRAPH ASIA 2016 Technical Briefs,2016,1
Garment transfer for quadruped characters,Fumiya Narita; Shunsuke Saito; Takuya Kato; Tsukasa Fukusato; Shigeo Morishima,Abstract Modeling clothing to characters is one of the most time-consuming tasks for artistsin 3DCG animation production. Transferring existing clothing models is a simple andpowerful solution to reduce labor. In this paper; we propose a method to generate a clothingmodel for various characters from a single template model. Our framework consists of threesteps: scale measurement; clothing transformation; and texture preservation. By introducinga novel measurement of the scale deviation between two characters with different shapesand poses; our framework achieves pose-independent transfer of clothing even forquadrupeds (eg; from human to horse). In addition to a plausible clothing transformationmethod based on the scale measurement; our method minimizes texture distortion resultingfrom large deformation. We demonstrate that our system is robust for a wide range of …,Eurographics 2016-Short Papers,2016,1
MusicMixer: Automatic DJ System Considering Beat and Latent Topic Similarity,Tatsunori Hirai; Hironori Doi; Shigeo Morishima,Abstract This paper presents MusicMixer; an automatic DJ system that mixes songs in aseamless manner. MusicMixer mixes songs based on audio similarity calculated via beatanalysis and latent topic analysis of the chromatic signal in the audio. The topic representslatent semantics about how chromatic sounds are generated. Given a list of songs; a DJselects a song with beat and sounds similar to a specific point of the currently playing songto seamlessly transition between songs. By calculating the similarity of all existing pairs ofsongs; the proposed system can retrieve the best mixing point from innumerablepossibilities. Although it is comparatively easy to calculate beat similarity from audio signals;it has been difficult to consider the semantics of songs as a human DJ considers. Toconsider such semantics; we propose a method to represent audio signals to construct …,International Conference on Multimedia Modeling,2016,1
Automatic facial animation generation system of dancing characters considering emotion in dance and music,Wakana Asahina; Narumi Okada; Naoya Iwamoto; Taro Masuda; Tsukasa Fukusato; Shigeo Morishima,Abstract In recent years; a lot of 3D character dance animation movies are created byamateur users using 3DCG animation editing tools (eg MikuMikuDance). Whereas; most ofthem are created manually. Then automatic facial animation system for dancing characterwill be useful to create dance movies and visualize impressions effectively. Therefore; weaddress the challenging theme to estimate dancing character's emotions (we call" danceemotion"). In previous work considering music features; DiPaola et al.[2006] proposed music-driven emotionally expressive face system. To detect the mood of the input music; they useda hierarchical framework (Thayer model); and achieved to generate facial animation thatmatches music emotion. However; their model can't express subtleties of emotion betweentwo emotions because input music divided into few moods sharply using Gaussian …,SIGGRAPH Asia 2015 Posters,2015,1
BGMaker: example-based anime background image creation from a photograph,Shugo Yamaguchi; Chie Furusawa; Takuya Kato; Tsukasa Fukusato; Shigeo Morishima,Abstract Anime designers often paint actual sceneries to serve as background imagesbased on photographs to complement characters. As painting background scenery is timeconsuming and cost ineffective; there is a high demand for techniques that can convertphotographs into anime styled graphics. Previous approaches for this purpose; such asImage Quilting [Efros and Freeman 2001] transferred a source texture onto a targetphotograph. These methods synthesized corresponding source patches with the targetelements in a photograph; and correspondence was achieved through nearest-neighborsearch such as PatchMatch [Barnes et al. 2009]. However; the nearest-neighbor patch is notalways the most suitable patch for anime transfer because photographs and animebackground images differ in color and texture. For example; real-world color need to be …,ACM SIGGRAPH 2015 Posters,2015,1
ラリーシーンに着目した映像自動要約によるラケットスポーツ動画鑑賞システム,河村俊哉， 福里司， 平井辰典， 森島繁生,概要: 本論文では; ラケットスポーツ動画を対象とした映像要約手法に基づくシステムを提案する.既存研究では重要なラリーのみで構成される要約映像を生成したが; 事前に手作業で音響情報をアノテーションする必要があり実用的な手法ではなかった. そこで本研究では; ラケットスポーツ動画に対する新たなラリーシーンの検出方法とラリーの重要度評価尺度およびその鑑賞方法を提案する.提案手法では; 画像特徴量を用いたショットクラスタリングおよびラリーシーンを含むクラスタの自動選定により; 高精度なラリーシーン検出を実現する. その後; 各ラリーのショット中の音響情報を考慮したラリーの重要度評価を行うことで; ラケットスポーツ動画の中から重要度の高いシーンのみで構成された要約映像の生成を可能とする. さらに; ラケットスポーツに特化した高速再生機能を用いた動画鑑賞インタフェースを提案し; 試合内容の理解度を保持したまま短時間での動画鑑賞を実現する.,情報処理学会論文誌,2015,1
Automatic photorealistic 3D inner mouth restoration from frontal images,Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,Abstract In this paper; we propose a novel method to generate highly photorealistic three-dimensional (3D) inner mouth animation that is well-fitted to an original ready-made speechanimation using only frontal captured images and a small-size database. The algorithms arecomposed of quasi-3D model reconstruction and motion control of teeth and the tongue; andfinal compositing of photorealistic speech animation synthesis tailored to the original.,International Symposium on Visual Computing,2014,1
Pose-independent garment transfer,Fumiya Narita; Shunsuke Saito; Takuya Kato; Tsukasa Fukusato; Shigeo Morishima,Abstract Dressing virtual characters is necessary for many applications such as film andgame. However; modeling clothing for characters is a significant bottleneck; because itrequires manual effort to design clothing; position it correctly on the body; and adjusting thefitting. Therefore; even if we wish to design similar looking clothing for characters that havevery different poses and shapes; we would need to repeat the tedious process practicallyfrom scratch. We then propose a method for automatic design-preserving transfer of clothingbetween characters in various poses and shapes. As shown in the results; our systemenables us to automatically generate a clothing model for a target character.,SIGGRAPH Asia 2014 Posters,2014,1
Facial fattening and slimming simulation considering skull structure,Masahiro Fujisaki; Daiki Kuwahara; Taro Nakamura; Akinobu Maejima; Takayoshi Yamashita; Shigeo Morishima,An accurate simulation of facial fattening or slimming is required in many areas; includingbeautification; health; and entertainment.[Blanz et al 1999] simulated facial fattening from asingle image based on 3D morphable model. They fattened all faces in the same wayconsidering only difference in facial surfaces; however; human face fattening depends onindividual skull structure of the face. Further; without considering the skull structure; there isthe possibility of deformed faces; for example facial surfaces penetrating into the underlyingskull. Pei et al [2008] achieved accurate facial reconstruction that focused on the skullstructure; ie the shapes of facial surfaces were determined primarily on the basis of skullstructure. Therefore; considering skull structure is crucial for facial fattening. In this paper; wepropose a realistic facial fattening and slimming simulation method that uses a single …,ACM SIGGRAPH 2014 Posters,2014,1
Macroscopic and microscopic deformation coupling in up‐sampled cloth simulation,Shunsuke Saito; Nobuyuki Umetani; Shigeo Morishima,ABSTRACT Various methods of predicting the deformation of fine-scale cloth from coarserresolutions have been explored. However; the influence of fine-scale deformation has notbeen considered in coarse-scale simulations. Thus; the simulation of highlynonhomogeneous detailed cloth is prone to large errors. We introduce an effective methodto simulate cloth made of nonhomogeneous; anisotropic materials. We precompute amacroscopic stiffness that incorporates anisotropy from the microscopic structure; using thedeformation computed for each unit strain. At every time step of the simulation; we computethe deformation of coarse meshes using the coarsened stiffness; which saves computationaltime and add higher-level details constructed by the characteristic displacement of simulatedmeshes. We demonstrate that anisotropic and inhomogeneous cloth models can be …,Computer Animation and Virtual Worlds,2014,1
Photorealistic inner mouth expression in speech animation,Masahide Kawai; Tomoyori Iwao; Daisuke Mima; Akinobu Maejima; Shigeo Morishima,Abstract We often see close-ups of CG characters' faces in movies or video games. In suchsituations; the quality of a character's face (mainly in dialogue scenes) primarily determinesthat of the entire movie. Creating highly realistic speech animation is essential becauseviewers watch these scenes carefully. In general; such speech animations are createdmanually by skilled artists. However; creating them requires a considerable effort and time.,ACM SIGGRAPH 2013 Posters,2013,1
年齢別パッチを用いた画像再構成による経年変化顔画像合成,前島謙宣， 溝川あい， 松田龍英， 森島繁生,概要: 安全安心な社会実現を目的とした犯罪捜査支援システム構築のため; 顔写真から対象者の過去・未来の顔を合成可能な経年変化顔合成技術が求められている. 本稿では;同一環境で撮影された顔画像のデータベースから構成される年齢別の小片画像群を用いて顔を再構成することで; 経年変化顔画像を合成する手法を提案する. 提案手法は; 入力顔の再構成を通じて従来手法では困難であったしみやくすみといった細かな肌の特徴を表現し; さらに皺の統計モデルを用いて再構成の結果を変調することで経年変化をシミュレーションすることが可能である.提案手法の有効性を検証するため; 年齢推定と個人識別に関する主観評価実験を行い;提案手法が; 元の人物の印象を持つ目標年齢らしい経年顔画像を合成可能であることを示す.,研究報告コンピュータビジョンとイメージメディア (CVIM),2013,1
顔器官の輪郭情報を用いた経年変化にロバストな認証システムの一検討,中井宏紀， 平井辰典， 前島謙宣， 森島繁生,顔認証において; 同一の被写体であっても顔の外見に経年変化が生じる場合は認証精度が低下するという問題がある. 本稿では; 経年変化が生じても見た目に大きな変化を及ぼさない顔器官(目・鼻・口等) の輪郭情報を特徴量にすることで経年変化を含む顔認証の精度向上を目指す.具体的には; 顔の幾何学特徴として先行研究と同様に顔グラフを; テクスチャ特徴として顔特徴点周辺の Histogram of Oriented Gradient (HOG) 特徴量を認証に用いた. 結果として;公開顔画像データベースである FG-NET Aging Database を用いた認証実験により;先行研究を上回る認証精度を示し; 本手法の有効性を確認した.,研究報告コンピュータビジョンとイメージメディア (CVIM),2013,1
アニメ作品におけるキーフレーム自動抽出に基づく映像要約手法の提案,福里司， 平井辰典， 大矢隼士， 森島繁生,抄録 近年; 映像要約を目的として; プレイログや画像特徴量を用いた漫画形式の映像要約手法が提案されている. しかし; 従来手法の多くで; 動画を適切に要約するために必要な情報量や;粒度が言及されていない. これは; 取得したキーフレームを漫画として適切であるか判定するための評価尺度が存在しないからである. そこで本研究では; 原作漫画からアニメ作品を制作する際;原作漫画のコマの補間が行われていることに着目し; アニメ作品に対する原作漫画との一致度合いを; 映像要約の評価尺度として提案する. さらに; アニメ作品を漫画として適切に要約するためのキーフレームを; 画像特徴量から判定する手法を提案する. アニメ作品をショットごとに分割し;ショット単位; フレーム単位の 2 つの階層においての重要度を算出することで; アニメ作品から;原作漫画のコマに相当するキーフレームの高精度な抽出を実現し; 定量的な評価結果により;その有効性を示した.,画像電子学会誌,2013,1
確率モデルに基づく対話時の眼球運動の分析及び合成,岩尾知頼， 三間大輔， 久保尋之， 前島謙宣， 森島繁生,< あらまし> 写実的なキャラクタアニメーションの実現のためには眼球運動を正しく再現することが重要である. しかしながら現在は; キャラクタの対話時の眼球運動を再現する際にアーティストの手作業による作りこみが必要となるため; 多大なコストや労力がかかることが問題である.そこで本研究では人間の対話時の眼球運動に着目し; 計測結果を基に眼球運動と瞬きを確率関数を用いてモデル化することで; 眼球運動を自動生成する. まず; 瞬きを含む対話時の眼球運動と固視微動とをそれぞれ計測する. 次に; 計測結果を基に対話時の眼球運動を跳躍運動と固視微動に分類する. さらに分類された跳躍運動と固視微動及び瞬きを; それぞれ確率モデルを用いて近似した後に;それらの確率モデルをキャラクタに適用することにより; リアルな眼球運動の自動生成を可能とした.,画像電子学会誌,2013,1
Automatic Music Video Generation System by Reusing Posted Web Content with Hidden Markov Model (Special Issue on Image Electronics and Visual Computing...,Hayato OHYA; Shigeo MORISHIMA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,IIEEJ transactions on image electronics and visual computing,2013,1
笑顔表出過程の表情の動きと受け手の印象の相関分析,藤代裕紀， 前島謙宣， 森島繁生,顔の表情は; 人と人との円滑なコミュニケーションにおいて重要な役割を担っている.特に笑顔は相手に対して肯定的な印象を与えるため様々な研究が行われている.笑顔が相手に与える印象に関する研究はこれまでにもいくつか報告されているが;そのほとんどが静止画を対象とした研究であった. しかし最近では; 表出後のみだけでなく;表出過程が相手に与える印象に大きく影響するとの指摘もある. そこで本研究では;笑いの自然さに注目し; 表出過程において目; 頬; 口元の動きが笑いの自然さの印象に寄与するかを調べた. また; 実際の笑顔表出に対して; 笑顔の自然さを生じさせる客観的な指標を実験により明らかにし; その指標に基づき表情合成を行うことで; より自然な笑顔の合成動画像を作成することを目的とした. 更に合成された動画像に対する主観評価実験を通じて; 客観的指標と笑顔の自然さとの対応関係の妥当性を示した.,電子情報通信学会論文誌 A,2012,1
Automatic music video creation system by reusing existing contents in video-sharing service based on HMM,Hayato Ohya; Shigeo Morishima,ABSTRACT We propose automatic music video creating system by reusing music videosthat are created by amateur users and existed on the video sharing service. These musicvideos are created by combining existing music with frames of video games; anime; and soon. They are called “MAD” video. In this paper; we improved learning method ofDanceReProducer that is the system which our group proposed previously. We enabled thesystem to consider the time series information of videos by using Markov chain; and togenerate movie by using Forward Viterbi algorithm. And we compared our system withDanceReProducer to carrying out subjective assessment experiment.,Proc. of Image Electronics and Visual Computing Workshop,2012,1
Automatic 3D face generation from video with sparse point constraint and dense deformable model,Tomoya Hara; Akinobu Maejima; Shigeo Morishima,Abstract 3D face models have been widely applied in various fields (eg biometrics; movies;video games). Especially; it is one of the most popular and challenging tasks in computervision and computer graphics to reconstruct a 3D face model only with single camerawithout attaching landmarks and projecting laser dots or structured light patterns on a face.For example; Maejima proposed a method; based on generic-model approach; which canquickly reconstruct a 3D face model from 2D single photograph using a deformable facemodel [Maejima et al. 2008]. However; since it supposes input as a frontal face image; thismethod cannot express the individual facial parts' geometry such as height of nose andcheek contour accurately.,SIGGRAPH Asia 2011 Posters,2011,1
テクスチャ-デプスパッチタイリングに基づく正面顔画像からの 3 次元形状推定,郷原裕明， 前島謙宣， 森島繁生,本稿では; 正面顔画像から 3 次元形状を推定する; 新たな手法を提案する. 提案手法では; 3次元形状を持つ顔モデルからカラーマップとデプスマップを取得し; パッチに区切りデータベースを生成する. そして;. 入力顔画像のカラーとデータベース中のパッチのカラーマップと比較し;評価関数によりパッチを選択しパッチタイリングを行う. その際にデプスマップも同様に選択しタイリングをすることでテクスチャ情報と奥行情報の関係を利用した; テクスチャ-デプスパッチタイリングに基づく正面顔画像からの 3 次元顔形状推定なる新たな手法を提案する.,研究報告コンピュータビジョンとイメージメディア (CVIM),2011,1
特徴量の経年変化解析に基づく個人識別手法の検討,原田健希， 田副佑典， 前島謙宣， 森島繁生,あらまし 本稿では; 特徴量の経年変化を解析し; その結果に基づき特徴量に重みづけを行うことで;経年変化を考慮した個人識別手法を提案する. 顔認証分野では; 個人内分散が小さくかつ個人間分散の大きい特徴量に大きな重みを与えるのが一般的であるが; 登録時と認証時の画像に年齢差がある場合には; 却って識別性能を低下させる要因になる. 提案手法では;「経年変化の影響を受けにくい特徴量」 に加え;「経年変化に伴い一定の割合で変化する特徴量」 も識別に用いる.各年齢における特徴量の平均値を用いて; 年齢に対する相関係数を求める. 相関係数の高い特徴量に対して; 経年変化の影響を軽減するような補正を行う. 13 サンプルを用いた評価実験を通じて;一様な重みづけをした場合と比較し; 1 位照合率が 46.2% から 76.9% に向上したことを確認した.,画像の認識・理解シンポジウム (MIRU2011) 論文集,2011,1
幾何学的制約を考慮した Linear Predictors に基づく顔特徴点自動検出,松田龍英， 原朋也， 前島謙宣， 森島繁生,Page 1. 「画像の認識・理解シンポジウム(MIRU2011)」 2011 年 7 月 幾何学的制約を考慮した LinearPredictors に基づく顔特徴点自動検出 松田 龍英 原 朊也 前島 謙宣 森島繁生 早稲田大学先進理工学研究科 〒169-8555 東京都新宿区大久保 3-4-1 E-mail: {matsuda-t@ruri.; wap.0921@akane.;maejima@aoni.; shigeo@}waseda.jp あらまし 本稿では，Eng-Jon らが提案した Linear Predictorsに，幾何学的制約を加味した新しい顔特徴点検出手 法を提案する．Linear Predictorsは，注目画素周辺の輝度値と，特徴点の正解位置への移動ベクトルを回帰によって 対応付ける手法であり，20 枚程度の学習データで高精度な推定移動ベクトルが得られる．提案手法では，各顔器官の重心を基準とした特徴点の有効範囲を定め，移動ベクトル推定時に特徴点が有効範囲を超えないような制約を加 え，特徴点検出精度の向上を実現した．また，事前に顔向き角度推定を行い，推定結果に基づいた学習データの選 …,画像の認識・理解シンポジウム (MIRU2011) 論文集,2011,1
Curvature-Dependent Reflectance Function for Interactive Rendering of Subsurface Scattering.,Hiroyuki Kubo; Yoshinori Dobashi; Shigeo Morishima,Abstract—Simulating sub-surface scattering is one of the most effective ways to realisticallysynthesize translucent materials such as marble; milk or human skin. We have developed acurvature-dependent reflectance function (CDRF) which mimics the presence of subsurfacescattering. In this approach; we provide only a single parameter that represents the intensityof theincident light scattered in a translucent material. The parameter is not only provided bycurve-fitting to a simulated data-set; but also manipulated by an artist. Furthermore; thisapproach is easily implementable on a GPU and does not require any complicated pre-processing and multi-pass rendering as is often the case in this area of research.,International Journal of Virtual Reality,2011,1
スナップ写真からの 3 次元顔モデル高速自動生成,前島謙宣， 森島繁生,抄録 本報告では; スナップ写真から写真中の人物らしい 3 次元顔モデルを高速自動生成する手法について述べる. 提案手法は 3 次元顔形状の事前知識として 1153 人の 3 次元顔モデルから構築される顔変形モデルと; 顔形状の分布に対してフィッティングされた混合ガウス分布モデルを用いる.3 次元顔形状は; 画像から自動検出される顔特徴点と顔変形モデル上の対応頂点との残差の二乗和が最小かっその時の顔変形モデルのモデルパラメータに対する混合ガウス分布モデルの尤度が最大となるようなエネルギー最小化問題を解くことにより推定される. 提案手法に対する性能評価実験の結果から; 平均 1.2 秒の処理時間で 2.1 mm の精度誤差を持つ 3 次元顔モデルが生成可能であることを示す.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2010,1
個人顔の 3 次元形状変形とテクスチャ変換に基づくエージングシミュレーション (テーマ関連; 顔・人物・ジェスチャ・行動),田副佑典， 藤代裕紀， 中野真也， 笠井聡子， 前島謙宣， 森島繁生,抄録 本稿では; レンジスキャンデータ (3 次元顔形状データ+ 正面顔テクスチャ)に基づいて生成された正確な 3 次元形状と正面顔テクスチャを有する個人顔モデルに対し; 3次元形状とテクスチャの双方に年齢特徴を加えた年齢変化顔の合成手法を提案する. まず;被験者の年齢に基づき 3 次元顔形状データと正面顔テクスチャを分類し年齢別顔データベースを構築する. 次に; データベース中の 3 次元顔形状データから学習される年齢特徴ベクトルを入力顔に付加することで; 3 次元顔形状を所望の年齢へと変換する. 続いて; 入力顔に対し;入力顔が属する年代の平均顔テクスチャの輝度値とターゲットとなる年代の平均顔テクスチャの輝度値との差分を加えることにより; テクスチャをターゲットの年齢へ変換する. 最後に;両者を統合することにより年齢変化顔を合成する. 提案手法により; 年齢変化に伴う 3次元形状変化およびしみ; しわ等の表現を可能とした.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2010,1
Interactive shadow design tool for cartoon animation-KAGEZOU,Shiori Sugimoto; Hidehito Nakajima; Yohei Shimotori; Shigeo Morihsima,In traditional 2D Anime; shadows are drawn by hand; and play a significant role in thecreation of symbolic visual effects related to the character's position and shape. Howevershadows are not always drawn as a result of time constraints and a lack of animators. Wedevelop a shadow generation system that enables animators to easily create shadowanimation layers based on character outlines. Our system is both simple and intuitive. Theonly inputs required are the character animation layers generally used in the Anime industry.Shadows are automatically generated based on these inputs; and then generated shadowsare fine-tuned by simple mouse operations. First; shadows are rendered using Shadow MapMethod based on the transparency information of the character animation layers.Subsequently; our system applies some filters that enable the generated shadow shape …,*,2010,1
キャラクターの個性を表現可能な顔画像合成技術 (特集 ここまで来た! 顔情報処理技術の最先端),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,O plus E,2009,1
MRI を用いた前腕皮膚形状変化モデルの構築と運動生成 (一般セッション; コミュニケーション支援; 共生コミュニケーション及び一般),山中健太郎， 中村槙介， 小林昭太， 森島繁生,抄録 This paper presents a new methodology for constructing an example-based skindeformation model of human forearm based on MRI images. Generating realistic skinanimation of forearm movement is generally difficult in CG domain because there is a crucialdifference between a structure of forearm of a virtual human and a real human. So wepropose a new kind of skin deformation model based on MRI images. Using MRI images; wecan model example skin shapes associated with location of bones with accuracy. We alsomention how to apply the model to characters to generate skin animation. For this purpose;we employ RBF; Radial Basis Functions. Once the model is constructed; skin animation iseasily generated by applying the model to the forearm of a character by means of RBF. Inthis paper; we describe how to construct our model; first. Then we explain the method to …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2009,1
CG キャラクタの存在感,森島繁生,より抽象的・記号的である. しかし; 映画やアニメ等の映像制作の観点で言えば;まず演出意図が前提にあり; それを満たすという意味でのライティングと陰影付けが必要となる.ハイライトの演出に関しては; 手続き型のハイライトモデルをマウスドラッグベースの直感的方法で制御可能な “HighlightShader” がすでに開発されている [3]. 陰影の演出に関しては; 3次元モデルの陰影領域をリアルタイムに編集制御できる局所制御可能なトゥーンシェーダ“LoCoStySh” が開発された [4]. これらのツールはいずれも Maya プラグインとして実現され;アニメ制作現場において活用されている.,日本バーチャルリアリティ学会誌= Journal of the Virtual Reality Society of Japan,2009,1
Scenario speech assignment technique for instant casting movie system,Shin-ichi Kawamoto; Yoshihiro Adachi; Yamato Ohtani; Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,In this paper; we propose an improved Future Cast System (FCS) that enables anyone to bea movie star while retraining their indivisuality in terms of how they look and how they sound.The proposed system produces voices that are significantly matched to their targets byintegrating the results of multiple methods: similar speaker selection and voice morphing.After assigning one CG character to the audience; the system producesvoices insynchronization with the CG character's movement. We constructed the speechsunchronization system using a voice actor database wiht 60 different kinds of voices. Oursystem achieved higher voice similarity than conventional system; the preference score ofour system was 56.5% over other conventional system.,*,2009,1
3. 顔表情の CG 合成と感動評価,森島繁生,スチャから; 10 秒以内でモデルが完成する. まずパターン投影された顔画像から顔の奥行き情報を取得する (図 2). さらに正面顔画像の特徴点抽出 6) を行い; 標準顔モデルを整合する.また性別と年齢も同時に推定する. 自動抽出される特徴点数は 89 点であり (図 3 (a));パーツ毎の輪郭に対して曲線近似により 128 点の特徴点を計算した (図 3 (b)) 後に;これらの点の影響力をカーネル関数で定義する Radial Basis Function Transformation 補間 7)∼9) により顔モデルの全頂点が決定され; 個人顔モデルが完成する (図 3 (c)).特に瞼や上下唇の境界輪郭については; リアルな表情合成やリップシンクを実現するために高い抽出精度が要求される. この個人にカスタマイズされた顔モデルから; リアルタイム表情変形用のキーシェープが自動的に生成される. 図 4 にカスタマイズされたキーシェープの一例を示す. 2.2実時間表情合成表情の実時間合成のためブレンドシェープを採用している. ブレンドシェープとは …,映像情報メディア学会誌,2008,1
1. キャラクタアニメーション制作の高能率化手法,森島繁生， 栗山繁， 川本真一,に; 外観が人と異なるキャラクタにリアルな動きを付けた際に感じられる違和感も; MoCapデータの欠点として指摘されている. MoCaToon は; リミテッドアニメーションで制作されるメリハリの利いた動きを生成して; 編集作業の効率化を図る MoCap データの自動変換技術であり;モーション編集の要素技術として位置づけられる (図 1 に操作画面を示す). すなわち;動きの本来の意味やニュアンスを失わないように; MoCap データから抜き出したキーフレームアニメーションにおけるキーフレームでの姿勢 (以後; キー姿勢) のみを描画に用いることにより;手作業で制作されるアニメ作品と同じ印象となる映像を生成する. この技術により;不気味の谷の現象 1) を取り除く副次的な効果も同時に期待される. MoCap データの圧縮のため;関節角度の軌跡を曲線近似する方法 2) が提案されているが; 基本的には元の動きの再現が目的であり; アニメのメリハリのある表現は考慮されていない. 一方; Action Synopsis 法 3) では; 曲線 …,映像情報メディア学会誌,2008,1
直感的に影を演出可能な編集ツール,中嶋英仁， 杉崎英嗣， 森島繁生,抄録 Shadows in cartoon animation are generally used for dramatizing scenes. In hand-drawn animation; these shadows reflect the animators' intention and style rather thanphysical phenomena. On the other hand; shadows in 3 DCG animation are photorealisticallyrendered; and animators can not fully reflect their intention. This is because; in 3DCGanimation; shadows are automatically generated once the light source is defined. Therefore;we develop an interactive tool for editing shadows that combines the advantages of hand-drawn animation and 3DCG technology. the advantage of our tool is that shadow attributesare inherited once animators edit the shape and location of shadows. Animators are onlyrequired mouse operations for editing shadows. Consequently; our tool enables animatorsto create shadows automatically and easily to reflect their intention and style.,映像情報メディア学会誌,2008,1
ストーリヘの没入感を実現するダイブイントゥザムービープロジェクト,森島繁生， 八木康史， 中村哲,* Wase da University; 3-4-l Okubo Shinjuku; Tokyo 1 69-8555 Japan" The Institute ofScientific and Industrial Research; 8-1 Mihogaoka; Ibaraki; Osaka; 567-0047** ATR SLC.; 2Hikaridai; Seika cho; Sorakugun; Kyoto; 6 19-0288 E-mail:* shigeo waseda. jp'yagi 9am.sanken. osaka-u. ac. jp** shinichi. kawamoto atr. jp,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),2008,1
Car designing tool using multivariate analysis,Yusuke Sekine; Akinobu Maejima; Eiji Sugisaki; Shigeo Morishima,We construct novel car designing tool; which helps car designers to design cars efficientlyand stimulates their artistic sensibilities. Cars are synthesized by analyzing various existingcar body shapes and extracting meaningful attributes that represent the features of theseshapes. First; a database of existing cars is created. We analyze all shapes using principalcomponent analysis (PCA); then; gain attributes that indicate the typical features of those carshapes. By manipulating the weight of each attribute in the PCA space; we can create carshapes. Second; by analyzing the features of specific car groups; ie manufacturer; genre;era; etc; we discover attributes that discriminate between these groups. We can generate anew type of cars by adding these features onto any existing car ie a BMW-like Mercedes..,ACM SIGGRAPH 2006 Research posters,2006,1
Subjective evaluation of a synthetic talking face in an acoustically noisy environment,Akinobu Maejima; Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,Abstract The realization of an anthropomorphic agent which looks like a real human is animportant research topic for the broadening of the range of human-to-humancommunications through the use of a computer. We have proposed a technique forsynthesizing natural talking-face animation that permits such communications. How toevaluate the performance of talking-face animation; however; has remained an outstandingissue. The performance of talking-face animation is determined in three parameters:(1) Doesit reproduce human talking to an extent that permits lipreading?(2) Does it appear visuallynatural?(3) Is it accurately synchronized with voice? In this paper; we first presented talking-face animation along with the voice to subjects and conducted experiments on how well thesubjects heard the contents of the spoken words to examine Parameter (1). In the next …,Electronics and Communications in Japan (Part III: Fundamental Electronic Science),2006,1
顔情報データベース構築の基礎的検討 (3): 表情画像の認知的評価とデータベースの信頼性について (顔とコミュニケーション及び一般),鈴木竜太， 吉田宏之， 渡邊伸行， 前田亜希， 番場あやの， 續木大介， 北村麻梨， 時田学， 和田万紀， 森島繁生， 山田寛,抄録 顔情報処理の問題に対する学術的関心が高まっており; 我々はそのような学問的発展に資する顔情報データベースの構築を目指したプロジェクトを進めている. 現在;この顔情報データベースには; 統一された撮影環境と撮影手続により収集された表情画像が;ある程度の規模で収録されつつある. 本報では; 収録された顔画像情報に対する表情判断実験を行い; 表情画像の認知的評価による信頼性の向上について検討を行った. このことにより本プロジェクトで構築を進めている顔情報データベースは; その発展に向けた新たな局面を迎えている.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2005,1
Speech to talking heads system based on hidden Markov models,Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,This paper describes a technique to create human-like talking head speech animation withlevels of naturalness and realism by mapping from speech information to facial movementsequences. Speech animation techniques for human-like natural talking head systems havetraditionally included both key-framing methods [Cohen and Massaro 1993] and physics-based methods [Waters 1987]. Recent machine learning methods provide a new techniquefor speech animation systems [Yamamoto 1998]; allowing them to be trained from recordeddata and then used to synthesize new motion. Hidden markov models (HMMs) arefrequently used for these methods and have demonstrated their effectiveness for speechanimation. However; those methods lack producing high-quality; natural facial speechanimation. In particular key-framing and physics-based methods are difficult to animate in …,ACM SIGGRAPH 2005 Posters,2005,1
Quantitative representation of face expression using motion capture system,Hiroaki Yanagisawa; Akinobu Maejima; Tatsuo Yotsukura; Shigeo Morishima,5 Results In this research; we measured temporal transition on skin surface during AUsusing the motion capture system; and engineered quantify AUs. Moreover; we proposedsynthesis method for generating facial expression animations by combination of EigenAction Units. In future works; we have to consider to an individual dependency in the EAUs.It's necessary to consider generalization of EAU and controlling speed of facial expressionchange.,ACM SIGGRAPH 2005 Posters,2005,1
MRI を用いた骨格・関節のモーションキャプチャリング (ヒューマンコミュニケーショングループ (HCG) シンポジウム),西村昌平， 小島潔， 岩澤昭一郎， 森島繁生,抄録 本研究では; MRI を用いて被験者の体内部を計測することにより; 人物の骨格構造を忠実に再現した. また; モーションキャプチャシステムを用いて取得できるマーカの 3 次元移動量と; MRIで得られた骨格情報を用いて人体運動を実現した. MRI の撮影で取得した骨格情報より人体モデルを作成し; モーションキャプチャ撮影時のマーカを; 人体モデルの皮膚上に仮想的に 1 対 1で対応させ; 配置させる. この手法により; 今までモーションキャプチャマーカの位置から骨格構造上の統計的ルールで決定されていた関節位置を; 被験者個人の関節位置に修正することが可能となり;忠実な骨格および関節の動作を実現した. また; 動作時における皮膚と骨との位置関係の変化も検証した.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2005,1
MRI イメージからの骨格抽出と高忠実な骨格および関節のモーションキャプチャリング,小島潔， 西村昌平， 岩澤昭一郎， 森島繁生,2 研究概要本研究は同一の被験者に対して; MRI (Magnetic Resonance Image: 磁気共鳴画像)の撮影とモーションキャプチャ測定を行う. まず; MRI 撮影によって; 各被験者の皮膚表面と骨格の形状の 3 次元モデルを取得する. このプロセスは各人物に対して一度だけ撮影を行い;画像処理によって MRI 画像から骨格および皮膚表面の輪郭が断面スライスごとに半自動的に決定される.. MRI 撮影および個人モデルの構築の後; 体表面の任意の位置にマーカー配置を行って;モーションキャプチャすることによってマーカの動きの 3 次元座標の時系列を取得する. MRI撮影によって取得された個人の皮膚表面モデルに; モーションキャプチャ時のマーカ座標を仮想的に指定することで; マーカと骨格との相対位置関係が計算され; 実際に取得されたモーションキャプチャデータから; 各骨格のパーツの位置および動きが自動的に計算されるというしくみである.,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),2005,1
Face and Gesture Cloning for Life-like Agent,S Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,11th International Conference on Human-Computer Interaction (HCI International 2005),2005,1
Human-Machine Communication by Audio-Visual Integration,Satoshi Nakamura; Tatsuo Yotsukura; Shigeo Morishima,Abstract The use of audio-visual information is inevitable in human communication.Complementary usage of audio-visual information enables more accurate; robust; natural;and friendly human communication in real environments. These types of information arealso required for computers to realize natural and friendly interfaces; which are currentlyunreliable and unfriendly. In this chapter; we focus on synchronous multi-modalities;specifically audio information of speech and image information of a face for audio-visualspeech recognition; synthesis and translation. Human audio speech and visual speechinformation both originate from movements of the speech organs triggered by motorcommands from the brain. Accordingly; such speech signals represent the information of anutterance in different ways. Therefore; these audio and visual speech modalities have …,*,2005,1
A-14-2 音声のパラメータ変換によるイントネーション変換システムの構築,足立吉広， 前島謙宣， 四倉達夫， 森島繁生,を提案する. 従来か ら自然音声に イ ン トネーシ ョ ン を付加し; 声質変換をする研究が行われてきた [1][:] が作成した音声の 劣化が著し か っ た. そこ で 本研究で は所望の イ ン トネ,電子情報通信学会総合大会講演論文集,2003,1
影響力マップを用いた任意表情モデル上での表情合成,祖川慎治， 四倉達夫， 森島繁生,抄録 フェーストゥーフェースのコミュニケーションシステムやヒューマンインターフェースを実現するためにアバタの表情合成の研究が活発に進められている. しかし; 従来の表情合成技術はモデル依存性が強く; 基本的に詳細な表情変形ルールが定義された特定のワイヤフレームモデルを用いて表情合成する必要があった. 本稿では特定のモデルに依存しない表情合成のために;極めて細かいメッシュ構造を持つ影響力マップを定義し; 表情変形ルールの定義された顔モデルからマップ上に 3 次元移動量としての表情変形ルールを射影して; ユーザが独自に定義した顔モデルにこのルールを移植する方法を提案した. 具体的にはルールの定義された顔モデルとユーザの定義した顔モデルの格子点の対応関係を放射基底関数を用いて実現する.格子点の異なるモデル間においてもルールの変換が出来る点が特徴で; このための GUIも実現している.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2003,1
古典バレエのモーションキャプチャリング,小島潔， 杉崎， 英嗣， 四倉， 達夫， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会; 2003,2003,1
Face analysis and synthesis for interactive entertainment,Shoichiro Iwasawa; Tatsuo Yotsukura; Shigeo Morishima,Abstract A stand-in is a common technique for movies and TV programs in foreignlanguages. The current stand-in that only substitutes the voice channel results awkwardmatching to the mouth motion. Videophone with automatic voice translation are expected tobe widely used in the near future; which may face the same problem without lip-synchronized speaking face image translation. In this paper; we propose a method to trackmotion of the face from the video image and then replace the face part or only mouth partwith synthesized one which is synchronized with synthetic voice or spoken voice. This is oneof the key technologies not only for speaking image translation and communication system;but also for an interactive entertainment system. Finally; an interactive movie system isintroduced as an application of entertainment system.,*,2003,1
A-14-11 有声音区間の対応づけによる自動イントネーション変換,前島謙宣， 緒方信， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2002,1
A-16-34 仮想人物の舌モデル構成と発話アニメーション作成,鳥飼友美， 伊藤圭， 緒方信， 森島繁生,シ ョ ン を可能 に す るた めに; サ イ バ ース ペ ース 上 に お い て; 人物 の 口 の 動 き を リア ル にす る方 法 につ い て述 べ る. 2. ア ニ メ ー シ ョ ン シ ス テ ム 要まず; 人物の 原 画 像 と標準 ワイヤ,電子情報通信学会総合大会講演論文集,2002,1
テクスチャブレンディングによる皺の表現と基本顔モデルによる感情空間の構築,柳澤尋輝， 高橋光紀， 森島繁生,抄録 特定の顔表情を予めレンジセンサにより取得された 3 次元構造とデジタルカメラ撮影されたテクスチャを含む 11 種類の基本表情モデルのブレンディングによって表現する手法を提案した.これは従来のモデルでは; 表現が難しかった皺の表現や; 微妙な表情変形を可能にした. また;この各基本表情モデルの融合強度を; 入出力層に与えて 5 層ニューラルネットワークを恒等写像学習させ; その中間層から 3 次元感情空間を構成した. これによりリアルな表情アニメーションが感情空間上の点の制御と後半の 3 層の変換によって; 容易に実現できるようになった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2002,1
擬人化音声対話エージェントのための表情合成技術,四倉達夫， 森島繁生,The multi-modal communication between man and machine style is to have a virtual humanor an avatar appearing on the computer terminal that should be able to understand and expressnot only linguistic information but also non-verbal information. This is similar to human-to-humancommunication with a face-to-face style. Very important factor to make an avatar look believableor alive depends on how well an avatar can duplicate a real human's expression and impressionon a face precisely. Especially in case of communication application using avatar; a real-timeprocessing with low delay is inevitable. In this paper; we describe a current situation of our faceimage synthesis technology … SIG Technical Reports are nonrefereed and hence may laterappear in any journals; conferences; symposia; etc.,情報処理学会研究報告ヒューマンコンピュータインタラクション (HCI),2002,1
自発・演技表情表出時における顔面動作分析および表情合成,四倉達夫， 内田英子， 山田寛， 赤松茂， 森島繁生,抄録 本稿では; 高速度カメラを用いて人間が自然な表情した場合 (自発表出)と普遍的かつ典型的な表情を演じる際の顔表情 (演技表出) を撮影し; 顔の各部位に設定した特徴点の変位置に基づき顔の動きの定量的な測定を分析した. また測定結果から CGによって構築した顔モデルのアニメーション生成を行った. 自発表出条件; 演技表出条件ともに顔の各部位の動き出しの差は微細であり高速度カメラを用いる有効性が示された.また情動ごとおよび表出条件ごとに顔の動き量や速さに特徴的な違いが認められたが;動きの変化そのものの様相には興味深い共通性が認められた. 顔モデルのアニメーションに関しても; 線形補間によるキーフレームアニメーションと比べ自然な顔表情表出が可能となった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2001,1
Video translation system using face tracking and lip synchronization,Shigeo Morishima; Shin Ogata; Satoshi Nakamura,ABSTRACT We introduce a multi-modal English-to-Japanese and Japanese-to-Englishtranslation system that also translates the speaker's speech motion while synchronizing it tothe translated speech. To retain the speaker's facial expression; we substitute only thespeech organ's image with the synthesized one; which is made by a three-dimensional wire-frame model that is adaptable to any speaker. Our approach enables image synthesis andtranslation with an extremely small database. Also; we propose a method to track motion ofthe face from the video image. In this system; movement and rotation of the head is detectedby template matching using a 3D personal face wire-frame model. By this technique; anautomatic video translation can be achieved.,Multimedia and Expo; 2001. ICME 2001. IEEE International Conference on,2001,1
Automatic face tracking and model match-move automatic face tracking and model match-move in video sequence using 3D face model in video sequence using 3D...,Takafumi Misawa; Kazumasa Murai; Satoshi Nakamura; Shigeo Morishima,ABSTRACT A stand-in is a common technique for movies and TV programs in foreignlanguages. The current stand-in that only substitutes the voice channel results awkwardmatching to the mouth motion. Videophone with automatic voice translation are expected tobe widely used in the near future; which may face the same problem without lip-synchronized speaking face image translation. In this paper; we propose a method to trackmotion of the face from the video image; that is one of the key technologies for speakingimage translation. Almost all the old tracking algorithms aim to detect feature points of theface. However; these algorithms had problems; such as blurring of a feature point betweenframes and occlusion of the hidden feature point by rotation of the head; and so on. In thispaper; we propose a method which detects movement and rotation of the head given the …,Multimedia and Expo; 2001. ICME 2001. IEEE International Conference on,2001,1
色ヒストグラムインターセクションを用いたリアルタイム人物顔抽出,吉村哲也， 市川忠嗣， 森島繁生， 相澤清晴， 齊藤隆弘,抄録 人物頭部が動いたり; 顔表情が変化しても顔を抽出するため; 色ヒストグラムインタセクションの類似尺度を用いて物体を抽出する手法を改良し; 顔の抽出に適用した. 改良では;従来のアクティブ探索法をより高速化する手法を考案した. 本手法を用いて; 顔抽出実験を行い;顔抽出の安定性と処理速度に関する評価を行った.,映像情報メディア学会誌,2001,1
A-14-1 テキスト情報を利用した発話音声の自動セグメンテーションと感情音声分析への応用,吉住悟， 緒方信， 森島繁夫,有声 爵区間を検出する. 音 声情報か ら得 られた有声音 区問 とテ キ ス ト情報か ら得 られた有声音区間を対応付 け (図 1 参照); その 区間内お い て推定 され る音声 とテ ン,電子情報通信学会総合大会講演論文集,2001,1
ネットワークシアタ: 仮想環境とコンピュータネットワークによるコンテンツ作成システム,高橋和彦， 楜沢順， 四倉達夫， 森島繁生， 鉄谷信二， 中津良平,ントーサーバシステムによりプロトタイプを構築し; 演技者の演技によりストーリを展開可能である事;監督がイベントオブジェクトの変更とストーリスクリプトの編集によりストーリを制御可能である事;及びインターネットを経由してコンテンツを鑑賞できるというネットワークシアタ基本機能の動作とその実現性が確認された.,画像電子学会誌,2001,1
Hypermask: talking head projected onto moving surface,SHIGEO Morishima; TATSUO Yotsukura,Hypermask is a system which projects an animated face onto a physical mask; worn by anactor. As the mask moves within a prescribed area; its position and orientation are detectedby a camera; and the projected image changes with respect to the viewpoint of theaudience. The lips of the projected face are automatically synthesized in real time with thevoice of the actor; who also controls the facial expressions. As a theatrical tool; Hypermaskenables a new style of storytelling. As a prototype system; we propose to put a self-contained Hypermask system in a trolley (disguised as a linen cart); so that it projects ontothe mask worn by the actor pushing the trolley.,Image Processing; 2001. Proceedings. 2001 International Conference on,2001,1
Multimodal translation.,Shigeo Morishima; Shin Ogata; Satoshi Nakamura,ABSTRACT A stand-in is a common technique for movies and TV programs in foreignlanguages. The current stand-in that only substitutes the voice channel results awkwardmatching to the mouth motion. Videophone with automatic voice translation are expected tobe widely used in the near future; which may face the same problem without lip-synchronized speaking face image translation. We introduce a multi-modal English-to-Japanese and Japanese-to-English translation system that also translates the speaker sspeech motion while synchronizing it to the translated speech. To retain the speaker s facialexpression; we substitute only the speech organ s image with the synthesized one; which ismade by a three-dimensional wire-frame model that is adaptable to any speaker. Ourapproach enables image synthesis and translation with an extremely small database …,AVSP,2001,1
韻律情報の制御による感情音声合成のための声質変換,緒方信， 四倉達夫， 森島繁生,抄録 感情音声が合成可能となれば; 人間と機会とのノンバーバルなコミュニケーションが実現できるのみならず; 人間同士の対話も円滑化する新しいコミュニケーションシステムが実現可能となる.しかし自然音声に感情を付加する為には; 原音声のクオリティ; 発話内容; 話者の情報を保ちつつ;韻律情報を制御しなくてはならない. 本稿では; 音声系列中の各母音を切り出してピッチ制御を行い;文節単位でイントネーションを変化させ; さらに発話速度や音の強弱の制御によって;感情表現付加が可能なシステムを開発した. 本手法により無感情音声から原音声のクオリティを保ったまま合成感情音声の作成が可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2000,1
レイヤモデルによるヘアスタイルデザインシステムの構築,佐野和成， 岸啓補， 森島繁生,ク ス) に よ る 人物の 画像合成が; 様々 な分野で 行われ て い る. しか し; リアル な画像合成を CGによ り表現する こ と は 困難で あ り; 中でも頭髪等の 表現 は 複雑で 過去様々 な手法が提案されて きた. 運動を踏まえた上で の CG に よ る頭髪の 表現方法に は頭髪,2000 年電子情報通信学会総合大会; 情報システム 2,2000,1
Face image analysis and synthesis for human-computer interaction,S Morishima,In this paper; we describe research results about how to generate an avatar's face on arealtime process exactly copying a real person's face. It is very important for synthesis of areal avatar to duplicate emotion and impression precisely included in the original face imageand voice. A face fitting tool from multi-angle camera images is introduced to make a real 3Dface model with real texture and geometry very close to the original. When the avatar isspeaking something; the voice signal is very essential to decide the mouth shape feature. Soa real-time mouth shape control mechanism is proposed for conversion from speechparameters to lip shape parameters using a multi-layer neural network. For dynamicmodeling of the facial expression; a muscle structure constraint is introduced to generate thefacial expression naturally with a few parameters. We also tried to get muscle parameters …,Signal Processing Proceedings; 2000. WCCC-ICSP 2000. 5th International Conference on,2000,1
Hyper Mask: 3 次元顔モデルを用いた仮面の表現技法,四倉達夫， Kim Binsted， Frank Nielsen， Claudio Pinhanez， 鉄信二， 森島繁生,概要 Hyper Mask とは従来単一の顔を表現する仮面の概念を進化させ; 1 つの仮面からあらゆる表情や人物を自由に生成可能なシステムである. このシステムを用いることで; その仮面を装着した役者の表現の幅や新しい演出方法が生み出されていくと考えられる. 白色に塗装された仮面上に 5つの LED が装着されており; プロジェクタによって任意の顔画像を投影し; ストーリーや観客とのインタラクションによって仮面の顔を変化する. 仮面に顔画像を正しく投影させるため; カメラから 5つの LED を検出し仮面の位置を求めている. また投影されている顔画像は演技者の音声を分析することによりリアルタイムに音声同期して口形状のアニメーションを行い; 顔表情もまたユーザが任意に変形可能である. 本稿では Hyper Mask システムを用いた演出支援装置を紹介し;新たな仮面の表現技法を確立した.,芸術科学会第 16 回 NICOGRAPH/MULTIMEDIA 論文集,2000,1
Image Processing Tools for Human Facial Analysis and Synthesis,S Morishima; Y Yagi,*,SYSTEMS CONTROL AND INFORMATION,2000,1
Real-time voice driven facial animation system,Shigeo Morishima,Recently computers have enabled users to walk through cyberspace using interactive virtualreality techniques. An avatar in cyberspace can allow a virtual face-to-face communicationenvironment. The article realizes an avatar which has a real face in cyberspace to constructa multi-user communication system by voice transmission through a network. A voice from amicrophone is transmitted and analyzed; then mouth shape and facial expression of theavatar are synchronously estimated and synthesized in real time. The author also introducesan entertainment application of a real time voice driven synthetic face. This project is named"Fifteen Seconds of Fame" which is an example of an interactive movie.,Systems; Man; and Cybernetics; 1999. IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on,1999,1
顔の情報処理--表情認識・合成技術の現状と課題,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,1998,1
3D estimation of facial muscle parameter from the 2D marker movement using neural network,Takahiro Ishikawa; Hajime Sera; Shigeo Morishima; Demetri Terzopoulos,Abstract Muscle based face image synthesis is one of the most realistic approach to realizelife-like agent in computer. Facial muscle model is composed of facial tissue elements andmuscles. In this model; forces are calculated effecting facial tisssue element by contraction ofeach muscle strength; so the combination of each muscle parameter decide a specific facialexpression. Now each muscle parameter is decided on trial and error procedure comparingthe sample photograph and generated image using our Muscle-Editor to generate a specificface image. In this paper; we propose the strategy of automatic estimation of facial muscleparameters from 2D marker movements using neural network. This corresponds to the non-realtime 3D facial motion tracking from 2D image under the physics based condition.,Asian Conference on Computer Vision,1998,1
顔面筋肉モデルに基うく表情トラッキングと再合成,石川貴博， 矢崎和彦， 世良元， 森島繁生,抄録 コンピュータ上で顔表情を表現する手法の 1 つである顔面筋肉モデル [1][2][3] は;モデル化された皮膚組織及び表情筋を持ち筋肉が皮膚組織に与える影響力を計算し;皮膚組織を変形させることによって; 表情表出が可能である. 表情を決定するパラメータは;筋肉が収縮するカ (筋肉パラメータ) である. 筋肉パラメータから顔表情への変換は;カの重ね合わせによって行われるが; 特定の表情に対応する筋肉パラメータの決定は;試行錯誤的に行う必要があった. しかし; 本稿では 1 台のカメラで撮影した顔面の 2次元移動量から筋肉パラメータの自動推定し; 顔表情の再合成を行った. これは同時に筋肉モデルという制約下の元で正面画像のみから得られる 2 次元の顔表情の情報から 3次元の表情を追跡・合成することに相当している.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,1997,1
4-5 顔画像によるアドバンストエージェント,金子正秀， 森島繁生,「顔」 は人 間 に とって非 常 に身近 な存 在 で あ り; 様々な情報を担 っている.「顔」 によって表現される情報 としては; ま ず;「顔」 の持 ち主で ある一人一人の人間の個人情報 (誰 であるか; 性 別;年 齢等) が 挙げられ る. 一 方;「顔」 に様々な形で現れる; 心 理状態等の人間の内面に関わる情報も重要である. 後 者 はコミュニケーションにおいて; 言 語情報 とは異なる非言語的な独自の情報を担 ってお り; 感 性的 な; ま た; より多様 なコミュニケーションを行う上で重要な役割を果た している. このような観点から; 人 間が計算機や コミュニケーションメディアを利用する際 に; 人 間を相手にす るのと同じような形態で情報のや り取 りを行 うことを可能にするユーザインタフェース として; 計 算機や通信端末上 に擬人化されたエージェントを用意することが検討 されてきている 1). 擬 人化 されたエージェン トの役割 としては; ユ ーザインタフェースとしてユーザ とシステムとの対話の円滑化 を図 り; ま た; ユ ーザにシステムに対 …,映像情報メディア学会誌,1997,1
Expression Recognition and Synthesis for Face-to-Face Communication,Shigeo Morishima,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Advances in Human Factors/Ergonomics; B,1997,1
空間周波数を利用した実時間顔表情認識,坂口竜己， 森島繁生,抄録 A considerable number of studies have been made on the facial expression recognitiontechniques in pshycological field; engineering and so on. However; it remains an unsettledproblem that trade-off between recognition accuracy and calculation cost. It disturbsrealizing real-time processing. In the last few years; real-time recognition systems which usea high-speed graphics workstation or a transputer have been seen. They need such a high-end computer system; so it is difficult to use it as a simple interface between computer andhuman. In this paper; we propose a real-time facial expression recognition method on theassumption that it runs on generic (low-cost) work-station or PC with video capture function.A face extraction is based on simple temporal differential image. One-dimensionalcorrelation matching method is used for a feature tracking. And performing discrete …,テレビジョン学会技術報告,1996,1
GUI を用いたヘアスタイルデザインシステムの開発,三枝太， 安藤真， 森島繁生,抄録 ヒューマン・インタフェース; 知的画像符号化などの分野での表情合成技術においては;人物頭部画像のリアルな合成が必要不可欠なものとなっている. 筆者らは; 頭髪を 「空間曲線」によって近似し; 近似的なアンチエイリアシングや予測を用いた効率的なレンダリングを取り入れることで; より高速で質の高い画像の生成に成功した. 頭髪の生成には; 予め与えられた人物頭部の 3次元モデル表面に自動的に生成する方法を提案した. しかし; この手法では髪型をインタラクティブにデザインできないという問題点が残されていた. そこで髪型をインタラクティブに編集するインタフェースの実現により; より自然な頭髪画像の生成に成功したので報告する.,電子情報通信学会総合大会講演論文集,1996,1
分析・合成のための表情記述パラメータ推定,川上文雄， 森島繁生， 山田寛， 原島博,抄録 人とコンピュータが Face-to-Facc でコミュニケーションできるような環境の構築を目指している.これまで; 5 層ニューラルネットの恒等写像能力を用いて; 17 次元の表情パラメータ空間をその中間層に 3 次元に圧縮して感情空間と仮定し; 表情からこの空間への写像とその逆写像 (感情空間→表情) を実現し表情の分析・合成を行うシステムの構築を行った. 一方で; この逆写像能力の心理学的妥当性を獲得すべく感情空間から合成される表情を刺激に主観評価実験もすでに行っている.しかしながら; ニューラルネットによって生成された感情空間は; 主に心理学の分野で用いられているFACS (Facial Action Coding Systcm) に基づき生成されたものである. したがって; 表情→感情空間のマッピングの入力は AU パラメータであるために入力された顔画像からまず AUを推定する必要がある. 本論文では主に; ニューラルネットを用いた AU 推定の手法について述べる.,電子情報通信学会総合大会講演論文集,1996,1
インタフェース エージェントのための表情分析 合成,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,知能情報メディア,1995,1
3 次元構造モデルに基づく自然顔画像の陰影制御,佐々木康， 小野英太， 森島繁生， 原島博,Page 1. グラフィクスとCAD 60ー3 (1992. 12. 17) 3次元構造モデルに基づく 自然魔E互像の陰影#微 °佐令木康 *小野英太 °森島繁生 *原島 博 AAAAA ſ成躁大学工学部#束京大学工学部 あらまし、実画像には照明の影響が含まれており、テクスチャマッピング手法により画像を合成 する場合、原画像に含まれる照明成分は対象物体の表面の模様として合成されている。 本論文では、対象を人物顔画像に限定し、人の顔の構造は一般的に滑らかで、構造に よって生じる陰影成分も空間的に滑らかであるという仮定を用いて、人物顔画像からテクスチャと照明による影響を分離し、照明による影響のみをCGによる手法を用いて再合成する試みについて述べる。 和文キーワード テクスチャ・マッピング 陰影 分析合成符另化 ControlShade effect of Human Face Image by 3D Model "Yasushi SASAKI …,情報処理学会研究報告グラフィクスと CAD (CG),1992,1
A realtime model‐based facial image synthesis based on multiprocessor network,Shigeo Morishima; Seiji Kobayashi; Hiroshi Harashima,Abstract Model-based image coding has been highlighted recently as a high-efficiencycoding method for TV telephone and TV conference systems. In a model-based codingsystem; an ultralow-rate image transmission is realized by obtaining common models of afacial image at both sides of a communication and by transmitting only modificationparameters between them. However; it is difficult to realize a realtime processing of a model-based coding with a conventional iterative-processing type computer since the amount ofmaterial to analyze and synthesize at both sides is very large.,Systems and computers in Japan,1991,1
マルチプロセッサ構成による知的画像符号化のためのリアルタイム表情合成の試み,森島繁生， 小林誠司， 原島博,近年; テレビ電話; テレビ会議システムにおける高能率符号化の一つの方法として知的符号化が注目を集めている. 知的符号化では; 送受信側で人物の顔画像に合わせたモデルを共有し;その変形パラメータのみを伝送することで; 超低レートでの画像通信を実現する. しかし;知的符号化では送受信側での分析・合成の処理量が大きくなり; 従来の逐次処理型コンピュータではリアルタイムでの処理は実現困難である. 通信手段としての実用化にはリアルタイムでの処理が絶対条件であり; 送信側; 受診側での処理の高速化が必要とされている. そこで本論文では;「トランスピュータ」 を用いたマルチプロセッサ構成によるリアルタイム表情合成法について報告する.トランスピュータは複数のプロセッサとの通信機能をもつマイクロコンピュータであり; 単体でも10MIPS の性能をもつ. 本方式により; プロセッサ 20 個を用いたネットワークでパイプライン処理することで; 毎秒およそ 16 コマのリアルタイム表情合成が可能となった. また; 処理対象を唇部分に …,電子情報通信学会論文誌 D,1990,1
知的通信と知的符号化,原島博， 森島繁生,ト ワ ーク も; 事業所や ビ ル を中心 に急速に普及 しつ つ ある. 光 フ ァ イバ を用いた B・ISDN(広帯域 工 SDN) の研究開発 も急で ある. そ して 今; 更 に次 の 世代の 新 しい通信形態を探ろうとする動きが少 しずつ 始まろうと して い る. その 一つ が; 本稿で 対象 とす る. r知的通信」(intelligent communica− tion) へ向けた動きで ある. 従来の電気通信の技術開発が;伝送路の大容量化や 高速 化な ど; い わば量的拡大を指向 して い たの に対 し; 知的通信で は「よ り便利な」 通信の 実現を 目指 して い る.,日本音響学会誌,1989,1
顔画像によるヒューマンインタフェースの試み,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,第 3 回情報伝送と信号処理ワークショップ資料,*,1
Wrinkles Individuality Preserving Aged Texture Generation using Multiple Expression Images,Pavel A Savkin; Tsukasa Fukusato; Takuya Kato; Shigeo Morishima,Abstract: Aging of a human face is accompanied by visible changes such as sagging; spots;somberness; and wrinkles. Age progression techniques that estimate an aged facial imageare required for long-term criminal or missing person investigations; and also in 3DCG facialanimations. This paper focuses on aged facial texture and introduces a novel ageprogression method based on medical knowledge; which represents an aged wrinklesshapes and positions individuality. The effectiveness of the idea including expressionwrinkles in aging facial image synthesis is confirmed through subjective evaluation.,*,2018,*
Voice Animator: Automatic Lip-Synching in Limited Animation by Audio,Shoichi Furukawa; Tsukasa Fukusato; Shugo Yamaguchi; Shigeo Morishima,Abstract Limited animation is one of the traditional techniques for producing cartoonanimations. Owing to its expressive style; it has been enjoyed around the world. However;producing high quality animations using this limited style is time-consuming and costly foranimators. Furthermore; proper synchronization between the voice-actor's voice and thecharacter's mouth and lip motion requires well-experienced animators. This is essentialbecause viewers are very sensitive to audio-lip discrepancies. In this paper; we propose amethod that automatically creates high-quality limited-style lip-synched animations usingaudio tracks. Our system can be applied for creating not only the original animations but alsodubbed ones independently of languages. Because our approach follows the standardworkflow employed in cartoon animation production; our system can successfully assist …,International Conference on Advances in Computer Entertainment,2017,*
Quasi-developable garment transfer for animals,Fumiya Narita; Tsukasa Fukusato; Shunsuke Saito; Shigeo Morishima,Abstract In this paper; we present an interactive framework to model garments for animalsfrom a template garment model based on correspondences between the source and thetarget bodies. We address two critical challenges of garment transfer across significantlydifferent body shapes and postures (eg; for quadruped and human);(1) ambiguity in thecorrespondences and (2) distortion due to large variation in scale of each body part. Ourefficient cross-parameterization algorithm and intuitive user interface allow us to interactivelycompute correspondences and transfer the overall shape of garments. We also introduce anovel algorithm for local coordinate optimization that minimizes the distortion of transferredgarments; which leads a resulting model to a quasi-developable surface and hence readyfor fabrication. Finally; we demonstrate the robustness and effectiveness of our approach …,SIGGRAPH Asia 2017 Technical Briefs,2017,*
Outside-in monocular IR camera based HMD pose estimation via geometric optimization,Pavel A Savkin; Shunsuke Saito; Jarich Vansteenberge; Tsukasa Fukusato; Lochlainn Wilson; Shigeo Morishima,Abstract Accurately tracking a Head Mounted Display (HMD) with a 6 degree of freedom isessential to achieve a comfortable and a nausea free experience in Virtual Reality. Existingcommercial HMD systems using synchronized Infrared (IR) camera and blinking IR-LEDscan achieve highly accurate tracking. However; most of the off-the-shelf cameras do notsupport frame synchronization. In this paper; we propose a novel method for real time HMDpose estimation without using any camera synchronization or LED blinking. We extendedover the state of the art pose estimation algorithm by introducing geometrically constrainedoptimization. In addition; we propose a novel system to increase robustness to the blurred IR-LEDs patterns appearing at high-velocity movements. The quantitative evaluations showedsignificant improvements in pose stability and accuracy over wide rotational movements …,Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology,2017,*
DanceDJ: A 3D Dance Animation Authoring System for Live Performance,Naoya Iwamoto; Takuya Kato; Hubert PH Shum; Ryo Kakitsuka; Kenta Hara; Shigeo Morishima,Abstract. Dance is an important component of live performance for expressing emotion andpresenting visual context. Human dance performances typically require expert knowledge ofdance choreography and professional rehearsal; which are too costly for casualentertainment venues and clubs. Recent advancements in character animation and motionsynthesis have made it possible to synthesize virtual 3D dance characters in real-time. Themajor problem in existing systems is a lack of an intuitive interfaces to control the animationfor real-time dance controls. We propose a new system called the DanceDJ to solve thisproblem. Our system consists of two parts. The first part is an underlying motion analysissystem that evaluates motion features including dance features such as the postures andmovement tempo; as well as audio features such as the music tempo and structure. As a …,*,2017,*
Court-aware volleyball video summarization,Takahiro Itazuri; Tsukasa Fukusato; Shugo Yamaguchi; Shigeo Morishima,Abstract We propose a rally-rank evaluation based on the court transition information forvolleyball video summarization considering the contents of the game. Our method uses thecourt transition information instead of non-robust visual features such as the position of aball and players. Experimental results demonstrate the effectiveness that our method reflectsviewers' preferences over previous methods.,ACM SIGGRAPH 2017 Posters,2017,*
3D model partial-resizing via normal and texture map combination,Naoki Nozawa; Tsukasa Fukusato; Shigeo Morishima,Abstract Resizing of 3D model is necessary for computer graphics animation and applicationsuch as games and movies. In general; when users deform a target model; they built on abounding box or a closed polygon mesh (cage) to enclose a target model. Then; the resizingis done by deforming the cage with target model. However; these approaches are not goodfor detailed adjustment of 3D shape because they do not preserve local information. Incontrast; based on a local information (eg; edge set and weight map); Sorkine et al.[Sorkineand Alexa 2007; Sorkine et al. 2004] can generate smooth and conformal deformationresults with only a few control points. While these approaches are useful for some situations;the results depend on resolution and topology of the target model. In addition; theseapproaches do not consider texture (UV) information.,ACM SIGGRAPH 2017 Posters,2017,*
Retexturing under self-occlusion using hierarchical markers,Shoki Miyagawa; Yoshihiro Fukuhara; Fumiya Narita; Norihiro Ogata; Shigeo Morishima,Abstract Marker-based retexturing is to superimpose the texture on a target object bydetecting and identifying markers from within the captured image. We propose a new markerthat can be identified under a large deformation that involves self-occlusion; which was nottaken into consideration in the following markers. Bradley et al.[Bradley et al. 2009]designed the independent markers; but it is difficult to recognize them under complicatedocclusion. Scholz et al.[Scholz and Magnor 2006] created a circular marker with a singlecolor selected from multiple colors. They created ID corresponding to the alignment of colorsby one marker and the markers around it and identified by placing the marker so that the IDwould be unique. However; when some markers are covered by self-occlusion; thepositional relationship of the markers appears to be different from the original; so markers …,ACM SIGGRAPH 2017 Posters,2017,*
Beautifying font: effective handwriting template for mastering expression of Chinese calligraphy,Masanori Nakamura; Shugo Yamaguchi; Shigeo Morishima,Abstract We propose a novel font called Beautifying Font to assist learning techniques inwriting Chinese calligraphy. Chinese calligraphy has various expressions but they are hardto acquire for beginners. Beautifying Font visualizes the speed and pressure of brush-strokes so that users can intuitively understand how to write.,ACM SIGGRAPH 2017 Posters,2017,*
Court-Based Volleyball Video Summarization Focusing on Rally Scene,Takahiro Itazuri; Tsukasa Fukusato; Shugo Yamaguchi; Shigeo Morishima,Abstract In this paper; we propose a video summarization system for volleyball videos. Oursystem automatically detects rally scenes as self-consumable video segments andevaluates rally-rank for each rally scene to decide priority. In the priority decision; featuresrepresenting the contents of the game are necessary; however such features have not beenconsidered in most previous methods. Although several visual features such as the positionof a ball and players should be used; acquisition of such features is still nonrobust andunreliable in low resolution or low frame rate volleyball videos. Instead; we utilize the courttransition information caused by camera operation. Experimental results demonstrate therobustness of our rally scene detection and the effectiveness of our rally-rank to reflectviewers' preferences over previous methods.,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,2017,*
Facial video age progression considering expression change,Shintaro Yamamoto; Pavel A Savkin; Takuya Kato; Shoichi Furukawa; Shigeo Morishima,Abstract This paper proposes an age progression method for facial videos. Age is one of themain factors that changes the appearance of the face; due to the associated sagging; spots;and wrinkles. These aging features change in appearance depending on facial expressions.As an example; we see wrinkles appear in the face of the young when smiling; but the shapeof wrinkles changes in older faces. Previous work has not considered the temporal changesof the face; using only static images as databases. To solve this problem; we extend thetexture synthesis approach to use facial videos as source videos. First; we spatio-temporallyalign the videos of database to match the sequence of a target video. Then; we synthesizean aging face and apply the temporal changes of the target age to the wrinkles appearing inthe facial expression image in the target video. As a result; our method successfully …,Proceedings of the Computer Graphics International Conference,2017,*
ラリーシーンに着目したラケットスポーツ映像鑑賞システム,板摺貴大， 福里司， 河村俊哉， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2017,*
原曲スコアの音楽特徴量に基づくピアノアレンジ,高森啓史， 佐藤晴紀， 中塚貴之， 森島繁生,ピアノ演奏の楽しみ方の一つとして; 複数の演奏パートから構成される楽曲のピアノ編曲した譜面を演奏するようになった. しかし; 必ずしも好みの楽曲のピアノ編曲譜面を入手できるとは限らない.一方; 編曲譜面の作成を行うためには多大な労力と専門知識を必要とする. 本研究では原曲スコアから得られる音楽特徴量に基づきピアノアレンジを行う. 提案手法では; 原曲スコアを入力とし;右手・左手の編曲譜面をそれぞれ生成する. 右手部分はメロディにコード構成音を付加し;左手部分は既存のピアノ譜面より構成した伴奏データベースから音楽特徴量を計算し;原曲と類似度の高い伴奏を選択することで原曲の雰囲気に近い編曲譜面を生成する. 本手法により;原曲の雰囲気を反映したピアノアレンジを可能とした.,研究報告音楽情報科学 (MUS),2017,*
解析・計測 曲率に依存する反射関数を用いた半透明物体における法線マップ推定手法の提案,久保尋之， 岡本翠， 向川康博， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2017,*
コート情報に基づくバレーボール映像の鑑賞支援と戦術解析への応用の検討,板摺貴大， 福里司， 山口周悟， 森島繁生,我々はバレーボール映像を効率的に鑑賞するための一手法を提案する. 効率的に鑑賞するために;我々は試合内容把握において重要なラリーシーンを自動抽出する方法と; ラリーシーンを解析・検索する機能を開発した. これらの機能を実現するため; 我々はロバストなコート検出手法を紹介する.本手法は事前情報となるコートモデルの種類を変更することで; 他スポーツ映像への適用も可能である. また鑑賞支援に留まらず; 試合内容の解析や戦術解析技術としての拡張を検討する.,研究報告コンピュータビジョンとイメージメディア (CVIM),2017,*
光学的最短経路長を用いた表面下散乱の高速計算による半透明物体のリアルタイム・レンダリング (ビジュアルコンピューティング論文特集号),小澤禎裕， 谷田川達也， 久保尋之， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像電子学会誌 The journal of the Institute of Image Electronics Engineers of Japan: visual computing; devices & communications,2017,*
頭蓋骨形状を考慮した肥痩変化顔画像合成,福里司， 藤崎匡裕， 加藤卓哉， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像電子学会誌 The journal of the Institute of Image Electronics Engineers of Japan: visual computing; devices & communications,2017,*
Authoring System for Choreography Using Dance Motion Retrieval and Synthesis,Ryo Kakitsuka; Kosetsu Tsukuda; Satoru Fukayama; Naoya Iwamoto; Masataka Goto; Shigeo Morishima,Abstract Dance animations of a three-dimensional (3D) character rendered with computergraphics (CG) have been created by using motion capture systems or 3DCG animationediting tools. Since these methods require skills and a large amount of work from the creator;automated choreography synthesis system has been developed to make dance motionsmuch easier. However; those systems are not designed to reflect artists' preferred context ofchoreography into the synthesized result. Therefore; supporting the user's preference is stillimportant. In this paper; we propose a novel dance creation system that supports the user tocreate choreography with their preferences. A user firstly selects a preferred motion from thedatabase; and then assigns it to an arbitrary section of the music. With a few mouse clicks;our system helps a user to search for alternative dance motion that reflects his or her …,The 30th International Conference on Computer Animation and Social Agents (CASA 2017),2017,*
肖像画からの写実的な顔画像生成手法,中村優文， 山口周悟， 福里司， 森島繁生,情報学広場 情報処理学会電子図書館.,研究報告コンピュータグラフィックスとビジュアル情報学 (CG),2016,*
前景・背景分離に基づくカメラワーク特徴量の提案,加藤卓哉， 深山覚， 後藤真孝， 森島繁生,動画コンテンツの視聴が普及により; 動画コンテンツ検索の為の特徴量への注目が高まっている.中でもカメラワークは; 動画を撮影する技術を特徴づける重要な動画特徴であり;その分析技術の研究は多くなされている. 既存研究では; カメラワークは背景のみに依存すると定義し; カメラワーク特徴量が提案されている. しかしこうした手法では; トラッキングショットなどの前景の特徴に依存するカメラワークの違いを表現できず; 分類できる動画の種類が限られている.本研究では; 動画中の前景と背景を分離し; それぞれを特徴量化することで; カメラワークを分類する手法を提案する. 提案法では; 背景をカメラの動きと定義し; 前景をカメラと演者の位置関係や演者の行動と定義する新たな特徴量化手法を提案する. 本手法を用いることで; 従来手法では判別が困難であった様々なカメラワークの違いの分類に成功した.,研究報告コンピュータグラフィックスとビジュアル情報学 (CG),2016,*
Automatic dance generation system considering sign language information,Wakana Asahina; Naoya Iwamoto; Hubert PH Shum; Shigeo Morishima,Abstract In recent years; thanks to the development of 3DCG animation editing tools (egMikuMikuDance); a lot of 3D character dance animation movies are created by amateurusers. However it is very difficult to create choreography from scratch without any technicalknowledge. Shiratori et al.[2006] produced the dance automatic generation systemconsidering rhythm and intensity of dance motions. However each segment is selectedrandomly from database; so the generated dance motion has no linguistic or emotionalmeanings. Takano et al.[2010] produced a human motion generation system consideringmotion labels. However they use simple motion labels like" running" or" jump"; so theycannot generate motions that express emotions. In reality; professional dancers makechoreography based on music features or lyrics in music; and express emotion or how …,ACM SIGGRAPH 2016 Posters,2016,*
3D facial geometry reconstruction using patch database,Tsukasa Nozawa; Takuya Kato; Pavel A Savkin; Naoki Nozawa; Shigeo Morishima,Abstract 3D facial shape reconstruction in the wild environments is an important researchtask in the field of CG and CV. This is because it can be applied to a lot of products; such as3DCG video games and face recognition. One of the most popular 3D facial shapereconstruction techniques is 3D Model-based approach. This approach approximates afacial shape by using 3D face model; which is calculated by principal componentanalysis.[Blanz and Vetter 1999] performed a 3D facial reconstruction by fitting points fromfacial feature points of an input of single facial image to vertex of template 3D facial modelnamed 3D Morphable Model. This method can reconstruct a facial shape from a variety ofimages which include different lighting and face orientation; as long as facial feature pointscan be detected. However; representation quality of the result depends on the number of …,ACM SIGGRAPH 2016 Posters,2016,*
Video reshuffling: automatic video dubbing without prior knowledge,Shoichi Furukawa; Takuya Kato; Pavel Savkin; Shigeo Morishima,Abstract Numerous video have been translated using" dubbing;" spurred by the recentgrowth of video market. However; it is very difficult to achieve the visual-audiosynchronization. That is to say in general a new audio does not synchronize with actor'smouth motion. This discrepancy can disturb comprehension of video contents. There-foremany methods have been researched so far to solve this problem.,ACM SIGGRAPH 2016 Posters,2016,*
適合フィードバックに基づく好みを反映したダンス編集手法,柿塚亮， 佃洸摂， 深山覚， 岩本尚也， 後藤真孝， 森島繁生,概要: 本稿では; ユーザの好みを反映したキャラクターダンスアニメーション制作システムを提案する.本システムで; ユーザは入力楽曲の任意の部分に対して; データベース中の好みの振付を割り当てる. ここで; ユーザは適合フィードバックに基づくダンス検索システムを通して;マウスクリックのみでデータベース中の好みの振付を探すことができる. そして;ユーザが振付を指定しなかった楽曲部分は; システムが自動的にダンスをつなぎ合わせ;楽曲に合った一連のダンスを生成する. 本システムが実現されることで; ユーザが簡単に好みを反映したダンスを制作できることが期待される.,研究報告音楽情報科学 (MUS),2016,*
Perception of drowsiness based on correlation with facial image features,Yugo Sato; Takuya Kato; Naoki Nozawa; Shigeo Morishima,Abstract This paper presents a video-based method for detecting drowsiness. Generally;human beings can perceive their fatigue and drowsiness through looking at faces. Theability to perceive the fatigue and the drowsiness has been studied in many ways. Thedrowsiness detection method based on facial videos has been proposed [Nakamura et al.2014]. In their method; a set of the facial features calculated with the Computer Visiontechniques and the k-nearest neighbor algorithm are applied to classify drowsiness degree.However; the facial features that are ineffective against reproducing the perception of humanbeings with the machine learning method are not removed. This factor can decrease thedetection accuracy.,Proceedings of the ACM Symposium on Applied Perception,2016,*
老化時の皺の個人性を考慮した経年変化顔画像合成,加藤卓哉， 福里司， 森島繁生,An appearance of a human face changes with aging: sagging; spots; somberness; and wrinkleswould be observed. Considering these changes; aging simulation techniques that estimate anaged facial image is required for a long-term criminal or missing person investigation. One ofthe latest works proposed a photorealistic aging simulation method using a patch-based facialimage reconstruction. However; including this method; the latest works had a problem that theycannot represent an individuality of wrinkles which is one of the most important features thatrepresent the human individuality. The individuality of wrinkles is defined by the shape and theposition. In this paper; we introduce a novel aging simulation method with patch-based facialimage reconstruction; which could overcome problem mentioned above. To preserve a wrinklesindividuality; wrinkles which is in an expressive facial image is synthesized to an input …,情報処理学会論文誌,2016,*
Creating a realistic face image from a cartoon character.,Masanori Nakamura; Shugo Yamaguchi; Tsukasa Fukusato; Shigeo Morishima,Abstract We propose a method of creating a realistic face image from a 2D non-realisticcharacter such as a cartoon. Our system allows us to create a high quality face image that isapplicable for some application such as 3D character animation. Our system uses two keyalgorithms; one is an algorithm for synthesizing a novel face image without the warpingprocess; and the other is a searching algorithm; which search each optimal patch from thedatabase based on gradient distribution.,Symposium on Computer Animation (Posters),2016,*
Friction sound synthesis of deformable objects based on adhesion theory,Takayuki Nakatsuka; Shigeo Morishima,Abstract Friction sound is one of the closest sounds for us in any situations. Most of thosesounds are created by Foley artists in computer animations. However; synthesizing soundsin all scenes need technical skills and take high costs. In this research; we propose a novelphysically-based approach to synthesize various kinds of friction sounds based on dynamicssimulation.,Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation: Posters,2016,*
A choreographic authoring system for character dance animation reflecting a user's preference,Ryo Kakitsuka; Kosetsu Tsukuda; Satoru Fukayama; Naoya Iwamoto; Masataka Goto; Shigeo Morishima,Abstract We propose a new system for constructing character dance animation byconsidering animator's preferences. First; a user of the proposed system assigns a preferredmotion; obtained through a searching algorithm; to arbitrary part in the music. Then theproposed system automatically assigns motions to the other remaining parts of the music byusing motions in a database. By this system; we can create a new dance performance forcharacter animation considering a user's preference.,Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation: Posters,2016,*
Acquiring Curvature-Dependent Reflectance Function from Translucent Material,Midori Okamoto; Hiroyuki Kubo; Yasuhiro Mukaigawa; Tadahiro Ozawa; Keisuke Mochida; Shigeo Morishima,Acquiring scattering parameters from real objects is still a challenging work. To obtain thescattering parameters; physics-based analysis is ineffective because huge computationalcost is required to simulate subsurface scattering effect accurately. Thus; we focus onCurvature-Dependent Reflectance Function (CDRF); the plausible approximation of thesubsurface scattering effect. In this paper; we propose a novel technique to obtain scatteringparameters from real objects by revealing the relation between curvature and translucency.,Nicograph International (NicoInt); 2016,2016,*
LyricsRadar: 歌詞の潜在的意味に基づく歌詞検索インタフェース,佐々木将人， 吉井和佳， 中野倫靖， 後藤真孝， 森島繁生,概要: 本論文では; 歌詞の潜在的な意味に基づいた検索インタフェース LyricsRadarについて述べる. 従来の歌詞中の語句に対する全文検索システムでは; ある単語 (例:「涙」)をクエリとして入力すると; まったく異なる意味の歌詞を持つ楽曲 (例: 失恋の 「涙」 と感動の 「涙」)が混在した検索結果となり; ユーザの検索意図を十分反映できない問題があった.歌詞の意味をクエリとして的確に言葉で表現して入力するのは困難なため; 本研究では潜在的ディリクレ配分法を用いて歌詞が潜在的に持つ意味 (トピック) を自動的に分析し;歌詞のトピックの可視化を行った. 歌詞のトピックの比率を 5 角形内に着色して可視化するトピックレーダーチャートおよび; データベース中のすべての歌詞をトピックの類似度に応じて二次元平面上にマッピングした歌詞マップという 2 つの可視化が特徴である. 6;902 曲の歌詞データを用いたトピック分析結果の妥当性の評価および; 既存検索システムとの比較による被験者実験により …,情報処理学会論文誌,2016,*
Real-time rendering of heterogeneous translucent objects using voxel number map,Keisuke Mochida; Midori Okamoto; Hiroyuki Kubo; Shigeo Morishima,Abstract Rendering of tranlucent objects enhances the reality of computer graphics;however; it is still computationally expensive. In this paper; we introduce a real-timerendering technique for heterogeneous translucent objects that contain complex structureinside. First; for the precomputation; we convert mesh models into voxels and generate Look-Up-Table in which the optical thickness between two surface voxels is stored. Second; wecompute radiance in real-time using the precomputed optical thickness. At this time; wegenerate Voxel-Number-Map to fetch the texel value of the Look-Up-Table in GPU. UsingLook-Up-Table and Voxel-Number-Map; our method can render translucent objects withcavities and different media inside in real-time.,Proceedings of the 37th Annual Conference of the European Association for Computer Graphics: Posters,2016,*
Real-time rendering of heterogeneous translucent materials with dynamic programming,Tadahiro Ozawa; Midori Okamoto; Hiroyuki Kubo; Shigeo Morishima,Abstract Subsurface scattering is important to express translucent materials such as skin;marble and so on realistically. However; rendering translucent materials in real-time ischallenging; since calculating subsurface light transport requires large computational cost.In this paper; we present a novel algorithm to render heterogeneous translucent materialsusing Dijkstra's Algorithm. Our two main ideas are follows: The first is fast construction of thegraph by solid voxelization. The second is voxel shading like initialization of the graph. Fromthese ideas; we obtain maximum contribution of emitted light on whole surface in singlecalculation. We realize real-time rendering of animated heterogeneous translucent objectswith simple compute and our approach does not require any precomputation.,Proceedings of the 37th Annual Conference of the European Association for Computer Graphics: Posters,2016,*
四肢キャラクタ間の衣装転写システムの提案,成田史弥， 齋藤隼介， 福里司， 森島繁生,本稿では; 1 つのテンプレートとなる衣装モデルを任意のキャラクタに自動転写する手法を提案する.衣装を自動モデリングする研究として; テンプレートとなる衣装モデルを基に; 任意のキャラクタの体型に合った衣装モデルを生成する衣装転写に関する手法が数多く提案されてきた. しかし;これまでの衣装転写手法は主に人間に類似した体型を持つキャラクタを対象とし;人間と大きく異なる体型を持つキャラクタを対象とした手法はいまだに提案されてこなかった.そこで本稿では; スケール計測; 衣装転写; テクスチャ保存の 3 つの工程からなる総合的な衣装転写フレームワークを提案する. 現実世界における体型の測定方法に基づいた転写元と転写先のCG キャラクタのスケール測定という新しい概念の導入によって; 衣装のフィッティングを保存する.同時に; 生成した衣装モデルの形状を利用し; テクスチャ情報を最適化する機能も提案する.本技術は; 既存の衣装モデルを再利用することにより; ユーザが手軽に衣装モデリングをすること …,情報処理学会論文誌,2016,*
フレームリシャッフリングに基づく事前知識を用いない吹替映像の生成,古川翔一， 加藤卓哉， 野澤直樹， 森島繁生,近年; コンテンツの多言語化に伴い; 吹替処理の需要が高まっている. しかし;多くの吹替映像において口の動きと吹替音声は一致していない. 口の動きと音声の不一致は視聴者に違和感を与えるだけでなく; 内容の理解度を低下させてしまう. そのため; 口の動きと吹替音声が一致した吹替映像が求められる. Ezzat ら [1] は音素情報と合う口画像を生成し;それを既存の映像に合成することで口の動きと音声が一致する映像を生成した. しかし; Ezzat ら [1]の手法では 3 点問題がある. 1 つ目は; 口画像を複数画像の線形和で生成するため;結果映像が不鮮明になることである. 2 つ目は; 調音結合という; ある音を発する際の口の形が次の音に依存する現象を考慮していないことである. そのため; 一つの音に対して単一の口形状しか生成できず; 不自然な結果になってしまう. 3 つ目は; 言語ごとに異なる音素情報を必要とすることである.そこで本研究では; 声優と同じタイミングで同じ口形状が現れるように吹替対象の動画のフレーム …,第 78 回全国大会講演論文集,2016,*
要素間補間による共回転系弾性体の高速化,福原吉博， 斎藤隼介， 成田史弥， 森島繁生,弾性体は肌や布; ゴムなど多くの物体の運動を記述するモデルであり; コンピュータグラフィックスにとって欠かせないものである. しかし; 弾性体シミュレーションには大きな計算コストを要するため;多くの高速化の研究がなされてきた. 共回転系弾性体は大変形に対して頑強であるという特徴から;頻繁に使用されている弾性体モデルの 1 つである. 共回転系弾性体のシミュレーションの過程では;各ステップにおいてモデルを構成する全要素の回転行列が必要である. そのため;毎ステップごとに全要素に対して; 特異値分解を実行し; 回転行列を計算しなければならない.特異値分解については McAdams ら [2] の手法など高速な計算方法が提案されているが;膨大な要素からなるモデルの全要素に対して特異値分解を行うには; 依然として大きな計算コストを要する. そこで; 本稿ではサンプリングされた代表要素の回転行列から残りの要素の回転行列を滑らかに補間することにより; 特異値分解の工程の計算時間を削減し; シミュレーションを高速化 …,第 78 回全国大会講演論文集,2016,*
輝度の最大寄与値を用いた半透明物体のリアルタイムレンダリング,小澤禎裕， 岡本翠， 森島繁生,近年; ハードウェアの性能や描画技術の向上により; 様々な質感を高速に描画することが可能となった. しかし; 半透明物体のリアルタイムな描画は; 膨大な計算コストがかかるため困難である.半透明物体に入射した光の一部は; 物体表面で反射されずに内部に浸透する. 浸透した光は内部で複雑に散乱した後; 入射点と異なる点から再び出射する. この現象を表面下散乱と呼ぶ.表面下散乱を近似的に扱うことで; 半透明物体を高速描画する手法が数多く提案されている.Jensen ら [1] はダイポールモデルを用い; 半透明物体の高速な描画を実現した. しかし;リアルタイムな描画を行うためには; 依然として高コストである他; 均一な材質からなる物体にしか適用することができない. また; 拡散方程式を用いることで; 物体内部での光の広がりを記述する手法も提案されている. Wang ら [2] は物体を四面体で分割し; 四面体単位で拡散方程式を解くことによって; 不均一な材質から成る半透明物体を高速に描画した. しかし; 四面体への分割を高速に …,第 78 回全国大会講演論文集,2016,*
凝着説に基づく物体表面の弾性変形を考慮した摩擦音の生成手法の提案,中塚貴之， 森島繁生,CG 映像を演出する効果音は; 実際の音の録音やシンセサイザを用いた合成により作成されている.効果音の中でも摩擦音は車両のブレーキ音や擦弦楽器の音など多岐にわたり;個々の映像に合った音を作成するためには多くの労力と高い技術力が必要となる. したがって;摩擦音を手軽に生成することに対する需要は高い. このような問題に取り組んだ研究として;物理シミュレーションを用いたものがある. Ren ら [1] は三段階のスケールでの物体の表面モデルから摩擦音を生成したが; 剛体以外には適用できない問題があった. An ら [2]は布が他物体に接触している時のメッシュ頂点の速度とデータベース上の摩擦音を対応付けることで摩擦音を生成した. しかし; データベース作成の際には布同士を様々な速度で摩擦させて生じる音を録音する必要があり; 多大な労力がかかるという問題があった. そこで本研究では;摩擦現象を説明する理論の一つである凝着説に基づく物理シミュレーションによる摩擦音の生成 …,第 78 回全国大会講演論文集,2016,*
不均一な半透明物体の描画のための Translucent Shadow Maps の拡張,持田恵佑， 岡本翠， 久保尋之， 森島繁生,CG における人間の肌や歯; 大理石などの半透明物体表現は; 写実的 CG において重要である.しかし半透明物体における表面下散乱現象が及ぼす光の影響を物理的に正しく計算し;描画を行うためには; 計算コストの高い手法 [1][2] を用いる必要がある. したがって; ゲームなどのリアルタイムで動作することが求められるシーンにそのまま適用することは困難である. そこで;表面下散乱現象を近似することで; 半透明物体の高速描画を実現する様々な手法が研究されてきた. Dachsbacher らは; シャドウマップ [3] を用いることで; 物体の厚みに依存しやすい半透明物体の高速描画をおこなう Translucent Shadow Maps (TSM)[4] を提案した. TSM には;凹形状物体で正しい厚みが得られないという問題があるが; Kosaka ら [5] は誤差マップを作成することでこれを改善した. しかし; どちらの手法も; 物体に空洞や異なる媒質が含まれるような場合には;それらの影響を考慮することができず; 不自然な描画となってしまう問題がある. 本研究では …,第 78 回全国大会講演論文集,2016,*
顔画像特徴と眠気の相関に基づくドライバーの眠気検出,佐藤優伍， 野澤直樹， 森島繁生,近年; 自動車産業における; 安全運転支援システムの技術発展が目覚ましい. しかし;自動車による交通事故は依然として多発している. 警察庁交通局によると; 平成 26年の自動車死亡事故の原因において; 漫然運転が最大の割合を占めている [1]. したがって;漫然運転を未然に防ぐことが求められる. そこで私たちは; 漫然運転の一つである居眠り運転に着目した. ドライバーの眠気を検出する手法として; 脳波などの生体情報を測定するものが提案されている. しかし; 多くの既存システムは; ドライバーの身体にセンサーを装着する必要があるため;運転に集中することへの妨げとなる. したがって; ドライバーの身体に非接触な眠気検出手法が求められる. そこで中村ら [2] は; 正面から撮影した顔動画像を用いて; ドライバーの眠気度合を推定する手法を提案した. 中村らの眠気検出では; 顔動画像より得られる複数の特徴量に対して;機械学習が用いられている. しかし; 各特徴量の眠気に対する有効性を考慮していないため; 推定 …,第 78 回全国大会講演論文集,2016,*
パッチ単位の法線推定による三次元顔形状復元,野沢綸佐， 加藤卓哉， 野澤直樹， 森島繁生,近年; 一枚の顔画像から三次元顔形状を復元する試みが多くなされている. 例えば;顔形状を復元することで犯罪捜査における犯人の追跡や; エンタテイメントの分野において撮影された人物の顔から 3D アバターを作成し; ゲームに登場させることなどが挙げられる.一枚の顔画像のみを入力とし三次元顔形状復元を行う手法として前島ら [1] は;顔特徴点情報と統計モデルを用いた三次元顔形状復元の手法を提案した. しかし;頬など顔特徴点の分布が疎な領域では; 個人の特徴を反映した顔形状の復元を行うことが困難である. 一方 Kemelmacher ら [2] は; 陰影情報から三次元形状の復元を行う Shape-from-Shadingを顔画像に用いる手法を提案した. この手法では; 統計データを用いずに; 細部の個性を反映した顔形状の復元に成功している. しかし; 本来顔は部位ごとに異なる反射特性を持つにも関わらず;顔全体をランバート面と仮定しているため; 皺などの情報が失われてしまう場合がある. そこで本 …,第 78 回全国大会講演論文集,2016,*
似顔絵の個性を考慮した実写化手法の提案,中村優文， 山口周悟， 福里司， 古澤知英， 森島繁生,我々は似顔絵画像の実写化手法を提案する. 似顔絵はモデルに対して忠実に描かれた肖像画と;顔の特徴を誇張して描かれたイラストの二種類に分けられるが; 本研究では前者を対象とする.肖像画は顔の詳細な特徴を視覚的に伝える手段として用いられ; 主に人物の顔を記録するために描かれていた. 肖像画の特徴は; 顔の陰影や形状は写実的であり; 肌のテクスチャや色味は描画材料で描かれているため非写実的な点である. 従来; 様々な顔の研究・分析が行われてきたが;多くは写真を対象としている. 肖像画を分析するためには; 肌のテクスチャと色味を写実的に表現した画像が必要である. そこで我々は; 入力の肖像画の陰影や形状は保持しつつ;実写真の肌テクスチャと色味を参照することで; 肖像画の実写化を行う. 本研究により;肖像画の分析が容易になり; 歴史学や考古学への貢献が期待される. 似顔絵から写実的な画像を生成する手法として; 溝川ら [1] は実写真データベースを用いて; 陰影情報に基づいたパッチ合成 …,第 78 回全国大会講演論文集,2016,*
好みを反映したダンス生成のための振付編集手法,柿塚亮， 岩本尚也， 朝比奈わかな， 森島繁生,近年; キャラクタを音楽に合わせて踊らせる動画コンテンツが増加している. 一方;このようなコンテンツの制作にはダンスに関する知識や多大な労力を要する. このような背景から;入力音楽にシンクロしたダンスモーションを自動生成する技術が発展してきた. 白鳥ら [1] は;入力楽曲に対して盛り上がりやテンポが合うようなダンスモーションをデータベースから検索してつなぎ合わせていくことで; 新たな振付の提示を行う手法を提案した. しかし;こういった自動生成手法では振付の編集ができないため; ユーザの好みを生成される振付に反映できない. モーション編集の代表的なものとして Mukai らの研究 [2] がある. Mukai らの手法では;ソースモーションに対してタイムライン上の姿勢; 動作のタイミングや運動の速度を変更することで出力となるモーションが得られる. しかし; ソースモーションと大きく異なるモーションは得られないため; こういった編集手法は音楽の中の同一部分に多様な振付が割り当てられるダンスにおいては不 …,第 78 回全国大会講演論文集,2016,*
Frame-Wise Continuity-Based Video Summarization and Stretching,Tatsunori Hirai; Shigeo Morishima,Abstract This paper describes a method for freely changing the length of a video clip; leavingits content almost unchanged; by removing video frames considering both audio and videotransitions. In a video clip that contains many video frames; there are less important framesthat only extend the length of the clip. Taking the continuity of audio and video frames intoaccount; the method enables changing the length of a video clip by removing or insertingframes that do not significantly affect the content. Our method can be used to change thelength of a clip without changing the playback speed. Subjective experimental resultsdemonstrate the effectiveness of our method in preserving the clip content.,International Conference on Multimedia Modeling,2016,*
MultiMedia Modeling: 22nd International Conference; MMM 2016; Miami; FL; USA; January 4-6; 2016; Proceedings,Qi Tian; Nicu Sebe; Guo-Jun Qi; Benoit Huet; Richang Hong; Xueliang Liu,The two-volume set LNCS 9516 and LNCS 9517 constitutes the refereed proceedings of the22nd International Conference on Multimedia Modeling; MMM 2016; held in Miami; FL; USA;in January 2016. The 32 revised full papers and 52 poster papers presented were carefullyreviewed and selected from 117 submissions. In addition 20 papers were accepted for fivespecial sessions out of 38 submissions as well as 7 demonstrations (from 11 submissions)and 9 video showcase papers. The papers are organized in topical sections on videocontent analysis; social media analysis; object recognition and system; multimedia retrievaland ranking; multimedia representation; machine learning in multimedia; and interactionand mobile. The special sessions are: good practices in multimedia modeling; semanticsdiscovery from multimedia big data; perception; aesthetics; and emotion in multimedia …,*,2016,*
Computational Cartoonist: A Comic-Style Video Summarization System for Anime Films,Shigeo Morishima,Abstract. This paper presents Computational Cartoonist; a comicstyle anime summarizationsystem that detects key frame and generates comic layout automatically. In contract toprevious studies; we define evaluation criteria based on the correspondence betweenanime films and original comics to determine whether the result of comic-stylesummarization is relevant. To detect key frame detection for anime films; the proposedsystem segments the input video into a series of basic temporal units; and computes frameimportance using image characteristics such as motion. Subsequently; comic-style layoutsare decided on the basis of pre-defined templates stored in a database. Several resultsdemonstrate the efficiency of our key frame detection over previous methods by evaluatingthe matching accuracy between key frames and original comic panels.,MultiMedia Modeling: 22nd International Conference; MMM 2016; Miami; FL; USA; January 4-6; 2016; Proceedings,2016,*
Active Comicing for Freehand Drawing Animation,Tsukasa Fukusato; Shigeo Morishima,Abstract This paper presents Active Comicing; a prototype sketching system that providesenhanced frame interpolation capability for freehand drawing animation. In this system; theuser draws several 2D freeform strokes interactively on multiple frames; and the systemautomatically constructs stroke-to-stroke interpolation frames. To compose a comprehensiveand coherent least-distorting interpolation; we assume input stroke has ghost points; whichare additional points defined on stroke edges; and define affine transformations. In addition;the system semi-automatically guides the template motion of each stroke. For example; if theuser draws an arrow; the system assigns the stroke moves in the direction of the arrow. Toassign template motion; we compute the stroke similarity between the user's input and strokeinformation from a database. With this method; it is possible to generate stroke animation …,*,2016,*
MATHEMATICAL PROGRESS IN EXPRESSIVE IMAGE SYNTHESIS I.,Ken Anjyo,Over the last decade; computer graphics (CG) modeling has developed considerably;bringing greater fidelity and interaction in many applications. However; subjects such asfluids and virtual humans continue to pose CG challenges. The symposium ''MathematicalProgress in Expressive Image Synthesis''(MEIS2013) was held in Fukuoka; Japan; inOctober 21–23; 2013; as a unique venue where mathematicians; CG researchers; and thosewho work in industry came together to investigate these difficult subjects. Moreover; thesymposium was intended to trigger novel research directions such as exploring themathematics of visual perception. This volume presents the papers selected from theMEIS2013 proceedings; which was originally issued as MI Lecture Notes Vol. 50; KyushuUniversity; 2013. The book comprises five parts in order to organize the papers for …,*,2016,*
MATHEMATICAL PROGRESS IN EXPRESSIVE IMAGE SYNTHESIS I.,Ken Anjyo,Over the last decade; computer graphics (CG) modeling has developed considerably;bringing greater fidelity and interaction in many applications. However; subjects such asfluids and virtual humans continue to pose CG challenges. The symposium ''MathematicalProgress in Expressive Image Synthesis''(MEIS2013) was held in Fukuoka; Japan; inOctober 21–23; 2013; as a unique venue where mathematicians; CG researchers; and thosewho work in industry came together to investigate these difficult subjects. Moreover; thesymposium was intended to trigger novel research directions such as exploring themathematics of visual perception. This volume presents the papers selected from theMEIS2013 proceedings; which was originally issued as MI Lecture Notes Vol. 50; KyushuUniversity; 2013. The book comprises five parts in order to organize the papers for …,*,2016,*
Facial Fattening and Slimming Simulation Based on Skull Structure,Masahiro Fujisaki; Shigeo Morishima,Abstract In this paper; we propose a novel facial fattening and slimming deformation methodin 2D images that preserves the individuality of the input face by estimating the skullstructure from a frontal face image and prevents unnatural deformation (eg penetration intothe skull). Our method is composed of skull estimation; optimizing fattening and slimmingrules appropriate to the estimated skull; mesh deformation to generate fattening andslimming face; and generation background image adapted to the generated face contour.Finally; we verify our method by comparison with other rules; precision of skull estimation;subjective experiment; and execution time.,International Symposium on Visual Computing,2015,*
視線追跡データから算出された注目領域に基づく視線移動の少ない字幕配置法の提案と評価,赤堀渉， 平井辰典， 森島繁生,概要: 本稿では; 字幕付き映像における視線移動の少ない字幕配置法を提案する.従来の画面端に固定された字幕では; 映像で視線が集中する領域と字幕が離れている場合に視線移動が増えるため; 映像を満足に鑑賞することができなかった. 既存研究では; 映像内容の理解度の向上のために動的に字幕を配置したが; 字幕なし映像のどこに視線が集まるかを考慮していないため; 字幕に視線が誘導され映像を集中して鑑賞することができなかった. そこで本研究では;字幕付き映像における新たな字幕配置方法を提案する. 提案手法では; 字幕なし映像に対して視線が集中する領域と字幕が重なることを避けるため; 複数人の視線情報を測定し映像において視線が集まりやすい領域を算出する. その後; 鑑賞者の字幕に対する認知的な負荷を考慮するため;字幕配置のルールとして注目される領域の下部に固定して字幕を配置する. また;評価実験により提案手法の有効性を示した.,研究報告エンタテインメントコンピューティング (EC),2015,*
三次元形状を考慮した半透明物体のリアルタイムレンダリング,持田恵佑， 岡本翠， 小澤禎裕， 久保尋之， 森島繁生,概要: 人肌; 歯; 大理石などといった半透明物体のレンダリング技術は; 高品質な CGを制作する上で欠かすことができないが; 表面下散乱の影響を考慮する必要があるため;膨大な計算時間を必要となることが課題であった. 本研究では; メッシュデータをボクセライズすることで; 物体の表面だけでなく内部の構造を含めた三次元形状を把握することで; 物体の厚みに依存しやすい単一散乱の影響による放射輝度の近似計算が可能となり; 半透明物体のリアルタイムレンダリングを実現する. 本手法を用いることで; 既存手法では表現できなかった物体内部に空洞などの不均一な構造を含むような形状においても; 破綻のない高品質な描画が可能である.また三次元形状を Look Up Table (LUT) に格納して各シェーダステージから参照することで; GPUを用いた高速な描画を実現する.,研究報告コンピュータビジョンとイメージメディア (CVIM),2015,*
Region-based painting style transfer,Shugo Yamaguchi; Takuya Kato; Tsukasa Fukusato; Chie Furusawa; Shigeo Morishima,Abstract In this paper; we present a novel method for creating a painted image from aphotograph using an existing painting as a style source. The core idea is to identify thecorresponding objects in the two images in order to select patches more appropriately. Weautomatically make a region correspondence between the painted source image and thetarget photograph by computing color and texture feature distances. Next; we conduct apatch-based synthesis that preserves the appropriate source and target features. Unlikeprevious example-based approaches of painting style transfer; our results successfullyreflect the features of the source images even if the input images have various colors andtextures. Our method allows us to automatically render a new painted image preserving thefeatures of the source image.,SIGGRAPH Asia 2015 Technical Briefs,2015,*
主成分分析に基づく類似口形状検出によるビデオ翻訳動画の生成,古川翔一， 加藤卓哉， 野澤直樹， 森島繁生,概要: 映画やテレビ番組などの映像コンテンツでは; 吹替処理を施すことによって多言語化を実現している. 一般に吹替処理では映像中の話者の口の動きに合わせて翻訳内容が考案される. しかし;これは翻訳家に大きな労力を強いる上; 本来の意味と異なる台詞に翻訳される恐れがある. そこで;従来の吹替処理とは逆に; 吹替音声に合わせて映像内の口形状を直接編集する 「ビデオ翻訳」が提案されている. ビデオ翻訳を行う多くの既存手法では 3D 顔モデルが用いられるが;これらは首などの形状復元できない部位に適用できない問題がある. また結果映像はレンダリングやテクスチャマッピングの精度に依存するため; 顔としての自然さに欠ける恐れがある.そこで本研究では自然な結果が得られる画像処理ベースのビデオ翻訳手法を提案する.入力には俳優と声優の 2 つの発話動画を用いる. 俳優の動画フレームのうち声優口形状と最も類似するものを主成分分析を用いて選択し; フレーム間の時間連続性を考慮しつつ並び替える. これ …,研究報告コンピュータビジョンとイメージメディア (CVIM),2015,*
動的計画法を用いた半透明物体のリアルタイムレンダリング,小澤禎裕， 岡本翠， 久保尋之， 森島繁生,概要: 人肌をはじめとした半透明物体は; 物体内部で生じる表面下散乱現象のシミュレーションに膨大な計算が必要となるため; 高速な描画は依然として困難であると言える. 本研究では;歩数マップを用いて表面下散乱における光輸送の経路を近似的に計算することで;半透明物体を高速に描画する手法を提案する. まず; 対象とするオブジェクト上に歩数マップを作成し; 散乱光の入射点と出射点との測地線距離を取得する. 次に; その距離に応じて入射光の光線強度を減衰させることで; 半透明物体における光の伝播をモデル化する. 本研究によって多重散乱の支配的な半透明物体を実現した.,研究報告コンピュータビジョンとイメージメディア (CVIM),2015,*
パッチタイリングを用いた法線推定による 3 次元顔形状復元,野沢綸佐， 加藤卓哉， 藤崎匡裕， 森島繁生,概要: 顔画像からの 3 次元顔形状復元は; セキュリティなどの分野で応用が期待される重要な研究課題である. 復元手法は既にいくつか提案されているが; 代表的な既存手法として顔の特徴点情報を用いるものがある. しかし; 特徴点情報は疎な情報であるため; 頬など特徴点が取れない領域では精度が悪くなる問題があった. そこで本手法では; 輝度値と法線が密接な関係にあると仮定し;輝度値と法線を対応付けた小片画像 (以下パッチ) のデータベースを作成する.このデータベースを用いて; 1 枚の入力正面顔画像の法線をパッチの再構成によって推定することで;3 次元顔形状の復元を行う. パッチという密な情報を用いて 3 次元顔形状の復元を行うため;従来手法と比較してより高精度な結果を得ることが可能となる.,研究報告グラフィクスと CAD (CG),2015,*
VRMixer: 動画コンテンツと現実世界の融合とその適用可能性の検証,牧良樹， 中村聡史， 平井辰典， 湯村翼， 森島繁生,本稿では; 動画コンテンツと現実世界とを融合させ動画の世界の中に入り込むことを可能にするVRMixer をもとに改良を行い; その動画世界内で体の大小を変化させながら自在に楽しむことを可能とする仕組みを実現する. また; 種々のコンテンツに対して本手法を適用し;ユーザ実験を実施することによって動画コンテンツに入り込む楽しさについて検証を行うとともに;その問題についても明らかにする.,エンタテインメントコンピューティングシンポジウム 2015 論文集,2015,*
曲率依存反射関数を用いた半透明物体における照度差ステレオ法の改善,岡本翠， 久保尋之， 向川康博， 森島繁生,本研究では; 多重散乱が支配的な半透明物体における; 表面下散乱現象を考慮した照度差ステレオ法の改善を提案する. 照度差ステレオ法は; 複数の照明条件下における物体の陰影情報から法線を推定する手法である. 入射光が物体内で表面下散乱を繰り返す半透明物体においては;単純なランバート反射を仮定した解析では散乱による影響を考慮できていない. そこで我々は;半透明物体における表面下散乱現象の近似的なモデルである曲率依存反射関数を用いることで;高速に法線を推定する手法を提案する. まず; ランバート拡散反射を仮定して取得した初期法線情報から; 物体表面における曲率を計算する. つぎに; 計算された曲率の値に応じて反射関数を適用し;法線の再計算を行うことで; 表面下散乱の影響を考慮した法線の推定を行う.,研究報告コンピュータビジョンとイメージメディア (CVIM),2015,*
Screen Space Hair Self Shadowing by Translucent Hybrid Ambient Occlusion,Zhuopeng Zhang; Shigeo Morishima,Abstract Screen space ambient occlusion is a very efficient means to capture the shadowscaused by adjacent objects. However it is incapable of expressing transparency of objects.We introduce an approach which behaves like the combination of ambient occlusion andtranslucency. This method is an extension of the traditional screen space ambient occlusionalgorithm with extra density field input. It can be applied on rendering mesh objects; andmoreover it is very suitable for rendering complex hair models. We use the new algorithm toapproximate light attenuation though semi-transparent hairs at real-time. Our method isimplemented on common GPU; and independent from pre-computation. When it is used inenvironment lighting; the hair shading is visually similar to however one order of magnitudefaster than existing algorithm.,International Symposium on Smart Graphics,2015,*
楽曲のビート類似度及び潜在トピックの類似度に基づく DJ プレイの自動化,平井辰典， 土井啓成， 森島繁生,概要: 本稿では; 楽曲同士を滑らかにミックスして切り替える DJ プレイの自動化手法を検討する. DJプレイにおいて最も重視されるものの一つにビートであり; ビートを一致させることで曲を繋げる研究は提案されてきた. 一方で人間の DJ はビート等の機械的な事象だけでなく; 楽曲の意味等の高等な事象についても考慮して曲を繋げることが多い. 本研究では; 楽曲のビートの繋がりを維持するためのビート類似度の考慮と; さらに楽曲の音響的な意味にまで踏み込んだ潜在トピックの類似度を考慮する. それによって; 人間が行う DJ プレイに近い楽曲同士のミックスの実現を目指す.本研究では楽曲の意味を考慮するために; 音響信号に対するトピック分析を行う. そこで;各トピックに明確な意味を持たせるために音響信号の言語化手法を導入し; 従来の自然言語処理におけるトピック分析に近い形で楽曲の音響信号の潜在的な意味を分析する. これによって;ビートの類似度だけでなく潜在的トピックの類似度までも考慮した楽曲同士のミックスとそれに …,研究報告音楽情報科学 (MUS),2015,*
要素間補間による共回転系弾性体シミュレーションの高速化,福原吉博， 斎藤隼介， 成田史弥， 森島繁生,概要: 弾性体は肌や布など多くの変形可能な物体の運動を記述するモデルとして; コンピューターグラフィクス (CG) において必要不可欠なものである. 本稿では; CG の分野において多用されている弾性体モデルの 1 つである共回転系弾性体のシミュレーションについて; クォータニオンブレンディングを用いた回転行列の要素間補間による高速化手法を提案する. 本手法により;従来の共回転系弾性体シミュレーションとほぼ同等の結果をより高速に得ることが可能となる.,研究報告グラフィクスと CAD (CG),2015,*
Automatic synthesis of eye and head animation according to duration and point of gaze,Hiroki Kagiyama; Masahide Kawai; Daiki Kuwahara; Takuya Kato; Shigeo Morishima,Abstract In movie and video game productions; synthesizing subtle eye and correspondinghead movements of CG character is essential to make a content dramatic and impressive.However; to complete them costs a lot of time and labors because they often have to bemade by manual operations of skilled artists.,ACM SIGGRAPH 2015 Posters,2015,*
Texture preserving garment transfer,Fumiya Narita; Shunsuke Saito; Takuya Kato; Tsukasa Fukusato; Shigeo Morishima,Abstract Dressing virtual characters is necessary for many applications; while modelingclothing is a significant bottleneck. Therefore; it has been proposed that the idea of GarmentTransfer for transfer-ring clothing model from one character to another character [Brouet etal. 2012]. In recent years; this idea has been extended to be applicable between charactersin various poses and shapes [Narita et al. 2014]. However; texture design of clothing is notpreserved in their method since they deform the source clothing model to fit the target body(see Figure 1 (a)(c)).,ACM SIGGRAPH 2015 Posters,2015,*
3D face reconstruction from a single non-frontal face image,Naoki Nozawa; Daiki Kuwahara; Shigeo Morishima,Abstract A reconstruction of a human face shape from a single image is an important themefor criminal investigation such as recognition of suspected people from surveillance cameraswith only a few frames. It is; however; still difficult to recover a face shape from a non-frontalface image. Method using shading cues on a face depends on the lighting circumstance andcannot be adapted to images in which shadows occurs; for example [Kemelmacher et al.2011]. On the other hand;[Blanz et al. 2004] reconstructed a shape by 3D Morphable Model(3DMM) only with facial feature points. This method; however; requires the pose-wisecorrespondences of vertices in the model to feature points of input image because a facecontour cannot be seen when the facial direction is not the front. In this paper; we propose amethod which can reconstruct a facial shape from a non-frontal face image only with a …,ACM SIGGRAPH 2015 Posters,2015,*
頬のシルエット情報を活用した単一斜め向き顔画像に対する顔形状 3 次元復元手法,野澤直樹， 森島繁生,犯罪捜査において; 監視カメラから得られる顔の情報は非常に大きな役割を持っている. しかし;監視カメラから得られる顔画像のほとんどが斜め方向を向いてしまっているため;人物の追跡や特定に支障をきたしているといえる. そこで; 画像上でこれら斜めを向いた顔画像から正面顔を作成する手法が提案されているが; 顔の歪みや本人との低い類似性などの問題があり;形状を考慮する必要性が浮上している. 本研究では; 犯罪捜査を支援するために;斜めを向いた顔画像から 3 次元形状復元を行う手法を提案する.,研究報告グラフィクスと CAD (CG),2015,*
音声と映像の変化に注目したフレーム間引きによる動画要約手法,平井辰典， 森島繁生,概要: 本稿では; 動画を短時間で視聴することを目的としたフレーム間引きによる動画要約手法を提案する. 本要約手法では; 動画における音声と映像の変化に注目して; 変化の少ない冗長なフレームを間引いていくことで動画の内容を保持したまま動画の長さを短くする.フレームを間引くごとに; 間引いた箇所の音声と映像の変化を再計算することで;動画の連続性を補償したままの要約が可能となる. 本手法によって; より短時間で動画を視聴することができるだけでなく; 変化の少ないフレームを挿入することで音声や映像がスロー再生されることなく動画の長さを伸ばすことも可能となる. 主観評価実験によって本手法の有効性について検討した.本稿ではさらに; 本手法を応用することによる動画中の移動物体の削除や音楽と映像の同期についても検討する.,研究報告音楽情報科学 (MUS),2015,*
VoiceDub: 複数タイミング情報をともなう映像エンタテイメント向け音声同期収録支援システム,川本真一， 森島繁生， 中村哲,概要: 本稿は; 映像コンテンツに対して同期のとれた台詞音声の収録を発話者自身による簡単な操作によって実現するためのシステムを提案する. 映像に合わせて音声を収録するアフレコは;アニメ制作や映画の吹き替えなどで幅広く用いられているが; 声優のように映像と同期した発話を行うことが要求される. また; サウンドエンジニアが手動で収録した台詞音声を映像と同期させ; BGMや効果音を付加するといった編集作業にも時間を要する. 本システムは; 声優やサウンドエンジニアのような技能を持たない人でもアフレコができるよう配慮し; 1) 発話者が発話のタイミングをとるための様々な情報を提示し; 2) 収録した音声に対して BGM や効果音を重ね合わせるなどの後処理を自動的に行い; 3) 利用者が簡単に操作できるインタフェースを備え; 4) 多人数の利用者が並行して短時間で収録できる特徴を有するシステムを設計し; 試作した. 本システムの運用テストと主観評価実験により; 操作の容易さや複数提供したタイミングをとるための情報の有用性を示した. また …,情報処理学会論文誌,2015,*
単一楽曲の切り貼りによる動画の盛り上がりに同期した BGM 自動付加手法,佐藤晴紀， 平井辰典， 中野倫靖， 後藤真孝， 森島繁生,近年; 動画共有サービスの利用者の増加に伴い; 動画を共有する文化が広がっており;個人が動画制作に携わる機会が増加している. 動画制作における重要な過程に BGMの付加があり; 視聴者の動画に対する印象を強くさせる効果がある [1]. また; 動画をより印象的に見せるために; 編集者が 「動画の決定的なシーンに楽曲のサビを合わせる」 などのこだわりを持ってBGM を付加することがある. こだわりを実現するために; 楽曲の切り貼りによる編集 (以降;再編と呼ぶ) がたびたび行われているが; 再編した BGM に違和感が残らないようにしつつ;長さを調節するためには反復的な編集作業が必要であり; これらを同時に考慮した再編には多くの労力が必要となる. この問題を解決するため; これまでにも動画に BGM を付加する手法の研究が行われてきた [2];[3]. しかし; 既存研究の多くは BGM を自動で付加することを重視しており;こだわりを反映させて動画の特定のタイミングに楽曲の特定の箇所を付加するための手法は提案 …,第 77 回全国大会講演論文集,2015,*
ダンスモーションに同期した表情自動合成のための楽曲印象解析手法の提案,朝比奈わかな， 岡田成美， 岩本尚也， 増田太郎， 福里司， 森島繁生,1. はじめに近年; 3DCG 制作ツール (MikuMikuDance 等) の普及により; 楽曲に合わせて CGキャラクタを踊らせる作品が増加傾向にある. このようなダンス動画においてキャラクタの表情はダンスモーション全体の印象に大きく影響する. 例えば; 楽曲やダンスモーションの印象と全く異なる印象の表情を付与した場合; その動画は違和感のある作品となってしまう. また;楽曲の印象が一定でも; 例えば; ダンスモーションの激しさの度合いやキャラクタの姿勢によって;全体の印象が大きく左右される場合がある. そのため; 作品全体の印象を決定する要素として;楽曲の印象だけでなく; ダンスモーションの情報も考慮する必要がある. しかし;一つの作品を作るために; それらの要素に関する専門的知識を取得することは容易ではない.そこで我々は; キャラクタの表情を自動合成するための; ダンスモーションに同期した楽曲印象推定手法を提案する. 本手法を用いることで; 楽曲とダンスモーションにマッチした自然な表情の合成を …,第 77 回全国大会講演論文集,2015,*
実測に基づくゲームキャラクタの頭部および眼球運動の自動合成,鍵山裕貴， 川井正英， 桑原大樹， 森島繁生,1. はじめに CG コンテンツにおいてリアルな眼球運動とそれに伴う頭部運動を生成することは重要である. しかし; 現在キャラクタの頭部及び眼球の動きはアーティストが生成しているため;大きな労力とコストがかかる. Kokkinara ら [1] は 3D 空間上にあるオブジェクトからキャラクタの視界内の顕著性を計算し; 自動で頭部及び眼球運動のアニメーションを作成した. しかしながら;この手法には問題点がある. 第 1 に顕著性を計算する際に; 輝度や色相を考慮していない. 第 2に注視点が同じであるならば頭部及び眼球運動は常に同じである. 第 3 に前庭動眼反射と呼ばれる;頭が動いたときにこれと反対方向に眼球を動かして; 視界内のぶれを防ぐための反射を再現できていない. そこで本研究では; 物体の顕著性を考慮した実測に基づくゲームキャラクタの頭部および眼球運動を自動合成する手法を提案する (図 1). まず; Xu ら [2] の手法を用い;輝度や色相を考慮した顕著性マップを作成し; 顕著性の最も高い部分を注視点とする. 次に; 実測 …,第 77 回全国大会講演論文集,2015,*
フィッティングを保持した体型の変化に頑健な衣装転写システムの提案,成田史弥， 齋藤隼介， 加藤卓哉， 福里司， 森島繁生,ゲームや映画などにおいて; キャラクタの衣装は重要な役割を果たす. CG コンテンツ上では人間以外のキャラクタにも衣装を着せることが多くあり; それらの衣装モデリングは頻繁に行われる.しかし; 衣装モデリングはデザインが同じでもキャラクタの体型が変われば; 同様の工程を繰り返す必要がある. 従って; あるキャラクタの衣装を別の任意キャラクタに着せるシステムには大きな需要がある. Brouet ら [1] は; 衣装の制作者が標準寸法の衣装の型紙をもとに異なるサイズの衣装の型紙を作る際の基準を公式化した. そして物理的な妥当性を確保しながら; その基準を満たす自動衣装転写アルゴリズムを提案した. この研究により一つの衣装モデルを体型の異なる人間のキャラクタに転写することが可能になった. また; 転写した衣装の型紙を出力することで転写した衣装を実際に製造することも可能にした. 我々はこの衣装転写という概念を CG コンテンツ上の任意のキャラクタに適用することを考える. 既存のパターングレーディングにおいて使う基準は; 人間の衣装を制作する …,第 77 回全国大会講演論文集,2015,*
顔画像のシルエット情報に基づく 3 次元顔形状復元,野澤直樹， 桑原大樹， 森島繁生,1. はじめに 3 次元の顔形状を一枚の画像から復元する試みが近年多くなされている. 例えば;エンターテイメントの分野では撮影された人物の顔を持つキャラクターをゲーム上に登場させたり;犯罪捜査では 3 次元復元された人物の顔モデルを指名手配犯の追跡に役立てようとしたりしている.3 次元復元を行う際; 顔形状を変形させるために主成分分析 (PCA) によって作成された顔変形モデルを顔特徴点に合わせてフィッティングする手法が数多く提案されている. しかし;その手法の多くは精度を求めるために入力を正面顔に限定している場合が多い [1]. その中で;Blanz ら [2] は; 斜めを向いた入力顔画像にも対応した 3 次元復元に成功している. しかし; Blanz ら[2] の手法は得られた顔特徴点の位置とそれに対応する 3D Morphable Model の頂点の関係を;顔向きごとに事前に決定しておく必要がある. 輪郭の場合; 顔向きによっては見えなくなる場合があるため; 目や口の特徴点とは異なる事前準備が必要となり; これに多くの労力を必要とする. そこで …,第 77 回全国大会講演論文集,2015,*
実写画像を入力とする画風を考慮したアニメ背景画像生成システム,山口周悟， 古澤知英， 福里司， 森島繁生,漫画やアニメ作品では; 作品の世界観を表現するために背景を詳細に描きこむ傾向にある. しかし;アーティスト独自の色使い; 塗り (ベタ塗り; 厚塗り; 水彩塗り) を表現する必要があるため;多くの技術と手間を要する. 実際の風景とアニメ作品の違い (色使い; 塗り等) を分析し;転写することができれば; 任意の画像をアニメ風に加工・デザインすることが可能となる.そこで本稿では; アニメ背景の画風を実写画像に転写することを研究目的とする.絵画のような手描き画像の画風を実写画像に転写する先行研究として; Efros ら [1]は入力画像の輝度値を基に; 手描き画像をパッチ単位で 転 写 す る 手 法 を 提 案 し た. ま た;Hertzmann ら [2] は教師学習ペア (実写画像と手描き画像) の間に見られるテクスチャ特徴の対応関係を; 任意の実写画像に転写する手法を提案した. 両手法に共通する問題点として;入力手描き画像のテクスチャ情報が大きく損なわれていることがあげられる. 問題の原因としては …,第 77 回全国大会講演論文集,2015,*
雑音下での音源定位・音源分離に与える伝達関数測定法の影響の評価,赤堀渉， 増田太郎， 奥乃博， 森島繁生,実環境下で 1m~ 3m 程度離れた話者; 特に; 同時発話が想定される状況での音声認識を行うためには; マイクロフォンアレイ (MA と略記) で取得された音から音源定位や音源分離を行う必要がある.MA は通常ロボットの頭部や筐体に組み込まれているので; 各マイクロフォンへの伝達関数を活用して部分空間を構築し; 処理性能の向上を狙うことができる. 例えば; 音源定位法の一つ MUSIC(Multiple Signal Classification) 法では; MA の幾何学的な配置から部分空間を構築する. また;MA の伝達関数を利用すれば; さらに定位精度が向上する [1]. 音源分離法の一つ GHDSS(Geometric High-order Decorrelation-based Source Separation) でも; 同様に伝達関数を利用し; MA の指向性の形成を行い; 分離精度の向上を図っている [2]. MA の伝達関数は;方向を変えてスピーカから流した測定信号を各マイクで収録し; 収録音からインパルス応答を求めた上で算出される. インパルス応答はインパルスを演奏し; 収録すれば求まるが; 理想的な …,第 77 回全国大会講演論文集,2015,*
皺の個人性を考慮した経年変化顔画像合成,桑原大樹， 川井正英， 加藤卓哉， 森島繁生,犯罪者や行方不明者の長期的な捜索において; 対象人物の顔の印象は加齢に伴い変化する.具体的には; 顔にしみやくすみ; 皺が発生し; 顔形状のたるみといった特徴が見られるようになる.このような変化を推測し; 作成した顔画像は対象人物発見のための手がかりとして有用である.そのため; 対象人物の過去・未来の顔を合成する経年変化顔合成技術が求められている. Maejimaら [1] は; 実データの統計に基づく皺モデルを入力顔画像に転写し; 同一環境で撮影された顔画像の年齢別データベースを用いて入力顔画像を小片画像 (以下; パッチ) 単位で再構成することで;経年変化顔画像を合成する手法を提案した. この手法は; 顔の形状や各パーツの個人性を保持し;パッチ単位の再構成によりぼけた印象のないしみやくすみといった肌の特徴や皺の合成を可能にしている. しかし; 統計に基づく皺モデルの転写により合成される皺の特徴は一様になり;人物の皺の位置や形状によって決まる皺の個人性を考慮していないという問題があった. これに …,第 77 回全国大会講演論文集,2015,*
映像の盛り上がり箇所に音楽のサビを同期させる BGM 付加支援手法,佐藤晴紀， 平井辰典， 中野倫靖， 後藤真孝， 森島繁生,概要: 本稿では; 入力映像の指定箇所と入力楽曲の指定箇所を同期させながら;映像の全区間に対して BGM を付加する手法を提案する. 従来研究では; 既存動画から音響特徴量と映像特徴量を学習し; 映像に BGM を自動付加する手法が提案されている. しかし; 自動で映像にBGM を付加しているため;「映像の決定的なシーンに楽曲のサビを合わせたい」「映像の始端と終端に楽曲の始端と終端を合わせたい」 といったユーザが指定した楽曲と映像の特定の箇所を同期するための BGM 付加については言及されていない. そこで本研究では; 楽曲と映像の長さを揃えながら;ユーザが指定した楽曲と映像の箇所を同期させるように楽曲を断片的につなぎ合わせることで;映像の全区間に対して BGM を付加する. 具体的には; 動的計画法に基づく小節単位での楽曲の切り貼りによりユーザが指定した箇所を同期させた BGM の付加を実現する. 被験者実験の結果;本手法は同じ音色の箇所が多いインストゥルメンタルの楽曲に対して特に有効であった. また …,研究報告エンタテインメントコンピューティング (EC),2015,*
ダンスモーションにシンクロした音楽印象推定手法の提案とダンサーの表情自動合成への応用,朝比奈わかな， 岡田成美， 岩本尚也， 増田太郎， 福里司， 森島繁生,概要: 近年; 3DCG 制作ツール (MikuMikuDance 等) の普及により; 楽曲に合わせて CGキャラクタを踊らせる動画作品が増加傾向にある. このようなダンス動画においてキャラクタの表情は作品全体の印象に大きく影響する. 例えば; 楽曲やダンスモーションの印象と全く異なる印象の表情を付与した場合; その動画は違和感のある作品となってしまう. また; 楽曲の印象が一定でも;ダンスモーションの激しさの度合いやキャラクタの姿勢などによって; 印象が大きく左右される場合がある. そのため; 作品の印象を決定する要素として; 楽曲の印象だけでなく; ダンスモーションの情報も考慮する必要がある. そこで我々は; ダンス動画の印象についての主観評価実験に基づき;音響特徴量とモーション特徴量を用いて重回帰分析を行うことで; ダンスモーションにシンクロした楽曲印象推定を可能にした.,研究報告エンタテインメントコンピューティング (EC),2015,*
実写画像に基づく特定画風を反映したアニメ背景画像への自動変換,山口周悟， 古澤知英， 福里司， 森島繁生,概要: 手描きアニメーション制作において; 背景画像の作成は多大な労力を必要とする.そこで本研究では; 実写の風景画像をアニメ背景画像へ自動変換する手法を提案する.提案手法では; 各アニメータの個性的な” 色使い”;” 輪郭”;” ブラシの塗り方” を再現するために;アニメ作品中の風景画像の特徴を任意の実写風景画像に転写する. 各領域の色調;塗り方の違いを考慮することで; 従来研究で不十分であった輪郭の保持と; 各領域の特徴を考慮した転写の両立を可能とした.,研究報告グラフィクスと CAD (CG),2015,*
人物の皺の発生位置と形状を反映した経年変化顔画像合成,桑原大樹， 川井正英， 加藤卓哉， 森島繁生,長期的な犯罪捜査において; 対象となる人物の過去・未来の顔を推定する経年変化顔合成技術が求められている. しかし; 従来手法では経年変化後の印象の決定において重要となる;皺の個人性が表現できていない. 皺の個人性は皺の位置や形状によって決まる. そこで本稿では;加齢による皺が表情皺の形状や位置に起因するという知見に基づき; 皺の個人性を反映した経年変化顔合成手法を提案する. 初めに; 表情変化時の顔画像を入力として; その表情皺から入力人物の皺の形状及び発生位置を推定する. 推定された結果を基に; 年代別の顔画像データベースで入力人物の顔を再構成することで目標年代の印象を付加した経年変化顔画像を合成する. これにより;皺の個人性を反映する経年変化顔合成を可能にした.,研究報告グラフィクスと CAD (CG),2015,*
注視点の変化に追随するゲームキャラクタの頭部および眼球運動の自動合成,鍵山裕貴， 川井正英， 桑原大樹， 森島繁生,概要: 眼球運動とそれに伴う頭部運動は; ゲームキャラクタの動きをよりリアルにみせるために重要な要素である. しかし; 現状では眼球運動はアーティストの手作業によって作成されており;その表現には多大な労力と時間がかかっている. そこで本研究では; 注視点の位置と表示時間を考慮したキャラクタアニメーションを自動合成する手法を提案する. 具体的には;注視点が移る際の頭部及び眼球運動を実測し; その実測データから運動を決定づける複数のパラメータを抽出する. このパラメータをモデル化し; 注視点の位置と表示時間に応じて決定することで; キャラクタの詳細な頭部及び眼球運動の合成が可能となる.,研究報告グラフィクスと CAD (CG),2015,*
半透明物体における曲率と透過度合の相関分析 (マルチメディア・仮想環境基礎),岡本翠， 安達翔平， 久保尋之， 向川康博， 森島繁生,抄録 本研究では; 半透明物体の内部で生じる光線の表面下散乱現象の解析を目的として;物体表面の曲率と表面下散乱現象との相関を検討する. コンピュータグラフィックス分野では;画像の生成を目的として表面下散乱現象のモデル化が積極的に行われてきたが;物理ベースの光学シミュレーションは計算負荷が高いため; このようなモデルを用いて画像の解析を行うことは非常に困難である. そこで本研究は; 表面下散乱現象のモデルとして;計算コストが低い近似的なモデルである曲率に依存する反射関数 (CDRF) に着目する.様々な曲率の半透明物体における表面下散乱の計測結果をもとに; 曲率と光の透過度合との相関を分析することによって; CDRF を用いた半透明物体の画像解析の有効性を検証する.,電子情報通信学会技術研究報告= IEICE technical report: 信学技報,2015,*
半透明物体における曲率と透過度合の相関分析 (質感の計測・認識・提示; 災害),岡本翠， 安達翔平， 久保尋之， 向川康博， 森島繁生,抄録 本研究では; 半透明物体の内部で生じる光線の表面下散乱現象の解析を目的として;物体表面の曲率と表面下散乱現象との相関を検討する. コンピュータグラフィックス分野では;画像の生成を目的として表面下散乱現象のモデル化が積極的に行われてきたが;物理ベースの光学シミュレーションは計算負荷が高いため; このようなモデルを用いて画像の解析を行うことは非常に困難である. そこで本研究は; 表面下散乱現象のモデルとして;計算コストが低い近似的なモデルである曲率に依存する反射関数 (CDRF) に着目する.様々な曲率の半透明物体における表面下散乱の計測結果をもとに; 曲率と光の透過度合との相関を分析することによって; CDRF を用いた半透明物体の画像解析の有効性を検証する.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2015,*
半透明物体における曲率と透過度合の相関分析,岡本翠， 安達翔平， 久保尋之， 向川康博， 森島繁生,本研究では; 半透明物体の内部で生じる光線の表面下散乱現象の解析を目的として;物体表面の曲率と表面下散乱現象との相関を検討する. コンピュータグラフィックス分野では;画像の生成を目的として表面下散乱現象のモデル化が積極的に行われてきたが;物理ベースの光学シミュレーションは計算負荷が高いため; このようなモデルを用いて画像の解析を行うことは非常に困難である. そこで本研究は; 表面下散乱現象のモデルとして;計算コストが低い近似的なモデルである曲率に依存する反射関数 (CDRF) に着目する.様々な曲率の半透明物体における表面下散乱の計測結果をもとに; 曲率と光の透過度合との相関を分析することによって; CDRF を用いた半透明物体の画像解析の有効性を検証する.,研究報告コンピュータビジョンとイメージメディア (CVIM),2015,*
FG2015 age progression evaluation,Nicolas Tsapatsoulis; Kleanthis Soteriou; Daiki Kuwahara; Shigeo Morishima; Andreas Lanitis,The topic of face-aging received increased attention by the computer vision communityduring the recent years. This interest is motivated by important real life applications whereaccurate age progression algorithms can be used. However; age progressionmethodologies may only be used in real applications provided that they have the ability toproduce accurate age progressed images. Therefore it is of utmost importance to encouragethe development of accurate age progression algorithms through the formulation ofperformance evaluation protocols that can be used for obtaining accurate performanceevaluation results for different algorithms reported in the literature. In this paper we describethe organization of the; first ever; pilot independent age progression competition that aims toprovide the basis of a robust framework for assessing age progression methodologies …,*,2015,*
キャラクターの身体構造を考慮した実時間肉揺れ生成手法,岩本尚也， 森島繁生,抄録 本論文では; キャラクターの肉揺れを実時間で生成する手法を提案する. ここで肉揺れとは;骨格の変形を起点としたキャラクター動作によって生じる脂肪層の揺れといった二次的動作を指す.忠実な肉揺れの表現手法として; 計算コストの高い有限要素法を用いた手法が多く提案されている一方; 近年では簡略的な弾性体の計算手法を用いた頑健な実時間肉揺れ生成手法も提案されている. しかしながら; 骨の変形による剛体スキニングの影響がシミュレーション領域である脂肪層や皮膚にまで適用していたため; 皮膚表面上のわずかな揺れしか表現できていない点が問題であった.そこで本論文では; より大きな肉揺れの実時間表現を目指し; キャラクターの皮下に存在する内部構造を自動で階層化し; スキニング領域とシミュレーション領域を切り分ける手法を提案する.本手法により; 各階層の体積や弾性体のパラメータもユーザーが自由に変更できるため;内部構造を考慮した弾性体の材質設定や肉揺れ部位の指定も行うことが可能である. 最後に; 本 …,画像電子学会誌,2015,*
Automatic Generation of Photorealistic 3D Inner Mouth Animation only from Frontal Images,Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,抄録 In this paper; we propose a novel method to generate highly photorealistic three-dimensional (3D) inner mouth animation that is well-fitted to an original ready-made speechanimation using only frontal captured images and small-size databases. The algorithms arecomposed of quasi-3D model reconstruction and motion control of teeth and the tongue; andfinal compositing of photorealistic speech animation synthesis tailored to the original. Ingeneral; producing a satisfactory photorealistic appearance of the inner mouth that issynchronized with mouth movement is a very complicated and time-consuming task. This isbecause the tongue and mouth are too flexible and delicate to be modeled with the largenumber of meshes required. Therefore; in some cases; this process is omitted or replacedwith a very simple generic model. Our proposed method; on the other hand; can …,Journal of Information Processing,2015,*
顔画像処理の過去・現在・未来,森島繁生,1987 年知的音声画像符号化の研究に着手した. これは送信信号に含まれる意味や感情等を理解し; 伝送路はシンボル情報のみを伝え; 受信側でリアルな信号を復元するという究極の情報圧縮を目指すものであった. これらの成果の具現化システムとして 1995 年の SIGGRAPH で; 本人の 1枚の正面画像を基に; 逐次送られてきた音声信号にリップシンクさせ; 表情変化もさせながら;実時間で来場者同志が画面を通して対話できる” Better Face Communication” を発表した.顔画像を撮影して瞬時にアバタ化し; SGI Onyx をフル稼働させながら; 音声解析結果から顔形状変化を自動生成して; 10fps のリアルタイムで face-to-face 対話できるシステムを; 今でいうEmerging Technology の前身のデモセッションで発表した (SGI がマシンを無償で貸与してくれた良い時代). その後; これは臨場感通信のプロジェクトへと発展し; 1999 年から ATRにおいて音声翻訳と顔画像の入れ替えを含むビデオ翻訳の研究に従事した. また万博の名古屋 …,研究報告グラフィクスと CAD (CG),2014,*
個人性を保持したデータドリブンなパーツベース経年変化顔画像合成,桑原大樹， 前島謙宣， 藤崎匡裕， 森島繁生,概要: 犯罪捜査や行方不明者の捜索での利用を目的とし; 顔写真から対象人物の過去や未来の顔を予測する技術が求められている. そこで本研究では; 対象人物の顔画像を目標年代の顔のパーツ画像を用いて再構成することで; 経年変化顔合成を行う. 提案手法では; 再構成に用いる画像選択基準として本人の個人性を強く残すような制約を加えることで; 従来研究で不十分であった個人性の保持と年齢特徴の再現の両立を可能とした. また; パーツ選択において選択候補に順位付けをすることで; 合成結果を複数パターン提示することが可能となった.,研究報告グラフィクスと CAD (CG),2014,*
VRMixer: Mixing Video and Real World with Video Segmentation,Shigeo Morishima Tatsunori Hirai; Satoshi Nakamura; Tsubasa Yumura,*,ACE 2014,2014,*
振り付けの構成要素を考慮したダンスモーションのセグメンテーション手法の提案,岡田成美， 福里司， 岩本尚也， 森島繁生,本研究では; CG のダンスモーションにおけるセグメンテーション手法を提案する.ダンスの振り付けには時間幅を持つ単位が存在し; それらを次々と繋げることで一つの振り付けが構成される. よって; ダンスモーションを再利用するにあたり; 適切な箇所でのセグメンテーションは不可欠である. 本研究では; ダンサーへの評価実験により; ダンスモーションのセグメンテーションについて重要である要素の検討を行い; セグメンテーションルールの構築を行う. 具体的には;ダンス全体のテンポ; 動作の左右対称性に加え; 足の着地のタイミングに着目することにより;振り付けの構成要素を考慮したセグメンテーションを実現する.,研究報告グラフィクスと CAD (CG),2014,*
正面および側面のイラストからのキャラクタ顔回転シーンの自動生成,古澤知英， 福里司， 岡田成美， 平井辰典， 森島繁生,アニメーションを制作するには; 大量の絵を描く必要がある. また; 時間連続性を考慮しながら大量の絵を描くことは難しい. したがって; 数少ない絵からアニメーションを作ることができれば;制作における労力を削減できるだけでなく; 手描きアニメーションを制作の敷居を下げることが出来る.そこで本稿では; アニメーション制作における” 中割り” に注目し; キャラクタの顔回転シーンを対象とした中割り自動生成システムを提案する. 提案手法では; 2 つの方向間の 0~ 90のキャラクタの顔回転シーンを; 2 枚の入力画像から生成するために; 入力画像間の対応付け;およびパーツごとの回転による位置と形状の変化のルールをについて検討を行う.,研究報告グラフィクスと CAD (CG),2014,*
Face retrieval system by similarity of impression based on hair attribute,Takahiro Fuji; Tsukasa Fukusato; Shoto Sasaki; Taro Masuda; Tatsunori Hirai; Shigeo Morishima,There is a great demand for human image retrieval on the Web. On searching person'simages; not only whether he or she is the person you are looking for; but also theappearance of the person is an important factor. Since current image search systems suchas Google Image Search are based on textual data; they are useful when we want to lookover a particular person's face; but they cannot consider the appearance or the impressionof the person sufficiently. Most of the images on the Web are not annotated because it isoften difficult to describe the human appearances in words. While there are many factors;hair is especially an important feature of human appearance. According to Davis et al.[1981];hair appearance is the most important feature in the recognition of familiar people.Therefore; in this paper we present a human image retrieval system based on hair …,ACM SIGGRAPH 2014 Posters,2014,*
The efficient and robust sticky viscoelastic material simulation,Kakuto Goto; Naoya Iwamoto; Shunsuke Saito; Shigeo Morishima,We present a method to simulate a sticky viscoelastic material that; considers the effects ofcollision against other objects. In computer graphics; viscoelastic simulation is widely usedin films and games. Games with real-time properties require high efficiency and robustnessof such simulations. Takamatsu et al. presented a fast and practical method for animatingparticle-based viscoelastic fluids. They compute the behavior of plausible viscoelastic fluidsby combining two well-established approaches; Lattice Shape Matching (LSM) andSmoothed Particle Hydrodynamics (SPH). Their method allows for viscoelastic materialsimulation that requires a significant amount of computational cost but is implementedeasily. However; their method does not consider the expression of stickiness. Therefore; wepropose a new method that can represent stickiness by adding friction to the velocity …,ACM SIGGRAPH 2014 Posters,2014,*
Material parameter editing system for volumetric simulation models,Naoya Iwamoto; Shigeo Morishima,This poster presents a novel editing system for the material parameters of volumetricmodels. Physically-based character animation is a trend for film making. Most recentapproaches require volumetric lattices for simulation using finite element method such as[Hahn et al. 2012]. These approaches can represent secondary deformation and volumepreservation in character animation. However; the anatomical structures for human-likecharacters are not considered; because it is difficult for rigging artists to edit anatomicalfeatures or material parameters to interior vertices enclosed in skin meshes. Our system canallow artists to adjust interior deformable parts (such as muscle; flesh; and skin) or materialparameters using a texture-based approach (ie using a” material map”) in Figure 1. In thisposter; we demonstrate the effectiveness using various characters.,ACM SIGGRAPH 2014 Posters,2014,*
Photorealistic facial image from monochrome pencil sketch,Ai Mizokawa; Taro Nakamura; Akinobu Maejima; Shigeo Morishima,People often draw the rough portrait using only a pencil. However; it is difficult for viewers toimagine an actual face from monochrome sketch. If photorealistic facial image can begenerated from pencil sketch; it is very useful. Additionally; this system will be applicable tocriminal investigation; because the police often draw the portrait using only a pencil basedon the testimony of an eyewitness. Many studies on sketch-based images have beenreported. Klare et al.[2011] proposed a matching system between a forensic sketch and agallery of mug shot images. However; in their method; it cannot generate an original face. Inthis paper; we can generate an original facial image which preserves individuality of aninput image. In brief; we propose a method that can generate a photorealistic facial imagefrom monochrome pencil sketch.,ACM SIGGRAPH 2014 Posters,2014,*
Example-based blendshape sculpting with expression individuality,Takuya Kato; Shunsuke Saito; Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,One of the biggest drawbacks of Blendshape Animation is the enormous labor of sculptingits blendshapes with “expression individuality”. Expression individuality is diverse facialexpression in which each of the characters' facial expressions has on their own; other thanits semantics. It was problematic for artists to sculpt blendshapes with unified expressionindividuality for every expression of the characters. Methods that generate coarsely sculptedblendshape; Deformation Transfer [Sumner et al. 2004] for instance; often yield artifactswhich are caused by the individuality of each character's facial expressions. Example-basedFacial Rigging [Li et al 2010] was one of the solutions to supplement these artifacts; yet itwas not effective when the numbers of training examples are limited. In this paper; wepropose a method to generate blendshapes with expression individuality with small …,ACM SIGGRAPH 2014 Posters,2014,*
Patch-based fast image interpolation in spatial and temporal direction,Shunsuke Saito; Ryuuki Sakamoto; Shigeo Morishima,Recently; a plausible interpolation of images has been proposed for many applications suchas smooth view interpolation; upsampling low frame rate video; and making animation fromstill images. However; producing an intermediate frame from two images is still achallenging task. The moving gradient method enables one to obtain intermediate frameswithout suffering from blurring or ghost effects; by computing the displacement of each pixel[Mahajan et al 2009]. However; their method requires many iterations of the optimizationprocess for pixel displacements by graph-cut. Hence their method is not suitable forinteractive applications. To compute correspondence between two images rapidly; weemploy the PatchMatch framework [Bames et al 2009]; which is a significantly faster methodfor reconstructing an image from plenty of patches. However; an issue is that it is not …,ACM SIGGRAPH 2014 Posters,2014,*
Automatic deblurring for facial image based on patch synthesis,Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,Deblurring of a facial image is one of the most important topics in computer vision. Blurrinesscan occasionally appear on a facial image in different parts; such as a eyes; a nose; a cheekand a mouth because of alignment errors. For example; the morphable model and activeappearance model are statistical models that can represent a variety of facial images bychanging parameters; however; synthesized facial images can include the same type ofblurs as mentioned previously. To solve these problems; Li et al.[2010] proposed a robustmotion deblurring system. Their method enables deblurring of the flat cheek region of facialimages; but it cannot fix inconsistent images; which are defined as images that includeconsiderably stronger blurs; such as the eyes and the nose. For example; there are strangeblurs; such as the white of an eyeball tinged with black color and the nasal cavity tinged …,ACM SIGGRAPH 2014 Posters,2014,*
Measured curvature-dependent reflectance function for synthesizing translucent materials in real-time,Midori Okamoto; Shohei Adachi; Hiroaki Ukaji; Kazuki Okami; Shigeo Morishima,Simulating the effects of sub-surface scattering is one of the most important factors forsynthesizing realistic translucent materials.“Curvature-Dependent Reflectance Function(CDRF)” proposed by Kubo et al.[2010] enables to represent the effects of subsurfacescattering in real-time; according to the correlation between curvature and translucency.Since CDRF is an approximation of scattering effects using Gauss function; whole details ofthe effect cannot be represented. To represent the effects of sub-surface scattering; thatcannot be approximated using Gauss function; we propose measured CDRF; acquired fromreal objects. Using a digital camera and polarizers; we obtain the correlation betweencurvature and scattering effect; then store it as a look-up-table. As a result; we are able torealize real-time rendering of translucent materials from measured CDRF.,ACM SIGGRAPH 2014 Posters,2014,*
エンタテインメント応用のための人物顔パターン計測・合成技術,森島繁生,＊早稲田大学理工学術院総合研究所 東京都新宿区大久保 3–4–1 ＊Research Institute for Scienceand Engineering; Waseda University; Okubo; Shinjuku-ku; Tokyo; Japan ＊E-mail:shigeo@waseda.jp … キーワード：3D 顔モデリング (3D face modeling)，経年変化顔合成 (faceaging synthesis)，肥痩顔モデル (face thick and thin model)，表情 リターゲティング (expressionretargetting)，顔アニメーション (facial animation)，エンタテインメント応用 (entertainmentapplications)． JL 0007/14/5307–0593 C 2014 SICE … 計測と制御 第 53 巻 第 7 号 2014 年 7月号 … 計測と制御 第 53 巻 第 7 号 2014 年 7 月号 … 計測と制御 第 53 巻 第 7 号 2014 年 7 月号… 計測と制御 第 53 巻 第 7 号 2014 年 7 月号 … 入力人間の 表情 Samner ら の手法 本方式 アーティスト作成の表情 … 1） O. Alexander; et.al.: The digital emily project : Achieving a photorealistic digitalactor; IEEE CGA; 30; 20/31 (2010) 2） S. Agarwal; et.al: Building Rome in a Day; ICCV …,計測と制御,2014,*
歌手映像と歌声の解析に基づく音楽動画中の歌唱シーン検出手法の検討 (ポスターセッション; 音学シンポジウム 2014),平井辰典， 中野倫靖， 後藤真孝， 森島繁生,抄録 本稿では; ライヴ動画や PV などに代表される音楽動画において; 歌手が歌っているシーンである 「歌唱シーン」 を検出する手法について検討する. 音楽において歌手は最も主要な役割を担っており; 音楽動画における歌唱シーンも同様に動画のハイライトの一つであると言える.歌唱シーンは動画サムネイル生成や; 大量の音楽動画の短時間ブラウジングなどにおいて有用である. 歌唱シーンを検出するためには; 歌手の顔認識; 楽曲中の歌声区間検出といった要素手法及びそれらを組み合わせる手法についての検討が必要である. 本稿では; 顔認識を用いた映像解析;歌声区間検出を用いた音響解析; それらを複合した Audio-visual 解析のそれぞれについて比較・検討しながら歌唱シーン検出の実現可能性について議論する.,電子情報通信学会技術研究報告. SP; 音声,2014,*
歌手映像と歌声の解析に基づく音楽動画中の歌唱シーン検出手法の検討,平井辰典， 中野倫靖， 後藤真孝， 森島繁生,概要: 本稿では; ライヴ動画や PV などに代表される音楽動画において; 歌手が歌っているシーンである 「歌唱シーン」 を検出する手法について検討する. 音楽において歌手は最も主要な役割を担っており; 音楽動画における歌唱シーンも同様に動画のハイライトの一つであると言える.歌唱シーンは動画サムネイル生成や; 大量の音楽動画の短時間ブラウジングなどにおいて有用である. 歌唱シーンを検出するためには; 歌手の顔認識; 楽曲中の歌声区間検出といった要素手法及びそれらを組み合わせる手法についての検討が必要である. 本稿では; 顔認識を用いた映像解析;歌声区間検出を用いた音響解析; それらを複合した Audio-visual 解析のそれぞれについて比較・検討しながら歌唱シーン検出の実現可能性について議論する.,研究報告音楽情報科学 (MUS),2014,*
ラケットスポーツ動画の構造解析に基づく映像要約と鑑賞インタフェースの提案,河村俊哉， 福里司， 平井辰典， 森島繁生,ラケットスポーツは世界中の人々に親しまれており; テレビやウェブ上で手軽に鑑賞されている.一方で; 動画鑑賞に費やすことのできる時間で; ラケットスポーツのような長い試合の動画を多く鑑賞することは現実的でない. 一試合を短時間で鑑賞する一般的な方法としてウェブ上の人手で作成されたハイライト動画の鑑賞があるが; ハイライト動画のラリーは作成者の主観で選出されているため;鑑賞時間や観たいラリーと合致するとは限らない. また; 現存するハイライト動画は一部の動画のみで作成されているため; 多くの試合は一試合全てを鑑賞しなければならない. そこで本研究では;試合内容を把握できるラリーシーンを基にしたラケットスポーツ動画の要約手法と要約時間の指定のみで重要なラリーを鑑賞できるインタフェースを提案する. 本インタフェースにより; ユーザはラケットスポーツ動画の重要度に基づきインタラクティブな映像要約が可能となる.,第 76 回全国大会講演論文集,2014,*
実測に基づく反射関数による半透明物体のリアルタイムレンダリング,岡本翠， 安達翔平， 宇梶弘晃， 岡見和樹， 森島繁生,映画やゲームなどの CG コンテンツの品質を向上させるためには; リアルな半透明物体の表現が必要である. しかし; 厳密な半透明物体の描画を行うためには表面下散乱の影響を考慮する必要があり; 高い計算コストがかかる. この問題を解決するために半透明物体のリアルタイムレンダリングに関する様々な研究が行われてきた. 本手法と関連が深い研究として; 久保ら [1]の考案した物体の曲率に着目した半透明物体の高速描画手法が挙げられる.曲率と光の透過度合の関係を解析し関数近似することで; 半透明物体らしい光の透過現象のリアルタイムレンダリングを可能とした. しかし; 既存研究はフォトン追跡法を用いているため;表面下散乱による光の減衰を正しく表現できているとはいえない. 本稿では; 実測により取得した曲率と光の透過度合の関係性に基づく; 高速な半透明物体の描画方法を提案する.実際に様々な曲率の半透明球に光を照射し; 撮影画像を解析することにより; 球の曲率及び法線 …,第 76 回全国大会講演論文集,2014,*
正面および側面の手描き顔画像からの顔回転シーン自動生成,古澤知英， 福里司， 岡田成美， 平井辰典， 森島繁生,リミテッド・アニメーションには; 基準となるキーフレーム同士を補間する” 中割り” が存在する.中割りとは; アニメーションの動きを生み出す役割を担い; わずか 1 秒間のシーンに対して; 20枚以上もの絵が制作される. しかし中割りの制作には; 質の高い絵を大量に描けるだけでなく;滑らかな映像に見えるよう; 時間の連続性を考慮するなど; 経験的な技術が要求される.以上の点から; 簡単な絵を描くスキルがある人でも; アニメーションを制作することは困難である.アニメーション制作の敷居を下げることに加え; 時間的コスト削減のため; キーフレームを入力とした中割り自動生成の研究が盛んに行われている. しかし; 多くの中割り生成手法は 2次元平面上の動きに限定される. そのため; アニメ作品の中で頻繁に登場し;中割りが特に制作されるシーンの 1 つ; キャラクタの振り向き等を含む 3 次元的なキャラクタ顔回転シーンを制作することができない. そこで; キャラクタの顔の回転を伴った中割り生成手法が提案 …,第 76 回全国大会講演論文集,2014,*
頭蓋骨の形状を考慮した顔の肥痩シミュレーション,藤崎匡裕， 桑原大樹， 中村太郎， 溝川あい， 前島謙宣， 森島繁生,美容; 映像制作の分野において; 人の顔の肥痩 (痩せ太り) のシミュレーションが必要とされている.例えば; 本人の痩せシミュレーション結果を見せることで; それがダイエットの目標になる. また;映像中の登場人物の肥痩を任意に調節することが出来るようになる. Blanz ら [1] は; 顔の 3D形状データベースを主成分分析し; 領域分割をした顔の部位ごとにその成分を変化させ;顔の肥痩変形を行った. また; Baek ら [2] は; 体全体の 3D 形状データベースを大きさと形状に分けて主成分分析し; ユーザのパラメータ制御による体全体の肥痩を表現した. しかし;これらの手法には問題点がある. 第 1 に; 顔内部の頭蓋骨形状を考慮せず; 表面形状しか扱っていないため; 輪郭が頭蓋骨を越えた非現実的な痩せ変形が行われてしまう. 第 2 に;全ての人物に対して一様な変形を行っているため; 肥痩変形の個人性を表現できていない.犯罪捜査で使われる復顔法は; 頭蓋骨に統計的な皮膚の厚みデータを付加することで; 生前の …,第 76 回全国大会講演論文集,2014,*
キャラクタに固有な表情変化の特徴を反映したキーシェイプ自動生成手法の提案,加藤卓哉， 川井正英， 桑原大樹， 齋藤隼介， 岩尾知頼， 前島謙宣， 森島繁生,1. はじめに CG コンテンツにおいて; キャラクタの表情再現には高い需要がある. 一般的に; CGキャラクタの表情生成には; ブレンドシェイプという手法が用いられている. ブレンドシェイプとは;キャラクタの基本表情であるキーシェイプの線形結合で任意の表情を表現する手法である.しかしこの手法は; アーティストが各キャラクタに対し; 多数のキーシェイプを手作業で作成する必要があり; 多大な労力を要することが問題となっている. そこで; キーシェイプを自動的に生成するための手法が提案されてきた. Sumner らは; あるキャラクタの表情変化を別のキャラクタに転写することで; キーシェイプを生成する” Deformation Transfer” と呼ばれる手法を提案した [1]. しかし;彼らの手法は; 幾何構造の異なるキャラクタに; 転写元キャラクタの表情変化そのまま転写先キャラクタに適用しようとするため; 転写先のキャラクタが持つ固有な表情変化の特徴を反映することができなかった. また; Li らは; アーティストが作成したトレーニングサンプルを正解値として …,第 76 回全国大会講演論文集,2014,*
髪の特徴に基づく類似顔画像検索,藤賢大， 福里司， 増田太郎， 平井辰典， 森島繁生,近年; Web 上の画像コンテンツの増加に伴い; 効率的に画像検索を行うシステムが注目されている.既存の画像検索システムとして Google 画像検索などが挙げられる. Google 画像検索は; Web上の画像検索エンジンであり; 例えばユーザが名前だけを知っている人物の顔を調べたいときや;同一人物の他の画像を見てみたいといった状況で利用できる. 一方; 人物を検索する際;人物の外見; 特に髪に対する印象は重要な要素である. その理由は; 髪型や髪色は個人を認識するために最も重要な役割を果たしていることが心理学的に証明されているためである. また;髪色や髪型は時代の流行を表しており; 美容院やヘアカタログの存在は髪の印象に基づく検索技術に需要があることを示している. しかし; 既存の人物画像検索システムの多くはテキスト情報を用いているため; 髪など人物の外見の特徴や印象を十分に考慮できていない. それは;人物の外見の特徴がテキスト情報として画像に付与されることが少なく; また外見の特徴を表す …,第 76 回全国大会講演論文集,2014,*
Query by Phrase: 半教師あり非負値行列因子分解を用いた音楽信号中のフレーズ検出,増田太郎， 吉井和佳， 後藤真孝， 森島繁生,概要: 本稿では; 楽曲群の中から; ユーザがクエリとして与えたフレーズが演奏されている時刻を検出する新たな音楽情報検索手法 「Query by Phrase (QBP)」 を提案する. 本研究において;「フレーズ」とは; 何らかの楽器 (通常は単一の楽器) による短時間の演奏 (音響信号) を指すものとする. QBPでは; 様々な楽器音が重なり合って構成されている音楽音響信号から所望のクエリを検出する必要がある. この問題を解決するため; 本稿では; クエリと楽曲の一部の構成要素との距離を計算することができる手法を提案する. まず; ガンマ過程非負値行列因子分解 (GaP-NMF) を用いて; クエリのスペクトログラムを適切な個数の基底スペクトルと対応するアクティベーションとの積に分解する.同様に; 楽曲のスペクトログラムも GaP-NMF を用いて分解を行う. このとき;楽曲中にクエリが含まれているという仮定のもと; 基底スペクトルの一部にクエリから得られた基底スペクトルをそのまま再利用する (半教師あり GaP-NMF). こうすることで; クエリにおける基底 …,研究報告音楽情報科学 (MUS),2014,*
LYRICS RADAR: 歌詞の潜在的意味分析に基づく歌詞検索インタフェース,佐々木将人， 吉井和佳， 中野倫靖， 後藤真孝， 森島繁生,概要: 本稿では; 歌詞検索インタフェース LYRICS RADAR について述べる. 従来の歌詞中の語句に対する全文検索システムでは; ある単語 (例:「涙」) をクエリとして入力すると;全く異なる意味の歌詞を持つ楽曲 (例: 失恋の 「涙」 と感動の 「涙」) が混在した検索結果となり;ユーザの検索意図を十分反映できない問題があった. 歌詞の意味をクエリとして的確に言葉で表現して入力するのは困難なため; 本研究では; クエリとして既存の歌詞を活用する検索インタフェースを提案する. 具体的には; 潜在的ディリクレ配分法を用いて多数の楽曲の歌詞から各楽曲の歌詞のもつ意味 (トピックの比率) を推定することで; クエリとなる楽曲の歌詞と意味的に類似した歌詞をもつ楽曲が検索できる. 歌詞のトピックの比率を 5 角形内に着色して可視化するトピックレーダーチャートや; データベース中のすべての歌詞をトピックの類似度に応じて二次元平面上にマッピングすることで; ユーザがトピックを言語表現する必要がなくなり; どのようなトピックの歌詞を探したいのか …,研究報告音楽情報科学 (MUS),2014,*
頭蓋骨形状に基づく顔の痩せ太りシミュレーション,藤崎匡裕， 桑原大樹， 溝川あい， 岩尾知頼， 中村太郎， 前島謙宣， 森島繁生,美容やヘルスケア; エンターテイメントの分野において; 正確な顔の痩せ太りシミュレーションに対する需要が高まっている. 従来手法では; 全入力人物の顔画像に対して; データベースから定義された同様の変形ルールを適用していたため; 変形の個人特徴が無視されていた. また;顔の表面情報のみを基にして痩せ太り変形ルールを定義していたため; 頭蓋骨の形状を無視した不自然な変形が行われていた. そこで我々は; 正面顔画像一枚から; 顔内部の頭蓋骨形状を推定し;それを基にして入力人物ごとの個人性を考慮した痩せ太り変形を行う手法を提案する. また同時に;頭蓋骨の形状を越えた変形を防ぐ. 本手法により; 入力人物個人の個人性を残した上で;頭蓋骨形状を越えるような不自然な変形のない; 顔のリアルな痩せ太りシミュレーションを実現した.,研究報告グラフィクスと CAD (CG),2014,*
キャラクタ特有の特徴再現を考慮したリアルな表情リターゲッティング手法の提案,加藤卓哉， 川井正英， 斉藤隼介， 岩尾知頼， 前島謙宣， 森島繁生,CG キャラクタの表情作成に用いられるブレンドシェイプでは; それぞれのキャラクタ特有の表情変化を再現したキーシェイプ作成に多大な労力を要していた. そこで本研究では; 人間の表情を転写することによって生成されたキーシェイプからアーティストが作成したキーシェイプへの写像を;少数の表情を用いて学習し; 人間の表情が転写された他の表情のキーシェイプに適用する手法を提案する. 本手法で生成したキーシェイプを用いることで; アーティストが少数の表情で定義した;キャラクタ特有の表情特徴を考慮したリターゲッティングを実現した.,研究報告グラフィクスと CAD (CG),2014,*
物理的特徴に基づく擬音語可視化手法の検討,福里司， 森島繁生,本稿では; 物理パラメータを基に擬音語を推定する手法を検討する. 擬音語とは;アニメや漫画作品に登場するキャラクタの質感や動きを言語化したものであり;受け手に内容や状況を直感的に認識させる表現技法である. しかしアニメータが;状況や設定資料を基に感覚的に決定しているのが現状である. そこで; CG アニメーションの制作において必要となる複数のパラメータを基に; 擬音語を自動推定及び付与する手法を提案する.この技術により; 映像のみでは把握できない環境情報 (素材感や速度感など)を可視化することが可能となる.,研究報告グラフィクスと CAD (CG),2014,*
実計測による半透明物体の反射関数推定とリアルタイムレンダリング,岡本翠， 安達翔平， 宇梶弘晃， 岡見和樹， 森島繁生,様々な CG コンテンツにおいて; 半透明物体を写実的に表現することは重要である. 本研究では;実測データをもとに曲率と光の透過度合を関連付け; 半透明物体を高速に描画する手法を提案する.様々な半径の半透明球に光を照射し; 球の法線と光源方向の成す角に対する輝度値の変化を実測する. さらに; 球の法線と視点方向の成す角に対する輝度分布を分析することにより;半透明物体における光の指向性が与える影響を考察する. 実測により得られたデータをもとに;半透明物体における曲率と表面下散乱との関連性を導き; 高速かつ高品質な半透明物体の描画を実現する.,研究報告グラフィクスと CAD (CG),2014,*
LYRICS RADAR,佐々木将人， 吉井和佳， 中野倫靖， 後藤真孝， 森島繁生,*,*,2014,*
Query by Phrase,増田太郎， 吉井和佳， 後藤真孝， 森島繁生,*,*,2014,*
Character Transfer: Example-based individuality retargeting for facial animations,Takuya Kato; Shunsuke Saito; Masahide Kawai; Tomoyori Iwao; Akinobu Maejima; Shigeo Morishima,A key disadvantage of blendshape animation is the labor-intensive task of sculptingblendshapes with individual expressions for each character. In this paper; we propose anovel system” Character Transfer”; that automatically sculpts blendshapes with individualexpressions by extracting them from training examples; this extraction creates a mappingthat drives the sculpting process. Comparing our approach with the naïve method oftransferring facial expressions from other characters; Character Transfer effectively sculptedblendshapes without the need to create such unnecessary blendshapes for othercharacters. Character Transfer is applicable even the training examples are limited to only afew number by using region segmentations of the face and the blending of the mappings.,*,2014,*
ラケットスポーツ動画の構造解析による映像要約手法の提案,河村俊哉， 福里司， 平井辰典， 森島繁生,近年; スポーツ動画を手軽に鑑賞できるようになり; 効率的な鑑賞方法が必要とされている.その解決策として; 従来のラケットスポーツ動画に対する映像要約では; 重要なラリーシーンの要約映像を生成したが; ラリーシーンの検出及びその評価方法に問題点が見られ; 要約映像の効率的な鑑賞方法についても考慮が無かった. そこで本稿では; ラケットスポーツ動画に対する新たなラリーシーンの検出方法と各ラリーの重要度を用いた映像要約手法及びその鑑賞方法を提案する.提案手法では; ショット分割された動画に対し類似ショットのクラスタリング及びラリーを含むクラスタの選定により; 精度の高いラリーシーンの検出方法を実現する. その後; 各ラリーに対して音響情報を考慮した重要度評価を行い; その結果をユーザが調整することで; 任意の時間内での動画の鑑賞を可能とする. さらに; ラケットスポーツに特化した高速再生による動画視聴方法を提案し;さらなる効率的な動画の鑑賞方法を実現する.,研究報告グラフィクスと CAD (CG),2013,*
髪の特徴に基づく顔画像の印象類似検索,藤賢大， 福里司， 増田太郎， 平井辰典， 森島繁生,概要: Web 上の人物画像検索は; 名前だけを知っている人物の顔を調べたいときや;同一人物の他の画像を見てみたいといった状況で有用である. 人物を検索する際には;その人物であるかどうかはもちろんのこと; 外見の印象; 特に髪の印象は重要な要素である. しかし;既存の人物画像検索システムの多くはテキスト情報に基づいて検索しているため;人物の外見の特徴や印象を十分に考慮できていない. そこで; 本研究では人物の外見の印象を大きく左右する髪に注目した画像検索手法を提案する. 髪の特徴を定量化する手法は今までに提案されているが; 人間がどういった髪特徴に注目するかについて明確にされていない.そこで本研究では; ユーザの主観評価の結果に基づいて髪特徴量を組み合わせて評価関数を設計する. それを基にユーザの注目する髪の特徴を考慮した人物画像検索手法を提案する.,研究報告グラフィクスと CAD (CG),2013,*
Interactive Aged-Face Simulation with Freehand Wrinkle Drawing,Ai Mizokawa; Akinobu Maejima; Shigeo Morishima,Recently; many studies on facial aging synthesis have been reported for the purpose ofsecurity applications such as criminal investigation; kidnappings; and entertainmentapplications such as movies or video games. However; the representation of wrinkles; whichis one of the most important elements when reflecting age characteristics; remains difficult.Additionally; the influence of lighting conditions and every individual's skin color issignificant; and it is difficult to infer the location and shape of future wrinkles because theydepend on factors such as one's living environment; eating habits; and DNA. Therefore; wemust consider several possibilities for the locations of wrinkles. In this paper; we propose afacial aging synthesis method that can create plausible aged facial images; and is able torepresent wrinkles at any desired location by drawing artificial freehand wrinkles while …,Pattern Recognition (ACPR); 2013 2nd IAPR Asian Conference on,2013,*
Facial Aging Simulator Based on Patch-Based Facial Texture Reconstruction,Akinobu Maejima; Ai Mizokawa; Daiki Kuwahara; Shigeo Morishima,We propose a facial aging simulator which can synthesize a photo realistic human aged-face image for criminal investigation. Our aging simulator is based on the patch-based facialtexture reconstruction with a wrinkle aging pattern model. The advantage of our method is tosynthesize an aged-face image with detailed skin texture such as spots and somberness offacial skin; as well as age-related facial wrinkles without blurs that are derived from lack ofaccurate pixel-wise alignments as in the linear combination model; while maintaining theidentity of the original face.,Pattern Recognition (ACPR); 2013 2nd IAPR Asian Conference on,2013,*
CG キャラクタのための感情と時系列性を考慮した眼球運動の分析と合成,岩尾知頼， 久保尋之， 前島謙宣， 森島繁生,ゲームや映画のシーンにおいて; CG キャラクタのリアルな眼球運動を再現することは重要である.特に; 感情によって変化する眼球運動を写実的に表現する需要は多い. そこで我々は本稿において;マルコフモデルを用いて感情を含んだ実際の眼球運動を分析し; その結果をキャラクタに適用することによりリアルな眼球運動を表現する手法を提案する. 我々の手法により; 感情と時系列性を考慮した眼球運動の自動生成が可能となる.,エンタテインメントコンピューティングシンポジウム 2013 論文集,2013,*
データドリブンなフォトリアル口内アニメーションの自動生成,川井正英， 岩尾知頼， 前島謙宣， 森島繁生,概要: CG 発話アニメーションの自動生成手法は既に数多く提案されている. しかし; 口内 CGアニメーションの自動生成手法は未だ提案されておらず; 特に舌の複雑な表現を行うことは大きな課題である. そこで本研究では; 既存手法によって生成された発話アニメーションの口内領域に対して実際に撮影した口内画像を挿入し; その口唇部に複数人の口唇画像を用いて Visio-lization法を施すことで; 複雑な口内表現を可能にした.,エンタテインメントコンピューティングシンポジウム 2013 論文集,2013,*
Efficient speech animation synthesis with vocalic lip shapes,Daisuke Mima; Akinobu Maejima; Shigeo Morishima,Abstract Computer-generated speech animations are commonly seen in video games andmovies. Although high-quality facial motions can be created by the hand crafted work ofskilled artists; this approach is not always suitable because of time and cost constraints. Adata-driven approach [Taylor et al. 2012]; such as machine learning to concatenate videoportions of speech training data; has been utilized to generate natural speech animation;while a large number of target shapes are often required for synthesis. We can obtainsmooth mouth motions from prepared lip shapes for typical vowels by using an interpolationof lip shapes with Gaussian mixture models (GMMs)[Yano et al. 2007]. However; theresulting animation is not directly generated from the measured lip motions of someone'sactual speech.,ACM SIGGRAPH 2013 Posters,2013,*
Generating eye movement during conversations using Markov process,Tomoyori Iwao; Daisuke Mima; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Figure 1. Proposed Markov Process 1. Introduction Generating realistic eye movements is a significanttopic in Computer Graphics(CG) contents production field. Appropriate modeling and synthesisfor eye movements are greatly difficult because they have a lot of important features. Gu etal[2007] proposed a method for automatically synthesizing realistic eye movements during conversationsaccording to probability models. Despite eye movements during conversations include both saccadesand fixational eye movements (FEMs); they synthesized only saccades which are relatively largeeye movements. We proposed a method for automatically synthesizing both saccades andFEMs[Iwao et al 2012]. While we classified eye movements accurately; we did not consider thetime dependency of eye movements at all. For example; an eyeball often moves in the samedirection to the previous direction; however; we could not express such eye movements …,ACM SIGGRAPH 2013 Posters,2013,*
Expressive dance motion generation,Narumi Okada; Kazuki Okami; Tsukasa Fukusato; Naoya Iwamoto; Shigeo Morishima,Abstract The power of expression such as accent in motion and movement of arms is anindispensable factor in dance performance because there is a large difference inappearance between natural dance and expressive motions. Needless to say; expressivedance motion makes a great impression on viewers. However; creating such a dance motionis challenging because most of the creators have little knowledge about dance performance.Therefore; there is a demand for a system that generates expressive dance motion withease. Tsuruta et al.[2010] generated expressive dance motion by changing only the speedof input motion or altering joint angles. However; the power of expression was not evaluatedwith certainty; and the generated motion did not synchronize with music. Therefore; thegenerated motion did not always satisfy the viewers.,ACM SIGGRAPH 2013 Posters,2013,*
Real-time dust rendering by parametric shell texture synthesis,Shohei Adachi; Hiroaki Ukaji; Takahiro Kosaka; Shigeo Morishima,Abstract When we synthesize a realistic appearance of dust-covered object by CG; it isnecessary to express a large number of fabric components of dust accurately with manyshort fibers; and as a result; this process is a time-consuming task. The dust amountprediction function suggested by Hsu [1995] proposed modeling and rendering techniquesfor dusty surfaces. These techniques only describe dust accumulation as a shading function;however; they cannot express the volume of dust on the surfaces. In this study; we present anovel method to model and render the appearance and volume of dust in real-time by usingshell texturing. Each shell texture; which can express several components; is automaticallygenerated in our procedural approach. Therefore; we can draw any arbitrary appearance ofdust rapidly and interactively by solely controlling simple parameters.,ACM SIGGRAPH 2013 Posters,2013,*
Reflectance estimation of human face from a single shot image,Kazuki Okami; Naoya Iwamoto; Akinobu Maejima; Shigeo Morishima,Abstract Simulation of the reflectance of translucent materials is one of the most importantfactors in the creation of realistic CG objects. Estimating the reflectance characteristics oftranslucent materials from a single image is a very efficient way of re-rendering objects thatexist in real environments. However; this task is considerably challenging because thisapproach leads to problems such as the existence of many unknown parameters. Munoz etal.[2011] proposed a method for the estimation of the bidirectional surface scatteringreflectance distribution function (BSSRDF) from a given single image. However; it is difficultor impossible to estimate the BSSRDF of materials with complex shapes because thismethod's target was the convexity of objects therefore; it used a rough depth recoverytechnique for global convex objects. In this paper; we propose a method for accurately …,ACM SIGGRAPH 2013 Posters,2013,*
ダンスモーションにおける表現力付与システムの提案,岡田成美， 福里司， 森島繁生,本研究では; 光学式モーションキャプチャシステムを用いて基準となるダンスモーションデータと;異なる表現を行っているダンスモーションデータを取得し; それらを用いて; テンポを考慮しつつ表現力の乗ったダンスモーションを容易に作成する手法を提案する. 踊り手の意識が鑑賞者に伝わるかの主観評価実験を行い; その結果を基に; 基準となるモーションに表現力を付加するフィルタを作成する. なお作成するフィルタは; テンポを一定に保ったまま; 緩急と関節角度に注目して変換を行う.これにより得たフィルタを任意のダンスモーションに付加することで; 表現力の豊かなモーションの作成を可能にした.,研究報告グラフィクスと CAD (CG),2013,*
認識・検出 顔形状による制約を考慮した線形回帰に基づく顔特徴点自動検出,松田龍英， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2013,*
音楽と映像が同期した音楽動画の自動生成システム,平井辰典， 大矢隼士， 森島繁生,概要: 本稿では; 著者らがこれまでに研究してきた; 任意の入力楽曲を基に既存の音楽動画コンテンツを再利用し音楽と映像が同期した音楽動画を自動生成するシステムを紹介する.本研究では; まずシステムの土台となる音楽と映像の同期手法を主観評価実験により検討した.その結果に基づき; 音のエネルギーを表す RMS の変化に; 映像のアクセント (明滅や動きなど)を対応させるように既存の音楽動画コンテンツを切り貼りする音楽動画自動生成システムを実装した. また; システムによる生成動画の評価も行った.,研究報告音楽情報科学 (MUS),2013,*
レコむし: 画像と楽曲の印象の一致による楽曲推薦システム,佐々木将人， 平井辰典， 大矢隼士， 森島繁生,概要: 入力した画像に対して感性的にマッチした楽曲を推薦するシステム; レコむし(RECOmmendation of MUSic using an input Image) を提案する. 音楽を楽しむ上で;現在の情景は重要な要素の一つである. なぜなら; 楽曲の印象がその情景と調和しているほど;楽曲を聴いたときの感動は増すためである. しかし; 膨大な楽曲群の中から現在の情景に的確にマッチした楽曲を手動で探し出すことは容易ではない. そこで本研究では; AV (Arousal-Valence)空間と呼ばれる心理空間に画像と楽曲を配置することで; 情景に対する印象と楽曲に対する印象を対応付ける. レコむしはこの対応付けを用いることで; 現在の情景に合った印象を与える楽曲をプレイリストとして推薦する. また; ランダム選曲による楽曲と比較することで; レコむしの評価を行った.,研究報告音楽情報科学 (MUS),2013,*
ユーザの簡易的操作によるインタラクティブな加齢変化顔合成,溝川あい， 中井宏紀， 前島謙宣， 森島繁生,近年; エンタテイメントやセキュリティへの応用を目的とした顔の経年変化画像を合成する研究が多くなされている. Tazoe らの提案した経年変化顔合成手法では; きめの乱れやくすみといった年齢特有の肌の質感の表現が可能である. しかし; 皺が鮮明に表現できないといった問題や;結果が元画像の照明環境や肌の色に大きく影響されるという問題があった. また;加齢に伴う皺の付き方は多岐に渡るため; 多様な皺を考慮する必要がある. 本稿では;皺領域を二値化し照明環境や肌の色による影響を軽減し; 且つユーザが書き足した曲線からリアルな年齢皺を付加した経年変化顔合成手法を提案する.,研究報告コンピュータビジョンとイメージメディア (CVIM),2013,*
注目領域中の画像類似度に基づく動画中のキャラクター登場シーンの推薦手法,増田太郎， 平井辰典， 大矢隼士， 森島繁生,1. はじめに近年; インターネット上での動画コンテンツの数は増加の一途をたどっている. そんな中;現在の動画検索は主に動画名やタグなどのテキスト情報に基づき行われている. しかし;テキストによる検索では情報量の多い動画コンテンツを適切に表現することは難しい. 例えば;同一キャラクタにおける微妙な作風の違いなど; 表現に幅があるような動画の検索を行う際には;映像内容そのものに基づく検索が必要である. また; 映像内容に基づく検索であれば;動画中の時系列において何が映っているかを考慮できる. そこで本研究では; 画像類似度に基づく類似キャラクタの検索手法について検討することに加え; ユーザが注目したいキャラクタが映る他の動画を検索; 推薦する手法を提案する. 2. 関連研究キャラクタ登場シーンに基づく動画検索に関する研究として; 佃らはニコニコ動画 1 の動画に時刻に同期して付与されたコメント情報を利用し;キャラクタの登場シーンと活躍度を推定した [1]. しかし; 短いテキスト情報であるコメントからは …,第 75 回全国大会講演論文集,2013,*
入力画像に感性的に一致した楽曲を推薦するシステム,佐々木将人， 平井辰典， 大矢隼士， 森島繁生,音楽のデジタル化に伴い個人が持ち歩ける楽曲の数は増加している. 一方; 所有する大量の楽曲の中から最適な 1 曲を探し出すことは困難である. 具体的に聴きたい楽曲はないが;音楽を楽しみたい場合; 視覚と聴覚の調和を感じられるような楽曲を聴くと心地良い気分になることは容易に想像できる. しかし現状ではシャッフル機能や手作業で楽曲を逐一検索しなければならない. そこで本研究では; その場の情景に感性的に一致した楽曲及びプレイリストを自動で推薦するシステムを提案する. これにより; 視覚と聴覚が調和した楽曲を聴くことができ;音楽の楽しみが広がる.,第 75 回全国大会講演論文集,2013,*
リアルな口内表現を実現する発話アニメーションの自動生成,川井正英， 岩尾知頼， 三間大輔， 前島謙宣， 森島繁生,実写ベースの人間の発話シーンを目にする機会が増加している. リアルな発話シーンの作成には;技術力のあるアーティストによる繊細な作り込みが必要となり; 多大な労力を要するという問題がある. この問題を解決するために; Chang らは; Multidimensional Morphable Model を用いて;個人性を反映した発話シーンを生成する手法を提案している [1]. しかしながら; Changらの手法では口形の動きに合わせて口内の画像をモーフィングしているため; 口内の歯や舌の伸縮による破綻が生ずるという問題があった. また; Taylor らは” Viseme” で定義される口形をつなぎ合わせることで; リアルにリップシンクした発話シーンを生成する手法を提案している [2].しかしながら;” Viseme” は口内に特化した分類ではないため; つなぎ合わせの際;口内は時間的な不連続が生じて; 破綻が生ずるという問題があった. さらにこれらの手法には;舌の複雑な動き・歯の細部構造 (以下; リアルな口内) の表現に課題があり; 改善の余地があると …,第 75 回全国大会講演論文集,2013,*
ダンスモーションにおける表現のバリエーション生成,岡田成美， 岡見和樹， 福里司， 岩本尚也， 森島繁生,1 序章近年 CG キャラクタのモーションを手軽に生成するツールの普及に伴い; キャラクタを音楽に合わせて踊らせて楽しむユーザの数が年々増加傾向にある. しかし; ダンスの動き一つを取っても;腕の運びやタイミング・テンポなど幅広い表現が考えられる. そのため; ダンス知識のないユーザにとって; そういった表現力の幅を持ったダンスモーション生成を行うことは難しく;多くの経験と時間が必要となる. 以上のような背景から; 幅広いダンスモーションを手軽に生成できるシステムに大きな需要がある. Tsuruta らは; 速度・ダンス動作の大きさに注目し;それらのパラメータを変化させることにより; ダンスモーションの表現力を豊かにする手法を提案している [1]. しかし; 本文において表現力がどのような要素に現れるかが明言されていないこと;またテンポについて未考慮であることから; 付与された表現力が必ずしもユーザの意思と合致しているとは言い切れない. そこで本研究では; 光学式 Motion Capture System (以下; Mocap) を …,第 75 回全国大会講演論文集,2013,*
短繊維を考慮した埃の描画手法,安達翔平， 宇梶弘晃， 小坂昂大， 森島繁生,CG において; 汚れの表現は写実性の付与に大きく貢献している. 本稿では;どのような場所にも存在する埃の堆積効果に着目した. Hsu らは埃の堆積範囲を数式化し;その関数に従った Shading を行うことで埃の堆積を表現した [1]. また; Bo らは物体に堆積する埃の反射特性を実測し; 計測結果を BRDF により再現した [2]. しかし; どちらの手法も埃の堆積による厚みを表現できておらず; 更に; 構成要素の大半を占める繊維自体も表現できていない.よって本稿では; Leyngel らによる Shell 法 [3] と呼ばれる階層状の Texture Mappingを用いることで; 近似的なボリュームレンダリングを行い; 堆積の厚みの表現の実時間処理を実現する. また; Shell 法に用いる Texture として; 埃の構成要素である繊維構造を考慮した Textureを作成することで; 従来の表現手法よりも; より写実的な埃の表現を行う手法を提案する.,第 75 回全国大会講演論文集,2013,*
D-12-31 疑似的な嬢特徴の追加によるリアルな加齢変化顔合成 (D-12. パターン認識・メディア理解 B (コンピュータビジョンとコンピュータグラフィックス)),溝川あい， 中井宏紀， 前島謙宣， 森島繁生,近年; セ キ ュ リテ ィ やエ ン タ テ イ ン メン トへ の 応用 を目的 とし た顔の 経年変化画像を合成する研 究が 多くな されてい る. Tazee らは Visio− lization 法 [1」 に 基づ き; 経年変化顔,電子情報通信学会総合大会講演論文集,2013,*
D-12-40 Structure From Motion と照度差ステレオの統合による 3 次元顔形状復元の高精度化 (D-12. パターン認識・メディア理解 B (コンピュータビジョンとコンピュータグラフィ...,桑原大樹， 松田龍英， 前島謙宣， 森島繁生,顔の 法 線 π (x; y) を も とに; 次 の エ ネル} i・一 関数 E の 最小化問題 を解 くこ とで; 最適 な z (x.y) の 推定を行 う. E= α Σ ノ (z;・; y)+ Σ9 (▽ z;・; y) α・c・n・t (1) x. ン x; yf (z;・x; Y) 一 ΣΨ (x.ylP・・P;・σ)(z (x・y) 一…(x・Y)) 2 (2) P・・・・… y・一,電子情報通信学会総合大会講演論文集,2013,*
口内情報のリアルな表現を可能とするデータドリブンな発話アニメーション自動生成,川井正英， 岩尾知頼， 三間大輔， 前島謙宣， 森島繁生,3. 研究概要人間は; 発話の際に歯と舌をある程度独立に動かすことができる. 例えば;舌の動きが同じ音素を発話する際でも; 発話時の声量で; 歯の開き具合は全く異なっている.そこで本稿では; 音素に対して口形と口内の動きを別々に定義することで; リアルな口内表現を可能とする発話アニメーションの自動生成手法を提案する. 提案手法は; 任意の方法で作成された発話アニメーションに対して適用可能である. 従って本稿では; 口形の表現に既存の手法を利用することにする (以後; 既存の方法で作成された発話アニメーションのことを; 既作成のアニメーションと呼ぶ).本研究の概要を図 1 に示す. システムの入力は; 既作成のアニメーション; アニメーション中の個人の歯の見える正面顔画像 (以後; 個人歯画像と呼ぶ) 1 枚; 及びセンテンス情報である. また;事前に任意の 1 人物の開口の際の歯の動き; 舌の動き; 及び多人数の通常発話を動画撮影し;それぞれ歯画像データベース; 連番舌画像データベース; 及び口唇画像データベースを構築して …,研究報告グラフィクスと CAD (CG),2013,*
Perlin noise を用いた短繊維生成法による埃の高速描画手法,安達翔平， 宇梶弘晃， 小坂昂大， 森島繁生,埃の表現は; 物体の経年変化の描画を写実的に行う重要な要素である. 本研究では;埃が短繊維の集合であることに着目し; UV 平面上で点をランダムに運動させ;その軌跡を描画することにより; 埃の無秩序な短繊維の形状を表現するテクスチャの生成を行った.そして; テクスチャを階層状に積層させ; 高速に描画する Shell 法を用いることで;従来法において実現し得なかった繊維構造由来の立体表現; 及び短繊維のリアルタイムでの描画を実現した.,研究報告グラフィクスと CAD (CG),2013,*
Facial Feature Point Tracking using Linear Predictors with Time Continuity and Geometrical Constraint,Tatsuhide MATSUDA; Akinnobu MAEJIMA; Shigeo MORISHIMA,Abstract 本研究では; Eng-Jon らが提案した linear Predictors に時間連続性と顔形状制約を考慮した新しい顔特徴点追跡手法を提案する. Linear Predictors は注目画素周辺の画像特徴量と注目画素から正解位置への移動ベクトルを線形回帰によって対応づける手法であり; 動画像内の十数フレームを学習画像として用いることで正確な特徴点追跡を可能とする. 提案手法では; LinearPredictors によって特徴点を検出し; その位置から前フレームの推定位置周辺の輝度値を基にオプティカルフローによって特徴点を移動させる. さらに; 従来手法で提案された主成分分析に基づく幾何学的制約によって特徴点の位置を補正することで; 未学習人物に対してもロバストな特徴点追跡を実現した.,Technical report of IEICE. Multimedia and virtual environment,2013,*
時間連続性と顔形状制約を考慮した線形予測に基づく特徴点追跡 (パターン認識・メディア理解),松田龍英， 前島謙宣， 森島繁生,抄録 本研究では; Eng-Jon らが提案した linear Predictors に時間連続性と顔形状制約を考慮した新しい顔特徴点追跡手法を提案する. Linear Predictors は注目画素周辺の画像特徴量と注目画素から正解位置への移動ベクトルを線形回帰によって対応づける手法であり; 動画像内の十数フレームを学習画像として用いることで正確な特徴点追跡を可能とする. 提案手法では; LinearPredictors によって特徴点を検出し; その位置から前フレームの推定位置周辺の輝度値を基にオプティカルフローによって特徴点を移動させる. さらに; 従来手法で提案された主成分分析に基づく幾何学的制約によって特徴点の位置を補正することで; 未学習人物に対してもロバストな特徴点追跡を実現した.,電子情報通信学会技術研究報告: 信学技報,2013,*
時間連続性と顔形状制約を考慮した線形予測に基づく特徴点追跡,松田龍英， 前島謙宣， 森島繁生,In this paper; we propose a novel method that tracks facial feature points using linearpredictors with time continuity and geometrical constraint. In previous studies; a facialfeature point tracking method using linear predictors was proposed which is a recursivelinear regression method for displacement vectors from the current position to the correctposition by considering pixel features around a target point. This method can accuratelytrack feature points using multiple frames in a target facial video in training. In our proposedmethod; we performed facial feature point tracking for unknown persons robustly with timecontinuity using optical flow and geometrical constraint using PCA.,研究報告コンピュータビジョンとイメージメディア (CVIM),2013,*
Rapid and authentic rendering of translucent materials using depth-maps from multi-viewpoint,Takahiro Kosaka; Tomohito Hattori; Hiroyuki Kubo; Shigeo Morishima,Abstract We present a real-time rendering method of translucent materials with complexshape by estimating object's thickness between light source and view point precisely. Wanget al.[2010] has already proposed a real-time rendering method treating arbitrary shapes;but it requires such huge computational costs and graphics memories that it is very difficult toimplement in a practical rendering pipe-line. Inside a translucent object; the energy ofincident light attenuates highly depends on the object's optical thickness. TranslucentShadow Maps (TSM)[2003] is able to compute object's thickness using depth map at lightposition. However; TSM is not able to calculate thickness accurately in concave objects.,SIGGRAPH Asia 2012 Posters,2012,*
動画フレームの時間連続性と顔類似度に基づく動画コンテンツ中の同一人物抽出手法,平井辰典， 中野倫靖， 後藤真孝， 森島繁生,*,*,2012,*
肌の質感の周波数解析に基づく年齢変化顔合成,中井宏紀， 松田龍英， 前島謙宣， 森島繁生,抄録 本稿では; 正面顔画像に対して肌の質感と形状の年齢特徴を操作することにより年齢変化顔を合成する手法を提案する. テクスチャの年齢変換において; まず; データベース中の顔画像から凸凹の少ない平坦領域に対して二次元フーリエ変換を行い; 周波数空間における周波数成分の年齢との相関係数を求める. 次に; 入力画像について相関係数の高い周波数成分を所望する年齢へと変換し;逆フーリエ変換により画像空間に戻すことで目的のテクスチャ画像を得る. 提案手法により;入力画像の肌の個人情報を維持した年齢変化顔画像の生成が可能となった.,映像情報メディア学会技術報告 36.35,2012,*
3D human head geometry estimation from a speech,Akinobu Maejima; Shigeo Morishima,Abstract We can visualize acquaintances' appearance by just hearing their voice if we havemet them in past few years. Thus; it would appear that some relationships exist in betweenvoice and appearance. If 3D head geometry could be estimated from a voice; we can realizesome applications (eg; avatar generation; character modeling for video game; etc.).Previously; although many researchers have been reported about a relationship betweenacoustic features of a voice and its corresponding dynamical visual features including lip;tongue; and jaw movements or vocal articulation during a speech; however; there havebeen few reports about a relationship between acoustic features and static 3D headgeometry. In this paper; we focus on estimating 3D head geometry from a voice. Acousticfeatures vary depending on a speech context and its intonation. Therefore we restrict a …,ACM SIGGRAPH 2012 Posters,2012,*
Acquiring shell textures from a single image for realistic fur rendering,Hiroaki Ukaji; Takahiro Kosaka; Tomohito Hattori; Hiroyuki Kubo; Shigeo Morishima,Abstract To synthesize a realistic appearance of mammals; it is necessary to expressdisorderly lie of hairs." Shell texturing method"; proposed by Lengyel [2001]; is possible tosynthesize realistic fur appearance over arbitrary surfaces in real-time. Prior to rendering; itis necessary to prepare several shell textures as a pre-process. However; acquiringappropriate shell textures is a complicated and time consuming work. In this paper; wepresent a novel method to acquire shell textures only from a single input picture of an actualanimal fur. Since every shell textures are automatically computed by a pixel shader in run-time; it is not necessary any complicated pre-computation. Furthermore; conventional shelltexturing method employs typically 16 textures which require huge graphics memory.Because our method requires only a single texture; we realize a significant reduction in …,ACM SIGGRAPH 2012 Posters,2012,*
Automatic music video generating system by remixing existing contents in video hosting service based on hidden Markov model,Hayato Ohya; Shigeo Morishima,Abstract User-generated music video clip called MAD movie; which is a derivative (mixtureor combination) of some original video clips; are gaining popularity on the web and a lot ofthem have been uploaded and are available on video hosting web services. Such a MADmusic video clip consists of audio signals and video frames taken from other original videoclips. In a MAD video clip; good music-to-image synchronization with respect to rhythm;impression; and context is important. Although it is easy to enjoy watching MAD videos; it isnot easy to generate them. It is because a creator needs high-performance video editingsoftware and spends a lot of time for editing video. Additionally; a creator is required videoediting skill. DanceReProducer (Nakano et al [2011]) is a dance video authoring system thatcan automatically generate dance video appropriate to music by reusing existing dance …,ACM SIGGRAPH 2012 Posters,2012,*
Fast-automatic 3D face generation using a single video camera,Tomoya Hara; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Abstract Reconstructing a 3D face model only with a single camera without attachinglandmarks and pattern projecting on a face is still a challenging task in computer vision andcomputer graphics. FaceGen [2011] is able to reconstruct 3D shape and texture from 2Dsingle image based on 3D morphable model. Since the users have to determine of the facialfeature points manually; the method does not work automatically. Moreover; it requires acouple of minutes to generate a 3D face model. Thus; the computational cost is not enoughcheap for practical usage.,ACM SIGGRAPH 2012 Posters,2012,*
Hair motion capturing from multiple view videos,Tsukasa Fukusato; Naoya Iwamoto; Shoji Kunitomo; Hirofumi Suda; Shigeo Morishima,Abstract To create a realistic virtual human; hair animation is an indispensable factor. Manyhair simulation methods have been proposed so far; but simulating realistic hair motion suchas considering an effect of turbulent flow and friction among enormous amount of hairs is stillone of the challenging phenomena. Thus; to reproduce hair motion which includes thesedesirable features; capturing real hair motion has advantages compared with simulated one.Ishikawa et al.[2007] used a motion capture system and tracked some reflective makersplaced on some strands. They successfully reproduce hair motions including an effect ofturbulent flow; but since they put on some markers which have weight and only capturedsparse strands which mean friction among others is ignored; it is still far from real hairmotion.,ACM SIGGRAPH 2012 Posters,2012,*
顔形状の制約を付加した Linear Predictors に基づく特徴点自動検出,松田龍英， 原朋也， 前島謙宣， 森島繁生,近年; 顔画像を用いたアプリケーションが普及しつつあるが; これらの多くは特徴量を抽出する際に特徴点を基点としている. そのため; 顔画像から正確に特徴点を検出する手法が求められている.本研究では; Eng-Jon らが提案した Linear Predictors に; 幾何学的制約を加味した新しい顔特徴点検出手法を提案する. Linear Predictors は; 注目画素周辺の輝度値と; 特徴点の正解位置への移動ベクトルを線形回帰によって対応づける手法であり; 20 枚程度の学習データで正確な推定移動ベクトルが得られる. 提案手法では; 各顔器官の重心を基準とした特徴点の有効範囲を定め;移動ベクトル推定時に特徴点が有効範囲を超えないような制約を加えることにより;特徴点検出の正確度の向上を実現した. また; 事前に顔向き角度推定を行い; 推定結果に基づいた学習データの選択を行うことで; 姿勢によらない特徴点検出を可能にした.,電子情報通信学会論文誌 D,2012,*
隠れマルコフモデルに基づく既存コンテンツ学習による音楽動画自動生成システムの提案,大矢隼士， 森島繁生,概要: インターネットの動画共有サイト上に存在するアマチュア制作の音楽動画を再利用することにより; 自動的に音楽動画を生成するシステムを提案する. この音楽動画は; 既存の音楽にゲームやアニメなどの映像を切り貼りして制作されたものであり; MAD 動画と呼ばれている. 本稿では;以前筆者らグループが提案した DanceReProducer の学習手法を; マルコフ連鎖を使うことにより映像の時系列情報を考慮できるように改善し; Forward Viterbi アルゴリズムを用いて動画生成をおこなう. 提案システムは; まずインターネット上にアップロードされている MAD 動画を大量に取得し;データベースとする. その後; データベースの動画から音楽特徴量; 映像特徴量を抽出し一小節ごとにまとめ; 楽曲の構造情報やテンポの推定をおこなう. 次に; 各特徴量をクラスタリングし;状態変数を音楽特徴量; 潜在変数を映像特徴量として; 潜在変数のマルコフ連鎖モデルを使用して学習する. 動画の生成は; 任意の楽曲 (入力楽曲) に対し; 学習した同調関係から最も入力楽曲と …,研究報告音楽情報科学 (MUS),2012,*
音楽動画コンテンツにおける類似性評価尺度の提案,長谷川裕記， 森島繁生,動画投稿サイトに存在する動画群に対する類似度を測る研究を試みた. この類似尺度を提案することにより; 二次創作された動画群における関係性を明らかにすることが本研究の目的である.対象として動画サイト内で同一のアノテーション情報をユーザーから付与された動画群を選別する.このデータベース内部の動画群の動画特徴; 画像特徴などを比較し; 類似動画を作成し;主観評価を行う. 主観評価から得られた結果などを用いて; 最終的に動画間の類似度を測る尺度を定義した.,研究報告エンタテインメントコンピューティング (EC),2012,*
D-12-79 実写画像に基づく毛皮の実時間描画手法 (D-12. パターン認識・メディア理解 B (コンピュータビジョンとコンピュータグラフィックス); 一般セッション),宇梶弘晃， 小坂昂大， 服部智仁， 久保尋之， 森島繁生,抄録 近年の 3DCG コンテンツにおいて; 動物がキャラクターとして登場する場面は数多く挙げられる. 動物はその大部分が毛皮で覆われているため; 毛皮の描画の写実性はシーン全体の質を大きく左右する. 実時間処理が可能な毛皮の描画手法の代表として; Shell 法があげられる. Shell法は毛皮の断面図に相当する Shell Texture を積層状に描画することによって毛皮を表現する手法であり; リアルタイムで比較的写実性の高い毛皮の描画が可能となっている. しかし Shell法によって写実的な毛を生成する際には; 目的の質感をもった毛皮を描画できる Shell Textureをあらかじめ積層数分だけ用意しなければならず; テクスチャの準備の段階でコストを要するという問題が挙げられる. 本手法では; 毛皮の実写画像一枚のみを入力として; 指定した枚数の ShellTexture を自動的に生成する. これにより; 入力画像に見られる毛並を 3D オブジェクト上に再現させることが可能である. 各層のテクスチャは入力画像一枚から自動的に生成されるため; 従来 …,電子情報通信学会総合大会講演論文集,2012,*
A-15-21 足元条件の変化に伴う歩行動作特徴変化のモデリング (A-15. ヒューマン情報処理; 一般セッション),岡見和樹， 岩本尚也， 國友翔次， 須田洋文， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2012,*
A-15-10 会話時の不随意な眼球運動の分析及び合成手法の提案 (A-15. ヒューマン情報処理; 一般セッション),岩尾知頼， 三間大輔， 久保尋之， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2012,*
D-12-72 複数台のビデオ映像解析による頭髪モーションキャプチャ (D-12. パターン認識・メディア理解 B (コンピュータビジョンとコンピュータグラフィックス); 一般セッション),福里司， 岩本尚也， 國友翔次， 須田洋文， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2012,*
平常時歩行動作からの足元の環境変化に伴う動作特徴変化のモデリング,岡見和樹， 岩本尚也， 國友翔次， 須田洋文， 森島繁生,3. 研究概要本研究は; 足元の環境変化が起きた場合の歩行動作を主成分分析を用いて比較することで; 歩行動作に対して環境変化が及ぼす影響を示す成分を抽出することを目的とする. まず;モーションキャプチャを用いて歩行動作を取得し; 1 つ 1 つの動作それぞれについて; 1周期分のサンプル動作を作成する. そのサンプル動作はそれぞれ; フレーム間の意味合いが異なるため; 時間的同期をとり; それぞれのフレームにおける動作の意味を一致させる. その後;主成分空間内で歩行動作を比較し; それぞれの主成分得点の推移と足元環境変化のパラメータとの相関が高いものを選択することで; 動作変化に対して足元の環境変化が及ぼす影響を抽出し;その成分を用いて実際に動作合成を行う. なお; 上述した相関関係を調べるときは;個人差によるばらつきを排除するために; 平均得点の推移を用いるものとする.,研究報告グラフィクスと CAD (CG),2012,*
実写画像に基づく毛皮の特徴抽出と実時間描画手法,宇梶弘晃， 小坂昂大， 服部智仁， 久保尋之， 森島繁生,CG において; 毛皮の質感表現は非常に重要であるが; 従来の実時間性に特化した描画手法では;動物が持つような不規則な毛並を再現することは難しかった. そこで; 本研究では毛皮の実写画像を入力に用いることにより; 毛並のリアリティを再現しながらも高速に動作する描画法を新たに提案する. 提案法では; 毛皮を散乱媒質と仮定し; 毛皮画像の輝度値と表面からの深度の対応をとったテクスチャを積層する事で; 実時間内での毛皮の描画を行う.,研究報告グラフィクスと CAD (CG),2012,*
動画像解析に基づくリアルな頭髪運動再現,福里司， 岩本尚也， 國友翔次， 須田洋文， 森島繁生,This paper describes a new technique of capturing 3D hair motion based on multi viewsanalysis. We assume that hair is a model of several strips of hairbundle. Therefore; usinghair extensions dyed in horizontal stripes; we define center of colored areas as a skeletonposition and capture them by color detection. According to our results; we achieved morerealistic hair motion; for example friction among hairs than past techniques.,研究報告グラフィクスと CAD (CG),2012,*
音楽動画コンテンツ中のアーティスト名とその登場シーンの同定手法,平井辰典， 中野倫靖， 後藤真孝， 森島繁生,本稿では; 音楽動画コンテンツに対して 「どのアーティストがいつ映像中に登場しているか」というアノテーション情報を自動付加する手法を提案する. 従来の人物顔認証手法は映像中の照明や顔向きなどの撮影環境の変動に脆弱で; その変動が大きい音楽動画コンテンツにおいて;アーティスト名とその登場シーンを同定することは困難であった. そこで本研究では;映像のフレームの時間的連続性を利用して同一人物の顔をクラスタリングすることで;撮影環境の違いを吸収し; アーティストの顔認証をおける問題を解決した. 本手法により;従来の単一フレーム毎に顔認証を行う手法に比べ; 約 2~ 3 倍の精度向上を実現した. また;音楽の歌声区間と映像中にボーカリストが登場するシーンとの関係についても調査し;それを利用した今後の精度向上の可能性について考察した.,研究報告音楽情報科学 (MUS),2012,*
会話時の眼球の跳躍運動と固視微動の分析及び合成手法の提案,岩尾知頼， 三間大輔， 久保尋之， 前島謙宣， 森島繁生,コンピュータグラフィクスで人間の動作を自然に表現するためには眼球運動をリアルに再現することが非常に重要である. 本研究では実際の眼球運動の測定結果をもとにしたリアルな眼球のアニメーションの生成を目的とする. まず; 眼球運動を跳躍運動と固視微動の 2 種類に区別する.さらにそれぞれの眼球運動を確率モデルで表すことによって; リアルな眼球運動の自動的な生成が可能となった.,研究報告コンピュータビジョンとイメージメディア (CVIM),2012,*
Automatic Texture Generation for Face-Shaped Display from a Frontal Face Image,Akinobu Maejima; Takaaki Kuratate; Brennand Pierce; Gordon Cheng; Shigeo Morishima,リアルな見た目を持ち自然な対話が可能なヒューマノイドロボットの実現は; 人-ロボット間のインタラクションを円滑にするために重要であり; この目的を達成するために様々な研究が行われている [1~ 5]. Kuratate らは; Mask-bot と呼ばれるヒューマノイドロボットの顔を開発している (図 1).Mask-bot は; 主に 3 次元顔形状スクリーン; プロジェクタ; パンチルトユニットの 3つから構成されており; CG 合成されたフェイシャルアニメーションを顔形状スクリーンに投影することで; 自身の表情を制御することが可能で; これにより; 立体視とは異なり実空間で存在感のある顔を表現することができる. また; 顔のテクスチャを変更することにより; 自身の顔を様々な人の顔に入れ替えることも原理的に可能である. しかしながら; 実際に対象人物の顔の 3次元形状を反映して顔の入れ替えを行うためには; 3 次元顔モデルを生成した上で;さらにそれと顔形状スクリーンとの間のキャリブレーションを逐一とらねばならず; この作業に手間 …,Proceedings of the Media Computing Conference 2012; Tokyo; Japan; June 23-34; 2012.,2012,*
Curvature-approximated Estimation of Real-time Ambient Occlusion.,Tomohito Hattori; Hiroyuki Kubo; Shigeo Morishima,Abstract: We present a novel technique for computing ambient occlusion (Akenine-Möller etal.; 2008) on real-time graphics hardware. Our method approximates the occlusion for alocal illumination model by introducing curvature-dependent function. Using our method; weare able to acquire occlusion at lower computational cost than conventional methods suchas SSAO (Bavoil et al.; 2008). Our method requires a multi-pass algorithm with the graphicsprocessing unit (GPU). In the first pass curvature is acquired; and in the second pass theocclusion is computed from the curvature. In the calculating occlusion from the curvature; weapproximate the geometric shape by a quadric surface function; and then obtain a curvaturedependent function which is an approximation of geometric surface. This function dependsonly on local variables and we are able to calculate the ambient occlusion for the local …,GRAPP/IVAPP,2012,*
Automatic generation of facial wrinkles according to expression changes,Daisuke Mima; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Abstract In order to synthesize attractive facial expressions; it is necessary to considerdetailed expression changes such as facial wrinkles. Nevertheless; most techniques ofexpression synthesis (ie blend shape; image morphing; simulation of mimic muscle and soon) focus entirely on large-scale deformation of a face and ignore small-scale details suchas wrinkles and bulges. Therefore; hand crafted work of skilled artists is inevitable to make aface attractive finally; unless using a huge photographic equipment.,SIGGRAPH Asia 2011 Posters,2011,*
疎な特徴点と顔変形モデルに基づく動画像からの 3 次元顔モデル自動生成手法,原朋也， 前島謙宣， 森島繁生,本稿では; 1 台のビデオカメラで撮影された顔を自由に振る動作の動画像から; 対象人物の 3次元顔モデルを高速自動生成する手法を提案する. 著者らは先行研究において;顔変形モデルと顔形状の尤度制約を用いた顔の奥行推定に基づき; 正面顔画像から高速に 3次元顔モデルを生成する手法を提案している. しかしながら; 従来手法は奥行推定に入力顔画像から得られる特徴点の 2 次元位置座標を用いており; 3 次元的な制約がないため; 鼻の高さや頬部分の凹凸などの個人特徴を忠実に再現できないという問題点があった. そこで; 提案手法では; 一般的な3 次元復元手法として知られる Structure-from-Motion のフレームワークと顔変形モデルに基づく手法を統合することで; より対象人物に近い 3 次元顔モデルの生成を実現した.,研究報告グラフィクスと CAD (CG),2011,*
DanceReProducer: Web 上の音楽動画を再利用して新たな音楽動画を自動生成する N 次創作支援システム,中野倫靖， 室伏空， 後藤真孝， 森島繁生,*,*,2011,*
Estimating fluid simulation parameters from videos,Naoya Iwamoto; Ryusuke Sagawa; Shoji Kunitomo; Shigeo Morishima,(a)Captured fluid motion (b)Reconstructed fluid depth (c)Optimized simulation result (d)Retargetedby optimized parameter Figure 1: Fluid simulator optimization result based on camera capturedfluid motion; (a): Capturing 3D fluid motion by single camera with patterns projection; (b): Reconstructeddepth of fluid surface when a ball is swinging; (c): Result of particle simulation with optimizedparameters. (d): Result of retargeting another action … Recently; a video-based high quality3D shape and motion mod- eling methods for fluid are proposed. [Huamin et al. 2009]However; this approach only aims to capture and generate origi- nal fluid action as it is.Therefore; we propose a new method to capture and retarget an original liquid feature to generateanother action for fluid by optimizing particle simulation parameters. Even if the correct physicalparameters of a target liquid is known; sometimes the impression of liquid action …,ACM SIGGRAPH 2011 Posters,2011,*
3D reconstruction of detail change on dynamic non-rigid objects,Daichi Taneda; Hirofumi Suda; Akinobu Maejima; Shigeo Morishima,Abstract 3D reconstruction of a moving object is notable technique in fields such as digitalarchives or entertainments.[de Aguiar et al. 2008] is able to reconstruct motion and spatio-temporally coherent time-varying geometry using by eight multi-view video and a rangescanner. However; they couldn't capture the numerous high-frequency folds of garments; itmake the realism of the re-constructed dynamic models reduce. On the otherhand;[Hernández et al. 2007] proposed the technique to reconstruct detailed 3D shape ofmoving cloth including high frequency folds by using one camera. They also use threeprimary colored lights to acquire the shading information independently at the same time.This property made it possible to estimate surface normals on the object from the videosequence by using photometric stereo. However; there is a defect that the area which can …,ACM SIGGRAPH 2011 Posters,2011,*
三色光源下における動物体の高精度かつ詳細な三次元形状再現,須田洋文， 前島謙宣， 森島繁生,あらまし 近年; 人物など動く物体の三次元形状復元の研究が盛んである. しかし物体が動くことによって生じる皺などの細部を; 全周囲に渡って動的に復元することは現状困難である.そこで本研究では; 人物が動いた際の衣服の動的な三次元形状変化を 360 度全周囲で高精度に再現する手法を提案する. まず; 対象物体の基本となる概形を得るため; 初期状態の姿勢におけるレンジスキャンデータを取得し; これを基本形状とする. 次に; 動画像のフレームごとに;被写体の形状に合わせて基本形状を変形させる. さらに; 三色光の照射により作成した反射モデルを用いて対象物体の法線ベクトルを推定することで; 変形された概形に皺などの微細な形状を法線変化として付け加え; 物体全周囲の動的立体形状を細部まで高精度に再現することを実現した.,画像の認識・理解シンポジウム (MIRU2011) 論文集,2011,*
アノテーション情報を付加した画像内容推定結果に基づく自動ダンス動画生成システム,長谷川裕記， 前島謙宣， 森島繁生,本研究では動画に付随するアノテーション情報とユーザーが指定した情報に基き;画像に描写されているターゲット要素の特徴を機械学習することによって; データベース内の動画選択を行い音楽にマッチしたダンス動画を自動生成するシステムを構築した. 画像内の輪郭特徴を表す特徴量; アノテーション情報を表す動画コンテンツに割り振られたタグ情報を用いて画像内容推定を行っており; 先行研究より画像内の構図を考慮したダンス動画生成ができ;ユーザーがシステムを利用する際の自由度を上げる事が可能となった.,研究報告音楽情報科学 (MUS),2011,*
Personalized voice assignment techniques for synchronized scenario speech output in entertainment systems,Shin-ichi Kawamoto; Tatsuo Yotsukura; Satoshi Nakamura; Shigeo Morishima,Abstract The paper describes voice assignment techniques for synchronized scenariospeech output in an instant casting movie system that enables anyone to be a movie starusing his or her own voice and face. Two prototype systems were implemented; and bothsystems worked well for various participants; ranging from children to the elderly.,International Conference on Virtual and Mixed Reality,2011,*
Real-time and interactive rendering for translucent materials such as human skin,Hiroyuki Kubo; Yoshinori Dobashi; Shigeo Morishima,Abstract To synthesize a realistic human animation using computer graphics; it is necessaryto simulate subsurface scattering inside a human skin. We have developed a curvature-dependent reflectance functions (CDRF) which mimics the presence of a subsurfacescattering effect. In this approach; we provide only a single parameter that represents theintensity of incident light scattering in a translucent material. We implemented our algorithmas a hardware-accelerated real-time renderer with a HLSL pixel shader. This approach iseasily implementable on the GPU and does not require any complicated pre-processing andmulti-pass rendering as is often the case in this area of research.,Symposium on Human Interface,2011,*
リアルタイムスキンシェーダとしての曲率に依存する反射関数の提案と実装,久保尋之， 土橋宜典， 津田順平， 森島繁生,人間の肌のような半透明物体のリアルな質感を再現するためには; 表面下散乱を考慮して描画することが必要不可欠である. そこで本研究では半透明物体の高速描画を目的とし;曲率に依存する反射関数 (CDRF) を提案する. 実際の映像作品ではキャラクタの肌はそれぞれに特徴的で誇張した表現手法がとられるため; 本研究では材質の散乱特性の調整だけでなく;曲率自体を強調する手法を導入することで; 表面下散乱の影響が誇張された印象的な肌を表現可能なスキンシェーダを実現する.,研究報告 グラフィクスと CAD (CG),2011,*
既存動画コンテンツを再利用して音楽にマッチした動画を自動生成するシステム (テーマセッション; 大規模マルチメディアデータを対象とした次世代検索およびマイニング),平井辰典， 大矢隼士， 長谷川裕記， 森島繁生,抄録 本稿では; 入力された任意の音楽を元に; 既存の動画コンテンツを再利用し;人間が音楽と映像が同期していると感じる音楽動画を自動的に生成するシステムを提案する.本システムの土台となる音楽と映像の同期手法は; 音楽のエネルギーを示す特徴量である RMSに対し; 映像のアクセント (明滅や動きなど) を付加するというものである. これは;本研究で行った主観評価実験により人が音楽と映像が 「合っている」 と感じると確かめられた同期手法である. 本システムの動画生成は; まずデータベースの構築として既存の動画シーケンスから各フレームの明滅; 動きに関する映像特徴量の計算を行う. それに対し入力音楽の RMSの挙動に最も近い挙動を示す映像特徴量を持つ動画シーケンスをデータベース中から探索し;それらの映像シーケンスを切り貼りすることで; 音楽に最も同期している音楽動画の生成を行うというものである.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2011,*
新映像技術 「ダイブイントゥザムービー」,森島繁生， 八木康史， 中村哲， 伊勢史郎， 向川康博， 槇原靖， 間下以大， 近藤一晃， 榎本成悟， 川本真一， 四倉達夫， 池田雄介， 前島謙宣， 久保尋之,映像コンテンツの全く新しい実現形態として; 観客自身が映画等の登場人物となり;時には友人や家族と一緒にこの作品を鑑賞することによって; 自身がストーリーへ深く没入し;かつてない感動を覚えたり; 時にはヒロイズムに浸ることを実現可能とする技術 「ダイブイントゥザムービー」 について本稿で解説する. この実現には; 観客に全く負担をかけることなく本人そっくりの個性を有する登場人物を自動生成する技術と; 自ら映像中のストーリーに参加しているという感覚を満足するためのキャラクタ合成のクオリティ; 映像シーンの環境に没入していると錯覚させる高品質な映像・音響再現技術及びその収録技術が; 観客の感動の強さを決定する重要な要素となる. 2005年の愛・地球博にて実証実験を行った 「フユーチャーキャスト」 に端を発するこの技術は;ハードウェアの進歩と 2007 年にスタートした文部科学省の支援による科学技術振興調整費プロジェクトの実施によって; 格段の進歩を遂げた. その結果; 様々なバリエーションの観客の …,*,2011,*
D-12-38 幾何学的制約を考慮した Linear Predictors による顔特徴点自動抽出 (D-12. パターン認識・メディア理解; 一般セッション),松田龍英， 原朋也， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
D-12-30 基準形状変形による多視点動画像からの動的立体形状再現 (D-12. パターン認識・メディア理解; 一般セッション),種田大地， 山中健太郎， 國友翔次， 須田洋文， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
D-12-39 顔画像における陰影変化を伴う表情生成 (D-12. パターン認識・メディア理解; 一般セッション),三間大輔， 鑓水裕刀， 久保尋之， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
D-12-9 複数視点からの深度マップを用いた半透明物体の高速描画 (D-12. パターン認識・メディア理解; 一般セッション),小坂昂大， 服部智仁， 久保尋之， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
D-12-10 動的な水の表面形状を考慮した流体のパラメータ推定 (D-12. パターン認識・メディア理解; 一般セッション),岩本尚也， 國友翔次， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
D-12-37 経年変化を考慮した個人識別手法の検討 (D-12. パターン認識・メディア理解; 一般セッション),原田健希， 田副佑典， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2011,*
音楽と映像の同期手法に基づくダンス動画生成システム,平井辰典， 大矢隼士， 長谷川裕記， 森島繁生,本稿では; システムの第一段階として; ダンスという動的な映像対象のみに限定したシステムの構築をしていくため; 音楽と映像のムードの一致による意味的な調和の要因をすべて排除したうえで;時間軸上での調和の実現を目指した同期手法について検討した. ここで; 楽曲のテンポに映像のアクセントを一致させると人が同期を感じるという報告を元に; さらに同期を感じるような同期手法について考える. テンポとは; 楽曲における 4 分音符の長さを左右するものであり; その曲が 1 分間に4 分音符を何回分弾ける速さかを示す BPM (Beats Per Minute) で表される. つまり;楽曲のテンポと映像のアクセントが一致しているというのは; 音楽 1 小節に対し; 4つの点において映像のアクセントが付加されている状態を示している. これでは; 楽曲の小節中での4 分音符よりも細かい単位での音の変化への対応が十分にできているとは言えない. 例えば; 16分音符でのピアノ演奏に対して; 4 分音符のリズムで照明が明滅する場合を考えると; テンポは一 …,研究報告 音楽情報科学 (MUS),2011,*
2-6. 顔• 人体メディアが拓く新産業の画像技術,川出雅人， 持丸正明， 森島繁生,2.1. 1 顔検出技術さまざまな組込み機器や LSI に顔画像センシング技術を内蔵するためには;非常に小型; 高速; 省メモリーなアルゴリズムが必要である. 2000 年に; Viola と Jones が発表したHaar タイプの特徴量+ 積分画像+ AdaBoost 学習を使った顔検出技術 1) の登場により;実装を中心としたアルゴリズムが進展し; この技術をベースにしたさまざまな改良技術が提案され;実用化が一気に進んだ. 特徴量には; 弱識別器として; 軽い演算と少ないメモリー使用量で算出できる; 局所特徴量がよく使われている. Haar-like 特徴量; EOH (Edge of OrientationHistograms) 特徴量; HOG (Histograms of Oriented Gradients) 特徴量; Edgelet 特徴量;Sparse Granular 特徴量 2)(図 2) などがある. それらの特徴量のさらなる改良として;局所領域の関連性に着目した特徴量が実用化されている. Joint Haar-like 特徴量 3); Shapelet特徴量; Joint HOG 特徴量; Joint Sparse Granular 特徴量などがある. 学習には; 汎化性能の …,映像情報メディア学会誌,2011,*
Example-based Deformation with Support Joints,Kentaro Yamanaka; Akane Yano; Shigeo Morishima,In character animation field; many deformation techniques have been proposed. Example-based deformation methods are widely used especially for interactive applications. Example-based methods are mainly divided into two types. One is Interpolation. Methods in this typeare designed to interpolate examples in a pose space. The advantage is that the deformedmeshes can precisely correspond to the example meshes. On the other hand; thedisadvantage is that larger number of examples is needed to generate arbitrary plausibleinterpolated meshes between each example. The other is Example-based Skinning whichoptimizes particular parameters referencing examples to represent example meshes asaccurately as possible. These methods provide plausible deformations with fewer examples.However they cannot perfectly depict example meshes. In this paper; we present an idea …,*,2011,*
Facial animation reflecting personal characteristics by automatic head modeling and facial muscle adjustment,Akinobu Maejima; Hiroyuki Kubo; Shigeo Morishima,We propose a new automatic character modeling system which can generate anindividualized head model only from a facial range scan data and an individualized facialanimation with expression change. The head modeling system consists of two coremodules: the head modeling module which can generate a head model from a personalfacial range scan data using automatic mesh completion; and the key shape generationmodule which can generate key shapes for the generated head model based on physics-based facial muscle simulation with a personal muscle layout estimated from subject's facialexpression videos. As a result; we can generate a head model which can synthesize facialexpressions and impression similar to the target person. The experimental result shows thatwe archive to synthesize CG characters that subjects can identify themselves with 84 …,Communications and Information Technologies (ISCIT); 2010 International Symposium on,2010,*
歩行における知覚的類似性尺度に基づく個人性を強調した動作合成手法,中村槙介， 森島繁生,< あらまし> 人間の歩行動作には; 個人性情報が含まれており; 最近では歩容個人認証の研究も盛んである. しかし個人の特徴を強調し; 反映する歩容アニメーションを作ることは困難である.本研究では; 歩行動作における個人性とは平均的な歩行動作からの差異によって表現されるものであると仮定し; その差異を増大させることによって個人性を強調した歩行動作を合成する.合成される歩行動作は; 複数のサンプル歩行動作の主成分分析によって構築される空間において表現する. また; 増大させる差異の大きさについては; 複数の人物の歩行動作の中から特定の人物の歩行動作を探す主観評価実験によって最も認識率の高くなる割合を推定し; それを用いる.,画像電子学会誌,2010,*
BT-2-2 Consumer Participable Digital Contents,Shigeo MORISHIMA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proceedings of the Society Conference of IEICE,2010,*
A skinning technique considering the shape of human skeletons,Hirofumi Suda; Kentaro Yamanaka; Shigeo Morishima,Copyright is held by the author / owner(s). SIGGRAPH 2010; Los Angeles; California; July 25– 29; 2010. ISBN 978-1-4503-0210-4/10/0007 … A Skinning Technique Considering the Shapeof Human Skeletons … Hirofumi SUDA i Kentaro YAMANAKA Waseda University Shigeo MORISHIMA… 1. Abstract We propose a skinning technique to improve expressive power of Skeleton SubspaceDeformation (SSD) by adding the influence of the shape of skeletons to the deformation resultby post- processing … 2. Introduction It is general that the deformation of 3DCG character modelis expressed by motion of the skeleton. The representative example of the transformation methodis SSD[1]. SSD is a technique to deform character by linear blending of transform matrices withweight for each skeleton. The expression is as follows (1) … 0 1 ;0 1 ' S TTw S i ii b i 6 (1) …0 is a vertex on the original surface in rest pose … 3. Modeling the skeleton motion from …,ACM SIGGRAPH 2010 Posters,2010,*
A study of relationship between speaker identification and acoustic features using perceptual similarity of imitated voice (音声),Mari Tanaka; Hideki Kawahara; Shigeo Morishima,抄録 Physical correlates of perceived personal identity are investigated using imitated 16utterances spoken by 11 mimicry speakers and 24 test subjects. Our unique strategy to usenon-professional impersonators enabled to prepare test utterances with wide range ofperceived similarities. Reasonably high correlations (0.46 and 0.44) in multiple regressionanalysis were attained by grouping subjects into three groups based on cluster analysis ofthe subjective test results. Without clustering; the correlation was only 0.17. The clusteranalysis also revealed differences in their focusing physical correlates between threegroups indicating importance of individual differences both in speakers and listeners.,電子情報通信学会技術研究報告. SP; 音声,2010,*
人物頭部モデル自動生成システムの実現--最適化局所アフィン変換に基づく人物頭部モデルの自動生成,前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2010,*
遮蔽度の曲率近似によるアンビエントオクルージョンの局所照明モデル化 (グラフィクスと CAD (CG) Vol. 2009-CG-138),服部智仁， 久保尋之， 森島繁生,*,情報処理学会研究報告,2010,*
主観評価に基づく個人性を強調した歩行動作合成手法の提案 (グラフィクスと CAD (CG) Vol. 2009-CG-138),中村槙介， 森島繁生,*,情報処理学会研究報告,2010,*
3D Face Reconstruction from Multi-view Images,Tomoya HARA; Hiroki FUJISHIRO; Shinya NAKANO; Akinobu MAEJIMA; Shigeo MORISHIMA,Abstract 3 次元顔モデルは; 映像制作や個人認証など様々な分野で応用されている. 筆者らは;先行研究において; 大型な装置を必要とせずに 3 次元顔モデルを構築する手法として;頭部変形モデルに基づき正面顔画像から高速に 3 次元顔モデルを構築する手法を提案している.しかしながら; 鼻の高さや頬部分の凹凸の個人特徴を忠実に再現出来ないという問題点があった.本稿では; 顔向きを変化させて撮影した複数枚の画像を入力として; 各画像から構築される 3次元顔モデルを; 顔の部位ごとに最適な重みを与えて統合することで; より本人らしい 3次元顔モデルを生成する手法を提案する. 提案手法をオープンテストによって評価した結果;従来手法と比べて; 特に鼻や口領域に関して; 精度の向上を確認することが出来た.,IEICE technical report,2010,*
Fast-Automatic 3D Face Model Generation from Single Snapshot,Akinobu MAEJIMA; Shigeo MORISHIMA,Abstract 本報告では; スナップ写真から写真中の人物らしい 3 次元顔モデルを高速自動生成する手法について述べる. 提案手法は 3 次元顔形状の事前知識として 1153 人の 3次元顔モデルから構築される顔変形モデルと; 顔形状の分布に対してフィッティングされた混合ガウス分布モデルを用いる. 3 次元顔形状は; 画像から自動検出される顔特徴点と顔変形モデル上の対応頂点との残差の二乗和が最小かつその時の顔変形モデルのモデルパラメータに対する混合ガウス分布モデルの尤度が最大となるようなエネルギー最小化問題を解くことにより推定される.提案手法に対する性能評価実験の結果から; 平均 1.2 秒の処理時間で 2.1 mm の精度誤差を持つ3 次元顔モデルが生成可能であることを示す.,IEICE technical report,2010,*
多視点顔画像に基づく 3 次元顔形状推定,原朋也， 藤代裕紀， 中野真也， 前島謙宣， 森島繁生,抄録 3 次元顔モデルは; 映像制作や個人認証など様々な分野で応用されている. 筆者らは;先行研究において; 大型な装置を必要とせずに 3 次元顔モデルを構築する手法として;頭部変形モデルに基づき正面顔画像から高速に 3 次元顔モデルを構築する手法を提案している.しかしながら; 鼻の高さや頬部分の凹凸の個人特徴を忠実に再現出来ないという問題点があった.本稿では; 顔向きを変化させて撮影した複数枚の画像を入力として; 各画像から構築される 3次元顔モデルを; 顔の部位ごとに最適な重みを与えて統合することで; より本人らしい 3次元顔モデルを生成する手法を提案する. 提案手法をオープンテストによって評価した結果;従来手法と比べて; 特に鼻や口領域に関して; 精度の向上を確認することが出来た.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2010,*
3 種の色光源を用いた多視点動画像からの動的立体構造再現 (テーマ関連; 顔・人物・ジェスチャ・行動),小林昭太， 森島繁生,抄録 カメラを用いた撮影画像の陰影情報から; 対象の 3 次元形状を復元する手法が研究されている. しかし; 複数のカメラに同時に陰影情報を与える照明環境を作成することは難しく;対象の全体像の動的な形状変化を再現することは困難である. そこで; 本研究では 3色の色光源を用い; 各照明に対応する陰影情報を撮影画像から分離して取得できる環境を作成することで複数の陰影情報を同時に取得し; 視体積交差法により得られる対象の初期形状の法線を再計算する. 陰影情報から法線を推定する際; 反射モデルが必要になるが; 本研究では;撮影環境に特化した反射モデルを; 十分なキャリブレーションを行うことで作成した.再計算された法線から; 初期形状の頂点座標を再計算することで 3 次元形状を再現する.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2010,*
D-11-81 非線形モーフィングに基づく手描き顔アニメーションの中割り画像生成 (D-11. 画像工学; 一般セッション),郷原裕明， 杉本志織， 森島繁生,琳 ブ. ン; 轟.. ブ. ン ド率を部位 に よっ て変え; さらに通 常線形で あるブレ ン ド率を; 入力画像の 顔の 角度; 生成対象の 顔 の 角度; 顔角度 閾値 によっ て 変えるこ とによ り; 特徴点星 μ を導く. 尚;今回図 2 (d) を作成する の に 用い た ブ レ ン ド率は; 左 目; 口; 眉毛に関して,電子情報通信学会総合大会講演論文集,2010,*
D-12-8 多視点顔画像に基づく顔器官毎の重みを考慮した 3 次元顔形状推定 (D-12. パターン認識・メディア理解; 一般セッション),原朋也， 藤代裕紀， 中野真也， 前島謙宣， 森島繁生,近年; 映像製作や個 人認証な どにおける 3 次元顔モ デルの 需要増加を背景に; 2 次元顔画像から 3 次元顔モ デル を生成す る手法の 研究が 行わ れ て い る [ 1; 2]. こ れ らの 手 法の最大 の 利点は; レ ン ジ ス キ ャ ナ の よ うな 大型 で 高額 な計測装置を必 要 とせず; 手軽に 3 次元顔モデル を作成出来 る 点 で あ る. Blanz らは; Morphable Model に 基づ き正面および側 面から撮影 された顔画像か ら対象人物の 3 次元顔モ デル を生成する手法を提案 して い る が [1];処理 に 必要 な手 間や計算時間 に 課 題が あ る と言 える. 筆者 らは; 先行研 究におい て;頭部変形モ デル に 基 づ き正面顔 画像か ら高速に 3 次元頭部モ デル を生成する手法を提案してい る [2]; しか しなが ら; 鼻の 高さや頬部分の 凹凸 の 個人特徴を忠実に再現 出来ない という問題点が あっ た; 本稿では; 入力 とし て; 顔向きを変化 させ て 撮影 された画像を使用し;各画像か ら復元される 3 次元顔モ デル を重み付き線形和によ り統合するこ とで; よ り本人 …,電子情報通信学会総合大会講演論文集,2010,*
A-15-10 3 次元形状とテクスチャの双方の変換による年齢変化顔の生成 (A-15. ヒューマン情報処理; 一般セッション),田副佑典， 藤代裕紀， 中野真也， 野中悠介， 笠井聡子， 前島謙宣， 森島繁生,い る [1'3]. Scherbaum らは; Morphable Model に 基づい て 顔画像か ら推定 された 3 次元顔モデル に対する年齢変化手法を提案して い る田. し か し なが ら; モ デル の 3次元形状はあくまで推定された物で あり; 個入の 正 確な 3 次元形状が反映されてい る とは言い難い. そこ で 本稿で は; レ ン ジス キャ ン デ,電子情報通信学会総合大会講演論文集,2010,*
A-15-11 人体の骨格形状を考慮したスキニング手法の提案 (A-15. ヒューマン情報処理; 一般セッション),須田洋文， 中村槙介， 山中健太郎， 森島繁生,ヤ ラ ク タモ デル の 動作に 伴 う変形 は; 骨格 モ デルの 変形 によ りモ デル 化 され る の が 般的で ある; その 変形法の 代表 例 と して; Skeietal Subspace Defb ation (以 下 SSD) が挙げられ る. SSD と は; 以下の 式 (1) の よ うに; キャ ラ ク タメ ッ シ ュの各頂点に 対 し; 埋 め 込まれた各骨格モ テル へ の 重み を定義し; 骨格 モ デル の 運動 をその 重み に従っ て 線形に ブ レン ドす る こ と で メ ッ シ ュ を変形す る手法で ある.,電子情報通信学会総合大会講演論文集,2010,*
A-16-10 静的・動的特徴を考慮した布の物理パラメータ推定 (A-16. マルチメディア・仮想環境基礎; 一般セッション),國友翔次， 中村槙介， 森島繁生,い るこ とで; リア ル な CG が作られて い る. しか し; 布の 質感の 違い を表現するた めには; 多くのパ ラメータを調節 しなけれ ばなら ない た め; ア ニ メ ータの 直感に頼 る とこ ろが大きくなっ て しまう;,電子情報通信学会総合大会講演論文集,2010,*
遮蔽度の曲率近似によるアンビエントオクルージョンの局所照明モデル化,服部智仁， 久保尋之， 森島繁生,間接光を考慮した大域照明モデルによって生成される柔らかい陰影は; より物体を立体的にリアル表現するために必要であり; 局所照明モデルによって得難いものである. 現在; Ambient Occlusionと呼ばれる大域照明モデルを局所照明モデルに付加することにより; ソフトシャドウを低コストに得る手法が知られている. 本稿では; Ambient Occlusion を; 曲率を用いた局所照明モデルに落とし込むことにより; 従来のモデルより少ない計算量により効果を得る手法を提案する.,研究報告グラフィクスと CAD (CG),2010,*
主観評価に基づく個人性を強調した歩行動作合成手法の提案,中村槙介， 森島繁生,人間の歩行動作には; 個人性情報が含まれており; 最近では歩容個人認証の研究も盛んである.しかし個人の特徴を強調し; 反映する歩容アニメーションを作ることは困難である. 本研究では;歩行動作における個人性とは平均的な歩行動作からの差異によって表現されるものであると仮定し;その差異を増大させることによって個人性を強調した歩行動作を合成する. 合成される歩行動作は;複数のサンプル歩行動作の主成分分析によって構築される空間において表現する. また;増大させる差異の大きさについては; 複数の人物の歩行動作の中から特定の人物の歩行動作を探す主観評価実験によって最も認識率の高くなる割合を推定し; それを用いる.,研究報告グラフィクスと CAD (CG),2010,*
Dive into the movie: an instant casting and immersive experience in the story,Shigeo Morishima,Abstract Our research project; Dive into Movie (DIM) aims to build a new genre of interactiveentertainment which enables anyone to easily participate in a movie by assuming a role andenjoying an embodied; first-hand theater experience. This is specifically accomplished byreplacing the original roles of the precreated traditional movie with user created; high-realism; 3-D CG characters. DIM movie is in some sense a hybrid entertainment form;somewhere between a game and storytelling. We hope that DIM movies might enhanceinteraction and offer more dramatic presence; engagement; and fun for the audience. In DIMmovie; audiences can experience highrealism 3-D CG character action with individualizedfacial characteristics; expression; gait and voice. The DIM system has two key features: First;it can full-automatically create a CG character in a few minutes from capturing the face …,Proceedings of the 16th ACM Symposium on Virtual Reality Software and Technology,2009,*
ストーリへの没入感を体験可能な高臨場感コンテンツ (高臨場感ディスプレイフォーラム 2009 臨場感とは何か?),森島繁生,抄録 観客全員を映画のキャストとして登場させ; ストーリへの没入感を体験可能な新しいコンテンツ形態を提案している. 2005 年の愛・地球博; 三井・東芝館において; 世界で初めて実現されたフューチャーキャストシステムは; 視聴者体験型の映像コンテンツ 『グランオデッセイ』 を実現し; 6か月間の体験者 163 万人を記録し; 人気を博した. この技術は; 2007 年にハウステンボスにおいて;フューチャーキャストシアターとして商用化された. いかに短時間で負担を与えることなく;観客の個性を反映する CG キャラクタを自動カスタマイズ合成するかが成功のカギとなるが; 今回;当初の顔の 3 次元形状計測に加えて; 表情の個性; 体系・歩容の個性; 声質の個性;髪形の選択などを短時間かつ自動的にモデル化する手法を新たに実現し; 頭髪を含む全身モデルのリアルタイムレンダリングを実現することで; 自分自身を映像中に発見できる確率を; 万博システムの59% から約 90% に劇的に向上させることができた.,映像情報メディア学会技術報告 33.47,2009,*
A natural smile synthesis from an artificial smile,Hiroki Fujishiro; Takanori Suzuki; Shinya Nakano; Akinobu Mejima; Shigeo Morishima,Abstract Everyone is interested in being more attractive. Leyvand et al have been proposedthe method which can enhances of facial attractiveness into a photograph [Leyvand et al.2008]. However; their method can't synthesize more attractive facial expression than that ofan input face photograph. Meanwhile; amateur subjects' facial expressions often becomeunnatural when they act in front of the camera in experimental environments. At such asituation; a natural expression can be synthesized without performance skills.,SIGGRAPH'09: Posters,2009,*
Curvature-dependent local illumination approximation for translucent materials,Hiroyuki Kubo; Mai Hariu; Shuhei Wemler; Shigeo Morishima,Abstract Simulating sub-surface scattering is one of the most effective ways to realisticallysynthesize translucent materials such as marble; milk and human skin. In previous work; themethod developed by Jensen et al.[2002] improved significantly on the speed of thesimulation; yet still cannot produce real-time rendering. Thus; we have developed a simplelocal illumination model which mimics the presence of a subsurface scattering effect.Furthermore; this approach is easy implementable on the GPU and doesn't require anycomplicated pre-processing as is often the case in this area of research [Mertens et al.2003].,SIGGRAPH'09: Posters,2009,*
Accurate skin deformation model of forearm using MRI,Kentaro Yamanaka; Shinsuke Nakamura; Shota Kobayashi; Akane Yano; Masashi Shiraishi; Shigeo Morishima,Abstract This paper presents a new methodology for constructing a skin deformation modelusing MRI and generating accurate skin deformations based on the model. Many methods togenerate skin deformations have been proposed and they are classified into three maintypes. The first type is anatomically based modeling. Anatomically accurate deformationscan be reconstructed but computation time is long and controlling generated motion isdifficult. In addition; modeling whole body is very difficult. The second is skeleton-subspacedeformation (SSD). SSD is easy to implement and fast to compute so it is the most commontechnique today. However; accurate skin deformations can't be easily realized with SSD.The last type consists of data-driven approaches including example-based methods. Inorder to construct our model from MRI images; we employ an example-based method …,SIGGRAPH'09: Posters,2009,*
Directable anime-like shadow based on water mapping filter,Yohei Shimotori; Shiori Sugimoto; Shigeo Morishima,Abstract Shadows in 2D Anime play a significant role for expressing symbolic visual effectssuch as the character's position and shape. However; animators frequently can't drawdetailed shadows according to their intentions because of time constraints and a lack ofskilled animators. For solving this problem; we have developed a system that can generateshadows automatically. Our system provides simple shadows and shadows on the water byapplying Simplification Filter and Water Mapping Filter. Also; our system only requires inputsof the 2D character animation layers generally composed in the Anime industry.Consequently; our system enables animators to intuitively produce Anime-like shadowanimation in a short time.,SIGGRAPH'09: Posters,2009,*
調音結合モデルを用いた母音交換に基づく話者変換法 (音声変換; 認識; 理解; 対話; 一般),山本達也， 室伏空， 森島繁生,抄録 合成音声の声質変換技術として研究されている統計的スペクトル変換法は;ターゲットとなる話者から事前に発話データを取得する必要があり; 多くの発話データを得ることによってより品質の高い話者変換が可能になることが知られている. しかし一方でターゲットとなる話者から膨大な発話データを得ることは話者にとって大きな負担となる. 一方; 母音交換法による話者変換はターゲットとなる話者から母音のみを取得してあらゆる発話内容に対して話者変換を行うことができる. 筆者らは母音交換法の問題点であった音声の不連続性を改善するため音素境界付近に調音結合モデルを用いて自然な音声を合成する手法を提案した. また; 調音結合モデルの適用区間を入力話者の発話内容から学習することでより自然発話に近い音声を合成する手法を提案する.,電子情報通信学会技術研究報告. SP; 音声,2009,*
A-10-2 楽器音テンプレートマッチングによる倍音誤り補正システム (A-10. 応用音響; 一般セッション),藤澤賢太郎， 室伏空， 近藤康治郎， 森島繁生,われてい る. こ れ までの 研究では; 音源数を仮定 したもの や 「1]・[2]・[3]; 市販 CD の メロ ディを推定する PreFEst [4] がある. しか し; こ の 中に倍音誤 りがあるこ とが報告さ,電子情報通信学会総合大会講演論文集,2009,*
A-15-15 MRI を用いた前腕運動時の皮膚形状変化の精密な再現 (A-15. ヒューマン情報処理; 一般セッション),山中健太郎， 中村槙介， 矢野茜， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2009,*
D-11-109 データベースに基づく車体形状デザイン GUI の構築 (D-11. 画像工学; 一般セッション),仲田真輝， 早川達順， 杉本志織， 森島繁生,次元モ デ リン グ ソ フ トを用い; 手作業で 立体的な形 状をモデ リン グす る; こ の 作業には長い時間 と多大な コ ス トを要し効率化が求め られ て い る. こ の よ うな背景か ら; 本研 究で は;過去の 車種の 3 次元形状デ,電子情報通信学会総合大会講演論文集,2009,*
A-15-17 多様な表情を合成可能な固有顔空間の構築 (A-15. ヒューマン情報処理; 一般セッション),高見澤涼， 鈴木孝章， 久保尋之， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2009,*
D-14-4 調音結合補正を用いた母音交換法に基づく話者変換法 (D-14. 音声; 一般セッション),山本達也， 室伏空， 近藤康治郎， 森島繁生,タベ ース の 大幅な削減が 可能 と なっ た. しか し音声の 品質とい う点で は; 未だ問題がある田. そこ で 本研究で は; 井上 らの 母音変換法 [1] に よ る 話者変換法に 調音結合を考慮し; データベ,電子情報通信学会総合大会講演論文集,2009,*
A-15-16 顔動画像のオプティカルフローに基づく作り笑い・自然な笑いの識別 (A-15. ヒューマン情報処理; 一般セッション),藤代裕紀， 鈴木孝章， 中野真也， 野中悠介， 前島謙宣， 森島繁生,入力 された顔表情が; 喜怒哀楽等の 基本表情の い ずれで あるか 判別す る研究が 多く; 実際の感情状態を伴っ て 表出された表情を推定するもの は少ない. 特定の 表情 に 対 し て作為的な感情状態か 自然な感情状態か を推定する研究 として; 四 fi [1] らの 研究 がある が;高価なハ イ ス ピ ードカ メ ラを用 いて顔 部位毎変動の 微少時間の ずれ に着 目する もの で あった. また; 特徴点の 抽出を手動で 行 うなど; 認識 シ ス テム の 自動化は 考慮 されて い ない. そ こで; 本研究で は; 特に無表情か ら笑い の 表情 への 変化に 注目し; 表情筋の 動作に密接に関連する領域 に追跡点を定めて; こ の 時間的な変化をオプテ ィ カル フ ロ ー によ り記述する.表情表出前後の 追跡点の 差分を特徴ベ ク トル とし;「作り笑い」 と 「自然な笑い」 とを識別するシ ステム を構築 した;,電子情報通信学会総合大会講演論文集,2009,*
A-15-14 MRI に基づく皮膚下構造を反映した顔面筋肉モデルの構築 (A-15. ヒューマン情報処理; 一般セッション),鑓水裕刀， 石橋康， 久保尋之， 前島謙宣， 森島繁生,て い る. そ; cr; 本稿ては群来の 筋肉ヤ デル [厂冫門題 「' ある反麟 ド搆疋 を; MRI J り取得しノ実際の 欟 L を反映 曳 せ る 二 と tt 餌構築 し; 表栴の 個軽を 舷 缺可能な表傭合 1 え丁法を提案寸る; 本 f 法 こ よ り; 倒人の 表 {青をよ り自撚に含」 歳寸 ろ こ とか 叮能と なっ た.,電子情報通信学会総合大会講演論文集,2009,*
Post-recording tool for instant casting movie system,Shin-ichi Kawamoto; Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,Abstract This paper proposes a universal user-friendly post-recording tool for an InstantCasting Movie System (ICS) that enables anyone to be a movie star using his or her ownvoice and faces. A personal CG character is automatically generated by scanning one's facegeometry and image in ICS. Voice is as essential to identify a person as face. However; acharacter's voice is only based on gender in ICS. We proposed a novel voice recording toolfor participants of all ages in a short time. Post-recording tasks are very difficult becausespeakers should speak in synchronization with the mouth movements of the CG characters.Therefore this task is generally recorded by professional voice actors. Our proposed tool hasthe following four features: 1) various supporting information for synchronization with voiceand mouth movement timing for users; 2) automatic post-processing of recorded voices …,Proceedings of the 16th ACM international conference on Multimedia,2008,*
A computer graphic three‐dimensional tongue and lip movement synchronized with English fricatives for Japanese learners.,Toshiko Isei‐Jaakkola; Shigeki Suzuki; Shigeo Morishima; Keikichi Hirose,Some English fricatives are difficult specifically for Japanese learners of English (JL2) toproduce. Simultaneous articulation of the lip and teeth (as in lebiodentals); or the tongueand teeth (as in dentals); or protruding lips (as in postalveolars) do not exist in standardJapanese. To understand partially or completely; invisible articulatory movements areunavoidable for JL2 in order to produce these fricatives properly. Thus; as an aid for thebasic pronunciation training; a visualized automatic lip and tongue movement programsynchronized with these fricatives was developed; utilizing three‐dimensional computergraphic technologies. In this program; not only the lips; teeth; and tongue but also the othernecessary speech organs were made to be half transparent. Consequently; it enables thelearner to listen to and repeatedly model the target segmental sound while looking at …,The Journal of the Acoustical Society of America,2008,*
Synthesizing facial animation using dynamical property of facial muscle,Hiroyuki Kubo; Yasushi Ishibashi; Akinobu Maejima; Shigeo Morishima,*,ACM SIGGRAPH 2008 posters,2008,*
3D facial animation from high speed video,Takanori Suzuki; Yasushi Ishibashi; Hiroyuki Kubo; Akinobu Maejima; Shigeo Morishima,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references … JY.Bouguet; Pyramidal implementation of the Lucas Kanade feature tracker description of thealgorithm; Intel Corporation; Microprocessor Research Labs; 1999 … N. Arad; D. Reisfeld; ImageWarping Using few Anchor Points and Radial Functions; The Eurographics Association1994 … Note: Larger/Darker text within each node indicates a higher relevance of the materialsto the taxonomic classification.,ACM SIGGRAPH 2008 posters,2008,*
Hair animation and styling based on 3D range scanning data,Shiori Sugimoto; Shigeo Morishima,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references … TomoakiMizuno; Toyohisa Kaneko; Shigeru Kuriyama; "3D Hair Modeling from Photographs"; IPSJ SIJTechnical Report; Vol. 2005; No. 13(20050207) pp. 43--48;(2005) … Hadap and N;Magnenat-Thalmann; "Modeling dynamic hair as continuum." In Eurographics Proceedings.Computer Graphics Forum; Vol. 20; No. 3; 2001 … Note: Larger/Darker text within each nodeindicates a higher relevance of the materials to the taxonomic classification.,ACM SIGGRAPH 2008 posters,2008,*
Automatic and accurate mesh fitting based on 3D range scanning data,Shinya Nakano; Yusuke Nonaka; Akinobu Maejima; Shigeo Morishima,*,ACM SIGGRAPH 2008 posters,2008,*
アニメ作品制作の高能率化をめざす研究開発,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2008,*
Instant Casting Movie System for Entertainment Revolution,Shigeo Morishima,Our research project; Dive into Movie (DIM) aims to build a new genre of interactiveentertainment which enables anyone to easily participate in a movie by assuming a role andenjoying an embodied; first-hand theater experience. This is specifically accomplished byreplacing the original roles of the precreated traditional movie with user created; high-realism; 3-D CG characters. DIM movie is in some sense a hybrid entertainment form;somewhere between a game and storytelling. We hope that DIM movies might enhanceinteraction and offer more dramatic presence; engagement; and fun for the audience. Ourwork on DIM is ongoing; but its initial version; Future Cast System (FCS); is up and running.In the initial version; we focus on creating audiences' highrealism 3-D CG characters withpersonal facial characteristics; replacing the original characters' faces in the original …,*,2008,*
英語音声教育のための 3DCG による舌の動きと音声のリンク開発の試み--語彙との同期 (ヒューマンインフォメーション・立体映像技術),ヤーッコラ伊勢井敏子， 鈴木茂樹， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,映像情報メディア学会技術報告,2008,*
椎骨骨格形状モデルに基づくデータドリブンな脊椎動作モデリング,関根孝雄， 森島繁生,Abstract In this research; we create a 3-Dimentional backbone model with the shape ofvertebra based on the structure of actual humans; Additionally we develop a systemgenerating natural backbone motions of character in CG. With the technologicaladvancement in CG; character animations are drawn using skeleton models in movies andvideo games. It is difficult for users to model a backbone composed of detailed bonescompared to other parts of a body such as arms and feet with a simple composition.Therefore; we create natural backbone motions and control these motions by dividing abackbone,情報処理学会研究報告グラフィクスと CAD (CG),2008,*
直感的に影を演出可能な編集ツール (< 小特集> ディジタルコンテンツ制作-DCS'07 関連-),中嶋英仁， 杉崎英嗣， 森島繁生,抄録 Shadows in cartoon animation are generally used for dramatizing scenes. In hand-drawn animation; these shadows reflect the animators' intention and style rather thanphysical phenomena. On the other hand; shadows in 3DCG animation are photorealisticallyrendered; and animators can not fully reflect their intention. This is because; in 3DCGanimation; shadows are automatically generated once the light source is defined. Therefore;we develop an interactive tool for editing shadows that combines the advantages of hand-drawn animation and 3DCG technology. the advantage of our tool is that shadow attributesare inherited once animators edit the shape and location of shadows. Animators are onlyrequired mouse operations for editing shadows. Consequently; our tool enables animatorsto create shadows automatically and easily to reflect their intention and style.,映像情報メディア学会誌: 映像情報メディア,2008,*
Advances in Multimedia Modeling: 14th International Multimedia Modeling Conference; MMM 2008; Kyoto; Japan; January 9-11; 2008; Proceedings,Shin'ichi Satoh; Frank Nack; Minoru Etoh,Welcometothe14thInternationalMultimedia… (MMM2008); held January 9–11; 2008 at KyotoUniversity; Kyoto; Japan. MMM is a leading international conference for researchersandindustry practitioners to share their new ideas; original research results and practicaldevelopment experiences from all multimedia related areas. It was a great honor to haveMMM2008; one of the most long-standing m-timedia conferences; at one of the mostbeautiful and historically important Japanese cities. Kyoto was an ancient capital of Japan;and was and still is at the heartofJapanesecultureandhistory. Kyotoinwintermaydistinctivelyo? er the sober atmosphere of an ink painting. You can enjoy old shrines and temples whichare designated as World Heritage Sites. The conference venue was the Clock TowerCentennial Hall in Kyoto University; which is one of the oldest universities in Japan …,*,2008,*
ストーリへの没入感を実現するダイブイゥザムービープロジェクト (オーガナイズドセッション; CV/PR 技術の VR への応用),森島繁生， 八木康史， 中村哲,抄録 『ダイブイントゥザムービー』 は; 視聴者自らが映画の登場人物として演技し;それを観客として家族や友人と鑑賞し感動を共有する 1 つ形態と; 忠実な環境を仮想再現することによって観客自らが主人公の存在する空間や音場を体験共有する形態という; 2つのスタイルでストーリへの没入感を実現する; 世界に類を見ないエンタテインメントの提案である.この実現のためには; 視聴者そっくりのキャラクタモデルを短時間のうちに生成し;本人の特徴を忠実に再現しながら実時間合成する必要がある技術開発と; 演者を取り巻く映像および音場環境を忠実に記録し; 自動的に再現する技術が必要となる.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2008,*
ディジタルコンテンツ制作を支える新技術 1. キャラクタアニメーション制作の高能率化手法,森島繁生， 栗山繁， 川本真一,に; 外観が人と異なるキャラクタにリアルな動きを付けた際に感じられる違和感も; MoCapデータの欠点として指摘されている. MoCaToon は; リミテッドアニメーションで制作されるメリハリの利いた動きを生成して; 編集作業の効率化を図る MoCap データの自動変換技術であり;モーション編集の要素技術として位置づけられる (図 1 に操作画面を示す). すなわち;動きの本来の意味やニュアンスを失わないように; MoCap データから抜き出したキーフレームアニメーションにおけるキーフレームでの姿勢 (以後; キー姿勢) のみを描画に用いることにより;手作業で制作されるアニメ作品と同じ印象となる映像を生成する. この技術により;不気味の谷の現象 1) を取り除く副次的な効果も同時に期待される. MoCap データの圧縮のため;関節角度の軌跡を曲線近似する方法 2) が提案されているが; 基本的には元の動きの再現が目的であり; アニメのメリハリのある表現は考慮されていない. 一方; Action Synopsis 法 3) では; 曲線 …,映像情報メディア学会誌,2008,*
情報技術が支えるアートとコンテンツの世界-Art with Science Science with Art-: 5. 効率的アニメ制作支援のための 3 次元 CG 技術,森島繁生， 安生健一， 中村哲,日本のアニメーション作品は; アニメ (Anime) という名称とともに; 今や世界的な市場規模で発展し;またそのクオリティの高さが評価され; アカデミー賞を受賞するものも現れた. 元来は;アニメ生産効率向上のために; 秒あたりの作画枚数を減らすという手法 (リミテッドアニメーション)が一般的であったが; そこに日本独特のノウハウと表現技法が付加され; 今日の隆盛に至った.しかし現状のアニメ制作体制は; テレビシリーズから劇場用クオリティの作品まで;増大する需要に対応しきれず; コストの飛躍的増大と過酷な労働とを引き起こし;中国や韓国など海外への技術流出さえ始まっている. このような状況においてクオリティの高いアニメーションを効率よく制作することが現場の声として強く求められている. 伝統的な 2次元アニメの制作過程は; すでに長年の実績と経験により制作方法が確立されている.近年そのディジタル化は進んでいるが; 一部の工程がアナログからディジタルに置き換わっている …,情報処理,2007,*
Acoustic features for estimation of perceptional similarity,Yoshihiro Adachi; Shinichi Kawamoto; Shigeo Morishima; Satoshi Nakamura,Abstract This paper describes an examination of acoustic features for the estimation ofperceptional similarity between speeches. We firstly extract some acoustic features includingpersonality from speeches of 36 persons. Secondly; we calculate each distance betweenextracted features using Gaussian Mixture Model (GMM) or Dynamic Time Warping (DTW);and then we sort speeches based on the physical similarity. On the other hand; there is thepermutation based on the perceptional similarity which is sorted according to the subject.We evaluate the physical features by the Spearman's rank correlation coefficient with twopermutations. Consequently; the results show that DTW distance with high STRAIGHTCepstrum is an optimum feature for estimation of perceptional similarity.,Pacific-Rim Conference on Multimedia,2007,*
ユーザ参加型エンタテインメント 「ダイブイントゥザムービー」,森島繁生,< 和文抄録> 本稿では; 視聴者自身が映画の登場人物として演技することができ;さらに登場人物の環境を再現することによって; ストーリーへの没入感を体験できる新しいエンタテインメント 「ダイブイントウザムービー」 について述べる. 視聴者そっくりのキャラクタモデルを短時間のうちに生成し; 映画の本編にてこのキャラクタが実時間合成されて演技するという過去に類を見ないエンタテインメントの形態である. この技術を実現するためには; リアルタイム CG合成技術は当然必要であるが; 個人性を表現するためのさまざまな技術; 個人性をキャプチャするための CV 技術; そして音声信号処理技術も必要となる.,情報処理学会研究報告グラフィクスと CAD (CG),2007,*
骨格性下顎前突症患者における口唇周囲軟組織の三次元運動解析,松原大樹， 寺田員人， 中村康雄， 林豊彦， 森嶋繁生， 齋藤功,Aim: The purpose of this study was to introduce our newly developed three-dimensionalsystem of analyzing the lip movement synchronized with mandibular movement and toanalyze the relationships between lip movement and mandibular movement before and afterorthognathic surgery in skeletal Class III patients using this system. Materials and Methods:The subjects comprised 8 skeletal Class III patients who had undergone bilateral sagittalsplitting mandibular ramus osteotomy and 8 persons with individual normal occlusion.Materials consisted of three-dimensional movement values obtained by a motion capturesystem equipped with two infrared CCD cameras; and three-dimensional static valuesmeasured by the three-dimensional optical laser scanner. Four landmarks affixed on the softtissue around the mouth and four other landmarks to estimate the lower incisor movement …,日本顎変形症学会雑誌,2007,*
Facial muscle adaptation for expression customization,Yasushi Ishibashi; Hiroyuki Kubo; Akinobu Maejima; Demetri Terzopoulos; Shigeo Morishima,Abstract There are two major approaches to creating 3DCG facial expressions: The first isbased on facial muscle simulation and the second is the blend-shape approach. The blendshape approach is more familiar to creators than the facial muscle approach when theysynthesize the facial expressions of 3DCG characters. However; the facial muscle modelhas the advantage of being physics-based. It can; therefore; produce realistic facialexpressions and create facial expressions using fewer parameters than the blend shapeapproach; thereby reducing processing time and computational requirements. We introducea method which can be used to synthesize individual facial expressions based on the facialmuscle model [Waters 1987].,ACM SIGGRAPH 2007 posters,2007,*
Designing a new car body shape by PCA of existing car database,Tatsunori Hayakawa; Yusuke Sekine; Akinobu Maejima; Shigeo Morishima,Abstract We have developed a tool to design car body shapes. It enables any user even withno experience and skill to design car body shapes interactively and easily. We report howwe synthesize car body shapes by following process. Firstly; we create a database ofexisting cars and analyze all shapes in the database using Principal Component Analysis(PCA). Secondly we extract attributes that indicate the geometrical features of those car bodyshapes. The attributes are then mapped onto the Eigen space obtained as a result ofanalysis. We construct a GUI in which the distribution of the attributes on the Eigen space isvisualized. By manipulating the weight of each attribute on our GUI; we can synthesize carbody shapes which nobody has ever seen.,ACM SIGGRAPH 2007 posters,2007,*
魅せる表情--似せるキャラクター (シンポジウム 表情と動きから見た未来のアニメーション),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,美術解剖学雑誌,2007,*
Facial muscles adaptation for reproduction of individualized facial expression,Yasushi ISHIBASHI; Hiroyuki KUBO; Hiroaki YANAGISAWA; Akinobu MAEJIMA; Shigeo MORISHIMA,抄録 本稿では; 表情筋モデルを用いて目標表情を従来より少ない本数の表情筋で合成する手法を提案する. 従来 44 本の表情筋を用いて表情合成を行っているが; 本研究では 20本以下の表情筋を用いて表情を合成した. まず 17 本の表情筋で目標表情を合成し;さらに表情が正確に合成されないものに対してはさらに 1~ 2 本追加することでより近い表情の合成を実現した. ブレンドシェイプのために用意されたキーシェイプに対して; それを表現するための表情筋収縮強度を自動推定することで; 表情筋モデルを用いてキーシェイプを記述することが可能となった. これにより; あらゆるモデルに対して同様の表情筋を配置すれば; 元のモデルの表情をリターゲッティングすることが可能である.,Proceedings of the Annual Conference of the Institute of Image Electronics Engineers of Japan Proceedings of Visual Computing/Graphics and CAD Joint Symposium 2007,2007,*
Representation of Car Body Shape using Standard Car Model and GUI Development for Car Body Shape Synthesis,Tatsunori HAYAKAWA; Yusuke SEKINE; Akinobu MAEJIMA; Shigeo MORISHIMA,抄録 本稿において; 我々は自動車の車体形状を合成する新たな手法を提案する. そして;本手法を用いることにより; 多くの経験や技術を要する自動車の車体形状のデザインをユーザが容易に行えることを可能にする. 本手法は; まず; 既存車の車体形状を基にデータベースを構築する.そして; データベースに対して主成分分析を行い既存車の特徴量とそれに伴う固有空間を得る. また;分析により得られる既存車の特徴量の固有空間上での分布を可視化することにより車体形状の合成が可能な GUI を構築する. その結果; 固有空間と特性を直感的に捉えることが可能となる.その後; GUI 上でマウスの操作を行うことにより固有空間における特徴量を変化させ新たな車体形状を合成する.,Proceedings of the Annual Conference of the Institute of Image Electronics Engineers of Japan Proceedings of Visual Computing/Graphics and CAD Joint Symposium 2007,2007,*
モーションキャプチャシステムを用いた頭髪アニメーション手法の提案,杉崎英嗣， 風間祥介， 石川貴仁， 白石允梓， 西村昌平， 森島繁生,抄録 モーションキャプチャシステムで取得したデータを用いるコンピュータグラフィックスでの人物動作表現; 顔表情表現は; データドリブン手法の一つとして非常に有効であり;そのデータを用いた応用研究発表が毎年 SIGGRAPH などの学会で盛んに行われている. また;その技術は実際の映画制作の分野でも広く使用されており; それに対応したソフトウェアの開発も行われている. 本稿は; モーションキャプチャシステムを用いて頭髪運動をシミュレーションする手法を提案する. 実際の人間の頭髪は; 個人差が存在するもののおよそ 10 万本からなる. それゆえ;その頭髪すべてにモーションキャプチャのマーカを貼り付け; その動きをキャプチャすることは現実的に不可能である. そこで本手法では; 頭髪デザインのプロフェッショナルである美容師が実際に頭髪をデザインする際に用いるレイヤモデルを基に; 計測対象となる頭髪房を各レイヤの端点に配置する.それらを主たる頭髪運動の軸としてキャプチャを行い; 配置された頭髪房から頭髪全体を補間 …,画像電子学会誌,2007,*
Motion Capturing Approach for Hair Animation,E Sugisaki; Y Kazama; T Ishikawa; M Shiraishi; S Nishimura; S Morishima,*,JOURNAL-INSTITUTE OF IMAGE ELECTRONICS ENGINEERS OF JAPAN,2007,*
Hair motion re-modeling from cartoon animation sequence,Yosuke Kazama; Eiji Sugisaki; Natsuko Tanaka; Akiko Sato; Shigeo Morishima,The creation of hair is specialized work in cel animation that is challenging to achieve in3DCG because cel animation may not be consistent with all the hair's features from allcamera positions [Noble and Tang 2004]. This paper describes a new approach for cartoonhair animation by using an existing character animation sequence. The novelty of thismethod is that a creator can easily make a 3D character model to apply environmentalinformation such as winds in an existing character animation with hair. In fact; the user canre-use existing cartoon sequences as inputs to give environmental information to anothercharacter as if both characters existed in the same scene.,ACM SIGGRAPH 2006 Research posters,2006,*
IVF with clomiphene citrate stimulation produces high pregnancy rates,J Zhang; Q Zhan; T Okimura; M Kuwayama; S Silber; C Acosta; O Kato,*,HUMAN REPRODUCTION,2006,*
表情分析入門表情分析入門; 1987,久保尋之， 柳澤博昭， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2006,*
表情筋変形パラメータの推定による表情合成,久保尋之， 柳澤博昭， 前島謙宣， 森島繁生,抄録 本稿では; 表情筋の収縮による顔形状の変形に基づき; 少ない制御点からリアルな表情合成を実現する手法を提案する. 本研究では 3 次元スキャナにより測定された顔形状をモデル化し;表情筋変形に基づく表情合成を行った. さらにモーションキャプチャを用いて表情表出時の顔表面の動作遷移を計測する. 制御点としてある瞬間の顔表面に配置されたモーションキャプチャマーカの座標を用いることで; 制御点と合成された表情をと比較し; 表情筋変形パラメータを推定し;表情を合成した. 表情筋変形パラメータから顔形状の変形は一意に定まるため; 本手法では;モーションキャプチャによって 3 次元座標が測定されない顔表面の頂点も; 表情筋による顔形状変形ルールに基づき; 表現することが可能である. 本手法により; 表情筋の変形に基づくリアルな表情を合成することが可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2006,*
インタラクティブな声質変換システムの構築 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),井上隆大， 足立吉広， 森島繁生,抄録 本稿では自然発話の声質を制御するシステムを提案する. まず目標の話者から日本語の発話に必要な一通りの音節または母音を収録し; スペクトログラムに変換して保持しておく.そして声質を変換させる人力音声をスペクトログラムに変換し; 音節または母音ごとにターゲット話者のスベタトログラムに入れ替えて音声合成することで; 声質が変換された音声を出力する.この音節交換と母音交換による合成音声は; 入力音声のイントネーションを維持している. これら 2手法による出力音声の個人性は; ほぼ同程度であることが主観評価実験によって明らかになった.よって特に母音のみの交換による声質変換法は; 比較的小さなデータベースでの声質変換を可能にすると考えられる.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2006,*
リアルな頭部動作のモデリング (メディア処理・コンテンツ生成; ヒューマンコミュニケーショングループ (HCG) シンポジウム),関根孝雄， 足立吉広， 森島繁生,抄録 本研究では; CG による人物合成における自然な頭部動作の生成を目的とし;その中でも頻繁に行われる動きの一つである頷き動作に注目して; 頷きにおける自然な首の動きを生成するシステムを提案する. 人が頷きを行う場合; 頸椎のみでなく脊椎全体の回転により姿勢が作られる. 首の動きを作る際に; インバースキネマティックスの概念により個々の頸椎の関節角度を求める方法が考えられるが; IK による頷き動作は関節間の長さが一定であるため;肩から上のモデルでは頭部動作を表現しきれない. そこで; 本研究では首の動きを関節ごとの回転と伸縮の組み合わせによって自然な頷きを生成した.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2006,*
D-14-20 手本音声を用いた声質変換システム (D-14. 音声・聴覚; 一般講演),井上隆大， 足立吉広， 森島繁生,の イ メージとの 間の 違和感や; 収録音声の 部分的な修正に手間が 掛かる等 の 問題点が存在してい る. こ れ らは; 音声合成技術を用い るこ とが 解決の 1 つ に なると考え られ る. 話者の特徴を保存するクオリテ ィ の 高い 音声合成手法として; CVC を合成単位と し て; データ ベ ース に基づ く 素片合成方式 が注 目されてい る [t;. しか し; こ れはパ ターン 数が多く; デ,電子情報通信学会総合大会講演論文集,2006,*
A-15-3 リアルな頭部動作のモデリング (A-15. ヒューマン情報処理; 一般講演),関根孝雄， 足立吉広， 森島繁生,シ ョ ンを行 う際に; 表情変化やジェ スチャ が 自然に生 じてい る. こ れ らは感情を伴 うもの で あり; 発話内容の 強調や補足の役割を担い; 会話の流れを 円滑にする働きがある. これまで CGによる顔の 表情や ジェ スチャ を表現するさまざまな研究田 がなされて きてい るが;頷きなどの動作に含まれる首の 動きにつ い てはまだ自然性が十分である とはいえない.もし自然な首の 動きが生成で きれば表情変化やジェ スチャに加えてさらに違和感の ない非言語情報の 表現が可能 となると考えられる.,電子情報通信学会総合大会講演論文集,2006,*
D-11-81 アニメ映像からの頭髪運動の構築 (D-11. 画像工学 D (画像処理・計測); 一般講演),風間祥介， 杉崎英嗣， 森島繁生,ア ニ メにおい て; 頭髪運動は風などの 自然現象; 感情; キャ ラクタらしさを表現する上で重要な役割を担っ て い る. し か し; 手書きア ニ メ に お い て; 頭髪運動表現は非常に困難な作業の 1 っ で あ り; 聴 衆を魅了 するよ うなア ニ メ,電子情報通信学会総合大会講演論文集,2006,*
D-11-80 アニメーションのための影編集ツールの開発 (D-11. 画像工学 D (画像処理・計測); 一般講演),中嶋英仁， 杉崎英嗣， 前島謙宣， 森島繁生,ーム 法で 作成し た 影 に 対 し て; 場面に 応じた編集を行う. 光源と遮 蔽物体の 位置情報を変化させずに 影の み変化させ るた めには; シ ャ ドウボ リューム 作成時の 遮蔽物体の 輪郭か らの ベク トル を編集操作方法ご と に 変更する. 例 えば; 元の 影に対 して移動の 操作 を行 い たい 場,電子情報通信学会総合大会講演論文集,2006,*
B-18-2 顔動画像の 3 次元周波数成分を用いた顔認証システムの研究 (B-18. バイオメトリクス・セキュリティ; 一般講演),山名信弘， 井辺昭人， 三浦文裕， 前島謙宣， 森島繁生,別 された グル ープに 対し て テ ス トサ ン プル の 3D 特徴量空間に おけるマ ハ ラノ ビ ス距離を測定 し; 閾値以上なら非登録者として 棄却するよ うに した. 5; 曇 男 1 擂',電子情報通信学会総合大会講演論文集,2006,*
D-11-114 車体形状の定量表現によるカーデザインツールの構築 (D-11. 画像工学 D (画像処理・計測); 一般講演),関根佑介， 前島謙宜， 杉崎英嗣， 森島繁生,近年; 多くの 分野で CG が利用 されて い る. 自動車産業に おい て も例外なく; 自動車の 開発;宣伝活動 に 利用 されて い る. そ して 自動車の デザイン におい て作業効率改善の意味で; カーデザイ ナ の 思考を反 映で き; か つ デザイ ン に不 慣れ な人で も容易なデ ザイ ン ツ ール が求め られて い る. 本手法で は; 既存の 自動車の 車体形状 を 3 次元ポ リゴ ンモ デル で 記述する. そ し て; 特定の 自動車; 又 は任意の 自動車 の グル ープ (メ ーカ; 車種; 年代等) が もつ 特徴を分析し; それ らの 特徴を有する よ うな車体形状を合成す るこ とが可能なカーデザイ ン ツ ールを提案する.,電子情報通信学会総合大会講演論文集,2006,*
ディジタル音声処理ディジタル音声処理; 1995,足立吉広， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. SP; 音声,2006,*
リファレンス音声に基づく韻律・声質・話者変換システム,足立吉広， 森島繁生,抄録 本稿では; 任意の自然発語音声に対するインタラクティブな韻律・声質・話者変換システムについて述べる. このシステムでは過去に収録された声質に対して; 別話者が発声した手本となる音声 (リファレンス音声) を与えることにより; このリファレンス音声から抽出された発話速度;基本周波数の遷移; 音量の遷移の情報に基づいて収録音声の韻律を制御する. また;声質は母音の特徴に大きく依存すると考え; 母音のスペクトルを変換させることにより声質変換を行う. 話者性は韻律と声質から成り立つと仮定し; 先のパラメータ及びスペクトログラムの情報を制御することにより; 話者変換を実現する.,電子情報通信学会技術研究報告. SP; 音声,2006,*
A Car Designing Tool by Car Feature Parameterization,Yusuke SEKINE; Akinobu MAEJIMA; Eiji SUGISAKI; Shigeo MORISHIMA,抄録 本稿では既存の自動車の 3 次元形状を解析する事で; 本来感覚的でデザイナの感性に大きく依存していた自動車のデザインに定量的な意味や重みを見出し; 自動車のデザインの持つ特徴を定量的に抽出する. そして; それらの結果から新たな車体形状を合成するカーデザインツールを構築する. まず; 自動車の 3 次元 CAD データから車体形状データベースを作成する.そしてそれらの数値情報を元に主成分分析; 判別分析により車体形状の持つ特徴を分析し;分析された結果から車体形状の特徴を抽出し; それらを用いた新たな合成手法を導入する.,Proceedings of the Annual Conference of the Institute of Image Electronics Engineers of Japan Proceedings of Visual Computing/Graphics and CAD Joint Symposium 2006,2006,*
Hair Motion Reproduction method for cartoon animation,Yosuke KAZAMA; Eiji SUGISAKI; Natsuko TANAKA; Akiko SATO; Shigeo MORISHIMA,抄録 本論文では; セルアニメで表現される風などの自然現象によって生じるアニメキャラクタの頭髪運動から運動データ取得し; その取得した頭髪運動データを基に物理シミュレーションベースでアニメらしい頭髪運動を実現する手法を提案する. 本手法により; ユーザが既存のセルアニメ映像を再利用し; 基にしたセルアニメのシーン; またはキャラクタと同一場面に存在しているかのようなアニメらしい頭髪運動を他のキャラクタに適用可能となる.,Proceedings of the Annual Conference of the Institute of Image Electronics Engineers of Japan Proceedings of Visual Computing/Graphics and CAD Joint Symposium 2006,2006,*
Special Section on Life-like Agent and its Communication,Mitsuru Ishizuka,*,IEICE Transactions on Information and Systems,2005,*
フューチャーキャストシステム三井・東芝館 (特集: 最先端映像--愛・地球博),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,2005,*
Reconstructing motion using a human structure model,Shohei Nishimura; Shoichiro Iwasawa; Eiji Sugisaki; Shigeo Morishima,The reality in human motions is often dominant for success of animation recently.Commercial mocap systems prevail to obtain human motion from actual motions. On thebasis of the data obtained by a mocap system; a skeletal structure model is needed toconstruct a CG character. As a method of creating a skeletal structure model; there is alwaysa locating rule to put markers to right place on the subject's skin; which is oftenrecommended by a mocap system vender. However; the rule cannot be applied everysubject person easily. This is the reason why it is difficult to find subject-specific jointpositions. In this research; therefore; observing inside of human body with a help ofMagnetic Resonance Imaging (MRI); our final goal is to construct an accurate and detailedbone structure model in order to make more realistic animation.,ACM SIGGRAPH 2005 Posters,2005,*
Interactive speech conversion system cloning speaker intonation automatically: Copyright restrictions prevent ACM from providing the full text for this work.,Yoshihiro Adachi; Shigeo Morishima,*,ACM SIGGRAPH 2005 Posters,2005,*
Automatic head-movement control for emotional speech,Shin-ichi Kawamoto; Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,Creation of Speaking animation for CG characters needs to synchronously control variousfactors; such as lips; facial expression; head-movement; gestures; and so on. Lip movementhelps a listener to understand speech. In addition; facial expression; head movement; andgestures help to make emotions understandable. There have been many studies thatfocused on automatic lip animation [Ezzat et al. 2002; Brand 1999]. However; in thesestudies; facial expression and gesture were realized heuristically or using directly motion-captured data. There have been no studies that tried to automatically control headmovement for emotional speech. In this paper; an automatic head-movement control methodfor emotional speech is proposed.,ACM SIGGRAPH 2005 Posters,2005,*
数字発話時の唇動作に基づく顔認証システムの構築 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),三浦文裕， 佐藤康之， 森島繁生,抄録 番号を発話する時の人の顔動画を用い; 個人識別精度の向上を目指した. まず;取得した顔動画像上の顔器官 (目; 鼻; 口など) の特徴的な位置に対し; 特徴点を定義し;その点の座標によって個人を記述した. その際; サンプルによって被験者の顔の位置・傾き・大きさが異なるため; アフィン変換によって特徴点座標の正規化をした. 正規化後特徴点座標に対し判別分析を行い; 識別結果を得た. 顔動画の初期フレームのみを識別に用いた場合と顔動画を 10フレーム用いた場合の識別率の比較を行い; 動画による認証の有効性を示した.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2005,*
B-18-2 特徴点の 3 次元情報を利用した顔認証システム (B-18. バイオメトリクス・セキュリティ; 通信 2),井辺昭人， 佐藤康之， 前島謙宣， 森島繁生,厂 rI? で 得られた判別関数は分幣 r に 適 して い ると; える u オ ープン テ ス 1・に お い て し; 本人父 岬 は 確実が 分預が 1」 わ れたが; 登録外 二 の み 一哉別 があ っ た (表 1). こ の 詰果 か'i,電子情報通信学会総合大会講演論文集,2005,*
A-15-10 感情音声と表情動画像を同時に提示した場合の印象評価 (A-15. ヒューマン情報処理; 基礎・境界),比留間庸介， 足立吉広， 森島繁生,} 青ごと} こ 判断材料とた りや すい 要事≠ 明 Lt コ f. LLt るこ とを試 みた; 平価ノ・甚 二つ い て は;従来の 評価 方法] で 冂題で k; っ プ 1 はキ 斛決す ろべ く; 新 しい 占 1 価動画像 乞 採用 した. これ 1 こよ り欣 情固有,電子情報通信学会総合大会講演論文集,2005,*
A-15-12 骨格モーションキャプチャ (A-15. ヒューマン情報処理; 基礎・境界),西村昌平， 小島潔， 岩澤昭一郎， 森島繁生,筋肉な どの 体丙部の 動 きまでは F 及 す るこ とがで きない;; つ ま り; 従 来の方法の 体表面の動きの 情報の みで は複暮な人間の 動きを忠実に冉 現す るの は困難で あ る; そ こ て. 本削窺 では沐内,電子情報通信学会総合大会講演論文集,2005,*
B-18-3 表情動画像に基づく顔認証システムの構築 (B-18. バイオメトリクス・セキュリティ; 通信 2),三浦文裕， 佐藤康之， 森島繁生,を劫つ; そ こ で 木研究 では; 静 1 卜画を和用す るの ではなく; 表情顔動画像 喚 顔からの 表情変1 匕を娵影した動画) を利用し識 別精度の向 トとデメリッ トの解消を 円的とし,電子情報通信学会総合大会講演論文集,2005,*
Special Section on Life-like Agent and its Communication-Construction of Audio-Visual Speech Corpus Using Motion-Capture System and Corpus Based Facial Ani...,Tatsuo Yotsukura; Shigeo Morishima; Satoshi Nakamura,*,IEICE Transactions on Information and Systems,2005,*
Face and gesture capturing and cloning for life-like agent,Shigeo Morishima,Face and gesture cloning is essential to make a life-like agent more believable and to give ita personality and a character of target person. To realize cloning; an accurate face captureand motion capture are inevitable to get corpus data about face expressions; speakingscenes and gestures. In this paper; our recent approach to capture the personal feature offace and gesture is presented. For the face capturing; a face location and angles areestimated from video sequence with personal 3D face model and then a synthetic facemodel data is imposed into frames to realize automatic stand-in system or multimodaltranslation system. A stand-in is a common technique for movies and TV programs in foreignlanguages. The current stand-in that only substitutes the voice channel results awkwardmatching to the mouth motion. Videophone with automatic voice translation are expected …,Robot and Human Interactive Communication; 2004. ROMAN 2004. 13th IEEE International Workshop on,2004,*
モーションキャプチャシステムを用いたマルチモーダル音声コーパスの構築,四倉達夫， 森島繁生， 中村哲,Abstract: In this paper; we describe the construction and the processing method of the multi-modal speech corpus; which contain speech data; facial movie data and position andmovements of facial organs. One female speaker uttered ATR Japanese phoneme balancedsentences. Measurement of the facial movements is done by an optical motion capturesystem. We captured high-resolution 3D data by arranging many makers on the speaker'sface. Furthermore; we propose the method of acquiring the facial movements and removinghead movements using affine transformation for computing displacements of pure facialorgans. Finally in order to represent facial animation from this motion data easily; wedescribe the technique of assigning the facial polygon model.,情報処理学会研究報告ヒューマンコンピュータインタラクション (HCI),2004,*
Mocap+ MRI=?,Shoichiro Iwasawa; Kenji Mase; Shigeo Morishima,Abstract This poster proposes a basic idea to observe differences that exist between skeletalpostures coming from two methods: postures generated by an only ordinary mocap process;and anatomically and individually accurate skeletal postures generated by a ordinary mocapprocess together with a medical imaging method such as MRI.,ACM SIGGRAPH 2004 Posters,2004,*
コミュニケーションギャップを埋める顔画像処理技術 (特集: 知的画像処理への最先端研究動向),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,光技術コンタクト,2004,*
モーションキャプチャを用いた内部骨格の動作再現モーションキャプチャを用いた内部骨格の動作再現; 19-24; 2003,田代和己， 小島潔， 岩澤昭一郎， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2004,*
音声の韻律情報の変換によるイントネーション変換システム (ヒューマンコミュニケーショングループ (HCG) シンポジウム),中野篤志， 足立吉広， 森島繁生,抄録 音声への感情付加や発話強調; 方言の付加等を目的として; 任意の自然音声若しくは合成音声に対して声質変換する手法を提案する. 従来から; 音声の韻律情報を制御し;イントネーションを制御する研究が行われてきたが; 一部の処理が波形レベルで行われていた為声質変換後の音質劣化が目立つものとなっていた. そこで波形レベルでの処理をやめ;スペクトログラムに対して韻律情報を変化させることで目立っていた音質劣化を防いだイントネーション変換システムを構築した. 本システムにより; 手本となる参照音声から発話速度;基本周波数遷移; パワー遷移の分析結果を同一内容の音声に反映させ自然音声のイントネーションを変換することが可能になった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2004,*
雑音環境下における合成発話アニメーションの評価,前島謙宣， 四倉達夫， 森島繁生， 中村哲,抄録 筆者らは; すでに自然な発話アニメーションの合成手法を提案してきた. しかし;その評価は主観評価実験によるところが大きかった. 本稿では; 発話アニメーションの客観的評価尺度を含む新しい評価手法について提案する. この評価手法では; 発話アニメーションの性能は以下の 3 つの要素によって評価される. 読唇が可能か. 視覚的に自然か. 音声と正確に同期しているか.読唇の可能性は; まず雑音環境下において顔アニメーションと音声とを被験者に提示し;発話単語がどの程度正しく聞き取ることができたかという実験により判断する. 次に;発話アニメーションの視覚的な自然さと発話口形変化の滑らかさを MOS5 段階評価する.音声との自然な同期に関しては; 一定間隔で音声と発話アニメーションとの同期をずらしたものを被験者に提示し; 主観的な同期のずれを調査するとともに; 違和感の程度を 5段階評価によって評価する. 加えて; 音声と発話アニメーションとの同期のずれが音声の知覚に …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2004,*
MRI とモーションキャプチャシステムを用いた精度の良い骨格の動き推定 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),田代和己， 小島潔， 岩澤昭一郎， 森島繁生,抄録 コンピュータグラフィックスの分野では; 人物描写をより写実的に実現するため;実際の人物をモーションキャプチャしキャラクタ上にその動きを投影する手法がしばしば取られる.本研究では; その動作情報を利用して人物の動きをより実際に忠実に; また正確に再現するために;骨格の動作を再現する手法について述べる. 今回は特に腕部分の動作に着目し; MRIで取得した体内のデータから骨格部分を抽出して骨格モデルを生成する. 光学式モーションキャプチャシステムより得られる各部位ごとの皮膚動作情報からこの骨格モデルを制御する.この際; 皮膚と骨格の間に生ずるすべりも考慮して; 高い精度での骨格の制御を実現した.これにより; 通常のモーションキャプチャデータから再現可能なスケルトンモデルよりも;より実際に忠実で; リアルな人物運動表現が可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2004,*
ポリゴン細分割とテクスチャブレンディングによるリアルな表情合成,岩上圭吾， 大橋俊介， 森島繁生,抄録 近年; 仮想空間での擬人化エージェントによるコミュニケーションを実現するために; CGによる表情合成の研究が注目を集めている. 従来からテクスチャマッピングによる表情合成の研究が進められてきたが; 皺がきれいに合成されないという問題点があった. そこで本稿では;皺の表現のためにポリゴンを細かくしたワイヤフレームを作成し; 画像へのフィッティングの際に皺をきちんと位置あわせすることで; 皺の合成がきれいに実現できる手法を提案する. これにより;複数の基本顔テクスチャのテクスチャブレンディングによって; 任意表情を表現でき;皺が薄くなるという従来の問題点を解決することができた. さらに; ポリゴンの細分割を実施して;ワイヤフレームの頂点数を増やし; レンジデータを反映させることで; より原画像に忠実な頭部モデルを作成することが可能となった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2004,*
頭髪運動のリアルタイムアニメーションツールの開発 (ヒューマンコミュニケーショングループ (HCG) シンポジウム),杉森大輔， 森島繁生,抄録 本研究は頭髪の運動アニメーションをリアルタイム合成するためのいくつかの手法を提案する.この手法とは頭髪をモデリングしているセグメントの再分割; およびポクセルによる風および衝突のモデル化である. これらの手法の導入によってスーパーリアルなアニメーションを合成することはできないが; 処理時間の大幅な削減を図ることができる. これによって本研究では一般的な PCで最大 10 [frame/sec] のフレームレートによるアニメーション生成を可能とした.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2004,*
かたちと印象かたちと印象; 2003,加藤絢子， 古川利博， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. CAS; 回路とシステム,2004,*
コンピュータディスプレイによる形状処理工学コンピュータディスプレイによる形状処理工学 II; 1984,加藤絢子， 古川利博， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. CAS; 回路とシステム,2004,*
コンピュータディスプレイによる形状処理工学コンピュータディスプレイによる形状処理工学 II; 1988,林長軍， 尾崎弘明， 下川哲司， 加藤絢子， 古川利博， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. CAS; 回路とシステム,2004,*
房単位で編集可能なヘアスタイルデザインシステム,加藤絢子， 古川利博， 森島繁生,抄録 人物のイメージを決める要素として; 頭髪は重要な役割を果たす. そのため; 髪の長さ;色などを変化させ; ヘアスタイルでの自己表現をする人が多くなっており; CGによる人物表現においても頭髪に対する多くの研究がなされている. 本研究では; CGを用いたヘアスタイルデザインシステムについて述べる. 頭髪の表現に B-Spline 曲線;頭部の表現に NURBS 曲面を用いたヘアスタイルデザインシステムがある. このシステムは;領域の数が少なく; 頭髪の編集が領域ごとにしかできないため表現できるヘアスタイルが限られてしまうという問題点がある. 本研究ではこのシステムに機能を追加し; より多くの種類のヘアスタイルを表現できるようにした. 頭髪の細かい編集を行うためにパラメータ変換を用いて頭部の領域分割を行い; B-Spline 曲線を分割することによってカット機能を追加した.,電子情報通信学会技術研究報告. CS; 通信方式,2004,*
D-8-11 擬人化音声対話システム構築のための顔モデル生成ツールの開発 (D-8. 人工知能と知識処理),林壮， 祖川慎治， 四倉達夫， 森島繁生,タ との 対話では言語情報 が大部分 を 占め てい る;; 本稿で は 2000 年 か ら 2002 年に情報処理技術振興協会 (IPA) の 支援を 受け て 様々 な大学や研究所が共 同で作成した擬人化工,電子情報通信学会総合大会講演論文集,2004,*
D-12-98 テクスチャと細分割を利用した皺のある表情合成の構築 (D-12. パターン認識・メディア理解 B),岩上圭吾， 大橋俊介， 森島繁生,ン を取 り扱う研究が 注目 を集め て い る. 人間がコ ミ ュ ニ ケーシ ョン を取る うえ で表情が重要な要素となる こ とより; CG に よる コ ミ ュ ニ ケーシ ョ ン を実現させ る ためにも表情は重要な要素で ある.,電子情報通信学会総合大会講演論文集,2004,*
D-11-141 制御点の増減による頭髪アニメーション合成のリアルタイム処理 (D-11. 画像工学 D),土田洋平， 杉森大輔， 森島繁生,CG に よる頭髪の 運動表現は そ の 本数の 多さと細か さ か ら; 従来の 描画方法で は膨大な量の計算を必要 として しまい; リア ル タイ ム で 頭髪の 運動表現を行 うの は困難で あっ た. 本研究 では; リア リテ ィ を大きく落 とさずに 頭髪運動 に かか る 計 算量 を減 らす 事 で; 頭髪の 運 動表現の リア ル タ イ ム 処理 を 行 う.,電子情報通信学会総合大会講演論文集,2004,*
D-12-96 3 次元レンジセンサを利用した表情合成における汎用性の実現 (D-12. パターン認識・メディア理解 B),山口智之， 大橋俊介， 祖川慎治， 森島繁生,1. は じめに映画 や CM な ど; 様 々 な分野 で CG を利用 したア バ タを見かけ る 事が 多くな っ てきた. ア バ ダを 使用 す る 際に 顔 の 表情が重要 とな る. 顔 の 表情 を合成す るには膨大 な時間と; 熟練 した 技術が 必 要 で あ る. そ こ で 本稿 で は; 表情変形ル ール を見直す こ とに より表情の 精度 を向上 させ; 影響カマ ッ プ に 反映させる事で 汎用性 が あり容易 に生成 可能 な表情合成を 目指 した.,電子情報通信学会総合大会講演論文集,2004,*
D-11-142 基本動作の合成による多様な人物動作の作成 (D-11. 画像工学 D),吉田憲彦， 仲野陽介， 小島潔， 森島繁生,本研究では腕の 動きに 注 目し; 様々 な腕の 動きを撮影 し; データ を 分析する 事で 人 物の動作の 複雑さを保 っ た 上で; 少ない 次 元の 変化で よ リ リア ル か つ 容易 に 再現で きる シ ステ ムを提案 し; 最終的に は 無形 文化財の ア ーカ イ ビン グ へ 汎用 で,電子情報通信学会総合大会講演論文集,2004,*
D-11-143 前腕部の骨格動作推定 (D-11. 画像工学 D),田代和己， 小島潔， 岩澤昭一郎， 森島繁生,を見 張 る もの が ある. しか しな が ら CG と して 人 物 を 再 現する場合; 顔の ように特徴的な箇所に比べ; 体の ような特徴の 少ない 部分は そ れ ほ ど注力 され て い ない の が実際で ある. そ こ で 本稿で は; 体部の 表現能力を発展 させ る,電子情報通信学会総合大会講演論文集,2004,*
D-12-52 奥行き情報を用いた個人認証における顔の最適な角度の検討 (D-12. パターン認識・メディア理解 A),高橋悠， 森島繁生,近年; 個人認証シ ス テ ム が 実用化されつ つ ある. こ れ らのシ ス テム も含め; 現在開発 され てい る顔を用い た 個人認 証シ ス テ ム は 正 面 か ら撮影され た顔画 像を用 い て 人物の 特定を行うもの が 主流で ある. し か し 設備 を設置 する環境 によっ て は 正 面画像を得る こ とが で きない場合が あ る. 本 稿で は 3 次 元 顔 モ デル を 用 い て さま ざま な 方向 か ら撮影 された顔画像 におい て どの 角度の もの が 最 も個人の 違 いが 表れやす く; 認証 しやすい か を検討 する.,電子情報通信学会総合大会講演論文集,2004,*
D-14-5 発話速度; ピッチ; パワー制御による自然音声のイントネーション変換 (D-14. 音声・聴覚),中野篤志， 足立吉広， 前島謙宣， 森島繁生,人間が発 声 した音声は 伝えたい 内容で ある言語的情報以外に も年齢; 性別; 発話者が 誰であるか と い う個人情報が 含まれて い る. さらに 情緒や気分を表す感情的情報 も含ん で いると考え ら れ る.,電子情報通信学会総合大会講演論文集,2004,*
レンジサンサを用いた表情の計測および変形ルールの記述,大橋俊介， 山口智之， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,映像情報メディア学会技術報告,2003,*
レンジセンサを用いた表情の計測および変形ルールの記述 (顔とコミュニケーション),大橋俊介， 山口智之， 森島繁生,抄録 近年; アバタを介したコミュニケーションシステムやヒューマンインタフェースにおいて; CGによる表情合成の研究が注目を集めている. コンピュータと人間とのコミュニケーションを円滑に行うには; 人間同士がフェーストゥーフェースで対話しているような環境が重要であり; 表情を含めたノンバーバルな情報の表現が課題となる. 本稿では; レンジセンサを利用して; 高精度に顔の 3次元形状を計測すると同時に; 無表情と基本表情変化後の対応関係から; 表情変形ルールを抽出する方法について述べる. 基本的には; 標準顔モデルの整合による特徴点の対応付けに基づくが;表情移動量分布図の概念を導入し; 任意の顔形状モデルに対して; 表情変形ルールが適用できることを示す.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2003,*
Galatea: 音声対話擬人化エージェント開発キット,新田恒雄， 西本卓也， 川本真一， 下平博， 森島繁生， 四倉達夫， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 峯松信明， 山田篤， 伝康晴， 宇津呂武仁， 伊藤克亘， 甲斐充彦， 李晃伸， 中村哲， 嵯峨山茂樹,2. Galatea の構成と特長図 1 に開発キットの全体構成を示した. 各モジュールは独立のプロセスとして設計されており; 対話システムは単一 PC (Note PC (Mobile Pentium III 1.2 GHz; 512MB)上で動作確認済); あるいは分散環境 (複数 PC による並行動作) で使用することができる.以下に各モジュールを概説する.(1) 音声認識: Julian [2] をベースに音声対話システムで要求される;(a) 文法に基づく音声認識;(b) 発話中の逐次的な認識結果出力;(c) 認識処理の動的制御 (中断;文法の切替等) の諸機能を提供している.(2) 音声合成: 日本語テキスト音声合成に必要な基本機能(形態素解析 (茶筌 [3]); 読み・アクセント付与; 韻律生成; 合成波形生成) のほか;(a)音素継続時間長を出力し顔画像の口唇との同期が可能;(b) テキスト埋め込みタグ (JEIDA規格準拠) による韻律制御が可能;(c) 合成音を出力途中で中断可能 (barge in 等)といった特長を持つ. 合成器は HMM に基づく方式 [4] を採用し; 男女各 1 名の話者モデルを提供 …,第 8 回日本顔学会大会 (フォーラム顔学 2003),2003,*
< 研究装置設備紹介> 3 次元動作解析システム (平成 14 年度購入),森島繁生， 柴田昌明,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,成蹊大学工学研究報告,2003,*
顔の分析・合成とその応用,森島繁生,概要 近年バーチャルヒューマンのアプリケーションとして; エンタテインメントシステムやコミュニケーションシステムが注目されている. ここではいかに人物を忠実にコピーして合成するかが課題であるが; ビジョンベースのシステムは性能に限界があり; クオリティの高い合成画像を再現することは難しい. モーションキャプチャシステムは; 映画やゲームの世界で一般的に利用されるが;オンラインで合成することは難しく; 後処理で多くのマニュアル操作が必要とされるのが現状である.本稿では; 特に顔のアクションを高い忠実度と自然なクオリティでコピーすることを目的とし;顔画像分析・合成技術を駆使して; 準リアルタイムでバーチャルヒューマンを制御する手法について述べる. さらにエンタテインメントおよびコミュニケーションを目的とするアプリケーションシステムを紹介する.,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),2003,*
擬人化インタフェース,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,ヒューマンインタフェース学会誌= Journal of Human Interface Society: human interface,2003,*
モデル・テクスチャ・皺のブレンディングによる顔表情の表現,高橋光紀， 森島繁生,1. 概要顔表情の表現法として; 一般に FACS (Facial Action Coding System) が用いられるが;本稿では皺などの詳細な表現を; 効果的に行う方法を提案する. 皺などの情報を持った顔モデル群(基本顔) を用意し; それらのブレンディングによって任意の顔表情を構築する. さらに;テクスチャについてもブレンディングすることで; 顔表面の詳細な特徴の表現力を飛躍的に向上させた. また応用として; VISEME の口形モデルを構築し; それらのブレンディングによって;発話口形の表現を行うことも試みている.,第 65 回全国大会講演論文集,2003,*
場を用いて擬似的に表現した風の力による頭髪の運動アニメーション,浅井崇， 杉森大輔， 杉崎英嗣， 森島繁生,1 概要 CG アニメーションによる人物表現において頭髪は個性を表現する要因の 1つとして重要な役割を果たす. 頭髪を合成する試みはこれまでに多く行われているが;運動表現の点で優れた決定的な手法は存在しない. これは人の頭髪は数十万本あり;形状は細く糸状で変形しやすいためである. そこで本研究では; 頭髪に作用する様々な力の定義が自然で印象的な運動の表現に重要な要素を成すという観点から; 2 つの新たな力を提案する.一つは風の表現として吸い込みと湧き出しが作る場を擬似力として導入した. また;空気の粘性を付加することにより髪質の表現が自然となった. もう一つは頭髪の運動表現として;頭髪同士に引力を作用させた点である. 一本の頭髪を任意の制御点により制御し;各頭髪間の制御点同士に引力を持たせることで頭髪同士のまとまり感を表現した.,第 65 回全国大会講演論文集,2003,*
影響力マップを用いた顔画像合成の提案,祖川慎治， 四倉達夫， 森島繁生,本稿では; 任意形状かつ格子点の異なるモデルを容易かつ短時間で表情の生成を可能にする手法を紹介する. 従来の表情合成技術 [1] はモデルの依存性が強く; 基本的には詳細な表情変形ルールが定義された特定のワイヤフレームモデルを用いて表情合成する必要があったが;レンジセンサから無表情から表情変形までの顔の移動量の 3 次元実測データをマップ化した影響力マップを用いて; 特定のモデルと影響力マップを整合させ; この結果から自動的に特定モデルの表情変形ルールの算出を行うことにより; モデル依存性を解消することが可能となった.本手法は次に述べる 3 つのプロセスが存在する.,第 65 回全国大会講演論文集,2003,*
擬人化音声対話エージェントのための顔画像合成モジュールの開発,四倉達夫， 森島繁生,人間と人間との対面対話は; 言語情報のみならず手振りや首の動作等のジェスチャや顔表情等の非言語情報を含めた情報を用いてコミュニケーションを行う. 擬人化エージェントを構築する際にもこれら情報は円滑なコミュニケーション時には欠かせないものとなっている. また;エージェント自身も実際の顔と遜色ないリアルな顔モデルを構築する必要があり;表情を付加させるためのルール付けもまた重要な要素となる. 著者らの研究の最終目標は現実世界の臨場感を機械と人間とのインタフェースにおいても実現し; 合成された顔によるノンバーバルなコミュニケーション環境の構築である. このような目標に向けてのファーストステップとしてエージェントの顔モデルをある人物の顔のみならず表情や印象も含めコピーし; 再現することである.そこで本稿ではこの目的を解決するためのエージェント構築方法を述べ; 顔モデルの構築手法や表情・唇動作のルール化; アニメーション手法を述べていく.,第 65 回全国大会講演論文集,2003,*
グラフマッチングを利用した顔特徴部位の位置推定と追跡,大室学， 森島繁生,本研究では; 初期設定部; 顔特徴部位追跡部において顔の特徴部位を推定 顔の特徴部位を推定・追跡を行なった. まず; 初期設定部では; 顔重心領域の指定; 顔重心領域の指定;ノード位置の指定を手 ノード位置の指定を手動で行い; 顔重心とノード位置を取得した.顔重心とノード位置を取得した. また; これにより; 参照グラフを作成した. 参照グラフを作成した.顔特徴部位追跡部においては; 肌色である画素を検出; 肌色である画素を検出;顔重心領域を推定後; 色ヒストグラムインターセクションを用いて顔 色ヒストグラムインターセクションを用いて顔重心を求め; この顔重心から各ノードの検索領域の推 この顔重心から各ノードの検索領域の推定; 色ヒストグラムインターセクションを用いたノー 色ヒストグラムインターセクションを用いたノード候補位置の推定し; ド候補位置の推定し; 候補グラフを作成した. 候補グラフを作成した.この候補グラフと初期設定部にて作成した参照グラフをマッチングすることで顔特徴部位を推定 …,第 65 回全国大会講演論文集,2003,*
モーションキャプチャシステムによる複雑な人物動作の表現,仲野陽介， 四倉達夫， 杉崎英嗣， 森島繁生,現在; コンピュータグラフィックスによる人物の画像合 コンピュータグラフィックスによる人物の画像合成は様々な分野で行われており; 成は様々な分野で行われており; その中の一つに無形文化その中の一つに無形文化財の記録･ 保存･ 伝承を目的としたディジタルアーカイビン伝承を目的としたディジタルアーカイビングの分野があげられる. グの分野があげられる.本研究では古典バレエのディジタ 本研究では古典バレエのディジタルアーカイビングを目標としており; ルアーカイビングを目標としており; 近年多くの研究者達 近年多くの研究者達がこの分野で様々なツールやシステムを構築している がこの分野で様々なツールやシステムを構築している [1].アーカイビングにとって; アーカイビングにとって; それによる伝承; またそこからの発展を考えた場合に必要となることは; の発展を考えた場合に必要となることは; 動作を正確に記録･ 保存し;その動作を CG によって必要な時にリアルな映像として再現できることである. しかし; 実際に人物 …,第 65 回全国大会講演論文集,2003,*
スナップ写真中の人物の同定; 表情認識; 表情変換,八島隆， 高橋悠， 森島繁生,本稿では; ディジタルカメラによって撮影された静止画像内の人物の顔に対して; 3次元情報を持ったテンプレートを用いて; テンプレートマッチングをすることにより照明条件に依存しない個人認証; 表情認識を行う. また; テンプレートマッチングをする際に; 人物の顔の向きが極端に横を向いているもの (真横を向いているもの) でも推定できるようにしている. その結果得られた人物の顔の位置や向きを用いて; テンプレートを入れ替えることで; いろいろな表情に変換することができる. この時のテンプレートの表情は; 表情をブレンドすることによりいろいろな表情に対応できるようにしている.,第 65 回全国大会講演論文集,2003,*
テクスチャブレンディングによる皺の表現と口形アニメーション,石山慎一郎， 高橋光紀， 大橋俊介， 森島繁生,1. はじめに現在; 映画など多くの分野で CG が利用されている. その技術は驚くべき速さで進歩している. その中でも特に人間の表情をリアルに再現することは非常に困難である.本稿では人間の顔面をより精密に再現し; さらにコミュニケーションツールへの応用を目指し;人間の発話アニメーションを作成する. 従来はワイヤフレームで構築された 3 次元顔面モデルを基本顔と呼ばれる表情ごとに用意し; その各頂点の座標; 貼り付けるテクスチャの画素値をブレンディングすることによって; 様々な感情状態; 表情をを再現することが行われてきた.[1],第 65 回全国大会講演論文集,2003,*
モーションキャプチャを用いた内部骨格の動作再現 (ヒューマン情報処理及び一般)(ヒューマンコミュニケーショングループ (HCG) 大会),小島潔， 杉崎英嗣， 四倉達夫， 森島繁生,抄録 モーションキャプチャシステムは; 動作計測およびアニメーション再現を目的として;映画やゲームソフトの製作プロセスで最近よく用いられているが; その計測精度には限界があり;実際の動作再現に際して手作業による膨大なポスト処理が必要とされる. 本研究では;このモーションキャプチャシステムを利用して; 動作計測を精度よく行う手法を提案する.キャプチャのための独白のマーカ配置を行い; このマーカデータから人体骨格を忠実に表現したスケルトンモデルを利用している. また; 骨格は体の内部に存在し; マーカは体表面の位置特定を行っているため; マーカに対する骨格の正確な位置を特定するために MRI を利用した.この提案システムを; 特徴的な動作が多くモデリングが困難とされる古典バレエのダンスのキャプチャリングに適用し; その精度よい骨格の動き再現とバレエ動作の再現を行った.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,2003,*
3 次元顔テンプレートを用いた静止画中の顔認識と表情変換,高橋悠， 八島隆， 森島繁生,抄録 デジタルカメラが飛躍的に普及している昨今; さまざまな人物が写ったスナップ写真を大量にパソコンに保管することが多くなってきた. この大量の写真画像データベースの中から;ある特定の人物が写っている写真のみを探し出したり; 笑顔の写真を選択する作業を;手作業によるラベリング処理なして行う必要性がある. そこで本稿では; ターゲットとなる人物の 3次元顔モデルを生成し; この顔モデルに基づいて 3 次元テンプレートを構成することで;静止画像であるスナップ写真の中から対象人物の写っているものを自動選択し; さらにこの 3Dモデルを対象人物像に自動的に重ね合わせる手法を提案する. また表情変形規則を 3次元顔モデルに定義することで表情変化を含む探索を実現し; ある人物が特定の表情をしている写真を選択することを実現した. また 3 次元テンプレートマッチングにより; 顔の位置・向きが同時に推定可能なため; この顔部分をモデル変形によって別の表情に変換し; 違和感のない画像生成を …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2003,*
音声のパラメータ変換によるイントネーション変換システムの構築 (ヒューマンコミュニケーション及び一般)(ヒューマンコミュニケーショングループ (HCG) 大会),足立吉広， 前島謙宣， 四倉達夫， 森島繁生,抄録 音声への感情付加や発話強調; 方言の付加等を目的として; 任意の自然音声もしくは合成音声に対して声質を変換する手法を提案する. 従来から; 音声の韻律情報を制御し;イントネーションを制御する研究が行われてきたが; 波形レベルでの変形を行っていることから;再現された音声の自然性の劣化が著しかった. そこで本研究では; 声質変換した音声の自然性の劣化を抑えるために STRAIGHT の考え方を導入し; セグメンテーションした音節区間毎に; 継続長;ピッチ; パワーを制御する方法を新たに付加することで; 発話速度とイントネーションを変換するシステムを構築した. これにより喋り方の手本となる参照音声の分析結果から; 発話速度;ピッチ推移; パワー推移をセグメントごとに自動抽出して; サンプル音声にこの韻律情報をそのままコピーし; 声質変換することが可能となった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2003,*
A-14-1 幾何構造とテクスチャの平均化による人物に依存しない表情変形モデルの構築,大橋俊介， 井上真実， 森島繁生,国立情報学研究所による学協会向け論文電子化・公開サービス (NII-ELS) の終了にともない;利用者のみなさまにご不便をおかけしておりますことをお詫び申し上げます.本件について;学協会ならびに契約機関等に向けては周知に努めて参りましたが; 利用者のみなさまに対する事前のご案内が行き届かず; ご心配をおかけしていることについても重ねてお詫び申し上げます.,電子情報通信学会総合大会講演論文集,2003,*
A-14-3 複数人話者会話シーンにおける動画像翻訳システムの構築,前島謙宣， 森島繁生， 中村哲,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2003,*
擬人化エージェントに必要な顔のリアリティとそのモデル化,森島繁生,た. 特に被の表出の仕方などは個人によって大きく異なるものである. そこでここでは;特に競のような詳細な特徴を失うことなく; 顔表情を作成する手法を提案する.様々なバリエーションの簡単な表情変形をした顔モデルを用意し; それらをブレンデイングすることによって; 顔表情を作成する方法である. ここで用意する顔モデル群は基本顔モデルと呼ばれる 2l.基本顔モデルには 3 次元の幾何構造変形情報だけでなく; 被などの詳細な特徴も含まれており;また AU のような再現性の難しい表情ではなく; 比較的簡単に誰でも表出できる表情を選択している.これを図 2 に示した. ここでは; 基本顔モデルとして; 11 種類の顔を定義している.これらの基本顔は; AU を参考にしながら; 被の表出しやすさを考慮して選ばれたものである.基本顔ひとつひとつについて; 前節で説明した方法で顔モデルを作成し; 無表情の顔モデルとブレンディングをすることで表情変形を表現する. ブレンデイングとは; 各基本顔に重みを 0.0~ 1.0 …,情報処理学会研究報告音声言語情報処理 (SLP),2003,*
複数人話者会話シーンの動画像翻訳,前島謙宣， 森島繁生， 中村哲,抄録 本論文では; 複数人の話者の会話シーンにおける画像翻訳の手法について述べる.会話シーンにおいて; ビデオ映像中の人物の顔の動きを推定し; 映像中に存在する各話者について発話判定を行う. 発話が検出された話者の口領域を; 別に用意された音声に同期して合成された口唇映像で置き換えることにより; 他言語もしくは変換された発話内容へのリップシンクを実現する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2003,*
Model-based Talking Face Synthesis for Anthropomorphic Spoken Dialog Agent System Tatsuo Yotsukura Shigeo Morishima Satoshi Nakamura ATR Spoken Lang...,Tatsuo Yotsukura Shigeo Morishima Satoshi Nakamura,*,Proceedings; ACM multimedia...,2003,*
Life-Like Characters. Tools; Affective Functions; and Applications,Hiroshi Shimodaira; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Keikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro; Shigeki Sagayama,Galatea is a software toolkit to develop a human-like spoken dialog agent. In order to easilyintegrate the modules of different characteristics including speech recognizer; speechsynthesizer; facial animation synthesizer [facial-image synthesizer] and dialog controller;each module is modeled as a virtual machine having a simple common interface andconnected to each other through a broker (communication manager). Galatea employsmodel-based speech and facial animation [facial-image] synthesizers whose modelparameters are adapted easily to those for an existing person if his/her training data is given.The software toolkit that runs on both UNIX/Linux and Windows operating systems will bepublicly available in the middle of 2003 [1; 2].,*,2003,*
擬人化音声対話システムにおけるエージェント画像生成,四倉達夫， 森島繁生,抄録 機械と人間とのコミュニケーション形態の 1 つとして擬人化エージェントが挙げられる.このエージェントがコンピュータディスプレイ上に表示し; 言語情報やジェスチャ;表情等の非言語情報を理解・表出しあたかも人間同士が対面対話するようなリアルなコミュニケーション環境を構築可能なシステムが求められる. エージェントを構築するにあたり;最終的な目標として; いかにエージェント自体をリアルなものとし; コミュニケーションの際;現実世界との対話と遜色なくすることである. 本稿ではこのシステムのエージェントの構築技術を紹介し; エージェントの顔モデル構築; 表情合成; アニメーション手法について紹介する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,2002,*
TAO-5 空間共有コミュニケーションプロジェクト (大型プロジェクト紹介; 学術系企画),齊藤隆弘， 森島繁生， 相澤清晴， 望月研二,自然の 周 囲環境を 3 次元画像空 間 と し て 人 の 視覚特性 を考慮 し分析 す ると; 視点 か らの距離に応 じて; 遠景; 中景; 近景の 3 層 に 構造化 を行 うこ とがで きる. 特に 中景を中心にセ ッ ティ ン グ表現 と呼ばれ る舞台の セ ッ トで 用い られ る書 き割 り的 な表現を用い る こ とで; オ ブジェク トを複数の平面で 近似 し; 効率的な 3 次元 モ デル の 表現 を狙 うこ とがで きる.,情報科学技術フォーラム学術系・企業系予稿集,2002,*
空間共有コミュニケーションプロジェクト,齊藤隆弘， 森島繁生， 相澤清晴， 望月研二,通信・放送メディアなどの映像メディアでは; 画 像情報を 2 次 元から; よ り臨場感のある 3 次元情報 とする要求が高 くなってきている. 2 次 元映像 に比べ 3 次 元映像は伝送される情報量が極端 に増大するため立体映像 を効率的に伝送する画像通信; 高 速に処理する画像処理装置; また; それに適 した立体表示装置が必要である. 原 島らは生の立体画像情報ではなく空間情報を伝送することにより; 効 率的で柔軟性の高い統合的な 3 次 元画像通信;「3 次 元統合画像通信」 の構想;ま た; 将 来的に画像通信のためのインフラが充分整備 された時代 を見越 して 3 次元画像通信を発展させた 「空間共有メディア; 空 間共有 コミュニケーション」 の概念を提案 した 1).一 方; CG 技 術 を用い 3 次 元画像空間を構築 して通信で利用するような研究開発も行われているが; 利 用される 3 次 元画像空間の リア リティや自然さはいまだ十分な状況にはない. 本 郷空間共有 リサーチセンタでは; 人 類の夢の画像 コミュニケーションである 「空間共有 …,計測と制御,2002,*
擬人化音声対話エージェント開発プロジェクト,嵯峨山茂樹， 伊藤克亘， 宇津呂武仁， 甲斐充彦， 小林隆夫， 下平博， 伝康晴， 徳田恵一， 中村哲， 西本卓也， 新田恒雄， 広瀬啓吉， 森島繁生， 峯松信明， 山下洋一， 山田篤， 李晃伸,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,日本音響学会研究発表会講演論文集,2002,*
A-15-3 空間周波数成分と十字テンプレートを用いたリアルタイム顔器官形状認識と表情合成,中村一雄， 大室学， 島田昌実， 森島繁生,い て 形状認識 を行 っ た. 形状認 識の 手法は 目; 口 そ れ ぞれの 特徴に合わせ別々 の 方 法で行 っ た. 目領域で は 十字テ ンプ レ ートを利 用 し特徴点 を捕 らえ; 口 領域 で は周 波 数領域上で の 変化を分析す るこ とに よ り顔器官形状認識 を行っ た; a;. 11. Ill* iELIgel ww,電子情報通信学会総合大会講演論文集,2002,*
A-15-5 レンジファインダと複数アングル画像を用いた 3 次元顔モデルの生成とその表情合成,伊藤圭， 森島繁生,顔モ デル の 作成の 手 法 と して; あらか じ め用意 された標準的なモ デル を複数ア ン グル からの 画像か ら整合す るツ ール が提案され てい る: 2]. しか し; 実際の 本人 に忠実な顔モ デルを生成 させ るために は多大な時間 を要する. そ こ で本稿 では; 新たに NEC 製の レ ン ジ フ ァ イン ダを導入す るこ とに より; 短時間で 三次元顔モ デル の 生 成を行い; ま た複数ア ン グル の画像を貼 り重ねる こ とに よ り任意の ア ン グル か らで も違和感 のない モ デル 生成 を行 う.また; 同時 に表情合成を 日指す.,電子情報通信学会総合大会講演論文集,2002,*
D-11-173 自然な頭髪の運動と髪型を保存する復元力の表現,杉森大輔， 杉崎英嗣， 佐野和成， 森島繁生,*,電子情報通信学会総合大会講演論文集,2002,*
D-11-169 人物アニメーションに同期した頭髪の運動表現,佐野和成， 森島繁生,*,電子情報通信学会総合大会講演論文集,2002,*
A-14-14 奥行き情報を利用した正面顔画像への標準顔モデルの自動整合,井上洋信， 大室学， 伊藤圭， 森島繁生,顔器 官領域の 推定 に は 従来か ら研究され て い る 特徴点検出手法: 1] を導入す る. た だ し検出する 顔器官領域に よっ て は 本研 究に最適にな るよ うに変更 した処理 を行 う. 鼻領 域: レ ンジ フ ァ イ ン ダで 撮影 した正 面顔 画像か ら奥行 き情報 で ある Z 座標 を取 得 して; Z 座 標の最大値が 鼻の 頭で あ る と仮定 し; それ を鼻領 域候補点と し て 領域 を推定す る方法を用い る.,電子情報通信学会総合大会講演論文集,2002,*
A-15-3 空間周波数成分と十字テンプレートを用いたリアルタイム顔器官形状認識と表情合成,中村一雄， 大室学， 島田昌実， 森島繁生,い て 形状認識 を行 っ た. 形状認 識の 手法は 目; 口 そ れ ぞれの 特徴に合わせ別々 の 方 法で行 っ た. 目領域で は 十字テ ンプ レ ートを利 用 し特徴点 を捕 らえ; 口 領域 で は周 波 数領域上で の 変化を分析す るこ とに よ り顔器官形状認識 を行っ た; a;. 11. Ill* iELIgel ww,電子情報通信学会総合大会講演論文集,2002,*
A-14-13 レンジファインダを用いた表情編集ツールの構築,大橋俊介， 杉崎英嗣， 伊藤圭， 森島繁生,Codeng System) に よっ て 解剖学的に 分類 された 44 種類の 顔 の運動 単位 AU (Action Unit)の パ ラメータを作成を し て きたが; そ の 移動 量が 曖昧 な うえ; 頭 部形状 生成が 困難 で あ った. そこ で 本稿で は; 正 面 か らの 撮影で 3 次元座標 が取得 可能 な レン ジフ ァ イ ン ダを用 い実物 に 忠実な 3 次元モ デル を生成 し; 表 情変化を リア ル に記 述す る表情編集 ツ ール を実現し た. 2. シ ス テ ム 要,電子情報通信学会総合大会講演論文集,2002,*
D-12-100 ズーム変化を含む動画中の顔自動トラッキング,長田誉弘， 大室学， 緒方信， 森島繁生,*,電子情報通信学会総合大会講演論文集,2002,*
A-15-4 テクスチャブレンディングによる皺の表現と基本顔モデルによる感情空間の構築,柳澤尋輝， 高橋光紀， 森島繁生,うシ ス テ ム を構築 して きた. 本稿 にお い て も感情空 間を用 い た表情合成 シス テ ム の 構築を行 う; こ こ で は AU では ない 簡易的な基本顔 を複数用意し; それ らを混合率に よっ て合成するこ とで 表情 を表現; また; テ クス チ ャ に お い て も皺 を中心 と し たブ レ ン デ ィ ン グを行 うことで; 表情の 表 現力を高 め る. また基本 6 感 情に加 え; 中間感情表情 を学 習 させ; 感情 空 間 の構 築 を試 み る.,電子情報通信学会総合大会講演論文集,2002,*
D-12-112 スナップ写真データベース中の人物検索,高橋悠， 伊藤圭， 緒方信， 森島繁生,*,電子情報通信学会総合大会講演論文集,2002,*
D-12-14 パンチルト制御可能な複数のカメラの連携による顔領域追跡,島田昌実， 森島繁生,*,電子情報通信学会総合大会講演論文集,2002,*
Styling and animating human hair,Keisuke Kishi; Shigeo Morishima,Abstract Synthesizing facial images by computer graphics (CG) has attracted attention inconnection with the current trends toward synthesizing virtual faces and realizingcommunication systems in cyberspace. In this paper; a method for representing human hair;which is known to be difficult to synthesize in computer graphics; is presented. In spite of thefact that hair is visually important in human facial imaging; it has frequently been replaced bysimple curved surfaces or a part of the background. Although the methods of representinghair by mapping techniques have achieved results; such methods are inappropriate inrepresenting motions of hair. Thus; spatial curves are used to represent hair; without usingtextures or polygons. In addition; hair style design is simplified by modeling hair in units oftufts; which are partially concentrated areas of hair. This paper describes the collision …,Systems and Computers in Japan,2002,*
マルチモーダル翻訳インタフェース,森島繁生， 四倉達夫,抄録 Speech-to-speech translation has been studied to realize natural humancommunication beyond language barriers. Toward further multi-modal naturalcommunication; visual information such as face and lip movements will be necessary. In thispaper; we introduce a multi-modal English-to-Japanese and Japanese-to-Englishtranslation system that also translates the speaker's speech motion while synchronizing it tothe translated speech.,計測自動制御学会 部門大会/部門学術講演会資料 SICE SI2002 システムインテグレーション部門 講演会,2002,*
人物モデルの構築と歩行動作のルール化によるアニメーション生成,佐藤大,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索. 日本の論文をさがす;大学図書館の本をさがす; 日本の博士論文をさがす. 日本の論文をさがす; 大学図書館の本をさがす;日本の博士論文をさがす. ログイン; English. 検索. すべて. CiNiiに本文あり. CiNiiに本文あり・連携サービスへのリンクあり. すべて. CiNiiに本文あり. CiNiiに本文あり・連携サービスへのリンクあり.タイトル. 著者名. 著者ID. 著者所属. 刊行物名. ISSN. 巻号ページ. 出版者. 参考文献. 出版年. 年から年まで. 検索. 閉じる. 検索. 検索. 利用者のみなさまにご不便をおかけしておりますことをお詫び申し上げます。NII-ELSの終了にともない学協会との調整が必要な論文を除き、従前通りのサービス（ダウンロード機能を含む）を再開しました。詳細についてはこちらをご覧ください。人物モデルの構築と歩行動作のルール化によるアニメーション生成. 佐藤大; 被引用文献: 1件. 著者.佐藤大; 収録刊行物. 電子情報通信学会総合大会 2002 電子情報通信学会総合大会 2002; 2002 …,電子情報通信学会総合大会 2002,2002,*
HYPERMASK: 3 次元顔モデルを用いた仮面の構築,四倉達夫， 鉄谷信二， 中津良平， 森島繁生,HYPERMASK とは従来単一の顔表情や人物を表現する仮面の概念を進化させ;一つの仮面からあらゆる表情や人物を自由に生成及び表現可能なシステムである.本システムを用いることで; その仮面を装着した役者の表現の幅や新しい演出方法が生み出されていくと考えられる. 顔の表出手法として; 仮面に装着された五つの LED を; カメラにより追跡することで仮面の位置及び方向を求め; プロジェクタによって算出されたパラメータをもとに顔画像の投影を行う. また投影されている顔画像は演技者の音声を分析することによりリアルタイムで音声同期して口形状のアニメーションを行い; 顔表情や人物の切換はユーザが任意に選択可能である.本論文では HYPERMASK システムを用いた演出支援装置を紹介し; 新たな仮面の表現技法の確立を目指す.,電子情報通信学会論文誌 D,2002,*
2002 IEEE Workshop on Multimedia Signal Processing 9-11 December 2002,L Jacob; Jagannadha R Jakilinki; Nikil Jayant; Oliver Jokisch; Ton Kalker; Li-Wei Kang; Mohan Kankanhalli; Aggelos K Katsaggelos; Trish Keaton; Shahadat Khan; Chang-Su Kim; Kim JongWeon; Darko Kirovski; Takafusa Kitazume; Jan Olav Kjede; H Kokubo; Alex C Kot; C-C Jay Kuo; Michael KQstner; Joao Lacerda; Junqiang Lan; Rosa Lancini; Villa Lappalainen; Benghai Lee; Jae Jun Lee; Olli Lehtoranta; R Leonardi; Jin-Jang Leou; Oongge Li; Huiping Li; Liyuan Li; Ming Li; Wen-Nung Lie; Chia-Wen Lin; KJ Uu; Kuo-Cheng Liu; Qiang Liu; Ruey-Wen Liu; Angelos D Livens; Cristina V Lopes; Chun-Shien Lu; Haiping Lu; Yansheng Lu; Luca Lucchese; Hui Luo; Lei Luo; Ning Luo; Xiaohu Ma; Marka Madhavi; E Magli; Marcus Magnor; M Mahajan; Hong Man; Constantine Manikopoulos; Yinian Mao; John Mason; Nasir Memon; Matthew L Miller; Bijan G Mobasseri; F Mohanna; F Mokhtarian; DM Monro; Shigeo Morishima; Yannick Le Moullec; Jose MF Moura; Niloy Mukherjee; Paisam Muneesawang; Mitsuru Nakai; Milind R Naphade; Alessandro Neri; Zhicheng Ni; Hakan Norell; Mattias O'Nils; Job Oostveen; Antonio Ortega; Tomoshi Otsuki; R Otte; D Ovidiu; Charles Pandana; Thrasyvoulos N Pappas; Fernando Pereira; Andrew Perkis; Jean-Luc Philippe; Rajat Prakash; MP Queluz; FJ Quiles,*,Proceedings,2002,*
Open-source Software for Developing Anthropomorphic Spoken Dialog Agents,Kawamoto Shin-ichi; Hiroshi Shimodaira; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Keikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro; Shigeki Sagayama,An architecture for highly-interactive human-like spoken-dialog agent is discussed in thispaper. In order to easily integrate the modules of different characteristics including speechrecognizer; speech synthesizer; facial-image synthesizer and dialog controller; each moduleis modeled as a virtual machine that has a simple common interface and is connected toeach other through a broker (communication manager). The agent system underdevelopment is supported by the IPA and it will be publicly available as a software toolkit thisyear.,*,2002,*
自発・演技表情表出時における顔面動作および表情合成,四倉達夫， 内田英子， 山田寛， 赤松茂， 森島繁生,本稿では; 高速度カメラを用いて人間が自然な表情した場合 (自発表出) と普遍的かつ典型的な表情を演じる際の顔表情 (演技表出) を撮影し; 顔の各部位に設定した特徴点の変位置に基づき顔の動きの定量的な測定を分析した. また測定結果から CG によって構築した顔モデルのアニメーション生成を行った. 自発表出条件; 演技表出条件ともに顔の各部位の動き出しの差は微細であり高速度カメラを用いる有効性が示された. また情動ごとおよび表出条件ごとに顔の動き量や速さに特徴的な違いが認められたが; 動きの変化そのものの様相には興味深い共通性が認められた.顔モデルのアニメーションに関しても; 線形補間によるキーフレームアニメーションと比べ自然な顔表情表出が可能となった. The present study investigated the dynamic adpects of facialmovements in spontaneously elicited and posed facial expressions of emotion. We alsosimulated facial sunthesis by using results from an analysis; the animations that we …,*,2001,*
リアルな英語口形と発話アニメーション,石川行一， 森島繁生， 柴田昌明,また; 11 月 17 日の AM 2: 00-AM 5: 00 の時間帯においては; 約 30 秒~ 1 分間の無応答または以上応答が発生する可能性がございます.,[D] 平成 13 年電気学会産業応用部門大会講演論文集,2001,*
Trends of Learning Technology Standard,Shigeo Morishima; Shin Ogata; Satoshi Nakamura,*,null,2001,*
[パネル討論] あなたは人の計測に CV 技術を使いますか?,山本正信， 美濃導彦， 岩井儀雄， 白井良明， 上田博唯， 岡本浩幸， 八村広三郎， 坂本浩， 森島繁生， 中村祐一,オーガナイズドセッション 「人を観る」 に伴い; 人間を計測・認識する技術に関する注文・批判・今後の期待を討論する場を用意致しました. 各界で活躍されている研究者の方々に忌憚なく意見を述べて頂き; 人間を計測する技術への理解を深めつつ; 将来の方向性を探ります. なぜ画像センサを使うのか? 何を情報として抽出したいのか? 画像センサを使うことの利点/欠点は何か?対象が人間であることによって; 簡単になっている部分はどこか? 逆に難しくなっている部分は?どんな人体モデルが必要なのか? 等々の議論が期待されてます.,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),2001,*
人工知能学会全国大会論文集= Proceedings of the Annual Conference of JSAI 11; 404-407; 1997-06-24,安善， 小沢慎治， 橋本周司， 四倉達夫， 藤井英史， 森島繁生， 柳澤博昭， 祖川慎治， 前島謙宣， 大関和夫， 齊藤隆弘， 金子正秀， 原島博， 海老原一之， 楜沢順， 大谷淳， 中津良平， 岸野文郎， 青木義満， 村上和人， 輿水大和， 原文雄， 坂口竜己， 別所弘章， 渡辺孝弘， 木村聡， 谷内田正彦， 佐々木大輔， 目加田慶人， 春日正男， 植田信夫,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,失語症研究: 日本失語症研究会誌,2001,*
空間曲線上の点の直接操作によるヘアスタイルデザインシステム及びカット機能の実現,杉崎英嗣， 佐野和成， 森島繁生,抄録 サイバースペースにおける仮想人物の合成やコンピュータグラフィックス (CG)による人物合成等が注目を集めている. 本論文では特に人物の CG の中でも合成が難しいとされる頭髪の表現について述べる. 現在; 頭髪をマッピング技術を用いて表現する方法が成果を上げているが; 運動表現には不向きである. そこで頭髪の表現にテクスチャやポリゴンを用いずに 3 次 B-Spline 空間曲線を用いる. またインタフェースはレイヤモデルを用いたシステムを使用する.本論文では空間曲線上を通過する 3 次 B-Spline 空間曲線; 空間曲線の切断について述べる.また実際にレイヤモデルによるシステム用いて頭髪のモデリング; カットを実現した.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2001,*
3 次元個人顔モデルを用いたビデオ映像中の顔の自動トラッキング及びモデルマッチムーブ処理,三澤貴文， 村井和昌， 中村哲， 森島繁生,抄録 近年の著しい技術進歩により; 携帯情報端末で動画像を送受信したり;音声翻訳システムを介して海外の人々と母国語で会話ができる時代もそう遠くはなく;音声のみならず画像も翻訳できれば会話がより自然なものになると思われる.この画像翻訳の実現には; 映像中の人物顔を正確にトラッキングする技術が必要となる.顔のトラッキングは多くの研究者により研究されてきたが; その多くが顔の特徴点を追うものであり;特徴点のフレーム間でのブレや顔の回転による特徴点の隠れなどの問題が残されていた.そこで本論文では画像翻訳の核となる技術として; 3 次元個人モデルを用いたテンプレートマッチングによる顔のトラッキング手法を提案した. そして; 評価実験により; ある軸での角度の平均誤差が約 0.28 という結果を得た. この結果は; 提案した手法が効果的な方法であることを示すものである.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,2001,*
D-11-110 HyperMask-任意表情及び人物表出可能な仮面の構築,四倉達夫， 鉄谷信二， 森島繁生,Hyper Mask とは従来単一の 顔表情や 人物を表現する仮面 の概念 を進化させ; 1 つ の 仮面 からあ らゆる表情や 人物を自由に生成 お よび表現可能なシ ス テム で ある. 顔表出の手法 と して;仮面に 装着 された 5 つ の LED をカメ ラ に より追跡する こ とで 仮面の位置お よび方向を,電子情報通信学会総合大会講演論文集,2001,*
D-12-32 パンチルトカメラの多段接続による顔の特徴部位追跡,島田昌実， 島田直幸， 森島繁生,顔や 口 の 位置探索 には; そ の RGBColer に 着目 した 2 値化の 重心 を用い て い る. 探索領域を制限 し; 判別式を 1 次の 不等式で 表現 して シス テム の 処理速度を向上させ て い る. 図 3は室内; タン グ ス テ ン 照明下に お ける顔 と背景の RGB 分布で あ る.,電子情報通信学会総合大会講演論文集,2001,*
A-15-11 唇の特徴点抽出と音声分析を併用した音声に同期する唇画像の実時間合成,大室学， 伊藤圭， 島田直幸， 森島繁生,肌色領域の 重心を基準に唇領域 を確定; Q 植を用い て唇を抽出する ll1. なお; こ こで 用いる閾値は 自動設定を行 っ て い る. 抽 出 した唇か ら唇の 両端;. ヒ唇. ヒ側;,電子情報通信学会総合大会講演論文集,2001,*
D-12-35 十字型テンプレートを利用したリアルタイム視線追跡と瞬き検出,笠嶋健吾， 島田直幸， 森島繁生,R; G; B 値 が確保 しす る こ とで 初期テ ン プ レ ー トを作成 し; それ を元に テ ン プ レ ー トマ ッ チン グ法 を用 い目を探 索す る. ま た; 同 時に追跡 中の 目の R; G; B 値も確保 し; 更新テ ン プ レ ートを作成 し; よ り画像 との 誤差 の 小 さい ほ うを とる.,電子情報通信学会総合大会講演論文集,2001,*
A-14-22 細分割曲面を用いた顔表情合成,伊東大介， 森島繁生,2. 細分割曲蔵局所曲颪近似理論 を用 い て; 図 1 (b) の よ う i; こ: 各頂点の近傍 に局所的に近似薩面を生成 し; その近似曲面 上に新しい 分割点を作っ て三角形ポ リ ゴ ンを分割す る. 各頂点 Vとその 近傍 点 を用 い て 最小 自飛法 よ り係数を決定す る こ とに よっ て近似 2 次 曲甌 勲; ソ)(式勾 を生 成する. 図 1 (a> の よ うに yfiP; を分 割する・g? と して s (u! 2. 幽 を計算 して 補間点の 座標 とす る・,電子情報通信学会総合大会講演論文集,2001,*
A-16-5 舌モデルの付加によるリアルな英語口形の実現と発話アニメーション,石川行一， 伊藤圭， 三澤貴文， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2001,*
A-15-24 複数の基本顔の恒等写像学習による感情空間構成,高橋光紀， 伊東大介， 森島繁生,ラル ネッ トの 恒等写像学 習を用い て; 17 次元の 表情パ ラメ ー タをその 中問層 に 3 次元 に圧縮 して 感情空 間 と仮定 し; 同時 に表情 か らこ の 空間へ の 写像 とその 逆写像 (感情 空間→表情) を実現 して 表情 の 分析,電子情報通信学会総合大会講演論文集,2001,*
A-14-19 顔特徴点抽出に基づく正面顔画像への標準顔モデルの自動フィッティング,坂本将人， 伊藤圭， 三澤貴文， 森島繁生,デル を個人 の顔画像に フ ィ ッ テ ィ ン グ させ る作業は未だ手動で あ り; 自動化する こ とが 課題とな っ て い た. そ こで 本稿は; 顔特徴点抽 出 を行い; その 特徴点を,電子情報通信学会総合大会講演論文集,2001,*
A-14-21 高速度カメラを用いた表情表出時の顔面動作の分析および微妙な表情の合成,長谷川佳之， 四倉達夫， 内田英子， 山田寛， 森島繁生,悲 しみ) の 表情表出時にお ける顔器官の 微小 な変化を測定 した. そ して それ を もとに表情合成をお こ ない通常 フ レーム数に より合成され たア ニ メ ーシ ョ ン との比較 をする.,電子情報通信学会総合大会講演論文集,2001,*
A-15-10 複数のカメラによる口・目領域追跡と表情合成のための特徴分析,島田直幸， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2001,*
A-14-18 仮想空間を用いた多人数コミュニケーションシステム構築,伊藤圭， 四倉達夫， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2001,*
A-14-23 実画像とワイヤフレームモデルを併用した自動翻訳合成音声とのリップシンクシステムの構築,緒方信， 中村哲， 森島繁生,現で きるで あ ろ う. 従来 よ りあ る音声翻訳シ ス テ ム ln に加え; 顔画像翻訳に おい て; 話者の表情を保つ 為に; 口や その 周囲 の情報以外は 原言語発話時の 顔動画像 をその まま用 い; 口領域 につ い て は任意の 話者に適合で きる 3 次元モ デ,電子情報通信学会総合大会講演論文集,2001,*
A-14-6 ネットワークシアタ,高橋和彦， 楜沢順， 四倉達夫， 森島繁生， 鉄谷信二,ジェ ン k) の 3 次元形状 デ ー タが制作者に よ っ て サ ーバ 上 に構築 される. こ こで ア バ タとは仮想 空 間内で 演技者の 動作 に応 じた 動 きをす る キ ャ ラク タで あ り; エ,電子情報通信学会総合大会講演論文集,2001,*
D-11-113 空間曲線上の点の直接操作による頭髪スタイル制御とカット機能を有するヘアスタイルデザインシステム,杉崎英嗣， 佐野和成， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,2001,*
D-12-126 3 次元顔モデルを用いたビデオ映像中の自動顔トラッキングとモデルマッチムーブ,三澤貴文， 村井和昌， 中村哲， 森島繁生,1. は じめに海外の 映画や TV な どの音声吹き替えは何年 もの 間行われ て お り; そこには常に音声 と口の 動きが 同期 してい ない とい う状態があっ た. そこで本稿で はビデオ映像中の 3 次元形状が既知である人物の 顔の 動 きをテ ン プレー トマ ッ チ ン グを用い てトラ ッ キ ン グし; その対象人物の ロ を 3 次元モ デルに置き換えて 口形状の合成をするこ とに より;ある言語の 口形状か ら他の 言語の 口形状へと変換する 画像翻訳 の手法を紹介す る. 2; 3次元顔モデル 生成人物の 顔画像 を用い て 3 次元顔モ デ ル 生 成する ッ ー,電子情報通信学会総合大会講演論文集,2001,*
3 次元空間共有コミュニケーション技術の研究開発: 実写画像をベースとしたマルチメディア・アンビアンスコミュニケーションの実現に向けて,望月研二， 齊藤隆弘， 市川忠嗣， 森島繁生， 相澤清晴， 山田邦男， 須賀弘道， 岩澤昭一郎， 山本健一郎， 児玉和也， 苗村健， 斎藤英雄,抄録 現実感のある 3 次元画像の共有空間を構築することにより; 遠隔地で離れた人同士があたかも同じ空間を共有しているような感覚でコミュニケーションしたり CSCW などに利用する事が;高度な画像メディア通信の領域で求められている. 広い周囲環境画像をビデオカメラやディジタルカメラで撮影し; この実写画像を元に 3 次元画像の共有空間を再構成する方式では;いまだ品質の良いものは実現できてない. 人の視覚特性に基づけば遠景・中景・近景の 3レイヤ構造やセッティング表現の考えをベースに; 現実感が高くかつ効率的な 3次元表示が可能である. 本稿では; 視点に基づく立体視のための近似的表現を利用した空間共有コミュニケーション (Multimedia Ambiance Communication) のための 3 次元画像空間の構築方法について述べる.,電子情報通信学会技術研究報告. PRMU; パターン認識・メディア理解,2001,*
映像情報の検索技術と編集処理 色ヒストグラムインターセクションを用いたリアルタイム人物顔抽出,吉村哲也， 市川忠嗣， 森島繁生， 相澤清晴， 齊藤隆弘,人物頭部が動いたり; 顔表情が変化しても顔を抽出するため; 色ヒストグラムインタセクションの類似尺度を用いて物体を抽出する手法を改良し; 顔の抽出に適用した. 改良では;従来のアクティブ探索法をより高速化する手法を考案した. 本手法を用いて; 顔抽出実験を行い;顔抽出の安定性と処理速度に関する評価を行った.,映像情報メディア学会誌,2001,*
Hypermask: talking head projected onto moving surface.,Tatsuo Yotsukura; Shigeo Morishima,*,ICIP (3),2001,*
Networked Theater: Contents Production Systems Using Virtual Environment and Computer Network,K Takahashi; J Kurumisawa; T Yotsukura; S Morishima; N Tetsutani; R Nakatsu,*,JOURNAL-INSTITUTE OF IMAGE ELECTRONICS ENGINEERS OF JAPAN,2001,*
Audio-Visual Tracking System for Multi-Modal Interface,Dmitry ZOTKIN; Kazuhiko Takahashi; Tatsuo Yotsukura; Shigeo MORISHIMA; Nobuji TETSUTANI,抄録 In this paper; a front end system which uses audio and video information to track thepeople or other sound sources in the ordinary room has developed. The microphone array isused for determining the spatial location of the sound; the active video camera acquires theimage of the area where the sound is detected; detects the people in the image by usingskin color and can zoom and track a speaker. Several add-ons to the system include variousvisualization tools such as on-screen displays of waveforms; correlation plots; spectrumplots; spatial acoustic energy distribution; running time-frequency acoustic energy plots; andthe possibility of real-time beamforming with real-time output to the headphones. The systemcan be used as a front-end for the non-encumbering human-computer interaction by videoand audio means.,The Journal of the Institute of Image Electronics Engineers of Japan,2001,*
2. 通信・放送機構 本郷空間共有リサーチセンターの研究紹介: 空間共有コミュニケーションプロジェクト,市川忠嗣， 相澤清晴， 森島繁生， 齊藤隆弘,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像電子学会誌,2000,*
Hyper Mask: 表情・口形状制御可能な顔モデルを用いた仮面の構築,四倉達夫， 鉄谷信二， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,研究会講演予稿,2000,*
擬人化音声対話エージェント開発と周辺技術 (3) 対話における顔画像生成,森島繁生， 四倉達夫,概要 近年; ユーザフレンドリーなヒューマンインタフェースの研究が盛んである.ユーザフレンドリーとは言うまでもコンピュータと人間との対話を円滑にするものである. 1つの形態として; 擬人化エージェントをディスプレイ上に表示して; この擬人化エージェントが言語情報のみならず非言語情報をも理解・表出することによって人間同士が面と向かって対話するような自然さでコンピュータとコミュニケーションできるシステムが考えられる. このシステムで重要な要素はいかに本物の人間と見分けがつかないくらいリアルで生命の息吹をも感じさせるような擬人化エージェントを生成するかである. この実現のためにはまず; 実在する人物の表情や印象をいかに忠実にアバタの表情としてコピーできるかが; 表情合成に課せられた課題である. 特に対話の場合は;時間遅れが少なくリアルタイムに行えることが必須条件となる. 本稿では; 著者らが現在取り組んでいる顔合成の技術を紹介し; 擬人化音声対話エージェント開発に向けてのこの研究の位置づけを …,情報処理学会研究報告音声言語情報処理 (SLP),2000,*
擬人化技術: リアルなコミュニケーション環境生成のための表情分析・合成,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,ヒューマンインタフェース学会誌= Journal of Human Interface Society: human interface,2000,*
顔の認識・合成のための標準ツール (豊かなヒューマンコミュニケーションのための顔とジェスチャの認識・合成技術特集号),森島繁生， 八木康史,Page 1. Institute of Systems; Control and Information Engineers NII-Electronic Library Service工nstitute of Systems ， Control and 工nformation Engineers シス テ ム ／制 御ノ情報，vol ，44 ，No 、3 ， pp ．121 −127，2000 121 党 − 冒 口 解 難 戀 覇 纛 た 韆 韈 攤 鞴 鑞 難 合 韈 森島 繁生 ＊ ・八 木 康史† 1同川1「II」Ill」川11HllllllNil 川Ill 川Ill川II川II川II目lllllH 川Ill 川Illllllllllllllll 門llllllllrMlllll川IIIII 川1［1目目川II川IIII 川11rlllllH 川lll川111川1「亅川ll日Ill旧llll 目IIII 川川II川IIIIIIII 川IIIIIII 「IIIIIII」旧II川川ll川HFIIII 川rli川川lilll 旧illilliF 川1［“！1川II 1． は じめ に 顔画像処理に対する様々 な分野での関心の 高ま りや，工 学分野 に おける 顔画像処理技術 の 研究成果の 蓄積 を背景 に し て ，手軽 に入手可 能 な顔 の 認識 と合成 に 関連 し た ブ リ ーソ フ トウヱ ア，商用 ソ フ トウェ ア ，さらに 顔のデ ータ ベ ース などが 整備 されて きて い る， 本稿で は ，と くに イ ン ターネ ッ ト経 由で ダウ ン ロ ード可能 な顔の 認識ソ フ トウェ ア と合成 ソ フ トウ ェ ア につ い …,システム/制御/情報,2000,*
ボクセル表現による衝突判定法とバネモデルによる頭髪運動表現,岸啓補， 森島繁生,1. 頭 髪の モ デ ル 化頭髪を表現す る ために は 大別 して 2 種頏の 方法が ある; 前述の よ うに;頭髪を短冊状 の 物 体な どで 表現 し; 様 々なマ ッ ピ ン グ を行 う方法 と; 頭髪,電子情報通信学会総合大会講演論文集,2000,*
自然音声の分析に基づく音声への感情情報の付加,加川大志郎， 四倉達夫， 森島繁生,佐藤ら [1] は; TV ドラ マ か ら 録音した 感情音声に 対 し SD 法に よ る 聴取実験を行い; 因子分析を行 っ た. こ れか ら; rtirt 性化」;「快 一 不快」 とい う 2 つ の 因子につ い て因予得,電子情報通信学会総合大会講演論文集,2000,*
EMG による筋肉モデルの制御と精密な表情合成,塚田章之， 伊東大介， 森島繁生,つ に 顔面筋肉モ デ ル が あ る. 顔面筋肉モ デ ル は モ デ ル 化さ れ た皮 膚 組 織及 び表情筋を持ち 筋肉が 皮膚組織に 与え る 影響力を計算し; 皮膚組織を. 変形 させ る こ と に よ って; 表 情 表 出 が 可 能で あ る. 表情を 決定する パ ラメ ータは 筋肉が 収縮する 力 (筋肉パ ラ メ,電子情報通信学会総合大会講演論文集,2000,*
高速度カメラを用いた顔面動作の分析,四倉達夫， 内田英子， 山田寛， 森島繁生， 赤松茂， 大谷淳,為; 本稿で は表情変化時における顔部位の 移動量; 形状の微小変化に注 目し; 表情表出プロ セ スを高速度カ メ ラで撮影を行い; 各器官の動 きの定量化を図る. 表情表出にお いて; 意図的 なコ ントロ ール を与 えた もの(演技表情); そして 何らかの 情動喚起に 伴 い 自発的 に 現れ た もの(自発的表情); 計 2 種類の 表情を追う c; また; 撮影 された顔画像シークエ ン ス の 円滑な顔器官抽出や追跡 を行 うため GUI,電子情報通信学会総合大会講演論文集,2000,*
韻律情報制御のための感情音声合成 GUI ツール,緒方信， 四倉達夫， 森島繁生,人間同士の 対話も円滑化する 新し い コ ミュ ニ ケーシ ョ ンシ ス テ ム が実現 可能とな る. 佐藤ら[1] は 音声か ら得 られる 感情情報を 13 の カ テ ゴ リーに分類 し; その 分析結果から; 韻律に 関する規則 を定め; 感情音声合成 を行っ て いる. しか し白然 齢声に 感情を付加する 為に は; 原脅声の クオ リテ ィ; 発 li 舌内容; 話者の情報を保 ちつ つ; 韻律情報を制御 しなくて はならない;;,電子情報通信学会総合大会講演論文集,2000,*
判別分析法による音声の感情推定及び実時間メディア変換システム,廣瀬陽介， 四倉達夫， 森島繁生,ザ に 意識させ ない 表情合成 と音声との 同期表示が 必要に なる; この ような環境の 実現 に 向け;マ イクか らの 入力音声の 分析に基づ き; 会話時の 唇の 動き及 び表情をリア ル タ イム に合成 する実,電子情報通信学会総合大会講演論文集,2000,*
動画像分析からの 3 次元表情パラメータの推定と表情再合成,小野哲治， 伊東大介， 森島繁生,ら; 筋肉が皮膚組織に与 える影響力 を物 理的に 計算 して; 皮膚組織 を変形 させ るこ とに より表情 を表すこ とが で きる. その 表情は筋肉が収縮する 力⊂ 筋肉バ ラ メ ー. 一夕) によ っ て; 表されて,電子情報通信学会総合大会講演論文集,2000,*
流体場を用いた頭髪アニメーションと頭部との衝突判定法,奥谷敦， 岸啓補， 森島繁生,6. ボク セ ル に よる衝突判定実 際 の 頭 髪の 動 きを 観察 す る と; 頭 髪が 頭部に 進 入 した場合; 頭髪は 頭 部に 接 触 した まま頭 i 午 B に 添っ て 運 動す る と み なせ 嶺 こ のような頭髪の 動 きを実現す る た め; まず 空 間 をボ ケ セ ル に 分 割し; 頭 部 を形 成する ポ リ ゴ ンを内部 に含むボ ク セ ル を衝突判定用の ボ ク セ ル とする [図 4). こ の 時; ボ ク セ ル に 含 ま れて い る ポリゴ ン の 法線ベ ク トル をそ の ボ クセ ル に お け る 法. 線 ベ,電子情報通信学会総合大会講演論文集,2000,*
複数アングル画像からの 3 次元頭部モデルの生成と表情合成,河合丈治， 三澤貴文， 武藤淳一， 森島繁生,成 され た頭部モ デ ル は 図 2 の よ うに なる L こ の 図 を 見 る と; モデ ル は 3 次 元化はなされて い るが; 頬 の 部分が乎坦 に なっ て いて 対象人物に 忠実な モ デ ル を生成で きた とは言え な い. そ こ で; 頬 の 部 分な ど を表現す る た め に; 正面; 側面以 外の他の ア ン グル からの画像 を用 い たフ ィ ッ テ ィ ン グ をロ r 能に した.,電子情報通信学会総合大会講演論文集,2000,*
画像と音声を併用したリアルタイムリップシンク,川又正太， 島田直幸， 武藤淳一， 森島繁生,タ 同士が互 い に その 空 間を共有す る 事に よっ て 現実世界あよ うな対面会話 を 叮能 としている.「11 ア バ タの 唇合成の アニ メ ーシ ョ ン は音声分析に よ り母音認 識を行 い; それ を基 に して口形 パ ラ メ ータ に よ りを唇合成 して い る ため子音形状 が不鮮明で あ る. そ こ で本研究では音声情報 と画像情報 を併用する 事に よっ て 音声分析 シ ス テム を補い; 分析 した特徴か らリ ア ル な唇形状 を実時間で 合 成す る事を H 的 とする. z. lllSWt:. illi,電子情報通信学会総合大会講演論文集,2000,*
複数視点画像を用いた 3 次元頭部モデルの構築,武藤淳一， 森島繁生,る手法 と して Cyberware 等の 3 次元 ス キ ャ ナ を用い る方法が ある. しか し; こ れ に よ り作成されたモ デル は高 精細で あ るが; モ デ ル を構成 する格子 点が 膨大な数に な る ため取 り扱いが困難で あ り; 表情変形などを行 う際も目や口 とい っ た特徴 点の 対応 が取 りづ らい とい った問題点がある. そ こで 本稿で は; 複数視点か らの 画像 を用い て 扱い や すい 3 次元頭部モ デル を構築する手法につ い て述べる. 本手法で は わずか な手間で リ アル なモ デ ル が 作成可能で あ る.,電子情報通信学会総合大会講演論文集,2000,*
3 次元映像情報メディア技術 実写画像をベースとしたマルチメディア・アンビアンスコミュニケーションの提案,市川忠嗣， 吉村哲也， 山田邦男， 金丸利文， 須賀弘道， 岩澤昭一郎， 苗村健， 相澤清晴， 森島繁生， 齊藤隆弘,実写画像 をベ ース と したマルチ メデ ィア ・ア ン ビア ンス … Multimedia Ambiance Communicationbased on Video and Image … 正会員 市 川 忠 嗣 †1; 吉 村 哲 也 †1; 山 田 邦 男 †1; 正会員 金丸 利 文 †1 … 正会員 須 賀 弘 道 †1; 岩 澤 昭 一 郎 †1; 正会員 苗 村 健 †1;†2; 正会員 相 澤 清晴 †1;†2 … 正会員 森 島 繁 生 †1;†3; 正会員 齊 藤 隆 弘 †1;†4 … Tadashi Ichikawa†1; TetsuyaYoshimura†1; Kunio Yamada†1; Toshifumi Kanamaru†1 … Hiromichi Suga†1; ShoichiroIwasawa†1; Takeshi Naemura†1;†2; Kiyoharu Aizawa†1;†2; Sigeo Morishima†1;†3 and TakahiroSaito†1;†4 … あ らま し ビデオ カメラや ディジタル カメラで撮影 された実写画像 を用 い;遠 …景;中 景;近 景の レイヤ構 造で … 表現 し;再 構成 す るフォ トリア リステ ィックな3次 元画像 空間 を提案す る と共 に;こ の画像空 間 による コ ミュニ … ケー シ ョンを提案 し;そ の実現方法 を報告す る …キーワ-ド: 空間共有通信;3次 元画像空間;実 写画像;イ メージベーストレンダリング;遠 景 ・中景 …,映像情報メディア学会誌,2000,*
パン; チルト; ズーム制御可能なカメラによる顔特徴の実時間追跡,森島繁生， 島田直幸,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,成蹊大学工学研究報告,2000,*
Regular Section-PAPERS-Image Processing; Image Pattern Recognition-3D Face Expression Estimation and Generation from 2D Image Based on a Physically Con...,Takahiro Ishikawa; Shigeo Morishima; Demetri Terzopoulos,*,IEICE Transactions on Information and Systems,2000,*
コンピュータグラフィックスを用いた矯正治療による表情の変化,寺田員人， 宮永美知代， 森島繁生， 花田晃治,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,歯科審美= Journal of esthetic dentistry,1999,*
生理学的手法を用いた顔面筋肉モデルの構築,伊東大介， 四倉達夫， 森島繁生,抄録 近年; 人間の顔表情を CG (Computer Graphics) にて表現することは映画の特殊効果や;ヒューマンインタフェースのためのエージェントの表現として-般的になっており;そのクオリティは実写に近いレベルまで達している. しかしながらそれらの構築に対しアニメータ等の膨大な労力と資金が必要であり; 製作期間も長期間にわたるのが現状である.そこで本論文ではリアルな顔画像生成のため; 皮膚組織や表情筋を持つ顔面筋肉モデルを用いて表情表出を行うシステムを構築し; 各表情筋の変化に対応した筋電を測定する装置を用いて各々の筋電を測定し; 各筋肉の収縮をモデル化する. 測定データから顔面筋肉モデルの表情筋をコントロールして; リアルな口形状のモデル化を実現するシステムも可能となった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1999,*
表情筋運動モデルによる顔面モデルの構築,石川貴博， 森島繁生,抄録 コンピュータグラフィクス (Computer Graphics 以下 CG) による顔表現方法の 1つとして顔面筋肉モデルが挙げられる. 現在; 顔面筋肉モデルの表情筋は; 単なる線分で表現され;実際の表情筋の様な複雑な形状を与えられていない. そこで; 本論文ではこれらのことに着目し;表情筋の形状データを使用し; 同時にその表情筋の運動表現を試みる. 表情筋の運動表現は;表情筋がバネの集合体であると仮定し; 運動方程式を解くことで実現させる. また下顎骨を剛体と仮定し; 咀嚼筋による下顎骨の運動制御を提案する. 下顎骨は咀嚼筋の収縮時の力を受けて回転運動を行う. これらの結果; 表情筋の形状を考慮した運動表現および咀嚼筋による下顎骨の運動表現が可能になった.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1999,*
仮想人物によるサイバースペース上でのコミュニケーションシステムの構築,四倉達夫， 武藤淳一， 今村達也， 藤井英史， 森島繁生,抄録 コンピュータの処理能力の急速な発展により; 複数のユーザがネットワークを介してサイバースペースを共有し; 対話や協調作業を行うインタラクション環境が整ってきている.この仮想空間への没入感覚と臨場感を向上させるためには; 実空間と同等の自然さで; 人間同士のコミュニケーションを実現する必要がある. そこで本稿では; 自分自身の姿を投影した顔を持つアバタ(Avatar) を仮想空間上に生成し; マイクから入力された自然音声に同期させて会話時の口形状および感情推定をリアルタイムに実施し; アバタの表情合成を行うシステムを提案する.このシステムによりサイバースペース上で多人数が参加可能なフェイストゥフェイスの対話環境が実現可能となった.,電子情報通信学会技術研究報告. HIP; ヒューマン情報処理,1999,*
A-16-12 3 次元頭部モデルを用いた実時間メディア変換,藤井英史， 森島繁生,この ような環境の 実現に 向けて; 入力音声から会話時の 唇の 動 きお よ び表情 を実 時間で表現す る メ ディ ア 変換シ ス テ Al 11 を提案す る. 本稿で は; 3 次元 頭部モ デ,電子情報通信学会総合大会講演論文集,1999,*
A-16-42 実写動画像をベースとしたマルチメディアアンビエンスコミュニケーションの提案,市川忠嗣， 吉村哲也， 山田邦男， 金丸利文， 須賀弘道， 岩澤昭一郎， 苗村健， 相澤清晴， 森島繁生， 齊藤隆弘,を利用 して 臨場感を高める検討も盛んに 行われて 来て い る/コ [1) K4) しか し; 画像として;実写画像をベ ースとした場合に; 画像品質の 良い 画像空間は まだ; 実現で きて おらず; 空間,電子情報通信学会総合大会講演論文集,1999,*
A-15-19 体積を持った表情筋運動モデルによる顔面モデルの構築,石川貴博， 森島繁生,節点の 運動方程式を考え る. 表情筋モ デ ル は ポ リ ゴ ンで構成され; 節点及 び節点同士を結ぶ線分で 構成 される. こ れ らの 節点同士 を結ぶ 線分 をバ ネ と仮定 し運 動,電子情報通信学会総合大会講演論文集,1999,*
D-12-71 Pan Tilt Zoom Controllable Camera による目及び唇形状抽出・追跡,島田直幸， 武藤淳一， 森島繁生,顔領域 下部 に仮日領域 を設定 し; 領域内につ い て YIQ 表色系の Q 殖を用 い て 2値化及び平滑化 に よる雑音除去を行 う. 唇の 色 は個人差がある の で 閾値は イ ン タ ラ ク,電子情報通信学会総合大会講演論文集,1999,*
D-12-91 全身像の三眼シルエット画像に基づく姿勢推定の検討,岩澤昭一郎， 大谷淳， 森島繁生,人物の 全身姿勢を記述する 姿勢パ ラメ ータ と して 身体各部を表す点; 頭頂; 両手先; 両肘関節;両足先; 両膝関節の 計 9 点を定義 し; こ れ らを特徴点と呼ぶ; そして 特徴点,電子情報通信学会総合大会講演論文集,1999,*
A-15-17 Gabor Wavelet を使用した動画像からの顔表情認識,近藤淳， 森島繁生,クで 1 次元の シ ーク エ ン ス に変換する. その シ ーク エ ン ス を mm で 学習する.} 恐照 ま 9cft−to− right 型と呼ばれ る もの で あり; 学習に は Forward− Backvard ア ル ゴ リズ ム を使用する.GaborWavelet フ ィ ル タ の 応答は 多次元 のベ ク トル で あ り; 目や 口 とい っ た時間変化の 大きい とこ ろ と額 とい っ た時間変化の 小 さ い とこ ろが ある u 顔表情を認識する 際; で きるだけ表情変化の 特徴を含むベ ク トル 成分を ema {の 学習デ ータ とし,電子情報通信学会総合大会講演論文集,1999,*
A-15-6 筋肉パラメータに基づく 3 次元感情空間の構築,重松陽志， 石川貴博， 森島繁生,3 研究概要本 研 究 は; 筋 肉パ ラ メ ータ か ら 感情空間に 写像 し; 感情空 間に 与え られた空間内の 点か ら; 筋肉パ ラメ ータに変換する部分 の シ ス テム の 構築を行 う. こ の シス テムの 構築の ため に; 汎化性能を持ち; 非線型写像能力に優れ た 5 層ニ ュ,電子情報通信学会総合大会講演論文集,1999,*
A-14-3 リアルな 3 次元表情編集ツールの作成,千明太郎， 藤井英史， 四倉達夫， 森島繁生,人間同士の 会話におい て; 顔の 表情とい うの は さまざまに変化する. こ れは人間同士がコ ミ ュニ ケーシ ョ ン を取る 場合に 重要な役割を持っ て お り; 顔 の 表情が情報伝達の 一. 一つ の 手段とし て活用 されて い る こ と が わ か る. そ こ で; コ ン ピュ,電子情報通信学会総合大会講演論文集,1999,*
A-14-7 音声に含まれる感情のモデル化と感情音声合成ツール,古村祐子， 四倉達夫， 森島繁生,音声 と なれ ば; 人間と機械の 間で 円滑 に会話が で き る よ うに なる と考えられる. 佐藤ら [1]は収録した無感情音声と感情音声 との 分析結果か ら韻律に 関する規則を定め; 感情音声合成を行っ,電子情報通信学会総合大会講演論文集,1999,*
D-12-154 流体力学に基づく CG による風に靡くリアルな頭髪の運動表現,平山聡， 岸啓補， 森島繁生,い た人物画像の 合成が行われて お り; よ リリア ル な人物の 合成画像が求め られて い るりこ れまで に 頭髪を 「空問曲線」 によ っ て近似 し;「剛体セ グ メン トモ デル」 を用い て 運動 を制御す る方法を考案して きた,電子情報通信学会総合大会講演論文集,1999,*
D-12-82 オプティカルフローによる筋肉パラメータの自動推定と表情再合成,伊東大介， 岩澤昭一郎， 森島繁生,て 平滑化フ ィル ダ をかける. 次に; 運続す る 2 フ レーム 間で オプテ ィ カ ル フ ロ ーを求め; それをフ レーム 毎に 計算 し加 え脊わせ る. そ して 次節で 述べ る 筋肉モ デ ル に おい て 筋 肉があると 思わ れ る位置に 窓領域 を自動で 設定 し; 各窓内で フ ロ ーを 平均化を 行うこ とに よ っ て;筋肉の 方向成分の フ ロ・. の 大 きさを計算,電子情報通信学会総合大会講演論文集,1999,*
A-16-11 3 次元アバタの構築とリアルタイム対話システム,三澤貴文， 四倉達夫， 藤井英史， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,1999,*
A-14-4 音声からの感情推定と実時間メディア変換システム,今村達也， 四倉達夫， 森島繁生,音声か ら顔画像へ の メ デ ィ ア 変換とは; テ キス ト等の 段階を経る こ とな く; 音声か ら直接発話時 の 口 の 形状及び感情を推定す るこ とで 顔画像を合成する もの で ある. こ れ は;音声か ら特徴パ ラメ ー タを抽出 し; そ の 音声 の 発話時の 口 の 形状を規定する 口 領域 の変形パ ラ メ ータ (口 形パ ラ メ ータ) 及 び表情変形パ ラ メ ータ (表情パ ラ メ ータ) へ と変換する こと と換言で きる. 本稿で 述べ るシス テ ム で は; 音声か ら感情推定を行い; その 結果から対応する表情パ ラ メ ータ を用い る.,電子情報通信学会総合大会講演論文集,1999,*
D-12-72 空間周波数を用いた口領域のモーションキャプチャ,川俣充， 武藤淳一， 近藤淳， 森島繁生,か らの 発話内容; 感情状態の 推定が重要 な課題 となる. 本論文で は預器官の 中で も寄与が 大きい H 領域の モ ーシ ョ ン を捉え; モ デ ル で形状 を再合成する こ とを試み,電子情報通信学会総合大会講演論文集,1999,*
A-16-12 3 次元頭部モデルを用いた実時間メディア変換,藤井英史， 森島繁生,この ような環境の 実現に 向けて; 入力音声から会話時の 唇の 動 きお よ び表情 を実 時間で表現す る メ ディ ア 変換シ ス テ Al 11 を提案す る. 本稿で は; 3 次元 頭部モ デ,電子情報通信学会総合大会講演論文集,1999,*
表情の分析・合成を用いたサイバースペース内でのフェーストローフェース対話システム,森島繁生,抄録 Recently computer can make cyberspace to walk through by an interactive virtualreality technique. An avatar in cyberspace can bring us a virtual face-to-face communicationenvironment. In this paper; an avatar is realized which has a real face in cyberspace and amulti-user communication system is constructed by voice transmission through network.Voice from microphone is transmitted and analyzed; then mouth shape and facialexpression of avatar are synchronously estimated and synthesized on real time. And also anentertainment application of a real-time voice driven synthetic face is introduced and this isan example of interactive movie. Finally; face motion capture system using physics basedface model is introduced.,映像情報メディア学会技術報告,1999,*
文法カテゴリ対制約を用いた A* 探索に基づく大語彙連続音声認識パーザ,坂井信行， 船田哲男， 池田卓史， 梶田将司， 武田一哉， 板倉文忠， 河原達也， 李晃伸， 小林哲則， 峯松信明， 伊藤克亘， 伊藤彰則， 山本幹雄， 山田篤， 宇津呂武仁， 鹿野清宏， 南條浩輝， 加藤一臣， 嵯峨山茂樹， 川本真一， 下平博， 新田恒雄， 西本卓也， 中村哲， 森島繁生， 四倉達夫， 甲斐充彦， 李晃伸， 山下洋一， 小林隆夫， 徳田恵一， 広瀬啓吉， 伝康晴， 石黒勝彦， 大津展之， 國吉康夫,河原 達也; 李 晃伸; 小林 哲則; 武田 一哉; 峯松 信明; 嵯峨山 茂樹; 伊藤 克亘; 伊藤 彰則; 山本幹雄; 山田 篤; 宇津呂 武仁; 鹿野 清宏,Journal of the Acoustical Society of Japan (E),1999,*
コンピュ-タを利用した表情の研究 (特集 顔学入門),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,言語,1998,*
コンピュータ画像処理入門コンピュータ画像処理入門; 1985,椎山弘隆， 正木克己， 白仁田和彦， 林健一郎， 大坪昭文， 瀧山龍三， 小林景， 杉原厚吉， 岩澤昭一郎， 海老原一之， 大谷淳， 中津良平， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像電子学会誌,1998,*
複数画像からの実時間身体姿勢推定の検討,岩澤昭一郎， 竹松克浩， 大谷淳， 森島繁生,ま とめ複数画像 を用い た身体姿勢 の 三 次元推定手法 に つ い て 述べ た. 今後は誤差等の 検討を行 うとと もに; さ らに多くの 姿勢につ い て も対応 で きる よ うに改良を進め る. なお 本手法 はACM SIGGRAPH98 の 技術展 示部 門 に,電子情報通信学会ソサイエティ大会講演論文集,1998,*
11) 音声による実時間口形・表情制御可能なサイバースペース上での仮想人物の実現 (ネットワーク映像メディア研究会),四倉達夫， 藤井英史， 小林智典， 森島繁生,ィ ッ ク ス に よる人物画像合成等が注目を集め て い る. 本稿で は; 特に 人 物の CG の中でも合成が難しい と され る頭 髪 の 表 現 方法 に つ い て 述 べ る. 人 物 画像 に お い て頭髪は;視覚的に 重 要で あ る に も関わ らず; 簡単な 曲 面 や背景 の 一部 で代用され る こ と が 多い.頭髪を! つ の 物体 と して扱い; マ ッ ピ ン グ技術を用 い て 表現す る 手法が 成果を あ げて い るが; 運 動 の 表 現 は不 可能,映像情報メディア学会誌: 映像情報メディア,1998,*
10) 房モデルによるヘアスタイルデザインシステムの開発 (ネットワーク映像メディア研究会),岸啓輔， 三枝太， 森島繁生,ィ ッ ク ス に よる人物画像合成等が注目を集め て い る. 本稿で は; 特に 人 物の CG の中でも合成が難しい と され る頭 髪 の 表 現 方法 に つ い て 述 べ る. 人 物 画像 に お い て頭髪は;視覚的に 重 要で あ る に も関わ らず; 簡単な 曲 面 や背景 の 一部 で代用され る こ と が 多い.頭髪を! つ の 物体 と して扱い; マ ッ ピ ン グ技術を用 い て 表現す る 手法が 成果を あ げて い るが; 運 動 の 表 現 は不 可能,映像情報メディア学会誌: 映像情報メディア,1998,*
HTML ブラウザを用いた感情音声刺激の SD 法評価実験 (研究発表 B; IV. 第 16 回大会発表要旨),佐藤順， 森島繁生， 山田寛,実験で は; テ レ ビ ドラ マ か ら収録 した; 怒 り; 嫌悪; 恐れ; 喜び; 悲 しみ; 驚き; 無感情の 103 の音声を刺激と した. こ れらを 72 名の 被験者に提示 し (各被験者 に 20 の 音声提示); SD 法 に従っ て 25 の 形容詞対項 目で 評価させ た; 実験の 結果を因子分析 した とこ ろ; 2 つ の 因了が得られ; ひ とっ は 「活性化」 の 因子で; もうひ とっは 「'り aLZ< 快」 の 因子で あるこ とが分か っ た.,基礎心理学研究,1998,*
顔情報処理のための共通プラットホームの構築,八木康史， 森島繁生， 金子正秀， 原島博， 谷内田正彦， 原文雄， 橋本周司,顔画像処理に対する様々な分野での関心の高まりや; 工学分野における顔画像処理技術の研究成果の蓄積を背景にして; 顔画像処理に関する共通ソフトウェアのツールの作成に向けた活動が進められている. この活動は;「感性擬人化エージェントのための顔情報処理システムの開発」(略称;アドバンストエージェントプロジェクト) と呼ばれ; 情報処理振興技術協会 (IPA) における独創的情報技術育成事業に関わる開発テーマの一つとして; 平成 7 年度より 3 年間の計画で精力的に活動を行ってきた. 擬人化エージェント技術はさまざまな技術要素から構成されているが;本プロジェクトでは; この中で特に 「顔」 の役割に着目し; 顔画像の認識・合成に関わる顔情報処理システムの開発に主眼をおいた. これと同時に; 本システムでは工学のみならず心理学や医学などの分野も含めた顔関連分野における共通の実験用ツールを広く提供することも目標としている.本稿では; 平成 10 年 3 月で終了するこのプロジェクトの概要と; 共通ソフトウェアの紹介を行う.,情報処理学会研究報告コンピュータビジョンとイメージメディア (CVIM),1998,*
解剖学に基づいた顔面筋肉モデルによる顔表情合成 (< 特集>-ヒューマンコミュニケーショングループ大会-「ヒューマンコミュニケーション基礎一般」),世良元， 井土哲也， 森島繁生,抄録 近年; コンピュータグラフィックスにより顔表情を合成する手法が注目を集めている.最も効果的で自然なモデリング手法として顔面筋肉モデルがあげられる. 顔面筋肉モデルでは「筋肉の収縮によって皮膚組織が移動して; 顔表情が表出される」 という; 実際の表情表出過程をシミュレーションすることによって顔表情を合成する手法である. つまり顔面筋肉モデルでは;このシミュレーションの正確さによって合成画像の自然さが決定されるといえる. 本研究では;頭部の骨と筋肉の解剖学的形状および構造を正確に再現したモデルを新たに構築した.骨の構造再現およびその整合手法の提案により; 従来のモデルでは実現できなかった顎の運動の正確な再現が可能となった. さらに自然な表情合成を行なうために皺の再現も行なっている.この新たに構築した顔面モデルを用いて; FACS に基づいた表情編集ツールを開発した.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1998,*
サイバースペース上での多人数コミュニケーションシステム,四倉達夫， 藤井英史， 森島繁生,(2) 音声分析プロ セ ス・パ ラ メ ー タ変換プロ セ ス (Server) 各 Client か ら送 られ て きた自然音声 を音声分析 プロ セ ス で線形 予灘分析 (LPC 分析) を使 い; 各 PIayer の 1. i) C ケ プスト ラ ム を算出 し; また感情推定 の ため に ピ ッ チ分析 も併せ て 行 う. 次に パ ラ メ ータ変換 プ ロセ ス で ニ ュ H,電子情報通信学会総合大会講演論文集,1998,*
感情を理解する実時間インタラクションシステムの構築,小林智典， 藤井英史， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会総合大会講演論文集,1998,*
オプティカルフローを用いた顔面筋パラメータの自動推定,矢崎和彦， 石川貴博， 森島繁生,顔 面 筋 肉モ デ ル は モ デ ル 化 され た 皮 膚組 織 及 び表情筋 を持ち筋肉 が 皮膚 組織 に 与え る 影響力を物理 的 に 計算 し; 皮 膚組織 を変形させ る こ と;: よ っ て 表 情表出が 可能で あるc 表情 を決定する パ ラ メ ー タは 筋肉が 収縮す る 力 [: 筋肉パ ラ メ ー一タ; で ある. 筋肉パ ラメータ か ら表 情へ の 変換 は力の 重ね 合わ せに よ っ て 行われ る が; 特定の 顔表情に 対応す る筋 肉パ ラ メ ータ の 決定は 試行錯誤的に 行う必 要が あ v た. そ こ で; 石川 は正 面顔画 像から顔 に 貼 り付けた マ,電子情報通信学会総合大会講演論文集,1998,*
解剖学的構造を考慮した顔面筋肉モデルの構築,世良元， 森島繁生,近年; コ ン ピュ ータ グラ フ ィ ッ クス CCG) 技術 に よっ て 仮想 さ らに顎の 開閉運動 に つ い ての 改 良も行 なっ た. 顎の 開閉人物像 を構築す る研 究 が盛 ん に 行わ れて お り; その た めの表 運動 は 通 常; 顎関節 を軸と した回転 運 動 と して 再現 され る こ情合 成手法が数多 く提案され て い る. そ れ らの 中の,電子情報通信学会総合大会講演論文集,1998,*
房モデルを用いた GUI ヘアスタイルデザインシステム,岸啓補， 三枝太， 森島繁生,を決定す る.(b) 房モ デ ル の 編集図 1 の 様 な房モ デ ル の 形 状制御四 角形 を移動; 回転; 変形させ て様々 な房モ デ ル を編 集す る. 3 次 B 一ス プラ イン 曲線の 制御点の 数は 7 点 が最低であるが; 制御点 つ まり形 状制御四 角形 の 数 を増 やす事 よ り複雑な表現が 可,電子情報通信学会総合大会講演論文集,1998,*
リアルな頭髪アニメーションの生成と頭部との自動衝突判定,今村顕， 三枝太， 森島繁生,タグ ラフ イ ッ クス に よる人物画像の 合成が様 々 な分野 で 行 わ れ て お り; よ りリア ルな人物の 合成画像が 求め られ て い る. 人物頭部領域 に お いて は; こ れ まで に 頭髪 を 「空間曲線」 に よ っ て 近似 し;「剛体セ グメ ン トモ デ ル」 を用 い て運動を制御す る方法を考案してきた. こ れ は頭髪を空間 曲線に よ っ て 近似する こ と に よ り; 頭髪の 複雑な形状 を少 ない デ,電子情報通信学会総合大会講演論文集,1998,*
印象語に基づく表現合成,武山聡史， 藤井英史， 森島繁生,悲 しみ) に つ い の 表情合成が行 なわれ て きた. した が っ て; 優 しい; 激 しい な ど とい っ た感性 的 に 表現 され た表情の 生成 は な さ れ て い な い. 実 際の 入間の 表情 に は 様々 な ものが ある ため; よ り豊か な表情 を作成する 必要 が あ る. そ こ で 本研究で は; AU(ActionUnit]/の組み 合 わせ に よ り様 々 な表情 を作成 し; その 表情か ら受け る印象に つ い て主観評価実験 を行 っ た. そ して; 表情を構成する AU と印象諳 との 関係 を検討 し; 印象 fi 吾に基づ く表情合成を行な っ た.,電子情報通信学会総合大会講演論文集,1998,*
サイバースペース上での多人数コミュニケーションシステム,四倉達夫， 藤井英史， 森島繁生,(2) 音声分析プロ セ ス・パ ラ メ ー タ変換プロ セ ス (Server) 各 Client か ら送 られ て きた自然音声 を音声分析 プロ セ ス で線形 予灘分析 (LPC 分析) を使 い; 各 PIayer の 1. i) C ケ プスト ラ ム を算出 し; また感情推定 の ため に ピ ッ チ分析 も併せ て 行 う. 次に パ ラ メ ータ変換 プ ロセ ス で ニ ュ H,電子情報通信学会総合大会講演論文集,1998,*
物理モデルを用いた表情合成方法の構築および皺の表現,井土哲也， 世良元， 森島繁生,デ ル 化 し た筋 肉で 構成 され る. 各節 点 と各層はバ ネ で つ ないで あ り; 筋膜 と骨の 間に 筋肉が 挿入 されてい る. この 筋 肉の収縮 す る力; い わ ゆ る筋 肉パ ラメ ータ に よ っ て 筋膜 と表皮が変形しモ デ ル の 表情が変化す る b こ の 筋肉モ デ ル は; 解剖解,電子情報通信学会総合大会講演論文集,1998,*
房モデルによるヘアスタイルデザインシステムの開発,岸啓補， 三枝太， 森島繁生,抄録 サイバースペースにおける仮想人物の合成やコミュニケーションシステムの実現にむけ;コンピュータグラフィクスによる人物画像合成等が注目を集めている. 本稿では; 特に人物の CGの中でも合成が難しいとされる頭髪の表現方法について述べる. 人物画像において頭髪は視覚的に重要であるにも関わらず; 簡単な曲面や背景の一部で代用されることが多い.頭髪を一つの物体として扱い; マッピング技術を用いて表現する手法が成果をあげているが運動の表現は不可能である. そこで頭髪をテクスチャを用いずに空間曲線を用いて作成する.頭髪の部分的な集まりである房をモデル化することで簡略化したヘアスタイルデザインシステムを提案し; 房をモデル化する手法; レンダリング手法について述べ; 実際にこのヘアスタイルデザインシステムを用いて作成した頭髪画像を示す.,映像情報メディア学会技術報告 22.12,1998,*
音声による実時間口形・表情制御可能なサイバースペース上での仮想人物の実現,四倉達夫， 藤井英史， 小林智典， 森島繁生,抄録 コンピュータの発展に伴い; 近年コンピュータ上にサイバースペースを作り出し;あたかもその環境に没入する感覚を生み出させるバーチャルリアリティ技術が急速に発展してきた.このサイバースペースは仮想空間をより実空間に近づけ; そこで人間同士のコミュニケーションをリアルに実現することを一つの目標としている. そこで; 自分自身を投影した人間同様の顔を持つ分身としてのエージェント (Avatar: アバタ) を仮想空間上に生成し; マイクから入力された自然音声の分析から会話時の口形状および表情の推定を行って; リアルタイムにアバタの表情合成を行うシステムを提案する. このシステムによりネットワークを通して伝送された音声に同期してアバタの表情が変形し; サイバースペース上で多人数間での Face-to-Face の対話が可能となった.,映像情報メディア学会技術報告 22.12,1998,*
The Fifteen Seconds of Fame: 視聴者参加型インタラクティブ映画の提案,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,フォーラム顔学'98 第 3 回日本顔学会大会予稿集,1998,*
コンピュ-タヒュ-マンインタラクションのための人物表情の合成・認識技術 (技術解説),森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,成蹊大学工学研究報告,1998,*
3D Facial Expression Tracking and Regeneration from Single Camera Image Based on Muscle Constraint Face Model,S Morishima,*,INTERNATIONAL ARCHIVES OF PHOTOGRAMMETRY AND REMOTE SENSING,1998,*
臨場感通信臨場感通信; 1994,長谷川修， 森島繁生， 金子正秀,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会論文誌. A; 基礎・境界,1997,*
乱れた積層構造 YIG 薄膜の磁気光学効果の理論解析,村田佳洋， 柴田直樹， 伊藤実， 岩澤昭一郎， 海老原一之， 大谷淳， 中津良平， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,映像情報メディア学会誌,1997,*
感情音声の印象に関する主観評価実験,佐藤順， 森島繁生,抄録 我々は音声に含まれる感情情報の分析; およびニューラルネットワークを用いた感情情報のモデル化の試みを行っている. しかし; 現在のところ決定的な感情情報のモデル化の方法は見いだせていない. そこで; 人間が音声に込められた感情を理解するメカニズムを解明する手がかりを得るために; 感情が込められた音声の主観評価実験を行った. 本研究では; まず音声資料をテレビドラマから収集し; これらの音声資料に感情が込められているか; またどのような感情であるかを調べるために聴取実験を行った. 次に SD 法による主観評価実験で用いる形容詞をサンプリングし;これらが感情を込めた音声を評価するのに妥当であるか調べるために評価実験を行った. 最後に;SD 法による主観評価実験を行った. なお; 全ての刺激の提示と回答は HTML ブラウザを用い;被験者がコンピュータを直接操作して行った. 本稿では; 上述の評価実験の実験方法と;結果に対して因子分析を行い; 得られた知見について報告する.,電子情報通信学会ソサイエティ大会講演論文集,1997,*
頭髪の質感および運動の表現,三枝太， 森島繁生,抄録 人物の画像の中でも特に CG による合成が難しい頭髪を表現する手法について述べる.これまでに; 頭髪を 「空間曲線」 によって近似し; 空間曲線を極めて細い円筒形チューブである仮定し; 曲線上の任意の点における法線ベクトルを計算することで空間曲線のレンダリングを可能にする手法や;「剛体セグメントモデル」 を用いて頭髪の運動を制御する手法を提案してきた. 本稿では;ディスプレイの数倍の解像度を持つイメージバッファを用いたアンチエイリアシング手法について述べ; 従来では考慮されていなかったセグメント相互の影響を考慮した運動モデルを提案する.,電子情報通信学会ソサイエティ大会講演論文集,1997,*
空間周波数を使用した実時間顔表情認識,近藤淳， 森島繁生,抄録 人間とコンピュータとの自然なコミュニケーションを実現するため; 顔画像の解析により顔表情の認識を行う. 顔表情認識には空間周波数の有効性が指摘されている. それにより得られた特徴をもとに顔表情を基本 6 表情 (怒り; 嫌悪; 恐れ; 喜び; 悲しみ; 驚き) に分類する. 本研究では;実時間顔表情認識システムを提案する. 各表情の特徴を取得するため; 表情変化の際に大きな形状変化をする目; 口領域を抽出する. 各領域を空間周波数領域に変換し; その帯域分割を行い;各帯域における周波数特徴を取得する. その特徴をもとに; 顔表情認識を行う.,電子情報通信学会ソサイエティ大会講演論文集,1997,*
30-2 2 次元マーカ移動量からの顔面筋パラメータ自動推定,石川貴博， 世良元， 森島繁生,抄録 コンピュータ上で顔表情を合成するモデルの 1 つとして顔面筋肉モデルがある.顔面筋肉モデルは; モデル化された皮膚組織及び表情筋を持ち筋肉が皮膚組織に与える影響力を計算し; 皮膚組織を変形させることによって; 表情表出が可能である. 表情を決定するパラメータは;筋肉が収縮する力 (筋肉パラメータ) である. 筋肉パラメータから顔表情への変換は;力の重ね合わせによって行われるが; 特定の表情に対応する筋肉パラメータの決定は;試行錯誤的に行う必要があった. しかし; 本稿では 1 台のカメラで正面から撮影した顔面上のマーカの 2 次元移動量から筋肉パラメータの自動推定を行った. これは同時に筋肉モデルという制約下のもとで正面画像のみから得られる 2 次元のマーカ情報から 3 次元の表情をモーションキャプチャすることに相当している.,映像情報メディア学会年次大会講演予稿集 1997,1997,*
6-2 3 次元モデルを用いた発話アニメーションの作成,藤井英史， 宮下直也， 森島繁生,抄録 コンピュータ上で表情や口形状の動きを表現する方法として; 3 次元モデルを用いている.口形状に関しては; 各制御点に各口形のパラメータ値を定め; 口形状の動きを表現している.表情に関しては; Action Unit (AU) と呼ばれる 44 種類の顔各部の動きの単位を組み合わせることにより; 表情の表出を行っている. そこで; 本稿では口形状の動きを自然に表現するために; 2台のカメラを用いて顔面上のマーカの移動量を求め; 各口形パラメータ値を決定した.このパラメータを用いて各口形を合成し; それを滑らかに表現するアニメーション作成システムの構築を行った.,映像情報メディア学会年次大会講演予稿集 1997,1997,*
音響学大辞典音響学大辞典; 1976,佐藤順， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
現代形容詞用法辞典現代形容詞用法辞典; 1991,佐藤順， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
続 多変量解析法続 多変量解析法; 1976,佐藤順， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
センソリー エバリュエーション-官能検査へのいざない-センソリー エバリュエーション-官能検査へのいざない-; 1989,佐藤順， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
グループウェアと CSCW: 第 1 回 その概要と組織活動の支援,石井裕， 葛岡英明， 宗森純， 五郎丸秀樹， 長澤庸二， 長谷川修， 森島繁生， 金子正秀， 森島敏生， 安達一寿， 綿井雅康， 中尾茂子， 石出勉， 三木紘武， 森下隆広， 中尾健,鉄道車両規格データベースシステム構築における EDMS/ASP システム構築法とその適用・評価:企業間コラボレーションのための知識情報システム構築,日本ロボット学会誌,1997,*
口内解剖学口内解剖学; 1986,石川貴博， 世良元， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
ニューラルネットに基づくマーカ移動量からの顔面筋パラメータの推定,石川貴博， 世良元， 森島繁生,抄録 コンピュータ上で顔表情を表現する手法の 1 つである顔面筋肉モデル^ lt [1][2][3] gt は;モデル化された皮膚組織及び表情筋を持ち筋肉が皮膚組織に与える影響力を計算し;皮膚組織を変形させることによって; 表情表出が可能である. 表情を決定するパラメータは;筋肉が収縮する力 (筋肉パラメータ) である. 筋肉パラメータから顔表情への変換は;力の重ね合わせによって行われるが; 特定の表情に対応する筋肉パラメータの決定は;試行錯誤的に行う必要があった. しかし; 本稿では 1 台のカメラで撮影した顔面上のマーカの 2次元移動量から筋肉パラメータの自動推定を行った. これは同時に筋肉モデルという制約下のもとで正面画像のみから得られる 2 次元のマーカ情報から 3 次元の表情をモーションキャプチャすることに相当している.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
熱画像を用いた人体姿勢の実時間推定の検討,岩澤昭一郎， 海老原一之， 大谷淳， 森島繁生,抄録 本報告では赤外線カメラにより獲得される熱画像を用いて; 背景・照明条件に対して実時間で人物姿勢を推定する手法を提案する. 熱画像への閾値処理により得られる人物領域に距離変換を施すことにより全身像の重心を求め; 上半身の傾きを得た後; 頭頂・手先・足先の各特徴部位を求める. さらに; これら特徴部位から遺伝的アルゴリズムを用いて肘と膝の位置を学習的に推定する.本手法は非接触方式であり; 任意の人物に対して適用が可能である.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1997,*
実時間インタラクションシステムの構築,宮下直也， 佐藤昌代， 森島繁生,抄録 人間同様の顔を持つエージェント (擬人化エージェント) がヒューマンインタフェース分野のホットなトピックとなっている. これは; あたかも人と人とが直接; 接しているような高度な現実感を持った環境を実現することが要求される. その第一の条件としてエージェントが作り物であることをユーザに意識させない; 自然な顔表情合成と実時間での音声との同期表示が挙げられる.このような環境の実現に向けて; マイクから入力された; あるいは記録された自然音声の分析に基づいて会話時の口唇の動き; および表情をリアルタイムに合成するリアルタイムメディア変換システムを提案する. 本稿では; ユーザとエージェントとのインタラクション [1] を実現するプロトタイプシステムについて報告する.,電子情報通信学会総合大会講演論文集,1997,*
3 次元モデルを用いた口形状の制御,藤井英史， 宮下直也， 森島繁生,抄録 コンピュータと人間とのコミュニケーションを円滑に行うには; 人と人が直接対話しているような環境を実現することが理想である. このためには; 画像と音声の同期がとれることはもちろんのこと;発話時の口の動きを自然に表現することが求められる. 現在; 実時間メディア変換システム [1]に用いられている口形動作には違和感があるため; その口形動作のクオリティの向上が必要である.本稿では; 日本語と英語の基本口形を実測に基づいて作成し; これらの時間方向の補間により自然なアニメーションを実現した.,電子情報通信学会総合大会講演論文集,1997,*
実計測に基づく頭髪の運動表現,近藤淳， 三枝太， 森島繁生,抄録 現在; コンピュータグラフィックスによる人物画像の合成が; さまざまな分野で行なわれており;よりリアルな人物の合成画像が求められている. 人物頭部領域においては; これまでに頭髪を「空間曲線」 によって近似し;「剛体セグメントモデル」 を用いて運動を制御する方法を提案してきた.しかし従来の方法は頭髪の運動を統括的に制御せず; 部分的なセグメントの運動を考えているため時間の経過と共に運動に破綻をきたすといった問題があった. そこで運動を統括的に制御するため糸状物体の運動の実計測を行ない; それを運動アニメーションに反映させることによって;よりリアルな頭髪の運動を表現する.,電子情報通信学会総合大会講演論文集,1997,*
顔画像の空間周波数特徴を用いた実時間表情合成,酒井典子， 森島繁生,抄録 人間とコンピュータとのインタラクティブな対話を実現するための環境の構築を目指している.すでに筆者らの提案した表情分析・合成システムでは; 合成画像の顔器官上の特徴点の移動量を表情パラメータに変換し; 顔表情を合成している [1]. しかし; 静止画像をターゲットとしていた点や表情表出過程における度合いの弱い表情の分析が難しいといった問題点があった. そこで; 本論文では;無表情から表情表出までの顔動画像を合成する方法を提案する. 分析対象となる実動画像の領域抽出および領域追跡を画像の 2 値化と加算投影により行い; 顔表情分析は高速フーリエ変換 (FFT)とニューラルネットを用いて特徴点の移動量を推定し; 表情合成を行う.,電子情報通信学会総合大会講演論文集,1997,*
熱画像からの人体の姿勢推定の高度化の検討,岩澤昭一郎， 海老原一之， 大谷淳， 森島繁生,抄録 筆者らは既に; 熱画像を用いた非接触な人物の姿勢推定手法を提案している. しかし;筆者らの従来法では; 上半身の左右への大幅な傾きや; 足の広範な動作に対応できず;また単眼のため 3 次元情報が得られないなどの課題があった. 本報告ではより多くの姿勢に対応できるよう; 従来の単眼用アルゴリズムを改良する. 即ち単眼赤外線カメラから入力される熱画像から獲得される人物領域と; その輪郭の情報に基づいてヒューリスティックに頭頂・手先・足先の各位置を実時間で検出する手法; および遺伝的アルゴリズム (GA) を利用して肘および膝の位置を推定する手法を提案する. さらに; ステレオ視による 3 次元位置の獲得について検討する.,電子情報通信学会総合大会講演論文集,1997,*
正面顔画像のマーカ移動量からの顔面筋パラメータの自動推定,石川貴博， 世良元， 森島繁生,抄録 顔面筋肉モデル [1] は; モデル化された皮膚組織及び表情筋を持ち筋肉が皮膚組織に与える影響力を計算し; 皮膚組織を変形させることによって; 表情表出が可能である. 表情を決定するパラメータは; 筋肉の力 (筋肉パラメータ) である. 筋肉のパラメータから顔表情への変換は;力の重ね合わせによって行われるが; 特定の表情に対応する筋肉パラメータの決定は;試行錯誤的に行う必要があった. が; 本稿では 1 台のカメラで撮影した顔面上のマーカの 2次元移動量から筋肉パラメータを自動推定する試みを行った. これは同時に筋肉モデルという制約下のもとで正面画像のみから得られる 2 次元のマーカ情報から 3 次元の表情のモーションキャプチャをすることに相当している.,電子情報通信学会総合大会講演論文集,1997,*
英語発声のための筋肉モデルによる口形制御,関矢正樹， 世良元， 森島繁生,抄録 筋肉モデル [1] を用いて口形を作成する研究を進めている. 今まで日本語の口形が作成されてきた [2]. しかし; 従来のままでは英語特有の口形 ([f];[v];[θ];[δ]) の表現が出来ない.また日本語よりも英語の方が; より多くの口形があるために; より幅広い筋肉の制御レンジが必要となる. そこで; モデルの筋肉の形状; 顎の制御の改良を行い; モデルに舌を加えた.また現実感のある画像の作成のために; 口内のモデルを付加した.,電子情報通信学会総合大会講演論文集,1997,*
画像処理技術の基本を学ぶ (第 3 回) 顔画像の処理技術,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,画像ラボ,1997,*
人体と顔の画像処理 4. 応用 4‐5 顔画像によるアドバンストエージェント,金子正秀， 森島繁生,「顔」 は人 間 に とって非 常 に身近 な存 在 で あ り; 様々な情報を担 っている.「顔」 によって表現される情報 としては; ま ず;「顔」 の持 ち主で ある一人一人の人間の個人情報 (誰 であるか; 性 別;年 齢等) が 挙げられ る. 一 方;「顔」 に様々な形で現れる; 心 理状態等の人間の内面に関わる情報も重要である. 後 者 はコミュニケーションにおいて; 言 語情報 とは異なる非言語的な独自の情報を担 ってお り; 感 性的 な; ま た; より多様 なコミュニケーションを行う上で重要な役割を果た している. このような観点から; 人 間が計算機や コミュニケーションメディアを利用する際 に; 人 間を相手にす るのと同じような形態で情報のや り取 りを行 うことを可能にするユーザインタフェース として; 計 算機や通信端末上 に擬人化されたエージェントを用意することが検討 されてきている 1). 擬 人化 されたエージェン トの役割 としては; ユ ーザインタフェースとしてユーザ とシステムとの対話の円滑化 を図 り; ま た; ユ ーザにシステムに対 …,映像情報メディア学会誌,1997,*
ヘアスタイルモデリングツールの開発,酒井文騰， 三枝， 森島繁生,抄録 自然物体の形状; 質感; 及び運動表現は複雑であり; この中で特に頭髪などの細く;柔らかい糸状物体は CG による表現は困難とされ確立された表現方法はない. すでに;頭部モデルへのインタラクティブな頭髪の生成方法について報告しているが; 固定された頭髪生成領域や; 2 枚の編集画面による間接的な頭髪の生成方法など; まだ忠実にイメージどうりのヘアスタイルを表現するのは容易ではなかった. 本稿ではこの問題点を考慮し; マウスを用いて;より複雑なヘアスタイルを簡単にデザインできるモデリングツールを開発した.,信学'97 春大; 03,1997,*
8) モデルフィッティングのための正面顔画像からの特徴点自動抽出 ([マルチメディア情報処理研究会 ネットワーク映像メディア研究会] 合同),岩澤昭一郎， 森島繁生,ィ ア の 暗号化を要求さ れ て い る. そ こ で; これ ら の デー タを暗号化す る専用 LSI を開発した.暗号ア ル ゴ リズ ム は産業標準で ある DES. トリプル DES を採用 した. 本稿では; マ ル チメ ディアデ,テレビジョン学会誌,1996,*
9) 空間周波数を利用した実時間顔表情認識 ([マルチメディア情報処理研究会 ネットワーク映像メディア研究会] 合同,坂口竜己， 森島繁生,ィ ア の 暗号化を要求さ れ て い る. そ こ で; これ ら の デー タを暗号化す る専用 LSI を開発した.暗号ア ル ゴ リズ ム は産業標準で ある DES. トリプル DES を採用 した. 本稿では; マ ル チメ ディアデ,テレビジョン学会誌,1996,*
A face to face communication using real-time media conversion system,N Miyashita; T Sakaguchi; S Morishima,User friendly human interfaces have received great attention recently. Our goal is to realizea natural human-machine communication environment by giving a face to the computerterminal or communication system. In order to construct such an interface; a real synthesisedimage in real-time is needed. In this paper; we develop a real-time media conversion systemand examine the communication between the user and a virtual agent with a human face onthe computer monitor.,Robot and Human Communication; 1996.; 5th IEEE International Workshop on,1996,*
動画像からの実時間表情認識,坂口竜己， 森島繁生,抄録 人物顔表情の認識は; 心理学や工学など様々な分野での応用を期待され;多くの研究が成されてきている. しかしそのほとんどは; 認識の精度と計算量とのトレードオフにより;実時間での処理は困難であった. 最近ではトランスピュータや高速なワークステーションを使った実時間認識の例も見られるようになってはいるが; ハードウェア規模の巨大化が問題である. 本稿では;比較的低速なワークステーションや PC 上での動作を前提とした実時間表情認識システムを提案する. 顔の領域抽出および領域追跡は; 単純な差分画像により行い; 表情認識は離散コサイン変換とニューラルネットワークにより行う. 特定個人の 4 表情認識では 90% 以上の精度が得られることが確認されている.,電子情報通信学会ソサイエティ大会講演論文集,1996,*
流体場解析における頭髪の運動表現,三枝太， 森島繁生,抄録 擬人化エージェントの実現に向けて人物像のリアルな画像合成の検討を行っている.特に人物頭部領域においては; これまでに; 頭髪を 「空間曲線」 によって近似し;「剛体セグメントモデル」 を用いて運動を制御する手法を提案してきた. これは頭髪を空間曲腺によって近似することにより; 頭髪の複雑な形状を少ないデータで表現することを可能にし; 剛体セグメントモデルにより;簡単な数値計算で空間曲線の運動を可能にした. 本稿では; 頭髪の運動を表現するのに流体力学の理論を基盤に少ない演算量で近似的に流体場を求める手法を提案し; その手法による運動アニメーションの評価を行う.,電子情報通信学会ソサイエティ大会講演論文集,1996,*
顔動画像からの周波数特性解析による表情認識,小島孝治， 坂口竜己， 森島繁生,抄録 顔表情の認識はソフトウェアエージェント等によって実現される仮想人物とユーザである人間とコンピュータの自然なコミュニケーションを実現するために重要な技術である. これまでの顔表情認識の研究においては静止画像を用いたものが多かったが; 対象を動画像とすることにより表情変化の過程を考慮することが可能となり; より正確な認識ができると思われる. 本稿では顔画像を空間周波数成分で表現し; 表情変化の過程におけるこの周波数成分の変化を求める.実際に表情認識を行う段階では; 表情表出過程で主に変化する顔部位 (口と目) を抽出し;その周波数成分の時間的変化を学習パターンと比較することで結果的にどの表情であるかを識別する. 表情を画像で判断する場合個人差が大きく生じるので; まず最初にその人の基本 6 表情 (怒り;嫌悪; 恐れ; 喜び; 悲しみ; 驚き) を学習パターンとして用意し; 新たに入力された画像がそのどれに近いかを識別する方法を採用した.,電子情報通信学会総合大会講演論文集,1996,*
3 台のカメラによるリモートセンシング画像からの標高測定,安藤裕幸， 安藤真， 森島繁生,抄録 リモートセンシングデータの利用法の 1 つとして地形図作成があげられる.現状ではマッチングをすべて自動で行うことはできず; 人間の手作業によるマッチングを行うことにより標高測定をしている. そこで; ここでは 3 方向のセンサデータとそれらの時間軸正規化法としてDP マッチングを用いることによりリモートセンシングデータから標高測定を行う手法を提案した.ここで 3 方向カメラを用いたのはオクルージョン対策であり; 前方視一直下視と;後方視一直下視とのスイッチングにより; 正しい高度を求められるように改良した.,電子情報通信学会総合大会講演論文集,1996,*
陰影を考慮した頭髪の表現に関する一検討,安藤真， 森島繁生,抄録 筆者らは; 人物画像の中の頭髪に着目し; 頭髪全体をモデル化することで;従来のようなテクスチャでは困難であった頭髪表面のハイライトの変化や外力による頭髪の運動などを表現した. しかしながら頭髪のレングリングは Z バッファを用いた局所照明アルゴリズムであるため; このままでは陰影を表現することができない. 手軽に陰影を表現する手法としてはレイ・トレーシングなどの大域照明アルゴリズムがあるが; 頭髪のように物体の数が極めて多い場合には;計算コストが膨大になり実用的ではない. そこで本稿では; より少ない計算時間で陰影を含む頭髪のレングリングを行うために; 陰影バッファを用いたアルゴリズムを提案する.,電子情報通信学会総合大会講演論文集,1996,*
新しい表情記述パラメータ (直交化 FACS) の提案とその表情分析・合成への応用,松川和正， 川上丈雄， 森島繁生， 山田寛,抄録 表情からの感性情報を扱うことのできるコンピュータと人とのコミュニケーションを実現する環境の構築を目指している. これまで; 顔表晴を記述するのに FACS (Facil Action Coding System)を拡張し用いていたが; これらの運動単位 (Action Unit) には相関があり顔特徴点の移動量からパラメータを一意に求めることは容易ではない. そこで; 本論文では FACS に基づいた新たな互いに直交化された表情記述パラメータを提案する. このパラメータから; 表情の再合成も可能であり;更に顔に表出される感情状態も定量的に記述可能である.,電子情報通信学会総合大会講演論文集,1996,*
物理法則に基づいた顔モデルによる口形状の表現,世良元， 岩澤昭一郎， 森島繁生,抄録 筋肉モデルを用いて人の顔表情を作成する研究を進めている. しかし; 人の会話時の口形を作成するには現在の物理モデルは筋肉の本数; 形状が適していない. そこで新たに筋肉の種類;形状の改良を行い会話時の口形の作成を可能とした. また現実感のある画像の作成のために; 歯;目; まぶたなど顔の他の部分のモデル化が行われている.,電子情報通信学会総合大会講演論文集,1996,*
表情の三次元計測に基づく顔画像合成規則の検討,高橋成晴， 坂口竜己， 森島繁生,抄録 筆者らはモデルに基づいた顔表情画像合成のために顔に 49 点の測定用マーカを張りつけた被験者を; 直交した正面と側面から同期した 2 台のカメラと VTR で撮影し; 表情表出時のマーカの移動を追跡することによって 3 次元的な移動量を求めた. 従来; 筆者らが用いている FACS表情合成規則は; 平面的であり経験的に移動量を求めたものであった. これに対し実画像から測定することによって 3 次元的変形を捉えることが出来るため; より自然な顔画像の合成が可能となった.,電子情報通信学会総合大会講演論文集,1996,*
仮想人物とユーザの対話を実現するための音声から画像への実時間メディア変換システム,宮下直也， 岩澤昭一郎， 坂口竜己， 森島繁生,抄録 人物の表情や会話シーンなどのリアルな顔画像を合成する研究が; 近年盛んに行なわれている. テレビ会議や臨場感通信; 知的インタフェースの表示部への応用を考慮して; 顔画像の合成をリアルタイムに実行する研究を進めている. より人に優しいインタフェースを構築するためには;よりリアルな画像をリアルタイムに合成することが必要である. 本稿では; この実時間メディア変換の技術を用いて; 仮想人物 (VirtualAgent-VA; 現時点では Agent を操るのは人間)とユーザとの対話をコンピュータのディスプレイ上で行うシステムについて述べる.,電子情報通信学会総合大会講演論文集,1996,*
顔動画像からの周波数特性解析による表情認識,小島孝治， 坂口竜己， 森島繁生,抄録 顔表情の認識はソフトウェアエージェント等によって実現される仮想人物とユーザである人間とコンピュータの自然なコミュニケーションを実現するために重要な技術である. これまでの顔表情認識の研究においては静止画像を用いたものが多かったが; 対象を動画像とすることにより表情変化の過程を考慮することが可能となり; より正確な認識ができると思われる. 本稿では顔画像を空間周波数成分で表現し; 表情変化の過程におけるこの周波数成分の変化を求める.実際に表情認識を行う段階では; 表情表出過程で主に変化する顔部位 (口と目) を抽出し;その周波数成分の時間的変化を学習パターンと比較することで結果的にどの表情であるかを識別する. 表情を画像で判断する場合個人差が大きく生じるので; まず最初にその人の基本 6 表情 (怒り;嫌悪; 恐れ; 喜び; 悲しみ; 驚き) を学習パターンとして用意し; 新たに入力された画像がそのどれに近いかを識別する方法を採用した.,電子情報通信学会総合大会講演論文集,1996,*
顔モデルにおける口唇形状動作の改善,川原光貴， 岩澤昭一郎， 森島繁生,抄録 筆者らはコンピュータシステムと利用者の間でより円滑なコミュニケーションを実現するために;伝達される情報の半分以上が顔表情に基づくともいわれる人間同士のコミュニケーションに習いインタフェースシステムに顔やその表情を利用することを考えている. 顔を用いた対話型インタフェースシステムにとって; あたかも実在の人物と対面しているという現実感を与えることが一つの理想であろう. このような高度な対話の現実感を得るためには; 現実に忠実な発話時の口唇形状を再現できることが求められる. 現在ある実時間メディア変換システムにおける顔モデルの口唇形状動作は正確とは言えず; システムと対話時に違和感を生じる. この違和感を取り除く為には新たな口唇動作を定義することが求められる. 本稿では 3 次元計測に基づく新たな口唇形状動作のルール定義について述べる.,電子情報通信学会総合大会講演論文集,1996,*
ダイナミックスモデルに基づく自然な頭髪アニメーション,安藤真， 三枝太， 松坂秀治， 森島繁生,抄録 現在さまざまな分野でコンピュータグラフィックスによる人物の画像が用いられている.本稿では; これら人体頭部の画像の中でも特に CG による合成が難しい対象である頭髪を表現する手法について述べる. 筆者らは; 既に頭髪を空間曲線で近似することで形状データの容量を大幅に削減し; また剛体セグメントモデルによる連動制御を可能にした. 本稿では; この連動制御方法をさらに現実の連動に即したモデルで記述することで; 頭髪の動きをより自然なものにした.また新しい衝突判定アルゴリズムを用いて; 高速な衝突処理を行った. 加えて; 従来は考慮されていなかった頭髪による陰影を高速に実現するための新たなアルゴリズムについても提案する.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1996,*
仮想人物との対話を実現するための音声から画像への実時間メディア変換システムの研究,宮下直也， 佐藤順， 坂口竜己， 森島繁生,抄録 ヒューマンインタフェースにおける擬人化エージェントの姿形を実現するため;あるいは知的通信の受信側の表示部として人物の表情や会話シーンの合成について研究を進めている. 擬人化エージェントとの対応では; あたかも人と人とか直接接しているような高度な現実感をもった環境を実現することが目標であるが; これには; 画像と音声とが同期し; 相手が真の人物であると思わせるほどの自然な画像合成と実時間での画像表示が必要不可欠である. 本稿では;このような擬人化エージェントの実現に向けて; 実用に近いプロトタイプシステムを構築することを目指す. 人物の顔形状に忠実な 3 次元モデルを導入して; 実際に人物が会話している音声を入力し;この音声の分析結果から口の形状を推定して; リアルタイムに表情・口形合成するシステムについて述べる. またこの実時間メディア変換の技術を用いて仮想人物 (virtual agent; 現時点で agentを操るのは人間) とユーザとの対話をディスプレイ上で行うシステムについても述べる. 最後に …,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,1996,*
顔面筋の物理モデルに基づく会話時の口形状の制御,世良元， 岩澤昭一郎， 森島繁生,抄録 ヒューマンインタフェースにおける擬人化エージェントの実現やエンタティメント映像生成に向けCG による人物像の生成が望まれている. 本稿では現実感の高い人の顔を表現することを目的として物理法則に基づく筋肉モデルを提案する. 一方; 人の表情を作成する研究は数多く見られるが;会話時の口形状に対しての研究は少ない. 特に自然な会話のアニメーションの合成のため;口形状の表現に適した筋肉の種類と形状の改良を行った. また; 実画像からの測定結果に基づき口形状の作成を行った. また音韻継続時間を考慮に入れ; 音と同期したアニメーションを生成した.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,1996,*
仮想現実的顔画像処理システムを用いた顔面表情知覚の精神物理学的研究特に基本表情の強度と感覚量の関係について,山田寛， 中村宏信， 森島繁生， 原島博,抄録 近年のめざましい顔画像処理技術の発展により; 現実的な顔を用いた表情知覚の研究にも;厳密な精神物理学的測定法を適用することが可能になってきた. 本研究では; その第一歩として;顔面表情の強度とそれに対する人間の感覚量の関係を調べるための 3 つの精神物理学的実験を試みた. 実験 1 では 『驚き』 と 『怒り』 の 2 表情を取り上げ; 5 つの異なる表情強度における弁別閾を測定した. 実験 2 では;『喜び (開口)』;『喜び (閉口)』;『悲しみ』 の表情について同様の測定を行い;表情強度と感覚量の関係には; 個々の表情の物理的特性に起因する 2 つのタイプが存在することを明らかにした. 実験 3 では; 倒立した 『驚き』 の表情刺激を用いて同様の測定を行い;それまでの実験結果で被験者が提示刺激を弁別する際に確かに表情知覚を行っていたことを検証した.,電子情報通信学会技術研究報告. HCS; ヒューマンコミュニケーション基礎,1996,*
A psychophysical study on perception of facial expressions especially on the perceived strengths of expressions,H Nakamura; N Matsuzaki; S Morishima; H Yamada,*,INTERNATIONAL JOURNAL OF PSYCHOLOGY,1996,*
2-1 ニューラルネットを用いた表情パラメータ推定の試み,川上文雄， 山田寛， 原島博， 森島繁生,J-STAGE: My J-STAGEとは？ ログイン; 新規登録; ショッピングカート; ヘルプ. View in English.テレビジョン学会映像メディア部門冬季大会講演予稿集. 一般社団法人 映像情報メディア学会. ONLINEISSN: 2433-0922 PRINT ISSN: 1342-0402. 2017年10月03日現在 収録数: 122記事. 記事; 巻号頁;DOI. 詳細検索. 巻 号 頁 詳細検索. 詳細検索. 閲覧する; 発行機関について. 最新巻号. 発行機関連絡先;他のJ-STAGE内発行資料. J-STAGE トップ > 資料トップ > 書誌事項. p. 53-. 記事言語: English.前の記事; |; 次の記事. http://doi.org/10.11485/tvwac.1995.0_53. 主催: 本文. 2-1ニューラルネットを用いた表情パラメータ推定の試み. 川上 文雄 1) ; 山田 寛 2) ; 原島 博 3) ; 森島 繁生1). 1) 成蹊大学工学部 2) 川村短期大学 3) 東京大学工学部. 公開日: 20171002. 本文PDF [208K].抄録. 本文PDF [208K]. Copyright © 1995 一般社団法人映像情報メディア学会. ページトップへ.記事ツール. お気に入り登録; 被引用アラート; 認証解除アラート; 追加情報アラート; …,テレビジョン学会映像メディア部門冬季大会講演予稿集 1995,1995,*
1-3 流体モデルに基づく髪の毛の運動制御,安藤真， 松坂秀治， 森島繁生,っ た表現 を可 能に しだ 1}. しか し風 な ど頭髪に 与える頭部周辺 の外力は; これ ま で 一 様な流れ に よっ て近 似 して い た. そ こで本稿で は; 頭部 の 存在 を考慮 した外力,テレビジョン学会映像メディア部門冬季大会講演予稿集 1995,1995,*
1-4 直交化表情記述パラメータによる感情の分析・合成,松川和正， 川上文雄， 山田寛， 森島繁生,J-STAGE: My J-STAGEとは？ ログイン; 新規登録; ショッピングカート; ヘルプ. View in English.テレビジョン学会映像メディア部門冬季大会講演予稿集. 一般社団法人 映像情報メディア学会. ONLINEISSN: 2433-0922 PRINT ISSN: 1342-0402. 2017年10月03日現在 収録数: 122記事. 記事; 巻号頁;DOI. 詳細検索. 巻 号 頁 詳細検索. 詳細検索. 閲覧する; 発行機関について. 最新巻号. 発行機関連絡先;他のJ-STAGE内発行資料. J-STAGE トップ > 資料トップ > 書誌事項. p. 50-. 記事言語: English.前の記事; |; 次の記事. http://doi.org/10.11485/tvwac.1995.0_50. 主催: 本文. 1-4直交化表情記述パラメータによる感情の分析・合成. 松川 和正 1) ; 川上 文雄 1) ; 山田 寛 2) ; 森島繁生 1). 1) 成蹊大学工学部 2) 川村短期大学. 公開日: 20171002. 本文PDF [258K]. 抄録. 本文PDF[258K]. Copyright © 1995 一般社団法人映像情報メディア学会. ページトップへ. 記事ツール.お気に入り登録; 被引用アラート; 認証解除アラート; 追加情報アラート; …,テレビジョン学会映像メディア部門冬季大会講演予稿集 1995,1995,*
音声駆動による実時間表情変形システム:" Better Face Communication" at SIGGRAPH'95,森島繁生,抄録 擬人化仮想エージェントやアニメーションキャラクタの発する音声と表情情報の同期が重要な研究テーマとなっている. 特に; 唇の動きと音声を同期させるリップシンクは多くの報告がなされている. 本稿では; コミュニケーションシステムへの応用を想定して; リップシンクを実時間で実現する手法について報告する. マイクロフォンから入力された音声から; フレーム単位にスペクトル情報が計算され; ニューラルネットによって口形のパラメータに変換される. 表情はこのパラメータ情報に基づいて 3 次元ワイヤフレームモデルを変形し; 人物のテクスチャを貼り付けることによって実現される. 実際にこのアルゴリズムをリアルタイムシステムとして実現し; 任意の訪問者の正面静止画像と僅かな音声サンプルの取得後; 自らの囗の動きを自身でコントロールしたり表情を付加できるデモシステムを SIGGRAPH'95 に出展して評価を行った.,電子情報通信学会技術研究報告. MVE; マルチメディア・仮想環境基礎,1995,*
頭髪と人体の高速な衝突判定に関する一検討,安藤真， 森島繁生， 原島博,抄録 筆者らは; ヒューマンインターフェースなどの分野における人間の表情合成をよりリアルなものとするために; その頭髪に着目し; 頭髪全体をモデル化することで; 外力による頭髪の変形や頭髪表面の変化など; 従来のテクスチャでは難しかった環境の変化を実現した. しかし実際の物理モデルに基づいて自然な頭髪の動きを再現しようとすると; 多くの計算負荷に頼らざるを得ない.中でも視覚的に重要な作業である頭髪と人体との衝突判定は; 人体を構成する全てのポリゴンと全ての頭髪が同時に関わってくるので; 極めて多くの計算時間を要する. これまでに提案された主な衝突判定としては; 頭部を包むように疑似外力領域を設け; 疑似外力の作用によって頭部内部への頭髪の進入を防ぐ手法; 円筒座標系における頭部中心から表面までの距離をあらかじめ衝突判定用バッファとして用意しておく手法がある. 前者の手法は; 特別な衝突処理が必要ない分計算が高速化されるが; 厳密な衝突回避はなされていない. また頭髪が東部に潜り込まないような疑似外力を …,電子情報通信学会総合大会講演論文集,1995,*
リモートセンシング画像の圧縮とその評価,野村晴尊， 森島繁生， 原島博,抄録 現在; 資源探査や地球観測の手段としてリモートセンシングが注目されている.より高解像度のデータを取得するために情報圧縮が強く望まれており; 将来的には十分の一程度に圧縮することが目標となっている. 本稿では圧縮手法および今後の評価法について述べる. 圧縮率;S/N は次式によって定義する. rate (%)= P_c/P_o× 100 (1) S/N (dB)= 10× log_< 10> Σ {(255)^2/(i_c-i_o)^ 2}(2) ここで; P_c: 圧縮画像のデータ量; P_o: 原画像のデータ量; i_c:復元画像の輝度値; i_o: 原画像の輝度値とした.,電子情報通信学会総合大会講演論文集,1995,*
3 次元感情空間を用いた心理学実験,川上文雄， 森島繁生， 山田寛， 原島博,抄録 筆者らは 5 層ニューラルネットの恒等写像能力を用いて; 17 次元の表情バラメータ空間をその中間層に 3 次元に圧縮して感情空間と仮定し; 同時に表情からこの空間への写像とその逆写像(感情空間→ 表情) を実現して表情の分析・合成を行うシステムの構築をすでに行った. しかし; この3 次元感情空間の評価には心理学的妥当性が必要である. そこで; 被験者を対象に感情空間から合成される表情を刺激に心理学実験を行った.,電子情報通信学会総合大会講演論文集,1995,*
音声から口唇形状への実時間メディア変換,岩澤昭一郎， 上野雅俊， 森島繁生， 原島博,抄録 筆者らはコンピュータシステムと利用者の間でより円滑なコミュニケーションを実現するために;伝達される情報の半分以上が顔表情に基づくともいわれる人間同士のコミュニケーションに習いインタフェースシステムに顔やその表情を利用することを考えている. 顔を用いたインターフェースシステムにとって; あたかも実在の人物と対面しているという現実感を与えることが一つの理想である. このような高度な現実感を得るためには; 画像と音声とが同期していることはもちろん;動きの自然さや画像生成・表示は実時間に近いことが求められる. 本稿では音声から口形状を推定し顔動画像へと変換する実時間メディア変換システムについて述べる.,電子情報通信学会総合大会講演論文集,1995,*
インターフェース マネージメントのための表情分析,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,知能情報メディア,1995,*
Better Face Communication Visual,Shigeo MORISHIMA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proceedings of ACM SIGGRAPH'95; Interactive Communities,1995,*
A modeling of facial expression and emotion for recognition and synthesis,Fumio KAWAKAMI' Shigeo MORISHIMA; Hiroshi YAMADA; Hiroshi HARASHIMAC,*,Symbiosis of Human and Artifact: Future computing and design for human-computer interaction,1995,*
SA-6-2 高精細 3 次元モデルを用いた音声から画像への実時間メディア変換の一検討 (SA-6. メディア変換・統合技術とヒューマンコミュニケーション; シンポジウム),上野雅俊， 岩澤昭一郎， 森島繁生， 原島博,に人物の 表情や 会 話シ ーンな ど の リ ア ル な 顔 画像 を 合成す る研究が; 近年盛ん に行 なわれ て い る. テ レ ビ会議や 臨場感通信など; 人 と人との 聞の コ ミュ ニ ケーシ ョ ン に適用しようとする研究 [11 や; 人とコ ン ピュ,電子情報通信学会秋季大会講演論文集,1994,*
知的インタフェースのための表情分析 合成とメディア変換技術,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,コンピュータイメージフロンティア,1994,*
顔の再認における意図的学習時の符号化処理の影響: 部分的特徴と全体的印象 (VI. 第 12 回大会発表要旨),伊東裕司,果，「ある基本表惰」に 認識 され る表11青刺激が UP の 方 が よ り 「中立」に 近い ．すなわ ち，表情の変化が少な く て も UP の 方があ る表情 に 認識 されやすい とい う，一 種の 履歴現象があるこ とが確認 された … 顔認識過程に 及1ます BLURRING の効果 八戸短期大学 遠藤光男 尚綱女学院短期大学桐田隆博 資生堂 BS 研究所 阿部恒之 … こ れ ま で ，表情判断過程に つ い て 空間周波数の 観点から研究するため に ，HAPPY と SAD の 表情判断に 及ぼ す画豫の ぼか しの効果を 検討 してぎた，そ して ，SAD の 判断で は刺激をぼか した こ とに よ っ て 成績が低下 し， ぼ か しの 影響が認められ た，一方， HAPPY の 判断の 方 … は ，刺激をぼか した条件で も成績の 低下は ほ と ん どなく ぼ か しの 影響はか な り小 さか っ k ，今回は，こ れ まで よ りも強 くぼか した刺激を用 い て HAPPY判断の 成績が チ ャ ソ ス レ ベ ル まで 低下する経緯を 調べ ， SAD の それ … 断で は，ぼか しの程度がか な り大ぎ くなっ て か ら急激に 成績が低下 し た ．HAPPY で は ぼか しの 程 度が小 さい 段 …,基礎心理学研究,1993,*
表情の分析・合成を同時に実現する多層ニューラルネットによる感情空間の構成 (VI. 第 12 回大会発表要旨),森島繁生， 山田寛， 原島博,果，「ある基本表惰」に 認識 され る表11青刺激が UP の 方 が よ り 「中立」に 近い ．すなわ ち，表情の変化が少な く て も UP の 方があ る表情 に 認識 されやすい とい う，一 種の 履歴現象があるこ とが確認 された … 顔認識過程に 及1ます BLURRING の効果 八戸短期大学 遠藤光男 尚綱女学院短期大学桐田隆博 資生堂 BS 研究所 阿部恒之 … こ れ ま で ，表情判断過程に つ い て 空間周波数の 観点から研究するため に ，HAPPY と SAD の 表情判断に 及ぼ す画豫の ぼか しの効果を 検討 してぎた，そ して ，SAD の 判断で は刺激をぼか した こ とに よ っ て 成績が低下 し， ぼ か しの 影響が認められ た，一方， HAPPY の 判断の 方 … は ，刺激をぼか した条件で も成績の 低下は ほ と ん どなく ぼ か しの 影響はか な り小 さか っ k ，今回は，こ れ まで よ りも強 くぼか した刺激を用 い て HAPPY判断の 成績が チ ャ ソ ス レ ベ ル まで 低下する経緯を 調べ ， SAD の それ … 断で は，ぼか しの程度がか な り大ぎ くなっ て か ら急激に 成績が低下 し た ．HAPPY で は ぼか しの 程 度が小 さい 段 …,基礎心理学研究,1993,*
1993 Picture Coding Symposium (PCS'93),森島繁生,で あっ た; そ の 意味で 画像の 研 究者に と っ て は Lausanne は 懐か しい 場所 であ る. 今回のPCS は 発表件数が 過 去最高の 149 件に 及 び; 従来の ようなシ リア ル 配置の セ ッ シ ョン構成で は 時間内に 収ま り切れ ず; 初めて の パ ラ レ ル 構成となっ た; また; 当初 4 ペ,テレビジョン学会誌,1993,*
5) マルチメディア電子メールシステムの提案 (画像通信システム研究会),森島繁生， 原島博,定である こ とに 着目し; こ の コ セ カン トカ ーブ群 を用 い た テ レ ビ・ラジオ電界強度図表を開発した. 横軸に距 離; 縦軸 に伝搬 ロ ス を目盛 り; パ ラメ ー タとして 電界強度,テレビジョン学会誌,1992,*
マルチメディア電子メールシステムの提案,森島繁生， 原島博,抄録 We have already proposed an automatic facial motion image synthesis schemes drivenby speech and text as media conversion schemes. The purpose of this scheme is to realizean intelligent human-machine interface or intelligent communication system with talkinghead images. Human face is reconstructed with 3D surface model and texture mappingtechnique. In this paper; we applied these schemes to multi-media human-machineinterface. One example is multi-media E-mail system. Scenerio expression tool and real-timeimage playback system are realized on workstation window.,テレビジョン学会技術報告,1991,*
ICASSP'91,森島繁生,(Session No.) 総数 米国 日本 欧州 その 他 VLSI 関連 (V 1− V 416541512 7,テレビジョン学会誌,1991,*
ニューラルネットに基づく画像圧縮: 高速化と重み行列について,風山雅裕， 片山泰男， 森島繁生,バックプロパゲーション型ニューラルネットワークを用いた画像データの圧縮符号化の実験において;学習を高速化する方法を提案する. また符号量を減らす試みとして; 中間層の出力のいくつかをカットしてみた. そのときの画像評価を行なった結果についても報告する. さらに入力層-中間層間; 出力層-中間層間の重みを示し; これらの関係について考察する.,全国大会講演論文集,1989,*
符号駆動線形予測に基づく低ビットレ-ト音声符号化,熊野聡， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,成蹊大学工学部工学報告,1988,*
音声情報に基づく表情の自動合成の研究,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,第 4 回 NICOGRAPH 論文コンテスト論文集,1988,*
音響処理と記号処理とを融合した単語音声認識システムの構成,森島繁生， 原島博,学習時の話者の負担軽減; あるいは処理時間や記憶容量の節約という観点から;孤立単語音声や連続音声を音韻 (音素) 単位に認識する手法が有効である. しかし;セグメンテーションプロセスが必要となるため; 精度向上の工夫が必要である. 本稿で提案する音声認識システムでは; 音韻特徴や調音結合に関する知識だけでなく; 経験的に抽出した知識も併用し入力単語音声に対しボトムアップ的に複数の単語仮説を作成する. この際; 継続時間長が比較的長く定常的である母音および撥音部分を先に同定し; 一般に認識が困難な子音部分は位置の同定だけにとどめている. 次にこの仮説に基づいて辞書の中から単語候補の限定を行い;未定子音音素部分の同定をトップダウン的に効率よく行っている. これらのプロセスはプロローグでインプリメントされており; 辞書中の探索やルールの適用などが高速に実現できる.認識に要する処理時間は語彙 (い) 数に依存せず; 抽出される単語候補間の音素判別の難易に …,電子情報通信学会論文誌 D,1987,*
電子情報通信学会 編; 中前栄八郎 著; ニューメディア技術シリーズ;" コンピュータグラフィックス"; オーム社; A5 判; 244p.;\3;500; 1987,森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,情報処理,1987,*
統計的手法に基づくプロダクションルールの自動抽出法とファジィ木探索,森島繁生， 原島博,一般にエキスパートシステムを構成する際には; 知識を専門家から聞き出して移植する作業が必要である. しかし; 医療診断やパターン認識分野の中には; 経験や直感に大きく依存するため決定論理が曖昧なものも存在し; このような分野では知識抽出は容易に行えない. 本稿ではデータの統計解析に基づき知識を自動抽出する手法を提案する. 閾論理が; 様々な論理関数を表現しうることを利用し; プロダクションルールを閾論理の形式で表現する. 閾論理の評価関数は入力ベクトルと重み係数との線形結合であることから; 判別係数を求めるのと同様の手法によって重み係数を計算する.閾値を一通りに定めれば通常の論理関数を導出できるが; 閾値を複数通り定め;それぞれに集合への帰属度を割当てれば; 曖昧な推論を含むルールも記述できる.このような曖昧な推論と木構造を組み合わせ; 少ない計算量で性能よくクラスタを決定するアルゴリズムとしてファジィ木探索を後半で提案する. また; 実際に診断論理の曖昧とされる精神 …,電子情報通信学会論文誌 D,1986,*
Multimodal Translation System Using Texture-Mapped Lip-Sync Images for Video Mail and Automatic Dubbing Applications,Morishima Shigeo; Nakamura Satoshi,*,EURASIP Journal on Advances in Signal Processing,1900,*
Voice Animator: Automatic Lip-Synching in Limited Animation by Audio,Shigeo Morishima,Abstract. Limited animation is one of the traditional techniques for producing cartoonanimations. Owing to its expressive style; it has been enjoyed around the world. However;producing high quality animations using this limited style is time-consuming and costly foranimators. Furthermore; proper synchronization between the voice-actor's voice and thecharacter's mouth and lip motion requires well-experienced animators. This is essentialbecause viewers are very sensitive to audio-lip discrepancies. In this paper; we propose amethod that automatically creates high-quality limited-style lip-synched animations usingaudio tracks. Our system can be applied for creating not only the original animations but alsodubbed ones independently of languages. Because our approach follows the standardworkflow employed in cartoon animation production; our system can successfully assist …,Advances in Computer Entertainment Technology,*,*
多視点顔画像に基づ く 3 次元顔形状推定,Shigeo MORISHIMA,*,*,*,*
「顔」 の情報処理,Osamu HASEGAWA; Shigeo MORISHIMA,*,*,*,*
Prosody; voice Quality and Speaker Conversion System using Reference Speech,Yoshihiro ADACHI; Shigeo MORISHIMA,*,*,*,*
表情分析入門表情分析入門; 1-162; 1990,寺田員人， 宮永美知代， 森島繁生， 花田晃治,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,*,*,*
表情分析入門表情に隠された意味をさぐる表情分析入門表情に隠された意味をさぐる; 1987,藤代裕紀， 前島謙宣， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,*,*,*
人はなぜ笑うのか人はなぜ笑うのか; 1994,志水彰,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,*,*,*
Automatic Comic-Style Video Summarization of Anime Films by Key-frame Detection,Tsukasa Fukusato; Tatsunori Hirai; Hayato Ohya; Shigeo Morishima,Video content in the web site continues to explosively increase. However; it is difficult forpeople to find videos that match their specific requirements or satisfy their favor. Videosummarization approaches; which are designed to help users find and review theinformation they need; have been studied; and various techniques to produce comic-stylesummarizations such as the use of play logs and image features; have been proposed.However; in many previous methods; the amount and granularity of information has notbeen considered because there is no standard evaluation scale to determine whether theobtained key frames can be appropriately used in comic-style summarizations. Therefore;we propose a scale to evaluate video summarization using matching accuracy betweenanime content and the original comic from which the anime content is created. In addition …,Recall,*,*
NICOInt 2017,Robin Bing-Yu Chen; Yasuo Ebara; Issei Fujishiro; Osama Halabi; Iwao Haruguchi; Masaki Hayashi; Yuki Igarashi; Andreas Iglesias; Masataka Imura; Takayuki Itoh; Xiaogang Jin; Kiyoshi Kiyokawa; Kouichi Konno; Yusup Martyastiadi; Shinya Miyazaki; Shigeo Morishima; Tomohiko Mukai; Makoto Okabe; RPC Janaka Rajapakse; Peeraya Sripian; Yigang Wang; Taichi Watanabe; Yonghao Yue; Hongbin Zha,Yosuke Bando; MIT; USA Robin Bing-Yu Chen; National Taiwan University; Taiwan YasuoEbara; Kyoto University; Japan Issei Fujishiro; Keio University; Japan Osama Halabi; QatarUniversity; Qatar Iwao Haruguchi; Shobi University; Japan Masaki Hayashi; UppsalaUniversity; Sweden Yuki Igarashi; Meiji University; Japan Andreas Iglesias; University ofCantabria; Spain Masataka Imura; Kwansei Gakuin University; Japan Takayuki Itoh; OchanomizuUniversity; Japan Xiaogang Jin; Zhejiang University; China Kiyoshi Kiyokawa; Nara Instituteof Science and Technology; Japan Kouichi Konno; Iwate University; Japan YusupMartyastiadi; Universitas Multimedia Nusantara; Indonesia Kazunori Miyata; JAIST; Japan ShinyaMiyazaki; Chukyo University; Japan Shigeo Morishima; Waseda University; Japan TomohikoMukai; Tokai University; Japan Makoto Okabe; The University of …,*,*,*
SIMULATING THE FRICTION SOUNDS USING A FRICTION-BASED ADHESION THEORY MODEL,Takayuki Nakatsuka; Shigeo Morishima,ABSTRACT Synthesizing a friction sound of deformable objects by a computer ischallenging. We propose a novel physics-based approach to synthesize friction soundsbased on dynamics simulation. In this work; we calculate the elastic deformation of an objectsurface when the object comes in contact with other objects. The principle of our method isto divide an object surface into microrectangles. The deformation of each microrectangle isset using two assumptions: the size of a microrectangle (1) changes by contacting otherobject and (2) obeys a normal distribution. We consider the sound pressure distribution andits space spread; consisting of vibrations of all microrectangles; to synthesize a frictionsound at an observation point. We express the global motions of an object by position baseddynamics where we add an adhesion constraint. Our proposed method enables the …,*,*,*
International Joint Workshop on Advanced Sensing/Visual Attention and Interaction-Toward Creation of Human-Harmonized Information Technology,Toyoaki Nishida; Yasushi Yagi; Ko Nishino; Hanako Yoshida; T Nakamura; A Maejima; S Morishima; M Hayashi; T Yamamoto; Y Aoki; K Ohshima; M Tanabiki; D Duan; L Tian; J Cui; L Wang; H Zha; H Aghajan; A Mizokawa; H Habe; K Kajiwara; I Mitsugami; Y Yagi; K Tanaka; M Iwata; K Kunze; M Iwamura; K Kise; A Hiratani; R Nakashima; K Matsumiya; I Kuriki; S Shioiri; A Iwatsuki; T Hirayama; K Mase; H Yoshimoto; Y Nakamura,Page 1. 09:00 | 09:15 Opening Sponsor Venue Prof. Toyoaki Nishida (Kyoto University) CRESTTenpei 09:15 | 09:55 Organized Session 1 Enabling a Mobile Social Robot to Adapt to a PublicSpace in a City Dr. Masahiro Shiomi (ATR) CREST Tenpei Behavior Understanding Based onIntention-Gait Model Prof. Yasushi Yagi (Osaka University) Core-to- Core Program Break(09:55-10:05) 10:05 | 11:05 Keynote Talk 1 Going with the Flow: Modeling and Exploiting CrowdFlow in Videos Prof. Ko Nishino (Drexel University) Core-to- Core Program Tenpei Break(11:05-11:15) 11:15 | 12:15 Keynote Talk 2 The Role of Parent and Infant Hand Actions in CreatingCritical Information for the Developing Visual System Prof. Hanako Yoshida (University of Houston)KAKEN HI Tenpei Lunch (12:15-13:15) 12:15 | 14:20 Poster Session …,*,*,*
Fiber-dependent Approach for Fast Dynamic Character Animation,Ayano Kaneda; Tsukasa Fukusato; Yoshihiro Fukuhara; Takayuki Nakatsuka; Shigeo Morishima,Abstract Creating secondary motion of character animation including jiggling of fat isdemanded in the computer animation. In general; secondary motion from the primary motionof the character are expressed based on shape matching approaches. However; theprevious methods do not account for the directional stretch characteristics and local stiffnessat the same time; that is problematic to represent the effect of anatomical structure such asmuscle fiber. Our framework allows user to edit the anatomical structure of the charactermodel corresponding to creature's body containing muscle and fat from the tetrahedralmodel and bone motion. Our method then simulates the elastic deformation consideringanatomical structures defined directional stretch characteristics and stiffness on each layer.In addition; our method can add the constraint for local deformation (eg biceps) …,*,*,*
デザインサークルとカットの原理; Modern Cut Manual デザインサークルとカットの原理; Modern Cut Manual; 2003,杉崎英嗣， 風間祥介， 石川貴仁， 白石允梓， 西村昌平， 森島繁生,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,*,*,*
3 次元形状とテクスチャの双方の変換による年齢変化顔の生成,Yusuke Tazoe Hiroki Fujishiro Shinya Nakano; Yusuke Nonaka Satoko Kasai Akinobu Maejima; Shigeo Morishima,*,Age,*,*
Culture and Computing 2015,Jens Allwood; Richard Beacham; Nadia Berthouze; Timothy Bickmore; Emmanuel G Blanchard; Philippe Codognet; Nick Degens; Shlomo Dubnov; Colette Faucher; Christiane D Fellbaum; Lynne Hall; Rüdiger Heimgärtner; Dirk Heylen; Michitaka Hirose; Gert Jan Hofstede; Jieh Hsiang; Jane Hunter; Katsushi Ikeuchi; Jean Ippolito; W Lewis Johnson; Kristiina Jokinen; Yasuhiro Katagiri; Rilla Khaled; Tomoko Koda; Sadao Kurohashi; Lydia Lau; Michihiko Minoh; Shigeo Morishima; Yohei Murakami; Atsushi Nakazawa; Shohei Nobuhara; Hiroaki Ohshima; Yoshihiro Okada; Mario Paolucci; Jong-Il Park; Andrew Prescott; Matthias Rehm; Geoffrey Rockwell; Kasper Rodil; Tetsuo Sawaragi; Suleman Shahid; Vibeke Sorensen; Virach Sornlertlamvanich; Juan-Luis Suárez,Jens Allwood; University of Göteborg; Sweden Richard Beacham; King's College London; UnitedKingdom Nadia Berthouze; University College London; United Kingdom Timothy Bickmore; NortheasternUniversity – Boston; USA Emmanuel G. Blanchard; IDÛ Interactive Inc. – Montréal; CanadaJean-Pierre Briot; CNRS; Brazil Philippe Codognet; CNRS & University of Tokyo; Japan NickDegens; Wageningen University; Netherlands Shlomo Dubnov; University of California at SanDiego; USA Colette Faucher; Aix-Marseille University; France Christiane D. Fellbaum; PrincetonUniversity; USA Lynne Hall; University of Sunderland; United Kingdom RüdigerHeimgärtner; IUIC – Undorf; Germany Dirk Heylen; University of Twente; Netherlands MichitakaHirose; University of Tokyo; Japan Gert Jan Hofstede; Wageningen University; Netherlands JiehHsiang; National Taiwan University; Taiwan Jane Hunter; University of Queensland …,*,*,*
AUTOMATIC SINGING VOICE TO MUSIC VIDEO GENERATION VIA MASHUP OF SINGING VIDEO CLIPS,Tatsunori Hirai; Yukara Ikemiya; Kazuyoshi Yoshii; Tomoyasu Nakano; Masataka Goto; Shigeo Morishima,ABSTRACT This paper presents a system that takes audio signals of any song sung by asinger as the input and automatically generates a music video clip in which the singerappears to be actually singing the song. Although music video clips have gained thepopularity in video streaming services; not all existing songs have corresponding videoclips. Given a song sung by a singer; our system generates a singing video clip by reusingexisting singing video clips featuring the singer. More specifically; the system retrieves shortfragments of singing video clips that include singing voices similar to that in target song; andthen concatenates these fragments using a technique of dynamic programming (DP). Toachieve this; we propose a method to extract singing scenes from music video clips bycombining vocal activity detection (VAD) with mouth aperture detection (MAD). The …,*,*,*
新たな音楽動画を自動生成する N 次創作支援システム,中野倫靖， 室伏空， 後藤真孝， 森島繁生,に映像を自動付与する研究事例はあったが; 既存の音楽動画を再利用した動画生成はできなかった. そこで我々は; Web 上で公開されている二次創作のダンス動画を多数収集し;音楽と映像の多様な対応関係を機械学習によってモデル化した. 次に; ユーザが与える入力音楽を分析してテンポや楽曲構造等を自動推定し; そのテンポに同期し内容が合うように; 収集した動画,*,*,*
節体幹型移動ロボット KRⅡ の開発,広瀬茂男， 森島昭男， 塚越真一， 妻木俊道， 物部宏之,*,JRSJ,*,*
高精細 3 次元モデルを用いた音声から画像への実時間メディア変換の一検討,上野雅俊， 岩澤昭一郎， 森島繁生， 原島博,*,*,*,*
既存のダンス動画の再利用により音楽に合った動画を作成できるシステム,室伏空， 中野倫靖， 後藤真孝， 森島繁生,Summary. 本研究では; 既存のダンス動画コンテンツの動画像を分割・伸縮して連結 (切り貼り)することで; 音楽に合ったダンス動画を自動生成し; ユーザの好みに合うようにインタラクティブに編集可能なシステム DanceReProducer を提案する. これによってユーザは; 単に音楽を聴取するだけでなく; 音楽に合った好みの映像を容易に作成でき; 視覚的にも楽しむことができる. 従来;音楽に合わせた映像付与に関する研究はあったが; それらは既存のダンス動画コンテンツを再利用して; 音楽に合った動画を作成することはできなかった. 本システムでは; Web上で公開されている大量の二次創作動画を利用して; 映像と音楽の対応関係をモデル化し;それに基づいて音楽に合った映像を自動的に作成する. さらに; 自動的に作成された映像にユーザが「ダメ出し」 するだけで; 好みに合わない映像を容易に訂正・編集できる. 本システムを実装して運用した結果; 音楽に合わせたダンス動画の作成が効率的に行えて; また二次創作の経験の無い …,*,*,*
音声から画像へのメディア変換を用いた,四倉達夫， 小林智典， 藤井英史， 森島繁生,Page 1. 「インタラクション，98」平成10年3月 音声から画像へのメディア変換を用いたサイバースペース上での多人数コミュニケーションシステム 四倉 達夫 小林 智典 藤井 英史 森島繁生 Tatsuo YOTSUKURA TomonoriKOBAYASHI EishiFUJII Shigeo MORISHIMA（tatsuo，koba，eishi，Shigeo）＠ee．seikei．ac．jp 成膜大学工学部電気電子工学科情報通信研究室 〒180東京都武蔵野市吉祥寺北町3−3−1 Tel．0422−37−3742，Fax．0422−37−38711．はじめに コンピュータの発展に伴い、近年コンピュータ上 にサイバースペースを作り出し、あたかもその環境 にいるような感覚を生み出させるバーチャルリアリ ティ技術が急速に進歩してきた。これは仮想空間を より現実感を持った空間に近づけ、そこで人間同士 のコミュニケーションをリアルに表現することが要 求される。 そこで自分自身を投影した人間同様の顔を持つ分 …,*,*,*
Proceedings of AVSP 2001,Dominic W Massaro; Joanna Light; Kristin Geraci,*,*,*,*
Developments of Anthropomorphic Dialog Agent: A Plan and Development and its Significance,Shin-ichi Kawamoto; Hiroshi Shimodaira; Shigeki Sagayama; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Teikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro,1 Abstract With financial support from Japan's Information-technology Promotion Agency(IPA); a three-year project was launched in 2000 to develop the basic software for ananthropomorphic spoken dialog agent. The basic software consists of four modules forspeech recognition; speech synthesis; facial image synthesis; and multi-modal dialogintegration. This interim report describes the basic design approach and an implementationof the software focusing on efforts to ensure the interactive capability of the spoken dialogagent.,*,*,*
Eric Bateson ATR; Japan Josef Bigun University of Halmstad; Sweden,Hyeran Byun; Rama Chellappa; James Crowley; INRIA Rhones Alpes; France Shaogang Gong; Luc van Gool; Adrian Hilton; Radu Horaud; France Koh Kakusho; Stan Z Li; Jiri Matas; Shigeo Morishima; Tieniu Tan; Nadia Thalmann; Matthew Turk; Thomas Vetter; Juan J Villanueva; Masahiko Yachida; Hyun Seung Yang; Juneho Yi; Pong Chi Yuen; David Zhang,Page 1. xvii PROGRAM COMMITTEE Eric Bateson ATR; Japan Josef Bigun University of Halmstad;Sweden Hyeran Byun Yonsei University; Korea Rama Chellappa University of Maryland; USAJames Crowley INRIA Rhones Alpes; France Shaogang Gong Queen Mary University of London;UK Luc van Gool Catholic University of Leuven; Belgium Adrian Hilton University of Surrey; UKRadu Horaud INRIA Rhones Alpes; France Koh Kakusho Kyoto University; Japan Stan Z. LiMicrosoft Research Asia; China Jiri Matas CVUT; Czech Republic Shigeo Morishima SeikeiUniversity; Japan Shree Nayar Columbia University; USA Yuichi Ohta University of Tsukuba;Japan Yoichi Sato University of Tokyo; Japan Tieniu Tan Chinese Academy of Science; ChinaNadia Thalmann University of Geneva; Switzerland Matthew Turk Univ. of California; SantaBarbara; USA Thomas Vetter University of Basel; Switzerland …,*,*,*
Automatic 3D Face Generation from Video with Sparse Point Constraint and Dense Deformable Model-Supplemental Materials,Tomoya Hara; Akinobu Maejima; Shigeo Morishima,*,*,*,*
正面顔画像からの顔形状ディスプレイ用テクスチャ自動生成,Akinobu MAEJIMA; Takaaki KURATATE; Brennand PIERCE; Gordon CHENG; Shigeo MORISHIMA,リアルな見た目を持ち自然な対話が可能なヒューマノイドロボットの実現は; 人-ロボット間のインタラクションを円滑にするために重要であり; この目的を達成するために様々な研究が行われている [1~ 5]. Kuratate らは; Mask-bot と呼ばれるヒューマノイドロボットの顔を開発している (図 1).Mask-bot は; 主に 3 次元顔形状スクリーン; プロジェクタ; パンチルトユニットの 3つから構成されており; CG 合成されたフェイシャルアニメーションを顔形状スクリーンに投影することで; 自身の表情を制御することが可能で; これにより; 立体視とは異なり実空間で存在感のある顔を表現することができる. また; 顔のテクスチャを変更することにより; 自身の顔を様々な人の顔に入れ替えることも原理的に可能である. しかしながら; 実際に対象人物の顔の 3次元形状を反映して顔の入れ替えを行うためには; 3 次元顔モデルを生成した上で;さらにそれと顔形状スクリーンとの間のキャリブレーションを逐一とらねばならず; この作業に手間 …,*,*,*
Acoustic Features Affecting Speaker Identification by Imitated Voice Analysis,Mari TANAKA; Hideki KAWAHARA; Shigeo MORISHIMA,ABSTRACT In this paper; physical correlates of perceived personal identity are investigatedusing imitated 16 utterances spoken by 11 mimicry speakers and 24 test subjects. Ourunique strategy to use non-professional impersonators enabled to prepare test utteranceswith wide range of perceived similarities. Reasonably high correlations (0.46 and 0.44) inmultiple regression analysis were attained by grouping subjects into three groups based oncluster analysis of the subjective test results. Without clustering; the correlation was only0.17. Cluster analysis also revealed differences in their focusing physical correlatesbetween three groups indicating importance of individual differences both in speakers andlisteners.,*,*,*
Freeware for Developing Anthropomorphic Spoken Dialog Agent,Shin-ichi Kawamoto; Hiroshi Shimodaira; Tsuneo Nitta; Takuya Nishimoto; Satoshi Nakamura; Katsunobu Itou; Shigeo Morishima; Tatsuo Yotsukura; Atsuhiko Kai; Akinobu Lee; Yoichi Yamashita; Takao Kobayashi; Keiichi Tokuda; Keikichi Hirose; Nobuaki Minematsu; Atsushi Yamada; Yasuharu Den; Takehito Utsuro; Shigeki Sagayama,Abstract An architecture for highly-interactive human-like spoken-dialogue agent isdiscussed in this paper. In order to easily integrate the modules of different characteristicsincluding speech recognizer; speech synthesizer; facial-image synthesizer and dialoguecontroller; each module is modeled as a virtual machine that has a simple common interfaceand is connected to each other through a broker (communication manager). The agentsystem under development is supported by the IPA and it will be publicly available as asoftware toolkit this year.,*,*,*
Shigeru Akamatsu Human Information Processing Research Laboratories ATR; Kyoto; Japan,Andrew Blake; Shigeo Morishima; Peter Belhumeur; Yuichi Ohta; Aaron Bobick; Ryuichi Oka; Sandy Pentland; Trevor Darrell; Tomaso Poggio; Irfan Essa; Fumio Hara; Bill Freeman; Shuji Hashimoto; Michael Gleicher; Rainer Herpers; David Hogg; Kenji Mase; Thomas S Huang; Takashi Matsuyama; Takeo Kanade,*,*,*,*
