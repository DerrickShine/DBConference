Efficient similarity search in sequence databases,Rakesh Agrawal; Christos Faloutsos; Arun Swami,Abstract We propose an indexing method for time sequences for processing similarityqueries. We use the Discrete Fourier Transform (DFT) to map time sequences to thefrequency domain; the crucial observation being that; for most sequences of practicalinterest; only the first few frequencies are strong. Another important observation is Parseval'stheorem; which specifies that the Fourier transform preserves the Euclidean distance in thetime or frequency domain. Having thus mapped sequences to a lower-dimensionality spaceby using only the first few Fourier coefficients; we use R*-trees to index the sequences andefficiently answer similarity queries. We provide experimental results which show that ourmethod is superior to search based on sequential scanning. Our experiments show that afew coefficients (1–3) are adequate to provide good performance. The performance gain …,International Conference on Foundations of Data Organization and Algorithms,1993,2468
Structural joins: A primitive for efficient XML query pattern matching,Shurug Al-Khalifa; HV Jagadish; Nick Koudas; Jignesh M Patel; Divesh Srivastava; Yuqing Wu,XML queries typically specify patterns of selection predicates on multiple elements that havesome specified tree structured relationships. The primitive tree structured relationships areparent-child and ancestor-descendant; and finding all occurrences of these relationships inan XML database is a core operation for XML query processing. We develop two families ofstructural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithmsare a natural extension of traditional merge joins and the multi-predicate merge joins; whilethe stack-tree algorithms have no counterpart in traditional relational join processing. Wepresent experimental results on a range of data and queries using the TIMBER native XMLquery engine built on top of SHORE. We show that while; in some cases; tree-mergealgorithms can have performance comparable to stack-tree algorithms; in many cases …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,1206
The TV-tree: An index structure for high-dimensional data,King-Ip Lin; Hosagrahar V Jagadish; Christos Faloutsos,Abstract We propose a file structure to index high-dimensionality data; which are typicallypoints in some feature space. The idea is to use only a few of the features; using additionalfeatures only when the additional discriminatory power is absolutely necessary. We presentin detail the design of our tree structure and the associated algorithms that handle such“varying length” feature vectors. Finally; we report simulation results; comparing theproposed structure with the R*-tree; which is one of the most successful methods for low-dimensionality spaces. The results illustrate the superiority of our method; which saves up to80% in disk accesses.,The VLDB Journal,1994,777
Efficient retrieval of similar time sequences under time warping,Byoung-Kee Yi; HV Jagadish; Christos Faloutsos,Fast similarity searching in large time sequence databases has typically used Euclideandistance as a dissimilarity metric. However; for several applications; including matching ofvoice; audio and medical signals (eg; electrocardiograms); one is required to permit localaccelerations and decelerations in the rate of sequences; leading to a popular; field testeddissimilarity metric called the" time warping" distance. From the indexing viewpoint; thismetric presents two major challenges:(a) it does not lead to any natural indexable" features";and (b) comparing two sequences requires time quadratic in the sequence length. Toaddress each problem; we propose to use:(a) a modification of the so called" FastMap"; tomap sequences into points; with little compromise of" recall"(typically zero); and (b) a fastlinear test; to help us discard quickly many of the false alarms that FastMap will typically …,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,730
Analysis of the clustering properties of the Hilbert space-filling curve,Bongki Moon; Hosagrahar V Jagadish; Christos Faloutsos; Joel H.  Saltz,Several schemes for the linear mapping of a multidimensional space have been proposedfor various applications; such as access methods for spatio-temporal databases and imagecompression. In these applications; one of the most desired properties from such linearmappings is clustering; which means the locality between objects in the multidimensionalspace being preserved in the linear space. It is widely believed that the Hilbert space-fillingcurve achieves the best clustering (Abel and Mark; 1990; Jagadish; 1990). We analyze theclustering property of the Hilbert space-filling curve by deriving closed-form formulas for thenumber of clusters in a given query region of an arbitrary shape (eg; polygons andpolyhedra). Both the asymptotic solution for the general case and the exact solution for aspecial case generalize previous work. They agree with the empirical results that the …,IEEE Transactions on knowledge and data engineering,2001,711
Approximate string joins in a database (almost) for free,Luis Gravano; Panagiotis G Ipeirotis; Hosagrahar Visvesvaraya Jagadish; Nick Koudas; Shanmugauelayut Muthukrishnan; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data especially formore complex queries involving joins. This is due; for example; to the prevalence oftypographical errors in data; and multiple conventions for recording attributes such as nameand address. Commercial databases do not support approximate string joins directly; and itis a challenge to implement this functionality efficiently with user-defined functions (UDFs). Inthis paper; we develop a technique for building approximate string join capabilities on top ofcommercial databases by exploiting facilities already available in them. At the core; ourtechnique relies on matching short substrings of length а; called а-grams; and taking intoaccount both positions of individual matches and the total number of such matches. Our …,VLDB,2001,634
Approximate string joins in a database (almost) for free,Luis Gravano; Panagiotis G Ipeirotis; Hosagrahar Visvesvaraya Jagadish; Nick Koudas; Shanmugauelayut Muthukrishnan; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data especially formore complex queries involving joins. This is due; for example; to the prevalence oftypographical errors in data; and multiple conventions for recording attributes such as nameand address. Commercial databases do not support approximate string joins directly; and itis a challenge to implement this functionality efficiently with user-defined functions (UDFs). Inthis paper; we develop a technique for building approximate string join capabilities on top ofcommercial databases by exploiting facilities already available in them. At the core; ourtechnique relies on matching short substrings of length а; called а-grams; and taking intoaccount both positions of individual matches and the total number of such matches. Our …,VLDB,2001,634
Challenges and opportunities with big data,Alexandros Labrinidis; Hosagrahar V Jagadish,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of" Big Data;" including the recent announcementfrom the White House about new funding initiatives across different agencies; that targetresearch for Big Data. While the promise of Big Data is real--for example; it is estimated thatGoogle alone contributed 54 billion dollars to the US economy in 2009--there is no clearconsensus on what is Big Data. In fact; there have been many controversial statementsabout Big Data; such as" Size is the only thing that matters." In this panel we will try toexplore the controversies and debunk the myths surrounding Big Data.,Proceedings of the VLDB Endowment,2012,604
Linear clustering of objects with multiple attributes,Hosagrahar V Jagadish,Abstract There is often a need to map a multi-dimensional space on to a one-dimensionalspace. For example; this kind of mapping has been proposed to permit the use of one-dimensional indexing techniques to a multi-dimensional index space such as in a spatialdatabase. This kind of mapping is also of value in assigning physical storage; such asassigning buckets to records that have been indexed on multiple attributes; to minimize thedisk access effort. In this paper; we discuss what the desired properties of such a mappingare; and evaluate; through analysis and simulation; several mappings that have beenproposed in the past. We present a mapping based on Hilbert's space-filling curve; whichout-performs previously proposed mappings on average over a variety of different operatingconditions.,ACM SIGMOD Record,1990,598
Timber: A native xml database,Hosagrahar V Jagadish; Shurug Al-Khalifa; Adriane Chapman; Laks VS Lakshmanan; Andrew Nierman; Stelios Paparizos; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu,Abstract This paper describes the overall design and architecture of the Timber XMLdatabase system currently being implemented at the University of Michigan. The system isbased upon a bulk algebra for manipulating trees; and natively stores XML. New accessmethods have been developed to evaluate queries in the XML context; and new costestimation and query optimization techniques have also been developed. We presentperformance numbers to support some of our design decisions. We believe that the keyintellectual contribution of this system is a comprehensive set-at-a-time query processingability in a native XML store; with all the standard components of relational queryprocessing; including algebraic rewriting and a cost-based optimizer.,The VLDB Journal—The International Journal on Very Large Data Bases,2002,568
iDistance: An adaptive B+-tree based indexing method for nearest neighbor search,Hosagrahar V Jagadish; Beng Chin Ooi; Kian-Lee Tan; Cui Yu; Rui Zhang,Abstract In this article; we present an efficient B &plus;-tree based indexing method; callediDistance; for K-nearest neighbor (KNN) search in a high-dimensional metric space.iDistance partitions the data based on a space-or data-partitioning strategy; and selects areference point for each partition. The data points in each partition are transformed into asingle dimensional value based on their similarity with respect to the reference point. Thisallows the points to be indexed using a B &plus;-tree structure and KNN search to beperformed using one-dimensional range search. The choice of partition and reference pointsadapts the index structure to the data distribution. We conducted extensive experiments toevaluate the iDistance technique; and report results demonstrating its effectiveness. We alsopresent a cost model for iDistance KNN search; which can be exploited in query …,ACM Transactions on Database Systems (TODS),2005,565
Composite event specification in active databases: Model & implementation,Narain H Gehani; Hosagrahar V Jagadish; Oded Shmueli,ABSTRACT Active database systems require facilities to specify triggers that fire whenspecified events occur. We propose a language for specifying composite events as evetiexpressions; formed using event operators and events (primitive or composite). An eventexpression maps an event history to anothe-r event history that contains only the events atwhich the event expression is “satisfied” and at which the trigger should 6re. We presentseveral examples illustrating how quite complex event specifications are possible usingevent expressions. In addition to the basic event operators; we also provide facilities thatmake it easier to specify composite events.“Pipes” allow users to isolate sub-histories ofinterest.“Correlation variables” allow users to ensure that different parts of an eventexpression are satisfied by the same event; thereby facilitating the coordination of sub …,VLDB,1992,522
Efficiently supporting ad hoc queries in large datasets of time sequences,Flip Korn; Hosagrahar V Jagadish; Christos Faloutsos,Abstract Ad hoc querying is difficult on very large datasets; since it is usually not possible tohave the entire dataset on disk. While compression can be used to decrease the size of thedataset; compressed data is notoriously difficult to index or access. In this paper we considera very large dataset comprising multiple distinct time sequences. Each point in the sequenceis a numerical value. We show how to compress such a dataset into a format that supportsad hoc querying; provided that a small error can be tolerated when the data isuncompressed. Experiments on large; real world datasets (AT&T customer calling patterns)show that the proposed method achieves an average of less than 5% error in any data valueafter compressing to a mere 2.5% of the original space (ie; a 40: 1 compression ratio); withthese numbers not very sensitive to dataset size. Experiments on aggregate queries …,Acm Sigmod Record,1997,518
Efficient management of transitive relationships in large data and knowledge bases,Rakesh Agrawal; Alexander Borgida; Hosagrahar Visvesvaraya Jagadish,Abstract We argue that accessing the transitive closure of relationships is an importantcomponent of both databases and knowledge representation systems in ArtificialIntelligence. The demands for efficient access and management of large relationshipsmotivate the need for explicitly storing the transitive closure in a compressed and local way;while allowing updates to the base relation to be propagated incrementally. We present atransitive closure compression technique; based on labeling spanning trees with numericintervals; and provide both analytical and empirical evidence of its efficacy; including a proofof optimality.,ACM SIGMOD Record,1989,515
Baton: A balanced tree structure for peer-to-peer networks,Hosagrahar V Jagadish; Beng Chin Ooi; Quang Hieu Vu,Abstract We propose a balanced tree structure overlay on a peer-to-peer network capable ofsupporting both exact queries and range queries efficiently. In spite of the tree structurecausing distinctions to be made between nodes at different levels in the tree; we show thatthe load at each node is approximately equal. In spite of the tree structure providingprecisely one path between any pair of nodes; we show that sideways routing tablesmaintained at each node provide sufficient fault tolerance to permit efficient repair.Specifically; in a network with N nodes; we guarantee that both exact queries and rangequeries can be answered in O (log N) steps and also that update operations (to both dataand network) have an amortized cost of O (log N). An experimental assessment validates thepracticality of our proposal.,Proceedings of the 31st international conference on Very large data bases,2005,491
Evaluating Structural Similarity in XML Documents.,Andrew Nierman; HV Jagadish,Abstract XML documents on the web are often found without DTDs; particularly when thesedocuments have been created from legacy HTML. Yet having knowledge of the DTD can bevaluable in querying and manipulating such documents. Recent work (cf.[10]) has given us ameans to (re-) construct a DTD to describe the structure common to a given set of documentinstances. However; given a collection of documents with unknown DTDs; it may not beappropriate to construct a single DTD to describe every document in the collection. Instead;we would wish to partition the collection into smaller sets of “similar” documents; and theninduce a separate DTD for each such set. It is this partitioning problem that we address inthis paper. Given two XML documents; how can one measure structural (DTD) similaritybetween the two? We define a tree edit distance based measure suited to this task; taking …,webdb,2002,483
Schema-free xquery,Yunyao Li; Cong Yu; HV Jagadish,Abstract The widespread adoption of XML holds out the promise that document structure canbe exploited to specify precise database queries. However; the user may have only a limitedknowledge of the XML structure; and hence may be unable to produce a correct XQuery;especially in the context of a heterogeneous information collection. The default is to usekeyword-based search and we are all too familiar with how difficult it is to obtain preciseanswers by these means. We seek to address these problems by introducing the notion ofMeaningful Lowest Common Ancestor Structure (MLCAS) for finding related nodes within anXML document. By automatically computing MLCAS and expanding ambiguous tag names;we add new functionality to XQuery and enable users to take full advantage of XQuery inquerying XML data precisely and efficiently without requiring (perfect) knowledge of the …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,476
Optimal histograms with quality guarantees,Hosagrahar Visvesvaraya Jagadish; Nick Koudas; S Muthukrishnan; Viswanath Poosala; Kenneth C Sevcik; Torsten Suel,Abstract Histograms are commonly used to capture attribute value distribution statistics forquery optimizers. More recently; histograms have also been considered as a way to producequick approximate answers to decision support queries. This widespread interest inhistograms motivates the problem of computing histograms that are good under a givenerror metric. In particular; we are interested in an e® cient algorithm for choosing the bucketboundaries in a way that either minimizes the estimation error for a given amount of space(number of buckets) or; conversely; minimizes the space needed for a given upper bound onthe error. Under the assumption that finding optimal bucket boundaries is computationallyine® cient; previous research has focused on heuristics with no provable bounds on thequality of the solutions. In this paper; we present algorithms for computing optimal bucket …,VLDB,1998,468
Ode as an Active Database: Constraints and Triggers.,Narain H Gehani; Hosagrahar Visvesvaraya Jagadish,Ode [2; 3] is a database system and environment based on the object paradigm. Thedatabase is defined; queried; and manipulated using the database programming languageO++; which is an upward-compatible extension of the object-oriented programminglanguage C++[Stroustrup 1986]. O++ extends C++ by providing facilities suitable fordatabase applications; such as facilities for creating persistent and versioned objects;defining and manipulating sets; organizing persistent objects into clusters; iterating overclusters of persistent objects; and associating constraints and triggers with objects. Theconstraint and trigger facilities in Ode make Ode an active database. Providing integrityconstraint facilities in a database is not a new issue since all major commercial databasestoday provide some level of integrity maintenance. The novel aspect of our work is in …,VLDB,1991,461
Finding k-dominant skylines in high dimensional space,Chee-Yong Chan; HV Jagadish; Kian-Lee Tan; Anthony KH Tung; Zhenjie Zhang,Abstract Given a d-dimensional data set; a point p dominates another point q if it is betterthan or equal to q in all dimensions and better than q in at least one dimension. A point is askyline point if there does not exists any point that can dominate it. Skyline queries; whichreturn skyline points; are useful in many decision making applications. Unfortunately; as thenumber of dimensions increases; the chance of one point dominating another point is verylow. As such; the number of skyline points become too numerous to offer any interestinginsights. To find more important and meaningful skyline points in high dimensional space;we propose a new concept; called k-dominant skyline which relaxes the idea of dominanceto k-dominance. A point p is said to k-dominate another point q if there are k≤ d dimensionsin which p is better than or equal to q and is better in at least one of these k dimensions. A …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,432
Event specification in an active object-oriented database,Narain H Gehani; Hosagrahar V Jagadish; Oded Shmueli,Abstract The concept of a trigger is central to any active database. Upon the occurrence of atrigger event; the trigger is “fired”; ie; the trigger action is executed. We describe a model anda language for specifying basic and composite trigger events in the context of an object-oriented database. The specified events can be detected efficiently using finite automata.We integrate our model with O++; the database programming language for the ode objectdatabase being developed at AT&T Bell Labs. We propose a new Event-Action model;which folds into the event specification the condition part of the well-known Event-Condition-Action model and avoids the multiple coupling modes between the event; condition; andaction trigger components.,ACM sigmod record,1992,425
Big data and its technical challenges,HV Jagadish; Johannes Gehrke; Alexandros Labrinidis; Yannis Papakonstantinou; Jignesh M Patel; Raghu Ramakrishnan; Cyrus Shahabi,The growth rate of the output of current NGS methods in terms of the raw sequence data producedby a single NGS machine is shown in Figure 1; along with the performance increase for the SPECintCPU benchmark. Clearly; the NGS sequence data growth far outstrips the performance gainsoffered by Moore's Law for single-threaded applications (here; SPECint). Note the sequencedata size in Figure 1 is the output of analyzing the raw images that are actually produced by theNGS instruments. The size of these raw image datasets themselves is so large (many TBs perlab per day) that it is impractical today to even consider storing them. Rather; these images areanalyzed on the fly to produce sequence data; which is then retained … Big Data has the potentialto revolutionize much more than just research. Google's work on Google File System andMapReduce; and subsequent open source work on systems like Hadoop; have led to …,Communications of the ACM,2014,409
A retrieval technique for similar shapes,Hosagrahar V Jagadish,ABSTRACT We propose an organization for aclatabase of objects that permits the efficientretrieval of all objects in the database with a shape similar to a search template. The retrievaltechnique wepropse isrobust indtepresence of noise; and can handle several differentnotions of similarity inchrding changes in scale; position; and even relative sizes ofcomponents. We can thus have the computer reproduce; on a huge database of images; theprocess performed by a human in ''riffling” through a book; using an index structure toretrieve likely candidates quickly.,ACM SIGMOD Record,1991,392
Answering queries with aggregation using views,Divesh Srivastava; Shaul Dar; Hosagrahar V Jagadish; Alon Y Levy,Abstract We present novel algorithms for the problem of using materialized views to computeanswers to SQL queries with grouping and aggregation; in the presence of multiset tables. Inaddition to its obvious potential in query optimization; this problem is important in manyapplications; such as data warehousing; very large transaction recording systems; globalinformation systems and mobile computing; where access to local or cached materializedviews may be cheaper than access to the underlying database. Our contributions are thefollowing: First; we show that in the case where the query has grouping and aggregation butthe views do not; a view is usable in answering a query only if there is an isomorphismbetween the view and a portion of the query. Second; when the views also have groupingand aggregation we identify conditions under which the aggregation information present …,VLDB,1996,381
Indexing the distance: An efficient method to knn processing,Cui Yu; Beng Chin Ooi; Kian-Lee Tan; HV Jagadish,Abstract In this paper; we present an efficient method; called iDistance; for K-nearestneighbor (KNN) search in a high-dimensional space. iDistance partitions the data andselects a reference point for each partition. The data in each cluster are transformed into asingle dimensional space based on their similarity with respect to a reference point. Thisallows the points to be indexed using a B·-tree structure and KNN search be performedusing onedimensional range search. The choice of partition and reference point providesthe iDistance technique with degrees of freedom most other techniques do not have. Wedescribe how appropriate choices here can effectively adapt the index structure to the datadistribution. We conducted extensive experiments to evaluate the iDistance technique; andreport results demonstrating its effectiveness.,Vldb,2001,375
TAX: A tree algebra for XML,Hosagrahar Visvesvaraya Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Keith Thompson,Abstract Querying XML has been the subject of much recent investigation. A formal bulkalgebra is essential for applying database-style optimization to XML queries. We developsuch an algebra; called TAX (Tree Algebra for XML); for manipulating XML data; modeled asforests of labeled ordered trees. Motivated both by aesthetic considerations of intuitiveness;and by efficient computability and amenability to optimization; we develop TAX as a naturalextension of relational algebra; with a small set of operators. TAX is complete for relationalalgebra extended with aggregation; and can express most queries expressible in popularXML query languages. It forms the basis for the Timber XML database system currentlyunder development by us.,International Workshop on Database Programming Languages,2001,358
The Asilomar report on database research,Phil Bernstein; Michael Brodie; Stefano Ceri; David DeWitt; Mike Franklin; Hector Garcia-Molina; Jim Gray; Jerry Held; Joe Hellerstein; HV Jagadish; Michael Lesk; Dave Maier; Jeff Naughton; Hamid Pirahesh; Mike Stonebraker; Jeff Ullman,Abstract The database research community is rightly proud of success in basic research;and its remarkable record of technology transfer. Now the field needs to radically broaden itsresearch focus to attack the issues of capturing; storing; analyzing; and presenting the vastarray of online data. The database research community should embrace a broader researchagenda—broadening the definition of database management to embrace all the content ofthe Web and other online data stores; and rethinking our fundamental assumptions in light oftechnology shifts. To accelerate this transition; we recommend changing the way researchresults are evaluated and presented. In particular; we advocate encouraging morespeculative and long-range work; moving conferences to a poster format; and publishing allresearch literature on the Web.,ACM Sigmod record,1998,339
The New Jersey data reduction report,Daniel Barbar'a; William DuMouchel; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,Abstract this paper we describe and evaluate several popular techniques for data reduction.Historically; the primary need for data reduction has been internal to a database system; in acost-based query optimizer. The need is for the query optimizer to estimate the cost ofalternative query plans cheaply--clearly the effort required to do so must be much smallerthan the effort of actually executing the query; and yet the cost of executing any query plandepends strongly upon the numerosity of specified attribute values and the selectivities ofspecified predicates. To address these query optimizer needs; many databases keepsummary statistics. Sampling techniques have also been proposed. More recently; there hasbeen an explosion of interest in the analysis of data in warehouses. Data warehouses canbe extremely large; yet obtaining answers quickly is important. Often; it is quite acceptable …,IEEE Data Engineering Bulletin,1997,321
Independent quantization: An index compression technique for high-dimensional data spaces,Stefan Berchtold; Christian Bohm; Hosagrahar V Jagadish; H-P Kriegel; Jörg Sander,Two major approaches have been proposed to efficiently process queries in databases:speeding up the search by using index structures; and speeding up the search by operatingon a compressed database; such as a signature file. Both approaches have their limitations:indexing techniques are inefficient in extreme configurations; such as high-dimensionalspaces; where even a simple scan may be cheaper than an index-based search.Compression techniques are not very efficient in all other situations. We propose to combineboth techniques to search for nearest neighbors in a high-dimensional space. For thispurpose; we develop a compressed index; called the IQ-tree; with a three-level structure: thefirst level is a regular (flat) directory consisting of minimum bounding boxes; the second levelcontains data points in a compressed representation; and the third level contains the …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,262
On high dimensional skylines,Chee-Yong Chan; HV Jagadish; Kian-Lee Tan; Anthony KH Tung; Zhenjie Zhang,Abstract In many decision-making applications; the skyline query is frequently used to find aset of dominating data points (called skyline points) in a multi-dimensional dataset. In a high-dimensional space skyline points no longer offer any interesting insights as there are toomany of them. In this paper; we introduce a novel metric; called skyline frequency thatcompares and ranks the interestingness of data points based on how often they are returnedin the skyline when different number of dimensions (ie; subspaces) are considered.Intuitively; a point with a high skyline frequency is more interesting as it can be dominated onfewer combinations of the dimensions. Thus; the problem becomes one of finding top-kfrequent skyline points. But the algorithms thus far proposed for skyline computation typicallydo not scale well with dimensionality. Moreover; frequent skyline computation requires …,International Conference on Extending Database Technology,2006,261
ProTDB: Probabilistic Data in XML** Work supported in part by NSF under grant IIS-0002356.,Andrew Nierman; HV Jagadish,Traditional databases allow for the storage and retrieval of large amounts of data; but do notmake any concessions for uncertainty in the data. In many domains; it is difficult; if notimpossible; to state all information with 100% certainty. Scientific research; for example; issubject to a great deal of uncertainty and error that cannot be modeled by traditionaldatabase systems. Error-prone experimental machinery; polluted samples; and simplehuman error are a few of the many possible sources of this uncertainty. With the recentimportance of the web; and the many textual (and HTML encoded) sources of informationthat it makes available; information extraction has become a hot area. The idea is to usenatural language analysis tools to create structured representations of free-form textdocuments. This information extraction is an error-prone endeavor: even the best systems …,*,2002,259
Data integration using self-maintainable views,Ashish Gupta; Hosagrahar V Jagadish; Inderpal Singh Mumick,Abstract In this paper we define the concept of self-maintainable views—these are views thatcan be maintained using only the contents of the view and the database modifications;without accessing any of the underlying databases. We derive tight conditions under whichseveral types of select-project-join are self-maintainable upon insertions; deletions andupdates. Self-Maintainability is a desirable property for efficiently maintaining large views inapplications where fast response and high availability are important. One example of suchan environment is data warehousing wherein views are used for integrating data frommultiple databases.,International Conference on Extending Database Technology,1996,240
Structural join order selection for XML query optimization,Yuqing Wu; Jignesh M Patel; HV Jagadish,Structural join operations are central to evaluating queries against XML data; and aretypically responsible for consuming a lion's share of the query processing time. Thus;structural join order selection is at the heart of query optimization in an XML database; justas (value-based) join order selection is central to relational query optimization. We introducefive algorithms for structural join order optimization for XML tree pattern matching andpresent an extensive experimental evaluation. Our experiments demonstrate that manyrelational rules of thumb are no longer appropriate: for instance; using dynamicprogramming style optimization is not efficient; limiting consideration to left-deep plansusually misses the best solution. Our experiments also show that a dynamic programmingoptimization with pruning (DPP) algorithm can find the optimal solution; with low cost …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,228
Making database systems usable,HV Jagadish; Adriane Chapman; Aaron Elkiss; Magesh Jayapandian; Yunyao Li; Arnab Nandi; Cong Yu,Abstract Database researchers have striven to improve the capability of a database in termsof both performance and functionality. We assert that the usability of a database is asimportant as its capability. In this paper; we study why database systems today are sodifficult to use. We identify a set of five pain points and propose a research agenda toaddress these. In particular; we introduce a presentation data model and recommend directdata manipulation with a schema later approach. We also stress the importance ofprovenance and of consistency across presentation models.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,225
Online data mining for co-evolving time sequences,B-K Yi; Nikolaos D Sidiropoulos; Theodore Johnson; HV Jagadish; Christos Faloutsos; Alexandros Biliris,In many applications; the data of interest comprises multiple sequences that evolve overtime. Examples include currency exchange rates and network traffic data. We develop a fastmethod to analyze such co-evolving time sequences jointly to allow (a)estimation/forecasting of missing/delayed/future values;(b) quantitative data mining; and (c)outlier detection. Our method; MUSCLES; adapts to changing correlations among timesequences. It can handle indefinitely long sequences efficiently using an incrementalalgorithm and requires only a small amount of storage and less I/O operations. To make itscale for a large number of sequences; we present a variation; the Selective MUSCLESmethod and propose an efficient algorithm to reduce the problem size. Experiments on realdatasets show that MUSCLES outperforms popular competitors in prediction accuracy up …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,207
ASSET: A system for supporting extended transactions,Alexandros Biliris; Shaul Dar; N Gehani; HV Jagadish; Krithi Ramamritham,Abstract Extended transaction models in databases were motivated by the needs of complexapplications such as CAD and software engineering. Transactions in such applications havediverse needs; for example; they may be long lived and they may need to cooperate. Wedescribe ASSET; a system for supporting extended transactions. ASSET consists of a set oftransaction primitives that allow users to define custom transaction semantics to match theneeds of specific applications. We show how the transaction primitives can be used tospecify a variety of transaction models; including nested transactions; split transactions; andsagas. Application-specific transaction models with relaxed correctness criteria; andcomputations involving workflows; can also be specified using the primitives. We describethe implementation of the ASSET primitives in the context of the Ode database.,ACM SIGMOD Record,1994,202
Efficient provenance storage,Adriane P Chapman; Hosagrahar V Jagadish; Prakash Ramanan,Abstract As the world is increasingly networked and digitized; the data we store has moreand more frequently been chopped; baked; diced and stewed. In consequence; there is anincreasing need to store and manage provenance for each data item stored in a database;describing exactly where it came from; and what manipulations have been applied to it.Storage of the complete provenance of each data item can become prohibitively expensive.In this paper; we identify important properties of provenance that can be used toconsiderably reduce the amount of storage required. We identify three different techniques:a family of factorization processes and two methods based on inheritance; to decrease theamount of storage required for provenance. We have used the techniques described in thiswork to significantly reduce the provenance storage costs associated with constructing …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,197
Approximate XML joins,Sudipto Guha; HV Jagadish; Nick Koudas; Divesh Srivastava; Ting Yu,Abstract XML is widely recognized as the data interchange standard for tomorrow; becauseof its ability to represent data from a wide variety sources. Hence; XML is likely to be theformat through which data from multiple sources is integrated. In this paper we study theproblem of integrating XML data sources through correlations realized as join operations. Achallenging aspect of this operation is the XML document structure. Two documents mightconvey approximately or exactly the same information but may be quite different in structure.Consequently approximate match in structure; in addition to; content has to be folded in thejoin operation. We quantify approximate match in structure and content using well definednotions of distance. For structure; we propose computationally inexpensive lower and upperbounds for the tree edit distance metric between two trees. We then show how the tree …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,189
Vbi-tree: A peer-to-peer framework for supporting multi-dimensional indexing schemes,HV Jagadish; Beng Chin Ooi; Quang Hieu Vu; Rong Zhang; Aoying Zhou,Multi-dimensional data indexing has received much attention in a centralized database.However; not so much work has been done on this topic in the context of Peerto-Peersystems. In this paper; we propose a new Peer-to-Peer framework based on a balanced treestructure overlay; which can support extensible centralized mapping methods and queryprocessing based on a variety of multidimensional tree structures; including R-Tree; X-Tree;SSTree; and M-Tree. Specifically; in a network with N nodes; our framework guarantees thatpoint queries and range queries can be answered within O (logN) hops. We also provide aneffective load balancing strategy to allow nodes to balance their work load efficiently. Anexperimental assessment validates the practicality of our proposal.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,185
Similarity-based queries,HV Jagadish; Alberto O Mendelzon; Tova Milo,Abstract We develop a domain-independent framework for defining qneries in terms ofsimilarity of objects. Our framework has three components: a pattern language; atransformation rule language; and a query language. The pattern language specifiesclasses of objects; the transformation rule language defines similarity by specifying thesimilarity-preserving transformations; and the whole package is wrapped in a general querylanguage. The framework can be “tuned” to the needs of a specific application domain; suchas time sequences; molecules; text strings or images; by the choice of these languages. Wedemonstrate the framework by presenting a specific instance on a specific domain–thedomain of sequences. We start with sequences over a finite alphabet; and then considersequences over infinite ordered domains. The basic pattern language weuseis regular …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,183
Keyword search on structured and semi-structured data,Yi Chen; Wei Wang; Ziyang Liu; Xuemin Lin,Abstract Empowering users to access databases using simple keywords can relieve theusers from the steep learning curve of mastering a structured query language andunderstanding complex and possibly fast evolving data schemas. In this tutorial; we give anoverview of the state-of-the-art techniques for supporting keyword search on structured andsemi-structured data; including query result definition; ranking functions; result generationand top-k query processing; snippet generation; result clustering; query cleaning;performance optimization; and search quality evaluation. Various data models will bediscussed; including relational data; XML data; graph-structured data; data streams; andworkflows. We also discuss applications that are built upon keyword search; such askeyword based database selection; query generation; and analytical processing. Finally …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,180
Database modeling and design: logical design,Toby J Teorey; Sam S Lightstone; Tom Nadeau; HV Jagadish,Database Modeling and Design; Fifth Edition; focuses on techniques for database design inrelational database systems. This extensively revised fifth edition features clearexplanations; lots of terrific examples and an illustrative case; and practical advice; withdesign rules that are applicable to any SQL-based system. The common examples arebased on real-life experiences and have been thoroughly class-tested. This book isimmediately useful to anyone tasked with the creation of data models for the integration oflarge-scale enterprise data. It is ideal for a stand-alone data management course focused onlogical database design; or a supplement to an introductory text for introductory databasemanagement. In-depth detail and plenty of real-world; practical examples throughoutLoaded with design rules and illustrative case studies that are applicable to any SQL …,*,2011,179
View maintenance issues for the chronicle data model,Hosagrahar V Jagadish; Inderpal Singh Mumick; Abraham Silberschatz,Abstract To meet the stringent performance requirements of transaction recording systems;much of the recording and query processing functionality; which should preferably be in thedatabase; is actually implemented in the procedural application code; with the attendantdifficulties in development; modularization; maintenance; and evolution. To combat thisdeficiency; we propose a new data model; the chronicle model; which permits the capture;within the data model; of many computations common to transactional data recordingsystems. A central issue in our model is the incremental maintenance of materialized viewsin time independent of the size of the recorded stream. Within the chronicle model we studythe type of summary queries that can be answered by using persistent views. We measurethe complexity of a chronicle model by the complexity of incrementally maintaining its …,Proceedings of the fourteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1995,178
The RightPages image-based electronic library for alerting and browsing,Guy A.  Story; Lawrence O'Gorman; David Fox; Louise Levy  Schaper; HV Jagadish,The RightPages electronic library prototype system; which gives users full online libraryservices; is described. The prototype takes advantage of fast hardware; multimediaworkstations; and broadband networks to process scientific and technical journals for usersand to offer a service that: alerts them to the arrival of new journal articles matching theirinterest profiles; lets them immediately examine images of pages in the alerted articles andbrowse through other articles in the database; and enables them to order paper copies ofany articles in the database. The system runs on a local area network that connects one ormore scanning stations; a centralized document database server and multiple user stationsrunning X Windows servers. The RightPages interface runs as an X Windows application onSun workstations or X terminals. The system's image and document processing; including …,Computer,1992,172
-From Tree Patterns to Generalized Tree Patterns: On Efficient Evaluation of XQuery,Zhimin Chen; HV Jagadish; Laks VS Lakshmanan; Stelios Paparizos,This chapter discusses the efficient evaluation of XQuery. XQuery is the current de factostandard XML query language. A core operation in the evaluation of XQuery is the finding ofmatches for specified tree patterns; and there has been much work towards algorithms forfinding such matches efficiently. Multiple XPath expressions can be evaluated by computingone or more tree pattern matches. Several XQuery implementation efforts have beenreported around the world. A key construct in most XML query models is the so-called treepattern (query)(TP (Q)); which is a tree T with nodes labeled by variables; together with aBoolean formula F specifying constraints on the nodes and their properties; including theirtags; attributes; and contents. The chapter proposes a structure called generalized treepattern (GTP) for concise representation of a whole XQuery expression. Evaluating the …,*,2003,170
Estimating answer sizes for XML queries,Yuqing Wu; Jignesh M Patel; HV Jagadish,Abstract Estimating the sizes of query results; and intermediate results; is crucial to manyaspects of query processing. In particular; it is necessary for effective query optimization.Even at the user level; predictions of the total result size can be valuable in “next-step”decisions; such as query refinement. This paper proposes a technique to obtain query resultsize estimates effectively in an XML database. Queries in XML frequently specify structuralpatterns; requiring specific relationships between selected elements. Whereas traditionaltechniques can estimate the number of nodes (XML elements) that will satisfy a node-specific predicate in the query pattern; such estimates cannot easily be combined to provideestimates for the entire query pattern; since element occurrences are expected to have highcorrelation. We propose a solution based on a novel histogram encoding of element …,International Conference on Extending Database Technology,2002,170
Counting twig matches in a tree,Zhiyuan Chen; HV Jagadish; Flip Korn; Nick Koudas; S Muthukrishnan; Raymond Ng; Divesh Srivastava,Describes efficient algorithms for accurately estimating the number of matches of a smallnode-labeled tree; ie a twig; in a large node-labeled tree; using a summary data structure.This problem is of interest for queries on XML and other hierarchical data; to provide queryfeedback and for cost-based query optimization. Our summary data structure scalablyrepresents approximate frequency information about twiglets (ie small twigs) in the data tree.Given a twig query; the number of matches is estimated by creating a set of query twiglets;and combining two complementary approaches: set hashing; used to estimate the number ofmatches of each query twiglet; and maximal overlap; used to combine the query twigletestimates into an estimate for the twig query. We propose several estimation algorithms thatapply these approaches on query twiglets formed using variations on different twiglet …,Data Engineering; 2001. Proceedings. 17th International Conference on,2001,166
A compression technique to materialize transitive closure,HV Jagadish,Abstract An important feature of database support for expert systems is the ability of thedatabase to answer queries regarding the existence of a path from one node to another inthe directed graph underlying some database relation. Given just the database relation;answering such a query is time-consuming; but given the transitive closure of the databaserelation a table look-up suffices. We present an indexing scheme that permits the storage ofthe pre-computed transitive closure of a database relation in a compressed form. Theexistence of a specified tuple in the closure can be determined from this compressed storeby a single look-up followed by an index comparision. We show how to add nodes and arcsto the compressed closure incrementally. We also suggest how this compression techniquecan be used to reduce the effort required to compute the transitive closure.,ACM Transactions on Database Systems (TODS),1990,161
Using q-grams in a DBMS for approximate string processing,Luis Gravano; Panagiotis G.  Ipeirotis; Hosagrahar Visvesvaraya Jagadish; Nick Koudas; Shanmugauelayut Muthukrishnan; Lauri Pietarinen; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data. This is due; forexample; to the prevalence of typographical errors in data; and multiple conventions forrecording attributes such as name and address. Commercial databases do not supportapproximate string queries directly; and it is a challenge to implement this functionalityefficiently with user-defined functions (UDFs). In this paper; we develop a technique forbuilding approximate string processing capabilities on top of commercial databases byexploiting facilities already available in them. At the core; our technique relies on generatingshort substrings of length Õ; called Õ-grams; and processing them using standard methodsavailable in the DBMS. The proposed technique enables various approximate string …,IEEE Data Eng. Bull.,2001,158
Why not?,Adriane Chapman; HV Jagadish,Abstract As humans; we have expectations for the results of any action; eg we expect at leastone student to be returned when we query a university database for student records. Whenthese expectations are not met; traditional database users often explore datasets via aseries of slightly altered SQL queries. Yet most database access is via limited interfaces thatdeprive end users of the ability to alter their query in any way to garner better understandingof the dataset and result set. Users are unable to question why a particular data item is Notin the result set of a given query. In this work; we develop a model for answers to WHY NOT?queries. We show through a user study the usefulness of our answers; and describe twoalgorithms for finding the manipulation that discarded the data item of interest. Moreover; wework through two different methods for tracing the discarded data item that can be used …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,155
Efficient skyline computation over low-cardinality domains,Michael Morse; Jignesh M Patel; Hosagrahar V Jagadish,Abstract Current skyline evaluation techniques follow a common paradigm that eliminatesdata elements from skyline consideration by finding other elements in the dataset thatdominate them. The performance of such techniques is heavily influenced by the underlyingdata distribution (ie whether the dataset attributes are correlated; independent; or anti-correlated). In this paper; we propose the Lattice Skyline Algorithm (LS) that is built around anew paradigm for skyline evaluation on datasets with attributes that are drawn from low-cardinality domains. LS continues to apply even if one attribute has high cardinality. Manyskyline applications naturally have such data characteristics; and previous skyline methodshave not exploited this property. We show that for typical dimensionalities; the complexity ofLS is linear in the number of input tuples. Furthermore; we show that the performance of …,Proceedings of the 33rd international conference on Very large data bases,2007,155
Dali: A high performance main memory storage manager,Hosagrahar V Jagadish; Daniel Lieuwen; Rajeev Rastogi; Abraham Silberschatz; S Sudarshan,Abstract Performance needs of many database applications dictate that the entire databasebe stored in main memory. The Dali system is a main memory storage manager designed toprovide the persistence; availability and safety guarantees one typically expects from adiskresident database; while at the same time providing very high performance by virtue ofbeing tuned to support in-memory data. Dali follows the philosophy of treating all data;including system data; uniformly as database files that can be memory mapped and directlyaccessed/updated by user processes. Direct access provides high performance; slower; butmore secure; access is also provided through the use of a server process. Various featuresof Dali can be tailored to the needs of an application to achieve high performance-forexample; concurrency control and logging can be turned off if not desired; which enables …,VLDB,1994,153
Direct transitive closure algorithms: Design and performance evaluation,Rakesh Agrawal; Shaul Dar; HV Jagadish,Abstract We present new algorithms for computing transitive closure of large databaserelations. Unlike iterative algorithms; such as the seminaive and logarithmic algorithms; thetermination of our algorithms does not depend on the length of paths in the underlying graph(hence the name direct algorithms). Besides reachability computations; the proposedalgorithms can also be used for solving path problems. We discuss issues related to theefficient implementation of these algorithms; and present experimental results that show thedirect algorithms perform uniformly better than the iterative algorithms. A side benefit of thiswork is that we have proposed a new methodology for evaluating the performance ofrecursive queries.,ACM Transactions on Database Systems (TODS),1990,153
Mining Deviants in a Time Series Database.,HV Jagadish; Nick Koudas; S Muthukrishnan,Abstract Identifying outliers is an important data analysis function. Statisticians have longstudied techniques to identify outliers in a data set in the context of fitting the data to somemodel. In the case of time series data; the situation is more murky. For instance; the" typical"value could" drift" up or down over time; so the extrema may not necessarily be interesting.We wish to identify data points that are somehow anomalous or" surprising". We formallydefine the notion of a deviant in a time series; based on a representation sparsity metric. Wedevelop an efficient algorithm to identify deviants in a time series. We demonstrate how thistechnique can be used to locate interesting artifacts in time series data; and presentexperimental evidence of the value of our technique. As a side benefit; our algorithm areable to produce histogram representations of data; that,VLDB,1999,151
Spatial search with polyhedra,H Vi Jagadish,Range searches in multidimensional space have been studied extensively; and severalexcellent search structures have been devised. However; all of these require that the rangesin the different dimensions be specified independently. In other words; only rectangularregions can be specified and searched for. Similarly; nonpoint objects can be indexed onlyin terms of their bounding rectangles. However; polyhedral search regions and polyhedralbounding rectangles can often provide a much greater selectivity in the search. It is shownhow to use multiattribute search structures for polyhedral regions by mapping polyhedralregions into rectangular regions of a higher dimension. In particular; the P-tree (polyhedraltree) is introduced and shown to be an effective multiattribute index structure.,Data Engineering; 1990. Proceedings. Sixth International Conference on,1990,148
NaLIX: an interactive natural language interface for querying XML,Yunyao Li; Huahai Yang; HV Jagadish,Abstract Database query languages can be intimidating to the non-expert; leading to theimmense recent popularity for keyword based search in spite of its significant limitations. Theholy grail has been the development of a natural language query interface. We presentNaLIX; a generic interactive natural language query interface to an XML database. Oursystem can accept an arbitrary English language sentence as query input; which caninclude aggregation; nesting; and value joins; among other things. This query is translated;potentially after reformulation; into an XQuery expression that can be evaluated against anXML database. The translation is done through mapping grammatical proximity of naturallanguage parsed tokens to proximity of corresponding elements in the result XML. In thisdemonstration; we show that NaLIX; while far from being able to pass the Turing test; is …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,146
Direct Algorithms for Computing the Transitive Closure of Database Relations.,Rakesh Agrawal; HV Jagadish,We present new algorithms for computing the transitive closure of large database relations.Unlike iterative algorithms; such as the semi-naive and the logarithmic algorithms; thetermination of our algorithms does not depend on the length of paths in the underlying graph(hence; the name direct algorithms). We also present simulation results that show that thesedirect algorithms perform uniformly better than the best of the iterative algorithms. A sidebenefit of this work is that we have proposed a new methodology for evaluating theperformance of recursive queries.,VLDB,1987,144
A study of transitive closure as a recursion mechanism,HV Jagadish; Rakesh Agrawal; Linda Ness,Abstract We show that every linearly recursive query can be expressed as a transitiveclosure possibly preceded and followed by operations already available in relationalalgebra. This reduction is possible even if there are repeated variables in the recursiveliterals and if some of the arguments in the recursive literals are constants. Such anequivalence has significant theoretical and practical ramifications. One the one hand itinfluences the design of expressive notations to capture recursion as an augmentation ofrelational query languages. On the other hand implementation of deductive databases isimpacted in that the design does not have to provide the generality that linear recursionwould demand. It suffices to study the single problem of transitive closure and to provide anefficient implementation for it.,ACM SIGMOD Record,1987,137
Metscape 2 bioinformatics tool for the analysis and visualization of metabolomics and gene expression data,Alla Karnovsky; Terry Weymouth; Tim Hull; V Glenn Tarcea; Giovanni Scardoni; Carlo Laudanna; Maureen A Sartor; Kathleen A Stringer; HV Jagadish; Charles Burant; Brian Athey; Gilbert S Omenn,Abstract Motivation: Metabolomics is a rapidly evolving field that holds promise to provideinsights into genotype–phenotype relationships in cancers; diabetes and other complexdiseases. One of the major informatics challenges is providing tools that link metabolite datawith other types of high-throughput molecular data (eg transcriptomics; proteomics); andincorporate prior knowledge of pathways and molecular interactions. Results: We describe anew; substantially redesigned version of our tool Metscape that allows users to enterexperimental data for metabolites; genes and pathways and display them in the context ofrelevant metabolic networks. Metscape 2 uses an internal relational database that integratesdata from KEGG and EHMN databases. The new version of the tool allows users to identifyenriched pathways from expression profiling data; build and analyze the networks of …,Bioinformatics,2011,133
Michigan Molecular Interactions (MiMI): putting the jigsaw puzzle together,Magesh Jayapandian; Adriane Chapman; V Glenn Tarcea; Cong Yu; Aaron Elkiss; Angela Ianni; Bin Liu; Arnab Nandi; Carlos Santos; Philip Andrews; Brian Athey; David States; HV Jagadish,Abstract Protein interaction data exists in a number of repositories. Each repository has itsown data format; molecule identifier and supplementary information. Michigan MolecularInteractions (MiMI) assists scientists searching through this overwhelming amount of proteininteraction data. MiMI gathers data from well-known protein interaction databases and deep-merges the information. Utilizing an identity function; molecules that may have differentidentifiers but represent the same real-world object are merged. Thus; MiMI allows the usersto retrieve information from many different databases at once; highlighting complementaryand contradictory information. To help scientists judge the usefulness of a piece of data;MiMI tracks the provenance of all data. Finally; a simple yet powerful user interface aidsusers in their queries; and frees them from the onerous task of knowing the data format or …,Nucleic acids research,2006,130
Schema summarization,Cong Yu; HV Jagadish,Abstract Real database systems can often be very complex. A person wishing to access datafrom an unfamiliar database has the daunting task of understanding its schema before beingable to pose a correct query against it. A schema summary can be of great help; providing asuccinct overview of the entire schema; and making it possible to explore in depth only therelevant schema components. In this paper we formally define a schema summary and twodesirable properties (in addition to minimizing size) of a summary: presenting importantschema elements and achieving broad information coverage. We develop algorithms thatallow us to automatically generate schema summaries based on these two goals. We furtherdevelop an objective metric for assessing the quality of a schema summary using queryinformation. Experimental evaluation using this metric demonstrates that the summaries …,Proceedings of the 32nd international conference on Very large data bases,2006,130
ConceptGen: a gene set enrichment and gene set relation mapping tool,Maureen A Sartor; Vasudeva Mahavisno; Venkateshwar G Keshamouni; James Cavalcoli; Zachary Wright; Alla Karnovsky; Rork Kuick; HV Jagadish; Barbara Mirel; Terry Weymouth; Brian Athey; Gilbert S Omenn,Abstract Motivation: The elucidation of biological concepts enriched with differentiallyexpressed genes has become an integral part of the analysis and interpretation of genomicdata. Of additional importance is the ability to explore networks of relationships amongpreviously defined biological concepts from diverse information sources; and to exploreresults visually from multiple perspectives. Accomplishing these tasks requires a unifiedframework for agglomeration of data from various genomic resources; novel visualizations;and user functionality. Results: We have developed ConceptGen; a web-based gene setenrichment and gene set relation mapping tool that is streamlined and simple to use.ConceptGen offers over 20 000 concepts comprising 14 different types of biologicalknowledge; including data not currently available in any other gene set enrichment or …,Bioinformatics,2009,129
Automated creation of a forms-based database query interface,Magesh Jayapandian; HV Jagadish,Abstract Forms-based query interfaces are widely used to access databases today. Thedesign of a forms-based interface is often a key step in the deployment of a database. Eachform in such an interface is capable of expressing only a very limited range of queries.Ideally; the set of forms as a whole must be able to express all possible queries that any usermay have. Creating an interface that approaches this ideal is surprisingly hard. In this paper;we seek to maximize the ability of a forms-based interface to support queries a user mayask; while bounding both the number of forms and the complexity of any one form. Given adatabase schema and content we present an automated technique to generate a good set offorms that meet the above desiderata. While a careful analysis of real or expected queryworkloads are useful in designing the interface; these query sets are often unavailable or …,Proceedings of the VLDB Endowment,2008,129
Method for reducing the delay between the time a data page is requested and the time the data page is displayed,*,The apparent speed of a connection between a browser at a user station and a proxy orgateway on a network such as the Internet is increased by providing a local proxy at the userstation which interacts with a remote proxy. While the remote proxy is retrieving a newlyrequested World Wide Web page; for example; from the appropriate content provider; it mayalso be sending to the local proxy a stale cached version of that page. When the newversion of the page is finally retrieved; the remote proxy determines the differences betweenthe new version and the stale version; and; assuming the differences do not exceed the newpage in size; sends the differences to the local proxy which then reconstructs the new pagefrom the differences and the stale version. The local proxy delivers the new page to thebrowser; which need not even be aware that a local proxy exists; it is aware only that it …,*,1999,129
Integrating and annotating the interactome using the MiMI plugin for cytoscape,Jing Gao; Alex S Ade; V Glenn Tarcea; Terry E Weymouth; Barbara R Mirel; HV Jagadish; David J States,Abstract Summary: The MiMI molecular interaction repository integrates data from multiplesources; resolves interactions to standard gene names and symbols; links to annotation datafrom GO; MeSH and PubMed and normalizes the descriptions of interaction type. Here; wedescribe a Cytoscape plugin that retrieves interaction and annotation data from MiMI andlinks out to multiple data sources and tools. Community annotation of the interactome issupported. Availability: MiMI plugin v3. 0.1 can be installed from within Cytoscape 2.6 usingthe Cytoscape plugin manager in 'Network and Attribute I/0'category. The plugin is alsopreloaded when Cytoscape is launched using Java WebStart at http://mimi. ncibi. org byquerying a gene and clicking 'View in MiMI Plugin for Cytoscape'link. Contact: dstates@umich. edu,Bioinformatics,2008,128
Querying structured text in an XML database,Shurug Al-Khalifa; Cong Yu; HV Jagadish,Abstract XML databases often contain documents comprising structured text. Therefore; it isimportant to integrate" information retrieval style" query evaluation; which is well-suited fornatural language text; with standard" database style" query evaluation; which handlesstructured queries efficiently. Relevance scoring is central to information retrieval. In thecase of XML; this operation becomes more complex because the data required for scoringcould reside not directly in an element itself but also in its descendant elements. In thispaper; we propose a bulk-algebra; TIX; and describe how it can be used as a basis forintegrating information retrieval techniques into a standard pipelined database queryevaluation engine. We develop new evaluation strategies essential to obtaining goodperformance; including a stack-based TermJoin algorithm for efficiently scoring composite …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,125
Local logging in a distributed database management computer system,*,A distributed database management computer system includes a plurality of nodes and aplurality of database pages. When a first node in the computer system updates a firstdatabase page; the first node generates a log record. The first node determines whether itmanages the first database page. If the first node determines that it manages the firstdatabase page; the first node writes the log record to a log storage local to the first node.However; if the first node determines that it does not manage the first database page; the firstnode then determines whether it includes a local log storage. If the first node includes a locallog storage; the first node writes the log record to the local log storage; even if the first nodedoes not manage the first database page. If the first node does not include a local logstorage; the first node sends the log record to a second node managing the first database …,*,1999,122
Speeding up search in peer-to-peer networks with a multi-way tree structure,HV Jagadish; Beng Chin Ooi; Kian-Lee Tan; Quang Hieu Vu; Rong Zhang,Abstract Peer-to-Peer systems have recently become a popular means to share resources.Effective search is a critical requirement in such systems; and a number of distributed searchstructures have been proposed in the literature. Most of these structures provide" log timesearch" capability; where the logarithm is taken base 2. That is; in a system with N nodes;the cost of the search is O (log 2 N). In database systems; the importance of large fanoutindex structures has been well recognized. In P2P search too; the cost could be reducedconsiderably if this logarithm were taken to a larger base. In this paper; we propose a multi-way tree search structure; which reduces the cost of search to O (log m N); where m is thefanout. The penalty paid is a larger update cost; but we show how to keep this penalty to beno worse than linear in m. We experimentally explore this tradeoff between search and …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,121
What can hierarchies do for data warehouses?,Hosagrahar V Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Abstract Data in a warehouse typically has multiple dimensions of interest; such as location;time; and product. It is well-recognized that these dimensions have hierarchies defined onthem; such as" store-city-state-region" for location. The standard way to model such data iswith a star/snowflake schema. However; current approaches do not give a first-class statusto dimensions. Consequently; a substantial class of interesting queries involving dimensionhierarchies and their interaction with the fact tables are quite verbose to write; hard to read;and difficult to optimize. We propose the SQL (%) model and a natural extension to the SQLquery language; that gives a first-class status to dimensions; and we pin down its semantics.Our model permits structural and schematic heterogeneity in dimension hierarchies;situations often arising in practice that cannot be modeled satisfactorily using the star …,VLDB,1999,119
Tree logical classes for efficient evaluation of XQuery,Stelios Paparizos; Yuqing Wu; Laks VS Lakshmanan; HV Jagadish,Abstract XML is widely praised for its flexibility in allowing repeated and missing sub-elements. However; this flexibility makes it challenging to develop a bulk algebra; whichtypically manipulates sets of objects with identical structure. A set of XML elements; say oftype book; may have members that vary greatly in structure; eg in the number of author sub-elements. This kind of heterogeneity may permeate the entire document in a recursivefashion: eg; different authors of the same or different book may in turn greatly vary instructure. Even when the document conforms to a schema; the flexible nature of schemas forXML still allows such significant variations in structure among elements in a collection. Bulkprocessing of such heterogeneous sets is problematic. In this paper; we introduce the notionof logical classes (LC) of pattern tree nodes; and generalize the notion of pattern tree …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,117
COMPOSE: A system for composite specification and detection,Narain Gehani; HV Jagadish; Oded Shmueli,An" event" is a happening of interest. Events can be simple such as; the stock price goingabove a certain price; the beginning of a transaction; the update of an object; or thetemperature going above a specified limit. New events can also be formed as a combinationof other events; for example; three successive discount rate cuts without an interveningincrease; all withdrawals following a'large deposit; and the temperature going above aspecified limit and staying there for more than some time period. We call such events"composite events". We have developed a model for specifying composite events [216; 215].We were motivated to explore the specification of composite events as part of an effort todesign" trigger" facilities for the Ode object database [7;217]. Triggers are the key facility thatdistinguishes active databases [138;524; 40;378;556;395] from passive databases. A …,*,1993,112
Assisted querying using instant-response interfaces,Arnab Nandi; HV Jagadish,Abstract We demonstrate a novel query interface that enables users to construct a richsearch query without any prior knowledge of the underlying schema or data. The interface;which is in the form of a single text input box; interacts in real-time with the users as theytype; guiding them through the query construction. We discuss the issues of schema anddata complexity; result size estimation; and query validity; and provide novel approaches tosolving these problems. We demonstrate our query interface on two popular applications; anenterprise-wide personnel search; and a biological information database.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,110
Regular expression learning for information extraction,Yunyao Li; Rajasekar Krishnamurthy; Sriram Raghavan; Shivakumar Vaithyanathan; HV Jagadish,Abstract Regular expressions have served as the dominant workhorse of practicalinformation extraction for several years. However; there has been little work on reducing themanual effort involved in building high-quality; complex regular expressions for informationextraction tasks. In this paper; we propose ReLIE; a novel transformation-based algorithm forlearning such complex regular expressions. We evaluate the performance of our algorithmon multiple datasets and compare it against the CRF algorithm. We show that ReLIE; inaddition to being an order of magnitude faster; outperforms CRF under conditions of limitedtraining data and cross-domain data. Finally; we show how the accuracy of CRF can beimproved by using features extracted by ReLIE.,Proceedings of the Conference on Empirical Methods in Natural Language Processing,2008,109
Challenges and Opportunities with big data 2011-1,Divyakant Agrawal; Philip Bernstein; Elisa Bertino; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Alon Halevy; Jiawei Han; HV Jagadish; Alexandros Labrinidis; Sam Madden; Yannis Papakonstantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Cyrus Shahabi; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom,Abstract The promise of data-driven decision-making is now being recognized broadly; andthere is growing enthusiasm for the notion of``Big Data.''While the promise of Big Data is real--for example; it is estimated that Google alone contributed 54 billion dollars to the USeconomy in 2009--there is currently a wide gap between its potential and its realization.,*,2011,107
Semantic compression and pattern extraction with fascicles,HV Jagadish; Jason Madar; Raymond T Ng,Abstract Often many records in a database share similar values for several attributes. If oneis able to identify and group together records that share similar values for some—even if notall—at—tributes; one can both obtain a more parsimo—nious representation of the data; andgain useful insight into the data from a mining perspective. In this paper; we introduce thenotion of fasci-cles. A fascicle F (k; t) is a subset of records that have If compact attributes. Anattribute A of a collection F of records is compact if the width of the range of A—values (fornumeric attributes) or the number of distinct A—values (for categorical attributes) of all therecords in F does not ex—ceed t. We introduce and study two problems re—lated tofascicles. First; we consider how to ﬁnd fascicles such that the total storage of the rela—tionis minimized. Second; we study how best to extract fascicles whose sizes exceed a given …,VLDB,1999,106
Partitioning techniques for large-grained parallelism,Rakesh Agrawal; HV Jagadish,A model is presented for parallel processing in loosely coupled multiprocessingenvironments; such as networks of computer workstations; that are amenable to large-grained parallelism. The model takes into account the overhead involved in datacommunication to and from a remote processor and can be used to partition a large class ofcomputations optimally; consisting of computations that can be organized as a one-level treeand are homogeneous and separable. The optimal partition can be determined for a givennumber processors; and; if required; the optimal number of processors to use can also bederived. Experimental results validate the model and demonstrate its effectiveness.,IEEE Transactions on Computers,1988,105
Integrity maintenance in an object-oriented database,HV Jagadish; Xiaolei Qian,ABSTRACT We present an approach for integrating inter-object constraint maintenanceseamlessly into an objectoriented database system. We develop a constraint compilationscheme that accepts declarative global specification of constraints; including relationalintegrity; referential integrity; and uniqueness requirements; and generates an efficientrepresentation that permits localized processing. We demonstrate the feasibility of ourapproach by designing a constraint pre-processor for O++; the programming languageinterface to the Ode object-oriented database.,VLDB,1992,102
Michigan molecular interactions r2: from interacting proteins to pathways,V Glenn Tarcea; Terry Weymouth; Alex Ade; Aaron Bookvich; Jing Gao; Vasudeva Mahavisno; Zach Wright; Adriane Chapman; Magesh Jayapandian; Arzucan Özgür; Yuanyuan Tian; Jim Cavalcoli; Barbara Mirel; Jignesh Patel; Dragomir Radev; Brian Athey; David States; HV Jagadish,Abstract Molecular interaction data exists in a number of repositories; each with its own dataformat; molecule identifier and information coverage. Michigan molecular interactions (MiMI)assists scientists searching through this profusion of molecular interaction data. The originalrelease of MiMI gathered data from well-known protein interaction databases; and deepmerged this information while keeping track of provenance. Based on the feedback receivedfrom users; MiMI has been completely redesigned. This article describes the resulting MiMIRelease 2 (MiMIr2). New functionality includes extension from proteins to genes and topathways; identification of highlighted sentences in source publications; seamless two-waylinkage with Cytoscape; query facilities based on MeSH/GO terms and other concepts;approximate graph matching to find relevant pathways; support for querying in bulk; and a …,Nucleic acids research,2008,99
X^ 3: A cube operator for xml olap,Nuwee Wiwatwattana; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,With increasing amounts of data being exchanged and even generated or stored in XML; anatural question is how to perform OLAP on XML data; which can be structurallyheterogeneous (eg; parse trees) and/or marked-up text documents. A core operator forOLAP is the data cube. While the relational cube can be extended in a straightforward wayto XML; we argue such an extension would not address the specific issues posed by XML.While in a relational warehouse; facts are flat records and dimensions may have hierarchies;in an XML warehouse; both facts and dimensions may be hierarchical. Second; XML isflexible:(a) an element may have missing or repeated subelements;(b) different instances ofthe same element type may have different structure. We identify the challenges introducedby these features of XML for cube definition and computation. We propose a definition for …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,99
The Michigan benchmark: towards XML query performance diagnostics,Kanda Runapongsa; Jignesh M Patel; HV Jagadish; Yun Chen; Shurug Al-Khalifa,Abstract We propose a micro-benchmark for XML data management to aid engineers indesigning improved XML processing engines. This benchmark is inherently different fromapplication-level benchmarks; which are designed to help users choose between alternativeproducts. We primarily attempt to capture the rich variety of data structures and distributionspossible in XML; and to isolate their effects; without imitating any particular application. Thebenchmark specifies a single data set against which carefully specified queries can be usedto evaluate system performance for XML data with various characteristics. We have used thebenchmark to analyze the performance of three database systems: two native XML DBMSs;and a commercial ORDBMS. The benchmark reveals key strengths and weaknesses ofthese systems. We find that commercial relational techniques are effective for XML query …,Information Systems,2006,98
Effective phrase prediction,Arnab Nandi; HV Jagadish,Abstract Autocompletion is a widely deployed facility in systems that require user input.Having the system complete a partially typed" word" can save user time and effort. In thispaper; we study the problem of autocompletion not just at the level of a single" word"; but atthe level of a multi-word" phrase". There are two main challenges: one is that the number ofphrases (both the number possible and the number actually observed in a corpus) iscombinatorially larger than the number of words; the second is that a" phrase"; unlike a"word"; does not have a well-defined boundary; so that the autocompletion system has todecide not just what to predict; but also how far.,Proceedings of the 33rd international conference on Very large data bases,2007,96
Analysis of the Hilbert curve for representing two-dimensional space,Hosagrahar V Jagadish,Abstract We give closed-form expressions for the average number of runs to cover anarbitrary square region in two dimensions using a Hilbert curve. The practical use of thederived formula is that it allows the estimation of the quality of the linearization obtained byusing the Hilbert curve to map from a two-dimensional space to a one-dimensional space.Hilbert curves are used extensively as a basis for multi-dimensional indexing structures; andfor declustering multi-dimensional data.,Information Processing Letters,1997,96
Array architectures for iterative algorithms,Hosagrahar V Jagadish; Sailesh K Rao; Thomas Kailath,Regular mesh-connected arrays are shown to be isomorphic to a class of so-called regulariterative algorithms. For a wide variety of problems it is shown how to obtain appropriateiterative algorithms and then how to translate these algorithms into arrays in a systematicfashion. Several" systolic" arrays presented in the literature are shown to be specific cases ofthe variety of architectures that can be derived by the techniques presented here. Theseinclude arrays for Fourier Transform; Matrix Multiplication; and Sorting.,Proceedings of the IEEE,1987,96
Grouping in XML,Stelios Paparizos; Shurug Al-Khalifa; HV Jagadish; Laks Lakshmanan; Andrew Nierman; Divesh Srivastava; Yuqing Wu,Abstract XML permits repeated and missing sub-elements; and missing attributes. Wediscuss the consequent implications on grouping; both with respect to specification and withrespect to implementation. The techniques described here have been implemented in theTIMBER native XML database system being developed at the University of Michigan.,International Conference on Extending Database Technology,2002,89
XML schema refinement through redundancy detection and normalization,Cong Yu; HV Jagadish,Abstract As XML becomes increasingly popular; XML schema design has become anincreasingly important issue. One of the central objectives of good schema design is to avoiddata redundancies: redundantly stored information can lead not just only to a higher datastorage cost but also to increased costs for data transfer and data manipulation.Furthermore; such data redundancies can lead to potential update anomalies; rendering thedatabase inconsistent. One strategy to avoid data redundancies is to design redundancy-free schema from the start on the basis of known functional dependencies. We observe thatXML databases are often" casually designed" and XML FDs may not be determined inadvance. Under such circumstances; discovering XML data redundancies from the dataitself becomes necessary and is an integral part of the schema refinement (or re-design) …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,88
Expressive query specification through form customization,Magesh Jayapandian; HV Jagadish,Abstract A form-based query interface is usually the preferred means to provide anunsophisticated user access to a database. Not only is such an interface easy to use;requiring no technical training; but it also requires little or no knowledge of how the data isstructured in the database. However; a typical form is static and can express only a verylimited set of queries; Without room for change; query specification is limited by the expertiseand vision of the interface developer at the time the form was created. If an available formcannot express a desired query; the user is stuck. In this paper; we propose a mechanism tolet a user modify an existing form to express the desired query. These modifications arethemselves specified through filling forms to create an expression in an underlying formmanipulation expression language we define. The technical sophistication required to …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,86
Colorful XML: one hierarchy isn't enough,HV Jagadish; Laks VS Lakshmanan; Monica Scannapieco; Divesh Srivastava; Nuwee Wiwatwattana,Abstract XML has a tree-structured data model; which is used to uniformly representstructured as well as semi-structured data; and also enable concise query specification inXQuery; via the use of its XPath (twig) patterns. This in turn can leverage the recentlydeveloped technology of structural join algorithms to evaluate the query efficiently. In thispaper; we identify a fundamental tension in XML data modeling:(i) data represented as deeptrees (which can make effective use of twig patterns) are often un-normalized; leading toupdate anomalies; while (ii) normalized data tends to be shallow; resulting in heavy use ofexpensive value-based joins in queries. Our solution to this data modeling problem is anovel multi-colored trees (MCT) logical data model; which is an evolutionary extension of theXML data model; and permits trees with multi-colored nodes to signify their participation …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,86
Compressed accessibility map: Efficient access control for XML,Ting Yu; Laks VS Lakshmanan; Divesh Srivastava; HV Jagadish,This chapter proposes a space-and time-efficient solution to the access control problem foreXtensible Markup Language (XML) data. The solution is based on a novel notion of acompressed accessibility map (CAM); which compactly identifies the XML data items towhich a user has access; by exploiting structural locality of accessibility in tree-structureddata. Transacting business over the Internet using XML is becoming more and more of areality as one moves towards a world of Internet-worked data; with applications requiringaccess to data on the Internet. In this setting; there is a need for much more sophisticatedtypes of access control than is permitted by simple firewalls. XML is widely regarded as apromising means for data representation integration; and exchange. As companies transactbusiness over the Internet; the sensitive nature of the information mandates that access …,*,2002,86
Metscape: a Cytoscape plug-in for visualizing and interpreting metabolomic data in the context of human metabolic networks,Jing Gao; V Glenn Tarcea; Alla Karnovsky; Barbara R Mirel; Terry E Weymouth; Christopher W Beecher; James D Cavalcoli; Brian D Athey; Gilbert S Omenn; Charles F Burant; HV Jagadish,Summary: Metscape is a plug-in for Cytoscape; used to visualize and interpret metabolomicdata in the context of human metabolic networks. We have developed a metabolite databaseby extracting and integrating information from several public sources. By querying thisdatabase; Metscape allows users to trace the connections between metabolites and genes;visualize compound networks and display compound structures as well as information forreactions; enzymes; genes and pathways. Applying the pathway filter; users can createsubnetworks that consist of compounds and reactions from a given pathway. Metscapeallows users to upload experimental data; and visualize and explore compound networksover time; or experimental conditions. Color and size of the nodes are used to visualizethese dynamic changes. Metscape can display the entire metabolic network or any of the …,Bioinformatics,2010,85
A signature technique for similarity-based queries,Christos Faloutsos; HV Jagadish; Alberto O Mendelzon; Tova Milo,Jagadish et al.(see Proc. ACM SIGACT-SIGMOD-SIGART PODS; p. 36-45; 1995) developeda general framework for posing queries based on similarity. The framework enables a formaldefinition of the notion of similarity for an application domain of choice; and then its use inqueries to perform similarity-based search. We adapt this framework to the specializeddomain of real-valued sequences.(Although some of the ideas we present are applicable toother types of data as well). In particular we focus on whole-match queries. By whole-matchquery we mean the case where the user has to specify the whole sequence. Similarity-based search can be computationally very expensive. The computation cost dependsheavily on the length of sequences being compared. To make such similarity testing feasibleon large data sets; we propose the use of a signature based technique. In a nutshell; our …,Compression and Complexity of Sequences 1997. Proceedings,1997,85
Recovering from Main-Memory Lapses.,HV Jagadish; Abraham Silberschatz; S Sudarshan,Abstract Recovery activities; like logging; checkpointing and restart; are used to restore adatabase to a consistent state after a system crash has occurred. Recovery related overheadis particularly troublesome in a mainmemory database where I/O activities are performed forthe sole purpose of ensuring data durability. In this paper we present a recovery techniquefor main-memory databases; whose bene ts are as follows. First; disk I/O is reduced bylogging to disk only redo records during normal execution. The undo log is normally residentonly in main memory; and is garbage collected after transaction commit. Second; ourtechnique reduces lock contention on account of the checkpointer by allowing actionconsistent checkpointing| to do so; the checkpointer writes to disk relevant parts of the undolog. Third; the recovery algorithm makes only a single pass over the log. Fourth; our …,VLDB,1993,85
Multiprocessor transitive closure algorithms,Rakesh Agrawal; HV Jagadish,Abstract We present parallel algorithms to compute the transitive closure of a databaserelation. These algorithms are applicable both on shared-memory and message-passingarchitectures. Experimental verification shows an almost linear speed-up with thesealgorithms.,Proceedings of the first international symposium on Databases in parallel and distributed systems,2000,84
Incremental organization for data recording and warehousing,HV Jagadish; PPS Narayan; Sridhar Seshadri; S Sudarshan; Rama Kanneganti,Abstract Data warehouses and recording systems typically have a large continuous streamof incoming data; that must be stored in a manner suitable for future access. Access to storedrecords is usually based on a key. Organizing the data on disk as the data arrives usingstandard techniques would result in either (a) one or more I/Os to store each incomingrecord (to keep the data clustered by the key); which is too expensive when data arrival ratesare very high; or (b) many I/Os to locate records for a particular customer (if data is storedclustered by arrival order). We study two techniques; inspired by external sorting algorithms;to store data incrementally as it arrives; simultaneously providing good performance forrecording and querying. We present concurrency control and recovery schemes for bothtechniques. We show the bene ts of our techniques both analytically and experimentally.,VLDB,1997,84
Automating the design and construction of query forms,Magesh Jayapandian; HV Jagadish,One of the simplest ways to query a database is through a form where a user can fill inrelevant information and obtain desired results by submitting the form. Designing good formsis a nontrivial manual task; and the designer needs a sound understanding of both the dataorganization and the querying needs. Furthermore; form design usually has conflictinggoals: each form should be simple and easy to understand; while collectively; the interfacemust support as many queries as possible. In this paper; we present a framework forgenerating forms in an automatic and principled way; given a database and a sample queryworkload. We design a tunable clustering algorithm for establishing form structure based onmultiple" similar" queries; which includes a mechanism for extending forms to support future"similar" queries. The algorithm is adaptive and can incrementally adjust forms to reflect …,IEEE Transactions on Knowledge and Data Engineering,2009,82
Recovering information from summary data,Christos Faloutsos; HV Jagadish; Nikolaos D Sidiropoulos,Data is often stored in summarized form; as a histogram of aggregates (COUNTs; SUMs; orAVeraGes) over specified ranges. Queries regarding specific values; or ranges different fromthose stored; cannot be answered exactly from the summarized data. In this paper we studyhow to estimate the original detail data from the stored summary. We formulate this task asan inverse problem; specifying a well-defined cost function that has to be optimized underconstraints. In particular; we propose the use of a Linear Regularization method; whichﲭaximizes the smoothness of the estimate. Our main theoretical contribution is a Theorem;which shows that; for smooth enough distributions; we can achieve full recovery fromsummary data. Our theorem is closely related to the well known Shannon-Nyquist samplingtheorem. We describe how to apply this theory to a variety of database problems; that …,*,1997,82
Algorithms for searching massive graphs,Rakesh Agrawal; HV Jagadish,Given a large graph; stored on disk; there is often a need to perform a search over thisgraph. Such a need could arise; for example; in the search component of a data-intensiveexpert system or to solve path problems in deductive database systems. In this paper; wepresent a novel data structuring technique and show how a branch-and-bound searchalgorithm can use this data structuring to prune the search space. Simulation results confirmthat; using these techniques; a search can be expedited significantly without incurring alarge storage penalty. As a side benefit; it is possible to organize the search to obtainsuccessive approximations to the desired solution with considerable reduction in the totalsearch.,IEEE Transactions on Knowledge and Data Engineering,1994,82
Querying network directories,HV Jagadish; Laks VS Lakshmanan; Tova Milo; Divesh Srivastava; Dimitra Vista,Abstract Heirarchically structured directories have recently proliferated with the growth of theInternet; and are being used to store not only address books and contact information forpeople; but also personal profiles; network resource information; and network and servicepolicies. These systems provide a means for managing scale and heterogeneity; whileallowing for conceptual unity and autonomy across multiple directory servers in the network;in a way for superior to what conventional relational or object-oriented databases offer. Yet;in deployed systems today; much of the data is modeled in an ad hoc manner; and many ofthe more sophisticated “queries” involve navigational access. In this paper; we develop thecore of a formal data model for network directories; and propose a sequence of efficientlycomputable query languages with increasing expressive power. The directory data model …,ACM SIGMOD Record,1999,80
On effective multi-dimensional indexing for strings,HV Jagadish; Nick Koudas; Divesh Srivastava,Abstract As databases have expanded in scope from storing purely business data to includeXML documents; product catalogs; e-mail messages; and directory data; it has becomeincreasingly important to search databases based on wild-card string matching: prefixmatching; for example; is more common (and useful) than exact matching; for such data. Inmany cases; matches need to be on multiple attributes/dimensions; with correlationsbetween the dimensions. Traditional multi-dimensional index structures; designed with(fixed length) numeric data in mind; are not suitable for matching unbounded length stringdata. In this paper; we describe a general technique for adapting a multi-dimensional indexstructure for wild-card indexing of unbounded length string data. The key ideas are (a) acarefully developed mapping function from strings to rational numbers;(b) representing an …,ACM SIGMOD Record,2000,79
Telephone billing with customized billing information,*,A method and system for automatically generating telephone bills that include customerdefined or requested billing information. Customer specific data including billing parametersfor a customer is stored. A record describing a telephone call between the customer and aparty is generated at a network switch. The record is received at a billing analysis system;the billing information relating to the party is determined using the record and at least onebilling parameter and a telephone bill for the customer including billing information relatingto the party is generated. In one embodiment of the present invention; customer specific dataentries including at least one billing parameter and customer information are stored for eachof a plurality of a customers and a telephone bill for the first customer including billinginformation relating to the second customer is generated based on a billing parameter for …,*,1999,79
Substring selectivity estimation,HV Jagadish; Raymond T Ng; Divesh Srivastava,Abstract With the explosion of the Internet; LDAP directories and XML; there is an evergreater need to evaluate queries involving (sub) string matching. Effective queryoptimization in this context requires good selectivity estimates. In this paper; we use prunedcount-suffix trees as the basic framework for substring selectivity estimation. We present anovel technique to obtain a good estimate for a given substring matching query; called MO(for Maximal Overlap); that estimates the selectivity of a query based on all maximalsubstrings of the query in the pruned count-suffix tree. We show that MO is provably betterthan the (independence-based) substring selectivity estimation technique proposed byKrishnan et al.[6]; called KVI; under the natural assumption that strings exhibit the so-called“short memory” property. We complement our analysis with an experiment; using a real …,Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,1999,74
Declarative message addressing,*,A messaging system; and method of operation thereof; which supports combinations ofdirectory and mailing list addressing mechanisms. Intended message recipients arespecified as declarative addresses; which may include combinations of directory andmailing list information. The messaging system includes a messaging server and an addressresolution module. The messaging server receives a message from a sender system andtransmits the message to the recipient system. The address resolution module; which iscoupled to the messaging server; receives a declarative address associated with themessage; resolves the declarative address into at least one messaging address andtransmits the at least one messaging address to the messaging server. In one embodiment;a database system may be coupled to the address resolution module to allow address …,*,2001,73
Method and system for using materialized views to evaluate queries involving aggregation,*,The present invention is a method and system for using materialized views to computeanswers to SQL queries with grouping and aggregation. A query is evaluated a using amaterialized view. The materialized view is semantically analyzed to determine whether thematerialized view is usable in evaluating an input query. The semantic analysis includesdetermining that the materialized view does not project out any columns needed to evaluatethe input query and determining that the view does not discard any tuple that satisfies acondition enforced in the input query. If the view is usable; the input query is rewritten toproduce an output query that is multi-set equivalent to the input query and that specifies oneor more occurrences of the materialized view as a source of information to be returned bythe output query. The output query is then evaluated. The semantic analysis and rewriting …,*,1999,72
Hybrid Transitive Closure Algorithms.,Rakesh Agrawal; HV Jagadish,*,VLDB,1990,72
Reducing uncertainty of schema matching via crowdsourcing,Chen Jason Zhang; Lei Chen; HV Jagadish; Chen Caleb Cao,Abstract Schema matching is a central challenge for data integration systems. Automatedtools are often uncertain about schema matchings they suggest; and this uncertainty isinherent since it arises from the inability of the schema to fully capture the semantics of therepresented data. Human common sense can often help. Inspired by the popularity and thesuccess of easily accessible crowdsourcing platforms; we explore the use of crowdsourcingto reduce the uncertainty of schema matching. Since it is typical to ask simple questions oncrowdsourcing platforms; we assume that each question; namely CorrespondenceCorrectness Question (CCQ); is to ask the crowd to decide whether a given correspondenceshould exist in the correct matching. We propose frameworks and efficient algorithms todynamically manage the CCQs; in order to maximize the uncertainty reduction within a …,Proceedings of the VLDB Endowment,2013,71
Constructing a generic natural language interface for an XML database,Yunyao Li; Huahai Yang; HV Jagadish,Abstract We describe the construction of a generic natural language query interface to anXML database. Our interface can accept an arbitrary English sentence as a query; which canbe quite complex and include aggregation; nesting; and value joins; among other things.This query is translated; potentially after reformulation; into an XQuery expression. Thetranslation is based on mapping grammatical proximity of natural language parsed tokens inthe parse tree of the query sentence to proximity of corresponding elements in the XML datato be retrieved. Our experimental assessment; through a user study; demonstrates that thistype of natural language interface is good enough to be usable now; with no restrictions onthe application domain.,International Conference on Extending Database Technology,2006,71
Database management for life sciences research,HV Jagadish; Frank Olken,Abstract The life sciences provide a rich application domain for data management research;with a broad diversity of problems that can make a significant difference to progress in lifesciences research. This article is an extract from the Report of the NSF Workshop on DataManagement for Molecular and Cell Biology; edited by HV Jagadish and Frank Olken. Theworkshop was held at the National Library of Medicine; Bethesda; MD; Feb. 2-3; 2003.,ACM SIGMOD Record,2004,71
Analysis of the n-dimensional quadtree decomposition for arbitrary hyperrectangles,Christos Faloutsos; HV Jagadish; Yannis Manolopoulos,We give a closed-form expression for the average number of n-dimensional quadtree nodes(" pieces" or" blocks") required by an n-dimensional hyperrectangle aligned with the axes.Our formula includes as special cases the formulae of previous efforts for two-dimensionalspaces. It also agrees with theoretical and empirical results that the number of blocksdepends on the hypersurface of the hyperrectangle and not on its hypervolume. Thepractical use of the derived formula is that it allows the estimation of the space requirementsof the n-dimensional quadtree decomposition. Quadtrees are used extensively in two-dimensional spaces (geographic information systems and spatial databases in general); aswell in higher dimensionality spaces (as oct-trees for three-dimensional spaces; eg; ingraphics; robotics; and three-dimensional medical images). Our formula permits the …,IEEE Transactions on Knowledge and Data Engineering,1997,70
Apparatus and methods for performing electronic scene analysis and enhancement,*,Apparatus and methods are provided for analyzing and enhancing a received data signalrepresenting one or more views of a captured scene. The received data signal includes aplurality of data points. Each view includes a plurality of image points. Ones of the plurality ofdata points are representative of ones of the plurality of image points. A plurality of data setsare identified wherein each one of the data sets includes ones of the plurality of data points.Ones of the identified data sets are compared with one or more standard values and; inresponse thereto; are enhanced selectively to generate an output signal representing aprocessed 3-D scene estimate.,*,1998,69
Towards a gigabit IP router,Abhaya Asthana; Catherine Delph; HV Jagadish; Paul Krzyzanowski,Abstract In this paper we illustrate the application of SWIM's Active Storage Element (ASE)module in constructing high performance IP routers. The logic associated with each ASE is awide-instruction-word micro-programmable engine; that has been specially designed toefficiently perform operations such as pointer dereferencing; memory indirection; boundschecking; and so forth. This makes it well suited to performing operations such as parsing ofthe IP header; routing table lookup; checksum computation and exception processing. Ourresults show that a single ASE running at 20 MHz can process 400;000 packets per second:well over that required to sustain a gigabit router. Multiple ASEs can be used in parallel toachieve even higher processing rates.,Journal of High Speed Networks,1992,69
Constructing an interactive natural language interface for relational databases,Fei Li; HV Jagadish,Abstract Natural language has been the holy grail of query interface designers; but hasgenerally been considered too hard to work with; except in limited specific circumstances. Inthis paper; we describe the architecture of an interactive natural language query interface forrelational databases. Through a carefully limited interaction with the user; we are able tocorrectly interpret complex natural language queries; in a generic manner across a range ofdomains. By these means; a logically complex English language sentence is correctlytranslated into a SQL query; which may include aggregation; nesting; and various types ofjoins; among other things; and can be evaluated against an RDBMS. We have constructed asystem; NaLIR (Natural Language Interface for Relational databases); embodying theseideas. Our experimental assessment; through user studies; demonstrates that NaLIR is …,Proceedings of the VLDB Endowment,2014,66
A compressed accessibility map for XML,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,Abstract XML is the undisputed standard for data representation and exchange. Ascompanies transact business over the Internet; letting authorized customers directly access;and even modify; XML data offers many advantages in terms of cost; accuracy; andtimeliness. Given the complex business relationships between companies; and the sensitivenature of information; access must be provided selectively; using sophisticated accesscontrol specifications. Using the specification directly to determine if a user has access to anXML data item can be extremely inefficient. The alternative of fully materializing; for eachdata item; the users authorized to access it can be space-inefficient. In this article; weintroduce a compressed accessibility map (CAM) as a space-and time-efficient solution tothe access control problem for XML data. A CAM compactly identifies the XML data items …,ACM Transactions on Database Systems (TODS),2004,65
MARS: A multiprocessor-based programmable accelerator,P Agrawal; WJ Dally; WC Fischer; HV Jagadish; AS Krishnakumar; R Tutundjian,MARS; short for microprogrammable accelerator for rapid simulations; is a multiprocessor-based hardware accelerator that can efficiently implement a wide range of computationallycomplex algorithms. In addition to accelerating many graph-related problem solutions;MARS is ideally suited for performing event-driven simulations of VLSI circuits. Its highlypipelined and parallel architecture yields a performance comparable to that of existingspecial-purpose hardware simulators. MARS has the added advantage of flexibility becauseits VLSI processors are custom-designed to be microprogrammable and reconfigurable.When programmed as a logic simulator; MARS should be able to achieve 1 million gateevaluations per second.,IEEE Design & Test of Computers,1987,65
Querying complex structured databases,Cong Yu; HV Jagadish,Abstract Correctly generating a structured query (eg; an XQuery or a SQL query) requiresthe user to have a full understanding of the database schema; which can be a daunting task.Alternative query models have been proposed to give users the ability to query the databasewithout schema knowledge. Those models; including simple keyword search and labeledkeyword search; aim to extract meaningful data fragments that match the structure-free queryconditions (eg; keywords) based on various matching semantics. Typically; the matchingsemantics are content-based: they are defined on data node inter-relationships and incursignificant query evaluation cost. Our first contribution is a novel matching semantics basedon analyzing the database schema. We show that query models employing a schema-basedmatching semantics can reduce query evaluation cost significantly while maintaining or …,Proceedings of the 33rd international conference on Very large data bases,2007,64
Compressed representation of a data base that permits AD HOC querying,*,A method and system for compressing a data base that permits queries on the compressedrepresentation of the data base. Another feature is that an approximation of the values of thedata base are derivable directly from the compressed representation of the data base. Yetanother feature is correction of poor approximations of the reconstructed data. Still anotherfeature is the capability of performing aggregate queries of the compressed representationof the data base.,*,1999,64
The identification of gene expression profiles associated with progression of human diabetic neuropathy,Junguk Hur; Kelli A Sullivan; Manjusha Pande; Yu Hong; Anders AF Sima; Hosagrahar V Jagadish; Matthias Kretzler; Eva L Feldman,Abstract Diabetic neuropathy is a common complication of diabetes. While multiplepathways are implicated in the pathophysiology of diabetic neuropathy; there are no specifictreatments and no means to predict diabetic neuropathy onset or progression. Here; weidentify gene expression signatures related to diabetic neuropathy and developcomputational classification models of diabetic neuropathy progression. Microarrayexperiments were performed on 50 samples of human sural nerves collected during a 52-week clinical trial. A series of bioinformatics analyses identified differentially expressedgenes and their networks and biological pathways potentially responsible for theprogression of diabetic neuropathy. We identified 532 differentially expressed genesbetween patient samples with progressing or non-progressing diabetic neuropathy; and …,Brain,2011,63
A spreadsheet algebra for a direct data manipulation query interface,Bin Liu; HV Jagadish,A spreadsheet-like" direct manipulation" interface is more intuitive for many non-technicaldatabase users compared to traditional alternatives; such as visual query builders. Theconstruction of such a direct manipulation interfacemay appear straightforward; but there aresome significant challenges. First; individual direct manipulation operations cannot be toocomplex; so expressive power has to be achieved through composing (long) sequences ofsmall operations. Second; all intermediate results are visible to the user; so grouping andordering are material after every small step. Third; users often find the need to modifypreviously specified queries. Since manipulations are specified one step at a time; there isno actual queryexpression to modify. Suitable means must be provided to address this need.Fourth; the order in which manipulations are performed by the user should not affect the …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,62
Telephone billing with customer-defined budgeting,*,A method and system of telephone call processing that provides direct control of telephoneusage based on customer established budgets. According to one aspect of the presentinvention; customer specific information including pricing data and budget parameters for acustomer are stored; as is summary information relating to customer telephone usage. A callsetup query is received from a network switch. A call setup response is generated based onthe budget parameters and the summary information and the call setup response istransmitted to the network switch. In order to generate the stored summary information; aplurality of records are received wherein each record describes a telephone call. A pricedcall value for each call is determined using the record and the customer specific information.Summary information for the customer is determined using the record; the priced call …,*,1999,62
Telephone line aggregated billing,*,A method and system in which calls made on two or more phone lines of a customer areaggregated for billing and discount billing plans to which the customer subscribes areapplied to the aggregated phone usage of the customer. In order to price a call made over anetwork by a customer of the network; information specifying a billing plan of the customerand information specifying a plurality of telephone lines to which the billing plan applies arestored. A call made from one of the plurality of telephone lines is received at a networkswitch and record that describes the call is generated. The record is received at a billinganalysis system and a priced call value for the call is determined based on the record andthe information specifying the customer billing plan. The record that describes the callcomprises an identifier of the telephone line from which the call was made and priced call …,*,1999,62
On B-tree indices for skewed distributions,Christos Faloutsos; HV Jagadish,ABSTRACT It is often the case that the set of values over which a B-Tree is constructed hasa skewed distribution. We present a geometric growth technique to manage postingsrecords in such cases; and show that the performance of such a technique is better than thatof a straightforward fixed length postings list: It guarantees 1 disk access on searching; and ittakes a fraction of the space that its competitor requires (55% to 66%; in our experiments).,*,1992,61
Materialization and incremental update of path information,Rakesh Agrawal; HV Jagadish,The problem of efficiently processing recursive path queries in deductive database systemsis discussed; and a semimaterialized encoding structure is proposed as an attractiveapproach that provides a balance between efficiency of retrieval and feasibility of storage.Incremental algorithms are presented that enable the effects of updates to the underlyingdatabase to be reflected in the materialized information. Performance simulations indicatethat these techniques can significantly speed up the processing of path queries at anacceptable level of storage overhead.,Data Engineering; 1989. Proceedings. Fifth International Conference on,1989,61
Big data and science: Myths and reality,HV Jagadish,“Big Data” now impacts nearly every aspect of our modern society; including business;government; health care; and research in almost every discipline: life sciences; engineering;natural sciences; art & humanities. As it has drawn much attention; and become economicallyimportant; there are many who have preferred angles on the interpretation of Big Data. At thesame time; as many have been exposed to the term with little prior knowledge of computing ortechnology; they are easily swayed by the “experts.” In consequence; there has been a rushto use the term Big Data in ways that are inappropriate but self-serving. In many cases; theseerroneous interpretations have then been taken up and amplified by others; including even technicallysophisticated people. In this article; I discuss some of the more common myths … The very word“Big” indicates size. It is also the case that measures of size are very easily conveyed. We …,*,2015,59
Optimized algorithms for predictive range and knn queries on moving objects,Rui Zhang; HV Jagadish; Bing Tian Dai; Kotagiri Ramamohanarao,Abstract There have been many studies on management of moving objects recently. Most ofthem try to optimize the performance of predictive window queries. However; not muchattention is paid to two other important query types: the predictive range query and thepredictive k nearest neighbor query. In this article; we focus on these two types of queries.The novelty of our work mainly lies in the introduction of the Transformed Minkowski Sum;which can be used to determine whether a moving bounding rectangle intersects a movingcircular query region. This enables us to use the traditional tree traversal algorithms toperform range and kNN searches. We theoretically show that our algorithms based on theTransformed Minkowski Sum are optimal in terms of the number of tree node accesses. Wealso experimentally verify the effectiveness of our technique and show that our algorithms …,Information Systems,2010,59
Efficient discovery of XML data redundancies,Cong Yu; HV Jagadish,Abstract As XML becomes widely used; dealing with redundancies in XML data has becomean increasingly important issue. Redundantly stored information can lead not just to a higherdata storage cost; but also to increased costs for data transfer and data manipulation.Furthermore; such data redundancies can lead to potential update anomalies; rendering thedatabase inconsistent. One way to avoid data redundancies is to employ good schemadesign based on known functional dependencies. In fact; several recent studies havefocused on defining the notion of XML Functional Dependencies (XML FDs) to capture XMLdata redundancies. We observe further that XML databases are often" casually designed"and XML FDs may not be determined in advance. Under such circumstances; discoveringXML data redundancies (in terms of FDs) from the data itself becomes necessary and is …,Proceedings of the 32nd international conference on Very large data bases,2006,59
Sample-driven schema mapping,Li Qian; Michael J Cafarella; HV Jagadish,Abstract End-users increasingly find the need to perform light-weight; customized schemamapping. State-of-the-art tools provide powerful functions to generate schema mappings;but they usually require an in-depth understanding of the semantics of multiple schemas andtheir correspondences; and are thus not suitable for users who are technicallyunsophisticated or when a large number of mappings must be performed. We propose asystem for sample-driven schema mapping. It automatically constructs schema mappings; inreal time; from user-input sample target instances. Because the user does not have toprovide any explicit attribute-level match information; she is isolated from the possiblycomplex structure and semantics of both the source schemas and the mappings. In addition;the user never has to master any operations specific to schema mappings: she simply …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,58
Automatic aggregation of network management information in spatial; temporal and functional forms,*,A method and apparatus provide for the automatic aggregation of network managementinformation in spatial; temporal and functional forms. Management information relating to anetwork is automatically aggregated by computational means in the form of a attribute name-value pair which is stored in an Aggregation Managed Object (AMO). The aggregation ofnetwork management information in the form of an AMO supports the spatial; temporal andfunctional aggregations. The AMOs themselves are stored in a database of a specialmanagement agent; the Management Aggregation and Visualization Server (MAVS) whichallows network managers to access and set network information to and from the differentaggregation forms.,*,2002,57
Method and apparatus for sending an electronic mail message to a receiving party,*,The present invention provides a centralized electronic mail apparatus and method in whicha message is sent to each of a subscriber's electronic mail receiving devices. When asending party wishes to send an electronic mail message to a receiving party; the sendingparty creates the electronic mail message along with any attachments using his/her userdevice and sends the electronic mail message to the centralized electronic mail apparatus.The centralized electronic mail apparatus receives the electronic mail message andretrieves profile information from a profile database corresponding to the receiving party.Based on the profile information; the centralized electronic mail device further determinesthe receiving party's electronic mail receiving devices to which the electronic mail messageis to be sent. The centralized electronic mail apparatus further determines the portions of …,*,2001,57
Global optimization of histograms,HV Jagadish; Hui Jin; Beng Chin Ooi; Kian-Lee Tan,Abstract Histograms are frequently used to represent the distribution of data values in anattribute of a relation. Most previous work has focused on identifying the optimal histogram(given a limited number of buckets) for a single attribute independent of otherattributes/histograms. In this paper; we propose the idea of global optimization ofhistograms; ie; single-attribute histograms for a set of attributes are optimized collectively soas to minimize the overall error in using the histograms. The idea is to allocate more bucketsto histograms whose attributes are more frequently used and/or distributions are highlyskewed. While the accuracy of some histograms is penalized (being assigned fewerbuckets); we expect the global error to be low compared to the traditional method (ofallocating equal number of buckets to each histogram). We propose two algorithms to …,ACM SIGMOD Record,2001,57
Managing conflicts between rules,HV Jagadish; Alberto O Mendelzon; Inderpal Singh Mumick,Abstract Rules are used as a programming paradigm in several application domains;including active databases; planning; expert systems; and billing. For example; activedatabases have rules that execute upon the occurrence of particular events if specifiedcondition predicates are satisfied. It is often the case that multiple rules are fireable when aparticular event occurs. We propose a declarative mechanism to control the interaction andexecution of multiple rules. The mechanism is based upon logical meta-rules that canexpress various types of relationships between rules. The meta-rules allow us to reasonstatically about the rule behavior. We can determine; in polynomial time; whether a rule willnever execute; whether two rules can ever be executed together; and whether a rule systemis guaranteed to have a unique execution set for all possible rules that become fireable …,Journal of Computer and System Sciences,1999,55
On Indexing Line Segments.,Hosagrahar V Jagadish,ABSTRACT In several image applications; it is necessary to retrieve specific line segmentsborn a potentially very large set. In this paper; we consider the problem of indexing straightline segments to enable efficient retrieval of all line segments that (i) go through a specifiedpoint; or (ii) intersect a specified line segment. We propose a data organization; based onthe Hough transform; that can be used to solve both retrieval problems efficiently. In addition;the proposed structure can be used for approximate retrievals; finding all line segments thatpass close to a specified point. We show; through analysis and experiment; that theproposed technique always does as well as or better than retrieval based on minimumbounding rectangles or line segment end-points.,VLDB,1990,55
Customer profile based telephone card billing,*,A method and system in which calls made by a customer using a calling card are billed inaccordance with the customer's subscribed calling plans. In order to price a call made over anetwork by a customer of the network; information specifying a billing plan of the customer isstored. A call made from a customer station or using a customer calling card or prepaidtelephone card is received at a network switch and record that describes the call isgenerated. The record is received at a billing analysis system and a priced call value for thecall is determined based on the record and the information specifying the customer billingplan. If the call is placed using a prepaid telephone card; it is verified that a sufficientbalance remains before completing the call.,*,1998,53
Method for accessing information in a computer,*,An electronic library which comprises a user interface such as a computer screen; aspeaker; a mouse and/or a keyboard; a processor for handling communication with the userand for responding to user requests; and a data store. The data store maintains scannedsegments of video data; audio data; or both; and translated replicas of the scannedsegments. Searching for specific data is performed by perusing through the translatedreplicas; but the information that is provided to the user is primarily the scanned segmentsthemselves. The translated versions contain the immediately translatable version of thedisplayable information and processed information that forms various aggregations of thedisplayable information. This processing imposes a syntactically logical structure on thedisplayable information.,*,1994,52
An object model for image recognition,HV Jagadish; Lawrence O'Gorman,The use of object-oriented database principles to help model an image for computer vision;specifically; for line-image analysis; is described. The resulting representation; called thinline code (TLC); is general across known applications and extensible to new applications.TLC's advantages; and also some difficulties it has in strictly adhering to traditional notionsof object orientation; are addressed. A review of relevant aspects of object modeling isincluded.,Computer,1989,52
Itcompress: An iterative semantic compression algorithm,HV Jagadish; Raymond T Ng; Beng Chin Ooi; Anthony KH Tung,Real datasets are often large enough to necessitate data compression. Traditional'syntactic'data compression methods treat the table as a large byte string and operate at the byte level.The tradeoff in such cases is usually between the ease of retrieval (the ease with which onecan retrieve a single tuple or attribute value without decompressing a much larger unit) andthe effectiveness of the compression. In this regard; the use of semantic compression hasgenerated considerable interest and motivated certain recent works. We propose a semanticcompression algorithm called ItCompress ITerative Compression; which achieves goodcompression while permitting access even at attribute level without requiring thedecompression of a larger unit. ItCompress iteratively improves the compression ratio of thecompressed output during each scan of the table. The amount of compression can be …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,51
Odefs: A file system interface to an object-oriented database,Narain H Gehani; Hosagrahar V Jagadish; William D Roome,ABSTRACT OdeFS is a file-like interface to the Ode object-oriented database. Databaseobjects are accessed and manipulated like files in a traditional file system using standardUNIX® commands. For example; the ls command can be used to list the objects in adirectory and the cat command can be used to display the contents of an object. Editors suchas vi and emacs can be used to update objects. OdeFS is implemented as a network fileserver; using the NFS protocol. OdeFS commands are translated into calls to the underlyingUNIX file systems (to manipulate files or directories) or the Ode object manager (tomanipulate persistent objects). OdeFS requires no storage of its own. In consequence;OdeFS is object-compatible with the other Ode interfaces: O++(C++ interface); CQL++(SQLinterface); and OdeView (graphical interface). Objects created with one interface can be …,VLDB,1994,51
Guided interaction: Rethinking the query-result paradigm,Arnab Nandi; HV Jagadish,ABSTRACT Many decades of research; coupled with continuous increases in computingpower; have enabled highly efficient execution of queries on large databases. Inconsequence; for many databases; far more time is spent by users formulating queries thanby the system evaluating them. It stands to reason that; looking at the overall queryexperience we provide users; we should pay attention to how we can assist users in theholistic process of obtaining the information they desire from the database; and not just theconstituent activity of efficiently generating a result given a complete precise query. In thispaper; we examine the conventional query-result paradigm employed by databases anddemonstrate challenges encountered when following this paradigm for an informationseeking task. We recognize that the process of query specification itself is a major …,Proceedings of the VLDB Endowment,2011,48
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,47
Customer profile based customized messaging,*,A method and system of telephone call processing that provides information to customerswhile telephone calls are made. Summary information for a customer is stored. A call fromthe customer is received alt a network switch. The network switch transmits informationidentifying the customer to a billing analysis system. The billing analysis system transmitsthe summary information for the customer to the network switch and an audio messagebased on the summary information is played to the customer.,*,2000,47
Method and system for compressing a data stream in a database log so as to permit recovery of only selected portions of the data stream,*,The invention relates to a system for maintaining a log of incoming records for a databasesystem. Seek points are inserted into the compressed data log in a manner that allowsrecovery to start from a specified point without a need for decompressing earlier portions ofthe log. The initial block of data is used as the compression dictionary. A new compressionsequence using the same initial compression dictionary is started at each seek point.,*,1999,47
Range selectivity estimation for continuous attributes,Flip Korn; Theodore Johnson; HV Jagadish,Many commercial database systems maintain histograms to efficiently estimate queryselectivities as part of query optimization. Most work on histogram design is implicitly gearedtowards discrete or categorical attribute value domains. We consider approaches that arebetter suited for the continuous valued attributes commonly found in scientific and statisticaldatabases. We propose two methods based on spline functions for estimating the selectivityof range queries over univariate and multivariate data. These methods are more accuratethan histograms. As the results from our experiments on both real and synthetic data setsdemonstrate; the proposed methods achieved substantially better (up to 5.5 times)estimation error than the state-of-the-art histograms; at exactly the same storage space andwith comparable CPU runtime overhead; moreover; the superiority of the proposed spline …,Scientific and Statistical Database Management; 1999. Eleventh International Conference on,1999,47
Snakes and sandwiches: Optimal clustering strategies for a data warehouse,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Abstract Physical layout of data is a crucial determinant of performance in a data warehouse.The optimal clustering of data on disk; for minimizing expected I/O; depends on the queryworkload. In practice; we often have a reasonable sense of the likelihood of different classesof queries; eg; 40% of the queries concern calls made from some specific telephone numberin some month. In this paper; we address the problem of finding an optimal clustering ofrecords of a fact table on disk; given an expected workload in the form of a probabilitydistribution over query classes. Attributes in a data warehouse fact table typically havehierarchies defined on them (by means of auxiliary dimension tables). The product of thedimensional hierarchy levels forms a lattice and leads to a natural notion of query classes.Optimal clustering in this context is a combinatorially explosive problem with a huge …,ACM SIGMOD Record,1999,46
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,A group of database researchers meets periodically to discuss the state of the field and its keydirections going forward. Past meetings were held in 1989; 6 1990; 11 1995; 12 1996; 101998; 7 2003; 1 and 2008. 2 Continuing this tradition; 28 database researchers and two invitedspeakers met in October 2013 at the Beckman Center on the University of California-Irvine campusfor two days of discussions. The meeting attendees represented a broad cross-section ofinterests; affiliations; seniority; and geography. Attendance was capped at 30 so the meetingwould be as interactive as possible. This article summarizes the conclusions from thatmeeting; an extended report and participant presentations are available at http://beckman.cs.wisc.edu … The meeting participants quickly converged on big data as a defining challengeof our time. Big data arose due to the confluence of three major trends. First; it has …,Communications of the ACM,2016,45
Event-based messaging,*,A messaging system that handles messages of any kind; and a method of operation thereof;in which advanced messaging services can be implemented for multiple users; acrossmultiple mail clients; in a more flexible and less limited fashion than previous messagingsystems. The messaging system includes a plurality of messaging entities together having astate. An event supplier detects a change in the state of the messaging entities andgenerates an event announcement. An event manager receives the event announcementand generates an event notification. An event consumer receives the event notification andexamines or manipulates at least one messaging entity.,*,2002,45
Interrupt-based system,*,An apparatus and method provide an interrupt-based add-on service for use in a servicesubscription system having a central service provider and at least one subscriber unit. Theinterrupt-based system includes a database for storing subscriber information specific to atleast one subscriber; where the subscriber information includes at least one subscriberselectable condition. The system further includes a computer for interrupting a servicerequest when the service request satisfies at least one subscriber selectable condition andfor making a service connection when the service request does not match the subscriberselectable condition.,*,1998,45
Automatic rule refinement for information extraction,Bin Liu; Laura Chiticariu; Vivian Chu; HV Jagadish; Frederick R Reiss,Abstract Rule-based information extraction from text is increasingly being used to populatedatabases and to support structured queries on unstructured text. Specification of suitableinformation extraction rules requires considerable skill and standard practice is to refinerules iteratively; with substantial effort. In this paper; we show that techniques developed inthe context of data provenance; to determine the lineage of a tuple in a database; can beleveraged to assist in rule refinement. Specifically; given a set of extraction rules and correctand incorrect extracted data; we have developed a technique to suggest a ranked list of rulemodifications that an expert rule specifier can consider. We implemented our technique inthe SystemT information extraction system developed at IBM Research--Almaden andexperimentally demonstrate its effectiveness.,Proceedings of the VLDB Endowment,2010,43
TIMBER: A native system for querying XML,Stelios Paparizos; Shurug Al-Khalifa; Adriane Chapman; HV Jagadish; Laks VS Lakshmanan; Andrew Nierman; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu,Abstract XML has become ubiquitous; and XML data has to be managed in databases. Thecurrent industry standard is to map XML data into relational tables and store this informationin a relational database. Such mappings create both expressive power problems andperformance problems. In the T IMBER [7] project we are exploring the issues involved instoring XML in native format. We believe that the key intellectual contribution of this system isa comprehensive set-at-a-time query processing ability in a native XML store; with all thestandard components of relational query processing; including algebraic rewriting and acost-based optimizer.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,43
Hybrid index organizations for text databases,Christos Faloutsos; HV Jagadish,Abstract Due to the skewed nature of the frequency distribution of term occurrence (eg; Zipf'slaw) it is unlikely that any single technique for indexing text can do well in all situations. Inthis paper we propose a hybrid approach to indexing text; and show how it can outperformthe traditional inverted B-tree index both in storage overhead; in time to perform a retrieval;and; for dynamic databases; in time for an insertion; both for single term and for multiple termqueries. We demonstrate the benefits of our technique on a database of stories from theAssociated Press news wire; and we provide formulae and guidelines on how to makeoptimal choices of the design parameters in real applications.,International Conference on Extending Database Technology,1992,43
Using trees to depict a forest,Bin Liu; Hosagrahar V Jagadish,Abstract When a database query has a large number of results; the user can only be shownone page of results at a time. One popular approach is to rank results such that the" best"results appear first. However; standard database query results comprise a set of tuples; withno associated ranking. It is typical to allow users the ability to sort results on selectedattributes; but no actual ranking is defined. An alternative approach to the first page is not totry to show the best results; but instead to help users learn what is available in the wholeresult set and direct them to finding what they need. In this paper; we demonstrate through auser study that a page comprising one representative from each of k clusters (generatedthrough a k-medoid clustering) is superior to multiple alternative candidate methods forgenerating representatives of a data set.,Proceedings of the VLDB Endowment,2009,42
Telephone billing with summary information,*,A method and system for automatically generating telephone bills that include customerdefined or requested summary information. Customer specific data including pricing dataand summary parameters are stored. A plurality of records; each record describing atelephone call; are generated. The record is received at a billing analysis system; whichdetermines a priced call value for each call using the record and the customer specific dataand determines summary information for the customer using the record; the priced callvalues and the summary parameters. A telephone bill for the customer is generated basedon the summary information for the customer. The bill may be generated periodically; on apredetermined schedule; or upon demand of the customer. Online access to summaryinformation is also provided.,*,2000,42
Enabling Schema-Free XQuery with meaningful query focus,Yunyao Li; Cong Yu; HV Jagadish,Abstract The widespread adoption of XML holds the promise that document structure can beexploited to specify precise database queries. However; users may have only a limitedknowledge of the XML structure; and may be unable to produce a correct XQueryexpression; especially in the context of a heterogeneous information collection. The defaultis to use keyword-based search and we are all too familiar with how difficult it is to obtainprecise answers by these means. We seek to address these problems by introducing thenotion of Meaningful Query Focus (MQF) for finding related nodes within an XML document.MQF enables users to take full advantage of the preciseness and efficiency of XQuerywithout requiring (perfect) knowledge of the document structure. Such a Schema-FreeXQuery is potentially of value not just to casual users with partial knowledge of schema …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,41
NaLIX: A generic natural language search environment for XML data,Yunyao Li; Huahai Yang; HV Jagadish,Abstract We describe the construction of a generic natural language query interface to anXML database. Our interface can accept a large class of English sentences as a query;which can be quite complex and include aggregation; nesting; and value joins; among otherthings. This query is translated; potentially after reformulation; into an XQuery expression.The translation is based on mapping grammatical proximity of natural language parsedtokens in the parse tree of the query sentence to proximity of corresponding elements in theXML data to be retrieved. Iterative search in the form of followup queries is also supported.Our experimental assessment; through a user study; demonstrates that this type of naturallanguage interface is good enough to be usable now; with no restrictions on the applicationdomain.,ACM Transactions on database systems (TODS),2007,40
Getting work done on the web: supporting transactional queries,Yunyao Li; Rajasekar Krishnamurthy; Shivakumar Vaithyanathan; HV Jagadish,Abstract Many searches on the web have a transactional intent. We argue that pagessatisfying transactional needs can be distinguished from the more common pages that havesome information and links; but cannot be used to execute a transaction. Based on thishypothesis; we provide a recipe for constructing a transaction annotator. By constructing anannotator with one corpus and then demonstrating its classification performance on another;we establish its robustness. Finally; we show experimentally that a search procedure thatexploits such pre-annotation greatly outperforms traditional search for retrievingtransactional pages.,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,40
A family of new efficient arrays for matrix multiplication,HV Jagadish; Thomas Kailath,The authors present a regular iterative algorithm for matrix multiplication and show thatseveral well-known matrix multiplication arrays are directly obtained from it; differing only inthe choice of iteration vector. They then present a regular iterative algorithm for matrixmultiplication using the S. Winograd method (1968) and show in detail how to derive onearray from this algorithmic description. Other arrays in the same family can similarly beobtained for different choices of the iteration space. The new arrays compute the product oftwo matrices faster than available conventional arrays and use a smaller number ofprocessor cells.,IEEE Transactions on computers,1989,40
Sender-paid electronic messaging,*,The present invention is a messaging system; and method of operation thereof; whichprovides message recipients with control over the delivery of message and charges the costof a message to the sender of the message. A message is received at a messaging serverfrom a sender system; the message including an indication of a recipient system. Anotification message is transmitted to the recipient system; allowing the message recipient todetermine whether they desire the message to be delivered. If so; an activation message isreceived from the recipient system and the message is transmitted to the recipient system. Acharge for the message is assessed to the sender of the message. The message is stored inthe messaging server until the activation message is received. At least a portion of theassessed charge may be credited or debited to the recipient of the message. The …,*,2000,39
Independence Diagrams: A Technique for Visual Data Mining.,Stefan Berchtold; HV Jagadish; Kenneth A Ross,Abstract An important issue in data mining is the recognition of complex dependenciesbetween attributes. Past techniques for identifying attribute dependence include correlationcoefficients; scatterplots; and equiwidth histograms. These techniques are sensitive tooutliers; and often are not sufficiently informative to identify the kind of attribute dependencepresent. We propose a new approach; which we call independence diagrams. We divideeach attribute into ranges; for each pair of attributes; the combination of these rangesdefines a two-dimensional grid. For each cell of this grid; we store the number of data itemsin it. We display the grid; scaling each attribute axis so that the displayed width of a range isproportional to the total number of data items within that range. The brightness of a cell isproportional to the density of data items in it. As a result; both attributes are independently …,KDD,1998,38
The rejuvenation of materialized views,Inderpal Singh Mumick,Abstract This is a short summary of a talk presented at the sixth International Conference onInformation Systems and Management of Data (CISMOD 95) held in Bombay in November1995. The summary describes some of the applications that are causing a renewed interestin materialized views; the problems in supporting the applications; and sketches how thecurrent work in this area is addressing some of the problems.,*,1995,38
Qunits: queried units in database search,Arnab Nandi; HV Jagadish,Abstract: Keyword search against structured databases has become a popular topic ofinvestigation; since many users find structured queries too hard to express; and enjoy thefreedom of a``Google-like''query box into which search terms can be entered. Attempts toaddress this problem face a fundamental dilemma. Database querying is based on the logicof predicate evaluation; with a precisely defined answer set for a given query. On the otherhand; in an information retrieval approach; ranked query results have long been acceptedas far superior to results based on boolean query evaluation. As a consequence; whenkeyword queries are attempted against databases; relatively ad-hoc ranking mechanismsare invented (if ranking is used at all); and there is little leverage from the large body of IRliterature regarding how to rank query results. Our proposal is to create a clear separation …,arXiv preprint arXiv:0909.1765,2009,37
The Michigan Benchmark: A microbenchmark for XML query processing systems,Kanda Runapongsa; Jignesh M Patel; HV Jagadish; Shurug Al-Khalifa,With the continuing increasing popularity of the eXtensible Markup Language (XML) as arepresentation format for a wide variety of data; and it is clear that large repositories of XMLdata sets will soon emerge. The effective management of XML in a database thus becomesa pressing issue. Several methods for managing XML databases have emerged; rangingfrom retrofitting commercial RDBMSs to building native XML database systems. There hasnaturally been an interest in benchmarking the performance of these systems; and a numberof benchmarks have been proposed [?;?;?]. The focus of currently proposed benchmarks isto assess the performance of a given XML database in performing a variety of representativetasks. Such benchmarks are valuable to potential users of a database system in providingan indication of the performance that the user can expect on their specific application. The …,EEXTT,2002,37
Multi-dimensional substring selectivity estimation,HV Jagadish; Olga Kapitskaia; Raymond T Ng; Divesh Srivastava,Abstract With the explosion of the Internet; LDAP directories and XML; there is an evergreater need to evaluate queries involving (sub) string matching. In many cases; matchesneed to be on multiple attributes/dimensions; with correlations between the dimensions.Effective query optimization in this context requires good selectivity estimates. In this paper;we use multi-dimensional countsuffix trees as the basic framework for substring selectivityestimation. Given the enormous size ofthese trees for large databases; we develop a spaceand time efficient probabilistic algorithm to construct multi-dimensional pruned count-suffixtrees directly. We then present two techniques to obtain good estimates for a given multi-dimensional substring matching query; using a pruned countsuffix tree. The first one; calledGNO (for Greedy Non-Overlap); generalizes the greedy parsing suggested by Krishnan et …,VLDB,1999,36
Queries in an object-oriented graphical interface,Shaul Dar; Narain H.  Gehani; HV Jagadish; J Srinivasan,Abstract The graphical user interface OdeView for the Ode object-oriented database systemallows users to perform complex operations against sets of objects. These include selection;projection; display; creation; deletion and update of objects; and the invocation of memberfunctions (methods). OdeView utilizes type information for displaying objects and therelationships between objects; and for guiding the user in constructing syntactically correctquery predicates. We present a formal model that underlies the OdeView display and queryinterface; illustrate the graphical query facilities through examples; and discuss our designdecisions. We also describe our implementation; focusing on the usage of the Ode typecatalog and the dynamic translation and execution of queries. As object database systemsare becoming increasingly popular; we hope that the design and implementation details …,Journal of Visual Languages & Computing,1995,36
NaLIR: an interactive natural language interface for querying relational databases,Fei Li; Hosagrahar V Jagadish,Abstract In this demo; we present NaLIR; a generic interactive natural language interface forquerying relational databases. NaLIR can accept a logically complex English languagesentence as query input. This query is first translated into a SQL query; which may includeaggregation; nesting; and various types of joins; among other things; and then evaluatedagainst an RDBMS. In this demonstration; we show that NaLIR; while far from being able topass the Turing test; is perfectly usable in practice; and able to handle even quite complexqueries in a variety of application domains. In addition; we also demonstrate how carefullydesigned interactive communication can avoid misinterpretation with minimum user burden.,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,35
Method for computing transitive closure,*,A method and apparatus for creating a transitive closure of a database when the database isstored on a secondary storage in the form of links connecting nodes. The method consists ofpartitioning the database; transferring one partition at a time from the secondary storage tothe main memory; and processing a partition in such a way that accesses to the portions ofthe database not in main memory are minimized. As much of the unprocessed database aswould fit a predetermined fraction of main memory is fetched as one partition; and if; duringthe processing of this partition; the main memory becomes full; the size of the partition isreduced dynamically by discarding a portion of the database in the current partition; andincluding this portion in the next partition. The processing of a partition involves; for eachnode in the partition; the operation of creating a direct connection between every pair of …,*,1990,35
Direction-preserving trajectory simplification,Cheng Long; Raymond Chi-Wing Wong; HV Jagadish,Abstract Trajectories of moving objects are collected in many applications. Raw trajectorydata is typically very large; and has to be simplified before use. In this paper; we introducethe notion of direction-preserving trajectory simplification; and show both analytically andempirically that it can support a broader range of applications than traditional position-preserving trajectory simplification. We present a polynomial-time algorithm for optimaldirection-preserving simplification; and another approximate algorithm with a qualityguarantee. Extensive experimental evaluation with real trajectory data shows the benefit ofthe new techniques.,Proceedings of the VLDB Endowment,2013,34
Pattern tree algebras: sets or sequences?,Stelios Paparizos; HV Jagadish,Abstract XML and XQuery semantics are very sensitive to the order of the produced output.Although pattern-tree based algebraic approaches are becoming more and more popular forevaluating XML; there is no universally accepted technique which can guarantee both acorrect output order and a choice of efficient alternative plans. We address the problemusing hybrid collections of trees that can be either sets or sequences or something inbetween. Each such collection is coupled with an Ordering Specification that describes howthe trees are sorted (full; partial or no order). This provides us with a formal basis fordeveloping a query plan having parts that maintain no order and parts with partial or fullorder. It turns out that duplicate elimination introduces some of the same issues as ordermaintenance: it is expensive and a single collection type does not always provide all the …,Proceedings of the 31st international conference on Very large data bases,2005,34
Method for transferring and displaying data pages on a data network,*,The apparent speed of a connection between a browser at a user station and a proxy orgateway on a network such as the Internet is increased by providing a local proxy at the userstation which interacts with a remote proxy. While the remote proxy is retrieving a newlyrequested World Wide Web page; for example; from the appropriate content provider; it mayalso be sending to the local proxy a stale cached version of that page. When the newversion of the page is finally retrieved; the remote proxy determines the differences betweenthe new version and the stale version; and; assuming the differences do not exceed the newpage in size; sends the differences to the local proxy which then reconstructs the new pagefrom the differences and the stale version. The local proxy delivers the new page to thebrowser; which need not even be aware that a local proxy exists; it is aware only that it …,*,2005,34
Untitled,*,*,*,*,34
Method and apparatus for asynchronous version advancement in a three version database,*,A method and apparatus is provided for asynchronous version advancement in a threeversion database. For a distributed database; read transactions are executed using a firstversion of a database. Update transactions are executed such that information is written intoa second version of the database. The second version may include less than all of theinformation contained in the first version. A database version begins to be advanced at eachnode such that the information in the second version becomes available for readtransactions. For an update transaction that starts on a node after the database version hasbeen advanced; the update transaction is executed such that the update transaction writesinformation into a third version of the database. The advancement of the database version iscompleted such that the second version becomes the first version and the third version …,*,2002,33
Fast failure recovery in distributed graph processing systems,Yanyan Shen; Gang Chen; HV Jagadish; Wei Lu; Beng Chin Ooi; Bogdan Marius Tudor,Abstract Distributed graph processing systems increasingly require many compute nodes tocope with the requirements imposed by contemporary graph-based Big Data applications.However; increasing the number of compute nodes increases the chance of node failures.Therefore; provisioning an efficient failure recovery strategy is critical for distributed graphprocessing systems. This paper proposes a novel recovery mechanism for distributed graphprocessing systems that parallelizes the recovery process. The key idea is to partition thepart of the graph that is lost during a failure among a subset of the remaining nodes. To doso; we augment the existing checkpoint-based and log-based recovery schemes with apartitioning mechanism that is sensitive to the total computation and communication cost ofthe recovery process. Our implementation on top of the widely used Giraph system …,Proceedings of the VLDB Endowment,2014,32
Method and apparatus for providing telephony over a computer network,*,A method and system facilitate telephony over computer based networks. The parties to atelephone call need not have the same telephone application software capabilities beforethe call is initiated nor does the network have to provide standard signaling. Instead; inaccordance with the method and system one of the parties defines to the other party theencoding/decoding technique to be used in the processing of the call. The definition can beeither by supplying a copy of the application to the other party or by providing the other partywith an indirect reference to where the application can be obtained.,*,2002,32
One-dimensional and multi-dimensional substring selectivity estimation,HV Jagadish; Olga Kapitskaia; Raymond T Ng; Divesh Srivastava,Abstract With the increasing importance of XML; LDAP directories; and text-basedinformation sources on the Internet; there is an ever-greater need to evaluate queriesinvolving (sub) string matching. In many cases; matches need to be on multipleattributes/dimensions; with correlations between the multiple dimensions. Effective queryoptimization in this context requires good selectivity estimates. In this paper; we use prunedcount-suffix trees (PSTs) as the basic data structure for substring selectivity estimation. Forthe 1-D problem; we present a novel technique called MO (Maximal Overlap). We thendevelop and analyze two 1-D estimation algorithms; MOC and MOLC; based on MO and aconstraint-based characterization of all possible completions of a given PST. For the kDproblem; we first generalize PSTs to multiple dimensions and develop a space-and time …,The VLDB Journal—The International Journal on Very Large Data Bases,2000,32
DSH: data sensitive hashing for high-dimensional k-nnsearch,Jinyang Gao; Hosagrahar Visvesvaraya Jagadish; Wei Lu; Beng Chin Ooi,Abstract The need to locate the k-nearest data points with respect to a given query point in amulti-and high-dimensional space is common in many applications. Therefore; it is essentialto provide efficient support for such a search. Locality Sensitive Hashing (LSH) has beenwidely accepted as an effective hash method for high-dimensional similarity search.However; data sets are typically not distributed uniformly over the space; and as a result; thebuckets of LSH are unbalanced; causing the performance of LSH to degrade. In this paper;we propose a new and efficient method called Data Sensitive Hashing (DSH) to address thisdrawback. DSH improves the hashing functions and hashing family; and is orthogonal tomost of the recent state-of-the-art approaches which mainly focus on indexing and queryingstrategies. DSH leverages data distributions and is capable of directly preserving the …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,31
Biological data management: Research; practice and opportunities,Thodoros Topaloglou; Susan B Davidson; HV Jagadish; Victor M Markowitz; Evan W Steeg; Mike Tyers,Biological research and drug development are routinely producing terabytes of data thatneed to be organized; queried and reduced to useful scientific knowledge. Although datamanagement technology can provide solutions to problems; in practice the data needs ofbiomedical research are not well served. The goal of this panel is to expose the barriersblocking the effective application of advanced data management technology to biologicaldata.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,31
Using histograms to estimate answer sizes for XML queries,Yuqing Wu; Jignesh M Patel; HV Jagadish,Abstract Estimating the sizes of query results; and intermediate results; is crucial to manyaspects of query processing. In particular; it is necessary for effective query optimization.Even at the user level; predictions of the total result size can be valuable in “next-step”decisions; such as query refinement. This paper proposes a technique to obtain query resultsize estimates effectively in an XML database. Queries in XML frequently specify structuralpatterns; requiring specific relationships between selected elements. Whereas traditionaltechniques can estimate the number of nodes (XML elements) that will satisfy a node-specific predicate in the query pattern; such estimates cannot easily be combined to provideestimates for the entire query pattern; since element occurrences are expected to have highcorrelation. We propose a solution based on a novel histogram encoding of element …,Information Systems,2003,31
Issues in Building Practical Provenance Systems.,Adriane Chapman; HV Jagadish,Abstract The importance of maintaining provenance has been widely recognized;particularly with respect to highly-manipulated data. However; there are few deployeddatabases that provide provenance information with their data. We have constructed adatabase of protein interactions (MiMI); which is heavily used by biomedical scientists; bymanipulating and integrating data from several popular biological sources. The provenancestored provides key information for assisting researchers in understanding and trusting thedata. In this paper; we describe several desiderata for a practical provenance system; basedon our experience from this system. We discuss the challenges that these requirementspresent; and outline solutions to several of these challenges that we have implemented. Ourlist of a dozen or so desiderata includes: efficiently capturing provenance from external …,IEEE Data Eng. Bull.,2007,30
Method for reducing perceived delay between a time data is requested and a time data is available for display,*,The apparent speed of a connection between a browser at a user station and a proxy orgateway on a network such as the Internet is increased by providing a local proxy at the userstation which interacts with a remote proxy. While the remote proxy is retrieving a newlyrequested World Wide Web page; for example; from the appropriate content provider; it mayalso be sending to the local proxy a stale cached version of that page. When the newversion of the page is finally retrieved; the remote proxy determines the differences betweenthe new version and the stale version; and; assuming the differences do not exceed the newpage in size; sends the differences to the local proxy which then reconstructs the new pagefrom the differences and the stale version. The local proxy delivers the new page to thebrowser; which need not even be aware that a local proxy exists; it is aware only that it …,*,2001,30
Special Issue on Data Reduction Techniques,HV Jagadish,*,Bulletin of the Technical Committee on Data Engineering,1997,30
A spanning tree transitive closure algorithm,Shaul Dar; HV Jagadish,The authors present a transitive closure algorithm that maintains a spanning tree ofsuccessors for each node rather than a simple successor list. This spanning tree structurepromotes sharing of information across multiple nodes and leads to more efficientalgorithms. An effective relational implementation of the spanning tree storage structure issuggested; and it is shown how blocking can be applied to reduce the input/output cost ofthe algorithm. The algorithm can handle path problems also. Analytical and experimentalevidence is presented that demonstrates the utility of the algorithm; especially in a graphwith many alternate paths between the nodes. The spanning tree storage structure can becompressed and updated incrementally in response to changes in the underlying graph.,Data Engineering; 1992. Proceedings. Eighth International Conference on,1992,30
Wisemarket: a new paradigm for managing wisdom of online social users,Caleb Chen Cao; Yongxin Tong; Lei Chen; HV Jagadish,Abstract The benefits of crowdsourcing are well-recognized today for an increasingly broadrange of problems. Meanwhile; the rapid development of social media makes it possible toseek the wisdom of a crowd of targeted users. However; it is not trivial to implement thecrowdsourcing platform on social media; specifically to make social media users as workers;we need to address the following two challenges: 1) how to motivate users to participate intasks; and 2) how to choose users for a task. In this paper; we present Wise Market as aneffective framework for crowdsourcing on social media that motivates users to participate ina task with care and correctly aggregates their opinions on pairwise choice problems. TheWise Market consists of a set of investors each with an associated individual confidence inhis/her prediction; and after the investment; only the ones whose choices are the same as …,Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,2013,29
Literature-based discovery of diabetes-and ROS-related targets,Junguk Hur; Kelli A Sullivan; Adam D Schuyler; Yu Hong; Manjusha Pande; HV Jagadish; Eva L Feldman,Reactive oxygen species (ROS) are known mediators of cellular damage in multiplediseases including diabetic complications. Despite its importance; no comprehensivedatabase is currently available for the genes associated with ROS. We present ROS-anddiabetes-related targets (genes/proteins) collected from the biomedical literature through atext mining technology. A web-based literature mining tool; SciMiner; was applied to 1;154biomedical papers indexed with diabetes and ROS by PubMed to identify relevant targets.Over-represented targets in the ROS-diabetes literature were obtained through comparisonsagainst randomly selected literature. The expression levels of nine genes; selected from thetop ranked ROS-diabetes set; were measured in the dorsal root ganglia (DRG) of diabeticand non-diabetic DBA/2J mice in order to evaluate the biological relevance of literature …,BMC medical genomics,2010,29
iTools: a framework for classification; categorization and integration of computational biology resources,Ivo D Dinov; Daniel Rubin; William Lorensen; Jonathan Dugan; Jeff Ma; Shawn Murphy; Beth Kirschner; William Bug; Michael Sherman; Aris Floratos; David Kennedy; HV Jagadish; Jeanette Schmidt; Brian Athey; Andrea Califano; Mark Musen; Russ Altman; Ron Kikinis; Isaac Kohane; Scott Delp; D Stott Parker; Arthur W Toga,The advancement of the computational biology field hinges on progress in threefundamental directions–the development of new computational algorithms; the availability ofinformatics resource management infrastructures and the capability of tools to interoperateand synergize. There is an explosion in algorithms and tools for computational biology;which makes it difficult for biologists to find; compare and integrate such resources. Wedescribe a new infrastructure; iTools; for managing the query; traversal and comparison ofdiverse computational biology resources. Specifically; iTools stores information about threetypes of resources–data; software tools and web-services. The iTools design;implementation and resource meta-data content reflect the broad research; computational;applied and scientific expertise available at the seven National Centers for Biomedical …,PLoS One,2008,29
Database management for life science research: Summary report of the workshop on data management for molecular and cell biology at the National Library of Medi...,HV Jagadish; Frank Olken,OVER THE PAST 15 YEARS; we have witnessed a dramatic transformation in the practice ofmolecular biology. What was once a cottage industry marked by scarce; expensive dataobtained largely by the manual efforts of small groups of graduate students; post-docs; and afew technicians has become industrialized (routinely and robustly high throughput) and data-rich; marked by factory scale sequencing organizations (such as the Joint Genome Institute;the Whitehead Institute; and the Institute for Genomic Research). Such sequencing factoriesrely on extensive automation of both sequencing and sample preparation. Commencing withsequencing; such industrialization is being extended to high-throughput proteomics andmetabolomics; for example. While this industrialization of biological research is partly theresult of technological improvements in sequencing instrumentation and automated …,OMICS A Journal of Integrative Biology,2003,29
Messaging system with application-defined states,*,A messaging system in which a core messaging infrastructure stores and managesmessaging attributes; but applications external to the core infrastructure define and modifymost attributes. Attribute types may be easily defined or modified; the manner in whichattribute values are obtained may be easily defined or modified; and the entity types to whichattributes are assigned may be easily defined or modified. The messaging system includes aplurality of messaging entities; such as messages; folders; and users; a plurality of attributesassociated with the messaging entities; and a plurality of applications. Each application isoperable to examine and modify at least some of the messaging entities and attributes. Anapplication selection device is operable to examine at least some of the messaging entitiesand at least some of the attributes and to select an application to be invoked; from among …,*,2002,29
System for compression and buffering of a data stream with data extraction requirements,*,A data base system buffers incoming records according to destination in the disk or non-volatile memory. The data is compressed and transferred to disk when sufficient data hasbeen accumulated for a particular disk destination. Techniques for compressing thecompression dictionary as well as the data stream are described.,*,2000,29
Computer with intelligent memory system,*,A programmable memory system that interfaces with a computer's control and datamanipulation units; and is capable of performing the manipulation; bookkeeping; andchecking that would normally be performed by the computer. The memory system comprisesactive structure modules that are interconnected in a network to form clusters. The clustersare interconnected to form an aggregate memory system. Each ASE contains a processorsection and a conventional memory section.,*,1992,29
A study of pipelining in computing arrays,HOSAGRAHAR V Jagadish; Rob G.  Mathews; Thomas Kailath; John A.  Newkirk,In this paper; we take a hard look at scheduling considerations in computing arrays. Asimple sufficient condition is developed for determining whether a computing array can bepipelined. If the array cannot be pipelined in the form given; the condition also indicates thedirection in which to proceed to make it pipelineable. The overall framework andmethodology take a good part of the load off the logical architect of the array; and make thetranslation from the logical to the physical architecture a mechanical process.,IEEE transactions on computers,1986,29
On sequential shape descriptions,HV Jagadish; Alfred M Bruckstein,Abstract Given a shape; we wish to describe it as the union and/or difference of primitive;possibly parameterized; shapes that constitute an alphabet. We would like this description tobe ordered such that “most” of the description is conveyed within the first few terms of thedescription. In other words; we want as small an error as possible for any possible truncationof a description. We present a new criterion for evaluating such sequential descriptions. Forthe specific case of right-angled; or rectilinear; polygons in a plane; and using only arectangle as the primitive shape; we present an algorithm for finding optimal sequentialdescriptions. Though the running time of this algorithm is exponential in the worst case; weshow how running time can be traded off against optimality; and how “reasonable” solutionscan be found quickly.,Pattern Recognition,1992,28
Incorporating hierarchy in a relational model of data,HV Jagadish,Abstract We extend the relational model of data to allow classes as attribute values; therebypermitting the representation of hierarchies of objects. Inheritance; including multipleinheritance with exceptions; is clearly supported. Facts regarding classes of objects can bestored and manipulated in the same way as facts regarding object instances. Our model isupwards compatible with the standard relational model.,ACM SIGMOD Record,1989,28
Natural language query interface; systems; and methods for a database,*,A method for translating a natural language query into a structured query for a database isprovided. The method generally includes: generating a parse tree which represents anatural language query for a database; mapping terms in the parse tree to components of astructured query language for the database; and grouping the components of the structuredquery language.,*,2008,27
Danalix: a domain-adaptive natural language interface for querying xml,Yunyao Li; Ishan Chaudhuri; Huahai Yang; Satinder Singh; HV Jagadish,Abstract We present DaNaLIX; a prototype domain-adaptive natural language interface forquerying XML. Our system is an extension of NaLIX; a generic natural language interface forquerying XML. While retaining the portability of a purely generic system like NaLIX; DaNaLIXcan exploit domain knowledge; whenever available; to its advantage for query translation.More importantly; in DaNaLIX such domain knowledge does not have to be pre-defined;instead it can be automatically obtained from the interactions between a user and thesystem. In this demonstration; we describe the overall architecture of DaNaLIX. We alsodemonstrate how a generic system like DaNaLIX can take advantage of domain knowledgeto improve its usability and query translation accuracy. In addition; we show DaNaLIX stillpossesses the portability of a generic system by using data collections from three different …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,27
Integrating XML data sources using approximate joins,Sudipto Guha; HV Jagadish; Nick Koudas; Divesh Srivastava; Ting Yu,Abstract XML is widely recognized as the data interchange standard of tomorrow because ofits ability to represent data from a variety of sources. Hence; XML is likely to be the formatthrough which data from multiple sources is integrated. In this article; we study the problemof integrating XML data sources through correlations realized as join operations. Achallenging aspect of this operation is the XML document structure. Two documents mightconvey approximately or exactly the same information but may be quite different in structure.Consequently; an approximate match in structure; in addition to content; has to be foldedinto the join operation. We quantify an approximate match in structure and content for pairsof XML documents using well defined notions of distance. We show how notions of distancethat have metric properties can be incorporated in a framework for joins between XML …,ACM Transactions on Database Systems (TODS),2006,27
CQL++: A SQL for the Ode object-oriented DBMS,Shaul Dar; Narain H Gehani; HV Jagadish,Abstract CQL++ is a declarative front end to the Ode object-oriented database. It combines aSQL-like syntax with the C++ class model. CQL++ provides facilities for defining classes;and for creating; querying; displaying; and updating objects. Classes and objects createdusing CQL++ can freely be intermixed with those created using O++; the primaryprogramming language of Ode. CQL++ gives its users a relatively straightforward interface;by hiding from them such O++ details as object-ids; public and private members of objects;and the implementations (bodies) of member functions. CQL++ is based on an objectalgebra that preserves the closure property of SQL. CQL++ queries operate upon sets ofobjects and return sets of objects. CQL++ supports a rich collection of set types to maximizefunctionality and expressive power. CQL++ allows the user to define new classes; which …,International Conference on Extending Database Technology,1992,27
On correctly configuring versioned objects,Rakesh Agrawal; HV Jagadish,We consider the problem of configuring a system in software and design database domains;where a system comprises a version for each of its constituent objects. We present asyntactic characterization of a correct configuration; tied to a transaction model; that makes itpossible to generate automatically all correct configurations of a system. One can alsogenerate configurations that satisfy some selection criteria such as the absence andpresence of specified features; or check whether a user-specified configuration is correct.,Proc. 15th Int. Conf. on Very Large Databases,1989,27
Recovery algorithms for database machines with non-volatile main memory,Rakesh Agrawal; HV Jagadish,1. INTRODUCTION We consider a hypothetical database machine whose main memory isnon-volatile; and study its implications for the recovery subsystem; since that is the aspect ofdatabase systems most impacted by the volatility of today's memory systems. We emphasizethat we are assuming that the whole main memory is non-volatile; in contrast with some of theearlier studies [1;7;18] that stressed the desirability of some non-volatile random-access memoryin addition to the volatile main memory. However; we do not assume that the whole databasecan fit in main memory; and assume that it has been stored on disks. The immediate impetusfor this work is the recent introduction of Ferroelectric Random Access Memories (FRAMs)[4;5;13] that show promise of becoming a DRAM replacement in the not too distant future. FRAMsappear to solve at least the first two of the following problems with EEPROMs; the current …,International Workshop on Database Machines,1989,27
Architecture and design of the MARS hardware accelerator,Prathima Agrawal; William J Dally; Ahmed K Ezzat; WC Fischer; HV Jagadish; AS Krishnakumar,MARS (Microprogrammable Accelerator for Rapid Simulations) is a multiprocessor basedhardware accelerator capable of efficiently implementing a wide range of computationallycomplex algorithms. Its architecture is ideally suited for performing event driven simulationsof VLSI circuits. The highly pipelined and parallel architecture of MARS provides aperformance comparable to existing hardware simulation engines while its highly flexiblearchitecture supports a wide range of applications. Flexibility is achieved through customdesigned microprogrammable and reconfigurable VLSI processors. Logic simulationperformance of about one million events per second is easily achievable.,Design Automation; 1987. 24th Conference on,1987,27
Network analysis of genes regulated in renal diseases: implications for a molecular-based classification,Suresh K Bhavnani; Felix Eichinger; Sebastian Martini; Paul Saxman; HV Jagadish; Matthias Kretzler,Chronic renal diseases are currently classified based on morphological similarities such aswhether they produce predominantly inflammatory or non-inflammatory responses.However; such classifications do not reliably predict the course of the disease and itsresponse to therapy. In contrast; recent studies in diseases such as breast cancer suggestthat a classification which includes molecular information could lead to more accuratediagnoses and prediction of treatment response. This article describes how we extractedgene expression profiles from biopsies of patients with chronic renal diseases; and usednetwork visualizations and associated quantitative measures to rapidly analyze similaritiesand differences between the diseases. The analysis revealed three main regularities:(1)Many genes associated with a single disease; and fewer genes associated with many …,BMC bioinformatics,2009,26
Method of performing approximate substring indexing,*,Approximate substring indexing is accomplished by decomposing each string in a databaseinto overlapping “positional q-grams”; sequences of a predetermined length q; andcontaining information regarding the “position” of each q-gram within the string (ie; 1st q-gram; 4th q-gram; etc.). An index is then formed of the tuples of the positional q-gram data(such as; for example; a B-tree index or a hash index). Each query applied to the database issimilarly parsed into a plurality of positional q-grams (of the same length); and a candidateset of matches is found. Position-directed filtering is used to remove the candidates whichhave the q-grams in the wrong order and/or too far apart to form a “verified” output ofmatching candidates. If errors are permitted (defined in terms of an edit distance betweeneach candidate and the query); an edit distance calculation can then be performed to …,*,2006,26
Method for associating integrity maintenance constraints with data object classes in an object-oriented database,*,A technique is provided for the association of constraints with data object classes in anobject-oriented database. Given a constraint comprising zero or more quantifiers followed bya quantifier-free boolean expression; a non-primitive reference expression is identified in theboolean expression. The constraint is then modified as follows. First; the primitive referenceexpression at the head of the identified non-primitive reference expression is replaced with avariable. Then a universal quantifier which quantifies the added variable over all dataobjects in the class comprising that primitive reference expression is added to the constraint.Finally; a disjunctive condition specifying that the added variable is not equal to the primitivereference expression is added to the boolean expression. The modified constraint may thenbe associated with the class comprising the aforesaid primitive reference expression. This …,*,1996,25
Optimization of generalized transitive closure queries,Shaul Dar; Rakesh Agrawal; Hosagrahar V Jagadish,Two complementary techniques for optimizing generalized transitive closure queries arepresented:(i) selections on paths are applied during the closure computation; so that pathsthat are not in the result and that are not needed to compute the result are pruned as earlyas possible and (ii) paths that are in the result; or needed to compute the result; arerepresented in a condensed form. The condensed representation holds the minimalinformation that is necessary for the specified label computations and selections to beperformed. The combined impact of these techniques is that the number of paths generatedduring the closure computation and the storage required for each such path are both greatlyreduced.,Data Engineering; 1991. Proceedings. Seventh International Conference on,1991,24
A Compressed Transitive Closure Technique for Efficient Fixed-Point Query Processing.,HV Jagadish,*,Expert Database Conf.,1988,24
CRIUS: user-friendly database design,Li Qian; Kristen LeFevre; HV Jagadish,Abstract Non-technical users are increasingly adding structures to their data. This gives riseto the need for database design. However; traditional database design is deliberate andheavy-weight; requiring technical expertise that everyday users may not possess. For thisreason; we propose that users of personal data management applications should be able tocreate and refine data structures in an ad-hoc way over time; thereby" organically" growingtheir schemas. For this purpose; we develop a spreadsheet-like direct manipulationinterface. We show how integrity constraints can still provide value; even in this scenario offrequent schema and data modifications. We also develop a back-end databaseimplementation to support this interface; with a design that permits schema changes at a lowcost. We have folded these ideas into a system; called CRIUS; which supports a nested …,Proceedings of the VLDB Endowment,2010,23
Multi-level operator combination in XML query processing,Shurug Al-Khalifa; HV Jagadish,Abstract A core set of efficient access methods is central to the development of any databasesystem. In the context of an XML database; there has been considerable effort devoted todefining a good set of primitive operators and inventing efficient access methods for eachindividual operator. These primitive operators have been defined either at the macro-level(using a" pattern tree" to specify a selection; for example) or at the micro-level (using multipleexplicit containment joins to instantiate a single XPath expression). In this paper we arguethat it is valuable to consider operations at each level. We do this through a study of operatormerging: the development of a new access method to implement a combination of two ormore primitive operators. It is frequently the case that access methods for merged operatorsare superior to a pipelined execution of separate access methods for each operator. We …,Proceedings of the eleventh international conference on Information and knowledge management,2002,23
Scalable versioning in distributed databases with commuting updates,HV Jagadish; Inderpal Singh Mumick; Michael Rabinovich,Presents a multiversioning scheme for a distributed system with the workload consisting ofread-only transactions and update transactions;(most of) which commute on individualnodes. The scheme introduces a version advancement protocol that is completelyasynchronous with user transactions; thus allowing the system to scale to very hightransaction rates and frequent version advancements. Moreover; the scheme never createsmore than three copies of a data item. Combined with existing techniques to avoid globalconcurrency control for commuting transactions that execute in a particular version; ourmultiversioning scheme results in a protocol where no user transaction on a node can bedelayed by any activity (either version advancement or another transaction) occurring onanother node. Non-commuting transactions are gracefully handled. Our technique is of …,Data Engineering; 1997. Proceedings. 13th International Conference on,1997,22
Method and apparatus for analyzing co-evolving time sequences,*,An analyzer system that analyzes a plurality of co-evolving time sequences to; for example;perform correlation or outlier detection on the time sequences. The plurality of co-evolvingtime sequences comprise a delayed time sequence and one or more known timesequences. A goal is to predict the delayed value given the available information. Theplurality of time sequences have a present value and (N-1) past values; where N is thenumber of samples (time-ticks) of each time sequence. The analyzer system receives theplurality of co-evolving time sequences and determines a window size (" w"). The analyzerthen assigns the delayed time sequence as a dependent variable and the present value of asubset of the known time sequences; and the past values of the subset of known timesequences and the delayed time sequence; as a plurality of independent variables. Past …,*,2000,21
CQL++: A SQL for a c++ based object-oriented DBMS,S Dar; NH Gehani; HV Jagadish,Abstract INTRODUCTION Ode is a database system and environment based on the objectparadigm. It offers an integrated data model for both database and general purposemanipulation [1; 2]. The database is defined; queried; and manipulated in the databaseprogramming language O++ which is based on C++[24]. O++ borrows and extends theobject definition facility of C++; called the class. O++ provides facilities for creating andmanipulating persistent objects; defining sets; and iterating over sets and clusters ofpersistent objects. It also provides facilities for specifying constraints and triggers. CQL++ isan adaptation of SQL [17] to the Ode object-oriented model. We were motivated to design aSQL-like interface to Ode because SQL is by far the most popular database interface.Typical database users are not likely to use a programming language such as O++. A …,In Proceedings of the International Conference on Extending Database Technology,1992,21
Efficient Search in Very Large Databases.,Rakesh Agrawal; HV Jagadish,ABSTRACT We consider the problem of performing efficient search in a large databasesystem. We present a novel data structuring technique and show how a branch and boundsearch algorithm can use the proposed data organization to prune the search space.Simulation results confirm that; using these techniques; a search can be expeditedsignificantly without incurring a large storage penalty. As a side benefit; it is possible toorganize the search to obtain successive approximations to the desired solution withconsiderable reduction in total search.,VLDB,1988,21
Challenges and opportunities with big data—A community white paper developed by leading researchers across the United States. 2012,D Agrawal; P Bernstein; E Bertino; S Davidson; U Dayal; M Franklin; J Gehrke; L Haas; A Halevy; J Han; HV Jagadish; A Labrinidis; S Madden; Y Papakonstantinou; JM Patel; R Ramakrishnan; K Ross; C Shahabi; D Suciu; S Vaithyanathan; J Widom,*,Google Scholar,*,21
Using Data for Systemic Financial Risk Management.,Mark Flood; HV Jagadish; Albert Kyle; Frank Olken; Louiqa Raschid,ABSTRACT The recent financial collapse has laid bare the inadequacies of the informationinfrastructure supporting the US financial system. Technical challenges around large-scaledata systems interact with significant economic forces involving innovation; transparency;confidentiality; complexity; and organizational change; to create a very difficult problem. Thepost-crisis reform legislation has created a unique opportunity to rebuild financial riskmanagement on a solid foundation of information management principles. This should helpreduce operating costs and operational risk. More importantly; it will support both themonitoring and the containment of financial risk on a previously unprecedented scale. Theseobjectives will pose several information management challenges; including issues ofknowledge representation; information quality; data integration; and presentation. This …,CIDR,2011,20
Flexible list management in a directory,HV Jagadish; Mark A Jones; Divesh Srivastava; Dimitra Vista,Abstract Lists of entities must often be speciﬁed in many real-world applications such ascustomer lists; electronic distribution lists and access control lists. These lists are typicallyspeciﬁed through explicit enumeration; frequently aided by recursive expansion. In thispaper; we discuss the declara-tive speciﬁcation and extraction of members of such lists asqueries over a directory that maintains information both about individuals and about lists;and identify key features that the directory must support to manage lists in a ﬂexible manner.X. 500 is the industry standard for modeling infor-mation about individuals in a directory; andLDAP is the proposed standard for accessing directory information. We have designed andbuilt a system to represent and manage lists in the X. 500 information model; and developedeffi-ciently evaluable extensions to the LDAP query language for the location and …,Proceedings of the seventh international conference on Information and knowledge management,1998,20
A physical algebra for XML,Stelios Paparizos; Shurug Al-Khalifa; HV Jagadish; Andrew Nierman; Yuqing Wu,Abstract We present a physical algebra for the manipulation of XML in a database. We showhow to map logical algebra operators to this physical algebra. We also present severalphysical algebra identities that are useful for query optimization. This physical algebra is thebasis for the implementation of the TIMBER native XML database system at the University ofMichigan.,*,2002,19
Client-based logging for high performance distributed architectures,Euthimios Panagos; Alexandros Biliris; HV Jagadish; Rajeev Rastogi,Proposes logging and recovery algorithms for distributed architectures that use local diskspace to provide transactional facilities locally. Each node has its own log file where all logrecords for updates to locally cached pages are written. Transaction rollback and node crashrecovery are handled exclusively by each node and log files are not merged at any time. Ouralgorithms do not require any form of time synchronization between nodes and nodes cantake checkpoints independently of each other. Finally; our algorithms make possible a newparadigm for distributed transaction management that has the potential to exploit allavailable resources and improve scalability and performance.,Data Engineering; 1996. Proceedings of the Twelfth International Conference on,1996,19
Federation in cloud data management: Challenges and opportunities,Gang Chen; HV Jagadish; Dawei Jiang; David Maier; Beng Chin Ooi; Kian-Lee Tan; Wang-Chiew Tan,Companies are increasingly moving their data processing to the cloud; for reasons of cost;scalability; and convenience; among others. However; hosting multiple applications andstorage systems on the same cloud introduces resource sharing and heterogeneous dataprocessing challenges due to the variety of resource usage patterns employed; the variety ofdata types stored; and the variety of query interfaces presented by those systems.Furthermore; real clouds are never perfectly symmetric-there often are differences betweenindividual processors in their capabilities and connectivity. In this paper; we introduce afederation framework to manage such heterogeneous clouds. We then use this framework todiscuss several challenges and their potential solutions.,IEEE Transactions on Knowledge and Data Engineering,2014,18
Method of pattern searching,*,Structural join mechanisms provide efficient query pattern matching.In one embodiment; tree-merge mechanisms are provided. In anotherembodiment; stack-tree mechanisms are provided.,*,2008,18
Indexing for retrieval by similarity,HV Jagadish,Summary In multimedia databases; it is often the case that objects to be retrieved onlyapproximately meet the conditions specified in the query. Notions of similarity are diverseand application dependent. Nevertheless; the need for indexing is still present. The standardtechnique for this purpose is to map the query and each object to a point is some multi-dimesional “feature space” such that two similar objects are guaranteed not to be too farapart in this space. Then multidimensional point index structures can be used; with the queryregion appropriately expanded around the specified query point. In this chapter we illustratethe use of this technique with experimental results from two domains: the first a set ofmachine-generated rectiliner shapes; and the second a set of English words from an onlinedictionary.,*,1996,18
Method and apparatus for substring selectivity estimation,*,A method for estimating string-occurrence probability in a database comprises receiving afirst probability of occurrence for each maximal substring from a plurality of substrings; eachmaximal substring in the plurality of substrings belonging to the string; obtaining an overallprobability of occurrence; receiving a probability of occurrence for a maximal overlap of eachmaximal substring in the plurality of maximal substrings; obtaining a normalization factor;and dividing the overall probability of occurrence by the normalization factor to obtain theestimate.,*,2002,17
The michigan benchmark,Kanda Runapongsa; Jignesh M Patel; HV Jagadish; Shurug Al-Khalifa,*,Technical Report,2002,17
Maintenance and Self Maintenance of Outer-Join Views.,Ashish Gupta; Hosagrahar V Jagadish; Inderpal Singh Mumick,*,NGITS,1997,17
Answering SQL queries using materialized views,Divesh Srivastava; Shaul Dar; HV Jagadish; Alon Y Levy,*,Proc. of VLDB. Bombay; India,1996,17
Method of performing approximate substring indexing,*,Approximate substring indexing is accomplished by decomposing each string in a databaseinto overlapping “positional q-grams”; sequences of a predetermined length q; andcontaining information regarding the “position” of each q-gram within the string (ie; 1st q-gram; 4th q-gram; etc.). An index is then formed of the tuples of the positional q-gram data(such as; for example; a B-tree index or a hash index). Each query applied to the database issimilarly parsed into a plurality of positional q-grams (of the same length); and a candidateset of matches is found. Position-directed filtering is used to remove the candidates whichhave the q-grams in the wrong order and/or too far apart to form a “verified” output ofmatching candidates. If errors are permitted (defined in terms of an edit distance betweeneach candidate and the query); an edit distance calculation can then be performed to …,*,2008,16
Hierarchical or relational? A case for a modern hierarchical data model,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Much of the data we deal with every day is organized hierarchically: file systems; libraryclassification schemes and yellow page categories are salient examples. Business data too;benefits from a hierarchical organization; and indeed the hierarchical data model was quiteprevalent thirty years ago. Due to the recently increased importance of X. 500/LDAPdirectories; which are hierarchical; and the prevalence of aggregation hierarchies indatacubes; there is now renewed interest in the hierarchical organization of data. Wedevelop a framework for a modern hierarchical data model; substantially improved from theoriginal version by taking advantage of the lessons learned in the relational databasecontext. We argue that this new hierarchical data model has many benefits with respect tothe ubiquitous flat relational data model. We argue also that this model is well-suited for …,Knowledge and Data Engineering Exchange; 1999.(KDEX'99) Proceedings. 1999 Workshop on,1999,16
Method for parsing images,*,Parsing of partially orderable sets of symbols is achieved by first forming a total order of thegiven terminal symbols. The relationships between the terminal symbols are formulated; andthe given grammar is broken up into a set of production rules. Based on the production rules;a determination is made as to what kind of symbol is needed to comply with the givengrammar. This determination encompasses both the type of terminal symbol that is to befound as well as the partial order relationships that such a symbol must have. The set oftotally ordered symbols are searched; in order; and the first symbol that meets the specifiedcriteria is selected. When appropriate; the symbols retrieved from the totally ordered set arereduced to non-terminal symbols; with the ultimate goal being the inclusion of all of thesymbols in the totally ordered set within one reduced non-terminal symbol.,*,1994,16
SIGOPT: Using schema to optimize XML query processing,Stelios Paparizos; Jignesh M Patel; HV Jagadish,There has been a great deal of work in recent years on processing and optimizing queriesagainst XML data. Typically in these previous works; schema information is not considered;so that evaluation techniques can continue to be used even in the absence of one. However;schema information is often available and; in this paper; we show that when available it canbe exploited to great advantage in ways that complement" traditional" XML queryoptimization. To be usable in practice; we require that aspects of schema; essential for ourpurposes; be captured in a schema information graph (SIG). We exploit such meta-dataknowledge with a preprocessing enumeration phase that detects potentiallyinterchangeable evaluation units-we call such units alternate paths. We show; within analgebraic framework; methods that can break down a pattern tree into elementary paths …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,15
Skimmer: rapid scrolling of relational query results,Manish Singh; Arnab Nandi; HV Jagadish,Abstract A relational database often yields a large set of tuples as the result of a query.Users browse this result set to find the information they require. If the result set is large; theremay be many pages of data to browse. Since results comprise tuples of alphanumericvalues that have few visual markers; it is hard to browse the data quickly; even if it is sorted.In this paper; we describe the design of a system for browsing relational data by scrollingthrough it at a high speed. Rather than showing the user a fast changing blur; the systempresents the user with a small number of representative tuples. Representative tuples areselected to provide a" good impression" of the query result. We show that the informationloss to the user is limited; even at high scrolling speeds; and that our algorithms can pickgood representatives fast enough to provide for real-time; high-speed scrolling over large …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,14
Compacting music signatures for efficient music retrieval,Bin Cui; HV Jagadish; Beng Chin Ooi; Kian-Lee Tan,Abstract Music information retrieval is becoming very important with the ever-increasinggrowth of music content in digital libraries; peer-to-peer systems and the internet. While it iseasy to quantize music into a discrete string representation; retrieval by content requires(approximate) sub-string matching; which is hard. In this paper; we present a novel system;called MUSIG; that uses compact MUsic SIGnatures for efficient contentbased musicretrieval. The signature is computed as follows:(a) each music file is split into a set of(overlapping) segments;(b) similar segments are clustered together; the number of clusterscorresponds to the number of dimensions;(c) for each music file; the number of its segmentsthat fall into a cluster determines the key value in that dimension.,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,14
Sprite: A learning-based text retrieval system in dht networks,Yingguang Li; HV Jagadish; Kian-Lee Tan,In this paper; we propose SPRITE (selective progressive index tuning by examples); ascalable system for text retrieval in a structured P2P network. Under SPRITE; each peer isresponsible for a certain number of terms. However; for each document; SPRITE learns from(past) queries to select only a small set of representative terms for indexing; and these termsare progressively refined with subsequent queries. We implemented the proposed strategy;and compare its retrieval effectiveness in terms of both precision and recall against a staticscheme (without learning) and a centralized system (ideal). Our experimental results showthat SPRITE is nearly as effective as the centralized system; and considerably outperformsthe static scheme.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,14
Customer group billing,*,A method and system in which calls made by customers who are members of a group ofcustomers having a group calling plan; but who have separate billing accounts; are pricedbased on the group calling plan and on the usage by other members of the group. Typicalgroups may include customers in specific geographic locations; such as residents in anapartment building or in a residential neighborhood; and offices and stores in a commercialbuilding. Other groups may include customers in widely separated geographic locations;such as members of a nation-wide club or organization; alumni association members; etc. Inorder to price a call made over a network by a customer of the network; the customer being amember of a billing group; information specifying a group billing plan of the billing group isstored. A call made by a member of the billing group is priced based on the group billing …,*,2001,14
Content-based indexing and retrieval,HV Jagadish,*,The handbook of multimedia information management,1997,14
A proclamation-based model for cooperating transactions,HV Jagadish; Oded Shmueli,ABSTRACT We propose a transaction model that provides a framework for transactions tocooperate without sacrificing serializability as a notion of correctness. Cooperation does notdepend on detailed knowledge of the semantics of transaction operations. Semanticproperties such as data dependent commutativity can be ''discovered''automatically at runtime without a need to declare these properties explicitly. When transactions wish tocooperate; they do so by issuing ''proclamations''. A proclamation is an (implicitly or explicitlyspecified) set of values; one of which the transaction ''promises''to write if it commits. So; aproclamation provides incomplete information concerning future possible database states.Transactions can compute with this incomplete information; and can commit after writingconditional multivalues.,VLDB,1992,14
BLAST: a loosely schema-aware meta-blocking approach for entity resolution,Giovanni Simonini; Sonia Bergamaschi; HV Jagadish,Abstract Identifying records that refer to the same entity is a fundamental step for dataintegration. Since it is prohibitively expensive to compare every pair of records; blockingtechniques are typically employed to reduce the complexity of this task. These techniquespartition records into blocks and limit the comparison to records co-occurring in a block.Generally; to deal with highly heterogeneous and noisy data (eg semi-structured data of theWeb); these techniques rely on redundancy to reduce the chance of missing matches.,Proceedings of the VLDB Endowment,2016,13
Long-tail vocabulary dictionary extraction from the web,Zhe Chen; Michael Cafarella; HV Jagadish,Abstract A dictionary---a set of instances belonging to the same conceptual class---is centralto information extraction and is a useful primitive for many applications; including query loganalysis and document categorization. Considerable work has focused on generatingaccurate dictionaries given a few example seeds; but methods to date cannot obtain long-tail (rare) items with high accuracy and recall. In this paper; we develop a novel method toconstruct high-quality dictionaries; especially for long-tail vocabularies; using just a few user-provided seeds for each topic. Our algorithm obtains long-tail (ie; rare) items by building andexecuting high-quality webpage-specific extractors. We use webpage-specific structural andtextual information to build more accurate per-page extractors in order to detect the long-tailitems from a single webpage. These webpage-specific extractors are obtained via a co …,Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,2016,13
The NIH national center for integrative biomedical informatics (NCIBI),Brian D Athey; James D Cavalcoli; HV Jagadish; Gilbert S Omenn; Barbara Mirel; Matthias Kretzler; Charles Burant; Raphael D Isokpehi; Charles DeLisi; NCIBI faculty; trainees; and staff,Abstract The National Center for Integrative and Biomedical Informatics (NCIBI) is one of theeight NCBCs. NCIBI supports information access and data analysis for biomedicalresearchers; enabling them to build computational and knowledge models of biologicalsystems to address the Driving Biological Problems (DBPs). The NCIBI DBPs have includedprostate cancer progression; organ-specific complications of type 1 and 2 diabetes; bipolardisorder; and metabolic analysis of obesity syndrome. Collaborating with these and otherpartners; NCIBI has developed a series of software tools for exploratory analysis; conceptvisualization; and literature searches; as well as core database and web services resources.Many of our training and outreach initiatives have been in collaboration with the ResearchCenters at Minority Institutions (RCMI); integrating NCIBI and RCMI faculty and students …,Journal of the American Medical Informatics Association,2011,13
Integration of IR into an XML Database.,Cong Yu; Hong Qi; HV Jagadish,ABSTRACT Structure matching has been the focus and strength of standard XML querying.However; textual content is still an essential component of XML data. It is therefore importantto extend the standard XML database engine to allow for “Information Retrieval” stylequeries; namely;“keyword” based retrieval and “result ranking”. In this paper; we describeour effort in integrating information retrieval techniques into the Timber XML databasesystem being developed at the University of Michigan; and our participation in the INitiativefor the Evaluation of XML Retrieval (INEX).,INEX Workshop,2002,13
Research directions in image database management,William I Grosky; Rajiv Mehrotra; F Golshani; HV Jagadish; Ramesh Jain; Wayne Niblack,This analysis is a panel discussion. There are many problems in the field of image databasemanagement. The object-oriented paradigm has been and continues to be a great impetusto this work. The semantics of images is essentially what they contain; and unless there is aneffective method to identify their contents and index them on that basis; the database willdegenerate to a collection of patterns with no semantics. This is the most challenging issuefacing multimedia information systems in general; and image databases in particular. Workon query by image content has barely begun to scratch the surface. A few key queryprimitives will become well-understood and widely supported. To allow users to browse andsearch through information domains using sophisticated querying techniques that includeimprecise queries; user-directed query processing; and queries that use similarity …,Data Engineering; 1992. Proceedings. Eighth International Conference on,1992,13
Obtaining schedules for digital systems,HV Jagadish; Thomas Kailath,A systematic technique is presented to derive correct schedules for a synchronous digitalsystem; given a signal flow graph for an algorithm. It is also shown how to use this techniqueto derive designs that are optimal in having the lowest latency; the highest throughput; or thesmallest number of registers. The same technique can also be used to verify digital systemsthat have already been designed.,IEEE Transactions on signal processing,1991,13
A high bandwidth intelligent memory for supercomputers,Abhaya Asthana; HV Jagadish; Jonathan A Chandross; Daniel Lin; Scott C Knauer,*,Proceedings Third International Conference on Supercomputing,1988,13
The Materials Commons: a collaboration platform and information repository for the global materials community,Brian Puchala; Glenn Tarcea; Emmanuelle A Marquis; Margaret Hedstrom; HV Jagadish; John E Allison,Abstract Accelerating the pace of materials discovery and development requires newapproaches and means of collaborating and sharing information. To address this need; weare developing the Materials Commons; a collaboration platform and information repositoryfor use by the structural materials community. The Materials Commons has been designedto be a continuous; seamless part of the scientific workflow process. Researchers upload theresults of experiments and computations as they are performed; automatically wherepossible; along with the provenance information describing the experimental andcomputational processes. The Materials Commons website provides an easy-to-useinterface for uploading and downloading data and data provenance; as well as for searchingand sharing data. This paper provides an overview of the Materials Commons. Concepts …,Jom,2016,12
Trajectory simplification: on minimizing the direction-based error,Cheng Long; Raymond Chi-Wing Wong; HV Jagadish,Abstract Trajectory data is central to many applications with moving objects. Raw trajectorydata is usually very large; and so is simplified before it is stored and processed. Manytrajectory simplification notions have been proposed; and among them; the direction-preserving trajectory simplification (DPTS) which aims at protecting the direction informationhas been shown to perform quite well. However; existing studies on DPTS require users tospecify an error tolerance which users might not know how to set properly in some cases(eg; the error tolerance could only be known at some future time and simply setting one errortolerance does not meet the needs since the simplified trajectories would usually be used inmany different applications which accept different error tolerances). In these cases; a bettersolution is to minimize the error while achieving a pre-defined simplification size. For this …,Proceedings of the VLDB Endowment,2014,12
Lost source provenance,Jing Zhang; HV Jagadish,Abstract As the use of derived information has grown in recent years; the importance ofprovenance has been recognized; and there has been a great deal of effort devoted todeveloping techniques to identify individual source tuples used in the derivation of any resulttuple. Often; however; the source database may have been updated since the result wasderived; and the source tuples of interest are not in the database any more. In suchsituations; the provenance management system has to reconstruct relevant historicalfragments of the source database as they were at derivation time. In this paper; we developtechniques to address this problem. Our experimental assessment shows that thesetechniques do so efficiently; and with low storage overhead.,Proceedings of the 13th International Conference on Extending Database Technology,2010,12
Optimal indexing using near-minimal space,Cinda Heeren; HV Jagadish; Leonard Pitt,Abstract We consider the index selection problem. Given either a fixed query workload or anunknown probability distribution on possible future queries; and a bound B on how muchspace is available to build indices; we seek to build a collection of indices for which theaverage query response time is minimized. We give strong negative and positivepeformance bounds. Let m be the number of queries in the workload. We show how toobtain with high probability a collection of indices using space O (B ln m) for which theaverage query cost is opt B; the optimal performance possible for indices using at most Btotal space. Moreover; this space relaxation is necessary: unless NP⊆ n O (log log n); nopolynomial time algorithm can guarantee average query cost less than M 1--ε opt B usingspace αB; for any constant α; where M is the size of the dataset. We quantify the error in …,Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2003,12
On bounding-schemas for LDAP directories,Sihem Amer-Yahia; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Abstract As our world gets more networked; ever increasing amounts of information arebeing stored in LDAP directories. While LDAP directories have considerable flexibility in themodeling and retrieval of information for network applications; the notion of schema theyprovide for enabling consistent and coherent representation of directory information is ratherweak. In this paper; we propose an expressive notion of bounding-schemas for LDAPdirectories; and illustrate their practical utility. Bounding-schemas are based on lower boundand upper bound specifications for the content and structure of an LDAP directory. Given abounding-schema specification; we present algorithms to efficiently determine:(i) if an LDAPdirectory is legal wrt the bounding-schema; and (ii) if directory insertions and deletionspreserve legality. Finally; we show that the notion of bounding-schemas has wider …,International Conference on Extending Database Technology,2000,12
Composite Events in a Distributed Object-Oriented Database.,HV Jagadish; Oded Shmueli,*,IWDOM,1992,12
Composition of database relations,Rakesh Agrawal; Shaul Dar; HV Jagadish,The authors argue for implementing composition as a primitive operation and present asingle-sided composition algorithm that performs join protection and duplicate elimination asone unified operation. They report experimental results that show an operating region inwhich this algorithm outperforms composition by the standard method. This operating regionis characterized by a join result many times larger than the source relations; and manyduplicates after projection over the nonjoin attributes. This occurs; for example; in deductivedatabases when computing transitive closures of dense graphs.,Data Engineering; 1989. Proceedings. Fifth International Conference on,1989,12
Schema-free SQL,Fei Li; Tianyin Pan; Hosagrahar V Jagadish,Abstract Querying data in relational databases is often challenging since SQL requires itsusers to know the exact schema of the database; the roles of various entities in a query; andthe precise join paths to be followed. On the other hand; keyword search is unable toexpress much desired query semantics. In this paper; we propose a query language;Schema-free SQL; which enables its users to query a relational database using whateverpartial schema they know. If they know the full schema; they can write full SQL. But; to theextent they do not know the schema; Schema-free SQL is tolerant of unknown orinaccurately specified relation names and attribute names; and it also does not requireinformation regarding which relations are involved and how they are joined. We presenttechniques to evaluate Schema-free SQL by first converting it to full SQL. We show …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,11
CrowdMatcher: crowd-assisted schema matching,Chen Jason Zhang; Ziyuan Zhao; Lei Chen; Hosagrahar Visvesvaraya Jagadish; Chen Caleb Cao,Abstract Schema matching is a central challenge for data integration systems. Due to theinherent uncertainty arose from the inability of schema in fully capturing the semantics of therepresented data; automatic tools are often uncertain about suggested matching results.However; human is good at understanding data represented in various forms andcrowdsourcing platforms are making the human annotation process more affordable. Thus inthis demo; we will show how to utilize the crowd to find the right matching. In order to do that;we need to make the tasks posted on the crowdsouricng platforms extremely simple; to beperformed by non-expert people; and reduce the number of tasks as less as possible to savethe cost. We demonstrate CrowdMatcher; a hybrid machine-crowd system for schemamatching. The machine-generated matchings are verified by correspondence correctness …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,11
THINK Back: knowledge-based interpretation of high throughput data,Fernando Farfán; Jun Ma; Maureen A Sartor; George Michailidis; Hosagrahar V Jagadish,Results of high throughput experiments can be challenging to interpret. Current approacheshave relied on bulk processing the set of expression levels; in conjunction with easilyobtained external evidence; such as co-occurrence. While such techniques can be used toreason probabilistically; they are not designed to shed light on what any individual gene; ora network of genes acting together; may be doing. Our belief is that today we have theinformation extraction ability and the computational power to perform more sophisticatedanalyses that consider the individual situation of each gene. The use of such techniquesshould lead to qualitatively superior results. The specific aim of this project is to developcomputational techniques to generate a small number of biologically meaningful hypothesesbased on observed results from high throughput microarray experiments; gene …,BMC bioinformatics,2012,11
Understanding provenance black boxes,Adriane Chapman; HV Jagadish,Abstract Current provenance stores associated with workflow management systems(WfMSs) capture enough coarse-grained information to describe which datasets were usedand which processes were run. While this information is enough to rebuild a workflow run; itis not enough to facilitate user understanding. Because the data is manipulated via a seriesof black boxes; it is often impossible for a human to understand what happened to the data.In this work; we highlight the missing information that can assist user understanding.Unfortunately; provenance information is already very complex and difficult for a user tocomprehend; which can be exacerbated by adding the extra information needed for deeperblackbox understanding. In order to alleviate this; we develop a model of provenanceanswers that follow a “roll up”;“drill down” strategy. We evaluate these techniques to …,Distributed and Parallel Databases,2010,11
Relaxed space bounding for moving objects: A case for the buddy tree,Shuqiao Guo; Zhiyong Huang; HV Jagadish; Beng Chin Ooi; Zhenjie Zhang,Abstract Rapid advancements in positioning systems and wireless communications enableaccurate tracking of continuously moving objects. This development poses new challengesto database technology since maintaining up-to-date information regarding the location ofmoving objects incurs an enormous amount of updates. There have been many efforts toaddress these challenges; most of which depend on the use of a minimum boundingrectangle (MBR) in a multi-dimensional index structure such as R-tree. The maintenance ofMBRs causes lock contention and association of moving speeds with the MBRs cause largeoverlap between them. This problem becomes more severe as the number of concurrentoperations increases. In this paper; we propose a" new" simple variant of the Buddy-tree; inwhich we enlarge the query rectangle to account for object movement rather than use an …,ACM SIGMOD Record,2006,11
Toward efficient multifeature query processing,HV Jagadish; Beng Chin Ooi; Heng Tao Shen; K-L Tan,In many advanced applications; data are described by multiple high-dimensional features.Moreover; different queries may weight these features differently; some may not even specifyall the features. In this paper; we propose our solution to support efficient query processingin these applications. We devise a novel representation that compactly captures f featuresinto two components. The first component is a 2D vector that reflects a distance range(minimum and maximum values) of the f features with respect to a reference point (the centerof the space) in a metric space and the second component is a bit signature; with two bitsper dimension; obtained by analyzing each feature's descending energy histogram. Thisrepresentation enables two levels of filtering: the first component prunes away points that donot share similar distance ranges; while the bit signature filters away points based on the …,IEEE transactions on knowledge and data engineering,2006,11
Querying XML using structures and keywords in Timber,Cong Yu; HV Jagadish; Dragomir R Radev,Abstract This demonstration will describe how Timber; a native XML database system; hasbeen extended with the capability to answer XML-style structured queries (eg; XQuery) withembedded IR-style keyword-based non-boolean conditions. With the original structuredquery processing engine and the IR extensions built into the system; Timber is well suited forefficiently and effectively processing queries with both structural and textual contentconstraints.,Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,2003,11
Compatibility determination in web services,Yunyao Li; HV Jagadish,ABSTRACT Determining the compatibility between Web services plays a critical role insupporting dynamic discovery and collaboration of Web services in the inherentlyheterogeneous web environment. In this paper we present a compatibility determinationalgorithm. The algorithm takes two graphs (each representing the external interface of aWeb service) as inputs; and produces the following as outputs (1) judgment of compatibility(true or false); and (2) differences between two graphs. The outputs can be used by a Webservice to enable dynamic collaboration between Web services. A visualized representationof differences found can enable a human to determine the criticality of the differences.,Order,2000,11
Impact of Advanced VLSI packaging on the design of a large parallel computer,Abhaya Asthana; HV Jagadish; BOYDT MATHEWS,*,1989 International Conference on Parallel Processing; University Park; PA,1989,11
An intelligent memory system,A Asthana; HV Jagadish; JA Chandross; D Lin; SC Knauer,Abstract SWIM (Structured Wafer-Scale Intelligent Memory) is a high bandwidth; multi-ported; disk-sized memory system capable of storing; maintaining; and manipulating datastructures within it; independent of the main processing units. Up to thousands of activestorage elements; each element having some storage and some associated processinglogic; function independently or in groups to implement userdefined objects. SWIMincreases memory functionality to better balance the time spent in moving data with thatinvolved in actually manipulating it. Just as one may associate a cache with each processor;each memory module has processing logic associated with it. Such logic decreases theprocessormemory bandwidth requirements; improves memory utilization; scales better in amultiprocessor; and yields a faster response from memory. The faster response results …,ACM SIGARCH Computer Architecture News,1988,11
Big data challenges and opportunities in financial stability monitoring,Mark D Flood; HV Jagadish; Louiqa Raschid,1) Background “Big data” means more than simply larger storage requirements; or collectingdata from social media platforms with millions of participants.“Bigness” is a symptom ofscalability issues in one or more dimensions—the four Vs of volume; velocity; variety; andveracity (IBM; 2016).“Big data” is a misnomer; suggesting that “bigness” is an intrinsiccharacteristic of a dataset. Rather; bigness describes the relationship between a dataset andits usage context. 1 A dataset is too big for a particular use case when it becomescomputationally infeasible to process the dataset using traditional tools (MongoDB; 2016).Scalability is a binding constraint for any process; of course; if extrapolated too far. Big datacan create an inflection point where differences in scale imply transformational differences inthe costs and benefits associated with using the data. Big data is not the only challenge …,Banque de France; Financial Stability Review,2016,10
Object semantics for XML keyword search,Thuy Ngoc Le; Tok Wang Ling; HV Jagadish; Jiaheng Lu,Abstract It is well known that some XML elements correspond to objects (in the sense ofobject-orientation) and others do not. The question we consider in this paper is what benefitswe can derive from paying attention to such object semantics; particularly for the problem ofkeyword queries. Keyword queries against XML data have been studied extensively inrecent years; with several lowest-common-ancestor based schemes proposed for thispurpose; including SLCA; MLCA; VLCA; and ELCA. It can be seen that identifying objectscan help these techniques return more meaningful answers than just the LCA node (orsubtree) by returning objects instead of nodes. It is more interesting to see that objectsemantics can also be used to benefit the search itself. For this purpose; we introduce anovel Nearest Common Object Node semantics (NCON); which includes not just common …,International Conference on Database Systems for Advanced Applications,2014,10
Breaking out of the mismatch trap,Yong Zeng; Zhifeng Bao; Tok Wang Ling; HV Jagadish; Guoliang Li,When users issue a query to a database; they have expectations about the results. If whatthey search for is unavailable in the database; the system will return an empty result or;worse; erroneous mismatch results. We call this problem the MisMatch Problem. In thispaper; we solve the MisMatch problem in the context of XML keyword search. Our solution isbased on two novel concepts that we introduce: Target Node Type and Distinguishability.Using these concepts; we develop a low-cost post-processing algorithm on the results ofquery evaluation to detect the MisMatch problem and generate helpful suggestions to users.Our approach has three noteworthy features:(1) for queries with the MisMatch problem; itgenerates the explanation; suggested queries and their sample results as the output tousers; helping users judge whether the MisMatch problem is solved without reading all …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,10
Swst: A disk based index for sliding window spatio-temporal data,Manish Singh; Qiang Zhu; HV Jagadish,Numerous applications such as wireless communication and telematics need to keep trackof evolution of spatio-temporal data for a limited past. Limited retention may even berequired by regulations. In general; each data entry can have its own user specified lifetime.It is desired that expired entries are automatically removed by the system through somegarbage collection mechanism. This kind of limited retention can be achieved by using asliding window semantics similar to that from stream data processing. However; due to thelarge volume and relatively long lifetime of data in the aforementioned applications (incontrast to the real-time transient streaming data); the sliding window here needs to bemaintained for data on disk rather than in memory. It is a new challenge to provide fastaccess to the information from the recent past and; at the same time; facilitate efficient …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,10
The conference reviewing crisis and a proposed solution,Hosagrahar Visvesvaraya Jagadish,Abstract In Computer Science; we have developed a vibrant conference culture; which hasserved us well thus far. However; with the growth of our field; the number of submissions tomany conferences has sky-rocketed; leading to a downward spiral in reviewing quality andauthor satisfaction. This article proposes to break this downward spiral for the databasecommunity through JDMR; a journal for short" conference style" papers with rapid turn-around. An initial step toward this vision has been taken by VLDB.,ACM SIGMOD Record,2008,10
Term disambiguation in natural language query for XML,Yunyao Li; Huahai Yang; HV Jagadish,Abstract Converting a natural language query sentence into a formal database query is amajor challenge. We have constructed NaLIX; a natural language interface for querying XMLdata. Through our experience with NaLIX; we find that failures in natural language queryunderstanding can often be dealt with as ambiguities in term meanings. These failures aretypically the result of either the user's poor knowledge of the database schema or thesystem's lack of linguistic coverage. With automatic term expansion techniques andappropriate interactive feedback; we are able to resolve these ambiguities. In this paper; wedescribe our approach and present results demonstrating its effectiveness.,International Conference on Flexible Query Answering Systems,2006,10
TIMBER: A native XML database,Stelios Paparizos; Shurug Al-Khalifa; Y Wu; N Wiwatwattana; HV Jagadish; Andrew Nierman; C Yu; LVS Lakshmanan; D Srivastava; A Chapman; Jignesh M Patel,This paper describes the overall design and architecture of the Timber XML databasesystem currently being implemented at the University of Michigan. The system is based upona bulk algebra for manipulating trees; and natively stores XML. New access methods havebeen developed to evaluate queries in the XML context; and new cost estimation and queryoptimization techniques have also been developed. We present performance numbers tosupport some of our design decisions. We believe that the key intellectual contribution of thissystem is a comprehensive set-at-a-time query processing ability in a native XML store; withall the standard components of relational query processing; including algebraic rewritingand a cost-based optimizer.,*,2002,10
Fine-granularity locking and client-based logging for distributed architectures,Euthimios Panagos; Alexandros Biliris; HV Jagadish; Rajeev Rastogi,Abstract We present algorithms for fine-granularity locking and clientbased logging where alltransactional facilities in a distributed clientserver architecture are provided locally. Multipleclients are allowed to concurrently modify different objects on the same page withoutsynchronizing their updates. Each client has its own log disk where all log records forupdates to locally cached data are written. Transaction rollback and client crash recoveryare handled exclusively by the clients and local logs are not merged at any time. Clients cantake checkpoints independently; and client clocks do not have to be synchronized.,International Conference on Extending Database Technology,1996,10
Managing rule conflicts in an active database,HV Jagadish; Alberto O Mendelzon,*,PROCEEDINGS OF THE ACM SIGACT SIGMOD SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS,1996,10
On transitive closure problems involving path computations,R Agrawal; S Dar; HV Jagadish,*,AT&T Bell Laboratories Technical Memorandum,1988,10
Transcriptional networks of murine diabetic peripheral neuropathy and nephropathy: common and distinct gene expression patterns,Junguk Hur; Phillipe D O’Brien; Viji Nair; Lucy M Hinder; Brett A McGregor; Hosagrahar V Jagadish; Matthias Kretzler; Frank C Brosius; Eva L Feldman,Abstract Aims/hypothesis Diabetic peripheral neuropathy (DPN) and diabetic nephropathy(DN) are two common microvascular complications of type 1 and type 2 diabetes mellitusthat are associated with a high degree of morbidity. In this study; using a variety of systemsbiology approaches; our aim was to identify common and distinct mechanisms underlyingthe pathogenesis of these two complications. Methods Our previously publishedtranscriptomic datasets of peripheral nerve and kidney tissue; derived from murine models oftype 1 diabetes (streptozotocin-injected mice) and type 2 diabetes (BKS-db/db mice) andtheir respective controls; were collected and processed using a unified analysis pipeline sothat comparisons could be made. In addition to looking at genes and pathways dysregulatedin individual datasets; pairwise comparisons across diabetes type and tissue type were …,Diabetologia,2016,9
Usability; Databases; and HCI.,Fei Li; HV Jagadish,Abstract As usability is becoming recognized as a crucial feature for databases; there isgrowing reason for us to pay attention to the principles of Human Computer Interaction(HCI). This article explores the interaction of HCI with databases. We can learn from specificHCI areas; such as information visualization; from general HCI principles; such as directmanipulation; and from HCI methodology; such as running good user studies. However;there is much required for database usability that goes beyond HCI.,IEEE Data Eng. Bull.,2012,9
A native system for quering XML,Stelios Paparizos; Shurug Al-Khalifa; Adriane Chapman; HV Jagadish; Laks VS Lakshmanan; Andrew Nierman; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu Timber,*,Proc. SIGMOD Conf,2003,9
Indexing hidden markov models for music retrieval,Hui Jin; HV Jagadish,ABSTRACT Hidden Markov Models (HMMs) have been suggested as an effective techniqueto represent music. Given a collection of musical pieces; each represented by its HMM; anda query; the retrieval task reduces to finding HMM most likely to have generated the query.The musical piece represented by this HMM is frequently the one rendered by the user;possibly imperfectly. This method might be inefficient if there is a very large music database;since each HMM to be tested requires the evaluation of a dynamic programming algorithm.In this paper; we propose an indexing mechanism that can aggressively prune the set ofcandidate HMMs to be evaluated in response to a query. Our experiments on a musicdatabase showed an average of a seven-fold speed up with no false dismissals.,Ann Arbor,2002,9
Revisiting the hierarchical data model,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,Much of the data we deal with every day is organized hierarchically: file systems; libraryclassification schemes and yellow page categories are salient examples. Business data too;benefits from a hierarchical organization; and indeed the hierarchical data model was quiteprevalent thirty years ago. Due to the recently increased importance of X. 500/LDAPdirectories; which are hierarchical; and the prevalence of aggregation hierarchies indatacubes; there is now renewed interest in the hierarchical organization of data. In thispaper; we develop a framework for a modern hierarchical data model; substantiallyimproved from the original version by taking advantage of the lessons learned in therelational database context. We argue that this new hierarchical data model has manybenefits with respect to the ubiquitous flat relational data model.,IEICE TRANSACTIONS on Information and Systems,1999,9
Event specification in an object-oriented database,NH Gehani; HV Jagadish; O Shmueli,*,Proc. ACM SIGMOD Int’l Conf. on Management of Data,1993,9
Techniques for the design of parallel and pipelined VLSI systems for numerical computation with special reference to Signal Processing Applications,HV Jagadish,*,*,1985,9
Foofah: Transforming data by example,Zhongjun Jin; Michael R Anderson; Michael Cafarella; HV Jagadish,Abstract Data transformation is a critical first step in modern data analysis: before anyanalysis can be done; data from a variety of sources must be wrangled into a uniform formatthat is amenable to the intended analysis and analytical software package. This datatransformation task is tedious; time-consuming; and often requires programming skillsbeyond the expertise of data analysts. In this paper; we develop a technique to synthesizedata transformation programs by example; reducing this burden by allowing the analyst todescribe the transformation with a small input-output example pair; without being concernedwith the transformation steps required to get there. We implemented our technique in asystem; FOOFAH; that efficiently searches the space of possible data transformationoperations to generate a program that will perform the desired transformation. We …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,8
Algebraic visual analysis: The catalano phone call data set case study,Anna A Shaverdian; Hao Zhou; George Michailidis; HV Jagadish,Abstract While many clever techniques have been proposed for visual analysis; most ofthese are" one of" and it is not easy to see how to combine multiple techniques. We proposean algebraic model capable of representing a large class of visual analysis operations ongraph data. We demonstrate the value of this model by showing how it can simulate theanalyses performed by several groups on the Catalano family cell phone call record data setas part of the VAST 2008 challenge.,Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery: Integrating Automated Analysis with Interactive Exploration,2009,8
Asynchronous version advancement in a distributed three version database,HV Jagadish; I Singh Mumick; Michael Rabinovich,We present an efficient protocol for multi-version concurrency control in distributeddatabases. The protocol creates no more than three versions of any data item; whileguaranteeing that: update transactions never interfere with read-only transactions; theversion advancement mechanism is completely asynchronous with (both update and read-only) user transactions; and read-only transactions do not acquire locks and do not writecontrol information into the data items being read. This is an improvement over existing multi-versioning schemes for distributed databases; which either require a potentially unlimitednumber of versions; or require coordination between version advancement and usertransactions. Our protocol can be applied in a centralized system also; where theimprovement over existing techniques is in reducing the number of versions from four to …,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,8
Database meets deep learning: challenges and opportunities,Wei Wang; Meihui Zhang; Gang Chen; HV Jagadish; Beng Chin Ooi; Kian-Lee Tan,Abstract Deep learning has recently become very popular on account of its incrediblesuccess in many complex datadriven applications; including image classification andspeech recognition. The database community has worked on data-driven applications formany years; and therefore should be playing a lead role in supporting this new wave.However; databases and deep learning are different in terms of both techniques andapplications. In this paper; we discuss research problems at the intersection of the two fields.In particular; we discuss possible improvements for deep learning systems from a databaseperspective; and analyze database applications that may benefit from deep learningtechniques.,ACM SIGMOD Record,2016,7
Learning user preferences by adaptive pairwise comparison,Li Qian; Jinyang Gao; HV Jagadish,Abstract Users make choices among multi-attribute objects in a data set in a variety ofdomains including used car purchase; job search and hotel room booking. Individual userssometimes have strong preferences between objects; but these preferences may not beuniversally shared by all users. If we can cast these preferences as derived from aquantitative user-specific preference function; then we can predict user preferences bylearning their preference function; even though the preference function itself is not directlyobservable; and may be hard to express. In this paper we study the problem of preferencelearning with pairwise comparisons on a set of entities with multiple attributes. We formalizethe problem into two subproblems; namely preference estimation and comparison selection.We propose an innovative approach to estimate the preference; and introduce a binary …,Proceedings of the VLDB Endowment,2015,7
Appearance frequency modulated gene set enrichment testing,Jun Ma; Maureen A Sartor; HV Jagadish,Gene set enrichment testing has helped bridge the gap from an individual gene to a systemsbiology interpretation of microarray data. Although gene sets are defined a priori based onbiological knowledge; current methods for gene set enrichment testing treat all genes equal.It is well-known that some genes; such as those responsible for housekeeping functions;appear in many pathways; whereas other genes are more specialized and play a uniquerole in a single pathway. Drawing inspiration from the field of information retrieval; we havedeveloped and present here an approach to incorporate gene appearance frequency (inKEGG pathways) into two current methods; Gene Set Enrichment Analysis (GSEA) andlogistic regression-based LRpath framework; to generate more reproducible and biologicallymeaningful results. Two breast cancer microarray datasets were analyzed to identify gene …,BMC bioinformatics,2011,7
BPI: XML query evaluation using bitmapped path indices,Neamat El Tazi; HV Jagadish,Abstract We introduce a new technique for fast computation of structural join" pattern trees"in XML. Using a small amount of pre-computed path information; typically small enough to fiteasily in main memory; we are able to render structural join computation almostindependent of data set size. Our technique is amenable to bit-mapped processing; leadingto further speed-up. In this paper; we present our technique and experimentally evaluate itsperformance.,Proceedings of the 2009 EDBT/ICDT Workshops,2009,7
Provenance and the Price of Identity,Adriane Chapman; HV Jagadish,Abstract As developers acknowledge that provenance is essential; more and more datasetsare attempting to keep provenance records describing how they were created. Some ofthese datasets are constructed using workflows; others cobble together processes andapplications to manipulate the data. While the provenance needs are the same; the inputsand set of processes used must be kept; the identity needs are very different. We outlineseveral identification strategies that can be used for data manipulation outside of workflows.We evaluate these strategies in terms of time to create and store identity; and the spaceneeded to keep this information. Additionally; we discuss the strengths and weaknesses ofeach strategy.,International Provenance and Annotation Workshop,2008,7
Enabling domain-awareness for a generic natural language interface,Yunyao Li; Ishan Chaudhuri; Huahai Yang; Satinder Singh; HV Jagadish,Abstract In this paper; we present a learning-based approach for enabling domain-awareness for a generic natural language interface. Our approach automatically acquiresdomain knowledge from user interactions and incorporates the knowledge learned toimprove the generic system. We have embedded our approach in a generic naturallanguage interface and evaluated the extended system against two benchmark datasets. Wefound that the performance of the original generic system can be substantially improvedthrough automatic domain knowledge extraction and incorporation. We also show that thegeneric system with domain-awareness enabled by our approach can achieve performancesimilar to that of previous learning-based domain-specific systems.,AAAI,2007,7
Making designer schemas with colors,Nuwee Wiwatwattana; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava,XML schema design has two opposing goals: elimination of update anomalies requires thatthe schema be as normalized as possible; yet higher query performance and simpler queryexpression are often obtained through the use of schemas that permit redundancy. In thispaper; we show that the recently proposed MCT data model; which extends XML by addingcolors; can be used to address this dichotomy effectively. Specifically; we formalize theintuition of anomaly avoidance in MCT using notions of node normal and edge normalforms; and the goal of efficient query processing using notions of association recoverabilityand direct recoverability. We develop algorithms for transforming design specifications givenas ER diagrams into MCT schemas that are in a node or edge normal form and satisfyassociation or direct recoverability. Experimental results using a wide variety of ER …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,7
The importance of algebra for XML query processing,Stelios Paparizos; HV Jagadish,Abstract Relational algebra has been a crucial foundation for relational database systems;and has played a large role in enabling their success. A corresponding XML algebra forXML query processing has been more elusive; due to the comparative complexity of XML;and its history. We argue that having a sound algebraic basis remains importantnonetheless. In this paper; we show how the complexity of XML can be modeled effectivelyin a simple algebra; and how the conceptual clarity attained thereby can lead to significantbenefits.,International Conference on Extending Database Technology,2006,7
Distributed caching scheme for database systems,*,A caching scheme for spatially distributed databases in which a central database maintainsall information to be stored. A plurality of local databases include copies of the informationstored in the central database. For each data record in the central database; there is at mostone copy of the data record in the set of local databases. The caching scheme includes amethod for serving database requests in which a service request is made to a first localdatabase located in the area from which the request originates. If the first local database failsto service the request; the central database services the request. After serving the request; asecond local database that contains the data record is identified and the data record thereinis destroyed. A new copy is written to the first local database.,*,2000,7
Unified fine-granularity buffering of index and data: approach and implementation,Qiang Cao; Josep Torrellas; HV Jagadish,Disk I/O is recognized as a major performance bottleneck in many database applications.Consequently; a topic of considerable study in database systems has traditionally beenbuffer management. Recently; disk pages have been increasing in size; enabling more andmore data to fit in a single page. Such a trend suggests that buffering the data at a grain sizefiner than a page may use memory better. As a result; there has been some interest in fine-granularity buffering. Past approaches to fine-granularity buffering have proposed bufferingeither data tuples alone or index entries alone. In this paper; we propose a scheme tosupport fine-granularity buffering of both index and data entries in a unified manner. Thescheme; which we call Hot-Entry buffering; can be used in combination with conventionalpage-level buffering. Through the experimental evaluation of a simple system; we …,Computer Design; 2000. Proceedings. 2000 International Conference on,2000,7
Transitive Closure Algorithms Revisited: The Case of Path Computations,R Agrawal; S Dar; HV Jagadish,*,Submitted for publication,1988,7
Parallel Computation on Loosely-Coupled Workstations,R Agrawal; HV Jagadish,*,Computer Technology Research Laboratory Technical Report; AT&T Bell Laboratories,1986,7
0. Shmueli. Event Specification in an Object-Oriented Database,NH Gehani; HV Jagadish,*,Proceedings; International Conference on Management of Data,*,7
Understanding natural language queries over relational databases,Fei Li; HV Jagadish,Abstract Natural language has been the holy grail of query interface designers; but hasgenerally been considered too hard to work with; except in limited specific circumstances. Inthis paper; we describe the architecture of an interactive natural language query interface forrelational databases. Through a carefully limited interaction with the user; we are able tocorrectly interpret complex natural language queries; in a generic manner across a range ofdomains. By these means; a logically complex English language sentence is correctlytranslated into a SQL query; which may include aggregation; nesting; and various types ofjoins; among other things; and can be evaluated against an RDBMS. We have constructeda\system; NaLIR (Natural Language Interface for Relational databases); embodying theseideas. Our experimental assessment; through user studies; demonstrates that NaLIR is …,ACM SIGMOD Record,2016,6
META: an efficient matching-based method for error-tolerant autocompletion,Dong Deng; Guoliang Li; He Wen; HV Jagadish; Jianhua Feng,Abstract Autocompletion has been widely adopted in many computing systems because itcan instantly provide users with results as users type in queries. Since the typing task istedious and prone to error; especially on mobile devices; a recent trend is to tolerate errorsin autocompletion. Existing error-tolerant autocompletion methods build a trie to index thedata; utilize the trie index to compute the trie nodes that are similar to the query; called activenodes; and identify the leaf descendants of active nodes as the results. However thesemethods have two limitations. First; they involve many redundant computations to identify theactive nodes. Second; they do not support top-k queries. To address these problems; wepropose a matching-based framework; which computes the answers based on matchingcharacters between queries and data. We design a compact tree index to maintain active …,Proceedings of the VLDB Endowment,2016,6
Selective hashing: Closing the gap between radius search and k-nn search,Jinyang Gao; HV Jagadish; Beng Chin Ooi; Sheng Wang,Abstract Locality Sensitive Hashing (LSH) and its variants; are generally believed to be themost effective radius search methods in high-dimensional spaces. However; manyapplications involve finding the k nearest neighbors (k-NN); where the k-NN distances ofdifferent query points may differ greatly and the performance of LSH suffers. We propose anovel indexing scheme called Selective Hashing; where a disjoint set of indices are builtwith different granularities and each point is only stored in the most effective index.Theoretically; we show that k-NN search using selective hashing can achieve the samerecall as a fixed radius LSH search; using a radius equal to the distance of the c 1 k thnearest neighbor; with at most c 2 times overhead; where c 1 and c 2 are small constants.Selective hashing is also easy to build and update; and outperforms all the state-of-the-art …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,6
From labor to trader: Opinion elicitation via online crowds as a market,Caleb Chen Cao; Lei Chen; Hosagrahar Visvesvaraya Jagadish,Abstract We often care about people's degrees of belief about certain events: eg causalitybetween an action and the outcomes; odds distribution among the outcome of a horse raceand so on. It is well recognized that the best form to elicit opinion from human is probabilitydistribution instead of simple voting; because the form of distribution retains the delicateinformation that an opinion expresses. In the past; opinion elicitation has relied on experts;who are expensive and not always available. More recently; crowdsourcing has gainedprominence as an inexpensive way to get a great deal of human input. However; traditionalcrowdsourcing has primarily focused on issuing very simple (eg binary decision) tasks to thecrowd. In this paper; we study how to use crowds for Opinion Elicitation. There are threemajor challenges to eliciting opinion information in the form of probability distributions …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,6
Method of pattern searching,*,The present application is a continuation of US patent application Ser. No. 10/748;832 filedDec. 30; 2003; now US Pat. No. 7;451;144 now pending; which claims the benefit of US ProvisionalPatent Application No. 60/450;222; filed on Feb. 25; 2003; where each of the above cited applicationsis incorporated herein by reference … The government may have certain rights in the inventionpursuant to a National Science Foundation grant under Grant Numbers IIS-9986030 andIIS-0208852 … The present invention relates generally to processing queries in a computersystem and; more particularly; to processing computer queries using pattern matching … Asis known in the art; the eXtensible Markup Language (XML) employs a tree-structured modelfor representing data. Queries in XML query languages typically specify patterns of selectionpredicates on multiple elements that have some specified tree structured relationships …,*,2012,6
Refining Information Extraction Rules using Data Provenance.,Bin Liu; Laura Chiticariu; Vivian Chu; HV Jagadish; Frederick Reiss,Abstract Developing high-quality information extraction (IE) rules; or extractors; is an iterativeand primarily manual process; extremely time consuming; and error prone. In each iteration;the outputs of the extractor are examined; and the erroneous ones are used to drive therefinement of the extractor in the next iteration. Data provenance explains the origins of anoutput data; and how it has been transformed through a query. As such; one can expect dataprovenance to be valuable in understanding and debugging complex IE rules. In this paperwe discuss how data provenance can be used beyond understanding and debugging; toautomatically refine IE rules. In particular; we overview the main ideas behind a recentprovenance-based solution for suggesting a ranked list of refinements to an extractor aimedat increasing its precision; and outline several related directions for future research.,IEEE Data Eng. Bull.,2010,6
Datalens: making a good first impression,Bin Liu; HV Jagadish,Abstract When a database query has a large number of results; the user can only be shownone page of results at a time. One popular approach is to rank results such that the" best"results appear first. This approach is well-suited for information retrieval; and for somedatabase queries; such as similarity queries or under-specified (or keyword) queries withknown (or guessable) user preferences. However; standard database query resultscomprise a set of tuples; with no associated ranking. It is typical to allow users the ability tosort results on selected attributes; but no actual ranking is defined. An alternative approachis not to try to show the estimated best results on the first page; but instead to help userslearn what is available in the whole result set and direct them to finding what they need. Wepresent DataLens; a framework that: i) generates the most representative data points to …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,6
Structural joins: Efficient matching of XML query patterns,S Al-Khalifa; HV Jagadish; N Koudas; JM Patel; D Srivastava; Y Wu,*,Proceedings of ICDE,2002,6
Temporal queries for active database support,NH Gehani; HV Jagadish; Inderpal Singh Mumick; Oded Shmueli,Abstract An active database monitors events (such as access; insert; delete; and update oftuples/objects; and invocation of methods on objects). Each event potentially changes thedatabase state. Applications may wish to react to sequences of events and database statessatisfying certain properties. Speci cation of such sequences can be viewed as a formulationof temporal queries. Further; monitoring such sequences; or equivalently; the evaluation ofsuch temporal queries; must be e cient; and must not require storage of the entire databasehistory.,Proceedings of the International Workshop on an Infrastructure for Temporal Databases; Arlington; TX,1993,6
On hardware description from block diagrams,H Jagadish; Thomas Kailath; J Newkirk; R Mathews,In this paper; we present a systematic way to describe the logical architecture of the systemimplementing a given algorithm by means of an extended block diagram; and develop astandard notation to describe each block. We then show a technique for correctlytransforming a block diagram into a physical circuit description by introducing the notion oftime into the block diagram; and describe an algorithm that optimally assigns a limitednumber of hardware modules to all the function blocks in the diagram.,Acoustics; Speech; and Signal Processing; IEEE International Conference on ICASSP'84.,1984,6
On pipelining systolic arrays,HV Jagadish; T Kailath; JA Newkirk; RG Mathews,*,Conf. Record 17th Asilomar Conf. on Circuits and Systems (Pacific Grove; Calif,1983,6
Exploratory keyword search with interactive input,Zhifeng Bao; Yong Zeng; HV Jagadish; Tok Wang Ling,Abstract Due to the intrinsic ambiguity of keyword queries; users usually need to reformulatetheir queries multiple times to get the desired information. Even worse; users either have noway to precisely specify their search intention; or have limited domain knowledge on thedata to precisely express their search intention. Moreover; they may just have a generalinterest to explore the data by keyword query. Therefore; our goal is to design an exploratorysearch paradigm that is able to bring humans more actively into the search process; in orderto meet various user information needs; ranging from simple lookup to learning andunderstanding of the data. Besides; keyword queries against data with structure; such asXML; can run into multiple difficulties: how to identify the search target; more types ofambiguity arise as a keyword can be part of the structure as well as content of data; etc …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,5
Revision provenance in text documents of asynchronous collaboration,Jing Zhang; HV Jagadish,Many text documents today are collaboratively edited; often with multiple small changes.The problem we consider in this paper is how to find provenance for a specific part ofinterest in the document. A full revision history; represented as a version tree; can tell usabout all updates made to the document; but most of these updates may apply to other partsof the document; and hence not be relevant to answer the provenance question at hand. Inthis paper; we propose the notion of a revision unit as a flexible unit to capture the necessaryprovenance. We demonstrate through experiments the capability of the revision units inkeeping only relevant updates in the provenance representation and the flexibility of therevision units in adjusting to updates reflected in the version tree.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,5
Big Data: It’s not just the analytics,HV Jagadish,*,ACM SIGMOD Blog; May,2012,5
Time for our field to grow up,Anastassia Ailamaki; Laura Haas; HV Jagadish; David Maier; Tamer Özsu; Marianne Winslett,Abstract Compared to centuries of physics and millennia of mathematics; the 50-year-historyof computer science and information management research makes us the toddlers of thescientific community. Yet during our brief existence; we've revolutionized the world and; notcontent with that; gone on to build and study virtual worlds. We have justly taken pride in ouraccomplishments; and developed our own unique way of conducting research; unlike otherscientific and engineering fields. But cracks have appeared in this edifice we have built. Theconference system that served us so well for our first 50 years is falling apart. Our ever-increasing population competes ever more energetically for a finite set of resources. Otherscientific and engineering disciplines still think that our field equates to programming; andlook down on us. While we may also look down on them; it is undeniably true that high …,Proceedings of the VLDB Endowment,2010,5
Querying graphs with uncertain predicates,Hao Zhou; Anna A Shaverdian; HV Jagadish; George Michailidis,Abstract In many applications the available data give rise to an attributed graph; with thenodes corresponding to the entities of interest; edges to their relationships and attributes onboth provide additional characteristics. To mine such data structures we have proposed avisual analytic algebra that enhances the atomic operators of selection; aggregation and avisualization step that allows the user to interact with the data. However; in many settings theuser has a certain degree of uncertainty about the desired query; the problem is furthercompounded if the final results are the product of a series of such uncertain queries. Toaddress this issue; we introduce a probabilistic framework that incorporates uncertainty inthe queries and provides a probabilistic assessment of the likelihood of the obtainedoutcomes. We discuss its technical characteristics and illustrate it on a number of …,Proceedings of the Eighth Workshop on Mining and Learning with Graphs,2010,5
Multiple step social structure analysis with Cytoscape,Hao Zhou; Anna A Shaverdian; HV Jagadish; George Michailidis,Cytoscape is a popular open source tool for biologists to visualize interaction networks. Wefind that it offers most of the desired functionality for visual analytics on graph data to guideus in the identification of the underlying social structure. We demonstrate its utility in theidentification of the social structure in the VAST 2009 Flitter Mini Challenge.,Visual Analytics Science and Technology; 2009. VAST 2009. IEEE Symposium on,2009,5
Data Management for the Biosciences. Report of the NSF,HV Jagadish; Frank Olken,*,NLM Workshop of Data Management for Molecular and Cell Biology,2003,5
Method and system for continuing billing arrangements,*,A method of processing transaction charges from continuing billing arrangements thatallows such charges to continue to be authorized after the account number to which thecharges are directed has been invalidated; such as would result if the account numberrelated to a credit or debit card that was lost; stolen or expired; or to a checking or savingsaccount that was closed or transferred. A security key is used to indicate charges that are tobe authorized after the account number has been invalidated. Charges that include the keyare authorized; while charges that do not include the key are rejected.,*,2000,5
The INCINERATE data model,HV Jagadish,Abstract In this article; we present an extended relational algebra with universally orexistentially quantified classes as attribute values. The proposed extension can greatlyenhance the expressive power of relational systems; and significantly reduce the size of adatabase; at small additional computational cost. We also show how the proposedextensions can be built on top of a standard relational database system.,ACM Transactions on Database Systems (TODS),1995,5
The design of a back-end object management system,Abhaya Asthana; HV Jagadish; Paul Krzyzanowski,Abstract We describe the architecture and design of a back-end object manager; designedas an “active memory” system on a plugin board for a standard workstation (or personalcomputer). We show how; with minimal modification to existing code; it is possible to achievesignificant performance improvement for the execution of data-intensive methods on objects;simply by using our back-end object manager.,*,1992,5
Cohort query processing,Dawei Jiang; Qingchao Cai; Gang Chen; HV Jagadish; Beng Chin Ooi; Kian-Lee Tan; Anthony KH Tung,Abstract Modern Internet applications often produce a large volume of user activity records.Data analysts are interested in cohort analysis; or finding unusual user behavioral trends; inthese large tables of activity records. In a traditional database system; cohort analysisqueries are both painful to specify and expensive to evaluate. We propose to extenddatabase systems to support cohort analysis. We do so by extending SQL with three newoperators. We devise three different evaluation schemes for cohort query processing. Two ofthem adopt a non-intrusive approach. The third approach employs a columnar basedevaluation scheme with optimizations specifically designed for cohort query processing. Ourexperimental results confirm the performance benefits of our proposed columnar databasesystem; compared against the two non-intrusive approaches that implement cohort …,Proceedings of the VLDB Endowment,2016,4
A structured query model for the deep relational web,Hasan M Jamil; Hosagrahar V Jagadish,Abstract The deep web is very large and diverse and queries evaluated against the deepweb can provide great value. While there have been attempts at accessing the data in thedeep web; these are clever" one-of''systems and techniques. In this paper; we describe anongoing research of a generic structured query model that can be used against the deepweb. Using this query model; the contributions of a community of researchers can becombined freely; leading to a system that can be improved incrementally each timesomeone develops a specific novel technique to improve a particular operator.,Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,2015,4
A general framework to resolve the MisMatch problem in XML keyword search,Zhifeng Bao; Yong Zeng; Tok Wang Ling; Dongxiang Zhang; Guoliang Li; HV Jagadish,Abstract When users issue a query to a database; they have expectations about the results.If what they search for is unavailable in the database; the system will return an empty resultor; worse; erroneous mismatch results. We call this problem the MisMatch problem. In thispaper; we solve the MisMatch problem in the context of XML keyword search. Our solution isbased on two novel concepts that we introduce: target node type and Distinguishability.Target Node Type represents the type of node a query result intends to match; andDistinguishability is used to measure the importance of the query keywords. Using theseconcepts; we develop a low-cost post-processing algorithm on the results of queryevaluation to detect the MisMatch problem and generate helpful suggestions to users. Ourapproach has three noteworthy features:(1) for queries with the MisMatch problem; it …,The VLDB Journal,2015,4
Yannis Papakon stantinou; Jignesh Patel; Raghu Ramakrishnan; Kenneth Ross; Shahabi Cyrus; Dan Suciu; Shiv Vaithyanathan; Jennifer Widom; Challenges and O...,Divyakant Agrawal; Philip Bernstein; Elisa Bertino; Susan Davidson; Umeshwas Dayal; Michael Franklin; Johannes Gehrke; Laura Haas; Jiawei Han Alon Halevy; HV Jagadish; Alexandros Labrinidis; Sam Madden,*,*,2011,4
A query processing architecture for an XML data warehouse,Nuwee Wiwatwattana; HV Jagadish,Data warehousing accounts for a significant fraction of database use today. As XMLbecomes ever more popular; more and more XML data finds its way into data warehouserepositories. This paper examines the modeling mismatch between the tree structure of XMLdata model and the multidimensional model of a typical data warehouse; and proposes anXML warehouse model based on the Multi-Colored Trees (MCT) logical data model thatresolves the modeling issue naturally. Furthermore; this data model ameliorates some well-known modeling limitations of the XML data warehouse. To cope with ad-hoc OLAP queries;we extend bitmap join indices to the XML context. We then tackle the difference between thebit-map and the stack-based structural join processing paradigm popular in XML queryprocessing; permitting both styles of query processing to be used seamlessly in consort to …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,4
Data management for the biosciences: report of the NLM Workshop on Data Management for Molecular and Cell Biology,HV Jagadish; Frank Olken,*,*,2003,4
Effective integration of protein data through better data modeling,Adriane Chapman; Cong Yu; HV Jagadish,Protein data; from sequence and structure to interaction; is being generated through manydiverse methodologies; it is stored and reported in numerous forms and multiple places. Themagnitude of the data limits researchers abilities to utilize all information generated.Effective integration of protein data can be accomplished through better data modeling. Wedemonstrate this through the MIPD project.,OMICS A Journal of Integrative Biology,2003,4
Incompleteness in Data Mining.,Hosagrahar Visvesvaraya Jagadish; Raymond T Ng,Abstract Database technology; as well as the bulk of data mining technology; is foundedupon logic; with absolute notions of truth and falsehood; at least with respect to the data set.Patterns are discovered exhaustively; with carefully engineered algorithms devised todetermine all patterns in a data set that belong to a certain class. For large data sets; manysuch data mining techniques are extremely expensive; leading to considerable researchtowards solving these problems more cheaply. In this paper; we argue that the central goalof data mining is to find sOME interesting patterns; and not necessarily ALL of them. As such;techniques that can find most of the answers cheaply are clearly more valuable thancomputationally much more expensive techniques that can guarantee completeness. In fact;it is probably the case that patterns that can be found cheaply are indeed the most …,Lecture notes in computer science,2001,4
Composite Event Specification in Active Database Systems,NH Gehani; HV Jagadish; O Shmuheli,*,Proc. of the 18th Int. Conference on VLDB,1992,4
Architectural support for real-time database systems,N Soparkar; A Asthana; HV Jagadish; P Krzyzanowski,*,Proceedings of the Workshop on Architectural Support for Real-Time Systems,1991,4
Hardware support for debugging in a small-grain parallel system,A Asthana; HV Jagadish,*,Proc. SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging; Madison; WI,1988,4
An efficient method for encoding path information in the transitive closure of a database relation,R Agrawal; HV Jagadish,*,AT&T Bell Laboratories Technical Memorandum,1987,4
Techniques for the design of parallel and pipelined VLSI systems for numerical computation with special reference to signal processing applications,HV Jagadish,*,*,1986,4
Challenges and opportunities with big data; 2012,HV Jagadish,*,Google Scholar,*,4
O. Shmueli-Composite event model specification in active databases: model and implementation,NH Gehani; HV Jagadish,*,Proceedings VLDB,*,4
Diversity in big data: A review,Marina Drosou; HV Jagadish; Evaggelia Pitoura; Julia Stoyanovich,Abstract Big data technology offers unprecedented opportunities to society as a whole andalso to its individual members. At the same time; this technology poses significant risks tothose it overlooks. In this article; we give an overview of recent technical work on diversity;particularly in selection tasks; discuss connections between diversity and fairness; andidentify promising directions for future work that will position diversity as an importantcomponent of a data-responsible society. We argue that diversity should come to theforefront of our discourse; for reasons that are both ethical—to mitigate the risks of exclusion—and utilitarian; to enable more powerful; accurate; and engaging data analysis and use.,*,2017,3
Tuning crowdsourced human computation,Chen Cao; Jiayang Tu; Zheng Liu; Lei Chen; HV Jagadish,As crowdsourcing has been dramatically investigated and utilized to address problems inthe real world; it is essential and important to think about performance optimization.Analogous to computer systems with CPUs; treating each worker as a HPU (HumanProcessing Unit [1]) and studying the performance optimization on top of HPUs areinteresting perspectives to resolve crowdsourcing issues. However; as we characterizeHPUs in detail for this purpose; we find that there are significant differences between CPUsand HPUs; leading to the need of completely new optimization algorithms. In this paper; westudy the specific optimization problem of obtaining results the fastest for a crowdsourcedjob with a fixed total budget. In crowdsourcing; jobs are usually broken down into sets ofsmall tasks; which are assigned to workers one at a time. We consider three scenarios of …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,3
Generic inverted index on the GPU,Jingbo Zhou; Qi Guo; HV Jagadish; Wenhao Luan; Anthony KH Tung; Yueji Yang; Yuxin Zheng,Abstract: Data variety; as one of the three Vs of the Big Data; is manifested by a growingnumber of complex data types such as documents; sequences; trees; graphs and highdimensional vectors. To perform similarity search on these data; existing works mainlychoose to create customized indexes for different data types. Due to the diversity ofcustomized indexes; it is hard to devise a general parallelization strategy to speed up thesearch. In this paper; we propose a generic inverted index on the GPU (called GENIE);which can support similarity search of multiple queries on various data types. GENIE caneffectively support the approximate nearest neighbor search in different similarity measuresthrough exerting Locality Sensitive Hashing schemes; as well as similarity search on originaldata such as short document data and relational data. Extensive experiments on different …,arXiv preprint arXiv:1603.08390,2016,3
PI: a Parallel in-memory skip list based Index,Zhongle Xie; Qingchao Cai; HV Jagadish; Beng Chin Ooi; Weng-Fai Wong,Abstract: Due to the coarse granularity of data accesses and the heavy use of latches;indices in the B-tree family are not efficient for in-memory databases; especially in thecontext of today's multi-core architecture. In this paper; we present PI; a Parallel in-memoryskip list based Index that lends itself naturally to the parallel and concurrent environment;particularly with non-uniform memory access. In PI; incoming queries are collected; anddisjointly distributed among multiple threads for processing to avoid the use of latches. Foreach query; PI traverses the index in a Breadth-First-Search (BFS) manner to find the listnode with the matching key; exploiting SIMD processing to speed up the search process. Inorder for query processing to be latch-free; PI employs a light-weight communicationprotocol that enables threads to re-distribute the query workload among themselves such …,arXiv preprint arXiv:1601.00159,2016,3
Nested propositions in open information extraction,Nikita Bhutani; HV Jagadish; Dragomir Radev,Abstract The challenges of Machine Reading and Knowledge Extraction at a web scalerequire a system capable of extracting diverse information from large; heterogeneouscorpora. The Open Information Extraction (OIE) paradigm aims at extracting assertions fromlarge corpora without requiring a vocabulary or relation-specific training data. Most systemsbuilt on this paradigm extract binary relations from arbitrary sentences; ignoring the contextunder which the assertions are correct and complete. They lack the expressiveness neededto properly represent and extract complex assertions commonly found in the text. To addressthe lack of representation power; we propose NESTIE; which uses a nested representationto extract higher-order relations; and complex; interdependent assertions. Nesting theextracted propositions allows NESTIE to more accurately reflect the meaning of the …,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,2016,3
DBExplorer: Exploratory Search in Databases.,Manish Singh; Michael J Cafarella; HV Jagadish,ABSTRACT A traditional relational database can evaluate complex queries but requiresusers to precisely express their information need. But users often do not know whatinformation is available in a database; and hence cannot correctly express their informationneed. Traditional databases do not provide convenient means for users to gain familiaritywith the data. In this paper; we study the problem of exploratory search; which a user maywish to perform to get an understanding of the data set. We note that users often have somedecisions already made; so what they need is not an overall database summary; but rather asummary “in context” of the relevant portion of the database. Towards this end; we devise anovel data summarization technique called the Conditional Attribute Dependency (CAD)View; which shows the conditional dependencies between attribute values conditioned …,EDBT,2016,3
Active Sampler: Light-weight Accelerator for Complex Data Analytics at Scale,Jinyang Gao; HV Jagadish; Beng Chin Ooi,Abstract: Recent years have witnessed amazing outcomes from" Big Models" trained by" BigData". Most popular algorithms for model training are iterative. Due to the surging volumes ofdata; we can usually afford to process only a fraction of the training data in each iteration.Typically; the data are either uniformly sampled or sequentially accessed. In this paper; westudy how the data access pattern can affect model training. We propose an Active Sampleralgorithm; where training data with more" learning value" to the model are sampled morefrequently. The goal is to focus training effort on valuable instances near the classificationboundaries; rather than evident cases; noisy data or outliers. We show the correctness andoptimality of Active Sampler in theory; and then develop a light-weight vectorizedimplementation. Active Sampler is orthogonal to most approaches optimizing the …,arXiv preprint arXiv:1512.03880,2015,3
Data for Systemic Risk,HV Jagadish,Whatever models may be constructed to assess systemic risk; they will require data toevaluate. This Part of this book explores the data needs for evaluating systemic risk; anddescribes the many challenges in meeting these data needs effectively. Systemic riskmodels are particularly complicated because they are aggressively non-linear—an entity iseither able to meet its contractual obligations or it is not; and which of the two scenarios weare in can affect the solvency of many other entities—and furthermore have heavyinterlinkage between model entities so that independence assumptions are almost neverpossible. In consequence; the common mathematical simpliﬁcations; of linearity andindependence; cannot be made. This leads to the need for models that are far morecomplex; potentially require Monte Carlo simulations to solve; and rely upon a large body …,Handbook on Systemic Risk,2013,3
BPI-TWIG: XML twig query evaluation,Neamat El-Tazi; HV Jagadish,Abstract We propose a new algorithm; BPI-TWIG; to evaluate XML twig queries. Thealgorithm uses a set of novel twig indices to reduce the number of comparisons needed forthe twig evaluation and transform the join operation to an intersection operation between thecontributing twig paths inside the query. In this paper; we present our technique andexperimentally evaluate its performance.,International XML Database Symposium,2009,3
Querying XML in Timber.,Yuqing Wu; Stelios Paparizos; HV Jagadish,Abstract In this paper; we describe the TIMBER XML database system implemented atUniversity of Michigan. TIMBER was one of the first native XML database systems; designedfrom the ground up to store and query semi-structured data. A distinctive principle of TIMBERis its algebraic underpinning. Central contributions of the TIMBER project include:(1) treealgebras that capture the structural nature of XML queries;(2) the stack-based family ofalgorithms to evaluate structural joins;(3) new rule-based query optimization techniques thattake care of the heterogeneous nature of the intermediate results and take the schemainformation into consideration;(4) cost-based query optimization techniques and summarystructures for result cardinality estimation; and (5) a family of structural indices for moreefficient query evaluation. In this paper; we describe not only the architecture of TIMBER …,IEEE Data Eng. Bull.,2008,3
Evaluating universal quantification in XML,Shurug Al-Khalifa; Bin Liu; HV Jagadish,Queries posed to database systems often involve universal quantification. Such queries aretypically expensive to evaluate. Although they can be handled by basic access methods; forselection; grouping; and so forth; new access methods specifically tailored to evaluateuniversal quantification can greatly decrease the computational cost. In this paper; we studythe efficient evaluation of universal quantification in an XML database. Specifically; wedevelop a small taxonomy of universal quantification types and define a family of algorithmssuitable for handling each. We experimentally demonstrate the performance benefits of thenew family of algorithms.,IEEE Transactions on Knowledge and Data Engineering,2007,3
Special issue on data management; analysis; and mining for the life sciences,Terry Gaasterland; HV Jagadish; Louiqa Raschid,During the last decade; biologists have experienced a fundamental revolution fromtraditional R&D involving the study of single genes and isolated cellular mechanisms to e-biology that addresses whole cell physiology and complex biological systems. This hasbeen accompanied by an explosion in the number and size of public data resources; and arapid growth in the variety and volume of laboratory data from microarrays measuringtranscription levels simultaneously for hundreds of genes and to protein interaction screensmeasuring multiple components of protein complexes. Life science data is complex and datasets have complex inter-relationships. The data is often incomplete; uncertain; and can varysignificantly across biological replicates. Data can evolve more quickly than thetechnologies developed to interpret the data. Thus; life sciences data is not well suited for …,The VLDB Journal,2005,3
Biological Data management: Research; Practice and Opportunities,BS Davidson; HV Jagadish; VM Markowitz; EW Steeg; M Tyers,*,Proceedings of the 30th VLDB Conference,2004,3
Data management for the biosciences,HV Jagadish; F Olken,*,Report of the NSF/NLM workshop on the data management for molecular and cell biology; Workshop Report LBNL-52767,2003,3
Combining Operators in XML Query Processing,S Al-Khalifa; HV Jagadish,*,University of Michigan technical report,2002,3
ProTDB: Probabilistic Data in XML,Andrew Nierman Jagadish; HV Jagadish,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda):Whereas traditional databases manage only deterministic information; manyapplications that use databases involve uncertain data.,In Proceedings of the 28th VLDB Conference,2002,3
Counting Twigs in a Tree,Z Chen; HV Jagadish; F Korn; N Koudas; R Ng; S Muthukrishnan; D Srivastava,*,Proc. of ICDE,2001,3
Hermod: A distributed database infrastructure for electronic messaging,A Biliris; R Gruber; G Hjalmtysson; HV Jagadish; MA Jones; MF McGroary; E Panagos; M Rabinovich; AW Robinson; S Spear; D Srivastava; D Vista,*,Submitted for publication,1998,3
The New Jersey Data Reduction Report,C Faloutsos; D Barbara; W DuMouchel; Hellerstein Haas; YE Ioannidis; HV Jagadish; T Johnson; RT Ng; V Poosala; KA Ross; KC Sevcik,*,IEEE Data Engineering Bulletin,1997,3
Information recovery from partial data,Christos Faloutsos; HV Jagadish; Nikolaos Sidiropoulos,*,*,1997,3
Mining optimized association rules for numeric data,T Fukuda; Y Morimoto; S Morishita; T Tokuyama; HV Jagadish; IS Mumick,*,Proceedings of the ACM SIGMOD International Conference on Management of Data,1996,3
Data warehousing using self-maintainable views,Ashish Gupta; HV Jagadish; Inderpal S Mumick,*,Proc. EDBT,1996,3
Indexing for retrieval by similarity; Multimedia database systems: issues and research directions,HV Jagadish,*,*,1996,3
Databases for networks,HV Jagadish,The communications industry is on the brink of a revolution; as evidenced by almost dailynewspaper articles on cable TV; wireless phones; and “information highways”. Just asCAD/CASE applications have had a profound effect on the database community in the 80s;so are network applications likely to in the 90s. This tutorial explores the crucial role thatdatabases play in wide-area communications networks. Wide area communicationsnetworks have been around for a long time. Ad hoc application-specific solutions have beenadopted for data management. With rapid changes the communications industry is currentlyundergoing; and exponential growth in traffic volume; such “hard-wired” solutions are nolonger acceptable; and generic database software is desired. However; traditionaldatabases do not provide all the features required in networks; as we discuss below …,ACM SIGMOD Record,1994,3
A Flexible Transaction Facility for an Object-Oriented Database,A Biliris; S Dar; N Gehani; H Jagadish; K Ramamritham,Abstract Object-oriented databases were motivated by the needs of complex applicationssuch as CAD and software engineering. Transactions in such applications have diverseneeds: they may be long lived and they may need to cooperate. This paper describes aexible transaction facility; for an object-oriented database; consisting of a set of transactionmodeling primitives that allow users to de ne customized transaction semantics matching theneeds of speci c applications. We show how the transaction primitives can be used tospecify di erent transaction models; including nested transactions; split transactions; sagas;and several other extended transaction models described in the literature.,Submitted for publication,1992,3
Synchronizing trigger events in a distributed objectoriented database,HV Jagadish; O Shmueli,*,Proc. Int'l Workshop on Distributed Object Management,1992,3
A proclamation-based model for cooperation transactions; proceedings of the 18th VLDB Conference,HV Jagadish; O Shmueli,*,*,1992,3
Ode as an Active Database,N Gehani; HV Jagadish,*,Proceedings of the 17th International Conference on Very Large Data Bases (VLDB’91),1991,3
SWIM network architecture,A Asthana; MR Cravatts; BC Fischer; HV Jagadish; SC Knauer; P Krzyzanowski; D Lin,*,*,1989,3
On Bounded Linear Recursion,R Agrawal; HV Jagadish,*,AT&T Bell Laboratories Technical Memorandum,1986,3
Silberschatz; 1994.\Dali: A high performance main memory storage manager,HV Jagadish; D Lieuwen; R Rastogi,*,Proceedings of the 20th International Conference on Very Large Databases,86,3
Hardware support for debugging in a small-grain parallel system: A case study,Abhaya Asthana; HV Jagadish,*,Proceedings of the ACM SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging,*,3
A transitive closure compression technique,R Agrawal; A Borgida; HV Jagadish,*,Proc. ACM SIGMOD,*,3
A graph algebra for scalable visual analytics,Anna A Shaverdian; Hao Zhou; George Michailidis; Hosagrahar V Jagadish,Visual analytics (VA); which combines analytical techniques with advanced visualizationfeatures; is fast becoming a standard tool for extracting information from graph data.Researchers have developed many tools for this purpose; suggesting a need for formalmethods to guide these tools' creation. Increased data demands on computing requiresredesigning VA tools to consider performance and reliability in the context of analysis ofexascale datasets. Furthermore; visual analysts need a way to document their analyses forreuse and results justification. A VA graph framework encapsulated in a graph algebra helpsaddress these needs. Its atomic operators include selection and aggregation. Theframework employs a visual operator and supports dynamic attributes of data to enablescalable visual exploration of data.,IEEE computer graphics and applications,2012,2
Organic databases,HV Jagadish; Arnab Nandi; Li Qian,Abstract Databases today are carefully engineered: there is an expensive and deliberatedesign process; after which a database schema is defined; during this design process;various possible instance examples and use cases are hypothesized and carefullyanalyzed; finally; the schema is ready and then can be populated with data. All of this effortis a major barrier to database adoption. In this paper; we explore the possibility of organicdatabase creation instead of the traditional engineered approach. The idea is to let the userstart storing data in a database with a schema that is just enough to cove the instances athand. We then support efficient schema evolution as new data instances arrive. Bydesigning the database to evolve; we can sidestep the expensive front-end cost of carefullyengineering the design of the database. The same set of issues also apply to database …,International Workshop on Databases in Networked Information Systems,2011,2
Provenance in Dynamic Data Systems.,Jing Zhang; HV Jagadish,Abstract Most digital data sets are subject to modifications. For example; scientific data maybe updated according to the new experimental results; and sales data updated periodicallyaccording to new sales made. We often have data derived from these digital data sets. Ourconcern in this paper is the provenance of such derived data. Can we explain what aparticular derived datum depends on; even if a value used in its derivation has since beenmodified. Can we determine if a particular derived value is still valid without performing fullview maintenance. Questions of this sort are likely to arise when we derive results frommodifiable data. We present in this paper an overview of problems that arise in this context;with regard to fine-grain data provenance; and outline solutions to some of these problems.,TaPP,2011,2
PrivatePond: Outsourced Management of Web Corpuses.,Daniel Fabbri; Arnab Nandi; Kristen LeFevre; HV Jagadish,ABSTRACT With the rise of cloud computing; it is increasingly attractive for end-users(organizations and individuals) to outsource the management of their data to a small numberof largescale service providers. In this paper; we consider a user who wants to outsourcestorage and search for a corpus of web documents (eg; an intranet). At the same time; thecorpus may contain confidential documents that the organization does not want to reveal tothe service provider. While past work has considered the problems of secure,WebDB,2009,2
XQuery processors,Torsten Grust; HV Jagadish; Fatma Özcan; Cong Yu,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,2
iDistance techniques,HV Jagadish; Beng Chin Ooi; Rui Zhang,The iDistance is an indexing and query processing technique for k nearest neighbor (kNN)queries on point data in multi-dimensional metric spaces. The kNN query is one of thehardest problems on multi-dimensional data. It has been shown analytically andexperimentally that any algorithm using hierarchical index structure based on either space-or data-partitioning is less efficient than the naive method of sequentially checking everydata record (called the sequential scan) in high-dimensional spaces [4]. Some datadistributions including the uniform distribution are particularly hard cases [1]. The iDistanceis designed to process kNN queries in high-dimensional spaces efficiently and it isespecially good for skewed data distributions; which usually occur in real-life data sets. Foruniform data; the iDistance beats the sequential scan up to 30 dimensions as reported in …,*,2008,2
Simplifying access to a Clinical Data Repository using schema summarization.,Cong Yu; DA Hanauer; Brian D Athey; HV Jagadish,Abstract The University of Michigan Clinical Data Repository (CDR) integrates over 25 datasources; and as a result has a schema that is too complex to be directly queried by clinicalresearchers. Schema summarization uses abstract elements and links to summarize acomplex schema and allows users with limited knowledge of the underlying databasestructure to effectively issue queries to the CDR for clinical and translational research.,AMIA... Annual Symposium proceedings. AMIA Symposium,2007,2
Speeding up Search in Peer-to-Peer Networks with A Multi-way Tree Structure. ACM SIGMOD Int’l,HV Jagadish; Beng Chin Ooi; Kian-Lee Tan; Quang Hieu Vu; Rong Zhang,*,Conference on Management of Data (SIGMOD),2006,2
Constructing a Generic Natural Language Interface for an XML Databases,L Yunyao; Y Huahai; HV Jagadish,*,EDBT,2006,2
Run-time parallelization of sequential database programs,NR Soparkar; Paul Krzyzanowski; HV Jagadish; Abhaya Asthana,Abstract In order to execute a database program written in sequential code efficiently on aparallel processor; we develop the use of transaction concurrency control paradigms toresolve data dependencies dynamically. The sequential code is divided into small units forexecution; and these units are executed concurrently as separate“transactions.” Ourapproach en. sures that the concurrent execution of the smaller units is logically equivalentto the original sequential program. We present an order-preserving concurrency controlstrategy to execute concurrently the nested invocations that are generated by theparallelized execution of the database program. We present performance figures from apreliminary implementation to indicate the benefits of our strategy. Finally; we provide arough analysis to gauge the overheads associated with our approach that would impact …,Proceedings of the fourth international conference on Information and knowledge management,1995,2
Multi-Granularity Locks in an Object-Oriented Database,HV Jagadish; Daniel F Lieuwen,Abstract Multi-granularity locking is widely accepted today as an important performancebooster for concurrency control in relational databases. In this paper; we address the issuesthat arise in applying the same idea to an objectoriented database. First; with encapsulatedimplementations providing interfaces to objects; it is both possible and bene cial to de necon ict relationships between pairs of operations allowed on objects of a particular class.This; in turn; translates into a need for a rich and extensible set of lock types that can beused at di erent granularities (eg object; le; database). Second; objects often participate inmultiple collections. Thus; there may be no simple hierarchy of locking granules but ratheran arbitrary graph with inclusion and intersection relationships. Moreover; updates to thedatabase could result in modi cations to this graph. Thus; this graph becomes a point of …,*,1993,2
The outstanding problem for today's database technology,N Prabhakaran; Chaitanya K Baru; Don S Batory; David K Hsiao; HV Jagadish; Carlton Pu; Sham Navathe,The paper constitutes the proceedings of a panel in which distinguished researchers fromacademia and industrial research organizations were invited to identify the crucial problemfor today's database technology. CK Baru notes that the outstanding problem for today'sdatabase technology is the exploitation of available and anticipated hardware to provideefficient support for the processing of current and proposed data models employed at theconceptual level. D. Batory points out that object-oriented DBMSs (database managementsystems) offer a tight coupling of programming languages with database systems; extensibleDBMSs offer the ability to customize and optimize the internals of a DBMS to suit specificapplication. He believes that these combined technologies offer the ideal system of thefuture. D. Hsiao examines problems in database software and database hardware. HV …,Databases; Parallel Architectures and Their Applications;. PARBASE-90; International Conference on,1990,2
An intelligent memory transaction engine,Abhaya Asthana; HV Jagadish; Scott C Knauer,Abstract In this paper; we describe the structure and utilization of a high bandwidth; multi-ported; disk-sized memory system capable of storing; maintaining; and manipulatingpersistent shared data within it; independent of any external processing units. Up tothousands of active storage elements; each element having some storage and someassociated processing logic; function independently or in groups to implement user-definedobjects and data structures. Hundreds of transactions can concurrently be processed bymutually exclusive sets of elements. A fast response time is obtained due to the proximity ofthe processing with the memory; a specialized micro-architecture; and parallelism.,International Workshop on Database Machines,1989,2
A systematic scheduling notation for digital design,HV Jagadish; T Kailath; RG Mathews; JA Newkirk,*,Proc. IEEE Workshop on Languages for Automation,1984,2
Integrity management in an object-oriented database,HV Jagadish; X Qian,*,Proceedings of the 18th VLDB Conference,*,2
Foofah: a programming-by-example system for synthesizing data transformation programs,Zhongjun Jin; Michael R Anderson; Michael Cafarella; HV Jagadish,Abstract Advancements in new data analysis and visualization technologies have resulted inwide applicability of data-driven decision making. However; raw data from various sourcesmust be wrangled into a suitable form before they are processed by the downstream datatools. People traditionally write data transformation programs to automate this process; andsuch work is cumbersome and tedious. We built a system called FOOFAH for helping theuser easily synthesize a desired data transformation program. Our system minimizes theuser's effort by only asking for a small illustrative example comprised of the raw input dataand the target transformed output; FOOFAH then synthesizes a program that can perform thedesired data transformation. This demonstration showcases how the user can applyFOOFAH to real-world data transformation tasks.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
Paper presentation at conferences: time for a reset,HV Jagadish,can do much better by reading the paper and going to a poster session. The reason to go toa research session is to benefit from the presence in one room of multiple experts interestedin some topic (or; at least; closely related topics). The question becomes how best toaccomplish this. In some fields; conferences are organized by session; and papers areinvited to particular sessions. Such conferences find it easy to have cohesive sessions. Butthis flexibility is not available to most computing conferences; which have a carefully devisedreview process for paper acceptance. In short; in putting a conference program together; weget input at the unit of papers but must produce output at the unit of sessions. This is difficult;but not impossible. I was recently program chair of the VLDB conference; and this gave mean opportunity to try some things out. So let me describe a few of the things we did; and …,Communications of the ACM,2016,1
The Values Challenge for Big Data,HV Jagadish,Abstract As Big Data and analytics defined on top of Big Data have increasingly greaterimpacts on society; we humans are becoming incorporated in a Big Data loop: our activities;transactions; posts; and images; are all being recorded as Big Data; and in turn the analysisof Big Data is being used to make decisions that affect us. This paper explorescharacteristics of this grand loop of Big Data and begins the definition of a research agendato address associated challenges.,Bulletin of the IEEE Computer Society Technical Committee on Data Engineering,2016,1
Organic databases,HV Jagadish; Li Qian; Arnab Nandi,Databases today are carefully engineered: there is an expensive and deliberate designprocess; after which a database schema is defined; during this design process; variouspossible instance examples and use cases are hypothesised and carefully analysed; finally;the schema is ready and then can be populated with data. All of this effort is a major barrierto database adoption. In this paper; we explore the possibility of organic database creationinstead of the traditional engineered approach. The idea is to let the user start storing data ina database with a schema that is just enough to cove the instances at hand. We then supportefficient schema evolution as new data instances arrive. By designing the database toevolve; we can sidestep the expensive front-end cost of carefully engineering the design ofthe database. Indeed; the deliberate design model complicates not only database …,International Journal of Computational Science and Engineering,2015,1
A flexible and extensible contract aggregation framework (caf) for financial data stream analytics,Bryan Ball; Mark Flood; Hosagrahar Visvesvaraya Jagadish; Joe Langsam; Louiqa Raschid; Peratham Wiriyathammabhum,Abstract The paper presents the Contract Aggregation Framework (CAF) for the modelingand analysis of data streams representing arbitrary financial contracts; ranging from privatelynegotiated deals to exchange-traded securities. We discuss the need for a flexible andextensible data model and provide an exemplar representing trading in corporate equitiesand bonds. Using a measure of Market volume; we review several analytical methods toexplore the data. Initial observations support the benefits of the framework to integrate andanalyze disparate sources of data.,Proceedings of the International Workshop on Data Science for Macro-Modeling,2014,1
Envisioning the next generation financial cyberinfrastructure: Transforming the monitoring and regulation of systemic risk,HV Jagadish; A Kyle; L Raschid,The Great Recession of 2008 and the continuing reverberations in the Eurozone havehighlighted significant limitations in the ability of regulators and analysts/researchers tomonitor and model the national and global financial ecosystem. This includes the lack offinancial cyberinfrastructure to ingest and process numerous streams of financialtransactions; as well as the accompanying data streams of economic activity; in real time.Also absent are open standards and shared semantics so that this data can be used topopulate models of individual markets; financial networks and the interconnected ecosystemrepresenting the global financial system. A key element of open standards and open data isthe definition of a legal entity identifier (LEI) for financial institutions; similar in spirit to CUSIP(or ISIN) for securities. The most important challenge is the need to develop …,*,2012,1
An Algebra for Visual Analysis,Anna A Shaverdian; Hao Zhou; George Michailidis; HV Jagadish,Abstract—Visual analytics that combine analytical techniques with advanced visualizationfeatures is fast becoming a standard tool in extracting information from complex data. Anumber of sophisticated tools have been developed for this purpose; which necessitatesformal methods to guide the creation of such tools and also compare them. Further; there isa need for visual analysts to document the steps in their analysis; for reuse; sharing; resultjustification; and so forth. This calls for a visual analytic framework encapsulated in a formalalgebra. In this paper; we develop such an algebra for graph data and introduce its atomicoperators; which include selection; aggregation; and labeling. We then build a frameworkaround this algebra; which enables visual exploration of data. We employ visual operatorsand support dynamic attributes of data to complete this visual analytic framework. We …,*,2010,1
MiMI: Michigan molecular interactions,Adriane Chapman; Magesh Jayapandian; Cong Yu; HV Jagadish,ABSTRACT There is a proliferation of data sources in biology. A complete understanding ofa biological problem often requires the integration of multiple data sources; each providinginsights on certain aspects of the problem. Furthermore; different sources often representdata in different ways; even when they cover the same information. Researchers interestedin a particular biological problem are forced to search for and understand multiple; oftenconflicting sources; and piece the jigsaw puzzle of information together for themselves. TheMichigan Molecular Interactions Database (MiMI) attempts to relieve scientists of this burden(MiMI; 2005). By integrating popular; well-known datasets; MiMI combines all the power ofeach individual dataset; like BIND (Bader et al.; 2003); and multiplies their benefits toindividual researchers by merging them with other known facts from diverse datasets. By …,*,2005,1
The Michigan Benchmark: A Micro-Benchmark for XML Query Performance Diagnostics,Jignesh M Patel; HV Jagadish,With the increasing popularity of the eXtensible Markup Language (XML) as arepresentation format for a wide variety of data; and it is clear that large repositories of XMLdata sets will soon emerge. The effective management of XML in a database thus becomesa pressing issue. Several methods for managing XML databases have emerged; rangingfrom retrofitting commercial RDBMSs to building native XML database systems. There hasnaturally been an interest in benchmarking the performance of these systems; and a numberof benchmarks have been proposed [4; 10; 12]. The focus of currently proposed benchmarksis to assess the performance of a given XML database in performing a variety ofrepresentative tasks. Such benchmarks are valuable to potential users of a database systemin providing an indication of the performance that the user can expect on their specific …,XML Data Management: Native XML and XML-Enable Database systems,2003,1
Independence diagrams: A technique for data visualization,Stefan Berchtold; HV Jagadish; Kenneth A Ross,An important issue in data visualization is the recognition of complex dependenciesbetween attributes. Past techniques for identifying attribute dependence include correlationcoefficients; scatterplots; and equi-width histograms. These techniques are sensitive tooutliers; and often are not sufficiently informative to identify the kind of attribute dependencepresent. We propose a new approach; which we call independence diagrams. We divideeach attribute into ranges; for each pair of attributes; the combination of these rangesdefines a two-dimensional grid. For each cell of this grid; we store the number of data itemsin it. We display the grid; scaling each attribute axis so that the displayed width of a range isproportional to the total number of data items within that range. The brightness of a cell isproportional to the density of data items in it. As a result; both attributes are independently …,Journal of Electronic Imaging,2000,1
Optimal histograms with quality guarantees,Torsten Suel; H Jagadish; N Koudas; S Muthukrishnan; V Poosala; K Sevcik,Suel; T; Jagadish; H; Koudas; N; Muthukrishnan; S; Poosala; V & Sevcik; K 1998; Optimal histogramswith quality guarantees. in 24th International Conference on Very Large Data Bases (VLDB'98). pp. 275-286 … Suel T; Jagadish H; Koudas N; Muthukrishnan S; Poosala V; Sevcik K.Optimal histograms with quality guarantees. In 24th International Conference on Very LargeData Bases (VLDB '98). 1998. p. 275-286 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2018 Elsevier BV.,*,1998,1
SUNRISE II: A Scalable and Timely Billing Architecture,HV Jagadish; Inderpal Singh Mumick,Abstract SUNRISE II is an architecture describing the management of customer and usagedata for billing and other account services in any transaction based system. The architectureprovides real-time rating; discounting; and information services based on the chronicle datamodel without using any specialized database software. Further; the architecture allows for aconnection between the billing data stream and network management data; quick creation ofnew features; automatic detection of interactions between features; a model of using self-maintainable views to integrate customer data coming from multiple sources; andtechniques to store large volumes of billed transactional data in an on-line system.,Online at: http://citeseer. nj. nec. com/cache/papers/cs/3257/http: zSzzSzwww. savera. comzSzmumickzSzpaperszSzpspaperszSzteleconf,1997,1
The New Jersey Data Reduction Report,Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,There is often a need to get quick approximate answers from large databases. This leads toa need for data reduction. There are many di erent approaches to this problem; some ofthem not traditionally posed as solutions to a data reduction problem. In this paper wedescribe and evaluate several popular techniques for data reduction. Historically; theprimary need for data reduction has been internal to a database system; in a cost-basedquery optimizer. The need is for the query optimizer to estimate the cost of alternative queryplans cheaply {clearly the e ort required to do so must be much smaller than the e ort ofactually executing the query; and yet the cost of executing any query plan depends stronglyupon the numerosity of speci ed attribute values and the selectivities of speci ed predicates.To address these query optimizer needs; many databases keep summary statistics …,Bulletin of the IEEE Computer Society Technical Committee on Data Engineering,1997,1
SUNRISE: An architecture for billing; rating and information services in a telephone network,HV Jagadish; Rama Kanneganti; Inderpal Singh Mumick; Abraham Silberschatz,Abstract SUNRISE is a data architecture for the usage and billing information managementin a telephone network. Its aim is to provide real-time rating and information services; inaddition to on-line billing services. The SUNRISE system is based on the chronicle datamodel JMS94] that provides for summary information to be maintained about the callingpatterns of each customer. A logically integrated view is maintained of the customer pro lesmanaged by di erent business units or service providers. The summary information and aportion of the customer pro le information permanently resides in a main memory storagesystem (such as the Dali system JLR+ 94] or the HP Smallbase system). A wide range ofqueries can be answered in real-time using the summary information and the customer proles. Such queries can also be asked by a telephone switch during call set-up. More …,*,1994,1
Issues in multimedia databases,HV Jagadish,Abstract Multimedia is a popular term these days; and the database community; naturally; istalking about multimedia databases. The reason multimedia is getting so much attention isclear: technology trends are now beginning to make it possible to store and display; at areasonable price; audio and still images through a computer. It is expected that videostorage will also be affordable in the near future. The purpose of this panel is to explore whatnew challenges this multimedia explosion brings to the database community.,ACM SIGMOD Record,1993,1
Database research at AT&T Bell Laboratories,HV Jagadish,Abstract Bell Laboratories is the Research and Development arm of AT&T. There is todaytremendous support for database research in Bell Labs and in AT&T. This is expected tocontinue since database technology is recognized as being central not just to Teradata;NCR; and AT&T's computing business but to its core communication business as well. Whatyou see described below is a survey of the current state of a young and growing researcheffort in the database area. You should expect to see a stronger “resume” a few years downthe road. Research at AT&T Bell Labs is never directed from above; towards specific projectsor objectives. Individual researchers select their areas of research based on a combinationof factors that include individual skill and interest; other on-going activity and potential forcollaboration; and the likelihood of intellectual or monetary impact. While research activity …,ACM Sigmod Record,1993,1
Architectural Support for Real-Time Database Systems,A Asthana; HV Jagadish; Paul Krzyzanowski; Nandit Soparkar,Abstract The requirements of a real-time database management system indicate thatstandard architectural platforms may be unable to provide adequate support. We present thedesign of a back-end object-oriented database management board and argue for its use inimplementing real-time applications. This board is based on the use of" intelligent" memory(ie; data memory with select computing abilities). We explain the utility of such memory inmeeting the needs of a real-time database management system.,*,1992,1
Diamond-Tree: An Index Structure for High-Dimensionality Approximate Searching,Christos Faloutsos; HV Jagadish,A selection query applied to a database often has the selection predicate imperfectlyspecified. We present a technique; called the Diamond-tree; for indexing fields to performsimilarity-based retrieval; given some applicable measures of approximation. Typically; thenumber of features (or dimensions of similarity) is large; so that the search space has a high-dimensionality; and most traditional methods perform poorly. As a test case; we show howthe Diamond-tree technique can be used to perform retrievals based on incorrectly orapproximately specified values for string fields. Experimental results show that our methodcan respond to approximately match queries by examining a small portion (1%-5%) of thedatabase.,*,1992,1
Sorting on an array of processors,HV Jagadish,The author presents a practical sorting algorithm for a nearest-neighbor connected array ofMIMD (multiple-instruction; multiple-data-stream) processors; each handling a significantfraction of the elements to be sorted. The algorithm is communication and CPU-optimal for alinear array; is almost order-preserving; and requires little working memory.,Computer Design: VLSI in Computers and Processors; 1988. ICCD'88.; Proceedings of the 1988 IEEE International Conference on,1988,1
The trap as a control flow mechanism,Jonathan A Chandross; HV Jagadish; Abhaya Asthana,Abstract In this paper we show how traditional hardware trap handlers can be generalizedinto an efficient vehicle for conditional branches. These ideas are being used in a VLSIprocessor under design. Conditional branches are often a major bottleneck in schedulingmicroinstructions on a horizontally microcoded machine. Several tests and conditionalbranches are frequently ready for scheduling simultaneously; but only one test and branch ispossible in a given cycle. The trap facility is traditionally treated as an interrupt scheme forthe notification of exceptional conditions. In this paper we study how the role of the trapmechanism may be expanded to include the parallel evaluation of arbitrary user-specifiedtests; and the concomitant performance benefits.,Proceedings of the 21st annual workshop on Microprogramming and microarchitecture,1988,1
Periodic time slot scheduling in unbuffered interconnection networks,U Mukherji; HV Jagadish,The use of unbuffered interconnection networks is suggested for interconnecting optical fiberlocal area networks. Control is based on a schedule in which slots in time-periodic framesare assigned to input-output paths. A blocking interconnection network typically has fewerincreased transmission rates to support equal load. This tradeoff is investigated. A specialclass of interconnection networks that includes omega blocking networks is defined. Asimple algorithm is presented for scheduling uniform load consisting of one slot for eachinput-output path in a frame with number of slots equal to the network size. Loads that arenot necessarily uniform but that total at each input and output to a number of slots equal tothe network size are generated at random for omega networks of three sizes; and resultsobtained using two scheduling heuristics are summarized.,Global Telecommunications Conference; 1988; and Exhibition.'Communications for the Information Age.'Conference Record; GLOBECOM'88.; IEEE,1988,1
Choosing Bucket Boundaries for Histograms,HV Jagadish; Nick Koudas; Kenneth C Sevcik,Abstract Histograms have long been used to capture attribute value distribution statistics forquery optimizers. More recently; there has been a growing interest in the use of histogramsto produce quick approximate answers to decision support queries. This motivates ndinggood strategies for specifying histogram buckets. Under the assumption that nding optimalbucket boundaries is computationally ine cient; previous research has focused on ndingheuristics that produce good solutions. In this paper; we present an algorithm to determinebucket boundaries optimally; in time proportional to the square of the number of distinct datavalues; for a broad class of optimality metrics. Through experimentation; we show thatoptimal histograms can have substantially lower reconstruction error than histogramsproduced according to popular heuristics. We also present a new heuristic; based on our …,Computer Systems Research Group; Paper,*,1
Hermod: A Distributed Infrastructure for Electronic Messaging,A Biliris; R Gruber; G Hjalmtysson; HV Jagadish; MA Jones; MF McGroary; E Panagos; M Rabinovich; AW Robinson; S Spear; D Srivastava; D Vista,Abstract Recent trends in the use of messaging for marketing; electronic commerce;information exchange; and entertainment have resulted in a vast increase in the number ofmessages being sent and received. The large volume of messages and the many purposesfor which messages are exchanged requires considerably richer facilities for managingmessages and creating messagingenabled services. We describe an extensible frameworkfor developing applications and services for uni ed messaging called Hermod. The view of amessaging system as a distributed active database is central to this framework. The core ofthe system comprises multiple heterogeneous data stores: for depositing and retrievingmessage contents; for managing user folders; for representing and managing thesemistructured nature of messages; and for maintaining information about users and user …,Submitted for publication,*,1
Shared and distinct lipid-lipid interactions in plasma and affected tissues in a diabetic mouse model,Kelli M Sas; Jiahe Lin; Thekkelnaycke M Rajendiran; Tanu Soni; Viji Nair; Lucy M Hinder; Hosagrahar V Jagadish; Thomas W Gardner; Steven F Abcouwer; Frank C Brosius; Eva L Feldman; Matthias Kretzler; George Michailidis; Subramaniam Pennathur,Abstract Lipids are ubiquitous metabolites with diverse functions; abnormalities in lipidmetabolism appear to be related to complications from multiple diseases; including type 2diabetes. Through technological advances; the entire lipidome has been characterized andresearchers now need computational approaches to better understand lipid networkperturbations in different diseases. Using a mouse model of type 2 diabetes withmicrovascular complications; we examined lipid levels in plasma and in renal; neural; andretinal tissues to identify shared and distinct lipid abnormalities. We used correlationanalysis to construct interaction networks in each tissue; to associate changes in lipids withchanges in enzymes of lipid metabolism; and to identify overlap of coregulated lipidsubclasses between plasma and each tissue to define subclasses of plasma lipids to use …,Journal of lipid research,2018,*
SLADE: A Smart Large-Scale Task Decomposer in Crowdsourcing,Yongxin Tong; Lei Chen; Zimu Zhou; Hosagrahar Visvesvaraya Jagadish; Lidan Shou; Weifeng Lv,Crowdsourcing has been shown to be effective in a wide range of applications; and isseeing increasing use. A large-scale crowdsourcing task often consists of thousands ormillions of atomic tasks; each of which is usually a simple task such as binary choice orsimple voting. To distribute a large-scale crowdsourcing task to limited crowd workers; acommon practice is to pack a set of atomic tasks into a task bin and send to a crowd workerin a batch. It is challenging to decompose a large-scale crowdsourcing task and executebatches of atomic tasks; which ensures reliable answers at a minimal total cost. Largebatches lead to unreliable answers of atomic tasks; while small batches incur unnecessarycost. In this paper; we investigate a general crowdsourcing task decomposition problem;called the Smart Large-scAle task DEcomposer (SLADE) problem; which aims to …,IEEE Transactions on Knowledge and Data Engineering,2018,*
Research Challenges in Financial Data Modeling and Analysis,Lewis Alexander; Sanjiv R Das; Zachary Ives; HV Jagadish; Claire Monteleoni,Abstract Significant research challenges must be addressed in the cleaning; transformation;integration; modeling; and analytics of Big Data sources for finance. This article surveys theprogress made so far in this direction and obstacles yet to be overcome. These are issuesthat are of interest to data-driven financial institutions in both corporate finance andconsumer finance. These challenges are also of interest to the legal profession as well as toregulators. The discussion is relevant to technology firms that support the growing field ofFinTech.,Big data,2017,*
QUIS: in-situ heterogeneous data source querying,Javad Chamanara; Birgitta König-Ries; HV Jagadish,Abstract Existing data integration frameworks are poorly suited for the special requirementsof scientists. To answer a specific research question; often; excerpts of data from differentsources need to be integrated. The relevant parts and the set of underlying sources maydiffer from query to query. The analyses also oftentimes involve frequently changing dataand exploratory querying. Additionally; The data sources not only store data in differentformats; but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fitwell with interest and expertise of the scientists. With QUIS (QUery In-Situ); we provide asolution for this problem. QUIS is an open source heterogeneous in-situ data queryingsystem. It utilizes a federated query virtualization approach that is built upon plugged-in …,Proceedings of the VLDB Endowment,2017,*
Parallelizing Skip Lists for In-memory Multi-core Database Systems,Zhongle Xie; Qingchao Cai; HV Jagadish; Beng Chin Ooi; Weng-Fai Wong,Due to the coarse granularity of data accesses and the heavy use of latches; indices in the B-tree family are not efficient for in-memory databases; especially in the context of today's multi-core architecture. In this paper; we study the parallelizability of skip lists for the parallel andconcurrent environment; and present PSL; a Parallel in-memory Skip List that lends itselfnaturally to the multi-core environment; particularly with non-uniform memory access. Foreach query; PSL traverses the index in a Breadth-First-Search (BFS) to find the list node withthe matching key; and exploits SIMD processing to speed up this process. Furthermore; PSLdistributes incoming queries among multiple execution threads disjointly and uniformly toeliminate the use of latches and achieve a high parallelizability. The experimental resultsshow that PSL is comparable to a readonly index; FAST; in terms of read performance …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,*
Bsmooth: Learning from user feedback to disambiguate query terms in interactive data retrieval,Bernardo Gonçalves; HV Jagadish,Abstract: There is great interest in supporting imprecise queries (eg; keyword search ornatural language queries) over databases today. To support such queries; the databasesystem is typically required to disambiguate parts of the user-specified query against thedatabase; using whatever resources are intrinsically available to it (the database schema;data values distributions; natural language models etc). Often; systems will also have a user-interaction log available; which can serve as an extrinsic resource to supplement their modelbased on their own intrinsic resources. This leads to a problem of how best to combine thesystem's prior ranking with insight derived from the user-interaction log. Statistical inferencetechniques such as maximum likelihood or Bayesian updates from a subjective prior turn outnot to apply in a straightforward way due to possible noise from user search behavior and …,arXiv preprint arXiv:1610.04789,2016,*
ExRank: an exploratory ranking interface,Ramon Bespinyowong; Wei Chen; HV Jagadish; Yuxin Ma,Abstract Even with simple everyday tasks like online shopping or choosing a restaurant;users are easily overwhelmed with the large number of choices available today; each with alarge number of inter-related attributes. We present ExRank; an interactive interface forexploring data that helps users understand the relationship between attribute values andfind interesting items in the dataset. Based on a kNN graph and a PageRank algorithm;ExRank suggests which attributes the user should look at; and how expressed choices inparticular attributes affect the distribution of values in other attributes for candidate objects. Itsolves the problem of empty result by showing similar items and when there are too manyresults; it ranks the data for the user. This demo consists of 1) the description of the softwarearchitecture and the user interface 2) the logic and reason behind our solution and 3) a …,Proceedings of the VLDB Endowment,2016,*
Special issue on best papers of VLDB 2014,HV Jagadish; Aoying Zhou,Papers for the VLDB conference are chosen through a year-round reviewing process withcutoff dates for conference inclusion. The VLDB 2014 conference roughly coincided withVolume 7 of PVLDB; which received 695 submissions. Of these; 139 were accepted; but 21were accepted too late for inclusion in VLDB 2014. The remaining 118 were presented atVLDB 2014. In addition; there were 47 papers from PVLDB Vol. 6 presented at VLDB 2014;for a total of 165 papers. A best paper committee; comprising Dimitris Papadias (Chair);Jayant Haritsa; and Kian-Lee Tan; chose seven papers out of these 165 to invite forinclusion in VLDB Journal. Of these seven; five papers were finally accepted in extendedform for publication in this special issue; after two additional rounds of review. These papersprovide a nice sampling of the rich frontier of database research today; touching upon five …,The VLDB Journal,2016,*
Moving past the" Wild West" era for Big Data.,HV Jagadish,Page 1. Moving Past the Wild West Era for Big Data HV Jagadish University of Michigan Supportedin part by NSF grant IIS 1250880 © HV Jagadish. You may reproduce under licensecc-by-nc-nd-2.0 Page 2. You take a photo at the beach mystuart cc-by-nc-nd-2.0 Page 3. AndPost it on Facebook People who happened to be on the beach are in your photo. You do not knowthem; they are not tagged; and don't even know about this photo you have made public Page 4.Face Recognition Software With face recognition software; Don Juan and Lolita are identifiedin your photo. Don Juan Lolita inabeanpod cc-by-nc-nd-2.0 Page 5. Search Technology • Lolita'sspouse finds this photo and her affair with Don. • Their marriage is on the rocks. • Who is at fault?Page 6. Lolita and Don Juan? Of course … Page 7. Chain of Steps • You took photo • You postedon Facebook • I used face recognition software with Big Data …,Big Data,2015,*
Big Data Research,HV Jagadish,“Big Data” now impacts nearly every aspect of our modern society; including business;government; health care; and research in almost every discipline: life sciences; engineering;natural sciences; art & humanities. As it has drawn much attention; and becomeeconomically important; there are many who have preferred angles on the interpretation ofBig Data. At the same time; as many have been exposed to the term with little priorknowledge of computing or technology; they are easily swayed by the “experts.” Inconsequence; there has been a rush to use the term Big Data in ways that are inappropriatebut self-serving. In many cases; these erroneous interpretations have then been taken upand amplified by others; including even technically sophisticated people. In this article; Idiscuss some of the more common myths.,*,2015,*
Proceedings of the VLDB Endowment Volume 7 Issue 10,HV Jagadish; Aoying Zhou,*,*,2014,*
Dedicatória,Toby Teorey; Sam Lightstone; Tom Nadeau; HV Jagadish,*,*,2014,*
Projeto e Modelagem de Banco de Dados: Tradução da 5a Edição,Sam S Lightstone; Toby J Teorey; Tom Nadeau; HV Jagadish,A tecnologia de projeto de banco de dados passou por uma evolução significativa nosúltimos anos; à medida que as aplicações comerciais têm sido dominadas pelo modelo dedados relacional e por sistemas de bancos de dados relacionais. O modelo relacional tempermitido que o projetista de banco de dados focalize separadamente o projeto lógico-definindo os relacionamentos de dados e tabelas-e o projeto físico-armazenando erecuperando dados do repositório físico de forma eficiente. Em Projeto e Modelagem deBanco de dados; o leitor inicia a discussão do projeto lógico de banco de dados com aabordagem entidade-relacionamento (ER) para a especificação de requisitos de dados emodelagem conceitual. Depois é oferecida uma visão detalhada de uma outra abordagemdominante de modelagem de dados; a Unified Modeling Language (UML). As duas …,*,2013,*
The Design of a Back-end Object Management System,HV Jagadish,Abstract We describe the architecture and design of a back-end object manager; designedas an “active memory” system on a plug-in board for a standard workstation (or personalcomputer). We show how; with minimal modification to existing code; it is possible to achievesignificant performance improvement for the execution of data-intensive methods on objects;simply by using our back-end object manager.,Code Generation—Concepts; Tools; Techniques: Proceedings of the International Workshop on Code Generation; Dagstuhl; Germany; 20–24 May 1991,2013,*
Provenance in a Modifiable Data Set,Jing Zhang; HV Jagadish,Abstract Provenance of data is now widely recognized as being of great importance; thanksin large part to pioneering work [4; 6] by Peter Buneman and his collaborators in a streamthat continues to produce influential papers today [1-3; 7]. When we consume data from adatabase; we often care about where these data come from; how they were derived; and soforth. We may desire answers to such questions to establish trust in the data; to investigatesuspicious values; to debug code in the system; or for a host of other reasons. Considerablerecent work has addressed many issues related to provenance. However; the standardassumption is that data sources; from which result data have been derived; are static. Inreality; we know that most data are modified over time; including data sources used forderiving results of interest. When we consider provenance in the context of such …,*,2013,*
Gene Expression Signatures Predictive Of Human Diabetic Neuropathy,J Hur; KA Sullivan; M Pande; Y Hong; A Af Sima; HV Jagadish; M Kretzler; EL Feldman,Read 'GENE EXPRESSION SIGNATURES PREDICTIVE OF HUMANDIABETIC NEUROPATHY' on Ovid Insights.,Journal of the Peripheral Nervous System,2011,*
The Identification of Gene Expression Profiles Predictive of Human Diabetic Neuropathy Progression,Junguk Hur; Kelli A Sullivan; Manusha Pande; Yu Hong Anders AF Sima; Hosagrahar V Jagadish; Matthias Kretzler; Eva L Feldman,*,ANNALS OF NEUROLOGY,2010,*
Gene Expression Profile Predictive Diabetic Neuropathy Progression,Junguk Hur; Matthias Kretzler; Viji Nair; Hosagrahar V Jagadish; Kelli A Sullivan; Anders AF Sima; Eva L Feldman,*,ANNALS OF NEUROLOGY,2009,*
Conference on Very Large Data Bases; Auckland; New Zealand,HV Jagadish; Peter Apers; Peter Buneman; Martin Kersten,*,Proceedings of the 34th International,2008,*
Tom Crecelius 1480 Carlo A. Curino 761; 882 Emiran Curtmola 1408; 1448 D Harish D. 1124; 1325,Florian Daniel; David DeWitt; Amol Deshpande; AnHai Doan; Marcus Fontoura; Juliana Freire; Venkatesh Ganti; Hong Gao; Hector Garcia-Molina; Minos Garofalakis; Charles Garrod; Tingjian Ge; Lise Getoor; Phillip Gibbons; Lukasz Golab; Wojciech Golab; Yihong Gong; Albert Greenberg; Maxim Grinev; Peter Haas; Wook-Shin Han; Michael Hay; Monika Henzinger; Mauricio Hernandez; Mark Hill; Howard Ho; Allison Holloway; Mingsheng Hong; Chien-Yi Hou; Yanli Hu; Kien Hua; Jiansheng Huang; Ihab Francis Ilyas; Zachary G Ives; Marie Jacob; HV Jagadish; Magesh Jayapandian; David Jensen; Haifeng Jiang; Cheqing Jin; Ryan Johnson; Theodore Johnson; Vanja Josifovski,*,*,2008,*
Some Challenges in Integrating Information on Protein Interactions and a Partial Solution,HV Jagadish,Summary form only given. Independently constructed sources of (scientific) data frequentlyhave overlapping; and sometimes contradictory; information content. Current methods of usefall into two categories: force the integration step onto the user; or merely collate the data; atmost transforming it into a common format. The first method places an undue burden on theuser to fit all of the jigsaw puzzle pieces together. The second leads to redundancy andpossible inconsistency. We propose a third: deep data integration. The idea is to provide acohesive view of all information currently available for a protein; interaction; or other objectof scientific interest. Doing so requires that multiple pieces of data about the object; indifferent sources; first be identified as referring to the same object; if required through" thirdparty" information; then that a single" record" be created comprising the union of the …,Scientific and Statistical Database Management; 2007. SSBDM'07. 19th International Conference on,2007,*
Special issue on data management; analysis; and mining for the life sciences,Louiqa Raschid; HV Jagadish; Terry Gaasterland,During the last decade; biologists have experienced a fundamental revolution fromtraditional R&D involving the study of single genes and isolated cellular mechanisms to e-biology that addresses whole cell physiology and complex biological systems. This hasbeen accompanied by an explosion in the number and size of public data resources; and arapid growth in the variety and volume of laboratory data from microarrays measuringtranscription levels simultaneously for hundreds of genes and to protein interaction screensmeasuring multiple components of protein complexes. Life science data is complex and datasets have complex inter-relationships. The data is often incomplete; uncertain; and can varysignificantly across biological replicates. Data can evolve more quickly than thetechnologies developed to interpret the data. Thus; life sciences data is not well suited for …,*,2005,*
Using delay to defend against database extraction,Magesh Jayapandian; Brian Noble; James Mickens; HV Jagadish,Abstract For many data providers; the “crown jewels” of their business are the data that theyhave organized. If someone could copy their entire database; it would be a competitivecatastrophe. Yet; a data provider is in the business of providing data; so access to thedatabase cannot be restricted entirely. How is the data provider to permit legitimate accessto users who request access to small portions of the database while protecting the databasefrom wholesale copying? We suggest that delay can be used for this purpose. We show;under reasonable assumptions; that it is possible to slow down the copying of the entiredataset by an arbitrary amount ensuring that queries that return a significant portion of thedatabase introduce a delay that is orders of magnitude higher than that for legitimate userqueries. We then consider issues of change; and show; under reasonable assumptions of …,Workshop on Secure Data Management,2004,*
Efficient Evaluation of XQuery,Zhimin Chen; HV Jagadish,Abstract XQuery is the de facto standard XML query language; and it is important to haveefficient query evaluation techniques available for it. A core op-eration in the evaluation ofXQuery is the finding of matches for specified tree patterns; and there has been much worktowards algorithms for finding such matches efficiently. Multiple XPath expressions can beevaluated by computing one or more tree pattern matches. However; relatively little hasbeen done on efficient evaluation of XQuery queries as a whole. In this paper; we argue thatthere is much more to XQuery evaluation than a tree pattern match. We propose a structurecalled generalized tree patterm (GTP) for concise representation of a whole XQueryexpression. Evaluating the query reduces to finding matches for its GTP. Using this idea wedevelop efficient evaluation plans for XQuery expressions; possibly involving join …,Proceedings 2003 VLDB Conference: 29th International Conference on Very Large Databases (VLDB),2003,*
LBNL Report LBNL-52767,HV Jagadish; Frank Olken,Abstract Technological; institutional; economic and budgetary changes over the past decadehave transformed the life sciences to become increasingly" data rich". Hundreds of millionsof dollars have been (and are being) spent to develop large biological informationresources; eg; the human genome sequence; protein structures; and assembling thisinformation in public databases; eg; Genbank; PDB. Data management tools to facilitateaccess and analysis such data are necessary to obtain the full benefits of the investments incollecting these large datasets. S equence data w ould be of little use if confined topublication in traditional print media. O ur conclusion is that data management technologyhas not kept pace w ith data generationinbiology. W e believe that further research anddevelopment of data management technology is needed to e ff ectively utiliz e and ex …,*,2003,*
Data Management for the Biosciences: Report of the Workshop on,HV Jagadish; Frank Olken,This is the report of a workshop held on Feb. 2-3; 2003 at the National Library of Medicine;Bethesda; MD on Data Management Technology for Molecular and Cell Biology.Technological; institutional; economic and budgetary changes over the past decade havetransformed the life sciences to become increasingly “data rich”. Hundreds of millions ofdollars have been (and are being) spent to develop large biological information resources;eg; the human genome sequence; protein structures; and assembling this information inpublic databases; eg; Genbank; PDB. Data management tools to facilitate access andanalysis such data are necessary to obtain the full benefits of the investments in collectingthese large datasets. Sequence data would be of little use if confined to publication intraditional print media. Our conclusion is that data management technology has not kept …,*,2003,*
Efficiency and Effectiveness of XML Tools and Techniques (EEXTT)-Benchmarking XML-The Michigan Benchmark: A Microbenchmark for XML Query Processing Sy...,Kanda Runapongsa; Jignesh M Patel; HV Jagadish; Shurug Al-Khalifa,*,Lecture Notes in Computer Science,2003,*
Database concurrency control,HV Jagadish,Abstract A transaction is a set of actions; such that either all complete execution successfullyor none do (a property known as atomicity). For example; if I move money from one bankaccount to another at an automatic teller machine; the withdrawal from one account and thedeposit in the other account are both part of one transaction; guaranteeing that it is never thecase that money has been withdrawn from one account but never deposited in the other.The notion of a transaction is central to database systems and is now being adopted in otherareas of computer science as well; such as in operating systems (qv) and in distributedsystems (qv).,*,2003,*
Information warfare and security,HV Jagadish,I never thought I would find a technical book that makes pleasant bedtime reading-I waswrong. This book is a joy to read; with page after page of" war stories" the author hascollected over the years.(It isn't quite a" you cannot put it down page-turner" like a goodmystery novel; but that is only because the average story is less than a page long. And anyway; that is asking for too much in a serious piece of nonfiction.) In terms of content; whatblew me away was the staggering scope of subject matter addressed by the author. As acomputer-geek; if you asked me to define" information warfare/security" I would talk aboutthe hacking of websites; networks; and computers. As a database researcher; my concern isoften limited only to database security. Here is a book that places these concerns in context.Information has had value through the ages; and much before the computer was in …,ACM SIGMOD Record,2001,*
gAnalysis of the clustering properties of the Hilbert space-filling curve;• h IEEE Trans,Bongki Moon; HV Jagadish; C Faloutsos,*,Knowl. Data Eng,2001,*
ACM-SIGMOD Digital Review,H Jagadish,*,SIGMOD RECORD,2000,*
Analysis of the Clustering Properties of,Bongki Moon; HV Jagadish; Christos Faloutsos; Joel H Saltz,*,*,1999,*
Article ID jcss. 1999.1641; available online at http: ย ย www. idealibrary. com on,Serge Abiteboul; Joseph Albert; Noga Alon; Gheorghe Antonoiu; Sanjeev Arora; Paolo Atzeni; Robert Beals; Amir Ben-Dor; Avrim Blum; Jonathan F Buss; Jin-Yi Cai; John Case; Jianer Chen; Stephen A Cook; Jurgen Dassow; Tamal K Dey; Freddy Dumortier; Ronald Fagin; Gudmund S Frandsen; Donald K Friesen; Takeshi Fukuda; Mark A Fulk; Leslie Ann Goldberg; Sumanta Guha; Marc Gyssens; Armin Haken; Lane A Hemaspaandra; Laurent Herr; Yannis Ioannidis; HV Jagadish,*,Journal of Computer and System Sciences,1999,*
DR: KNOW: The Data Mining Project at AT&T Research,Stefan Berchtold; Christos Faloutsos; HV Jagadish; Theodore Johnson; Raymond Ng; Ken Ross,Revision : 6:1 The goal of this white paper is to de ne research directions. We start with the problemde nition; present a classi cation of tools and issues; and list completed or on-going e orts atAT&T Research … This is a description of the DR: KNOW project (Data Reduction and KNowledgeextraction for Online data Warehouses). The focus of this project is to manage massive collectionsof data; in order to enable data analysis. Speci cally; we have two major goals … 1. We wantto \ nd interesting things" (= patterns = knowledge = rules) … 2. We also want to provide fast;approximate answers to ad-hoc queries … Examples of \interesting things" include: rules(eg.; whoever places many calls on Saturdays; typically places many calls on Sundays; too );correlations (eg.; negative correlation between call volume on weekdays and weekends); andoutliers (eg; unexpected phonecall-patterns; probably due to calling- card fraud). Figure …,*,1997,*
Effi cient Retrieval of Sim ilar Tim e Sequences U nder,Byoung-Kee Yi; HV Jagadish; Christos Faloutsos,*,*,1997,*
Readings in Object-Oriented Database Systems Readings in Object-Oriented Database Systems; 1990,Hiroyuki KITAGAWA; Yoshiharu ISHIKAWA; HV JAGADISH; Laks VS LAKSHMANAN; Divesh SRIVASTAVA,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,IEICE transactions on information and systems,1997,*
Logic-enhanced memory for high performance databases,Abhaya Asthana; Nandit Soparkar; HV Jagadish; Paul Krzyzanowski,Several emerging applications need a high performance; main memory database for whichtypical memory systems may be inadequate. In particular; the processor memory bandwidthproves to be a major performance limiting factor. We present the design of a back end objectoriented database board and argue for its use in implementing high performance; real timeapplications. Our board uses programmable logic enhanced memory (ie; data memory withselect computing abilities) which is a novel way to obviate the low bandwidth problem fordata intensive processing. Our efforts differ significantly from database machines by beingmore general purpose and targeting the bandwidth issue at a different level of the memoryhierarchy. We describe the utility of logic enhanced memory in meeting the needs of highperformance databases.,Knowledge-Based Intelligent Electronic Systems; 1997. KES'97. Proceedings.; 1997 First International Conference on,1997,*
ACM SIGMOD Record Volume 25 Issue 2,HV Jagadish; Inderpal Singh Mumick; TH Chairman-Merrett,*,*,1996,*
Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data: SIGMOD'96; Montreal; Quebec; Canada June 4-6; 1996,HV Jagadish; Inderpal Singh Mumick,*,*,1996,*
Linear Clustering of Objects with Multiple,HV Jagadish; Tian Zhang; Miron Livny; HV Jagadish; Divesh Srivastava,*,Information Processing and Management,1995,*
Interactive Spatial Directories,Brian C.  Schmult; HV Jagadish; S. Kicha  Ganapathy,Abstract A spatial directory is a directory of places; and information; things and eventsassociated with places; where classi cation and searching are based on location (or spatialproximity) as well as on the name or type of an item. The result of a query could bealphanumeric or multimedia information; or a map. In this paper we sketch some of theissues we are tackling in the Interactive Spatial Directory (ISD) project.,IEEE Data Eng. Bull.,1993,*
Rose; T. 455 Rotem; D. 3,HV Jagadish,*,Very Large Data Bases: VLDB'92; Proceedings of the 18th International Conference on Very Large Data Bases; August 23-27; 1992; Vancouver; Canada,1992,*
Hybrid Index Organizations Tor Text uaiaoases,Christos Faloutsost; HV Jagadish,*,Advances in Database Technology: EDBT...: Proceedings,1992,*
Optimization of Generalized Transitive Closure Queries,Shaul Dar'l Rakesh Agrawal; H v Jagadish,*,Data engineering,1991,*
THE RIGHTPAGES-AN ELECTRONIC LIBRARY ALTERING AND DISTRIBUTION SERVICE,L OGORMAN; GA STORY; D FOX; HV JAGADISH; LL SCHAPER,*,PROCEEDINGS OF THE ASIS ANNUAL MEETING,1991,*
ACM SIGMOD Record Volume 19 Issue 2,HV Jagadish,*,*,1990,*
Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data: May 23-25; 1990; Atlantic City; NJ,Hector Garcia-Molina; HV Jagadish,*,*,1990,*
Data Engineering,William DuMouchel Barbará; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,There is often a need to get quick approximate answers from large databases. This leads toa need for data reduction. There are many different approaches to this problem; some ofthem not traditionally posed as solutions to a data reduction problem. In this paper wedescribe and evaluate several popular techniques for data reduction. Historically; theprimary need for data reduction has been internal to a database system; in a cost-basedquery optimizer. The need is for the query optimizer to estimate the cost of alternative queryplans cheaply–clearly the effort required to do so must be much smaller than the effort ofactually executing the query; and yet the cost of executing any query plan depends stronglyupon the numerosity of specified attribute values and the selectivities of specified predicates.To address these query optimizer needs; many databases keep summary statistics …,*,*,*
Program Vice-Chairs,Jeff Naughton; Sunita Sarawagi; Hank Korth; Arnie Rosenthal; Jeff Ullman; Hans Schek; Phil Bernstein; Donald Kossmann; Stavros Christodoulakis; Theo Haerder; Beng Chin Ooi; HV Jagadish; Gerhard Weikum,Page 1. xix Program Vice-Chairs Jeff Naughton; University of Wisconsin; USA Sunita Sarawagi;IBM Almaden; USA Hank Korth; Lucent - Bell Labs; USA Arnie Rosenthal; Mitre; USA Jeff Ullman;Stanford University; USA Hans Schek; ETH Zurich; Switzerland Phil Bernstein; Microsoft; USADonald Kossmann; University of Passau; Germany Stavros Christodoulakis; University of Crete;Greece Theo Haerder; University of Kaiserslautern; Germany Beng Chin Ooi; National Universityof Singapore; Singapore HV Jagadish; University of Illinois at Urbana-Champaign; USA AwardCommittee Members Hank Korth Donald Kossmann Arnie Rosenthal Gerhard Weikum,*,*,*
VLDB Endowment Board of Trustees,Gerhard Weikum; Laura M Haas; Paolo Atzeni; Michael J Franklin; Amr El Abbadi; Gustavo Alonso; Peter MG Apers; Elisa Bertino; Peter Buneman; Johann Christoph Freytag; HV Jagadish; Christian S Jensen; Donald Kossmann; David Lomet; Renée J Miller; Shojiro Nishio; Beng Chin Ooi; Meral Ozsoyoglu; Krithi Ramamritham; Raghu Ramakrishnan; Stanley B Zdonik,The VLDB Endowment is a non-profit foundation whose objective is to promote scientific andeducational activities in the area of large-scale data; information; and knowledgemanagement. The Endowment serves as the steering committee for the VLDB conferenceseries. The Endowment also sponsors various scholarly activities. It has established aprogram that supports summer schools; tutorials; and other training activities of this kind; incountries that could otherwise not afford the expenses for such events. The Endowment isalso the main sponsor of the biennial Conference on Innovative Data Systems Research(CIDR); and it runs the VLDB Journal; one of the most successful journals in the databasearea. On various activities; the Endowment closely cooperates with ACM SIGMOD. TheVLDB Endowment has a board of 21 elected trustees; who are the legal guardians of the …,*,*,*
ICDE ‘98 Program Committee Members,Nabil Adam; Gustavo Alonso; BR Badrinath; Sujata Banerjee; Lubomir F Bit; Alexandros Biliris; Patrick Bobbie; Michael H Boehlen; Arbee LP Chen; Ming-Syan Chen; Boris Chidlosvkii; Munir Cochinwala; Robert Demolombe; Suzanne Dietrich; Klaus Dittrich; Asuman Dogac; Maggie Dunham; Curtis Dyreson; Ophir Frieder; Narain Gehani; Shahram Ghandeharizadeh; Joachim Hammer; Jiawei Han; Ralf Hartmut Gueting; Waqar Hasan; Sandra Heiler; HV Jagadish; Yahiko Kambayashi; Vijay Kumar; Alon Levy,*,*,*,*
FAR EAST,William Armstrong; Christos Faloutsos; Vassos Hadzilacos; HV Jagadish; Ravi Krishnamurthy; David Lomet; Dennis McLeod; Shamkant Navathe; Ekow Otoo; Raghu Ramakrishnan; Betty Salzberg; Jacob Slonim; Irving Traiger; Carlo Zaniolo; Serge Abiteboul; Peter Apers; Horst Biller; Peter Dadam; Robert Demolombe; Frank Eliassen; Georg Gottlob; Peter Lockmann; Robert Meersman; Andreas Reuter; Hans-Joerg Schek; Nicolas Spyratos; Yannis Vasiliou; Bruce Croft; Hector Garcia-Molina; Paula Hawthorn; Randy Katz; Bruce Lindsay; Wo-Shun Luk; Albert Mendelzon; Jack Orenptein; Z Meral Ozsoyoglu; Arnon Rosenthal; Timos Sellis; Toby Teorey; Grant Weddell; Stanley Zdonik; Michele Adiba; Janis Bubenko; Klaus Dittrich; Norbert Fuhr; Theo Haerder; Heikki Mannila; Antoni Olive; Yehoshua Sagiv; Gunter Schlageter; Bernd Walter; Umeshwar Dayal; Goetz Graefe; Yannis Ioannidis; Roger King; Guy Lohman; Ian McLeod; Tim Merrett; Sylvia Osborn; M Tamer Ozsu; Nick Roussopoulos; Kenneth Sevcik; Frank Tompa; Clement Yu; Antonio Albano; Francois Bancilhon; Stefano Ceri; Hartmut Ehrig; Georges Gardarin; Witold Litwin; Rainer Manthey; Peter Pistor; Felix Saltor; Amilcar Sernadas; Henry Tirri; Roberto Zicari,William Armstrong (Canada) Christos Faloutsos (USA) Vassos Hadzilacos (Canada) HV Jagadish(USA) Ravi Krishnamurthy (USA) David Lomet (USA) Dennis McLeod (USA) Shamkant Navathe(USA) Ekow Otoo (Canada) Raghu Ramakrishnan (USA) Betty Salzberg (USA) Jacob Slonim(Canada) Irving Traiger (USA) Carlo Zaniolo (USA) … Serge Abiteboul (France) Peter Apers(Netherlands) Horst Biller (Germany) Peter Dadam (Germany) Robert Demolombe (France) FrankEliassen (Norway) Georg Gottlob (Austria) Peter Lockmann (Germany) Robert Meersman(Netherlands) Andreas Reuter (Germany) Hans-Joerg Schek (Switzerland) Nicolas Spyratos(France) Yannis Vasiliou (Greece) … Ching-Chen Chang (Taiwan; China) Yahiko Kambayashi(Japan) Sukho Lee (Korea) Leszek Maciaszek (Australia) Maria Orlowska (Australia) RonSacks-Davis (Australis) KP Tan (Singapore) KY Whang (Korea),*,*,*
and Shurug Al-Khalifa Department of Electrical Engineering and Computer Science The University of Michigan; Ann Arbor; MI 48109; USA {krunapon; jignesh; jag; y...,Kanda Runapongsa; Jignesh M Patel; HV Jagadish; Yun Chen,*,*,*,*
Predictive Range and Nearest Neighbor Queries over Moving Objects,Rui Zhang; Bing Tian Dai; HV Jagadish; Kotagiri Ramamohanarao,*,*,*,*
Heterogeneous and Fe&rated Databases Semantic Integration in Heterogeneous Databases Using Neural Networks,Wen-Syan Li; Chris Clifton; NB Uris; WA Gray; RF Churchhouse; Dirk Jonscher; Klaus R D&rich; Pipellned Parallelism; Waqar Hasan; Rajeev Motwani; HV Jagadish; Daniel Lieuwen; Rajeev Rastogi; Avi Silberschatz; S Sudarshan; Mukesh K Mohania; NL Sarda; C&u Galindo-Legaria; Arjan Pellet&oft; Martin Kersten; Eric Amiel; Marie-Jo Bellosta; Eric Dujardin; Eric Simon; Janet L Wiener; Jeffrey F Naughton; C Collet; T Coupaye; T Svensen,Semantic Integration in Heterogeneous Databases Using Neural Networks Wen-Syan Liand Chris Clifton.............................................................................................. 1 Providing DynamicSecurity Control in a Federated Database NB Uris; WA Gray and RF Churchhouse...................................................................... 13 An Approach for Building Secure Database FederationsDirk Jonscher and Klaus R. D&rich..................................................................................... 24,*,*,*
The work of refereeing was undertaken largely by the Program Committee. Invaluable help was provided; however; by the referees listed below.,C Arapis; B Badrinath; R Balter; D Batory; Y Bernard; J Bocca; S Boettcher; H Boral; R Brachman; H Brueggemann; A Buchmann; E Casais; U Chakravarthy; I Chomicki; H Chou; C Collet; R Cooper; L Dami; U Deppisch; K Dittrich; C Esculier; J Ferrie; S Finkelstein; J Freitag; F Garzotto; G Graefe; G Grahne; H Gunadhi; J Hagelstein; U Hahn; G Hulin; T Imielinski; H Jagadish; P Jeng; M Jeusfeld; H Kangassalo; E Kantorowitz; G Qp; R Kemp; W Kim; W Kohler; D Konstantas; H Korth; R Krishnamurthy; A Kumar; H Lame; T Lehman; V Linnemann; M Livny; G L&man; D Lubinsky; H Mannila; L Mark; S Mazumdar; J McPherson; M Missikoff; S Nagvi; F Olken; J Orenstein; M Papathomas; G Pelagatti; B Pemici; X Pintado; H Pirahesh; P Pistor; R Ramakrishnan; U Reiner; D Roelants; T Rose; D Rotem; B Rubenstein; K Sabnani; Y Sagiv; S Salza; M Scholl; F Schreiber; P Selinger; A Shoshani; E Soisalon-Soininen; S Stemple; M Terranova; B Thalheim; A Tomasic; S Tsur; I Vatton; M Vauclair; M Vernon; M Wallace; M Weigle; D Woelk; N Woo,Arapis; C. Badrinath; B. Balter; R. Batory; D. Bernard; Y. Bocca; J. Boettcher; S. Boral; H.Brachman; R. Brueggemann; H. By; F. Buchmann; A. Casais; E. Chakravarthy; U. Chang; E.Chomicki; I. Chou; H. Collet; C. Cooper; R. Dami; L. Deppisch; U. Dittrich; K. Esculier; C.Ferrie; J. Finkelstein; S. Freitag; J. Garzotto; F. Graefe; G. Grahne; G. Gunadhi; H. Hagelstein;J. Hahn; U. Hermenegildo; M … Hulin; G. Imielinski; T. Jagadish; H. Jeng; P. Jeusfeld; M.Kangassalo; H. Kantorowitz; E. Qp4 G. Kemp; R. Kim; W. Kohler; W. Konstantas; D. Korth; H.Krishnamurthy; R. Kumar; A. Lame; H. Le; c. Lehman; T. Linnemann; V. Livny; M. L&man; G.Lubinsky; D. * Mannila; H. Mark; L. Mazumdar; S. McPherson; J. Missikoff; M. Nagvi; S.Nierstrasz; 0. Olken; F. Orenstein; J. ozarow; L. Papathomas; M … Pelagatti; G. Pemici; B.Pintado; X. Pirahesh; H. Pistor; P. Ramakrishnan; R. Reiner; U. Roelants; D. Rose; T …,*,*,*
Conference Organisers,Alexandros Biliris; Inderpal Singh Mumick; Advisory Council; Stavros Christodoulakis; Ron Sacks-Davis; David Dewitt; Peter Lockemann; Sham Navathe; Klaus Dittrich; Yannis Ioannidis; HV Jagadish; Nelson M Mattos; Andy Witkowski; Kenneth A Ross; Alex Delis; Euthimios Panagos; Vibby Gottemukkala; Ashish Gupta; Tamer Ozsu; Area Coordinators; Francois Bancilhon; Hector Garcia-Molina,General Conference Chairs Alexandros Biliris (AT&T Labs; USA) Inderpal Singh Mumick (SaveraSystems; USA) Advisory Council Stavros Christodoulakis (University of Crete; Greece) RonSacks-Davis (CITRI) David Dewitt (University of Wisconsin; USA) Peter Lockemann (Universityof Karlsruhe; Germany) Sham Navathe (Georgia Tech; USA) Research Program ChairsAmericas/Australia: Jennifer Widom (Stanford University; USA) Africa/Asia/Europe: Oded Shmueli(Technion; Israel) Industrial Program Chairs Americas/Australia: Dennis Shasha (CourantInstitute; USA) Africa/Asia/Europe: Patrick Valduriez (INRIA; France) Panel Program Chairs KlausDittrich (Universitat Zurich; Switzerland) Yannis Ioannidis (University of Athens; Greece) TutorialProgram Chair HV Jagadish (AT&T Labs; USA) Exhibits Program Chairs Nelson M. Mattos(IBM; USA) Andy Witkowski (Oracle Corporation; USA) Treasurer Kenneth A. Ross …,*,*,*
Envisioning the Next Generation Financial Cyberinfrastructure: Transforming the Monitoring and Regulation of Systemic Risk,M Flood; HV Jagadish; A Kyle; L Raschid,*,*,*,*
Compressed Accessibility Map: E cient Access Control for XML,HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Ting Yu,*,*,*,*
Friday; October 28; 2016 at 10: 45 am University of Michigan Law School; Hutchins Hall 100,HV Jagadish; Lewis Alexander; Sanjiv Das; Zachary Ives; Claire Monteleoni,In many fields of endeavor today; data provide the basis for informed decision-making. Thisis particularly true of macro-prudential analysis: determination of financial stability requirescleaning; integration; and analysis of multiple disparate large and complex sources of datain a timely way. In fact; the use of Big Data requires technical advances in multiple stages ofthe Big Data pipeline; as discussed by Jagadish et al (2014). These needs for data cleaning;integration; and analytics are universal; across many domains; and there is considerableexcellent research expanding the frontiers of what we are capable of doing in this regard.This panel will provide an overview of some of the successes we have had.,*,*,*
Compressed Accessibility Map: E cient Access Control for Hierarchical Data,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,*,*,*,*
Incremental Organization for Data Recording and Warehousing,PPS Narayan; S Seshadri; S Sudarshan; HV Jagadish; Rama Kanneganti,*,*,*,*
Reasoning with Aggregation Constraints in Views,Shaul Dar; HV Jagadish; Alon Y Levy; Divesh Srivastava,Abstract We investigate the problem of using materialized views to compute answers to SQLqueries with grouping and aggregation; in the presence of multiset tables. This problem isimportant in many applications; such as data warehousing; mobile computing; globalinformation systems; and maintaining physical data independence; where access to local orcached materialized views may be cheaper than access to the underlying database. Inaddition; this problem has obvious potential in optimizing query evaluation. The problem isformally stated as nding a rewriting of an SQL query Q where the materialized views occur inthe FROM clause; and the rewritten query is multiset-equivalent to Q. First; we study the casewhere the query has grouping and aggregation but the views do not; and show that usabilityof a view in evaluating a query essentially requires an isomorphism between the view …,*,*,*
A Generic Inverted Index Framework for Similarity Search on the GPU,Jingbo Zhou; Qi Guo; HV Jagadish; Wenhao Luan; Anthony KH Tung; Yueji Yang; Yuxin Zheng,ABSTRACT To perform similarity search; existing works mainly choose to create customizedindexes for different data types. Due to the diversity of customized indexes; it is hard todevise a general parallelization strategy to speed up the search. In this paper; we propose ageneric inverted index framework on the GPU (called GENIE); which can support parallelsimilarity search of multiple queries on various data types. We also give an in-depthinvestigation to demonstrate how to properly fit both the Locality Sensitive Hashing schemeand the Shotgun and Assembly scheme to our framework for similarity search on the GPU.Extensive experiments on different real-life datasets demonstrate the efficiency andeffectiveness of our framework. The implemented system has been released as opensource1.,*,*,*
ICDM 2005 Workshop Proposal,Katy Borner; Alon Halevy; H Jagadish; Munindar Singh,Recent advances in high performance computing; high speed and high bandwidthcommunication; massive storage; and software (eg; web services) that can be remotelyinvoked on the Internet present unprecedented opportunities in data-driven knowledgeacquisition in a broad range of applications in virtually all areas of human endeavorincluding collaborative cross-disciplinary discovery in e-science; bioinformatics; e-government; environmental informatics; health informatics; security informatics; e-business;education; social informatics; among others. Given the explosive growth in the number anddiversity of potentially useful information sources in many domains; there is an urgent needfor sound approaches to integrative and collaborative analysis and interpretation ofdistributed; autonomous (and hence; inevitably semantically heterogeneous) data …,*,*,*
A Theorem on Signal Reconstruction from Partial Sums,ND Sidiropoulos; HV Jagadish; C Faloutsos,*,*,*,*
Visualizing and Analyzing Metabolomic Data Using MetScape plugin for Cytoscape,Jing Gao; Alla Karnovsky; Glenn Tarcea; Christopher Beecher; Charles Burant; Terry Weymouth; Aaron Bookvich; James Cavalcoli; Barbara Mirel; Brian Athey; Gilbert Omenn; HV Jagadish,Abstract MetScape is a Cytoscape< http://cytoscape. org> plugin which is used to visualizeand analyze metabolomic data. A number of public sources contain information abouthuman metabolic networks consisting of compounds; chemical reactions; pathways;enzymes and genes like KEGG; and EHMN. In MetScape; we use EHMN; a high-qualityhuman metabolic network manually reconstructed by integrating genome annotationinformation from different databases and metabolic reaction information from literature (Maet al. 2007); which contains almost 3000 reactions involved in about 70 pathways. MetScapeallows users to visualize networks of compounds and display related information aboutreactions; enzymes; and pathways. In order to identify and quantify metabolites that arepresent in a cell or tissue at a given moment and under particular conditions; MetScape …,*,*,*
Approximate Substring Indexing,HV Jagadish; Nick Koudas; S Muthukrishnan; Divesh Srivastava,*,*,*,*
ICDE 1999 Program Committee Members,Karl Aberer; Nabil Adam; Peter Apers; Roger Barga; Phil Bernstein; Elisa Bertino; Bharat Bhargava; Luc Bouganim; Mic Bowman; Silvana Castano; Sang K Cha; Surajit Chaudhuri; Ming-Syan Chen; King Chen; David W Cheung; Panos Chrysanthis; Anindya Datta; Umeshwar Dayal; Suzanne Dietrich; Asuman Dogac; Guozhu Dong; Pamela Drew; Maggie Dunham; Frank Eliassen; Georgios Evangelidis; Mike Franklin; Dieter Gawlick; Luis Gravano; Jiawei Han; Lilian Harada; Wilhelm Hasselbring; Paula Hawthorn; Meichun Hsu; Svein-Olaf Hvasshovd; HV Jagadish; Sushi Jajodia; Christian Jensen; Manfred Jeusfeld; Anant Jhingran; Vipul Kashyap,*,*,*,*
Data Engineering,Richard L Cole; Mark J Anderson; Robert J Bestgen; A Chen; YF Kao; M Pong; D Shak; S Sharma; J Vaishnav; H Zeller,*,Urbana,*,*
Paper Number 510,Flip Korn; HV Jagadish; Christos Faloutsos,*,*,*,*
Semantic Compression and Pattern Extraction with,HV Jagadish; U Michigan; Ann Arbor,*,*,*,*
Св м в и зи в в Ц ви Х и г иг УЦЦ Шжг зз в,Cui Yu; Beng Chin Ooi; Kian-Lee Tan; HV Jagadish,Abstract In this paper; we present an efficient method; called iDistance; for K-nearestneighbor (KNN) search in a high-dimensional space. iDistance partitions the data andselects a reference point for each partition. The data in each cluster are transformed into asingle dimensional space based on their similarity with respect to a reference point. Thisallows the points to be indexed using a BЗ-tree structure and KNN search be performedusing onedimensional range search. The choice of partition and reference point providesthe iDistance technique with degrees of freedom most other techniques do not have. Wedescribe how appropriate choices here can effectively adapt the index structure to the datadistribution. We conducted extensive experiments to evaluate the iDistance technique; andreport results demonstrating its effectiveness. l Introduction Many emerging database …,*,*,*
Browsing Singapore-MIT Alliance (SMA) by Title Research and Teaching Output of the MIT Community,Young-Su Lee; Marco Buongiorno Nardelli; Nicola Marzari; Stephen C Graves; HV Jagadish; Beng Chin Ooi; Martin C Rinard; Quang Hieu Vu; Ping Josephine Xu; Russell Allgor; Kyle Jensen; Gregory Stephanopoulos; Jianping Xie; Jim Yang Lee; Daniel IC Wang; Yen Peng Ting; Kunal Agrawal; Saman P Amarasinghe; Weng Fai Wong; Yang Ding; Dimitris J Bertsimas; Constantine Caramanis; Rensheng Deng; Chi-Hwa Wang; Kenneth A Smith; Kiok Lim Tan; Boo Cheong Khoo; Jacob K White; Y Liu; Han Tong Loh; Shu Beng Tor; Yong Zhang; Hao Tan; Yi Li; HQ Nguyen; R Krishnan; KW Choi; Carl V Thompson; FY Lim; Hong-Kiat Tan; Miranda GS Yap; Tan Hong Kiat; NC Nguyen; Guirong Liu; Anthony T Patera,We determined the Landauer ballistic conductance of pristine nanotubes at finite temperaturevia a novel scheme that combines ab-initio molecular dynamics; maximally-localized Wannierfunctions; and a tight-binding formulation … We developed and implemented a first-principlesbased theory of the Landauer ballistic conductance; to determine the transport properties of nanostructuresand molecular-electronics devices. Our approach starts from a … We developed and implementeda first-principles based theory of the Landauer ballistic conductance; to determine the transportproperties of nanostructures and molecular-electronics devices. Our approach starts from a… We report on an industrial project in which we developed an inventory model to providedecision support for the design and deployment of the field service support system for a remanufacturableproduct. The product was a … We propose a balanced tree structure overlay on a …,*,*,*
Com pressed Accessibility M ap,Ting Yu; Divesh Srivastava; Laks VS Lakshmanan; HV Jagadish,*,*,*,*
Linear Clustering of Objects with Multiple Atributes.</title,HV Jagadish; Tian Zhang; Raghu Ramakrishnan; Miron Livny; HV Jagadish; Laks VS Lakshmanan; Divesh Srivastava; Keith Thompson,*,*,*,*
DaNaLIX: for Querying XML,Yunyao Li; Ishan Chaudhuri; Huahai Yang; Satinder Singh; HV Jagadish,ABSTRACT We present DaNaLIX; a prototype domain-adaptive natural language interfacefor querying XML. Our system is an extension of NaLIX; a generic natural language interfacefor querying XML [4; 5]. While retaining the portability of a purely generic system like NaLIX;DaNaLIX can exploit domain-specific knowledge; whenever available; to its advantage forquery translation. More importantly; in DaNaLIX such domain-specific knowledge does nothave to be pre-defined; instead it can be automatically obtained from the interactionsbetween a user and the system. In this demonstration; we describe the overall architecture ofDaNaLIX. We also demonstrate how a generic system like DaNaLIX can take advantage ofdomain-specific knowledge to improve its usability and query translation accuracy. Inaddition; we show DaNaLIX still possesses the portability of a generic system by using …,Ann Arbor,*,*
Recovering Information from Summary Data,HV Jagadish; ND Sidiropoulost,Abstract Data is often stored in summarized form; as a histogram of aggregates (COUNTS;SUMS; or AVeraGes) over specified ranges. We study how to estimate the original detaildata from the stored summary. We formulate this task as an inverse problem; specifying awell-defined cost function that has to be optimized under constraints. We show that ourformulation includes the uniformity and independence assumptions as a special case; andthat it can achieve better reconstruction results if we maximize the smoothness as opposedto the uniformity. In our experiments on real and synthetic datasets; the proposed methodalmost consistently outperforms its competitor; improving the rootmean-square error by up to20 per cent for stock price data; and up to 90 per cent for smoother data sets.,*,*,*
Data Engineering,Bin Liu; Laura Chiticariu; Vivian Chu; HV Jagadish; Frederick R Reiss; Anno Langen; Jayant Madhavan; Rod McChesney; Rebecca Shapley; Warren Shen; Jonathan Goldberg-Kidon,The SMDB Workshop series sponsored by the IEEE TCDE Workgroup on Self-ManagingDatabase Systems brings together researchers and practitioners to exchange ideas relatedto autonomic data management systems. Previous workshops of the SMDB series focusedon core topics in self-managing databases like physical design tuning; problem diagnosisand recovery; and database integration and protection. In addition to core topics; the 2010workshop aimed to broaden the interest range by covering emerging research areas likeCloud computing; multitenant databases; large-scale storage systems; and datacenteradministration.,*,*,*
String...,Luis Gravano; Panagiotis G Ipeirotis; HV Jagadish; Nick Koudas; S Muthukrishnan; Divesh Srivastava,Abstract String data is ubiquitous; and its management has taken on particular importance inthe past few years. Approximate queries are very important on string data especially formore complex queries involving joins. This is due; for example; to the prevalence oftypographical errors in data; and multiple conventions for recording attributes such as nameand address. Commercial databases do not support approximate string joins directly; and itis a challenge to implement this functionality efficiently with user-defined functions (UDFs). Inthis paper; we develop a technique for building approximate string join capabilities on top ofcommercial databases by exploiting facilities already available in them. At the core; ourtechnique relies on matching short substrings of length; called-grams; and taking intoaccount both positions of individual matches and the total number of such matches. Our …,*,*,*
KDD-98 Organization,Gregory Piatetsky-Shapiro; Rakesh Agrawal; Paul Stolorz; Foster Provost; Padhraic Smyth; Ronny Kohavi; Ismail Parsa; David Jensen; Kyusoek Shim; Ramasamy Uthurusamy; Alexander Gray; Tej Anand; Chid Apte; Roberto Bayardo; Carla Brodley; Wray Buntine; Michael Burl; Soumen Chakrabarti; Ernest Chan; Surajit Chaudhuri; Corinna Cortes; Bruce Croft; Umeshwar Dayal; Pedro Domingos; Sue Dumais; William Eddy; Charles Elkan; Christos Faloutsos; Tom Fawcett; Usama M Fayyad; Ronen Feldman; Stephen Gallant; Clark Glymour; Moises Goldszmidt; Georges Grinstein; Dimitrios Gunopulos; Jiawei Han; David Hand; David Heckerman; Tomas Imielinski; Yannis Ioannidis; HV Jagadish; George H John; Pete Johnson; Michael Jordan; Daniel Keim; Hans-Peter Kriegel; TY Lin; Peter Lockemann; Hongjun Lu; Neil Mackin; David Madigan; Heikki Mannila; Brij Masand,Page 1. KDD-98 Organization General Conference Chair Gregory Piatetsky-Shapiro; KnowledgeStream Partners Program Cochairs Rakesh Agrawal; IBM Almaden Research Center Paul Stolorz;Jet Propulsion Laboratory Publicity Chair Foster Provost; Bell Atlantic Science and TechnologyTutorial Chair Padhraic Smyth; University of California; Irvine Panel Chair Willi Kloesgen; GMD;Germany Workshops Chair Ronny Kohavi; Silicon Graphics Exhibits Chair Ismail Parsa; EpsilonPoster Sessions Chair David Jensen; University of Massachusetts; Amherst Local ArrangementsChair Kyusoek Shim; Bell Laboratories Sponsorship Chair Ramasamy Uthurusamy; General MotorsCorporation Conference Webmaster Alexander Gray Program …,*,*,*
Bulletin of the Technical Committee on,Gonzalo Navarro; Ricardo Baeza-Yates; Erkki Sutinen; Jorma Tarhio; Panagiotis G Ipeirotis; HV Jagadish; Nick Koudas; S Muthukrishnan; Lauri Pietarinen; Divesh Srivastava; Sriram Raghavan; Héctor Garcıa-Molina,Bulletin of the Technical Committee on Ø Ò Ò Ö Ò December 2001 Vol. 24 No. 4 IEEE ComputerSociety Letters Letter from the Editor-in-Chief...................................................... David Lomet 1 NewTCDE Chair for 2002-2003........................ Paul Larson; Masaru Kitsuregawa; Betty Salzberg 2Letter from the Special Issue Editor.................................................. Luis Gravano 2 Special Issueon Text and Databases DB2 Optimization in Support of Full Text … Editorial BoardEditor-in-Chief David B. Lomet Microsoft Research One Microsoft Way; Bldg. 9 Redmond WA98052-6399 lomet@ microsoft. com Associate Editors Luis Gravano Computer Science DepartmentColumbia University 1214 Amsterdam Avenue New York; NY 10027 Alon Halevy University ofWashington Computer Science and Engineering Dept. Sieg Hall; Room 310 Seattle; WA 98195Sunita Sarawagi School of Information Technology Indian Institute of Technology …,*,*,*
Bulletin of the Technical Committee on,William DuMouchel Barbará; Christos Faloutsos; Peter J Haas; Joseph M Hellerstein; Yannis Ioannidis; HV Jagadish; Theodore Johnson; Raymond Ng; Viswanath Poosala; Kenneth A Ross; Kenneth C Sevcik,*,*,*,*
Workshop on Data; Text; Web; and Social Network Mining,Dragomir Radev; HV Jagadish; Farnam Jahanian; Raghu Ramakrishnan,The Temporal-Informatics research group (http://www. cond. org) seeks to understand andenhance interactions with temporally varying; Web-scale information.“Understanding”involves the study; and modeling; of the evolution of information both in terms of the contentitself and the production and consumption behaviors of large populations. To enhanceinteractions we implement solutions that provide better ways to access and manipulatedynamic information. Research methodologies range from text and log mining; informationretrieval; human-computer interaction; network analysis and visualization (with some sidetrips into security and peer-to-peer systems).,*,*,*
