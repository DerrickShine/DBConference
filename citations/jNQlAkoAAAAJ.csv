Task search in a human computation market,Lydia B Chilton; John J Horton; Robert C Miller; Shiri Azenkot,Abstract In order to understand how a labor market for human computation functions; it isimportant to know how workers search for tasks. This paper uses two complementarymethods to gain insight into how workers search for tasks on Mechanical Turk. First; weperform a high frequency scrape of 36 pages of search results and analyze it by looking atthe rate of disappearance of tasks across key ways Mechanical Turk allows workers to sorttasks. Second; we present the results of a survey in which we paid workers for self-reportedinformation about how they search for tasks. Our main findings are that on a large scale;workers sort by which tasks are most recently posted and which have the largest number oftasks available. Furthermore; we find that workers look mostly at the first page of the mostrecently posted tasks and the first two pages of the tasks with the most available instances …,Proceedings of the ACM SIGKDD workshop on human computation,2010,135
Touch behavior with different postures on soft smartphone keyboards,Shiri Azenkot; Shumin Zhai,Abstract Text entry on smartphones is far slower and more error-prone than on traditionaldesktop keyboards; despite sophisticated detection and auto-correct algorithms. Tostrengthen the empirical and modeling foundation of smartphone text input improvements;we explore touch behavior on soft QWERTY keyboards when used with two thumbs; anindex finger; and one thumb. We collected text entry data from 32 participants in a lab studyand describe touch accuracy and precision for different keys. We found that distinct patternsexist for input among the three hand postures; suggesting that keyboards should adapt todifferent postures. We also discovered that participants' touch precision was relatively highgiven typical key dimensions; but there were pronounced and consistent touch offsets thatcan be leveraged by keyboard algorithms to correct errors. We identify patterns in our …,Proceedings of the 14th international conference on Human-computer interaction with mobile devices and services,2012,110
Input finger detection for nonvisual touch screen text entry in Perkinput,Shiri Azenkot; Jacob O Wobbrock; Sanjana Prasain; Richard E Ladner,Abstract We present Input Finger Detection (IFD); a novel technique for nonvisual touchscreen input; and its application; the Perkinput text entry method. With IFD; signals are inputinto a device with multi-point touches; where each finger represents one bit; either touchingthe screen or not. Maximum likelihood and tracking algorithms are used to detect whichfingers touch the screen based on user-set reference points. The Perkinput text entry methoduses the 6-bit Braille encoding with audio feedback; enabling one-and two-handed input. Alongitudinal evaluation with 8 blind participants who are proficient in Braille showed that one-handed Perkinput was significantly faster and more accurate than iPhone's VoiceOver.Furthermore; in a case study to evaluate expert performance; one user reached an averagesession speed of 17.56 words per minute (WPM) with an average uncorrected error rate …,Proceedings of Graphics Interface 2012,2012,102
Improving public transit accessibility for blind riders by crowdsourcing bus stop landmark locations with google street view: An extended analysis,Kotaro Hara; Shiri Azenkot; Megan Campbell; Cynthia L Bennett; Vicki Le; Sean Pannella; Robert Moore; Kelly Minckler; Rochelle H Ng; Jon E Froehlich,Abstract Low-vision and blind bus riders often rely on known physical landmarks to helplocate and verify bus stop locations (eg; by searching for an expected shelter; bench; ornewspaper bin). However; there are currently few; if any; methods to determine thisinformation a priori via computational tools or services. In this article; we introduce andevaluate a new scalable method for collecting bus stop location and landmark descriptionsby combining online crowdsourcing and Google Street View (GSV). We conduct and reporton three studies:(i) a formative interview study of 18 people with visual impairments to informthe design of our crowdsourcing tool;(ii) a comparative study examining differences betweenphysical bus stop audit data and audits conducted virtually with GSV; and (iii) an onlinestudy of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of …,ACM Transactions on Accessible Computing (TACCESS),2015,68
Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis,Kotaro Hara; Shiri Azenkot; Megan Campbell; Cynthia L Bennett; Vicki Le; Sean Pannella; Robert Moore; Kelly Minckler; Rochelle H Ng; Jon E Froehlich,Abstract Low-vision and blind bus riders often rely on known physical landmarks to helplocate and verify bus stop locations (eg; by searching for an expected shelter; bench; ornewspaper bin). However; there are currently few; if any; methods to determine thisinformation a priori via computational tools or services. In this article; we introduce andevaluate a new scalable method for collecting bus stop location and landmark descriptionsby combining online crowdsourcing and Google Street View (GSV). We conduct and reporton three studies:(i) a formative interview study of 18 people with visual impairments to informthe design of our crowdsourcing tool;(ii) a comparative study examining differences betweenphysical bus stop audit data and audits conducted virtually with GSV; and (iii) an onlinestudy of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of …,ACM Transactions on Accessible Computing (TACCESS),2015,68
Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis,Kotaro Hara; Shiri Azenkot; Megan Campbell; Cynthia L Bennett; Vicki Le; Sean Pannella; Robert Moore; Kelly Minckler; Rochelle H Ng; Jon E Froehlich,Abstract Low-vision and blind bus riders often rely on known physical landmarks to helplocate and verify bus stop locations (eg; by searching for an expected shelter; bench; ornewspaper bin). However; there are currently few; if any; methods to determine thisinformation a priori via computational tools or services. In this article; we introduce andevaluate a new scalable method for collecting bus stop location and landmark descriptionsby combining online crowdsourcing and Google Street View (GSV). We conduct and reporton three studies:(i) a formative interview study of 18 people with visual impairments to informthe design of our crowdsourcing tool;(ii) a comparative study examining differences betweenphysical bus stop audit data and audits conducted virtually with GSV; and (iii) an onlinestudy of 153 crowd workers on Amazon Mechanical Turk to examine the feasibility of …,ACM Transactions on Accessible Computing (TACCESS),2015,68
Enhancing independence and safety for blind and deaf-blind public transit riders,Shiri Azenkot; Sanjana Prasain; Alan Borning; Emily Fortuna; Richard E Ladner; Jacob O Wobbrock,Abstract Blind and deaf-blind people often rely on public transit for everyday mobility; butusing transit can be challenging for them. We conducted semi-structured interviews with 13blind and deaf-blind people to understand how they use public transit and what humanvalues were important to them in this domain. Two key values were identified: independenceand safety. We developed GoBraille; two related Braille-based applications that provideinformation about buses and bus stops while supporting the key values. GoBraille is built onMoBraille; a novel framework that enables a Braille display to benefit from many features ina smartphone without knowledge of proprietary; device-specific protocols. Finally; weconducted user studies with blind people to demonstrate that GoBraille enables people totravel more independently and safely. We also conducted co-design with a deaf-blind …,Proceedings of the SIGCHI conference on Human Factors in computing systems,2011,67
PassChords: secure multi-touch authentication for blind people,Shiri Azenkot; Kyle Rector; Richard Ladner; Jacob Wobbrock,Abstract Blind mobile device users face security risks such as inaccessible authenticationmethods; and aural and visual eavesdropping. We interviewed 13 blind smartphone usersand found that most participants were unaware of or not concerned about potential securitythreats. Not a single participant used optional authentication methods such as a password-protected screen lock. We addressed the high risk of unauthorized user access bydeveloping PassChords; a non-visual authentication method for touch surfaces that is robustto aural and visual eavesdropping. A user enters a PassChord by tapping several times on atouch surface with one or more fingers. The set of fingers used in each tap defines thepassword. We give preliminary evidence that a four-tap PassChord has about the sameentropy; a measure of password strength; as a four-digit personal identification number …,Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility,2012,51
Exploring the use of speech input by blind people on mobile devices,Shiri Azenkot; Nicole B Lee,Abstract Much recent work has explored the challenge of nonvisual text entry on mobiledevices. While researchers have attempted to solve this problem with gestures; we explore adifferent modality: speech. We conducted a survey with 169 blind and sighted participants toinvestigate how often; what for; and why blind people used speech for input on their mobiledevices. We found that blind people used speech more often and input longer messagesthan sighted people. We then conducted a study with 8 blind people to observe how theyused speech input on an iPod compared with the on-screen keyboard with VoiceOver. Wefound that speech was nearly 5 times as fast as the keyboard. While participants were mostlysatisfied with speech input; editing recognition errors was frustrating. Participants spent anaverage of 80.3% of their time editing. Finally; we propose challenges for future work …,Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility,2013,40
BraillePlay: educational smartphone games for blind children,Lauren R Milne; Cynthia L Bennett; Richard E Ladner; Shiri Azenkot,Abstract There are many educational smartphone games for children; but few are accessibleto blind children. We present BraillePlay; a suite of accessible games for smartphones thatteach Braille character encodings to promote Braille literacy. The BraillePlay games arebased on VBraille; a method for displaying Braille characters on a smartphone. BraillePlayincludes four games of varying levels of difficulty: VBReader and VBWriter simulate Brailleflashcards; and VBHangman and VBGhost incorporate Braille character identification andrecall into word games. We evaluated BraillePlay with a longitudinal study in the wild witheight blind children. Through logged usage data and extensive interviews; we found that allbut one participant were able to play the games independently and found them enjoyable.We also found evidence that some children learned Braille concepts. We distill …,Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility,2014,28
Smartphone haptic feedback for nonvisual wayfinding,Shiri Azenkot; Richard E Ladner; Jacob O Wobbrock,Abstract We explore using vibration on a smartphone to provide turn-by-turn walkinginstructions to people with visual impairments. We present two novel feedback methodscalled Wand and ScreenEdge and compare them to a third method called Pattern. We built aprototype and conducted a user study where 8 participants walked along a pre-programmedroute using the 3 vibration feedback methods and no audio output. Participants interpretedthe feedback with an average error rate of just 4 percent. Most preferred the Pattern method;where patterns of vibrations indicate different directions; or the ScreenEdge method; whereareas of the screen correspond to directions and touching them may induce vibration.,The proceedings of the 13th international ACM SIGACCESS conference on Computers and accessibility,2011,27
How Blind People Interact with Visual Content on Social Networking Services,Violeta Voykinska; Shiri Azenkot; Shaomei Wu; Gilly Leshed,Abstract In this paper; we explore blind peopleâ s motivations; challenges; interactions; andexperiences with visual content on Social Networking Services (SNSs). We present findingsfrom an interview study of 11 individuals and a survey study of 60 individuals; all with little tono functional vision. Compared to sighted SNS users; our blind participants faced profoundaccessibility challenges; including the prevalence of photos without sufficient textdescriptions. To overcome the challenges; they developed creative strategies; includingusing a variety of methods to access SNS features (eg; opening the mobile site on a desktopbrowser); and inferring photo content from textual cues and social interactions. Whenstrategies failed; participants reached out for help from trusted friends; or avoided certainfeatures. We discuss our findings in the context of CSCW research and SNS accessibility …,Computer Supported Cooperative Work (CSCW),2016,23
Improving public transit usability for blind and deaf-blind people by connecting a braille display to a smartphone,Shiri Azenkot; Emily Fortuna,Abstract We conducted interviews with blind and deaf-blind people to understand how theyuse the public transit system. In this paper; we discuss key challenges our participants facedand present a tool we developed to alleviate these challenges. We built this tool onMoBraille; a novel framework that enables a Braille display to benefit from many features inan Android phone without knowledge of proprietary; device-specific protocols. Weconducted participatory design with a deaf-blind person and describe the lessons learnedabout designing an interface for a deaf-blind person.,Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility,2010,20
DigiTaps: eyes-free number entry on touchscreens with minimal audio feedback,Shiri Azenkot; Cynthia L Bennett; Richard E Ladner,Abstract Eyes-free input usually relies on audio feedback that can be difficult to hear in noisyenvironments. We present DigiTaps; an eyes-free number entry method for touchscreendevices that requires little auditory attention. To enter a digit; users tap or swipe anywhereon the screen with one; two; or three fingers. The 10 digits are encoded by combinations ofthese gestures that relate to the digits' semantics. For example; the digit 2 is input with a 2-finger tap. We conducted a longitudinal evaluation with 16 people and found that DigiTapswith no audio feedback was faster but less accurate than with audio feedback after everyinput. Throughout the study; participants entered numbers with no audio feedback at anaverage rate of 0.87 characters per second; with an uncorrected error rate of 5.63%.,Proceedings of the 26th annual ACM symposium on User interface software and technology,2013,18
Foresee: A customizable head-mounted vision enhancement system for people with low vision,Yuhang Zhao; Sarit Szpiro; Shiri Azenkot,Abstract Most low vision people have functional vision and would likely prefer to use theirvision to access information. Recently; there have been advances in head-mounteddisplays; cameras; and image processing technology that create opportunities to improvethe visual experience for low vision people. In this paper; we present ForeSee; a head-mounted vision enhancement system with five enhancement methods: Magnification;Contrast Enhancement; Edge Enhancement; Black/White Reversal; and Text Extraction; intwo display modes: Full and Window. ForeSee enables users to customize their visualexperience by selecting; adjusting; and combining different enhancement methods anddisplay modes in real time. We evaluated ForeSee by conducting a study with 19 low visionparticipants who performed near-and far-distance viewing tasks. We found that …,Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility,2015,17
Octopus: evaluating touchscreen keyboard correction and recognition algorithms via,Xiaojun Bi; Shiri Azenkot; Kurt Partridge; Shumin Zhai,Abstract The time and labor demanded by a typical laboratory-based keyboard evaluationare limiting resources for algorithmic adjustment and optimization. We propose Remulation;a complementary method for evaluating touchscreen keyboard correction and recognitionalgorithms. It replicates prior user study data through real-time; on-device simulation. Wehave developed Octopus; a Remulation-based evaluation tool that enables keyboarddevelopers to efficiently measure and inspect the impact of algorithmic changes withoutconducting resource-intensive user studies. It can also be used to evaluate third-partykeyboards in a" black box" fashion; without access to their algorithms or source code.Octopus can evaluate both touch keyboards and word-gesture keyboards. Two empiricalexamples show that Remulation can efficiently and effectively measure many aspects of …,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2013,13
Tapulator: A non-visual calculator using natural prefix-free codes,Vaspol Ruamviboonsuk; Shiri Azenkot; Richard E Ladner,Abstract A new non-visual method of numeric entry into a smartphone is designed;implemented; and tested. Users tap the smartphone screen with one to three fingers orswipe the screen in order to enter numbers. No buttons are used--only simple; easy-to-remember gestures. A preliminary valuation with sighted users compares the method to astandard accessible numeric keyboard with a VoiceOver-like screen reader interface for non-visual entry. We found that users entered numbers faster and with higher accuracy with ournumber entry method than with a VoiceOver-like interface; showing there is potential for useamong blind people as well. The Tapulator; a complete calculator based on this non-visualnumeric entry that uses simple gestures for arithmetic operations and other calculatoractions is described.,Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility,2012,13
How people with low vision access computing devices: Understanding challenges and opportunities,Sarit Felicia Anais Szpiro; Shafeka Hashash; Yuhang Zhao; Shiri Azenkot,Abstract Low vision is a pervasive condition in which people have difficulty seeing even withcorrective lenses. People with low vision frequently use mainstream computing devices;however how they use their devices to access information and whether digital low visionaccessibility tools provide adequate support remains understudied. We addressed thesequestions with a contextual inquiry study. We observed 11 low vision participants using theirsmartphones; tablets; and computers when performing simple tasks such as reading email.We found that participants preferred accessing information visually than aurally (eg; screenreaders); and juggled a variety of accessibility tools. However; accessibility tools did notprovide them with appropriate support. Moreover; participants had to constantly performmultiple gestures in order to see content comfortably. These challenges made …,Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility,2016,12
Finding a store; searching for a product: a study of daily challenges of low vision people,Sarit Szpiro; Yuhang Zhao; Shiri Azenkot,Abstract Visual impairments encompass a range of visual abilities. People with low visionhave functional vision and thus their experiences are likely to be different from people withno vision. We sought to answer two research questions:(1) what challenges do low visionpeople face when performing daily activities and (2) what aids (high-and low-tech) do lowvision people use to alleviate these challenges? Our goal was to reveal gaps in currenttechnologies that can be addressed by the UbiComp community. Using contextual inquiry;we observed 11 low vision people perform a wayfinding and shopping task in an unfamiliarenvironment. The task involved wayfinding and searching and purchasing a product. Wefound that; although there are low vision aids on the market; participants mostly used theirsmartphones; despite interface accessibility challenges. While smartphones helped them …,Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,2016,11
Enabling Building Service Robots to Guide Blind People: A Participatory Design Approach,Shiri Azenkot; Catherine Feng; Maya Cakmak,Building service robots-robots that perform various services in buildings-are becoming morecommon in large buildings such as hotels and stores. We aim to leverage such robots toserve as guides for blind people. In this paper; we sought to design specifications that detailhow a building service robot could interact with and guide a blind person through a buildingin an effective and socially acceptable way. We conducted participatory design sessionswith three designers and five non-designers. Two of the designers and all of the non-designers had a vision disability. Primary features of the design include allowing the user to(1) summon the robot after entering the building;(2) choose from three modes of assistance(Sighted Guide; Escort; and Information Kiosk); and (3) receive information about thebuilding's layout from the robot. We conclude with a discussion of themes and a reflection …,In Proceedings of the 11th Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI 2016),2016,10
Using physical signaling to support collaborative mobile search,Jaime Teevan; Meredith Morris; Shiri Azenkot,Abstract Often when people search the web from their phones; they do so collaboratively.We present a mobile application that supports in-person collaborative search by allowingusers to physically signal a willingness to share. While the core application providesstandard mobile search functionality; users rotate their devices to landscape orientation toindicate (to the device and others) they are entering a collaborative mode. We study twouses of collaborative mode; one where users rate results to create a group list; and anotherwhere screens and actions are shared across devices.,Proceedings of the companion publication of the 17th ACM conference on Computer supported cooperative work & social computing,2014,10
CueSee: exploring visual cues for people with low vision to facilitate a visual search task,Yuhang Zhao; Sarit Szpiro; Jonathan Knighten; Shiri Azenkot,Abstract Visual search is a major challenge for low vision people. Conventional visionenhancements like magnification help low vision people see more details; but cannotindicate the location of a target in a visual search task. In this paper; we explore visual cues---a new approach to facilitate visual search tasks for low vision people. We focus on productsearch and present CueSee; an augmented reality application on a head-mounted display(HMD) that facilitates product search by recognizing the product automatically and usingvisual cues to direct the user's attention to the product. We designed five visual cues thatusers can combine to suit their visual condition. We evaluated the visual cues with 12 lowvision participants and found that participants preferred using our cues to conventionalenhancements for product search. We also found that CueSee outperformed participants' …,Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,2016,9
Tickers and Talker: An Accessible Labeling Toolkit for 3D Printed Models,Lei Shi; Idan Zelzer; Catherine Feng; Shiri Azenkot,Abstract Three-dimensional models are important learning resources for blind people. Withadvances in 3D printing; 3D models are becoming more available. However; unlike visual ortactile graphics; there is no standard accessible way to label components in 3D models. Wepresent a labeling toolkit that enables users to add and access audio labels to 3D printedmodels. The toolkit includes Tickers; small 3D printed percussion instruments added to 3Dmodels; and Talker; a signal processing application that detects and classifies Tickersounds. To use the toolkit; a model designer adds Tickers to a model using 3D modelingsoftware. A user then prints the model with Tickers and records audio labels for each Ticker.Finally; users can strum the Tickers and Talker will play the corresponding labels. Weevaluated Tickers and Talker with three models in a study with nine blind participants. Our …,In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'16),2016,9
Designing a robot guide for blind people in indoor environments,Catherine Feng; Shiri Azenkot; Maya Cakmak,Abstract Navigating indoors is challenging for blind people and they often rely on assistancefrom sighted people. We propose a solution for indoor navigation involving multi-purposerobots that will likely reside in many buildings in the future. In this report; we present adesign for how robots can guide blind people to an indoor destination in an effective andsocially-acceptable way. We used participatory design; creating a design team with threedesigners and five non-designers. All but one member of the team had a visual impairment.Our resulting design specifies how the robot and the user initially meet; how the robot guidesthe user through hallways and around obstacles; and how the robot and user conclude theirsession.,Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts,2015,6
Supporting interpersonal interaction during collaborative mobile search,Jaime Teevan; Meredith Ringel Morris; Shiri Azenkot,O-SNAP is a mobile application that explicitly supports in-person collaborative search byenabling users to physically signal their willingness to share and by facilitating face-to-facesearch-related communication. The Web extra at http://youtu. be/AKoITuxB9BY is a video inwhich author Meredith Ringel Morris discusses scenarios that can prompt collaborativeinformation seeking.,Computer,2014,6
Understanding Low Vision People's Visual Perception on Commercial Augmented Reality Glasses,Yuhang Zhao; Michele Hu; Shafeka Hashash; Shiri Azenkot,Abstract People with low vision have a visual impairment that affects their ability to performdaily activities. Unlike blind people; low vision people have functional vision and canpotentially benefit from smart glasses that provide dynamic; always-available visualinformation. We sought to determine what low vision people could see on mainstreamcommercial augmented reality (AR) glasses; despite their visual limitations and the device'sconstraints. We conducted a study with 20 low vision participants and 18 sighted controls;asking them to identify virtual shapes and text in different sizes; colors; and thicknesses. Wealso evaluated their ability to see the virtual elements while walking. We found that lowvision participants were able to identify basic shapes and read short phrases on the glasseswhile sitting and walking. Identifying virtual elements had a similar effect on low vision …,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,2017,5
Magic Touch: Interacting with 3D Printed Graphics,Lei Shi; Ross McLachlan; Yuhang Zhao; Shiri Azenkot,Abstract Graphics like maps and models are important learning materials. With recentlydeveloped projects; we can use 3D printers to make tactile graphics that are moreaccessible to blind people. However; current 3D printed graphics can only convey limitedinformation through their shapes and textures. We present Magic Touch; a computer vision-based system that augments printed graphics with audio files associated with specificlocations; or hotspots; on the model. A user can access an audio file associated with ahotspot by touching it with a pointing gesture. The system detects the user's gesture anddetermines the hotspot location with computer vision algorithms by comparing a video feedof the user's interaction with the digital representation of the model and its hotspots. Toenable MT; a model designer must add a single tracker with fiducial tags to a model. After …,Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility,2016,5
Eyes-free input on mobile devices,Shiri Azenkot,I present new methods and studies that aim to improve eyes-free data entry for blind mobiledevice users. Currently; mobile devices are generally accessible to blind people; but textentry is almost prohibitively slow. Studies show that blind people enter text on an iPhone at arate of just 4 words per minute. I describe Perkinput; a chording text entry method whereusers touch the screen with one to three fingers at a time in patterns based on Braille.Instead of soft keys; Perkinput uses concepts from signal detection theory to determine theuser's input. Based on Perkinput; I developed PassChords; a touchscreen authenticationmethod that has no audio feedback. Unlike current eyes-free input methods; PassChordsdoesn't echo a user's input; so it won't broadcast the user's password for others to hear.Finally; I will discuss another modality for eyes-free input: speech. I conducted a survey …,*,2014,5
Iwalk: a lightweight navigation system for low-vision users,Amanda J Stent; Shiri Azenkot; Ben Stern,Abstract Smart phones typically support a range of GPS-enabled navigation services.However; most navigation services on smart phones are of limited use to people with visualdisabilities. In this paper; we present iWalk; a speech-enabled local search and navigationprototype for people with low vision. iWalk runs on smart phones. It supports speech input;and provides real-time turn-by-turn walking directions in speech and text; using distancesand time-to-turn information in addition to street names so that users are not forced to readstreet signs. In between turns iWalk uses non-speech cues to indicate to the user that s/he is'on-track'.,Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility,2010,4
Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations,Lei Shi; Yuhang Zhao; Shiri Azenkot,Abstract As three-dimensional printers become more available; 3D printed models can serveas important learning materials; especially for blind people who perceive the modelstactilely. Such models can be much more powerful when augmented with audio annotationsthat describe the model and their elements. We present Markit and Talkit; a low-barriertoolkit for creating and interacting with 3D models with audio annotations. Makers (eg;hobbyists; teachers; and friends of blind people) can use Markit to mark model elements andassociate then with text annotations. A blind user can then print the augmented model;launch the Talkit application; and access the annotations by touching the model andfollowing Talkit's verbal cues. Talkit uses an RGB camera and a microphone to sense users'inputs so it can run on a variety of devices. We evaluated Markit with eight sighted" …,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology,2017,2
Designing Interactions for 3D Printed Models with Blind People,Lei Shi; Yuhang Zhao; Shiri Azenkot,Abstract Three-dimensional printed models have the potential to serve as powerfulaccessibility tools for blind people. Recently; researchers have developed methods to furtherenhance 3D prints by making them interactive: when a user touches a certain area in themodel; the model speaks a description of the area. However; these interactive models werelimited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactivemodel technologies and end users' needs; and explore design opportunities. In the firstsection of the study; we observed participants' behavior as they explored and identifiedmodels and their components. In the second section; we elicited user-defined inputtechniques that would trigger various functions from an interactive model. We identified …,Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,2017,2
Touchscreen keyboard simulation for performance evaluation,*,A computerized system sends a series of touchscreen keyboard touch data to a touchscreenkeyboard device that receives the touchscreen keyboard touch data and processes thereceived string of touchscreen keyboard touch data to simulate touches to a touchscreen ofthe touchscreen keyboard device. A touchscreen keyboard algorithm is applied to thesimulated touches; producing a corrected text string. The effectiveness of the touchscreenkeyboard algorithm at producing a correct text string can then be evaluated.,*,2013,2
Ubiquitous text interaction,Keith Vertanen; Kyle Montague; Mark Dunlop; Ahmed Sabbir Arif; Xiaojun Bi; Shiri Azenkot,Computer-based interactions increasingly pervade our everyday environments. Be it on amobile device; a wearable device; a wall-sized display; or an augmented reality device;interactive systems often rely on the consumption; composition; and manipulation of text.The focus of this workshop is on exploring the problems and opportunities of text interactionsthat are embedded in our environments; available all the time; and used by people who maybe constrained by device; situation; or disability. This workshop welcomes all researchersinterested in interactive systems that rely on text input or output. Participants should submit ashort position statement outlining their background; past work; future plans; and suggestinga use-case they would like to explore in-depth during the workshop. During the workshop;small teams will form around common or compelling use-cases. Teams will spend time …,*,2017,1
Reading and Learning Smartfonts,Danielle Bragg; Shiri Azenkot; Adam Tauman Kalai,Abstract As small displays on devices like smartwatches become increasingly common;many people have difficulty reading the text on these displays. Vision conditions likepresbyopia that result in blurry near vision make reading small text particularly hard. Wedesign multiple different scripts for displaying English text; legible at small sizes even whenblurry; for small screens such as smartphones and smartwatches. These" smartfonts"redesign visual character presentations to improve the reading experience. Like cursive;Grade 1 Braille; and ordinary fonts; they preserve orthography and spelling. They have thepotential to enable people to read more text comfortably on small screens; eg; withoutreading glasses. To simulate presbyopia; we blur images and evaluate their legibility usingpaid crowdsourcing. We also evaluate the difficulty of learning to read smartfonts and …,Proceedings of the 29th Annual Symposium on User Interface Software and Technology,2016,1
Automated description generation for indoor floor maps,Devi A Paladugu; Hima Bindu Maguluri; Qiongjie Tian; Baoxin Li,Abstract People with visual impairment generally suffer from diminished freedom innavigating an environment. A practical need is to navigate through unfamiliar indoorenvironments such as school buildings; hotels; etc.; for which commonly-used existing toolslike canes; seeing-eye dogs and GPS devices cannot provide adequate support. Wedemonstrate a prototype system that aims at addressing this practical need. The input to thesystem is the name of the building/establishment supplied by a user; which is used by a webcrawler to determine the availability of a floor map on the corresponding website. Ifavailable; the map is downloaded and used by the proposed system to generate a verbaldescription giving an overview of the locations of key landmarks inside the map with respectto one another. Our preliminary survey and experiments indicate that this is a promising …,Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility,2012,1
Overcoming barriers among Israeli and Palestinian students via computer science,Shiri Azenkot; Theodore Golfinopoulos; Adam Marcus; Alessondra Springmann; Jonathan S Varsanik,Abstract The Middle East Education Through Technology (MEET) program is a non-profitorganization based in Jerusalem; that aims to empower future Israeli and Palestinianleaders by teaching them computer science and business. From the perspective of MEET'sinstructors; this paper describes how MEET uses computer science education to fosterprofessional and personal contact among Israeli and Palestinian high school students; twogroups who otherwise would have little or no interaction with each other. MEET's primarymethod of overcoming the barrier is teamwork: students are divided into groups that includeboth Israelis and Palestinians and are assigned software engineering tasks. We believe thatthe techniques used by MEET can serve as examples for other computer science programsthat overcome barriers between groups in the United States and other countries.,Proceedings of the 42nd ACM technical symposium on Computer science education,2011,1
Designing smartglasses applications for people with low vision,Shiri Azenkot; Yuhang Zhao,Abstract While our community has many active projects involving blind people; low vision israrely addressed. People with low vision have functional vision; but their visual impairmentadversely affects their daily life and it cannot be corrected with glasses or contact lenses.Over the last few years; we have been conducting research with this understudieddemographic: understanding low vision people's needs and designing applications toaddress the challenges they face. In this article; we discuss our ongoing research in thisarea; focusing on designing augmented reality applications for low vision users. We beginthis article by describing low vision and motivating our focus on augmented realityapplications on smartglasses for low vision people. We then provide overviews of threeresearch projects that exemplify our research agenda: a study where we observed low …,ACM SIGACCESS Accessibility and Computing,2017,*
Designing and Evaluating Livefonts,Danielle Bragg; Shiri Azenkot; Kevin Larson; Ann Bessemans; Adam Tauman Kalai,Abstract The emergence of personal computing devices offers both a challenge andopportunity for displaying text: small screens can be hard to read; but also support higherresolution. To fit content on a small screen; text must be small. This small text size can makecomputing devices unusable; in particular to low-vision users; whose vision is notcorrectable with glasses. Usability is also decreased for sighted users straining to read thesmall letters; especially without glasses at hand. We propose animated scripts calledlivefonts for displaying English with improved legibility for all users. Because paper does notsupport animation; traditional text is static. However; modern screens support animation; andlivefonts capitalize on this capability. We evaluate our livefont variations' legibility through acontrolled lab study with low-vision and sighted participants; and find our animated scripts …,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology,2017,*
Collaborative Mobile Interaction,*,Some examples include transitioning between an individual mode and a collaborative modein response to an orientation change of a device. Further; some implementations includeidentifying data to be shared with one or more other devices (eg; co-located devices) in thecollaborative mode. In some examples; the individual mode may be associated with anindividual search; the collaborative mode may be associated with a collaborative search;and the devices may transition between the individual mode and the collaborative mode inresponse to orientation changes.,*,2017,*
Collaborative mobile interaction,*,Some examples include transitioning between an individual mode and a collaborative modein response to an orientation change of a device. Further; some implementations includeidentifying data to be shared with one or more other devices (eg; co-located devices) in thecollaborative mode. In some examples; the individual mode may be associated with anindividual search; the collaborative mode may be associated with a collaborative search;and the devices may transition between the individual mode and the collaborative mode inresponse to orientation changes.,*,2017,*
Key selection of a graphical keyboard based on user input posture,*,In one example; a method includes outputting; by a computing device and for display; agraphical keyboard comprising a plurality of keys; and receiving; by the computing device;an indication of a gesture detected at a presence-sensitive input device. The method furtherincludes determining; by the computing device; an input posture of the gesture at thepresence-sensitive input device; and applying; by the computing device and based at leastin part on the input posture; at least one offset to a location associated with a key from theplurality of keys to determine an offset location associated with the key; and in response toreceiving the indication of the gesture; selecting; by the computing device and based at leastin part on the offset location; the key as a selected key.,*,2016,*
HCIL-2015-14 Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View: An Extended Analysis,K Hara; S Azenkot; M Campbell; C Bennett; V Le; S Pannella; R Moore; K Minckler; R Ng; J Froehlich,The HCIL has a long; rich history of transforming the experience people have with newtechnologies. From understanding user needs; to developing and evaluating thesetechnologies; the lab's faculty; staff; and students have been leading the way in HCIresearch and teaching.,To appear in ACM Transactions on Accessibility,2015,*
The need for research on mobile technologies for people with low-vision.,Shiri Azenkot; Kyle Rector; Richard E. Ladner; Jacob O. Wobbrock,*,Workshop on Mobile Accessibility; ACM CHI 2013,2013,*
In Proceedings of ASSETS 2013; Bellevue; Washington. Received best paper award. HCIL-2013-35,K Hara; S Azenkot; M Campbell; C Bennett; V Le; S Pannella; R Moore; K Minckler; R Ng; J Froehlich,The HCIL has a long; rich history of transforming the experience people have with newtechnologies. From understanding user needs; to developing and evaluating thesetechnologies; the lab's faculty; staff; and students have been leading the way in HCIresearch and teaching.,*,*,*
MoBraille: connecting a Braille display to a smartphone to increase safety and independence for blind and deaf-blind people,Shiri Azenkot; Emily Fortuna,*,*,*,*
