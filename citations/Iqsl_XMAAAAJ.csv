How is the weather tomorrow?: towards a benchmark for the cloud,Carsten Binnig; Donald Kossmann; Tim Kraska; Simon Loesing,Abstract Traditionally; the goal of benchmarking a software system is to evaluate itsperformance under a particular workload for a fixed configuration. The most prominentexamples for evaluating transactional database systems as well as other components on top(such as a application-servers or web-servers) are the various TPC benchmarks. In thispaper we argue that traditional benchmarks (like the TPC benchmarks) are not sufficient foranalyzing the novel cloud services. Moreover; we present some initial ideas how such a newbenchmark should look like that fits better to the characteristics of cloud computing (eg;scalability; pay-per-use and fault-tolerance). The main challenge of such a new benchmarkis to make the reported results comparable because different providers offer differentservices with different capabilities and guarantees.,Proceedings of the Second International Workshop on Testing Database Systems,2009,180
Dictionary-based order-preserving string compression for main memory column stores,Carsten Binnig; Stefan Hildenbrand; Franz Färber,Abstract Column-oriented database systems [19; 23] perform better than traditional row-oriented database systems on analytical workloads such as those found in decision supportand business intelligence applications. Moreover; recent work [1; 24] has shown thatlightweight compression schemes significantly improve the query processing performance ofthese systems. One such a lightweight compression scheme is to use a dictionary in order toreplace long (variable-length) values of a certain domain with shorter (fixedlength) integercodes. In order to further improve expensive query operations such as sorting andsearching; column-stores often use order-preserving compression schemes. In contrast tothe existing work; in this paper we argue that orderpreserving dictionary compression doesnot only pay off for attributes with a small fixed domain size but also for long string …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,120
Reverse query processing,Carsten Binnig; Donald Kossmann; Eric Lo,Generating databases for testing database applications (eg; OLAP or business objects) is adaunting task in practice. There are a number of commercial tools to automatically generatetest databases. These tools take a database schema (table layouts plus integrity constraints)and table sizes as input in order to generate new tuples. However; the databases generatedby these tools are not adequate for testing a database application. If an application query isexecuted against such a synthetic database; then the result of that application query is likelyto be empty or contain weird results; such as a report on the performance of a sales personthat contains negative sales. To solve this problem; this paper proposes a new techniquecalled reverse query processing (RQP). RQP gets a query and a result as input and returnsa possible database instance that could have produced that result for that query. RQP …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,120
QAGen: generating query-aware test databases,Carsten Binnig; Donald Kossmann; Eric Lo; M Tamer Özsu,Abstract Today; a common methodology for testing a database management system (DBMS)is to generate a set of test databases and then execute queries on top of them. However; forDBMS testing; it would be a big advantage if we can control the input and/or the output (eg;the cardinality) of each individual operator of a test query for a particular test case.Unfortunately; current database generators generate databases independent of queries. Asa result; it is hard to guarantee that executing the test query on the generated test databasescan obtain the desired (intermediate) query results that match the test case. In this paper; wepropose a novel way for DBMS testing. Instead of first generating a test database and thenseeing how well it matches a particular test case (or otherwise use a trial-and-errorapproach to generate another test database); we propose to generate a query-aware …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,108
An architecture for compiling udf-centric workflows,Andrew Crotty; Alex Galakatos; Kayhan Dursun; Tim Kraska; Carsten Binnig; Ugur Cetintemel; Stan Zdonik,Abstract Data analytics has recently grown to include increasingly sophisticated techniques;such as machine learning and advanced statistics. Users frequently express these complexanalytics tasks as workflows of user-defined functions (UDFs) that specify each algorithmicstep. However; given typical hardware configurations and dataset sizes; the core challengeof complex analytics is no longer sheer data volume but rather the computation itself; andthe next generation of analytics frameworks must focus on optimizing for this computationbottleneck. While query compilation has gained widespread popularity as a way to tackle thecomputation bottleneck for traditional SQL workloads; relatively little work addresses UDF-centric workflows in the domain of complex analytics. In this paper; we describe a novelarchitecture for automatically compiling workflows of UDFs. We also propose several …,Proceedings of the VLDB Endowment,2015,32
Multi-RQP: generating test databases for the functional testing of OLTP applications,Carsten Binnig; Donald Kossmann; Eric Lo,Abstract OLTP applications usually implement use cases which execute a sequence ofactions whereas each action usually reads or updates only a small set of tuples in thedatabase. In order to automatically test the correctness of the different execution paths of theuse cases implemented by an OLTP application; a set of test cases and test databasesneeds to be created. In this paper; we suggest that a tester specifies a test databaseindividually for each test case using SQL as a declarative test database specificationlanguage. Moreover; we also discuss the design of a database generator which creates atest database based on such a specification. Consequently; our approach allows togenerate a tailor-made test database for each test case and to bundle them together for thetest case execution phase.,Proceedings of the 1st international workshop on Testing database systems,2008,29
The end of slow networks: It's time for a redesign,Carsten Binnig; Andrew Crotty; Alex Galakatos; Tim Kraska; Erfan Zamanian,Abstract The next generation of high-performance networks with remote direct memoryaccess (RDMA) capabilities requires a fundamental rethinking of the design of distributed in-memory DBMSs. These systems are commonly built under the assumption that the networkis the primary bottleneck and should be avoided at all costs; but this assumption no longerholds. For instance; with InfiniBand FDR 4×; the bandwidth available to transfer data acrossthe network is in the same ballpark as the bandwidth of one memory channel. Moreover;RDMA transfer latencies continue to rapidly improve as well. In this paper; we first argue thattraditional distributed DBMS architectures cannot take full advantage of high-performancenetworks and suggest a new architecture to address this problem. Then; we discuss initialresults from a prototype implementation of our proposed architecture for OLTP and OLAP …,Proceedings of the VLDB Endowment,2016,28
RODI: A benchmark for automatic mapping generation in relational-to-ontology data integration,Christoph Pinkel; Carsten Binnig; Ernesto Jiménez-Ruiz; Wolfgang May; Dominique Ritze; Martin G Skjæveland; Alessandro Solimando; Evgeny Kharlamov,Abstract A major challenge in information management today is the integration of hugeamounts of data distributed across multiple data sources. A suggested approach to thisproblem is ontology-based data integration where legacy data systems are integrated via acommon ontology that represents a unified global view over all data sources. However; datais often not natively born using these ontologies. Instead; much data resides in legacyrelational databases. Therefore; mappings that relate the legacy relational data sources tothe ontology need to be constructed. Recent techniques and systems that automaticallyconstruct such mappings have been developed. The quality metrics of these systems are;however; often only based on self-designed benchmarks. This paper introduces a newpublicly available benchmarking suite called RODI; which is designed to cover a wide …,European Semantic Web Conference,2015,26
Testing database applications,Carsten Binnig; Donald Kossmann; Eric Lo,Abstract Testing database application is challenging because most methods and toolsdeveloped for application testing do not consider the database state during the test. In thispaper we demonstrate three different tools for testing database applications: HTDGen;HTTrace and HTPar. HTDGen generates meaningful test databases for databaseapplications. HTTrace executes database applications testing efficiently and HTPar extendsHTTrace to run tests in parallel.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,26
IncMap: pay as you go matching of relational schemata to OWL ontologies.,Christoph Pinkel; Carsten Binnig; Evgeny Kharlamov; Peter Haase,Abstract. Ontology Based Data Access (OBDA) enables access to relational data with acomplex structure through ontologies as conceptual domain models. A key component of anOBDA system are mappings between the schematic elements in the ontology and theircorrespondences in the relational schema. Today; in existing OBDA systems thesemappings typically need to be compiled by hand; which is a complex and labor intensivetask. In this paper we address the problem of creating such mappings and present IncMap; asystem that supports a semi-automatic approach for matching relational schemata andontologies. Our approach is based on a novel matching technique that represents theschematic elements of an ontology and a relational schema in a unified way. IncMap isdesigned to work in a query-driven; pay as you go fashion and leverages partial; user …,OM,2013,25
IncMap: pay as you go matching of relational schemata to OWL ontologies.,Christoph Pinkel; Carsten Binnig; Evgeny Kharlamov; Peter Haase,Abstract. Ontology Based Data Access (OBDA) enables access to relational data with acomplex structure through ontologies as conceptual domain models. A key component of anOBDA system are mappings between the schematic elements in the ontology and theircorrespondences in the relational schema. Today; in existing OBDA systems thesemappings typically need to be compiled by hand; which is a complex and labor intensivetask. In this paper we address the problem of creating such mappings and present IncMap; asystem that supports a semi-automatic approach for matching relational schemata andontologies. Our approach is based on a novel matching technique that represents theschematic elements of an ontology and a relational schema in a unified way. IncMap isdesigned to work in a query-driven; pay as you go fashion and leverages partial; user …,OM,2013,25
A framework for testing DBMS features,Eric Lo; Carsten Binnig; Donald Kossmann; M Tamer Özsu; Wing-Kai Hon,Abstract Testing a specific feature of a DBMS requires controlling the inputs and outputs ofthe operators in the query execution plan. However; that is practically difficult to achievebecause the inputs/outputs of a query depend on the content of the test database. In thispaper; we propose a framework to test DBMS features. The framework includes a databasegenerator called QAGen so that the generated test databases are able to meet the testrequirements defined on the test queries. The framework also includes a set of tools toautomate test case constructions and test executions. A wide range of DBMS feature testingtasks can be facilitated by the proposed framework.,The VLDB Journal,2010,25
How to best find a partner? An evaluation of editing approaches to construct R2RML mappings,Christoph Pinkel; Carsten Binnig; Peter Haase; Clemens Martin; Kunal Sengupta; Johannes Trame,Abstract R2RML defines a language to express mappings from relational data to RDF. Thatway; applications built on top of the W3C Semantic Technology stack can seamlesslyintegrate relational data. A major obstacle to using R2RML; though; is the effort for manuallycurating the mappings. In particular in scenarios that aim to map data from huge andcomplex relational schemata (eg;[5]) to more abstract ontologies efficient ways to support themapping creation are needed. In previous work we presented a mapping editor that aims toreduce the human effort in mapping creation [12]. While assisting users in mappingconstruction the editor imposed a fixed editing approach; which turned out to be not optimalfor all users and all kinds of mapping tasks. Most prominently; it is unclear on which of thetwo data models users should best start with the mapping construction. In this paper; we …,European Semantic Web Conference,2014,22
Query Processing on Encrypted Data in the Cloud by,Stefan Hildenbrand; Donald Kossmann; Tahmineh Sanamrad; Carsten Binnig; Franz Faerber; Johannes Woehler,Abstract—This paper explores a new encryption technique called POP. POP addresses theneed to encrypt databases in the cloud and to execute complex SQL queries on theencrypted data efficiently. POP can be configured to meet different privacy requirements andattacker scenarios. Two such scenarios; referred to as domain attack and frequency attack;are studied in detail in this paper. Privacy and performance experiments conducted usingthe TPC-H benchmark show that POP makes it indeed possible to achieve good privacy withaffordable performance overheads in many cases.,Technical report,2011,22
Vizdom: Interactive analytics through pen and touch,Andrew Crotty; Alex Galakatos; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Machine learning (ML) and advanced statistics are important tools for drawinginsights from large datasets. However; these techniques often require human intervention tosteer computation towards meaningful results. In this demo; we present V izdom; a newsystem for interactive analytics through pen and touch. V izdom's frontend allows users tovisually compose complex workflows of ML and statistics operators on an interactivewhiteboard; and the back-end leverages recent advances in workflow compilationtechniques to run these computations at interactive speeds. Additionally; we are exploringapproximation techniques for quickly visualizing partial results that incrementally refine overtime. This demo will show V izdom's capabilities by allowing users to interactively buildcomplex analytics workflows using real-world datasets.,Proceedings of the VLDB Endowment,2015,21
Towards Automatic Test Database Generation.,Carsten Binnig; Donald Kossmann; Eric Lo,Abstract Testing is one of the most expensive and time consuming activities in the softwaredevelopment cycle. In order to reduce the cost and the time to market; many approaches toautomate certain testing tasks have been devised. Nevertheless; a great deal of testing isstill carried out manually. This paper gives an overview of different testing scenarios andshows how database techniques (eg; declarative specifications and logical dataindependence) can help to optimize the generation of test databases.,IEEE Data Eng. Bull.,2008,19
Locality-aware partitioning in parallel database systems,Erfan Zamanian; Carsten Binnig; Abdallah Salama,Abstract Parallel database systems horizontally partition large amounts of structured data inorder to provide parallel data processing capabilities for analytical workloads in shared-nothing clusters. One major challenge when horizontally partitioning large amounts of datais to reduce the network costs for a given workload and a database schema. A commontechnique to reduce the network costs in parallel database systems is to co-partition tableson their join key in order to avoid expensive remote join operations. However; existingpartitioning schemes are limited in that respect since only subsets of tables in complexschemata sharing the same join key can be co-partitioned unless tables are fully replicated.In this paper we present a novel partitioning scheme called predicate-based referencepartition (or PREF for short) that allows to co-partition sets of tables based on given join …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,18
SQLScript: Efficiently analyzing big enterprise data in SAP HANA,Carsten Binnig; Dhbw Mannheim; Norman May,Abstract: Today; not only Internet companies such as Google; Facebook or Twitter do haveBig Data but also Enterprise Information Systems store an ever growing amount of data(called Big Enterprise Data in this paper). In aclassical SAP system landscape acentral datawarehouse (SAP BW) is used to integrate and analyze all enterprise data. In SAP BW mostof the business logic required for complex analytical tasks (eg; acomplexcurrencyconversion) is implemented in the application layer on top of astandardrelational database. While being independent from the underlying database when usingsuch an architecture; this architecture has twomajor drawbacks when analyzing BigEnterprise Data:(1) algorithms in ABAP do not scale with the amount of data and (2) datashipping is required. To this end; we present anovel programming language called …,*,2013,17
FunSQL: It is time to make SQL functional,Carsten Binnig; Robin Rehrmann; Franz Faerber; Rudolf Riewe,Abstract With the rise of cloud-computing and cloud-scale data management the importanceof shipping the code of an application to its data has increased tremendously. Especiallywhen offering data analytics on top of traditional relational databases as a service in thecloud; new data-centric programming paradigms become necessary. Traditionally; relationaldatabases offer two approaches to ship code close to the data: declarative SQL statementsand imperative stored procedures. While SQL statements can be efficiently optimized andparallelized; stored procedures allow more complex logic that can be efficientlydecomposed. In this paper; we propose a novel functional language which extends SQLcalled FunSQL. FunSQL combines the best of both worlds:(1) it allows applicationsdevelopers to implement more complex application logic as in SQL only;(2) the …,Proceedings of the 2012 Joint EDBT/ICDT Workshops,2012,11
Cost-based fault-tolerance for parallel data processing,Abdallah Salama; Carsten Binnig; Tim Kraska; Erfan Zamanian,Abstract In order to deal with mid-query failures in parallel data engines (PDEs); differentfault-tolerance schemes are implemented today:(1) fault-tolerance in parallel databases istypically implemented in a coarse-grained manner by restarting a query completely when amid-query failure occurs; and (2) modern MapReduce-style PDEs implement a fine-grainedfault-tolerance scheme; which either materializes intermediate results or implements alineage model to recover from mid-query failures. However; neither of these schemes canefficiently handle mixed workloads with both short running interactive queries as well aslong running batch queries nor do these schemes efficiently support a wide range ofdifferent cluster setups which vary in cluster size and other parameters such as the meantime between failures. In this paper; we present a novel cost-based fault-tolerance …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,10
Estimating the impact of unknown unknowns on aggregate query results,Yeounoh Chung; Michael Lind Mortensen; Carsten Binnig; Tim Kraska,Abstract It is common practice for data scientists to acquire and integrate disparate datasources to achieve higher quality results. But even with a perfectly cleaned and merged dataset; two fundamental questions remain:(1) is the integrated data set complete and (2) what isthe impact of any unknown (ie; unobserved) data on query results? In this work; we developand analyze techniques to estimate the impact of the unknown data (aka; unknownunknowns) on simple aggregate queries. The key idea is that the overlap between differentdata sources enables us to estimate the number and values of the missing data items. Ourmain techniques are parameter-free and do not assume prior knowledge about thedistribution. Through a series of experiments; we show that estimating the impact ofunknown unknowns is invaluable to better assess the results of aggregate queries over …,Proceedings of the 2016 International Conference on Management of Data,2016,9
The case for interactive data exploration accelerators (ideas),Andrew Crotty; Alex Galakatos; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Enabling interactive visualization over new datasets at" human speed" is key todemocratizing data science and maximizing human productivity. In this work; we first arguewhy existing analytics infrastructures do not support interactive data exploration and thenoutline the challenges and opportunities of building a system specifically designed forinteractive data exploration. Finally; we present an Interactive Data Exploration Accelerator(IDEA); a new type of system for interactive data exploration that is specifically designed tointegrate with existing data management landscapes and allow users to explore their datainstantly without expensive data preparation costs.,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,8
Making the case for query-by-voice with echoquery,Gabriel Lyons; Vinh Tran; Carsten Binnig; Ugur Cetintemel; Tim Kraska,Abstract Recent advances in automatic speech recognition and natural languageprocessing have led to a new generation of robust voice-based interfaces. Yet; there is verylittle work on using voice-based interfaces to query database systems. In fact; one mighteven wonder who in her right mind would want to query a database system using voicecommands! With this demonstration; we make the case for querying database systems usinga voice-based interface; a new querying and interaction paradigm we call Query-by-Voice(QbV). We will demonstrate the practicality and utility of QbV for relational DBMSs using ausing a proof-of-concept system called EchoQuery. To achieve a smooth and intuitiveinteraction; the query interface of EchoQuery is inspired by casual human-to-humanconversations. Our demo will show that voice-based interfaces present an intuitive means …,Proceedings of the 2016 International Conference on Management of Data,2016,8
Spotgres-parallel data analytics on spot instances,Carsten Binnig; Abdallah Salama; Erfan Zamanian; Muhammad El-Hindi; Sebastian Feil; Tobias Ziegler,Market-based IaaS offers such as Amazon's EC2 Spot Instances represent a cost-efficientway to operate a cluster. Compared to traditional IaaS offers which follow a fixed pricingscheme; the per hour price of Spot Instances changes dynamically; whereas the Spot priceis often significantly less when compared to On-demand and even the Reserved Instances.When deploying a Parallel Data-Processing Engine (PDE) on a cluster of Spot Instances amajor obstacle is to find a bidding strategy that is optimal for a given workload and satisfiesuser constraints such as the maximal budget. Moreover; another obstacle is that existingPDEs implement rigid fault-tolerance schemes which do not adapt to different failure ratesresulting from different bidding strategies. In this paper; we present a novel PDE calledSpotgres that tackles these issues. Spotgres extends a typical PDE architecture by (1) a …,Data Engineering Workshops (ICDEW); 2015 31st IEEE International Conference on,2015,8
Distributed snapshot isolation: global transactions pay globally; local transactions pay locally,Carsten Binnig; Stefan Hildenbrand; Franz Färber; Donald Kossmann; Juchang Lee; Norman May,Abstract Modern database systems employ Snapshot Isolation to implement concurrencycontrol and isolationbecause it promises superior query performance compared to lock-based alternatives. Furthermore; Snapshot Isolation never blocks readers; which is animportant property for modern information systems; which have mixed workloads of heavyOLAP queries and short update transactions. This paper revisits the problem ofimplementing Snapshot Isolation in a distributed database system and makes threeimportant contributions. First; a complete definition of Distributed Snapshot Isolation is given;thereby extending existing definitions from the literature. Based on this definition; a set ofcriteria is proposed to efficiently implement Snapshot Isolation in a distributed system.Second; the design space of alternative methods to implement Distributed Snapshot …,The VLDB Journal,2014,8
Controlling false discoveries during interactive data exploration,Zheguang Zhao; Lorenzo De Stefani; Emanuel Zgraggen; Carsten Binnig; Eli Upfal; Tim Kraska,Abstract Recent tools for interactive data exploration significantly increase the chance thatusers make false discoveries. They allow users to (visually) examine many hypotheses andmake inference with simple interactions; and thus incur the issue commonly known instatistics as the" multiple hypothesis testing error." In this work; we propose a solution tointegrate the control of multiple hypothesis testing into interactive data exploration systems.A key insight is that existing methods for controlling the false discovery rate (such as FDR)are not directly applicable to interactive data exploration. We therefore discuss a set of newcontrol procedures that are better suited for this task and integrate them in our system;QUDE. Via extensive experiments on both real-world and synthetic data sets wedemonstrate how QUDE can help experts and novice users alike to efficiently control …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,7
RODI: Benchmarking relational-to-ontology mapping generation quality,Christoph Pinkel; Carsten Binnig; Ernesto Jiménez-Ruiz; Evgeny Kharlamov; Wolfgang May; Andriy Nikolov; Ana Sasa Bastinos; Martin G Skjæveland; Alessandro Solimando; Mohsen Taheriyan; Christian Heupel; Ian Horrocks,Abstract Accessing and utilizing enterprise or Web data that is scattered across multiple datasources is an important task for both applications and users. Ontology-based dataintegration; where an ontology mediates between the raw data and its consumers; is apromising approach to facilitate such scenarios. This approach crucially relies on usefulmappings to relate the ontology and the data; the latter being typically stored in relationaldatabases. A number of systems to support the construction of such mappings have recentlybeen developed. A generic and effective benchmark for reliable and comparable evaluationof the practical utility of such systems would make an important contribution to thedevelopment of ontology-based data integration systems and their application in practice.We have proposed such a benchmark; called RODI. In this paper; we present a new …,Semantic Web,2016,7
System and method of performing snapshot isolation in distributed databases,*,A system and method of performing snapshot isolation in distributed databases. Each nodestores local snapshot information that enforces snapshot isolation for that node. The methodincludes partially processing a distributed transaction by a first node; receiving a globalcommit identifier from a coordinator; and continuing to process the distributed transaction; bythe first node and a second node; in accordance with the global commit identifier.,*,2015,7
Die SIKOSA-Methodik,Daniel Weiß; Jörn Kaack; Stefan Kirn; Maike Gilliot; Lutz Lowis; Günter Müller; Andrea Herrmann; Carsten Binnig; Timea Illes; Barbara Paech; Donald Kossmann,Kernpunkte Die SIKOSA-Methodik beschreibt einen durchgängigen Lösungsansatz zurSoftwareentwicklung. Die Methodik überführt funktionale und nicht funktionaleProzessanforderungen in entsprechende testbare Unternehmenssoftware-Anforderungenund erhöht die Validität. Die Methodik wird vor dem Hintergrund eines industriellenBeschaffungsprozesses veranschaulicht.,Wirtschaftsinformatik,2007,6
The end of a myth: distributed transactions can scale,Erfan Zamanian; Carsten Binnig; Tim Harris; Tim Kraska,Abstract The common wisdom is that distributed transactions do not scale. But what ifdistributed transactions could be made scalable using the next generation of networks and aredesign of distributed databases? There would no longer be a need for developers to worryabout co-partitioning schemes to achieve decent performance. Application developmentwould become easier as data placement would no longer determine how scalable anapplication is. Hardware provisioning would be simplified as the system administrator canexpect a linear scale-out when adding more machines rather than some complex sub-linearfunction; which is highly application specific. In this paper; we present the design of ournovel scalable database system NAM-DB and show that distributed transactions with thevery common Snapshot Isolation guarantee can indeed scale using the next generation …,Proceedings of the VLDB Endowment,2017,5
Vistrees: fast indexes for interactive data exploration,Muhammad El-Hindi; Zheguang Zhao; Carsten Binnig; Tim Kraska,Abstract Visualizations are arguably the most important tool to explore; understand andconvey facts about data. As part of interactive data exploration; visualizations might be usedto quickly skim through the data and look for patterns. Unfortunately; database systems arenot designed to efficiently support these workloads. As a result; visualizations often take verylong to produce; creating a significant barrier to interactive data analysis. In this paper; wefocus on the interactive computation of histograms for data exploration. To address thisissue; we present a novel multi-dimensional index structure called VisTree. As a keycontribution; this paper presents several techniques to better align the design of multi-dimensional indexes with the needs of visualization tools for data exploration. Ourexperiments show that the VisTree achieves a speed increase of up to three orders of …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,5
Generic node including stored script,*,Methods and apparatus; including computer program products; are provided for providingfor processing calculation plans. In one aspect; there is provided a computer-implementedmethod. The method may include generating a calculation plan including a plurality ofnodes; determining whether at least one of the nodes includes a function node; andcompiling the function node into executable code to enable execution of the plurality ofnodes including the function node at the database. Related apparatus; systems; methods;and articles are also described.,*,2015,5
XDB-A novel Database Architecture for Data Analytics as a Service,Carsten Binnig; Abdallah Salama; Erfan Zamanian; Harald Kornmayer; Sven Listing; Alexander C Mueller,Parallel shared-nothing database systems are major platforms for efficiently analyzing largeamounts of structured data. However; in order to offer SQL-like services for data analytics inthe cloud; providers such as Amazon and Google do not use these systems as a basis. Amajor reason for this trend is that existing parallel shared-nothing database systems areexpensive and that they do not fulfill many of the requirements such as elasticity and fault-tolerance needed for providing a service for data analytics in the cloud. In this paper; wepresent an overview of an elastic and fault-tolerant database system called XDB; whichsupports complex analytics. XDB builds on the following novel concepts:(1) a partitioningscheme that supports elasticity with regard to data and queries;(2) a cost-based fault-tolerance scheme that allows to recover from mid-query faults; and (3) adaptive …,Big Data (BigData Congress); 2014 IEEE International Congress on,2014,5
Data warehousing,CARSTEN BINNIG,Page 1. DATA WAREHOUSING INTRODUCTION TO DATA SCIENCE CARSTEN BINNIG BROWNUNIVERSITY Page 2. DATA WAREHOUSES Definition: A data warehouse is a database thatis optimized for analytical workloads which integrates data from independent and heterogeneousdata sources DB1 Data Warehouse Heterogeneous Data Sources Decision Support / Data MiningData Loading + Integration CSV Web Page 3. ENTERPRISE SCENARIO: WHOLEFOODS DBDB DB DB DB DB DB DB DB DB DB Data Warehouse Business Questions: • What are thebestselling products? • Is there difference between states? • … Page 4. OTHER APPLICATIONDOMAINS Restaurant Chains (McDonalds; etc.) Retailers (Nike; …) Insurance Companies Banks …Page 5. HISTORY OF DATABASES Age of Online Transaction Processing - OLTP (> 1970) • Goal:have access to up-to-date business transactions …,*,2002,5
Automatic result verification for the functional testing of a query language,Carsten Binnig; Donald Kossmann; Eric Lo; Angel Saenz-Badillos,Functional testing of a query language is a challenging task in practice. In order to revealerrors in the query processing functionality; it is necessary to verify the actual result of a testquery with the expected correct result. However; automatically computing the expectedquery result of an arbitrary test query is not trivial. One solution is to first generate a set of testdatabase instances and test queries and then to compute the expected result for each testquery over the individual test database instances. The problem of this solution is that manytest queries might return an empty query result; which is not interesting for the functionaltesting of a query language. In this paper; we present a new approach to verify the result of atest query so as to facilitate the functional testing of a query language. Instead of firstgenerating the database instance and then computing the expected result for each test …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,4
Generating Meaningful Test Databases,Carsten Binnig,Testing is one of the most time-consuming and cost-intensive tasks in software developmentprojects today. A recent report of the NIST [RTI02] estimated the costs for the economy of theUnites States of America caused by software errors in the year 2000 to range from $22.2 to$59.5 billion. Consequently; in the past few years; many techniques and tools have beendeveloped to reduce the high testing costs. Many of these techniques and tools are devotedto automate various testing tasks (eg; test case generation; test case execution; and testresult checking). However; almost no research work has been carried out to automate thetesting of database applications (eg; an E-Shop application) and relational databasemanagement systems (DBMSs). The testing of a database application and of a DBMSrequires different solutions because the application logic of a database application or of a …,*,2008,3
Durchgängige Qualität von Unternehmenssoftware,Andrea Herrmann; B Paech; S Kirn; D Kossmann; G Müller; C Binnig; M Gilliot; T Illes; L Lowis; D Weiß,Kurzfassung: IT-Systeme; die in einem dynamischen Umfeld betrieben werden; müssen sichständig ändernde betriebliche Anforderungen anpassen. Wichtig ist hierbei; dass dieQualität des Systems nicht leidet. Daher wurde eine Methodik entwickelt; mit der klardefiniert wird; welche Qualität des IT-Systems gewünscht wird; wie diese durch das IT-System sichergestellt wird; und wie diese Qualität geprüft wird. Die durch die SIKOSA-Methodik unterstützte durchgängige Nachverfolgbarkeit der Qualitäts-anforderungen ist dieGrundlage für das Änderungsmanagement; denn sie stellt auch bei Änderungen am IT-System sicher; dass die Qualität des Systems erhalten bleibt oder sich sogar verbessert.,Industrie Management,2006,3
Revisiting reuse in main memory database systems,Kayhan Dursun; Carsten Binnig; Ugur Cetintemel; Tim Kraska,Abstract Reusing intermediates in databases to speed-up analytical query processing wasstudied in prior work. Existing solutions require intermediate results of individual operators tobe materialized using materialization operators. However; inserting such materializationoperations into a query plan not only incurs additional execution costs but also ofteneliminates important cache-and register-locality opportunities; resulting in even higherperformance penalties. This paper studies a novel reuse model for intermediates; whichcaches internal physical data structures materialized during query processing (due topipeline breakers) and externalizes them so that they become reusable for upcomingoperations. We focus on hash tables; the most commonly used internal data structure inmain memory databases to perform join and aggregation operations. As queries arrive …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,2
Rethinking Distributed Query Execution on High-Speed Networks.,Abdallah Salama; Carsten Binnig; Tim Kraska; Ansgar Scherp; Tobias Ziegler,Abstract In modern high-speed RDMA-capable networks; the bandwidth to transfer dataacross machines is getting close to the bandwidth of the local memory bus. Recent work hasstarted to investigate how to redesign individual distributed query operators to best leverageRDMA. However; all these novel RDMA-based query operators are still designed for aclassical shared-nothing architecture that relies on a shuffle-based execution model toredistribute the data. In this paper; we revisit query execution for distributed databasesystems on fast networks in a more holistic manner by reconsidering all aspects from theoverall database architecture; over the partitioning scheme to the execution model. Ourexperiments show that in the best case our prototype database system called I-Store; whichis designed for fast networks from scratch; provides 3× speed-up over a shuffle-based …,IEEE Data Eng. Bull.,2017,2
Towards a Benchmark for Interactive Data Exploration.,Philipp Eichmann; Emanuel Zgraggen; Zheguang Zhao; Carsten Binnig; Tim Kraska,Abstract Existing benchmarks for analytical database systems such as TPC-DS and TPC-Hare designed for static reporting scenarios. The main metric of these benchmarks is theperformance of running different SQL queries over a predefined database. In this paper; weargue that such benchmarks are not suitable for evaluating modern interactive dataexploration (IDE) systems; which allow data scientists of varying skill levels to manipulate;analyze; and explore large data sets; as well as to build models and apply machine learningat interactive speeds. While query performance is still important for data exploration; webelieve that a much better metric would reflect the number and complexity of insights usersgain in a given amount of time. This paper discusses challenges of creating such a metricand presents ideas towards a new benchmark that simulates typical user behavior and …,IEEE Data Eng. Bull.,2016,2
Development of a UIML Renderer for Different Target Languages: Experiences and Design Decisions,Carsten Binnig; Andreas Schmidt,Abstract Generic development of user interfaces allows to considerably reduce the timeconsuming manual coding expenditure that may reach up to 50% of the development work.With UIML; the specification of an XML meta language was made available recently; whichclaims to define user interfaces independently of the platform and target language. Thepresent article describes our experience gained from the development of a UIML rendererfor various target languages. Division into target language-dependent and independentgeneration steps as well as various types of final target code generation play an importantrole in minimizing the expenditure of incorporating new target languages.,*,2002,2
A-Tree: A Bounded Approximate Index Structure,Alex Galakatos; Michael Markovitch; Carsten Binnig; Rodrigo Fonseca; Tim Kraska,Abstract: Index structures are one of the most important tools that DBAs leverage in order toimprove the performance of analytics and transactional workloads. However; with theexplosion of data that is constantly being generated in a wide variety of domains includingautonomous vehicles; Internet of Things (IoT) devices; and E-commerce sites; buildingseveral indexes can often become prohibitive and consume valuable system resources. Infact; a recent study has shown that indexes created as part of the TPC-C benchmark canaccount for 55% of the total memory available in a state-of-the-art in-memory DBMS. Thisoverhead consumes valuable and expensive main memory; and limits the amount of spacethat a database has available to store new data or process existing data. In this paper; wepresent a novel approximate index structure called A-Tree. At the core of our index is a …,arXiv preprint arXiv:1801.10207,2018,1
Revisiting reuse for approximate query processing,Alex Galakatos; Andrew Crotty; Emanuel Zgraggen; Carsten Binnig; Tim Kraska,Abstract Visual data exploration tools allow users to quickly gather insights from newdatasets. As dataset sizes continue to increase; though; new techniques will be necessary tomaintain the interactivity guarantees that these tools require. Approximate query processing(AQP) attempts to tackle this problem and allows systems to return query results at" humanspeed." However; existing AQP techniques start to break down when confronted with ad hocqueries that target the tails of the distribution. We therefore present an AQP formulation thatcan provide low-error approximate results at interactive speeds; even for queries over raresubpopulations. In particular; our formulation treats query results as random variables inorder to leverage the ample opportunities for result reuse inherent in interactive dataexploration. As part of our approach; we apply a variety of optimization techniques that …,Proceedings of the VLDB Endowment,2017,1
IncMap: A Journey towards Ontology-based Data Integration,Christoph Pinkel; Carsten Binnig; Ernesto Jimenez-Ruiz; Evgeny Kharlamov; Andriy Nikolov; Andreas Schwarte; Christian Heupel; Tim Kraska,Ontology-based data integration (OBDI) allows users to federate over heterogeneous datasources using a semantic rich conceptual data model. An important challenge in ODBI is thecuration of mappings between the data sources and the global ontology. In the last years;we have built IncMap; a system to semi-automatically create mappings between relationaldata sources and a global ontology. IncMap has since been put into practice; both foracademic and in industrial applications. Based on the experience of the last years; we haveextended the original version of IncMap in several dimensions to enhance the mappingquality:(1) IncMap can detect and leverage semantic-rich patterns in the relational datasources such as inheritance for the mapping creation.(2) IncMap is able to leveragereasoning rules in the ontology to overcome structural differences from the relational data …,*,2017,1
Spotlytics: How to Use Cloud Market Places for Analytics?,Tim Kraska; Elkhan Dadashov; Carsten Binnig,In contrast to fixed-priced cloud computing services; Amazon's Spot market uses a demand-driven pricing model for renting out virtual machine instances. This allows for remarkablesavings when used intelligently. However; a peculiarity of Amazon's Spot market is; thatmachines can suddenly be taken away from the user if the price on the market increases.This can be considered as a distinct form of a machine failure. In this paper; we first analyzeAmazon's current spot market rules and based on the results develop a general marketmodel. This model is valid for Amazon's current Spot service but also many potentialvariations of it; as well as other cloud computing markets. Using the developed marketmodel; we then make recommendations on how to deploy analytical systems with thefollowing three fault-tolerance/recovery strategies: re-execution as used by traditional …,*,2017,1
HILDA 2016 Workshop: A Report.,Arnab Nandi; Alan Fekete; Carsten Binnig,Abstract The first annual workshop “Human-in-the-Loop Data Analytics”(HILDA) was held onJune 26; 2016 in association with ACM SIGMOD in San Francisco. The workshop sparkedsome excellent conversations between speakers and attendees from many disciplinesspanning both industry and academia; and covered a variety of sub-themes related tohuman-in-the-loop data challenges.,IEEE Data Eng. Bull.,2016,1
SOMM: industry oriented ontology management tool,Evgeny Kharlamov; Bernardo Cuenca Grau; Ernesto Jimenez-Ruiz; Steffen Lamparter; Gulnar Mehdi; Martin Ringsquandl; Yavor Nenov; Stephan Grimm; Mikhail Roshchin; Ian Horrocks; Evgeny Kharlamov; Bernardo Cuenca Grau; Ernesto Jimenez-Ruiz; Steffen Lamparter; Gulnar Mehdi; Martin Ringsquandl; Yavor Nenov; Stephan Grimm; Mikhail Roshchin; Ian Horrocks; Yujiao Zhou; Bernardo Cuenca Grau; Yavor Nenov; Mark Kaminski; Ian Horrocks; Yavor Nenov; Robert Piro; Boris Motik; Ian Horrocks; Zhe Wu; Jay Banerjee; Yujiao Zhou; Yavor Nenov; Bernardo Cuenca Grau; Ian Horrocks; Boris Motik; Yavor Nenov; Robert Piro; Ian Horrocks; Yujiao Zhou; Bernardo Cuenca Grau; Yavor Nenov; Ian Horrocks; Boris Motik; Yavor Nenov; Robert Edgar Felix Piro; Ian Horrocks; Boris Motik; Yavor Nenov; Robert Edgar Felix Piro; Ian Horrocks; Yujiao Zhou; Yavor Nenov; Bernardo Cuenca Grau; Ian Horrocks; Yujiao Zhou; Yavor Nenov; Bernardo Cuenca Grau; Ian Horrocks; Mark Kaminski; Yavor Nenov; Bernardo Cuenca Grau; Mark Kaminski; Yavor Nenov; Bernardo Cuenca Grau; Mark Kaminski; Yavor Nenov; Bernardo Cuenca Grau; Boris Motik; Yavor Nenov; Robert Piro; Ian Horrocks; Dan Olteanu; Boris Motik; Yavor Nenov; Robert Piro; Ian Horrocks; Dan Olteanu; Yujiao Zhou; Yavor Nenov; Bernardo Cuenca Grau; Ian Horrocks; Yavor Nenov,Publications; by bibtex; Department of Computer Science; Oxford; Yavor Nenov.,Proc. of International Semantic Web Conference (ISWC); Posters and Demonstrations Track,2015,1
DoomDB: kill the query,Carsten Binnig; Abdallah Salama; Erfan Zamanian,Abstract Typically; fault-tolerance in parallel database systems is handled by restarting aquery completely when a node failure happens. However; when deploying a paralleldatabase on a cluster of commodity machines or on IaaS offerings such as Amazon's SpotInstances; node failures are a common case. This requires a more fine-granular fault-tolerance scheme. Therefore; most recent parallel data management platforms such asHadoop or Shark use a fine-grained fault-tolerance scheme; which materializes allintermediate results in order to be able to recover from mid-query faults. While such a fine-grained fault-tolerance scheme is able to efficiently handle node failures for complex andlong-running queries; it is not optimal for short-running latency-sensitive queries since theadditional costs for materialization often outweigh the costs for actually executing the …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,1
Experience Gained in the Development of a Route Planner for Mobile End Devices.,Andreas Schmidt; Carsten Binnig,EXPERIENCE GAINED IN THE DEVELOPMENT OF A ROUTE PLANNER FOR MOBILE ENDDEVICES Andreas Schmidt; Carsten Binnig Institute for Applied Computer Sciences ForschungszentrumKarlsruhe Postfach 3640 76021 Karlsruhe Germany {schmidt; binnig}@iai.fzk.de ABSTRACTIn the mobile information society; location-based services play an increasingly important role[1]. These services are to be accessed by users with a mobile end device (mobile phone; personaldigital assistant or smart phone) for the use of city maps; route planning; navigation; trafficinformation; etc. Mobile end devices; however; do not have a computing power or storage capacitycomparable to those of a personal computer or laptop. These deficits can be bypassed by employingspecial methods in the development of the respective applications. The present article outlinesthe experience gained in the development of a prototype for route computation in public …,Communications; Internet; and Information Technology,2002,1
DeepVizdom: Deep Interactive Data Exploration,Carsten Binnig; Kristian Kersting; Alejandro Molina; Emanuel Zgraggen,ABSTRACT We make a case for a new generation of interactive data exploration systemsthat seamlessly integrate deep models as first-class citizens into the data exploration stack.Based on three case studies; we argue that this not only enables users to gain much deeperinsights into a broader range of data sets but also helps to improve the performance andquality of existing data exploration systems.,*,2018,*
Appreciation to distributed and parallel databases reviewers,Ailidani Ailijiang; Murat Ali Bayir; Nihat Altiparmak; Douglas Alves Peixoto; Vaibhav Arora; Ira Assent; Manos Athanassoulis; Erman Ayday; Samira Babalou; Mehdi Bahrami; Madhushi Bandara; Fuat Basik; Kaustubh Beedkar; Ladjel Bellatreche; Carsten Binnig; Klemens Böhm; Angela Bonifati; Vanessa Braganholo; Guadalupe Canahuate; Alberto Cano; Lei Cao; Nicholas Car; Fabio Casati; Aniket Chakrabarti; Lijun Chang; Aleksey Charapko; Shimin Chen; Ling Chen; Yu Cheng; Fei Chiang; Byron Choi; Bin Cui; Khuzaima Daudjee; Engin Demir; Murat Demirbas; Anton Dignoes; Bailu Ding; Xiaofeng Ding; Jaeyoung Do; Bin Dong; Hai Dong; Laurent D’orazio; Ahmed Eldawy; Iman Elghandour; Mohammed Eunus Ali; Liyue Fan,For helping us deliver timely decisions to our authors; the Editor-in-Chief and Publisherwould like to thank the following individuals who contributed their reviews between January1; 2017 and December 31; 2017. We applaud all your efforts and dedication to thecommunity,*,2018,*
RODI: Benchmarking relational-to-ontology mapping generation quality,Ian Horrocks Christoph Pinkel; Carsten Binnig; Ernesto Jiménez-Ruiz; Evgeny Kharlamov; Wolfgang May; Andriy Nikolov; Ana Sasa Bastinos; Martin G. Skjæveland; Alessandro Solimando; Mohsen Taheriyan; Christian Heupel,*,Semantic Web Journal,2018,*
Towards Interactive Curation & Automatic Tuning of ML Pipelines,Carsten Binnig; Benedetto Buratti; Yeounoh Chung; Cyrus Cousins; Dylan Ebert; Tim Kraska; Zeyuan Shang; Isabella Tromba; Eli Upfal; Linnan Wang; Robert Zeleznik; Emanuel Zgraggen,*,1st Inaugural Conference on Systems ML (SysML),2018,*
What you see is not what you get!: Detecting Simpson's Paradoxes during Data Exploration,Yue Guo; Carsten Binnig; Tim Kraska,Abstract Visual data exploration tools; such as Vizdom or Tableau; significantly simplify dataexploration for domain experts and; more importantly; novice users. These tools allow todiscover complex correlations and to test hypotheses and differences between variouspopulations in an entirely visual manner with just a few clicks; unfortunately; often ignoringeven the most basic statistical rules. For example; there are many statistical pitfalls that auser can" tap" into when exploring data sets. As a result of this experience; we started tobuild QUDE [1]; the first system to Quantifying the Uncertainty in Data Exploration; which ispart of Brown's Interactive Data Exploration Stack (called IDES). The goal of QUDE is toautomatically warn and; if possible; protect users from common mistakes during the dataexploration process. In this paper; we focus on a different type of error; the Simpson's …,Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,2017,*
SiliconDB: rethinking DBMSs for modern heterogeneous co-processor environments,Kayhan Dursun; Carsten Binnig; Ugur Cetintemel; Robert Petrocelli,Abstract In the last decade; the work centered around specialized co-processors for DBMSshas largely focused on efficient query processing algorithms for individual operators.However; a major limitation of existing co-processor systems is the PCI bottleneck; whichseverely limits the efficient use of this type of hardware in current systems. In recent years;we have seen the emergence of a new class of co-processor systems that includespecialized accelerators; implemented as ASICs or FPGAs; which co-reside with the CPU onthe same socket. Here we revisit DBMS architectures in this context; and take an initial steptowards the design of a new database system called SiliconDB that targets these newdensely integrated heterogeneous co-processor environments.,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,*
Safe Visual Data Exploration,Zheguang Zhao; Emanuel Zgraggen; Lorenzo De Stefani; Carsten Binnig; Eli Upfal; Tim Kraska,Abstract Exploring data via visualization has become a popular way to understand complexdata. Features or patterns in visualization can be perceived as relevant insights by users;even though they may actually arise from random noise. Moreover; interactive dataexploration and visualization recommendation tools can examine a large number ofobservations; and therefore result in further increasing chance of spurious insights. Thuswithout proper statistical control; the risk of false discovery renders visual data explorationunsafe and makes users susceptible to questionable inference. To address these problems;we present QUDE; a visual data exploration system that interacts with users to formulatehypotheses based on visualizations and provides interactive control of false discoveries.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Interactive Data Analytics for the Humanities,Iryna Gurevych; Christian M Meyer; Carsten Binnig; Johannes Fürnkranz; Kristian Kersting; Stefan Roth; Edwin Simpson,Abstract. In this vision paper; we argue that current solutions to data analytics are notsuitable for complex tasks from the humanities; as they are agnostic of the user and focusedon static; predefined tasks with large-scale benchmarks. Instead; we believe that the humanmust be put into the loop to address small data scenarios that require expert domainknowledge and fluid; incrementally defined tasks; which are common for many humanitiesuse cases. Besides the main challenges; we discuss existing and urgently requiredsolutions to interactive data acquisition; model development; model interpretation; andsystem support for interactive data analytics. In the envisioned interactive systems; humanusers not only provide annotations to a machine learner; but train a model by using thesystem and demonstrating the task. The learning system will actively query the user for …,*,2017,*
The ETH Zurich systems group and enterprise computing center,Gustavo Alonso; Donald Kossmann; Timothy Roscoe; Nesime Tatbul; Andrew Baumann; Carsten Binnig; Peter Fischer; Oriana Riva; Jens Teubner,Computer science is facing a fundamental paradigm shift. Multicore architectures;application virtualization; and cloud computing each present on their own radical departuresin the way software is built; deployed; and operated. Taken together; these trends frame ascenario that makes sense from many points of view (usability; scalability; flexibility;development cost) but is also radically different from what we know today. It is fair to say thata coherent answer to the challenges raised by the combination of these trends has yet toemerge from either academia or industry. From an academic perspective; these challengesare particularly difficult because they imply a considerable departure from establishedprocedures. To start with; multicore computers; the virtualization of computing platforms; andthe replacement of the one-computer-one-local-copy-ofa-program approach by cloud …,ACM SIGMOD Record,2009,*
The SIKOSA method-Support of the industrial software production by methodically integrated software engineering processes,Daniel Weiss; Jorn Kaack; Stefan Kirn; Maike Gilliot; Lutz Lowis; Gunter Mueller; Andrea Herrmann; Carsten Binnig; Timea Illes; Barbara Paech; Donald Kossman,*,Wirtschaftsinformatik,2007,*
Screen Shot 2017-03-13 at 5.15. 39 PM,Sam Madden; Jane Greenberg; Carsten Binnig; Tim Kraska; Danny Weitzner; Sam Grabus,Summary A part of the NSF Big Data regional innovation hub program; the Northeast hub; isaddressing key data sharing challenges by:• Creating a licensing model for data thatfacilitates sharing data that is not necessarily open or free between different organizations;•Developing a prototype data sharing software platform; ShareDB; which will enforces theterms and restrictions of the developed licenses; and• Developing and integrating relevantmetadata that will accompany the datasets shared under the different licenses; making themeasily searchable and interpretable.,*,*,*
Data Engineering,Abdallah Salama; Carsten Binnig; Tim Kraska; Ansgar Scherp; Tobias Ziegler; Animesh Trivedi Stuedi; Jonas Pfefferle; Radu Stoica; Bernard Metzler; Nikolas Ioannou; Ioannis Koltsidas; Xiaoyi Lu; Dipti Shankar; Dhabaleswar K DK Panda; Chinmay Kulkarni; Aniraj Kesavan; Robert Ricci; Ryan Stutsman,The Technical Committee on Data Engineering held an election last fall for chair of the TC.The voting deadline was December 22 of last year. The candidates were Xiaofang Zhou andErich Neuhold. My thanks both candidates for being willing to run. Being chair of theTechnical Committee is largely invisible; but it is an important responsibility for the successof the data engineering community. The winner; with 69% of the vote is the current chair;Xiaofang Zhou; who has now won his second term. Congratulations to Xiaofang for hiselectoral victory. Xiaofang knows what the job entails; is experienced in doing it; and does itwell. I very much appreciate Xiaofang's efforts and his continued involvement; both at theTCDE and at the Computer Society more widely.,*,*,*
ICDE 2017 Reviewers,Yannis Papakonstantinou; Lei Chen; Reynold Cheng; Wolfgang Gatterbauer; Bingsheng He; Stratos Idreos; Christopher Jermaine; Chen Li; Gerome Miklau; Tamer Özsu; Olga Papaemmanouil; Evimaria Terzi; Eugene Wu; Ashraf Aboulnaga; Alex Alves; Amazon Gabriel Antoniu; INRIA Arvind Arasu; Andrey Balmin; Workday Zhifeng Bao; Sumita Barahmand; Srikanta Bedathur; Carsten Binnig; Spyros Blanas; Marco Brambilla; Stephane Bressan; K Selcuk Candan; Zhao Cao; James Cheng; Fei Chiang; Panos K Chrysanthis; Philippe Cudre-Mauroux,ICDE 2017 Program Committee Chairs Yannis Papakonstantinou; University of California; SanDiego Yanlei Diao; Ecole Polytechnique; France; and University of Massachusetts; Amherst …ICDE 2017 Area Chairs Lei Chen; Hong Kong University of Science and Technology ReynoldCheng; University of Hong Kong Wolfgang Gatterbauer; Carnegie Mellon University BingshengHe; National University of Singapore Stratos Idreos; Harvard University ChristopherJermaine; Rice University Chen Li; University of California Irvine Gerome Miklau; University ofMassachusetts Tamer Özsu; University of Waterloo Olga Papaemmanouil; Brandeis UniversityEvimaria Terzi; Boston University Eugene Wu; Columbia University … ICDE 2017 Program CommitteeAshraf Aboulnaga; Qatar Computing Research Institute Alex Alves; Amazon Gabriel Antoniu;INRIA Arvind Arasu; Microsoft Research Andrey Balmin; Workday Zhifeng Bao; RMIT …,*,*,*
Data Engineering,Aditya Parameswarany; Akash Das Sarma; Vipul Venkataramani; Olga Papaemmanouil; Yanlei Diao; Kyriaki Dimitriadou; Liping Peng; Philipp Eichmann; Emanuel Zgraggen; Zheguang Zhao; Carsten Binnig; Tim Kraska; Michael R Anderson; Dolan Antenucci; Michael Cafarella,Abstract As one of the successful forms of using Wisdom of Crowd; crowdsourcing; has beenwidely used for many human intrinsic tasks; such as image labeling; natural languageunderstanding; market predication and opinion mining. Meanwhile; with advances inpervasive technology; mobile devices; such as mobile phones; tablets; and PDA; havebecome extremely popular. These mobile devices can work as sensors to collect varioustypes of data; such as pictures; videos; audios and texts. Therefore; in crowdsourcing; arequester can unitize power of mobile devices and their location information to ask for datarelated a specific location; subsequently; the mobile users who would like to perform the taskwill travel to the target location and collect the data (videos; audios; or pictures); which isthen sent to the requester. This type of crowdsourcing is called spatial crowdsourcing …,*,*,*
Program Committees,Adam Belloum; Adriana Iamnitchi; Adrien Lèbre; Ajay Deshpande; Albert Levi; Alexandra Carpen-Amarie; Alexandru Costan; Alexandru Iosup; Ali R Butt; Alvaro Navas; Amr Alasaad; Ana-Maria Oprescu; Andre Brinkmann; Andrea Perego; Andres Martinez; Angel Kuri-Morales; Anirban Basu; B Annappa; Anne-Cécile Orgerie; Apostolos Papageorgiou; Athanasios Vasilakos; Ben Liang; Ben Smyth; Bingsheng He; Bo Yang; Bogdan Nicolae; Bruce Hendrickson; Carmelo Ragusa; Carmen Fernández-Gago; Carson K Leung; Carsten Binnig; Chandra Sekaran; Chang Hong Lin; Chen Jin; Chendong Li; Cheng-Kang Chu; Chi-Hung Chi; Ching-Seh Wu; Christophe Cerin; Chuliang Weng; Chunming Hu; Claudio Martella; Craig Lee; Dan Reed; Dana Petcu,Main Conference Adam Belloum; University of Amsterdam Adriana Iamnitchi; University of SouthFlorida Adrien Lèbre; Ecole des Mines Ajay Deshpande; IBM Albert Levi; Sabanci UniversityAlexandra Carpen-Amarie; Vienna University of Technology Alexandru Costan; INRIA AlexandruIosup; Delft University of Technology Ali R. Butt; Virginia Tech Alvaro Navas; Santander BankCentre for Open Middleware Amr Alasaad; The University of British Columbia Ana-MariaOprescu; Vrije University Andre Brinkmann; Johannes Gutenberg-Universität Mainz AndreaPerego; European Commission - Joint Research Centre Andres Martinez; Google AngelKuri-Morales; ITAM Anirban Basu; Tokai University Annappa B; NITK Anne-Cécile Orgerie; Institutde Recherche en Informatique et Systèmes Aléatoires Apostolos Papageorgiou; NEC Lab AthanasiosVasilakos; Kuwait University Ben Liang; University of Toronto Ben Smyth; INRIA Paris …,*,*,*
I-Store: Data Management for Fast Networks,Carsten Binnig; Ugur Cetintemel; Tim Kraska; Stan Zdonik; Erfan Zamanian; Andrew Crotty,Motivation: Existing distributed data management systems typically assume that the networkis a major bottleneck [10]. Consequently; avoiding remote data transfers is an importantdesign aspect of existing systems. In extreme cases; this has lead to system designs; whichexplicitly do not support certain distributed operations (eg; BigTable only supports joins if theinner table contains less than 8 MB of data). A common design principle; however; is tominimize remote data transfer by the two following techniques: First; existing systems try tofind an optimal partitioning scheme to co-partition data in order to avoid remote datatransfers for operations such as joins or to avoid distributed transactions. Second; locality-aware scheduling strategies aim to increase data-locality by shipping computation to thenodes where the data is stored. However; these techniques still result in major limitations …,*,*,*
Generierung Relevanter Testdatenbanken,Carsten Binnig,Abstract: In heutigen Softwareentwicklungsprojekten ist das Testen eine der kostenundzeitintensivsten Tätigkeiten. Wie ein aktueller Bericht des NIST [RTI02] zeigt; verursachtenSoftwarefehler in den USA im Jahr 2000 zwischen 22; 2 und 59; 5 Milliarden Dollar anKosten. Demzufolge wurden in den letzten Jahren verschiedene Methoden und Werkzeugeentwickelt; um diese hohen Kosten zu reduzieren. Viele dieser Werkzeuge dienen dazu dieverschiedenen Testaufgaben (zB das Erzeugen von Testfällen; die Ausführung vonTestfällen und das Uberprüfen der Testergebnisse) zu automatisieren. Jedoch existieren nurwenige Forschungsarbeiten zur Automatisierung der Tests von Datenbankanwendungen(wie zB eines E-Shops) bzw. von relationalen Datenbankmanagementsystemen (DBMS).Die diesem Artikel zugrunde liegende Doktorarbeit diskutiert ein wichtiges Problem aus …,Gesellschaft für Informatik eV (GI) publishes this series in order to make available to a broad public recent findings in informatics (ie computer science and informa-tion systems); to document conferences that are organized in co-operation with GI and to publish the annual GI Award dissertation.,*,*
