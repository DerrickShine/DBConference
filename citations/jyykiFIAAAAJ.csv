Starfish: a self-tuning system for big data analytics.,Herodotos Herodotou; Harold Lim; Gang Luo; Nedyalko Borisov; Liang Dong; Fatma Bilgen Cetin; Shivnath Babu,ABSTRACT Timely and cost-effective analytics over “Big Data” is now a key ingredient forsuccess in many businesses; scientific and engineering disciplines; and governmentendeavors. The Hadoop software stack—which consists of an extensible MapReduceexecution engine; pluggable distributed storage engines; and a range of procedural todeclarative interfaces—is a popular choice for big data analytics. Most practitioners of bigdata analytics—like computational scientists; systems researchers; and business analysts—lack the expertise to tune the system to get good performance. Unfortunately; Hadoop'sperformance out of the box leaves much to be desired; leading to suboptimal use ofresources; time; and money (in payas-you-go clouds). We introduce Starfish; a self-tuningsystem for big data analytics. Starfish builds on Hadoop while adapting to user needs and …,Cidr,2011,543
Profiling; what-if analysis; and cost-based optimization of mapreduce programs,Herodotos Herodotou; Shivnath Babu,ABSTRACT MapReduce has emerged as a viable competitor to database systems in bigdata analytics. MapReduce programs are being written for a wide variety of applicationdomains including business data processing; text analysis; natural language processing;Web graph and social network analysis; and computational science. However; MapReducesystems lack a feature that has been key to the historical success of database systems;namely; cost-based optimization. A major challenge here is that; to the MapReduce system;a program consists of black-box map and reduce functions written in some programminglanguage like C++; Java; Python; or Ruby. We introduce; to our knowledge; the first Cost-based Optimizer for simple to arbitrarily complex MapReduce programs. We focus on theoptimization opportunities presented by the large space of configuration parameters for …,Proceedings of the VLDB Endowment,2011,337
No one (cluster) size fits all: automatic cluster sizing for data-intensive analytics,Herodotos Herodotou; Fei Dong; Shivnath Babu,Abstract Infrastructure-as-a-Service (IaaS) cloud platforms have brought two unprecedentedchanges to cluster provisioning practices. First; any (nonexpert) user can provision a clusterof any size on the cloud within minutes to run her data-processing jobs. The user canterminate the cluster once her jobs complete; and she needs to pay only for the resourcesused and duration of use. Second; cloud platforms enable users to bypass the traditionalmiddleman---the system administrator---in the cluster-provisioning process. These changesgive tremendous power to the user; but place a major burden on her shoulders. The user isnow faced regularly with complex cluster sizing problems that involve finding the clustersize; the type of resources to use in the cluster from the large number of choices offered bycurrent IaaS cloud platforms; and the job configurations that best meet the performance …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,219
Hadoop performance models,Herodotos Herodotou,Abstract: Hadoop MapReduce is now a popular choice for performing large-scale dataanalytics. This technical report describes a detailed set of mathematical performance modelsfor describing the execution of a MapReduce job on Hadoop. The models describe dataflowand cost information at the fine granularity of phases within the map and reduce tasks of ajob execution. The models can be used to estimate the performance of MapReduce jobs aswell as to find the optimal configuration settings to use when running the jobs.,arXiv preprint arXiv:1106.0940,2011,134
Stubby: A transformation-based optimizer for mapreduce workflows,Harold Lim; Herodotos Herodotou; Shivnath Babu,Abstract There is a growing trend of performing analysis on large datasets using workflowscomposed of MapReduce jobs connected through producer-consumer relationships basedon data. This trend has spurred the development of a number of interfaces---ranging fromprogram-based to query-based interfaces---for generating MapReduce workflows. Studieshave shown that the gap in performance can be quite large between optimized andunoptimized workflows. However; automatic cost-based optimization of MapReduceworkflows remains a challenge due to the multitude of interfaces; large size of the executionplan space; and the frequent unavailability of all types of information needed foroptimization. We introduce a comprehensive plan space for MapReduce workflowsgenerated by popular workflow generators. We then propose Stubby; a cost-based …,Proceedings of the VLDB Endowment,2012,88
RIOT: I/O-efficient numerical computing without SQL,Yi Zhang; Herodotos Herodotou; Jun Yang,Abstract: R is a numerical computing environment that is widely popular for statistical dataanalysis. Like many such environments; R performs poorly for large datasets whose sizesexceed that of physical memory. We present our vision of RIOT (R with I/O Transparency); asystem that makes R programs I/O-efficient in a way transparent to the users. We describeour experience with RIOT-DB; an initial prototype that uses a relational database system asa backend. Despite the overhead and inadequacy of generic database systems in handlingarray data and numerical computation; RIOT-DB significantly outperforms R in many large-data scenarios; thanks to a suite of high-level; inter-operation optimizations that integrateseamlessly into R. While many techniques in RIOT are inspired by databases (and; for RIOT-DB; realized by a database system); RIOT users are insulated from anything database …,arXiv preprint arXiv:0909.1766,2009,50
Query optimization techniques for partitioned tables,Herodotos Herodotou; Nedyalko Borisov; Shivnath Babu,Abstract Table partitioning splits a table into smaller parts that can be accessed; stored; andmaintained independent of one another. From their traditional use in improving queryperformance; partitioning strategies have evolved into a powerful mechanism to improve theoverall manageability of database systems. Table partitioning simplifies administrative taskslike data loading; removal; backup; statistics maintenance; and storage provisioning. Querylanguage extensions now enable applications and user queries to specify how their resultsshould be partitioned for further use. However; query optimization techniques have not keptpace with the rapid advances in usage and user control of table partitioning. We address thisgap by developing new techniques to generate efficient plans for SQL queries involvingmultiway joins over partitioned tables. Our techniques are designed for easy …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,46
Mapreduce programming and cost-based optimization? crossing this chasm with starfish,Herodotos Herodotou; Fei Dong; Shivnath Babu,ABSTRACT MapReduce has emerged as a viable competitor to database systems in bigdata analytics. MapReduce programs are being written for a wide variety of applicationdomains including business data processing; text analysis; natural language processing;Web graph and social network analysis; and computational science. However; MapReducesystems lack a feature that has been key to the historical success of database systems;namely; cost-based optimization. A major challenge here is that; to the MapReduce system;a program consists of black-box map and reduce functions written in some programminglanguage like C++; Java; Python; or Ruby. Starfish is a self-tuning system for big dataanalytics that includes; to our knowledge; the first Cost-based Optimizer for simple toarbitrarily complex MapReduce programs. Starfish also includes a Profiler to collect …,Proceedings of the VLDB Endowment,2011,46
Automated Experiment-Driven Management of (Database) Systems.,Shivnath Babu; Nedyalko Borisov; Songyun Duan; Herodotos Herodotou; Vamsidhar Thummala,Abstract In this position paper; we argue that an important piece of the system administrationpuzzle has largely been left untouched by researchers. This piece involves mechanisms andpolicies to identify as well as collect missing instrumentation data; the missing data isessential to generate the knowledge required to address certain administrative taskssatisfactorily and efficiently. We introduce the paradigm of experiment-driven managementwhich encapsulates such mechanisms and policies for a given administrative task. Weoutline the benefits that automated experiment-driven management brings to several long-standing problems in databases as well as other systems; and identify research challengesas well as initial solutions.,HotOS,2009,35
Massively Parallel Databases and MapReduce Systems,Shivnath Babu; Herodotos Herodotou,Abstract Timely and cost-effective analytics over" big data" has emerged as a key ingredientfor success in many businesses; scientific and engineering disciplines; and governmentendeavors. Web clicks; social media; scientific experiments; and datacenter monitoring areamong data sources that generate vast amounts of raw data every day. The need to convertthis raw data into useful information has spawned considerable innovation in systems forlarge-scale data analytics; especially over the last decade. This monograph covers thedesign principles and core features of systems for analyzing very large datasets usingmassively-parallel computation and storage techniques on large clusters of nodes. We firstdiscuss how the requirements of data analytics have evolved since the early work onparallel database systems. We then describe some of the major technological innovations …,*,2013,34
Cost-based optimization of configuration parameters and cluster sizing for hadoop,*,Cost-based optimization of configuration parameters and cluster sizing for distributed dataprocessing systems are disclosed. According to an aspect; a method includes receiving atleast one job profile of a MapReduce job. The method also includes using the at least onejob profile to predict execution of the MapReduce job within a plurality of differentpredetermined settings of a distributed data processing system. Further; the method includesdetermining one of the predetermined settings that optimizes performance of theMapReduce job. The method may also include automatically adjusting the distributed dataprocessing system to the determined predetermined setting.,*,2016,32
A What-if Engine for Cost-based MapReduce Optimization.,Herodotos Herodotou; Shivnath Babu,Abstract The Starfish project at Duke University aims to provide MapReduce users andapplications with good performance automatically; without any need on their part tounderstand and manipulate the numerous tuning knobs in a MapReduce system. This paperdescribes the What-if Engine; an indispensable component in Starfish; which serves asimilar purpose as a costing engine used by the query optimizer in a Database system. Wediscuss the problem and challenges addressed by the What-if Engine. We also discuss thetechniques used by the What-if Engine and the design decisions that led us to thesetechniques.,IEEE Data Eng. Bull.,2013,16
Xplus: a sql-tuning-aware query optimizer,Herodotos Herodotou; Shivnath Babu,Abstract The need to improve a suboptimal execution plan picked by the query optimizer fora repeatedly run SQL query arises routinely. Complex expressions; skewed or correlateddata; and changing conditions can cause the optimizer to make mistakes. For example; theoptimizer may pick a poor join order; overlook an important index; use a nested-loop joinwhen a hash join would have done better; or cause an expensive; but avoidable; sort tohappen. SQL tuning is also needed while tuning multi-tier services to meet service-levelobjectives. The difficulty of SQL tuning can be lessened considerably if users and higher-level tuning tools can tell the optimizer:" I am not satisfied with the performance of the plan pbeing used for the query Q that runs repeatedly. Can you generate a (δ%) better plan?" Thispaper designs; implements; and evaluates Xplus which; to our knowledge; is the first …,Proceedings of the VLDB Endowment,2010,16
Automated SQL tuning through trial and (sometimes) error,Herodotos Herodotou; Shivnath Babu,Abstract SQL tuning---the attempt to improve a poorly-performing execution plan producedby the database query optimizer---is a critical aspect of database performance tuning.Ironically; as commercial databases strive to improve on the manageability front; SQL tuningis becoming more of a black art. It requires a high level of expertise in areas like (i) queryoptimization; run-time execution of query plan operators; configuration parameter settings;and other database internals;(ii) identification of missing indexes and other accessstructures;(iii) statistics maintained about the data; and (iv) characteristics of the underlyingstorage system. Since database systems; their workloads; and the data that they manageare not getting any simpler; database users and administrators often rely on trial and errorfor SQL tuning. In this paper; we take the position that the trial-and-error (or; experiment …,Proceedings of the Second International Workshop on Testing Database Systems,2009,15
Scalable near real-time failure localization of data center networks,Herodotos Herodotou; Bolin Ding; Shobana Balakrishnan; Geoff Outhred; Percy Fitter,Abstract Large-scale data center networks are complex---comprising several thousandnetwork devices and several hundred thousand links---and form the critical infrastructureupon which all higher-level services depend on. Despite the built-in redundancy in datacenter networks; performance issues and device or link failures in the network can lead touser-perceived service interruptions. Therefore; determining and localizing user-impactingavailability and performance issues in the network in near real time is crucial. Traditionally;both passive and active monitoring approaches have been used for failure localization.However; data from passive monitoring is often too noisy and does not effectively capturesilent or gray failures; whereas active monitoring is potent in detecting faults but limited in itsability to isolate the exact fault location depending on its scale and granularity. Our key …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,12
SNPpy-database management for SNP data from Genome wide association studies,Faheem Mitha; Herodotos Herodotou; Nedyalko Borisov; Chen Jiang; Josh Yoder; Kouros Owzar,Background We describe SNPpy; a hybrid script database system using the PythonSQLAlchemy library coupled with the PostgreSQL database to manage genotype data fromGenome-Wide Association Studies (GWAS). This system makes it possible to merge studydata with HapMap data and merge across studies for meta-analyses; including data filteringbased on the values of phenotype and Single-Nucleotide Polymorphism (SNP) data. SNPpyand its dependencies are open source software. Results The current version of SNPpy offersutility functions to import genotype and annotation data from two commercial platforms. Weuse these to import data from two GWAS studies and the HapMap Project. We then exportthese individual datasets to standard data format files that can be imported into statisticalsoftware for downstream analyses. Conclusions By leveraging the power of relational …,PloS one,2011,10
Automatic Tuning of Data-Intensive Analytical Workloads,Herodotos Herodotou,*,*,2016,8
Automatic tuning of data-intensive analytical workloads,Herodotos Herodotou,Abstract Modern industrial; government; and academic organizations are collecting massiveamounts of data (" Big Data") at an unprecedented scale and pace. The ability to performtimely and cost-effective analytical processing of such large datasets in order to extract deepinsights is now a key ingredient for success. These insights can drive automated processesfor advertisement placement; improve customer relationship management; and lead to majorscientific breakthroughs.,*,2012,8
PStorM: Profile Storage and Matching for Feedback-Based Tuning of MapReduce Jobs,Mostafa Ead; Herodotos Herodotou; Ashraf Aboulnaga; Shivnath Babu,The MapReduce programming model has become widely adopted for large scale analyticson big data. MapReduce systems such as Hadoop have many tuning parameters; many ofwhich have a significant impact on performance. The map and reduce functions that makeup a MapReduce job are developed using arbitrary programming constructs; which makesthem black-box in nature and prevents users from making good parameter tuning decisionsfor a submitted MapReduce job. Some research projects; such as the Starfish system; aim toprovide automatic tuning decisions for input MapReduce jobs. Starfish and similar systemsrely on an execution profile of a MapReduce job being tuned; and this profile is assumed tocome from a previous execution of the same job. Managing these execution profiles has notbeen previously studied. This thesis presents PStorM; a profile store that organizes the …,17th International Conference on Extending Database Technology (EDBT ’14),2014,6
Enhancing virtual reality systems with smart wearable devices,Salah Eddin Alshaal; Stylianos Michael; Andreas Pamporis; Herodotos Herodotou; George Samaras; Panayiotis Andreou,The proliferation of wearable and smartphone devices with embedded sensors has enabledresearchers and engineers to study and understand user behavior at an extremely highfidelity; particularly for use in industries such as entertainment; health; and retail. However;identified user patterns are yet to be integrated into modern systems with immersivecapabilities; such as VR systems; which still remain constrained by limited applicationinteraction models exposed to developers. In this paper; we present Smart VR; a platformthat allows developers to seamlessly incorporate user behavior into VR apps. We presentthe high-level architecture of Smart VR; and show how it facilitates communication; dataacquisition; and context recognition between smart wearable devices and mediator systems(eg; smartphones; tablets; PCs). We demonstrate Smart VR in the context of a VR app for …,Mobile Data Management (MDM); 2016 17th IEEE International Conference on,2016,3
SNPpy-Database Management for SNP Data from GWAS Studies,Faheem Mitha; Herodotos Herodotou; Nedyalko Borisov; Chen Jiang; Josh Yoder; Kouros Owzar,Background: We describe SNPpy; a hybrid script database system using the PythonSQLAlchemy library coupled with the PostgreSQL database to manage genotype data fromGenome-Wide Association Studies (GWAS). This system makes it possible to merge studydata with HapMap data; and merge across studies for meta-analyses; including data filteringbased on the values of phenotype and SNP data. Results: The current version of SNPpyoffers utility functions to import genotype and annotation data from two commercial platforms.We use these to import data from two GWAS studies and the HapMap Project. We thenexport these individual datasets to standard data format files that can be imported intostatistical software for downstream analyses. Conclusions: By leveraging the power ofrelational databases; SNPpy offers integrated management and manipulation of …,Duke Biostatistics and Bioinformatics (B&B) Working Paper Series,2011,2
H. Dynamic Concurrency Control while Scheduling Query Mixes,B Herodotou Fasy; Herodotos Herodotou,*,*,2007,2
OctopusFS: A Distributed File System with Tiered Storage Management,Elena Kakoulli; Herodotos Herodotou,Abstract The ever-growing data storage and I/O demands of modern large-scale dataanalytics are challenging the current distributed storage systems. A promising trend is toexploit the recent improvements in memory; storage media; and networks for sustaining highperformance and low cost. While past work explores using memory or SSDs as local storageor combine local with network-attached storage in cluster computing; this work focuses onmanaging multiple storage tiers in a distributed setting. We present OctopusFS; a noveldistributed file system that is aware of heterogeneous storage media (eg; memory; SSDs;HDDs; NAS) with different capacities and performance characteristics. The system offers avariety of pluggable policies for automating data management across the storage tiers andcluster nodes. The policies employ multi-objective optimization techniques for making …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Business Intelligence and Analytics: Big Systems for Big Data,Herodotos Herodotou,Abstract The amount of data collected by modern industrial; government; and academicorganizations has been increasing exponentially and will continue to grow at anaccelerating rate for the foreseeable future. At companies across all industries; servers areoverflowing with usage logs; message streams; transaction records; sensor data; businessoperations records; and mobile device data. Effectively analyzing these massive collectionsof data (“big data”) can create significant value for the world economy by enhancingproductivity; increasing efficiency; and delivering more value to consumers. The need toconvert raw data into useful information has led to the development of advanced and uniquedata storage; management; analysis; and visualization technologies; especially over the lastdecade. This monograph is an attempt to cover the design principles and core features of …,*,2017,*
Towards a distributed multi-tier file system for cluster computing,Herodotos Herodotou,Distributed storage systems running on clusters of commodity hardware are challenged bythe ever-growing data storage and I/O demands of modern large-scale data analytics. Apromising trend is to exploit the recent improvements in memory; storage media; andnetwork technologies for sustaining high performance at low cost. While recent workexplores using memory and SSDs as a cache for local storage or combining local withnetwork-attached storage; no work has ever looked at all layers together in a distributedsetting. We present a novel design for a distributed file system that is aware ofheterogeneous storage media (eg; memory; SSDs; HDDs; NAS) with different capacities andperformance characteristics. The storage media are explicitly exposed to users andapplications; allowing them to choose the distribution and placement of replicas in the …,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,*
A Distributed File System with Storage-Media Awareness,Herodotos Herodotou,Improvements in memory; storage devices; and network technologies are constantlyexploited by distributed systems in order to meet the increasing data storage and I/Odemands of modern large-scale data analytics. Some systems use memory and SSDs as acache for local storage while others combine local with network-attached storage to increaseperformance. However; no work has ever looked at all layers together in a distributedsetting. We present a novel design for a distributed file system that is aware of storage media(eg; memory; SSDs; HDDs; NAS) with different capacities and performance characteristics.The storage media are explicitly exposed to users; allowing them to choose the distributionand placement of replicas in the cluster based on their own performance and fault tolerancerequirements. Meanwhile; the system offers a variety of pluggable policies for automating …,Utility and Cloud Computing (UCC); 2015 IEEE/ACM 8th International Conference on,2015,*
zTuned: Automated SQL Tuning through Trial and (Sometimes) Error,Herodotos Herodotou,Abstract SQL tuning—the attempt to improve a poorly-performing execution plan producedby the database query optimizer—is a critical aspect of database performance tuning.Ironically; as commercial databases strive to improve on the manageability front; SQL tuningis becoming more of a black art. It requires a high level of expertise in areas like (i) queryoptimization; run-time execution of query plan operators; configuration parameter settings;and other database internals;(ii) identification of missing indexes and other accessstructures;(iii) statistics maintained about the data; and (iv) characteristics of the underlyingstorage system. Since database systems; their workloads; and the data that they manageare not getting any simpler; database users and administrators often rely on intuition andtrial and error for SQL tuning. This work takes the position that the trial and error (or …,*,2009,*
ACADEMIC AWARDS/HONORS,Herodotos Herodotou,*,*,*,*
