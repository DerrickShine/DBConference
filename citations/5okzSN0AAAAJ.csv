PARMA: A Parallel Randomized Algorithm for Approximate Association Rules Mining in MapReduce,Matteo Riondato; Justin A DeBrabant; Rodrigo Fonseca; Eli Upfal,Abstract Frequent Itemsets and Association Rules Mining (FIM) is a key task in knowledgediscovery from data. As the dataset grows; the cost of solving this task is dominated by thecomponent that depends on the number of transactions in the dataset. We address this issueby proposing PARMA; a parallel algorithm for the MapReduce framework; which scales wellwith the size of the dataset (as number of transactions) while minimizing data replication andcommunication cost. PARMA cuts down the dataset-size-dependent part of the cost by usinga random sampling approach to FIM. Each machine mines a small random sample of thedataset; of size independent from the dataset size. The results from each machine are thenfiltered and aggregated to produce a single output collection. The output will be a very closeapproximation of the collection of Frequent Itemsets (FI's) or Association Rules (AR's) with …,CIKM 2012,2012,98
Learning-based query performance modeling and prediction,Mert Akdere; Ugur Çetintemel; Matteo Riondato; Eli Upfal; Stanley B Zdonik,Accurate query performance prediction (QPP) is central to effective resource management;query optimization and query scheduling. Analytical cost models; used in current generationof query optimizers; have been successful in comparing the costs of alternative query plans;but they are poor predictors of execution latency. As a more promising approach to QPP; thispaper studies the practicality and utility of sophisticated learning-based models; which haverecently been applied to a variety of predictive tasks with great success; in both static (ie;fixed) and dynamic query workloads. We propose and evaluate predictive modelingtechniques that learn query execution behavior at different granularities; ranging fromcoarse-grained plan-level models to fine-grained operator-level models. We demonstratethat these two extremes offer a tradeoff between high accuracy for static workload queries …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,87
Fast approximation of betweenness centrality through sampling,Matteo Riondato; Evgenios M Kornaropoulos,Abstract Betweenness centrality is a fundamental measure in social network analysis;expressing the importance or influence of individual vertices (or edges) in a network in termsof the fraction of shortest paths that pass through them. Since exact computation in largenetworks is prohibitively expensive; we present two efficient randomized algorithms forbetweenness estimation. The algorithms are based on random sampling of shortest pathsand offer probabilistic guarantees on the quality of the approximation. The first algorithmestimates the betweenness of all vertices (or edges): all approximate values are within anadditive factor ε ∈ (0; 1) ε∈(0; 1) from the real values; with probability at least 1-δ 1-δ. Thesecond algorithm focuses on the top-K vertices (or edges) with highest betweenness andestimate their betweenness value to within a multiplicative factor ε ε; with probability at …,Data Mining and Knowledge Discovery,2016,82
Space-round tradeoffs for MapReduce computations,Andrea Pietracaprina; Geppino Pucci; Matteo Riondato; Francesco Silvestri; Eli Upfal,Abstract This work explores fundamental modeling and algorithmic issues arising in the well-established MapReduce framework. First; we formally specify a computational model forMapReduce which captures the functional flavor of the paradigm by allowing for a flexibleuse of parallelism. Indeed; the model diverges from a traditional processor-centric view byfeaturing parameters which embody only global and local memory constraints; thus favoringa more data-centric view. Second; we apply the model to the fundamental computation taskof matrix multiplication presenting upper and lower bounds for both dense and sparse matrixmultiplication; which highlight interesting tradeoffs between space and round complexity.Finally; building on the matrix multiplication results; we derive further space-round tradeoffson matrix inversion and matching.,Proceedings of the 26th ACM international conference on Supercomputing,2012,47
The Case for Predictive Database Systems: Opportunities and Challenges.,Mert Akdere; Ugur Cetintemel; Matteo Riondato; Eli Upfal; Stanley B Zdonik,ABSTRACT This paper argues that next generation database management systems shouldincorporate a predictive model management component to effectively support both inward-facing applications; such as self management; and user-facing applications such as data-driven predictive analytics. We draw an analogy between model management and datamanagement functionality and discuss how model management can leverage profiling;physical design and query optimization techniques; as well as the pertinent challenges. Wethen describe the early design and architecture of Longview; a predictive DBMS prototypethat we are building at Brown; along with a case study of how models can be used to predictquery execution performance.,CIDR,2011,31
Triest: Counting local and global triangles in fully dynamic streams with fixed memory size,Lorenzo De Stefani; Alessandro Epasto; Matteo Riondato; Eli Upfal,Additional Key Words and Phrases: Cycle counting; reservoir sampling; subgraph counting ACMReference Format: Lorenzo De Stefani; Alessandro Epasto; Matteo Riondato; and Eli Upfal.2017. TRI `EST: Counting local and global triangles in fully dynamic streams with fixed memorysize. ACM Trans. Knowl. Discov. Data 11; 4; Article 43 (June 2017); 50 pages. DOI: http://dx.doi.org/10.1145/3059194 … 1. INTRODUCTION Exact computation of characteristic quantities ofWeb-scale networks is often imprac- tical or even infeasible due to the humongous size of thesegraphs. It is natural in these … This work was supported in part by the National Science Foundationgrant IIS-1247581 (https://www. nsf.gov/awardsearch/showAward?AWD_ID=1247581) and theNational Institutes of Health grant R01- CA180776 (https://projectreporter.nih.gov/project_info_details.cfm?icde=0&aid=8685211). This work is supported; in part; by funding from Two Sigma …,ACM Transactions on Knowledge Discovery from Data (TKDD),2017,30
Mining top-K frequent itemsets through progressive sampling,Andrea Pietracaprina; Matteo Riondato; Eli Upfal; Fabio Vandin,Abstract We study the use of sampling for efficiently mining the top-K frequent itemsets ofcardinality at most w. To this purpose; we define an approximation to the top-K frequentitemsets to be a family of itemsets which includes (resp.; excludes) all very frequent (resp.;very infrequent) itemsets; together with an estimate of these itemsets' frequencies with abounded error. Our first result is an upper bound on the sample size which guarantees thatthe top-K frequent itemsets mined from a random sample of that size approximate the actualtop-K frequent itemsets; with probability larger than a specified value. We show that theupper bound is asymptotically tight when w is constant. Our main algorithmic contribution isa progressive sampling approach; combined with suitable stopping conditions; which onappropriate inputs is able to extract approximate top-K frequent itemsets from samples …,Data Mining and Knowledge Discovery,2010,30
Efficient discovery of association rules and frequent itemsets through sampling with tight performance guarantees,Matteo Riondato; Eli Upfal,Abstract The tasks of extracting (top-K) Frequent Itemsets (FIs) and Association Rules (ARs)are fundamental primitives in data mining and database applications. Exact algorithms forthese problems exist and are widely used; but their running time is hindered by the need ofscanning the entire dataset; possibly multiple times. High-quality approximations of FIs andARs are sufficient for most practical uses. Sampling techniques can be used for fastdiscovery of approximate solutions; but works exploring this technique did not providesatisfactory performance guarantees on the quality of the approximation due to the difficultyof bounding the probability of under-or oversampling any one of an unknown number offrequent itemsets. We circumvent this issue by applying the statistical concept of Vapnik-Chervonenkis (VC) dimension to develop a novel technique for providing tight bounds on …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2012,28
Graph Summarization with Quality Guarantees,Matteo Riondato; David Garcıa-Soriano; Francesco Bonchi,Abstract We study the problem of graph summarization. Given a large graph we aim atproducing a concise lossy representation (a summary) that can be stored in main memoryand used to approximately answer queries about the original graph much faster than byusing the exact representation. In this work we study a very natural type of summary: theoriginal set of vertices is partitioned into a small number of supernodes connected bysuperedges to form a complete weighted graph. The superedge weights are the edgedensities between vertices in the corresponding supernodes. To quantify the dissimilaritybetween the original graph and a summary; we adopt the reconstruction error and the cut-norm error. By exposing a connection between graph summarization and geometricclustering problems (ie; k-means and k-median); we develop the first polynomial-time …,ICDM,2015,24
ABRA: Approximating betweenness centrality in static and dynamic graphs with rademacher averages,Matteo Riondato; Eli Upfal,Abstract We present ABRA; a suite of algorithms to compute and maintain probabilistically-guaranteed; high-quality; approximations of the betweenness centrality of all nodes (oredges) on both static and fully dynamic graphs. Our algorithms use progressive randomsampling and their analysis rely on Rademacher averages and pseudodimension;fundamental concepts from statistical learning theory. To our knowledge; this is the firstapplication of these concepts to the field of graph analysis. Our experimental results showthat ABRA is much faster than exact methods; and vastly outperforms; in both runtime andnumber of samples; state-of-the-art algorithms with the same quality guarantees.,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2016,20
The VC-dimension of SQL queries and selectivity estimation through sampling,Matteo Riondato; Mert Akdere; Uǧur Çetintemel; Stanley B Zdonik; Eli Upfal,Abstract We develop a novel method; based on the statistical concept of VC-dimension; forevaluating the selectivity (output cardinality) of SQL queries–a crucial step in optimizing theexecution of large scale database and data-mining operations. The major theoreticalcontribution of this work; which is of independent interest; is an explicit bound on the VC-dimension of a range space defined by all possible outcomes of a collection (class) ofqueries. We prove that the VC-dimension is a function of the maximum number of Booleanoperations in the selection predicate; and of the maximum number of select and joinoperations in any individual query in the collection; but it is neither a function of the numberof queries in the collection nor of the size of the database. We develop a method based onthis result: given a class of queries; it constructs a concise random sample of a database …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2011,15
Mining frequent itemsets through progressive sampling with rademacher averages,Matteo Riondato; Eli Upfal,Abstract We present an algorithm to extract an high-quality approximation of the (top-k)Frequent itemsets (FIs) from random samples of a transactional dataset. With highprobability the approximation is a superset of the FIs; and no itemset with frequency muchlower than the threshold is included in it. The algorithm employs progressive sampling; witha stopping condition based on bounds to the empirical Rademacher average; a key conceptfrom statistical learning theory. The computation of the bounds uses characteristic quantitiesthat can be obtained efficiently with a single scan of the sample. Therefore; evaluating thestopping condition is fast; and does not require an expensive mining of each sample. Ourexperimental evaluation confirms the practicality of our approach on real datasets;outperforming approaches based on one-shot static sampling.,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,14
Centrality measures on big graphs: Exact; approximated; and distributed algorithms,Francesco Bonchi; Gianmarco De Francisci Morales; Matteo Riondato,Abstract Centrality measures allow to measure the relative importance of a node or an edgein a graph wrt~ other nodes or edges. Several measures of centrality have been developedin the literature to capture different aspects of the informal concept of importance; andalgorithms for these different measures have been proposed. In this tutorial; we survey thedifferent definitions of centrality measures and the algorithms to compute them. We start fromthe most common measures; such as closeness centrality and betweenness centrality; andmove to more complex ones such as spanning-edge centrality. In our presentation; we beginfrom exact algorithms and then progress to approximation algorithms; including sampling-based ones; and to highly-scalable MapReduce algorithms for huge graphs; both for exactcomputation and for keeping the measures up-to-date on dynamic graphs where edges …,Proceedings of the 25th International Conference Companion on World Wide Web,2016,10
The importance of being expert: Efficient max-finding in crowdsourcing,Aris Anagnostopoulos; Luca Becchetti; Adriano Fazzone; Ida Mele; Matteo Riondato,Abstract Crowdsourcing is a computational paradigm whose distinctive feature is theinvolvement of human workers in key steps of the computation. It is used successfully toaddress problems that would be hard or impossible to solve for machines. As we highlight inthis work; the exclusive use of nonexpert individuals may prove ineffective in some cases;especially when the task at hand or the need for accurate solutions demand some degree ofspecialization to avoid excessive uncertainty and inconsistency in the answers. We addressthis limitation by proposing an approach that combines the wisdom of the crowd with theeducated opinion of experts. We present a computational model for crowdsourcing thatenvisions two classes of workers with different expertise levels. One of its distinctive featuresis the adoption of the threshold error model; whose roots are in psychometrics and which …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,9
Finding the True Frequent Itemsets,Matteo Riondato; Fabio Vandin,Abstract Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. It requiresto identify all itemsets appearing in at least a fraction θ of a transactional dataset D. Oftenthough; the ultimate goal of mining D is not an analysis of the dataset per se; but theunderstanding of the underlying process that generated it. Specifically; in many applicationsD is a collection of samples obtained from an unknown probability distribution π ontransactions; and by extracting the FIs in D one attempts to infer itemsets that are frequently(ie; with probability at least θ) generated by π; which we call the True Frequent Itemsets(TFIs). Due to the inherently stochastic nature of the generative process; the set of FIs is onlya rough approximation of the set of TFIs; as it often contains a huge number of falsepositives; ie; spurious itemsets that are not among the TFIs. In this work we design and …,arXiv preprint arXiv:1301.1218,2013,6
Sampling-based Randomized Algorithms for Big Data Analytics,Matteo Riondato,Abstract Analyzing huge datasets becomes prohibitively slow when the dataset does not fitin main memory. Approximations of the results of guaranteed high quality are sufficient formost applications and can be obtained very fast by analyzing a small random part of thedata that fits in memory. We study the use of the Vapnik-Chervonenkis dimension theory toanalyze the trade-off between the sample size and the quality of the approximation forfundamental problems in knowledge discovery (frequent itemsets); graph analysis(betweenness centrality); and database management (query selectivity). We show that thesample size to compute a high-quality approximation of the collection of frequent itemsetsdepends only on the VC-dimension of the problem; which is (tightly) bounded from above byan easy-to-compute characteristic quantity of the dataset. This bound leads to a fast …,*,2014,4
Sampling-based data mining algorithms: modern techniques and case studies,Matteo Riondato,Abstract Sampling a dataset for faster analysis and looking at it as a sample from anunknown distribution are two faces of the same coin. We discuss the use of moderntechniques involving the Vapnik-Chervonenkis (VC) dimension to study the trade-offbetween sample size and accuracy of data mining results that can be obtained from asample. We report two case studies where we and collaborators employed these techniquesto develop efficient sampling-based algorithms for the problems of betweenness centralitycomputation in large graphs and extracting statistically significant Frequent Itemsets fromtransactional datasets.,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2014,2
Wiggins: Detecting Valuable Information in Dynamic Networks Using Limited Resources,Ahmad Mahmoody; Matteo Riondato; Eli Upfal,Abstract Detecting new information and events in a dynamic network by probing individualnodes has many practical applications: discovering new webpages; analyzing influenceproperties in network; and detecting failure propagation in electronic circuits or infections inpublic drinkable water systems. In practice; it is infeasible for anyone but the owner of thenetwork (if existent) to monitor all nodes at all times. In this work we study the constrainedsetting when the observer can only probe a small set of nodes at each time step to checkwhether new pieces of information (items) have reached those nodes. We formally definethe problem through an infinite time generating process that places new items in subsets ofnodes according to an unknown probability distribution. Items have an exponentiallydecaying novelty; modeling their decreasing value. The observer uses a probing …,Proceedings of the Ninth ACM International Conference on Web Search and Data Mining,2016,1
VC-Dimension and Rademacher Averages: From Statistical Learning Theory to Sampling Algorithms,Matteo Riondato; Eli Upfal,Abstract Rademacher Averages and the Vapnik-Chervonenkis dimension are fundamentalconcepts from statistical learning theory. They allow to study simultaneous deviation boundsof empirical averages from their expectations for classes of functions; by consideringproperties of the functions; of their domain (the dataset); and of the sampling process. In thistutorial; we survey the use of Rademacher Averages and the VC-dimension in sampling-based algorithms for graph analysis and pattern mining. We start from their theoreticalfoundations at the core of machine learning; then show a generic recipe for formulating datamining problems in a way that allows to use these concepts in efficient randomizedalgorithms for those problems. Finally; we show examples of the application of the recipe tograph problems (connectivity; shortest paths; betweenness centrality) and pattern mining …,Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2015,*
