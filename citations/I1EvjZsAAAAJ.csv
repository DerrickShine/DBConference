A view of cloud computing,Michael Armbrust; Armando Fox; Rean Griffith; Anthony D Joseph; Randy Katz; Andy Konwinski; Gunho Lee; David Patterson; Ariel Rabkin; Ion Stoica; Matei Zaharia,Cloud computing; the long-held dream of computing as a utility; has the potential to transforma large part of the IT industry; making software even more attractive as a service and shapingthe way IT hardware is designed and purchased. Developers with innovative ideas for new Internetservices no longer require the large capital outlays in hardware to deploy their service or thehuman expense to operate it. They need not be concerned about overprovisioning for a servicewhose popularity does not meet their predictions; thus wasting costly resources; or underprovisioningfor one that becomes wildly popular; thus missing potential customers and revenue.Moreover; companies with large batch-oriented tasks can get results as quickly as their programscan scale; since using 1;000 servers for one hour costs no more than using one server for 1;000hours. This elasticity of resources; without paying a premium for large scale; is …,Communications of the ACM,2010,15300
Spark: Cluster computing with working sets.,Matei Zaharia; Mosharaf Chowdhury; Michael J Franklin; Scott Shenker; Ion Stoica,Abstract MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; most of these systemsare built around an acyclic data flow model that is not suitable for other popular applications.This paper focuses on one such class of applications: those that reuse a working set of dataacross multiple parallel operations. This includes many iterative machine learningalgorithms; as well as interactive data analysis tools. We propose a new framework calledSpark that supports these applications while retaining the scalability and fault tolerance ofMapReduce. To achieve these goals; Spark introduces an abstraction called resilientdistributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across aset of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by …,HotCloud,2010,3064
Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing,Matei Zaharia; Mosharaf Chowdhury; Tathagata Das; Ankur Dave; Justin Ma; Murphy McCauley; Michael J Franklin; Scott Shenker; Ion Stoica,Abstract We present Resilient Distributed Datasets (RDDs); a distributed memory abstractionthat lets programmers perform in-memory computations on large clusters in a fault-tolerantmanner. RDDs are motivated by two types of applications that current computing frameworkshandle inefficiently: iterative algorithms and interactive data mining tools. In both cases;keeping data in memory can improve performance by an order of magnitude. To achievefault tolerance efficiently; RDDs provide a restricted form of shared memory; based oncoarse-grained transformations rather than fine-grained updates to shared state. However;we show that RDDs are expressive enough to capture a wide class of computations;including recent specialized programming models for iterative jobs; such as Pregel; and newapplications that these models do not capture. We have implemented RDDs in a system …,Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,2012,2692
Improving MapReduce performance in heterogeneous environments.,Matei Zaharia; Andy Konwinski; Anthony D Joseph; Randy H Katz; Ion Stoica,Abstract MapReduce is emerging as an important programming model for large-scale data-parallel applications such as web indexing; data mining; and scientific simulation. Hadoop isan open-source implementation of MapReduce enjoying wide adoption and is often used forshort jobs where low response time is critical. Hadoop's performance is closely tied to itstask scheduler; which implicitly assumes that cluster nodes are homogeneous and tasksmake progress linearly; and uses these assumptions to decide when to speculatively re-execute tasks that appear to be stragglers. In practice; the homogeneity assumptions do notalways hold. An especially compelling setting where this occurs is a virtualized data center;such as Amazon's Elastic Compute Cloud (EC2). We show that Hadoop's scheduler cancause severe performance degradation in heterogeneous environments. We design a …,Osdi,2008,1651
Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling,Matei Zaharia; Dhruba Borthakur; Joydeep Sen Sarma; Khaled Elmeleegy; Scott Shenker; Ion Stoica,Abstract As organizations start to use data-intensive cluster computing systems like Hadoopand Dryad for more applications; there is a growing need to share clusters between users.However; there is a conflict between fairness in scheduling and data locality (placing taskson nodes that contain their input data). We illustrate this problem through our experiencedesigning a fair scheduler for a 600-node Hadoop cluster at Facebook. To address theconflict between locality and fairness; we propose a simple algorithm called delayscheduling: when the job that should be scheduled next according to fairness cannot launcha local task; it waits for a small amount of time; letting other jobs launch tasks instead. Wefind that delay scheduling achieves nearly optimal data locality in a variety of workloads andcan increase throughput by up to 2x while preserving fairness. In addition; the simplicity of …,Proceedings of the 5th European conference on Computer systems,2010,1230
Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center.,Benjamin Hindman; Andy Konwinski; Matei Zaharia; Ali Ghodsi; Anthony D Joseph; Randy H Katz; Scott Shenker; Ion Stoica,Abstract We present Mesos; a platform for sharing commodity clusters between multiplediverse cluster computing frameworks; such as Hadoop and MPI. Sharing improves clusterutilization and avoids per-framework data replication. Mesos shares resources in a fine-grained manner; allowing frameworks to achieve data locality by taking turns reading datastored on each machine. To support the sophisticated schedulers of today's frameworks;Mesos introduces a distributed two-level scheduling mechanism called resource offers.Mesos decides how many resources to offer each framework; while frameworks decidewhich resources to accept and which computations to run on them. Our results show thatMesos can achieve near-optimal data locality when sharing the cluster among diverseframeworks; can scale to 50;000 (emulated) nodes; and is resilient to failures.,NSDI,2011,1072
Dominant Resource Fairness: Fair Allocation of Multiple Resource Types.,Ali Ghodsi; Matei Zaharia; Benjamin Hindman; Andy Konwinski; Scott Shenker; Ion Stoica,Abstract We consider the problem of fair resource allocation in a system containing differentresource types; where each user may have different demands for each resource. To addressthis problem; we propose Dominant Resource Fairness (DRF); a generalization of max-minfairness to multiple resource types. We show that DRF; unlike other possible policies;satisfies several highly desirable properties. First; DRF incentivizes users to shareresources; by ensuring that no user is better off if resources are equally partitioned amongthem. Second; DRF is strategy-proof; as a user cannot increase her allocation by lying abouther requirements. Third; DRF is envyfree; as no user would want to trade her allocation withthat of another user. Finally; DRF allocations are Pareto efficient; as it is not possible toimprove the allocation of a user without decreasing the allocation of another user. We …,Nsdi,2011,681
Discretized streams: Fault-tolerant streaming computation at scale,Matei Zaharia; Tathagata Das; Haoyuan Li; Timothy Hunter; Scott Shenker; Ion Stoica,Abstract Many" big data" applications must act on data in real time. Running theseapplications at ever-larger scales requires parallel platforms that automatically handle faultsand stragglers. Unfortunately; current distributed stream processing models provide faultrecovery in an expensive manner; requiring hot replication or long recovery times; and donot handle stragglers. We propose a new processing model; discretized streams (D-Streams); that overcomes these challenges. D-Streams enable a parallel recoverymechanism that improves efficiency over traditional replication and backup schemes; andtolerates stragglers. We show that they support a rich set of operators while attaining highper-node throughput similar to single-node systems; linear scaling to 100 nodes; sub-second latency; and sub-second fault recovery. Finally; D-Streams can easily be …,Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles,2013,550
Mllib: Machine learning in apache spark,Xiangrui Meng; Joseph Bradley; Burak Yavuz; Evan Sparks; Shivaram Venkataraman; Davies Liu; Jeremy Freeman; DB Tsai; Manish Amde; Sean Owen; Doris Xin; Reynold Xin; Michael J Franklin; Reza Zadeh; Matei Zaharia; Ameet Talwalkar,Abstract Apache Spark is a popular open-source platform for large-scale data processingthat is well-suited for iterative machine learning tasks. In this paper we present MLlib;Spark's open-source distributed machine learning library. MLlib provides efficientfunctionality for a wide range of learning settings and includes several underlying statistical;optimization; and linear algebra primitives. Shipped with Spark; MLlib supports severallanguages and provides a high-level API that leverages Spark's rich ecosystem to simplifythe development of end-to-end machine learning pipelines. MLlib has experienced a rapidgrowth due to its vibrant open-source community of over 140 contributors; and includesextensive documentation to support further growth and to let users quickly get up to speed.,The Journal of Machine Learning Research,2016,511
Spark sql: Relational data processing in spark,Michael Armbrust; Reynold S Xin; Cheng Lian; Yin Huai; Davies Liu; Joseph K Bradley; Xiangrui Meng; Tomer Kaftan; Michael J Franklin; Ali Ghodsi; Matei Zaharia,Abstract Spark SQL is a new module in Apache Spark that integrates relational processingwith Spark's functional programming API. Built on our experience with Shark; Spark SQL letsSpark programmers leverage the benefits of relational processing (eg declarative queriesand optimized storage); and lets SQL users call complex analytics libraries in Spark (egmachine learning). Compared to previous systems; Spark SQL makes two main additions.First; it offers much tighter integration between relational and procedural processing; througha declarative DataFrame API that integrates with procedural Spark code. Second; it includesa highly extensible optimizer; Catalyst; built using features of the Scala programminglanguage; that makes it easy to add composable rules; control code generation; and defineextension points. Using Catalyst; we have built a variety of features (eg schema inference …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,466
Job scheduling for multi-user mapreduce clusters,Matei Zaharia; Dhruba Borthakur; J Sen Sarma; Khaled Elmeleegy; Scott Shenker; Ion Stoica,Abstract Sharing a MapReduce cluster between users is attractive because it enablesstatistical multiplexing (lowering costs) and allows users to share a common large data set.However; we find that traditional scheduling algorithms can perform very poorly inMapReduce due to two aspects of the MapReduce setting: the need for data locality(running computation where the data is) and the dependence between map and reducetasks. We illustrate these problems through our experience designing a fair scheduler forMapReduce at Facebook; which runs a 600-node multiuser data warehouse on Hadoop. Wedeveloped two simple techniques; delay scheduling and copy-compute splitting; whichimprove throughput and response times by factors of 2 to 10. Although we focus on multi-user workloads; our techniques can also raise throughput in a single-user; FIFO workload …,*,2009,430
Shark: SQL and rich analytics at scale,Reynold S Xin; Josh Rosen; Matei Zaharia; Michael J Franklin; Scott Shenker; Ion Stoica,Abstract Shark is a new data analysis system that marries query processing with complexanalytics on large clusters. It leverages a novel distributed memory abstraction to provide aunified engine that can run SQL queries and sophisticated analytics functions (eg iterativemachine learning) at scale; and efficiently recovers from failures mid-query. This allowsShark to run SQL queries up to 100X faster than Apache Hive; and machine learningprograms more than 100X faster than Hadoop. Unlike previous systems; Shark shows that itis possible to achieve these speedups while retaining a MapReduce-like execution engine;and the fine-grained fault tolerance properties that such engine provides. It extends such anengine in several ways; including column-oriented in-memory storage and dynamic mid-query replanning; to effectively execute SQL. The result is a system that matches the …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of data,2013,411
Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters,Matei Zaharia; Tathagata Das; Haoyuan Li; Scott Shenker; Ion Stoica,Abstract Many important “big data” applications need to process data arriving in real time.However; current programming models for distributed stream processing are relatively low-level; often leaving the user to worry about consistency of state across the system and faultrecovery. Furthermore; the models that provide fault recovery do so in an expensive manner;requiring either hot replication or long recovery times. We propose a new programmingmodel; discretized streams (D-Streams); that offers a high-level functional programming API;strong consistency; and efficient fault recovery. D-Streams support a new recoverymechanism that improves efficiency over the traditional replication and upstream backupsolutions in streaming databases: parallel recovery of lost state across the cluster. We haveprototyped D-Streams in an extension to the Spark cluster computing framework called …,Proceedings of the 4th USENIX conference on Hot Topics in Cloud Computing,2012,406
Managing data transfers in computer clusters with orchestra,Mosharaf Chowdhury; Matei Zaharia; Justin Ma; Michael I Jordan; Ion Stoica,Abstract Cluster computing applications like MapReduce and Dryad transfer massiveamounts of data between their computation stages. These transfers can have a significantimpact on job performance; accounting for more than 50% of job completion times. Despitethis impact; there has been relatively little work on optimizing the performance of these datatransfers; with networking researchers traditionally focusing on per-flow traffic management.We address this limitation by proposing a global management architecture and a set ofalgorithms that (1) improve the transfer times of common communication patterns; such asbroadcast and shuffle; and (2) allow scheduling policies at the transfer level; such asprioritizing a transfer over other transfers. Using a prototype implementation; we show thatour solution improves broadcast completion times by up to 4.5 X compared to the status …,SIGCOMM,2011,396
Low-cost communication for rural internet kiosks using mechanical backhaul,Aaditeshwar Seth; Darcy Kroeker; Matei Zaharia; Shimin Guo; Srinivasan Keshav,Abstract Rural kiosks in developing countries provide a variety of services such as birth;marriage; and death certificates; electricity bill collection; land records; email services; andconsulting on medical and agricultural problems. Fundamental to a kiosk's operation is itsconnection to the Internet. Network connectivity today is primarily provided by dialuptelephone; although Very Small Aperture Terminals (VSAT) or long-distance wireless linksare also being deployed. These solutions tend to be both expensive and failure prone.Instead; we propose the use of buses and cars as" mechanical backhaul" devices to carrydata to and from a village and an internet gateway. Building on the pioneering lead ofDaknet [15]; and extending the Delay Tolerant Networking Research Group architecture [24];we describe a comprehensive solution; encompassing naming; addressing; forwarding …,Proceedings of the 12th annual international conference on Mobile computing and networking,2006,282
Sparrow: distributed; low latency scheduling,Kay Ousterhout; Patrick Wendell; Matei Zaharia; Ion Stoica,Abstract Large-scale data analytics frameworks are shifting towards shorter task durationsand larger degrees of parallelism to provide low latency. Scheduling highly parallel jobs thatcomplete in hundreds of milliseconds poses a major challenge for task schedulers; whichwill need to schedule millions of tasks per second on appropriate machines while offeringmillisecond-level latency and high availability. We demonstrate that a decentralized;randomized sampling approach provides near-optimal performance while avoiding thethroughput and availability limitations of a centralized design. We implement and deploy ourscheduler; Sparrow; on a 110-machine cluster and demonstrate that Sparrow performswithin 12% of an ideal scheduler.,Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles,2013,256
Learning spark: lightning-fast big data analysis,Holden Karau; Andy Konwinski; Patrick Wendell; Matei Zaharia,Data in all domains is getting bigger. How can you work with it efficiently? Recently updatedfor Spark 1.3; this book introduces Apache Spark; the open source cluster computing systemthat makes data analytics fast to write and fast to run. With Spark; you can tackle big datasetsquickly through simple APIs in Python; Java; and Scala. This edition includes newinformation on Spark SQL; Spark Streaming; setup; and Maven coordinates. Written by thedevelopers of Spark; this book will have data scientists and engineers up and running in notime. You'll learn how to express parallel jobs with just a few lines of code; and coverapplications from simple batch jobs to stream processing and machine learning. Quicklydive into Spark capabilities such as distributed datasets; in-memory caching; and theinteractive shell Leverage Spark's powerful built-in libraries; including Spark SQL; Spark …,*,2015,218
A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples,Samia N Naccache; Scot Federman; Narayanan Veeraraghavan; Matei Zaharia; Deanna Lee; Erik Samayoa; Jerome Bouquet; Alexander L Greninger; Ka-Cheung Luk; Barryett Enge; Debra A Wadford; Sharon L Messenger; Gillian L Genrich; Kristen Pellegrino; Gilda Grard; Eric Leroy; Bradley S Schneider; Joseph N Fair; Miguel A Martínez; Pavel Isa; John A Crump; Joseph L DeRisi; Taylor Sittler; John Hackett; Steve Miller; Charles Y Chiu,Abstract Unbiased next-generation sequencing (NGS) approaches enable comprehensivepathogen detection in the clinical microbiology laboratory and have numerous applicationsfor public health surveillance; outbreak investigation; and the diagnosis of infectiousdiseases. However; practical deployment of the technology is hindered by the bioinformaticschallenge of analyzing results accurately and in a clinically relevant timeframe. Here wedescribe SURPI (“sequence-based ultrarapid pathogen identification”); a computationalpipeline for pathogen identification from complex metagenomic NGS data generated fromclinical samples; and demonstrate use of the pipeline in the analysis of 237 clinical samplescomprising more than 1.1 billion sequences. Deployable on both cloud-based andstandalone servers; SURPI leverages two state-of-the-art aligners for accelerated …,Genome research,2014,192
Tachyon: Reliable; memory speed storage for cluster computing frameworks,Haoyuan Li; Ali Ghodsi; Matei Zaharia; Scott Shenker; Ion Stoica,Abstract Tachyon is a distributed file system enabling reliable data sharing at memory speedacross cluster computing frameworks. While caching today improves read workloads; writesare either network or disk bound; as replication is used for fault-tolerance. Tachyoneliminates this bottleneck by pushing lineage; a well-known technique; into the storagelayer. The key challenge in making a long-running lineage-based storage system is timelydata recovery in case of failures. Tachyon addresses this issue by introducing acheckpointing algorithm that guarantees bounded recovery cost and resource allocationstrategies for recomputation under commonly used resource schedulers. Our evaluationshows that Tachyon outperforms in-memory HDFS by 110x for writes. It also improves theend-to-end latency of a realistic workflow by 4x. Tachyon is open source and is deployed …,Proceedings of the ACM Symposium on Cloud Computing,2014,176
Apache spark: a unified engine for big data processing,Matei Zaharia; Reynold S Xin; Patrick Wendell; Tathagata Das; Michael Armbrust; Ankur Dave; Xiangrui Meng; Josh Rosen; Shivaram Venkataraman; Michael J Franklin; Ali Ghodsi; Joseph Gonzalez; Scott Shenker; Ion Stoica,The growth of data volumes in industry and research poses tremendous opportunities; as wellas tremendous computational challenges. As data sizes have outpaced the capabilities of singlemachines; users have needed new systems to scale out computations to multiple nodes. As aresult; there has been an explosion of new cluster programming models targeting diverse computingworkloads. 1;4;7;10 At first; these models were relatively specialized; with new models developedfor new workloads; for example; MapReduce 4 supported batch processing; but Google alsodeveloped Dremel 13 for interactive SQL queries and Pregel 11 for iterative graphalgorithms. In the open source Apache Hadoop stack; systems like Storm 1 and Impala 9 arealso specialized. Even in the relational database world; the trend has been to move away from"one-size-fits-all" systems. 18 Unfortunately; most big data applications need to combine …,Communications of the ACM,2016,138
Multi-resource fair queueing for packet processing,Ali Ghodsi; Vyas Sekar; Matei Zaharia; Ion Stoica,Abstract Middleboxes are ubiquitous in today's networks and perform a variety of importantfunctions; including IDS; VPN; firewalling; and WAN optimization. These functions differvastly in their requirements for hardware resources (eg; CPU cycles and memorybandwidth). Thus; depending on the functions they go through; different flows can consumedifferent amounts of a middlebox's resources. While there is much literature on weighted fairsharing of link bandwidth to isolate flows; it is unclear how to schedule multiple resources ina middlebox to achieve similar guarantees. In this paper; we analyze several natural packetscheduling algorithms for multiple resources and show that they have undesirableproperties. We propose a new algorithm; Dominant Resource Fair Queuing (DRFQ); thatretains the attractive properties that fair sharing provides for one resource. In doing so; we …,ACM SIGCOMM Computer Communication Review,2012,132
Shark: fast data analysis using coarse-grained distributed memory,Cliff Engle; Antonio Lupher; Reynold Xin; Matei Zaharia; Michael J Franklin; Scott Shenker; Ion Stoica,Abstract Shark is a research data analysis system built on a novel coarse-grained distributedshared-memory abstraction. Shark marries query processing with deep data analysis;providing a unified system for easy data manipulation using SQL and pushing sophisticatedanalysis closer to data. It scales to thousands of nodes in a fault-tolerant manner. Shark cananswer queries 40X faster than Apache Hive and run machine learning programs 25X fasterthan MapReduce programs in Apache Hadoop on large datasets.,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,121
Choosy: Max-min fair sharing for datacenter jobs with constraints,Ali Ghodsi; Matei Zaharia; Scott Shenker; Ion Stoica,Abstract Max-Min Fairness is a flexible resource allocation mechanism used in mostdatacenter schedulers. However; an increasing number of jobs have hard placementconstraints; restricting the machines they can run on due to special hardware or softwarerequirements. It is unclear how to define; and achieve; max-min fairness in the presence ofsuch constraints. We propose Constrained Max-Min Fairness (CMMF); an extension to max-min fairness that supports placement constraints; and show that it is the only policy satisfyingan important property that incentivizes users to pool resources. Optimally computing CMMFis challenging; but we show that a remarkably simple online scheduler; called Choosy;approximates the optimal scheduler well. Through experiments; analysis; and simulations;we show that Choosy on average differs 2% from the optimal CMMF allocation; and lets …,Proceedings of the 8th ACM European Conference on Computer Systems,2013,111
Fast and interactive analytics over Hadoop data with Spark,Matei Zaharia; Mosharaf Chowdhury; Tathagata Das; Ankur Dave; Justin Ma; Murphy Mccauley; M Franklin; Scott Shenker; Ion Stoica,Spark started out of our research group's discussions with Hadoop users at and outside UCBerkeley. We saw that as organizations began loading more data into Hadoop; they quicklywanted to run rich applications that the single-pass; batch processing model of MapReducedoes not support efficiently. In particular; users wanted to run: u More complex; multi-passalgorithms; such as the iterative algorithms that are common in machine learning and graphprocessing u More interactive ad hoc queries to explore the data Although theseapplications may at first appear quite different; the core problem is that both multi-pass andinteractive applications need to share data across multiple MapReduce steps (eg; multiplequeries from the user; or multiple steps of an iterative computation). Unfortunately; the onlyway to share data between parallel operations in MapReduce is to write it to a distributed …,Usenix Login,2012,110
Gossip‐based search selection in hybrid peer‐to‐peer networks,Matei Zaharia; Srinivasan Keshav,Abstract We present GAB; a search algorithm for hybrid peer-to-peer networks; that is;networks that search using both flooding and a distributed hash table (DHT). GAB uses agossip-style algorithm to collect global statistics about document popularity to allow eachpeer to make intelligent decisions about which search style to use for a given query.Moreover; GAB automatically adapts to changes in the operating environment. Synthetic andtrace-driven simulations show that compared to a simple hybrid approach that always floodsfirst; trying a DHT if too few results are found; GAB reduces the response time by 25–50%and the average query bandwidth cost by 45%; with no loss in recall. GAB scales well; withonly a 7% degradation in performance despite a tripling in system size. Copyright© 2007John Wiley & Sons; Ltd.,Concurrency and Computation: Practice and Experience,2008,90
Very low-cost internet access using KioskNet,Shimin Guo; Mohammad Hossein Falaki; Earl A Oliver; S Ur Rahman; Aaditeshwar Seth; Matei A Zaharia; Srinivasan Keshav,Abstract Rural Internet kiosks in developing regions can cost-effectively providecommunication and e-governance services to the poorest sections of society. A variety oftechnical and non-technical issues have caused most kiosk deployments to be economicallyunsustainable [1]. KioskNet addresses the key technical problems underlying kiosk failureby using robust'mechanical backhaul'for connectivity [2]; and by using low-cost and reliablekiosk-controllers to support services delivered from one or more recycled PCs. KioskNet alsoaddresses related issues such as security; user management; and log collection. In thispaper; we describe the KioskNet system; outlining its hardware; software; and securityarchitecture. We describe a pilot deployment; and how we used lessons from thisdeployment to re-design our initial proto-type.,ACM SIGCOMM Computer Communication Review,2007,88
Faster and more accurate sequence alignment with SNAP,Matei Zaharia; William J Bolosky; Kristal Curtis; Armando Fox; David Patterson; Scott Shenker; Ion Stoica; Richard M Karp; Taylor Sittler,Abstract: We present the Scalable Nucleotide Alignment Program (SNAP); a new short andlong read aligner that is both more accurate (ie; aligns more reads with fewer errors) and 10-100x faster than state-of-the-art tools such as BWA. Unlike recent aligners based on theBurrows-Wheeler transform; SNAP uses a simple hash index of short seed sequences fromthe genome; similar to BLAST's. However; SNAP greatly reduces the number and cost oflocal alignment checks performed through several measures: it uses longer seeds to reducethe false positive locations considered; leverages larger memory capacities to speed indexlookup; and excludes most candidate locations without fully computing their edit distance tothe read. The result is an algorithm that scales well for reads from one hundred to thousandsof bases long and provides a rich error model that can match classes of mutations (eg …,arXiv preprint arXiv:1111.5572,2011,76
Design and implementation of the KioskNet system,Shimin Guo; Mohammad Derakhshani; Mohammad Hossein Falaki; Usman Ismail; Rowena Luk; Earl A Oliver; S Ur Rahman; Aaditeshwar Seth; Matei A Zaharia; Srinivasan Keshav,Abstract Rural Internet kiosks in developing regions can cost-effectively providecommunication and information services to the poorest sections of society. Yet; a variety oftechnical and non-technical issues have caused most kiosk deployments to be economicallyunsustainable. KioskNet addresses the key technical problems underlying kiosk failure byusing robust 'mechanical backhaul'for connectivity; and by using low-cost and reliable kioskcontrollers to support services delivered from one or more recycled PCs. KioskNet alsoaddresses related issues such as security; user management; and log collection. In thispaper; we describe the KioskNet system; outlining its hardware; software; and securityarchitecture. We describe a pilot deployment and how we used lessons from thisdeployment to re-design our initial prototype.,Computer Networks,2011,73
Vuvuzela: Scalable private messaging resistant to traffic analysis,Jelle Van Den Hooff; David Lazar; Matei Zaharia; Nickolai Zeldovich,Abstract Private messaging over the Internet has proven challenging to implement; becauseeven if message data is encrypted; it is difficult to hide metadata about who iscommunicating in the face of traffic analysis. Systems that offer strong privacy guarantees;such as Dissent [36]; scale to only several thousand clients; because they use techniqueswith superlinear cost in the number of clients (eg; each client broadcasts their message to allother clients). On the other hand; scalable systems; such as Tor; do not protect against trafficanalysis; making them ineffective in an era of pervasive network monitoring. Vuvuzela is anew scalable messaging system that offers strong privacy guarantees; hiding both messagedata and metadata. Vuvuzela is secure against adversaries that observe and tamper with allnetwork traffic; and that control all nodes except for one server. Vuvuzela's key insight is …,Proceedings of the 25th Symposium on Operating Systems Principles,2015,61
Scaling the mobile millennium system in the cloud,Timothy Hunter; Teodor Moldovan; Matei Zaharia; Samy Merzgui; Justin Ma; Michael J Franklin; Pieter Abbeel; Alexandre M Bayen,Abstract We report on our experience scaling up the Mobile Millennium traffic informationsystem using cloud computing and the Spark cluster computing framework. MobileMillennium uses machine learning to infer traffic conditions for large metropolitan areas fromcrowdsourced data; and Spark was specifically designed to support such applications. Manystudies of cloud computing frameworks have demonstrated scalability and performanceimprovements for simple machine learning algorithms. Our experience implementing a real-world machine learning-based application corroborates such benefits; but we alsoencountered several challenges that have not been widely reported. These include:managing large parameter vectors; using memory efficiently; and integrating with theapplication's existing storage infrastructure. This paper describes these challenges and …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,57
An architecture for fast and general data processing on large clusters,Matei Zaharia,The past few years have seen a major change in computing systems; as growing datavolumes and stalling processor speeds require more and more applications to scale out toclusters. Today; a myriad data sources; from the Internet to business operations to scientificinstruments; produce large and valuable data streams. However; the processing capabilitiesof single machines have not kept up with the size of data. As a result; organizationsincreasingly need to scale out their computations over clusters. At the same time; the speedand sophistication required of data processing have grown. In addition to simple queries;complex algorithms like machine learning and graph analysis are becoming common. Andin addition to batch processing; streaming analysis of real-time data is required to letorganizations take timely action. Future computing platforms will need to not only scale …,*,2016,54
Scaling spark in the real world: performance and usability,Michael Armbrust; Tathagata Das; Aaron Davidson; Ali Ghodsi; Andrew Or; Josh Rosen; Ion Stoica; Patrick Wendell; Reynold Xin; Matei Zaharia,Abstract Apache Spark is one of the most widely used open source processing engines forbig data; with rich language-integrated APIs and a wide range of libraries. Over the past twoyears; our group has worked to deploy Spark to a wide range of organizations throughconsulting relationships as well as our hosted service; Databricks. We describe the mainchallenges and requirements that appeared in taking Spark to a wide set of users; andusability and performance improvements we have made to the engine in response.,Proceedings of the VLDB Endowment,2015,45
A Common Substrate for Cluster Computing.,Benjamin Hindman; Andy Konwinski; Matei Zaharia; Ion Stoica,Abstract The success of MapReduce has sparked many efforts to design cluster computingframeworks. We argue that no single framework will be optimal for all applications; and thatwe should instead enable organizations to run multiple frameworks efficiently in the samecloud. Furthermore; to ease development of new frameworks; it is critical to identify commonabstractions and modularize their architectures. To achieve these goals; we propose Nexus;a low-level substrate that provides isolation and efficient resource sharing acrossframeworks running on the same cluster; while giving each framework freedom to implementits own programming model and fully control the execution of its jobs. Nexus fostersinnovation in the cloud by letting organizations run new frameworks alongside existing onesand by letting framework developers focus on specific applications rather than building …,HotCloud,2009,42
Job scheduling with the fair and capacity schedulers,Matei Zaharia,Page 1. UC Berkeley Job Scheduling with the Fair and Capacity Schedulers Matei ZahariaWednesday; June 10; 2009 Santa Clara Marriott Page 2. Motivation » Provide fast response timesto small jobs in a shared Hadoop cluster » Improve utilization and data locality over separateclusters and Hadoop on Demand Page 3. Hadoop at Facebook » 600-node cluster running Hive »3200 jobs/day » 50+ users » Apps: statistical reports; spam detection; ad optimization; … Page4. Facebook Job Types » Production jobs: data import; hourly reports; etc » Small ad-hoc jobs:Hive queries; sampling » Long experimental jobs: machine learning; etc ➢ GOAL: fast responsetimes for small jobs; guaranteed service levels for production jobs Page 5. Outline » Fair schedulerbasics » Configuring the fair scheduler » Capacity scheduler » Useful links Page 6. FIFO SchedulingJob Queue Page 7. FIFO Scheduling Job Queue Page 8 …,Hadoop Summit,2009,39
Finding Content in File-Sharing Networks When You Can't Even Spell.,Matei A Zaharia; Amit Chandel; Stefan Saroiu; Srinivasan Keshav,Abstract: The query success rate in current filesharing systems is low; for example; only 7-10% in Gnutella. An often-overlooked cause for this low recall is simply that keywords inqueries and document descriptions are misspelled. Although many sophisticatedapproximate matching techniques have been developed by the Information Retrievalcommunity; to our knowledge; they have not been used in popular P2P systems. Wepropose two approaches to improving query recall in file-sharing systems. For unstructuredP2P networks; we show that “q-gram”-based approaches nearly double recall with little lossin precision. Unfortunately; such techniques cannot be used in structured P2P networks.Instead; we propose a simple alternative: encoding keywords using Soundex; a century-oldphonetic algorithm for indexing names by their sound. We evaluate both approaches on a …,IPTPS,2007,39
Cloud Terminal: Secure Access to Sensitive Applications from Untrusted Systems.,Lorenzo Martignoni; Pongsin Poosankam; Matei Zaharia; Jun Han; Stephen McCamant; Dawn Song; Vern Paxson; Adrian Perrig; Scott Shenker; Ion Stoica,Abstract Current PC-and web-based applications provide insufficient security for theinformation they access; because vulnerabilities anywhere in a large client software stackcan compromise confidentiality and integrity. We propose a new architecture for secureapplications; Cloud Terminal; in which the only software running on the end host is alightweight secure thin terminal; and most application logic is in a remote cloud renderingengine. The secure thin terminal has a very small TCB (23 KLOC) and no dependence onthe untrusted OS; so it can be easily checked and remotely attested to. The terminal is alsogeneral-purpose: it simply supplies a secure display and input path to remote software. Thecloud rendering engine runs an off-the-shelf application in a restricted VM hosted by theprovider; but resource sharing between VMs lets one server support hundreds of users …,USENIX Annual Technical Conference,2012,36
Thermophysical and anion diffusion properties of (Ux; Th1− x) O2,Michael WD Cooper; Samuel T Murphy; Paul CM Fossati; Michael JD Rushton; Robin W Grimes,Using molecular dynamics; the thermophysical properties of the (U x; Th 1− x) O 2 systemhave been investigated between 300 and 3600 K. The thermal dependence of latticeparameter; linear thermal expansion coefficient; enthalpy and specific heat at constantpressure is explained in terms of defect formation and diffusivity on the oxygen sublattice.Vegard9s law is approximately observed for solid solution thermal expansion below 2000 K.Different deviations from Vegard9s law above this temperature occur owing to the differenttemperatures at which the solid solutions undergo the superionic transition (2500–3300 K).Similarly; a spike in the specific heat; associated with the superionic transition; occurs atlower temperatures in solid solutions that have a high U content. Correspondingly; oxygendiffusivity is higher in pure UO 2 than in pure ThO 2. Furthermore; at temperatures below …,Proceedings of the Royal Society of London A: Mathematical; Physical and Engineering Sciences,2014,31
Tachyon: Memory throughput i/o for cluster computing frameworks,Haoyuan Li; Ali Ghodsi; Matei Zaharia; Eric Baldeschwieler; Scott Shenker; Ion Stoica,Abstract As ever more big data computations start to be in-memory; I/O throughputdominates the running times of many workloads. For distributed storage; the read throughputcan be improved using caching; however; the write throughput is limited by both disk andnetwork bandwidth due to data replication for fault-tolerance. This paper proposes a new filesystem architecture to enable frameworks to both read and write reliably at memory speed;by avoiding synchronous data replication on writes.,memory,2013,31
Discretized streams: A fault-tolerant model for scalable stream processing,Matei Zaharia; Tathagata Das; Haoyuan Li; Timothy Hunter; Scott Shenker; Ion Stoica,Abstract: Many big data applications need to act on data arriving in real time. However;current programming models for distributed stream processing are relatively low-level oftenleaving the user to worry about consistency of state across the system and fault recovery.Furthermore; the models that provide fault recovery do so in an expensive manner; requiringeither hot replication or long recovery times. We propose a new programming modeldiscretized streams (D-Streams); that offers a high-level functional API; strong consistency;and efficient fault recovery. D-Streams support a new recovery mechanism that improvesefficiency over the traditional replication and upstream backup schemes in streamingdatabases-parallel recovery of lost state-and unlike previous systems also mitigatestragglers. We implement D-Streams as an extension to the Spark cluster computing …,*,2012,31
Matrix computations and optimization in apache spark,Reza Bosagh Zadeh; Xiangrui Meng; Alexander Ulanov; Burak Yavuz; Li Pu; Shivaram Venkataraman; Evan Sparks; Aaron Staple; Matei Zaharia,Abstract We describe matrix computations available in the cluster programming framework;Apache Spark. Out of the box; Spark provides abstractions and implementations fordistributed matrices and optimization routines using these matrices. When translating single-node algorithms to run on a distributed cluster; we observe that often a simple idea isenough: separating matrix operations from vector operations and shipping the matrixoperations to be ran on the cluster; while keeping vector operations local to the driver. In thecase of the Singular Value Decomposition; by taking this idea to an extreme; we are able toexploit the computational power of a cluster; while running code written decades ago for asingle core. Another example is our Spark port of the popular TFOCS optimization package;originally built for MATLAB; which allows for solving Linear programs as well as a variety …,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,2016,29
The datacenter needs an operating system,Matei Zaharia; Benjamin Hindman; Andy Konwinski; Ali Ghodsi; Anthony D Joesph; Randy Katz; Scott Shenker; Ion Stoica,*,Proceedings of the 3rd USENIX conference on Hot topics in cloud computing,2011,29
Fast and optimal scheduling over multiple network interfaces,M Zaharia; Srinivasan Keshav,Abstract Most of today's mobile devices have at least two wireless network interface cards(NICs). Future devices are expected to have as many as twelve wireless NICs; with differentdata transmission capacities and a wide range of energy and dollar costs. When multi-NICdevices are running multiple applications; each unit of application data ought to bescheduled on to the network interface that maximizes user satisfaction. The schedulingalgorithm must take into account not only user and application preferences; such as usercost-tolerance and application delay-tolerance; but also future availability of NICs due todevice mobility. Furthermore; to run on small; resource-limited devices; it must be highlyefficient. We addressed the problem of efficiently computing an optimal transmissionschedule if future connectivity is known. We showed that simple greedy algorithms are …,University of Waterloo Technical Report; CS-2007-36,2007,27
Large-scale estimation in cyberphysical systems using streaming data: a case study with arterial traffic estimation,Timothy Hunter; Tathagata Das; Matei Zaharia; Pieter Abbeel; Alexandre M Bayen,Controlling and analyzing cyberphysical and robotics systems is increasingly becoming aBig Data challenge. We study the case of predicting drivers' travel times in a large urbanarea from sparse GPS traces. We present a framework that can accommodate a wide varietyof traffic distributions and spread all the computations on a cluster to achieve small latencies.Our framework is built on Discretized Streams; a recently proposed approach to streamprocessing at scale. We demonstrate the usefulness of Discretized Streams with a novelalgorithm to estimate vehicular traffic in urban networks. Our online EM algorithm canestimate traffic on a very large city network (the San Francisco Bay Area) by processing tensof thousands of observations per second; with a latency of a few seconds.,IEEE Transactions on Automation Science and Engineering,2013,26
ICTD for healthcare in Ghana: two parallel case studies,Rowena Luk; Matei Zaharia; Melissa Ho; Brian Levine; Paul M Aoki,This paper examines two parallel case studies to promote remote medical consultation inGhana. These projects; initiated independently by different researchers in differentorganizations; both deployed ICT solutions in the same medical community in the sameyear. The Ghana Consultation Network currently has over 125 users running a Web-basedapplication over a delay-tolerant network of servers. OneTouch MedicareLine is currentlyproviding 1700 doctors in Ghana with free mobile phone calls and text messages to othermembers of the medical community. We present the consequences of (1) the institutionalcontext and identity of the investigators; as well as specific decisions made with respect to(2) partnerships formed;(3) perceptions of technological infrastructure; and (4) high-leveldesign decisions. In concluding; we discuss lessons learned and high-level implications …,Information and Communication Technologies and Development (ICTD); 2009 International Conference on,2009,26
Learning Spark: lightning-fast big data analytics,Mark Hamstra; Matei Zaharia,*,*,2013,23
FairRide: Near-Optimal; Fair Cache Sharing.,Qifan Pu; Haoyuan Li; Matei Zaharia; Ali Ghodsi; Ion Stoica,Abstract–Memory caches continue to be a critical component to many systems. In recentyears; there has been larger amounts of data into main memory; especially in sharedenvironments such as the cloud. The nature of such environments requires resourceallocations to provide both performance isolation for multiple users/applications and highutilization for the systems. We study the problem of fair allocation of memory cache formultiple users with shared files. We find that; surprisingly; no memory allocation policy canprovide all three desirable properties (isolation-guarantee; strategy-proofness andParetoefficiency) that are typically achievable by other types of resources; eg; CPU ornetwork. We also show that there exist policies that achieve any two of the three properties.We find that the only way to achieve both isolationguarantee and strategy-proofness is …,NSDI,2016,22
Weld: A common runtime for high performance data analytics,Shoumik Palkar; James J Thomas; Anil Shanbhag; Deepak Narayanan; Holger Pirk; Malte Schwarzkopf; Saman Amarasinghe; Matei Zaharia; Stanford InfoLab,Abstract Modern analytics applications combine multiple functions from different librariesand frameworks to build increasingly complex workflows. Even though each function mayachieve high performance in isolation; the performance of the combined workflow is often anorder of magnitude below hardware limits due to extensive data movement across thefunctions. To address this problem; we propose Weld; a runtime for data-intensiveapplications that optimizes across disjoint libraries and functions. Weld uses a commonintermediate representation to capture the structure of diverse dataparallel workloads;including SQL; machine learning and graph analytics. It then performs key data movementoptimizations and generates efficient parallel code for the whole workflow. Weld can beintegrated incrementally into existing frameworks like Tensor-Flow; Apache Spark …,Conference on Innovative Data Systems Research (CIDR),2017,21
M odel DB: a system for machine learning model management,Manasi Vartak; Harihar Subramanyam; Wei-En Lee; Srinidhi Viswanathan; Saadiyah Husnoo; Samuel Madden; Matei Zaharia,Abstract Building a machine learning model is an iterative process. A data scientist will buildmany tens to hundreds of models before arriving at one that meets some acceptance criteria(eg AUC cutoff; accuracy threshold). However; the current style of model building is ad-hocand there is no practical way for a data scientist to manage models that are built over time.As a result; the data scientist must attempt to" remember" previously constructed models andinsights obtained from them. This task is challenging for more than a handful of models andcan hamper the process of sensemaking. Without a means to manage models; there is noeasy way for a data scientist to answer questions such as" Which models were built using anincorrect feature?";" Which model performed best on American customers?" or" How did thetwo top models compare?" In this paper; we describe our ongoing work on ModelDB; a …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,20
Sparrow: Scalable scheduling for sub-second parallel jobs,Kay Ousterhout; Patrick Wendell; Matei Zaharia; Ion Stoica,Abstract Large-scale data analytics frameworks are shifting towards shorter task durationsand larger degrees of parallelism to provide low latency. However; scheduling highlyparallel jobs that complete in hundreds of milliseconds poses a major challenge for clusterschedulers; which will need to place millions of tasks per second on appropriate nodeswhile offering millisecond-level latency and high availability. We demonstrate that adecentralized; randomized sampling approach provides nearoptimal performance whileavoiding the throughput and availability limitations of a centralized design. We implementand deploy our scheduler; Sparrow; on a real cluster and demonstrate that Sparrowperforms within 14% of an ideal scheduler.,EECS Department; University of California; Berkeley; Tech. Rep. UCB/EECS-2013-29,2013,20
Dominant resource fairness: Fair allocation of heterogeneous resources in datacenters,Ali Ghodsi; Matei Zaharia; Benjamin Hindman; Andrew Konwinski; Scott Shenker; Ion Stoica,Abstract This paper investigates how different resources can be fairly allocated among usersthat possibly prioritize them differently. We introduce a fairness policy; called DominantResource Fairness (DRF); which is an adaptation of max-min fairness from networking todatacenter environments. We show that DRF; unlike other policies which we investigated;satisfies a number of desirable properties that a fair datacenter scheduler should have;including guaranteeing that every user gets 1 n of some resource and that users canrelinquish resources without hurting other users' allocations. DRF is also envy-free;incentivizing users to correctly report their resource demand. When compared to otherintuitive schedulers; as well as competing ones from microeconomic theory; DRF is morefair.,Proc. of NSDI,2010,20
Sparkr: Scaling r programs with spark,Shivaram Venkataraman; Zongheng Yang; Davies Liu; Eric Liang; Hossein Falaki; Xiangrui Meng; Reynold Xin; Ali Ghodsi; Michael Franklin; Ion Stoica; Matei Zaharia,Abstract R is a popular statistical programming language with a number of extensions thatsupport data processing and machine learning tasks. However; interactive data analysis inR is usually limited as the R runtime is single threaded and can only process data sets that fitin a single machine's memory. We present SparkR; an R package that provides a frontend toApache Spark and uses Spark's distributed computation engine to enable large scale dataanalysis from the R shell. We describe the main design goals of SparkR; discuss how thehigh-level DataFrame API enables scalable computation and present some of the keydetails of our implementation.,Proceedings of the 2016 International Conference on Management of Data,2016,19
Graysort on apache spark by databricks,Reynold Xin; Parviz Deyhim; Ali Ghodsi; Xiangrui Meng; Matei Zaharia,Apache Spark [1] is a general cluster compute engine for scalable data processing. It wasoriginally developed by researchers at UC Berkeley AMPLab [2]. The engine is faulttolerantand is designed to run on commodity hardware. It generalizes two stage Map/Reduce tosupport arbitrary DAGs of tasks and fast data sharing between operations.,GraySort Competition,2014,19
Graphframes: an integrated api for mixing graph and relational queries,Ankur Dave; Alekh Jindal; Li Erran Li; Reynold Xin; Joseph Gonzalez; Matei Zaharia,Abstract Graph data is prevalent in many domains; but it has usually required specializedengines to analyze. This design is onerous for users and precludes optimization acrosscomplete workflows. We present GraphFrames; an integrated system that lets users combinegraph algorithms; pattern matching and relational queries; and optimizes work across them.GraphFrames generalize the ideas in previous graph-on-RDBMS systems; such as GraphXand Vertexica; by letting the system materialize multiple views of the graph (not just thespecific triplet views in these systems) and executing both iterative algorithms and patternmatching using joins. To make applications easy to write; GraphFrames provide a concise;declarative API based on the" data frame" concept in R that can be used for both interactivequeries and standalone programs. Under this API; GraphFrames use a graph-aware join …,Proceedings of the Fourth International Workshop on Graph Data Management Experiences and Systems,2016,17
Achieving the KS threshold in the general stochastic block model with linearized acyclic belief propagation,Emmanuel Abbe; Colin Sandon,Abstract The stochastic block model (SBM) has long been studied in machine learning andnetwork science as a canonical model for clustering and community detection. In the recentyears; new developments have demonstrated the presence of threshold phenomena for thismodel; which have set new challenges for algorithms. For the {\it detection} problem insymmetric SBMs; Decelle et al.\conjectured that the so-called Kesten-Stigum (KS) thresholdcan be achieved efficiently. This was proved for two communities; but remained open fromthree communities. We prove this conjecture here; obtaining a more general result thatapplies to arbitrary SBMs with linear size communities. The developed algorithm is alinearized acyclic belief propagation (ABP) algorithm; which mitigates the effects of cycleswhile provably achieving the KS threshold in $ O (n\ln n) $ time. This extends prior …,Advances in Neural Information Processing Systems,2016,17
A policy-oriented architecture for opportunistic communication on multiple wireless networks,Aaditeshwar Seth; Matei Zaharia; Srinivasan Keshav; S Bhattacharyya,Abstract—Today's mobile devices are already equipped with multiple wireless interfacesthat differ in data rates; power consumption; monetary cost; and coverage areas. Previousresearch has shown that intelligent policy-based switching between wireless interfaces canobtain better performance than using a single interface [29]. We build upon this pioneeringwork to add the notion of user-defined policies that operate at the session layer; unlike thenetwork-and transport-layer policies of past work. Session-layer policy definitions allow us toinclude a delay component (such as a deadline for data transfer) at a timescale of minutesand hours. This helps us distinguish between online and offline applications; and allows usa high degree of flexibility in network selection. For example; we can defer data transfer oncurrently available networks if they are too costly; and wait until a low cost network (such …,IEEE Transactions on Mobile Computing,2006,14
Nexus: A common substrate for cluster computing,Benjamin Hindman; Andrew Konwinski; Matei Zaharia; Ali Ghodsi; Anthony D Joseph; Scott Shenker; Ion Stoica,Abstract The success of MapReduce has sparked the development of a diverse array ofcluster computing frameworks. We believe that no single framework will be optimal for allapplications; and that organizations will instead want to run multiple frameworks in the samecluster. Furthermore; to ease development of new frameworks; it is critical to identifycommon abstractions and modularize their architectures. To achieve these goals; wepropose Nexus; a low-level substrate that provides isolation and efficient resource sharingacross frameworks running on the same cluster; while giving each framework maximumcontrol over the scheduling and execution of its jobs. Nexus fosters innovation in the cloudby letting organizations run new frameworks alongside existing ones and by lettingframework developers focus on specific applications rather than building one-size-fits-all …,Workshop on Hot Topics in Cloud Computing,2009,13
Splinter: Practical Private Queries on Public Data.,Frank Wang; Catherine Yun; Shafi Goldwasser; Vinod Vaikuntanathan; Matei Zaharia,Abstract Many online services let users query public datasets such as maps; flight prices; orrestaurant reviews. Unfortunately; the queries to these services reveal highly sensitiveinformation that can compromise users' privacy. This paper presents Splinter; a system thatprotects users' queries on public data and scales to realistic applications. A user splits herquery into multiple parts and sends each part to a different provider that holds a copy of thedata. As long as any one of the providers is honest and does not collude with the others; theproviders cannot determine the query. Splinter uses and extends a new cryptographicprimitive called Function Secret Sharing (FSS) that makes it up to an order of magnitudemore efficient than prior systems based on Private Information Retrieval and garbled circuits.We develop protocols extending FSS to new types of queries; such as MAX and TOPK …,NSDI,2017,12
SPARK: Cluster Computing Withworking Sets.,Matei Zaharia,*,*,2010,12
Voodoo-a vector algebra for portable database performance on modern hardware,Holger Pirk; Oscar Moll; Matei Zaharia; Sam Madden,Abstract In-memory databases require careful tuning and many engineering tricks to achievegood performance. Such database performance engineering is hard: a plethora of data andhardware-dependent optimization techniques form a design space that is difficult to navigatefor a skilled engineer---even more so for a query compiler. To facilitate performance-orienteddesign exploration and query plan compilation; we present Voodoo; a declarativeintermediate algebra that abstracts the detailed architectural properties of the hardware;such as multi-or many-core architectures; caches and SIMD registers; without losing theability to generate highly tuned code. Because it consists of a collection of declarative; vector-oriented operations; Voodoo is easier to reason about and tune than low-level C and relatedhardware-focused extensions (Intrinsics; OpenCL; CUDA; etc.). This enables our Voodoo …,Proceedings of the VLDB Endowment,2016,11
Supporting fast iteration in model building,Manasi Vartak; Pablo Ortiz; Kathryn Siegel; Harihar Subramanyam; Samuel Madden; Matei Zaharia,Abstract Building real-world machine learning models involves an iterative process ofengineering features; fitting models; and tuning parameters until a model that meets specificacceptance criteria is identified. Over the course of a modeling session; tens to hundreds ofmodels may be built and evaluated. Although the modeling process is iterative; current toolsare optimized for individual models as opposed to modeling sessions. In this paper; weargue the need for a system to support the entire modeling process by making it cheap torun and track experiments. We describe the desiderata for such a system spanning systemsoptimizations to visual interfaces; and describe the ongoing implementation of SHERLOCK;a system optimized for iterative model building.,NIPS Workshop LearningSys,2015,10
Batch sampling: Low overhead scheduling for sub-second prallel jobs,Kay Ousterhout; Patrick Wendell; Matei Zaharia; Ion Stoica,Abstract Large-scale data analytics frameworks are shifting towards shorter task durationsand larger degrees of parallelism to provide low latency. However; scheduling highlyparallel jobs that complete in hundreds of milliseconds poses a major scaling challenge forcluster schedulers; which will need to place millions of tasks per second on appropriatenodes while offering millisecondlevel latency and high availability. This paper presents adecentralized load balancing approach called batch sampling; based on a generalization ofthe power of two random choices; that performs within a few percent of an optimalcentralized scheduler. We evaluate our approach through both analytical results and animplementation called Sparrow. Sparrow schedules tasks with less than 8ms of overheadand features a design that is inherently fault-tolerant and scalable.,University of California; Berkeley,2012,10
Optimally Designing Games for Cognitive Science Research,Anna N Rafferty; Matei Zaharia; Thomas L Griffiths,Abstract Collecting cognitive science data using games has the potential to be a powerfultool for recruiting participants and increasing their motivation. However; designing gamesthat provide useful data is a difficult task that often requires significant trial and error. In thiswork; we consider how to apply ideas from optimal experiment design to designing gamesfor cognitive science experiments. We use Markov decision processes to model players'actions within a game; and then make inferences about the parameters of a cognitive modelfrom these actions. We present a general framework for finding games with high expectedinformation gain based on this approach. We apply this framework to Boolean conceptlearning; inferring the difficulty of Boolean concepts from participants' behavior. We showthat using games with higher expected information gain allows us to make this inference …,Annual Conference of the Cognitive Science Society,2012,9
Performance and scalability of broadcast in Spark,Mosharaf Chowdhury; Matei Zaharia; Ion Stoica,ABSTRACT Although the MapReduce programming model has so far been highlysuccessful; not all applications are well suited to this model. Spark bridges this gap byproviding seamless support for iterative and interactive jobs that are hard to express usingthe acyclic data flow model pioneered by MapReduce. While benchmarking Spark; weidentified that the default broadcast mechanism implemented in the Spark prototype is ahindrance toward its scalability. In this report; we implement; evaluate; and compare fourdifferent broadcast mechanisms (including the default one) for Spark. We outline the basicrequirements of a broadcast mechanism for Spark and analyze each of the comparedbroadcast mechanisms under that guideline. Our experiments in high-speed; low-latency;and cooperative data center environments also shed light on characteristics of multicast …,*,2014,8
Arthur: Rich post-facto debugging for production analytics applications,Ankur Dave; Matei Zaharia; I Stoica,Abstract Debugging the massive parallel computations that run in today's datacenters ishard; as they consist of thousands of tasks processing terabytes of data. It is especially hardin production settings; where performance overheads of more than a few percent areunacceptable. To address this challenge; we present Arthur; a new debugger that provides arich set of analysis tools at close to zero runtime overhead through selective replay of dataflow applications. Unlike previous replay debuggers; which add high overheads due to theneed to log low-level nondeterministic events; Arthur takes advantage of the structure oflarge-scale data flow models (eg; MapReduce); which split work into deterministic tasks forfault tolerance; to minimize its logging cost. We use selective replay to implement a variety ofdebugging features; including rerunning any task in a single-process debugger; ad-hoc …,University of California; Berkeley,2013,8
A New Communications API,G Ananthanarayaran,ABSTRACT We present NetAPI; a flexible communications interface. Although theubiquitous Sockets API lets applications select among a number of mechanisms toaccomplish networking tasks; it binds them tightly to their chosen mechanisms.Consequently; the network stack has little freedom in selecting the best protocols andmechanisms for each application; and innovating below the API is extremely difficult. NetAPIallows applications to specify their communication intents against an abstract interface thathides implementation mechanisms; encouraging innovation below the API. Applicationintents are combined with user policies and environmental conditions to let the network meetapplication goals in varied ways. We describe the design of NetAPI; comparing it to othersystem APIs that have supported evolution. We have also implemented a prototype of …,Electrical Engineering and Computer SciencesUniversity of California at Berkeley,2009,8
Above the clouds: A Berkeley view of cloud computing (February 2009),Michael Armbrust; Armando Fox; Rean Griffith; Anthony D Joseph; Randy H Katz; Andrew Konwinski; Gunho Lee; David A Patterson; Ariel Rabkin; Ion Stoica; Matei Zaharia,*,URL https://www. eecs. berkeley. edu/Pubs/TechRpts/2009/EECS-2009-28. pdf,2009,8
Optimizing deep cnn-based queries over video streams at scale,Daniel Kang; John Emmons; Firas Abuzaid; Peter Bailis; Matei Zaharia,Abstract: Video is one of the fastest-growing sources of data and is rich with interestingsemantic information. Furthermore; recent advances in computer vision; in the form of deepconvolutional neural networks (CNNs); have made it possible to query this semanticinformation with near-human accuracy (in the form of image tagging). However; performinginference with state-of-the-art CNNs is computationally expensive: analyzing videos in realtime (at 30 frames/sec) requires a $1200 GPU per video stream; posing a seriouscomputational barrier to CNN adoption in large-scale video data management systems. Inresponse; we present NOSCOPE; a system that uses cost-based optimization to assemble aspecialized video processing pipeline for each input video stream; greatly acceleratingsubsequent CNNbased queries on the video. As NOSCOPE observes a video; it trains …,arXiv preprint arXiv:1703.02529,2017,7
Enabling innovation below the communication api,Ganesh Ananthanarayanan; Kurtis Heimerl; Matei Zaharia; Michael Demmer; Teemu Koponen; Arsalan Tavakoli; Scott Shenker; Ion Stoica,ABSTRACT Innovation in the network is notoriously difficult due to the need to supportlegacy applications. We argue that this difficulty stems from the API used to access thenetwork. The ubiquitous Sockets API lets applications choose from a number ofcommunication mechanisms; but binds them tightly to their chosen mechanism (egspecifying a destination using IPv4). Applications must therefore be modified in order tobenefit from new network technologies. To address this problem; we propose a newcommunication API called NetAPI that lets applications specify their communication intentswithout binding to particular network mechanisms; enabling evolution below the API. Wehave built a NetAPI prototype for the iPhone; and use it to show that we can adddisconnection tolerance; content shaping and power saving policies under NetAPI …,EECS Department; University of California; Berkeley; UCB/EECS-2009–141,2009,7
linalg: Matrix computations in apache spark,Reza Bosagh Zadeh; Xiangrui Meng; Burak Yavuz; Aaron Staple; Li Pu; Shivaram Venkataraman; Evan Sparks; Alexander Ulanov; Matei Zaharia,Abstract We describe matrix computations available in the cluster programming framework;Apache Spark. Out of the box; Spark comes with the mllib. linalg library; which providesabstractions and implementations for distributed matrices. Using these abstractions; wehighlight the computations that were more challenging to distribute. When translating single-node algorithms to run on a distributed cluster; we observe that often a simple idea isenough: separating matrix operations from vector operations and shipping the matrixoperations to be ran on the cluster; while keeping vector operations local to the driver. In thecase of the Singular Value Decomposition; by taking this idea to an extreme; we are able toexploit the computational power of a cluster; while running code written decades ago for asingle core. We conclude with a comprehensive set of benchmarks for hardware …,arXiv preprint,2015,6
NoScope: optimizing neural network queries over video at scale,Daniel Kang; John Emmons; Firas Abuzaid; Peter Bailis; Matei Zaharia,Abstract Recent advances in computer vision---in the form of deep neural networks---havemade it possible to query increasing volumes of video data with high accuracy. However;neural network inference is computationally expensive at scale: applying a state-of-the-artobject detector in real time (ie; 30+ frames per second) to a single video requires a $4000GPU. In response; we present N o S cope; a system for querying videos that can reduce thecost of neural network video analysis by up to three orders of magnitude via inference-optimized model search. Given a target video; object to detect; and reference neuralnetwork; N o S cope automatically searches for and trains a sequence; or cascade; ofmodels that preserves the accuracy of the reference network but is specialized to the targetvideo and are therefore far less computationally expensive. N o S cope cascades two …,Proceedings of the VLDB Endowment,2017,5
Introduction to spark 2.0 for database researchers,Michael Armbrust; Doug Bateman; Reynold Xin; Matei Zaharia,Abstract Originally started as an academic research project at UC Berkeley; Apache Spark isone of the most popular open source projects for big data analytics. Over 1000 volunteershave contributed code to the project; it is supported by virtually every commercial vendor;many universities are now offering courses on Spark. Spark has evolved significantly sincethe 2010 research paper: its foundational APIs are becoming more relational and structuralwith the introduction of the Catalyst relational optimizer; and its execution engine isdeveloping quickly to adopt the latest research advances in database systems such aswhole-stage code generation. This tutorial is designed for database researchers (graduatestudents; faculty members; and industrial researchers) interested in a brief hands-onoverview of Spark. This tutorial covers the core APIs for using Spark 2.0; including …,Proceedings of the 2016 International Conference on Management of Data,2016,5
Hypervisors as a foothold for personal computer security: An agenda for the research community,Matei Zaharia; Sachin Katti; Chris Grier; Vern Paxson; Scott Shenker; Ion Stoica; Dawn Song,Abstract: The purpose of this paper is to propose the creation of a security-enhancinghypervisor for PCs as a collaborative agenda for the research community. This agenda is notnecessarily about answering fundamentally new research questions. Rather; it is a call toaction about a rare chance for the community to have substantial impact. If researchersdemonstrate compelling near-term benefits from a modest security layer; then OS vendorsmay adopt such a layer as a way to increase security without costly reengineering. Theintroduction of this secure foothold into the consumer software stack could then yieldsignificant long-term benefits by providing a much better avenue for deploying securitysolutions. This agenda consists of two parts:(1) exploring how hypervisors can address end-user security issues and (2) exploring how to architect a small; secure hypervisor that …,*,2012,5
Adaptive peer-to-peer search,M Zaharia; S Keshav,Abstract: Peers in a peer-to-peer (P2P) system have widely differing performancecharacteristics and are subject to time-varying workloads. However; once a peer has beendeployed; reconfiguring it is difficult; if not impossible. It is; therefore; important for a peer toadapt its behavior to variations in network and workload characteristics. In this paper; wepresent adaptive mechanisms for two specific problems in peerto-peer search systems:efficient flooding in unstructured P2P systems and search algorithm selection in hybrid P2Psystems. Both algorithms use a mathematical model to predict an expected result; comparethe actual result to the prediction; and feed back the error to adapt future responses. Thisallows the system to automatically adapt to changes in the operating environment. Syntheticand trace-driven simulations show substantial gain due to these adaptive techniques …,University of Waterloo Technical Report,2004,5
Large-scale online expectation maximization with spark streaming,Timothy Hunter; Tathagata Das; Matei Zaharia; Alexandre Bayen; Pieter Abbeel,Abstract Many “Big Data” applications in Machine Learning (ML) need to react quickly tolarge streams of incoming data. The standard paradigm nowadays is to run ML algorithmson frameworks designed for batch operations; such as MapReduce or Hadoop. By design;these frameworks are not a good match for low-latency applications. This is why we exploreusing a new; recently proposed model for large-scale stream processing; discretizedstreams (D-Streams [19]); for online Machine Learning. Our application is an onlineExpectation-maximization algorithm; that estimates the state of car traffic in the SanFrancisco Bay Area. Using D-Streams; we are able to achieve near-perfect scaling of ourapplication on a commodity cluster in a reliable; fault-tolerant way. Our algorithm can updatethe state of traffic from hundreds of thousand of GPS observations within a few seconds. 1,*,2012,4
Mesos: Flexible resource sharing for the cloud,Benjamin Hindman; Andy Konwinski; Matei Zaharia; Ali Ghodsi; Anthony D Joseph; R Katz; Scott Shenker; Ion Stoica,Skip to main content …,USENIX (August 2011),2011,4
Weld: Rethinking the Interface Between Data-Intensive Applications,Shoumik Palkar; James Thomas; Deepak Narayanan; Anil Shanbhag; Rahul Palamuttam; Holger Pirk; Malte Schwarzkopf; Saman Amarasinghe; Samuel Madden; Matei Zaharia,Abstract: Data analytics applications combine multiple functions from different libraries andframeworks. Even when each function is optimized in isolation; the performance of thecombined application can be an order of magnitude below hardware limits due to extensivedata movement across these functions. To address this problem; we propose Weld; a newinterface between data-intensive libraries that can optimize across disjoint libraries andfunctions. Weld exposes a lazily-evaluated API where diverse functions can submit theircomputations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficientcode for diverse hardware. Weld can be integrated into existing frameworks such as Spark;TensorFlow; Pandas and NumPy without changing their user-facing APIs. We …,arXiv preprint arXiv:1709.06416,2017,3
Optimizing cache performance for graph analytics,Yunming Zhang; Vladimir Kiriansky; Charith Mendis; Matei Zaharia; Saman Amarasinghe,Abstract: Despite all the optimization work on in-memory graph analytics; today's graphframeworks still do not utilize modern hardware well. In this paper; we show that it ispossible to achieve up to 4$\times $ speedups over the fastest in-memory frameworks bygreatly improving graph applications' use of the cache. Previous systems have applied out-of-core processing techniques from the memory/disk boundary to the cache/DRAMboundary. However; we find that blindly applying such techniques is ineffective because ofthe much smaller performance gap between DRAM and cache. We present two techniquesthat take advantage of the cache with minimal or no instruction overhead. The first; vertexreordering; sorts the vertices by degree to improve the utilization of each cache line with noruntime overhead. The second; CSR segmenting; partitions the graph to restrict all …,arXiv preprint arXiv:1608.01362,2016,3
SiRen: Leveraging Similar Regions for Efficient and Accurate Variant Calling,Kristal Curtis; Ameet Talwalkar; Matei Zaharia; Armando Fox; David A Patterson,Abstract: Next-generation genomic sequencing costs are rapidly decreasing; having recentlyreached the $1000-per-genome barrier; a likely tipping point for widespread clinical use.However; genomic analysis techniques have failed to keep pace. In particular; the processof variant calling; or inferring a sample genome from the noisy sequencing data; introducesmajor computational and statistical challenges. In this work; we explore the feasibility of ahybrid approach that addresses these challenges by partitioning the genome into easier andharder regions; deploying efficient algorithms on the easier regions; and relying on moreexpensive and accurate technologies in the harder regions. We propose that nearduplication; or similarity; in the genome is a natural signal for identifying harder regions; andthen present a large-scale distributed clustering approach to identify these similar regions …,*,2015,3
Optimally designing games for behavioural research,Anna N Rafferty; Matei Zaharia; Thomas L Griffiths,Computer games can be motivating and engaging experiences that facilitate learning;leading to their increasing use in education and behavioural experiments. For theseapplications; it is often important to make inferences about the knowledge and cognitiveprocesses of players based on their behaviour. However; designing games that provideuseful behavioural data are a difficult task that typically requires significant trial and error.We address this issue by creating a new formal framework that extends optimal experimentdesign; used in statistics; to apply to game design. In this framework; we use Markovdecision processes to model players9 actions within a game; and then make inferencesabout the parameters of a cognitive model from these actions. Using a variety of conceptlearning games; we show that in practice; this method can predict which games will result …,Proc. R. Soc. A,2014,3
Resilient distributed datasets,Matei Zaharia; M Chowdhury; T Das; A Dave; J Ma; M McCauley; M Franklin; S Shenker; I Stoica,Page 1. Resilient Distributed Datasets A Fault-‐Tolerant Abstraction for In-‐Memory ClusterComputing Matei Zaharia; Mosharaf Chowdhury; Tathagata Das; Ankur Dave; Justin Ma; MurphyMcCauley; Michael Franklin; Scott Shenker; Ion Stoica UC Berkeley UC BERKELEY Page 2.Motivation MapReduce greatly simplified “big data” analysis on large; unreliable clusters Butas soon as it got popular; users wanted more: » More complex; multi-‐stage applications (egiterative machine learning & graph processing) » More interactive ad-‐hoc queries Response:specialized frameworks for some of these apps (eg Pregel for graph processing) Page 3.Motivation Complex apps and interactive queries both need one thing that MapReduce lacks:Efficient primitives for data sharing In MapReduce; the only way to share data across jobs is stablestorage → slow! Page 4. Examples iter. 1 iter. 2 . . . Input HDFS read HDFS write …,A fault-tolerant abstraction for in-memory cluster computing in Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,2014,3
Introduction to MapReduce and Hadoop,Matei Zaharia,Page 1. UC Berkeley Introduction to MapReduce and Hadoop Matei Zaharia UC Berkeley RADLab matei@eecs.berkeley.edu Page 2. What is MapReduce? • Data-parallel programming modelfor clusters of commodity machines • Pioneered by Google – Processes 20 PB of data per day •Popularized by open-source Hadoop project – Used by Yahoo!; Facebook; Amazon; … Page3. What is MapReduce used for? • At Google: – Index building for Google Search – Articleclustering for Google News – Statistical machine translation • At Yahoo!: – Index building forYahoo! Search – Spam detection for Yahoo! Mail • At Facebook: – Data mining – Ad optimization –Spam detection Page 4. Example: Facebook Lexicon www.facebook.com/lexicon Page 5.Example: Facebook Lexicon www.facebook.com/lexicon Page 6. What is MapReduce used for? •In research: – Analyzing Wikipedia conflicts (PARC) …,UC Berkeley RAD Lab,*,3
Stadium: A Distributed Metadata-Private Messaging System,Nirvan Tyagi; Yossi Gilad; Derek Leung; Matei Zaharia; Nickolai Zeldovich,Abstract Private communication over the Internet remains a challenging problem. Even ifmessages are encrypted; it is hard to deliver them without revealing metadata about whichpairs of users are communicating. Scalable anonymity systems; such as Tor; are susceptibleto traffic analysis attacks that leak metadata. In contrast; the largest-scale systems withmetadata privacy require passing all messages through a small number of providers;requiring a high operational cost for each provider and limiting their deployability in practice.This paper presents Stadium; a point-to-point messaging system that provides metadata anddata privacy while scaling its work efficiently across hundreds of low-cost providers operatedby different organizations. Much like Vuvuzela; the current largest-scale metadata-privatesystem; Stadium achieves its provable guarantees through differential privacy and the …,Proceedings of the 26th Symposium on Operating Systems Principles,2017,2
Flexible Resource Sharing for the Cloud,Benjamin Hindman; ANDY KONWINSKI; MATEI ZAHARIA; ALI GHODSI; ANTHONY D JOSEPH; RANDY H KATZ; SCOTT SHENKER; ION STOICA,Benjamin Hindman is a fourth-year PhD student at the University of California; Berkeley.Before working on resource management for cluster computing; he worked on resourcemanagement for singlenode parallel computing. His interests include operating systems;distributed systems; programming languages; and all the ways they intersect. benh@ eecs.berkeley. edu,*,2011,2
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Domenico Talia; Paolo Trunfio,Supercomputing is evolving toward hybrid and accelerator-based architectures with millionsof cores. The Hardware/Hybrid Accelerated Cosmology Code (HACC) framework exploitsthis diverse landscape at the largest scales of problem size; obtaining high scalability andsustained performance. Developed to satisfy the science requirements of cosmologicalsurveys; HACC melds particle and grid methods using a novel algorithmic structure thatflexibly maps across architectures; including CPU/GPU; multi/many-core; and Blue Genesystems. In this Research Highlight; we demonstrate the success of HACC on two verydifferent machines; the CPU/GPU system Titan and the BG/Q systems Sequoia and Mira;attaining very high levels of scalable performance. We demonstrate strong and weak scalingon Titan; obtaining up to 99.2% parallel efficiency; evolving 1.1 trillion particles. On …,Communications of the ACM,*,2
Nested vector language: Roofline performance for data parallel code,Shoumik Palkar; James Thomas; Matei Zaharia,Page 1. Overview and Examples ParallelizaTon Preliminary Results MoTvaTon ShoumikPalkar; James Thomas; Matei Zaharia Nested Vector Language: Roofline Performance forData Parallel Code VectorizaTon Future Work • Goal: leverage SIMD instruc<ons to exploitdata-‐ parallelism on a single CPU %res = add i32 %op1 %op2 %res = add <8 x i32> %op1%op2 Scalar Vectorized Difficult with branches. Transform to use predicaTon: if (5 < x) x +=100; x += 100 * (5 < x); Branched Predicated → → compare masked add result *Iden<tyelement inferred from builder type. Implemented using select instr. parallel • Wri<ng fastparallel code is hard. • Numerous complex evolving pla orms (GPUs; CPUs) and techniques(mulficore; SIMD). • Many common algorithms can be wri`en through “embarrassingly parallel”data opera<ons. • MapReduce is empirical example …,*,*,2
DAWNBench: An End-to-End Deep Learning Benchmark and Competition,Cody Coleman; Deepak Narayanan; Daniel Kang; Tian Zhao; Jian Zhang; Luigi Nardi; Peter Bailis; Kunle Olukotun; Chris Ré; Matei Zaharia,Abstract Despite considerable research on systems; algorithms and hardware to speed updeep learning workloads; there is no standard means of evaluating end-to-end deeplearning performance. Existing benchmarks measure proxy metrics; such as time to processone minibatch of data; that do not indicate whether the system as a whole will produce ahigh-quality result. In this work; we introduce DAWNBench; a benchmark and competitionfocused on end-to-end training time to achieve a state-of-the-art accuracy level; as well asinference time with that accuracy. Using time to accuracy as a target metric; we explore howdifferent optimizations; including choice of optimizer; stochastic depth; and multi-GPUtraining; affect end-to-end training performance. Our results demonstrate that optimizationscan interact in non-trivial ways when used in conjunction; producing lower speed-ups and …,Training,2017,1
Beating State-of-the-art By-10000%.,Reynold Xin; UC AMPLab; Joseph Gonzalez; Josh Rosen; Matei Zaharia; Michael Franklin; Scott Shenker; Ion Stoica,Page 1. Beating State-of-the-art By -10000% Reynold Xin; AMPLab; UC Berkeley with help fromJoseph Gonzalez; Josh Rosen; Matei Zaharia; Michael Franklin; Scott Shenker; Ion Stoica Page2. Beating State-of-the-art By -10000% NOT A TYPO Reynold Xin; AMPLab; UC Berkeley withhelp from Joseph Gonzalez; Josh Rosen; Matei Zaharia; Michael Franklin; Scott Shenker; Ion StoicaPage 3. MapReduce deterministic; idempotent tasks fault-tolerance elasticity resource sharingPage 4. “The bar for open source software is at historical low.” Page 5. “The bar for open sourcesoftware is at historical low.” ie “This is the right time to do grad school.” Page 6. iterative machinelearning OLAP strong temporal locality Page 7. Does in-memory computation help inpetabyte-scale warehouses? Page 8. Does in-memory computation help in petabyte-scalewarehouses? YES Page 9. Spark How to do in-memory computation …,CIDR,2013,1
Large Scale Estimation in Cyberphysical Systems using Streaming Data: a Case Study with Smartphone Traces,Timothy Hunter; Tathagata Das; Matei Zaharia; Pieter Abbeel; Alexandre M Bayen,Abstract: Controlling and analyzing cyberphysical and robotics systems is increasinglybecoming a Big Data challenge. Pushing this data to; and processing in the cloud is moreefficient than on-board processing. However; current cloud-based solutions are not suitablefor the latency requirements of these applications. We present a new concept; DiscretizedStreams or D-Streams; that enables massively scalable computations on streaming datawith latencies as short as a second. We experiment with an implementation of D-Streams ontop of the Spark computing framework. We demonstrate the usefulness of this concept with anovel algorithm to estimate vehicular traffic in urban networks. Our online EM algorithm canestimate traffic on a very large city network (the San Francisco Bay Area) by processing tensof thousands of observations per second; with a latency of a few seconds. Subjects …,arXiv preprint arXiv:1212.3393,2012,1
Distributed Pipeline for Genomic Variant Calling,Richard Xia; Sara Sheehan; Yuchen Zhang; Ameet Talwalkar; Matei Zaharia; Jonathan Terhorst; Michael Jordan; Yun S Song; Armando Fox; David Patterson,Abstract Due to recent advances in nucleotide sequencing technology; the cost of genomicsequencing is decreasing at a pace that vastly exceeds Moore's law. The computationalmethods needed to process short read data are struggling to keep pace; indeed; currentsequencing pipelines take days to execute for even a single human genome. In this work;we describe the Big Genomics Inference Engine (BIGGIE); a faster variant calling pipelinedesigned for modern distributed clusters that supports the efficient processing of thousandsto millions of genomes. Moreover; the modular design of our system should foster the rapiddevelopment and painless integration of new algorithms. BIGGIE hinges on the observationthat we do not need to perform the same computation on all regions of the genome; butrather can first classify the genome into high-and low-complexity regions and …,NIPS Workshop on BIG Learning,2012,1
Weld: Rethinking the Interface Between Data-Intensive Libraries,Shoumik Palkar; James Thomas; Deepak Narayanan; Anil Shanbhag; Rahul Palamuttam; Holger Pirk; Malte Schwarzkopf; Saman Amarasinghe; Samuel Madden; Matei Zaharia,Abstract Data analytics applications combine multiple functions from different libraries andframeworks. Even when each function is optimized in isolation; the performance of thecombined application can be an order of magnitude below hardware limits due to extensivedata movement across these functions. To address this problem; we propose Weld; a newinterface between data-intensive libraries that can optimize across disjoint libraries andfunctions. Weld exposes a lazily-evaluated API where diverse functions can submit theircomputations in a simple but general intermediate representation that captures their data-parallel structure. It then optimizes data movement across these functions and emits efficientcode for diverse hardware. Weld can be integrated into existing frameworks such as Spark;TensorFlow; Pandas and NumPy without changing their user-facing APIs. We …,*,*,1
Cluster instance management system,*,Abstract A system for cluster management comprises a status monitor and an instancereplacement manager. The status monitor is for monitoring status of an instance of a set ofinstances on a cluster provider. The instance replacement manager is for determining areplacement strategy for the instance in the event the instance does not respond. Thereplacement strategy for the instance is based at least in part on a management criteria foron-demand instances and spot instances on the cluster provider.,*,2018,*
System for exploring data in a database,*,Abstract A system for exploring data in a database comprises a query parser; a parametermanager; a query submitter; and a result formatter. The query parser is to receive a basequery and determine an input parameter from the base query. The parameter manager is toprovide a first request for a value for the input parameter; receive the value for the inputparameter; and provide a second request for the value for the input parameter. The querysubmitter is to determine a first query using the base query and the value for the inputparameter; and provide an indication to execute the first query. The result formatter is toreceive a result associated with the indication to execute the first query.,*,2018,*
Making Caches Work for Graph Analytics,Yunming Zhang; Vladimir Kiriansky; Charith Mendis; Saman Amarasinghe; Matei Zaharia; Stanford InfoLab,Abstract—Large-scale applications implemented in today's high performance graphframeworks heavily underutilize modern hardware systems. While many graph frameworkshave made substantial progress in optimizing these applications; we show that it is stillpossible to achieve up to 5× speedups over the fastest frameworks by greatly improvingcache utilization. Previous systems have applied out-of-core processing techniques from thememory/disk boundary to the cache/DRAM boundary. However; we find that blindly applyingsuch techniques is ineffective because the much smaller performance gap between cacheand DRAM requires new designs for achieving scalable performance and low overhead. Wepresent Cagra; a cache optimized inmemory graph framework. Cagra uses a noveltechnique; CSR Segmenting; to break the vertices into segments that fit in last level cache …,IEEE BigData 2017,2017,*
DIY Hosting for Online Privacy,Shoumik Palkar; Matei Zaharia,Abstract Web users today rely on centralized services for applications such as email; filetransfer and chat. Unfortunately; these services create a significant privacy risk: even with abenevolent provider; a single breach can put millions of users' data at risk. One alternativewould be for users to host their own servers; but this would be highly expensive for mostapplications: a single VM deployed in a high-availability mode can cost many dollars permonth. In this paper; we propose Deploy It Yourself (DIY); a new model for hostingapplications based on serverless computing platforms such as Amazon Lambda. DIY allowsusers to run a highly available service with much stronger privacy guarantees than currentcentralized providers; and at a dramatically lower cost than traditional server hosting. DIYonly relies on the security of container isolation and a key manager as opposed to the …,Proceedings of the 16th ACM Workshop on Hot Topics in Networks,2017,*
An Oblivious General-Purpose SQL Database for the Cloud,Saba Eskandarian; Matei Zaharia,Abstract: We present ObliDB; a secure SQL database for the public cloud that supports bothtransactional and analytics workloads and protects against access pattern leakage. Withdatabases being a critical component in many applications; there is significant interest inoutsourcing them securely. Hardware enclaves offer a strong practical foundation towardsthis goal by providing encryption and secure execution; but they still suffer from accesspattern leaks that can reveal a great deal of information. The naive way to address this issue--using generic Oblivious RAM (ORAM) primitives beneath a database--adds prohibitiveoverhead. Instead; ObliDB co-designs both its data structures (eg; oblivious B+ trees) andquery operators to accelerate SQL processing; giving up to 329x speedup over naiveORAM. On analytics workloads; ObliDB ranges from competitive to 19x faster than …,arXiv preprint arXiv:1710.00458,2017,*
Cluster instance management system,*,A system for cluster management comprises a status monitor and an instance replacementmanager. The status monitor is for monitoring status of an instance of a set of instances on acluster provider. The instance replacement manager is for determining a replacementstrategy for the instance in the event the instance does not respond. The replacementstrategy for the instance is based at least in part on a management criteria for on-demandinstances and spot instances on the cluster provider.,*,2017,*
System for exploring data in a database,*,A system for exploring data in a database comprises a query parser; a parameter manager;a query submitter; and a result formatter. The query parser is to receive a base query anddetermine an input parameter from the base query. The parameter manager is to provide afirst request for a value for the input parameter; receive the value for the input parameter;and provide a second request for the value for the input parameter. The query submitter is todetermine a first query using the base query and the value for the input parameter; andprovide an indication to execute the first query. The result formatter is to receive a resultassociated with the indication to execute the first query.,*,2017,*
SimDex: Exploiting Model Similarity in Exact Matrix Factorization Recommendations,Firas Abuzaid; Geet Sethi; Peter Bailis; Matei Zaharia,Abstract: We present SimDex; a new technique for serving exact top-K recommendations onmatrix factorization models that measures and optimizes for the similarity between users inthe model. Previous serving techniques presume a high degree of similarity (eg; L2 orcosine distance) among users and/or items in MF models; however; as we demonstrate; themost accurate models are not guaranteed to exhibit high similarity. As a result; brute-forcematrix multiply outperforms recent proposals for top-K serving on several collaborativefiltering tasks. Based on this observation; we develop SimDex; a new technique for servingmatrix factorization models that automatically optimizes serving based on the degree ofsimilarity between users; and outperforms existing methods in both the high-similarity andlow-similarity regimes. SimDexfirst measures the degree of similarity among users via …,arXiv preprint arXiv:1706.01449,2017,*
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,Peter Bailis; Kunle Olukoton; Christopher Ré; Matei Zaharia,Abstract: Despite incredible recent advances in machine learning; building machinelearning applications remains prohibitively time-consuming and expensive for all but thebest-trained; best-funded engineering organizations. This expense comes not from a needfor new and improved statistical models but instead from a lack of systems and tools forsupporting end-to-end machine learning application development; from data preparationand labeling to productionization and monitoring. In this document; we outline opportunitiesfor infrastructure supporting usable; end-to-end machine learning applications in the contextof the nascent DAWN (Data Analytics for What's Next) project at Stanford. Subjects: Learning(cs. LG); Databases (cs. DB); Machine Learning (stat. ML) Cite as: arXiv: 1705.07538 [cs.LG](or arXiv: 1705.07538 v1 [cs. LG] for this version) Submission history From: Peter …,arXiv preprint arXiv:1705.07538,2017,*
What {\textquoteright} s Changing in Big Data?,Matei Zaharia,Abstract: Big data analytics became a hot research topic nearly ten years ago; but since thattime; a lot of things have changed. On the hardware side; trends such as the slowdown ofprocessing with respect to I/O are starting to affect the design of big data systems. On theapplication side; big data systems are increasingly being used by non-programmers andrequire similar forms of interaction to" small data" analysis tools. Finally; big data systemsare increasingly provided" as a service" on cloud infrastructure. I'll talk about these changesfrom the perspective of the Apache Spark project and from my experience at a companyoffering a cloud service for big data (Databricks).,*,2016,*
Alignment in a SNAP: Cancer Diagnosis in the Genomic Age,M Zaharia; B Bolosky; K Curtis; D Patterson; A Fox; D Patterson; S Shenker; I Stoica; T Sittler,*,LABORATORY INVESTIGATION,2012,*
Research Statement,Matei Zaharia,*,*,2012,*
Selected Publications,KINAM PARK,[2] Park; H.; Park; K.; and Kim; D.: Preparation and swelling behavior of chitosan-basedsuperporous hydrogels for gastric retention application; J. Biomed. Mater. Res.; 76A: 144–150; 2006.[3] Jeong; JH; Cho; YW; Jung; B.; Park; K. and Kim; JD.: Self-assemblednanoparticles of ribozymes with poly (ethylene glycol)-b-poly (l-lysine) block copolymers;Japanese Journal of Applied Physics; 45: 591-595; 2006.,Journal of Polymer Science: Part B: Polymer Physics,2006,*
Design; Analysis and Simulation of a System for Free-Text Search in Peer-to-Peer Networks,Matei A Zaharia; Srinivasan Keshav,Abstract--We present an efficient peer-to-peer system architecture for keyword-based free-text search in environments with heterogeneous document popularities and user lifetimes;such as file-sharing applications. We analyze the characteristics of this system theoretically;and also present an efficient simulator and simulation results.,*,2004,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Akhilesh Chandra; Thomas Calderon,Information systems (IS) are quickly emerging as critical resources to be leveraged fororganizational productivity in many business; social; and economic enterprises. Theexplosive growth in information technology (IT) can be broadly attributed to the emergingnovel linkages of IS/IT with several base disciplines; extending the reach of IS/IT toapplication domains never previously considered.In this article; we focus on certainimportant and promising IS/IT frontiers identified from the perspectives of academia; industry;and federal research funding agencies. Our objective is to focus the collective awareness ofthe IS community and those in related disciplines on some of the frontier developments inIS/IT with a vision of the road ahead and point to challenges and opportunities [1].,Communications of the ACM,2000,*
BlazeIt: An Optimizing Query Engine for Video at Scale,Daniel Kang; Peter Bailis; Matei Zaharia,ABSTRACT Recent advances in deep learning allow us to query the increasing volumes ofvideo data for semantic information. However; these techniques are difficult to deploy inpractice and are incredibly computationally expensive. To address these limitations; wepropose BlazeIt; a system with a query language and optimizer to accelerate complexqueries over video. We demonstrate that BlazeIt can answer a wide range of queries andgive significant speed improvements; up to 16X.,*,*,*
Accelerating Model Search with Model Batching,Deepak Narayanan; Keshav Santhanam; Matei Zaharia,ABSTRACT GPUs have become the computing platform of choice for deep learningapplications. However; leveraging the ever-increasing computational power of these GPUsis challenging for many workloads; resulting in poor resource utilization. In this work; weexplore techniques to utilize the GPU more e ectively for certain deep learning tasks; inparticular; we propose simultaneously running multiple models (called a model batch) onthe same GPU (kernel-level parallelism); which also allows data preprocessing to be sharedamong the di erent models. Our results demonstrate performance gains of up to 9.2 X fortraining and 13.5 X for inference compared to the traditional 1-model-per-GPU con guration.,*,*,*
Research in Operating Systems Sparrow,K Ousterhout; P Wendell; M Zaharia; I Stoica,Page 1. Sparrow: Distributed; Low Latency Scheduling K. Ousterhout; P. Wendell; M. Zaharia andI. Stoica. In Proc. of SOSP 2013 Research in Operating Systems Sparrow *Slides partially basedon Ousternout's presentation Page 2. (Some) useful concepts picked up from OS Scheduler;preemption; shortest-job first; RPC; gang-scheduling; fair-share scheduling; … (In no particularorder) 2 Page 3. Jobs and scheduling for data analytics Large data analytics clusters Running evershorter and higher-fanout jobs What for? Finance; language translation; highly personalize search; …3 Page 4. Jobs and scheduling for data analytics Jobs composed of short tasks Produced fromframeworks that stripe work across 13 machines (eg; Dremel; Spark; …) Targeting task runningin ~100ms 4 Map Reduce/Spark/Dryad Job Task Task Task Map Reduce/Spark/Dryad Job TaskTask Task Task Page 5. Job latencies decreasing rapidly …,*,*,*
Dominant Resource Fairness: Fair,Ali Ghodsi; Matei Zaharia; Benjamin Hindman; Andy Konwinski; Scott Shenker; Ion Stoica,Page 1. Dominant Resource Fairness: Fair Allocation of Multiple Resource Types Ali Ghodsi;Matei Zaharia; Benjamin Hindman; Andy Konwinski; Scott Shenker; Ion Stoica University ofCalifornia; Berkeley Talk by: Yael Amsterdamer Page 2. Why Fairness? • Classic problem: acommon resource – machines; pastures… • Every user has different needs • How should theresources be split? – Not enough to satisfy everyone Introduction 2 Computational Game Theory;Spring 2012 – Yael Amsterdamer Page 3. First attempt • Split according to the needs • Example: –Alice needs 4GB for MATLAB; Bob needs 2GB for watching "The Dictator" in HD – The systemhas 4GB – Alice gets 2.6GB and Bob 1.4GB. • Downsides: – Why should Alice get more? –Everybody has an incentive to lie Introduction 3 Computational Game Theory; Spring 2012 –Yael Amsterdamer Page 4. Second attempt • Split evenly • In our example …,*,*,*
Optimally designing games for behavioural,Anna N Rafferty; Matei Zaharia; Thomas L Griffiths,Computer games can be motivating and engaging experiences that facilitate learning;leading to their increasing use in education and behavioural experiments. For theseapplications; it is often important to make inferences about the knowledge and cognitiveprocesses of players based on their behaviour. However; designing games that provideuseful behavioural data are a difficult task that typically requires significant trial and error.We address this issue by creating a new formal framework that extends optimal experimentdesign; used in statistics; to apply to game design. In this framework; we use Markovdecision processes to model players' actions within a game; and then make inferencesabout the parameters of a cognitive model from these actions. Using a variety of conceptlearning games; we show that in practice; this method can predict which games will result …,*,*,*
大型集群上的快速和通用数据处理架构,Matei Zaharia; CSDN CODE,在本章中; 我们提出了弹性分布式数据集(RDD) 的抽象概念; 论文其余部分基于此建立了一个通用的集群计算栈. RDD 对MapReduce [36] 和Dryad [61] 提出的数据流编程模型进行了扩展;这些模型是目前大数据分析使用最为广泛的编程模型. 数据流系统取得了成功;很重要的因素是用户通过使用比较高级的操作进行计算而无需担心任务分布和系统的容错问题.然而; 随着集群负载的增加; 数据流系统在很多重要的应用场景出现了低效率问题;比如迭代算法; 交互式查询和流式处理. 这引发了大量针对这些应用而定制的计算框架的发展[72;22; 71; 95; 60; 14; 2]. 我们的工作源于观察到很多数据流模型不适用的应用场景所共有的一个特征: 在计算过程中都需要高效率的数据共享. 例如; 迭代算法; 如PageRank; K-means 聚类;或逻辑回归; 都需要进行多次访问相同的数据集; 交互数据挖掘经常需要对于同一数据子集进行多个特定的查询; 而流式应用下则需要随时间对状态信息进行维护和共享. 不幸的是; 尽管 …,*,*,*
Improving Hybrid Keyword-Based Search,Matei A Zaharia; Srinivasan Keshav,Abstract: We present a hybrid peer-to-peer system architecture for keyword-based free-textsearch in environments with heterogeneous document popularities and user lifetimes; suchas file-sharing applications. Our system incorporates several novel design elements thatincrease its effectiveness. These include the use of central servers to collect global statistics;a search algorithm that uses these statistics to decide whether to flood or use a DHT;adaptive flooding; and delayed publishing. The gain due to these techniques is quantified bysimulation: compared to a simple hybrid approach where queries are sent to the DHT if aflood doesn't return enough results; our system can achieve 2.8 times smaller first responsetime; 6.8 times smaller last response time; and 2.6 times smaller bandwidth use; whilereceiving at least as many results as are desired for most queries and maintaining a …,*,*,*
