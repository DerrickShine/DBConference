The Niagara internet query system,Jeffrey F.  Naughton; David J.  DeWitt; David Maier; Ashraf Aboulnaga; Jianjun Chen; Leonidas Galanis; Jaewoo Kang; Rajasekar Krishnamurthy; Qiong Luo; Naveen Prakash; Ravishankar Ramamurthy; Jayavel Shanmugasundaram; Feng Tian; Kristin Tufte; Stratis Viglas; Yuan Wang; Chun Zhang; Bruce Jackson; Anurag Gupta; Rushan Chen,Abstract Recently; there has been a great deal of research into XML query languages toenable the execution of database-style queries over XML files. However; merely being anXML query-processing engine does not render a system suitable for querying the Internet. Auseful system must provide mechanisms to (a) find the XML files that are relevant to a givenquery; and (b) deal with remote data sources that either provide unpredictable data accessand transfer rates; or are infinite streams; or both. The Niagara Internet Query System wasdesigned from the bottom-up to provide these mechanisms. In this article we describe theoverall Niagara architecture; and how Niagara finds relevant XML documents by using acollaboration between the Niagara XML-QL query processor and the Niagara “text-in-context” XML search engine. The Niagara Internet Query System is public domain …,IEEE Data Eng. Bull.,2001,256
SystemML: Declarative machine learning on MapReduce,Amol Ghoting; Rajasekar Krishnamurthy; Edwin Pednault; Berthold Reinwald; Vikas Sindhwani; Shirish Tatikonda; Yuanyuan Tian; Shivakumar Vaithyanathan,MapReduce is emerging as a generic parallel programming paradigm for large clusters ofmachines. This trend combined with the growing need to run machine learning (ML)algorithms on massive datasets has led to an increased interest in implementing MLalgorithms on MapReduce. However; the cost of implementing a large class of MLalgorithms as low-level MapReduce jobs on varying data and machine cluster sizes can beprohibitive. In this paper; we propose SystemML in which ML algorithms are expressed in ahigher-level language and are compiled and executed in a MapReduce environment. Thishigher-level language exposes several constructs including linear algebra primitives thatconstitute key building blocks for a broad class of supervised and unsupervised MLalgorithms. The algorithms expressed in SystemML are compiled and optimized into a set …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,248
On the integration of structure indexes and inverted lists,Raghav Kaushik; Rajasekar Krishnamurthy; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract Several methods have been proposed to evaluate queries over a native XMLDBMS; where the queries specify both path and keyword constraints. These broadly consistof graph traversal approaches; optimized with auxiliary structures known as structureindexes; and approaches based on information-retrieval style inverted lists. We propose astrategy that combines the two forms of auxiliary indexes; and a query evaluation algorithmfor branching path expressions based on this strategy. Our technique is general andapplicable for a wide range of choices of structure indexes and inverted list join algorithms.Our experiments over the Niagara XML DBMS show the benefit of integrating the two formsof indexes. We also consider algorithmic issues in evaluating path expression queries whenthe notion of relevance ranking is incorporated. By integrating the above techniques with …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,230
A general technique for querying XML documents using a relational database system,Jayavel Shanmugasundaram; Eugene Shekita; Jerry Kiernan; Rajasekar Krishnamurthy; Efstratios Viglas; Jeffrey Naughton; Igor Tatarinov,Abstract There has been recent interest in using relational database systems to store andquery XML documents. Each of the techniques proposed in this context works by (a) creatingtables for the purpose of storing XML documents (also called relational schemageneration);(b) storing XML documents by shredding them into rows in the created tables;and (c) converting queries over XML documents into SQL queries over the created tables.Since relational schema generation is a physical database design issue--dependent onfactors such as the nature of the data; the query workload and availability of schemas--therehave been many techniques proposed for this purpose. Currently; each relational schemageneration technique requires its own query processor to efficiently convert queries overXML documents into SQL queries over the created tables. In this paper; we present an …,ACM SIGMOD Record,2001,209
XML-to-SQL query translation literature: The state of the art and open problems,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract Recently; the database research literature has seen an explosion of publicationswith the goal of using an RDBMS to store and/or query XML data. The problems addressedand solved in this area are diverse. This diversity renders it difficult to know how the variousresults presented fit together; and even makes it hard to know what open problems remain.As a first step to rectifying this situation; we present a classification of the problem space anddiscuss how almost 40 papers fit into this classification. As a result of this study; we find thatsome basic questions are still open. In particular; for the XML publishing of relational dataand for “schema-based” shredding of XML documents into relations; there is no publishedalgorithm for translating even simple path expression queries (with the axis) into SQL whenthe XML schema is recursive.,International XML Database Symposium,2003,166
SystemT: a system for declarative information extraction,Rajasekar Krishnamurthy; Yunyao Li; Sriram Raghavan; Frederick Reiss; Shivakumar Vaithyanathan; Huaiyu Zhu,Abstract As applications within and outside the enterprise encounter increasing volumes ofunstructured data; there has been renewed interest in the area of information extraction (IE)--the discipline concerned with extracting structured information from unstructured text.Classical IE techniques developed by the NLP community were based on cascadinggrammars and regular expressions. However; due to the inherent limitations ofgrammarbased extraction; these techniques are unable to:(i) scale to large data sets; and (ii)support the expressivity requirements of complex information tasks. At the IBM AlmadenResearch Center; we are developing SystemT; an IE system that addresses these limitationsby adopting an algebraic approach. By leveraging well-understood database concepts suchas declarative queries and costbased optimization; SystemT enables scalable execution …,ACM SIGMOD Record,2009,146
Avatar information extraction system.,Thathachar S Jayram; Rajasekar Krishnamurthy; Sriram Raghavan; Shivakumar Vaithyanathan; Huaiyu Zhu,Abstract The AVATAR Information Extraction System (IES) at the IBM Almaden ResearchCenter enables highprecision; rule-based; information extraction from text-documents.Drawing from our experience we propose the use of probabilistic database techniques asthe formal underpinnings of information extraction systems so as to maintain high precisionwhile increasing recall. This involves building a framework where rule-based annotators canbe mapped to queries in a database system. We use examples from AVATAR IES todescribe the challenges in achieving this goal. Finally; we show that deriving precisionestimates in such a database system presents a significant challenge for probabilisticdatabase systems.,IEEE Data Eng. Bull.,2006,134
SystemT: an algebraic approach to declarative information extraction,Laura Chiticariu; Rajasekar Krishnamurthy; Yunyao Li; Sriram Raghavan; Frederick R Reiss; Shivakumar Vaithyanathan,Abstract As information extraction (IE) becomes more central to enterprise applications; rule-based IE engines have become increasingly important. In this paper; we describe SystemT;a rule-based IE system whose basic design removes the expressivity and performancelimitations of current systems based on cascading grammars. SystemT uses a declarativerule language; AQL; and an optimizer that generates high-performance algebraic executionplans for AQL rules. We compare SystemT's approach against cascading grammars; boththeoretically and with a thorough experimental evaluation. Our results show that SystemTcan deliver result quality comparable to the state-of-the-art and an order of magnitude higherannotation throughput.,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,2010,132
An algebraic approach to rule-based information extraction,Frederick Reiss; Sriram Raghavan; Rajasekar Krishnamurthy; Huaiyu Zhu; Shivakumar Vaithyanathan,Traditional approaches to rule-based information extraction (IE) have primarily been basedon regular expression grammars. However; these grammar-based systems have difficultyscaling to large data sets and large numbers of rules. Inspired by traditional databaseresearch; we propose an algebraic approach to rule-based IE that addresses thesescalability issues through query optimization. The operators of our algebra are motivated byour experience in building several rule-based extraction programs over diverse data sets.We present the operators of our algebra and propose several optimization strategiesmotivated by the text-specific characteristics of our operators. Finally we validate thepotential benefits of our approach by extensive experiments over real-world blog data.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,132
-Mixed Mode XML Query Processing,Alan Halverson; Josef Burger; Leonidas Galanis; Ameet Kini; Rajasekar Krishnamurthy; Ajith Nagaraja Rao; Feng Tian; Stratis D Viglas; Yuan Wang; Jeffrey F Naughton; David J DeWitt,Querying XML documents typically involves both tree-based navigation and patternmatching similar to that used in structured information retrieval domains. This chapter showsthat for good performance; a native XML query processing system should support queryplans that mix these two processing paradigms. The chapter describes the prototype nativeXML system; and reports on experiments demonstrating that even for simple queries; thereare a number of options for how to combine tree-based navigation and structural joins basedon information retrieval-style inverted lists; and that these options can have widely varyingperformance. It presents the ways of transparently using both techniques in a single system;and provides a cost model for identifying efficient combinations of the techniques. Thepreliminary experimental results prove the viability of the approach.Mixed Mode XML …,*,2003,126
Domain adaptation of rule-based annotators for named-entity recognition tasks,Laura Chiticariu; Rajasekar Krishnamurthy; Yunyao Li; Frederick Reiss; Shivakumar Vaithyanathan,Abstract Named-entity recognition (NER) is an important task required in a wide variety ofapplications. While rule-based systems are appealing due to their well-known"explainability;" most; if not all; state-of-the-art results for NER tasks are based on machinelearning techniques. Motivated by these results; we explore the following natural question inthis paper: Are rule-based systems still a viable approach to named-entity recognition?Specifically; we have designed and implemented a high-level language NERL on top ofSystemT; a general-purpose algebraic information extraction system. NERL is tuned to theneeds of NER tasks and simplifies the process of building; understanding; and customizingcomplex rule-based named-entity annotators. We show that these customized annotatorsmatch or outperform the best published results achieved with machine learning …,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,2010,115
Extending RDBMSs to support sparse datasets using an interpreted attribute storage format,Jennifer L Beckmann; Alan Halverson; Rajasekar Krishnamurthy; Jeffrey F Naughton," Sparse" data; in which relations have many attributes that are null for most tuples; presentsa challenge for relational database management systems. If one uses the normal"horizontal" schema to store such data sets in any of the three leading commercial RDBMS;the result is tables that occupy vast amounts of storage; most of which is devoted to nulls. Ifone attempts to avoid this storage blowup by using a" vertical" schema; the storageutilization is indeed better; but query performance is orders of magnitude slower for certainclasses of queries. In this paper; we argue that the proper way to handle sparse data is notto use a vertical schema; but rather to extend the RDBMS tuple storage format to allow therepresentation of sparse attributes as interpreted fields. The addition of interpreted storageallows for efficient and transparent querying of sparse data; uniform access to all …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,115
Regular expression learning for information extraction,Yunyao Li; Rajasekar Krishnamurthy; Sriram Raghavan; Shivakumar Vaithyanathan; HV Jagadish,Abstract Regular expressions have served as the dominant workhorse of practicalinformation extraction for several years. However; there has been little work on reducing themanual effort involved in building high-quality; complex regular expressions for informationextraction tasks. In this paper; we propose ReLIE; a novel transformation-based algorithm forlearning such complex regular expressions. We evaluate the performance of our algorithmon multiple datasets and compare it against the CRF algorithm. We show that ReLIE; inaddition to being an order of magnitude faster; outperforms CRF under conditions of limitedtraining data and cross-domain data. Finally; we show how the accuracy of CRF can beimproved by using features extracted by ReLIE.,Proceedings of the Conference on Empirical Methods in Natural Language Processing,2008,109
Active query caching for database web servers,Qiong Luo; Jeffrey F Naughton; Rajasekar Krishnamurthy; Pei Cao; Yunrui Li,Abstract A substantial portion of web traffic consists of queries to database web servers.Unfortunately; a common technique to improve web scalability; proxy caching; is ineffectivefor database web servers because existing web proxy servers cannot cache queries. Toaddress this problem; we modify a recently proposed enhanced proxy server; called anactive proxy; to enable Active Query Caching. Our approach works by having the serversend the proxy a query applet; which can process simple queries at the proxy. This enablesthe proxy server to share the database server workload as well as to reduce the networktraffic. We show both opportunities and limitations of this approach through a performancestudy.,International Workshop on the World Wide Web and Databases,2000,100
Avatar semantic search: a database approach to information retrieval,Eser Kandogan; Rajasekar Krishnamurthy; Sriram Raghavan; Shivakumar Vaithyanathan; Huaiyu Zhu,Abstract We present Avatar Semantic Search; a prototype search engine that exploitsannotations in the context of classical keyword search. The process of annotations isaccomplished offline by using high-precision information extraction techniques to extractfacts; con-cepts; and relationships from text. These facts and concepts are represented andindexed in a structured data store. At runtime; keyword queries are interpreted in the contextof these extracted facts and converted into one or more precise queries over the structuredstore. In this demonstration we describe the overall architecture of the Avatar SemanticSearch engine. We also demonstrate the superiority of the AVATAR approach overtraditional keyword search engines using Enron email data set and a blog corpus.,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,95
Method for storing XML documents in a relational database system while exploiting XML schema,*,A method for storing XML documents in a relational database system while exploiting XMLSchema information to capture information about types; inheritance; equivalence classes;and integrity constraints in the generated relational schema; enabling efficient querying. Theinvention simplifies complex XML schema types; translates the simplified XML schema typesinto relational tables; and then stores relations corresponding to each XML element inrelational table rows. The simplification includes grouping all occurrences of a givenelement together; assembling two or more element types into element groups if the schemaindicates that elements of those element types will occur the same number of times; andapplying a number of transformation rules to the element groups. The translation includesconstructing a type graph from the simplified schema; building an element graph for each …,*,2006,91
Recursive XML schemas; recursive XML queries; and relational storage: XML-to-SQL query translation,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Raghav Kaushik; Jeffrey F Naughton,We consider the problem of translating XML queries into SQL when XML documents havebeen stored in an RDBMS using a schema-based relational decomposition. Surprisingly;there is no published XML-to-SQL query translation algorithm for this scenario that handlesrecursive XML schemas. We present a generic algorithm to translate path expressionqueries into SQL in the presence of recursion in the schema and queries. This algorithmhandles a general class of XML-to-relational mappings; which includes all techniquesproposed in literature. Some of the salient features of this algorithm are:(i) It translates a pathexpression query into a single SQL query; irrespective of how complex the XML schemais;(ii) It uses the" with" clause in SQL99 to handle recursive queries even over nonrecursiveschemas;(iii) It reconstructs recursive XML subtrees with a single SQL query and (iv) It …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,82
Systems and methods for processing machine learning algorithms in a MapReduce environment,*,Systems and methods for processing Machine Learning (ML) algorithms in a MapReduceenvironment are described. In one embodiment of a method; the method includes receivinga ML algorithm to be executed in the MapReduce environment. The method further includesparsing the ML algorithm into a plurality of statement blocks in a sequence; wherein eachstatement block comprises a plurality of basic operations (hops). The method also includesautomatically determining an execution plan for each statement block; wherein at least oneof the execution plans comprises one or more low-level operations (lops). The methodfurther includes implementing the execution plans in the sequence of the plurality of thestatement blocks.,*,2013,66
Extracting; linking and integrating data from public sources: A financial case study,Douglas Burdick; Mauricio Hernández; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Constantin Popa; Ioana Stanoi; Shivakumar Vaithyanathan; Sanjiv Ranjan Das,Abstract: We present Midas; a system that uses complex data processing to extract andaggregate facts from a large collection of structured and unstructured documents into a set ofunified; clean entities and relationships. Midas focuses on data for financial companies andis based on periodic filings with the US Securities and Exchange Commission (SEC) andFederal Deposit Insurance Corporation (FDIC). We show that; by using data aggregated byMidas; we can provide valuable insights about financial institutions either at the wholesystem level or at the individual company level. The key technology components that weimplemented in Midas and that enable the various financial applications are: informationextraction; entity resolution; mapping and fusion; all on top of a scalable infrastructure basedon Hadoop. We describe our experience in building the Midas system and also outline …,*,2015,59
Efficient XML-to-SQL query translation: Where to add the intelligence?,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract We consider the efficiency of queries generated by XML to SQL translation. We firstshow that published XML-to-SQL query translation algorithms are suboptimal in that theyoften translate simple path expressions into complex SQL queries even when much simplerequivalent SQL queries exist. There are two logical ways to deal with this problem. Onecould generate suboptimal SQL queries using a fairly naive translation algorithm; and thenattempt to optimize the resulting SQL; or one could use a more intelligent translationalgorithm with the hopes of generating efficient SQL directly. We show that optimizing theSQL after it is generated is problematic; becoming intractable even in simple scenarios; bycontrast; designing a translation algorithm that exploits information readily available attranslation time is a promising alternative. To support this claim; we present a translation …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,59
Getting work done on the web: supporting transactional queries,Yunyao Li; Rajasekar Krishnamurthy; Shivakumar Vaithyanathan; HV Jagadish,Abstract Many searches on the web have a transactional intent. We argue that pagessatisfying transactional needs can be distinguished from the more common pages that havesome information and links; but cannot be used to execute a transaction. Based on thishypothesis; we provide a recipe for constructing a transaction annotator. By constructing anannotator with one corpus and then demonstrating its classification performance on another;we establish its robustness. Finally; we show experimentally that a search procedure thatexploits such pre-annotation greatly outperforms traditional search for retrievingtransactional pages.,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,40
Getting work done on the web: supporting transactional queries,Yunyao Li; Rajasekar Krishnamurthy; Shivakumar Vaithyanathan; HV Jagadish,Abstract Many searches on the web have a transactional intent. We argue that pagessatisfying transactional needs can be distinguished from the more common pages that havesome information and links; but cannot be used to execute a transaction. Based on thishypothesis; we provide a recipe for constructing a transaction annotator. By constructing anannotator with one corpus and then demonstrating its classification performance on another;we establish its robustness. Finally; we show experimentally that a search procedure thatexploits such pre-annotation greatly outperforms traditional search for retrievingtransactional pages.,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,40
HIL: a high-level scripting language for entity integration,Mauricio Hernández; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ryan Wisnesky,Abstract We introduce HIL; a high-level scripting language for entity resolution andintegration. HIL aims at providing the core logic for complex data processing flows thataggregate facts from large collections of structured or unstructured data into clean; unifiedentities. Such flows typically include many stages of processing that start from the outcomeof information extraction and continue with entity resolution; mapping and fusion. A HILprogram captures the overall integration flow through a combination of SQL-like rules thatlink; map; fuse and aggregate entities. A salient feature of HIL is the use of logical indexes inits data model to facilitate the modular construction and aggregation of complex entities.Another feature is the presence of a flexible; open type system that allows HIL to handleinput data that is irregular; sparse or partially known. As a result; HIL can accurately …,Proceedings of the 16th international conference on extending database technology,2013,36
Uncertainty management in rule-based information extraction systems,Eirinaios Michelakis; Rajasekar Krishnamurthy; Peter J Haas; Shivakumar Vaithyanathan,Abstract Rule-based information extraction is a process by which structured objects areextracted from text based on user-defined rules. The compositional nature of rule-basedinformation extraction also allows rules to be expressed over previously extracted objects.Such extraction is inherently uncertain; due to the varying precision associated with the rulesused in a specific extraction task. Quantifying this uncertainty is crucial for querying theextracted objects in probabilistic databases; and for improving the recall of extraction tasksthat use compositional rules. In this paper; we provide a probabilistic framework for handlingthe uncertainty in rule-based information extraction. Specifically; for each extraction task; webuild a parametric exponential model of uncertainty that captures the interaction betweenthe different rules; as well as the compositional nature of the rules; the exponential form of …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,35
Midas: integrating public financial data,Sreeram Balakrishnan; Vivian Chu; Mauricio A Hernández; Howard Ho; Rajasekar Krishnamurthy; Shi Xia Liu; Jan H Pieper; Jeffrey S Pierce; Lucian Popa; Christine M Robson; Lei Shi; Ioana R Stanoi; Edison L Ting; Shivakumar Vaithyanathan; Huahai Yang,Abstract The primary goal of the Midas project is to build a system that enables easy andscalable integration of unstructured and semi-structured information present across multipledata sources. As a first step in this direction; we have built a system that extracts andintegrates information from regulatory filings submitted to the US Securities and ExchangeCommission (SEC) and the Federal Deposit Insurance Corporation (FDIC). Midas creates arepository of entities; events; and relationships by extracting; conceptualizing; integrating;and aggregating data from unstructured and semi-structured documents. This repositoryenables applications to use the extracted and integrated data in a variety of ways includingmashups with other public data and complex risk analysis.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,26
On the difficulty of finding optimal relational decompositions for xml workloads: A complexity theoretic perspective,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Jeffrey F Naughton,Abstract A key problem that arises in the context of storing XML documents in relationaldatabases is that of finding an optimal relational decomposition for a given set of XMLdocuments and a given set of XML queries over those documents. While there have been anumber of ad hoc solutions proposed for this problem; to our knowledge this paperrepresents a first step toward formalizing the problem and studying its complexity. It turns outthat to even define what one means by an optimal decomposition; one first needs to specifyan algorithm to translate XML queries to relational queries; and a cost model to evaluate thequality of the resulting relational queries. By examining an interesting problem embedded inchoosing a relational decomposition; we show that choices of different translation algorithmsand cost models result in very different complexities for the resulting optimization …,International Conference on Database Theory,2003,19
Surfacing time-critical insights from social media,Bogdan Alexe; Mauricio A Hernández; Kirsten W Hildrum; Rajasekar Krishnamurthy; Georgia Koutrika; Meenakshi Nagarajan; Haggai Roitman; Michal Shmueli-Scheuer; Ioana R Stanoi; Chitra Venkatramani; Rohit Wagle,Abstract We propose to demonstrate an end-to-end framework for leveraging time-sensitiveand critical social media information for businesses. More specifically; we focus onidentifying; structuring; integrating; and exposing timely insights that are essential tomarketing services and monitoring reputation over social media. Our system includescomponents for information extraction from text; entity resolution and integration; analytics;and a user interface.,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,17
Unleashing the power of public data for financial risk measurement; regulation; and governance,Mauricio Hernandez; Sanjiv Ranjan Das; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Constantin Popa; Ioana Stanoi; Shivakumar Vaithyanathan,Abstract: We present Midas; a system that uses complex data processing to extract andaggregate facts from a large collection of structured and unstructured documents into a set ofunified; clean entities and relationships. Midas focuses on data for financial companies andis based on periodic filings with the US Securities and Exchange Commission (SEC) andFederal Deposit Insurance Corporation (FDIC). We show that; by using data aggregated byMidas; we can provide valuable insights about financial institutions either at the wholesystem level or at the individual company level. To illustrate; we show how co-lendingrelationships that are extracted and aggregated from EC text filings can be used to constructa network of the major financial institutions. Centrality computations on this network enableus to identify critical hub banks for monitoring systemic risk. Financial analysts or …,*,2010,17
The SystemT IDE: an integrated development environment for information extraction rules,Laura Chiticariu; Vivian Chu; Sajib Dasgupta; Thilo W Goetz; Howard Ho; Rajasekar Krishnamurthy; Alexander Lang; Yunyao Li; Bin Liu; Sriram Raghavan; Frederick R Reiss; Shivakumar Vaithyanathan; Huaiyu Zhu,Abstract Information Extraction (IE)-the problem of extracting structured information fromunstructured text-has become the key enabler for many enterprise applications such assemantic search; business analytics and regulatory compliance. While rule-based IEsystems are widely used in practice due to their well-known" explainability;" developing high-quality information extraction rules is known to be a labor-intensive and time-consumingiterative process. Our demonstration showcases SystemT IDE; the integrated developmentenvironment for SystemT; a state-of-the-art rule-based IE system from IBMResearch that hasbeen successfully embedded in multiple IBM enterprise products. SystemT IDE facilitates thedevelopment; test and analysis of high-quality IE rules by means of sophisticatedtechniques; ranging from data management to machine learning. We show how to build …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,16
Method for storing text annotations with associated type information in a structured data store,*,A text annotation structured storage method stores text annotations with associated typeinformation in a structured data store. The present system persists or stores annotations in astructured data store in an indexable and queryable format. Exemplary structured datastores comprise XML databases and relational databases. The method exploits typeinformation in a type system to develop corresponding schemas in a structured data model.The method comprises techniques for mapping annotations to an XML data model and arelational data model. The method captures various features of the type system; such ascomplex types and inheritance; in the schema for the persistent store. In particular; therepository provides support for path navigation over the hierarchical type system starting atany type.,*,2009,16
Constructing consumer profiles from social media data,Mauricio Hernandez; Kirsten Hildrum; Prateek Jain; Rohit Wagle; Bogdan Alexe; Rajasekar Krishnamurthy; Ioana Roxana Stanoi; Chitra Venkatramani,Social media is playing a growing role in providing consumer feedback to companies abouttheir products and services. To maximize the benefit of this feedback; companies want toknow how different consumer-segments they are interested in; such as parents; frequenttravelers; and comic book fans react to their products and campaigns. In this paper; wedescribe how constructing consumer profiles is valuable to obtain such insights. We presentthe challenges in analyzing noisy social media data and the techniques we employ forbuilding the profiles. We also present detailed experimental results from the analysis of overseven billion messages to construct profiles of over 100 million consumers. We demonstratehow consumer profiles can help in understanding consumer feedback by different keysegments using a TV show analysis scenario.,Big Data; 2013 IEEE International Conference on,2013,15
System and method for exploiting semantic annotations in executing keyword queries over a collection of text documents,*,A query interpretation system exploits semantic annotations in keyword queries over acollection of text documents; casting semantic annotations produced by text analysisengines into a formal annotation type system. The system uses the annotation type system toenumerate various interpretations of a keyword query and automatically translate a keywordquery into a set of interpretations expressed in some intermediate query language. Thesystem returns a result list of documents by combining the results of executing one or moreof these interpretations. Even though the system generates and uses a complex type system;a user is able to use simple keyword queries to locate documents.,*,2009,14
Towards efficient named-entity rule induction for customizability,Ajay Nagesh; Ganesh Ramakrishnan; Laura Chiticariu; Rajasekar Krishnamurthy; Ankush Dharkar; Pushpak Bhattacharyya,Abstract Generic rule-based systems for Information Extraction (IE) have been shown towork reasonably well out-of-the-box; and achieve state-of-the-art accuracy with furtherdomain customization. However; it is generally recognized that manually building andcustomizing rules is a complex and labor intensive process. In this paper; we discuss anapproach that facilitates the process of building customizable rules for Named-EntityRecognition (NER) tasks via rule induction; in the Annotation Query Language (AQL). Givena set of basic features and an annotated document collection; our goal is to generate aninitial set of rules with reasonable accuracy; that are interpretable and thus can be easilyrefined by a human developer. We present an efficient rule induction process; modeled on afour-stage manual rule development process and present initial promising results with our …,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,12
System and method for storing text annotations with associated type information in a structured data store,*,A text annotation structured storage system stores text annotations with associated typeinformation in a structured data store. The present system persists or stores annotations in astructured data store in an indexable and queryable format. Exemplary structured datastores comprise XML databases and relational databases. The system exploits typeinformation in a type system to develop corresponding schemas in a structured data model.The system comprises techniques for mapping annotations to an XML data model and arelational data model. The system captures various features of the type system; such ascomplex types and inheritance; in the schema for the persistent store. In particular; therepository provides support for path navigation over the hierarchical type system starting atany type.,*,2012,11
Indexing for regular expressions in text-centric applications,*,A method; system; and article are provided for evaluating regular expressions over largedata collections. A general purpose index is built to handle complex regular expressions atthe character level. Characters; character classes; and associated metadata are identifiedand stored in an index of a collection of documents. Given a regular expression; a query isgenerated based on the contents of the index. This query is executed over the index toidentify a set of documents in the collection of documents over which the regular expressioncan be evaluated. Based upon the query execution; the identified set of documents isreturned for evaluation by the regular expression responsive to execution of the query overthe index.,*,2012,7
Using structured queries for keyword information retrieval,Rajasekar Krishnamurthy; Sriram Raghavan; Shivakumar Vaithyanathan; Huaiyu Zhu,ABSTRACT An increasingly important class of keyword search tasks are those where usersare looking for a specific piece of information buried within a few documents in a largecollection. Examples include searching for someone's phone number; the schedule for ameeting; or a package tracking URL; within a personal email collection. We refer to suchtasks as “precision-oriented search tasks”. While modern information extraction techniquescan be used to extract the concepts involved in these tasks (persons; phone numbers;schedules; etc.); since users only provide keywords as input; the problem of identifying thedocuments that contain the information of interest remains a challenge. In this paper; wepropose a solution to this problem based on the concept of automatically generating“interpretations” of keyword queries. Interpretations are precise structured queries; over …,IBM Technical Report RJ 10413,2006,7
XML views as integrity constraints and their use in query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,The SQL queries produced in XML-to-SQL query translation are often unnecessarilycomplex; even for simple input XML queries. In this paper we argue that relational systemscan do a better job of XML-to-SQL query translation with the addition of a simple newconstraint; which we term the" lossless from XML" constraint. Intuitively; this constraint statesthat a given relational data set resulted from the shredding of an XML document thatconformed to a given schema. We illustrate the power of this approach by giving analgorithm that exploits the" lossless from XML" constraint to translate path expressionqueries into efficient SQL; even in the presence of recursive XML schemas. We argue thatthis approach is likely to be simpler and more effective than the current state of the art inoptimizing XML-to-SQL query translation; which involves identifying and declaring …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,6
Unraveling the duplicate-elimination problem in XML-to-SQL query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract We consider the scenario where existing relational data is exported as XML. In thiscontext; we look at the problem of translating XML queries into SQL. XML query languageshave two different notions of duplicates: node-identity based and value-based. Pathexpression queries have an implicit node-identity based duplicate elimination built intothem. On the other hand; SQL only supports value-based duplicate elimination. In this paper;using a simple path expression query we illustrate the problems that arise when we attemptto simulate the node-identity based duplicate elimination using value-based duplicateelimination in the SQL queries. We show how a general solution for this problem coveringthe class of views considered in published literature requires a fairly complex mechanism.,Proceedings of the 7th International Workshop on the Web and Databases: colocated with ACM SIGMOD/PODS 2004,2004,6
Data science challenges in real estate asset and capital markets,Douglas Burdick; Michael Franklin; Paulo Issler; Rajasekar Krishnamurthy; Lucian Popa; Louiqa Raschid; Richard Stanton; Nancy Wallace,Abstract The real estate financial markets are complex supply chains. Understanding theirbehavior is limited by a lack of data that would capture the richly interconnected networks offinancial institutions and complex financial products; eg; asset backed securities. This lack oftransparency is further compounded by limited knowledge of the contractual rules thatcontrol the flow of funds from mortgage pools to securities; as well as the financial eventsthat regulate these flows. In this project; we will use the IBM Midas framework and tools toextract entities; relationships; events; contractual rules and risk profiles for financialinstitutions. Our source of information will be the MBS prospectus documents that are publicand are filed with the Securities and Exchange Commission. We will describe the datamanagement needs of the Haas Real Estate and Financial Markets (REFM) Lab and …,Proceedings of the international workshop on data science for macro-modeling,2014,5
Optimizing fixed-schema xml to sql query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,ABSTRACT Recently; there has been a lot of work on evaluating XML queries over datastored in relational database systems. The vast majority of this work has focused on thecases where either the relational schema is not fixed (so the problem is to find a goodrelational schema for a given XML workload) or the XML schema is not fixed (so the problemis to develop generic strategies for exporting XML views of relational data). While thesecases are interesting; in practice a third scenario; in which both the source relational andtarget XML schemas are fixed; seems highly relevant. We show that even in this highlyconstrained environment; there is a lot of freedom in the SQL that can be generated toevaluate a given XML query. Furthermore; we show through experiments with a commercialRDBMS that by exploiting the underlying relational constraints and the properties of a …,Proc of the VLDB Conf,2002,5
Entity integration using high-level scripting languages,*,Embodiments of the present invention relate to a new method of entity integration using high-level scripting languages. In one embodiment; a method of and computer product for entityintegration is provided. An entity declaration is read from a machine readable medium. Theentity declaration describes an entity including at least one nested entity. An indexdeclaration is read from a machine readable medium. The index declaration describes anindex of nested entities. An entity population rule is read from a machine readable medium.The entity population rule describes a mapping from an input schema to an output schema.The output schema conforms to the entity declaration. A plurality of input records is read froma first data store. The input records conform to the input schema. The entity population ruleapplies to the plurality of records to create a plurality of output records complying with the …,*,2017,4
Entity integration using high-level scripting languages,*,Embodiments of the present invention relate to a new method of entity integration using high-level scripting languages. In one embodiment; a method of and computer product for entityintegration is provided. An entity declaration is read from a machine readable medium. Theentity declaration describes an entity including at least one nested entity. An indexdeclaration is read from a machine readable medium. The index declaration describes anindex of nested entities. An entity population rule is read from a machine readable medium.The entity population rule describes a mapping from an input schema to an output schema.The output schema conforms to the entity declaration. A plurality of input records is read froma first data store. The input records conform to the input schema. The entity population ruleapplies to the plurality of records to create a plurality of output records complying with the …,*,2017,4
Financial analytics from public data,Doug Burdick; Alexandre Evfimievski; Rajasekar Krishnamurthy; Neal Lewis; Lucian Popa; Scott Rickards; Peter Williams,Abstract There is a significant amount of" public" unstructured content available that iscentered around the financial performance and counterparty relationships of a wide range oforganizations; including private and public companies; governments; and public utilities.However; the wealth of structured entity and relationship information is buried inside a largeamount of unstructured data. Converting such data into a structured format is essential forbuilding novel analytics applications including counterparty and credit risk monitoring;investment decision making; development of new financial indices; systemic risk analysis;etc. In this paper; we illustrate three different application use cases where large-scaleextraction and integration from the relevant public data enabled the creation of newapplications to perform novel modeling and analysis. We discuss the main technological …,Proceedings of the International Workshop on Data Science for Macro-Modeling,2014,4
Method to search transactional web pages,*,A method of performing transactional web page searches is disclosed. The method includesexamining a plurality of web pages; identifying transactional features within a set of theplurality of web pages; and classifying the set of web pages as transactional. The methodproceeds with annotating and indexing the transactional web pages; and; in response to auser-designated transactional query; providing only the set of web pages that have beenclassified as transactional. The identifying transactional features comprises checking for theexistence of positive patterns and verifying the absence of negative patterns with respect toa set of contents within each of the plurality of web pages and comprises identifyingtransactional actions to be performed and identifying transactional objects of thetransactional actions to be performed. The annotating and indexing the transactional …,*,2008,4
High-level rules for integration and analysis of data: New challenges,Bogdan Alexe; Douglas Burdick; Mauricio A Hernández; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana R Stanoi; Ryan Wisnesky,Abstract Data integration remains a perenially difficult task. The need to access; integrateand make sense of large amounts of data has; in fact; accentuated in recent years. There arenow many publicly available sources of data that can provide valuable information in variousdomains. Concrete examples of public data sources include: bibliographic repositories(DBLP; Cora; Citeseer); online movie databases (IMDB); knowledge bases (Wikipedia;DBpedia; Freebase); social media data (Facebook and Twitter; blogs). Additionally; anumber of more specialized public data repositories are starting to play an increasinglyimportant role. These repositories include; for example; the US federal government data;congress and census data; as well as financial reports archived by the US Securities andExchange Commission (SEC).,*,2013,3
Systems; methods and computer program products for an algebraic approach to rule-based information extraction,*,Systems; methods and computer program products for an algebraic approach to rule-basedinformation extraction. Exemplary embodiments include a method for rule-based informationextraction; the method including specifying an annotator using algebraic operators; whereineach algebraic operator describes annotations identification from text documents.,*,2009,3
AVATAR: Using text analytics to bridge the structured–unstructured divide,Huaiyu Zhu; Sriram Raghavan; Shivakumar Vaithyanathan; Jayram S Thathachar; Rajasekar Krishnamurthy; Prasad Deshpande; Rahul Gupta; Krishna P Chitrapura,Abstract There is a growing need in enterprise applications to query and analyze seamlesslyacross structured and unstructured data. We propose an information system in which textanalytics bridges the structured–unstructured divide. Annotations extracted by text analyticengines; with associated uncertainty; is automatically ingested into a structured data store.We propose an interface that is capable of supporting rich queries over this hybrid data.Uncertainty associated with the extracted information is addressed by building statisticalmodels. We show that different classes of statistical models can be built to address issuessuch as ranking and OLAP style reporting. We are currently building a prototype systemcalled AVATAR that utilizes an existing commercial relational DBMS system as theunderlying storage engine. We present the architecture of AVATAR and identify several …,Almaden. ibm.[Online]. Available: http://www. almaden. ibm. com/cs/projects/avatar/techrep04. pdf,2005,3
Xml-to-sql query translation,Rajasekar Krishnamurthy,Abstract Developing techniques for managing and querying the growing body of XML data isbecomingly increasingly important. A popular approach to evaluating XML queries is totranslate them to relational queries and then to use a relational database system to evaluatethe result. The XML and relational data models are significantly different; and as a result; thecorresponding query languages (XQuery and SQL respectively) also differ significantly. Thismismatch raises some interesting questions:(i) From a functionality perspective; is it possibleto handle all XML data sets using this approach or are there any fundamental limitations inSQL that create problems?(ii) From a performance perspective; are there any implicationson the quality of the SQL queries produced due to this mismatch between the two datamodels? In this thesis; we address the above two questions in two different scenarios …,*,2004,3
The problem of context sensitive string matching,Venkatesan T Chakaravarthy; Rajasekar Krishnamurthy,Abstract In the context sensitive string matching problem; we are given a pattern and a text.The pattern is a string over variables and constants and the text is a string of constants. Thegoal is to find if there is a mapping from variables to strings of constants so that on applyingthis mapping to the pattern we get the given text. Languages like Perl and Python supportsuch a sophisticated string matching. The problem is known to be NP-Complete. In thispaper; we consider a weighted version of this problem that checks how close the pattern canbe matched with the text. We show that this variation is MAXSNP-Complete and cannot beapproximated within a factor of 3313/3312. We show that even the restriction; where thepattern consists of variables only; is NP-Complete and MAXSNP-Complete. When thealphabet is bounded; we give an approximation algorithm for this restriction.,Annual Symposium on Combinatorial Pattern Matching,2002,3
Next generation data analytics at IBM research,Oktie Hassanzadeh; Anastasios Kementsietsidis; Benny Kimelfeld; Rajasekar Krishnamurthy; Fatma Özcan; Ippokratis Pandis,IBM Research has a rich history of innovation in information management with severalrevolutionary breakthroughs; including the invention of relational databases; advanced textanalytics demonstrated by Watson; and the first data mining algorithms to name a few. IBMResearch has been committed to contributing to the community via seminal papers;exemplified by several 10-year awards received by IBM researchers. This short abstract isintended as a quick tour of some of the current information management projects; and notmeant to be an exhaustive list by any means. There has been many disruptive technologicaldevelopments over the last decade. The emergence of cloud computing; and several largescale data processing platforms; advances in on-line social media; the explosion of datavolumes; and the advances in hardware have all forced us to rethink the information …,Proceedings of the VLDB Endowment,2013,2
Towards re-defining relation understanding in financial domain,Chenguang Wang; Doug Burdick; Laura Chiticariu; Rajasekar Krishnamurthy; Yunyao Li; Huaiyu Zhu,Abstract We describe our experiences in participating in the scored task for the 2017 FEIIIData Challenge. Our approach is to model the problem as a binary classification problemand train an ensemble model leveraging domain features that capture financial terminology.We share challenge results for our submission; which performed well achieving the highestscore in four out of six evaluation criteria. We describe semantic complexities encounteredwith regards to the task definition and ambiguities in the labeled dataset. We present analternative task formulation Relationship Validation that addresses some of these semanticcomplexities and demonstrate how our approach naturally extends to this simplified taskdefinition.,Proceedings of the 3rd International Workshop on Data Science for Macro--Modeling with Financial and Economic Datasets,2017,1
Towards high-precision and reusable entity resolution algorithms over sparse financial datasets,Douglas Burdick; Lucian Popa; Rajasekar Krishnamurthy,Abstract We describe our approach to the FEIII Data Challenge; which requires matchingentities across multiple financial datasets (FFIEC; SEC and LEI). By making use of a high-level language (HIL) that includes constructs for expressing both the matching logic and thepolicies to avoid or reduce the ambiguities among the matches; we are able to producehighly-accurate results in a sparse context; with only name and location attributes. As part ofthe high-level specification; we also make use of a Smart-Term Generation (STG)component; which provides us with a sophisticated subroutine for normalizing companynames. The high-level specification is reusable; in the sense that the same HIL specification(modulo changing the attribute names) is uniformly applicable not only between FFIEC andSEC; but also between FFIEC and LEI; and between LEI and SEC. Our approach used …,Proceedings of the Second International Workshop on Data Science for Macro-Modeling,2016,1
Entity resolution between datasets,*,Embodiments relate to entity resolution. One aspect includes creating a deterministic modelby defining an entity to be resolved; selecting two datasets for comparison; definingmatching predicates for attributes of the datasets to select a set of candidate matches; anddefining a precedence rule for the candidate matches to select a subset of the candidatematches. An aspect further includes running the deterministic model on the two datasets.Running the deterministic model includes applying the matching predicates and theprecedence rule to data in the datasets that correspond to the attributes. An aspect alsoincludes applying a cardinality rule to results of the running; and outputting the matchingcandidates for which the cardinality rule is satisfied.,*,2016,1
English-language translation of exact interpretations of keyword queries,*,The present invention relates to a methodology to translate exact interpretations of keywordqueries into meaningful and grammatically correct plain-language queries in order toconvey the meaning of these interpretations to the initiator of the search. The methodincludes the steps of generating at least one grammatically valid plain-language sentenceinterpretation for a keyword query from a generated sentence plain-language sentenceclauses; wherein the grammatically valid plain-language sentence is based upon differingmatching elements; and presenting at least one grammatically valid plain-languagesentence interpretation for the keyword query to a keyword query system user for the user'sreview.,*,2011,1
Web Information Extraction,Rajasekar Krishnamurthy; Yunyao Li; Sriram Raghavan; Frederick Reiss; Shivakumar Vaithyanathan; Huaiyu Zhu,W3C was founded in 1994 by the inventor of the World Wide Web Tim Berners-Lee as avendor-neutral forum for building consensus around Web technologies. The consortiumconsists of member organization and dedicated staff of technical experts. Membership isopen to any organization or individual whose application is reviewed and approved by theW3C. Usually W3C members invest significant resources into the Web technologies. W3Cfulfils its mission by creation of recommendations enjoying status of international standards.In the first 10 years of existence; it produced over eighty W3C recommendations. W3C isresponsible for such technologies as HTML; XHTML; XML; XML Schema; CSS; SOAP;WSDL and others. W3C members play a leading role in the development of therecommendations. W3C initiatives involve international; national; and regional …,*,2009,1
Dealing with (un) structuredness in XML Data and Queries Using Relational Databases,Rajasekar Krishnamurthy; Jeffrey F Naughton; Jayavel Shanmugasundaram; Eugene Shekita,Abstract An XML database can contain documents with varying degrees of schemainformation. The queries can also range from fully specified structured SQL like queries topartially specified regular path expression queries. Relational databases are widely used tostore and query XML data and various schemes have been proposed to this end. Theyeither use DTDs or assume schemaless data. We show how more advanced schemainformation (like XMLSchema); even if partially available; can be used effectively to answerqueries. We also show how the interaction between the amount of schema informationavailable and the query workload plays an important role in choosing a decompositionstrategy into relational tables; suited to that workload. We propose a simple cost metric forthis purpose. Our experiments indicate that this metric can provide a reasonable choice …,DB seminar at Wise university,2001,1
Creation and interaction with large-scale domain-specific knowledge bases,S Bharadwaj; L Chiticariu; M Danilevsky; S Dhingra; S Divekar; A Carreno-Fuentes; H Gupta; N Gupta; S-D Han; M Hernández; H Ho; P Jain; S Joshi; H Karanam; S Krishnan; R Krishnamurthy; Y Li; S Manivannan; A Mittal; F Özcan; A Quamar; P Raman; D Saha; K Sankaranarayanan; J Sen; P Sen; S Vaithyanathan; M Vasa; H Wang; H Zhu,Abstract The ability to create and interact with large-scale domain-specific knowledge basesfrom unstructured/semi-structured data is the foundation for many industry-focused cognitivesystems. We will demonstrate the Content Services system that provides cloud services forcreating and querying high-quality domain-specific knowledge bases by analyzing andintegrating multiple (un/semi) structured content sources. We will showcase an instantiationof the system for a financial domain. We will also demonstrate both cross-lingual naturallanguage queries and programmatic API calls for interacting with this knowledge base.,Proceedings of the VLDB Endowment,2017,*
Web Information Extraction,Laura Chiticariu; Marina Danilevsky; Howard Ho; Rajasekar Krishnamurthy; Yunyao Li; Sriram Raghavan; Frederick Reiss; Shivakumar Vaithyanathan; Huaiyu Zhu,Information extraction (IE) is the process of automatically extracting structured pieces ofinformation from unstructured or semi-structured text documents. Classical problems ininformation extraction include named-entity recognition (identifying mentions of persons;places; organizations; etc.) and relationship extraction (identifying mentions of relationshipsbetween such named entities). Web information extraction is the application of IE techniquesto process the vast amounts of unstructured content on the Web. Due to the nature of thecontent on the Web; in addition to named-entity and relationship extraction; there is growinginterest in more complex tasks such as extraction of reviews; opinions; and sentiments.,management,2016,*
Constructing concepts from a task specification,*,Embodiments relate to facilitating construction of concepts from a task specification. Amethod includes receiving; from a user via a user interface; a task specification in naturallanguage form. The method also includes parsing the task specification into a plurality ofcomponents; and searching a database for an existing concept having a pattern thatapproximates at least a portion of the plurality of components. The concept includessemantic meanings that are representable by textual patterns. The method further includesidentifying any components of the plurality of components that are not included in theexisting concept; and building a new concept that combines the existing concept and thecomponents of the plurality of components that are not included in the existing concept.,*,2015,*
1. IMPORTANT DATES,Carmen Tang,It is widely understood that China shelters one of the richest mycofloras in the world. Thevariety of ecological zones; topological relief; and geographical extent supports this fact. Theoldest and largest mycological collection in the country is that maintained at the Institute ofMicrobiology; Academia Sinica; in Beijing. The Beijing herbarium (acronym: HMAS) wasestablished in 1953 and now houses over 75;000 specimens of fungi. The rapiddevelopment of mycology in China during the last two decades has not been brought tointernational attention (but see Bartholomew; Brittonia 3 1: 1-25; 1979; Ma; Taxon 38: 617-620; 1989). It is a pleasure; therefore; to introduce the mycological herbarium at Kunming;Yunnan; and to welcome foreign visitors and loan requests.,*,2013,*
English-language translation of exact interpretations of keyword queries,*,The present invention relates to a methodology to translate exact interpretations of keywordqueries into meaningful and grammatically correct plain-language queries in order toconvey the meaning of these interpretations to the initiator of the search. The methodincludes the steps of generating at least one grammatically valid plain-language sentenceinterpretation for a keyword query form a generated sentence is based upon differingmatching elements; and presenting at least one grammatically valid plain-languagesentence interpretation for the keyword query to a keyword query system user for the user'sreview.,*,2008,*
Setting of Injection Molding Packing profile,Furong Gao; Xi Chen,Setting of Injection Molding Packing profile.,Advanced Molding and Mold Conference,2003,*
Semistructured versus Structured Data-On the Difficulty of Finding Optimal Relational Decompositions for XML Workloads: A Complexity Theoretic Perspective,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Jeffrey F Naughton,*,Lecture Notes in Computer Science,2003,*
Engineering Challenges for Effective Supply Chain Control,R Krishnamurthy,*,icde,2002,*
Frost resistance of concrete incorporated with a natural polymer-based admixture,Zongjin Li; CK Chau,Frost resistance of concrete incorporated with a natural polymer-based admixture.,Magazine of Concrete Research,2001,*
Change Management in Web Database Systems,Rajasekar Krishnamurthy; Ravishankar Ramamurthy,There has been a lot of recent interest in the integration of data from a large number ofsources. This has surfaced mainly due to the growing popularity of the Web. The Web initself can be viewed as a large distributed database which is composed of hundreds ofthousands of nodes. Even though there have been reasonable solutions for building DDBs;these have been successful only over a small number of nodes like a LAN and do not scaleto the magnitude of the internet. A lot of challenges exist in the areas of query optimization;searching for relevant data; creating integrated repositories and sharing data acrossorganizations. These issues are discussed in 1] to some extent. In this project we will bemainly dealing with change management issues. In other words; how can the latest data bemade available to interested users. The Gestalts framework provides a mechanism for …,*,1999,*
Data Engineering,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath; Elisa Bertino; Federica Paci; Rodolfo Ferrini; Ning Shang; Kevin Beyer; Vuk Ercegovac; Rajasekar Krishnamurthy; Sriram Raghavan; Jun Rao; Frederick Reiss; Eugene J Shekita; David Simmen; Sandeep Tata; Shivakumar Vaithyanathan; Huaiyu Zhu,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Active Query Caching for Database Web Servers,Qiong Luo Jeffrey F Naughton; Rajasekar Krishnamurthy; Pei Cao; Yunrui Li,ABSTRACT A substantial portion of web traffic consists of queries to database web servers.Unfortunately; a common technique to improve web scalability; proxy caching; is ineffectivefor database web servers because existing web proxy servers cannot cache queries. Toaddress this problem; we modify a recently proposed enhanced proxy server; called anactive proxy; to enable Active Query Caching. Our approach works by having the serversend the proxy aquery applet; which can process simple queries at the proxy. This enablesthe proxy server to share the database server workload as well as to reduce the networktraffic. We show both opportunities and limitations of this approach through a performancestudy.,*,*,*
On the Integration of Structure Indexes and Inverted Lists Paper Id: 616,Raghav Kaushik; Rajasekar Krishnamurthy; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract We consider the problem of how to combine structure indexes and inverted lists toanswer queries over a native XML DBMS; where the queries specify both path and keywordconstraints. We augment the inverted list entries to integrate them with a given structureindex; and present novel algorithms for evaluating branching path expressions. Ourexperiments show the benefit of integrating the two forms of indexes. We also consider theproblem of evaluating path expression queries with (several alternative) extensions thatincorporate relevance ranking. By integrating the above techniques with the ThresholdAlgorithm proposed by Fagin et al.; we obtain instance optimal algorithms to push down topk computation.,*,*,*
Data Engineering,Jens Bleiholder; Melanie Herschel; Felix Naumann; Lukasz Golab; Flip Korn; Divesh Srivastava; Arvind Arasu; Surajit Chaudhuri; Zhimin Chen; Kris Ganjam; Raghav Kaushik; Vivek Narasayya; Doug Burdick; Mauricio Hernandez; Howard Ho; Georgia Koutrika; Rajasekar Krishnamurthy; Lucian Popa; Ioana R Stanoi; Shivakumar Vaithyanathan; Sanjiv Das,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
