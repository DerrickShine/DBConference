Scicumulus: A lightweight cloud middleware to explore many task computing paradigm in scientific workflows,Daniel de Oliveira; Eduardo Ogasawara; Fernanda Baião; Marta Mattoso,Most of the large-scale scientific experiments modeled as scientific workflows produce alarge amount of data and require workflow parallelism to reduce workflow execution time.Some of the existing Scientific Workflow Management Systems (SWfMS) explore parallelismtechniques-such as parameter sweep and data fragmentation. In those systems; severalcomputing resources are used to accomplish many computational tasks in homogeneousenvironments; such as multiprocessor machines or cluster systems. Cloud computing hasbecome a popular high performance computing model in which (virtualized) resources areprovided as services over the Web. Some scientists are starting to adopt the cloud model inscientific domains and are moving their scientific workflows (programs and data) from localenvironments to the cloud. Nevertheless; it is still difficult for the scientist to express a …,Cloud Computing (CLOUD); 2010 IEEE 3rd International Conference on,2010,158
An algebraic approach for data-centric scientific workflows,Eduardo Ogasawara; Jonas Dias; D Oliveira; Fábio Porto; Patrick Valduriez; Marta Mattoso,ABSTRACT Scientific workflows have emerged as a basic abstraction for structuring andexecuting scientific experiments in computational environments. In many situations; theseworkflows are computationally and data intensive; thus requiring execution in large-scaleparallel computers. However; parallelization of scientific workflows remains low-level; ad-hoc and laborintensive; which makes it hard to exploit optimization opportunities. To addressthis problem; we propose an algebraic approach (inspired by relational algebra) and aparallel execution model that enable automatic optimization of scientific workflows. Weconducted a thorough validation of our approach using both a real oil exploitationapplication and synthetic data scenarios. The experiments were run in Chiron; a data-centricscientific workflow engine implemented to support our algebraic approach. Our …,Proc. of VLDB Endowment,2011,122
Towards supporting the life cycle of large scale scientific experiments,Marta Mattoso; Claudia Werner; Guilherme Horta Travassos; Vanessa Braganholo; Eduardo Ogasawara; Daniel Oliveira; Sergio Cruz; Wallace Martinho; Leonardo Murta,One of the main challenges of scientific experiments is to allow scientists to manage andexchange their scientific computational resources (data; programs; models; etc.). Theeffective management of such experiments requires a specific set of cardinal facilities; suchas experiment specification techniques; workflow derivation heuristics and provenancemechanisms. These facilities may characterise the experiment life cycle into three phases:composition; execution; and analysis. Works concerned with supporting scientific workflowsare mainly concerned with the execution and analysis phase. Therefore; they fail to supportthe scientific experiment throughout its life cycle as a set of integrated experimentationtechnologies. In large scale experiments this represents a research challenge. We proposean approach for managing large scale experiments based on provenance gathering …,International Journal of Business Process Integration and Management,2010,112
A provenance-based adaptive scheduling heuristic for parallel scientific workflows in clouds,Daniel de Oliveira; Kary ACS Ocaña; Fernanda Baião; Marta Mattoso,Abstract In the last years; scientific workflows have emerged as a fundamental abstractionfor structuring and executing scientific experiments in computational environments. Scientificworkflows are becoming increasingly complex and more demanding in terms ofcomputational resources; thus requiring the usage of parallel techniques and highperformance computing (HPC) environments. Meanwhile; clouds have emerged as a newparadigm where resources are virtualized and provided on demand. By using clouds;scientists have expanded beyond single parallel computers to hundreds or even thousandsof virtual machines. Although the initial focus of clouds was to provide high throughputcomputing; clouds are already being used to provide an HPC environment where elasticresources can be instantiated on demand during the course of a scientific workflow …,Journal of Grid Computing,2012,86
SciPhy: a cloud-based workflow for phylogenetic analysis of drug targets in protozoan genomes,Kary ACS Ocaña; Daniel de Oliveira; Eduardo Ogasawara; Alberto MR Dávila; Alexandre AB Lima; Marta Mattoso,Abstract Bioinformatics experiments are rapidly evolving with genomic projects that analyzelarge amounts of data. This fact demands high performance computation and opens up forexploring new approaches to provide better control and performance when runningexperiments; including Phylogeny/Phylogenomics. We designed a phylogenetic scientificworkflow; named SciPhy; to construct phylogenetic trees from a set of drug target enzymesfound in protozoan genomes. Our contribution is the development; implementation and testof SciPhy in public cloud computing environments. SciPhy can be used in otherBioinformatics experiments to control a systematic execution with high performance whileproducing provenance data.,Brazilian Symposium on Bioinformatics,2011,74
Towards a taxonomy for cloud computing from an e-science perspective,Daniel de Oliveira; Fernanda Araujo Baião; Marta Mattoso,Abstract In the last few years; cloud computing has emerged as a computational paradigmthat enables scientists to build more complex scientific applications to manage large datasets or high-performance applications; based on distributed resources. By following thisparadigm; scientists may use distributed resources (infrastructure; storage; databases; andapplications) without having to deal with implementation or configuration details. In fact;there are many cloud computing environments already available for use. Despite its fastgrowth and adoption; the definition of cloud computing is not a consensus. This makes itvery difficult to comprehend the cloud computing field as a whole; correlate; classify; andcompare the various existing proposals. Over the years; taxonomy techniques have beenused to create models that allow for the classification of concepts within a domain. The …,*,2010,68
Towards a taxonomy for cloud computing from an e-science perspective,Daniel de Oliveira; Fernanda Araujo Baião; Marta Mattoso,Abstract In the last few years; cloud computing has emerged as a computational paradigmthat enables scientists to build more complex scientific applications to manage large datasets or high-performance applications; based on distributed resources. By following thisparadigm; scientists may use distributed resources (infrastructure; storage; databases; andapplications) without having to deal with implementation or configuration details. In fact;there are many cloud computing environments already available for use. Despite its fastgrowth and adoption; the definition of cloud computing is not a consensus. This makes itvery difficult to comprehend the cloud computing field as a whole; correlate; classify; andcompare the various existing proposals. Over the years; taxonomy techniques have beenused to create models that allow for the classification of concepts within a domain. The …,*,2010,68
Chiron: a parallel engine for algebraic scientific workflows,Eduardo Ogasawara; Jonas Dias; Vitor Silva; Fernando Chirigati; Daniel Oliveira; Fabio Porto; Patrick Valduriez; Marta Mattoso,SUMMARY Large-scale scientific experiments based on computer simulations are typicallymodeled as scientific workflows; which eases the chaining of different programs. Thesescientific workflows are defined; executed; and monitored by scientific workflowmanagement systems (SWfMS). As these experiments manage large amounts of data; itbecomes critical to execute them in high-performance computing environments; such asclusters; grids; and clouds. However; few SWfMS provide parallel support. The ones that doso are usually labor-intensive for workflow developers and have limited primitives tooptimize workflow execution. To address these issues; we developed workflow algebra tospecify and enable the optimization of parallel execution of scientific workflows. In thispaper; we show how the workflow algebra is efficiently implemented in Chiron; an …,Concurrency and Computation: Practice and Experience,2013,62
Exploring many task computing in scientific workflows,Eduardo Ogasawara; Daniel de Oliveira; Fernando Chirigati; Carlos Eduardo Barbosa; Renato Elias; Vanessa Braganholo; Alvaro Coutinho; Marta Mattoso,Abstract One of the main advantages of using a scientific workflow management system(SWfMS) to orchestrate data flows among scientific activities is to control and register thewhole workflow execution. The execution of activities within a workflow with highperformance computing (HPC) presents challenges in SWfMS execution control. Currentsolutions leave the scheduling to the HPC queue system. Since the workflow executionengine does not run on remote clusters; SWfMS are not aware of the parallel strategy of theworkflow execution. Consequently; remote execution control and provenance registry of theparallel activities is very limited from the SWfMS side. This work presents a set ofcomponents to be included on the workflow specification of any SWMfS to controlparallelization of activities as MTC. In addition; these components can gather provenance …,Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers,2009,55
Capturing and querying workflow runtime provenance with PROV: a practical approach,Flavio Costa; Vítor Silva; Daniel de Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Abstract Scientific workflows are commonly used to model and execute large-scale scientificexperiments. They represent key resources for scientists and are enacted and managed byScientific Workflow Management Systems (SWfMS). Each SWfMS has its particularapproach to execute workflows and to capture and manage their provenance data. Due tothe large scale of experiments; it may be unviable to analyze provenance data only after theend of the execution. A single experiment may demand weeks to run; even in highperformance computing environments. Thus scientists need to monitor the experimentduring its execution; and this can be done through provenance data. Runtime provenanceanalysis allows for scientists to monitor workflow execution and to take actions before theend of it (ie. workflow steering). This provenance data can also be used to fine-tune the …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,52
An adaptive parallel execution strategy for cloud‐based scientific workflows,Daniel Oliveira; Eduardo Ogasawara; Kary Ocaña; Fernanda Baião; Marta Mattoso,SUMMARY Many of the existing large-scale scientific experiments modeled as scientificworkflows are compute-intensive. Some scientific workflow management systems alreadyexplore parallel techniques; such as parameter sweep and data fragmentation; to improveperformance. In those systems; computing resources are used to accomplish manycomputational tasks in high performance environments; such as multiprocessor machines orclusters. Meanwhile; cloud computing provides scalable and elastic resources that can beinstantiated on demand during the course of a scientific experiment; without requiring itsusers to acquire expensive infrastructure or to configure many pieces of software. In fact;because of these advantages some scientists have already adopted the cloud model in theirscientific experiments. However; this model also raises many challenges. When scientists …,Concurrency and Computation: Practice and Experience,2012,44
Supporting dynamic parameter sweep in adaptive and user-steered workflow,Jonas Dias; Eduardo Ogasawara; Daniel de Oliveira; Fabio Porto; Alvaro LGA Coutinho; Marta Mattoso,Abstract Large-scale experiments in computational science are complex to manage. Due toits exploratory nature; several iterations evaluate a large space of parameter combinations.Scientists analyze partial results and dynamically interfere on the next steps of thesimulation. Scientific workflow management systems can execute those experiments byproviding process management; distributed execution and provenance data. However;supporting scientists in complex exploratory processes involving dynamic workflows is still achallenge. Features; such as user steering on workflows to track; evaluate and adapt theexecution need to be designed to support iterative methods. We provide an approach tosupport dynamic parameter sweep; in which scientists can use the results obtained in a sliceof the parameter space to improve the remainder of the execution. We propose new …,Proceedings of the 6th workshop on Workflows in support of large-scale science,2011,38
Optimizing Phylogenetic Analysis Using SciHmm Cloud-based Scientific Workflow,Kary ACS Ocaña; Daniel de Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Phylogenetic analysis and multiple sequence alignment (MSA) are closely relatedbioinformatics fields. Phylogenetic analysis makes extensive use of MSA in the constructionof phylogenetic trees; which are used to infer the evolutionary relationships betweenhomologous genes. These bioinformatics experiments are usually modeled as scientificworkflows. There are many alternative workflows that use different MSA methods to conductphylogenetic analysis and each one can produce MSA with different quality. Scientists haveto explore which MSA method is the most suitable for their experiments. However; workflowsfor phylogenetic analysis are both computational and data intensive and they may runsequentially during weeks. Although there any many approaches that parallelize theseworkflows; exploring all MSA methods many become a burden and expensive task. If …,E-Science (e-Science); 2011 IEEE 7th International Conference on,2011,37
Dynamic steering of HPC scientific workflows: A survey,Marta Mattoso; Jonas Dias; Kary ACS Ocaña; Eduardo Ogasawara; Flavio Costa; Felipe Horta; Vítor Silva; Daniel de Oliveira,Abstract The field of scientific workflow management systems has grown significantly asapplications start using them successfully. In 2007; several active researchers in scientificworkflow developments presented the challenges for the state of the art in workflowtechnologies at that time. Many issues have been addressed; but one of them named'dynamic workflows and user steering'remains with many open problems despite thecontributions presented in the recent years. This article surveys the early and current effortsin this topic and proposes a taxonomy to identify the main concepts related to addressingissues in dynamic steering of high performance computing (HPC) in scientific workflows. Themain concepts are related to putting the human-in-the-loop of the workflow lifecycle;involving user support in real-time monitoring; notification; analysis and interference by …,Future Generation Computer Systems,2015,34
A performance evaluation of x-ray crystallography scientific workflow using scicumulus,Daniel de Oliveira; Kary Ocana; Eduardo Ogasawara; Jonas Dias; Fernanda Baiao; Marta Mattoso,X-ray crystallography is an important field due to its role in drug discovery and its relevancein bioinformatics experiments of comparative genomics; phylogenomics; evolutionaryanalysis; ortholog detection; and three-dimensional structure determination. Managing theseexperiments is a challenging task due to the orchestration of legacy tools and themanagement of several variations of the same experiment. Workflows can model a coherentflow of activities that are managed by scientific workflow management systems (SWfMS).Due to the huge amount of variations of the workflow to be explored (parameters; input data)it is often necessary to execute X-ray crystallography experiments in High PerformanceComputing (HPC) environments. Cloud computing is well known for its scalable and elasticHPC model. In this paper; we present a performance evaluation for the X-ray …,Cloud Computing (CLOUD); 2011 IEEE International Conference on,2011,33
Performance evaluation of parallel strategies in public clouds: A study with phylogenomic workflows,Daniel De Oliveira; Kary ACS Ocana; Eduardo Ogasawara; Jonas Dias; Joao Gonçalves; Fernanda Baiao; Marta Mattoso,Abstract Data analysis is an exploratory process that demands high performance computing(HPC). SciPhylomics; for example; is a data-intensive workflow that aims at producingphylogenomic trees based on an input set of protein sequences of genomes to inferevolutionary relationships among living organisms. SciPhylomics can benefit from parallelprocessing techniques provided by existing approaches such as SciCumulus cloudworkflow engine and MapReduce implementations such as Hadoop. Despite someperformance fluctuations; computing clouds provide a new dimension for HPC due to itselasticity and availability features. In this paper; we present a performance evaluation forSciPhylomics executions in a real cloud environment. The workflow was executed using twoparallel execution approaches (SciCumulus and Hadoop) at the Amazon EC2 cloud. Our …,Future Generation Computer Systems,2013,32
UNCERTAINTY QUANTIFICATION IN COMPUTATIONAL PREDICTIVE MODELS FOR FLUID DYNAMICS USING A WORKFLOW MANAGEMENT ENGINE,Gabriel Guerra; Fernando A Rochinha; Renato Elias; Daniel de Oliveira; Eduardo Ogasawara; Jonas Furtado Dias; Marta Mattoso; Alvaro LGA Coutinho,ABSTRACT Computational simulation of complex engineered systems requires intensivecomputation and a significant amount of data management. Today; this management is oftencarried out on a case-by-case basis and requires great effort to track it. This is due to thecomplexity of controlling a large amount of data flowing along a chain of simulations.Moreover; many times there is a need to explore parameter variability for the same set ofdata. On a case-by-case basis; there is no register of data involved in the simulation; makingthis process prone to errors. In addition; if the user wants to analyze the behavior of asimulation sample; then he/she must wait until the end of the whole simulation. In thiscontext; techniques and methodologies of scientific workflows can improve the managementof simulations. Parameter variability can be put in the general context of uncertainty …,International Journal for Uncertainty Quantification,2012,31
Adaptive normalization: A novel data normalization approach for non-stationary time series,Eduardo Ogasawara; Leonardo C Martinez; Daniel De Oliveira; Geraldo Zimbrão; Gisele L Pappa; Marta Mattoso,Data normalization is a fundamental preprocessing step for mining and learning from data.However; finding an appropriated method to deal with time series normalization is not asimple task. This is because most of the traditional normalization methods makeassumptions that do not hold for most time series. The first assumption is that all time seriesare stationary; ie; their statistical properties; such as mean and standard deviation; do notchange over time. The second assumption is that the volatility of the time series isconsidered uniform. None of the methods currently available in the literature address theseissues. This paper proposes a new method for normalizing non-stationary heteroscedastic(with non-uniform volatility) time series. The method; named Adaptive Normalization (AN);was tested together with an Artificial Neural Network (ANN) in three forecast problems …,Neural Networks (IJCNN); The 2010 International Joint Conference on,2010,29
Data parallelism in bioinformatics workflows using Hydra,Fábio Coutinho; Eduardo Ogasawara; Daniel de Oliveira; Vanessa Braganholo; Alexandre AB Lima; Alberto MR Dávila; Marta Mattoso,Abstract Large scale bioinformatics experiments are usually composed by a set of data flowsgenerated by a chain of activities (programs or services) that may be modeled as scientificworkflows. Current Scientific Workflow Management Systems (SWfMS) are used toorchestrate these workflows to control and monitor the whole execution. It is very common inbioinformatics experiments to process very large datasets. In this way; data parallelism is acommon approach used to increase performance and reduce overall execution time.However; most of current SWfMS still lack on supporting parallel executions in highperformance computing (HPC) environments. Additionally keeping track of provenance datain distributed environments is still an open; yet important problem. Recently; Hydramiddleware was proposed to bridge the gap between the SWfMS and the HPC …,Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing,2010,29
Optimizing virtual machine allocation for parallel scientific workflows in federated clouds,Rafaelli de C Coutinho; Lúcia MA Drummond; Yuri Frota; Daniel de Oliveira,Abstract Cloud computing has established itself as an interesting computational model thatprovides a wide range of resources such as storage; databases and computing power forseveral types of users. Recently; the concept of cloud computing was extended with theconcept of federated clouds where several resources from different cloud providers are inter-connected to perform a common action (eg execute a scientific workflow). Users can benefitfrom both single-provider and federated cloud environment to execute their scientificworkflows since they can get the necessary amount of resources on demand. In several ofthese workflows; there is a demand for high performance and parallelism techniques sincemany activities are data and computing intensive and can execute for hours; days or evenweeks. There are some Scientific Workflow Management Systems (SWfMS) that already …,Future Generation Computer Systems,2015,27
User-steering of HPC workflows: state-of-the-art and future directions,Marta Mattoso; Kary Ocaña; Felipe Horta; Jonas Dias; Eduardo Ogasawara; Vitor Silva; Daniel de Oliveira; Flavio Costa; Igor Araújo,Abstract In 2006 a group of leading researchers was gathered to discuss several challengesto scientific workflow supporting technologies and many of which still remain openchallenges; such as the steering of workflows by users. Due to big data and long lastingworkflows; many users demand steering features such as real-time monitoring; analysis andspecially execution interference. The workflow execution should respond dynamically tosuch interference in the execution; to support the experimentation process in highperformance computing. This paper revisits the issues in the user steering and dynamicworkflows; presenting the state-of-the-art in it; and the open challenges. Our goal is todiscuss research issues related to scientists' steering and present some ideas on how thesedemands may be supported in current scientific workflow technologies.,Proceedings of the 2nd ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies,2013,23
Exploring molecular evolution reconstruction using a parallel cloud based scientific workflow,Kary ACS Ocaña; Daniel de Oliveira; Felipe Horta; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Abstract Recent studies of evolution at molecular level address two important issues:reconstruction of the evolutionary relationships between species and investigation of theforces of the evolutionary process. Both issues experienced an explosive growth in the lasttwo decades due to massive generation of genomic data; novel statistical methods andcomputational approaches to process and analyze this large volume of data. Mostexperiments in molecular evolution are based on computing intensive simulations precededby other computation tools and post-processed by computing validators. All these tools canbe modeled as scientific workflows to improve the experiment management while capturingprovenance data. However; these evolutionary analyses experiments are very complex andmay execute for weeks. These workflows need to be executed in parallel in High …,Brazilian Symposium on Bioinformatics,2012,22
Algebraic dataflows for big data analysis,Jonas Dias; Eduardo Ogasawara; Daniel de Oliveira; Fabio Porto; Patrick Valduriez; Marta Mattoso,Analyzing big data requires the support of dataflows with many activities to extract andexplore relevant information from the data. Recent approaches such as Pig Latin propose ahigh-level language to model such dataflows. However; the dataflow execution is typicallydelegated to a MapRe-duce implementation such as Hadoop; which does not follow analgebraic approach; thus it cannot take advantage of the optimization opportunities ofPigLatin algebra. In this paper; we propose an approach for big data analysis based onalgebraic workflows; which yields optimization and parallel execution of activities andsupports user steering using provenance queries. We illustrate how a big data processingdataflow can be modeled using the algebra. Through an experimental evaluation using realdatasets and the execution of the dataflow with Chiron; an engine that supports our …,Big Data; 2013 IEEE International Conference on,2013,21
Similarity-based workflow clustering,Vıtor Silva; Fernando Chirigati; Kely Maia; Eduardo Ogasawara; D Oliveira; Vanessa Braganholo; Leonardo Murta; Marta Mattoso,ABSTRACT Scientists have been using scientific workflow management systems (SWfMS) tosupport scientific experiments. However; SWfMS expect a modeled workflow to berepresented on its workflow language to be executed. The scientist does not have anassistance or guidance to obtain a modeled workflow. Experiment lines; which are a novelapproach to deal with these limitations; allow for the abstract representation and systematiccomposition of experiments. Since there are many scientific workflows already modeled andsuccessfully executed; they can be used to leverage the construction of new abstractrepresentations. These previous experiments can be helpful by identifying scientificworkflow clusters that are generated according to similarity criteria. This paper proposesSimiFlow; which is an architecture for similarity-based comparison and clustering to build …,Journal of Computational Interdisciplinary Sciences,2011,21
Using ontologies to support deep water oil exploration scientific workflows,Daniel de Oliveira; Luiz Cunha; Luiz Tomaz; Vinicius Pereira; Marta Mattoso,Scientific experiments generate a large amount of data to be processed and analyzed. Asthe amount of data increases; the way engineers define their own experiments; analyze theoutput and share them is becoming complex to manage. Scientific Workflow ManagementSystems (WfMS) are being used to orchestrate a sequence of programs; services andresources; defined by scientific workflows. However; current WfMS are focused on theworkflow execution and present limitations on the semantic support to design theexperiment. These tools lack on semantic descriptions of available resources to designscientific workflow. This paper presents an ontology for deep water oil exploration workflow.This ontology has been used to present some semantic concepts to help defining a workflowto be further executed by a WfMS. We evaluate this semantic support on a real workflow …,Services-I; 2009 World Conference on,2009,18
Designing a parallel cloud based comparative genomics workflow to improve phylogenetic analyses,Kary ACS Ocaña; Daniel De Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Abstract Over the last years; comparative genomics analyses have become more compute-intensive due to the explosive number of available genome sequences. Comparativegenomics analysis is an important a prioristep for experiments in various bioinformaticsdomains. This analysis can be used to enhance the performance and quality of experimentsin areas such as evolution and phylogeny. A common phylogenetic analysis makesextensive use of Multiple Sequence Alignment (MSA) in the construction of phylogenetictrees; which are used to infer evolutionary relationships between homologous genes. Eachphylogenetic analysis aims at exploring several different MSA methods to verify whichexecution produces trees with the best quality. This phylogenetic exploration may run duringweeks; even when executed in High Performance Computing (HPC) environments …,Future Generation Computer Systems,2013,17
Towards a cost model for scheduling scientific workflows activities in cloud environments,Vitor Viana; Daniel De Oliveira; Marta Mattoso,Cloud computing has emerged as a new paradigm that enables scientists to benefit fromseveral distributed resources such as hardware and software. Clouds poses as anopportunity for scientists that need high performance computing infrastructure to executetheir scientific experiments. Most of the experiments modeled as scientific workflowsmanage the execution of several activities and work with large amounts of data. In this wayparallel techniques are often a key factor. Parallelizing a scientific workflow in the cloudenvironment is not trivial. One of the complex tasks is to define the number and types ofvirtual machines and to design the parallel execution strategy. Due to the number of optionsfor configuring an environment it is a hard task to do it manually and it may produce negativeimpact on performance. This paper initially proposes a cost model based on concepts of …,Services (SERVICES); 2011 IEEE World Congress on,2011,17
GExpLine: a tool for supporting experiment composition,Daniel de Oliveira; Eduardo Ogasawara; Fernando Seabra; Vítor Silva; Leonardo Murta; Marta Mattoso,Abstract Scientific experiments present several advantages when modeled at highabstraction levels; independent from Scientific Workflow Management System (SWfMS)specification languages. For example; the scientist can define the scientific hypothesis interms of algorithms and methods. Then; this high level experiment can be mapped intodifferent scientific workflow instances. These instances can be executed by a SWfMS andtake advantage of its provenance records. However; each workflow execution is oftentreated by the SWfMS as independent instances. There are no tools that allow modeling theconceptual experiment and linking it to the diverse workflow execution instances. This workpresents GExpLine; a tool for supporting experiment composition through provenance. In ananalogy to software development; it can be seen as a CASE tool while a SWfMS can be …,International Provenance and Annotation Workshop,2010,17
Dimensioning the virtual cluster for parallel scientific workflows in clouds,Daniel de Oliveira; Vitor Viana; Eduardo Ogasawara; Kary Ocaña; Marta Mattoso,Abstract Cloud computing has established itself as a solid computational model that allowsfor scientists to use a series of distributed virtual resources to execute a wide range ofscientific experiments. In several cases; there is a demand for high performance in executingthese experiments since many activities are data and computing intensive. Parallelismtechniques are a key issue in this experimentation process. There are approaches thatprovide parallelism capabilities for scientific workflows in clouds. However; most of them relyon the scientist to dimension the virtual cluster to be instantiated. Dimensioning the virtualcluster to execute the workflow in parallel may be a hard task to accomplish; ie it is hard todefine and adapt the optimal number of virtual machines to be used. Most systems followthis manual configuration of the scientist for the whole workflow execution; using adaptive …,Proceedings of the 4th ACM workshop on Scientific cloud computing,2013,16
Is Cloud Computing the Solution for Brazilian Researchers?*,Daniel De Oliveira; Eduardo Ogasawara,*,International Journal of Computer Applications,2012,16
Is cloud computing the solution for Brazilian researchers?,Daniel De Oliveira; Eduardo Ogasawara,*,International Journal of Computer Applications,2012,16
Enabling re-executions of parallel scientific workflows using runtime provenance data,Flávio Costa; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract Capturing provenance data in scientific workflows is a key issue since it allows forreproducibility and evaluation of results. Many of these workflows generate around 100;000tasks that execute in parallel in High Performance Computing environments; such as largeclusters and clouds. SciCumulus is a workflow engine for parallel execution in clouds.Activity failure is almost inevitable in clouds where virtual machine failures are a realityrather than a possibility. We present SciMultaneous; a service architecture that manages re-executions of failed scientific workflow tasks using runtime provenance. Experimental resultson clouds showed that SciMultaneous considerably increases the workflow completion andreduces the total execution time of the workflow (considering executions and re-executions)up to 11.5%; when compared to ad-hoc approaches.,International Provenance and Annotation Workshop,2012,15
Using domain-specific data to enhance scientific workflow steering queries,João Carlos de AR Gonçalves; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract In scientific workflows; provenance data helps scientists in understanding;evaluating and reproducing their results. Provenance data generated at runtime can alsosupport workflow steering mechanisms. Steering facilities for workflows is considered achallenge due to its dynamic demands during execution. To steer; for example; scientistsshould be able to suspend (or stop) a workflow execution when the approximate solutionmeets (or deviates) preset criteria. These criteria are commonly evaluated based onprovenance data (execution data) and domain-specific data. We claim that the final decisionon whether to interfere on the workflow execution may only become feasible whenworkflows can be steered by scientists using provenance data enriched with domain-specificdata. In this paper we propose an approach based on specialized software components …,International Provenance and Annotation Workshop,2012,14
Evaluating parameter sweep workflows in high performance computing,Fernando Chirigati; Vítor Silva; Eduardo Ogasawara; Daniel de Oliveira; Jonas Dias; Fábio Porto; Patrick Valduriez; Marta Mattoso,Abstract Scientific experiments based on computer simulations can be defined; executedand monitored using Scientific Workflow Management Systems (SWfMS). Several SWfMSare available; each with a different goal and a different engine. Due to the exploratoryanalysis; scientists need to run parameter sweep (PS) workflows; which are workflows thatare invoked repeatedly using different input data. These workflows generate a large amountof tasks that are submitted to High Performance Computing (HPC) environments. Differentexecution models for a workflow may have significant differences in performance in HPC.However; selecting the best execution model for a given workflow is difficult due to theexistence of many characteristics of the workflow that may affect the parallel execution. Wedeveloped a study to show performance impacts of using different execution models in …,Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies,2012,14
Capturing Distributed Provenance Metadata from Cloud-Based Scientific Workflows,Sergio Manuel Serra da Cruz; Carlos Eduardo Paulino; Daniel de Oliveira; Maria Luiza Machado Campos; Marta Mattoso,Abstract Workflows are scientific abstractions used in the modeling of scientific experiments.High performance computing environments such as clusters and grids are often required torun the experiments. Cloud computing is starting to be adopted by the scientific community.However; the cloud environment is still incipient in collecting and recording retrospectiveworkflow provenance. This paper presents an approach to capturing distributed provenancemetadata from cloud-based scientific workflows. The approach was implemented through anevolution of the Matrioshka architecture that was refactored for cloud environments.Preliminary results show that provenance metadata captured from the virtual componentsrunning at the cloud can aid scientists to manage and reproduce their large scale in silicoexperiments.,Journal of Information and Data Management,2011,14
Analyzing related raw data files through dataflows,Vítor Silva; Daniel Oliveira; Patrick Valduriez; Marta Mattoso,Summary Computer simulations may ingest and generate high numbers of raw data files.Most of these files follow a de facto standard format established by the application domain;for example; Flexible Image Transport System for astronomy. Although these formats aresupported by a variety of programming languages; libraries; and programs; analyzingthousands or millions of files requires developing specific programs. Database managementsystems (DBMS) are not suited for this; because they require loading the raw data andstructuring it; which becomes heavy at large scale. Systems like NoDB; RAW; and FastBithave been proposed to index and query raw data files without the overhead of using adatabase management system. However; these solutions are focused on analyzing onesingle large file instead of several related files. In this case; when related files are …,Concurrency and Computation: Practice and Experience,2016,12
Athena: text mining based discovery of scientific workflows in disperse repositories,Flavio Costa; Daniel de Oliveira; Eduardo Ogasawara; Alexandre AB Lima; Marta Mattoso,Abstract Scientific workflows are abstractions used to model and execute in silico scientificexperiments. They represent key resources for scientists and are enacted and managed byengines called Scientific Workflow Management Systems (SWfMS). Each SWfMS has aparticular workflow language. This heterogeneity of languages and formats poses ascomplex scenario for scientists to search or discover workflows in distributed repositories forreuse. The existing workflows in these repositories can be used to leverage the identificationand construction of families of workflows (clusters) that aim at a particular goal. However it ishard to compare the structure of these workflows since they are modeled in different formats.One alternative way is to compare workflow metadata such as natural language descriptions(usually found in workflow repositories) instead of comparing workflow structure. In this …,International Workshop on Resource Discovery,2010,12
MiningFlow: adding semantics to text mining workflows,D Oliveira; F Baião; M Mattoso,*,First Poster Session of the Brazilian Symposium on Databases,2007,12
Improving Many-Task computing in scientific workflows using P2P techniques,Jonas Dias; Eduardo Ogasawara; Daniel De Oliveira; Esther Pacitti; Marta Mattoso,Large-scale scientific experiments are usually supported by scientific workflows that maydemand high performance computing infrastructure. Within a given experiment; the sameworkflow may be explored with different sets of parameters. However; the parallelization ofthe workflow instances is hard to be accomplished mainly due to the heterogeneity of itsactivities. Many-Task computing paradigm seems to be a candidate approach to supportworkflow activity parallelism. However; scheduling a huge amount of workflow activities onlarge clusters may be susceptible to resource failures and overloading. In this paper; wepropose Heracles; an approach to apply consolidated P2P techniques to improve Many-Task computing of workflow activities on large clusters. We present a fault tolerancemechanism; a dynamic resource management and a hierarchical organization of …,Many-Task Computing on Grids and Supercomputers (MTAGS); 2010 IEEE Workshop on,2010,11
A conception process for abstract workflows: an example on deep water oil exploitation domain,W Martinho; E Ogasawara; D Oliveira; F Chirigati; I Santos; GHT Travassos; M Mattoso,*,5th IEEE International Conference on e-Science,2009,11
Multi-objective scheduling of Scientific Workflows in multisite clouds,Ji Liu; Esther Pacitti; Patrick Valduriez; Daniel de Oliveira; Marta Mattoso,Abstract Clouds appear as appropriate infrastructures for executing Scientific Workflows(SWfs). A cloud is typically made of several sites (or data centers); each with its ownresources and data. Thus; it becomes important to be able to execute some SWfs at morethan one cloud site because of the geographical distribution of data or available resourcesamong different cloud sites. Therefore; a major problem is how to execute a SWf in amultisite cloud; while reducing execution time and monetary costs. In this paper; we proposea general solution based on multi-objective scheduling in order to execute SWfs in amultisite cloud. The solution consists of a multi-objective cost model including execution timeand monetary costs; a Single Site Virtual Machine (VM) Provisioning approach (SSVP) andActGreedy; a multisite scheduling approach. We present an experimental evaluation …,Future Generation Computer Systems,2016,10
Many task computing for orthologous genes identification in protozoan genomes using Hydra,Fábio Coutinho; Eduardo Ogasawara; Daniel Oliveira; Vanessa Braganholo; Alexandre AB Lima; Alberto MR Dávila; Marta Mattoso,SUMMARY One of the main advantages of using a scientific workflow management system(SWfMS) is to orchestrate data flows among scientific activities and register provenance ofthe whole workflow execution. Nevertheless; the execution control of distributed activities inhigh performance computing environments by SWfMS presents challenges such as steeringcontrol and provenance gathering. Such challenges may become a complex task to beaccomplished in bioinformatics experiments; particularly in Many Task Computingscenarios. This paper presents a data parallelism solution for a bioinformatics experimentsupported by Hydra; a middleware that bridges SWfMS and high performance computing toenable workflow parallelization with provenance gathering. Hydra Many Task Computingparallelization strategies can be registered and reused. Using Hydra; provenance may …,Concurrency and Computation: Practice and Experience,2011,10
A P2P approach to many tasks computing for scientific workflows,Eduardo Ogasawara; Jonas Dias; Daniel Oliveira; Carla Rodrigues; Carlos Pivotto; Rafael Antas; Vanessa Braganholo; Patrick Valduriez; Marta Mattoso,Abstract Scientific Workflow Management Systems (SWfMS) are being used intensively tosupport large scale in silico experiments. In order to reduce execution time; current SWfMShave exploited workflow parallelization under the arising Many Tasks Computing (MTC)paradigm in homogeneous computing environments; such as multiprocessors; clusters andgrids with centralized control. Although successful; this solution no longer applies toheterogeneous computing environments; such as hybrid clouds; which may combine users'own computing resources with multiple edge clouds. A promising approach to address thischallenge is Peer-to-Peer (P2P) which relies on decentralized control to deal with scalabilityand dynamic behavior of resources. In this paper; we propose a new P2P approach to applyMTC in scientific workflows. Through the results of simulation experiments; we show that …,International Conference on High Performance Computing for Computational Science,2010,10
Scientific workflow management system applied to uncertainty quantification in large eddy simulation,Gabriel Guerra; Fernando Rochinha; Renato Elias; Alvaro Coutinho; Vanessa Braganholo; D de Oliveira; Eduardo Ogasawara; F Chirigati; Marta Mattoso,Abstract. Currently Large Eddy Simulation (LES) requires intensive computation and a lot ofdata management. Today this management is often carried out in a case by case basis andrequires great effort to track it. This is due to the large amount of data involved; thus makingthis process prone to errors. Moreover; there is a need to explore parameter variability (eg;eddy viscosities) for the same set of data. In this context; techniques and methodologies ofscientific workflows can improve the management of simulations. This variability can be putin the general context of Uncertainty Quantification (UQ); which provides a rationalperspective for analysts and decision makers. The objective of this work is to provide asystematic approach in:(i) modeling of LES numerical experiments;(ii) managing the UQanalysis (iii) running each variation in parallel under the control of the Scientific Workflow …,Congresso Ibero Americano de Métodos Computacionais em Engenharia,2009,10
Parallel computing in genomic research: advances and applications,Kary Ocaña; Daniel de Oliveira,Abstract Today's genomic experiments have to process the so-called “biological big data”that is now reaching the size of Terabytes and Petabytes. To process this huge amount ofdata; scientists may require weeks or months if they use their own workstations. Parallelismtechniques and high-performance computing (HPC) environments can be applied forreducing the total processing time and to ease the management; treatment; and analyses ofthis data. However; running bioinformatics experiments in HPC environments such asclouds; grids; clusters; and graphics processing unit requires the expertise from scientists tointegrate computational; biological; and mathematical techniques and technologies. Severalsolutions have already been proposed to allow scientists for processing their genomicexperiments using HPC capabilities and parallelism techniques. This article brings a …,Advances and Applications in Bioinformatics and Chemistry: AABC,2015,9
Exploring Large Scale Receptor-Ligand Pairs in Molecular Docking Workflows in HPC Clouds,Kary Ocaña; Silvia Benza; Daniel De Oliveira; Jonas Dias; Marta Mattoso,Computer-aided drug design techniques are important assets in pharmaceutical industrybecause of their support for research and development of new drugs. Molecular docking(MD) predicts specific compound's binding modes within the active site of target proteins.Since MD is a time-consuming process; existing approaches reduce the number of receptorsor ligands in docking by evaluating only small sets of compounds. This restriction in thesearch space reduces the chances to uniformly cover the diverse space of compounds andmisses opportunities to recognize whether new drugs can be identified. Another difficultywith large-scale is analyzing the results; eg browsing all directories manually to find whichpairs were docked successfully. To address these issues we explored the potential of dataprovenance analysis and parallel processing of SciCumulus; a cloud Scientific Workflow …,Parallel & Distributed Processing Symposium Workshops (IPDPSW); 2014 IEEE International,2014,9
Runtime dynamic structural changes of scientific workflows in clouds,Igor Dos Santos; Jonas Dias; Daniel De Oliveira; Eduardo Ogasawara; Kary Ocaña; Marta Mattoso,Abstract Existing Scientific Workflow Management Systems (ie SWfMS) effectively supportworkflows that do not need dynamic changes at runtime. SWfMS execute workflows byproviding process management; provenance data and distributed execution on clusters andclouds. However; the support for dynamic changes in workflows is still an open; yetimportant; problem. For example; when the program associated to an activity of the workflowis taking more time than expected to produce results or if the results do not comply withsome quality criteria; the scientist may want to try an alternative algorithm implementation.However; scientists may not want to re-execute the entire workflow for each change theymake in the workflow structure. Alternatively; changing the structure of the workflowdynamically (ie change the programs associated with workflow activities) can improve the …,Proceedings of the 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing,2013,9
Discovering drug targets for neglected diseases using a pharmacophylogenomic cloud workflow,Kary ACS Ocana; Daniel de Oliveira; Jonas Dias; Eduardo Ogasawara; Marta Mattoso,Illnesses caused by parasitic protozoan are a research priority. A representative group ofthese illnesses is the commonly known as Neglected Tropical Diseases (NTD). NTDspecially attack low socioeconomic population around the world and new anti-protozoaninhibitors are needed and several drug discovery projects focus on researching new drugtargets. Pharmacophylogenomics is a novel bioinformatics field that aims at reducing thetime and the financial cost of the drug discovery process. Pharmacophylogenomic analysesare applied mainly in the early stages of the research phase in drug discovery.Pharmacophylogenomic analysis executes several bioinformatics programs in a coherentflow to identify homologues sequences; construct phylogenetic trees and executeevolutionary and structural experiments. This way; it can be modeled as scientific …,E-Science (e-Science); 2012 IEEE 8th International Conference on,2012,9
Uma Abordagem de Apoio à Execução Paralela de Workflows Científicos em Nuvens de Computadores,Daniel Oliveira,TESE SUBMETIDA AO CORPO DOCENTE DO INSTITUTO ALBERTO LUIZ COIMBRA DEPÓS-GRADUAÇÃO E PESQUISA DE ENGENHARIA (COPPE) DA UNIVERSIDADEFEDERAL DO RIO DE JANEIRO COMO PARTE DOS REQUISITOS NECESSÁRIOS PARAA OBTENÇÃO DO GRAU DE DOUTOR EM CIÊNCIAS EM ENGENHARIA DE SISTEMAS ECOMPUTAÇÃO.,*,2012,9
Experiencing PROV-Wf for Provenance Interoperability in SWfMSs,Wellington Oliveira; Daniel de Oliveira; Vanessa Braganholo,Abstract Analyzing disperse and heterogeneous provenance data usually requires usinghigher-level tools which scientists need to learn. In our view; scientists should be able toanalyze provenance in the SWfMS of their choice. In this paper; we propose Géfyra; anarchitecture based on the PROV-Wf model; which provides a way to capture heterogeneousprovenance data from different SWfMSs into a single format. Géfyra exports and importsprovenance data to/from different SWfMSs; allowing scientists to use the system of theirchoice.,*,2014,8
SciLightning: a cloud provenance-based event notification for parallel workflows,Julliano Trindade Pintas; Daniel de Oliveira; Kary ACS Ocaña; Eduardo Ogasawara; Marta Mattoso,Abstract Conducting scientific experiments modeled as workflows is a challenging task dueto the complex management of several (often inter-related) computer-based simulations.Many of these scientific workflows are compute intensive and demand High PerformanceComputing environments to run; such as virtual parallel machines in a cloud computingenvironment. These workflows commonly present long-term" black-box" executions (ieseveral days or weeks); thus making it very difficult for scientists to monitor its executioncourse. We present a workflow event notification mechanism based on runtime monitoring ofprovenance data produced by parallel scientific workflow systems in clouds. Thismechanism queries provenance data generated at runtime for identifying preconfiguredevents and notifying scientists using technologies such as Android devices and message …,International Conference on Service-Oriented Computing,2013,8
Cloud-based phylogenomic inference of evolutionary relationships: a performance study,D Oliveira; KACS Ocaña; E Ogasawara; J Dias; J Goncalves; M Mattoso,*,Proceedings of the 2nd International Workshop on Cloud Computing and Scientific Applications (CCSA),2012,8
Debugging Scientific Workflows with Provenance: Achievements and Lessons Learned.,Daniel de Oliveira; Flavio Costa; Vítor Silva Sousa; Kary ACS Ocaña; Marta Mattoso,Abstract. 1Scientific Workflow Management Systems manage experiments in large-scaleand deliver provenance data. Provenance data represents the workflow execution behavior;allowing for tracing the data-flow generation. When provenance is extended withperformance execution data; it becomes an important asset to identify and analyze errorsthat occurred during the workflow execution (ie debugging). Debugging is essential forworkflows that execute in parallel in large-scale distributed environments since theincidence of errors in this type of execution is high and difficult to track. By debugging atruntime; scientists can identify errors and take the necessary actions; while the workflow isstill running. We present provenance based debugging; in real use cases; running inparallel; with virtual machines in clouds. In these experiences scientists use provenance …,SBBD,2014,7
Provenance traces from Chiron parallel workflow engine,Felipe Horta; Vítor Silva; Flavio Costa; Daniel de Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Abstract Scientific workflows are commonly used to model and execute large-scale scientificexperiments. They represent key resources for scientists and are managed by ScientificWorkflow Management Systems (SWfMS). The different languages used by SWfMS mayimpact in the way the workflow engine executes the workflow; sometimes limitingoptimization opportunities. To tackle this issue; we recently proposed a scientific workflowalgebra [1]. This algebra is inspired by database relational algebra and it enables automaticoptimization of scientific workflows to be executed in parallel in high performance computing(HPC) environments. This way; the experiments presented in this paper were executed inChiron; a parallel scientific workflow engine implemented to support the scientific workflowalgebra. Before executing the workflow; Chiron stores the prospective provenance [2] of …,Proceedings of the Joint EDBT/ICDT 2013 Workshops,2013,7
Handling Failures in Parallel Scientific Workflows Using Clouds,Flavio Costa; Daniel de Oliveira; Kary Ocana; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,Failures are common in High Performance Computing (HPC) environments and cansignificantly impact the performance of scientific workflows executing on top of these largescale computing environments. Computing clouds are being used as promising HPCenvironments. Although clouds offer several advantages such as elasticity and availability;failures are very frequent in this type of environment; where virtualization; instabilities andproviders' actions directly impact on workflow execution. In this way; activity failures arealmost inevitable in clouds where virtual machine failures are a reality rather than apossibility. In this paper we present a set of failure handling heuristics based on cloudcharacteristics; which are implemented within SciMultaneous; a service-orientedarchitecture that manages re-executions of failed scientific workflow activities using …,High Performance Computing; Networking; Storage and Analysis (SCC); 2012 SC Companion:,2012,7
Using Provenance to Visualize Data from Large-Scale Experiments,Felipe Horta; Jonas Dias; Kary ACS Ocana; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Large-scale scientific computations are often organized as a composition of manycomputational tasks linked through data flow. The data that flows along this manytaskcomputing often moves from a desktop to a high-performance environment and to avisualization environment. Keeping track of this data flow is a challenge to provenancesupport in high-performance Scientific Workflow Management Systems. After the completionof a computational scientific experiment; a scientist has to manually select and analyze itsstaged-out data; for instance; by checking inputs and outputs along computational tasks thatwere part of the experiment. In this paper; we present a provenance management systemthat describes the production and consumption relationships between data artifacts; such asfiles; and the computational tasks that compose the experiment. We propose a query …,High Performance Computing; Networking; Storage and Analysis (SCC); 2012 SC Companion:,2012,7
SimiFlow: Uma Arquitetura para Agrupamento de Workflows por Similaridade,Vítor Silva; Fernando Chirigati; Kely Maia; Eduardo Ogasawara; D Oliveira; Vanessa Braganholo; Leonardo Murta; Marta Mattoso,Abstract. Scientists have been using scientific workflows to support scientific experiments.However; the Scientific Workflow Management Systems present some limitation on workflowcomposition. Experiment Lines; which are a novel approach to deal with these limitations;allow the representation and systematic composition of the experiment. Nevertheless; thereare many scientific workflows already modeled that can leverage the construction ofexperiment lines via the identification of scientific workflows clusters that are createdaccording to similarity. This paper proposes SimiFlow; an architecture for similarity-basedcomparison and clustering to build experiment lines following a bottom-up approach.Resumo. Workflows científicos vêm sendo utilizados no apoio aos experimentos científicos.Workflows em um mesmo experimento normalmente apresentam pequenas variações …,IV e-Science,2010,7
Raw data queries during data-intensive parallel workflow execution,Vítor Silva; José Leite; José J Camata; Daniel de Oliveira; Alvaro LGA Coutinho; Patrick Valduriez; Marta Mattoso,Abstract Computer simulations consume and produce huge amounts of raw data filespresented in different formats; eg; HDF5 in computational fluid dynamics simulations. Usersoften need to analyze domain-specific data based on related data elements from multiplefiles during the execution of computer simulations. In a raw data analysis; one shouldidentify regions of interest in the data space and retrieve the content of specific related rawdata files. Existing solutions; such as FastBit and RAW; are limited to a single raw data fileanalysis and can only be used after the execution of computer simulations. ScientificWorkflow Management Systems (SWMS) can manage the dataflow of computer simulationsand register related raw data files at a provenance database. This paper aims to combinethe advantages of a dataflow-aware SWMS and the raw data file analysis techniques to …,Future Generation Computer Systems,2017,6
Analyzing Provenance Across Heterogeneous Provenance Graphs,Wellington Oliveira; Paolo Missier; Kary Ocaña; Daniel de Oliveira; Vanessa Braganholo,Abstract Provenance generated by different workflow systems is generally expressed usingdifferent formats. This is not an issue when scientists analyze provenance graphs inisolation; or when they use the same workflow system. However; when analyzingheterogeneous provenance graphs from multiple systems poses a challenge. To addressthis problem we adopt ProvONE as an integration model; and show how differentprovenance databases can be converted to a global ProvONE schema. Scientists can thenquery this integrated database; exploring and linking provenance across several differentworkflows that may represent different implementations of the same experiment. To illustratethe feasibility of our approach; we developed conceptual mappings between theprovenance databases of two workflow systems (e-Science Central and SciCumulus). We …,International Provenance and Annotation Workshop,2016,6
Handling flash-crowd events to improve the performance of web applications,Ubiratam de Paula Junior; Lúcia Drummond; Daniel de Oliveira; Yuri Frota; Valmir C Barbosa,Abstract Cloud computing can offer a set of computing resources according to users'demand. It is suitable to be used to handle flash-crowd events in Web applications due to itselasticity and on-demand characteristics. Thus; when Web applications need morecomputing or storage capacity; they just instantiate new resources. However; providers haveto estimate the amount of resources to instantiate to handle with the flash-crowd event. Thisestimation is far from trivial since each cloud environment provides several kinds ofheterogeneous resources; each one with its own characteristics such as bandwidth; CPU;memory and financial cost. In this paper; the Flash Crowd Handling Problem (FCHP) isprecisely defined and formulated as an integer programming problem. A new algorithm forhandling with a flash crowd named FCHP-ILS is also proposed. With FCHP-ILS the Web …,Proceedings of the 30th Annual ACM Symposium on Applied Computing,2015,6
Handling flash-crowd events to improve the performance of web applications,Ubiratam de Paula Junior; Lúcia Drummond; Daniel de Oliveira; Yuri Frota; Valmir C Barbosa,Abstract Cloud computing can offer a set of computing resources according to users'demand. It is suitable to be used to handle flash-crowd events in Web applications due to itselasticity and on-demand characteristics. Thus; when Web applications need morecomputing or storage capacity; they just instantiate new resources. However; providers haveto estimate the amount of resources to instantiate to handle with the flash-crowd event. Thisestimation is far from trivial since each cloud environment provides several kinds ofheterogeneous resources; each one with its own characteristics such as bandwidth; CPU;memory and financial cost. In this paper; the Flash Crowd Handling Problem (FCHP) isprecisely defined and formulated as an integer programming problem. A new algorithm forhandling with a flash crowd named FCHP-ILS is also proposed. With FCHP-ILS the Web …,Proceedings of the 30th Annual ACM Symposium on Applied Computing,2015,6
Evaluating grasp-based cloud dimensioning for comparative genomics: a practical approach,Rafaelli Coutinho; Lúcia Drummond; Yuri Frota; Daniel de Oliveira; Kary Ocana,Cloud computing establishes a new computing model where a wide range of computingresources are provided to several types of users. Especially for bioinformatics experimentsmodeled as scientific workflows; clouds provide several types of resources as virtualmachines (VM); storage; databases and computing power that can be combined forempowering the scientific workflow execution. These workflows usually require highperformance environments and parallelism techniques since their activities are data andcomputing intensive and can execute for a long time. There are then some ScientificWorkflow Management Systems (SWfMS) that already manage the parallel execution ofscientific workflows in clouds. Most of them instantiate a virtual cluster for the execution.However; they rely on the user to estimate the amount of VMs to be instantiated to create …,Cluster Computing (CLUSTER); 2014 IEEE International Conference on,2014,6
SciCumulus 2.0: Um Sistema de Gerência de Workflows Científicos para Nuvens Orientado a Fluxo de Dados,Vítor Silva; D OLIVEIRA; Marta Mattoso,Resumo. Ao contrário dos workflows de negócio; os workflows científicos são centrados nogrande fluxo de transformação de dados. Entretanto; as abordagens dos sistemas deworkflows em larga escala ainda são voltadas à gerência da execução paralela de tarefasao invés de gerenciar as relações entre os dados ao longo fluxo de geração dos dados doworkflow. Este artigo apresenta a execução do SciCumulus 2.0 e sua nova camada desubmissão de execução paralela que oferece diferentes níveis para a modelagem deworkflows; assim como a configuração do ambiente de paralelismo e consultas aos dadosde domínio e de proveniência em tempo de execução.,Sessão de Demos do XXIX Simpósio Brasileiro de Banco de Dados,2014,6
Migrating Scientific Experiments to the Cloud,Daniel de Oliveira; Fernanda Araújo Baião; Marta Mattoso,*,*,2011,6
Migrating Scientific Experiments to the Cloud,D Oliveira; F Baiao; M Mattoso,*,HPC in the Cloud,2011,6
SciMulator: Um Ambiente de Simulação de Workflows Científicos em Redes P2P,Jonas Dias; Carla Rodrigues; Eduardo Ogasawara; Daniel De Oliveira; Vanessa Braganholo; Esther Pacitti; Marta Mattoso,The growth of large-scale scientific experiments motivates the search for computingenvironments that support the parallelization of computing activities; particularly those thatcomplies to the Many Task Computing (MTC) paradigm. Peer-to-Peer (P2P) environmentscan meet this demand due to the easy access and distributed control. However; building areal P2P infrastructure to evaluate this solution is very costly. Within this context; we presentthe simulator SciMulator developed to evaluate P2P architectures. We present the modelingof the simulator and an initial assessment of its performance when using the SciMulearchitecture for submission of scientific workflows activities on P2P networks.,VI Workshop de Redes Dinâmicas e Sistemas Peer-to-Peer 2010,2010,6
An adaptive approach for workflow activity execution in clouds,D Oliveira; Eduardo Ogasawara; Fernanda Baião; Marta Mattoso,Abstract. Many scientific workflows in various domains of knowledge are data-intensive. Thistype of workflow requires high performance environments to achieve the results at anacceptable response time (crucial for many critical experiments). Cloud environmentsrepresent an opportunity to provide the necessary high performance infra-structure to runthese experiments. However; this opportunity also raises many challenges. It is hard todecide a priori which resources to use and how long they are needed since the resourcesare elastic and instantiated on demand. This paper proposes an adaptive mechanism thatallows workflow activities to execute according to the environmental conditions in a cloudenvironment (eg; workload and resource availability) with respect to given performanceobjectives such as total execution time. We have evaluated the adaptive mechanism in a …,International Workshop on Challenges in e-Science-SBAC; Petrópolis; RJ-Brazil,2010,6
How much domain data should be in provenance databases?,Daniel De Oliveira; Vítor Silva; Marta Mattoso,ABSTRACT Provenance databases are an important asset in data analytics of large-scalescientific data. The data derivation path allows for identifying parameters; files and domaindata values of interest. In scientific workflows; provenance data is automatically captured byworkflow systems. However; the power of provenance data analyses depends on theexpressiveness of domain-specific data along the provenance traces. While much has beendone through the W3C PROV initiative and its PROV-DM to represent generic provenancedata; representing domain-specific data in provenance traces has received little attention;yet it accounts for a large number of provenance analytical queries. Such queries are basedon selections on data values from input/output artifacts along workflow activities. There areseveral problems in modeling and capturing values from domain-specific attributes; some …,Proceeding of the 7th USENIX Workshop on the Theory and Practice of Provenance (TaPP 15),2015,5
How much domain data should be in provenance databases?,Daniel de Oliveira; Vítor Silva; Marta Mattoso,ABSTRACT Provenance databases are an important asset in data analytics of large-scalescientific data. The data derivation path allows for identifying parameters; files and domaindata values of interest. In scientific workflows; provenance data is automatically captured byworkflow systems. However; the power of provenance data analyses depends on theexpressiveness of domain-specific data along the provenance traces. While much has beendone through the W3C PROV initiative and its PROV-DM to represent generic provenancedata; representing domain-specific data in provenance traces has received little attention;yet it accounts for a large number of provenance analytical queries. Such queries are basedon selections on data values from input/output artifacts along workflow activities. There areseveral problems in modeling and capturing values from domain-specific attributes; some …,7th USENIX Workshop on the Theory and Practice of Provenance (TaPP 15),2015,5
Towards an Adaptive and Distributed Architecture for Managing Workflow Provenance Data,Flavio Costa; Daniel de Oliveira; Marta Mattoso,Workflow provenance data represents the workflow execution behavior; allowing for tracingthe generation of the scientific data-flow. Provenance is an important asset to analyze data;identify and handle errors that occurred during the workflow execution through runtimemonitoring. The workflow execution engine can also use provenance data to set the initialamount of resources and plan adaptive task scheduling. However; efficiently managingprovenance data from distributed workflow execution has several challenges. As the size ofworkflows increases (in terms of number of activity executions or volume of data to process);the amount of provenance data to be managed also grows; especially in fine grain. Thus;centralized approaches become unviable. In this work we propose an architecture thatcombines distributed workflow management techniques with distributed provenance data …,e-Science (e-Science); 2014 IEEE 10th International Conference on,2014,5
Prov-Vis: Large-Scale Scientific Data Visualization Using Provenance,Felipe Horta; Jonas Dias; Renato Elias; Daniel Oliveira; Alvaro Coutinho; Marta Mattoso,Abstract—Large-scale scientific computing often rely on in-tensive tasks chained through aworkflow. Scientists need to check the status of the execution at particular points; to discoverif anything odd has happened and take actions. To achieve that; they need to track partialresult files; which is usually complex and laborious. When using a scientific workflow system;provenance data keeps track of every step of the execution. If traversing provenance data isallowed at runtime; it is easier to monitor and analyze partial results. However; visualizationof partial results is necessary to be done in sync to the workflow provenance. Prov-Vis is ascientific data visualization tool for large-scale workflows that is based on runtimeprovenance queries to organize and aggregate data for visualization. Prov-Vis helpsscientists to follow the steps of the running workflow and visualize the produced partial …,Proceedings of the International Conference on High Performance Computing; Networking; Storage and Analysis,2013,5
An Architecture Coupled with a Provenance Model for Affective Simulations in Games based on Real World,Rainier Sales; Esteban Clua; Daniel de Oliveira; Aline Paes,Abstract—Usually; characters of games based on the real world are simplified stereotypes ofthe human they represent; as such games disregard psychological aspects of the humanmind. However; emotive elements based on humans may be essential to achieve a properfun factor to the game player. Moreover; by applying emotional aspects related to humanbeings; we are able to insert aspects of the reality from a human point of view; which is goingto yield an expected behavior of the characters similar to those made outside the game.Thus; in this paper we propose an architecture to modify the behavior of non-playercharacter (NPC); making them an affective agent. Personality traits; mood and emotionmodels based on well-established theories of psychology are associated to the NPC so thatthe behavior and general way of thinking of humans are simulated.,Proceedings of SBGames 2013,2013,5
Especificação Formal e Verificação de Workflows Científicos,E Silva; Eduardo Ogasawara; D Oliveira; M Benevides; M Mattoso,Abstract. Workflows are used in several domains for scientific purposes. In the last yearsthese workflows are becoming more complex and scientists need methods to verify itscorrectness. Most of the available systems assume that a workflow is correct if it respectscontrol and dependencies specified by the scientist. In addition; many scientific workflowsmust be completely reliable; that is why they must be correctly specified. This paperproposes an approach that supports workflow verification based on process algebraspecifications (CCS) and model checking tools. We have evaluated our approach using theGExpLine workflow tool. Resumo. Workflows são utilizados em diversos domínios compropósitos científicos. Nos últimos anos estes workflows tornaram-se mais complexos e oscientistas necessitam de métodos para verificar a sua correção. A maioria dos sistemas …,IV e-Science,2010,5
Parallel Execution of Workflows Driven by a Distributed Database Management System,Renan Souza; Vítor Silva; Daniel de Oliveira; Patrick Valduriez; Alexandre AB Lima; Marta Mattoso,ABSTRACT Scientific Workflow Management Systems (SWfMS) that execute large-scalesimulations need to manage many tasks computing in high performance environments. Withthe scale of tasks and processing cores to be managed; SWfMS require efficient distributeddata structures to manage data related to scheduling; data movement and provenance datagathering. Although related systems store these data in multiple specific files; some existingapproaches store them using a Database Management System (DBMS); which providespowerful analytical capabilities; including execution monitoring; anticipated result analyses;and user steering; when available at runtime. Despite these advantages; approaches relyingon a centralized DBMS introduce a point of contention; jeopardizing performance in large-scale executions. In this paper; we propose an architecture relying on a distributed DBMS …,*,2015,4
A non-intrusive approach for 2D platform game design analysis based on provenance data extracted from game streaming,Lidson Barbosa Jacob; Troy C Kohwalter; Alex FV Machado; Esteban WG Clua; Daniel de Oliveira,The usage of provenance data drastically increases the potential for game data mining sinceit is able to record causes; effects and relationships of events and objects during a gamesession. However; it commonly requires modifications in the game engine in order to collectsuch provenance data. The modifications in the game engine may be unviable incommercial (and not open source) systems. In this paper; we propose a novel and non-intrusive approach for collecting provenance data in digital games. Our proposal collectsprovenance data using image processing mechanisms and pre-defined image patterns; thusavoiding accessing and modifying the source code of the game. Using our approach; we areable to generate; analyze and visualize game design features based on the gameplay flowusing provenance data. Furthermore; we evaluated our proposal with a well known …,Computer Games and Digital Entertainment (SBGAMES); 2014 Brazilian Symposium on,2014,4
Exploratory Analysis of Raw Data Files through Dataflows,Vitor Silva; Daniel de Oliveira; Marta Mattoso,Scientific applications generate raw data files in very large scale. Most of these files follow astandard format established by the domain area application; like HDF5; Net CDF and FITS.These formats are supported by a variety of programming languages; libraries andprograms. Since they are in large scale; analyzing these files require writing a specificprogram. Generic data analysis systems like database management systems (DBMS) arenot suited because of data loading and data transformation in large scale. Recently therehave been several proposals for indexing and querying raw data files without the overheadof using a DBMS; such as noDB; RAW and Fast Bit. Their goal is to offer query support to theraw data file after a scientific program has generated it. However; these solutions arefocused on the analysis of one single large file. When a large number of files are all …,Computer Architecture and High Performance Computing Workshop (SBAC-PADW); 2014 International Symposium on,2014,4
Captura e Consulta a Dados de Proveniência Retrospectiva Implícita Intra-Atividade,Wellington Oliveira; Vitor C Neves; Kary Ocaña; Leonardo Murta; Daniel de Oliveira; Vanessa Braganholo,Abstract. Executing scientific experiments in large scale generates huge amounts of datathat should be captured for future query. However; most of the available tools collect onlypart of the provenance (ie; only that which is explicitly defined in the workflow specification);leading to loss of important information to the analysis of the experiment. This work presentsan approach for collecting; storing; and querying intra-activity provenance obtained bymonitoring the directories and files manipulated by activities of the workflow. Therefore;information not referenced in the workflow specification are captured and related withpreviously specified provenance. Resumo. A execução de experimentos científicos em largaescala gera uma grande quantidade de dados que devem ser capturados para consultafutura. Porém; a maioria das ferramentas disponíveis coletam apenas parte da …,*,2014,4
Uso de SGBDs NoSQL na Gerência da Proveniência Distribuída em Workflows Científicos,Guilherme R Ferreira; Carlos Filipe Jr; Daniel de Oliveira,Resumo. 1 Um fator fundamental na gerência de experimentos modelados como workflowscientíficos são seus dados de proveniência. Esses dados basicamente são usados paragarantir a reprodutibilidade; porém nos últimos anos eles também vêm sendo usados paratarefas de monitoramento e escalonamento de atividades. Como essas tarefas demandamconsultas em tempo real; conforme a quantidade de dados de proveniência aumenta;mecanismos eficazes para armazenamento e consulta se fazem necessários. Uma dasopções mais comuns é utilizar os SGBDs relacionais para gerenciar a proveniência; dada atradição da tecnologia. Porém; novas tecnologias como os SGBDs NoSQL tem ganhadobastante atenção nos últimos anos e podem ser de grande valia nesse cenário;principalmente em ambientes distribuídos onde escalabilidade é essencial. Este artigo …,*,2014,4
Experiences in using provenance to optimize the parallel execution of scientific workflows steered by users,Marta Mattoso; Jonas Dias; Flavio Costa; Daniel de Oliveira; Eduardo Ogasawara,Abstract. The main advantages from using Scientific Workflow Management Systems tomanage a large-scale scientific experiment are their automatic parallel execution and theimprovement of result analysis through provenance data. Provenance data becomesespecially useful for scientists when it is clearly associated to their domain data. In ourexperience; provenance data also reveals important optimizations opportunities in parallelexecution and allows for user steering of workflows at run-time. The algebraic parallelexecution engine is fine tuned by provenance statistics and users have exploredprovenance through steering support to visualize partial results from computational fluiddynamics simulations; to improve iterative uncertainty quantification applications ingeophysics and to evaluate parameter setting and algorithms in several bioinformatics …,Workshop of Provenance Analytics,2014,4
Construção de ambiente para desenvolvimento de jogos educacionais baseados em interface de gestos,João Roberto de T Quadros; Rafael Castaneda; Myrna Amorim; Guilherme Herzog; Lucas Carneiro; Kaique Menezes; Matheus Pinheiro; Daniel de Oliveira; Eduardo Ogasawara,Resumo Este artigo apresenta um projeto para construção de um ambiente deprogramação de jogos educacionais baseado em interface de gestos; realizado dentro deuma escola federal de informática. Este artigo enfatiza a parte do projeto voltada paramontagem do ambiente por meio de equipamentos para captura de movimentos e de umaplataforma de programação didática para construção de aplicativos. O artigo tambémapresenta o desenvolvimento de protótipos visando o teste desse ambiente de modo aprepará-lo para que sejam desenvolvidos aplicativos mais complexos voltados para áreaeducacional.,Revista Brasileira de Computação Aplicada,2013,4
Performance Analysis of Data Filtering in Scientific Workflows,João Gonçalves; Daniel de Oliveira; Kary Ocaña; Eduardo Ogasawara; Jonas Dias; Marta Mattoso,*,Journal of Information and Data Management,2013,4
Oh Gosh!! Why is this game so hard? Identifying cycle patterns in 2D platform games using provenance data,Lidson Jacob; Esteban Clua; Daniel de Oliveira,Abstract There are many elements that make a game interesting for the player. The gameplay is a key element. It is the way in which players interact with a game. In general; thisinteraction starts easier and becomes more challenge along the game session. Commonlygame developers try to make balanced games; but due to the big variety of age; genre andexpertise among players; this is not a trivial task and sometimes the game may become toodifficult for some users; thus becoming boring. In this article we focus on identifying oneaspect in special: the cycles in a game session; which represent a sequence of actions thatare played repeatedly by the player in certain parts of the game. These cycles tend to makethe game tedious. This way; it is a top priority for game designers to identify cycles. In thisarticle we propose an approach identifies cycles in games using sequential pattern …,Entertainment Computing,2017,3
Monitoramento de Desempenho usando Dados de Proveniência e de Domínio durante a Execução de Aplicações Científicas,Renan Souza; Vítor Silva; Leonardo Neves; Daniel de Oliveira; Marta Mattoso,Resumo. Simulações computacionais; em geral; são compostas pelo encadeamento deaplicações científicas e executadas em ambientes de processamento de alto desempenho.Tais execuções comumente apresentam gargalos associados ao fluxo de dados entre asaplicações. Diversas ferramentas de perfilagem de código têm apoiado a análise de dadosde desempenho; como a Tuning and Analysis Utilities (TAU). Entretanto; essas ferramentasnão favorecem as análises do fluxo de dados. Essas análises podem ser realizadas com acaptura de dados de proveniência enriquecidos com dados de domínio extraídos ao longoda execução das simulações. Neste artigo; propomos o monitoramento de dados dedesempenho por meio de consultas a uma base de dados de proveniência que integradados sobre a execução; o fluxo de dados das simulações e os dados de domínio …,XIV Workshop em Desempenho de Sistemas Computacionais e de Comunicação,2015,3
An Artificial Emotional Agent-Based Architecture for Games Simulation,Rainier Sales; Esteban Clua; Daniel de Oliveira; Aline Paes,Abstract Emotions are fundamental in in any person's life; and would be no different ingames. While games are based at the gameplay for achieving the necessary fun factor;emotive elements are becoming common and in some cases even necessary. In somecomputing entertainment based systems; including and bringing emotional aspects from thereal world may aspects of reality; turning the virtual characters more believable. This posterproposes a novel architecture for supporting Non-player character (NPC) behaviormodeling; enhancing emotive agents. Personality; mood and emotive features aredetermined to the agents; based on well-established models developed in psychology. Wevalidate our system in simulation games.,International Conference on Entertainment Computing,2013,3
Ontology-based Semi-automatic Workflow Composition,Daniel de Oliveira; Eduardo Ogasawara; Jonas Dias; Fernanda Baião; Marta Mattoso,*,Journal of Information and Data Management,2012,3
Adding Ontologies to Scientific Workflow Composition.,Daniel de Oliveira; Eduardo S Ogasawara; Fernanda Araujo Baião; Marta Mattoso,Abstract. Scientific workflows are being used as an abstraction for the composition of largescale scientific experiments. As scientific workflows become more complex; theseabstractions isolate scientists from infrastructure issues. Although representing a workflow inan abstract level is a first step; there are many open issues; such as the ones related tosemantics. Adding semantics to abstract workflows enables the explicit representation ofwhich activities can be linked to each other; or which activities are equivalent to each other.However; representing an abstract workflow with semantics is an open problem. Existingapproaches address either the representation of abstract workflows or the use of domainontologies to add semantics to activities; but not both. In the latter case; these approachesfocus only on adding semantics to executable workflows; instead of working in different …,SBBD (Short Papers),2011,3
Uma avaliação da Distribuição de Atividades Estática e Dinâmica em Ambientes Paralelos usando o Hydra,Vítor Silva; Fernando Chirigati; Eduardo Ogasawara; Jonas Dias; D Oliveira; Fábio Porto; Patrick Valduriez; Marta Mattoso,Abstract. Scientific Workflows are used as a basic tool to design and execute scientificexperiments on different computational environments. These workflows can becomputational and data intensive; requiring high performance computing. The way in whichworkflow activities are parallelized and distributed over these environments affects theoverall performance of the workflow. This work evaluates two different strategies for activitydistribution (static and dynamic) using Hydra middleware integrated with VisTrails. Ourexperiments show that using the right strategy decreases elapsed time for activitydistribution in 30%. Resumo. Workflows Científicos são usados como uma abstração básicapara estruturação e execução de experimentos científicos em diferentes ambientescomputacionais. Estes workflows podem ser intensivos tanto computacionalmente …,V e-Science,2011,3
Captura de metadados de proveniencia para workflows científicos em nuvens computacionais,Carlos Paulino; D Oliveira; S Cruz; Maria Luiza Machado Campos; Marta Mattoso,Abstract. Workflows are scientific abstractions used in the modeling of scientific experiments.High performance capabilities such as clusters and grids are often required to run theexperiments. Cloud computing is starting to be adopted by the scientific community.However; the cloud environment is still incipient in collecting and recording workflowprovenance. This paper presents an approach to support collecting metadata provenance ofscientific experiments; based on an evolution of the Matrioshka architecture for the cloudenvironment. Preliminary results show that provenance metadata captured from the virtualcomponents of the cloud can aid scientists to manage and reproduce their in silicoexperiments. Resumo. Workflows científicos são abstrações utilizadas na modelagem deexperimentos científicos. Eles muitas vezes demandam recursos de alto desempenho …,Anais Do XXV Simpósio Brasileiro de Banco de Dados,2010,3
Using Ontologies to Provide Different Levels of Abstraction in Scientific Workflows,D Oliveira; Eduardo Ogasawara; F Baiao; Marta Mattoso,Abstract: Scientific experiments are usually modeled as scientific workflows; and may beenacted using Scientific Workflow Management Systems (SWfMS). There is a huge variety ofSWfMS available; such as Taverna and VisTrails; which enables the specification andexecution of a chain of activities usually represented by programs or services. However;while using SWfMS the workflow has to be designed; revised and tuned on top of anexecution format. Such format has many data conversion components and other auxiliarytools that can pollute the workflow definition. Working on a higher level of representation letsthe scientist closer to the experiment definition rather than implementation details. There is alack of semantic help for workflow analysis; ie to query on algorithms in a group ofexecutable scientific workflows; a scientist should know a priori which program is related …,5th IEEE International Conference on e-Science. Oxford; United Kingdom,2009,3
Uma Abordagem Semântica para Linhas de Experimentos Científicos Usando Ontologias,Daniel de Oliveira; Eduardo Ogasawara; Fernando Chirigati; Vítor Sousa; Leonardo Murta; Cláudia Werner; Marta Mattoso,Abstract. Scientific workflows have been used as an abstraction to compose scientificexperiments. However; the composition of these workflows is a complex task. Currently thereis no guidance or process to follow to reach a workflow specification. As workflows becomemore complex; composition needs abstraction and semantic support. Experiment lines arean innovative and promising solution to model and reuse scientific workflows in differentlevels of abstraction. This paper proposes an ontology coupled to experiment lines toprovide semantics and flexibility. Resumo. Workflows científicos vêm sendo usados comouma abstração para compor experimentos científicos. Entretanto; a composição destesworkflows é uma tarefa complexa. Atualmente não há um guia ou processo a ser seguidopara alcançar uma especificação de um workflow. Conforme os workflows se tornam …,*,2009,3
Paralelização de Tarefas de Mineração de Dados Utilizando Workflows Científicos,Carlos Eduardo Barbosa; Eduardo Ogasawara; Daniel de Oliveira; Marta Mattoso,Abstract. Classical data mining (DM) process is usually composed by a chain of programsand data that are modeled by a specialist. This chain of programs may be modeled as aworkflow and takes advantage of scientific workflow management systems (SWfMS). Thispaper proposes an approach for parallelizing tasks within a data mining workflow.Experimental results using timeseries forecast with neural networks reinforce theperformance gains and additional benefits provided by SWfMS; such as provenancerecording of the workflow.. Resumo. Um processo de mineração de dados (MD) clássicopode ser visto como o encadeamento de programas e dados modelados por umespecialista. Esta cadeia pode ser modelada como um workflow e usufruir das vantagensde sistemas de gerência de workflows científicos (SGWfC). Este artigo prop  e uma …,V Workshop em Algoritmos e Aplicações de Mineração de Dados,2009,3
MF-Ontology; uma ontologia para o processo de mineração de textos,Daniel de Oliveira; Fernanda Baião; Marta Mattoso,Este artigo apresenta uma ontologia voltada para o processo de mineração de textos(também chamado de Knowledge Discovery on Texts). Esta ontologia foi desenvolvida eposteriormente acoplada a um sistema de gerência de workflows científicos (SGWf) a fim decriar um vocabulário comum entre os usuários do SGWf e incorporar mais semântica àsetapas de definição e seleção dos serviços computacionais do workflow. A MF-Ontologyestende uma ontologia de mineração de dados; incluindo conceitos específicos para amineração de textos. O artigo apresenta a forma como a MF-Ontology foi validada em duasfases: a primeira fase concentrou-se nos aspectos estruturais independentes de domínio;utilizando a abordagem OntoClean; e a segunda fase concentrou-se nos aspectosespecíficos do domínio; através de entrevistas com especialistas.,*,2008,3
MF-Ontology; uma ontologia para o processo de mineração de textos,Daniel de Oliveira; Fernanda Baião; Marta Mattoso,Este artigo apresenta uma ontologia voltada para o processo de mineração de textos(também chamado de Knowledge Discovery on Texts). Esta ontologia foi desenvolvida eposteriormente acoplada a um sistema de gerência de workflows científicos (SGWf) a fim decriar um vocabulário comum entre os usuários do SGWf e incorporar mais semântica àsetapas de definição e seleção dos serviços computacionais do workflow. A MF-Ontologyestende uma ontologia de mineração de dados; incluindo conceitos específicos para amineração de textos. O artigo apresenta a forma como a MF-Ontology foi validada em duasfases: a primeira fase concentrou-se nos aspectos estruturais independentes de domínio;utilizando a abordagem OntoClean; e a segunda fase concentrou-se nos aspectosespecíficos do domínio; através de entrevistas com especialistas.,Seminário de Pesquisa em Ontologia no Brasil,2008,3
Deriving scientific workflows from algebraic experiment lines: A practical approach,Anderson Marinho; Daniel de Oliveira; Eduardo Ogasawara; Vitor Silva; Kary Ocaña; Leonardo Murta; Vanessa Braganholo; Marta Mattoso,Abstract The exploratory nature of a scientific computational experiment involves executingvariations of the same workflow with different approaches; programs; and parameters.However; current approaches do not systematize the derivation process from the experimentdefinition to the concrete workflows and do not track the experiment provenance down to theworkflow executions. Therefore; the composition; execution; and analysis for the entireexperiment become a complex task. To address this issue; we propose the AlgebraicExperiment Line (AEL). AEL uses a data-centric workflow algebra; which enriches theexperiment representation by introducing a uniform data model and its correspondingoperators. This representation and the AEL provenance model map concepts from theworkflow execution data to the AEL derived workflows with their corresponding …,Future Generation Computer Systems,2017,2
In Situ Data Steering on Sedimentation Simulation with Provenance Data,Vítor Silva; José Camata; Daniel De Oliveira; Alvaro Coutinho; Patrick Valduriez; Marta Mattoso,Parallel adaptive mesh refinement and coarsening (AMR) are optimal strategies for tacklinglarge-scale simulations. libMesh is an open-source finite-element library that supportsparallel AMR and is used in multiphysics applications. In complex simulation runs; usershave to track quantities of interest (residuals; errors estimates; etc.) to control as much aspossible the execution. However; this tracking is typically done only after the simulationends. This paper presents DfAnalyzer; a solution based on provenance data to extract andrelate strategic simulation data for online queries. We integrate DfAnalyzer to libMesh andParaView Catalyst; so that queries on quantities of interest are enhanced by in situvisualization.,SC: High Performance Computing; Networking; Storage and Analysis,2016,2
Integrating Domain-data Steering with Code-profiling Tools to Debug Data-intensive Workflows,Vítor Silva; Leonardo Neves; Renan Souza; Alvaro Coutinho; Daniel de Oliveira; Marta Mattoso,ABSTRACT Computer simulations may be composed of scientific programs chained in acoherent flow and executed in High Performance Computing environments. Theseexecutions may present anomalies associated to the data that flows in parallel amongprograms. Several parallel code-profiling tools already support performance analysis; suchas Tuning and Analysis Utilities (TAU) or provide fine-grained performance statistics such asthe System Activity Report (SAR). However; these tools do not associate their results to theircorresponding dataflows. Such analysis is fundamental to trace back the data origins of anerror. In this paper; we propose to couple a workflow monitoring data approach to parallelcode-profiling tools for workflow executions. The goal is to profile and debug parallelworkflow executions by querying a database that is able to integrate performance …,*,2016,2
Data Analytics in Bioinformatics: Data Science in Practice for Genomics Analysis Workflows,Kary ACS Ocaña; Vítor Silva; Daniel De Oliveira; Marta Mattoso,Workflow systems manage large-scale experiments and deliver a large volume ofprovenance data traces. The provenance repository of these systems contains informationabout the workflow execution; which allows for tracking and analyzing data transformations.However; provenance data may still be considered a black-box; when it comes to analyzethe contents of resulting data files. Current solutions are focused on data transformation atcoarse grain; they point to input and output files; but do not allow for exploring domain-specific data. Data analytics is essential for managing large-scale workflows executed inparallel; especially when tracking anomalous executions. In this paper; we present a dataanalytics approach; which is based on the use of provenance data enriched with domain-specific data coupled to a data mining tool. A real bioinformatics workflow was modeled …,e-Science (e-Science); 2015 IEEE 11th International Conference on,2015,2
A Forecasting Method for Fertilizers Consumption in Brazil,Eduardo Ogasawara; Daniel de Oliveira; Fabio Paschoal Junior; Rafael Castaneda; Myrna Amorim; Renato Mauro; Jorge Soares; João Quadros; Eduardo Bezerra,Abstract Tracking information about fertilizers consumption in the world is very importantsince they are used to produce agriculture commodities. Brazil consumes a large amount offertilizers due to its large-scale agriculture fields. Most of these fertilizers are currentlyimported. The analysis of consumption of major fertilizers; such as Nitrogen-Phosphorus-Potassium (NPK); Sulfur; Phosphate Rock; Potash; and Nitrogen become critical for long-term government decisions. In this paper we present a method for fertilizers consumptionforecasting based on both Autoregressive Integrated Moving Average (ARIMA) and logisticfunction models. Our method was used to forecast fertilizers consumption in Brazil for thenext 20 years considering different economic growth for the entire country.,International Journal of Agricultural and Environmental Information Systems (IJAEIS),2013,2
Monitoramento em Tempo Real de Workflows Científicos Executados em Paralelo em Ambientes Distribuídos,Julliano Pintas; D Oliveira; Kary Ocaña; Jonas Dias; Marta Mattoso,DISSERTAÇÃO SUBMETIDA AO CORPO DOCENTE DO INSTITUTO ALBERTO LUIZCOIMBRA DE PÓS-GRADUAÇÃO E PESQUISA DE ENGENHARIA (COPPE) DAUNIVERSIDADE FEDERAL DO RIO DE JANEIRO COMO PARTE DOS REQUISITOSNECESSÁRIOS PARA A OBTENÇÃO DO GRAU DE MESTRE EM CIÊNCIAS EMENGENHARIA DE SISTEMAS E COMPUTAÇÃO.,VI e-Science workshop,2012,2
Reprodução de Experimentos Científicos Usando Nuvens.,Ary Henrique M de Oliveira; Murilo de Souza Martins; Igor Modesto; Daniel de Oliveira; Marta Mattoso,Resumo. Workflows científicos são utilizados para modelar experimentos computacionais.Os resultados desses experimentos são publicados e compartilhados na forma de artigospublicados em veículos científicos. Entretanto; para que tais resultados sejamcientificamente válidos eles devem ser passíveis de reprodução. Pesquisadores da área dee-Science têm a necessidade de compartilhar os artefatos utilizados para a geração dosresultados; dentre eles; os dados de entrada do workflow e os parâmetros utilizados noexperimento. Entretanto; reproduzir um experimento baseado nestes artefatos não é umatarefa trivial. Apesar de o workflow especificar o protocolo de execução; com dados eparâmetros de entrada disponíveis; nem sempre o ambiente de execução está acessível.Programas que foram originalmente utilizados podem estar obsoletos; versões de …,SBBD (Short Papers),2012,2
Managing Provenance of Implicit Data Flows in Scientific Experiments,Vitor C Neves; Daniel De Oliveira; Kary ACS Ocaña; Vanessa Braganholo; Leonardo Murta,Abstract Scientific experiments modeled as scientific workflows may create; change; oraccess data products not explicitly referenced in the workflow specification; leading toimplicit data flows. The lack of knowledge about implicit data flows makes the experimentshard to understand and reproduce. In this article; we present ProvMonitor; an approach thatidentifies the creation; change; or access to data products even within implicit data flows.ProvMonitor links this information with the workflow activity that generated it; allowing forscientists to compare data products within and throughout trials of the same workflow;identifying side effects on data evolution caused by implicit data flows. We evaluatedProvMonitor and observed that it could answer queries for scenarios that demand specificknowledge related to implicit provenance.,ACM Transactions on Internet Technology (TOIT),2017,1
Scientific Workflow Execution with Multiple Objectives in Multisite Clouds,Ji Liu; Esther Pacitti; Patrick Valduriez; Daniel De Oliveira; Marta Mattoso,In this short paper (see [4] for the extended version); we propose a general solution basedon multi-objective scheduling to execute SWfs in a multisite cloud with the following maincontributions: the design of a multi-objective cost model; SSVP VM provisioning approach;ActGreedy scheduling algorithm and an extensive experimental evaluation.,BDA: Bases de Données Avancées,2016,1
SiAPP: Um Sistema para Análise de Ocorrências de Crimes Baseado em Aprendizado Lógico-Relacional,Vítor Lourenço; Paulo Mann; Aline Paes; Daniel de Oliveira,RESUMO O crescente aumento da criminalidade em cidades brasileiras é um temarecorrente tanto nos veículos de comunicação como nas pautas das autoridadesgovernamentais. Para combater efetivamente a criminalidade é necessário que recursoshumanos e infraestrutura sejam cuidadosamente aplicados; de forma a não apenas punirquem cometeu o crime; mas preferencialmente prever e evitar que o mesmo aconteça.Dada a dificuldade de coletar um grande volume de informações oficiais relacionadas acrimes em todas as regiões de um município; uma tendência é que os próprios cidadãosatuem como fonte de dados; a partir de sistemas colaborativos baseados na Web.Entretanto; tal fonte de dados pode se tornar muito complexa e vasta; dificultando a análisemanual de padrões de ocorrências de crimes; de forma a evitar que eles aconteçam …,*,2016,1
A Dynamic Cloud Dimensioning Approach for Parallel Scientific Workflows: a Case Study in the Comparative Genomics Domain,Rafaelli Coutinho; Yuri Frota; Kary Ocaña; Daniel de Oliveira; Lúcia MA Drummond,Abstract Usually; scientists need to execute experiments that demand high performancecomputing environments and parallel techniques. This is the scenario found in manybioinformatics experiments modeled as scientific workflows; such as phylogenetic andphylogenomic analyses. To execute these experiments; scientists have adopted virtualmachines (VMs) instantiated in clouds. Estimating the number of VMs to instantiate is acrucial task to avoid negative impacts on the execution performance and on the financialcosts with under or overestimations. Previously; the necessary number of VMs to executebioinformatics workflows have been estimated by a GRASP heuristic and have beencoupled to a Cloud-based Parallel Scientific Workflow Management System. Although thiswork was a step forward; this approach only provided a static dimensioning. If the …,Journal of Grid Computing,2016,1
Detecting and Handling Flash-Crowd Events on Cloud Environments,Ubiratam de Paula; Daniel de Oliveira; Yuri Frota; Valmir C Barbosa; Lúcia Drummond,Abstract: Cloud computing is a highly scalable computing paradigm where resources aredelivered to users on demand via Internet. There are several areas that can benefit fromcloud computing and one in special is gaining much attention: the flash-crowd handling.Flash-crowd events happen when servers are unable to handle the volume of requests for aspecific content (or a set of contents) that actually reach it; thus causing some requests to bedenied. For the handling of flash-crowd events in Web applications; clouds can offer elasticcomputing and storage capacity during these events in order to process all requests.However; it is important that flash-crowd events are quickly detected and the amount ofresources to be instantiated during flash crowds is correctly estimated. In this paper; a newmechanism for detection of flash crowds based on concepts of entropy and total …,arXiv preprint arXiv:1510.03913,2015,1
Aplicação de Árvores de Decisão para Recomendação de Parâmetros em Workflows Científicos,Renan Camara; Aline Paes; Daniel de Oliveira,*,IX BRESCI - Brazilian e-Science Workshop,2015,1
Exploiting the Parallel Execution of Homology Workflow Alternatives in HPC Compute Clouds,Kary ACS Ocaña; Daniel de Oliveira; Vítor Silva; Silvia Benza; Marta Mattoso,Abstract Homology modeling (HM) plays an important role in drug discovery. HM analysisaims at predicting a 3D model from a biological sequence in order to discover new drugs.There are several problems in executing an HM analysis in large-scale; such as multiplesoftware to be evaluated; the management of the parallel execution; and results analysis; egbrowsing manually all results to find which structure was derived from which program withgood quality. Scientific Workflow Management System (SWfMS) with parallelism andprovenance support can aid the large-scale HM executions by addressing the resultanalysis. However; before submitting the HM workflow for execution; it has to be specifiedalong with its several alternatives (also called variants); as considered in this paper.Managing HM workflow variations is a complex task to be accomplished even with the …,*,2015,1
Um Mapeamento Sistemático Sobre o Uso de Metodologias Ágeis no Processo de Experimentação Científica,Augusto Consulmagnos Romeiro; Daniel de Oliveira,Resumo. Muitos experimentos científicos são baseados em simulações computacionais.Assim; os cientistas de diversas áreas; como a biologia e a química; desenvolvem softwarescomo parte fundamental de sua pesquisa; seja diretamente escrevendo programas oudesenvolvendo novos workflows científicos. Assim como na área comercial; todo odesenvolvimento em projetos científicos deve ser estruturado; documentado e realizado deforma a reduzir o tempo de desenvolvimento e os custos envolvidos. No processo deexperimentação científica os requisitos são instáveis e podem mudar constantementedurante a execução do projeto; por isso; as metodologias ágeis já consagradas para odesenvolvimento de software comercial se mostram interessantes. O propósito desse artigoé mapear sistematicamente as abordagens existentes que aplicam metodologias ágeis …,*,2015,1
Aplicação de Árvores de Decisão para Recomendação de Parâmetros em Workflows Científicos,Renan Vinagre Câmara; Aline Paes; Daniel de Oliveira,Resumo. Diversos experimentos de larga escala modelados como workflows científicospodem executar em paralelo por diversos dias ou semanas em ambientes de altodesempenho. O tempo de execução é determinado por fatores como o volume de dados deentrada; a quantidade de parâmetros explorados; etc. Assim; se torna importante para ocientista que as execuções de workflows que não produzem resultados satisfatórios ou queproduzem resultados com erros sejam reduzidas ao máximo. Estimar quais execuções irãofalhar (ou não) é um problema importante; porém em aberto. De forma a reduzir esseproblema; propomos um mecanismo de recomendação de parâmetros para workflowsbaseado em algoritmos de mineração de dados para que o cientista possa configurar seuworkflow da melhor forma possível (eg; para evitar erros) antes da execução …,*,2015,1
Towards Supporting Provenance Gathering and Querying in Different Database Approaches,Flavio Costa; Vítor Silva; Daniel de Oliveira; Kary ACS Ocaña; Marta Mattoso,Abstract The amount of provenance data gathered from Scientific Workflow ManagementSystems (SWfMS) and stored in databases has been growing considerably. Some difficultiesare related to representation; access and query provenance databases. Despite the effort ofPROV W3C group; data analyses may require different strategies of query specificationbecause of the volume of data to be analyzed and the nature of queries. Another importantpoint is the new approaches to store and retrieve provenance; some technologies are moreappropriate than others. However; when applications are tightly coupled to specifictechnologies; it is difficult to take advantage of innovation. Based on these issues; we havebuilt WfP-API; an API to store and perform queries in different provenance databases.,International Provenance and Annotation Workshop,2014,1
Evaluation between humans and affective NPC in digital gaming scenario,Rainier Sales; Esteban Clua; Daniel de Oliveira; Aline Paes; Luiz Chaimowicz; Maria Augusta SN Nunes,The scenario of affective computing has expanded into different areas; demonstrating itsapplicability in various fields and specific scenarios. The area of digital games; in particularserious games; is one of the areas that can benefit from the use of affective computing;especially when used in artificial intelligence agents; also defined as NPC (Non-PlayerCharacter) simulating human affective aspects such as personality; emotion and mood.Many advances have been made for combining affective computing and serious games.However; since games present a direct interaction with the user; they require more attentionto one important factor: the fun factor provided by the game. This paper aims at evaluatingthe fun factor of a game by experiencing a memory game that makes use of affective agentas a NPC opponent. The results in this paper demonstrate significant advances in the …,Serious Games and Applications for Health (SeGAH); 2014 IEEE 3rd International Conference on,2014,1
Experiencing Affective Agents in Simulation Games,Rainier Sales; Esteban Clua; Aline Paes; Daniel de Oliveira,Resumo Usually; characters of games based on the real world are simplified stereotypes ofthe human they represent; as suchgames disregard psychological aspects of the humanmind. However; emotive elements based on humans may beessential to achieve a properfun factor to the game player. Moreover; by applying emotional aspects related tohumanbeings; we are able to insert aspects of the reality from a human point of view; whichis going to yield an expected behaviorof the characters similar to those made outside thegame. Thus; in this article we developed an architecture to modifythe behavior of non-playercharacter (NPC); making them an affective agent. Personality traits; mood andemotionmodels based on well-established theories of psy-chology are associated to theNPC so that the behavior and generalway of thinking of humans are simulated.,Revista de Informática Aplicada,2014,1
DynAdapt: Alterações na Definição de Atividades de Workflows Científicos em Tempo de Execução,Igor de Araújo dos Santos; Jonas Dias; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Abstract. Scientific workflows can represent experiments based on computer simulations.Generally; workflows executed in parallel are time consuming and manage a large amountof data. Such characteristics may make the exploratory process of the experiment moredifficult or very expensive. In this scenario; it is necessary to handle workflows with dynamicaspects; which allow for changes in workflow definitions during runtime. This way; this articleproposes an approach to allow for changes in the definition of the activities of the workflowduring runtime; according to criteria defined by scientists. Resumo. Os workflows científicossão capazes de representar experimentos baseados em simulações computacionais. Emgeral; workflows executados em paralelo manipulam uma grande massa de dados edemandam um elevado tempo de execução. Tais características podem dificultar ou …,*,2014,1
Uma Arquitetura P2P de Distribuição de Atividades para Execução Paralela de Workflows Científicos,Vítor Silva; Jonas Dias; Daniel de Oliveira; Eduardo Ogasawara; Marta Mattoso,Abstract. Scientific workflows are composed of activities that model scientific experiments.Many Scientific Workflow Management Systems use High Performance Computingenvironments to parallelize the execution of these activities in large-scale workflows. Datadistribution; control; and optimizing the parallel execution of these activities can be acomplex task due to scalability of involved resources. This paper presents DEW; a data andactivity distribution mechanism for a parallel workflow execution engine. DEW is based on ahierarchical P2P network that enables distributed control in workflow execution usingdistributed disk and in the presence of high occurrence of churn events. Resumo. Workflowscientíficos são compostos de atividades que modelam experimentos científicos. VáriosSistemas de Gerência de Workflows Científicos fazem uso de ambientes de …,VII e-Science,2013,1
An Experimental Analysis of NPC as Affective Agents in Real World-based Games,Rainier Sales; Esteban Clua; A Paes,Abstract—Existing games are based on the gameplay for achieving the necessary fun factor;emotive elements are becoming common and in some cases even necessary whenmodeling Non-player character (NPC) behaviors. In this way; including and bringingemotional aspects from the real world may make the game closer to the reality; turning thevirtual characters more like humans. This paper proposes an implementation of NPCs asaffective agents by including personality; mood and emotive features based on well-established psychology models. Our affective agent model is implemented as a NPC in amemory game; where the different emotive aspects are introduced in order to analyze thevariability of its behavior based on the emotions. The results of 2;200 gameplay sessionsusing affective agents and considering different emotions show that the agent behavior …,*,2013,1
Heurísticas para Controle de Execução de Atividades de Workflows Científicos na Nuvem,Flavio Costa; D Oliveira; M Mattoso,*,Anais do Workshop de Teses e Dissertações em bancos de Dados-SBBD 2011,2011,1
Captura de Metadados de Proveniência para Workflows Científicos em Nuvens Computacionais.,Carlos Eduardo Paulino Silva; Daniel de Oliveira; Sérgio Manuel Serra da Cruz; Maria Luiza Machado Campos; Marta Mattoso,*,SBBD (Posters),2010,1
Modeling parallel bioinformatics workflows using MapReduce,Daniel Vega; D Oliveira; Eduardo Ogasawara; Alexandre AB Lima; Marta Mattoso,Abstract. Scientific workflows are used to model scientific experiments. Due to theexploratory characteristic of scientific experiments; it may be necessary to try many largeinput datasets to execute the workflow and produce results. These results may be composedby a large volume of data to be analyzed and validated. This exploratory process usuallydemands high performance computing environments and parallel techniques. Manyscientific workflows are candidates to benefit from MapReduce; a framework for large-scaleparallel applications. However; existing Scientific Workflow Management Systems (SWfMS)do not provide support to run scientific workflows based on MapReduce. This paperpresents an integration between Hadoop and VisTrails SWfMS to run in parallel abioinformatics experiment using Amazon EC2 cloud infrastructure.,International Workshop on Challenges in e-Science-SBAC; Petrópolis; RJ-Brazil,2010,1
A Conception Process for Abstract Workflows: An Example on Deep Water Oil Exploitation Domain‖,WM PEREIRA; E OGOSAWARA; D OLIVEIRA; Fernando Chirigati; Fabrício Correa; Breno Jacob; Ismael Santos; Guilherme H Travassos; MLQ MATTOSO,Abstract: Experimentation is one of the ways used to support theories based on a scientificmethod. In-silico experiments are highly dependent of massive use of computationalresources to execute their simulations. One way to use in-silico experiments is through theuse of scientific work-flows. It is a model that represents the flow of programs; services anddata usually orchestrated to support a simulation. Scientific workflows are executed inengines called Scientific Workflow Management Systems (SWfMS); which are responsiblefor enacting; controlling and monitoring the workflow. Each one of the scientific workflowswithin an experiment follows specific phases regarding composition; execution and analysis.Usually; when conducting a scientific experiment; the first phase to be considered is calledComposition. One important sub-phase is the Conception; which is responsible for setting …,5th IEEE International Conference on e-Science; 2009b; Oxford; London. In Press; Poster,2009,1
MF-Ontology: an Ontology for the Text Mining Domain,D Oliveira; Fernanda Baião; Marta Mattoso,Text mining (TM) has emerged as a definitive technique for knowledge acquisition from text.The TM process is based on several phases that prepare the text for mining; process thetext; and analyze the results. Effective and efficient use of the combination of TM algorithmsand techniques is a challenge. Most of the research is focused on developing new datastructures; algorithms and methods to achieve that. However; the TM process is still lackingof modeling support. The TM analyst faces many options when modeling a TM process. Forinstance; the analyst needs to choose the most effective solution to extract the desiredknowledge. This is a complex decision involving choices for each one of the TM processphases where many algorithms and implementations are available for composition andseveral parameters must be tuned. This scenario tends to be chaotic and each time a new …,*,2009,1
A hybrid evolutionary algorithm for task scheduling and data assignment of data-intensive scientific workflows on clouds,Luan Teylo; Ubiratam de Paula; Yuri Frota; Daniel de Oliveira; Lúcia MA Drummond,Abstract A growing number of data-and compute-intensive experiments have been modeledas scientific workflows in the last decade. Meanwhile; clouds have emerged as a prominentenvironment to execute this type of workflows. In this scenario; the investigation of workflowscheduling strategies; aiming at reducing its execution times; became a top priority and avery popular research field. However; few work consider the problem of data file assignmentwhen solving the task scheduling problem. Usually; a workflow is represented by a graphwhere nodes represent tasks and the scheduling problem consists in allocating tasks tomachines to be executed at a predefined time aiming at reducing the makespan of the wholeworkflow. In this article; we show that the scheduling of scientific workflows can be improvedwhen both task scheduling and the data file assignment problems are treated together …,Future Generation Computer Systems,2017,*
Towards preserving results confidentiality in cloud-based scientific workflows,Isabel Rosseti; Kary Ocaña; Daniel de Oliveira,Abstract Cloud computing has established itself as a solid computational model that allowsfor scientists to deploy their simulation-based experiments on distributed virtual resources toexecute a wide range of scientific experiments. These experiments can be modeled asscientific workflows. Many of these workflows are data-intensive and produce a large volumeof data; which is also stored in the cloud using storage services by Scientific WorkflowManagement Systems (SWfMS). One main issue regarding cloud storage services isconfidentiality of stored data; ie if unauthorized people access data files they can inferknowledge about the results or even about the workflow structure. Encryption is a possiblesolution; but it may not be be sufficient and a new level of security can be added to preservedata confidentiality: data dispersion. In order to reduce this risk; generated data files …,Proceedings of the 12th Workshop on Workflows in Support of Large-Scale Science,2017,*
Querying Provenance along with External Domain Data Using Prolog,Wellington Oliveira; Kary ACS Ocaña; Daniel de Oliveira; Vanessa Braganholo,*,Journal of Information and Data Management,2017,*
Eeny Meeny Miny Moe: Choosing the Fault Tolerance Technique for my Cloud Workflow,Leonardo Araújo de Jesus; Lúcia MA Drummond; Daniel de Oliveira,Abstract Scientific workflows are models composed of activities; data and dependencieswhose objective is to represent a computer simulation. Workflows are managed by ScientificWorkflow Management System (SWfMS). Such workflows commonly demand for manycomputational resources once their executions may involve a number of different programsprocessing a huge volume of data. Thus; the use of High Performance Computing (HPC)environments allied to parallelization techniques provides the support for the execution ofsuch experiments. Some resources provided by clouds can be used to build HPCenvironments. Although clouds offer advantages such as elasticity and availability; failuresare a reality rather than a possibility. Thus; SWfMS must be fault-tolerant. There are severaltypes of fault tolerance techniques used in SWfMS such as checkpoint-restart and …,Latin American High Performance Computing Conference,2017,*
Uma Abordagem Hierárquica para Escalonamento de Workflows Cientıficos Executados em Nuvens,Igor Barreto Rodrigues; Daniel de Oliveira,*,XVI Workshop em Desempenho de Sistemas Computacionais e de Comunicação,2017,*
Prediç ao de Falhas em Workflows Cientıficos em Nuvens baseada em Aprendizado de Máquina,Daniel Pinheiro da Silva Junior; Aline Paes; Daniel de Oliveira,*,Brazilian e-Science Workshop,2017,*
Uma Estratégia para Versionamento dos Dados de Workflows Científicos Executados em Nuvem,Fabrício Nogueira; Kary Ocaña; Vítor Silva; Vanessa Braganholo; Daniel de Oliveira,*,Brazilian e-Science Workshop,2017,*
SciPhyloMiner: um Workflow para Mineração de Dados Filogenômicos de Protozários,Thaylon Guedes; Kary Ocaña; Daniel de Oliveira,*,Brazilian e-Science Workshop,2017,*
BioSciCumulus: um portal para análise de dados de proveniência em workflows de biologia computacional,Débora Pina; Vinícius Campos; Vítor Silva; Kary Ocaña; Daniel de Oliveira; Marta Mattoso,*,Brazilian eScience Workshop,2017,*
Mirror Mirror on the Wall; How Do I Dimension My Cloud After All?,Rafaelli Coutinho; Yuri Frota; Kary Ocaña; Daniel de Oliveira; Lúcia MA Drummond,Abstract Clouds are a reality both in commercial and scientific domains. It is a fact thatclouds are not only an IT outsourcing; but an opportunity to foster the development ofcomplex scientific applications over distributed resources in several domains frombioinformatics to astronomy. Although clouds provide several advantages such as elasticityand a pay-as-you-go model; such characteristics come at a price. One important drawbackof clouds is how do estimate the amount of resources to deploy. Depending on the type ofapplication; it may be not simple to estimate the necessary amount of resources. Thiscomplexity may lead to over-or under-dimensioning; which are not desired. This chapteraddresses the problem of dimensioning the amount of virtual machines (VMs) in clouds forexecuting high performance computing (HPC) scientific applications. The aim of this …,*,2017,*
Clouds and Reproducibility: A Way to Go to Scientific Experiments?,Ary HM de Oliveira; Daniel de Oliveira; Marta Mattoso,Abstract Scientific research is supported by computing techniques and tools that allow forgathering; management; analysis; visualization; sharing; and reproduction of scientific dataand its experiments. The simulations performed in this type of research are called in silicoexperiments; and they are commonly composed of several applications that executetraditional algorithms and methods. Reproducibility plays a key role and gives the ability tomake changes in the data and test environment of a scientific experiment to evaluate therobustness of the proposed scientific method. By verifying and validating generated resultsof these experiments; there is an increase in productivity and quality of scientific dataanalysis processes resulting in the improvement of science development and production ofcomplex data in various scientific domains. There are many challenges to enable …,*,2017,*
A Systematic Mapping of Software Requirements Negotiation Techniques,Lucas Tito; Alexandre Estebanez; Andréa Magalhaes Magdaleno; Daniel de Oliveira; Marcos Kalinowski,Abstract:[Context] Eliciting requirements is a commonly discussed task. However; after theyare ready; it is essentially important for a software project that these requirements aresufficient for stakeholders to reach their goals. Therefore; techniques to negotiate schedule;price; quality; and scope among stakeholders are important.[Goal] This paper aims atidentifying and presenting characteristics of techniques that have been proposed and/orused to negotiate software requirements.[Method] A mapping study was planned andconducted to identify techniques and to capture their characteristics. Those characteristicsinclude description; environment (eg academic; industrial); the types of research beingpublished; and the types of primary studies. The main findings of the papers; and theadvantages and disadvantages reported for theses techniques were also summarized …,*,2017,*
SciAgile: Aplicação de Metodologias Ágeis em Experimentos Científicos Baseados em Simulação,Augusto Romeiro; Kary Ocaña; Marcos Kalinowski; Daniel de Oliveira,Resumo Muitos dos riscos envolvidos no desenvolvimento de software tem associação comrequisitos voláteis. Tal volatilidade motivou a criação e a adoção de metodologias ágeis;que hoje são uma realidade nas organizações. A volatilidade dos requisitos também podeser encontrada na pesquisa científica; que cada vez mais depende de complexassimulações computacionais. Tal como software comercial; o software científico; suaexecução e a análise dos resultados precisam ser compreensíveis; passíveis dereprodução e livres de defeitos. Como requisitos de experimentos científicos também sãocomumente voláteis; resolvemos investigar se as metodologias ágeis podem também seraplicadas neste contexto. Esse artigo apresenta uma abordagem; denominada SciAgile;que aplica uma série de práticas ágeis no ciclo de vida de experimentos científicos. Para …,*,2017,*
Enhancing Energy Production with Exascale HPC Methods,Rafael Mayo-García; José J Camata; José M Cela; Danilo Costa; Alvaro LGA Coutinho; Daniel Fernández-Galisteo; Carmen Jiménez; Vadim Kourdioumov; Marta Mattoso; Thomas Miras; José A Moríñigo; Jorge Navarro; Philippe OA Navaux; Daniel de Oliveira; Manuel Rodríguez-Pascual; Vítor Silva; Renan Souza; Patrick Valduriez,Abstract High Performance Computing (HPC) resources have become the key actor forachieving more ambitious challenges in many disciplines. In this step beyond; an explosionon the available parallelism and the use of special purpose processors are crucial. Withsuch a goal; the HPC4E project applies new exascale HPC techniques to energy industrysimulations; customizing them if necessary; and going beyond the state-of-the-art in therequired HPC exascale simulations for different energy sources. In this paper; a generaloverview of these methods is presented as well as some specific preliminary results.,Latin American High Performance Computing Conference,2016,*
SiAPP: An Information System for Crime Analytics Based on Logical Relational Learning,Vitor Lourenco; Paulo Mann; Aline Paes; Daniel Oliveira,Abstract The growing of criminality in Brazilian cities is a common theme addressed bymedia as well as by the legal authorities. To effectively reduce the criminality; people andinfrastructure must be carefully involved to not only punish who had committed crimes; butalso predict and prevent it. Since acquiring official data about crimes is far from trivial;citizens have become important data sources through Web-based collaborative systems.These systems provide a huge volume of data that has to be analyzed. How to analyze thisvolume of data and identify patterns in crimes is an important; yet open; issue. Thus; thiswork presents a system called SiAPP. Its main objective is to support the analysis andprediction of crime patterns using a machine learning algorithm. SiAPP automaticallyacquires data from collaborative sources; generate logical rules and visualizes the found …,Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era-Volume 1,2016,*
An IS for Managing Scientific Projects,Leonardo S Ramos; Kary ACS Ocana; Daniel Oliveira,Abstract Managing scientific projects is a complex task. The project may be associated withseveral different scientific experiments that in turn require implementations of differentcomputer simulations (scientific workflows). This management becomes even more complexif we consider that the project tasks should be associated with the specification andexecution of these simulations (which can take days or weeks to finish) and that the project'steam can be geographically dispersed. This paper presents the SciManager informationsystem that aims at helping scientists managing scientific projects. The SciManager is ableto manage the project; its associated experiments and workflows in a single tool; making allinformation related. The SciManager is based on a cloud architecture; which means it iseasily available for project members. An experimental evaluation was held out and …,Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era-Volume 1,2016,*
Gerência de Incerteza em Bancos de Dados de Proveniência de Workflows de Bioinformática,Gustavo Tallarida; Kary Ocaña; Aline Paes; Vanessa Braganholo; Daniel de Oliveira,Resumo. Bancos de dados de proveniência de experimentos científicos desempenham umpapel fundamental na ciência. Os modelos utilizados para representar esses dadosassumem que existe uma certeza nos relacionamentos de proveniência. Entretanto;diversos experimentos não são determinísticos e seus resultados estão associados aincertezas. Realizar a análise dos dados de proveniência com tais incertezas não é trivial.Nesse artigo é proposta uma abordagem para gerência de incertezas em dados deproveniência baseada em um componente extrator que armazena os dados deproveniência e a incerteza associada em um banco de dados probabilístico. Experimentosmostraram um overhead aceitável da abordagem de cerca de 3% no tempo total deexecução do workflow e 16% no tempo de processamento da consulta. Abstract …,*,2016,*
Applying future Exascale HPC methodologies in the energy sector,José J Camata; José M Cela; Danilo Costa; Alvaro LGA Coutinho; Daniel Fernández-Galisteo; Carmen Jiménez; Vadim Kourdioumov; Marta Mattoso; Rafael Mayo-García; Thomas Miras; José A Moríñigo; Jorge Navarro; Daniel de Oliveira; Manuel Rodríguez-Pascual; Vítor Silva; Renan Souza; Patrick Valduriez; Energéticas Medioambientales y Tecnológicas; Zenith Team,Abstract The appliance of new exascale HPC techniques to energy industry simulations isabsolutely needed nowadays. In this sense; the common procedure is to customize thesetechniques to the specific energy sector they are of interest in order to go beyond the state-of-the-art in the required HPC exascale simulations. With this aim; the HPC4E project isdeveloping new exascale methodologies to three different energy sources that are thepresent and the future of energy: wind energy production and design; efficient combustionsystems for biomass-derived fuels (biogas); and exploration geophysics for hydrocarbonreservoirs. In this work; the general exascale advances proposed as part of HPC4E and itsoutcome to specific results in different domains are presented.,Russian Supercomputing Days 2016,2016,*
Tolerância a Falhas de Workflows Cientıficos Executados em Nuvens Usando Checkpoints,Leonardo A de Jesus; Daniel de Oliveira; Lúcia MA Drummond,*,XVII Simpósio em Sistemas Computacionais de Alto Desempenho,2016,*
Enhancing Energy Production with Exascale HPC Methods,José J Camata; José M Cela; Danilo Costa; Alvaro LGA Coutinho; Daniel Fernández-Galisteo; Carmen Jiménez; Vadim Kourdioumov; Marta Mattoso; Rafael Mayo-García; Thomas Miras; José A Moríñigo; Jorge Navarro; Philippe OA Navaux; Daniel de Oliveira; Manuel Rodríguez-Pascual; Vítor Silva; Renan Souza; Patrick Valduriez,*,Latin American High Performance Computing Conference (CARLA 2016),2016,*
Comparing Provenance Data Models for Scientific Workflows: an Analysis of PROV-Wf and ProvOne,Wellington Oliveira; Paolo Missier; Daniel de Oliveira; Vanessa Braganholo,Abstract. Scientific workflows rely on provenance to be understandable; reproducible andtrustworthy. Nowadays; there is a growing demand for interoperability between provenancedata generated from heterogeneous workflow management systems. To address this issue;some provenance models have been proposed by extending PROV to support specificrequirements of scientific workflows. In this paper; we present two prominent provenancemodels for scientific workflows; PROV-Wf and ProvOne; which are specializations of PROV;and compare their elements and relationships. Our goal is to provide an overview of eachone and to support the choice for the most suitable for a specific context.,*,2016,*
Verificação da Reprodução de Workflows Científicos por meio de Algoritmos de Detecção de Plágio,Filipe Tadeu Santiago; Daniel de Oliveira,Resumo. Atualmente; diversos experimentos científicos são realizados por intermédio desimulações computacionais. Além da especificação; execução e monitoramento dessesexperimentos; um dos principais desafios da comunidade científica é como podemosverificar a reprodução de tais experimentos. Para que um experimento possa serclassificado como científico; seus resultados devem ser passíveis de reprodução porterceiros em condições similares. Em experimentos modelados como workflows científicos eexecutados em sistemas de gerência de workflows científicos; os dados de proveniênciacapturados são capazes de descrever tanto a especificação do workflow quanto o históricoda execução do mesmo. Dessa forma; esses dados oferecerem a base para verificar seuma determinada execução de um experimento é a reprodução de uma execução …,*,2016,*
Um Sistema de Informação para Gerência de Projetos Científicos baseados em Simulações Computacionais,Leonardo S Ramos; Kary ACS Ocaña; Daniel de Oliveira,RESUMO Gerenciar projetos científicos é uma tarefa complexa. Um mesmo projeto podeestar associado a vários experimentos científicos diferentes que por sua vez requeremexecuções de diferentes simulações computacionais (ie workflows científicos). Essagerência ainda se torna mais complexa se considerarmos que as tarefas do projeto devemestar associadas a execução de tais simulações (que podem demorar dias ou semanas) eque a equipe do projeto pode se encontrar geograficamente dispersa. Esse artigoapresenta o sistema de informação SciManager que tem como objetivo auxiliar os cientistasa gerenciar seus projetos científicos. O SciManager apoia a gerência do projeto; seusexperimentos associados e workflows em uma única ferramenta; fazendo com que todas asinformações referentes ao projeto estejam associadas. O SciManager é baseado em …,*,2016,*
Running Multi-relational Data Mining Processes in the Cloud: A Practical Approach for Social Networks,Aline Paes; Daniel de Oliveira,Abstract Multi-relational Data Mining algorithms (MRDM) are the appropriate approach forinferring knowledge from databases containing multiple relationships between non-homogenous entities; which are precisely the case of datasets obtained from socialnetworks. However; to acquire such expressivity; the search space of candidate hypothesesin MRDM algorithms is more complex than those obtained from traditional data miningalgorithms. To allow a feasible search space of hypotheses; MRDM algorithms adoptseveral language biases during the mining process. Because of that; when running a MRDM-based system; the user needs to execute the same set of data mining tasks a number oftimes; each assuming a different combination of parameters in order to get a final goodhypothesis. This makes manual control of such complex process tedious; laborious and …,Latin American High Performance Computing Conference,2015,*
connect with us,Daniel de Oliveira; Vítor Silva; Marta Mattoso,Abstract: Provenance databases are an important asset in data analytics of large-scalescientific data. The data derivation path allows for identifying parameters; files and domaindata values of interest. In scientific workflows; provenance data is automatically captured byworkflow systems. However; the power of provenance data analyses depends on theexpressiveness of domain-specific data along the provenance traces. While much has beendone through the W3C PROV initiative and its PROV-DM to represent generic provenancedata; representing domain-specific data in provenance traces has received little attention;yet it accounts for a large number of provenance analytical queries. Such queries are basedon selections on data values from input/output artifacts along workflow activities. There areseveral problems in modeling and capturing values from domain-specific attributes; some …,*,2015,*
Um Mapeamento Sistemático Sobre o Uso de Metodologias Ágeis no Processo de Experimentação Científica,Augusto Consulmagnos; Daniel de Oliveira,*,IX BRESCI - Brazilian e-Science Workshop,2015,*
MONITORANDO EXPERIMENTOS CIENTÃ FICOS EXECUTADOS EM AMBIENTES DE NUVEM DE COMPUTADORES,Julliano Pintas; Daniel de Oliveira; Kary OcaÃ±a; Jonas Dias; Marta Mattoso,Resumo A maioria dos workflows científicos de larga escala apresenta execução de longaduração; tornando inviável para o cientista monitorar o estado da execução durante todo otempo em um terminal. Neste artigo; apresentamos uma nova abordagem paramonitoramento em tempo real de workflows científicos executados em paralelo; baseadoem consultas aos dados de proveniência gerados em tempo real; que identifica eventos pré-configurados e notifica o cientista através de tecnologias de dispositivos móveis e redessociais. A avaliação da solução proposta; chamada SciLightning; foi realizada através domonitoramento da execução em paralelo do workflow de análise filogenética chamadoSciPhy no ambiente de nuvem Amazon EC2 usando a máquina de execução de workflowsem nuvem chamada SciCumulus. A avaliação mostrou que esta nova abordagem é …,REVISTA DE TRABALHOS ACADÊMICOS,2014,*
2014 IEEE 28th International Parallel & Distributed Processing Symposium Workshops Exploring Large Scale Receptor-Ligand Pairs in Molecular Docking Workflow...,Kary Ocaña; Silvia Benza; Daniel De Oliveira; Jonas Dias; Marta Mattoso,Abstract—Computer-aided drug design techniques are important assets in pharmaceuticalindustry because of their support for research and development of new drugs. Moleculardocking (MD) predicts specific compound's binding modes within the active site of targetproteins. Since MD is a timeconsuming process; existing approaches reduce the number ofreceptors or ligands in docking by evaluating only small sets of compounds. This restrictionin the search space reduces the chances to uniformly cover the diverse space of compoundsand misses opportunities to recognize whether new drugs can be identified. Anotherdifficulty with large-scale is analyzing the results; eg browsing all directories manually to findwhich pairs were docked successfully. To address these issues we explored the potential ofdata provenance analysis and parallel processing of SciCumulus; a cloud Scientific …,*,2014,*
Towards a UML-based Reference Model for Blended Learning,João Roberto de Toledo Quadros; Daniel Oliveira; Ambrozio Queiroz; Eduardo Ogasawara; Carlos Schocair,Abstract Blended Learning (BL) environments are defined as tools that aim at aidingblended learning practices since they use new technologies to learners from constraints oftime and place. A research target of this area is the use of new modeling techniques for thistype of environment; taking into account not only aspects of data such as accounts; schoolinfrastructures; but also educational profiles of the course and educational agents involved.In this paper we developed a modeling technique for BL targeting blended learningapplications. Our modeling technique takes into account pedagogical observations andneeds identified by actors (students; professors; and the staff involved in the development ofthe learning environment) to prepare BL environments. Our approach was evaluated using adistance-learning environment in a pilot class of Software for Tourism Course. The class …,International Journal of Recent Contributions from Engineering; Science & IT (iJES),2014,*
Towards a Reference Model for ODL: a Case Study in the Tourism Undergraduate Course,Joao Roberto Quadros; Daniel de Oliveira; Ambrozio Queiroz; Eduardo Ogasawara,Abstract–Open and Distance Learning (ODL) environments are defined as tools that aims ataiding learning practices since they use new technologies to free learners from constraintsof time and place. A research target of this area is the use of new modeling techniques forthis type of environment; taking into account not only aspects of data such as accounts;school infrastructures; but also educational profiles of the course and educational agentsinvolved. In this paper we developed a modeling technique for ODL; which has practicalapplication. Our modeling technique takes into account pedagogical observations andneeds identified by actors (students; professors; and the staff involved in the development ofthe learning environment). Our approach was evaluated using a distance-learningenvironment in a pilot class of Software for Tourism Course. The class aims at teaching …,*,2014,*
Uma Ferramenta para Mineração Multi-relacional de Redes Sociais Baseada em Programação Lógica e Workflows*,Manuel Almeida; Aline Paes; Daniel de Oliveira,Resumo. Técnicas de mineração de dados multi-relacional (MDMR) constituem asestratégias mais apropriadas para lidar com bases de dados contendo múltiplosrelacionamentos entre entidades não homogêneas; o que é justamente o caso de basesobtidas a partir de redes sociais. Entretanto; o espaço de busca de hipóteses candidatas detais estratégias é mais complexo do que aqueles obtidos a partir de técnicas tradicionais demineração de dados. Para permitir uma busca factível no espaço de hipóteses; as técnicasde MDMR adotam vieses de linguagem e de busca ao processo de mineração. Porém; umaanálise experimental detalhada requer a combinação de vários parâmetros distintos; o quetorna o controle manual de tal processo complexo. Nesse artigo; é apresentada umaferramenta que instancia um workflow científico para a análise de um processo de …,*,2014,*
Towards a UML-based Reference Model for Blended Learning,JR Quadros; D Oliveira; A Queiroz; E Ogasawara; C Schocair,*,*,2014,*
O Uso de WebSockets no Desenvolvimento de Sistemas Baseados em uma Arquitetura Front-end com API,Guilherme SA Gonçalves; Paulo Henrique O Bastos; Daniel de Oliveira,*,Escola Regional de Sistemas de Informação,2014,*
Computação em Nuvem para Ciência: o Papel Fundamental da Área de Bancos de Dados,Daniel de Oliveira; Marta Mattoso,*,*,2013,*
Distribuição de Bases de Dados de Proveniência na Nuvem.,Edimar Santos; Vanessa Assis; Flavio Costa; Daniel de Oliveira; Marta Mattoso,Resumo. Dados de proveniência no contexto de workflows científicos são peçasfundamentais; pois; por meio deles; os experimentos são passíveis de reprodução evalidação. O histórico da execução dos workflows é fundamental também para a gerênciada execução de novos workflows uma vez que possibilitam às máquinas de workflowrealizar predições sobre desempenho ou custo financeiro de nuvens de computadores.Workflows; com dados em larga escala; executados em nuvens; são com frequênciaalocados em máquinas virtuais distribuídas fisicamente. As soluções existentes coletam osdados de proveniência de forma distribuída e os armazenam de modo centralizado emúnico repositório; após o término da execução do workflow. Além da capacidade dereprodução; dados de proveniência permitem um acompanhamento refinado por parte …,SBBD (Short Papers),2013,*
Using Provenance Analyzers to Improve the Performance of Scientific Workflows in Cloud Environments.,João Carlos de AR Gonçalves; Daniel de Oliveira; Kary ACS Ocaña; Eduardo S Ogasawara; Jonas Dias; Marta Mattoso,Abstract. A major issue during scientific workflow execution is how to manage the largevolume of data to be processed. This issue is even more complex in cloud computing whereall resources are configurable in a pay per use model. A possible solution is to takeadvantage of the exploratory nature of the experiment and adopt filters to reduce data flowbetween activities. During a data exploration evaluation; the scientist may discardsuperfluous data (which is producing results that do not comply with a given quality criteria)produced during the workflow execution; avoiding unnecessary computations in the future.These quality criteria can be evaluated based on provenance and domain-specific data. Weclaim that the final decision on whether to discard superfluous data may become feasibleonly when workflows can be steered by scientists at runtime using provenance data …,SBBD (Short Papers),2012,*
Poster: scientific data parallelism using P2P technique,Jonas Dias; Eduardo Ogasawara; Daniel de Oliveira; Marta Mattoso,Abstract The complexity and the processing time in scientific experiments based oncomputational simulation models bring challenges on the conduction of these experiments.Scientific workflows have being adopted on large-scale science. The intense utilization oflarge volumes of data on these workflows demands parallelism techniques. However;parallelize a workflow requires specific tools and programming skills; which may become ablunder for scientists. To address this issue; this paper proposes Heracles; which is anapproach that makes the workflow parallelization into a more transparent task for scientists.Our approach proposes a fault tolerance and dynamic resource management mechanisminspired on P2P techniques. The purpose of Heracles is to execute activities in parallelwithout asking the scientists to specify the number of nodes involved in the execution and …,Proceedings of the 2011 companion on High Performance Computing Networking; Storage and Analysis Companion,2011,*
Heurísticas para Controle de Execução de Atividades de Workflows Científicos na Nuvem,Flavio da Silva Costa; Marta Mattoso; Daniel de Oliveira,*,Workshop de Teses e Dissertações em Banco de Dados,2011,*
SciCumulus-ECM: Um Serviço de Custos para a Execução de Workflows Científicos em Nuvens Computacionais.,Vitor Viana; Daniel de Oliveira; Eduardo S Ogasawara; Marta Mattoso,Resumo O conceito de computação em nuvem vem se firmando como um novo modelocomputacional que proporciona aos cientistas uma oportunidade de se utilizar diversosrecursos distribuídos para a execução de experimentos científicos. Muitos dos experimentoscientíficos existentes; modelados como workflows científicos; devem controlar a execuçãode atividades que consomem e produzem grandes volumes de dados. Há uma demandapor alto desempenho na execução destes experimentos uma vez que muitas destasatividades são computacionalmente intensivas. As técnicas de paralelismo sãoconsideradas um ponto chave em todo o processo de experimentação. Entretanto;paralelizar um workflow científico em um ambiente de nuvem não é uma tarefa trivial. Umadas tarefas mais complexas é definir a melhor configuração possível do ambiente de …,SBBD (Short Papers),2011,*
Grid and Cloud Computing,Tony Cass,–In 1990s; many supercomputing and national computing centres existed; but these weredisconnected from researchers. Users needed a dedicated account at each centre theywanted to use and had to connect to a front-end system.–The Grid model foresaw integrationof such facilities with the researcher's local environment: the end-user's PC plugged intoelectricity on tap via the power socket and computing on tap via the network socket.,*,2010,*
Odontology: Um Ambiente Semântico para Diagnósticos Odontológicos,Daniel de Oliveira; Bruno de Oliveira,*,Workshop de Informática Médica,2010,*
ONTOEPFS: UMA ONTOLOGIA PARA EXPERIMENTOS EM MAPEAMENTO DIGITAL DE SOLOS,LUIZ MANOEL SILVA CUNHA; DANIEL DE OLIVEIRA; EDUARDO OGASAWARA; SANDRA FERNANDES ANDRADES; GERALDO ZIMBRÃO DA SILVA; MARIA DE LOURDES MENDONÇA BREFIN,*,Embrapa Informática Agropecuária-Artigo em anais de congresso (ALICE),2009,*
On the use of scientific workflows for digital soil mapping,Luiz Manoel Cunha; Daniel de Oliveira; Eduardo Ogasawara; Sandra de Andrade; Geraldo Zimbrão; Maria de Lourdes Mendonça Santos,*,Pedometrics 2009 Conference,2009,*
Experiencing data grids,Nicolaas Ruberg; Nelson Kotowski; Amanda Mattos; Luciana Matos; Melissa Machado; Daniel Oliveira; Rafael Monclar; Cláudio Ferraz; Talitta Sanchotene; Vanessa Braganholo,Abstract Many scientific experiments deal with data-intensive applications and theorchestration of computational workflow activities. These can benefit from data parallelismexploited in parallel systems to minimize execution time. Due to its complexity; robustnessand efficiency to exploit data parallelism; grid infrastructures are widely used in some e-Science areas like bioinformatics. Workflow techniques are very important to in-silicobioinformatics experiments; allowing the e-scientist to describe and enact experimentalprocess in a structured; repeatable and verifiable way. The main purpose of this paper is todescribe our experience with Tavena Workbench and PeDRo; which are part of my Gridproject. Taverna is provided with a workflow toolset and enactor; allowing the specification ofprocessing units; data transfer and execution constraints. As a data entry tool; PeDRo …,International Conference on High Performance Computing for Computational Science,2006,*
Análise de Dados Cientıficos: uma Análise Comparativa de Dados de Simulaç oes Computacionais,Thaylon Guedes; Vıtor Silva; José Camata; Marta Mattoso; Daniel de Oliveira,Resumo. Os avanços nas simulaçoes computacionais têm permitido o processamento devolumes de dados cada vez maiores. Para representar as estruturas de dados complexasinerentes de tais simulaç oes; elas sao armazenadas em arquivos de formatosheterogêneos. Carregar tais dados em um SGBD; como o SciDB; para apoiar as análisesdeles se torna uma tarefa complexa; ou mesmo inviável; devido ao seu volume e/ouestrutura. Para evitar esse carregamento; existem abordagens que realizam consultasadaptativas e/ou que indexam os arquivos. Escolher a mais adequada pode nao ser trivial.Neste artigo realizamos uma análise comparativa em termos de desempenho dasabordagens de consulta de dados produzidos por uma simulaç ao em dinâmica de fluıdoscomputacional.,*,*,*
2014 IEEE 10th International Conference on e-Science (e-Science)(2014),Flavio Costa; Daniel de Oliveira; Marta Mattoso,*,*,*,*
Satellite Events of the 32nd Brazilian Symposium on Databases,Carmem S Hara; Bernadette F Lóscio; Daniel de Oliveira; Carina F Dorneles; Vânia MP Vidal; Fernanda Baião; Mirella M Moro; Kary Ocaña; Humberto L Razente; Maria Camila N Barioni,Abstract. Bioinformatics workflows generate massive amounts of data and an efficientmanagement of the data and its provenance is a challenge. The use of the provenance ofdata has brought remarkable benefits to the research in Bioinformatics; allowing a greateraccuracy in the analyses due to the reproducibility and the refinement that it provides to theexperiments. Bioinformatics workflows demand scalability and performance. In such context;the use of non-relational database system (NoSQL) is becoming increasingly common. Thispaper presents a comparative study between the NoSQL databases Cassandra; MongoDBand OrientDB DBMS regarding the management of data and provenance; both collected inthe execution of a DNA assembly workflow. Resumo. Workflows em Bioinformática geramum grande volume de dados e o gerenciamento de tais dados e de sua proveniência é …,*,*,*
Provenance Capture,Yang Ji; Sangho Lee; Wenke Lee; João Felipe Pimentel; Juliana Freire; Vanessa Braganholo; Leonardo Murta; Manolis Stamatogiannakis; Hasanat Kazmi; Hashim Sharif; Remco Vermeulen; Ashish Gehani; Herbert Bos; Paul Groth; Peng Chen; Tom Evans; Beth Plale; Wellington Oliveira; Paolo Missier; Kary Ocaña; Daniel de Oliveira; Troy Kohwalter; Thiago Oliveira; Esteban Clua; Danius T Michaelides; Richard Parker; Chris Charlton; William J Browne; Luc Moreau; Darren P Richardson; David Koop,Page 1. Contents Provenance Capture RecProv: Towards Provenance-Aware User SpaceRecord and Replay. . . . . 3 Yang Ji; Sangho Lee; and Wenke Lee Tracking and Analyzing theEvolution of Provenance from Scripts . . . . . 16 João Felipe Pimentel; Juliana Freire; VanessaBraganholo; and Leonardo Murta Trade-Offs in Automatic Provenance Capture . . . . . 29 ManolisStamatogiannakis; Hasanat Kazmi; Hashim Sharif; Remco Vermeulen; Ashish Gehani; HerbertBos; and Paul Groth Analysis of Memory Constrained Live Provenance . . . . . 42 Peng Chen;Tom Evans; and Beth Plale Provenance Analysis and Visualization Analyzing ProvenanceAcross Heterogeneous Provenance Graphs . . . . . 57 Wellington Oliveira; Paolo Missier; KaryOcaña; Daniel de Oliveira; and Vanessa Braganholo …,*,*,*
Publicity Chairs,Ilia Pietri; Khalid Belhajjame; Ivona Brandic; Rajkumar Buyya; Daniel de Oliveira,Scientific workflows are routinely used in most scientific disciplines today; as they provide asystematic way to execute a number of applications in Science and Engineering. They are atthe interface of end-users and computing infrastructures; often relying on workflowmanagement systems and a variety of parallel and/or distributed computing resources. Inaddition; with the drastic increase of raw data volume in many domains; their role isimportant to assist scientists in organizing and processing their data and leverage High-Performance or High-Throughput computing resources. The Workshop on Workflows inSupport of Large-Scale Science (WORKS) has been established as a premier forum on allaspects in relation to scientific workflows. The 11th edition of the workshop; WORKS 2016;co-located with SC16 in Salt Lake City; Utah; USA; followed the successful tradition of …,*,*,*
Cliquez sur une des collections/équipes de recherche ci-dessous pour voir les publications afférentes: Département Informatique,Vítor Silva; José Camata; Daniel De Oliveira; Alvaro Coutinho; Patrick Valduriez,Yeter Akgul. Gestion de la consommation basée sur l'adaptation dynamique de la tension;fréquence et body bias sur les systèmes sur puce en technologie FD-SOI. Micro etnanotechnologies/Microélectronique. Université Montpellier II-Sciences et Techniques duLanguedoc; 2014. Français.〈 NNT: 2014MON20132〉.〈 tel-01654475〉,*,*,*
