Overview of SciDB: large scale array storage; processing and analysis,Paul G Brown,Abstract SciDB [4; 3] is a new open-source data management system intended primarily foruse in application domains that involve very large (petabyte) scale array data; for example;scientific applications such as astronomy; remote sensing and climate modeling; bio-scienceinformation management; risk management systems in financial applications; and theanalysis of web log data. In this talk we will describe our set of motivating examples and usethem to explain the features of SciDB. We then briefly give an overview of the project'inflight'; explaining our novel storage manager; array data model; query language; andextensibility frameworks.,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,280
A demonstration of SciDB: a science-oriented DBMS,Philippe Cudré-Mauroux; Hideaki Kimura; K-T Lim; Jennie Rogers; Roman Simakov; Emad Soroush; Pavel Velikhov; Daniel L Wang; Magdalena Balazinska; Jacek Becla; D DeWitt; Bobbi Heath; David Maier; Samuel Madden; J Patel; Michael Stonebraker; S Zdonik,Abstract In CIDR 2009; we presented a collection of requirements for SciDB; a DBMS thatwould meet the needs of scientific users. These included a nested-array data model; science-specific operations such as regrid; and support for uncertainty; lineage; and named versions.In this paper; we present an overview of SciDB's key features and outline a demonstration ofthe first version of SciDB on data and operations from one of our lighthouse users; the LargeSynoptic Survey Telescope (LSST).,Proceedings of the VLDB Endowment,2009,153
One size fits all? Part 2: Benchmarking results,Michael Stonebraker; Chuck Bear; Uğur Çetintemel; Mitch Cherniack; Tingjian Ge; Nabil Hachem; Stavros Harizopoulos; John Lifter; Jennie Rogers; Stan Zdonik,ABSTRACT Two years ago; some of us wrote a paper predicting the demise of “One SizeFits All (OSFA)”[Sto05a]. In that paper; we examined the stream processing and datawarehouse markets and gave reasons for a substantial performance advantage tospecialized architectures in both markets. Herein; we make three additional contributions.First; we present reasons why the same performance advantage is enjoyed by specializedimplementations in the text processing market. Second; the major contribution of the paper isto show “apples to apples” performance numbers between commercial implementations ofspecialized architectures and relational DBMSs in both stream processing and datawarehouses. Finally; we also show comparison numbers between an academic prototype ofa specialized architecture for scientific and intelligence applications; a relational DBMS …,Proc. CIDR,2007,122
Performance prediction for concurrent database workloads,Jennie Duggan; Ugur Cetintemel; Olga Papaemmanouil; Eli Upfal,Abstract Current trends in data management systems; such as cloud and multi-tenantdatabases; are leading to data processing environments that concurrently executeheterogeneous query workloads. At the same time; these systems need to satisfy diverseperformance expectations. In these newly-emerging settings; avoiding potential Quality-of-Service (QoS) violations heavily relies on performance predictability; ie; the ability toestimate the impact of concurrent query execution on the performance of individual queriesin a continuously evolving workload. This paper presents a modeling approach to estimatethe impact of concurrency on query performance for analytical workloads. Our solution relieson the analysis of query behavior in isolation; pairwise query interactions and samplingtechniques to predict resource contention under various query mixes and concurrency …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,114
The bigdawg polystore system,Jennie Duggan; Aaron J Elmore; Michael Stonebraker; Magda Balazinska; Bill Howe; Jeremy Kepner; Sam Madden; David Maier; Tim Mattson; Stan Zdonik,Abstract This paper presents a new view of federated databases to address the growingneed for managing information that spans multiple data models. This trend is fueled by theproliferation of storage engines and query languages based on the observation that “no onesize fits all”. To address this shift; we propose a polystore architecture; it is designed to unifyquerying over multiple data models. We consider the challenges and opportunitiesassociated with polystores. Open questions in this space revolve around query optimizationand the assignment of objects to storage engines. We introduce our approach to thesetopics and discuss our prototype in the context of the Intel Science and Technology Centerfor Big Data,ACM Sigmod Record,2015,62
A demonstration of the bigdawg polystore system,Aaron Elmore; Jennie Duggan; Mike Stonebraker; Magdalena Balazinska; Ugur Cetintemel; Vijay Gadepally; Jeffrey Heer; Bill Howe; Jeremy Kepner; Tim Kraska; Samuel Madden; David Maier; Timothy Mattson; Stavros Papadopoulos; Jeff Parkhurst; Nesime Tatbul; Manasi Vartak; Stan Zdonik,Abstract This paper presents BigDAWG; a reference implementation of a new architecturefor" Big Data" applications. Such applications not only call for large-scale analytics; but alsofor real-time streaming support; smaller analytics at interactive speeds; data visualization;and cross-storage-system queries. Guided by the principle that" one size does not fit all"; webuild on top of a variety of storage engines; each designed for a specialized use case. Toillustrate the promise of this approach; we demonstrate its effectiveness on a hospitalapplication using data from an intensive care unit (ICU). This complex application serves theneeds of doctors and researchers and provides real-time support for streams of patient data.It showcases novel approaches for querying across multiple storage engines; datavisualization; and scalable real-time analytics.,Proceedings of the VLDB Endowment,2015,55
E-store: Fine-grained elastic partitioning for distributed transaction processing systems,Rebecca Taft; Essam Mansour; Marco Serafini; Jennie Duggan; Aaron J Elmore; Ashraf Aboulnaga; Andrew Pavlo; Michael Stonebraker,Abstract On-line transaction processing (OLTP) database management systems (DBMSs)often serve time-varying workloads due to daily; weekly or seasonal fluctuations in demand;or because of rapid growth in demand due to a company's business success. In addition;many OLTP workloads are heavily skewed to" hot" tuples or ranges of tuples. For example;the majority of NYSE volume involves only 40 stocks. To deal with such fluctuations; anOLTP DBMS needs to be elastic; that is; it must be able to expand and contract resources inresponse to load fluctuations and dynamically balance load as hot tuples vary over time.This paper presents E-Store; an elastic partitioning framework for distributed OLTP DBMSs.It automatically scales resources in response to demand spikes; periodic events; andgradual changes in an application's workload. E-Store addresses localized bottlenecks …,Proceedings of the VLDB Endowment,2014,54
A generic auto-provisioning framework for cloud databases,Jennie Rogers; Olga Papaemmanouil; Ugur Cetintemel,We discuss the problem of resource provisioning for database management systemsoperating on top of an Infrastructure-As-A-Service (IaaS) cloud. To solve this problem; wedescribe an extensible framework that; given a target query workload; continually optimizesthe system's operational cost; estimated based on the IaaS provider's pricing model; whilesatisfying QoS expectations. Specifically; we describe two different approaches; a “white-box” approach that uses a fine-grained estimation of the expected resource consumption fora workload; and a “black-box” approach that relies on coarse-grained profiling tocharacterize the workload's end-to-end performance across various cloud resources. Weformalize both approaches as a constraint programming problem and use a genericconstraint solver to efficiently tackle them. We present preliminary experimental numbers …,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,44
Ss-db: A standard science dbms benchmark,Philippe Cudre-Mauroux; Hideaki Kimura; Kian-Tat Lim; Jennie Rogers; Samuel Madden; Michael Stonebraker; Stanley B Zdonik; Paul G Brown,*,Under submission,2010,21
The bigdawg polystore system and architecture,Vijay Gadepally; Peinan Chen; Jennie Duggan; Aaron Elmore; Brandon Haynes; Jeremy Kepner; Samuel Madden; Tim Mattson; Michael Stonebraker,Organizations are often faced with the challenge of providing data management solutions forlarge; heterogenous datasets that may have different underlying data and programmingmodels. For example; a medical dataset may have unstructured text; relational data; timeseries waveforms and imagery. Trying to fit such datasets in a single data managementsystem can have adverse performance and efficiency effects. As a part of the Intel Scienceand Technology Center on Big Data; we are developing a polystore system designed forsuch problems. BigDAWG (short for the Big Data Analytics Working Group) is a polystoresystem designed to work on complex problems that naturally span across differentprocessing or storage engines. BigDAWG provides an architecture that supports diversedatabase systems working with different data models; support for the competing notions …,High Performance Extreme Computing Conference (HPEC); 2016 IEEE,2016,18
Packing light: Portable workload performance prediction for the cloud,Jennie Duggan; Yun Chi; Hakan Hacigumus; Shenghuo Zhu; Ugur Cetintemel,We introduce a new learning-based solution for portable database workload performanceprediction. The current state of the art addresses performance prediction for individual; statichardware configurations and thus cannot generalize to new platforms without additionaltraining. In this work; we focus on analytical databases that might be deployed on differenthardware configurations; possibly offered by various Infrastructure-as-a-Service (IaaS)providers in the cloud. Enabling workload performance predictions that can be ported acrosshardware configurations and IaaS offerings could significantly help cloud users with theirservice-purchase decisions and cloud providers with their provisioning decisions. Oursolution is based on collaborative filtering modeling and prediction. We applied it tolightweight workload fingerprints that model the characteristics and behavior of …,Data Engineering Workshops (ICDEW); 2013 IEEE 29th International Conference on,2013,17
Incremental elasticity for array databases,Jennie Duggan; Michael Stonebraker,Abstract Relational databases benefit significantly from elasticity; whereby they execute on aset of changing hardware resources provisioned to match their storage and processingrequirements. Such flexibility is especially attractive for scientific databases because theirusers often have a no-overwrite storage model; in which they delete data only when theiravailable space is exhausted. This results in a database that is regularly growing andexpanding its hardware proportionally. Also; scientific databases frequently store their dataas multidimensional arrays optimized for spatial querying. This brings about several novelchallenges in clustered; skew-aware data placement on an elastic shared-nothing database.In this work; we design and implement elasticity for an array database. We address thischallenge on two fronts: determining when to expand a database cluster and how to …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,16
Contender: A resource modeling approach for concurrent query performance prediction,Jennie Duggan; Olga Papaemmanouil; Ugur Cetintemel; Eli Upfal,Abstract Predicting query performance under concurrency is a difficult task that has manyapplications in capacity planning; cloud computing; and batch scheduling. We introduceContender; a new resourcemodeling approach for predicting the concurrent queryperformance of analytical workloads. Contender's unique feature is that it can generateeffective predictions for both static as well as adhoc or dynamic workloads with low trainingrequirements. These characteristics make Contender a practical solution for real-worlddeployment. Contender relies on models of hardware resource contention to predictconcurrent query performance. It introduces two key metrics; Concurrent Query Intensity(CQI) and Query Sensitivity (QS); to characterize the impact of resource contention on queryinteractions. CQI models how aggressively concurrent queries will use the shared …,17th International Conference on Extending Database Technology; EDBT 2014,2014,15
Simultaneous equation systems for query processing on continuous-time data streams,Yanif Ahmad; Olga Papaemmanouil; Ugur Cetintemel; Jennie Rogers,We introduce Pulse; a framework for processing continuous queries over models ofcontinuous-time data; which can compactly and accurately represent many real-worldactivities and processes. Pulse implements several query operators; including filters;aggregates and joins; that work by solving simultaneous equation systems; which in manycases is significantly cheaper than processing a stream of tuples. As such; Pulse translatesregular queries to work on continuous-time inputs; to reduce computational overhead andlatency while meeting user-specified error bounds on query results. For error boundchecking; Pulse uses an approximate query inversion technique that ensures the solverexecutes infrequently and only in the presence of errors; or no previously known results. Wefirst discuss the high-level design of Pulse; which we fully implemented in a stream …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,14
Skew-aware join optimization for array databases,Jennie Duggan; Olga Papaemmanouil; Leilani Battle; Michael Stonebraker,Abstract Science applications are accumulating an ever-increasing amount ofmultidimensional data. Although some of it can be processed in a relational database; muchof it is better suited to array-based engines. As such; it is important to optimize the queryprocessing of these systems. This paper focuses on efficient query processing of joinoperations within an array database. These engines invariably``chunk''their data intomultidimensional tiles that they use to efficiently process spatial queries. As such; traditionalrelational algorithms need to be substantially modified to take advantage of array tiles.Moreover; most n-dimensional science data is unevenly distributed in array space becauseits underlying observations rarely follow a uniform pattern. It is crucial that the optimization ofarray joins be skew-aware. In addition; owing to the scale of science applications; their …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,9
The case for personal data-driven decision making,Jennie Duggan,Abstract Data-driven decision making (D3M) has shown great promise in professionalpursuits such as business and government. Here; policymakers collect and analyze data tomake their operations more efficient and equitable. Progress in bringing the benefits of D3Mto everyday life has been slow. For example; a student asks;" If I pursue an undergraduatedegree at this university; what are my expected lifetime earnings?". Presently there is noprincipled way to search for this; because an accurate answer depends on the student andschool. Such queries are personalized; winnowing down large datasets for specificcircumstances; rather than applying well-defined predicates. They predict decisionoutcomes by extrapolating from relevant examples. This vision paper introduces a newapproach to D3M that is designed to empower the individual to make informed choices …,Proceedings of the VLDB Endowment,2014,9
Bigdawg polystore query optimization through semantic equivalences,Zuohao She; Surabhi Ravishankar; Jennie Duggan,A polystore system evaluates queries that span multiple disparate data models; thischaracter introduces a unique query optimization challenge. Specialized database enginessuch as array and graph databases support partially overlapping sets of query processingoperations. Among their common or similar semantics; different systems could havecompletely different performance profiles for the same query; making their relativeusefulness vary from query to query. We hypothesize that a polystore system could exploitthis context-dependent disparity of performance by making choices between executing asub-query locally and migrating the inputs for remote executions. In this work; as part of thelarger ISTC BigDAWG project; we examine the challenges of polystore query optimizationthrough the lens of equivalent semantics among back-end databases.,High Performance Extreme Computing Conference (HPEC); 2016 IEEE,2016,8
SMCQL: secure querying for federated databases,Johes Bater; Gregory Elliott; Craig Eggen; Satyender Goel; Abel Kho; Jennie Rogers,Abstract People and machines are collecting data at an unprecedented rate. Despite thisnewfound abundance of data; progress has been slow in sharing it for open science;business; and other data-intensive endeavors. Many such efforts are stymied by privacyconcerns and regulatory compliance issues. For example; many hospitals are interested inpooling their medical records for research; but none may disclose arbitrary patient records toresearchers or other healthcare providers. In this context we propose the Private DataNetwork (PDN); a federated database for querying over the collective data of mutuallydistrustful parties. In a PDN; each member database does not reveal its tuples to its peersnor to the query writer. Instead; the user submits a query to an honest broker that plans andcoordinates its execution over multiple private databases using secure multiparty …,Proceedings of the VLDB Endowment,2017,6
Hephaestus: Data Reuse for Accelerating Scientific Discovery,Jennie Duggan; Michael L Brodie,Abstract Data-intensive science; wherein domain experts use big data analytics in thecourse of their research; is becoming increasingly common in the physical and socialsciences. Moreover; data reuse is becoming the new normal; owing to the open datamovement and arrival of big science experiments such as the Large Hadron Collider. Here;a small group of researchers with exotic equipment produce a dataset that is shared bythousands. Unfortunately; weak and spurious correlations are also on the rise in research.For example; Google Flu Trends published their algorithms in 2008 for use in public health;and in the intervening time its accuracy has plummeted. In the 2011-2012 flu season; thissystem produced estimates more than 50% higher than the number of cases reported by theUS Center for Disease Control.,CIDR,2015,5
SciDB DBMS research at MIT,Michael Stonebraker; Jennie Duggan; Leilani Battle; Olga Papaemmanouil,Abstract This paper presents a snapshot of some of our scientific DBMS research at MIT aspart of the Intel Science and Technology Center on Big Data. We focus our efforts primarilyon SciDB; although some of our work can be used for any backend DBMS. We summarizeour work on making SciDB elastic; providing skew-aware join strategies; and producingscalable visualizations of scientific data.,IEEE Data Eng. Bull.,2013,5
STeP: Scalable Tenant Placement for Managing Database-as-a-Service Deployments,Rebecca Taft; Willis Lang; Jennie Duggan; Aaron J Elmore; Michael Stonebraker; David DeWitt,Abstract Public cloud providers with Database-as-a-Service offerings must efficientlyallocate computing resources to each of their customers. An effective assignment of tenantsboth reduces the number of physical servers in use and meets customer expectations at aprice point that is competitive in the cloud market. For public cloud vendors like Microsoftand Amazon; this means packing millions of users' databases onto hundreds or thousandsof servers. This paper studies tenant placement by examining a publicly released dataset ofanonymized customer resource usage statistics from Microsoft's Azure SQL Databaseproduction system over a three-month period. We implemented the STeP framework toingest and analyze this large dataset. STeP allowed us to use this production dataset toevaluate several new algorithms for packing database tenants onto servers. These …,Proceedings of the Seventh ACM Symposium on Cloud Computing,2016,3
Bigdawg: a polystore for diverse interactive applications,Adam Dziedzic; Jennie Duggan; Aaron J Elmore; Vijay Gadepally; Michael Stonebraker,Abstract—Interactive analytics requires low latency queries in the presence of diverse;complex; and constantly evolving workloads. To address these challenges; we introduce apolystore; BigDAWG; that tightly couples diverse database systems; data models; and querylanguages through use of semantically grouped “Islands of Information”. BigDAWG; whichstands for the Big Data Working Group; seeks to provide location transparency by matchingthe right system for each workload using black-box model of query and system performance.In this paper we introduce BigDAWG as a solution to diverse web-based interactiveapplications and motivate our key challenges in building BigDAWG. BigDAWG continues toevolve and; where applicable; we have noted the current status of its implementation.,Data Syst. Interactive Anal. Workshop 2015,2015,3
Version 0.1 of the bigdawg polystore system,Vijay Gadepally; Kyle OBrien; Adam Dziedzic; Aaron Elmore; Jeremy Kepner; Samuel Madden; Tim Mattson; Jennie Rogers; Zuohao She; Michael Stonebraker,Abstract: A polystore system is a database management system (DBMS) composed ofintegrated heterogeneous database engines and multiple programming languages. Bymatching data to the storage engine best suited to its needs; complex analytics run fasterand flexible storage choices helps improve data organization. BigDAWG (Big Data WorkingGroup) is our reference implementation of a polystore system. In this paper; we describe thecurrent BigDAWG software release which supports PostgreSQL; Accumulo and SciDB. Wedescribe the overall architecture; API and initial results of applying BigDAWG to the MIMIC IImedical dataset.,arXiv preprint arXiv:1707.00721,2017,2
The bigdawg architecture and reference implementation,Jennie Duggan; Aaron Elmore; Tim Kraska; Sam Madden; Tim Mattson; Michael Stonebraker,ABSTRACT This paper presents the reference implementation of a new architecture forfuture “Big Data” applications. Such applications require “big analytics” as one might expect;but they also require real-time streaming support; real-time analytics; data visualization; andcross-storage queries. We are guided by the principle “one size does not fit all”[7]; and webuild on top of three storage engines; each designed for specialized use cases. In addition;we demonstrate novel support for querying across multiple storage engines as well aspioneering solutions to data visualization. In the remainder of this short paper; we describethe first of three BigDawg reference implementations; Bulldog. In the next two years weexpect to follow with Pitbull and Rottweiler releases.,New England Database Day,2015,2
BigDAWG version 0.1,Vijay Gadepally; Kyle O'Brien; Adam Dziedzic; Aaron Elmore; Jeremy Kepner; Samuel Madden; Tim Mattson; Jennie Rogers; Zuohao She; Michael Stonebraker,A polystore system is a database management system composed of integratedheterogeneous database engines and multiple programming languages. By matching datato the storage engine best suited to its needs; complex analytics run faster and flexiblestorage choices helps improve data organization. BigDAWG (Big Data Working Group) isour prototype implementation of a polystore system. In this paper; we describe the currentBigDAWG software release which supports PostgreSQL; Accumulo and SciDB. We describethe overall architecture; API and initial results of applying BigDAWG to the MIMIC II medicaldataset.,High Performance Extreme Computing Conference (HPEC); 2017 IEEE,2017,1
Bigdawg polystore release and demonstration,Kyle OBrien; Vijay Gadepally; Jennie Duggan; Adam Dziedzic; Aaron Elmore; Jeremy Kepner; Samuel Madden; Tim Mattson; Zuohao She; Michael Stonebraker,Abstract: The Intel Science and Technology Center for Big Data is developing a referenceimplementation of a Polystore database. The BigDAWG (Big Data Working Group) systemsupports" many sizes" of database engines; multiple programming languages and complexanalytics for a variety of workloads. Our recent efforts include application of BigDAWG to anocean metagenomics problem and containerization of BigDAWG. We intend to release anopen source BigDAWG v1. 0 in the Spring of 2017. In this article; we will demonstrate anumber of polystore applications developed with oceanographic researchers at MIT anddescribe our forthcoming open source release of the BigDAWG system. Subjects: Databases(cs. DB) Cite as: arXiv: 1701.05799 [cs. DB](or arXiv: 1701.05799 v1 [cs. DB] for this version)Submission history From: Vijay Gadepally [view email][v1] Thu; 19 Jan 2017 00: 29: 31 …,arXiv preprint arXiv:1701.05799,2017,1
SMCQL: secure query processing for private data networks,Johes Bater; Gregory Elliott; Craig Eggen; Satyender Goel; Abel Kho; Jennie Duggan,Abstract: People and machines are collecting data at an unprecedented rate. Despite thisnewfound abundance of data; progress has been slow in sharing information for openscience and other research initiatives. Many such efforts are stymied by privacy concernsand regulatory compliance issues. For example; many hospitals are interested in poolingtheir patient records for research; but none may disclose the individual tuples in theirdatabases without violating patient confidentiality. It is in this context that we propose thePrivate Data Network (PDN); a federated database for querying over the collective data ofmutually distrustful parties. In a PDN; member databases do not reveal their query inputs toone another or the query writer. Instead; the user submits their query to a honest broker thatplans and coordinates its execution over multiple private databases using secure …,arXiv preprint arXiv:1606.06808,2016,1
The BigDAWG Architecture,Vijay Gadepally; Jennie Duggan; Aaron Elmore; Jeremy Kepner; Samuel Madden; Tim Mattson; Michael Stonebraker,Abstract: BigDAWG is a polystore system designed to work on complex problems thatnaturally span across different processing or storage engines. BigDAWG provides anarchitecture that supports diverse database systems working with different data models;support for the competing notions of location transparency and semantic completeness viaislands of information and a middleware that provides a uniform multi-island interface. In thisarticle; we describe the current architecture of BigDAWG; its application on the MIMIC IImedical dataset; and our plans for the mechanics of cross-system queries. During thepresentation; we will also deliver a brief demonstration of the current version of BigDAWG.Subjects: Databases (cs. DB) Cite as: arXiv: 1602.08791 [cs. DB](or arXiv: 1602.08791 v1[cs. DB] for this version) Submission history From: Vijay Gadepally [view email][v1] Mon …,arXiv preprint arXiv:1602.08791,2016,1
What versus Why: Towards Computing Reality,Michael L Brodie; Jennie Duggan,In late 2013; a colleague; Herb Lin; Chief Scientist of the US National Academies of Science;expressed his concern about Big Data. Herb is responsible for overseeing Academy studiesfor scientific rigor. He was concerned about the growing use of Big Data since Big Dataanalytics can only suggest What has occurred within a specific probability or confidencelevel. Big Data analytics does not and cannot say anything about causation; or Why aphenomenon occurred. This observation has led us over the course of six months to somefairly profound observations. First; with Big Data we are moving from discrete; top-‐down;well understood models (data schemas and computational models) to a new world ofcomputing that is vague; bottom-‐up; and model-‐less. In fact if we impose models on BigData analytics; we may obscure or prevent its greatest value. These concerns are shared …,*,2014,*
JOURNALISM FOR THE 21ST-CENTURY-ONLINE INFORMATION; ELECTRONIC DATABASES AND THE NEWS-KOCH; T,J ROGERS,*,*,1992,*
Towards a Generic Data Compression Advisor,Jennie Rogers,Abstract—Compression allows scalable storage of large amounts of data and alleviates theI/O bottleneck for dataintensive applications. Over the years; a large number of compressionalgorithms have been developed to support various data types. Our goal is to develop ageneric tool that helps quickly and accurately decide what compression scheme (s) bestmatch the constraints and expectations of users with respect to factors such as compressiontime (encoding/decoding); space (data/meta-data); and quality (lossiness). As an initial step;we investigate techniques for generating and expanding time-space-quality Pareto frontiersfor multidimensional array data. To this end; we studied several enhancements to traditionalalgorithms; these include 1) partitioning the arrays into multidimensional tiles and applyingheterogeneous compression schemes across the tiles;(2) introducing lossiness to trade …,*,*,*
