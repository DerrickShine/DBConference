Information-theoretic metric learning,Jason V Davis; Brian Kulis; Prateek Jain; Suvrit Sra; Inderjit S Dhillon,Abstract In this paper; we present an information-theoretic approach to learning aMahalanobis distance function. We formulate the problem as that of minimizing thedifferential relative entropy between two multivariate Gaussians under constraints on thedistance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resultingalgorithm has several advantages over existing methods. First; our method can handle awide variety of constraints and can optionally incorporate a prior on the distance function.Second; it is fast and scalable. Unlike most existing methods; no eigenvalue computations orsemi-definite programming are required. We also present an online version and deriveregret bounds for the resulting algorithm. Finally; we evaluate our method on a recent …,Proceedings of the 24th international conference on Machine learning,2007,1551
Clustering on the unit hypersphere using von Mises-Fisher distributions,Arindam Banerjee; Inderjit S Dhillon; Joydeep Ghosh; Suvrit Sra,Abstract Several large scale data mining applications; such as text categorization and geneexpression analysis; involve high-dimensional data that is also inherently directional innature. Often such data is L 2 normalized so that it lies on the surface of a unit hypersphere.Popular models such as (mixtures of) multi-variate Gaussians are inadequate forcharacterizing such data. This paper proposes a generative mixture-model approach toclustering directional data based on the von Mises-Fisher (vMF) distribution; which arisesnaturally for data distributed on the unit hypersphere. In particular; we derive and analyzetwo variants of the Expectation Maximization (EM) framework for estimating the mean andconcentration parameters of this mixture. Numerical estimation of the concentrationparameters is non-trivial in high dimensions since it involves functional inversion of ratios …,Journal of Machine Learning Research,2005,592
Generalized nonnegative matrix approximations with Bregman divergences,Suvrit Sra; Inderjit S Dhillon,Abstract Nonnegative matrix approximation (NNMA) is a recent technique for dimensionalityreduction and data analysis that yields a parts based; sparse nonnegative representation fornonnegative input data. NNMA has found a wide variety of applications; including textanalysis; document clustering; face/image recognition; language modeling; speechprocessing and many others. Despite these numerous applications; the algorithmicdevelopment for computing the NNMA factors has been relatively deficient. This papermakes algorithmic progress by modeling and solving (using multiplicative updates) newgeneralized NNMA problems that minimize Bregman divergences between the input matrixand its lowrank approximation. The multiplicative update formulae in the pioneering work byLee and Seung [11] arise as a special case of our algorithms. In addition; the paper …,Advances in neural information processing systems,2006,356
Minimum sum-squared residue co-clustering of gene expression data,Hyuk Cho; Inderjit S Dhillon; Yuqiang Guan; Suvrit Sra,Abstract Microarray experiments have been extensively used for simultaneously measuringDNA expression levels of thousands of genes in genome research. A key step in theanalysis of gene expression data is the clustering of genes into groups that show similarexpression values over a range of conditions. Since only a small subset of the genesparticipate in any cellular process of interest; by focusing on subsets of genes andconditions; we can lower the noise induced by other genes and conditions—a co-clustercharacterizes such a subset of interest. Cheng and Church [3] introduced an effectivemeasure of co-cluster quality based on mean squared residue. In this paper; we use twosimilar squared residue measures and propose two fast k-means like co-clusteringalgorithms corresponding to the two residue measures. Our algorithms discover k row …,*,2004,353
Optimization for machine learning,Suvrit Sra; Sebastian Nowozin; Stephen J Wright,An up-to-date account of the interplay between optimization and machine learning;accessible to students and researchers in both communities. The interplay betweenoptimization and machine learning is one of the most important developments in moderncomputational science. Optimization formulations and methods are proving to be vital indesigning algorithms to extract essential knowledge from huge volumes of data. Machinelearning; however; is not simply a consumer of optimization technology but a rapidlyevolving field that is itself generating new optimization ideas. This book captures the state ofthe art of the interaction between optimization and machine learning in a way that isaccessible to researchers in both fields. Optimization approaches have enjoyed prominencein machine learning because of their wide applicability and attractive theoretical …,*,2012,292
Efficient filter flow for space-variant multiframe blind deconvolution,Michael Hirsch; Suvrit Sra; Bernhard Schölkopf; Stefan Harmeling,Ultimately being motivated by facilitating space-variant blind deconvolution; we present aclass of linear transformations; that are expressive enough for space-variant filters; but at thesame time especially designed for efficient matrix-vector-multiplications. Successful resultson astronomical imaging through atmospheric turbulences and on noisy magneticresonance images of constantly moving objects demonstrate the practical significance of ourapproach.,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,156
Fast Newton-type methods for the least squares nonnegative matrix approximation problem,Dongmin Kim; Suvrit Sra; Inderjit Dhillon,Abstract Nonnegative Matrix Approximation is an effective matrix decomposition techniquethat has proven to be useful for a wide variety of applications ranging from documentanalysis and image processing to bioinformatics. There exist a few algorithms fornonnegative matrix approximation (NNMA); for example; Lee & Seung's multiplicativeupdates; alternating least squares; and certain gradient descent based procedures. All ofthese procedures suffer from either slow convergence; numerical instabilities; or at worst;theoretical un-soundness. In this paper we present new and improved algorithms for theleast-squares NNMA problem; which are not only theoretically well-founded; but alsoovercome many of the deficiencies of other methods. In particular; we use non-diagonalgradient scaling to obtain rapid convergence. Our methods provide numerical results …,SIAM Data Mining,2007,109
Generative model-based clustering of directional data,Arindam Banerjee; Inderjit Dhillon; Joydeep Ghosh; Suvrit Sra,Abstract High dimensional directional data is becoming increasingly important incontemporary applications such as analysis of text and gene-expression data. A naturalmodel for multi-variate directional data is provided by the von Mises-Fisher (vMF) distributionon the unit hypersphere that is analogous to the multi-variate Gaussian distribution in R d. Inthis paper; we propose modeling complex directional data as a mixture of vMF distributions.We derive and analyze two variants of the Expectation Maximization (EM) framework forestimating the parameters of this mixture. We also propose two clustering algorithmscorresponding to these variants. An interesting aspect of our methodology is that thespherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a specialcase of both our algorithms. Thus; modeling text data by vMF distributions lends …,Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,2003,108
Jensen-bregman logdet divergence with application to efficient similarity search for covariance matrices,Anoop Cherian; Suvrit Sra; Arindam Banerjee; Nikolaos Papanikolopoulos,Covariance matrices have found success in several computer vision applications; includingactivity recognition; visual surveillance; and diffusion tensor imaging. This is because theyprovide an easy platform for fusing multiple features compactly. An important task in all ofthese applications is to compare two covariance matrices using a (dis) similarity function; forwhich the common choice is the Riemannian metric on the manifold inhabited by thesematrices. As this Riemannian manifold is not flat; the dissimilarities should take into accountthe curvature of the manifold. As a result; such distance computations tend to slow down;especially when the matrix dimensions are large or gradients are required. Further;suitability of the metric to enable efficient nearest neighbor retrieval is an importantrequirement in the contemporary times of big data analytics. To alleviate these difficulties …,IEEE transactions on pattern analysis and machine intelligence,2013,95
A short note on parameter approximation for von Mises-Fisher distributions: and a fast implementation of I s (x),Suvrit Sra,Abstract In high-dimensional directional statistics one of the most basic probabilitydistributions is the von Mises-Fisher (vMF) distribution. Maximum likelihood estimation forthe vMF distribution turns out to be surprisingly hard because of a difficult transcendentalequation that needs to be solved for computing the concentration parameter κ. This paper isa followup to the recent paper of Tanabe et al.(Comput Stat 22 (1): 145–157; 2007); whoexploited inequalities about Bessel function ratios to obtain an interval in which theparameter estimate for κ should lie; their observation lends theoretical validity to the heuristicapproximation of Banerjee et al.(JMLR 6: 1345–1382; 2005). Tanabe et al.(Comput Stat 22(1): 145–157; 2007) also presented a fixed-point algorithm for computing improvedapproximations for κ. However; their approximations require (potentially significant) …,Computational Statistics,2012,86
On variance reduction in stochastic gradient descent and its asynchronous variants,Sashank J Reddi; Ahmed Hefny; Suvrit Sra; Barnabas Poczos; Alexander J Smola,Abstract We study optimization algorithms based on variance reduction for stochasticgradientdescent (SGD). Remarkable recent progress has been made in thisdirectionthrough development of algorithms like SAG; SVRG; SAGA. These algorithmshavebeen shown to outperform SGD; both theoretically and empirically. However; asynchronousversions of these algorithms—a crucial requirement for modernlarge-scale applications—have not been studied. We bridge this gap by presentinga unifying framework that capturesmany variance reduction techniques. Subsequently; we propose an asynchronous algorithmgrounded in our framework; with fast convergence rates. An important consequence of ourgeneral approachis that it yields asynchronous versions of variance reduction algorithmssuch asSVRG; SAGA as a byproduct. Our method achieves near linear speedup in …,Advances in Neural Information Processing Systems,2015,83
Modeling data using directional distributions,Inderjit S Dhillon; Suvrit Sra,Abstract Traditionally multi-variate normal distributions have been the staple of datamodeling in most domains. For some domains; the model they provide is either inadequateor incorrect because of the disregard for the directional components of the data. We presenta generative model for data that is suitable for modeling directional data (as can arise in textand gene expression clustering). We use mixtures of von Mises-Fisher distributions to modelour data since the von Mises-Fisher distribution is the natural distribution for directional data.We derive an Expectation Maximization (EM) algorithm to find the maximum likelihoodestimates for the parameters of our mixture model; and provide various experimental resultsto evaluate the “correctness” of our formulation. In this paper we also provide some of themathematical background necessary to carry out all the derivations and to gain insight for …,*,2003,79
Stochastic Variance Reduction for Nonconvex Optimization,Sashank J Reddi; Ahmed Hefny; Suvrit Sra; Barnabás Póczós; Alex Smola,Abstract We study nonconvex finite-sum problems and analyze stochastic variance reducedgradient (SVRG) methods for them. SVRG and related methods have recently surged intoprominence for convex optimization given their edge over stochastic gradient descent(SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast; weobtain non-asymptotic rates of convergence of SVRG for nonconvex optimization; showingthat it is provably faster than SGD and gradient descent. We also analyze a subclass ofnonconvex problems on which SVRG attains linear convergence to the global optimum. Weextend our analysis to mini-batch variants; showing (theoretical) linear speedup due tominibatching in parallel settings.,International Conference on Machine Learning (ICML),2016,77
Positive definite matrices and the S-divergence,Suvrit Sra,Abstract: Hermitian positive definite (hpd) matrices form a self-dual convex cone whoseinterior is a Riemannian manifold of nonpositive curvature. The manifold view comes with anatural distance function but the conic view does not. Thus; drawing motivation from convexoptimization we introduce the S-divergence; a distance-like function on the cone of hpdmatrices. We study basic properties of the S-divergence and explore its connections to theRiemannian distance. In particular; we show that (i) its square-root is a distance; and (ii) itexhibits numerous nonpositive-curvature-like properties.,Proceedings of the American Mathematical Society,2016,75
A new metric on the manifold of kernel matrices with application to matrix geometric means,Suvrit Sra,Abstract Symmetric positive definite (spd) matrices are remarkably pervasive in a multitudeof scientific disciplines; including machine learning and optimization. We consider thefundamental task of measuring distances between two spd matrices; a task that is oftennontrivial whenever an application demands the distance function to respect the non-Euclidean geometry of spd matrices. Unfortunately; typical non-Euclidean distancemeasures such as the Riemannian metric $\riem (X; Y)=\frob {\log (X\inv {Y})} $; arecomputationally demanding and also complicated to use. To allay some of these difficulties;we introduce a new metric on spd matrices: this metric not only respects non-Euclideangeometry; it also offers faster computation than $\riem $ while being less complicated to use.We support our claims theoretically via a series of theorems that relate our metric to …,Advances in neural information processing systems,2012,70
Randomized nonlinear component analysis,David Lopez-Paz; Suvrit Sra; Alex Smola; Zoubin Ghahramani; Bernhard Schölkopf,Abstract Classical methods such as Principal Component Analysis (PCA) and CanonicalCorrelation Analysis (CCA) are ubiquitous in statistics. However; these techniques are onlyable to reveal linear relationships in data. Although nonlinear variants of PCA and CCAhave been proposed; these are computationally prohibitive in the large scale. In a separatestrand of recent research; randomized methods have been proposed to construct featuresthat help reveal nonlinear patterns in data. For basic tasks such as regression orclassification; random features exhibit little or no loss in performance; while achieving drasticsavings in computational requirements.,International Conference on Machine Learning,2014,68
Efficient similarity search for covariance matrices via the Jensen-Bregman LogDet divergence,Anoop Cherian; Suvrit Sra; Arindam Banerjee; Nikolaos Papanikolopoulos,Covariance matrices provide compact; informative feature descriptors for use in severalcomputer vision applications; such as people-appearance tracking; diffusion-tensor imaging;activity recognition; among others. A key task in many of these applications is to comparedifferent covariance matrices using a (dis) similarity function. A natural choice here is theRiemannian metric corresponding to the manifold inhabited by covariance matrices. Butcomputations involving this metric are expensive; especially for large matrices and evenmore so; in gradient-based algorithms. To alleviate these difficulties; we advocate a noveldissimilarity measure for covariance matrices: the Jensen-Bregman LogDet Divergence.This divergence enjoys several useful theoretical properties; but its greatest benefits are:(i)lower computational costs (compared to standard approaches); and (ii) amenability for …,Computer Vision (ICCV); 2011 IEEE International Conference on,2011,65
Nonnegative matrix approximation: Algorithms and applications,Suvrit Sra; Inderjit S Dhillon,Abstract Low dimensional data representations are crucial to numerous applications inmachine learning; statistics; and signal processing. Nonnegative matrix approximation(NNMA) is a method for dimensionality reduction that respects the nonnegativity of the inputdata while constructing a low-dimensional approximation. NNMA has been used in amultitude of applications; though without commensurate theoretical development. In thisreport we describe generic methods for minimizing generalized divergences between theinput and its low rank approximant. Some of our general methods are even extensible toarbitrary convex penalties. Our methods yield efficient multiplicative iterative schemes forsolving the proposed problems. We also consider interesting extensions such as the use ofpenalty functions; non-linear relationships via “link” functions; weighted errors; and multi …,*,2006,63
Combining spectral and probabilistic clustering,*,Data clustering is performed by executing a spectral technique; embedded within aprobabilistic technique. In one embodiment; the probabilistic technique is performed by agenerative model; and the spectral technique is performed within the generative model. Inanother embodiment; the probabilistic technique is performed by an aspect model; and thespectral technique is performed within the aspect model.,*,2010,60
Fast Newton-type methods for total variation regularization,Alvaro Barbero; Suvrit Sra,Abstract Numerous applications in statistics; signal processing; and machine learningregularize using Total Variation (TV) penalties. We study anisotropic (l1-based) TV and alsoa related l2-norm variant. We consider for both variants associated (1D) proximity operators;which lead to challenging optimization problems. We solve these problems by developingNewton-type methods that outperform the state-of-the-art algorithms. More importantly; our1D-TV algorithms serve as building blocks for solving the harder task of computing 2-(andhigher)-dimensional TV proximity. We illustrate the computational benefits of our methods byapplying them to several applications:(i) image denoising;(ii) image deconvolution (byplugging in our TV solvers into publicly available software); and (iii) four variants of fused-lasso. The results show large speedups—and to support our claims; we provide software …,Proceedings of the 28th International Conference on Machine Learning (ICML-11),2011,57
Tackling box-constrained optimization via a new projected quasi-Newton approach,Dongmin Kim; Suvrit Sra; Inderjit S Dhillon,Numerous scientific applications across a variety of fields depend on box-constrainedconvex optimization. Box-constrained problems therefore continue to attract researchinterest. We address box-constrained (strictly convex) problems by deriving two new quasi-Newton algorithms. Our algorithms are positioned between the projected-gradient [JBRosen; J. SIAM; 8 (1960); pp. 181–217] and projected-Newton [DP Bertsekas; SIAM J.Control Optim.; 20 (1982); pp. 221–246] methods. We also prove their convergence under asimple Armijo step-size rule. We provide experimental results for two particular box-constrained problems: nonnegative least squares (NNLS); and nonnegative Kullback–Leibler (NNKL) minimization. For both NNLS and NNKL our algorithms performcompetitively as compared to well-established methods on medium-sized problems; for …,SIAM Journal on Scientific Computing,2010,55
Scalable nonconvex inexact proximal splitting,Suvrit Sra,Abstract We study large-scale; nonsmooth; nonconconvex optimization problems. Inparticular; we focus on nonconvex problems with\emph {composite} objectives. This class ofproblems includes the extensively studied convex; composite objective problems as aspecial case. To tackle composite nonconvex problems; we introduce a powerful newframework based on asymptotically\emph {nonvanishing} errors; avoiding the commonconvenient assumption of eventually vanishing errors. Within our framework we derive bothbatch and incremental nonconvex proximal splitting algorithms. To our knowledge; ourframework is first to develop and analyze incremental\emph {nonconvex} proximal-splittingalgorithms; even if we disregard the ability to handle nonvanishing errors. We illustrate ourtheoretical framework by showing how it applies to difficult large-scale; nonsmooth; and …,Advances in Neural Information Processing Systems,2012,54
Fast Projection-Based Methods for the Least Squares Nonnegative Matrix Approximation Problem,Dongmin Kim; Suvrit Sra; Inderjit Dhillon,Abstract Nonnegative matrix approximation (NNMA) is a popular matrix decompositiontechnique that has proven to be useful across a diverse variety of fields with applicationsranging from document analysis and image processing to bioinformatics and signalprocessing. Over the years; several algorithms for NNMA have been proposed; eg Lee andSeung's multiplicative updates; alternating least squares (ALS); and gradient descent-basedprocedures. However; most of these procedures suffer from either slow convergence;numerical instability; or at worst; serious theoretical drawbacks. In this paper; we develop anew and improved algorithmic framework for the least-squares NNMA problem; which is notonly theoretically well-founded; but also overcomes many deficiencies of other methods. Ourframework readily admits powerful optimization techniques and as concrete realizations …,Statistical Analysis and Data Mining,2008,51
Riemannian sparse coding for positive definite matrices,Anoop Cherian; Suvrit Sra,Abstract Inspired by the great success of sparse coding for vector valued data; our goal is torepresent symmetric positive definite (SPD) data matrices as sparse linear combinations ofatoms from a dictionary; where each atom itself is an SPD matrix. Since SPD matrices followa non-Euclidean (in fact a Riemannian) geometry; existing sparse coding techniques forEuclidean data cannot be directly extended. Prior works have approached this problem bydefining a sparse coding loss function using either extrinsic similarity measures (such as thelog-Euclidean distance) or kernelized variants of statistical measures (such as the Steindivergence; Jeffrey's divergence; etc.). In contrast; we propose to use the intrinsicRiemannian distance on the manifold of SPD matrices. Our main contribution is a novelmathematical model for sparse coding of SPD matrices; we also present a …,European Conference on Computer Vision,2014,50
Modular proximal optimization for multidimensional total-variation regularization,Álvaro Barbero; Suvrit Sra,Abstract: One of the most frequently used notions of" structured sparsity" is that of sparse(discrete) gradients; a structure typically elicited through\emph {Total-Variation (TV)}regularizers. This paper focuses on anisotropic TV-regularizers; in particular on $\ell_p $-norm\emph {weighted TV regularizers} for which it develops efficient algorithms to computethe corresponding proximity operators. Our algorithms enable one to scalably incorporateTV regularization of vector; matrix; or tensor data into a proximal convex optimizationsolvers. For the special case of vectors; we derive and implement a highly efficient weighted1D-TV solver. This solver provides a backbone for subsequently handling the more complextask of higher-dimensional (two or more) TV by means of a modular proximal optimizationapproach. We present numerical experiments that demonstrate how our 1D-TV solver …,arXiv preprint arXiv:1411.0589,2014,44
Reflection methods for user-friendly submodular optimization,Stefanie Jegelka; Francis Bach; Suvrit Sra,Abstract Recently; it has become evident that submodularity naturally captures widelyoccurring concepts in machine learning; signal processing and computer vision. Inconsequence; there is need for efficient optimization procedures for submodular functions; inparticular for minimization problems. While general submodular minimization is challenging;we propose a new approach that exploits existing decomposability of submodular functions.In contrast to previous approaches; our method is neither approximate; nor impractical; nordoes it need any cumbersome parameter tuning. Moreover; it is easy to implement andparallelize. A key component of our approach is a formulation of the discrete submodularminimization problem as a continuous best approximation problem. It is solved through asequence of reflections and its solution can be automatically thresholded to obtain an …,Advances in Neural Information Processing Systems,2013,44
Generalized dictionary learning for symmetric positive definite matrices with application to nearest neighbor retrieval,Suvrit Sra; Anoop Cherian,Abstract We introduce Generalized Dictionary Learning (GDL); a simple but practicalframework for learning dictionaries over the manifold of positive definite matrices. Weillustrate GDL by applying it to Nearest Neighbor (NN) retrieval; a task of fundamentalimportance in disciplines such as machine learning and computer vision. GDL distinguishesitself from traditional dictionary learning approaches by explicitly taking into account themanifold structure of the data. In particular; GDL allows performing “sparse coding” ofpositive definite matrices; which enables better NN retrieval. Experiments on severalcovariance matrix datasets show that GDL achieves performance rivaling state-of-the-arttechniques.,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2011,44
Fast projections onto ℓ 1; q-norm balls for grouped feature selection,Suvrit Sra,Abstract Joint sparsity is widely acknowledged as a powerful structural cue for performingfeature selection in setups where variables are expected to demonstrate “grouped”behavior. Such grouped behavior is commonly modeled by Group-Lasso or Multitask Lasso-type problems; where feature selection is effected via ℓ 1; q-mixed-norms. Several particularformulations for modeling groupwise sparsity have received substantial attention in theliterature; and in some cases; efficient algorithms are also available. Surprisingly; forconstrained formulations of fundamental importance (eg; regression with an ℓ 1;∞-normconstraint); highly scalable methods seem to be missing. We address this deficiency bypresenting a method based on spectral projected-gradient (SPG) that can tackle ℓ 1; q-constrained convex regression problems. The most crucial component of our method is …,Joint European Conference on Machine Learning and Knowledge Discovery in Databases,2011,44
Online blind deconvolution for astronomical imaging,Stefan Harmeling; Michael Hirsch; Suvrit Sra; Berhard Schölkopf,Atmospheric turbulences blur astronomical images taken by earth-based telescopes. Takingmany short-time exposures in such a situation provides noisy images of the same object;where each noisy image has a different blur. Commonly astronomers apply a techniquecalled “Lucky Imaging” that selects a few of the recorded frames that fulfill certain criteria;such as reaching a certain peak intensity (“Strehl ratio”). The selected frames are thenaveraged to obtain a better image. In this paper we introduce and analyze a new methodthat exploits all the frames and generates an improved image in an online fashion. Our initialexperiments with controlled artificial data and real-world astronomical datasets yieldspromising results.,Computational Photography (ICCP); 2009 IEEE International Conference on,2009,44
Projected Newton-type Methods in Machine Learning,Mark Schmidt; Dongmin Kim; Suvrit Sra,We consider projected Newton-type methods for solving large-scale optimization problemsarising in machine learning and related fields. We first introduce an algorithmic frameworkfor projected Newton-type methods by reviewing a canonical projected (quasi-) Newtonmethod. This method; while conceptually pleasing; has a high computation cost per iteration.Thus; we discuss two variants that are more scalable: two-metric projection and inexactprojection methods. Finally; we show how to apply the Newton-type framework to handlenonsmooth objectives. Examples are provided throughout the chapter to illustrate machinelearning applications of our framework.,*,2011,43
The multivariate Watson distribution: Maximum-likelihood estimation and other aspects,Suvrit Sra; Dmitrii Karp,Abstract This paper studies fundamental aspects of modelling data using multivariateWatson distributions. Although these distributions are natural for modelling axially symmetricdata (ie; unit vectors where±x are equivalent); for high-dimensions using them can bedifficult—largely because for Watson distributions even basic tasks such as maximum-likelihood are numerically challenging. To tackle the numerical difficulties someapproximations have been derived. But these are either grossly inaccurate in high-dimensions [KV Mardia; P. Jupp; Directional Statistics; second ed.; John Wiley & Sons; 2000]or when reasonably accurate [A. Bijral; M. Breitenbach; GZ Grudic; Mixture of Watsondistributions: a generative model for hyperspherical embeddings; in: Artificial Intelligenceand Statistics; AISTATS 2007; 2007; pp. 35–42]; they lack theoretical justification. We …,Journal of Multivariate Analysis,2013,40
Multiframe blind deconvolution; super-resolution; and saturation correction via incremental EM,Stefan Harmeling; Suvrit Sra; Michael Hirsch; Bernhard Schölkopf,We formulate the multiframe blind deconvolution problem in an incremental expectationmaximization (EM) framework. Beyond deconvolution; we show how to use the sameframework to address:(i) super-resolution despite noise and unknown blurring;(ii) saturation-correction of overexposed pixels that confound image restoration. The abundance of dataallows us to address both of these without using explicit image or blur priors. The end resultis a simple but effective algorithm with no hyperparameters. We apply this algorithm to real-world images from astronomy and to super resolution tasks: for both; our algorithm yieldsincreased resolution and deconvolved images simultaneously.,Image Processing (ICIP); 2010 17th IEEE International Conference on,2010,40
Approximation algorithms for tensor clustering,Stefanie Jegelka; Suvrit Sra; Arindam Banerjee,Abstract We present the first (to our knowledge) approximation algorithm for tensorclustering—a powerful generalization to basic 1D clustering. Tensors are increasinglycommon in modern applications dealing with complex heterogeneous data and clusteringthem is a fundamental tool for data analysis and pattern discovery. Akin to their 1D cousins;common tensor clustering formulations are NP-hard to optimize. But; unlike the 1D case; noapproximation algorithms seem to be known. We address this imbalance and build onrecent co-clustering work to derive a tensor clustering algorithm with approximationguarantees; allowing metrics and divergences (eg; Bregman) as objective functions.Therewith; we answer two open questions by Anagnostopoulos et al.(2008). Our analysisyields a constant approximation factor independent of data size; a worst-case example …,International Conference on Algorithmic Learning Theory,2009,39
A non-monotonic method for large-scale non-negative least squares,Dongmin Kim; Suvrit Sra; Inderjit S Dhillon,We present a new algorithm for solving the non-negative least-squares (NNLS) problem.Our algorithm extends the unconstrained quadratic optimization algorithm of Barzilai andBorwein (BB)[J. Barzilai and JM Borwein; Two-Point Step Size Gradient Methods. IMA J.Numer. Anal. 1988.] to handle nonnegativity constraints. Our extension differs from otherconstrained BB variants in simple but crucial aspects; the most notable being ourmodification to the BB stepsize itself. Our stepsize computation takes into account thenonnegativity constraints; and is further refined by a stepsize scaling strategy. Thesechanges; in combination with orthogonal projections onto the nonnegative orthant; yield aneffective NNLS algorithm. We compare our algorithm with several competing approaches;including established bound-constrained solvers; popular BB-based methods; and also a …,Optimization Methods and Software,2013,38
A new projected quasi-newton approach for the nonnegative least squares problem,Dongmin Kim; Suvrit Sra; Inderjit S Dhillon,Abstract Constrained least squares estimation lies at the heart of many applications in fieldsas diverse as statistics; psychometrics; signal processing; or even machine learning.Nonnegativity requirements on the model variables are amongst the simplest constraintsthat arise naturally; and the corresponding least-squares problem is called NonnegativeLeast Squares or NNLS. In this paper we present a new; efficient; and scalable Quasi-Newton-type method for solving the NNLS problem; improving on several previousapproaches and leading to a superlinearly convergent method. We show experimentalresults comparing our method to well-known methods for solving the NNLS problem. Ourmethod significantly outperforms other methods; especially as the problem size becomeslarger.,*,2006,35
Online multi-frame blind deconvolution with super-resolution and saturation correction,Michael Hirsch; Stefan Harmeling; Suvrit Sra; Bernhard Schölkopf,Astronomical images taken by ground-based telescopes suffer degradation due toatmospheric turbulence. This degradation can be tackled by costly hardware-basedapproaches such as adaptive optics; or by sophisticated software-based methods such aslucky imaging; speckle imaging; or multi-frame deconvolution. Software-based methodsprocess a sequence of images to reconstruct a deblurred high-quality image. However;existing approaches are limited in one or several aspects:(i) they process all images in batchmode; which for thousands of images is prohibitive;(ii) they do not reconstruct a super-resolved image; even though an image sequence often contains enough information;(iii)they are unable to deal with saturated pixels; and (iv) they are usually non-blind; ie; theyassume the blur kernels to be known. In this paper we present a new method for multi …,Astronomy & Astrophysics,2011,34
A scalable trust-region algorithm with application to mixed-norm regression,Dongmin Kim; Suvrit Sra; Inderjit S Dhillon,Abstract We present a new algorithm for minimizing a convex loss-function subject toregularization. Our framework applies to numerous problems in machine learning andstatistics; notably; for sparsity-promoting regularizers such as l1 or l1;∞ norms; it enablesefficient computation of sparse solutions. Our approach is based on the trust-regionframework with nonsmooth objectives; which allows us to build on known results to provideconvergence analysis. We avoid the computational overheads associated with theconventional Hessian approximation used by trust-region methods by instead using asimple separable quadratic approximation. This approximation also enables use of proximityoperators for tackling nonsmooth regularizers. We illustrate the versatility of our resultingalgorithm by specializing it to three mixed-norm regression problems: group lasso [36] …,Proceedings of the 27th International Conference on Machine Learning (ICML-10),2010,34
Fast Stochastic Methods for Nonsmooth Nonconvex Optimization,Sashank J Reddi; Suvrit Sra; Barnabas Poczos; Alex Smola,*,Advances in Neural Information Processing Systems,2016,33
Towards an optimal stochastic alternating direction method of multipliers,Suvrit Sra; Samaneh Azadi,Abstract We study regularized stochastic convex optimization subject to linear equalityconstraints. This class of problems was recently also studied by Ouyang et al.(2013) andSuzuki (2013); both introduced similar stochastic alternating direction method of multipliers(SADMM) algorithms. However; the analysis of both papers led to suboptimal convergencerates. This paper presents two new SADMM methods:(i) the first attains the minimax optimalrate of O (1/k) for nonsmooth strongly-convex stochastic problems; while (ii) the secondprogresses towards an optimal rate by exhibiting an O (1/k2) rate for the smooth part. Wepresent several experiments with our new methods; the results indicate improvedperformance over competing ADMM methods.,International Conference on Machine Learning,2014,33
Conic geometric optimization on the manifold of positive definite matrices,Suvrit Sra; Reshad Hosseini,We develop geometric optimization on the manifold of Hermitian positive definite (HPD)matrices. In particular; we consider optimizing two types of cost functions:(i) geodesicallyconvex (g-convex) and (ii) log-nonexpansive (LN). G-convex functions are nonconvex in theusual Euclidean sense but convex along the manifold and thus allow global optimization. LNfunctions may fail to be even g-convex but still remain globally optimizable due to theirspecial structure. We develop theoretical tools to recognize and generate g-convex functionsas well as cone theoretic fixed-point optimization algorithms. We illustrate our techniques byapplying them to maximum-likelihood parameter estimation for elliptically contoureddistributions (a rich class that substantially generalizes the multivariate normal distribution).We compare our fixed-point algorithms with sophisticated manifold optimization methods …,SIAM Journal on Optimization,2015,31
Diversity networks,Zelda Mariet; Suvrit Sra,*,International Conference on Learning Representations (ICLR),2016,29
Entropic metric alignment for correspondence problems,Justin Solomon; Gabriel Peyré; Vladimir G Kim; Suvrit Sra,Abstract Many shape and image processing tools rely on computation of correspondencesbetween geometric domains. Efficient methods that stably extract" soft" matches in thepresence of diverse geometric structures have proven to be valuable for shape retrieval andtransfer of labels or semantic information. With these applications in mind; we present analgorithm for probabilistic correspondence that optimizes an entropy-regularized Gromov-Wasserstein (GW) objective. Built upon recent developments in numerical optimaltransportation; our algorithm is compact; provably convergent; and applicable to anygeometric domain expressible as a metric measure matrix. We provide comprehensiveexperiments illustrating the convergence and applicability of our algorithm to a variety ofgraphics tasks. Furthermore; we expand entropic GW correspondence to a framework for …,ACM Transactions on Graphics (TOG),2016,22
Denoising sparse noise via online dictionary learning,Anoop Cherian; Suvrit Sra; Nikolaos Papanikolopoulos,The idea of learning overcomplete dictionaries based on the paradigm of compressivesensing has found numerous applications; among which image denoising is considered oneof the most successful. But many state-of-the-art denoising techniques inherently assumethat the signal noise is Gaussian. We instead propose to learn overcomplete dictionarieswhere the signal is allowed to have both Gaussian and (sparse) Laplacian noise. Dictionarylearning in this setting leads to a difficult non-convex optimization problem; which is furtherexacerbated by large input datasets. We tackle these difficulties by developing an efficientonline algorithm that scales to data size. To assess the efficacy of our model; we apply it todictionary learning for data that naturally satisfy our noise model; namely; Scale InvariantFeature Transform (SIFT) descriptors. For these data; we measure performance of the …,Acoustics; Speech and Signal Processing (ICASSP); 2011 IEEE International Conference on,2011,22
Fast DPP Sampling for Nyström with Application to Kernel Methods,Chengtao Li; Stefanie Jegelka; Suvrit Sra,Abstract: The Nystr\" om method has long been popular for scaling up kernel methods. Itstheoretical guarantees and empirical performance rely critically on the quality of thelandmarks selected. We study landmark selection for Nystr\" om using Determinantal PointProcesses (DPPs); discrete probability models that allow tractable generation of diversesamples. We prove that landmarks selected via DPPs guarantee bounds on approximationerrors; subsequently; we analyze implications for kernel ridge regression. Contrary to priorreservations due to cubic complexity of DPPsampling; we show that (under certainconditions) Markov chain DPP sampling requires only linear time in the size of the data. Wepresent several empirical results that support our theoretical analysis; and demonstrate thesuperior performance of DPP-based landmark selection compared with existing …,International Conference on Machine Learning (ICML),2016,21
Sparse nonnegative matrix approximation: new formulations and algorithms,Rashish Tandon; Suvrit Sra,Abstract. We introduce several new formulations for sparse nonnegative matrixapproximation. Subsequently; we solve these formulations by developing genericalgorithms. Further; to help selecting a particular sparse formulation; we briefly discuss theinterpretation of each formulation. Finally; preliminary experiments are presented to illustratethe behavior of our formulations and algorithms.,MPI Technical Report,2010,21
The metric nearness problem,Justin Brickell; Inderjit S Dhillon; Suvrit Sra; Joel A Tropp,Metric nearness refers to the problem of optimally restoring metric properties to distancemeasurements that happen to be nonmetric due to measurement errors or otherwise. Metricdata can be important in various settings; for example; in clustering; classification; metric-based indexing; query processing; and graph theoretic approximation algorithms. Thispaper formulates and solves the metric nearness problem: Given a set of pairwisedissimilarities; find a “nearest” set of distances that satisfy the properties of a metric—principally the triangle inequality. For solving this problem; the paper develops efficienttriangle fixing algorithms that are based on an iterative projection method. An intriguingaspect of the metric nearness problem is that a special case turns out to be equivalent to theall pairs shortest paths problem. The paper exploits this equivalence and develops a new …,SIAM Journal on Matrix Analysis and Applications,2008,21
Riemannian SVRG: fast stochastic optimization on riemannian manifolds,Hongyi Zhang; Sashank J Reddi; Suvrit Sra,Abstract We study optimization of finite sums of\emph {geodesically} smooth functions onRiemannian manifolds. Although variance reduction techniques for optimizing finite-sumshave witnessed tremendous attention in the recent years; existing work is limited to vectorspace problems. We introduce\emph {Riemannian SVRG}(\rsvrg); a new variance reducedRiemannian optimization method. We analyze\rsvrg for both geodesically\emph {convex}and\emph {nonconvex}(smooth) functions. Our analysis reveals that\rsvrg inheritsadvantages of the usual SVRG method; but with factors depending on curvature of themanifold that influence its convergence. To our knowledge;\rsvrg is the first\emph {provablyfast} stochastic Riemannian method. Moreover; our paper presents the first non-asymptoticcomplexity analysis (novel even for the batch setting) for nonconvex Riemannian …,Advances in Neural Information Processing Systems,2016,20
Efficient large scale linear programming support vector machines,Suvrit Sra,Abstract This paper presents a decomposition method for efficiently constructing ℓ 1-normSupport Vector Machines (SVMs). The decomposition algorithm introduced in this paperpossesses many desirable properties. For example; it is provably convergent; scales well tolarge datasets; is easy to implement; and can be extended to handle support vectorregression and other SVM variants. We demonstrate the efficiency of our algorithm bytraining on (dense) synthetic datasets of sizes up to 20 million points (in ℝ 32). The resultsshow our algorithm to be several orders of magnitude faster than a previously publishedmethod for the same task. We also present experimental results on real data sets—ourmethod is seen to be not only very fast; but also highly competitive against the leading SVMimplementations.,European Conference on Machine Learning,2006,19
First-order methods for geodesically convex optimization,Hongyi Zhang; Suvrit Sra,Abstract Geodesic convexity generalizes the notion of (vector space) convexity to nonlinearmetric spaces. But unlike convex optimization; geodesically convex (g-convex) optimizationis much less developed. In this paper we contribute to the understanding of g-convexoptimization by developing iteration complexity analysis for several first-order algorithms onHadamard manifolds. Specifically; we prove upper bounds for the global complexity ofdeterministic and stochastic (sub) gradient methods for optimizing smooth and nonsmooth g-convex functions; both with and without strong g-convexity. Our analysis also reveals howthe manifold geometry; especially sectional curvature; impacts convergence rates. To thebest of our knowledge; our work is the first to provide global complexity analysis for first-order algorithms for general g-convex optimization.,Conference on Learning Theory (COLT),2016,18
Asynchronous Parallel Block-Coordinate Frank-Wolfe,Y-X Wang; Veeranjaneyulu Sadhanala; Wei Dai; Willie Neiswanger; Suvrit Sra; Eric P Xing,Abstract We develop mini-batched parallel Frank-Wolfe (conditional gradient) methods forsmooth convex optimization subject to block-separable constraints. Our work includes thebasic (batch) Frank-Wolfe algorithm as well as the recently proposed Block-CoordinateFrank-Wolfe (BCFW) method [22] as special cases. Our algorithm permits asynchronousupdates within the minibatch; and is robust to stragglers and faulty worker threads. Ouranalysis reveals how the potential speedups over BCFW depend on the minibatch size andhow one can provably obtain large problem dependent speedups. We present severalexperiments to indicate empirical behavior of our methods; obtaining significant speedupsover competing state-of-the-art (and synchronous) methods on structural SVMs.,International Conference on Machine Learning,2016,18
Efficient sampling for k-determinantal point processes,Chengtao Li; Stefanie Jegelka; Suvrit Sra,Abstract: Determinantal Point Processes (DPPs) are elegant probabilistic models ofrepulsion and diversity over discrete sets of items. But their applicability to large sets ishindered by expensive cubic-complexity matrix operations for basic tasks such as sampling.In light of this; we propose a new method for approximate sampling from discrete $ k $-DPPs. Our method takes advantage of the diversity property of subsets sampled from a DPP;and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter;it efficiently samples subsets based on the constructed coresets. As opposed to previousapproaches; our algorithm aims to minimize the total variation distance to the originaldistribution. Experiments on both synthetic and real datasets indicate that our samplingalgorithm works efficiently on large data sets; and yields more accurate samples than …,Artificial Intelligence and Statistics (AISTATS 2016),2015,18
Geometric Mean Metric Learning,Pourya Habib Zadeh; Reshad Hosseini; Suvrit Sra,Abstract We revisit the task of learning a Euclidean metric from data. We approach thisproblem from first principles and formulate it as a surprisingly simple optimization problem.Indeed; our formulation even admits a closed form solution. This solution possesses severalvery attractive properties:(i) an innate geometric appeal through the Riemannian geometryof positive definite matrices;(ii) ease of interpretability; and (iii) computational speed severalorders of magnitude faster than the widely used LMNN and ITML methods. Furthermore; onstandard benchmark datasets; our closed-form solution consistently attains higherclassification accuracy.,International Conference on Machine Learning (ICML),2016,17
Fixed-point algorithms for learning determinantal point processes,Zelda Mariet; Suvrit Sra,Abstract Determinantal point processes (DPPs) offer an elegant tool for encodingprobabilities over subsets of a ground set. Discrete DPPs are parametrized by a positivesemidefinite matrix (called the DPP kernel); and estimating this kernel is key to learningDPPs from observed data. We consider the task of learning the DPP kernel; and develop forit a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefitsover previous approaches:(a) it is much simpler;(b) it yields equally good and sometimeseven better local maxima; and (c) it runs an order of magnitude faster on large problems. Wepresent experimental results on both real and simulated data to illustrate the numericalperformance of our technique.,International Conference on Machine Learning,2015,17
Geometric optimisation on positive definite matrices for elliptically contoured distributions,Suvrit Sra; Reshad Hosseini,Abstract Hermitian positive definite matrices (HPD) recur throughout statistics and machinelearning. In this paper we develop\emph {geometric optimisation} for globally optimisingcertain nonconvex loss functions arising in the modelling of data via elliptically contoureddistributions (ECDs). We exploit the remarkable structure of the convex cone of positivedefinite matrices which allows one to uncover hidden geodesic convexity of objectivefunctions that are nonconvex in the ordinary Euclidean sense. Going even beyond manifoldconvexity we show how further metric properties of HPD matrices can be exploited toglobally optimise several ECD log-likelihoods that are not even geodesic convex. Wepresent key results that help recognise this geometric structure; as well as obtain efficientfixed-point algorithms to optimise the corresponding objective functions. To our …,Advances in Neural Information Processing Systems,2013,16
Adadelay: Delay adaptive distributed stochastic optimization,Suvrit Sra; Adams Wei Yu; Mu Li; Alex Smola,Abstract We develop distributed stochastic convex optimization algorithms under a delayedgradient model in which server nodes update parameters and worker nodes computestochastic (sub) gradients. Our setup is motivated by the behavior of real-world distributedcomputation systems; in particular; we analyze a setting wherein worker nodes can bedifferently slow at different times. In contrast to existing approaches; we do not impose aworst-case bound on the delays experienced but rather allow the updates to be sensitive tothe actual delays experienced. This sensitivity allows use of larger stepsizes; which can helpspeed up initial convergence without having to wait too long for slower machines; the globalconvergence rate is still preserved. We experiment with different delay patterns; and obtainnoticeable improvements for large-scale real datasets with billions of examples and …,Artificial Intelligence and Statistics,2016,15
Non-monotonic poisson likelihood maximization,Suvrit Sra; Dongmin Kim; Bernhard Schölkopf,Abstract. This report summarizes the theory and some main applications of a new non-monotonic algorithm for maximizing a Poisson Likelihood; which for Positron EmissionTomography (PET) is equivalent to minimizing the associated Kullback-Leibler Divergence;and for Transmission Tomography is similar to maximizing the dual of a maximum entropyproblem. We call our method non-monotonic maximum likelihood (NMML) and show itsapplication to different problems such as tomography and image restoration. We discusssome theoretical properties such as convergence for our algorithm. Our experimental resultsindicate that speedups obtained via our non-monotonic methods are substantial.,Max Planck Institute for Biological Cybernetics; Tech. Rep,2008,15
Incrementally building aspect models,*,The claimed subject matter relates to an unsupervised incremental learning framework; andin particular; to the creation and utilization of an unsupervised incremental learningframework that facilitates object discovery; clustering; characterization and/or grouping.Such an unsupervised incremental learning framework; once created; can thereafter beemployed to incrementally estimate a latent variable model through the utilization of spectraland/or probabilistic models in order to incrementally cluster; discover; group and/orcharacterize tightly knit themes/topics within document sets and/or streams; thus leading tothe generation of a set of themes/topics that better correlate with human perceptual labelingschemes.,*,2008,15
Incremental aspect models for mining document streams,Arun C Surendran; Suvrit Sra,Abstract In this paper we introduce a novel approach for incrementally building aspectmodels; and use it to dynamically discover underlying themes from document streams.Using the new approach we present an application which we call “query-line tracking” ie; weautomatically discover and summarize different themes or stories that appear over time; andthat relate to a particular query. We present evaluation on news corpora to demonstrate thestrength of our method for both query-line tracking; online indexing and clustering.,European Conference on Principles of Data Mining and Knowledge Discovery,2006,15
Fast incremental method for smooth nonconvex optimization,Sashank J Reddi; Suvrit Sra; Barnabás Póczos; Alex Smola,We analyze a fast incremental aggregated gradient method for optimizing nonconvexproblems of the form minΣ ifi (x). Specifically; we analyze the SAGA algorithm within anIncremental First-order Oracle framework; and show that it converges to a stationary pointprovably faster than both gradient descent and stochastic gradient descent. We also discussa Polyak's special class of nonconvex problems for which SAGA converges at a linear rate tothe global optimum. Finally; we analyze the practically valuable regularized and minibatchvariants of SAGA. To our knowledge; this paper presents the first analysis of fastconvergence for an incremental aggregated gradient method for nonconvex problems.,Decision and Control (CDC); 2016 IEEE 55th Conference on,2016,14
Stochastic frank-wolfe methods for nonconvex optimization,Sashank J Reddi; Suvrit Sra; Barnabás Póczos; Alex Smola,We study Frank-Wolfe methods for nonconvex stochastic and finite-sum optimizationproblems. Frank-Wolfe methods (in the convex case) have gained tremendous recentinterest in machine learning and optimization due to their projection-free property and theirability to exploit structured constraints. However; our understanding of these algorithms inthe nonconvex setting is fairly limited. In this paper; we propose nonconvex stochastic Frank-Wolfe methods and analyze their convergence properties. Furthermore; for objectivefunctions that decompose into a finite-sum; we leverage ideas from variance reduction forconvex optimization to obtain new variance reduced nonconvex Frank-Wolfe methods thathave provably faster convergence than the classical Frank-Wolfe method.,Communication; Control; and Computing (Allerton); 2016 54th Annual Allerton Conference on,2016,14
Large-scale randomized-coordinate descent methods with non-separable linear constraints,Sashank Reddi; Ahmed Hefny; Carlton Downey; Avinava Dubey; Suvrit Sra,Abstract: We develop randomized (block) coordinate descent (CD) methods for linearlyconstrained convex optimization. Unlike most CD methods; we do not assume theconstraints to be separable; but let them be coupled linearly. To our knowledge; ours is thefirst CD method that allows linear coupling constraints; without making the global iterationcomplexity have an exponential dependence on the number of constraints. We presentalgorithms and analysis for four key problem scenarios:(i) smooth;(ii) smooth+ nonsmoothseparable;(iii) asynchronous parallel; and (iv) stochastic. We illustrate empirical behavior ofour algorithms by simulation experiments. Subjects: Optimization and Control (math. OC);Machine Learning (stat. ML) Cite as: arXiv: 1409.2617 [math. OC](or arXiv: 1409.2617 v5[math. OC] for this version) Submission history From: Ahmed Hefny [view email][v1] Tue; 9 …,arXiv preprint arXiv:1409.2617,2014,13
Efficient Nearest Neighbors via Robust Sparse Hashing,Anoop Cherian; Suvrit Sra,This paper presents a new nearest neighbor (NN) retrieval framework: robust sparsehashing (RSH). Our approach is inspired by the success of dictionary learning for sparsecoding. Our key idea is to sparse code the data using a learned dictionary; and then togenerate hash codes out of these sparse codes for accurate and fast NN retrieval. But; directapplication of sparse coding to NN retrieval poses a technical difficulty: when data are noisyor uncertain (which is the case with most real-world data sets); for a query point; an exactmatch of the hash code generated from the sparse code seldom happens; thereby breakingthe NN retrieval. Borrowing ideas from robust optimization theory; we circumvent thisdifficulty via our novel robust dictionary learning and sparse coding framework called RSH;by learning dictionaries on the robustified counterparts of the perturbed data points. The …,IEEE Transactions on Image Processing,2014,13
Convex perturbations for scalable semidefinite programming,Brian Kulis; Suvrit Sra; Inderjit Dhillon,Abstract Many important machine learning problems are modeled and solved viasemidefinite programs; examples include metric learning; nonlinear embedding; and certainclustering problems. Often; off-the-shelf software is invoked for the associated optimization;which can be inappropriate due to excessive computational and storage requirements. Inthis paper; we introduce the use of convex perturbations for solving semidefinite programs(SDPs); and for a specific perturbation we derive an algorithm that has several advantagesover existing techniques: a) it is simple; requiring only a few lines of Matlab; b) it is a first-order method; and thereby scalable; and c) it can easily exploit the structure of a given SDP(eg; when the constraint matrices are low-rank; a situation common to several machinelearning SDPs). A pleasant byproduct of our method is a fast; kernelized version of the …,Artificial Intelligence and Statistics,2009,13
Row-action methods for compressed sensing,Suvrit Sra; Joel A Tropp,Compressed sensing uses a small number of random; linear measurements to acquire asparse signal. Nonlinear algorithms; such as 11 minimization; are used to reconstruct thesignal from the measured data. This paper proposes row-action methods as a computationalapproach to solving the 11 optimization problem. This paper presents a specific row-actionmethod and provides extensive empirical evidence that it is an effective technique for signalreconstruction. This approach offers several advantages over interior-point methods;including minimal storage and computational requirements; scalability; and robustness,Acoustics; Speech and Signal Processing; 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on,2006,13
Clustering on hyperspheres using Expectation Maximization,Arindam Banerjee; Inderjit S Dhillon; Joydeep Ghosh; Suvrit Sra,*,Department of Computer Sciences; University of Texas; Tech. Rep. TR-03-07,2003,13
A generic approach for escaping saddle points,Sashank J Reddi; Manzil Zaheer; Suvrit Sra; Barnabas Poczos; Francis Bach; Ruslan Salakhutdinov; Alexander J Smola,Abstract: A central challenge to using first-order methods for optimizing nonconvex problemsis the presence of saddle points. First-order methods often get stuck at saddle points; greatlydeteriorating their performance. Typically; to escape from saddles one has to use second-order methods. However; most works on second-order methods rely extensively onexpensive Hessian-based computations; making them impractical in large-scale settings. Totackle this challenge; we introduce a generic framework that minimizes Hessian basedcomputations while at the same time provably converging to second-order critical points. Ourframework carefully alternates between a first-order and a second-order subroutine; usingthe latter only close to saddle points; and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical …,arXiv preprint arXiv:1709.01434,2017,12
Triangle fixing algorithms for the metric nearness problem,Suvrit Sra; Joel Tropp; Inderjit S Dhillon,Abstract Various problems in machine learning; databases; and statistics involve pairwisedistances among a set of objects. It is often desirable for these distances to satisfy theproperties of a metric; especially the triangle inequality. Applications where metric data isuseful include clustering; classification; metric-based indexing; and approximationalgorithms for various graph problems. This paper presents the Metric Nearness Problem:Given a dissimilarity matrix; find the “nearest” matrix of distances that satisfy the triangleinequalities. For lp nearness measures; this paper develops efficient triangle fixingalgorithms that compute globally optimal solutions by exploiting the inherent structure of theproblem. Empirically; the algorithms have time and storage costs that are linear in thenumber of triangle constraints. The methods can also be easily parallelized for additional …,Advances in Neural Information Processing Systems,2005,12
Suvrit Sra; and Inderjit S,Jason V Davis; Brian Kulis; Prateek Jain,*,*,*,12
Matrix manifold optimization for Gaussian mixtures,Reshad Hosseini; Suvrit Sra,Abstract We take a new look at parameter estimation for Gaussian Mixture Model (GMMs).Specifically; we advance Riemannian manifold optimization (on the manifold of positivedefinite matrices) as a potential replacement for Expectation Maximization (EM); which hasbeen the de facto standard for decades. An out-of-the-box invocation of Riemannianoptimization; however; fails spectacularly: it obtains the same solution as EM; but vastlyslower. Building on intuition from geometric convexity; we propose a simple reformulationthat has remarkable consequences: it makes Riemannian optimization not only match EM (anontrivial result on its own; given the poor record nonlinear programming has had againstEM); but also outperform it in many settings. To bring our ideas to fruition; we develop a well-tuned Riemannian LBFGS method that proves superior to known competing methods (eg …,Advances in Neural Information Processing Systems,2015,11
Fast Newton methods for the group fused lasso.,Matt Wytock; Suvrit Sra; Jeremy Z Kolter,Abstract We present a new algorithmic approach to the group fused lasso; a convex modelthat approximates a multi-dimensional signal via an approximately piecewise-constantsignal. This model has found many applications in multiple change point detection; signalcompression; and total variation denoising; though existing algorithms typically using first-order or alternating minimization schemes. In this paper we instead develop a specializedprojected Newton method; combined with a primal active set approach; which we show to besubstantially faster that existing methods. Furthermore; we present two applications that usethis algorithm as a fast subroutine for a more complex outer loop: segmenting linearregression models for time series data; and color image denoising. We show that on theseproblems the proposed method performs very well; solving the problems faster than …,UAI,2014,11
Global optimality conditions for deep neural networks,Chulhee Yun; Suvrit Sra; Ali Jadbabaie,Abstract: We study the error landscape of deep linear and nonlinear neural networks withsquare error loss. We build on the recent results in the literature and present necessary andsufficient conditions for a critical point of the empirical risk function to be a global minimum inthe deep linear network case. Our simple conditions can also be used to determine whethera given critical point is a global minimum or a saddle point. We further extend these resultsto deep nonlinear neural networks and prove similar sufficient conditions for globaloptimality in the function space.,arXiv preprint arXiv:1707.02444,2017,9
Kronecker determinantal point processes,Zelda E Mariet; Suvrit Sra,Abstract Determinantal Point Processes (DPPs) are probabilistic models over all subsets aground set of N items. They have recently gained prominence in several applications thatrely on diverse subsets. However; their applicability to large problems is still limited due to O(N^ 3) complexity of core tasks such as sampling and learning. We enable efficient samplingand learning for DPPs by introducing KronDPP; a DPP model whose kernel matrixdecomposes as a tensor product of multiple smaller kernel matrices. This decompositionimmediately enables fast exact sampling. But contrary to what one may expect; leveragingthe Kronecker product structure for speeding up DPP learning turns out to be more difficult.We overcome this challenge; and derive batch and stochastic optimization algorithms forefficiently learning the parameters of a KronDPP.,Advances in Neural Information Processing Systems,2016,8
Method and device for recovering a digital image from a sequence of observed digital images,*,A computer-implemented method for recovering a digital image (x) from a sequence ofobserved digital images (y1;...; yT); includes: obtaining an observed digital image (yt);estimating a point spread function (ft) based on the observed image (yt); estimating therecovered digital image (x); based on the estimated point spread function (ft) and theobserved image (yt); and repeating the above steps. In order to correct optical aberrations ofa lens; a point spread function of the lens may be used.,*,2011,8
Inference and mixture modeling with the Elliptical Gamma Distribution,Reshad Hosseini; Suvrit Sra; Lucas Theis; Matthias Bethge,Abstract The authors study modeling and inference with the Elliptical Gamma Distribution(EGD). In particular; Maximum likelihood (ML) estimation for EGD scatter matrices isconsidered; a task for which the authors present new fixed-point algorithms. The algorithmsare shown to be efficient and convergent to global optima despite non-convexity. Moreover;they turn out to be much faster than both a well-known iterative algorithm of Kent & Tyler andsophisticated manifold optimization algorithms. Subsequently; the ML algorithms areinvoked as subroutines for estimating parameters of a mixture of EGDs. The performance ofthe methods is illustrated on the task of modeling natural image statistics—the proposedEGD mixture model yields the most parsimonious model among several competingapproaches.,Computational Statistics & Data Analysis,2016,7
Gaussian quadrature for matrix inverse forms with applications,Chengtao Li; Suvrit Sra; Stefanie Jegelka,Abstract We present a framework for accelerating a spectrum of machine learning algorithmsthat require computation of\emphbilinear inverse forms u^ TA^-1u; where A is a positivedefinite matrix and ua given vector. Our framework is built on Gauss-type quadrature andeasily scales to large; sparse matrices. Further; it allows retrospective computation of lowerand upper bounds on u^ TA^-1u; which in turn accelerates several algorithms. We prove thatthese bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge;ours is the first work to demonstrate these key properties of Gauss-type quadrature; which isa classical and deeply studied topic. We illustrate empirical consequences of our results byusing quadrature to accelerate machine learning tasks involving determinantal pointprocesses and submodular optimization; and observe tremendous speedups in several …,International Conference on Machine Learning,2016,7
Introduction: optimization and machine learning,Suvrit Sra; Sebastien Nowozin; Stephen J Wright,*,Optimization for Machine Learning,2012,7
Matrix nearness problems in data mining,Suvrit Sra,Abstract This thesis addresses some fundamental problems in data mining and machinelearning that may be cast as matrix nearness problems. Some examples of well-knownnearness problems are: low-rank approximations; sparse approximations; clustering; co-clustering; kernel learning; and independent components analysis. In this thesis we studytwo types of matrix nearness problems. In the first type; we compute a low-rank matrixapproximation to a given input matrix; thereby representing it more efficiently and hopefullydiscovering the latent structure within the input data. In the second kind of nearness problemwe seek to either learn a parameterized model of/from the input data; or the data representsnoisy measurements of some underlying objects and we wish to recover the originalmeasurements. Both types of problems can be naturally approached by computing an …,*,2007,7
The sum of squared logarithms inequality in arbitrary dimensions,Lev Borisov; Patrizio Neff; Suvrit Sra; Christian Thiel,Abstract We prove the sum of squared logarithms inequality(SSLI) which states that fornonnegative vectors x; y∈ R nx; y∈ R n whose elementary symmetric polynomials satisfy ek(x)≤ ek (y) ek (x)≤ ek (y)(for 1≤ k< n 1≤ k< n) and en (x)= en (y) en (x)= en (y); theinequality∑ i (log⁡ xi) 2≤∑ i (log⁡ yi) 2∑ i (log⁡ xi) 2≤∑ i (log⁡ yi) 2 holds. Our proof ofthis inequality follows by a suitable extension to the complex plane. In particular; we showthat the function f: M⊆ C n→ R f: M⊆ C n→ R with f (z)=∑ i (log⁡ zi) 2 f (z)=∑ i (log⁡ zi) 2has nonnegative partial derivatives with respect to the elementary symmetric polynomials ofz. This property leads to our proof. We conclude by providing applications and widerconnections of the SSLI.,Linear Algebra and its Applications,2017,6
Completely strong superadditivity of generalized matrix functions,Minghua Lin; Suvrit Sra,Abstract: We prove that generalized matrix functions satisfy a block-matrix strongsuperadditivity inequality over the cone of positive semidefinite matrices. Our result extendsa recent result of Paksoy-Turkmen-Zhang (V. Paksoy; R. Turkmen; F. Zhang; Inequalities ofgeneralized matrix functions via tensor products; Electron. J. Linear Algebra 27 (2014) 332-341.). As an application; we obtain a short proof of a classical inequality of Thompson (1961)on block matrix determinants.,arXiv preprint arXiv:1410.1958,2014,5
Efficient structured matrix rank minimization,Adams Wei Yu; Wanli Ma; Yaoliang Yu; Jaime Carbonell; Suvrit Sra,Abstract We study the problem of finding structured low-rank matrices using nuclear normregularization where the structure is encoded by a linear map. In contrast to most knownapproaches for linearly structured rank minimization; we do not (a) use the full SVD; nor (b)resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration.Instead; we formulate the problem differently so that it is amenable to a generalizedconditional gradient method; which results in a practical improvement with low per iterationcomputational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time; while effectively recovering low rank solutionsin stochastic system realization and spectral compressed sensing problems.,Advances in neural information processing systems,2014,5
Block-iterative algorithms for non-negative matrix approximation,Suvrit Sra,In this paper we present new algorithms for non-negative matrix approximation (NMA);commonly known as the NMF problem. Our methods improve upon the well-known methodsof Lee\& Seung~\cite {lee00} for both the Frobenius norm as well the Kullback-Leiblerdivergence versions of the problem. For the latter problem; our results are especiallyinteresting because it seems to have witnessed much lesser algorithmic progress ascompared to the Frobenius norm NMA problem. Our algorithms are based on aparticular\textbf {block-iterative} acceleration technique for EM; which preserves themultiplicative nature of the updates and also ensures monotonicity. Furthermore; ouralgorithms also naturally apply to the Bregman-divergence NMA algorithms of~\cite {suv.nips}. Experimentally; we show that our algorithms outperform the traditional Lee/Seung …,Data Mining; 2008. ICDM'08. Eighth IEEE International Conference on,2008,5
Column Subset Selection via Polynomial Time Dual Volume Sampling,Chengtao Li; Stefanie Jegelka; Suvrit Sra,Abstract: We study dual volume sampling; a method for selecting k columns from an n* mshort and wide matrix (n<= k<= m) such that the probability of selection is proportional to thevolume of the parallelepiped spanned by the rows of the induced submatrix. This methodwas studied in [3]; who motivated it as a promising method for column subset selection.However; the development of polynomial time sampling algorithms--exact or approximate--has been since open. We close this open problem by presenting (i) an exact (randomized)polynomial time sampling algorithm;(ii) its derandomization that samples subsets satisfyingthe desired properties deterministically; and (iii) an efficient approximate samplingprocedure using Markov chains that are provably fast mixing. Our algorithms can thusbenefit downstream applications of dual volume sampling; such as column subset …,arXiv preprint arXiv:1703.02674,2017,4
Positive definite matrices: data representation and applications to computer vision,Anoop Cherian; Suvrit Sra,Abstract Numerous applications in computer vision and machine learning rely onrepresentations of data that are compact; discriminative; and robust while satisfying severaldesirable invariances. One such recently successful representation is offered bysymmetricpositive definite (SPD) matrices. However; the modeling power of SPD matrices comes at aprice: rather than a flat Euclidean view; SPD matrices are more naturally viewed throughcurved geometry (Riemannian or otherwise) which often complicates matters. We focus onmodels and algorithms that rely on the geometry of SPD matrices; and make our discussionconcrete by casting it in terms of covariance descriptors for images. We summarize variouscommonly used distance metrics on SPD matrices; before highlighting formulations andalgorithms for solving sparse coding and dictionary learning problems involving SPD data …,Algorithmic Advances in Riemannian Geometry and Applications: For Machine Learning; Computer Vision; Statistics; and Optimization,2016,4
On inequalities for normalized Schur functions,Suvrit Sra,Abstract: We prove a conjecture of Cuttler et al.~[2011][A. Cuttler; C. Greene; and M.Skandera;\emph {Inequalities for symmetric means}. European J. Combinatorics; 32 (2011);745--761] on the monotonicity of\emph {normalized Schur functions} under the usual(dominance) partial-order on partitions. We believe that our proof technique may be helpfulin obtaining similar inequalities for other symmetric functions.,arXiv preprint arXiv:1502.04753,2015,4
Correlation matrix nearness and completion under observation uncertainty,Carlos M Alaíz; Francesco Dinuzzo; Suvrit Sra,Abstract This paper introduces the paradigm of optimization under uncertainty for modellingand solving matrix nearness problems. In particular; it considers the concrete problem ofrecovering correlation matrices from uncertain observations by introducing two differentapproaches to tackling uncertainty. The first approach invokes the framework of robustoptimization to construct low error solutions that are immune to worst-case uncertainty in theinput. The second approach takes a less pessimistic view on uncertainty; and considers asituation where instead of the worst one; it suffices to use any matrix in the uncertainty set.We formulate both our approaches as convex (possibly nonsmooth) optimization problems.Thereafter; we show how to exploit problem structure to obtain efficient iterative first-orderalgorithms. We present several numerical results on both nearness and completion …,IMA Journal of Numerical Analysis,2013,4
Modeling data using directional distributions: Part II,Suvrit Sra; Prateek Jain; Inderjit Dhillon,*,*,2007,4
Directional statistics in machine learning: a brief review,Suvrit Sra,Abstract: The modern data analyst must cope with data encoded in various forms; vectors;matrices; strings; graphs; or more. Consequently; statistical and machine learning modelstailored to different data encodings are important. We focus on data encoded as normalizedvectors; so that their" direction" is more important than their magnitude. Specifically; weconsider high-dimensional vectors that lie either on the surface of the unit hypersphere or onthe real projective plane. For such data; we briefly review common mathematical modelsprevalent in machine learning; while also outlining some technical aspects; software;applications; and open mathematical challenges.,arXiv preprint arXiv:1605.00316,2016,3
On the matrix square root via geometric optimization,Suvrit Sra,Abstract: This paper is triggered by the preprint"\emph {Computing Matrix Squareroot viaNon Convex Local Search}" by Jain et al.(\textit {\textcolor {blue}{arXiv: 1507.05854}}); whichanalyzes gradient-descent for computing the square root of a positive definite matrix.Contrary to claims of~\citet {jain2015}; our experiments reveal that Newton-like methodscompute matrix square roots rapidly and reliably; even for highly ill-conditioned matrices andwithout requiring commutativity. We observe that gradient-descent converges very slowlyprimarily due to tiny step-sizes and ill-conditioning. We derive an alternative first-ordermethod based on geodesic convexity: our method admits a transparent convergenceanalysis ($< 1$ page); attains linear rate; and displays reliable convergence even for rankdeficient problems. Though superior to gradient-descent; ultimately our method is also …,Electronic Journal of Linear Algebra,2016,3
Geometric optimization in machine learning,Suvrit Sra; Reshad Hosseini,Abstract Machine learning models often rely on sparsity; low-rank; orthogonality; correlation;or graphical structure. The structure of interest in this chapter is geometric; specifically themanifold of positive definite (PD) matrices. Though these matrices recur throughout theapplied sciences; our focus is on more recent developments in machine learning andoptimization. In particular; we study (i) models that might be nonconvex in the Euclideansense but are convex along the PD manifold; and (ii) ones that are neither Euclidean norgeodesic convex but are nevertheless amenable to global optimization. We cover basictheory for (i) and (ii); subsequently; we present a scalable Riemannian limited-memoryBFGS algorithm (that also applies to other manifolds). We highlight some applications fromstatistics and machine learning that benefit from the geometric structure studies.,*,2016,3
Text clustering with mixture of von Mises-Fisher distributions,Arindam Banerjee; Inderjit Dhillon; Joydeep Ghosh; Suvrit Sra,There is a long-standing folklore in the information retrieval community that a vector spacerepresentation of text data has directional properties; ie; the direction of the vector is muchmore important than its magnitude. This belief has led to practices such as using the cosinebetween two vectors for measuring similarity between the corresponding text documents;and to the scaling of vectors to unit L2 norm (41; 40; 20). In this chapter; we describe aprobabilistic generative model (44; 25) based on directional distributions (30) for modelingtext data. 1 Specifically; we suggest that a set of text documents that form multiple topics canbe well modeled by a mixture of von Mises-Fisher (vMF) distributions; with each componentcorresponding to a topic. Generative models often provide greater insights into the anatomyof the data as compared to discriminative approaches. Moreover; domain knowledge can …,Text Mining,2009,3
Polynomial time algorithms for dual volume sampling,Chengtao Li; Stefanie Jegelka; Suvrit Sra,Abstract We study dual volume sampling; a method for selecting k columns from an n* mshort and wide matrix (n<= k<= m) such that the probability of selection is proportional to thevolume spanned by the rows of the induced submatrix. This method was proposed by Avronand Boutsidis (2013); who showed it to be a promising method for column subset selectionand its multiple applications. However; its wider adoption has been hampered by the lack ofpolynomial time sampling algorithms. We remove this hindrance by developing an exact(randomized) polynomial time sampling algorithm as well as its derandomization.Thereafter; we study dual volume sampling via the theory of real stable polynomials andprove that its distribution satisfies the “Strong Rayleigh” property. This result has numerousconsequences; including a provably fast-mixing Markov chain sampler that makes dual …,Advances in Neural Information Processing Systems,2017,2
Elementary Symmetric Polynomials for Optimal Experimental Design,Zelda E Mariet; Suvrit Sra,Abstract We revisit the classical problem of optimal experimental design (OED) under a newmathematical model grounded in a geometric motivation. Specifically; we introduce modelsbased on elementary symmetric polynomials; these polynomials capture" partial volumes"and offer a graded interpolation between the widely used A-optimal and D-optimal designmodels; obtaining each of them as special cases. We analyze properties of our models; andderive both greedy and convex-relaxation algorithms for computing the associated designs.Our analysis establishes approximation guarantees on these algorithms; while our empiricalresults substantiate our claims and demonstrate a curious phenomenon concerning ourgreedy algorithm. Finally; as a byproduct; we obtain new results on the theory of elementarysymmetric polynomials that may be of independent interest.,Advances in Neural Information Processing Systems,2017,2
Fast sampling for strongly rayleigh measures with application to determinantal point processes,Chengtao Li; Stefanie Jegelka; Suvrit Sra,Abstract: In this note we consider sampling from (non-homogeneous) strongly Rayleighprobability measures. As an important corollary; we obtain a fast mixing Markov Chainsampler for Determinantal Point Processes. Subjects: Learning (cs. LG); Data Structures andAlgorithms (cs. DS); Probability (math. PR); Machine Learning (stat. ML),arXiv preprint arXiv:1607.03559,2016,2
Fast mixing markov chains for strongly Rayleigh measures; DPPs; and constrained sampling,Chengtao Li; Suvrit Sra; Stefanie Jegelka,Abstract We study probability measures induced by set functions with constraints. Suchmeasures arise in a variety of real-world settings; where prior knowledge; resourcelimitations; or other pragmatic considerations impose constraints. We consider the task ofrapidly sampling from such constrained measures; and develop fast Markov chain samplersfor them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures;for which we present sharp polynomial bounds on the mixing time. As a corollary; this resultyields a fast mixing sampler for Determinantal Point Processes (DPPs); yielding (to ourknowledge) the first provably fast MCMC sampler for DPPs since their inception over fourdecades ago. Beyond SR measures; we develop MCMC samplers for probabilistic modelswith hard constraints and identify sufficient conditions under which their chains mix …,Advances in Neural Information Processing Systems,2016,2
Fast mixing markov chains for strongly Rayleigh measures; DPPs; and constrained sampling,Chengtao Li; Suvrit Sra; Stefanie Jegelka,Abstract We study probability measures induced by set functions with constraints. Suchmeasures arise in a variety of real-world settings; where prior knowledge; resourcelimitations; or other pragmatic considerations impose constraints. We consider the task ofrapidly sampling from such constrained measures; and develop fast Markov chain samplersfor them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures;for which we present sharp polynomial bounds on the mixing time. As a corollary; this resultyields a fast mixing sampler for Determinantal Point Processes (DPPs); yielding (to ourknowledge) the first provably fast MCMC sampler for DPPs since their inception over fourdecades ago. Beyond SR measures; we develop MCMC samplers for probabilistic modelswith hard constraints and identify sufficient conditions under which their chains mix …,Advances in Neural Information Processing Systems,2016,2
Portfolio optimization with groupwise selection,Namhyoung Kim; Suvrit Sra,Portfolio optimization in the presence of estimation error can be stabilized by incorporatingnorm-constraints; this result was shown by DeMiguel et al.(A generalized approach toportfolio optimization: improving performance by constraining portfolio norms; ManagementScience; 5; 798-812; 2009); who reported empirical performance better than numerouscompeting approaches. We extend the idea of norm-constraints by introducing a powerfulenhancement; grouped selection for portfolio optimization. Here; instead of merelypenalizing norms of the assets being selected; we penalize groups; where within a groupassets are treated alike; but across groups; the penalization may differ. The idea ofgroupwise selection is grounded in statistics; but to our knowledge; it is novel in the contextof portfolio optimization. Novelty aside; the real benefits of groupwise selection are …,Industrial Engineering & Management Systems,2014,2
Sparse inverse covariance estimation via an adaptive gradient-based method,Suvrit Sra; Dongmin Kim,Abstract: We study the problem of estimating from data; a sparse approximation to theinverse covariance matrix. Estimating a sparsity constrained inverse covariance matrix is akey component in Gaussian graphical model learning; but one that is numerically verychallenging. We address this challenge by developing a new adaptive gradient-basedmethod that carefully combines gradient information with an adaptive step-scaling strategy;which results in a scalable; highly competitive method. Our algorithm; like its predecessors;maximizes an $\ell_1 $-norm penalized log-likelihood and has the same per iterationarithmetic complexity as the best methods in its class. Our experiments reveal that ourapproach outperforms state-of-the-art competitors; often significantly so; for large problems.,arXiv preprint arXiv:1106.5175,2011,2
Distributional Adversarial Networks,Chengtao Li; David Alvarez-Melis; Keyulu Xu; Stefanie Jegelka; Suvrit Sra,Abstract: We propose a framework for adversarial training that relies on a sample rather thana single sample point as the fundamental unit of discrimination. Inspired by discrepancymeasures and two-sample tests between probability distributions; we propose two suchdistributional adversaries that operate and predict on samples; and show how they can beeasily implemented on top of existing models. Various experimental results show thatgenerators trained with our distributional adversaries are much more stable and areremarkably less prone to mode collapse than traditional models trained with pointwiseprediction discriminators. The application of our framework to domain adaptation also resultsin considerable improvement over recent state-of-the-art. Subjects: Learning (cs. LG) Citeas: arXiv: 1706.09549 [cs. LG](or arXiv: 1706.09549 v1 [cs. LG] for this version) …,arXiv preprint arXiv:1706.09549,2017,1
Hlawka–Popoviciu inequalities on positive definite tensors,Wolfgang Berndt; Suvrit Sra,Abstract We prove inequalities on symmetric tensor sums of positive definite operators. Inparticular; we prove multivariable operator inequalities inspired by generalizations to thewell-known Hlawka and Popoviciu inequalities. As corollaries; we obtain generalizedHlawka and Popoviciu inequalities for determinants; permanents; and generalized matrixfunctions. The new operator inequalities and their corollaries contain a few recentlypublished inequalities on positive definite matrices as special cases.,Linear Algebra and its Applications,2015,1
Explicit diagonalization of an anti-triangular Cesar\'o matrix,Suvrit Sra,Eigenvalues of Markov chains lend insight into the speed of convergence to an invariant measureor stationary distribution. The corresponding eigenvector provides the distribution of the stationarystate. In this paper; we study a particular Markov transition matrix of Cesaró type; and determineits eigenvalues and eigenvectors in closed form. This explicit determination of the spectrum maybe of interest because our matrix is “anti-triangular”; a class of matrices for which eigenvalueproblems are considerably harder than for the usual triangular matrices. The matrix that we studyis a Cesáro-like matrix; a name inspired by the classic article [2] (see also the remark below).The specific matrix that we study is anti-lower triangular; such matrices have also been studiedby [6]; and very recently by Ochiai et al. [7]; who undertake a detailed theoreticaldevelopment. We also note in passing a potential connection to inverse eigenvalue …,arXiv preprint arXiv:1411.4107,2014,1
Optimization for Machine Learning (Neural Information Processing Series),Stephen J Wright; Sebastian Nowozin; Suvrit Sra,*,*,2011,1
Modeling Data using Directional Distributions,Suvrit Sra; Prateek Jain; Inderjit S Dhillon,*,*,2007,1
Matrix Differential Calculus,Suvrit Sra,Documents; Authors; Tables. Log in; Sign up; MetaCart; Donate. CiteSeerX logo. Documents:Advanced Search Include Citations. Authors: Advanced Search Include Citations. Tables: MatrixDifferential Calculus (2005). Cached. Download as a PDF. Download Links. [www.cs.utexas.edu]. Save to List; Add to Collection; Correct Errors; Monitor Changes. by Suvrit Sra. Summary;Citations; Active Bibliography; Co-citation; Clustered Documents; Version History. BibTeX.@MISC{Sra05matrixdifferential; author = {Suvrit Sra}; title = {Matrix Differential Calculus}; year ={2005} }. Share. Facebook; Twitter; Reddit; Bibsonomy. OpenURL. Abstract. Keyphrases. matrixdifferential calculus. Powered by: Apache Solr. About CiteSeerX; Submit and Index Documents;Privacy Policy; Help; Data; Source; Contact Us. Developed at and hosted by The College ofInformation Sciences and Technology. © 2007-2018 The Pennsylvania State University,*,2005,1
Co-cluster (v 1.1),Hyuk Cho; Yuqiang Guan; Suvrit Sra,*,Bregman co-clustering software,2004,1
Learning Determinantal Point Processes by Sampling Inferred Negatives,Zelda Mariet; Mike Gartrell; Suvrit Sra,Abstract: Determinantal Point Processes (DPPs) have attracted significant interest from themachine-learning community due to their ability to elegantly and tractably model the delicatebalance between quality and diversity of sets. We consider learning DPPs from data; a keytask for DPPs; for this task; we introduce a novel optimization problem; ContrastiveEstimation (CE); which encodes information about" negative" samples into the basiclearning model. CE is grounded in the successful use of negative information in machine-vision and language modeling. Depending on the chosen negative distribution (which maybe static or evolve during optimization); CE assumes two different forms; which we analyzetheoretically and experimentally. We evaluate our new model on real-world datasets; on achallenging dataset; CE learning delivers a considerable improvement in predictive …,arXiv preprint arXiv:1802.05649,2018,*
A Critical View of Global Optimality in Deep Learning,Chulhee Yun; Suvrit Sra; Ali Jadbabaie,Abstract: We investigate the loss surface of deep linear and nonlinear neural networks. Weshow that for deep linear networks with differentiable losses; critical points after themultilinear parameterization inherit the structure of critical points of the underlying loss withlinear parameterization. As corollaries we obtain" local minima are global" results thatsubsume most previous results; while showing how to distinguish global minima from saddlepoints. For nonlinear neural networks; we prove two theorems showing that even fornetworks with one hidden layer; there can be spurious local minima. Indeed; for piecewiselinear nonnegative homogeneous activations (eg; ReLU); we prove that for almost allpractical datasets there exist infinitely many local minima that are not global. We concludeby constructing a counterexample involving other activation functions (eg; sigmoid; tanh …,arXiv preprint arXiv:1802.03487,2018,*
Frank-Wolfe methods for geodesically convex optimization with application to the matrix geometric mean,Melanie Weber; Suvrit Sra,Abstract: We consider optimization of geodesically convex objectives over geodesicallyconvex subsets of the manifold of positive definite matrices. In particular; for this task; wedevelop Euclidean and Riemannian Frank-Wolfe (FW) algorithms. For both settings; weanalyze non-asymptotic convergence rates to global optimality. To our knowledge; these arethe first results on Riemannian FW and its convergence. We specialize our algorithms for thetask of computing the matrix geometric mean; ie; the Riemannian centroid of a set of positivedefinite matrices. For this problem; we provide concrete; closed-form realizations of thecrucial" linear oracle" required by FW that may be of independent interest. Moreover; underan additional hypothesis; we prove how Riemannian FW can even attain a linear rate ofconvergence. Experiments against recently published methods for the matrix geometric …,arXiv preprint arXiv:1710.10770,2017,*
Unsupervised robust nonparametric learning of hidden community properties,Mikhail A Langovoy; Akhilesh Gotmare; Martin Jaggi; Suvrit Sra,Abstract: We consider learning of fundamental properties of communities in large noisynetworks; in the prototypical situation where the nodes or users are split into two classes; eg;according to their opinions or preferences on a topic. We propose a nonparametric;unsupervised; and scalable graph scan procedure that is; in addition; robust against a classof powerful adversaries. In our setup; one of the communities can fall under the influence ofa strong and knowledgeable adversarial leader; who knows the full network structure; hasunlimited computational resources and can completely foresee our planned actions on thenetwork. We prove strong consistency of our results in a setup with minimal assumptions. Inparticular; the learning procedure estimates the baseline activity of normal usersasymptotically correctly with probability 1; the only assumption being the existence of a …,arXiv preprint arXiv:1707.03494,2017,*
An Alternative to EM for Gaussian Mixture Models: Batch and Stochastic Riemannian Optimization,Reshad Hosseini; Suvrit Sra,Abstract: We consider maximum likelihood estimation for Gaussian Mixture Models (Gmms).This task is almost invariably solved (in theory and practice) via the ExpectationMaximization (EM) algorithm. EM owes its success to various factors; of which is its ability tofulfill positive definiteness constraints in closed form is of key importance. We propose analternative to EM by appealing to the rich Riemannian geometry of positive definite matrices;using which we cast Gmm parameter estimation as a Riemannian optimization problem.Surprisingly; such an out-of-the-box Riemannian formulation completely fails and provesmuch inferior to EM. This motivates us to take a closer look at the problem geometry; andderive a better formulation that is much more amenable to Riemannian optimization. Wethen develop (Riemannian) batch and stochastic gradient algorithms that outperform EM …,arXiv preprint arXiv:1706.03267,2017,*
Sequence Summarization Using Order-constrained Kernelized Feature Subspaces,Anoop Cherian; Suvrit Sra; Richard Hartley,Abstract: Representations that can compactly and effectively capture temporal evolution ofsemantic content are important to machine learning algorithms that operate on multi-variatetime-series data. We investigate such representations motivated by the task of human actionrecognition. Here each data instance is encoded by a multivariate feature (such as via adeep CNN) where action dynamics are characterized by their variations in time. As thesefeatures are often non-linear; we propose a novel pooling method; kernelized rank pooling;that represents a given sequence compactly as the pre-image of the parameters of ahyperplane in an RKHS; projections of data onto which captures their temporal order. Wedevelop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective; we then propose to use the parameters of a …,arXiv preprint arXiv:1705.08583,2017,*
Combinatorial Topic Models using Small-Variance Asymptotics,Ke Jiang; Suvrit Sra; Brian Kulis,Abstract: Topic models have emerged as fundamental tools in unsupervised machinelearning. Most modern topic modeling algorithms take a probabilistic view and deriveinference algorithms based on Latent Dirichlet Allocation (LDA) or its variants. In contrast;we study topic modeling as a combinatorial optimization problem; and propose a newobjective function derived from LDA by passing to the small-variance limit. We minimize thederived objective by using ideas from combinatorial optimization; which results in a new;fast; and high-quality topic modeling algorithm. In particular; we show that our results arecompetitive with popular LDA-based topic modeling approaches; and also discuss the (dis)similarities between our approach and its probabilistic counterparts.,arXiv preprint arXiv:1604.02027,2016,*
Gauss quadrature for matrix inverse forms with applications,Chengtao Li; Suvrit Sra; Stefanie Jegelka,Abstract: We present a framework for accelerating a spectrum of machine learningalgorithms that require computation of bilinear inverse forms $ u^\top A^{-1} u $; where $ A $is a positive definite matrix and $ u $ a given vector. Our framework is built on Gauss-typequadrature and easily scales to large; sparse matrices. Further; it allows retrospectivecomputation of lower and upper bounds on $ u^\top A^{-1} u $; which in turn acceleratesseveral algorithms. We prove that these bounds tighten iteratively and converge at a linear(geometric) rate. To our knowledge; ours is the first work to demonstrate these key propertiesof Gauss-type quadrature; which is a classical and deeply studied topic. We illustrateempirical consequences of our results by using quadrature to accelerate machine learningtasks involving determinantal point processes and submodular optimization; and observe …,arXiv preprint arXiv:1512.01904,2015,*
Diversity Networks: Neural Network Compression Using Determinantal Point Processes,Zelda Mariet; Suvrit Sra,Abstract: We introduce Divnet; a flexible technique for learning networks with diverseneurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP)over neurons in a given layer. It uses this DPP to select a subset of diverse neurons andsubsequently fuses the redundant neurons into the selected ones. Compared with previousapproaches; Divnet offers a more principled; flexible technique for capturing neuronaldiversity and thus implicitly enforcing regularization. This enables effective auto-tuning ofnetwork architecture and leads to smaller network sizes without hurting performance.Moreover; through its focus on diversity and neuron fusing; Divnet remains compatible withother procedures that seek to reduce memory footprints of networks. We presentexperimental results to corroborate our claims: for pruning neural networks; Divnet is …,arXiv preprint arXiv:1511.05077,2015,*
Inequalities via symmetric polynomial majorization,Suvrit Sra,Abstract: We consider a partial order on positive vectors induced by elementary symmetricpolynomials. As a corollary we obtain a short proof of the SSLI inequality of Neff et al.(2012);which was first obtained via a more elaborate approach.\footnote {Added 18/9/2015: It hasbeen brought to our attention~\citep {neff. private} that a line of approach closely related toours has been developed by M.\v {S} ilhav\'y; who considers a rich generalization based onPick functions. This idea is natural and elegant (This idea is natural and elegant (and wasalso suggested by the author to P. Neff on June 1; 2015. Completely independent of us; M.\v{S} ilhav\'y has recently developed the Pick function approach fully~\citep {neff. private}); andparts of it are even implicit in the recent article of Jozsa and Mitchison (2015). The keycontribution of\v {S} ilhav\'y's work is presentation of necessary and sufficient conditions …,arXiv preprint arXiv:1509.05902,2015,*
Natter: A Python Natural Image Statistics Toolbox},Fabian H Sinz; Jörn-Philipp Lies; Sebastian Gerwinn; Matthias Bethge; R Hosseini; S Sra; L Theis; M Bethge; E Froudarakis; P Berens; AS Ecker; RJ Cotton; FH Sinz; D Yatsenko; P Saggau; M Bethge; AS Tolias; AS Ecker; P Berens; RJ Cotton; M Subramaniyan; GH Denfield; CR Cadwell; SM Smirnakis; M Bethge; AS Tolias; HE Gerhard; M Bethge; L Theis; AM Chagas; D Arnstein; C Schwarz; M Bethge; F Sinz; M Bethge; M Subramaniyan; AS Ecker; P Berens; AS Tolias; RM Haefner; S Gerwinn; JH Macke; M Bethge,abstract={The statistical analysis and modeling of natural images is an important branch ofstatistics with applications in image signaling; image compression; computer vision; andhuman perception. Because the space of all possible images is too large to be sampledexhaustively; natural image models must inevitably make assumptions in order to staytractable. Subsequent model comparison can then ﬁlter out those models that best capturethe statistical regularities in natural images. Proper model comparison; however; oftenrequires that the models and the preprocessing of the data match down to theimplementation details. Here we present the Natter; a statistical software toolbox for naturalimages models; that can provide such consistency. The Natter includes powerful buttractable baseline model as well as standardized data preprocessing steps. It has an …,Journal of Statistical Software},2014,*
Tractable Optimization in Machine Learning.,Suvrit SrA,Machine Learning (ML) broadly encompasses a variety of adaptive; autonomous; andintelligent tasks where one must “learn” to predict from observations and feedback.Throughout its evolution; ML has drawn heavily and successfully on optimization algorithms.This relation to optimization is not surprising as “learning” and “adapting” usually lead toproblems where some quality function must be optimized. But the interaction between MLand optimization is now undergoing rapid change. The increased size; complexity; andvariety seen in ML problems; not only prompt a refinement of existing optimizationtechniques but also spur development of new methods tuned to the specific needs of MLapplications. 1,*,2014,*
Statistical estimation for optimization problems on graphs,Mikhail Langovoy; Suvrit Sra,Abstract: Large graphs abound in machine learning; data mining; and several related areas.A useful step towards analyzing such graphs is that of obtaining certain summary statistics-eg; or the expected length of a shortest path between two nodes; or the expected weight of aminimum spanning tree of the graph; etc. These statistics provide insight into the structure ofa graph; and they can help predict global properties of a graph. Motivated thus; we proposeto study statistical properties of structured subgraphs (of a given graph); in particular; toestimate the expected objective function value of a combinatorial optimization problem overthese subgraphs. The general task is very difficult; if not unsolvable; so for concreteness wedescribe a more specific statistical estimation problem based on spanning trees. We hopethat our position paper encourages others to also study other types of graphical structures …,arXiv preprint arXiv:1311.7656,2013,*
Convex Optimization,Suvrit Sra,Def. If A ∈ Cn×n and x ∈ Cn. Consider the equation Ax = λx; x = 0; λ ∈ C. If scalar λ and vectorx satisfy this equation; then λ is called an eigenvalue and x and eigenvector of A … Def. If A∈ Cn×n and x ∈ Cn. Consider the equation Ax = λx; x = 0; λ ∈ C. If scalar λ and vector x satisfythis equation; then λ is called an eigenvalue and x and eigenvector of A … Above equationmay be rewritten equivalently as (λI − A)x = 0; x = 0 … Thus; λ is an eigenvalue; if and only ifdet(λI − A)=0 … Def. If A ∈ Cn×n and x ∈ Cn. Consider the equation Ax = λx; x = 0; λ ∈ C. Ifscalar λ and vector x satisfy this equation; then λ is called an eigenvalue and x and eigenvectorof A … Above equation may be rewritten equivalently as (λI − A)x = 0; x = 0 … Thus; λ is aneigenvalue; if and only if det(λI − A)=0 … Def. pA(t) := det(tI − A) is called characteristicpolynomial … Def. If A ∈ Cn×n and x ∈ Cn. Consider the equation Ax = λx; x = 0; λ ∈ …,*,2013,*
European Conference on Machine Learning (ECML); 2011,Anoop Cherian; Jonathan Andersh; Vassilios Morellas; Berenice Mettler; Nikolaos Papanikolopoulos,A Vision Based Ensemble Approach to Velocity Estimation for Miniature Rotorcraft. J.Andersh; A. Cherian; N. Papanikolopoulos; and B. Mettler. Autonomous Robots; 2015Preprint. Riemannian Sparse Coding for Positive Definite Matrices. Anoop Cherian andSuvrit Sra. European Conference on Computer Vision (ECCV); 2014 …,University of Minnesota; Minneapolis,2013,*
Explicit eigenvalues of certain scaled trigonometric matrices,Suvrit Sra,Abstract In a recent paper [D. Zhang; Z. Lin; Y. Liu; On eigenvalues and equivalenttransformation of trigonometric matrices; Linear Algebra Appl. 436 (2012) 71–78]; theauthors motivated and discussed a trigonometric matrix that arises in the design of finiteimpulse response (FIR) digital filters. The eigenvalues of this matrix shed light on the FIRfilter design; so obtaining them in closed form was investigated. Zhang et al. proved that theirmatrix is rank-4 and they conjectured closed form expressions for its eigenvalues. Thispaper studies trigonometric matrices more general than theirs; deduces their rank; andderives closed-forms for their eigenvalues. As a corollary; it yields an elementary proof of theconjecture in the aforementioned paper.,Linear Algebra and its Applications,2013,*
A new metric on the manifold of kernel matrices,Suvrit Sra,Abstract Symmetric positive definite (spd) matrices pervade numerous scientific disciplines;including machine learning and optimization. We consider the key task of measuringdistances between two spd matrices; a task that is often nontrivial whenever the distancefunction must respect the non-Euclidean geometry of spd matrices. Typical non-Euclideandistance measures such as the Riemannian metric δR (X; Y)= log (Y− 1/2XY− 1/2) F; arecomputationally demanding and also complicated to use. To allay some of these difficulties;we introduce a new metric on spd matrices; which not only respects non-Euclideangeometry but also offers faster computation than δR while being less complicated to use. Wesupport our claims theoretically by listing a set of theorems that relate our metric to δR (X; Y);and experimentally by studying the nonconvex problem of computing matrix geometric …,NIPS Workshop on Algebraic Topology and Machine Learning,2012,*
of Book: Optimization for Machine Learning,M Schmidt; D Kim; S Sra,*,*,2011,*
A Trivial Observation related to Sparse Recovery,Suvrit Sra,Khandekar (Gradient Descent with Sparsification ICML 2009) that replaces the standardRestricted Isometry Property (RIP); with another RIP-type property (which could be simplerthan the RIP; but we am not sure; it could be as hard as the RIP to check; thereby renderingthis little writeup totally worthless).,arXiv preprint arXiv:0906.4805,2009,*
Workshop summary: Numerical mathematics in machine learning,Matthias Seeger; Suvrit Sra; John P Cunningham,*,Proceedings of the 26th Annual International Conference on Machine Learning,2009,*
Sparse Recovery without the Restricted Isometry Property,Suvrit Sra,Abstract We make a trivial modification to the elegant analysis of Garg and Khandekar(Gradient Descent with Sparsification ICML 2009) to derive a simple gradient descentalgorithm that guarantees sparsity without requiring the Restricted Isometry Property (RIP).The analysis is embarrassingly simple; and could therefore be wrong; so we welcomecriticism and corrections.,arXiv preprint arXiv:0906.4805,2009,*
New Projected Quasi-Newton Methods with Applications,Suvrit Sra,Abstract Box-constrained convex optimization problems are central to several applications ina variety of fields such as statistics; psychometrics; signal processing; medical imaging; andmachine learning. Two fundamental examples are the non-negative least squares (NNLS)problem and the non-negative Kullback-Leibler (NNKL) divergence minimization problem.The non-negativity constraints are usually based on an underlying physical restriction; foreg; when dealing with applications in astronomy; tomography; statistical estimation; orimage restoration; the underlying parameters represent physical quantities such asconcentration; weight; intensity; or frequency counts and are therefore only interpretable withnon-negative values. Several modern optimization methods can be inefficient for simpleproblems such as NNLS and NNKL as they are really designed to handle far more …,Microsoft Research Tech-talk,2008,*
Learning with Bregman Divergences,Srujana Merugu; Suvrit Sra,Bregman's cyclic projection method: Start with appropriate x (0). Compute x (t+ 1) to be theBregman projection of x (t) onto the i-th hyperplane (i= t mod m) for t= 0; 1; 2;... Converges toglobally optimal solution. This cyclic projection method can be extended to halfspace andconvex constraints; where each projection is followed by a correction. Question: What roledo Bregman divergences play in machine learning?,*,2007,*
Matrix Differential Calculus {DRAFT–Version 1.1},Suvrit Sra,Elementary matrix analysis is the staple of problems in machine learning; and data mining.Most often one is engaged with optimizing objective functions; and traditionally thedevelopment of methods for such optimization begins with the finding of gradients andderivatives. In this manuscript I present a very basic introduction to the computation of matrixderivatives of simple functions. I have adopted two approaches. First; I proceed withelementwise differentiation; computing partial derivatives at each step and obtaining thematrix form derivative from them. Second; I introduce (some well known) methods that givean organized set of tools to allow one to compute matrix derivatives without having to laborthrough the elementwise (potentially error-prone because of the jungle of indices) process.The second method is more elegant; but sometimes can prove to be more confusing for …,*,2005,*
Generalized Nonnegative Matrix Approximations using Bregman Divergences,S Sra,Author: Sra; S; Genre: Report; Published in Print: 2005-06; Title: GeneralizedNonnegative Matrix Approximations using Bregman Divergences.,*,2005,*
HW4 Pattern Classification,Suvrit Sra,Expectation Maximization (EM) is a widely used method to maximize the likelihood givenincomplete data. It is applicable to complete data also by suitably defining appropriatemissing data. A common use of EM is to find the most likely parameters for data that isbelieved to follow some particular mixture model. In this homework; we looked at the EMalgorithm for finding the Maximum Likelihood Estimate (MLE) for data that was assumed tohave arisen out of a mixture of Gaussians.,*,2002,*
Pattern Classification: HW3,Suvrit Sra,This is easily seen to be zero. Hence equation (6) has been verified. All this was done withthe idea of introducing the sample mean into the picture and writing things in terms of thesample mean. On summing equation (6) over i= 1;...; n; the last term sums out to 0. Using thefact that xT Ax= Tr [AxxT] we can now write: n,*,2002,*
Metric Nearness: Problem Formulation and Algorithms,Inderjit S Dhillon; Suvrit Sra; Joel A Tropp,Abstract Many problems in machine learning; data mining; databases and statistics involvethe pairwise dissimilarities among a set of objects. It is often desirable for thesedissimilarities to satisfy the properties of a metric—especially the triangle inequality.Applications where metric data are crucial include clustering; classification; metric-basedindexing; query processing; and approximation algorithms. This paper presents the MetricNearness Problem: Given a dissimilarity matrix; find the “nearest” matrix of distances thatsatisfy the triangle inequalities. For lp nearness measures; the paper develops efficientalgorithms that compute globally optimal solutions by exploiting the deep structure of theproblem. Empirically; the algorithms have time and storage costs that are linear in thenumber of triangle constraints. The methods can easily be parallelized for additional …,Advances in Neural Information Processing,*,*
Supplementary Material for Combinatorial Topic Models using Small-Variance Asymptotics,Ke Jiang; Suvrit Sra; Brian Kulis,Here α and β are scalar-valued (ie; we are using a symmetric Dirichlet distribution). DenoteW as the vector denoting all words in all documents; Z as the topic indicators of all words inall documents; θ as the concatenation of all the θj variables; and ψ as the concatenation ofall the ψt variables. Also let Nj be the total number of word tokens in document j. The θjvectors are each of length K; the number of topics. The ψt vectors are each of length V; thenumber of words in the dictionary. We can write down the full joint likelihood of the model asp (W; Z; θ; ψ| α; β)=,*,*,*
Computational Statistics manuscript No.,Suvrit Sra,Documents; Authors; Tables. Log in; Sign up; MetaCart; Donate. CiteSeerX logo. Documents:Advanced Search Include Citations. Authors: Advanced Search Include Citations | Disambiguate.Tables: Computational Statistics manuscript No. Cached. Download as a PDF. Download Links.[www.kyb.tuebingen.mpg.de]. Save to List; Add to Collection; Correct Errors; Monitor Changes.by Suvrit Sra. Summary; Citations; Active Bibliography; Co-citation; Clustered Documents; VersionHistory. BibTeX. @MISC{Sra_computationalstatistics; author = {Suvrit Sra}; title = {ComputationalStatistics manuscript No.}; year = {} }. Share. Facebook; Twitter; Reddit; Bibsonomy. OpenURL.Abstract. (will be inserted by the editor) A short note on parameter approximation for vonMises-Fisher distributions And a fast implementation of Is(x). Keyphrases. computational statisticfast implementation short note von mises-fisher distribution parameter …,*,*,*
SUVRIT SRA,SUMMER Seminar Series,ABSTRACT I will talk about a simply stated but intriguing problem called” metric nearness”.The goal is to minimally modify a set of pairwise dissimilarities to recover a “nearest” set ofdistances—principally; to satisfy the triangle inequality. I present our formulation of metricnearness as a convex optimization problem; which we solve using triangle-fixing algorithms.I show empirical results indicating that triangle-fixing is computationally superior togeneralpurpose convex programming software. Metric data enjoy importance in variousapplications such as clustering; classification; database search; and metric nearness enjoysconnections with several other problems. In particular; I highlight connections tomultidimensional scaling; graph clustering; and the well-known All Pairs Shortest Paths(APSP) problem. Curiously; APSP turns out to be a special case of metric nearness; an …,*,*,*
AdaDelay: Delay Adaptive Distributed Stochastic Optimization,MITCMUCM CMU,*,*,*,*
Triangle Fixing Algorithms for the Metric Nearness Problem,Inderjit S Dhillon Suvrit Sra Joel; A Tropp,*,*,*,*
The sum of squared logarithms inequality,Lev Borisov; Patrizio Neff; Suvrit Sra; Christian Thiel,In 2013 Bîrsan; Neff and Lankeit in [2] found a proof for n∈{2; 3}. In 2015; Neff and Pompe[8] proved the SSLI for n= 4; based on a new idea that supports more functions than only logbut did not extend to higher dimensions without further complications. This line of thoughthas been recently taken up in [9] to yield a complete classification for arbitrary n.,*,*,*
The Tradeoffs of Large Scale Learning,Suvrit Sra; Sebastian Nowozin; Stephen J Wright,*,Optimization,*,*
A tiny remark on Cauchy-Schwarz via matrices,Suvrit Sra,Recently; Harvey [2014] observed a version of CS involving four vectors. He noted thatalthough simple; this version is absent in the literature and that its proof requires a simplebut additional argument. This tiny note shows that Harvey's inequality actually follows fromCS by using an idea based on matrices.,*,*,*
Towards optimal stochastic alternating direction method of multipliers: Supplementary material,Suvrit Sra; MPG DE,Up to (7); our analysis has paralleled the one in (Ouyang et al.; 2013). But now come twocrucial differences:(i) instead of using uniform averages of past iterates we use weightedaverages; and instead of stepsizes ηk= 1/(µk) we use ηk= 2/(µ (k+ 1)). This change is key tomaking our complexity bound optimal; though under slightly stronger assumptions on∥ B(yk− y∗)∥ 2 and∥ λk− λ∗∥ 2 than (Ouyang et al.; 2013)(namely; boundedness at eachiteration k; rather than just at k= 0). But this added assumption is the price one has to oftenpay for acceleration algorithm (Chambolle & Pock; 2011; Goldfarb et al.; 2012).,*,*,*
Sparse Nonnegative Matrix Factorization: New Formulations and Algorithms,Rashish Tandon; Suvrit Sra,*,*,*,*
Efficient Newton-type algorithms for total-variation optimization with application to fused-lasso problems within a trust-region framework,Alvaro Barbero; Suvrit Sra,*,*,*,*
