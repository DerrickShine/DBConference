Timber: A native XML database,Hosagrahar V Jagadish; Shurug Al-Khalifa; Adriane Chapman; Laks VS Lakshmanan; Andrew Nierman; Stelios Paparizos; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu,Abstract This paper describes the overall design and architecture of the Timber XMLdatabase system currently being implemented at the University of Michigan. The system isbased upon a bulk algebra for manipulating trees; and natively stores XML. New accessmethods have been developed to evaluate queries in the XML context; and new costestimation and query optimization techniques have also been developed. We presentperformance numbers to support some of our design decisions. We believe that the keyintellectual contribution of this system is a comprehensive set-at-a-time query processingability in a native XML store; with all the standard components of relational queryprocessing; including algebraic rewriting and a cost-based optimizer.,The VLDB Journal—The International Journal on Very Large Data Bases,2002,568
Schema-free xquery,Yunyao Li; Cong Yu; HV Jagadish,Abstract The widespread adoption of XML holds out the promise that document structure canbe exploited to specify precise database queries. However; the user may have only a limitedknowledge of the XML structure; and hence may be unable to produce a correct XQuery;especially in the context of a heterogeneous information collection. The default is to usekeyword-based search and we are all too familiar with how difficult it is to obtain preciseanswers by these means. We seek to address these problems by introducing the notion ofMeaningful Lowest Common Ancestor Structure (MLCAS) for finding related nodes within anXML document. By automatically computing MLCAS and expanding ambiguous tag names;we add new functionality to XQuery and enable users to take full advantage of XQuery inquerying XML data precisely and efficiently without requiring (perfect) knowledge of the …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,476
Web-scale data integration: You can only afford to pay as you go,Jayant Madhavan; Shawn R Jeffery; Shirley Cohen; Xin Dong; David Ko; Cong Yu; Alon Halevy,ABSTRACT The World Wide Web is witnessing an increase in the amount of structuredcontent–vast heterogeneous collections of structured data are on the rise due to the DeepWeb; annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper; we highlight these challenges intwo scenarios–the Deep Web and Google Base. We contend that traditional data integrationtechniques are no longer valid in the face of such heterogeneity and scale. We propose anew data integration architecture; PAYGO; which is inspired by the concept of dataspacesand emphasizes pay-as-you-go data management as means for achieving web-scale dataintegration.,*,2007,429
BRL1 and BRL3 are novel brassinosteroid receptors that function in vascular differentiation in Arabidopsis,Ana Caño-Delgado; Yanhai Yin; Cong Yu; Dionne Vafeados; Santiago Mora-García; Jin-Chen Cheng; Kyoung Hee Nam; Jianming Li; Joanne Chory,Plant steroid hormones; brassinosteroids (BRs); are perceived by the plasma membrane-localized leucine-rich-repeat-receptor kinase BRI1. Based on sequence similarity; we haveidentified three members of the BRI1 family; named BRL1; BRL2 and BRL3. BRL1 andBRL3; but not BRL2; encode functional BR receptors that bind brassinolide; the most activeBR; with high affinity. In agreement; only BRL1 and BRL3 can rescue bri1 mutants whenexpressed under the control of the BRI1 promoter. While BRI1 is ubiquitously expressed ingrowing cells; the expression of BRL1 and BRL3 is restricted to non-overlapping subsets ofvascular cells. Loss-of-function of brl1 causes abnormal phloem: xylem differentiation ratiosand enhances the vascular defects of a weak bri1 mutant. bri1 brl1 brl3 triple mutantsenhance bri1 dwarfism and also exhibit abnormal vascular differentiation. Thus …,Development,2004,375
Group recommendation: Semantics and efficiency,Sihem Amer-Yahia; Senjuti Basu Roy; Ashish Chawlat; Gautam Das; Cong Yu,Abstract We study the problem of group recommendation. Recommendation is an importantinformation exploration paradigm that retrieves interesting items for users based on theirprofiles and past activities. Single user recommendation has received significant attention inthe past due to its extensive use in Amazon and Netflix. How to recommend to a group ofusers who may or may not share similar tastes; however; is still an open problem. The needfor group recommendation arises in many scenarios: a movie for friends to watch together; atravel destination for a family to spend a holiday break; and a good restaurant for colleaguesto have a working lunch. Intuitively; items that are ideal for recommendation to a group maybe quite different from those for individual members. In this paper; we analyze the desiderataof group recommendation and propose a formal semantics that accounts for both item …,Proceedings of the VLDB Endowment,2009,253
Data integration with uncertainty,Xin Dong; Alon Y Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levels;and do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; queries to the system may be posed with keywordsrather than in a structured form. Third; the data from the sources may be extracted usinginformation extraction techniques and so may yield imprecise data. As a first step to buildingsuch a system; we introduce the concept of probabilistic schema mappings and analyzetheir formal foundations. We show that there are two possible semantics for such …,Proceedings of the 33rd international conference on Very large data bases,2007,251
Constraint-based XML query rewriting for data integration,Cong Yu; Lucian Popa,Abstract We study the problem of answering queries through a target schema; given a set ofmappings between one or more source schemas and this target schema; and given that thedata is at the sources. The schemas can be any combination of relational or XML schemas;and can be independently designed. In addition to the source-to-target mappings; weconsider as part of the mapping scenario a set of target constraints specifying additionalproperties on the target schema. This becomes particularly important when integrating datafrom multiple data sources with overlapping data and when such constraints can expressdata merging rules at the target. We define the semantics of query answering in such anintegration scenario; and design two novel algorithms; basic query rewrite and queryresolution; to implement the semantics. The basic query rewrite algorithm reformulates …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,247
Automatic construction of travel itineraries using social breadcrumbs,Munmun De Choudhury; Moran Feldman; Sihem Amer-Yahia; Nadav Golbandi; Ronny Lempel; Cong Yu,Abstract Vacation planning is one of the frequent---but nonetheless laborious---tasks thatpeople engage themselves with online; requiring skilled interaction with a multitude ofresources. This paper constructs intra-city travel itineraries automatically by tapping a latentsource reflecting geo-temporal breadcrumbs left by millions of tourists. For example; thepopular rich media sharing site; Flickr; allows photos to be stamped by the time of when theywere taken and be mapped to Points Of Interests (POIs) by geographical (ie latitude-longitude) and semantic (eg; tags) metadata. Leveraging this information; we constructitineraries following a two-step approach. Given a city; we first extract photo streams ofindividual users. Each photo stream provides estimates on where the user was; how long hestayed at each place; and what was the transit time between places. In the second step …,Proceedings of the 21st ACM conference on Hypertext and hypermedia,2010,225
Making database systems usable,HV Jagadish; Adriane Chapman; Aaron Elkiss; Magesh Jayapandian; Yunyao Li; Arnab Nandi; Cong Yu,Abstract Database researchers have striven to improve the capability of a database in termsof both performance and functionality. We assert that the usability of a database is asimportant as its capability. In this paper; we study why database systems today are sodifficult to use. We identify a set of five pain points and propose a research agenda toaddress these. In particular; we introduce a presentation data model and recommend directdata manipulation with a schema later approach. We also stress the importance ofprovenance and of consistency across presentation models.,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,225
Finding frequent items in probabilistic data,Qin Zhang; Feifei Li; Ke Yi,Abstract Computing statistical information on probabilistic data has attracted a lot of attentionrecently; as the data generated from a wide range of data sources are inherently fuzzy oruncertain. In this paper; we study an important statistical query on probabilistic data: findingthe frequent items. One straightforward approach to identify the frequent items in aprobabilistic data set is to simply compute the expected frequency of an item and decide if itexceeds a certain fraction of the expected size of the whole data set. However; this simpledefinition misses important information about the internal structure of the probabilistic dataand the interplay among all the uncertain entities. Thus; we propose a new definition basedon the possible world semantics that has been widely adopted for many query types inuncertain data management; trying to find all the items that are likely to be frequent in a …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,218
Semantics of ranking queries for probabilistic data and expected ranks,Graham Cormode; Feifei Li; Ke Yi,When dealing with massive quantities of data; top-k queries are a powerful technique forreturning only the k most relevant tuples for inspection; based on a scoring function. Theproblem of efficiently answering such ranking queries has been studied and analyzedextensively within traditional database settings. The importance of the top-k is perhaps evengreater in probabilistic databases; where a relation can encode exponentially many possibleworlds. There have been several recent attempts to propose definitions and algorithms forranking queries over probabilistic data. However; these all lack many of the intuitiveproperties of a top-k over deterministic data. Specifically; we define a number of fundamentalproperties; including exact-k; containment; unique-rank; value-invariance; and stability;which are all satisfied by ranking queries on certain data. We argue that all these …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,212
It takes variety to make a world: diversification in recommender systems,Cong Yu; Laks Lakshmanan; Sihem Amer-Yahia,Abstract Recommendations in collaborative tagging sites such as del. icio. us and Yahoo!Movies; are becoming increasingly important; due to the proliferation of general queries onthose sites and the ineffectiveness of the traditional search paradigm to address thosequeries. Regardless of the underlying recommendation strategy; item-based or user-based;one of the key concerns in producing recommendations; is over-specialization; which resultsin returning items that are too homogeneous. Traditional solutions rely on post-processingreturned items to identify those which differ in their attribute values (eg; genre and actors formovies). Such approaches are not always applicable when intrinsic attributes are notavailable (eg; URLs in del. icio. us). In a recent paper [20]; we introduced the notion ofexplanation-based diversity and formalized the diversification problem as a compromise …,Proceedings of the 12th international conference on extending database technology: Advances in database technology,2009,167
Semantic adaptation of schema mappings when schemas evolve,Cong Yu; Lucian Popa,Abstract Schemas evolve over time to accommodate the changes in the information theyrepresent. Such evolution causes invalidation of various artifacts depending on theschemas; such as schema mappings. In a heterogenous environment; where cooperationamong data sources depends essentially upon them; schema mappings must be adapted toreflect schema evolution. In this study; we explore the mapping composition approach foraddressing this mapping adaptation problem. We study the semantics of mappingcomposition in the context of mapping adaptation and compare our approach with theincremental approach of Velegrakis et al [21]. We show that our method is superior in termsof capturing the semantics of both the original mappings and the evolution. We design andimplement a mapping adaptation system based on mapping composition as well as …,Proceedings of the 31st international conference on Very large data bases,2005,162
Data integration with uncertainty,Xin Luna Dong; Alon Halevy; Cong Yu,Abstract This paper reports our first set of results on managing uncertainty in dataintegration. We posit that data-integration systems need to handle uncertainty at three levelsand do so in a principled fashion. First; the semantic mappings between the data sourcesand the mediated schema may be approximate because there may be too many of them tobe created and maintained or because in some domains (eg; bioinformatics) it is not clearwhat the mappings should be. Second; the data from the sources may be extracted usinginformation extraction techniques and so may yield erroneous data. Third; queries to thesystem may be posed with keywords rather than in a structured form. As a first step tobuilding such a system; we introduce the concept of probabilistic schema mappings andanalyze their formal foundations. We show that there are two possible semantics for such …,The VLDB Journal,2009,140
Michigan Molecular Interactions (MiMI): putting the jigsaw puzzle together,Magesh Jayapandian; Adriane Chapman; V Glenn Tarcea; Cong Yu; Aaron Elkiss; Angela Ianni; Bin Liu; Arnab Nandi; Carlos Santos; Philip Andrews; Brian Athey; David States; HV Jagadish,Abstract Protein interaction data exists in a number of repositories. Each repository has itsown data format; molecule identifier and supplementary information. Michigan MolecularInteractions (MiMI) assists scientists searching through this overwhelming amount of proteininteraction data. MiMI gathers data from well-known protein interaction databases and deep-merges the information. Utilizing an identity function; molecules that may have differentidentifiers but represent the same real-world object are merged. Thus; MiMI allows the usersto retrieve information from many different databases at once; highlighting complementaryand contradictory information. To help scientists judge the usefulness of a piece of data;MiMI tracks the provenance of all data. Finally; a simple yet powerful user interface aidsusers in their queries; and frees them from the onerous task of knowing the data format or …,Nucleic acids research,2006,130
Schema summarization,Cong Yu; HV Jagadish,Abstract Real database systems can often be very complex. A person wishing to access datafrom an unfamiliar database has the daunting task of understanding its schema before beingable to pose a correct query against it. A schema summary can be of great help; providing asuccinct overview of the entire schema; and making it possible to explore in depth only therelevant schema components. In this paper we formally define a schema summary and twodesirable properties (in addition to minimizing size) of a summary: presenting importantschema elements and achieving broad information coverage. We develop algorithms thatallow us to automatically generate schema summaries based on these two goals. We furtherdevelop an objective metric for assessing the quality of a schema summary using queryinformation. Experimental evaluation using this metric demonstrates that the summaries …,Proceedings of the 32nd international conference on Very large data bases,2006,130
Querying structured text in an XML database,Shurug Al-Khalifa; Cong Yu; HV Jagadish,Abstract XML databases often contain documents comprising structured text. Therefore; it isimportant to integrate" information retrieval style" query evaluation; which is well-suited fornatural language text; with standard" database style" query evaluation; which handlesstructured queries efficiently. Relevance scoring is central to information retrieval. In thecase of XML; this operation becomes more complex because the data required for scoringcould reside not directly in an element itself but also in its descendant elements. In thispaper; we propose a bulk-algebra; TIX; and describe how it can be used as a basis forintegrating information retrieval techniques into a standard pipelined database queryevaluation engine. We develop new evaluation strategies essential to obtaining goodperformance; including a stack-based TermJoin algorithm for efficiently scoring composite …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,125
Finding related tables,Anish Das Sarma; Lujun Fang; Nitin Gupta; Alon Halevy; Hongrae Lee; Fei Wu; Reynold Xin; Cong Yu,Abstract We consider the problem of finding related tables in a large corpus of heterogenoustables. Detecting related tables provides users a powerful tool for enhancing their tables withadditional data and enables effective reuse of available public data. Our first contribution is aframework that captures several types of relatedness; including tables that are candidatesfor joins and tables that are candidates for union. Our second contribution is a set ofalgorithms for detecting related tables that can be either unioned or joined. We describe aset of experiments that demonstrate that our algorithms produce highly related tables. Wealso show that we can often improve the results of table search by pulling up tables that areranked much lower based on their relatedness to top-ranked tables. Finally; we describehow to scale up our algorithms and show the results of running it on a corpus of over a …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,89
XML schema refinement through redundancy detection and normalization,Cong Yu; HV Jagadish,Abstract As XML becomes increasingly popular; XML schema design has become anincreasingly important issue. One of the central objectives of good schema design is to avoiddata redundancies: redundantly stored information can lead not just only to a higher datastorage cost but also to increased costs for data transfer and data manipulation.Furthermore; such data redundancies can lead to potential update anomalies; rendering thedatabase inconsistent. One strategy to avoid data redundancies is to design redundancy-free schema from the start on the basis of known functional dependencies. We observe thatXML databases are often" casually designed" and XML FDs may not be determined inadvance. Under such circumstances; discovering XML data redundancies from the dataitself becomes necessary and is an integral part of the schema refinement (or re-design) …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,88
Socialscope: Enabling information discovery on social content sites,Sihem Amer-Yahia; Laks Lakshmanan; Cong Yu,Abstract: Recently; many content sites have started encouraging their users to engage insocial activities such as adding buddies on Yahoo! Travel and sharing articles with theirfriends on New York Times. This has led to the emergence of {\em social content sites};which is being facilitated by initiatives like OpenID (this http URL) and OpenSocial (this httpURL). These community standards enable the open access to users' social profiles andconnections by individual content sites and are bringing content-oriented sites and socialnetworking sites ever closer. The integration of content and social information raises newchallenges for {\em information management and discovery} over such sites. We propose alogical architecture; named\kw {SocialScope}; consisting of three layers; for tackling thechallenges. The {\em content management} layer is responsible for integrating …,arXiv preprint arXiv:0909.2058,2009,84
Rex: explaining relationships between entity pairs,Lujun Fang; Anish Das Sarma; Cong Yu; Philip Bohannon,Abstract Knowledge bases of entities and relations (either constructed manually orautomatically) are behind many real world search engines; including those at Yahoo!;Microsoft; and Google. Those knowledge bases can be viewed as graphs with nodesrepresenting entities and edges representing (primary) relationships; and various studieshave been conducted on how to leverage them to answer entity seeking queries.Meanwhile; in a complementary direction; analyses over the query logs have enabledresearchers to identify entity pairs that are statistically correlated. Such entity relationshipsare then presented to search users through the" related searches" feature in modern searchengines. However; entity relationships thus discovered can often be" puzzling" to the usersbecause why the entities are connected is often indescribable. In this paper; we propose …,Proceedings of the VLDB Endowment,2011,82
Dynamic relationship and event discovery,Anish Das Sarma; Alpa Jain; Cong Yu,Abstract This paper studies the problem of dynamic relationship and event discovery. Alarge body of previous work on relation extraction focuses on discovering predefined andstatic relationships between entities. In contrast; we aim to identify temporally defined (eg; co-bursting) relationships that are not predefined by an existing schema; and we identify theunderlying time constrained events that lead to these relationships. The key challenges inidentifying such events include discovering and verifying dynamic connections amongentities; and consolidating binary dynamic connections into events consisting of a set ofentities that are connected at a given time period. We formalize this problem and introducean efficient end-to-end pipeline as a solution. In particular; we introduce two formal notions;global temporal constraint cluster and local temporal constraint cluster; for detecting …,Proceedings of the fourth ACM international conference on Web search and data mining,2011,70
Computational Journalism: A Call to Arms to Database Researchers.,Sarah Cohen; Chengkai Li; Jun Yang; Cong Yu,The digital age has brought sweeping changes to the news media. While onlineconsumption of news is on the rise; fewer people today read newspapers. Newspaperadvertising revenues fell by a total of 23% in 2007 and 2008; and tumbled 26% more in2009 [1]. This continuing decline of the traditional news media affects not only how news aredisseminated and consumed; but also how much and what types of news are produced;which have profound impact on the well-being of our society. In the past; we have come torely heavily upon the traditional news organizations for their investigative reporting to holdgovernments; corporations; and powerful individuals accountable to our society. The declineof traditional media has led to dwindling support for this style of journalism; which isconsidered as cost-intensive and having little revenue-generating potential. Today; there …,CIDR,2011,70
Distributed cube materialization on holistic measures,Arnab Nandi; Cong Yu; Philip Bohannon; Raghu Ramakrishnan,Cube computation over massive datasets is critical for many important analyses done in thereal world. Unlike commonly studied algebraic measures such as SUM that are amenable toparallel computation; efficient cube computation of holistic measures such as TOP-K is non-trivial and often impossible with current methods. In this paper we detail real-worldchallenges in cube materialization tasks on Web-scale datasets. Specifically; we identify animportant subset of holistic measures and introduce MR-Cube; a MapReduce basedframework for efficient cube computation on these measures. We provide extensiveexperimental analyses over both real and synthetic data. We demonstrate that; unlikeexisting techniques which cannot scale to the 100 million tuple mark for our datasets; MR-Cube successfully and efficiently computes cubes with holistic measures over billion …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,69
Recommendation diversification using explanations,Cong Yu; Laks VS Lakshmanan; Sihem Amer-Yahia,We introduce the novel notion of {\em explanation-based diversification} to address the well-known problem of {\em over-specialization} in item recommendations. Over-specialization inrecommender systems leads to result sets with items that are too similar to one another; thusreducing the diversity of results and limiting user choices. Traditionally; the problem isaddressed through {\em attribute-based diversification}| grouping items in the result set thatshare many common attributes (eg; genre for movies) and selecting only a limited number ofitems from each group. It is; however; not always applicable; especially for social contentrecommendations. For example; attributes may not be available as in the case ofrecommending URLs for users of del. icio. us. Explanation-based diversification provides anovel and complementary alternative| it leverages the {\em reason for which a particular …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,67
Querying complex structured databases,Cong Yu; HV Jagadish,Abstract Correctly generating a structured query (eg; an XQuery or a SQL query) requiresthe user to have a full understanding of the database schema; which can be a daunting task.Alternative query models have been proposed to give users the ability to query the databasewithout schema knowledge. Those models; including simple keyword search and labeledkeyword search; aim to extract meaningful data fragments that match the structure-free queryconditions (eg; keywords) based on various matching semantics. Typically; the matchingsemantics are content-based: they are defined on data node inter-relationships and incursignificant query evaluation cost. Our first contribution is a novel matching semantics basedon analyzing the database schema. We show that query models employing a schema-basedmatching semantics can reduce query evaluation cost significantly while maintaining or …,Proceedings of the 33rd international conference on Very large data bases,2007,64
Interactive itinerary planning,Senjuti Basu Roy; Gautam Das; Sihem Amer-Yahia; Cong Yu,Planning an itinerary when traveling to a city involves substantial effort in choosing Points-of-Interest (POIs); deciding in which order to visit them; and accounting for the time it takes tovisit each POI and transit between them. Several online services address different aspects ofitinerary planning but none of them provides an interactive interface where users givefeedbacks and iteratively construct their itineraries based on personal interests and timebudget. In this paper; we formalize interactive itinerary planning as an iterative processwhere; at each step:(1) the user provides feedback on POIs selected by the system;(2) thesystem recommends the best itineraries based on all feedback so far; and (3) the systemfurther selects a new set of POIs; with optimal utility; to solicit feedback for; at the next step.This iterative process stops when the user is satisfied with the recommended itinerary. We …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,61
Efficient discovery of XML data redundancies,Cong Yu; HV Jagadish,Abstract As XML becomes widely used; dealing with redundancies in XML data has becomean increasingly important issue. Redundantly stored information can lead not just to a higherdata storage cost; but also to increased costs for data transfer and data manipulation.Furthermore; such data redundancies can lead to potential update anomalies; rendering thedatabase inconsistent. One way to avoid data redundancies is to employ good schemadesign based on known functional dependencies. In fact; several recent studies havefocused on defining the notion of XML Functional Dependencies (XML FDs) to capture XMLdata redundancies. We observe further that XML databases are often" casually designed"and XML FDs may not be determined in advance. Under such circumstances; discoveringXML data redundancies (in terms of FDs) from the data itself becomes necessary and is …,Proceedings of the 32nd international conference on Very large data bases,2006,59
Recommendation System Using Social Behavior Analysis and Vocabulary Taxonomies,*,*,*,*,59
Leveraging Tagging to Model User Interests in del. icio. us.,Julia Stoyanovich; Sihem Amer-Yahia; Cameron Marlow; Cong Yu,*,AAAI Spring Symposium: Social Information Processing,2008,57
Getting recommender systems to think outside the box,Zeinab Abbassi; Sihem Amer-Yahia; Laks VS Lakshmanan; Sergei Vassilvitskii; Cong Yu,Abstract We examine the case of over-specialization in recommender systems; which resultsfrom returning items that are too similar to those previously rated by the user. We proposeOutside-The-Box (otb) recommendation; which takes some risk to help users make freshdiscoveries; while maintaining high relevance. The proposed formalization relies on itemregions and attempts to identify regions that are under-exposed to the user. We develop arecommendation algorithm which achieves a compromise between relevance and risk tofind otb items. We evaluate this approach on the MovieLens data set and compare our otbrecommendations against conventional recommendation strategies.,Proceedings of the third ACM conference on Recommender systems,2009,54
Structured data meets the Web: a few observations.,Jayant Madhavan; Alon Y Halevy; Shirley Cohen; Xin Luna Dong; Shawn R Jeffery; David Ko; Cong Yu,Abstract The World Wide Web is witnessing an increase in the amount of structured content–vast heterogeneous collections of structured data are on the rise due to the Deep Web;annotation schemes like Flickr; and sites like Google Base. While this phenomenon iscreating an opportunity for structured data management; dealing with heterogeneity on theweb-scale presents many new challenges. In this paper we articulate challenges based onour experience with addressing them at Google; and offer some principles for addressingthem in a general fashion.,IEEE Data Eng. Bull.,2006,52
Data cube materialization and mining over mapreduce,Arnab Nandi; Cong Yu; Philip Bohannon; Raghu Ramakrishnan,Computing interesting measures for data cubes and subsequent mining of interesting cubegroups over massive data sets are critical for many important analyses done in the realworld. Previous studies have focused on algebraic measures such as SUM that areamenable to parallel computation and can easily benefit from the recent advancement ofparallel computing infrastructure such as MapReduce. Dealing with holistic measures suchas TOP-K; however; is nontrivial. In this paper; we detail real-world challenges in cubematerialization and mining tasks on web-scale data sets. Specifically; we identify animportant subset of holistic measures and introduce MR-Cube; a MapReduce-basedframework for efficient cube computation and identification of interesting cube groups onholistic measures. We provide extensive experimental analyses over both real and …,IEEE transactions on knowledge and data engineering,2012,49
Constructing and exploring composite items,Senjuti Basu Roy; Sihem Amer-Yahia; Ashish Chawla; Gautam Das; Cong Yu,Abstract Nowadays; online shopping has become a daily activity. Web users purchase avariety of items ranging from books to electronics. The large supply of online products callsfor sophisticated techniques to help users explore available items. We propose to buildcomposite items which associate a central item with a set of packages; formed by satelliteitems; and help users explore them. For example; a user shopping for an iPhone (ie; thecentral item) with a price budget can be presented with both the iPhone and a package ofother items that match well with the iPhone (eg;{Belkin case; Bose sounddock; Kroo USBcable}) as a composite item; whose total price is within the user's budget. We define andstudy the problem of effective construction and exploration of large sets of packagesassociated with a central item; and design and implement efficient algorithms for solving …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,48
Space efficiency in group recommendation,Senjuti Basu Roy; Sihem Amer-Yahia; Ashish Chawla; Gautam Das; Cong Yu,Abstract Imagine a system that gives you satisfying recommendations when you want to renta movie with friends or find a restaurant to celebrate a colleague's farewell: at the core ofsuch a system is what we call group recommendation. While computing individualrecommendations have received lots of attention (eg; Netflix prize); group recommendationhas been confined to studying users' satisfaction with different aggregation strategies. In thispaper (Some results are published in an earlier conference paper (Amer-Yahia et al. inVLDB; 2009). See Sect." Paper contributions and outline" for details.); we describe thechallenges and desiderata of group recommendation and formalize different groupconsensus semantics that account for both an item's predicted ratings to the group membersand the disagreements among them. We focus on the design and implementation of …,The VLDB Journal—The International Journal on Very Large Data Bases,2010,45
TIMBER: A native system for querying XML,Stelios Paparizos; Shurug Al-Khalifa; Adriane Chapman; HV Jagadish; Laks VS Lakshmanan; Andrew Nierman; Jignesh M Patel; Divesh Srivastava; Nuwee Wiwatwattana; Yuqing Wu; Cong Yu,Abstract XML has become ubiquitous; and XML data has to be managed in databases. Thecurrent industry standard is to map XML data into relational tables and store this informationin a relational database. Such mappings create both expressive power problems andperformance problems. In the T IMBER [7] project we are exploring the issues involved instoring XML in native format. We believe that the key intellectual contribution of this system isa comprehensive set-at-a-time query processing ability in a native XML store; with all thestandard components of relational query processing; including algebraic rewriting and acost-based optimizer.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,43
Constructing travel itineraries from tagged geo-temporal breadcrumbs,Munmun De Choudhury; Moran Feldman; Sihem Amer-Yahia; Nadav Golbandi; Ronny Lempel; Cong Yu,Abstract Vacation planning is a frequent laborious task which requires skilled interactionwith a multitude of resources. This paper develops an end-to-end approach for constructingintra-city travel itineraries automatically by tapping a latent source reflecting geo-temporalbreadcrumbs left by millions of tourists. In particular; the popular rich media sharing site;Flickr; allows photos to be stamped by the date and time of when they were taken; and bemapped to Points Of Interest (POIs) by latitude-longitude information as well as semanticmetadata (eg; tags) that describe them.,Proceedings of the 19th international conference on World wide web,2010,42
Enabling Schema-Free XQuery with meaningful query focus,Yunyao Li; Cong Yu; HV Jagadish,Abstract The widespread adoption of XML holds the promise that document structure can beexploited to specify precise database queries. However; users may have only a limitedknowledge of the XML structure; and may be unable to produce a correct XQueryexpression; especially in the context of a heterogeneous information collection. The defaultis to use keyword-based search and we are all too familiar with how difficult it is to obtainprecise answers by these means. We seek to address these problems by introducing thenotion of Meaningful Query Focus (MQF) for finding related nodes within an XML document.MQF enables users to take full advantage of the preciseness and efficiency of XQuerywithout requiring (perfect) knowledge of the document structure. Such a Schema-FreeXQuery is potentially of value not just to casual users with partial knowledge of schema …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,41
Method and system for discovering dynamic relations among entities,*,Method; system; and programs for detecting dynamic relationship and discovering dynamicevents. Data from a first data source is first received. At least one dynamic relation candidateis identified and each dynamic relation candidate involves multiple entities. The at least onedynamic relation candidate is identified based on temporal properties with respect to theentities exhibited in the data from the first data source. Dynamic relations are then extractedby corroborating the temporal properties of the entities involved in the at least one dynamicrelation candidate with that of the same entities exhibited in data from a second data source.Then; a dynamic event that gives rise to the dynamic relations among different entities isdetected.,*,2016,40
On social event organization,Keqian Li; Wei Lu; Smriti Bhagat; Laks VS Lakshmanan; Cong Yu,Abstract Online platforms; such as Meetup and Plancast; have recently become popular forplanning gatherings and event organization. However; there is a surprising lack of studieson how to effectively and efficiently organize social events for a large group of peoplethrough such platforms. In this paper; we study the key computational problem involved inorganization of social events; to our best knowledge; for the first time. We propose the SocialEvent Organization (SEO) problem as one of assigning a set of events for a group of users toattend; where the users are socially connected with each other and have innate levels ofinterest in those events. As a first step toward Social Event Organization; we introduce aformal definition of a restricted version of the problem and show that it is NP-hard and is hardto approximate. We propose efficient heuristic algorithms that improve upon simple …,Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,2014,35
Mri: Meaningful interpretations of collaborative ratings,Mahashweta Das; Sihem Amer-Yahia; Gautam Das; Cong Yu,ABSTRACT Collaborative rating sites have become essential resources that many usersconsult to make purchasing decisions on various items. Ideally; a user wants to quicklydecide whether an item is desirable; especially when many choices are available. Inpractice; however; a user either spends a lot of time examining reviews before making aninformed decision; or simply trusts overall rating aggregations associated with an item. Inthis paper; we argue that neither option is satisfactory and propose a novel and powerfulthird option; Meaningful Ratings Interpretation (MRI); that automatically provides ameaningful interpretation of ratings associated with the input items. As a simple example;given the movie “Usual Suspects;” instead of simply showing the average rating of 8.7 fromall reviewers; MRI produces a set of meaningful factoids such as “male reviewers under …,Proceedings of the VLDB Endowment,2011,35
Constraint-based XML query rewriting for data integration,*,*,*,*,32
From del. icio. us to x. qui. site: recommendations in social tagging sites,Sihem Amer-Yahia; Alban Galland; Julia Stoyanovich; Cong Yu,Abstract We present X. QUI. SITE; a scalable system for managing recommendations forsocial tagging sites like del. icio. us. seamlessly incorporates various user behaviors into therecommendations and aims to recommend not only items of interest; but also other relevantinformation like interesting people and/or topics. Explanations are also provided so thatusers can obtain a better understanding of the recommendations and decide whichrecommendations to pursue further. We discuss the technical challenges involved incharacterizing different user behaviors and in efficiently computing recommendationexplanations.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,31
Applying WebTables in Practice.,Sreeram Balakrishnan; Alon Y Halevy; Boulos Harb; Hongrae Lee; Jayant Madhavan; Afshin Rostamizadeh; Warren Shen; Kenneth Wilder; Fei Wu; Cong Yu,*,CIDR,2015,30
Purple sox extraction management system,Philip Bohannon; Srujana Merugu; Cong Yu; Vipul Agarwal; Pedro DeRose; Arun Iyer; Ankur Jain; Vinay Kakade; Mridul Muralidharan; Raghu Ramakrishnan; Warren Shen,Abstract We describe the Purple SOX (PSOX) EMS; a prototype Extraction ManagementSystem currently being built at Yahoo!. The goal of the PSOX EMS is to manage a largenumber of sophisticated extraction pipelines across different application domains; at the webscale and with minimum human involvement. Three key value propositions are described:extensibility; the ability to swap in and out extraction operators; explainability; the ability totrack the provenance of extraction results; and social feedback support; the facility forgathering and reconciling multiple; potentially conflicting sources.,ACM SIGMOD Record,2009,29
Social behavior analysis and inferring social networks for a recommendation system,*,Systems and methods are provided for determining items or people of potential interest torecommend to users in a computer-based network. Implied social networks may bedetermined based at least in part on obtained social behavior information. Items or people ofpotential interest to users may be determined based at least in part on implied socialnetwork information. Vocabulary taxonomies may be associated with; or used indetermining; implied social networks.,*,2011,27
Toward computational fact-checking,You Wu; Pankaj K Agarwal; Chengkai Li; Jun Yang; Cong Yu,Abstract Our news are saturated with claims of" facts" made from data. Database researchhas in the past focused on how to answer queries; but has not devoted much attention todiscerning more subtle qualities of the resulting claims; eg; is a claim" cherry-picking"? Thispaper proposes a framework that models claims based on structured data as parameterizedqueries. A key insight is that we can learn a lot about a claim by perturbing its parametersand seeing how its conclusion changes. This framework lets us formulate practical fact-checking tasks---reverse-engineering (often intentionally) vague claims; and counteringquestionable claims---as computational problems. Along with the modeling framework; wedevelop an algorithmic framework that enables efficient instantiations of" meta" algorithmsby supplying appropriate algorithmic building blocks. We present real-world examples …,Proceedings of the VLDB Endowment,2014,26
Building community-centric information exploration applications on social content sites,Sihem Amer-Yahia; Jian Huang; Cong Yu,Abstract Social content sites [4]; which integrate traditional content sites with socialnetworking features; have recently emerged as an exciting new trend on the Web. Users onthose sites share content and form various communities based on explicit friendship; sharedinterest and common user properties. Recently; we proposed SOCIALSCOPE; a three-layered architecture to address the information management challenges in social contentsites. In this paper; we focus on the information discovery and the information presentationlayers; and describe how our previously proposed language; Jelly [3]; is supported inSOCIALSCOPE to build community-centric information exploration applications on socialcontent sites.,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,26
Scalable column concept determination for web tables using large knowledge bases,Dong Deng; Yu Jiang; Guoliang Li; Jian Li; Cong Yu,Abstract Tabular data on the Web has become a rich source of structured data that is usefulfor ordinary users to explore. Due to its potential; tables on the Web have recently attracted anumber of studies with the goals of understanding the semantics of those Web tables andproviding effective search and exploration mechanisms over them. An important part of tableunderstanding and search is column concept determination; ie; identifying the mostappropriate concepts associated with the columns of the tables. The problem becomesespecially challenging with the availability of increasingly rich knowledge bases that containhundreds of millions of entities. In this paper; we focus on an important instantiation of thecolumn concept determination problem; namely; the concepts of a column are determinedby fuzzy matching its cell values to the entities within a large knowledge base. We provide …,Proceedings of the VLDB Endowment,2013,23
Entity-relationship queries over wikipedia,Xiaonan Li; Chengkai Li; Cong Yu,Abstract Wikipedia is the largest user-generated knowledge base. We propose a structuredquery mechanism; entity-relationship query; for searching entities in the Wikipedia corpus bytheir properties and interrelationships. An entity-relationship query consists of multiplepredicates on desired entities. The semantics of each predicate is specified with keywords.Entity-relationship query searches entities directly over text instead of preextractedstructured data stores. This characteristic brings two benefits:(1) Query semantics can beintuitively expressed by keywords;(2) It only requires rudimentary entity annotation; which issimpler than explicitly extracting and reasoning about complex semantic information beforequery-time. We present a ranking framework for general entity-relationship queries and aposition-based Bounded Cumulative Model (BCM) for accurate ranking of query answers …,ACM Transactions on Intelligent Systems and Technology (TIST),2012,22
Yoopick: A Combinatorial Sports Prediction Market.,Sharad Goel; David M Pennock; Daniel M Reeves; Cong Yu,*,AAAI,2008,21
Cloning and mapping of the XRN2 gene to human chromosome 20p11. 1–p11. 2,Min Zhang; Long Yu; Yurong Xin; Peirong Hu; Qiang Fu; Cong Yu; Shouyuan Zhao,The Dhm1 gene is the mouse homologue of the dhp1+ gene of Schizosaccharomycespombe; which is involved in homologous recombination and RNA metabolism; such as RNAsynthesis and RNA trafficking; in S. pombe. Complementation analysis showed the Dhm1gene on a multicopy plasmid can rescue the temperature-sensitivity mutation of dhp1ts andthe lethality of the dhp1 null mutation. This finding suggests that Dhm1 has a function inmouse similar to that of dhp1+. The human homologue of this gene; XRN2; has beenidentified. A 3.6-kb transcript of XRN2 was detected in 16 tissues examined and was moreabundant in testis. By radiation hybrid panel mapping; the XRN2 gene was localized tochromosome 20p11. 1–p11. 2 between markers D20S180 and D20S871.,Genomics,1999,20
The quest to automate fact-checking,Naeemul Hassan; Bill Adair; James T Hamilton; Chengkai Li; Mark Tremayne; Jun Yang; Cong Yu,The growing movement of political fact-checking plays an important role in increasingdemocratic accountability and improving political discourse [7; 3]. Politicians and mediafigures make claims about “facts” all the time; but the new army of fact-checkers can oftenexpose claims that are false; exaggerated or half-truths. The number of active fact-checkingwebsites has grown from 44 a year ago to 64 in 2015; according the Duke Reporters's Lab.1 The challenge is that the human fact-checkers frequently have difficulty keeping up withthe rapid spread of misinformation. Technology; social media and new forms of journalismhave made it easier than ever to disseminate falsehoods and half-truths faster than the fact-checkers can expose them. There are several reasons that the falsehoods frequentlyoutpace the truth. One reason is that fact-checking is an intellectually demanding and …,world,2015,18
Who tags what?: an analysis framework,Mahashweta Das; Saravanan Thirumuruganathan; Sihem Amer-Yahia; Gautam Das; Cong Yu,Abstract The rise of Web 2.0 is signaled by sites such as Flickr; del. icio. us; and YouTube;and social tagging is essential to their success. A typical tagging action involves threecomponents; user; item (eg; photos in Flickr); and tags (ie; words or phrases). Analyzing howtags are assigned by certain users to certain items has important implications in helpingusers search for desired information. In this paper; we explore common analysis tasks andpropose a dual mining framework for social tagging behavior mining. This framework iscentered around two opposing measures; similarity and diversity; being applied to one ormore tagging components; and therefore enables a wide range of analysis scenarios suchas characterizing similar users tagging diverse items with similar tags; or diverse userstagging similar items with diverse tags; etc. By adopting different concrete measures for …,Proceedings of the VLDB Endowment,2012,18
Large scale entity-specific resource classification,*,A system and method is described for large scale entity-specific classification of each entity-specific set of candidates in a collection of candidates for each specific entity in a collectionof entities. The collection of entities may comprise a specific category or domain of entities(eg schools; restaurants; manufacturers; products; events; people). Candidates maycomprise webpages or other resources with resource identifiers. Entity specific sets ofcandidates may be found by leveraging search engine query results and user interactiontherewith for queries based on entity-specific attributes. The relationship (s) or class (es) forwhich candidate resources are being classified relative to a specific entity may comprise anauthoritative; official home page (OHP); or other class (eg fan page; review; aggregator)relative to a specific entity. A feature generator generates entity-specific features for …,*,2016,16
Key changes in denervated muscles and their impact on regeneration and reinnervation,Peng Wu; Aditya Chawla; Robert J Spinner; Cong Yu; Michael J Yaszemski; Anthony J Windebank; Huan Wang,Abstract The neuromuscular junction becomes progressively less receptive to regeneratingaxons if nerve repair is delayed for a long period of time. It is difficult to ascertain thedenervated muscle's residual receptivity by time alone. Other sensitive markers that closelycorrelate with the extent of denervation should be found. After a denervated muscledevelops a fibrillation potential; muscle fiber conduction velocity; muscle fiber diameter;muscle wet weight; and maximal isometric force all decrease; remodeling increasesneuromuscular junction fragmentation and plantar area; and expression of myogenesis-related genes is initially up-regulated and then down-regulated. All these changes correlatewith both the time course and degree of denervation. The nature and time course of thesedenervation changes in muscle are reviewed from the literature to explore their roles in …,Neural regeneration research,2014,16
Exploiting group recommendation functions for flexible preferences,Senjuti Basu Roy; Saravanan Thirumuruganathan; Sihem Amer-Yahia; Gautam Das; Cong Yu,We examine the problem of enabling the flexibility of updating one's preferences in grouprecommendation. In our setting; any group member can provide a vector of preferences that;in addition to past preferences and other group members' preferences; will be accounted forin computing group recommendation. This functionality is essential in many grouprecommendation applications; such as travel planning; online games; book clubs; orstrategic voting; as it has been previously shown that user preferences may vary dependingon mood; context; and company (ie; other people in the group). Preferences are enforced inan feedback box that replaces preferences provided by the users by a potentially differentfeedback vector that is better suited for maximizing the individual satisfaction whencomputing the group recommendation. The feedback box interacts with a traditional …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,15
On one of the few objects,You Wu; Pankaj K Agarwal; Chengkai Li; Jun Yang; Cong Yu,Abstract Objects with multiple numeric attributes can be compared within any"subspace"(subset of attributes). In applications such as computational journalism; users areinterested in claims of the form: Karl Malone is one of the only two players in NBA historywith at least 25;000 points; 12;000 rebounds; and 5;000 assists in one's career. Onechallenge in identifying such" one-of-the-k" claims (k= 2 above) is ensuring their"interestingness". A small k is not a good indicator for interestingness; as one can often makesuch claims for many objects by increasing the dimensionality of the subspace considered.We propose a uniqueness-based interestingness measure for one-of-the-few claims that isintuitive for non-technical users; and we design algorithms for finding all interesting claims(across all subspaces) from a dataset. Sometimes; users are interested primarily in the …,Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,2012,13
EntityEngine: Answering entity-relationship queries using shallow semantics,Xiaonan Li; Chengkai Li; Cong Yu,Abstract We introduce EntityEngine; a system for answering entity-relationship queries overtext. Such queries combine SQL-like structures with IR-style keyword constraints andtherefore; can be expressive and flexible in querying about entities and their relationships.EntityEngine consists of various offline and online components; including a position-basedranking model for accurate ranking of query answers and a novel entity-centric index forefficient query evaluation.,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,13
Integration of IR into an XML Database.,Cong Yu; Hong Qi; HV Jagadish,ABSTRACT Structure matching has been the focus and strength of standard XML querying.However; textual content is still an essential component of XML data. It is therefore importantto extend the standard XML database engine to allow for “Information Retrieval” stylequeries; namely;“keyword” based retrieval and “result ranking”. In this paper; we describeour effort in integrating information retrieval techniques into the Timber XML databasesystem being developed at the University of Michigan; and our participation in the INitiativefor the Evaluation of XML Retrieval (INEX).,INEX Workshop,2002,13
Display entity relationship,*,Method; system; and programs for providing one or more explanations. An inquiry isreceived via a communication platform where the inquiry is about how a set of entities arerelated. Information is retrieved from a knowledge storage in accordance with the set ofentities and such information describes a plurality of entities and relationships existingamong the plurality of entities. Based on such retrieved information; one or moreexplanations with respect to each relationship by which the set of entities are connected aregenerated. The one or more explanations are then transmitted as a response to the inquiry.,*,2015,12
Apparatus and methods for operator training in information extraction,*,After receipt of a training and execution plan; a trainer operator is automatically trainedbased on specified training documents so as to generate a new trained operator forextracting information from documents. The new trained operator is a new version of thetrainee operator. Both trainee operators are automatically retained for later use in extractinginformation from one or more unknown documents. After receipt of the training andexecution plan; the new trained operator is automatically executed on one or more unknowndocuments so as to extract information from such one or more unknown documents.,*,2013,12
Actively soliciting feedback for query answers in keyword search-based data integration,Zhepeng Yan; Nan Zheng; Zachary G Ives; Partha Pratim Talukdar; Cong Yu,Abstract The problem of scaling up data integration; such that new sources can be quicklyutilized as they are discovered; remains elusive: global schemas for integrated data aredifficult to develop and expand; and schema and record matching techniques are limited bythe fact that data and metadata are often under-specified and must be disambiguated bydata experts. One promising approach is to avoid using a global schema; and instead todevelop keyword search-based data integration--where the system lazily discoversassociations enabling it to join together matches to keywords; and return ranked results. Theuser is expected to understand the data domain and provide feedback about answers'quality. The system generalizes such feedback to learn how to correctly integrate data. Amajor open challenge is that under this model; the user only sees and offers feedback on …,Proceedings of the VLDB Endowment,2013,12
Constructing travel itineraries from tagged geo-temporal photographs,*,One embodiment accesses two or more photos taken by one or more travelers at one ormore destinations and one or more points-of-interest located within the destinations;constructs one or more photo streams for each unique traveler-destination combination;wherein each one of the photo streams comprises two or more of the photos taken by thecorresponding traveler at the corresponding destination; maps each one of the photos toone of the points-of-interest; constructs one or more timed paths for each unique traveler-destination combination based on the photo streams and the mapping between the photosand the points-of-interest; wherein each one of the timed paths comprises one or more of thepoints-of-interest located within the corresponding destination and visited by thecorresponding travel; and constructs an itinerary based on a start point-of-interest; an end …,*,2012,12
System and method for finding unexpected; but relevant content in an information retrieval system,*,An improved method for information retrieval in web query and recommendation systems;where items that are likely unfamiliar to the users of the system; but potentially relevant; arerecommended. In a recommendation system having ratings by a plurality of users for aplurality of items; items are assigned to one or more data regions based on item attributes oruser activity. Source regions are identified for each of the data regions. For a given user;data regions with which both the user and the user's social network are unfamiliar areidentified. Within a given data region; the relevance of items to the user within such regionsis evaluated using ratings provided by other users who have entered ratings similar to theuser in source regions for the data region. Items receiving the highest relevance score arerecommended to the user.,*,2012,12
Data in; fact out: automated monitoring of facts by FactWatcher,Naeemul Hassan; Afroza Sultana; You Wu; Gensheng Zhang; Chengkai Li; Jun Yang; Cong Yu,Abstract Towards computational journalism; we present FactWatcher; a system that helpsjournalists identify data-backed; attention-seizing facts which serve as leads to news stories.FactWatcher discovers three types of facts; including situational facts; one-of-the-few facts;and prominent streaks; through a unified suite of data model; algorithm framework; and factranking measure. Given an append-only database; upon the arrival of a new tuple;FactWatcher monitors if the tuple triggers any new facts. Its algorithms efficiently search forfacts without exhaustively testing all possible ones. Furthermore; FactWatcher providesmultiple features in striving for an end-to-end system; including fact ranking; fact-to-statement translation and keyword-based fact search.,Proceedings of the VLDB Endowment,2014,11
Incremental discovery of prominent situational facts,Afroza Sultana; Naeemul Hassan; Chengkai Li; Jun Yang; Cong Yu,We study the novel problem of finding new; prominent situational facts; which are emergingstatements about objects that stand out within certain contexts. Many such facts arenewsworthy-eg; an athlete's outstanding performance in a game; or a viral video'simpressive popularity. Effective and efficient identification of these facts assists journalists inreporting; one of the main goals of computational journalism. Technically; we consider anever-growing table of objects with dimension and measure attributes. A situational fact is a“contextual” skyline tuple that stands out against historical tuples in a context; specified by aconjunctive constraint involving dimension attributes; when a set of measure attributes arecompared. New tuples are constantly added to the table; reflecting events happening in thereal world. Our goal is to discover constraint-measure pairs that qualify a new tuple as a …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,11
Prioritization of Domain-Specific Web Information Extraction.,Jian Huang; Cong Yu,Abstract It is often desirable to extract structured information from raw web pages for betterinformation browsing; query answering; and pattern mining. Many such InformationExtraction (IE) technologies are costly and applying them at the web-scale is impractical. Inthis paper; we propose a novel prioritization approach where candidate pages from thecorpus are ordered according to their expected contribution to the extraction results andthose with higher estimated potential are extracted earlier. Systems employing this approachcan stop the extraction process at any time when the resource gets scarce (ie; not all pagesin the corpus can be processed); without worrying about wasting extraction effort onunimportant pages. More specifically; we define a novel notion to measure the value ofextraction results and design various mechanisms for estimating a candidate page's …,AAAI,2010,11
Querying XML using structures and keywords in Timber,Cong Yu; HV Jagadish; Dragomir R Radev,Abstract This demonstration will describe how Timber; a native XML database system; hasbeen extended with the capability to answer XML-style structured queries (eg; XQuery) withembedded IR-style keyword-based non-boolean conditions. With the original structuredquery processing engine and the IR extensions built into the system; Timber is well suited forefficiently and effectively processing queries with both structural and textual contentconstraints.,Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,2003,11
An expressive framework and efficient algorithms for the analysis of collaborative tagging,Mahashweta Das; Saravanan Thirumuruganathan; Sihem Amer-Yahia; Gautam Das; Cong Yu,Abstract The rise of Web 2.0 is signaled by sites such as Flickr; del. icio. us; and YouTube;and social tagging is essential to their success. A typical tagging action involves threecomponents; user; item (eg; photos in Flickr); and tag s (ie; words or phrases). Analyzinghow tags are assigned by certain users to certain items has important implications in helpingusers search for desired information. In this paper; we develop a dual mining framework toexplore tagging behavior. This framework is centered around two opposing measures;similarity and diversity; applied to one or more tagging components; and therefore enables awide range of analysis scenarios such as characterizing similar users tagging diverse itemswith similar tags or diverse users tagging similar items with diverse tags. By adoptingdifferent concrete measures for similarity and diversity in the framework; we show that a …,The VLDB Journal,2014,10
Jelly: A language for building community-centric information exploration applications,Sihem Amer-Yahia; Jian Huang; Cong Yu,Social content sites [3]; which integrate traditional content sites (eg; Yahoo! Travel) withsocial network features; have recently emerged as a significant new trend on the Web. Userson those sites share content and form various communities based on explicit friendships orshared interests. However; the existing information exploration mechanisms rarely leveragethe rich community structure. In this work; we aim to unlock the value of social content sitesby helping developers specify community-based information exploration strategies in aflexible and declarative way. Our solution makes use of two key notions; topics andcommunities; in order to identify socially and semantically relevant information for users.Specifically; we propose JELLY as a language for developing community-centric informationexploration applications. JELLY provides several primitives which exploit both content …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,10
TIMBER: A native XML database,Stelios Paparizos; Shurug Al-Khalifa; Y Wu; N Wiwatwattana; HV Jagadish; Andrew Nierman; C Yu; LVS Lakshmanan; D Srivastava; A Chapman; Jignesh M Patel,This paper describes the overall design and architecture of the Timber XML databasesystem currently being implemented at the University of Michigan. The system is based upona bulk algebra for manipulating trees; and natively stores XML. New access methods havebeen developed to evaluate queries in the XML context; and new cost estimation and queryoptimization techniques have also been developed. We present performance numbers tosupport some of our design decisions. We believe that the key intellectual contribution of thissystem is a comprehensive set-at-a-time query processing ability in a native XML store; withall the standard components of relational query processing; including algebraic rewritingand a cost-based optimizer.,*,2002,10
Synthesizing Union Tables from the Web.,Xiao Ling; Alon Y Halevy; Fei Wu; Cong Yu,Abstract Several recent works have focused on harvesting HTML tables from the Web andrecovering their semantics [Cafarella et al.; 2008a; Elmeleegy et al.; 2009; Limaye et al.;2010; Venetis et al.; 2011]. As a result; hundreds of millions of high quality structured datatables can now be explored by the users. In this paper; we argue that those efforts onlyscratch the surface of the true value of structured data on the Web; and study the challengingproblem of synthesizing tables from the Web; ie; producing never-before-seen tables fromraw tables on the Web. Table synthesis offers an important semantic advantage: when a setof related tables are combined into a single union table; powerful mechanisms; such astemporal or geographical comparison and visualization; can be employed to understandand mine the underlying data holistically. We focus on one fundamental task of table …,IJCAI,2013,8
Shallow information extraction for the knowledge web,Denilson Barbosa; Haixun Wang; Cong Yu,A new breed of Information Extraction tools has become popular and shown to be veryeffective in building massive-scale knowledge bases that fuel applications such as questionanswering and semantic search. These approaches rely on Web-scale probabilistic modelspopulated through shallow language processing of the text; pre-existing knowledge; andstructured data already on the Web. This tutorial provides an introduction to thesetechniques; starting from the foundations of information extraction; and covering some of itskey applications.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,8
Method and system for improving quality of web content,*,A method of improving quality of web content. The method includes analyzing search logsassociated with a plurality of web pages by a processor. The search logs are stored in anelectronic storage device. A plurality of queries from the search logs are assembled into oneor more query profiles. Concepts for the one or more query profiles are generated andclassified into one or more concept profiles. Further; the one or more concept profiles areranked based on one or more parameters. The one or more concept profiles are thentransmitted to one or more mediums.,*,2012,8
Active learning in keyword search-based data integration,Zhepeng Yan; Nan Zheng; Zachary G Ives; Partha Pratim Talukdar; Cong Yu,Abstract The problem of scaling up data integration; such that new sources can be quicklyutilized as they are discovered; remains elusive: Global schemas for integrated data aredifficult to develop and expand; and schema and record matching techniques are limited bythe fact that data and metadata are often under-specified and must be disambiguated bydata experts. One promising approach is to avoid using a global schema; and instead todevelop keyword search-based data integration—where the system lazily discoversassociations enabling it to join together matches to keywords; and return ranked results. Theuser is expected to understand the data domain and provide feedback about answers'quality. The system generalizes such feedback to learn how to correctly integrate data. Amajor open challenge is that under this model; the user only sees and offers feedback on …,The VLDB Journal,2015,7
Near neighbor join,Herald Kllapi; Boulos Harb; Cong Yu,An increasing number of Web applications such as friends recommendation depend on theability to join objects at scale. The traditional approach taken is nearest neighbor join (alsocalled similarity join); whose goal is to find; based on a given join function; the closest set ofobjects or all the objects within a distance threshold to each object in the input. Thescalability of techniques utilizing this approach often depends on the characteristics of theobjects and the join function. However; many real-world join functions are intricatelyengineered and constantly evolving; which makes the design of white-box methods that relyon understanding the join function impractical. Finding a technique that can join extremelylarge number of objects with complex join functions has always been a tough challenge. Inthis paper; we propose a practical alternative approach called near neighbor join that …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,7
Diversifying recommendation results through explanation,*,Methods and apparatus for making recommendations of content items to users of computersystems include compiling a database relating a list of items and correspondingexplanations; receiving from a user; through a computer user interface; a request for arecommendation; extracting from the database a preliminary list of items related to therequest; identifying distances between the extracted items based on the explanationcorresponding to each item; and identifying a subset of the preliminary list to form arecommendation list having a limited number of recommendation results with a desiredbalance of both high relevancy and high diversity relative to each other.,*,2013,7
Maprat: Meaningful explanation; interactive exploration and geo-visualization of collaborative ratings,Saravanan Thirumuruganathan; Mahashweta Das; Shrikant Desai; Sihem Amer-Yahia; Gautam Das; Cong Yu,Abstract Collaborative rating sites such as IMDB and Yelp have become rich resources thatusers consult to form judgments about and choose from among competing items. Most ofthese sites either provide a plethora of information for users to interpret all by themselves ora simple overall aggregate information. Such aggregates (eg; average rating over all userswho have rated an item; aggregates along pre-defined dimensions; etc.) can not help a userquickly decide the desirability of an item. In this paper; we build a system MapRat that allowsa user to explore multiple carefully chosen aggregate analytic details over a set of userdemographics that meaningfully explain the ratings associated with item (s) of interest.MapRat allows a user to systematically explore; visualize and understand user ratingpatterns of input item (s) so as to make an informed decision quickly. In the demo …,Proceedings of the VLDB Endowment,2012,7
Recent progress towards an ecosystem of structured data on the Web,Nitin Gupta; Alon Y Halevy; Boulos Harb; Heidi Lam; Hongrae Lee; Jayant Madhavan; Fei Wu; Cong Yu,Google Fusion Tables aims to support an ecosystem of structured data on the Web byproviding a tool for managing and visualizing data on the one hand; and for searching andexploring for data on the other. This paper describes a few recent developments in ourefforts to further the ecosystem.,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,6
Popularity-guided top-k extraction of entity attributes,Matthew Solomon; Cong Yu; Luis Gravano,Abstract Recent progress in information extraction technology has enabled a vast array ofapplications that rely on structured data that is embedded in natural-language text. Inparticular; the extraction of concepts from the Web---with their desired attributes---isimportant to provide applications with rich; structured access to information. In this paper; wefocus on an important family of concepts; namely; entities (eg; people or organizations) andtheir attributes; and study how to efficiently and effectively extract them from Web-accessibletext documents. Unfortunately; information extraction over the Web is challenging for bothquality and efficiency reasons. Regarding quality; many sources on the Web containmisleading or invalid information; furthermore; extraction systems often return incorrect data.Regarding efficiency; information extraction is a time-consuming process; often involving …,Procceedings of the 13th International Workshop on the Web and Databases,2010,6
Structured querying of annotation-rich web text with shallow semantics,Xiaonan Li; Chengkai Li; Cong Yu,Abstract Information discovery on the Web has so far been dominated by keyword-baseddocument search. However; recent years have witnessed arising needs from Web users tosearch for named entities; eg; finding all Silicon Valley companies. With existing Web searchengines; users have to digest returned Web pages by themselves to find the answers. Entitysearch has been introduced as a solution to this problem. However; existing entity searchsystems are limited in their capability to address complex information needs that involvemultiple entities and their interrelationships. In this report; we introduce a novel entity-centricstructured querying mechanism called Shallow Semantic Query (SSQ) to overcome thislimitation. We cover two key technical issues with regard to SSQ; ranking and queryprocessing. Comprehensive experiments show that (1) our ranking model beats state-of …,*,2010,6
Computational fact checking through query perturbations,You Wu; Pankaj K Agarwal; Chengkai Li; Jun Yang; Cong Yu,Abstract Our media is saturated with claims of “facts” made from data. Database researchhas in the past focused on how to answer queries; but has not devoted much attention todiscerning more subtle qualities of the resulting claims; for example; is a claim “cherry-picking”? This article proposes a framework that models claims based on structured data asparameterized queries. Intuitively; with its choice of the parameter setting; a claim presents aparticular (and potentially biased) view of the underlying data. A key insight is that we canlearn a lot about a claim by “perturbing” its parameters and seeing how its conclusionchanges. For example; a claim is not robust if small perturbations to its parameters canchange its conclusions significantly. This framework allows us to formulate practical fact-checking tasks—reverse-engineering vague claims; and countering questionable claims …,ACM Transactions on Database Systems (TODS),2017,5
Efficient evaluation of object-centric exploration queries for visualization,You Wu; Boulos Harb; Jun Yang; Cong Yu,Abstract The most effective way to explore data is through visualizing the results ofexploration queries. For example; an exploration query could be an aggregate of somemeasures over time intervals; and a pattern or abnormality can be discovered through a timeseries plot of the query results. In this paper; we examine a special kind of exploration query;namely object-centric exploration query. Common examples include claims made aboutathletes in sports databases; such as" it is newsworthy that LeBron James has scored 35 ormore points in nine consecutive games." We focus on one common type of visualization; ie;2d scatter plot with heatmap. Namely; we consider exploration queries whose results can beplotted on a two-dimensional space; possibly with colors indicating object densities inregions. While we model results as pairs of numbers; the types of the queries are limited …,Proceedings of the VLDB Endowment,2015,5
Finding; monitoring; and checking claims computationally based on structured data,Brett Walenz; Y Wu; S Song; Emre Sonmez; Eric Wu; Kevin Wu; Pankaj K Agarwal; Jun Yang; Naeemul Hassan; Afroza Sultana; Gensheng Zhang; Chengkai Li; Cong Yu,“Big data” have arrived. The increasing abundance of data brings many opportunities forjournalism; from discovering interesting stories to fact-checking claims. Unfortunately; theskill of making sense out of data is in short supply; software tools suitable for nontechnicalusers are sorely lacking. The gap between the abundance of data and the shortage ofhuman expertise seems to be widening. Unless we find a way to close this gap; big data willnot achieve its potential for journalism. Moreover; the public may become more susceptibleto “lies; d—ed lies; and statistics” that nitpick data to advance their own arguments.,Computation+ Journalism Symposium,2014,5
Searching Social Updates for Topic-centric Entities.,Maria Christoforaki; Ivie Erunse; Cong Yu,ABSTRACT With the growing popularity of social networking services; real time shortmessages; such as Facebook news feeds and Twitter tweets; are becoming increasinglyimportant information sources. People use these services to search for and consume contentabout interesting topics and events. Given a keyword search for a certain topic; simplyreturning those messages often does not give a comprehensive summary of the topic;primarily due to the brevity and redundancy of the messages. To address this challenge; wepropose a topic centric entity extraction system where interesting entities pertaining to atopic are mined and extracted from short messages returned as search results on the topic.Specifically; we leverage signals from three main aspects: message content; socialconnections (ie; message sender's follower network); and referenced Web pages (ie …,VLDS,2011,5
Sequential composition of schema mappings,*,*,*,*,5
Knowledge exploration using tables on the web,Fernando Chirigati; Jialu Liu; Flip Korn; You Will Wu; Cong Yu; Hao Zhang,Abstract The increasing popularity of mobile device usage has ushered in many features inmodern search engines that help users with various information needs. One of those needsis Knowledge Exploration; where related documents are returned in response to a userquery; either directly through right-hand side knowledge panels or indirectly throughnavigable sections underneath individual search results. Existing knowledge explorationfeatures have relied on a combination of Knowledge Bases and query logs. In this paper; wepropose Knowledge Carousels of two modalities; namely sideways and downwards; thatfacilitate exploration of IS-A and HAS-A relationships; respectively; with regard to an entity-seeking query; based on leveraging the large corpus of tables on the Web. This brings manytechnical challenges; including associating correct carousels with the search entity …,Proceedings of the VLDB Endowment,2016,4
iCheck: computationally combating lies; d--ned lies; and statistics,You Wu; Brett Walenz; Peggy Li; Andrew Shim; Emre Sonmez; Pankaj K Agarwal; Chengkai Li; Jun Yang; Cong Yu,Abstract Are you fed up with" lies; d---ned lies; and statistics" made up from data in ourmedia? For claims based on structured data; we present a system to automatically assessthe quality of claims (beyond their correctness) and counter misleading claims that cherry-pick data to advance their conclusions. The key insight is to model such claims asparameterized queries and consider how parameter perturbations affect their results. Wedemonstrate our system on claims drawn from US congressional voting records; sportsstatistics; and publication records of database researchers.,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,4
Battling Predictability and Overconcentration in Recommender Systems.,Sihem Amer-Yahia; Laks VS Lakshmanan; Sergei Vassilvitskii; Cong Yu,Today's recommendation systems have evolved far beyond the initial approaches from morethan a decade ago; and have seen a great deal of commercial success (cf; Amazon andNetflix). However; they are still far from perfect; and face tremendous challenges inincreasing the overall utility of the recommendations. These challenges are present in allstages of the recommendation process. They begin with the cold start problem: given a newuser how do we recommend items to the user without forcing her to give feedback on astarter set of items. Similarly; given a new item introduced into the system; how do we knowwhen (and to whom) we should recommend it. A different problem is that of data sparsity.Although every system has 'super'raters; that is people who rate thousands (and in somecases tens of thousands) of items; the number of such people is low. In fact; most people …,IEEE Data Eng. Bull.,2009,4
Effective integration of protein data through better data modeling,Adriane Chapman; Cong Yu; HV Jagadish,Protein data; from sequence and structure to interaction; is being generated through manydiverse methodologies; it is stored and reported in numerous forms and multiple places. Themagnitude of the data limits researchers abilities to utilize all information generated.Effective integration of protein data can be accomplished through better data modeling. Wedemonstrate this through the MIPD project.,OMICS A Journal of Integrative Biology,2003,4
Beyond simple parallelism: Challenges for scalable complex analysis over social data,Cong Yu,ABSTRACT With the increasing popularity of social platforms such as Facebook; twitter andGoogle+; applications built on those platforms are increasingly relying on the rich socialactivity data generated by their users to deliver personalized experiences. Those so-calledsocial applications perform various analysis tasks to gather insights from the data and; tohandle the data at large scale; adopt parallel programing paradigms for those tasks;MapReduce being the most notable model. In this position paper; we describe thechallenges facing sophisticated analysis tasks where simple parallelization no longer worksand pose a few questions for future research.,Proceedings of the NSF Workshop on Social Networks and Mobility in the Cloud,2012,3
XQuery processors,Torsten Grust; HV Jagadish; Fatma Özcan; Cong Yu,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,2
Simplifying access to a Clinical Data Repository using schema summarization.,Cong Yu; DA Hanauer; Brian D Athey; HV Jagadish,Abstract The University of Michigan Clinical Data Repository (CDR) integrates over 25 datasources; and as a result has a schema that is too complex to be directly queried by clinicalresearchers. Schema summarization uses abstract elements and links to summarize acomplex schema and allows users with limited knowledge of the underlying databasestructure to effectively issue queries to the CDR for clinical and translational research.,AMIA... Annual Symposium proceedings. AMIA Symposium,2007,2
A Comprehensive front-end architecture for the verisimple alpha pipeline,Scott Gifford; Chien-Wen Huang; Zimin Yang; Cong Yu,ABSTRACT The performance of a pipelined computer system depends heavily on thedegree of Instruction Level Parallelism (ILP) obtained. Pipeline stalling or flushing due tounpredicted or mispredicted branch instructions has become a significant bottleneck toachieving high ILP. A good branch predictor can reduce the number of such stalls andflushes and is therefore critical to the performance of modern day computers. In this project;we designed and implemented a comprehensive front-end architecture to achieve high ILPby experimenting with various branch predictors (including bimodal; local history table;global history table; gShare; and gSelect); a branch target buffer (BTB); an indirect jumppredictor (IJP); and a return address stack (RAS). The result is a highly efficient front-endbranch predicting architecture. Experimental results show that it can achieve an accuracy …,Online] http://citeseer. ist. psu. edu/gifford03comprehensive. html,*,2
LONLIES: estimating property values for long tail entities,Mina Farid; Ihab F Ilyas; Steven Euijong Whang; Cong Yu,Abstract Web search engines often retrieve answers for queries about popular entities froma growing knowledge base that is populated by a continuous information extraction process.However; less popular entities are not frequently mentioned on the web and are generallyinteresting to fewer users; these entities reside on the long tail of information. Traditionalknowledge base construction techniques that rely on the high frequency of entity mentions toextract accurate facts about these mentions have little success with entities that have lowtextual support. We present Lonlies; a system for estimating property values of long tailentities by leveraging their relationships to head topics and entities. We demonstrate (1) howLonlies builds communities of entities that are relevant to a long tail entity utilizing a textcorpus and a knowledge base;(2) how Lonlies determines which communities to use in …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,1
Inferencing in information extraction: Techniques and applications,Denilson Barbosa; Haixun Wang; Cong Yu,Information extraction at Web scale has become one of the most important research topics indata management since major commercial search engines started incorporating knowledgein their search results a couple of years ago [1]. Users increasingly expect structuredknowledge as answers to their search needs. Using Bing as an example; the result page for“Lionel Messi” is full of structured knowledge facts; such as his birthday and awards. Theresearch efforts towards improving the accuracy and coverage of such knowledge baseshave led to significant advances in Information Extraction techniques [2];[3]. As the initialchallenge of accurately extracting facts for popular entities are being addressed; moredifficult challenges have emerged such as extending knowledge coverage to long tailentities and domains; understanding interestingness and usefulness of facts within a …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,1
System and method for providing a graphical user interface for prediction markets,*,A system for providing a probability output and receiving an input includes a processor anda memory unit in communication with the processor. The memory has processor-executableinstructions that configure the processor to provide a user interface having the probabilityoutput; wherein the probability output is in a graphical form and receives an input from theuser via the user interface; wherein the user provides the input by marking portions of thegraphical form of the probability output.,*,2010,1
MiMI: Michigan molecular interactions,Adriane Chapman; Magesh Jayapandian; Cong Yu; HV Jagadish,ABSTRACT There is a proliferation of data sources in biology. A complete understanding ofa biological problem often requires the integration of multiple data sources; each providinginsights on certain aspects of the problem. Furthermore; different sources often representdata in different ways; even when they cover the same information. Researchers interestedin a particular biological problem are forced to search for and understand multiple; oftenconflicting sources; and piece the jigsaw puzzle of information together for themselves. TheMichigan Molecular Interactions Database (MiMI) attempts to relieve scientists of this burden(MiMI; 2005). By integrating popular; well-known datasets; MiMI combines all the power ofeach individual dataset; like BIND (Bader et al.; 2003); and multiplies their benefits toindividual researchers by merging them with other known facts from diverse datasets. By …,*,2005,1
Data Staging on NFS,Yunqing Chen; Jian Wu; Cong Yu,Abstract Due to the storage limitation and imperfect prediction; mobile computing devicesmay experience large delays when accessing data on the distributed file systems. Flinn etal.[4] have proposed a novel architecture; called Data Staging; in which nearby untrustedand unmanaged surrogates are used as the secondary file cache for the client to reduce theremote file operation latency. A prototype system built on top of the Coda file system; DataStaging on Coda (DS-CODA); has been developed and its success has been demonstratedin preliminary experiments. In this project; we implemented Data Staging on the widelydeployed Sun Network File System [15](DS-NFS) and evaluated its performance. Althoughthere are significant differences in the design of Coda and NFS; DS-NFS is able to show animprovement of read performance by up to 42.8% based on the synthetic PostMark …,*,2002,1
Investigating Rumor News Using Agreement-Aware Search,Jingbo Shang; Tianhang Sun; Jiaming Shen; Xingbang Liu; Anja Gruenheid; Flip Korn; Adam Lelkes; Cong Yu; Jiawei Han,Abstract: In recent years; rumor news has been generated by humans as well as robots inorder to attract readership; influence opinions; and increase internet click revenue. Itsdetrimental effects have become a worldwide phenomenon; leading to confusion over factsand causing mistrust about media reports. However; evaluating the veracity of news storiescan be a complex and cumbersome task; even for experts. One of the challenging problemsin this context is to automatically understand different points of view; ie; whether other newsarticles reporting on the same problem agree or disagree with the reference story. This canthen lead to the identification of news articles that propagate false rumors. Here; we proposea novel agreement-aware search framework; Maester; for dealing with the problem of rumordetection. Given an investigative question summarizing some news story or topic; it will …,arXiv preprint arXiv:1802.07398,2018,*
Synthesizing union tables from the web,*,Systems and techniques are provided for generating a union table with from stitchabletables. Tables may be extracted from web pages to obtain extracted tables. Stitchable tablesmay be determined from the extracted tables. Hidden attributes for the stitchable tables maybe extracted from the web pages from which the stitchable tables were extracted usingsegmentation of text for contextual data from the web pages into segment sequences; andalignment of the segment sequences. Iterative pairwise alignment may be used to align thesegment sequences and obtain aligned segments. The stitchable tables may be joined intoa union table. Hidden attributes from the aligned segments may be added to the union table.Headers for the hidden attributes in the union table may be labeled using a database ofentities and class labels.,*,2017,*
Constructing travel itineraries from tagged geo-temporal photographs,*,One embodiment accesses two or more photos taken by one or more travelers at one ormore destinations and one or more points-of-interest located within the destinations;constructs one or more photo streams for each unique traveler-destination combination;wherein each one of the photo streams comprises two or more of the photos taken by thecorresponding traveler at the corresponding destination; maps each one of the photos toone of the points-of-interest; constructs one or more timed paths for each unique traveler-destination combination based on the photo streams and the mapping between the photosand the points-of-interest; wherein each one of the timed paths comprises one or more of thepoints-of-interest located within the corresponding destination and visited by thecorresponding travel; and constructs an itinerary based on a start point-of-interest; an end …,*,2015,*
CloudDB 2013: fifth international workshop on cloud data management,Feifei Li; Xiaofeng Meng; Fusheng Wang; Cong Yu,Abstract The fifth ACM international workshop on cloud data management is held in SanFrancisco; California; USA on October 28; 2013 and co-located with the ACM 22ndConference on Information and Knowledge Management (CIKM). The main objective of theworkshop is to address the challenges of large scale data management based on the cloudcomputing infrastructure. The workshop brings together researchers and practitioners fromcloud computing; distributed storage; query processing; parallel algorithms; data mining;and system analysis; all attendees share common research interests in maximizingperformance; reducing cost of cloud data management and enlarging the scale of theirendeavors. We have constructed an exciting program of four refereed papers and an invitedkeynote talk that will give participants a full dose of emerging research.,Proceedings of the 22nd ACM international conference on Information & Knowledge Management,2013,*
Quality Impact of Value Matching and Scoring in Top-k Entity Attribute Extraction,Matthew Solomon; Luis Gravano; Cong Yu,ABSTRACT The entity attribute extraction problem; or how to extract entities and theirattribute values from natural language Web documents; is of critical importance for Websearch and information access in general. Unfortunately; because of the noisy nature of theWeb and its scale; entity attribute extraction is notoriously challenging in terms of bothextraction efficiency and quality. In our earlier work [24]; we proposed a top-k extractionprocessing approach that addressed the efficiency challenge: Our approach leveraged apopularitybased scoring function to rank Web pages according to their entity-specificimportance; and focused the extraction effort over the highly ranked pages for each entity ofinterest. The extraction quality resulting from this efficiency-motivated extraction approach;however; has not been studied and is the focus of this paper. Specifically; we make …,*,2011,*
Leveraging communities in social content sites,Sihem Amer-Yahia; Cong Yu,Abstract Social Content Sites; which integrate traditional content sites (eg; Yahoo! Travel)with social networks (eg; Facebook) have recently emerged as a popular Web destinationfor creating and sharing content and social links. We discuss new challenges in searchingcontent on those sites and expand the discussion to community-driven informationexploration. In particular; we present Social Scope; a new architecture which harnessesinformation from multiple social content sites and Jelly; a language to help developers buildscalable information exploration applications. At the core of our architecture and languageare user communities and topics which model users' interests. Finally; we examine how XMLtechnologies can help in modeling and processing social information.,Proceedings of the 2009 EDBT/ICDT Workshops,2009,*
Recommendation System Using Social Behavior Analysis and Vocabulary Taxonomies,*,Toggle navigation. HAL: HAL; HALSHS; TEL; MédiHAL; Liste des portails; AURéHAL; API;Documentation. Episciences.org; Sciencesconf.org; Support. Connexion: Connexion; Créerun compte; Mot de passe oublié ? Login oublié ? fr; en. Accueil; Dépôt; Consultation: Lesderniers dépôts; Par type de publication; Par discipline; Par année de publication; Parstructure de recherche; Les portails de l'archive; Les collections. Recherche; Documentation:Tutoriels; Compte et profil: Pourquoi créer un compte et un profil dans HAL; Créer son compteet son profil dans HAL; Modifier son compte ou son profil dans HAL; Modifier son mot depasse; Login ou mot de passe oublié; Les droits associés au profil. Déposer: Avant decommencer; Les types de publication acceptés …,*,2008,*
Progress Toward “the Holy Grail”: The Continued Quest to Automate Fact-Checking,Bill Adair; Chengkai Li; Jun Yang; Cong Yu,Two years ago at this conference we issued what we said was a “call to arms” to advanceautomated fact-checking [1]. We said the “Holy Grail” was “a completely automated fact-checking platform that can detect a claim as it appears in real time; and instantly provide thevoter with a rating about its accuracy.” We acknowledged that goal “may remain far beyondour reach for many; many years to come;” but we called on the journalism and computerscience communities to redouble their efforts to make progress. Since then there has beenremarkable progress and the “Holy Grail” is no longer a distant dream. Although computerscientists and journalists still have significant hurdles to overcome; recent advances with thecreation of a global database of structured factchecks and fact-checking tools such asClaimBuster1 and iCheck2 have laid a groundwork for additional advances in the next …,*,*,*
