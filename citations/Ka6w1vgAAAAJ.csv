A brief survey of web data extraction tools,Alberto HF Laender; Berthier A Ribeiro-Neto; Altigran S Da Silva; Juliana S Teixeira,Abstract In the last few years; several works in the literature have addressed the problem ofdata extraction from Web pages. The importance of this problem derives from the fact that;once extracted; the data can be handled in a way similar to instances of a traditionaldatabase. The approaches proposed in the literature to address the problem of Web dataextraction use techniques borrowed from areas such as natural language processing;languages and grammars; machine learning; information retrieval; databases; andontologies. As a consequence; they present very distinct features and capabilities whichmake a direct comparison difficult to be done. In this paper; we propose a taxonomy forcharacterizing Web data extraction fools; briefly survey major Web data extraction toolsdescribed in the literature; and provide a qualitative analysis of them. Hopefully; this work …,ACM Sigmod Record,2002,940
Automatic web news extraction using tree edit distance,Davi de Castro Reis; Paulo Braz Golgher; Altigran Soares Silva; AlbertoF Laender,Abstract The Web poses itself as the largest data repository ever available in the history ofhumankind. Major efforts have been made in order to provide efficient access to relevantinformation within this huge repository of data. Although several techniques have beendeveloped to the problem of Web data extraction; their use is still not spread; mostlybecause of the need for high human intervention and the low quality of the extraction results.In this paper; we present a domain-oriented approach to Web data extraction and discuss itsapplication to automatically extracting news from Web sites. Our approach is based on ahighly efficient tree structure analysis that produces very effective results. We have testedour approach with several important Brazilian on-line news sites and achieved very preciseresults; correctly extracting 87.71% of the news in a set of 4088 pages distributed among …,Proceedings of the 13th international conference on World Wide Web,2004,414
DEByE–data extraction by example,Alberto HF Laender; Berthier Ribeiro-Neto; Altigran S Da Silva,Abstract In this paper we present DEByE (Data Extraction By Example); an approach toextracting data from Web sources; based on a small set of examples specified by the user.The novelty is in the fact that the user specifies examples according to a structure of hisliking and that this structure is described at example specification time. For the specificationof the examples; the user interacts with a tool we developed which adopts nested tables asits visual paradigm. Nested tables are simple; intuitive; and allow shielding the user fromtechnical details (such as HTML tags; formatting operators; and learning automata) relatedto the extraction problem. The examples provided by the user are then used to generatepatterns which allow extracting data from new documents. For the extraction; DEByE adoptsa new bottom-up procedure we proposed which is very effective with various Web …,Data & Knowledge Engineering,2002,241
Automatic generation of agents for collecting hidden web pages for data extraction,Juliano Palmieri Lage; Altigran S da Silva; Paulo B Golgher; Alberto HF Laender,Abstract As the Web grows; more and more data has become available under dynamicforms of publication; such as legacy databases accessed by an HTML form (the so calledhidden Web). In situations such as this; integration of this data relies more and more on thefast generation of agents that can automatically fetch pages for further processing. As aresult; there is an increasing need for tools that can help users generate such agents. In thispaper; we describe a method for automatically generating agents to collect hidden Webpages. This method uses a pre-existing data repository for identifying the contents of thesepages and takes the advantage of some patterns that can be found among Web sites toidentify the navigation paths to follow. To demonstrate the accuracy of our method; wediscuss the results of a number of experiments carried out with sites from different …,Data & Knowledge Engineering,2004,112
A fast and robust method for web page template detection and removal,Karane Vieira; Altigran S Da Silva; Nick Pinto; Edleno S De Moura; Joao Cavalcanti; Juliana Freire,Abstract The widespread use of templates on the Web is considered harmful for two mainreasons. Not only do they compromise the relevance judgment of many web IR and webmining methods such as clustering and classification; but they also negatively impact theperformance and resource usage of tools that process web pages. In this paper we presenta new method that efficiently and accurately removes templates found in collections of webpages. Our method works in two steps. First; the costly process of template detection isperformed over a small set of sample pages. Then; the derived template is removed from theremaining pages in the collection. This leads to substantial performance gains whencompared to previous approaches that combine template detection and removal. We show;through an experimental evaluation; that our approach is effective for identifying terms …,Proceedings of the 15th ACM international conference on Information and knowledge management,2006,100
Extracting semi-structured data through examples,Berthier Ribeiro-Neto; Alberto HF Laender; Altigran S Da Silva,Abstract In this paper; we describe an innovative approach to extracting semi-structured datafrom Web sources. The idea is to collect a couple of example objects from the user and touse this information to extract new objects from new pages or texts. To perform the extractionof new objects; we introduce a bottom-up extration strategy and; through experimentation;demonstrate that it works quite effectively with distinct Web sources; even if only a fewexamples are provided by the user.,Proceedings of the eighth international conference on Information and knowledge management,1999,88
Organizing hidden-web databases by clustering visible web documents,Luciano Barbosa; Juliana Freire; Altigran Silva,In this paper we address the problem of organizing hidden-Web databases. Given aheterogeneous set of Web forms that serve as entry points to hidden-Web databases; ourgoal is to cluster the forms according to the database domains to which they belong. Wepropose a new clustering approach that models Web forms as a set of hyperlinked objectsand considers visible information in the form context-both within and in the neighborhood offorms-as the basis for similarity comparison. Since the clustering is performed over featuresthat can be automatically extracted; the process is scalable. In addition; because it uses arich set of metadata; our approach is able to handle a wide range of forms; including content-rich forms that contain multiple attributes; as well as simple keyword-based searchinterfaces. An experimental evaluation over real Web data shows that our strategy …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,78
FLUX-CIM: flexible unsupervised extraction of citation metadata,Eli Cortez; Altigran S da Silva; Marcos André Gonçalves; Filipe Mesquita; Edleno S de Moura,Abstract In this paper we propose a knowledge-base approach to help extracting the correctcomponents of citations in any given format. Differently from related approaches that rely onmanually built knowledge-bases (KBs) for recognizing the components of a citation; in ourcase; such a KB is automatically constructed from an existing set of sample metadatarecords from a given area (eg; computer science or health sciences). Our approach does notrely on patterns encoding specific delimitators of a particular citation style. It is alsounsupervised; in the sense that it does not rely on a learning method that requires a trainingphase. These features assign to our technique a high degree of automation and flexibility.To demonstrate the effectiveness and applicability of our proposed approach we have runexperiments in which we applied it to extract information from citations in papers of two …,Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries,2007,75
A source independent framework for research paper recommendation,Cristiano Nascimento; Alberto HF Laender; Altigran S da Silva; Marcos André Gonçalves,Abstract As the number of research papers available on the Web has increased enormouslyover the years; paper recommender systems have been proposed to help researchers onautomatically finding works of interest. The main problem with the current approaches is thatthey assume that recommending algorithms are provided with a rich set of evidence (eg;document collections; citations; profiles) which is normally not widely available. In this paperwe propose a novel source independent framework for research paper recommendation.The framework requires as input only a single research paper and generates severalpotential queries by using terms in that paper; which are then submitted to existing Webinformation sources that hold research papers. Once a set of candidate papers forrecommendation is generated; the framework applies content-based recommending …,Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries,2011,71
A genetic programming approach to record deduplication,Moisés G de Carvalho; Alberto HF Laender; Marcos André Gonçalves; Altigran S da Silva,Several systems that rely on consistent data to offer high-quality services; such as digitallibraries and e-commerce brokers; may be affected by the existence of duplicates; quasireplicas; or near-duplicate entries in their repositories. Because of that; there have beensignificant investments from private and government organizations for developing methodsfor removing replicas from its data repositories. This is due to the fact that clean and replica-free repositories not only allow the retrieval of higher quality information but also lead tomore concise data and to potential savings in computational time and resources to processthis data. In this paper; we propose a genetic programming approach to recorddeduplication that combines several different pieces of evidence extracted from the datacontent to find a deduplication function that is able to identify whether two entries in a …,IEEE Transactions on Knowledge and Data Engineering,2012,70
Finding similar identities among objects from multiple web sources,Joyce CP Carvalho; Altigran S da Silva,Abstract When integrating data from multiple Web sources; objects can exist in differentformats and structures; making it difficult to identify those that can be matched together. Inthis paper; we propose an identification approach to finding similar identities among objectsfrom multiple Web sources. In this approach; object identification works like the relationaljoin operation where a similarity function takes the place of the equality condition. Thissimilarity function is based on information retrieval techniques. Our approach differs fromothers in the literature since it can be used to identify objects more complexly structured (eg;XML documents) and not only objects with a flat structure such as relations. Theeffectiveness of our approach is demonstrated by experimental results with real Web datasources from different domains; that reach precision levels above 75%.,Proceedings of the 5th ACM international workshop on Web information and data management,2003,66
Improving web search efficiency via a locality based static pruning method,Edleno S De Moura; Célia F dos Santos; Daniel R Fernandes; Altigran S Silva; Pavel Calado; Mario A Nascimento,Abstract The unarguably fast; and continuous; growth of the volume of indexed (andindexable) documents on the Web poses a great challenge for search engines. This is trueregarding not only search effectiveness but also time and space efficiency. In this paper wepresent an index pruning technique targeted for search engines that addresses the latterissue without disconsidering the former. To this effect; we adopt a new pruning strategycapable of greatly reducing the size of search engine indices. Experiments using a realsearch engine show that our technique can reduce the indices' storage costs by up to 60%over traditional lossless compression methods; while keeping the loss in retrieval precisionto a minimum. When compared to the indices size with no compression at all; thecompression rate is higher than 88%; ie; less than one eighth of the original size. More …,Proceedings of the 14th international conference on World Wide Web,2005,65
Cobweb-a crawler for the brazilian web,Altigran Soares da Silva; Eveline A Veloso; Paulo Braz Golgher; Berthier Ribeiro-Neto; Alberto HF Laender; Nivio Ziviani,One of the key components of current Web search engines is the document collector. Thepaper describes CoBWeb; an automatic document collector whose architecture is distributedand highly scalable. CoBWeb aims at collecting large amounts of documents per time periodwhile observing operational and ethical limits in the crawling process. CoBWeb is part of theSIAM (Information Systems in Mobile Computing Environments) search engine which isbeing implemented to support the Brazilian Web. Thus; several results related to theBrazilian Web are presented.,String Processing and Information Retrieval Symposium; 1999 and International Workshop on Groupware,1999,63
Structure-driven crawler generation by example,Márcio LA Vidal; Altigran S da Silva; Edleno S de Moura; João Cavalcanti,Abstract Many Web IR and Digital Library applications require a crawling process to collectpages with the ultimate goal of taking advantage of useful information available on Websites. For some of these applications the criteria to determine when a page is to be presentin a collection are related to the page content. However; there are situations in which theinner structure of the pages provides a better criteria to guide the crawling process than theircontent. In this paper; we present a structure-driven approach for generating Web crawlersthat requires a minimum effort from users. The idea is to take as input a sample page and anentry point to a Web site and generate a structure-driven crawler based on navigationpatterns; sequences of patterns for the links a crawler has to follow to reach the pagesstructurally similar to the sample page. In the experiments we have carried out; structure …,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,60
Measuring similarity between collection of values,Carina F Dorneles; Carlos A Heuser; Andrei EN Lima; Altigran Soares da Silva; Edleno Silva de Moura,Abstract In this paper; we propose a set of similarity metrics for manipulating collections ofvalues occuring in XML documents. Following the data model presented in TAX algebra; wetreat an XML element as a labeled ordered rooted tree. Consider that XML nodes can beeither atomic; ie; they may contain single values such as short character strings; date; etc; orcomplex; ie; nested structures that contain other nodes; we propose two types of similaritymetrics: MAVs; for atomic nodes and MCVs; for complex nodes. In the first case; we suggestthe use of several application domain dependent metrics. In the second case; we definemetrics for complex values that are structure dependent; and can be distinctly applied for itand collections of values. We also present experiments showing the effectiveness of ourmethod.,Proceedings of the 6th annual ACM international workshop on Web information and data management,2004,49
Learning to deduplicate,Moisés G de Carvalho; Marcos André Gonçalves; Alberto HF Laender; Altigran S da Silva,Abstract Identifying record replicas in Digital Libraries and other types of digital repositoriesis fundamental to improve the quality of their content and services as well as to yieldeventual sharing efforts. Several deduplication strategies are available; but most of them relyon manually chosen settings to combine evidence used to identify records as being replicas.In this paper; we present the results of experiments we have carried out with a novelMachine Learning approach we have proposed for the deduplication problem. Thisapproach; based on Genetic Programming (GP); is able to automatically generate similarityfunctions to identify record replicas in a given repository. The generated similarity functionsproperly combine and weight the best evidence available among the record fields in order totell when two distinct records represent the same real-world entity. The results of the …,Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries,2006,46
Computing block importance for searching on web sites,David Fernandes; Edleno S de Moura; Berthier Ribeiro-Neto; Altigran S da Silva; Marcos André Gonçalves,Abstract In this paper we consider the problem of using the block structure of a Web page toimprove ranking results when searching for information on Web sites. Given the blockstructure of the Web pages as input; we propose a method for computing the importance ofeach block (in the form of block weights) in a Web collection. As we show throughexperiments; the deployment of our method may allow a significant improvement in thequality of search results. We ran experiments to compare the quality of search results whenusing our method to the quality obtained when using no structure information. Whencompared to a ranking method that considered pages as monolithic units; our block-basedranking method led to improvements in the quality of search results in experiments with twosites with heterogeneous structures. Further; our method does not increase the cost of …,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,45
An example-based environment for wrapper generation,Paulo B Golgher; Alberto HF Laender; Altigran S Da Silva; Berthier Ribeiro-Neto,Abstract In the so-called Web information systems; the role of extracting data of interest fromWeb sites is played by software components generically known as wrappers. As a result; theexistence of flexible tools for designing; developing and maintaining wrappers is crucial. Inthis paper; we present WByE (Wrapping By Example); a user-oriented set of tools for helpingthe user to build wrappers. WByE is based on information implicitly provided by the user bymeans of suitable and intuitive interfaces. It includes two components: the ASByE tool; usedfor generating specifications on how to fetch desired pages (be them static or dynamic); andthe DEByE tool; used for the extraction of data implicitly present in the fetched pages.,International Conference on Conceptual Modeling,2000,44
Replica identification using genetic programming,Moisés G Carvalho; Albero HF Laender; Marcos André Gonçalves; Altigran S da Silva,Abstract Identifying and handling replicas are important to guarantee the quality of theinformation made available by modern data storage services. There has been a largeinvestment from companies and governments in the development of effective methods forremoving replicas from large databases. Typically; this investment has produced significantresults; since cleaned replica-free databases not only allow the retrieval of higher-qualityinformation but also lead to a more concise data representation and to potential savings incomputational time and resources to process and maintaining this data. In this paper; wepropose a GP-based approach to automatic replica identification that combines evidencebased on the data content in order to find a similarity function that is able to identify whethertwo entries in a repository are replicas or not. As shown by our experiments; our approach …,Proceedings of the 2008 ACM symposium on Applied computing,2008,39
Named entity disambiguation in streaming data,Alexandre Davis; Adriano Veloso; Altigran S Da Silva; Wagner Meira Jr; Alberto HF Laender,Abstract The named entity disambiguation task is to resolve the many-to-manycorrespondence between ambiguous names and the unique real-world entity. This task canbe modeled as a classification problem; provided that positive and negative examples areavailable for learning binary classifiers. High-quality sense-annotated data; however; arehard to be obtained in streaming environments; since the training corpus would have to beconstantly updated in order to accomodate the fresh data coming on the stream. On theother hand; few positive examples plus large amounts of unlabeled data may be easilyacquired. Producing binary classifiers directly from this data; however; leads to poordisambiguation performance. Thus; we propose to enhance the quality of the classifiersusing finer-grained variations of the well-known Expectation-Maximization (EM) algorithm …,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,2012,36
The Web as a Data Source for Spatial Databases.,Karla AV Borges; Alberto HF Laender; Claudia Bauzer Medeiros; Altigran Soares da Silva; Clodoveu A Davis Jr,Abstract. With the phenomenal growth of the WWW; rich data sources on many differentsubjects have become available online. Some of these sources store daily facts that ofteninvolve textual geographic descriptions. These descriptions can be perceived as indirectlygeoreferenced data-eg; addresses; telephone numbers; zip codes and place names. Underthis perspective; the Web becomes a large geospatial database; often providing up-to-datelocal or regional information. In this work we focus on using the Web as an important sourceof urban geographic information and propose to enhance urban Geographic InformationSystems (GIS) using indirectly georeferenced data extracted from the Web. We describe anenvironment that allows the extraction of geospatial data from Web pages; converts them toXML format; and uploads the converted data into spatial databases for later use in urban …,GeoInfo,2003,36
Ondux: on-demand unsupervised learning for information extraction,Eli Cortez; Altigran S da Silva; Marcos André Gonçalves; Edleno S de Moura,Abstract Information extraction by text segmentation (IETS) applies to cases in which datavalues of interest are organized in implicit semi-structured records available in textualsources (eg postal addresses; bibliographic information; ads). It is an important practicalproblem that has been frequently addressed in the recent literature. In this paper weintroduce ONDUX (On Demand Unsupervised Information Extraction); a new unsupervisedprobabilistic approach for IETS. As other unsupervised IETS approaches; ONDUX relies oninformation available on pre-existing data to associate segments in the input string withattributes of a given domain. Unlike other approaches; we rely on very effective matchingstrategies instead of explicit learning strategies. The effectiveness of this matching strategyis also exploited to disambiguate the extraction of certain attributes through a …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,35
Exploring features for the automatic identification of user goals in web search,Mauro Rojas Herrera; Edleno Silva de Moura; Marco Cristo; Thomaz Philippe Silva; Altigran Soares da Silva,Abstract Queries submitted to search engines can be classified according to the user goalsinto three distinct categories: navigational; informational; and transactional. Suchclassification may be useful; for instance; as additional information for advertisementselection algorithms and for search engine ranking functions; among other possibleapplications. This paper presents a study about the impact of using several featuresextracted from the document collection and query logs on the task of automaticallyidentifying the users' goals behind their queries. We propose the use of new features notpreviously reported in literature and study their impact on the quality of the queryclassification task. Further; we study the impact of each feature on different web collections;showing that the choice of the best set of features may change according to the target …,Information processing & management,2010,35
A probabilistic approach for automatically filling form-based web interfaces,Guilherme A Toda; Eli Cortez; Altigran S da Silva; Edleno de Moura,Abstract In this paper we present a proposal for the implementation and evaluation of anovel method for automatically using data-rich text for filling form-based input interfaces. Oursolution takes a text as input; extracts implicit data values from it and fills appropriate fields.For this task; we rely on knowledge obtained from values of previous submissions for eachfield; which are freely obtained from the usage of the interfaces. Our approach; called iForm;exploits features related to the content and the style of these values; which are combinedthrough a Bayesian framework. Through extensive experimentation; we show that ourapproach is feasible and effective; and that it works well even when only a few previoussubmissions to the input interface are available.,Proceedings of the VLDB Endowment,2010,34
The effectiveness of automatically structured queries in digital libraries,Marcos André Gonçalves; Edward A Fox; Aaron Krowne; Pável Calado; Alberto HF Laender; Altigran S da Silva; Berthier Ribeiro-Neto,Structured or fielded metadata is the basis for many digital library services; includingsearching and browsing. Yet; little is known about the impact of using structure on theeffectiveness of such services. We investigate a key research question: do structuredqueries improve effectiveness in DL searching? To answer this question; we empiricallycompared the use of unstructured queries to the use of structured queries. We then testedthe capability of a simple Bayesian network system; built on top of a DL retrieval engine; toinfer the best structured queries from the keywords entered by the user. Experimentsperformed with 20 subjects working with a DL containing a large collection of computerscience literature clearly indicate that structured queries; either manually constructed orautomatically generated; perform better than their unstructured counterparts; in the …,Digital Libraries; 2004. Proceedings of the 2004 Joint ACM/IEEE Conference on,2004,34
LABRADOR: Efficiently publishing relational databases on the web by using keyword-based query interfaces,Filipe Mesquita; Altigran S da Silva; Edleno S de Moura; Pável Calado; Alberto HF Laender,Abstract A vast amount of valuable information; produced and consumed by people andinstitutions; is currently stored in relational databases. For many purposes; there is an everincreasing demand for having these databases published on the Web; so that users canquery the data available in them. An important requirement for this to happen is that queryinterfaces must be as simple and intuitive as possible. In this paper we present LABRADOR;a system for efficiently publishing relational databases on the Web by using a simple textbox query interface. The system operates by taking an unstructured keyword-based queryposed by a user and automatically deriving an equivalent SQL query that fits the user'sinformation needs; as expressed by the original query. The SQL query is then sent to aDBMS and its results are processed by LABRADOR to create a relevance-based ranking …,Information Processing & Management,2007,33
Bootstrapping for example-based data extraction,Paulo B Golgher; Altigran S da Silva; Alberto HF Laender; Berthier Ribeiro-Neto,Abstract The effortless generation of wrappers for Web data sources is a crucial task ifproper access to the huge amount of semi-structured data on the Web is to be granted. Inparticular; the development of strategies for wrapper generation based on user-givenexamples is currently one of the most promising research directions in Web data extraction.In this paper we show how to use a pre-existing data repository to automatically generateexamples and allow full automated example-based data extraction. To demonstrate thefeasibility of our approach we provide a number of results obtained from experiments wecarried out and discuss how our ideas can be used to improve extraction rates and forproviding resilience and adaptiveness for example-based generated wrappers.,Proceedings of the tenth international conference on Information and knowledge management,2001,33
Searching web databases by structuring keyword-based queries,Pável Calado; Altigran S da Silva; Rodrigo C Vieira; Alberto HF Laender; Berthier A Ribeiro-Neto,Abstract On-line information services have become widespread in the Web nowadays.However; Web users are non-specialized and have a great variety of interests. Thus;interfaces for Web databases must be simple and uniform. In this paper we present anapproach; based on Bayesian networks; for querying Web databases using keywords only.According to this approach; the user inputs a query through a simple search-box interface.From the input query; one or more plausible structured queries are derived and submitted toWeb databases. The results are then retrieved and presented to the user as rankedanswers. Our approach reduces the complexity of existing on-line interfaces and offers asolution to the problem of querying several distinct Web databases with a single interface.The applicability of the proposed approach was demonstrated by experimental results …,Proceedings of the eleventh international conference on Information and knowledge management,2002,31
Querying semistructured data by example: The QSByE interface,Irna MR Evangelista Filha; Irna MR Evangelista; Filha Alberto; Alberto HF Laender; Altigran S da Silva,Abstract This paper presents QSByE; an interface for querying semistructured data throughexamples. The interface supports QBE-like query operations that are based on the nestedrelational algebra; which has been extended to deal with common features found insemistructured data; such as nested and irregular structures. Therefore; QSByE is moresuitable for casual users than traditional query languages for semistructured data because itprovides graphical facilities that assist the user on the query formulation process. QSByEoperates over data repositories generated by DEByE; a tool for extracting data from Websources.,*,2001,31
Explorando redes sociais online: Da coleta e análise de grandes bases de dados às aplicações,Fabrício Benevenuto; Jussara M Almeida; Altigran S Silva,Resumo Redes sociais online têm se tornado extremamente populares; levando aosurgimento e à crescente popularização de uma nova onda de aplicações na Web.Associado a esse crescimento; redes sociais estão se tornando um tema central depesquisas em diversas áreas da Ciência da Computação. Este mini-curso oferece umaintrodução ao pesquisador que pretende explorar esse tema. Inicialmente; apresentamosas principais características das redes sociais mais populares atualmente. Em seguida;discutimos as principais métricas e análises utilizadas no estudo dos grafos que formam atopologia das redes sociais. Finalmente; sumarizamos as principais abordagens para coletae tratamento de dados de redes sociais online e discutimos trabalhos recentes que ilustramo uso de tais técnicas.,Porto Alegre: Sociedade Brasileira de Computação,2011,30
Avaliação das características dos usuários com hipertensão arterial e/ou diabetes mellitus em uma Unidade de Saúde Pública; no município de Jaboatão dos Guar...,DS Bezerra; AS Silva; ALM Carvalho,Resumo Hipertensão Arterial e Diabetes Mellitus constituem os principais fatores de riscopara doenças cardiovasculares. Neste contexto; o acompanhamento farmacoterapêuticodemonstra ser uma importante estratégia de controle destas condições de saúde. Comoobjetivo; avaliou-se as características dos usuários do Programa Nacional de Hipertensão eDiabetes (Hiperdia) em uma Unidade de Saúde Pública; para averiguar a necessidade deimplantar um serviço de Atenção Farmacêutica. Utilizaram-se questionários para verificar acompreensão do usuário sobre: doença; terapia medicamentosa e adesão ao tratamento.Observou-se que dos 50 usuários entrevistados; a média de idade foi 56; 68±10; 1 anos;sendo 82% gênero feminino. 62% apresentaram ensino fundamental; 12% eramanalfabetos e 32% tinham algum conhecimento sobre sua doença. Em relação aos …,Revista de Ciências Farmacêuticas Básica e Aplicada,2009,30
An evolutionary approach for combining different sources of evidence in search engines,Thomaz Philippe C Silva; Edleno Silva de Moura; João Marcos B Cavalcanti; Altigran S da Silva; Moisés Gomes de Carvalho; Marcos André Gonçalves,Abstract Modern Web search engines use different strategies to improve the overall qualityof their document rankings. Usually the strategy adopted involves the combination ofmultiple sources of relevance into a single ranking. This work proposes the use ofevolutionary techniques to derive good evidence combination functions using three differentsources of evidence of relevance: the textual content of documents; the reputation ofdocuments extracted from the connectivity information available in the processed collectionand the anchor text concatenation. The combination functions discovered by ourevolutionary strategies were tested using a collection containing 368 queries extracted froma real nation-wide search engine query log with over 12 million documents. Theexperiments performed indicate that our proposal is an effective and practical alternative …,Information Systems,2009,28
Exploiting genre in focused crawling,Guilherme T De Assis; Alberto HF Laender; Marcos André Gonçalves; Altigran S Da Silva,Abstract In this paper; we propose a novel approach to focused crawling that exploits genreand content-related information present in Web pages to guide the crawling process. Theeffectiveness; efficiency and scalability of this approach are demonstrated by a set ofexperiments involving the crawling of pages related to syllabi (genre) of computer sciencecourses (content). The results of these experiments show that focused crawlers constructedaccording to our approach achieve levels of F1 superior to 92%(an average gain of 178%over traditional focused crawlers); requiring the analysis of no more than 60% of the visitedpages in order to find 90% of the relevant pages (an average gain of 82% over traditionalfocused crawlers).,International Symposium on String Processing and Information Retrieval,2007,28
A Bayesian network approach to searching Web databases through keyword-based queries,Pável Calado; Altigran S da Silva; Alberto HF Laender; Berthier A Ribeiro-Neto; Rodrigo C Vieira,Abstract On-line information services have become widespread in the Web nowadays.However; Web users are non-specialized and have a great variety of interests. Interfaces forWeb databases must; therefore; be both simple and uniform. In this paper; we present asolution for querying Web databases using keywords only. A Bayesian network model isused to generate a set of one or more plausible structured queries derived form the initialuser input. These queries can then be submitted to Web databases and the retrieved resultspresented as a set of ranked answers. To structure the user queries; full access to thedatabase is not required. Instead; only a small portion of its content; extracted through apublic Web interface; is used by the network model. This approach not only reduces thecomplexity of existing on-line interfaces; but also offers a solution to the problem of …,Information processing & management,2004,28
Collecting hidden weeb pages for data extraction,Juliano Palmieri Lage; Altigran S da Silva; Paulo B Golgher; Alberto HF Laender,Abstract As the Web grows; more and more data has become available under dynamicforms of publication; such as a legacy database accessed by an HTML form (the so calledHidden Web). In situations such as this; integration of this data relies more and more on thefast generation of page fetching agents. As a result; there is an increasing need for tools thatcan help the user to generate such agents. In this paper; we describe an approach toautomatically generating agents to collect hidden Web pages that uses a pre-existing datarepository for identifying the contents of these pages and takes the advantage of someregularities that can be found among Web sites. To demonstrate the effectiveness of ourapproach; we discuss the results of a number of experiments carried out with sites fromdifferent domains. We also dicuss how such regularities among sites can be formalized.,Proceedings of the 4th international workshop on Web information and data management,2002,27
The Web-DL environment for building digital libraries from the Web,Pável P Calado; Marcos A Gonçalves; Edward A Fox; Berthier Ribeiro-Neto; Alberto HF Laender; Altigran S da Silva; Davi C Reis; Pablo A Roberto; Monique V Vieira; Juliano P Lage,Abstract The Web contains a huge volume of unstructured data; which is difficult to manage.In digital libraries; on the other hand; information is explicitly organized; described; andmanaged. Community-oriented services are built to attend specific information needs andtasks. In this paper; we describe an environment; Web-DL; that allows the construction ofdigital libraries from the Web. The Web-DL environment will allow us to collect data from theWeb; standardize it; and publish it through a digital library system. It provides support toservices and organizational structure normally available in digital libraries; but benefitingfrom the breadth of the Web contents. We experimented with applying the Web-DLenvironment to the Networked Digital Library of Theses and Dissertations (NDLTD); thusdemonstrating that the rapid construction of DLs from the Web is possible. Also; Web-DL …,Proceedings of the 3rd ACM/IEEE-CS joint conference on Digital libraries,2003,26
Fast document-at-a-time query processing using two-tier indexes,Cristian Rossi; Edleno S de Moura; Andre L Carvalho; Altigran S da Silva,Abstract In this paper we present two new algorithms designed to reduce the overall timerequired to process top-k queries. These algorithms are based on the document-at-a-timeapproach and modify the best baseline we found in the literature; Blockmax WAND (BMW);to take advantage of a two-tiered index; in which the first tier is a small index containing onlythe higher impact entries of each inverted list. This small index is used to pre-process thequery before accessing a larger index in the second tier; resulting in considerable speedingup the whole process. The first algorithm we propose; named BMW-CS; achieves higherperformance; but may result in small changes in the top results provided in the final ranking.The second algorithm; named BMW-t; preserves the top results and; while slower than BMW-CS; it is faster than BMW. In our experiments; BMW-CS was more than 40 times faster …,Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,2013,25
Active learning genetic programming for record deduplication,Junio De Freitas; Gisele L Pappa; Altigran S da Silva; Marcos A Gonc; Edleno Moura; Adriano Veloso; Alberto HF Laender; Moisés G de Carvalho,The great majority of genetic programming (GP) algorithms that deal with the classificationproblem follow a supervised approach; ie; they consider that all fitness cases available toevaluate their models are labeled. However; in certain application domains; a lot of humaneffort is required to label training data; and methods following a semi-supervised approachmight be more appropriate. This is because they significantly reduce the time required fordata labeling while maintaining acceptable accuracy rates. This paper presents the ActiveLearning GP (AGP); a semi-supervised GP; and instantiates it for the data deduplicationproblem. AGP uses an active learning approach in which a committee of multi-attributefunctions votes for classifying record pairs as duplicates or not. When the committee majorityvoting is not enough to predict the class of the data pairs; a user is called to solve the …,Evolutionary Computation (CEC); 2010 IEEE Congress on,2010,25
A flexible approach for extracting metadata from bibliographic citations,Eli Cortez; Altigran S da Silva; Marcos André Gonçalves; Filipe Mesquita; Edleno S de Moura,Abstract In this article we present FLUX-CiM; a novel method for extracting components (eg;author names; article titles; venues; page numbers) from bibliographic citations. Our methoddoes not rely on patterns encoding specific delimiters used in a particular citation style. Thisfeature yields a high degree of automation and flexibility; and allows FLUX-CiM to extractfrom citations in any given format. Differently from previous methods that are based onmodels learned from user-driven training; our method relies on a knowledge baseautomatically constructed from an existing set of sample metadata records from a given field(eg; computer science; health sciences; social sciences; etc.). These records are usuallyavailable on the Web or other public data repositories. To demonstrate the effectiveness andapplicability of our proposed method; we present a series of experiments in which we …,Journal of the Association for Information Science and Technology,2009,25
7th Brazilian Guideline of Arterial Hypertension: chapter 3-clinical and complementary assessment,Marcus Vinicius Bolivar Malachias; RMS Póvoa; AR Nogueira; D Souza; LS Costa; ME Magalhães,On behalf of the Brazilian Society of Cardiology; Brazilian Society of Hypertension andBrazilian Society of Nephrology; I present to you the 7th Brazilian Guideline of ArterialHypertension; result of the joint work of an expert team of renowned professionals appointedby their respective medical societies. The production of this new guideline was necessary toupdate the knowledge accumulated over the past years; considering the Brazilian realityand clinical practice. The guidance and recommendations contained in this document reflectthe evidence of the effectiveness of the interventions. The present text does not dealspecifically with cost-effectiveness analyses. The major objective of the medical societiesand authors is to guide health professionals on the preventive measures and care forindividuals with arterial hypertension; aiming at reducing the complications of arterial …,Arquivos brasileiros de cardiologia,2016,23
Joint unsupervised structure discovery and information extraction,Eli Cortez; Daniel Oliveira; Altigran S da Silva; Edleno S de Moura; Alberto HF Laender,Abstract In this paper we present JUDIE (Joint Unsupervised Structure Discovery andInformation Extraction); a new method for automatically extracting semi-structured datarecords in the form of continuous text (eg; bibliographic citations; postal addresses;classified ads; etc.) and having no explicit delimiters between them. While in state-of-the-artInformation Extraction methods the structure of the data records is manually supplied the byuser as a training step; JUDIE is capable of detecting the structure of each individual recordbeing extracted without any user assistance. This is accomplished by a novel StructureDiscovery algorithm that; given a sequence of labels representing attributes assigned topotential values; groups these labels into individual records by looking for frequent patternsof label repetitions among the given sequence. We also show how to integrate this …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,23
FS-NER: a lightweight filter-stream approach to named entity recognition on twitter data,Diego Marinho de Oliveira; Alberto HF Laender; Adriano Veloso; Altigran S da Silva,Abstract Microblog platforms such as Twitter are being increasingly adopted by Web users;yielding an important source of data for web search and mining applications. Tasks such asNamed Entity Recognition are at the core of many of these applications; but theeffectiveness of existing tools is seriously compromised when applied to Twitter data; sincemessages are terse; poorly worded and posted in many different languages. Also; Twitterfollows a streaming paradigm; imposing that entities must be recognized in real-time. In viewof these challenges and the inappropriateness of existing tools; we propose a novelapproach for Named Entity Recognition on Twitter data called FS-NER (Filter-StreamNamed Entity Recognition). FS-NER is characterized by the use of filters that processunlabeled Twitter messages; being much more practical than existing supervised CRF …,Proceedings of the 22nd International Conference on World Wide Web,2013,22
Representing web data as complex objects,Alberto HF Laender; Berthier Ribeiro-Neto; Altigran S Da Silva; Elaine S Silva,Abstract The popularization of the Web has made a huge volume of data available for alarge audience. In a large number of Web sites; such as bookstores; electronic catalogs;travel agencies; etc.; the pages constitute documents which are composed of pieces of datawhose overall structure can be easily recognized. Such pages are called data-rich and canbe seen as collections of complex objects. In this paper; we show how such objects can berepresented by nested tables; which are simple; intuitive; and quite convenient forexpressing their implicit structure. The assumption is that; for most sites of interest; only fewexamples are required to reveal the structure of the objects. To corroborate our assumption;we describe a data extraction tool that adopts this approach and present results of someexperiments carried out with this tool.,International Conference on Electronic Commerce and Web Technologies,2000,22
A genre-aware approach to focused crawling,Guilherme T De Assis; Alberto HF Laender; Marcos André Gonçalves; Altigran S Da Silva,Abstract Focused crawlers have as their main goal to crawl Web pages that are relevant to aspecific topic or user interest; playing an important role for a great variety of applications. Ingeneral; they work by trying to find and crawl all kinds of pages deemed as related to animplicitly declared topic. However; users are often not simply interested in any documentabout a topic; but instead they may want only documents of a given type or genre on thattopic to be retrieved. In this article; we describe an approach to focused crawling thatexploits not only content-related information but also genre information present in Webpages to guide the crawling process. This approach has been designed to addresssituations in which the specific topic of interest can be expressed by specifying two sets ofterms; the first describing genre aspects of the desired pages and the second related to …,World Wide Web,2009,21
Siphon++: a hidden-webcrawler for keyword-based interfaces,Karane Vieira; Luciano Barbosa; Juliana Freire; Altigran Silva,Abstract The hidden Web consists of data that is generally hidden behind form interfaces;and as such; it is out of reach for traditional search engines. With the goal of leveraging thehigh-quality information in this largely unexplored portion of the Web; in this paper; wepropose a new strategy for automatically retrieving data hidden behind keyword-based forminterfaces. Unlike previous approaches to this problem; our strategy adapts the querygeneration and selection by detecting features of the index. We describe an extensiveexperimental evaluation which shows that: our strategy is able to derive appropriate queriesto obtain high coverage while; at the same time; avoiding the retrieval of redundant data;and it obtains higher coverage and is more efficient approaches that use a fixed strategy forquery generation.,Proceedings of the 17th ACM conference on Information and knowledge management,2008,21
Structure-Based Crawling in the Hidden Web.,Márcio LA Vidal; Altigran Soares da Silva; Edleno Silva de Moura; Joao MB Cavalcanti,Abstract: The number of applications that need to crawl the Web to gather data is growing atan ever increasing pace. In some cases; the criterion to determine what pages must beincluded in a collection is based on theirs contents; in others; it would be wiser to use astructure-based criterion. In this article; we present a proposal to build structure-basedcrawlers that just requires a few examples of the pages to be crawled and an entry point tothe target web site. Our crawlers can deal with form-based web sites. Contrarily to otherproposals; ours does not require a sample database to fill in the forms; and does not requirethe user to interact heavily. Our experiments prove that our precision is 100% in seventeenreal-world web sites; with both static and dynamic content; and that our recall is 95% in theeleven static web sites examined.,J. UCS,2008,21
Locality-based pruning methods for web search,Edleno Silva de Moura; Celia Francisca dos Santos; Altigran Soares da Silva; Pavel Calado; Mario A Nascimento,Abstract This article discusses a novel approach developed for static index pruning thattakes into account the locality of occurrences of words in the text. We use this new approachto propose and experiment on simple and effective pruning methods that allow a fastconstruction of the pruned index. The methods proposed here are especially useful forpruning in environments where the document database changes continuously; such aslarge-scale web search engines. Extensive experiments are presented showing that theproposed methods can achieve high compression rates while maintaining the quality ofresults for the most common query types present in modern search engines; namely;conjunctive and phrase queries. In the experiments; our locality-based pruning approachallowed reducing search engine indices to 30&percnt; of their original size; with almost no …,ACM Transactions on Information Systems (TOIS),2008,21
Top-down extraction of semi-structured data,Berthier Ribeiro-Neto; Alberto HF Laender; Altigran Soares da Silva,We propose an innovative approach to extracting semi-structured data from Web sources.The idea is to collect a couple of example objects from the user and to use this information toextract new objects from new pages or texts. We propose a top-down strategy that extractscomplex objects; decomposing them in objects less complex; until atomic objects have beenextracted. Through experimentation; we demonstrate that with a small number of givenexamples; our strategy is able to extract most of the objects present in a Web source givenas input.,string processing and information retrieval symposium; 1999 and International workshop on groupware,1999,21
An approach to maintaining optimized relational representations of entity-relationship schemas,Altigran S da Silva; Alberto HF Laender; Marco A Casanova,Abstract This paper presents a redesign method for maintaining optimized relationalrepresentations of entity-relationship schemas. This method is based on the generation of atransient virtual database state that is used to construct the new database state and that canbe obtained without modifying the current relational representation. It also provides meansto collect additional data and to integrate them to the the current database state; as well as tocheck for new integrity constraints.,International Conference on Conceptual Modeling,1996,20
Using structural information to improve search in Web collections,Edleno S de Moura; David Fernandes; Berthier Ribeiro‐Neto; Altigran S da Silva; Marcos André Gonçalves,Abstract In this work; we investigate the problem of using the block structure of Web pages toimprove ranking results. Starting with basic intuitions provided by the concepts of termfrequency (TF) and inverse document frequency (IDF); we propose nine block-weightfunctions to distinguish the impact of term occurrences inside page blocks; instead of insidewhole pages. These are then used to compute a modified BM25 ranking function. Using fourdistinct Web collections; we ran extensive experiments to compare our block-weight rankingformulas with two other baselines:(a) a BM25 ranking applied to full pages; and (b) a BM25ranking that takes into account best blocks. Our methods suggest that our block-weightingranking method is superior to all baselines across all collections we used and that averagegain in precision figures from 5 to 20% are generated.,Journal of the Association for Information Science and Technology,2010,19
On finding templates on web collections,Karane Vieira; André Luiz da Costa Carvalho; Klessius Berlt; Edleno S de Moura; Altigran S da Silva; Juliana Freire,Abstract Templates are pieces of HTML code common to a set of web pages usuallyadopted by content providers to enhance the uniformity of layout and navigation of theirsWeb sites. They are usually generated using authoring/publishing tools or by programs thatbuild HTML pages to publish content from a database. In spite of their usefulness; thecontent of templates can negatively affect the quality of results produced by systems thatautomatically process information available in web sites; such as search engines; clusteringand automatic categorization programs. Further; the information available in templates isredundant and thus processing and storing such information just once for a set of pages maysave computational resources. In this paper; we present and evaluate methods for detectingtemplates considering a scenario where multiple templates can be found in a collection of …,World Wide Web,2009,19
A scalable parallel deduplication algorithm,Walter Santos; Thiago Teixeira; Carla Machado; Wagner Meira Jr; Renato Ferreira; Dorgival Guedes; Altigran S Da Silva,The identification of replicas in a database is fundamental to improve the quality of theinformation. Deduplication is the task of identifying replicas in a database that refer to thesame real world entity. This process is not always trivial; because data may be corruptedduring their gathering; storing or even manipulation. Problems such as misspelled names;data truncation; data input in a wrong format; lack of conventions (like how to abbreviate aname); missing data or even fraud may lead to the insertion of replicas in a database. Thededuplication process may be very hard; if not impossible; to be performed manually; sinceactual databases may have hundreds of millions of records. In this paper; we present ourparallel deduplication algorithm; called FER-APARDA. By using probabilistic record linkage;we were able to successfully detect replicas in synthetic datasets with more than 1 million …,Computer Architecture and High Performance Computing; 2007. SBAC-PAD 2007. 19th International Symposium on,2007,19
Mild stunting is associated with higher blood pressure in overweight adolescents,Ana Paula Grotti Clemente; Carla Danusa Santos; Ana Amelia Benedito Silva; Vinicius Jose Martins; Anna Carolina Marchesano; Mariana Belluca Fernandes; Maria Paula Albuquerque; Ana Lydia Sawaya,RESUMO FUNDAMENTO: Estudos têm demonstrado que a desnutrição pré/pós-natal levaa um maior risco de doenças não transmissíveis; como diabetes; hipertensão e obesidadena idade adulta. OBJETIVO: Determinar se os adolescentes com sobrepeso e desnutriçãoleve [escores-Z altura/idade (HAZ) na faixa de<-1 a>-2] têm pressão arterial mais elevadado que os indivíduos com sobrepeso e com estatura normal (HAZ>-1). MÉTODOS: Osparticipantes foram classificados como de baixa estatura leve ou de estatura normal; eestratificados de acordo com os percentis de massa corporal para a idade; comosobrepeso; peso normal ou abaixo do peso. As pressões arteriais sistólica (PAS) ediastólica (PAD) foram determinadas de acordo com as diretrizes e a gordura abdominal foianalisada por absorciometria de dupla emissão de raios-X. RESULTADOS: Indivíduos …,Arquivos brasileiros de cardiologia,2012,18
Representing and querying semistructured web data using nested tables with structural variants,Altigran S da Silva; Irna MR Evangelista Filha; Alberto HF Laender; David W Embley,Abstract This paper proposes an approach to representing and querying semistructuredWeb data. The proposed approach is based on nested tables; which may have internalnested structural variations to accommodate semistructured data. Our motivation is to reducethe complexity found in typical query languages for semistructured data and to provide userswith an alternative for quickly querying data obtained from multiple-record Web pages. Weshow the feasibility of our proposal by developing a prototype for a graphical query interfacecalled QSByE (Querying Semistructured data By Example). For QSByE; we define aparticular variation of nested tables and propose a set of QBE-like operations that extendstypical nested-relational-algebra operations to handle semistructured data. We showexamples of how users can pose interesting queries using QSByE.,International Conference on Conceptual Modeling,2002,18
Influence of maternal height and weight on low birth weight: a cross-sectional study in poor communities of northeastern Brazil,Revilane Parente de Alencar Britto; Telma Maria Toledo Florêncio; Ana Amelia Benedito Silva; Ricardo Sesso; Jairo Calado Cavalcante; Ana Lydia Sawaya,Background Low birth weight (LBW) is associated with an increased risk of mortality;adverse metabolic conditions; and long-term chronic morbidities. The relationship betweenLWB and short maternal stature coupled with nutritional status was investigated in poorcommunities. Methods/Principal Findings A cross-sectional population-based studyinvolving 2226 mother-child pairs was conducted during the period 2009-2010 inshantytowns of Maceió; Alagoas; Brazil. Associations between LBW and maternalsociodemographics; stature and nutritional status were investigated. The outcome variablewas birth weight (< 2500g and≥ 2500g). The independent variables were the age; income;educational background; stature and nutritional status (eutrophic; underweight; overweightand obese) of the mother. The frequency of LBW was 10%. Short-statured mothers (1st …,PLoS One,2013,17
An approach to xml path matching,Alexander R Vinson; Carlos A Heuser; Altigran S da Silva; Edleno S de Moura,Abstract In applications that accomplish XML data integration and XML instance querying;the problem of XML path matching plays a central role. This paper presents an approach formatching XML paths that consists of (1) PathSim; a similarity function specifically designedfor matching XML paths and (2) a set of pre-processing functions to be applied to XML pathsthat are to be compared by a similarity function. The reported experiments demonstrate thatPathSim achieves matches of higher quality than a similarity function for XML paths found inliterature. The experiments further show that matches of higher quality are achieved whenthe proposed pre-processing functions are employed.,Proceedings of the 9th annual ACM international workshop on Web information and data management,2007,17
A strategy for allowing meaningful and comparable scores in approximate matching,Carina F Dorneles; Marcos Freitas Nunes; Carlos A Heuser; Viviane P Moreira; Altigran S da Silva; Edleno S de Moura,Abstract Approximate data matching aims at assessing whether two distinct instances ofdata represent the same real-world object. The comparison between data values is usuallydone by applying a similarity function which returns a similarity score. If this score surpassesa given threshold; both data instances are considered as representing the same real-worldobject. These score values depend on the algorithm that implements the function and haveno meaning to the user. In addition; score values generated by different functions are notcomparable. This will potentially lead to problems when the scores returned by differentsimilarity functions need to be combined for computing the similarity between records. In thisarticle; we propose that thresholds should be defined in terms of the precision that isexpected from the matching process rather than in terms of the raw scores returned by the …,Information Systems,2009,16
Perfil da situação de saúde do homem no Brasil,EC Moura; ACM Neves; R Gomes; L Albernaz,*,Rio de Janeiro: Editora Fiocruz,2012,15
Um retrato da web brasileira,Eveline A Veloso; E de Moura; P Golgher; A da Silva; R Almeida; A Laender; RB Neto; Nivio Ziviani,This paper presents a snapshot of the content and structure of the Brazilian Web based ondata collected in April 2000. The study was carried out with the help of a page crawlerimplemented to collect documents of the Brazilian Web (HTML pages; PostScript; PDF; Wordand text files). During the collecting process; data related to the organization and structure ofthe Brazilian Web was stored. We present some important characteristics of the BrazilianWeb; such as the average file size; the number of hosts; the average number of links perpage; and we estimate the size of the Brazilian Web based on the data collected.,Anais do XXI Seminário Integrado de Hardware e Software (SEMISH 00); Curitiba; Paraná; Brasil,2000,15
An evolutionary approach to complex schema matching,MoiséS Gomes De Carvalho; Alberto HF Laender; Marcos André GonçAlves; Altigran S Da Silva,Abstract The schema matching problem can be defined as the task of finding semanticrelationships between schema elements existing in different data repositories. Despite theexistence of elaborated graphic tools for helping to find such matches; this task is usuallymanually done. In this paper; we propose a novel evolutionary approach to addressing theproblem of automatically finding complex matches between schemas of semantically relateddata repositories. To the best of our knowledge; this is the first approach that is capable ofdiscovering complex schema matches using only the data instances. Since we only exploitthe data stored in the repositories for this task; we rely on matching strategies that are basedon record deduplication (aka; entity-oriented strategy) and information retrieval (aka; value-oriented strategy) techniques to find complex schema matches during the evolutionary …,Information Systems,2013,14
A site oriented method for segmenting web pages,David Fernandes; Edleno Silva de Moura; Altigran Soares da Silva; Berthier Ribeiro-Neto; Edisson Braga,Abstract Information about how to segment a Web page can be used nowadays byapplications such as segment aware Web search; classification and link analysis. In thisresearch; we propose a fully automatic method for page segmentation and evaluate itsapplication through experiments with four separate Web sites. While the method may beused in other applications; our main focus in this article is to use it as input to segment awareWeb search systems. Our results indicate that the proposed method produces bettersegmentation results when compared to the best segmentation method we found inliterature. Further; when applied as input to a segment aware Web search method; itproduces results close to those produced when using a manual page segmentation method.,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,13
Learning to expand queries using entities,Wladmir C Brandão; Rodrygo LT Santos; Nivio Ziviani; Edleno S Moura; Altigran S Silva,Abstract A substantial fraction of web search queries contain references to entities; such aspersons; organizations; and locations. Recently; methods that exploit named entities havebeen shown to be more effective for query expansion than traditional pseudorelevancefeedback methods. In this article; we introduce a supervised learning approach that exploitsnamed entities for query expansion using Wikipedia as a repository of high-quality feedbackdocuments. In contrast with existing entity-oriented pseudorelevance feedback approaches;we tackle query expansion as a learning-to-rank problem. As a result; not only do we selecteffective expansion terms but we also weigh these terms according to their predictedeffectiveness. To this end; we exploit the rich structure of Wikipedia articles to devisediscriminative term features; including each candidate term's proximity to the original …,Journal of the Association for Information Science and Technology,2014,12
Coleta e análise de grandes bases de dados de redes sociais online,Fabrício Benevenuto; J Almeida; A Silva,Resumo Redes sociais online têm se tornado extremamente populares; levando aosurgimento e à crescente popularização de uma nova onda de aplicações na Web.Associado a esse crescimento; redes sociais estão se tornando um tema central depesquisas em diversas áreas da Ciência da Computação. Este mini-curso oferece umaintrodução ao pesquisador que pretende explorar esse tema. Inicialmente; apresentamosas principais características das redes sociais mais populares atualmente. Em seguida;discutimos as principais métricas e análises utilizadas no estudo dos grafos que formam atopologia das redes sociais. Finalmente; sumarizamos as principais abordagens para coletae tratamento de dados de redes sociais online; e discutimos trabalhos recentes que ilustramo uso de tais técnicas.,Jornadas de Atualização em Informática (JAI),2011,12
Adaptive and flexible blocking for record linkage tasks,Luiz Osvaldo Evangelista; Eli Cortez; Altigran S da Silva; Wagner Meira Jr,Abstract In data integration tasks; records from a single dataset or from different sourcesmust often be compared to identify records that represent the same real world entity. Thecost of this search process for finding duplicate records grows quadratically as the number ofrecords available in the data sources increases and; for this reason; direct approaches; suchas comparing all record pairs; must be avoided. In this context; blocking methods are used tocreate,Journal of Information and Data Management,2010,12
Labeling data extracted from the web,Altigran S Da Silva; Denilson Barbosa; Joao MB Cavalcanti; Marco AS Sevalho,Abstract We consider finding descriptive labels for anonymous; structured datasets; such asthose produced by state-of-the-art Web wrappers. We give a probabilistic model to estimatethe affinity between attributes and labels; and describe a method that uses a Web searchengine to populate the model. We discuss a method for finding good candidate labels forunlabeled datasets. Ours is the first unsupervised labeling method that does not rely onmining the HTML pages containing the data. Experimental results with data from 8 differentdomains show that our methods achieve high accuracy even with very few search engineaccesses.,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2007,12
Análise de dados em forma de pontos,Christovam Barcellos; AS Silva; ALSS Andrade,*,Introdução à estatística espacial para a Saúde Pública (Série Capacitação e atualização em geoprocessamento em saúde). Brasília: Ministério da Saúde,2007,12
Information retrieval aware web site modelling and generation,Keyla Ahnizeret; David Fernandes; Joao MB Cavalcanti; Edleno Silva de Moura; Altigran S da Silva,Abstract Design and maintenance of large corporate Web sites have become a challengingproblem due to the continuing increase in their size and complexity. One particular featurepresent in the majority of this sort of Web sites is searching for information. However thesolutions provided so far; which is based on the same techniques used for search in theopen Web; have not provided a satisfactory performance to specific Web sites; oftenresulting in too much irrelevant content in a query answer. This paper proposes an approachto Web site modelling and generation of intrasite search engines; combining applicationmodelling and information retrieval techniques. Our assumption is that giving searchengines access to the information provided by conceptual representations of the Web siteimproves their performance and accuracy. We demonstrate our proposal by describing a …,International Conference on Conceptual Modeling,2004,12
On the relational representation of complex specialization structures,Altigran S da Silva; Alberto HF Laender; Marco A Casanova,Abstract The mapping of entity-relationship schemas (ER schemas) that contain complexspecialization structures into the relational model requires the use of specific strategies toavoid inconsistent states in the final relational database. In this paper; we show that for thismapping to be correct it is required to enforce a special kind of integrity constraint; the keypairing constraint (KPC). We present a mapping strategy that use simple inclusiondependencies to enforce KPC and show that this strategy can be used to correctly mapspecialization structures that are more general than the simple specialization structuresconsidered by previous strategies.,Information Systems,2000,11
Semi-supervised genetic programming for classification,Filipe de Lima Arcanjo; Gisele Lobo Pappa; Paulo Viana Bicalho; Wagner Meira Jr; Altigran Soares da Silva,Abstract Learning from unlabeled data provides innumerable advantages to a wide range ofapplications where there is a huge amount of unlabeled data freely available. Semi-supervised learning; which builds models from a small set of labeled examples and apotential large set of unlabeled examples; is a paradigm that may effectively use thoseunlabeled data. Here we propose KGP; a semi-supervised transductive geneticprogramming algorithm for classification. Apart from being one of the first semi-supervisedalgorithms; it is transductive (instead of inductive); ie; it requires only a training dataset withlabeled and unlabeled examples; which should represent the complete data domain. Thealgorithm relies on the three main assumptions on which semi-supervised algorithms arebuilt; and performs both global search on labeled instances and local search on …,Proceedings of the 13th annual conference on Genetic and evolutionary computation,2011,10
Building a research social network from an individual perspective,Alberto HF Laender; Mirella M Moro; Marcos André Gonçalves; Clodoveu A Davis Jr; Altigran S da Silva; Allan JC Silva; Carolina AS Bigonha; Daniel Hasan Dalip; Eduardo M Barbosa; Eli Cortez; Peterson S Procópio Jr; Rafael Odon de Alencar; Thiago NC Cardoso; Thiago Salles,Abstract In this poster paper; we present an overview of CiênciaBrasil; a research socialnetwork involving researchers within the Brazilian INCT program. We describe itsarchitecture and the solutions adopted for data collection; extraction; and deduplication; andfor materializing and visualizing the network.,Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries,2011,10
Automatically filling form-based web interfaces with free text inputs,Guilherme A Toda; Eli Cortez; Filipe Mesquita; Altigran S da Silva; Edleno Moura; Marden Neubert,Abstract On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data inputfields; such as text boxes; radio buttons; pull-down lists; check boxes; etc. Although theseinterfaces are popular and effective; in many cases; free text interfaces are preferred overform-based ones. In this paper we discuss the proposal and the implementation of a novelIR-based method for using data rich free text to interact with form-based interfaces. Oursolution takes a free text as input; extracts implicitly data values from it and fills appropriatefields using them. For this task; we rely on values of previous submissions for each field;which are freely obtained from the usage of form-based interfaces,Proceedings of the 18th international conference on World wide web,2009,10
A cost-effective method for detecting web site replicas on search engine databases,André Luiz da Costa Carvalho; Edleno Silva de Moura; Altigran Soares da Silva; Klessius Berlt; Allan Bezerra,Abstract Identifying replicated sites is an important task for search engines. It can reducedata storage costs; improve query processing time and remove noise that might affect thequality of the final answers given to the user. This paper introduces a new approach todetect web sites that are likely to be replicas in a search engine database. Our method usesthe websites' structure and the content of their pages to identify possible replicas. As weshow through experiments; such a combination improves the precision and reduces theoverall costs related to the replica detection task. Our method achieves a qualityimprovement of 47.23% when compared to previously proposed approaches.,Data & Knowledge Engineering,2007,10
Integrating Web Data and Geographic Knowledge into Spatial Databases.,Alberto HF Laender; Karla AV Borges; Joyce CP Carvalho; Claudia Bauzer Medeiros; Altigran Soares da Silva; Clodoveu A Davis Jr,Abstract With the phenomenal growth of the World Wide Web; rich data sources on manydifferent subjects have become available online. Some of these sources store daily facts thatoften involve textual geographic descriptions. These descriptions can be perceived asindirectly georeferenced data–for example; addresses; telephone numbers; zip codes andplace names. In this chapter we focus on using the Web as an important source of urbangeographic information and propose to enhance urban Geographic Information Systems(GIS) using indirectly georeferenced data extracted,*,2005,10
The Debye environment for web data management,Alberto HF Laender; Altigran S Da Silva; Paulo B Golgher; Berthier Ribeiro-Neto; IMR Evangelista Filha; Karine V Magalhães,The paper discusses the Debye (Data Extraction By Example) environment which lets usersextract and manage semistructured data available from Web sources; using an extendedform of nested tables as its fundamental paradigm. Currently; the Debye tools are fullyimplemented as prototypes for applications that use data from heterogeneous Web sources.For instance; developers have recently used Debye to build a repository that integrates datafrom the digital libraries of several universities.,IEEE Internet computing,2002,10
Ritmos da vida,Mirian David Marques; Nelson Marques; Luiz Menna-Barreto; Ana Amélia Benedito Silva; José Cipolla-Neto,*,Ciência hoje,1989,10
Lepref: Learn to precompute evidence fusion for efficient query evaluation,André L Costa Carvalho; Cristian Rossi; Edleno S Moura; Altigran S Silva; David Fernandes,Abstract State-of-the-art search engine ranking methods combine several distinct sources ofrelevance evidence to produce a high-quality ranking of results for each query. The fusion ofinformation is currently done at query-processing time; which has a direct effect on theresponse time of search systems. Previous research also shows that an alternative toimprove search efficiency in textual databases is to precompute term impacts at indexingtime. In this article; we propose a novel alternative to precompute term impacts; providing ageneric framework for combining any distinct set of sources of evidence by using a machine-learning technique. This method retains the advantages of producing high-quality results;but avoids the costs of combining evidence at query-processing time. Our method; calledLearn to Precompute Evidence Fusion (LePrEF); uses genetic programming to compute a …,Journal of the Association for Information Science and Technology,2012,9
DEByE-Uma ferramenta para Extração de Dados Semi-Estruturados.,Alberto HF Laender; Elaine S Silva; Altigran Soares da Silva,*,SBBD,1999,9
A new approach for verifying URL uniqueness in web crawlers,Wallace Favoreto Henrique; Nivio Ziviani; Marco Antônio Cristo; Edleno Silva de Moura; Altigran Soares Da Silva; Cristiano Carvalho,Abstract The Web has become a huge repository of pages and search engines allow usersto find relevant information in this repository. Web crawlers are an important component ofsearch engines. They find; download; parse content and store pages in a repository. In thispaper; we present a new algorithm for verifying URL uniqueness in a large-scale webcrawler. The verifier of uniqueness must check if a URL is present in the repository of uniqueURLs and if the corresponding page was already collected. The algorithm is based on anovel policy for organizing the set of unique URLs according to the server they belong to;exploiting a locality of reference property. This property is inherent in Web traversals; whichfollows from the skewed distribution of links within a web page; thus favoring references toother pages from a same server. We select the URLs to be crawled taking into account …,International Symposium on String Processing and Information Retrieval,2011,8
GoGetIt!: a tool for generating structure-driven web crawlers,Márcio LA Vidal; Altigran S da Silva; Edleno S de Moura; João Cavalcanti,Abstract We present GoGetIt!; a tool for generating structure-driven crawlers that requires aminimum effort from the users. The tool takes as input a sample page and an entry point to aWeb site and generates a structure-driven crawler based on navigation patterns; sequencesof patterns for the links a crawler has to follow to reach the pages structurally similar to thesample page. In the experiments we have performed; structure-driven crawlers generated byGoGetIt! were able to collect all pages that match the samples given; including those pagesadded after their generation.,Proceedings of the 15th international conference on World Wide Web,2006,8
Crescimento de Carassius auratus (Actinopterygii: Cypriniformes) em tanques com e sem abrigo,AST SILVA; UH SCHULZ,*,Acta Biológica Leopondendia,2006,8
Using nested tables for representing and querying semistructured web data,Irna MR Evangelista Filha; Altigran S da Silva; Alberto HF Laender; David W Embley,Abstract This paper proposes an approach for representing and querying semistructuredWeb data; which is based on nested tables allowing internal nested structural variations. Ourmotivation is to reduce the complexity found in typical query languages for semistructureddata and to provide users with an alternative for quickly querying data obtained from multiple-record Web pages. We show the feasibility of our proposal by developing a prototype for agraphical query interface called QSByE (Querying Semistructured data By Example); whichimplements a set of QBE-like operations that extends typical nested-relational-algebraoperations to handle semistructured data.,International Conference on Advanced Information Systems Engineering,2002,8
Storing semistructured data in relational databases,Karine V Magalhaes; Alberto HF Laender; Altigran Soares da Silva,Abstract This paper presents an approach to storing semistructured data in relationaldatabases. We focus on semistructured data as extracted from Web pages by a tool calledDEBYE (Data Extraction By Example); and organized according to its data model; theDEByE Object Model (DEByE-OM). The approach presented here consists in representingthe structure of the objects extracted by DEByE by a relational schema and populating thecorresponding database accordingly. We also show how to retrieve such objects byautomatically transforming high-level query specifications (query patterns) into SQL queriesthat are executed over the relational database. Experiments results carried out to evaluateour approach are also described.,String Processing and Information Retrieval; 2001. SPIRE 2001. Proceedings. Eighth International Symposium on,2001,8
Ranking candidate networks of relations to improve keyword search over relational databases,Pericles de Oliveira; Altigran da Silva; Edleno de Moura,Relational keyword search (R-KwS) systems based on schema graphs take the keywordsfrom the input query; find the tuples and tables where these keywords occur and look forways to “connect” these keywords using information on referential integrity constraints; ie;key/foreign key pairs. The result is a number of expressions; called Candidate Networks(CNs); which join relations where keywords occur in a meaningful way. These CNs are thenevaluated; resulting in a number of join networks of tuples (JNTs) that are presented to theuser as ranked answers to the query. As the number of CNs is potentially very high; handlingthem is very demanding; both in terms of time and resources; so that; for certain queries;current systems may take too long to produce answers; and for others they may even fail toreturn results (eg; by exhausting memory). Moreover; the quality of the CN evaluation may …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,7
Learning URL normalization rules using multiple alignment of sequences,Kaio Wagner Lima Rodrigues; Marco Cristo; Edleno Silva de Moura; Altigran Soares da Silva,Abstract In this work; we present DUSTER; a new approach to detect and eliminateredundant content when crawling the web. DUSTER takes advantage of a multi-sequencealignment strategy to learn rewriting rules able to transform URLs to other likely to havesimilar content; when it is the case. We show the alignment strategy that can lead to areduction in the number of duplicate URLs 54% larger than the one achieved by our bestbaseline.,International Symposium on String Processing and Information Retrieval,2013,7
Sorted dominant local color for searching large and heterogeneous image databases,Márcio Luiz A Vidal; João MB Cavalcanti; Edleno Silva de Moura; Altigran Soares da Silva; Ricardo da Silva Torres,Recent work on Content-Based Image Retrieval (CBIR) have presented alternative methodsfor fast image indexing and retrieval using Bags of Visual Words (BoVW). In such methods;images are represented as sets of visual words; which can be indexed and searched usingwell-known text retrieval techniques; allowing fast search on large image databases. In thispaper we propose a novel method based on BoVW that improves over current methods byusing a new kind of local color descriptor; which we call SDLC; that encodes the mostpredominant color occurrences in blocks of different image regions. We report results ofexperiments we performed with two publicly available image databases. The results indicatethat the use of SDLC led to a quite competitive CBIR method in comparison to the state-of-the-art.,Pattern Recognition (ICPR); 2012 21st International Conference on,2012,7
Exploiting entity semantics for query expansion,Wladmir Cardoso Brandao; Altigran Silva; Edleno Moura; Nivio Ziviani,ABSTRACT Many user queries nowadays contain references to named entities; which hasmotivated the development of new methods that exploit entity semantics for queryexpansion. At the same time; Wikipedia has been widely recognized as a large network ofnamed entities; where entity-related articles are organized into a comprehensive hierarchyof categories and present summarized information on these entities in the so-calledinfoboxes. In this paper; we present a new query expansion method that uses entitysemantics derived from Wikipedia. The main appeal of our method is that; differently fromprevious methods in the literature; it exploits valuable human-refined information availablein infoboxes to obtain candidate query expansion terms and to associate entities identified inqueries with categories. Indeed; by taking advantage of the semantic structure implicitly …,Proceedings of the IADIS International Conference WWW/INTERNET,2011,7
Multiple keyword-based queries over XML streams,Felipe C Hummel; Altigran S da Silva; Mirella M Moro; Alberto HF Laender,Abstract In this paper; we propose that various keyword-based queries be processed overXML streams in a multi-query processing way. Our algorithms rely on parsing stacksdesigned for simultaneously matching terms from several distinct queries and use newquery indexes to speed up search operations when processing a large number of queries.Besides defining a new problem and novel solutions; we perform experiments in whichaspects related to performance and scalability are examined.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,7
Unsupervised strategies for information extraction by text segmentation,Eli Cortez; Altigran S da Silva,Abstract Information extraction by text segmentation (IETS) applies to cases in which datavalues of interest are organized in implicit semi-structured records available in textualsources (eg postal addresses; bibliographic information; ads). It is an important practicalproblem that has been frequently addressed in the recent literature. We report here partialresults from a PhD thesis work in which we introduce ONDUX (On Demand UnsupervisedInformation Extraction); a new unsupervised probabilistic approach for IETS. As otherunsupervised IETS approaches; ONDUX relies on information available on pre-existing datato associate segments in the input string with attributes of a given domain. Unlike otherapproaches; we rely on very effective matching strategies instead of explicit learningstrategies. The effectiveness of this matching strategy is also exploited to disambiguate …,Proceedings of the Fourth SIGMOD PhD Workshop on Innovative Database Research,2010,7
The impact of term selection in genre-aware focused crawling,Guilherme T de Assis; Alberto HF Laender; Altigran S da Silva; Marcos André Gonçalves,Abstract The genre-aware approach to focused crawling aims at crawling pages related tospecific topics that can be expressed in terms of both genre and content information. Suchan approach requires an expert to specify a set of terms that describe the genre and thecontent of the pages of interest. In this paper; we analyze the impact of term selection on thisapproach. Thus; we have performed an experimental study in which we vary the number ofgenre and content terms used in focused crawling processes aimed at crawling pagesrelated to syllabi (genre) of computer science courses (subject) and sale offers (genre) ofcomputer equipments (subject). This experimental study showed that a small set of termsselected by an expert is usually enough to produce good results. In addition; we proposeand experimentally evaluate a strategy for semi-automatic generation of terms to be used …,Proceedings of the 2008 ACM symposium on Applied Computing,2008,7
Cardiovascular Autonomic Response to Food Ingestion in Patients with Gastritis: A Comparison Between Helicobacter pylori‐Positive and‐Negative Patients,Jocemir Ronaldo Lugon; Mauro Diniz Moreira; José Mentor Ramos De Almeida; Agostinho Soares Silva; Eliane Cathala Bordalo Esberard; Kelb Bousquet Santos; Angela Gouvêa Carvalho; Luciana Claudia Mendes; Da Nobrega; Antonio Claudio Lucas,Background: Feeding evokes a cardiovascular response associated to an increasedsympathetic drive. The role of the parasympathetic component in this regard is less clear.Improvement of postprandial vasovagal complaints after Helicobacter pylori eradication inthree cases led us to assess autonomic response to feeding in H. pylori-positive patients insearch for an exacerbated parasympathetic response. Material and Methods: Patients withmild or moderate chronic histologic gastritis were studied. Subjects with the same diagnosisbut testing negative for H. pylori were used as controls. Noninvasive cardiovascular testswere applied before and after feeding. Results: On sympathetic tests; standing postprandialblood pressure was lower than preprandially in 5/12 H. pylori-positive patients and in 0/9controls; p=. 045. Resting postprandial BP on handgrip test was significantly lower than …,Helicobacter,2006,7
Keyword-Based Queries Over Web Databases.,Altigran Soares da Silva; Pável Calado; Rodrigo C Vieira; Alberto HF Laender; Berthier A Ribeiro-Neto,ABSTRACT In this chapter; we propose an approach to using keywords (as in a Web searchengine) for querying databases over the Web. The approach is based on a Bayesiannetwork model andprovides a suitable alternative to the use ofinterfaces based on multipleforms with severalfields. Two major steps are involved when querying a Web databaseusing this approach. First; structured (database-like) queries are derivedfrom a querycomposed only ofthe keywords specified by the user. Next; the structured queries aresubmitted to a Web database; and the retrieved results are presented to the user as rankedanswers. To demonstrate thefeasibilityofthe approach; a simple prototype Web searchsystem based on the approach is presented. Experimental results obtained with this systemindicate that the approach allows for accurately structuring the user queries and retrieving …,*,2003,7
Scalp EEG recording: interictal/ictal location and spreading of epileptiform events,A Silva; JP Cunha; P Oliveira,ABSTRACT The relevance of scalp EEG recording on the selection of patients for epilepsysurgery usually is based on the concordance between the location of the epileptogenicfocus and the presumed ictal origin of clinical seizures visually identified or video recorded.Bilateralisation and/or spreading of the epileptiform events are among the causes of therelative lack of agreement between scalp topography and origin of EEG potentials.Computer methods mainly if they are adaptive and use multistrategic approaches improvethe accuracy on the detection of the epileptogenic focus; extracting relevant information onthe location and spreading of epileptiform events.,Acta Neurologica Scandinavica,1994,7
Ciência Brasil-the brazilian portal of science and technology,Alberto Laender; Mirella Moro; Altigran Silva; Clodoveu Davis Junior; Marcos Gonçalves; Renata Galante; Allan Silva; Carolina Bigonha; Daniel Dalip; Eduardo Nunes Barbosa; Eduardo Nunes Borges; Eli Cortez; Peterson Procópio Junior; Rafael Alencar; Thiago Augusto Lima Cardoso; Thiago Salles,Research social networks are a potentially useful resource for studying science andtechnology indicators from specific communities (eg; acountry). However; building andanalyzing such networks beget challenges beyond those from regular social networks; sincedata about actors and their relationships are usually dispersed across various sources. Inthis paper; we present a research social network built from an individual perspective bygathering data from a Brazilian curricula vitae repository. We describe its architecture andthe solutions adopted for data collection; extraction and deduplication; and for materializingand visualizing the network.,*,2011,6
Automatically generating structured queries in XML keyword search,Felipe da C Hummel; Altigran S Da Silva; Mirella M Moro; Alberto HF Laender,Abstract In this paper; we present a novel method for automatically deriving structured XMLqueries from keyword-based queries and show how it was applied to the experimental tasksproposed for the INEX 2010 data-centric track. In our method; called StruX; users specify aschema-independent unstructured keyword-based query and it automatically generates atop-k ranking of schema-aware queries based on a target XML database. Then; one of thetop ranked structured queries can be selected; automatically or by a user; to be executed byan XML query engine. The generated structured queries are XPath expressions consistingof an entity path (eg; dblp/article) and predicates (eg;/dblp/article [author=” john” and title=”xml”]). We use the concept of entity; commonly adopted in the XML keyword searchliterature; to define suitable root nodes for the query results. Also; StruX uses IR …,International Workshop of the Initiative for the Evaluation of XML Retrieval,2010,6
Web-DL: an experience in building digital libraries from the Web,Pável P Calado; Altigran S da Silva; Berthier Ribeiro-Neto; Alberto HF Laender; Juliano P Lage; Davi C Reis; Pablo A Roberto; Monique V Vieira; Marcos A Gonçalves; Edward A Fox,Abstract The Web contains a huge volume of information; almost all unstructured and;therefore; difficult to manage. In Digital Libraries; however; information is explicitlyorganized; described; and managed. In this paper; we propose an architecture that allowsthe construction of digital libraries from the Web; using standard protocols and archivaltechnologies; and incorporating powerful digital library and data extraction tools; thusbenefiting from the breadth of the Web contents; but supporting services and organizationavailable in digital libraries. The proposed architecture was applied to the Networked DigitalLibrary of Theses and Dissertations; providing an important first step toward rapidconstruction of large DLs from the Web; as well as a large-scale solution for interoperabilitybetween independent digital libraries.,Proceedings of the eleventh international conference on Information and knowledge management,2002,6
A framework for generating attribute extractors for web data sources,Davi de Castro Reis; Robson Braga Araújo; Altigran S da Silva; Berthier A Ribeiro-Neto,Abstract To cope with the irregularities of typical semistructured Web data; extraction toolsusually break the extraction task in two phases: an extraction phase; in which atomicattribute values are extracted from Web pages; and an assembling phase; in which theseatomic values are grouped to form complex objects. As a consequence; the whole process ishighly dependent on the attribute values collected in the first phase. All attribute values ofinterest should be properly recognized and spurious values should be discarded. Thus;attribute values extraction is an important problem. In this paper; we propose a newframework for generating attribute value extractors. The main appeal of this framework is thatit can be adapted for dealing with specific types of data sources and to incorporate distincttypes of heuristics for achieving good extraction performance. To demonstrate the …,International Symposium on String Processing and Information Retrieval,2002,6
Fast top-k preserving query processing using two-tier indexes,Caio Moura Daoud; Edleno Silva de Moura; Andre Carvalho; Altigran Soares da Silva; David Fernandes; Cristian Rossi,Abstract In this paper we propose and evaluate the Block Max WAND with CandidateSelection and Preserving Top-K Results algorithm; or BMW-CSP. It is an extension of BMW-CS; a method previously proposed by us. Although very efficient; BMW-CS does notguarantee preserving the top-k results for a given query. Algorithms that do not preserve thetop results may reduce the quality of ranking results in search systems. BMW-CSP extendsBMW-CS to ensure that the top-k results will have their rankings preserved. In theexperiments we performed for computing the top-10 results; the final average time requiredfor processing queries with BMW-CSP was lesser than the ones required by the baselinesadopted. For instance; when computing top-10 results; the average time achieved byMBMW; the best multi-tier baseline we found in the literature; was 36.29 ms per query …,Information Processing & Management,2016,5
A genetic programming framework to schedule webpage updates,Aécio SR Santos; Cristiano R de Carvalho; Jussara M Almeida; Edleno S de Moura; Altigran S da Silva; Nivio Ziviani,Abstract The quality of a Web search engine is influenced by several factors; includingcoverage and the freshness of the content gathered by the web crawler. Focusingparticularly on freshness; one key challenge is to estimate the likelihood of a previouslycrawled webpage being modified. Such estimates are used to define the order in whichthose pages should be visited; and thus; can be exploited to reduce the cost of monitoringcrawled webpages for keeping updated versions. We here present a Genetic Programmingframework; called GP4C GP 4 C—Genetic Programming for Crawling; to generate scorefunctions that produce accurate rankings of pages regarding their probabilities of havingbeen modified. We compare GP4C GP 4 C with state-of-the-art methods using a largedataset of webpages crawled from the Brazilian Web. Our evaluation includes multiple …,Information Retrieval Journal,2015,5
Lightweight methods for large‐scale product categorization,Eli Cortez; Mauro Rojas Herrera; Altigran S da Silva; Edleno S de Moura; Marden Neubert,Abstract In this article; we present a study about classification methods for large-scalecategorization of product offers on e-shopping web sites. We present a study about theperformance of previously proposed approaches and deployed a probabilistic approach tomodel the classification problem. We also studied an alternative way of modelinginformation about the description of product offers and investigated the usage of price andstore of product offers as features adopted in the classification process. Our experimentsused two collections of over a million product offers previously categorized by human editorsand taxonomies of hundreds of categories from a real e-shopping web site. In theseexperiments; our method achieved an improvement of up to 9% in the quality of thecategorization in comparison with the best baseline we have found.,Journal of the Association for Information Science and Technology,2011,5
MS; Cavaleiro; JAS,CMM Santos; A Silva,*,Synlett,2007,5
A signature-based bag of visual words method for image indexing and search,Joyce Miranda dos Santos; Edleno Silva de Moura; Altigran Soares da Silva; João Marcos B Cavalcanti; Ricardo da Silva Torres; Márcio Luiz A Vidal,Abstract In this paper; we revisit SDLC; an image retrieval method that adopts a signature-based approach to identify visual words; instead of the more conventional approach thatidentifies them by using clustering techniques. We start by providing a formal andgeneralized definition of the approach adopted in SDLC; which we call Signature-BasedBag of Visual Words. After that; we present a detailed study of SDLC parameters andexperiments with distinct weighting schemes used to compute the ranking of results;comparing the method to well-known cluster-based bag of visual words approaches. Whencompared to the initial proposal of SDLC; the choice of different parameters and a newweighting scheme allowed us to considerably reduce the size of the textual representationgenerated by the method; reducing also the indexing times and the query processing …,Pattern Recognition Letters,2015,4
Removing DUST Using Multiple Alignment of Sequences,Kaio Rodrigues; Marco Cristo; Edleno S de Moura; Altigran da Silva,A large number of URLs collected by web crawlers correspond to pages with duplicate ornear-duplicate contents. To crawl; store; and use such duplicated data implies a waste ofresources; the building of low quality rankings; and poor user experiences. To deal with thisproblem; several studies have been proposed to detect and remove duplicate documentswithout fetching their contents. To accomplish this; the proposed methods learnnormalization rules to transform all duplicate URLs into the same canonical form. Achallenging aspect of this strategy is deriving a set of general and precise rules. In this work;we present DUSTER; a new approach to derive quality rules that take advantage of a multi-sequence alignment strategy. We demonstrate that a full multi-sequence alignment of URLswith duplicated content; before the generation of the rules; can lead to the deployment of …,IEEE Transactions on Knowledge and Data Engineering,2015,4
A self-training CRF method for recognizing product model mentions in web forums,Henry S Vieira; Altigran S da Silva; Marco Cristo; Edleno S de Moura,Abstract Important applications in product opinion mining such as opinion summarizationand aspect extraction require the recognition of product mentions as a basic task. In the caseof consumer electronic products; Web forums are important and popular sources of valuableopinions. Forum users often refer to products by means of their model numbers. In a post auser would employ model numbers; eg;“BDP-93” and “BDP-103”; to compare Blu-rayplayers. To properly handle opinions in such a scenario; applications need to correctlyrecognize products by their model numbers. Forums; however; are informal and manychallenges for undertaking automatic product model recognition arise; since users mentionmodel numbers in many different ways. In this paper we propose the use of a self-trainingstrategy to learn a suitable CRF model for this task. Our method requires only a set of …,European Conference on Information Retrieval,2015,4
Learning to schedule webpage updates using genetic programming,Aécio SR Santos; Nivio Ziviani; Jussara Almeida; Cristiano R Carvalho; Edleno Silva de Moura; Altigran Soares da Silva,Abstract A key challenge endured when designing a scheduling policy regarding freshnessis to estimate the likelihood of a previously crawled webpage being modified on the web.This estimate is used to define the order in which those pages should be visited; and can beexplored to reduce the cost of monitoring crawled webpages for keeping updated versions.We here present a novel approach to generate score functions that produce accuraterankings of pages regarding their probability of being modified when compared to theirpreviously crawled versions. We propose a flexible framework that uses geneticprogramming to evolve score functions to estimate the likelihood that a webpage has beenmodified. We present a thorough experimental evaluation of the benefits of our frameworkover five state-of-the-art baselines.,International Symposium on String Processing and Information Retrieval,2013,4
Using taxonomies for product recommendation,Osvaldo Matos-Junior; Nivio Ziviani; Fabiano Botelho; Marco Cristo; Anisio Lacerda; Altigran Soares da Silva,*,Journal of Information and Data Management,2012,4
On using wikipedia to build knowledge bases for information extraction by text segmentation,Elton Serra; Eli Cortez; Altigran S da Silva; Edleno S de Moura,*,Journal of Information and Data Management,2011,4
Unsupervised information extraction with the ondux tool,André Porto; Eli Cortez; Altigran S da Silva; Edleno S de Moura,Page 1. A Unsupervised Information Extraction with the ONDUX Tool Presented by André PortoAndré Porto; Eli Cortez; Altigran S. da Silva; Edleno S. de Moura Univ. Fed. do Amazonas (UFAM) -Brazil SBBD 2011 Florianópolis; Brazil Page 2. The IETS Problem ► Information Extraction byText Segmentation ► Goal: ► To extract attribute values occurring in implicit semi- structured datarecords ► Current IETS methods predict labels for sequence of text segments corresponding toattribute values ► HMM – Borkar et al. (SIGMOD01); CRF – Laferty et al. (ICML01); ONDUX –Cortez et. al (SIGMOD10) Page 3. Motivation ► Abundance of on-line sources of text documents ►Postal Addresses; Classified Ads; Bibliography references... ► Necessity of storing these datain structured format ► Relational DB; XML; ► Unsupervised Methods rely on attribute values frompre- existing data sources to perform extraction task …,Simpsio Brasileiro de Banco de Dados,2011,4
Extraç ao de dados e metadados em textos semi-estruturados usando HMMs,Roberto Oliveira dos Santos; Filipe de Sá Mesquita; Altigran Soares da Silva; Eli Cortez C Vilarinho,Resumo. A Web é abundante em páginas que armazenam dados de forma implıcita. Emmuitos casos; estes dados estao presentes em textos semiestruturados sem a presença dedelimitadores explıcitos e organizados em uma estrutura também implıcita. Neste artigoapresentamos uma nova abordagem para extraçao em textos semi-estruturados baseadaem Modelos de Markov Ocultos (Hidden Markov Models-HMM). Ao contrário de outrostrabalhos baseados em HMM; nossa abordagem dá ênfasea extraç ao de metadados alémdos dados propriamente ditos. Esta abordagem consiste no uso de uma estrutura aninhadade HMMs; onde um HMM principal identifica os atributos no texto e HMMs internos; um paracada atributo; identificam os dados e metadados. Os HMMs sao gerados a partir de umtreinamento com uma fraçao de amostras da base a ser extraıda. Nossos experimentos …,*,2006,4
Particle Swarm based Data Mining Algorithms for classification task,T Souda; A Silva; A Neves,*,Parallel Computing,2004,4
Cobweb-um coletor automático de documentos web,AS Silva; Eveline A Veloso; Paulo B Golgher; Berthier Ribeiro-Neto; Nivio Ziviani; Alberto HF Laender,*,Proc. XXVI Seminário Integrado de Software e Hardware (SEMISH 99); Rio de Janeiro; Brasil,1999,4
Using active learning techniques for improving database schema matching methods,Diego Rodrigues; Altigran da Silva; Rosiane Rodrigues; Eulanda dos Santos,The schema matching problem consists of finding semantic correspondences betweenelements (eg; attributes) of two database schemas. Typically; methods to solve this problemfirst use pair-wise functions called matchers to generate similarity scores values betweenpairs of elements from the two schemas. These scores are used as estimations of thecorrespondence between elements. Next; matchers are combined to establish which pairs ofelements must be mapped when integrating the two schemas. For this; the best-knownschema matching methods rely on fixed heuristics. We consider that using fixed heuristics isnot always helpful to cope with a variety of database and element mismatch cases andargue that machine learning (ML) techniques comprise suitable alternatives to properlycombine matchers. In this paper we propose ALMa (Active Learning Matching); a novel …,Neural Networks (IJCNN); 2015 International Joint Conference on,2015,3
A energy efficient WSN system for limited power source environments,Rodrigo S Semente; Alexandre Silva; Andrés O Salazar; Felipe DM Oliveira; Alberto S Lock,In this work; a Wireless Sensor Networks (WSN) is analyzed and implemented to control ainstrumented process in environments with limited power source. Present propose is basedon IEEE 802.15. 4 standard; using Zigbee and Modbus protocols. A simplified and robustsystem architecture is presented; emphasizing the control subsystem of charge anddischarge; using solar panels; as well as software optimized for task of network controllingand sensing; both characteristics which that a reduced consumption of energy. Thesecharacteristics were proved by energetic efficiency tests and a system study that will beautomated to define a rate optimal operation.,Sensing Technology (ICST); 2013 Seventh International Conference on,2013,3
Unsupervised information extraction by text segmentation,Eli Cortez; Altigran S Da Silva,Information Extraction (IE) refers to the automatic extraction of structured information such asentities; relationships between entities; and attributes describing entities from noisyunstructured textual sources. It derives from the necessity of having unstructured data storedin structured formats (tables; XML); so that it can be further queried; processed; andanalyzed. The IE problem encompasses many distinct sub-problems such as Named EntityRecognition (NER); Open Information Extraction; Relationship Extraction; and TextSegmentation. Information Extraction by Text Segmentation (IETS) is the problem ofsegmenting unstructured textual inputs to extract implicit data values contained in them. Inthis book; we present a novel unsupervised approach for the problem of IETS. Thisapproach relies on information available on pre-existing data to learn how to associate …,*,2013,3
A self-supervised approach for extraction of attribute-value pairs from wikipedia articles,Wladmir C Brandao; Edleno S Moura; Altigran S Silva; Nivio Ziviani,Abstract Wikipedia is the largest encyclopedia on the web and has been widely used as areliable source of information. Researchers have been extracting entities; relationships andattribute-value pairs from Wikipedia and using them in information retrieval tasks. In thispaper we present a self-supervised approach for autonomously extract attribute-value pairsfrom Wikipedia articles. We apply our method to the Wikipedia automatic infobox generationproblem and outperformed a method presented in the literature by 21.92% in precision;26.86% in recall and 24.29% in F1.,International Symposium on String Processing and Information Retrieval,2010,3
FleDEx: flexible data exchange,Filipe Mesquita; Denilson Barbosa; Eli Cortez; Altigran S da Silva,Abstract We propose a lightweight framework for data exchange that is suitable for non-expert and casual users sharing data on the Web or through peer-to-peer systems. Unlikeprevious work; we consider a simplistic data model and schema formalism that are suitablefor describing typical online data; and propose algorithms for mapping such schemas aswell as for translating the corresponding instances. Our solution requires minimal overheadand setup costs compared to existing data exchange systems; making it very attractive in theWeb data exchange setting. We report experimental results indicating that our method workswell with real Web data from various domains.,Proceedings of the 9th annual ACM international workshop on Web information and data management,2007,3
Detecç ao de réplicas utilizando conteúdo e estrutura,André Luiz da Costa Carvalho; Allan José de Souza Bezerra; Edleno Silva de Moura; Altigran Soares da Silva; Patrıcia Silva Peres,Abstract. Identifying replicated sites is an important task for search engines. It can reducedata storage costs; improve query processing time and remove noises that might affect thequality of the final answer given to the user. This paper introduces a new approach to detectreplicated sites in search engines databases; using as replication evidences the websites'structure and the content of their pages. It is also depicted the result of experimentsperformed with a real search engine database. Our approach found 8.43% of the web pagesstored in the database were in replicated web sites with 94.4% precision; result witch ismore accurate than the ones found in other works. Resumo. A identificaçao de sıtios Webreplicados é uma tarefa central para o bom funcionamento de uma máquina de busca;reduzindo custos de armazenamento dos dados; processamento de consultas e …,Simpósio Brasileiro de Banco de Dados,2005,3
Estratégias Baseadas em Exemplos para Extraçao de Dados Semi-Estruturados da Web,Altigran Soares da Silva,Resumo Neste trabalho; sao propostas; implementadas e avaliadas estratégias e técnicaspara o problema de extraçao de dados semi-estruturados de fontes de dados da Web;dentro do contexto de uma abordagem chamada DEByE (Data Extraction By Example). Osresultados obtidos com o trabalho foram usados na implementaçao de um ferramenta deextraçao de dados; também chamada DEByE; e tiveram sua eficácia verificada através deexperimentaçao. A abordagem DEByE é dita semi-automática; no sentido em que o papeldos usuários (ou seja; dos desenvolvedores de extratores) é limitado ao fornecimento deexemplos dos dados a serem extraıdos; o que os isola de ter que conhecer ascaracterısticas especificas de formaçao das páginas alvo. Os exemplos fornecidosdescrevem a estrutura dos objetos a serem extraıdos por meio de tabelas aninhadas; as …,*,2002,3
Managing Web data through views,Álisson R Arantes; Alberto HF Laender; Paulo B Golgher; Altigran S da Silva,Abstract The huge amount of data available on the Web creates a great demand for methodsand tools that allow the manipulation of such data. Thus; the notion of view as a mechanismfor providing access to Web data has been revisited. In this paper; we present anenvironment composed of a set of high-level tools that allow the fetching; extraction;integration; and refreshing of Web data. Using this environment; database designers canbuild and maintain Web views by defining schemas for data integration; specifying wrappers(agents for collecting Web pages and extracting data from them); and defining plans forrefreshing the view contents.,International Conference on Electronic Commerce and Web Technologies,2001,3
ASByE: uma Ferramenta Baseada em Exemplos para Especificação de Agentes para Coleta de Documentos Web.,Paulo Braz Golgher; Alberto HF Laender; Altigran Soares da Silva; Berthier A Ribeiro-Neto,*,SBBD,2000,3
Color and texture applied to a signature-based bag of visual words method for image retrieval,Joyce Miranda Dos Santos; Edleno Silva De Moura; Altigran Soares Da Silva; Ricardo da Silva Torres,Abstract This article addresses the problem of representation; indexing and retrieval ofimages through the signature-based bag of visual words (S-BoVW) paradigm; which mapsfeatures extracted from image blocks into a set of words without the need of clusteringprocesses. Here; we propose the first ever method based on the S-BoVW paradigm thatconsiders information of texture to generate textual signatures of image blocks. We alsopropose a strategy that represents image blocks with words which are generated based onboth color as well as texture information. The textual representation generated by thisstrategy allows the application of traditional text retrieval and ranking techniques to computethe similarity between images. We have performed experiments with distinct similarityfunctions and weighting schemes; comparing the proposed strategy to the well-known …,Multimedia Tools and Applications,2017,2
MKStream: An Efficient Algorithm for Processing Multiple Keyword Queries over XML Streams,Evandrino G Barros; Alberto HF Laender; Mirella M Moro; Altigran S da Silva,Abstract In this paper; we tackle the problem of processing various keyword-based queriesover XML streams in a scalable way; improving recent multi-query processing approaches.We propose a customized algorithm; called MKStream; that relies on parsing stacksdesigned for simultaneously matching several queries. Particularly; it explores the possibilityof adjusting the number of parsing stacks for a better trade-off between processing time andmemory usage. A comprehensive set of experiments evaluates its performance andscalability against the state-of-the-art; and shows that MKStream is the most efficientalgorithm for keyword search services over XML streams.,International Conference on Conceptual Modeling,2014,2
Filter-stream named entity recognition: A case study at the MSM2013 concept extraction challenge,Diego Marinho De Oliveira; Alberto HF Laender; Adriano Veloso; Altigran S Da Silva,Abstract. Microblog platforms such as Twitter are being increasingly adopted by Web users;yielding an important source of data for web search and mining applications. Tasks such asNamed Entity Recognition are at the core of many of these applications; but theeffectiveness of existing tools is seriously compromised when applied to Twitter data; sincemessages are terse; poorly worded and posted in many different languages. In this paper;we briefly describe a novel NER approach; called FS-NER (Filter Stream Named EntityRecognition) to deal with Twitter data; and present the results of a preliminary performanceevaluation conducted to assess it in the context of the Concept Extraction Challengeproposed by the 2013 Workshop on Making Sense of Microposts-MSM2013. FS-NER ischaracterized by the use of filters that process unlabeled Twitter messages; being much …,Making Sense of Microposts (# MSM2013),2013,2
RT-NED: Real-time named entity disambiguation on Twitter streams,Alexandre Davis; Walter Santos; Adriano Veloso; Wagner Meira Jr; Alberto Laender; Altigran Soares da Silva,Abstract. Mining data from social midia channels; such as Twitter; is an increasinglyimportant challenge. Messages continuously flow through the Web in high volumes; and thelack of a syntactical structure makes it hard to extract information from them. For instance;Identifying entities in such messages is a requirement for applications such as contextextraction and sentiment analysis. In this paper; we present a novel technique to solve themany-to-many correspondence between ambiguous names and a unique real-world entity;in real-time; using only a stream of messages as data source. This novel technique isimplemented using a three-stage pipeline. In the first stage; previously defined filtering rules(colocations; users; hash tags) are used to identify clearly positive examples of messagestruly mentioning the real-world entity (ies). These messages are given as input to a …,Anais no XXVI Simpósio Brasileiro de Banco de Dados-Sessão de Demos. Florianópolis,2011,2
Waves: a fast multi-tier top-k query processing algorithm,Caio Moura Daoud; Edleno Silva de Moura; David Fernandes; Altigran Soares da Silva; Cristian Rossi; Andre Carvalho,Abstract In this paper; we present Waves; a novel document-at-a-time algorithm for fastcomputing of top-k query results in search systems. The Waves algorithm uses multi-tierindexes for processing queries. It performs successive tentative evaluations of results whichwe call waves. Each wave traverses the index; starting from a specific tier level i. Each wavei may insert only those documents that occur in that tier level into the answer. Afterprocessing a wave; the algorithm checks whether the answer achieved might be changed bysuccessive waves or not. A new wave is started only if it has a chance of changing the top-kscores. We show through experiments that such lazy query processing strategy results insmaller query processing times when compared to previous approaches proposed in theliterature. We present experiments to compare Waves' performance to the state-of-the-art …,Information Retrieval Journal,2017,1
Lca-based algorithms for efficiently processing multiple keyword queries over XML streams,Evandrino G Barros; Alberto HF Laender; Mirella M Moro; Altigran S da Silva,Abstract In a stream environment; differently from traditional databases; data arrivecontinuously; unindexed and potentially unbounded; whereas queries must be evaluated forproducing results on the fly. In this article; we propose two new algorithms (calledSLCAStream and ELCAStream) for processing multiple keyword queries over XML streams.Both algorithms process keyword-based queries that require minimal or no schemaknowledge to be formulated; follow the lowest common ancestor (LCA) semantics; andprovide optimized methods to improve the overall performance. Moreover; SLCAStream;which implements the smallest LCA (SLCA) semantics; outperforms the state-of-the-art; withup to 49% reduction in response time and 36% in memory usage. In turn; ELCAStream is thefirst to explore the exclusive LCA (ELCA) semantics over XML streams. A comprehensive …,Data & Knowledge Engineering,2016,1
Methods and Techniques for Information Extraction by Text Segmentation.,Altigran Soares da Silva; Eli Cortez,Abstract. The growing use of text files for information exchange; such as HTML pages; XMLdocuments; e-mail; blogs posts; tweets; RSS and SMS messages; has brought manyproblems related to how properly exploit the information implicitly contained therein. Inparticular; problems related to Information Extraction from such sources have motivatedmany studies in various scientific communities in areas such as Databases; Data Mining;Information Retrieval and Artificial Intelligence. In this tutorial; we present recent methodsand techniques in the literature to address the problem of Information Extraction by TextSegmentation (IETS); which consists in extracting values of interest organized into semi-structured records (eg; postal addresses; bibliographic citations; classified ads; etc.);implicitly present in textual sources. We will discuss the most recent major approaches …,AMW,2012,1
Um método probabilístico para o preenchimento automático de formulários Web a partir de textos ricos em dados,Guilherme Alves Toda,On the Web of today the most prevalent solution for users to interact with data-intesiveapplications is the use of form-based interfaces composed by several data input fields; suchas text boxes; radio buttons; pull-down lists and check boxes. Although these interfaces arepopular and effectiver; in many cases; free text interfaces are preferred over form basedones. In this work we present; the implementation and the evaluation of a novel method forautomatically filling form-based input interfaces using data-rich text. Our solution takes adata-rich free text as input (eg; an ad); extracts implicit data values from it and fillsappropriate fields using them. For this task; we rely on knowledge obtained from values ofprevious submissions for each field; which are freely obtained from the usage of theinterfaces. Our approach; called iForm; exploits features related to the content and the …,*,2010,1
Blocagem Adaptativa e Flexível para o Pareamento Aproximado de Registros.,Luiz Osvaldo Evangelista; Eli Cortez; Altigran Soares da Silva; Wagner Meira Jr,In data integration tasks; records from a single dataset or from different sources must beoften compared to identify records that represent the same real world entity. The cost of thissearch process for finding duplicate records grows quadratically as the number of recordsavailable in the data sources increases and; for this reason; direct approaches; ascomparing all record pairs; must be avoided. In this context; blocking methods that arebased on machine learning processes are used to find the best blocking function; based onthe combination of low cost rules; which define how to perform the record blocking. Thiswork presents a new blocking method based on machine learning. Different from othermethods; this new approach is based on genetic programming; allowing the use of moreflexible rules and a larger number of such rules for defining blocking functions; leading to …,SBBD,2009,1
Learning to deduplicate,Alberto HF Laender; Altigran S Da Silva; Marcos Andre Goncalves; Moises G De Carvalho,Identifying record replicas in digital libraries and other types of digital repositories isfundamental to improve the quality of their content and services as well as to yield eventualsharing efforts. Several deduplication strategies are available; but most of them rely onmanually chosen settings to combine evidence used to identify records as being replicas. Inthis paper; we present the results of experiments we have carried out with a novel machinelearning approach we have proposed for the deduplication problem. This approach; basedon genetic programming (GP); is able to automatically generate similarity functions toidentify record replicas in a given repository. The generated similarity functions properlycombine and weight the best evidence available among the record fields in order to tellwhen two distinct records represent the same real-world entity. The results of the …,Digital Libraries; 2006. JCDL'06. Proceedings of the 6th ACM/IEEE-CS Joint Conference on,2006,1
Uma Abordagem para Armazenamento de Dados Semi-Estruturados em Bancos de Dados Relacionais.,Karine V Magalhães; Alberto HF Laender; Altigran Soares da Silva,This paper presents an approach to storing semistructured data in relational databases. Wefocus on semistructured data as extracted from Web pages by a tool called DEByE (DataExtraction By Example); and organized according to its data model; the DEByE Object Model(DEByE-OM). The approach presented here consists in representing the structure of objectsextracted by DEByE by a relational schema and populating the corresponding databaseaccordinly. We also show how to retrieve such data by automatically transforming high-levelquery speci? cations (query patterns) into SQL queries that are executed over the relationaldatabase. Results of experiments carried out to evaluate our approach are also described.,SBBD,2001,1
Effects of Long-Term Walking on Baropodometric Parameters and Manual Muscle Strength,JM Rezende; PVO Vitorino; AA Silva; EN Pereira; TV Lemos; ALL Sousa,Abstract Objectives: Assess the static baropodometric parameters and manual musclestrength before and during five days of long-distance walking. Methods: Longitudinal studythat assessed 25 male participants. Five assessments were made: baseline 20 days beforethe event (A0) and the remainder (A1; A2; A3 and A4) during the walk at the end of eachday. For the assessment; a baropodometer and the hydraulic manual dynamometer.Results: The participants' average age was 45.6±9.1 years and the mean body mass index23.1±2.6 kg/m2. The assessment included 250 feet: 204 neutral and 46 hollow. Themaximum pressure in the right feet increased between A1 and A4 (p= 0.025) and droppedbetween A2 and A3 (p= 0.051). The contact surface of the right feet decreased between A0and A1 (p< 0.001); increased between A1 and A2 (p= 0.001) and decreased between A2 …,J Fam Med,2017,*
Towards the Effective Linking of Social Media Contents to Products in E-Commerce Catalogs,Henry S Vieira; Altigran S da Silva; Pável Calado; Marco Cristo; Edleno S de Moura,Abstract Online social media has become an essential part of our life. This media is oftencharacterized by its diverse content; which is produced by ordinary users. The potential toeasily express ideas and opinions has made social media a source of valuable informationon a variety of topics. In particular; information containing comments about consumerproducts has become prevalent. Here; we are interested in linking products mentioned inunstructured user-generated content; namely open discussion forums; to their respectiveentities in consumer product catalogs. Among the issues associated with this task; ambiguityis a particularly hard problem; as users typically refer to the same product using manydifferent forms and different products may share the same form. We argue that this problemcan be effectively solved using a set of evidences that can be easily extracted from social …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,*
Finding seeds to bootstrap focused crawlers,Karane Vieira; Luciano Barbosa; Altigran Soares Da Silva; Juliana Freire; Edleno Moura,Abstract Focused crawlers are effective tools for applications requiring a high number ofpages belonging to a specific topic. Several strategies for implementing these crawlers havebeen proposed in the literature; which aim to improve crawling efficiency by increasing thenumber of relevant pages retrieved while avoiding non-relevant pages. However; animportant aspect of these crawlers has been largely overlooked: the selection of the seedpages that serve as the starting points for a crawl. In this paper; we show that the seeds cangreatly influence the performance of crawlers; and propose a new framework forautomatically finding seeds. We describe a system that implements this framework andshow; through a detailed experimental evaluation; that by providing crawlers a seed set thatis large and varied; they not only obtain higher harvest rates but also an improved topic …,World Wide Web,2016,*
Heuristics to Improve the BMW Method and Its Variants,Lídia Lizziane Serejo de Carvalho; Edleno Silva de Moura; Caio Moura Daoud; Altigran Soares da Silva,*,Journal of Information and Data Management,2016,*
Quien sabe por Algebra; sabe scientificamente: A tribute to José Nuno Oliveira,Luís Soares Barbosa; A Silva; Alcino Cunha,Software technology is pre-scientific in its lack of an effective basis for predicting computers'behaviour. My research aims at improving scientific standards in software design throughformal methods and calculational techniques. These include the application of mathematicaltransforms in refactoring and improving existing software theories. I do this with passion andfirmly believe this will make computing better in the future. Since his return from Manchesterto Minho; in 1984; José Nuno Oliveira has devoted all his academic life to this programme.In other words; to the development of software modelling frameworks; as well as theassociated calculi; the two pillars of any true Engineering discipline. Along the road; hemade remarkable contributions to a broad range of topics—from data refinement to systemsprototyping; from relational calculi to typed linear algebra; from functional dependence …,*,2016,*
1B. 07: EVALUATION OF CENTRAL BLOOD PRESSURE DURING A VERY LONG DISTANCE WALKING.,PV Vitorino; WK Souza; EN Pereira; JM Rezende; AC Arantes; NG Marques; AA Silva; MC Pinheiro; TS Jardim; PC Jardim; AL Sousa,Abstract To evaluate the behavior of central blood pressure (BP) variables in male athletesbefore and during a very long distance walking-310Km route in five days. Longitudinal studywith 25 participants. This walking nominated as' Ecological Walk'happens in Brazil every julysince 1991. Its main goals are environmental preservation awareness; health lifestyleincitement as well as exercise practice incitement. The participants traveled the 310 Kmduring five days alternating walking and light running; a 72 Km/day average. First datacollection occurred one month before the walking (V0) and the others; during the second(V2); third (V3) and fourth walking day (V4); just after the athletes finished the daily route.Mobil O Graph was the device used to get central BP variables: pulse wave velocity (PWV);augmentation index (AIx); peripheral vascular resistance (PVR); central pulse pressure …,Journal of hypertension,2015,*
Heurísticas para aprimorar o método BMW e suas variantes,Lídia Lizziane Serejo de Carvalho,Several research efforts have been conducted in the literature to develop methods to reducethe cost of query processing in search engines. This research aims to propose modificationsto improve the performance of the block-Max WAND (BMW) algorithm; one of the mostefficient algorithms proposed previously. The BMW algorithm uses heuristics to discard thedocuments entries at query processing; which makes it extremely fast. In this dissertation; wepropose and evaluate additional heuristics to improve the perfomance of BMW and yourvariant BMW-CS in an attempt to both further reduces query processing times and theamount of memory required for processing queries.,*,2015,*
Interactions between soybean and weeds in a replacement series system; considering the effects of water stress,R Vivian; D Dourado-Neto; AA Silva; RB Franco; STR Correa,ABSTRACT Due to the increase of water deficiency in many farm regions and its meaningon weed interference; competitive interactions between soybean and three weeds wereevaluated under water stress (20 to 40 days after transplanting) and no stress conditions.Three independent experiments were carried out in a growth chamber; being each onecomposed by the weeds Alternanthera tenella; Tridax procumbens or Digitaria ciliaris; alongwith the crop; in which soil water condition and plant composition effects were evaluatedwhile in competition. A replacement series system was used; including both monoculture ofeach species and a mixture with a ratio of 50% between weed and soybean. A completelyrandomized design was used in factorial arrangement; with treatments distributed in threelevels for plant composition factor (soybean and weeds monocultures; in addition to the …,Planta Daninha,2013,*
Nivel de ruido en una unidad de terapia intensiva pediátrica: estudio observacional correlativo,Fernanda Maria do Carmo da Oliveira; Márcia Barbosa de Paiva; Maria Aparecida de Luca Nascimento; Vivian Marinho Rezende; Alexandre Souza da Silva; Carlos Roberto Lyra da Silva,Resumo: Aim: To measure noise levels in a pediatric intensive care customer unit anddiscuss the consequences of such noise in relation to professional actions. Methods: This isan observational and correlational study; performed in the pediatric intensive care unit of afederal hospital in Rio de Janeiro. We measured the noise level by decibel DEC-460 for fivenon-consecutive days and nights at five different hours. After a descriptive analysis of data;we performed a study of linear regression. Results: We identified an average of 62.64 dBA;with a standard deviation of 6.893 dBA and peak of 82.5 dBA during the daytime. The linearregression found that 44% of the variability of the noise is explained by the covariables.Discussion: The noise levels identified exceed recommendations of national andinternational organizations. We identified the professional actions and interactions as the …,Online braz. j. nurs.(Online),2013,*
NÍVEL DE RUÍDO EM UMA UNIDADE DE TERAPIA INTENSIVA PEDIÁTRICA: ESTUDO OBSERVACIONAL,FMCD N' OLIVEIR; Márcia Barbosa de PAIVA; MAL NASCIMENTO; VM REZENDE; AS SILVA; C ROBERTO,*,*,2013,*
Isótopos de estrôncio no estudo da proveniência do ligante em argamassas: crónica de um insucesso,I Cardoso; P Moita; JF Santos; A Candeias; AS Silva; J Mirão,A cidade Romana de Ammaia é um importante sítio arqueológico; localizado no distrito dePortalegre (Portugal); que se desenvolveu durante o Império Romano. Pensa-se que tenhasido abandonada de forma abrupta; apesar de algumas evidências de posteriorapropriação durante a ocupação muçulmana. Este uso descontinuado provocou a não-ocupação da cidade em épocas mais recentes; o que terá contribuído para a preservaçãodas estruturas e dos vestígios da organização da cidade. Integrado num projeto de estudomais vasto; as argamassas de assentamento das alvenarias e rebocos foram estudadas. Asargamassas podem revelar a capacidade tecnológica e construtiva das sociedades e dasua disponibilidade em empenharem recursos. No estudo prévio das argamassas daAmmaia; conclui-se que os agregados são locais e que em sucessivos períodos …,*,2013,*
JUDIE,Eli Cortez; Altigran S da Silva,Abstract This chapter presents Joint Unsupervised Structure Discovery and InformationExtraction (JUDIE) a method for addressing the IETS problem. JUDIE was presented in(Cortez et al. 2011). First; it is introduced the scenario to which JUDIE is targeted to; then wego over the proposed solution detailing all the steps that comprise JUDIE. Finally; anexperimental evaluation of JUDIE is presented; comparing its result with different baselinesavailable in the literature.,*,2013,*
Exploiting Pre-Existing Datasets to Support IETS,Eli Cortez; Altigran S da Silva,Abstract This chapter describes in detail a new approach for exploiting preexisting datasetsto support Information Extraction by Text Segmentation methods. First; it presents a briefoverview of the approach and introduces the concept of knowledge base. Next; it discussesall the steps involved in the unsupervised approach; including how to learn content-basedfeatures from knowledge bases; how to automatically induce structure-based features withno previous human-driven training; a feature that is unique to this approach; and how toeffectively combine these features to label segments of a text input.,*,2013,*
Conclusions and Future Work,Eli Cortez; Altigran S da Silva,In this book; it was presented and evaluated an unsupervised approach for the problem of InformationExtraction by Text Segmentation (IETS). This approach relies on knowledge bases to associatesegments in the input string with attributes of a given domain by using a very effective setcontent-based features. The effectiveness of the content-based features is also exploited to directlylearn from test data structure-based features; with no previous human-driven training; a featureunique to this approach … It was studied different aspects regarding this approach and comparedit with state-of-the-art IE methods. Results indicate that this approach performs quite well whencompared with such methods; even without any user intervention … Based on thisapproach; it produced a number of results to address the IETS problem in a unsupervisedfashion. Particularly; it was developed; implemented; and evaluated distinct IETS …,*,2013,*
iForm,Eli Cortez; Altigran S da Silva,Abstract This chapter presents iForm; a method for automatically using data-rich text forfilling form-based input interfaces that rely on the presented unsupervised approach to dealwith the Information Extraction by Text Segmentation problem. iForm was first presented inToda et al.(2009; 2010). In the following is described the scenario where iForm is applied;and the method in detail. A set of experiments is also reported that shows that iForm iseffective and works well in different scenarios.,*,2013,*
ONDUX,Eli Cortez; Altigran S da Silva,Abstract This chapter presents ONDUX (On Demand Unsupervised Information Extraction) amethod that relies on the presented unsupervised approach to deal with the InformationExtraction by Text Segmentation problem. ONDUX was first presented in Cortez et al.(2010)and in Cortez and da Silva (2010). Following; a tool based on ONDUX was presented inPorto et al.(2011). As other unsupervised IETS approaches; ONDUX relies on informationavailable on pre-existing data; but; unlike previously proposed methods; it also relies on avery effective set of content-based features to bootstrap the learning of structure-basedfeatures. More specifically; structure-based features are exploited to disambiguate theextraction of certain attributes through a reinforcement step. The reinforcement step relies onsequencing and positioning of attribute values directly learned on-demand from test data …,*,2013,*
Impact of template removal on Web search,Kaio Wagner; Edleno Silva de Moura; David Fernandes; Marco Cristo; Altigran Soares da Silva,Resumo Previous work in literature has indicated that template of web pages representnoisy information in web collections; and advocate that the simple removal of template resultin improvements in quality of results provided by Web search systems. In this paper; westudy the impact of template removal in two distinct scenarios: large scale web searchcollections; which consist of several distinct websites; and intrasite web collections; involvingsearches inside of web sites. Our work is the first in literature to study the impact of templateremoval to search systems in large scale Web collections. The study was carried out usingan automatic template detection method previously proposed by us. As contributions; wepresent statistics about the application of this automatic template detection method to thewell known GOV2 reference collection; a large scale Web collection. We also present …,Abakós,2012,*
Information Retrieval Research at UFMG,Nivio Ziviani; Marcos André Gonçalves; Edleno Silva de Moura; Berthier Ribeiro-Neto; Altigran Soares da Silva; Adriano Veloso,*,Journal of Information and Data Management,2011,*
The Database and Information Retrieval Research Group at UFAM,João MB Cavalcanti; Marco A Cristo; David B Fernandes; Edleno S de Moura; Altigran S da Silva,*,Journal of Information and Data Management,2011,*
Information Systems Special Issue on SBBD 2007,Altigran Soares da Silva,*,*,2010,*
ONDUX: On-demand unsupervised learning for information extraction,Altigran Soares da Silva; Marcos André Gonçalves; Edleno Silva de Moura,ABSTRACT Information extraction by text segmentation (IETS) applies to cases in whichdata values of interest are organized in implicit semi-structured records available in textualsources (eg postal addresses; bibliographic information; ads). It is an important practicalproblem that has been frequently addressed in the recent literature. In this paper weintroduce ONDUX (On Demand Unsupervised Information Extraction); a new unsupervisedprobabilistic approach for IETS. As other unsupervised IETS approaches; ONDUX relies oninformation available on pre-existing data to associate segments in the input string withattributes of a given domain. Unlike other approaches; we rely on very effective matchingstrategies instead of explicit learning strategies. The effectiveness of this matching strategyis also exploited to disambiguate the extraction of certain attributes through a …,*,2010,*
Diagnóstico da biodeterioração por fungos e bactérias nas pinturas murais da Casa de Fresco de Sanches Baena (Vila Viçosa; Portugal),M Rosário Martins; Susana Fialho; Mónica Lima; Sara Valadas; António Candeias; J Mirão; AS Silva; Deolinda Tavares; M Botto,Resumo A Casa de Fresco de Sanches Baena; situada em Vila Viçosa (Sudeste dePortugal); é uma pequena construção semi-subterrânea; de planta quadrangular; anexa ànora do antigo Palácio dos Sanches Baena. Os frescos que cobrem a sua abóbada eparedes apresentam cenas mitológicas ricas conjugadas com anjos musicais; conchas;porcelana e outros elementos decorativos; os quais fazem dela um exemplar especialmenterico e invulgar. As pinturas apresentam uma policromia intensa que sugere o uso de umapaleta rica composta por diferentes pigmentos. Infelizmente; devido ao abandono parcial eà falta de preservação; as pinturas estão num estado avançado de degradação sendovisível o destacamento de camadas pictóricas e argamassas; eflorescências salinas e umaabundante colonização microbiológica. Com este trabalho pretende-se identificar as …,Conservar Património,2009,*
Cooperative Research on Web Data Management at UFMG and UFAM-A Brief Report,Alberto HF Laender; Altigran Soares da Silva,The World Wide Web has become a huge repository of data of interest for a variety ofapplication domains. However; the same features that have made the Web so useful andpopular also impose important restrictions on the way the data it contains can bemanipulated. Particularly; in the traditional Web scenario; there is an inherent difficulty ingaining access to data that is implicitly present in Web pages but is not readily available.The term Web Data Management (WDM) has been used to refer to the study of problemsrelated to fetching; extracting; querying; modeling; storing; transforming; and integrating dataavailable on the Web. These issues have been growing in importance in the scientificcommunity in the last years; as it can be be seen by the considerable space devoted to themin important publication venues. This interest is justified not only by the scientific and …,Web Conference; 2008. LA-WEB'08.; Latin American,2008,*
A Lightweight Framework for Exchanging Web Data,Filipe Mesquita; Denilson Barbosa; Eli Cortez; Altigran S da Silva,We propose a lightweight framework for data exchange that is suitable for non-expert andcasual users sharing data on theWeb and/or through peer-to-peer systems. Unlike previ-ouswork; we consider a minimalistic data model and schema formalism that are suitable fordescribing online data and propose algorithms for mapping such schemas as well as fortranslating the corresponding instances. Also our solution requires minimal overhead andsetup costs (eg; we consider data stored in tables; XML or CSV files) comparing to ex-istingdata exchange systems; making it very attractive in our setting. We report experimentalresults indicating that our method works well with real Web data from various do-mains.,*,2007,*
GERINDO: Managing and Retrieving Information in Large Document Collections,Nivio Ziviani; Alberto HF Laender; Edleno S de Moura; Altigran S da Silva; Carlos A Heuser; Wagner Meira Jr,Abstract We present in this report a summary of the main results produced in the five years ofthe GERINDO research project. The aim of this project is to address the increasing demandfor software tools capable of dealing with information available in large documentcollections; such as the World Wide Web. It involves efforts of researchers from threeBrazilian universities to develop core technologies for a number of document managementapplications demanded by today's information society. These efforts are concentrated in sixmain research topics: document categorization; semistructured data management; agentsand focused crawlers; information retrieval models and searching techniques; efficiencyissues; and data mining. Besides specific contributions in these five research topics; theproject has stimulated the interaction among the researchers of the three universities who …,Departamento de Ciência da Computação; UFMG; Belo Horizonte,2007,*
Extracting and Searching Useful Information Available on Web FAQs.,Edson Oliveira; Altigran Soares da Silva; Edleno Silva de Moura; Joao MB Cavalcanti,Abstract. This paper presents new methods for structuring and searching for informationstored on Web FAQs. These methods are based on the assumption that such pages areimplicitly organized as a set of question-answer pairs (QAPs). The ultimate goal is toimprove the retrieval of answers available in FAQs for queries that can be answered usingthe information contained in them. More specifically; we propose modifications for three ofthe main tasks performed by search engines: crawling; indexing and query processing. Toevaluate our proposed methods; we used a large collection of documents from a real searchengine. We present the results of this evaluation for each of the three tasks.,SBBD,2006,*
School of Business,Alcides Seiça; Alexandre Silva; Isa Filipa Ferreira Vala; Filipe Oliveira; Jorge Santos,Find researchers and browse publications; full-texts; contact details andgeneral information related to the School of Business at Instituto Superiorde Contabilidade e Administração de Coimbra.,Transportation Research Part F Traffic Psychology and Behaviour,2005,*
Representing Web Data as Complex Objects,Altigran S da Silva; Elaine S Silva,Abstract. The popularization of the Web has made a huge volume of data available for alarge audience. In a large number of Web sites; such as bookstores; electronic catalogs;travel agencies; etc.; the pages constitute documents which are composed of pieces of datawhose overall structure can be easily recognized. Such pages are called data-rich and canbe seen as collections of complex objects. In this paper; we show how such objects can berepresented by nested tables; which are simple; intuitive; and quite convenient forexpressing their implicit structure. The assumption is that; for most sites of interest; only fewexamples are required to reveal the structure of the objects. To corroborate our assumption;we describe a data extraction tool that adopts this approach and present results of someexperiments carried out with this tool.,Electronic Commerce and Web Technologies: First International Conference; EC-Web 2000 London; UK; September 4-6; 2000 Proceedings,2003,*
Verificaç ao Automática da Qualidade de Dados Extraıdos da Web,Olga Regina Fradico de Oliveira; Altigran Soares da Silva,Resumo Um problema crıtico na extraçao de dados da Web é verificar a qualidade dosdados que foram extraıdos. Neste artigo; apresentamos uma abordagem que permiterealizar automaticamente a verificaç ao da qualidade de dados extraıdos da Web; com baseno posicionamento desses dados na página de origem. Essa verificaçao é realizadaatravés do cálculo de medidas de similaridade probabilısticas entre propriedades definidaspara o posicionamento dos dados em um conjunto de exemplo e essas mesmaspropriedades para a saıda observada. O valor dessa similaridade é entao utilizado paradefinir a qualidade dos dados extraıdos. A efetividade da nossa abordagem é demonstradapor resultados experimentais com diferentes fontes reais da Web.,*,2003,*
Structuring keyword-based queries for web databases,Rodrigo C Vieira; Pavel Calado; Altigran S da Silva; Alberto HF Laender; Berthier A Ribeiro-Neto,Abstract This paper describes a framework; based on Bayesian belief networks; for queryingWeb databases using keywords only. According to this framework; the user inputs a querythrough a simple search-box. From the input query; one or more plausible structured queriesare derived and submitted to Web databases. The results are then retrieved and presentedto the user as ranked answers. To evaluate our framework; an experiment using 38 examplequeries was carried out. We found out that 97% of the time; one of the top three resultingstructured queries is the proper one. Further; when the user selects one of these three topqueries for processing; the ranked answers present average precision figures of 92%.,Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries,2002,*
Consultando Bancos de Dados Disponíveis na Web Usando Palavras-Chave.,Rodrigo C Vieira; Pavel Calado; Altigran Soares da Silva; Alberto HF Laender; Berthier A Ribeiro-Neto,*,SBBD,2002,*
An Environment for Building and Maintaining Web Views.,Alisson R Arantes; Alberto HF Laender; Paulo Braz Golgher; Altigran Soares da Silva,The huge amount of data available on the Web creates great demand for methods and toolsthat allow the manipulation of these data with the same functionality and flexibility commonlyfound when accessing traditional databases. Thus; the notion of view as a mechanism forproviding access to data of interest; regardless of their structure; origin; and form of storage;has been revisited for the Web context. In this paper; we present an environment composedof a set of high-level tools that allow fetching; extraction; and integration of Web data. Usingthis environment; database designers can build and maintain Web views by definingschemas for data integration; specifying wrappers for data extraction; and defining plans forrefreshing the view contents.,Workshop on Information Integration on the Web,2001,*
Second International Workshop on the World Wide Web and Conceptual Modeling (WCM2000)-Managing and Querying Web Data and Metadata-An Example-Base...,Paulo B Golgher; Alberto HF Laender; Altigran S da Silva; Berthier Ribeiro-Neto,*,Lecture Notes in Computer Science,2000,*
Projeto/Reprojeto de bancos de dados relacionais: a ferramenta DB-Tool.,Anderson Almeida Ferreira; Alberto Henrique Frade Laender; Altigran Soares da Silva,This paper describes a tool that supp orts the design and redesign of relational databasesThe tool produces optimized relational representations of entity relationship ER schemasand is implemented using Informix as its target database management system DBMS Thetool operates in two phases In the first phase it receives as input an ER schema andgenerates a list of commands to implement the corresponding Informix schema In thesecond phase it receives a list of redesign commands specifying changes to the ER schemaand generates a redesign plan to reestructure the database accordingly An exampleillustrates the use of the tool.,*,1997,*
[Rural education in the POLONORDESTE; adequacy or imposition?[Rio Grande do Norte; Development Programme for the Northeast Integrated Areas; Brazil]].[Port...,AR Barbosa; AS Silva; MA Morais; MCG Gomes; MLB Lima; RL Araujo,*,*,1980,*
Angelis; L.; see Tsoumakas; G. 223–242,R Barr; T Griffiths; V Bhat; T Oates; V Shanbhag; C Nicholas; L Chen; S Wang; EA Rundensteiner; RHL Chiang; AHF Laender; EP Lim; A Corral; Y Manolopoulos; Y Theodoridis; M Vassilakopoulos; AS da Silva; J Palmieri Lage; AAA Fernandes; M Golfarelli; V Maniezzo; S Rizzi; PB Golgher; AAA Fernandes; NW Paton; R Barr; D Katsaros; AHF Laender; F Li; Z Liu; EP Lim; WK Ng; F Li; V Maniezzo; Y Manolopoulos; T Tzouramanis; WK Ng; C Nicholas; K Nørvåg; T Oates; AS da Silva; PB Golgher; NW Paton,*,Management (WIDM 2002),*,*
Sedação para procedimentos em crianças e adolescentes: uma proposta a partir do sistema grade,Sarah de Lima; Alexandre Rodrigues Ferreira Silva; Adrianne Mary Leão Sette; Flávia Cordeiro Oliveira; Livia Uliana Jacome Valerio; Brenda Corrêa de Godoi; Jader Pinto Santos; Flávio dos Santos Campos,RESUMO A realização de intervenções diagnósticas e terapêuticas dolorosas oudesagradáveis tem mais chance de sucesso e é mais segura quando a dor e a ansiedadesão controladas efetivamente. Este artigo tem como objetivo elaborar recomendações sobresedação para procedimentos em crianças e adolescentes; por não anestesiologistas; apartir da melhor evidência disponível. Foi realizada busca na literatura; que incluiu osprincipais sumários e diretrizes sobre o tema; e o sistema GRADE foi utilizado parahierarquizar o conjunto de evidências que sustenta cada recomendação. A sedação paraprocedimentos consiste em cinco etapas: preparo; monitorização; intervenções nãofarmacológicas; intervenções farmacológicas e alta. O preparo envolve avaliação clínica;preparo da equipe e do material. A monitorização do paciente deve ser rigorosa; de …,*,*,*
Top-down Extraction of Semi-Structured Data,Berthier Ribeiro-Neto Alberto HF Laender; Altigran S da Silva,Abstract In this paper; we propose an innovative approach to extracting semi-structured datafrom Web sources. The idea is to collect a couple of example objects from the user and touse this information to extract new objects from new pages or texts. We propose a top-downstrategy that extracts complex objects decomposing them in objects less complex; untilatomic objects have been extracted. Through experimentation; we demonstrate that with asmall number of given examples our strategy is able to extract most of the objects present ina Web source given as input.,*,*,*
Tratamento de Dados em Escalas e Projeções Diferentes em Sistemas de Informações Geográficas,Jussara Dolfini de Oliveira; Diógenes Salas Alves,Abstract. An important application of computer graphic techniques is geographic informationsystems (GIS). This paper presents a methodology for the integration of data at differentsscales and projections; using GIS. The methodology uses two datasets: deforestation (1:250.000 scale;'UTM projection); and RADAMBRASIL vegetation data (1: 1.000. 000 scaleand lambert projection).'The method is reconunended for users that have geo-referenceddata from different sources.,*,*,*
A Brief Survey of Web Data Extraction Tools,Altigran S da Silva; Juliana S Teixeira,ABSTRACT In the last few years; several works in the literature have addressed the problemof data extraction from Web pages. The importance of this problem derives from the fact that;once extracted; the data can be handled in a way similar to instances of a traditionaldatabase. The approaches proposed in the literature to address the problem of Web dataextraction use techniques borrowed from areas such as natural language processing;languages and grammars; machine learning; information retrieval; databases; andontologies. As a consequence; they present very distinct features and capabilities whichmake a direct comparison difficult to be done. In this paper; we propose a taxonomy forcharacterizing Web data extraction tools; briefly survey major Web data extraction toolsdescribed in the literature; and provide a qualitative analysis ofthem. Hopefully; this work …,*,*,*
SPIRE’99 List of Reviewers,Alistair Moffat; Altigran Silva; Andrew Turpin; Arlindo Oliveira; Artur Pessoa; Berthier Ribeiro-Neto; Douglas Oard; Edgar Chavez; Edleno Moura; Eduardo Laber; Esko Ukkonen; Giri Narasimhan; Gonzalo Navarro; Ilmerio Silva; Jesper Larsson; Joao Meidanis; Joao Setubal; Joao Silva; Johan Horebeek; Jorge Stolfi; Jose Borbinha; Jose Marroquin; Lila Kari; Marcio Araujo; Marco Casanova; Marden Neubert; Marie-France Sagot; Mark Daley; Max Garzon; Miguel Silva; Natasha Jonoska; Nivio Ziviani; Owen Kretser; Paulo Neves; Pavel Calado; Ricardo Baeza-Yates; Rob Kitto; Ruy Milidiu; Timothy Bell; Zanoni Dias,*,*,*,*
SPIRE'2001 Program Committe Chair Santiago; Chile,Gonzalo Navarro,SPIRE'2001 is a Symposium on String Processing and Information Retrieval; now in itseighth edition. This series of meetings originated in South America (Belo Horizonte andRecife; Brazil; in 1993 and 7996; Valparaiso; Chile; in 1995 and 1997) and were originallycalled WSP (Workshop on String Processing). Starting in 1998; at Santa Cruz; Bolivia; thefocus of the workshop was broadened to include the area of information retrieval due to itsincreasing slevance and its inter-relationship with the area of string processing. SPIRE'I 999;at Cancun; Mexico; SPIRE'2000; at A Coruia; Spain; and SPIRE'2001 continued this trend.,*,*,*
Avaliação de Técnicas Paralelas de Blocagem para a Resolução de Entidades e Deduplicação,Charles F Goncalves; Walter Santos; Luis FD Flores; Matheus S Vilela; Carla Machado; Wagner Meira Jr; Altigran Silva,Abstract. Data quality in databases is fundamental to many information managementapplications. One key criterion while measuring quality is the occurrence of duplicatedrecords in a database; justifying the development of deduplication and entity resolutiontechniques. In deduplication; the main challenge is the high complexity involved incomparing every single register in a database. In order to minimize such problem; blockingtechniques are used to reduce the number of comparisons; using fast and cheap metrics toidentify the similarity between each pair of records. In the present study; we evaluate someexisting blocking techniques implemented in a distributed; parallel and high scalablededuplication framework. We analyze them comparatively and identify the main advantagesand disadvantages achieved by a parallel execution. Resumo. A qualidade da …,*,*,*
Detecção e Extração de Templates em Páginas Web,Karane Vieira; Altigran Soares da Silva,O difundido uso de templates na Web é considerado prejudicial por duas razoes principais.Não só eles comprometem o julgamento de relevância de muitos métodos de RI emineração para a Web; mas também influenciam negativamente o uso de recursos porferramentas que processam paginas web. Neste artigo; apresentamos dois novosalgoritmos baseados em mapeamentos de arvores que de forma eficiente e acuradaremovem templates encontrados em coleções de paginas web inspecionando apenaspoucas paginas exemplo. Mostramos que nossos algoritmos são efetivos em identificartermos que ocorrem em templates-obtendo valores de medida F por volta de 0; 9-e que elespodem melhorar a acurácia de métodos de agrupamento e classificação de paginas web.,*,*,*
Preenchimento Automático de Formulários Web Utilizando Texto Livre,Guilherme A Toda; Altigran S da Silva,Atualmente na web a solução mais comum para usuários interagirem com aplicações queutilizam banco de dados e o uso de interfaces baseadas em formulários web compostos porvários campos de entrada; como caixas de texto; listas de seleção; caixas de marcação; etc.Apesar destes formulários serem efetivos e populares; em muitos casos; aplicações comentradas para texto livre são as preferidas pelos usuários. Neste trabalho; discutimos aproposta e a implementação de um método para preenchimento de formulários webutilizando dados contidos em textos em formato livre rico em dados. Nosso método permiteutilizar documentos não estruturados; ou partes desses documentos; para preencher oscampos do formulário automaticamente evitando que esta tarefa muitas vezes repetitiva epassível de erros; tenha que ser realizada manualmente pelos usuários. Nosso método …,*,*,*
Uma Abordagem Flexıvel para Extraç ao de Metadados em Citaç oes Bibliográficas,Eli Cortez; Altigran Soares da Silva,Resumo. Neste artigo apresentamos o FLUX-CiM; um novo método de extraçao decomponentes de citaçoes bibliográficas; tais como nomes de autores; tıtulos de artigo; etc.Tal método nao se baseia em padroes especıficos de codificaç ao de delimitadores de umdeterminado estilo de citaç ao; o que lhe confere um alto grau de automaçao e flexibilidade.Diferentemente de abordagens anteriores que dependem de treinamento manual pararealizar o processo de extraçao; o nosso método necessita apenas de uma base deconhecimento que pode ser automaticamente construıda a partir de um conjunto existentede registros de metadados de um dado domınio; por exemplo: Ciência da Computaçao;Ciências da Saúde; etc. Para demonstrar a eficácia e aplicabilidade do método proposto;realizamos experimentos que de extraçao dados de citaç oes bibliográficas de artigos …,*,*,*
Detecção de Sitios Replicados Utilizando Conteúdo e Estrutura,André Luiz da Costa Carvalho; Allan José de Souza Bezerra; Edleno Silva de Moura; Altigran Soares da Silva; Patrícia Silva Peres,Identifying replicated sites is an important task for search engines. It can reduce data storagecosts; improve query processing time and remove noises that might affect the quality of thefinal answer given to the user. This paper introduces a new approach to detect replicatedsites in search engines databases; using as replication evidences the websites' structureand the content of their pages. It is also depicted the result of experiments performed with areal search engine database. Our approach found 8.43% of the web pages stored in thedatabase were in replicated web sites with 94.4% precision; result witch is more accuratethan the ones found in other works.,*,*,*
Gerindo: New Technologies for Processing Information on Electronic Documents,Mara Abel; Joao MB Cavalcanti; Renato A Ferreira; Carlos A Heuser; Alberto HF Laender; Wagner Meira; Edleno S de Moura; Berthier A Ribeiro-Neto; Altigran S Da Silva; Nivio Ziviani,*,*,*,*
Uma Interface Gráfica para Consulta a Fontes de Dados XML,Tatiana Coelho; Alberto Henrique Frade Laender; Altigran Soares da Silva; Karine de Goes Louly,*,*,*,*
On the Correct Relational Representation of Complex Specialization Structures,Altigran Soares da Silva; Alberto HF Laender; Marco A Casanova,Abstract The mapping of ER schemas containing complex specialization structures into therelational model requires the use of speci c strategies to avoid inconsistent states in the nalrelational database. In this paper; we generalize a strategy for mapping such structures andcharacterize the class of ER schemas for which it generates relational schemas that correctlycaptures their semantics. We also show that this strategy may be adapted for generatingoptimized relational schemas for which the number of inclusion dependencies to beenforced is reduced.,*,*,*
Especificação de perfis baseados em palavras-chave em Disseminação de documentos XML,Felipe C Hummel; Altigran S da Silva; Mirella M Moro,*,*,*,*
