Runtime measurements in the cloud: observing; analyzing; and reducing variance,Jörg Schad; Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Abstract One of the main reasons why cloud computing has gained so much popularity isdue to its ease of use and its ability to scale computing resources on demand. As a result;users can now rent computing nodes on large commercial clusters through several vendors;such as Amazon and rackspace. However; despite the attention paid by Cloud providers;performance unpredictability is a major issue in Cloud computing for (1) databaseresearchers performing wall clock experiments; and (2) database applications providingservice-level agreements. In this paper; we carry out a study of the performance variance ofthe most widely used Cloud infrastructure (Amazon EC2) from different perspectives. We useestablished microbenchmarks to measure performance variance in CPU; I/O; and network.And; we use a multi-node MapReduce application to quantify the impact on real …,Proceedings of the VLDB Endowment,2010,550
Hadoop++: making a yellow elephant run like a cheetah (without it even noticing),Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Alekh Jindal; Yagiz Kargin; Vinay Setty; Jörg Schad,Abstract MapReduce is a computing paradigm that has gained a lot of attention in recentyears from industry and research. Unlike parallel DBMSs; MapReduce allows non-expertusers to run complex analytical tasks over very large data sets on very large clusters andclouds. However; this comes at a price: MapReduce processes tasks in a scan-orientedfashion. Hence; the performance of Hadoop---an open-source implementation ofMapReduce---often does not match the one of a well-configured parallel DBMS. In thispaper we propose a new type of system named Hadoop++: it boosts task performancewithout changing the Hadoop framework at all (Hadoop does not even'notice it'). To reachthis goal; rather than changing a working system (Hadoop); we inject our technology at theright places through UDFs only and affect Hadoop from inside. This has three important …,Proceedings of the VLDB Endowment,2010,464
Efficient big data processing in Hadoop MapReduce,Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Abstract This tutorial is motivated by the clear need of many organizations; companies; andresearchers to deal with big data volumes efficiently. Examples include web analyticsapplications; scientific applications; and social networks. A popular data processing enginefor big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered fromsevere performance problems. Today; this is becoming history. There are many techniquesthat can be used with Hadoop MapReduce jobs to boost performance by orders ofmagnitude. In this tutorial we teach such techniques. First; we will briefly familiarize theaudience with Hadoop MapReduce and motivate its use for big data processing. Then; wewill focus on different data management techniques; going from job optimization to physicaldata organization like data layouts and indexes. Throughout this tutorial; we will highlight …,Proceedings of the VLDB Endowment,2012,191
Only aggressive elephants are fast elephants,Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Stefan Richter; Stefan Schuh; Alekh Jindal; Jörg Schad,Abstract Yellow elephants are slow. A major reason is that they consume their inputs entirelybefore responding to an elephant rider's orders. Some clever riders have trained their yellowelephants to only consume parts of the inputs before responding. However; the teachingtime to make an elephant do that is high. So high that the teaching lessons often do not payoff. We take a different approach. We make elephants aggressive; only this will make themvery fast. We propose HAIL (Hadoop Aggressive Indexing Library); an enhancement ofHDFS and Hadoop MapReduce that dramatically improves runtimes of several classes ofMapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create differentclustered indexes on each data block replica. An interesting feature of HAIL is that wetypically create a win-win situation: we improve both data upload to HDFS and the …,Proceedings of the VLDB Endowment,2012,122
Trojan data layouts: right shoes for a running elephant,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract MapReduce is becoming ubiquitous in large-scale data analysis. Several recentworks have shown that the performance of Hadoop MapReduce could be improved; forinstance; by creating indexes in a non-invasive manner. However; they ignore the impact ofthe data layout used inside data blocks of Hadoop Distributed File System (HDFS). In thispaper; we analyze different data layouts in detail in the context of MapReduce and arguethat Row; Column; and PAX layouts can lead to poor system performance. We propose anew data layout; coined Trojan Layout; that internally organizes data blocks into attributegroups according to the workload in order to improve data access times. A salient feature ofTrojan Layout is that it fully preserves the fault-tolerance properties of MapReduce. Weimplement our Trojan Layout idea in HDFS 0.20. 3 and call the resulting system Trojan …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,99
RAFTing MapReduce: Fast recovery on the RAFT,Jorge-Arnulfo Quiane-Ruiz; Christoph Pinkel; Jörg Schad; Jens Dittrich,MapReduce is a computing paradigm that has gained a lot of popularity as it allows non-expert users to easily run complex analytical tasks at very large-scale. At such scale; taskand node failures are no longer an exception but rather a characteristic of large-scalesystems. This makes fault-tolerance a critical issue for the efficient operation of anyapplication. MapReduce automatically reschedules failed tasks to available nodes; which inturn recompute such tasks from scratch. However; this policy can significantly decreaseperformance of applications. In this paper; we propose a family of Recovery Algorithms forFast-Tracking (RAFT) MapReduce. As ease-of-use is a major feature of MapReduce; RAFTfocuses on simplicity and also non-intrusiveness; in order to be implementation-independent. To efficiently recover from task failures; RAFT exploits the fact that …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,61
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,54
Scalable discovery of unique column combinations,Arvid Heise; Jorge-Arnulfo Quiané-Ruiz; Ziawasch Abedjan; Anja Jentzsch; Felix Naumann,Abstract The discovery of all unique (and non-unique) column combinations in a givendataset is at the core of any data profiling effort. The results are useful for a large number ofareas of data management; such as anomaly detection; data integration; data modeling;duplicate detection; indexing; and query optimization. However; discovering all unique andnon-unique column combinations is an NP-hard problem; which in principle requires toverify an exponential number of column combinations for uniqueness on all data values.Thus; achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper; we devise Ducc; a scalable and efficient approach to the problem of finding allunique and non-unique column combinations in big datasets. We first model the problem asa graph coloring problem and analyze the pruning effect of individual combinations. We …,Proceedings of the VLDB Endowment,2013,46
Towards zero-overhead static and adaptive indexing in Hadoop,Stefan Richter; Jorge-Arnulfo Quiané-Ruiz; Stefan Schuh; Jens Dittrich,Abstract Hadoop MapReduce has evolved to an important industry standard for massiveparallel data processing and has become widely adopted for a variety of use-cases. Recentworks have shown that indexes can improve the performance of selective MapReduce jobsdramatically. However; one major weakness of existing approaches is high index creationcosts. We present HAIL (Hadoop Aggressive Indexing Library); a novel indexing approachfor HDFS and Hadoop MapReduce. HAIL creates different clustered indexes over terabytesof data with minimal; often invisible costs; and it dramatically improves runtimes of severalclasses of MapReduce jobs. HAIL features two different indexing pipelines; static indexingand adaptive indexing. HAIL static indexing efficiently indexes datasets while uploadingthem to HDFS. Thereby; HAIL leverages the default replication of Hadoop and enhances …,The VLDB journal,2014,33
Cliquesquare: Flat plans for massively parallel RDF queries,François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge-Arnulfo Quiané-Ruiz; Stamatis Zampetakis,As increasing volumes of RDF data are being produced and analyzed; many massivelydistributed architectures have been proposed for storing and querying this data. Thesearchitectures are characterized first; by their RDF partitioning and storage method; andsecond; by their approach for distributed query optimization; ie; determining whichoperations to execute on each node in order to compute the query answers. We presentCliqueSquare; a novel optimization approach for evaluating conjunctive RDF queries in amassively parallel environment. We focus on reducing query response time; and thus seekto build flat plans; where the number of joins encountered on a root-to-leaf path in the plan isminimized. We present a family of optimization algorithms; relying on n-ary (star) equalityjoins to build flat plans; and compare their ability to find the flattest possibles. We have …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,31
Towards zero-overhead adaptive indexing in hadoop,Stefan Richter; Jorge-Arnulfo Quiané-Ruiz; Stefan Schuh; Jens Dittrich,Abstract: Several research works have focused on supporting index access in MapReducesystems. These works have allowed users to significantly speed up selective MapReducejobs by orders of magnitude. However; all these proposals require users to create indexesupfront; which might be a difficult task in certain applications (such as in scientific and socialapplications) where workloads are evolving or hard to predict. To overcome this problem; wepropose LIAH (Lazy Indexing and Adaptivity in Hadoop); a parallel; adaptive approach forindexing at minimal costs for MapReduce systems. The main idea of LIAH is to automaticallyand incrementally adapt to users' workloads by creating clustered indexes on HDFS datablocks as a byproduct of executing MapReduce jobs. Besides distributing indexing effortsover multiple computing nodes; LIAH also parallelises indexing with both map tasks …,arXiv preprint arXiv:1212.3480,2012,25
NADEEF: A generalized data cleaning system,Amr Ebaid; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang; Si Yin,Abstract We present NADEEF; an extensible; generic and easy-to-deploy data cleaningsystem. NADEEF distinguishes between a programming interface and a core to achievegenerality and extensibility. The programming interface allows users to specify data qualityrules by writing code that implements predefined classes. These classes uniformly definewhat is wrong with the data and (possibly) how to fix it. We will demonstrate the followingfeatures provided by NADEEF.(1) Heterogeneity: The programming interface can be used toexpress many types of data quality rules beyond the well known CFDs (FDs); MDs and ETLrules.(2) Interdependency: The core algorithms can interleave multiple types of rules todetect and repair data errors.(3) Deployment and extensibility: Users can easily customizeNADEEF by defining new types of rules; or by extending the core.(4) Metadata …,Proceedings of the VLDB Endowment,2013,24
Efficient or Hadoop: why not both?,Jens Dittrich; Stefan Richter; Stefan Schuh,Abstract In this article; we give an overview of research related to Big Data processing inHadoop going on at the Information Systems Group at Saarland University. We discuss howto make Hadoop efficient. We briefly survey three of our projects in this context: Hadoop++;Trojan Layouts; and HAIL.,Datenbank-Spektrum,2013,22
Divide & conquer-based inclusion dependency discovery,Thorsten Papenbrock; Sebastian Kruse; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,Abstract The discovery of all inclusion dependencies (INDs) in a dataset is an important partof any data profiling effort. Apart from the detection of foreign key relationships; INDs canhelp to perform data integration; query optimization; integrity checking; or schema (re-)design. However; the detection of INDs gets harder as datasets become larger in terms ofnumber of tuples as well as attributes. To this end; we propose Binder; an IND detectionsystem that is capable of detecting both unary and n-ary INDs. It is based on a divide &conquer approach; which allows to handle very large datasets--an important property on theface of the ever increasing size of today's data. In contrast to most related works; we do notrely on existing database functionality nor assume that inspected datasets fit into mainmemory. This renders Binder an efficient and scalable competitor. Our exhaustive …,Proceedings of the VLDB Endowment,2015,21
WWHow! Freeing Data Storage from Cages.,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract. Efficient data storage is a key component of data managing systems to achievegood performance. However; currently data storage is either heavily constrained by staticdecisions (eg fixed data stores in DBMSs) or left to be tuned and configured by users (egmanual data backup in File Systems). In this paper; we take a holistic view of data storageand envision a virtual storage layer. Our virtual storage layer provides a unified storageframework for several use-cases including personal; enterprise; and cloud storage.,CIDR,2013,18
A self-adaptable query allocation framework for distributed information systems,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract In large-scale distributed information systems; where participants are autonomousand have special interests for some queries; query allocation is a challenge. Much work inthis context has focused on distributing queries among providers in a way that maximizesoverall performance (typically throughput and response time). However; preserving theparticipants' interests is also important. In this paper; we make the following contributions.First; we provide a model to define the participants' perception of the system regarding theirinterests and propose measures to evaluate the quality of query allocation methods. Then;we propose a framework for query allocation called Satisfaction-based Query LoadBalancing (SQLB; for short); which dynamically trades consumers' interests for providers'interests based on their satisfaction. Finally; we compare SQLB; through experimentation …,The VLDB Journal—The International Journal on Very Large Data Bases,2009,16
Sqlb: A query allocation framework for autonomous consumers and providers,J-A Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract In large-scale distributed information systems; where participants are autonomousand have special interests for some queries; query allocation is a challenge. Much work inthis context has focused on distributing queries among providers in a way that maximizesoverall performance (typically throughput and response time). However; preserving theparticipants" interests is also important. In this paper; we make two main contributions. First;we provide a model to define participants' perception of the system wrt their interests andpropose metrics to evaluate the quality of query allocation methods. This model facilitatesthe design and evaluation of new query allocation methods that take into account theparticipants' interests. Second; we propose a framework for query allocation calledSatisfaction-based Query Load Balancing (SQLB). To be fair; SQLB dynamically trades …,Proceedings of the 33rd international conference on Very large data bases,2007,16
Lightning fast and space efficient inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which join relational tables on inequality conditions; are used invarious applications. While there have been a wide range of optimization methods for joinsin database systems; from algorithms such as sort-merge join and band join; to variousindices such as B+-tree; R*-tree and Bitmap; inequality joins have received little attentionand queries containing such joins are usually very slow. In this paper; we introduce fastinequality join algorithms. We put columns to be joined in sorted arrays and we usepermutation arrays to encode positions of tuples in one sorted array wrt the other sortedarray. In contrast to sort-merge join; we use space efficient bit-arrays that enableoptimizations; such as Bloom filter indices; for fast computation of the join results. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; and a …,Proceedings of the VLDB Endowment,2015,15
CliqueSquare: efficient Hadoop-based RDF query processing,François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge Quiané-Ruiz; Stamatis Zampetakis,Large volumes of RDF data collections are being created; published and used lately invarious contexts; from scientific data to domain ontologies and to open government data; inparticular in the context of the Linked Data movement. Managing such large volumes of RDFdata is challenging due to the sheer size and the heterogeneity. To tackle the sizechallenge; a single isolated machine is not an efficient solution anymore. The MapReduceparadigm is a promising direction providing scalability and massively parallel processing oflarge-volume data. We present CliqueSquare; an efficient RDF data management platformbased on Hadoop; an open source MapReduce implementation; and its file system; HadoopDistributed File System (HDFS). CliqueSquare relies on a novel RDF data partitioningscheme enabling queries to be evaluated efficiently; by minimizing both the number of …,BDA'13-Journées de Bases de Données Avancées,2013,15
Raft at work: speeding-up mapreduce applications under task and node failures,Jorge-Arnulfo Quiané-Ruiz; Christoph Pinkel; Jörg Schad; Jens Dittrich,Abstract The MapReduce framework is typically deployed on very large computing clusterswhere task and node failures are no longer an exception but the rule. Thus; fault-tolerance isan important aspect for the efficient operation of MapReduce jobs. However; currentlyMapReduce implementations fully recompute failed tasks (subparts of a job) from thebeginning. This can significantly decrease the runtime performance of MapReduceapplications. We present an alternative system that implements RAFT ideas. RAFT is afamily of powerful and inexpensive Recovery Algorithms for Fast-Tracking MapReduce jobsunder task and node failures. To recover from task failures; RAFT exploits the intermediateresults persisted by MapReduce at several points in time. RAFT piggybacks checkpoints onthe task progress computation. To recover from node failures; RAFT maintains a per-map …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,15
A semantic information system for services and traded resources in Grid e-markets,George A Vouros; Andreas Papasalouros; Konstantinos Tzonas; Alexandros Valarakos; Konstantinos Kotis; Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract This paper aims at presenting a semantic information system for the advertisement;retrieval and selection of application services; and markets that trade resources; in ademocratized Grid e-marketplace environment. The market-based environment in ademocratized Grid context poses new requirements concerning the functionality of asemantic information system. In this context; the paper motivates the development of theGrid4All Semantic Information System (G4A-SIS) and presents the design and thetechnologies used for the realization of this system. G4A-SIS is deployed as a web serviceusing a Java API which defines the functionality of the system and abstracts low-levelontology implementation details; providing to the clients an interface for registering andquerying semantically annotated market and application-specific services. In addition to …,Future Generation Computer Systems,2010,15
Road to Freedom in Big Data Analytics.,Divy Agrawal; Sanjay Chawla; Ahmed K Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,ABSTRACT The world is fast moving towards a data-driven society where data is the mostvaluable asset. Organizations need to perform very diverse analytic tasks using various dataprocessing platforms. In doing so; they face many challenges; chiefly; platform dependence;poor interoperability; and poor performance when using multiple platforms. We presentRHEEM; our vision for big data analytics over diverse data processing platforms. RHEEMprovides a threelayer data processing and storage abstraction to achieve both platformindependence and interoperability across multiple platforms. In this paper; we discuss ourvision as well as present multiple research challenges that we need to address to achieve it.As a case in point; we present a data cleaning application built using some of the ideas ofRHEEM. We show how it achieves platform independence and the performance benefits …,EDBT,2016,13
Satisfaction-Based query load balancing,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract We consider the query allocation problem in open and large distributed informationsystems. Provider sources are heterogeneous; autonomous; and have finite capacity toperform queries. A main objective in query allocation is to obtain good response time. Mostof the work towards this objective has dealt with finding the most efficient providers. But littleattention has been paid to satisfy the providers interest in performing certain queries. In thispaper; we address both sides of the problem. We propose a query allocation approachwhich allows providers to express their intention to perform queries based on theirpreference and satisfaction. We compare our approach to both query load balancing andeconomic approaches. The experimentation results show that our approach yields highefficiency while supporting the providers' preferences in adequacy with the query load …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2006,12
Rheem: Enabling multi-platform task execution,Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawla; Ahmed Elmagarmid; Hossam Hammady; Yasser Idris; Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,Abstract Many emerging applications; from domains such as healthcare and oil & gas;require several data processing systems for complex analytics. This demo paper showcasessystem; a framework that provides multi-platform task execution for such applications. Itfeatures a three-layer data processing abstraction and a new query optimization approachfor multi-platform settings. We will demonstrate the strengths of system by using real-worldscenarios from three different applications; namely; machine learning; data cleaning; anddata fusion.,Proceedings of the 2016 International Conference on Management of Data,2016,10
CARTILAGE: adding flexibility to the Hadoop skeleton,Alekh Jindal; Jorge Quiané-Ruiz; Samuel Madden,Abstract Modern enterprises have to deal with a variety of analytical queries over very largedatasets. In this respect; Hadoop has gained much popularity since it scales to thousand ofnodes and terabytes of data. However; Hadoop suffers from poor performance; especially inI/O performance. Several works have proposed alternate data storage for Hadoop in order toimprove the query performance. However; many of these works end up making deepchanges in Hadoop or HDFS. As a result; they are (i) difficult to adopt by several users; and(ii) not compatible with future Hadoop releases. In this paper; we present CARTILAGE; acomprehensive data storage framework built on top of HDFS. CARTILAGE allows users fullcontrol over their data storage; including data partitioning; data replication; data layouts; anddata placement. Furthermore; CARTILAGE can be layered on top of an existing HDFS …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,10
Managing virtual money for satisfaction and scale up in p2p systems,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Sylvie Cazalens; Patrick Valduriez,Abstract In peer-to-peer data management systems query allocation is a critical issue for thegood operation of the system. This task is challenging because participants may prefer toperform some queries than others. Microeconomic mechanisms aim at dealing with this; but;to the best of our knowledge; none of them has ever proposed experimental validations that;beyond query load or response time; use measures that are outside the microeconomicscope. The contribution of this paper is twofold. We present a virtual money-based queryallocation process that is suitable for large-scale super peer systems. We compare a nonmicroeconomic mediation with micro-economic ones from a satisfaction point of view. Theexperimental results show that the providers' invoice phase is as much important as theproviders' selection phase for a virtual money-based mediation.,Proceedings of the 2008 international workshop on Data management in peer-to-peer systems,2008,10
K n best-a balanced request allocation method for distributed information systems,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract In large-scale distributed information systems; providers are typically autonomous;ie free to leave the system at will or to perform certain requests. In this context; requestallocation is critical for the efficient system's operation. However; most methods used indistributed information systems aim at maximizing overall system performance (throughputand response times) by allocating requests to the most efficient providers; withoutconsidering providers' autonomy. In this paper; we propose a balanced request allocationmethod; K n Best; which considers providers' autonomy in addition to load balancing. Ourmethod is general and simple; so that it can be easily incorporated in existing distributedinformation systems. We describe the implementation of K n Best in different scenarios.Finally; we give an experimental evaluation which shows that K n Best significantly …,International Conference on Database Systems for Advanced Applications,2007,10
NADEEF/ER: Generic and interactive entity resolution,Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,ABSTRACT Entity resolution (ER); the process of identifying and eventually merging recordsthat refer to the same real-world entities; is an important and long-standing problem. Wepresent Nadeef/Er; a generic and interactive entity resolution system; which is built as anextension over our open-source generalized data cleaning system Nadeef. Nadeef/Erprovides a rich programming interface for manipulating entities; which allows generic;efficient and extensible ER. In this demo; users will have the opportunity to experience thefollowing features:(1) Easy specification–Users can easily define ER rules with a browser-based specification; which will then be automatically transformed to various functions;treated as black-boxes by Nadeef;(2) Generality and extensibility–Users can customize theirER rules by refining and fine-tuning the above functions to achieve both effective and …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,9
Generating concise entity matching rules,Rohit Singh; Vamsi Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration and cleaning. In manyapplications; the users need to understand why two entities are considered a match; whichreveals the need for interpretable and concise EM rules. We model EM rules in the form ofGeneral Boolean Formulas (GBFs) that allows arbitrary attribute matching combined byconjunctions (∨); disjunctions (∧); and negations.(¬) GBFs can generate more conciserules than traditional EM rules represented in disjunctive normal forms (DNFs). We useprogram synthesis; a powerful tool to automatically generate rules (or programs) thatprovably satisfy a high-level specification; to automatically synthesize EM rules in GBFformat; given only positive and negative matching examples. In this demo; attendees willexperience the following features:(1) Interpretability--they can see and measure the …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,6
Detecting unique column combinations on dynamic data,Ziawasch Abedjan; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,The discovery of all unique (and non-unique) column combinations in an unknown datasetis at the core of any data profiling effort. Unique column combinations resemble candidatekeys of a relational dataset. Several research approaches have focused on their efficientdiscovery in a given; static dataset. However; none of these approaches are suitable forapplications on dynamic datasets; such as transactional databases; social networks; andscientific applications. In these cases; data profiling techniques should be able to efficientlydiscover new uniques and non-uniques (and validate old ones) after tuple inserts or deletes;without re-profiling the entire dataset. We present the first approach to efficiently discoverunique and non-unique constraints on dynamic datasets that is independent of the initialdataset size. In particular; Swan makes use of intelligently chosen indices to minimize …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,6
Scaling up query allocation in the presence of autonomous participants,Joge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Sylvie Cazalens; Patrick Valduriez,Abstract In large-scale; heterogeneous information systems; mediators are widely used forquery processing and the good operation of a system strongly depends on the way themediator allocates queries. On the other hand; it is well known that a single mediator is apotential scalability and performance bottleneck as well as a single point of failure. Thus;multiple mediators should perform the query allocation process. This task is challenging inlarge-scale systems because participants typically have special interests that are notperformance-related. Mediators should satisfy participants interests as if there was a singlemediator in the system—ie; with no; or almost no; additional network traffic. In this paper; wepropose a virtual money-based query allocation method; called VM b QA; to perform queryallocation in the presence of multiple mediators and autonomous participants. A key …,International Conference on Database Systems for Advanced Applications,2011,6
RDfind: scalable conditional inclusion dependency discovery in RDF datasets,Sebastian Kruse; Anja Jentzsch; Thorsten Papenbrock; Zoi Kaoudi; Jorge-Arnulfo Quiané-Ruiz; Felix Naumann,Abstract Inclusion dependencies (INDs) form an important integrity constraint on relationaldatabases; supporting data management tasks; such as join path discovery and queryoptimization. Conditional inclusion dependencies (CINDs); which define including andincluded data in terms of conditions; allow to transfer these capabilities to RDF data.However; CIND discovery is computationally much more complex than IND discovery andthe number of CINDs even on small RDF datasets is intractable. To cope with bothproblems; we first introduce the notion of pertinent CINDs with an adjustable relevancecriterion to filter and rank CINDs based on their extent and implications among each other.Second; we present RDFind; a distributed system to efficiently discover all pertinent CINDsin RDF data. RDFind employs a lazy pruning strategy to drastically reduce the CIND …,Proceedings of the 2016 International Conference on Management of Data,2016,5
CliqueSquare in action: Flat plans for massively parallel RDF queries,Benjamin Djahandideh; François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge-Arnulfo Quiané-Ruiz; Stamatis Zampetakis,RDF is an increasingly popular data model for many practical applications; leading to largevolumes of RDF data; efficient RDF data management methods are crucial to allowapplications to scale. We propose to demonstrate CliqueSquare; an RDF data managementsystem built on top of a MapReduce-like infrastructure. The main technical novelty ofCliqueSquare resides in its logical query optimization algorithm; guaranteed to find a logicalplan as flat as possible for a given query; meaning: a plan having the smallest possiblenumber of join operators on top of each other. CliqueSquare's ability to build flat plansallows it to take advantage of a parallel processing framework in order to shorten responsetimes. We demonstrate loading and querying the data; with a particular focus on queryoptimization; and on the performance benefits of CliqueSquare's flat plans.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,5
Satisfaction balanced mediation,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Sylvie Cazalens; Patrick Valduriez,Abstract We consider a distributed information system that allows autonomous consumers toquery autonomous providers. We focus on the problem of query allocation from a new pointof view; by considering consumers and providers' satisfaction in addition to query load. Wedefine satisfaction as a long-run notion based on the consumers and providers' preferences.We propose and validate a mediation process; called SBMediation; which is compared toCapacity based query allocation. The experimental results show that SBMediationsignificantly outperforms Capacity based when confronted to autonomous participants.,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,5
Uguide: User-guided discovery of fd-detectable errors,Saravanan Thirumuruganathan; Laure Berti-Equille; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang,Abstract Error detection is the process of identifying problematic data cells that are differentfrom their ground truth. Functional dependencies (FDs) have been widely studied in supportof this process. Oftentimes; it is assumed that FDs are given by experts. Unfortunately; it isusually hard and expensive for the experts to define such FDs. In addition; automatic dataprofiling over dirty data in order to find correct FDs is known to be a hard problem. In thispaper; we propose an end-to-end solution to detect FD-detectable errors from dirty data. Thebroad intuition is that given a dirty dataset; it is feasible to automatically find approximateFDs; as well as data that is possibly erroneous. Arguably; at this point; only experts canconfirm true FDs or true errors. However; in practice; experts never have enough budget tofind all errors. Hence; our problem is; given a limited budget of expert's time; which …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
Fast and scalable inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which is to join relations with inequality conditions; are used invarious applications. Optimizing joins has been the subject of intensive research rangingfrom efficient join algorithms such as sort-merge join; to the use of efficient indices such asB^+ B+-tree; R^* R∗-tree and Bitmap. However; inequality joins have received little attentionand queries containing such joins are notably very slow. In this paper; we introduce fastinequality join algorithms based on sorted arrays and space-efficient bit-arrays. We furtherintroduce a simple method to estimate the selectivity of inequality joins which is then used tooptimize multiple predicate queries and multi-way joins. Moreover; we study an incrementalinequality join algorithm to handle scenarios where data keeps changing. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; a distributed …,The VLDB Journal,2017,3
A satisfaction balanced query allocation process for distributed information systems,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Sylvie Cazalens; Patrick Valduriez,We consider a distributed information system that allows autonomous consumers to queryautonomous providers. We focus on the problem of query allocation from a new point ofview; by considering consumers and providers' satisfaction in addition to query load. Wedefine satisfaction as a long-run notion based on the consumers and providers' intentions.Intuitively; a participant should obtain good satisfaction as far as it (the participant) isadequate to the system. We propose and validate a mediation process; called SBMediation;which is compared to CapacityBased query allocation. The experimental results show thatSBMediation significantly outperforms CapacityBased when confronted to autonomousparticipants.,Base de Données Avancées (BDA),2007,3
Libra: une méthode de médiation auto-adaptative en fonction des attentes des participants,Philippe Lamarre; Jorge-Arnulfo Quiané-Ruiz; Patrick Valduriez,Nous considérons le problème de l'allocation de tâches dans le cadre d'environnementsouverts où les participants (fournisseurs et clients) ont des attentes particulières. Denombreux travaux sur l'allocation de tâches se sont concentrés sur des problèmes demaximisation ou de minimisation de fonctions prédéfinies (temps de réponse; répartition decharge...). Cependant; ces objectifs sont définis de manière globale alors que lesparticipants peuvent avoir des objectifs dif-férents; voire divergents. Nous proposons ici uneméthode de médiation; Libra; qui s' auto-adapte aux attentes des participants. Libra effectueles allocations en fonction des intérêts individuels des fournisseurs et des clients. Cetteprise en compte est régulée par les satisfactions individuelles de sorte à obtenir unecertaine équité. Des simulations nous permettent de comparer Libra avec d'autres …,Journnées Francophones des Systèmes Multi-Agents,2007,3
A Cost-based Optimizer for Gradient Descent Optimization,Zoi Kaoudi; Jorge-Arnulfo Quiané-Ruiz; Saravanan Thirumuruganathan; Sanjay Chawla; Divy Agrawal,Abstract As the use of machine learning (ML) permeates into diverse application domains;there is an urgent need to support a declarative framework for ML. Ideally; a user will specifyan ML task in a high-level and easy-to-use language and the framework will invoke theappropriate algorithms and system configurations to execute it. An important observationtowards designing such a framework is that many ML tasks can be expressed asmathematical optimization problems; which take a specific form. Furthermore; theseoptimization problems can be efficiently solved using variations of the gradient descent (GD)algorithm. Thus; to decouple a user specification of an ML task from its execution; a keycomponent is a GD optimizer. We propose a cost-based GD optimizer that selects the bestGD plan for a given ML task. To build our optimizer; we introduce a set of abstract …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,2
How to deal with Cliques at Work,Benjamin Djahandideh; François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge Quiané-Ruiz; Stamatis Zampetakis,RDF is an increasingly popular data model for many practical applications; leading to largevolumes of RDF being created and exploited. Efficient RDF data management methods arecrucial to allow applications to scale. In this demonstration; we showcase the high efficiencyof CliqueSquare; an RDF data management system designed on top of a MapReduce-likeinfrastructure. We demonstrate three major aspects of CliqueSquare:(i) significantly reducingthe network traffic during query evaluation;(ii) evaluating even large queries into fewMapReduce jobs;(iii) improving query performance by producing DAG-shaped plans. In alldemonstration scenarios; the audience is invited to interact with the system to ask queries;explore and control the features of the platform's many optimization algorithms; and finallyselect and monitor the evaluation of plans in two available clusters.,BDA'2014: 30e journées Bases de Données Avancées,2014,2
CliqueSquare: Flat Plans for Massively Parallel RDF Queries,François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge Quiané-Ruiz; Stamatis Zampetakis,As increasing volumes of RDF data are being produced and analyzed; many massivelydistributed architectures have been proposed for storing and querying this data. Thesearchitectures are characterized first; by their RDF partitioning and storage method; andsecond; by their approach for distributed query optimization; ie; determining whichoperations to execute on each node in order to compute the query answers. We presentCliqueSquare; a novel optimization approach for evaluating conjunctive RDF queries in amassively parallel environment. We focus on reducing query response time; and thus seekto build flat plans; where the number of joins encountered on a root-to-leaf path in the plan isminimized. We present a family of optimization algorithms; relying on n-ary (star) equalityjoins to build flat plans; and compare their ability to find the flattest possibles. We have …,*,2014,2
Satisfaction-based query replication,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Abstract In large-scale Internet-based distributed systems; participants (consumers andproviders) are typically autonomous; ie they may have special interests towards queries andother participants. In this context; a way to avoid a participant to voluntarily leave the systemis satisfying its interests when allocating queries. However; participants satisfaction may alsobe negatively affected by the failures of other participants. Query replication is a solution todeal with providers failures; but; it is challenging because of autonomy: it cannot only quicklyoverload the system; but also it can dissatisfy participants with uninteresting queries. Thus; anatural question arises: should queries be replicated? If so; which ones? and how manytimes? In this paper; we answer these questions by revisiting query replication from asatisfaction and probabilistic point of view. We propose a new algorithm; called S b QR …,Distributed and Parallel Databases,2012,2
Un modele pour caractériser des participants autonomes dans un processus de médiation,Jorge-Amulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Nous considérons les systèmes d'information distribués dans lesquels les participants sontnon seulement libres de quitter le système; mais peuvent aussi manifester différents intérêts.La plupart des travaux dans ce contexte sont centrés sur la performance (répartition decharge; temps de réponse...) sans tenir compte des intérêts des participants. Pourtant; lenon respect de leurs intérêts particuliers peut conduire les participants à quitter le système.Nous proposons une nouveau modèle qui aide à caractériser la satisfaction des participantssur le long terme ainsi que leur adéquation. We consider distributed information systemswhere participants are autonomous and have also special interests. Most of the works in thiscontext are centered on the performences but do not take participants' particular interest intoaccount. However; not respeter the particular interests of the participants can lead them to …,*,2007,2
Replicated data storage system and methods,*,When analyzing a large web log; for example; the web log may contain different fields like'visitDate'; 'adRevenue' and 'sourceIP' that may serve as filter conditions. In the existing HDFSand Hadoop MapReduce stack; this log file may be uploaded to HDFS using the HDFSclient. HDFS then partitions the file into logical blocks using a constant block size (the HDFSdefault is 64 MB). Each block is then physically stored three times (assuming the default replicationfactor of three). Each physical copy of a block is called a replica. Each replica will sit on a differentdata node. Therefore; at least two node failures may be tolerated by HDFS. Information on thedifferent replicas for an HDFS block is kept in a central name node directory … After uploadinghis log file to HDFS; one may run an actual Map Reduce job. Assuming a user is interested inall source IPs with a visit date from 2011; a map-only MapReduce program may be …,*,2015,1
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1
Sbqa: A self-adaptable query allocation process,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,We present a flexible query allocation framework; called Satisfaction-based Query Allocation(SbQA for short); for distributed information systems where both consumers and providers(the participants) have special interests towards queries. A particularity of SbQA is that itallocates queries while considering both query load and participants' interests. To be fair; itdynamically trades consumers' interests for providers' interests based on their satisfaction. Inthis demo we illustrate the flexibility and efficiency of SbQA to allocate queries on theBerkeley Open Infrastructure for Network Computing (BOINC). We also demonstrate thatSbQA is self-adaptable to the participants' expectations. Finally; we demonstrate that SbQAcan be adapted to different kinds of applications by varying its parameters.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,1
SbQA: Une Méthode Auto-Adaptative pour l'Allocation de Requêtes,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,We present a flexible query allocation framework; called {\it Satisfaction-based QueryAllocation}(SbQA for short); for distributed information systems where both consumers andproviders (the participants) have special interests towards queries. A particularity of SbQA isthat it allocates queries while considering both query load and participants' interests. To befair; it dynamically trades consumers' interests for providers' interests based on theirsatisfaction. In this demo we illustrate the flexibility and efficiency of SbQA to allocate querieson the {\it Berkeley Open Infrastructure for Network Computing}(BOINC). We alsodemonstrate that SbQA is self-adaptable to the participants' expectations. Finally; wedemonstrate that SbQA can be adapted to different kinds of applications by varying itsparameters.,Bases de Données Avancées (BDA),2008,1
Allocation de requêtes dans des systèmes d’information distribués avec des participants autonomes,Jorge-Arnulfo Quiané-Ruiz,Résumé Nous nous intéressons aux systèmes d'informations où les participants (clients etfournisseurs) sont autonomes; c. à. d. ils peuvent décider de quitter le système à n'importequel moment; et qu'ils ont des intérêts particuliers pour certaines requêtes. Dans cesenvironnements; l'allocation de requêtes est un défi particulier car les attentes desparticipants ne sont pas seulement liées aux performances du système. Dans ce contexte;l'insatisfaction des participants est un problème car elle peut les conduire à quitter lesystème. Par conséquent; il est très important de répondre aux attentes des participants desorte à ce qu'ils soient satisfaits. Dans cette thèse; nous abordons ce problème en apportantquatre contributions principales. Primo; nous fournissons un modèle pour caractériser laperception des participants par rapport au système et proposons des mesures qui …,*,2008,1
Synthesizing entity matching rules by examples,Rohit Singh; Venkata Vamsikrishna Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration. We study how to synthesizeentity matching rules from positive-negative matching examples. The core of our solution isprogram synthesis; a powerful tool to automatically generate rules (or programs) that satisfya given high-level specification; via a predefined grammar. This grammar describes aGeneral Boolean Formula (GBF) that can include arbitrary attribute matching predicatescombined by conjunctions (∧); disjunctions (∨) and negations (¬); and is expressiveenough to model EM problems; from capturing arbitrary attribute combinations to handlingmissing attribute values. The rules in the form of GBF are more concise than traditional EMrules represented in Disjunctive Normal Form (DNF). Consequently; they are moreinterpretable than decision trees and other machine learning algorithms that output deep …,Proceedings of the VLDB Endowment,2017,*
Errata for Lightning Fast and Space Efficient Inequality Joins (PVLDB 8 (13): 2074--2085),Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract This is in response to recent feedback from some readers; which requires someclarifications regarding our IEJ oin algorithm published in [1]. The feedback revolves aroundfour points:(1) a typo in our illustrating example of the join process;(2) a naming error for theindex used by our algorithm to improve the bit array scan;(3) the sort order used in ouralgorithms; and (4) a missing explanation on how duplicates are handled by our self joinalgorithm.,Proceedings of the VLDB Endowment,2017,*
Optimized inequality join method,*,The optimized inequality join method is a method for joining relational tables on inputinequality conditions. The optimized inequality join method is a relatively fast inequality joinmethod using permutation arrays to store positional information for sorted attributed values.Additionally; space efficient bit arrays are used to enable optimization; such as Bloom filterindices; thus providing faster computation of the join results. The method may be used; forexample; for joining various inequalities associated with a variety of measuredenvironmental conditions for raising an alarm when certain conditions are met.,*,2017,*
INGESTBASE: A Declarative Data Ingestion System,Alekh Jindal; Jorge-Arnulfo Quiane-Ruiz; Samuel Madden,Abstract: Big data applications have fast arriving data that must be quickly ingested. At thesame time; they have specific needs to preprocess and transform the data before it could beput to use. The current practice is to do these preparatory transformations once the data isalready ingested; however; this is expensive to run and cumbersome to manage. As a result;there is a need to push data preprocessing down to the ingestion itself. In this paper; wepresent a declarative data ingestion system; called INGESTBASE; to allow applicationdevelopers to plan and specify their data ingestion logic in a more systematic manner. Weintroduce the notion of ingestions plans; analogous to query plans; and present adeclarative ingestion language to help developers easily build sophisticated ingestionplans. INGESTBASE provides an extensible ingestion optimizer to rewrite and optimize …,arXiv preprint arXiv:1701.06093,2017,*
CYBER SECURITY PART 2 AUTO-IMMUNITY,A Elmagarmid; P Cochrane; M Ouzzani; WJ Al Marri; Q Malluhi; M Tang; WG Aref; Z Abedjan; X Chu; D Deng; RC Fernandez; IF Ilyas; P Papotti; M Stonebraker; N Tang; Z Khayyat; W Lucia; J Quiane-Ruiz; T Nan; Y Yu; QM Malluhi; TK Saha; HM Hammady; AK Elmagarmid; H Hammady; Z Fedorowicz; M Yakout; H Elmeleegy; Y Qi,*,US Patent App,2016,*
Data Science at QCRI,Divy Agrawal; Laure Berti; Hossam Hammady; Prasenjit Mitra; Mourad Ouzzani; Paolo Papotti; Jorge Quiane Ruiz; Nan Tang; Yin Ye; Si Yin; Mohamed Zaki," The Data Analytics group at QCRI has embarked on an ambitious endeavor to become apremiere world-class research group in Data Science by tackling diverse research topicsrelated to information extraction; data quality; data profiling; data integration; and datamining. We will present our ongoing projects to overcome different challenges encounteredin Big Data Curation; Big Data Fusion; and Big Data Analytics.(1) Big Data Curation: Due tocomplex processing and transformation layers; data errors proliferate rapidly and sometimesin an uncontrolled manner; thus compromising the value of information and impacting dataanalysis and decision making. While data quality problems can have crippling effects and noend-to-end off-the-shelf solutions to (semi-) automate error detection and correction existed;we built a commodity platform; NADEEF that can be easily customized and deployed to …,Qatar Foundation Annual Research Conference,2014,*
In large-scale Internet-based distributed systems; participants (consumers and providers) are typically autonomous; ie they may have special interests towards queri...,Jorge-Arnulfo Quiané-Ruiz; Philippe Lamarre; Patrick Valduriez,Many applications of wireless sensor networks monitor the physical world and report eventsof interest. To facilitate event detection in these applications; in this paper we propose apattern-based event detection approach and integrate the approach into an in-networksensor query processing framework. Different from existing threshold-based event detection;we abstract events into patterns in sensory...,Distributed and Parallel Databases,2012,*
Scheduling Strategies in a Main-Memory MapReduce Framework; Approach for countering Reduce side skew,Mahendiran Venkatachalapathy; Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Zusammenfassung Over the past few decades; there is a multifold increase in the amount ofdigital data that is being generated. Various attempts are being made to process this vastamount of data in a fast and efficient manner. Hadoop-MapReduce is one such softwareframework that has gained popularity in the last few years. It provides a reliable and easierway to process huge amount of data in-parallel on large computing cluster. However;Hadoop always persists intermediate results to the local disk. As a result; Hadoop usuallysuffers from long execution runtimes as it typically pays a high I/O cost for running jobs. Thestate-of-the-art computing clusters have enough main memory capacity to hold terabytes ofdata in main memory. We have built M3R (Main Memory MapReduce) framework; aprototype for generic main memory-based data processing. M3R can execute …,*,2012,*
To Replicate or Not To Replicate Queries in the Presence of Autonomous Participants?,Quiané-Ruiz Jorge; Philippe Lamarre; Patrick Valduriez,In summary; the main contributions of this paper are as follows. 3 We formalize the queryallocation problem and make precise query replication in the presence of autonomousparticipants (Section II). We introduce a global satisfaction notion to characterize the fact that(i) queries have different criticality for consumers;(ii) a consumer may receive less resultsthan it expects; and (iii) a provider may perform queries for nothing (Section IV). We proposetwo automatic query replication algorithms; SbQR and SbQR+; that consider globalsatisfaction as the basis of their functionality to decide on-the-fly (i) which queries should bereplicated and (ii) how many query replicas should be created (Section V). Weexperimentally demonstrate that SbQR:(i) significantly outperforms popular baselinealgorithms and (ii) automatically adapts to the workload and the criticality of queries …,BDA: Bases de Données Avancées,2011,*
Query Replication in Distributed Information Systems with Autonomous Participants,Philippe Lamarre; Jorge-Arnulfo Quiané-Ruiz; Patrick Valduriez,We consider Distributed Information Systems with Autonomous Participants (DISAP); ie;participants (consumers and providers) may have special interests towards queries andother participants. Recent applications of DISAP on the Internet have emerged to share data;services; or computing resources at an unprecedented scale (eg SETI@ home). Withautonomous participants; the only way to avoid a participant to voluntarily leave the systemis to satisfy its interests when allocating queries. But; participants' satisfaction may also bebadly affected by other participants' failures or comportment. In this context; replicatingqueries is useful to address two different problems: tolerate providers' failures and deal withByzantine providers. In this paper; we make the following main contributions. First; weformalize the query allocation problem over faulty participants in the context of DISAP …,*,2009,*
A satisfaction-based query allocation framework for distributed information systems,Jorge-Arnulfo Quiane-Ruiz,In large-scale distributed information systems; where participants (consumers and providers)are autonomous and have special interests for some queries; query allocation is achallenge. Much work in this context has focused on distributing queries among providers ina way that maximizes overall performance (typically throughput and response time).However; participants usually have certain expectations with respect to the mediator; whichare not only performance-related. Such expectations mainly reflect their interests to allocateand perform queries; eg their interests towards: providers (based on reputation for example);quality of service; topics of interests; and relationships with other participants. In this context;because of participants' autonomy; dissatisfaction is a problem since it may lead participantsto leave the mediator. Participant's satisfaction means that the query allocation method …,*,2008,*
Cooperative Information Systems (CoopIS) 2006 International Conference-Distributed Information Systems I-Satisfaction-Based Query Load Balancing,Jorge-Arnulfo Lamarre; Philippe Quiane-Ruiz; Patrick Valduriez,*,Lecture Notes in Computer Science,2006,*
Benjamin Djahandideh; François Goasdoué; Zoi Kaoudi; Ioana Manolescu; Jorge Quiané-Ruiz; Stamatis Zampetakis,Benjamin Djahandideh,RESUM E RDF est un modele de donnéesa la popularité croissante dans les applicationsréelles; menant ainsia la création et l'exploitation de large volumes de données RDF. Desméthodes efficaces de gestion de données RDF sont cruciales pour permettre le passageal'échelle des applications. Dans cette démonstration; nous mettons en avant la grandeefficacité de CliqueSquare; un systeme de gestion de données RDF conçu au-dessus d'uneinfrastructurea la MapReduce. Nous montrons trois principaux aspects de CliqueSquare:(i)la réduction significative du traffic réseau pendant l'évaluation de requêtes;(ii) l'évaluationde requêtes potentiellement grandes en peu de jobs MapReduce et (iii) l'amélioration desperformances de traitement des requêtes par la production de plans de type DAG. Danstous les scénarios de démonstration; l'audience est invitéea interagir avec le systeme …,*,*,*
Special section: Security; trust and privacy in Grid systems Guest Editors: Alvaro E. Arenas and Philippe Massonet,J Secretan; M Georgiopoulos; A Koufakou; K Cardona; P Morillo; S Rueda; JM Orduña; J Duato; GA Vouros; A Papasalouros; K Tzonas; A Valarakos; K Kotis; JA Quiané-Ruiz; P Lamarre; P Valduriez; V Méndez Muñoz; G Amorós Vicente; F García Carballeira; J Salt Cairols; A Goscinski; M Brock; M Abdullah; M Othman; H Ibrahim; S Subramaniam; C Vázquez; E Huedo; RS Montero; IM Llorente; K Wang; J Li; L Pan; K Beghdad Bey; F Benhammadi; A Mokhtari; Z Gessoum; G Rodríguez; XC Pardo; MJ Martín; P González,*,*,*,*
Data Engineering,Jens Dittrich; Marcos Antonio Vaz Salles; Lukas Blunschi; Ziyang Liu; Peng Sun; Yu Huang; Yichuan Cai; Yi Chen; Eugene Agichtein; Evgeniy Gabrilovich; Hongyuan Zha,Abstract The Starfish project at Duke University aims to provide MapReduce users andapplications with good performance automatically; without any need on their part tounderstand and manipulate the numerous tuning knobs in a MapReduce system. This paperdescribes the What-if Engine; an indispensable component in Starfish; which serves asimilar purpose as a costing engine used by the query optimizer in a Database system. Wediscuss the problem and challenges addressed by the What-if Engine. We also discuss thetechniques used by the What-if Engine and the design decisions that led us to thesetechniques.,*,*,*
