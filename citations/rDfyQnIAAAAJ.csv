Imagenet: A large-scale hierarchical image database,Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei,The explosion of image data on the Internet has the potential to foster more sophisticatedand robust models and algorithms to index; retrieve; organize and interact with images andmultimedia data. But exactly how such data can be harnessed and organized remains acritical problem. We introduce here a new database called “ImageNet”; a large-scaleontology of images built upon the backbone of the WordNet structure. ImageNet aims topopulate the majority of the 80;000 synsets of WordNet with an average of 500-1000 cleanand full resolution images. This will result in tens of millions of annotated images organizedby the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet inits current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show thatImageNet is much larger in scale and diversity and much more accurate than the current …,Computer Vision and Pattern Recognition; 2009. CVPR 2009. IEEE Conference on,2009,5985
Imagenet large scale visual recognition challenge,Olga Russakovsky; Jia Deng; Hao Su; Jonathan Krause; Sanjeev Satheesh; Sean Ma; Zhiheng Huang; Andrej Karpathy; Aditya Khosla; Michael Bernstein; Alexander C Berg; Li Fei-Fei,Abstract The ImageNet Large Scale Visual Recognition Challenge is a benchmark in objectcategory classification and detection on hundreds of object categories and millions ofimages. The challenge has been run annually from 2010 to present; attracting participationfrom more than fifty institutions. This paper describes the creation of this benchmark datasetand the advances in object recognition that have been possible as a result. We discuss thechallenges of collecting large-scale ground truth annotation; highlight key breakthroughs incategorical object recognition; provide a detailed analysis of the current state of the field oflarge-scale image classification and object detection; and compare the state-of-the-artcomputer vision accuracy with human accuracy. We conclude with lessons learned in the 5years of the challenge; and propose future directions and improvements.,International Journal of Computer Vision,2015,4833
A bayesian hierarchical model for learning natural scene categories,Li Fei-Fei; Pietro Perona,We propose a novel approach to learn and recognize natural scene categories. Unlikeprevious work; it does not require experts to annotate the training set. We represent theimage of a scene by a collection of local regions; denoted as codewords obtained byunsupervised learning. Each region is represented as part of a" theme". In previous work;such themes were learnt from hand-annotations of experts; while our method learns thetheme distributions as well as the codewords distribution over the themes withoutsupervision. We report satisfactory categorization performances on a large set of 13categories of complex scenes.,Computer Vision and Pattern Recognition; 2005. CVPR 2005. IEEE Computer Society Conference on,2005,3650
Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,Li Fei-Fei; Rob Fergus; Pietro Perona,Abstract Current computational approaches to learning visual object categories requirethousands of training images; are slow; cannot learn in an incremental manner and cannotincorporate prior information into the learning process. In addition; no algorithm presented inthe literature has been tested on more than a handful of object categories. We present anmethod for learning object categories from just a few training images. It is quick and it usesprior information in a principled way. We test it on a dataset composed of images of objectsbelonging to 101 widely varied categories. Our proposed method is based on making use ofprior information; assembled from (unrelated) object categories which were previouslylearnt. A generative probabilistic model is used; which represents the shape andappearance of a constellation of features belonging to the object. The parameters of the …,Computer vision and Image understanding,2007,3031
Unsupervised learning of human action categories using spatial-temporal words,Juan Carlos Niebles; Hongcheng Wang; Li Fei-Fei,Abstract We present a novel unsupervised learning method for human action categories. Avideo sequence is represented as a collection of spatial-temporal words by extracting space-time interest points. The algorithm automatically learns the probability distributions of thespatial-temporal words and the intermediate topics corresponding to human actioncategories. This is achieved by using latent topic models such as the probabilistic LatentSemantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach canhandle noisy feature points arisen from dynamic background and moving cameras due tothe application of the probabilistic models. Given a novel video sequence; the algorithm cancategorize and localize the human action (s) contained in the video. We test our algorithmon three challenging datasets: the KTH human motion dataset; the Weizmann human …,International Journal of Computer Vision,2008,1813
Large-scale video classification with convolutional neural networks,Andrej Karpathy; George Toderici; Sanketh Shetty; Thomas Leung; Rahul Sukthankar; Li Fei-Fei,Abstract Convolutional Neural Networks (CNNs) have been established as a powerful classof models for image recognition problems. Encouraged by these results; we provide anextensive empirical evaluation of CNNs on largescale video classification using a newdataset of 1 million YouTube videos belonging to 487 classes. We study multipleapproaches for extending the connectivity of a CNN in time domain to take advantage oflocal spatio-temporal information and suggest a multiresolution; foveated architecture as apromising way of speeding up the training. Our best spatio-temporal networks displaysignificant performance improvements compared to strong feature-based baselines (55.3%to 63.9%); but only a surprisingly modest improvement compared to single-frame models(59.3% to 60.9%). We further study the generalization performance of our best model by …,Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,2014,1783
Deep visual-semantic alignments for generating image descriptions,Andrej Karpathy; Li Fei-Fei,Abstract We present a model that generates natural language descriptions of images andtheir regions. Our approach leverages datasets of images and their sentence descriptions tolearn about the inter-modal correspondences between language and visual data. Ouralignment model is based on a novel combination of Convolutional Neural Networks overimage regions; bidirectional Recurrent Neural Networks over sentences; and a structuredobjective that aligns the two modalities through a multimodal embedding. We then describea Multimodal Recurrent Neural Network architecture that uses the inferred alignments tolearn to generate novel descriptions of image regions. We demonstrate that our alignmentmodel produces state of the art results in retrieval experiments on Flickr8K; Flickr30K andMSCOCO datasets. We then show that the generated descriptions significantly …,Proceedings of the IEEE conference on computer vision and pattern recognition,2015,1347
One-shot learning of object categories,Li Fei-Fei; Rob Fergus; Pietro Perona,Learning visual models of object categories notoriously requires hundreds or thousands oftraining examples. We show that it is possible to learn much information about a categoryfrom just one; or a handful; of images. The key insight is that; rather than learning fromscratch; one can take advantage of knowledge coming from previously learned categories;no matter how different these categories might be. We explore a Bayesian implementation ofthis idea. Object categories are represented by probabilistic models. Prior knowledge isrepresented as a probability density function on the parameters of these models. Theposterior model for an object category is obtained by updating the prior in the light of one ormore observations. We test a simple implementation of our algorithm on a database of 101diverse object categories. We compare category models learned by an implementation of …,IEEE transactions on pattern analysis and machine intelligence,2006,989
Object bank: A high-level image representation for scene classification and semantic feature sparsification,Li-Jia Li; Hao Su; Eric P Xing; Li Fei-Fei,Abstract Robust low-level image features have been proven to be effective representationsfor a variety of visual recognition tasks such as object recognition and scene classification;but pixels; or even local image patches; carry little semantic meanings. For high level visualtasks; such low-level image representations are potentially not enough. In this paper; wepropose a high-level image representation; called the Object Bank; where an image isrepresented as a scale invariant response map of a large number of pre-trained genericobject detectors; blind to the testing dataset or visual task. Leveraging on the Object Bankrepresentation; superior performances on high level visual recognition tasks can beachieved with simple off-the-shelf classifiers such as logistic regression and linear SVM.Sparsity algorithms make our representation more efficient and scalable for large scene …,Advances in Neural Information Processing Systems,2010,838
Learning object categories from Google's image search,Robert Fergus; Li Fei-Fei; Pietro Perona; Andrew Zisserman,Current approaches to object category recognition require datasets of training images to bemanually prepared; with varying degrees of supervision. We present an approach that canlearn an object category from just its name; by utilizing the raw output of image searchengines available on the Internet. We develop a new model; TSI-pLSA; which extends pLSA(as applied to visual words) to include spatial information in a translation and scale invariantmanner. Our approach can handle the high intra-class variability and large proportion ofunrelated images returned by search engines. We evaluate tire models on standard testsets; showing performance competitive with existing methods trained on hand prepareddatasets,Computer Vision; 2005. ICCV 2005. Tenth IEEE International Conference on,2005,826
Learning Object Categories from Googles Image Search. Computer Vision; 2005. ICCV 2005,R Fergus; L Fei-Fei; P Perona; A Zisserman,*,Tenth IEEE International Conference on,2005,826
What; where and who? classifying events by scene and object recognition,Li-Jia Li; Li Fei-Fei,We propose a first attempt to classify events in static images by integrating scene and objectcategorizations. We define an event in a static image as a human activity taking place in aspecific environment. In this paper; we use a number of sport games such as snow boarding;rock climbing or badminton to demonstrate event classification. Our goal is to classify theevent in the image as well as to provide a number of semantic labels to the objects andscene environment within the image. For example; given a rowing scene; our algorithmrecognizes the event as rowing by classifying the environment as a lake and recognizing thecritical objects in the image as athletes; rowing boat; water; etc. We achieve this integrativeand holistic recognition through a generative graphical model. We have assembled a highlychallenging database of 8 widely varied sport events. We show that our system is capable …,Computer Vision; 2007. ICCV 2007. IEEE 11th International Conference on,2007,708
Rapid natural scene categorization in the near absence of attention,Fei Fei Li; Rufin VanRullen; Christof Koch; Pietro Perona,Abstract What can we see when we do not pay attention? It is well known that we can be“blind” even to major aspects of natural scenes when we attend elsewhere. The only tasksthat do not need attention appear to be carried out in the early stages of the visual system.Contrary to this common belief; we report that subjects can rapidly detect animals or vehiclesin briefly presented novel natural scenes while simultaneously performing anotherattentionally demanding task. By comparison; they are unable to discriminate large T's fromL's; or bisected two-color disks from their mirror images under the same conditions. Weconclude that some visual tasks associated with “high-level” cortical areas may proceed inthe near absence of attention.,Proceedings of the National Academy of Sciences,2002,686
Modeling temporal structure of decomposable motion segments for activity classification,Juan Niebles; Chih-Wei Chen; Li Fei-Fei,Abstract Much recent research in human activity recognition has focused on the problem ofrecognizing simple repetitive (walking; running; waving) and punctual actions (sitting up;opening a door; hugging). However; many interesting human activities are characterized bya complex temporal composition of simple actions. Automatic recognition of such complexactions can benefit from a good understanding of the temporal structures. We present in thispaper a framework for modeling motion by exploiting the temporal structure of the humanactivities. In our framework; we represent activities as temporal compositions of motionsegments. We train a discriminative model that encodes a temporal decomposition of videosequences; and appearance models for each motion segment. In recognition; a query videois matched to the model according to the learned appearances and motion segment …,Computer Vision–ECCV 2010,2010,583
Simultaneous image classification and annotation,Wang Chong; David Blei; Fei-Fei Li,Image classification and annotation are important problems in computer vision; but rarelyconsidered together. Intuitively; annotations provide evidence for the class label; and theclass label provides evidence for annotations. For example; an image of class highway ismore likely annotated with words “road;”“car;” and “traffic” than words “fish;”“boat;” and“scuba.” In this paper; we develop a new probabilistic model for jointly modeling the image;its class label; and its annotations. Our model treats the class label as a global description ofthe image; and treats annotation terms as local descriptions of parts of the image. Itsunderlying probabilistic assumptions naturally integrate these two sources of information.We derive an approximate inference and estimation algorithms based on variationalmethods; as well as efficient approximations for classifying and annotating new images …,Computer Vision and Pattern Recognition; 2009. CVPR 2009. IEEE Conference on,2009,538
Towards total scene understanding: Classification; annotation and segmentation in an automatic framework,Li-Jia Li; Richard Socher; Li Fei-Fei,Given an image; we propose a hierarchical generative model that classifies the overallscene; recognizes and segments each object component; as well as annotates the imagewith a list of tags. To our knowledge; this is the first model that performs all three tasks in onecoherent framework. For instance; a scene of apolo game'consists of several visual objectssuch ashuman';horse';grass'; etc. In addition; it can be further annotated with a list of moreabstract (egdusk') or visually less salient (egsaddle') tags. Our generative model jointlyexplains images through a visual model and a textual model. Visually relevant objects arerepresented by regions and patches; while visually irrelevant textual annotations areinfluenced directly by the overall scene class. We propose a fully automatic learningframework that is able to learn robust scene models from noisy Web data such as images …,Computer Vision and Pattern Recognition; 2009. CVPR 2009. IEEE Conference on,2009,480
A hierarchical model of shape and appearance for human action classification,Juan Carlos Niebles; Li Fei-Fei,We present a novel model for human action categorization. A video sequence isrepresented as a collection of spatial and spatial-temporal features by extracting static anddynamic interest points. We propose a hierarchical model that can be characterized as aconstellation of bags-of-features and that is able to combine both spatial and spatial-temporal features. Given a novel video sequence; the model is able to categorize humanactions in a frame-by-frame basis. We test the model on a publicly available human actiondataset [2] and show that our new method performs well on the classification task. We alsoconducted control experiments to show that the use of the proposed mixture of hierarchicalmodels improves the classification performance over bag of feature models. An additionalexperiment shows that using both dynamic and static features provides a richer …,Computer Vision and Pattern Recognition; 2007. CVPR'07. IEEE Conference on,2007,468
A Bayesian approach to unsupervised one-shot learning of object categories,Li Fe-Fei; Rob Fergus; Pietro Perona,Learning visual models of object categories notoriously requires thousands of trainingexamples; this is due to the diversity and richness of object appearance which requiresmodels containing hundreds of parameters. We present a method for learning objectcategories from just a few images (1/spl sim/5). It is based on incorporating" generic"knowledge which may be obtained from previously learnt models of unrelated categories.We operate in a variational Bayesian framework: object categories are represented byprobabilistic models; and" prior" knowledge is represented as a probability density functionon the parameters of these models. The" posterior" model for an object category is obtainedby updating the prior in the light of one or more observations. Our ideas are demonstratedon four diverse categories (human faces; airplanes; motorcycles; spotted cats). Initially …,Computer Vision; 2003. Proceedings. Ninth IEEE International Conference on,2003,435
What does classifying more than 10;000 image categories tell us?,Jia Deng; Alexander Berg; Kai Li; Li Fei-Fei,Abstract Image classification is a critical task for both humans and computers. One of thechallenges lies in the large scale of the semantic space. In particular; humans can recognizetens of thousands of object classes and scenes. No computer vision algorithm today hasbeen tested at this scale. This paper presents a study of large scale categorization includinga series of challenging experiments on classification with more than 10;000 image classes.We find that a) computational issues become crucial in algorithm design; b) conventionalwisdom from a couple of hundred image categories on relative performance of differentclassifiers does not necessarily hold when the number of categories increases; c) there is asurprisingly strong relationship between the structure of WordNet (developed for studyinglanguage) and the difficulty of visual categorization; d) classification can be improved by …,Computer Vision–ECCV 2010,2010,426
Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes,Liangliang Cao; Li Fei-Fei,We present a novel generative model for simultaneously recognizing and segmenting objectand scene classes. Our model is inspired by the traditional bag of words representation oftexts and images as well as a number of related generative models; including probabilisticlatent semantic analysis (pLSA) and latent Dirichlet allocation (LDA). A major drawback ofthe pLSA and LDA models is the assumption that each patch in the image is independentlygenerated given its corresponding latent topic. While such representation provides anefficient computational method; it lacks the power to describe the visually coherent imagesand scenes. Instead; we propose a spatially coherent latent topic model (spatial-LTM).Spatial-LTM represents an image containing objects in a hierarchical way by over-segmented image regions of homogeneous appearances and the salient image patches …,Computer Vision; 2007. ICCV 2007. IEEE 11th International Conference on,2007,421
Modeling mutual context of object and human pose in human-object interaction activities,Bangpeng Yao; Li Fei-Fei,Detecting objects in cluttered scenes and estimating articulated human body parts are twochallenging problems in computer vision. The difficulty is particularly pronounced inactivities involving human-object interactions (eg playing tennis); where the relevant objecttends to be small or only partially visible; and the human body parts are often self-occluded.We observe; however; that objects and human poses can serve as mutual context to eachother–recognizing one facilitates the recognition of the other. In this paper we propose anew random field model to encode the mutual context of objects and human poses in human-object interaction activities. We then cast the model learning task as a structure learningproblem; of which the structural connectivity between the object; the overall human pose;and different body parts are estimated through a structure search approach; and the …,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,418
Perceptual losses for real-time style transfer and super-resolution,Justin Johnson; Alexandre Alahi; Li Fei-Fei,Abstract We consider image transformation problems; where an input image is transformedinto an output image. Recent methods for such problems typically train feed-forwardconvolutional neural networks using a per-pixel loss between the output and ground-truthimages. Parallel work has shown that high-quality images can be generated by defining andoptimizing perceptual loss functions based on high-level features extracted from pretrainednetworks. We combine the benefits of both approaches; and propose the use of perceptualloss functions for training feed-forward networks for image transformation tasks. We showresults on image style transfer; where a feed-forward network is trained to solve theoptimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method; our network gives similar qualitative results but is three orders of …,European Conference on Computer Vision,2016,407
3D generic object categorization; localization and pose estimation,Silvio Savarese; Li Fei-Fei,We propose a novel and robust model to represent and learn generic 3D object categories.We aim to solve the problem of true 3D object categorization for handling arbitrary rotationsand scale changes. Our approach is to capture a compact model of an object category bylinking together diagnostic parts of the objects from different viewing points. We emphasizeon the fact that our" parts" are large and discriminative regions of the objects that arecomposed of many local invariant features. Instead of recovering a full 3D geometry; weconnect these parts through their mutual homographic transformation. The resulting model isa compact summarization of both the appearance and geometry information of the objectclass. We propose a framework in which learning is done via minimal supervision comparedto previous works. Our results on categorization show superior performances to state-of …,Computer Vision; 2007. ICCV 2007. IEEE 11th International Conference on,2007,405
Optimol: automatic online picture collection via incremental model learning,Li-Jia Li; Li Fei-Fei,Abstract The explosion of the Internet provides us with a tremendous resource of imagesshared online. It also confronts vision researchers the problem of finding effective methodsto navigate the vast amount of visual information. Semantic image understanding plays avital role towards solving this problem. One important task in image understanding is objectrecognition; in particular; generic object categorization. Critical to this problem are the issuesof learning and dataset. Abundant data helps to train a robust recognition system; while agood object classifier can help to collect a large amount of images. This paper presents anovel object recognition algorithm that performs automatic dataset collecting andincremental model learning simultaneously. The goal of this work is to use the tremendousresources of the web to learn robust object category models for detecting and searching …,International journal of computer vision,2010,387
Human action recognition by learning bases of action attributes and parts,Bangpeng Yao; Xiaoye Jiang; Aditya Khosla; Andy Lai Lin; Leonidas Guibas; Li Fei-Fei,In this work; we propose to use attributes and parts for recognizing human actions in stillimages. We define action attributes as the verbs that describe the properties of humanactions; while the parts of actions are objects and poselets that are closely related to theactions. We jointly model the attributes and parts by learning a set of sparse bases that areshown to carry much semantic meaning. Then; the attributes and parts of an action imagecan be reconstructed from sparse coefficients with respect to the learned bases. This dualsparsity provides theoretical guarantee of our bases learning and feature reconstructionapproach. On the PASCAL action dataset and a new “Stanford 40 Actions” dataset; we showthat our method extracts meaningful high-order interactions between attributes and parts inhuman actions while achieving state-of-the-art classification performance.,Computer Vision (ICCV); 2011 IEEE International Conference on,2011,349
What do we perceive in a glance of a real-world scene?,Li Fei-Fei; Asha Iyer; Christof Koch; Pietro Perona,Abstract What do we see when we glance at a natural scene and how does it change as theglance becomes longer? We asked naive subjects to report in a free-form format what theysaw when looking at briefly presented real-life photographs. Our subjects received nospecific information as to the content of each stimulus. Thus; our paradigm differs fromprevious studies where subjects were cued before a picture was presented and/or wereprobed with multiple-choice questions. In the first stage; 90 novel grayscale photographswere foveally shown to a group of 22 native-English-speaking subjects. The presentationtime was chosen at random from a set of seven possible times (from 27 to 500 ms). Aperceptual mask followed each photograph immediately. After each presentation; subjectsreported what they had just seen as completely and truthfully as possible. In the second …,Journal of vision,2007,340
Learning latent temporal structure for complex event detection,Kevin Tang; Li Fei-Fei; Daphne Koller,In this paper; we tackle the problem of understanding the temporal structure of complexevents in highly varying videos obtained from the Internet. Towards this goal; we utilize aconditional model trained in a max-margin framework that is able to automatically discoverdiscriminative and interesting segments of video; while simultaneously achievingcompetitive accuracies on difficult detection and recognition tasks. We introduce latentvariables over the frames of a video; and allow our algorithm to discover and assignsequences of states that are most discriminative for the event. Our model is based on thevariable-duration hidden Markov model; and models durations of states in addition to thetransitions between states. The simplicity of our model allows us to perform fast; exactinference using dynamic programming; which is extremely important when we set our …,Computer Vision and Pattern Recognition (CVPR); 2012 IEEE Conference on,2012,317
Deep fragment embeddings for bidirectional image sentence mapping,Andrej Karpathy; Armand Joulin; Li Fei-Fei,Abstract We introduce a model for bidirectional retrieval of images and sentences through adeep; multi-modal embedding of visual and natural language data. Unlike previous modelsthat directly map images or sentences into a common embedding space; our model workson a finer level and embeds fragments of images (objects) and fragments of sentences(typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments acrossmodalities. Extensive experimental evaluation shows that reasoning on both the global levelof images and sentences and the finer level of their respective fragments improvesperformance on image-sentence retrieval tasks. Additionally; our model providesinterpretable predictions for the image-sentence retrieval task since the inferred inter …,Advances in Neural Information Processing Systems,2014,279
Visualizing and understanding recurrent networks,Andrej Karpathy; Justin Johnson; Li Fei-Fei,Abstract: Recurrent Neural Networks (RNNs); and specifically a variant with Long Short-Term Memory (LSTM); are enjoying renewed interest as a result of successful applications ina wide range of machine learning problems that involve sequential data. However; whileLSTMs provide exceptional results in practice; the source of their performance and theirlimitations remain rather poorly understood. Using character-level language models as aninterpretable testbed; we aim to bridge this gap by providing an analysis of theirrepresentations; predictions and error types. In particular; our experiments reveal theexistence of interpretable cells that keep track of long-range dependencies such as linelengths; quotes and brackets. Moreover; our comparative analysis with finite horizon n-grammodels traces the source of the LSTM improvements to long-range structural …,arXiv preprint arXiv:1506.02078,2015,265
Grouplet: a structured image representation for recognizing human and object interactions,Bangpeng Yao; Li Fei-Fei,Psychologists have proposed that many human-object interaction activities form uniqueclasses of scenes. Recognizing these scenes is important for many social functions. Toenable a computer to do this is however a challenging task. Take people-playing-musical-instrument (PPMI) as an example; to distinguish a person playing violin from a person justholding a violin requires subtle distinction of characteristic image features and featurearrangements that differentiate these two scenes. Most of the existing image representationmethods are either too coarse (eg BoW) or too sparse (eg constellation models) forperforming this task. In this paper; we propose a new image feature representation called“grouplet”. The grouplet captures the structured information of an image by encoding anumber of discriminative visual features and their spatial configurations. Using a dataset …,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,261
Combining randomization and discrimination for fine-grained image categorization,Bangpeng Yao; Aditya Khosla; Li Fei-Fei,In this paper; we study the problem of fine-grained image categorization. The goal of ourmethod is to explore fine image statistics and identify the discriminative image patches forrecognition. We achieve this goal by combining two ideas; discriminative feature mining andrandomization. Discriminative feature mining allows us to model the detailed information thatdistinguishes different classes of images; while randomization allows us to handle the hugefeature space and prevents over-fitting. We propose a random forest with discriminativedecision trees algorithm; where every tree node is a discriminative classifier that is trainedby combining the information in this node as well as all upstream nodes. Our method istested on both subordinate categorization and activity recognition datasets. Experimentalresults show that our method identifies semantically meaningful visual information and …,Computer Vision and Pattern Recognition (CVPR); 2011 IEEE Conference on,2011,258
Natural scene categories revealed in distributed patterns of activity in the human brain,Dirk B Walther; Eamon Caddigan; Li Fei-Fei; Diane M Beck,Human subjects are extremely efficient at categorizing natural scenes; despite the fact thatdifferent classes of natural scenes often share similar image statistics. Thus far; however; it isunknown where and how complex natural scene categories are encoded and discriminatedin the brain. We used functional magnetic resonance imaging (fMRI) and distributed patternanalysis to ask what regions of the brain can differentiate natural scene categories (such asforests vs mountains vs beaches). Using completely different exemplars of six natural scenecategories for training and testing ensured that the classification algorithm was learningpatterns associated with the category in general and not specific exemplars. We found thatarea V1; the parahippocampal place area (PPA); retrosplenial cortex (RSC); and lateraloccipital complex (LOC) all contain information that distinguishes among natural scene …,Journal of neuroscience,2009,245
Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus,Garrett B Stanley; Fei F Li; Yang Dan,A major challenge in studying sensory processing is to understand the meaning of theneural messages encoded in the spiking activity of neurons. From the recorded responses ina sensory circuit; what information can we extract about the outside world? Here we used alinear decoding technique to reconstruct spatiotemporal visual inputs from ensembleresponses in the lateral geniculate nucleus (LGN) of the cat. From the activity of 177 cells;we have reconstructed natural scenes with recognizable moving objects. The quality ofreconstruction depends on the number of cells. For each point in space; the quality ofreconstruction begins to saturate at six to eight pairs of on and off cells; approaching theestimated coverage factor in the LGN of the cat. Thus; complex visual inputs can bereconstructed with a simple decoding algorithm; and these analyses provide a basis for …,Journal of Neuroscience,1999,241
Densecap: Fully convolutional localization networks for dense captioning,Justin Johnson; Andrej Karpathy; Li Fei-Fei,Abstract We introduce the dense captioning task; which requires a computer vision system toboth localize and describe salient regions in images in natural language. The densecaptioning task generalizes object detection when the descriptions consist of a single word;and Image Captioning when one predicted region covers the full image. To address thelocalization and description task jointly we propose a Fully Convolutional LocalizationNetwork (FCLN) architecture that processes an image with a single; efficient forward pass;requires no external regions proposals; and can be trained end-to-end with a single round ofoptimization. The architecture is composed of a Convolutional Network; a novel denselocalization layer; and Recurrent Neural Network language model that generates the labelsequences. We evaluate our network on the Visual Genome dataset; which comprises …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2016,240
Visual genome: Connecting language and vision using crowdsourced dense image annotations,Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma; Michael S Bernstein; Li Fei-Fei,Abstract Despite progress in perceptual tasks such as image classification; computers stillperform poorly on cognitive tasks such as image description and question answering.Cognition is core to tasks that involve not just recognizing; but reasoning about our visualworld. However; models used to tackle the rich content in images for cognitive tasks are stillbeing trained using the same datasets designed for perceptual tasks. To achieve success atcognitive tasks; models need to understand the interactions and relationships betweenobjects in an image. When asked “What vehicle is the person riding?”; computers will needto identify the objects in an image as well as the relationships riding (man; carriage) andpulling (horse; carriage) to answer correctly that “the person is riding a horse-drawncarriage.” In this paper; we present the Visual Genome dataset to enable the modeling of …,International Journal of Computer Vision,2017,236
Distributed cosegmentation via submodular optimization on anisotropic diffusion,Gunhee Kim; Eric P Xing; Li Fei-Fei; Takeo Kanade,The saliency of regions or objects in an image can be significantly boosted if they recur inmultiple images. Leveraging this idea; cosegmentation jointly segments common regionsfrom multiple images. In this paper; we propose CoSand; a distributed cosegmentationapproach for a highly variable large-scale image collection. The segmentation task ismodeled by temperature maximization on anisotropic heat diffusion; of which thetemperature maximization with finite K heat sources corresponds to a K-way segmentationthat maximizes the segmentation confidence of every pixel in an image. We show that ourmethod takes advantage of a strong theoretic property in that the temperature under linearanisotropic diffusion is a submodular function; therefore; a greedy algorithm guarantees atleast a constant factor approximation to the optimal solution for temperature maximization …,Computer Vision (ICCV); 2011 IEEE International Conference on,2011,232
Neural mechanisms of rapid natural scene categorization in human visual cortex,Marius V Peelen; Li Fei-Fei; Sabine Kastner,Abstract The visual system has an extraordinary capability to extract categorical informationfrom complex natural scenes. For example; subjects are able to rapidly detect the presenceof object categories such as animals or vehicles in new scenes that are presented verybriefly 1; 2. This is even true when subjects do not pay attention to the scenes andsimultaneously perform an unrelated attentionally demanding task 3; a stark contrast to thecapacity limitations predicted by most theories of visual attention 4; 5. Here we show aneural basis for rapid natural scene categorization in the visual cortex; using functionalmagnetic resonance imaging and an object categorization task in which subjects detectedthe presence of people or cars in briefly presented natural scenes. The multi-voxel pattern ofneural activity in the object-selective cortex evoked by the natural scenes contained …,Nature,2009,231
Spatial-temporal correlatons for unsupervised action classification,Silvio Savarese; Andrey DelPozo; Juan Carlos Niebles; Li Fei-Fei,Spatial-temporal local motion features have shown promising results in complex humanaction classification. Most of the previous works [6];[16];[21] treat these spatial-temporalfeatures as a bag of video words; omitting any long range; global information in either thespatial or temporal domain. Other ways of learning temporal signature of motion tend toimpose a fixed trajectory of the features or parts of human body returned by trackingalgorithms. This leaves little flexibility for the algorithm to learn the optimal temporal patterndescribing these motions. In this paper; we propose the usage of spatial-temporalcorrelograms to encode flexible long range temporal information into the spatial-temporalmotion features. This results into a much richer description of human actions. We then applyan unsupervised generative model to learn different classes of human actions from these …,Motion and video Computing; 2008. WMVC 2008. IEEE Workshop on,2008,230
Online detection of unusual events in videos via dynamic sparse coding,Bin Zhao; Li Fei-Fei; Eric P Xing,Real-time unusual event detection in video stream has been a difficult challenge due to thelack of sufficient training information; volatility of the definitions for both normality andabnormality; time constraints; and statistical limitation of the fitness of any parametricmodels. We propose a fully unsupervised dynamic sparse coding approach for detectingunusual events in videos based on online sparse re-constructibility of query signals from anatomically learned event dictionary; which forms a sparse coding bases. Based on anintuition that usual events in a video are more likely to be reconstructible from an eventdictionary; whereas unusual events are not; our algorithm employs a principled convexoptimization formulation that allows both a sparse reconstruction code; and an onlinedictionary to be jointly inferred and updated. Our algorithm is completely un-supervised …,Computer Vision and Pattern Recognition (CVPR); 2011 IEEE Conference on,2011,228
Novel dataset for fine-grained image categorization: Stanford dogs,Aditya Khosla; Nityananda Jayadevaprakash; Bangpeng Yao; Fei-Fei Li,We introduce a 120 class Stanford Dogs dataset; a challenging and large-scale datasetaimed at fine-grained image categorization. Stanford Dogs includes over 22;000 annotatedimages of dogs belonging to 120 species. Each image is annotated with a bounding boxand object class label. Fig. 1 shows examples of images from Stanford Dogs. This dataset isextremely challenging due to a variety of reasons. First; being a fine-grained categorizationproblem; there is little inter-class variation. For example the basset hound and bloodhoundshare very similar facial characteristics but differ significantly in their color; while theJapanese spaniel and papillion share very similar color but greatly differ in their facialcharacteristics. Second; there is very large intra-class variation. The images show that dogswithin a class could have different ages (eg beagle); poses (eg blenheim spaniel) …,Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC),2011,219
3d object representations for fine-grained categorization,Jonathan Krause; Michael Stark; Jia Deng; Li Fei-Fei,Abstract While 3D object representations are being revived in the context of multi-viewobject class detection and scene understanding; they have not yet attained wide-spread usein fine-grained categorization. State-of-the-art approaches achieve remarkable performancewhen training data is plentiful; but they are typically tied to flat; 2D representations thatmodel objects as a collection of unconnected views; limiting their ability to generalize acrossviewpoints. In this paper; we therefore lift two state-of-the-art 2D object representations to3D; on the level of both local feature appearance and location. In extensive experiments onexisting and newly proposed datasets; we show our 3D object representations outperformtheir state-of-the-art 2D counterparts for fine-grained categorization and demonstrate theirefficacy for estimating 3D geometry from images via ultrawide baseline matching and 3D …,Computer Vision Workshops (ICCVW); 2013 IEEE International Conference on,2013,216
Fine-grained crowdsourcing for fine-grained recognition,Jia Deng; Jonathan Krause; Li Fei-Fei,Abstract Fine-grained recognition concerns categorization at sub-ordinate levels; where thedistinction between object classes is highly local. Compared to basic level recognition; fine-grained categorization can be more challenging as there are in general less data and fewerdiscriminative features. This necessitates the use of stronger prior for feature selection. Inthis work; we include humans in the loop to help computers select discriminative features.We introduce a novel online game called “Bubbles” that reveals discriminative featureshumans use. The player's goal is to identify the category of a heavily blurred image. Duringthe game; the player can choose to reveal full details of circular regions (“bubbles”); with acertain penalty. With proper setup the game generates discriminative bubbles with assuredquality. We next propose the “BubbleBank” algorithm that uses the human selected …,Computer Vision and Pattern Recognition (CVPR); 2013 IEEE Conference on,2013,207
One-shot learning of object categories,Fei-Fei Li; Rob Fergus; Pietro Perona,Learning visual models of object categories notoriously requires hundreds or thousands oftraining examples. We show that it is possible to learn much information about a categoryfrom just one; or a handful; of images. The key insight is that; rather than learning fromscratch; one can take advantage of knowledge coming from previously learned categories;no matter how different these categories might be. We explore a Bayesian implementation ofthis idea. Object categories are represented by probabilistic models. Prior knowledge isrepresented as a probability density function on the parameters of these models. Theposterior model for an object category is obtained by updating the prior in the light of one ormore observations. We test a simple implementation of our algorithm on a database of 101diverse object categories. We compare category models learned by an implementation of …,IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,197
Hierarchical semantic indexing for large scale image retrieval,Jia Deng; Alexander C Berg; Li Fei-Fei,This paper addresses the problem of similar image retrieval; especially in the setting of large-scale datasets with millions to billions of images. The core novel contribution is an approachthat can exploit prior knowledge of a semantic hierarchy. When semantic labels and ahierarchy relating them are available during training; significant improvements over the stateof the art in similar image retrieval are attained. While some of this advantage comes fromthe ability to use additional information; experiments exploring a special case where noadditional data is provided; show the new approach can still outperform OASIS; the currentstate of the art for similarity learning. Exploiting hierarchical relationships is most importantfor larger scale problems; where scalability becomes crucial. The proposed learningapproach is fundamentally parallelizable and as a result scales more easily than previous …,Computer Vision and Pattern Recognition (CVPR); 2011 IEEE Conference on,2011,191
Using dependent regions for object categorization in a generative framework,Gang Wang; Ye Zhang; Li Fei-Fei," Bag of words" models have enjoyed much attention and achieved good performances inrecent studies of object categorization. In most of these works; local patches are modeled asbasic building blocks of an image; analogous to words in text documents. In most previousworks using the" bag of words" models (eg [4; 20; 7]); the local patches are assumed to beindependent with each other. In this paper; we relax the independence assumption andmodel explicitly the inter-dependency of the local regions. Similarly to previous work; werepresent images as a collection of patches; each of which belongs to a latent" theme" that isshared across images as well as categories. We learn the theme distributions and patchdistributions over the themes in a hierarchical structure [22]. In particular; we introduce alinkage structure over the latent themes to encode the dependencies of the patches. This …,Computer Vision and Pattern Recognition; 2006 IEEE Computer Society Conference on,2006,183
Learning a dense multi-view representation for detection; viewpoint classification and synthesis of object categories,Hao Su; Min Sun; Li Fei-Fei; Silvio Savarese,Recognizing object classes and their 3D viewpoints is an important problem in computervision. Based on a part-based probabilistic representation [31]; we propose a new 3D objectclass model that is capable of recognizing unseen views by pose estimation and synthesis.We achieve this by using a dense; multiview representation of the viewing sphereparameterized by a triangular mesh of viewpoints. Each triangle of viewpoints can bemorphed to synthesize new viewpoints. By incorporating 3D geometrical constraints; ourmodel establishes explicit correspondences among object parts across viewpoints. Wepropose an incremental learning algorithm to train the generative model. A cellphone videoclip of an object is first used to initialize model learning. Then the model is updated by a setof unsorted training images without viewpoint labels. We demonstrate the robustness of …,Computer Vision; 2009 IEEE 12th International Conference on,2009,177
Object-centric spatial pooling for image classification,Olga Russakovsky; Yuanqing Lin; Kai Yu; Li Fei-Fei,Abstract Spatial pyramid matching (SPM) based pooling has been the dominant choice forstate-of-art image classification systems. In contrast; we propose a novel object-centricspatial pooling (OCP) approach; following the intuition that knowing the location of the objectof interest can be useful for image classification. OCP consists of two steps:(1) inferring thelocation of the objects; and (2) using the location information to pool foreground andbackground features separately to form the image-level representation. Step (1) isparticularly challenging in a typical classification setting where precise object locationannotations are not available during training. To address this challenge; we propose aframework that learns object detectors using only image-level class labels; or so-calledweak labels. We validate our approach on the challenging PASCAL07 dataset. Our …,*,2012,173
Fine-grained recognition without part annotations,Jonathan Krause; Hailin Jin; Jianchao Yang; Li Fei-Fei,Abstract Scaling up fine-grained recognition to all domains of fine-grained objects is achallenge the computer vision community will need to face in order to realize its goal ofrecognizing all object categories. Current state-of-the-art techniques rely heavily upon theuse of keypoint or part annotations; but scaling up to hundreds or thousands of domainsrenders this annotation cost-prohibitive for all but the most important categories. In this workwe propose a method for fine-grained recognition that uses no part annotations. Our methodis based on generating parts using co-segmentation and alignment; which we combine in adiscriminative mixture. Experimental results show its efficacy; demonstrating state-of-the-artresults even when compared to methods that use part annotations during training.,Computer Vision and Pattern Recognition (CVPR); 2015 IEEE Conference on,2015,155
Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora,Richard Socher; Li Fei-Fei,We propose a semi-supervised model which segments and annotates images using veryfew labeled images and a large unaligned text corpus to relate image regions to text labels.Given photos of a sports event; all that is necessary to provide a pixel-level labeling ofobjects and background is a set of newspaper articles about this sport and one to fivelabeled images. Our model is motivated by the observation that words in text corpora sharecertain context and feature similarities with visual objects. We describe images using visualwords; a new region-based representation. The proposed model is based on kernelizedcanonical correlation analysis which finds a mapping between visual and textual words byprojecting them into a latent meaning space. Kernels are derived from context and adjectivefeatures inside the respective visual and textual domains. We apply our method to a …,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,153
Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses,Bangpeng Yao; Li Fei-Fei,Detecting objects in cluttered scenes and estimating articulated human body parts from 2Dimages are two challenging problems in computer vision. The difficulty is particularlypronounced in activities involving human-object interactions (eg; playing tennis); where therelevant objects tend to be small or only partially visible and the human body parts are oftenself-occluded. We observe; however; that objects and human poses can serve as mutualcontext to each other-recognizing one facilitates the recognition of the other. In this paper;we propose a mutual context model to jointly model objects and human poses in human-object interaction activities. In our approach; object detection provides a strong prior forbetter human pose estimation; while human pose estimation improves the accuracy ofdetecting the objects that interact with the human. On a six-class sports data set and a 24 …,IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,141
Attribute learning in large-scale datasets,Olga Russakovsky; Li Fei-Fei,Abstract We consider the task of learning visual connections between object categoriesusing the ImageNet dataset; which is a large-scale dataset ontology containing more than15 thousand object classes. We want to discover visual relationships between the classesthat are currently missing (such as similar colors or shapes or textures). In this work we learn20 visual attributes and use them in a zero-shot transfer learning experiment as well as tomake visual connections between semantically unrelated object categories.,ECCV 2010 Workshop on Parts and Attributes,2010,138
A codebook-free and annotation-free approach for fine-grained image categorization,Bangpeng Yao; Gary Bradski; Li Fei-Fei,Fine-grained categorization refers to the task of classifying objects that belong to the samebasic-level class (eg different bird species) and share similar shape or visual appearances.Most of the state-of-the-art basic-level object classification algorithms have difficulties in thischallenging problem. One reason for this can be attributed to the popular codebook-basedimage representation; often resulting in loss of subtle image information that are critical forfine-grained classification. Another way to address this problem is to introduce humanannotations of object attributes or key points; a tedious process that is also difficult togeneralize to new tasks. In this work; we propose a codebook-free and annotation-freeapproach for fine-grained image categorization. Instead of using vector-quantizedcodewords; we obtain an image representation by running a high throughput template …,Computer Vision and Pattern Recognition (CVPR); 2012 IEEE Conference on,2012,133
Visual7w: Grounded question answering in images,Yuke Zhu; Oliver Groth; Michael Bernstein; Li Fei-Fei,Abstract We have seen great progress in basic perceptual tasks such as object recognitionand detection. However; AI models still fail to match humans in high-level vision tasks due tothe lack of capacities for deeper reasoning. Recently the new task of visual questionanswering (QA) has been proposed to evaluate a model's capacity for deep imageunderstanding. Previous works have established a loose; global association between QAsentences and images. However; many questions and answers; in practice; relate to localregions in the images. We establish a semantic link between textual descriptions and imageregions by object-level grounding. It enables a new type of QA with visual answers; inaddition to textual answers used in previous work. We study the visual QA tasks in agrounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore; we …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2016,132
A multi-view probabilistic model for 3d object classes,Min Sun; Hao Su; Silvio Savarese; Li Fei-Fei,We propose a novel probabilistic framework for learning visual models of 3D objectcategories by combining appearance information and geometric constraints. Objects arerepresented as a coherent ensemble of parts that are consistent under 3D viewpointtransformations. Each part is a collection of salient image features. A generative frameworkis used for learning a model that captures the relative position of parts within each of thediscretized viewpoints. Contrary to most of the existing mixture of viewpoints models; ourmodel establishes explicit correspondences of parts across different viewpoints of the objectclass. Given a new image; detection and classification are achieved by determining theposition and viewpoint of the model that maximize recognition scores of the candidateobjects. Our approach is among the first to propose a generative probabilistic framework …,Computer Vision and Pattern Recognition; 2009. CVPR 2009. IEEE Conference on,2009,125
Objects as attributes for scene classification,Li-Jia Li; Hao Su; Yongwhan Lim; Li Fei-Fei,Abstract Robust low-level image features have proven to be effective representations for avariety of high-level visual recognition tasks; such as object recognition and sceneclassification. But as the visual recognition tasks become more challenging; the semanticgap between low-level feature representation and the meaning of the scenes increases. Inthis paper; we propose to use objects as attributes of scenes for scene classification. Werepresent images by collecting their responses to a large number of object detectors; or“object filters”. Such representation carries high-level semantic information rather than low-level image feature information; making it more suitable for high-level visual recognitiontasks. Using very simple; off-the-shelf classifiers such as SVM; we show that this object-levelimage representation can be used effectively for high-level visual tasks such as scene …,European Conference on Computer Vision,2010,124
Simple line drawings suffice for functional MRI decoding of natural scene categories,Dirk B Walther; Barry Chai; Eamon Caddigan; Diane M Beck; Li Fei-Fei,Abstract Humans are remarkably efficient at categorizing natural scenes. In fact; scenecategories can be decoded from functional MRI (fMRI) data throughout the ventral visualcortex; including the primary visual cortex; the parahippocampal place area (PPA); and theretrosplenial cortex (RSC). Here we ask whether; and where; we can still decode scenecategory if we reduce the scenes to mere lines. We collected fMRI data while participantsviewed photographs and line drawings of beaches; city streets; forests; highways;mountains; and offices. Despite the marked difference in scene statistics; we were able todecode scene category from fMRI data for line drawings just as well as from activity for colorphotographs; in primary visual cortex through PPA and RSC. Even more remarkably; in PPAand RSC; error patterns for decoding from line drawings were very similar to those from …,Proceedings of the National Academy of Sciences,2011,123
Social lstm: Human trajectory prediction in crowded spaces,Alexandre Alahi; Kratarth Goel; Vignesh Ramanathan; Alexandre Robicquet; Li Fei-Fei; Silvio Savarese,Abstract Pedestrians follow different trajectories to avoid obstacles and accommodate fellowpedestrians. Any autonomous vehicle navigating such a scene should be able to foresee thefuture positions of pedestrians and accordingly adjust its path to avoid collisions. Thisproblem of trajectory prediction can be viewed as a sequence generation task; where we areinterested in predicting the future trajectory of people based on their past positions.Following the recent success of Recurrent Neural Network (RNN) models for sequenceprediction tasks; we propose an LSTM model which can learn general human movementand predict their future trajectories. This is in contrast to traditional approaches which usehand-crafted functions such as Social forces. We demonstrate the performance of ourmethod on several public datasets. Our model outperforms state-of-the-art methods on …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2016,122
Object discovery in 3d scenes via shape analysis,Andrej Karpathy; Stephen Miller; Li Fei-Fei,We present a method for discovering object models from 3D meshes of indoorenvironments. Our algorithm first decomposes the scene into a set of candidate meshsegments and then ranks each segment according to its “objectness”-a quality thatdistinguishes objects from clutter. To do so; we propose five intrinsic shape measures:compactness; symmetry; smoothness; and local and global convexity. We additionallypropose a recurrence measure; codifying the intuition that frequently occurring geometriesare more likely to correspond to complete objects. We evaluate our method in bothsupervised and unsupervised regimes on a dataset of 58 indoor scenes collected using anOpen Source implementation of Kinect Fusion [1]. We show that our approach can reliablyand efficiently distinguish objects from clutter; with Average Precision score of. 92. We …,Robotics and Automation (ICRA); 2013 IEEE International Conference on,2013,122
End-to-end learning of action detection from frame glimpses in videos,Serena Yeung; Olga Russakovsky; Greg Mori; Li Fei-Fei,Abstract In this work we introduce a fully end-to-end approach for action detection in videosthat learns to directly predict the temporal bounds of actions. Our intuition is that the processof detecting actions is naturally one of observation and refinement: observing moments invideo; and refining hypotheses about when an action is occurring. Based on this insight; weformulate our model as a recurrent neural network-based agent that interacts with a videoover time. The agent observes video frames and decides both where to look next and whento emit a prediction. Since backpropagation is not adequate in this non-differentiable setting;we use REINFORCE to learn the agent's decision policy. Our model achieves state-of-the-artresults on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% orless) of the video frames.,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2016,119
Towards scalable dataset construction: An active learning approach,Brendan Collins; Jia Deng; Kai Li; Li Fei-Fei,Abstract As computer vision research considers more object categories and greater variationwithin object categories; it is clear that larger and more exhaustive datasets are necessary.However; the process of collecting such datasets is laborious and monotonous. We considerthe setting in which many images have been automatically collected for a visual category(typically by automatic internet search); and we must separate relevant images from noise.We present a discriminative learning process which employs active; online learning toquickly classify many images with minimal user input. The principle advantage of this workover previous endeavors is its scalability. We demonstrate precision which is often superiorto the state-of-the-art; with scalability which exceeds previous work.,Computer Vision–ECCV 2008,2008,118
Building and using a semantivisual image hierarchy,Li-Jia Li; Chong Wang; Yongwhan Lim; David M Blei; Li Fei-Fei,A semantically meaningful image hierarchy can ease the human effort in organizingthousands and millions of pictures (eg; personal albums); and help to improve performanceof end tasks such as image annotation and classification. Previous work has focused onusing either low-level image features or textual tags to build image hierarchies; resulting inlimited success in their general usage. In this paper; we propose a method to automaticallydiscover the “semantivisual” image hierarchy by incorporating both image and taginformation. This hierarchy encodes a general-to-specific image relationship. We payparticular attention to quantifying the effectiveness of the learned hierarchy; as well ascomparing our method with others in the end-task applications. Our experiments show thathumans find our semantivisual image hierarchy more effective than those solely based on …,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,113
Image retrieval using scene graphs,Justin Johnson; Ranjay Krishna; Michael Stark; Li-Jia Li; David Shamma; Michael Bernstein; Li Fei-Fei,Abstract This paper develops a novel framework for semantic image retrieval based on thenotion of a scene graph. Our scene graphs represent objects (“man”;“boat”); attributes ofobjects (“boat is white”) and relationships between objects (“man standing on boat”). We usethese scene graphs as queries to retrieve semantically related images. To this end; wedesign a conditional random field model that reasons about possible groundings of scenegraphs to test images. The likelihoods of these groundings are used as ranking scores forretrieval. We introduce a novel dataset of 5;000 human-generated scene graphs groundedto images and use this dataset to evaluate our method for image retrieval. In particular; weevaluate retrieval using full scene graphs and small scene subgraphs; and show that ourmethod outperforms retrieval methods that use only objects or low-level image features …,Proceedings of the IEEE conference on computer vision and pattern recognition,2015,111
Why does natural scene categorization require little attention? Exploring attentional requirements for natural and synthetic stimuli,Li Fei-Fei; Rufin VanRullen; Christof Koch; Pietro Perona,It was recently demonstrated that detecting target objects (eg; animals) in natural scenes canbe done in a dual-task paradigm; in the near absence of spatial attention. Under the sameconditions; subjects were unable to perform apparently simpler tasks involving syntheticstimuli (eg; discriminating randomly rotated Ts and Ls; or a bisected colour disc and itsmirror image). Classical theories predict that attention is more critical for the recognition ofcomplex stimuli that cannot be easily separated on a single feature dimension. Therefore;these puzzling results have raised a number of questions. If it is not the complexity of astimulus; what then determines the recognition task's attentional requirements? How doesthis differ between natural and artificial stimuli? What can these observations tell us aboutthe mechanism of natural scene processing as well as its relation to attention? Here we …,Visual Cognition,2005,110
Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition,Jia Deng; Jonathan Krause; Alexander C Berg; Li Fei-Fei,As visual recognition scales up to ever larger numbers of categories; maintaining highaccuracy is increasingly difficult. In this work; we study the problem of optimizing accuracy-specificity trade-offs in large scale recognition; motivated by the observation that objectcategories form a semantic hierarchy consisting of many levels of abstraction. A classifiercan select the appropriate level; trading off specificity for accuracy in case of uncertainty. Byoptimizing this trade-off; we obtain classifiers that try to be as specific as possible whileguaranteeing an arbitrarily high accuracy. We formulate the problem as maximizinginformation gain while ensuring a fixed; arbitrarily small error rate with a semantic hierarchy.We propose the Dual Accuracy Reward Trade-off Search (DARTS) algorithm and prove that;under practical conditions; it converges to an optimal solution. Experiments demonstrate …,Computer Vision and Pattern Recognition (CVPR); 2012 IEEE Conference on,2012,108
Reasoning about object affordances in a knowledge base representation,Yuke Zhu; Alireza Fathi; Li Fei-Fei,Abstract Reasoning about objects and their affordances is a fundamental problem for visualintelligence. Most of the previous work casts this problem as a classification task whereseparate classifiers are trained to label objects; recognize attributes; or assign affordances.In this work; we consider the problem of object affordance reasoning using a knowledgebase representation. Diverse information of objects are first harvested from images andother meta-data sources. We then learn a knowledge base (KB) using a Markov LogicNetwork (MLN). Given the learned KB; we show that a diverse set of visual inference taskscan be done in this unified framework without training separate classifiers; including zero-shot affordance prediction and object recognition given human poses.,European conference on computer vision,2014,104
Efficient image and video co-localization with frank-wolfe algorithm,Armand Joulin; Kevin Tang; Li Fei-Fei,Abstract In this paper; we tackle the problem of performing efficient co-localization in imagesand videos. Co-localization is the problem of simultaneously localizing (with boundingboxes) objects of the same class across a set of distinct images or videos. Building uponrecent state-of-the-art methods; we show how we are able to naturally incorporate temporalterms and constraints for video co-localization into a quadratic programming framework.Furthermore; by leveraging the Frank-Wolfe algorithm (or conditional gradient); we showhow our optimization formulations for both images and videos can be reduced to solving asuccession of simple integer programs; leading to increased efficiency in both memory andspeed. To validate our method; we present experimental results on the PASCAL VOC 2007dataset for images and the YouTube-Objects dataset for videos; as well as a joint …,European Conference on Computer Vision,2014,98
Discriminative segment annotation in weakly labeled video,Kevin Tang; Rahul Sukthankar; Jay Yagnik; Li Fei-Fei,Abstract The ubiquitous availability of Internet video offers the vision community the excitingopportunity to directly learn localized visual concepts from real-world imagery. Unfortunately;most such attempts are doomed because traditional approaches are ill-suited; both in termsof their computational characteristics and their inability to robustly contend with the labelnoise that plagues uncurated Internet content. We present CRANE; a weakly supervisedalgorithm that is specifically designed to learn under such conditions. First; we exploit theasymmetric availability of real-world training data; where small numbers of positive videostagged with the concept are supplemented with large quantities of unreliable negative data.Second; we ensure that CRANE is robust to label noise; both in terms of tagged videos thatfail to contain the concept as well as occasional negative videos that do. Finally; CRANE …,Computer Vision and Pattern Recognition (CVPR); 2013 IEEE Conference on,2013,97
Shifting weights: Adapting object detectors from image to video,Kevin Tang; Vignesh Ramanathan; Li Fei-Fei; Daphne Koller,Abstract Typical object detectors trained on images perform poorly on video; as there is aclear distinction in domain between the two types of data. In this paper; we tackle theproblem of adapting object detectors learned from images to work well on videos. We treatthe problem as one of unsupervised domain adaptation; in which we are given labeled datafrom the source domain (image); but only unlabeled data from the target domain (video). Ourapproach; self-paced domain adaptation; seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples; starting with theeasiest first. At each iteration; the algorithm adapts by considering an increased number oftarget domain examples; and a decreased number of source domain examples. To discovertarget domain examples from the vast amount of video data; we introduce a simple; robust …,Advances in Neural Information Processing Systems,2012,93
Co-localization in real-world images,Kevin Tang; Armand Joulin; Li-Jia Li; Li Fei-Fei,Abstract In this paper; we tackle the problem of co-localization in real-world images. Co-localization is the problem of simultaneously localizing (with bounding boxes) objects of thesame class across a set of distinct images. Although similar problems such as co-segmentation and weakly supervised localization have been previously studied; we focuson being able to perform co-localization in real-world settings; which are typicallycharacterized by large amounts of intraclass variation; inter-class diversity; and annotationnoise. To address these issues; we present a joint image-box formulation for solving the co-localization problem; and show how it can be relaxed to a convex quadratic program whichcan be efficiently solved. We perform an extensive evaluation of our method compared toprevious state-of-theart approaches on the challenging PASCAL VOC 2007 and Object …,Proceedings of the IEEE conference on computer vision and pattern recognition,2014,91
Target-driven visual navigation in indoor scenes using deep reinforcement learning,Yuke Zhu; Roozbeh Mottaghi; Eric Kolve; Joseph J Lim; Abhinav Gupta; Li Fei-Fei; Ali Farhadi,Two less addressed issues of deep reinforcement learning are (1) lack of generalizationcapability to new goals; and (2) data inefficiency; ie; the model requires several (and oftencostly) episodes of trial and error to converge; which makes it impractical to be applied toreal-world scenarios. In this paper; we address these two issues and apply our model totarget-driven visual navigation. To address the first issue; we propose an actor-critic modelwhose policy is a function of the goal as well as the current state; which allows bettergeneralization. To address the second issue; we propose the AI2-THOR framework; whichprovides an environment with high-quality 3D scenes and a physics engine. Our frameworkenables agents to take actions and interact with objects. Hence; we can collect a hugenumber of training samples efficiently. We show that our proposed method (1) converges …,Robotics and Automation (ICRA); 2017 IEEE International Conference on,2017,90
Action recognition with exemplar based 2.5 d graph matching,Bangpeng Yao; Li Fei-Fei,Abstract This paper deals with recognizing human actions in still images. We make two keycontributions.(1) We propose a novel; 2.5 D representation of action images that considersboth view-independent pose information and rich appearance information. A 2.5 D graph ofan action image consists of a set of nodes that are key-points of the human body; as well asa set of edges that are spatial relationships between the nodes. Each key-point isrepresented by view-independent 3D positions and local 2D appearance features. Thesimilarity between two action images can then be measured by matching theircorresponding 2.5 D graphs.(2) We use an exemplar based action classification approach;where a set of representative images are selected for each action class. The selectedimages cover large within-action variations and carry discriminative information …,European Conference on Computer Vision,2012,88
Crowdsourcing annotations for visual object detection,Hao Su; Jia Deng; Li Fei-Fei,Abstract A large number of images with ground truth object bounding boxes are critical forlearning object detectors; which is a fundamental task in compute vision. In this paper; westudy strategies to crowd-source bounding box annotations. The core challenge of buildingsuch a system is to effectively control the data quality with minimal cost. Our key observationis that drawing a bounding box is significantly more difficult and time consuming than givinganswers to multiple choice questions. Thus quality control through additional verificationtasks is more cost effective than consensus based algorithms. In particular; we present asystem that consists of three simple sub-tasks—a drawing task; a quality verification task anda coverage verification task. Experimental results demonstrate that our system is scalable;accurate; and cost-effective.,Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence,2012,87
Object bank: An object-level image representation for high-level visual recognition,Li-Jia Li; Hao Su; Yongwhan Lim; Li Fei-Fei,Abstract It is a remarkable fact that images are related to objects constituting them. In thispaper; we propose to represent images by using objects appearing in them. We introducethe novel concept of object bank (OB); a high-level image representation encoding objectappearance and spatial location information in images. OB represents an image based onits response to a large number of pre-trained object detectors; or 'object filters'; blind to thetesting dataset and visual recognition task. Our OB representation demonstrates promisingpotential in high level image recognition tasks. It significantly outperforms traditional lowlevel image representations in image classification on various benchmark image datasets byusing simple; off-the-shelf classification algorithms such as linear SVM and logisticregression. In this paper; we analyze OB in detail; explaining our design choice of OB for …,International journal of computer vision,2014,82
The unreasonable effectiveness of noisy data for fine-grained recognition,Jonathan Krause; Benjamin Sapp; Andrew Howard; Howard Zhou; Alexander Toshev; Tom Duerig; James Philbin; Li Fei-Fei,Abstract Current approaches for fine-grained recognition do the following: First; recruitexperts to annotate a dataset of images; optionally also collecting more structured data inthe form of part annotations and bounding boxes. Second; train a model utilizing this data.Toward the goal of solving fine-grained recognition; we introduce an alternative approach;leveraging free; noisy data from the web and simple; generic methods of recognition. Thisapproach has benefits in both performance and scalability. We demonstrate its efficacy onfour fine-grained datasets; greatly exceeding existing state of the art without the manualcollection of even a single label; and furthermore show first results at scaling to more than10;000 fine-grained categories. Quantitatively; we achieve top-1 accuracies of 92.3\;\%92.3% on CUB-200-2011; 85.4\;\% 85.4% on Birdsnap; 93.4\;\% 93.4% on FGVC-Aircraft …,European Conference on Computer Vision,2016,80
Revision: Automated classification; analysis and redesign of chart images,Manolis Savva; Nicholas Kong; Arti Chhajta; Li Fei-Fei; Maneesh Agrawala; Jeffrey Heer,Abstract Poorly designed charts are prevalent in reports; magazines; books and on the Web.Most of these charts are only available as bitmap images; without access to the underlyingdata it is prohibitively difficult for viewers to create more effective visual representations. Inresponse we present ReVision; a system that automatically redesigns visualizations toimprove graphical perception. Given a bitmap image of a chart as input; ReVision appliescomputer vision and machine learning techniques to identify the chart type (eg; pie chart; barchart; scatterplot; etc.). It then extracts the graphical marks and infers the underlying data.Using a corpus of images drawn from the web; ReVision achieves image classificationaccuracy of 96% across ten chart categories. It also accurately extracts marks from 79% ofbar charts and 62% of pie charts; and from these charts it successfully extracts data from …,Proceedings of the 24th annual ACM symposium on User interface software and technology,2011,80
What’s the point: Semantic segmentation with point supervision,Amy Bearman; Olga Russakovsky; Vittorio Ferrari; Li Fei-Fei,Abstract The semantic image segmentation task presents a trade-off between test timeaccuracy and training time annotation cost. Detailed per-pixel annotations enable trainingaccurate models but are very time-consuming to obtain; image-level class labels are anorder of magnitude cheaper but result in less accurate models. We take a natural step fromimage-level annotation towards stronger supervision: we ask annotators to point to an objectif one exists. We incorporate this point supervision along with a novel objectness potential inthe training loss function of a CNN model. Experimental results on the PASCAL VOC 2012benchmark reveal that the combined effect of point-level supervision and objectnesspotential yields an improvement of 12.9\;\% mIOU over image-level supervision. Further; wedemonstrate that models trained with point-level supervision are more accurate than …,European Conference on Computer Vision,2016,79
Visual relationship detection with language priors,Cewu Lu; Ranjay Krishna; Michael Bernstein; Li Fei-Fei,Abstract Visual relationships capture a wide variety of interactions between pairs of objectsin images (eg “man riding bicycle” and “man pushing bicycle”). Consequently; the set ofpossible relationships is extremely large and it is difficult to obtain sufficient trainingexamples for all possible relationships. Because of this limitation; previous work on visualrelationship detection has concentrated on predicting only a handful of relationships.Though most relationships are infrequent; their objects (eg “man” and “bicycle”) andpredicates (eg “riding” and “pushing”) independently occur more frequently. We propose amodel that uses this insight to train visual models for objects and predicates individually andlater combines them together to predict multiple relationships per image. We improve onprior work by leveraging language priors from semantic word embeddings to finetune the …,European Conference on Computer Vision,2016,77
Differential connectivity within the parahippocampal place area,Christopher Baldassano; Diane M Beck; Li Fei-Fei,Abstract The Parahippocampal Place Area (PPA) has traditionally been considered ahomogeneous region of interest; but recent evidence from both human studies and animalmodels has suggested that PPA may be composed of functionally distinct subunits. Toinvestigate this hypothesis; we utilize a functional connectivity measure for fMRI that canestimate connectivity differences at the voxel level. Applying this method to whole-brain datafrom two experiments; we provide the first direct evidence that anterior and posterior PPAexhibit distinct connectivity patterns; with anterior PPA more strongly connected to regions inthe default mode network (including the parieto-medial temporal pathway) and posteriorPPA more strongly connected to occipital visual regions. We show that object sensitivity inPPA also has an anterior–posterior gradient; with stronger responses to abstract objects …,Neuroimage,2013,69
Socially-aware large-scale crowd forecasting,Alexandre Alahi; Vignesh Ramanathan; Li Fei-Fei,Abstract In crowded spaces such as city centers or train stations; human mobility lookscomplex; but is often influenced only by a few causes. We propose to quantitatively studycrowded environments by introducing a dataset of 42 million trajectories collected in trainstations. Given this dataset; we address the problem of forecasting pedestrians' destinations;a central problem in understanding large-scale crowd mobility. We need to overcome thechallenges posed by a limited number of observations (eg sparse cameras); and change inpedestrian appearance cues across different cameras. In addition; we often have restrictionsin the way pedestrians can move in a scene; encoded as priors over origin and destination(OD) preferences. We propose a new descriptor coined as Social Affinity Maps (SAM) to linkbroken or unobserved trajectories of individuals in the crowd; while using the OD-prior in …,2014 IEEE Conference on Computer Vision and Pattern Recognition,2014,68
Exploring functional connectivity of the human brain using multivariate information analysis,Barry Chai; Dirk B Walther; Diane M Beck; Li Fei-Fei,Abstract In this study; we present a method for estimating the mutual information for alocalized pattern of fMRI data. We show that taking a multivariate information approach tovoxel selection leads to a decoding accuracy that surpasses an univariate inforamtionapproach and other standard voxel selection methods. Furthermore; we extend themultivariate mutual information theory to measure the functional connectivity betweendistributed brain regions. By jointly estimating the information shared by two sets of voxelswe can reliably map out the connectivities in the human brain during experiment conditions.We validated our approach on a 6-way scene categorization fMRI experiment. Themultivariate information analysis is able to ﬁnd strong information ﬂow between PPA andRSC; which conﬁrms existing neuroscience studies on scenes. Furthermore; by exploring …,Advances in neural information processing systems,2009,65
Every moment counts: Dense detailed labeling of actions in complex videos,Serena Yeung; Olga Russakovsky; Ning Jin; Mykhaylo Andriluka; Greg Mori; Li Fei-Fei,Abstract Every moment counts in action recognition. A comprehensive understanding ofhuman activity in video requires labeling every frame according to the actions occurring;placing multiple labels densely over a video sequence. To study this problem we extend theexisting THUMOS dataset and introduce MultiTHUMOS; a new dataset of dense labels overunconstrained internet videos. Modeling multiple; dense labels benefits from temporalrelations within and across classes. We define a novel variant of long short-term memorydeep networks for modeling these temporal relations via multiple input and outputconnections. We show that this model improves action labeling accuracy and furtherenables deeper understanding tasks ranging from structured retrieval to action prediction.,International Journal of Computer Vision,2018,63
Large scale visual recognition challenge 2010,Alex Berg; Jia Deng; L Fei-Fei,*,*,2010,62
Social role discovery in human events,Vignesh Ramanathan; Bangpeng Yao; Li Fei-Fei,Abstract We deal with the problem of recognizing social roles played by people in an event.Social roles are governed by human interactions; and form a fundamental component ofhuman event description. We focus on a weakly supervised setting; where we are provideddifferent videos belonging to an event class; without training role labels. Since social rolesare described by the interaction between people in an event; we propose a ConditionalRandom Field to model the inter-role interactions; along with person specific socialdescriptors. We develop tractable variational inference to simultaneously infer modelweights; as well as role assignment to all people in the videos. We also present a novelYouTube social roles dataset with ground truth role annotations; and introduce annotationson a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes …,Computer Vision and Pattern Recognition (CVPR); 2013 IEEE Conference on,2013,61
View synthesis for recognizing unseen poses of object classes,Silvio Savarese; Li Fei-Fei,Abstract An important task in object recognition is to enable algorithms to categorize objectsunder arbitrary poses in a cluttered 3D world. A recent paper by Savarese & Fei-Fei [1] hasproposed a novel representation to model 3D object classes. In this representation stableparts of objects from one class are linked together to capture both the appearance andshape properties of the object class. We propose to extend this framework and improve theability of the model to recognize poses that have not been seen in training. Inspired by worksin single object view synthesis (eg; Seitz & Dyer [2]); our new representation allows themodel to synthesize novel views of an object class at recognition time. This mechanism isincorporated in a novel two-step algorithm that is able to classify objects under arbitraryand/or unseen poses. We compare our results on pose categorization with the model and …,European Conference on Computer Vision,2008,61
Extracting moving people from internet videos,Juan Niebles; Bohyung Han; Andras Ferencz; Li Fei-Fei,Abstract We propose a fully automatic framework to detect and extract arbitrary humanmotion volumes from real-world videos collected from YouTube. Our system is composed oftwo stages. A person detector is first applied to provide crude information about the possiblelocations of humans. Then a constrained clustering algorithm groups the detections andrejects false positives based on the appearance similarity and spatio-temporal coherence. Inthe second stage; we apply a top-down pictorial structure model to complete the extraction ofthe humans in arbitrary motion. During this procedure; a density propagation techniquebased on a mixture of Gaussians is employed to propagate temporal information in aprincipled way. This method reduces greatly the search space for the measurement in theinference stage. We demonstrate the initial success of this framework both quantitatively …,Computer Vision–ECCV 2008,2008,61
CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning,Justin Johnson; Bharath Hariharan; Laurens van der Maaten; Li Fei-Fei; C Lawrence Zitnick; Ross Girshick,Abstract When building artificial intelligence systems that can reason and answer questionsabout visual data; we need diagnostic tests to analyze our progress and discovershortcomings. Existing benchmarks for visual question answering can help; but have strongbiases that models can exploit to correctly answer questions without reasoning. They alsoconflate multiple sources of error; making it hard to pinpoint model weaknesses. We presenta diagnostic dataset that tests a range of visual reasoning abilities. It contains minimalbiases and has detailed annotations describing the kind of reasoning each questionrequires. We use this dataset to analyze a variety of modern visual reasoning systems;providing novel insights into their abilities and limitations.,Computer Vision and Pattern Recognition (CVPR); 2017 IEEE Conference on,2017,60
Scalable multi-label annotation,Jia Deng; Olga Russakovsky; Jonathan Krause; Michael S Bernstein; Alex Berg; Li Fei-Fei,Abstract We study strategies for scalable multi-label annotation; or for efficiently acquiringmultiple labels from humans for a collection of items. We propose an algorithm that exploitscorrelation; hierarchy; and sparsity of the label distribution. A case study of labeling 200objects using 20;000 images demonstrates the effectiveness of our approach. The algorithmresults in up to 6x reduction in human computation time compared to the naive method ofquerying a human annotator for the presence of every object in every image.,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,2014,59
Learning object categories from internet image searches,Rob Fergus; Li Fei-Fei; Pietro Perona; Andrew Zisserman,In this paper; we describe a simple approach to learning models of visual object categoriesfrom images gathered from Internet image search engines. The images for a given keywordare typically highly variable; with a large fraction being unrelated to the query term; and thuspose a challenging environment from which to learn. By training our models directly fromInternet images; we remove the need to laboriously compile training data sets; required bymost other recognition approaches-this opens up the possibility of learning object categorymodels “on-the-fly.” We describe two simple approaches; derived from the probabilisticlatent semantic analysis (pLSA) technique for text document analysis; that can be used toautomatically learn object models from these data. We show two applications of the learnedmodel: first; to rerank the images returned by the search engine; thus improving the …,Proceedings of the IEEE,2010,58
Large margin learning of upstream scene understanding models,Jun Zhu; Li-Jia Li; Li Fei-Fei; Eric P Xing,Abstract Upstream supervised topic models have been widely used for complicated sceneunderstanding. However; existing maximum likelihood estimation (MLE) schemes can makethe prediction model learning independent of latent topic discovery and result in animbalanced prediction rule for scene classification. This paper presents a joint max-marginand max-likelihood learning method for upstream scene understanding models; in whichlatent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure;which iteratively solves an online loss-augmented SVM. We demonstrate the advantages ofthe large-margin approach on both an 8-category sports dataset and the 67-class MIT indoorscene dataset for scene categorization.,Advances in Neural Information Processing Systems,2010,58
Learning features and parts for fine-grained recognition,Jonathan Krause; Timnit Gebru; Jia Deng; Li-Jia Li; Li Fei-Fei,This paper addresses the problem of fine-grained recognition: recognizing subordinatecategories such as bird species; car models; or dog breeds. We focus on two majorchallenges: learning expressive appearance descriptors and localizing discriminative parts.To this end; we propose an object representation that detects important parts and describesfine grained appearances. The part detectors are learned in a fully unsupervised manner;based on the insight that images with similar poses can be automatically discovered for fine-grained classes in the same domain. The appearance descriptors are learned using aconvolutional neural network. Our approach requires only image level class labels; withoutany use of part annotations or segmentation masks; which may be costly to obtain. We showexperimentally that combining these two insights is an effective strategy for fine-grained …,Pattern Recognition (ICPR); 2014 22nd International Conference on,2014,56
Image segmentation with topic random field,Bin Zhao; Li Fei-Fei; Eric P Xing,Abstract Recently; there has been increasing interests in applying aspect models (eg; PLSAand LDA) in image segmentation. However; these models ignore spatial relationshipsamong local topic labels in an image and suffers from information loss by representingimage feature using the index of its closest match in the codebook. In this paper; we proposeTopic Random Field (TRF) to tackle these two problems. Specifically; TRF defines a MarkovRandom Field over hidden labels of an image; to enforce the spatial coherence betweentopic labels for neighboring regions. Moreover; TRF utilizes a noise channel to model thegeneration of local image features; and avoids the off-line process of building visualcodebook. We provide details of variational inference and parameter learning for TRF.Experimental evaluations on three image data sets show that TRF achieves better …,European Conference on Computer Vision,2010,56
Linking people in videos with “their” names using coreference resolution,Vignesh Ramanathan; Armand Joulin; Percy Liang; Li Fei-Fei,Abstract Natural language descriptions of videos provide a potentially rich and vast sourceof supervision. However; the highly-varied nature of language presents a major barrier to itseffective use. What is needed are models that can reason over uncertainty over both videosand text. In this paper; we tackle the core task of person naming: assigning names of peoplein the cast to human tracks in TV videos. Screenplay scripts accompanying the video providesome crude supervision about who's in the video. However; even the basic problem ofknowing who is mentioned in the script is often difficult; since language often refers topeople using pronouns (eg;“he”) and nominals (eg;“man”) rather than actual names(eg;“Susan”). Resolving the identity of these mentions is the task of coreference resolution;which is an active area of research in natural language processing. We develop a joint …,European Conference on Computer Vision,2014,55
Best of both worlds: human-machine collaboration for object annotation,Olga Russakovsky; Li-Jia Li; Li Fei-Fei,Abstract The long-standing goal of localizing every object in an image remains elusive.Manually annotating objects is quite expensive despite crowd engineering innovations.Current state-of-the-art automatic object detectors can accurately detect at most a fewobjects per image. This paper brings together the latest advancements in object detectionand in crowd engineering into a principled framework for accurately and efficiently localizingobjects in images. The input to the system is an image to annotate and a set of annotationconstraints: desired precision; utility and/or human cost of the labeling. The output is a set ofobject annotations; informed by human feedback and computer vision. Our modelseamlessly integrates multiple computer vision models with multiple sources of human inputin a Markov Decision Process. We empirically validate the effectiveness of our human-in …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2015,52
Combining the right features for complex event recognition,Kevin Tang; Bangpeng Yao; Li Fei-Fei; Daphne Koller,Abstract In this paper; we tackle the problem of combining features extracted from video forcomplex event recognition. Feature combination is an especially relevant task in video data;as there are many features we can extract; ranging from image features computed fromindividual frames to video features that take temporal information into account. To combinefeatures effectively; we propose a method that is able to be selective of different subsets offeatures; as some features or feature combinations may be uninformative for certain classes.We introduce a hierarchical method for combining features based on the AND/OR graphstructure; where nodes in the graph represent combinations of different sets of features. Ourmethod automatically learns the structure of the AND/OR graph using score-based structurelearning; and we introduce an inference procedure that is able to efficiently compute …,Computer Vision (ICCV); 2013 IEEE International Conference on,2013,52
Classifying actions and measuring action similarity by modeling the mutual context of objects and human poses,Bangpeng Yao; Aditya Khosla; Li Fei-Fei,Abstract In this paper; we consider two action recognition problems in still images. One is theconventional action classification task where we assign a class label to each action image;the other is a new problem where we measure the similarity between action images. Weachieve the goals by using a mutual context model to jointly model the objects and humanposes in images of human actions. Experimental results show that our method not onlyimproves action classification accuracy; but also learns a similarity measure that is largelyconsistent with human perception.,a) A,2011,49
Video event understanding using natural language descriptions,Vignesh Ramanathan; Percy Liang; Li Fei-Fei,Abstract Human action and role recognition play an important part in complex eventunderstanding. State-of-the-art methods learn action and role models from detailed spatiotemporal annotations; which requires extensive human effort. In this work; we propose amethod to learn such models based on natural language descriptions of the training videos;which are easier to collect and scale with the number of actions and roles. There are twochallenges with using this form of weak supervision: First; these descriptions only provide ahigh-level summary and often do not directly mention the actions and roles occurring in avideo. Second; natural language descriptions do not provide spatio temporal annotations ofactions and roles. To tackle these challenges; we introduce a topic-based semanticrelatedness (SR) measure between a video description and an action and role label; and …,Computer Vision (ICCV); 2013 IEEE International Conference on,2013,48
Detecting avocados to zucchinis: what have we done; and where are we going?,Olga Russakovsky; Jia Deng; Zhiheng Huang; Alexander C Berg; Li Fei-Fei,Abstract The growth of detection datasets and the multiple directions of object detectionresearch provide both an unprecedented need and a great opportunity for a thoroughevaluation of the current state of the field of categorical object detection. In this paper westrive to answer two key questions. First; where are we currently as a field: what have wedone right; what still needs to be improved? Second; where should we be going indesigning the next generation of object detectors? Inspired by the recent work of Hoiem etal.[10] on the standard PASCAL VOC detection dataset; we perform a large-scale study onthe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) data. First; wequantitatively demonstrate that this dataset provides many of the same detection challengesas the PASCAL VOC. Due to its scale of 1000 object categories; ILSVRC also provides an …,Computer Vision (ICCV); 2013 IEEE International Conference on,2013,45
Detecting events and key actors in multi-person videos,Vignesh Ramanathan; Jonathan Huang; Sami Abu-El-Haija; Alexander Gorban; Kevin Murphy; Li Fei-Fei,Abstract Multi-person event recognition is a challenging task; often with many people activein the scene but only a small subset contributing to an actual event. In this paper; wepropose a model which learns to detect events in such videos while automatically“attending” to the people responsible for the event. Our model does not use explicitannotations regarding who or where those people are during training and testing. Inparticular; we track people in videos and use a recurrent neural network (RNN) to representthe track features. We learn time-varying attention weights to combine these features at eachtime-instant. The attended features are then processed using another RNN for eventdetection/classification. Since most video datasets with multiple people are restricted to asmall number of videos; we also collected a new basketball dataset comprising 257 …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2016,42
Love thy neighbors: Image annotation by exploiting image metadata,Justin Johnson; Lamberto Ballan; Fei-Fei Li,Abstract Some images that are difficult to recognize on their own may become more clear inthe context of a neighborhood of related images with similar social-network metadata. Webuild on this intuition to improve multilabel image annotation. Our model uses imagemetadata nonparametrically to generate neighborhoods of related images using Jaccardsimilarities; then uses a deep neural network to blend visual information from the image andits neighbors. Prior work typically models image metadata parametrically; in contrast; ournonparametric treatment allows our model to perform well even when the vocabulary ofmetadata changes between training and testing. We perform comprehensive experimentson the NUS-WIDE dataset; where we show that our model outperforms state-of-the-artmethods for multilabel image annotation even when our model is forced to generalize to …,arXiv preprint arXiv:1508.07647,2015,41
Learning temporal embeddings for complex video analysis,Vignesh Ramanathan; Kevin Tang; Greg Mori; Li Fei-Fei,Abstract In this paper; we propose to learn temporal embeddings of video frames forcomplex video analysis. Large quantities of unlabeled video data can be easily obtainedfrom the Internet. These videos possess the implicit weak label that they are sequences oftemporally and semantically coherent images. We leverage this information to learntemporal embeddings for video frames by associating frames with the temporal context thatthey appear in. To do this; we propose a scheme for incorporating temporal context basedon past and future frames in videos; and compare this to other contextual representations. Inaddition; we show how data augmentation using multi-resolution samples and hardnegatives helps to significantly improve the quality of the learned embeddings. We evaluatevarious design decisions for learning temporal embeddings; and show that our …,Proceedings of the IEEE International Conference on Computer Vision,2015,37
ImageNet large scale visual recognition challenge 2010,Alex Berg; Jia Deng; Fei-Fei Li,*,ILSVRC2011,2011,37
Knowledge transfer in learning to recognize visual objects classes,Li Fei-Fei,Abstract—Learning to recognize of object classes is one of the most important functionalitiesof vision. It is estimated that humans are able to learn tens of thousands of visual categoriesin their life. Given the photometric and geometric variabilities displayed by objects as well asthe high degree of intra-class variabilities; we hypothesize that humans achieve such a featby using knowledge and information cumulated throughout the learning process. In recentyears; a handful of pioneering papers have applied various forms of knowledge transferalgorithms to the problem of learning object classes. We first review some of these papers byloosely grouping them into three categories: transfer through prior parameters; transferthrough shared features or parts; and transfer through contextual information. In the secondhalf of the paper; we detail a recent algorithm proposed by the author. This incremental …,Proceedings of the International Conference on Development and Learning (ICDL),2006,37
Recognizing and learning object categories,Fei-Fei Li; Rob Fergus; Antonio Torralba,*,Tutorial at ICCV,2005,37
What do reflections tell us about the shape of a mirror?,Silvio Savarese; Li Fei-Fei; Pietro Perona,Abstract Three-dimensional shape may be perceived from static images. Contours; shading;texture gradients; perspective and occlusion are well-studied cues to this percept. Whenlooking at a picture of a specular object; such as a silver vase; one additional cue ispotentially available: a deformed picture of the reflected environment is seen in the surfaceof the object and the amount and type of deformation depend on its shape. Can specularreflections be used as a visual cue for shape perception? Our experiments show that oursubjects are very poor at judging the shape of mirror surfaces in absence of other visualcues. However; for a considerable subset of the stimuli; subjects are highly consistent intheir (most often wrong) perception. This observation leads us to the hypothesis that oursubjects rather than'computing'a percept from each image based on geometrical …,Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization,2004,37
Learning semantic relationships for better action retrieval in images,Vignesh Ramanathan; Congcong Li; Jia Deng; Wei Han; Zhen Li; Kunlong Gu; Yang Song; Samy Bengio; Charles Rosenberg; Li Fei-Fei,Abstract Human actions capture a wide variety of interactions between people and objects.As a result; the set of possible actions is extremely large and it is difficult to obtain sufficienttraining examples for all actions. However; we could compensate for this sparsity insupervision by leveraging the rich semantic relationship between different actions. A singleaction is often composed of other smaller actions and is exclusive of certain others. We needa method which can reason about such relationships and extrapolate unobserved actionsfrom known actions. Hence; we propose a novel neural network framework which jointlyextracts the relationship between actions and uses them for training better action retrievalmodels. Our model incorporates linguistic; visual and logical consistency based cues toeffectively identify these relationships. We train and test our model on a largescale image …,Proceedings of the IEEE conference on computer vision and pattern recognition,2015,35
Efficient extraction of human motion volumes by tracking,Juan Carlos Niebles; Bohyung Han; Li Fei-Fei,We present an automatic and efficient method to extract spatio-temporal human volumesfrom video; which combines top-down model-based and bottom-up appearance-basedapproaches. From the top-down perspective; our algorithm applies shape priorsprobabilistically to candidate image regions obtained by pedestrian detection; and providesaccurate estimates of the human body areas which serve as important constraints for bottom-up processing. Temporal propagation of the identified region is performed with bottom-upcues in an efficient level-set framework; which takes advantage of the sparse top-downinformation that is available. Our formulation also optimizes the extracted human volumeacross frames through belief propagation and provides temporally coherent human regions.We demonstrate the ability of our method to extract human body regions efficiently and …,Computer Vision and Pattern Recognition (CVPR); 2010 IEEE Conference on,2010,35
Generating semantically precise scene graphs from textual descriptions for improved image retrieval,Sebastian Schuster; Ranjay Krishna; Angel Chang; Li Fei-Fei; Christopher D Manning,Abstract Semantically complex queries which include attributes of objects and relationsbetween objects still pose a major challenge to image retrieval systems. Recent work incomputer vision has shown that a graph-based semantic representation called a scenegraph is an effective representation for very detailed image descriptions and for complexqueries for retrieval. In this paper; we show that scene graphs can be effectively createdautomatically from a natural language scene description. We present a rule-based and aclassifierbased scene graph parser whose output can be used for image retrieval. We showthat including relations and attributes in the query graph outperforms a model that onlyconsiders objects and that using the output of our parsers is almost as effective as usinghuman-constructed scene graphs (Recall@ 10 of 27.1% vs. 33.4%). Additionally; we …,Proceedings of the fourth workshop on vision and language,2015,33
Inferring and executing programs for visual reasoning,Justin Johnson; Bharath Hariharan; Laurens van der Maaten; Judy Hoffman; Li Fei-Fei; C Lawrence Zitnick; Ross Girshick,Abstract Existing methods for visual reasoning attempt to directly map inputs to outputs usingblack-box architectures without explicitly modeling the underlying reasoning processes. As aresult; these black-box models often learn to exploit biases in the data rather than learning toperform visual reasoning. Inspired by module networks; this paper proposes a model forvisual reasoning that consists of a program generator that constructs an explicitrepresentation of the reasoning process to be performed; and an execution engine thatexecutes the resulting program to produce an answer. Both the program generator and theexecution engine are implemented by neural networks; and are trained using a combinationof backpropagation and REINFORCE. Using the CLEVR benchmark for visual reasoning;we show that our model significantly outperforms strong baselines and generalizes better …,arXiv preprint arXiv:1705.03633,2017,30
Recurrent attention models for depth-based person identification,Albert Haque; Alexandre Alahi; Li Fei-Fei,Abstract We present an attention-based model that reasons on human body shape andmotion dynamics to identify individuals in the absence of RGB information; hence in thedark. Our approach leverages unique 4D spatio-temporal signatures to address theidentification problem across days. Formulated as a reinforcement learning task; our modelis based on a combination of convolutional and recurrent neural networks with the goal ofidentifying small; discriminative regions indicative of human identity. We demonstrate thatour model produces state-of-the-art results on several published datasets given only depthimages. We further study the robustness of our model towards viewpoint; appearance; andvolumetric changes. Finally; we share insights gleaned from interpretable 2D; 3D; and 4Dvisualizations of our model's spatio-temporal attention.,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016,30
Discovering object functionality,Bangpeng Yao; Jiayuan Ma; Li Fei-Fei,Abstract Object functionality refers to the quality of an object that allows humans to performsome specific actions. It has been shown in psychology that functionality (affordance) is atleast as essential as appearance in object recognition by humans. In computer vision; mostprevious work on functionality either assumes exactly one functionality for each object; orrequires detailed annotation of human poses and objects. In this paper; we propose aweakly supervised approach to discover all possible object functionalities. Each objectfunctionality is represented by a specific type of human-object interaction. Our method takesany possible human-object interaction into consideration; and evaluates image similarity in3D rather than 2D in order to cluster human-object interactions more coherently.Experimental results on a dataset of people interacting with musical instruments show the …,Computer Vision (ICCV); 2013 IEEE International Conference on,2013,30
Binding is a local problem for natural objects and scenes,Rufin VanRullen; Lavanya Reddy; Li Fei-Fei,Abstract Current theories hold that attention is necessary for binding the features of a visualobject into a coherent representation; implying that interference should be observed whentwo objects must be recognized simultaneously: this is the well-known binding problem.Recent studies have suggested; however; that discriminating isolated natural scenes;objects or faces might be possible in the near absence of attention. It is still unclear whatmechanisms underlie this remarkable ability. Here; we investigate whether the bindingproblem affects natural objects in the same way as other stimuli: is interference observedwhen two natural objects or scenes must be simultaneously processed? We show that in thepresence of competing objects; performance in the near absence of attention depends onthe relative distance between stimuli: discrimination is good for stimuli far enough apart …,Vision Research,2005,30
Embracing error to enable rapid crowdsourcing,Ranjay A Krishna; Kenji Hata; Stephanie Chen; Joshua Kravitz; David A Shamma; Li Fei-Fei; Michael S Bernstein,Abstract Microtask crowdsourcing has enabled dataset advances in social science andmachine learning; but existing crowdsourcing schemes are too expensive to scale up withthe expanding volume of data. To scale and widen the applicability of crowdsourcing; wepresent a technique that produces extremely rapid judgments for binary and categoricallabels. Rather than punishing all errors; which causes workers to proceed slowly anddeliberately; our technique speeds up workers' judgments to the point where errors areacceptable and even expected. We demonstrate that it is possible to rectify these errors byrandomizing task order and modeling response latency. We evaluate our technique on abreadth of common labeling tasks such as image verification; word similarity; sentimentanalysis and topic classification. Where prior work typically achieves a 0.25 x to 1x …,Proceedings of the 2016 CHI conference on human factors in computing systems,2016,29
Integration or erasure? Modernizing medicine at Lhasa’s Mentsikhang,Vincanne Adams; Fei-Fei Li,*,Tibetan Medicine in the Contemporary World: Global Politics of Medical Knowledge and Practice; edited by Laurent Pordié,2008,28
Parcellating connectivity in spatial maps,Christopher Baldassano; Diane M Beck; Li Fei-Fei,Abstract A common goal in biological sciences is to model a complex web of connectionsusing a small number of interacting units. We present a general approach for dividing upelements in a spatial map based on their connectivity properties; allowing for the discoveryof local regions underlying large-scale connectivity matrices. Our method is specificallydesigned to respect spatial layout and identify locally-connected clusters; corresponding toplausible coherent units such as strings of adjacent DNA base pairs; subregions of the brain;animal communities; or geographic ecosystems. Instead of using approximate greedyclustering; our nonparametric Bayesian model infers a precise parcellation using collapsedGibbs sampling. We utilize an infinite clustering prior that intrinsically incorporates spatialconstraints; allowing the model to search directly in the space of spatially-coherent …,PeerJ,2015,27
Free your Camera: 3D Indoor Scene Understanding from Arbitrary Camera Motion.,Axel Furlan; Stephen D Miller; Domenico G Sorrenti; Li Fei-Fei; Silvio Savarese,Abstract Many works have been presented for indoor scene understanding; yet few of themcombine structural reasoning with full motion estimation in a real-time oriented approach. Inthis work we address the problem of estimating the 3D structural layout of complex andcluttered indoor scenes from monocular video sequences; where the observer can freelymove in the surrounding space. We propose an effective probabilistic formulation that allowsus to generate; evaluate and optimize layout hypotheses by integrating new image evidenceas the observer moves. Compared to state-of-the-art work; our approach makes significantlyless limiting hypotheses about the scene and the observer (eg; Manhattan worldassumption; known camera motion). We introduce a new challenging dataset and presentan extensive experimental evaluation; which demonstrates that our formulation reaches …,BMVC,2013,26
Voxel-level functional connectivity using spatial regularization,Christopher Baldassano; Marius Cătălin Iordan; Diane M Beck; Li Fei-Fei,Abstract Discovering functional connectivity between and within brain regions is a keyconcern in neuroscience. Due to the noise inherent in fMRI data; it is challenging tocharacterize the properties of individual voxels; and current methods are unable to flexiblyanalyze voxel-level connectivity differences. We propose a new functional connectivitymethod which incorporates a spatial smoothness constraint using regularized optimization;enabling the discovery of voxel-level interactions between brain regions from the smalldatasets characteristic of fMRI experiments. We validate our method in two separateexperiments; demonstrating that we can learn coherent connectivity maps that are consistentwith known results. First; we examine the functional connectivity between early visual areasV1 and VP; confirming that this connectivity structure preserves retinotopic mapping. Then …,NeuroImage,2012,24
Visual categorization is automatic and obligatory: Evidence from Stroop-like paradigm,Michelle R Greene; Li Fei-Fei,Abstract Human observers categorize visual stimuli with remarkable efficiency—a result thathas led to the suggestion that object and scene categorization may be automatic processes.We tested this hypothesis by presenting observers with a modified Stroop paradigm in whichobject or scene words were presented over images of objects or scenes. Terms were eithercongruent or incongruent with the images. Observers classified the words as being object orscene terms while ignoring images. Classifying a word on an incongruent image came at acost for both objects and scenes. Furthermore; automatic processing was observed for entry-level scene categories; but not superordinate-level categories; suggesting that not all rapidcategorizations are automatic. Taken together; we have demonstrated that entry-level visualcategorization is an automatic and obligatory process.,Journal of vision,2014,23
What; where and who? Telling the story of an image by activity classification; scene recognition and object categorization,Li Fei-Fei; Li-Jia Li,Abstract We live in a richly visual world. More than one third of the entire human brain isinvolved in visual processing and understanding. Psychologists have shown that the humanvisual system is particularly efficient and effective in perceiving high-level meanings incluttered real-world scenes; such as objects; scene classes; activities and the stories in theimages. In this chapter; we discuss a generativemodel approach for classifying complexhuman activities (such as croquet game; snowboarding; etc.) given a single static image. Weobserve that object recognition in the scene as well as scene environment classification ofthe image facilitate each other in the overall activity recognition task. We formulate thisobservation in a graphical model representation where activity classification is achieved bycombining information from both the object recognition and the scene classification …,*,2010,23
Connectionist temporal modeling for weakly supervised action labeling,De-An Huang; Li Fei-Fei; Juan Carlos Niebles,Abstract We propose a weakly-supervised framework for action labeling in video; where onlythe order of occurring actions is required during training time. The key challenge is that theper-frame alignments between the input (video) and label (action) sequences are unknownduring training. We address this by introducing the Extended Connectionist TemporalClassification (ECTC) framework to efficiently evaluate all possible alignments via dynamicprogramming and explicitly enforce their consistency with frame-to-frame visual similarities.This protects the model from distractions of visually inconsistent or degenerated alignmentswithout the need of temporal supervision. We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video. With less than 1% oflabeled frames per video; our method is able to outperform existing semi-supervised …,European Conference on Computer Vision,2016,22
Videoset: Video summary evaluation through text,Serena Yeung; Alireza Fathi; Li Fei-Fei,Abstract: In this paper we present VideoSET; a method for Video Summary Evaluationthrough Text that can evaluate how well a video summary is able to retain the semanticinformation contained in its original video. We observe that semantics is most easilyexpressed in words; and develop a text-based approach for the evaluation. Given a videosummary; a text representation of the video summary is first generated; and an NLP-basedmetric is then used to measure its semantic distance to ground-truth text summaries writtenby humans. We show that our technique has higher agreement with human judgment thanpixel-based distance metrics. We also release text annotations and ground-truth textsummaries for a number of publicly available video datasets; for use by the computer visioncommunity. Subjects: Computer Vision and Pattern Recognition (cs. CV); Computation …,arXiv preprint arXiv:1406.5824,2014,22
Task-set switching with natural scenes: measuring the cost of deploying top-down attention,Dirk B Walther; Li Fei-Fei,Abstract In many everyday situations; we bias our perception from the top down; based on atask or an agenda. Frequently; this entails shifting attention to a specific attribute of aparticular object or scene. To explore the cost of shifting top-down attention to a differentstimulus attribute; we adopt the task-set switching paradigm; in which switch trials arecontrasted with repeat trials in mixed-task blocks and with single-task blocks. Using twotasks that relate to the content of a natural scene in a gray-level photograph and two tasksthat relate to the color of the frame around the image; we were able to distinguish switchcosts with and without shifts of attention. We found a significant cost in reaction time of 23–31 ms for switches that require shifting attention to other stimulus attributes; but no significantswitch cost for switching the task set within an attribute. We conclude that deploying top …,Journal of Vision,2007,22
Unsupervised learning of long-term motion dynamics for videos,Zelun Luo; Boya Peng; De-An Huang; Alexandre Alahi; Li Fei-Fei,Abstract We present an unsupervised representation learning approach that compactlyencodes the motion dependencies in videos. Given a pair of images from a video clip; ourframework learns to predict the long-term 3D motions. To reduce the complexity of thelearning framework; we propose to describe the motion as a sequence of atomic 3D flowscomputed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for thedecoder to reconstruct these sequences; the encoder must learn a robust videorepresentation that captures long-term motion dependencies and spatial-temporal relations.We demonstrate the effectiveness of our learned temporal representations on activityclassification across multiple modalities and datasets such as NTU RGB+ D and MSR …,arXiv preprint arXiv:1701.01821,2017,21
What you see is what you expect: rapid scene understanding benefits from prior experience,Michelle R Greene; Abraham P Botros; Diane M Beck; Li Fei-Fei,Abstract Although we are able to rapidly understand novel scene images; little is knownabout the mechanisms that support this ability. Theories of optimal coding assert that priorvisual experience can be used to ease the computational burden of visual processing. Aconsequence of this idea is that more probable visual inputs should be facilitated relative tomore unlikely stimuli. In three experiments; we compared the perceptions of highlyimprobable real-world scenes (eg; an underwater press conference) with common imagesmatched for visual and semantic features. Although the two groups of images could not bedistinguished by their low-level visual features; we found profound deficits related to theimprobable images: Observers wrote poorer descriptions of these images (Exp. 1); haddifficulties classifying the images as unusual (Exp. 2); and even had lower sensitivity to …,Attention; Perception; & Psychophysics,2015,21
CS231n: Convolutional neural networks for visual recognition,Fei-Fei Li; Andrej Karpathy; Justin Johnson,*,University Lecture,2015,20
Crowdsourcing in computer vision,Adriana Kovashka; Olga Russakovsky; Li Fei-Fei; Kristen Grauman,Abstract Computer vision systems require large amounts of manually annotated data toproperly learn challenging visual concepts. Crowdsourcing platforms offer an inexpensivemethod to capture human knowledge and understanding; for a vast number of visualperception tasks. In this survey; we describe the types of annotations computer visionresearchers have collected using crowdsourcing; and how they have ensured that this datais of high quality while annotation effort is minimized. We begin by discussing data collectionon both classic (eg; object recognition) and recent (eg; visual story-telling) vision tasks. Wethen summarize key design decisions for creating effective data collection interfaces andworkflows; and present strategies for intelligently selecting the most important data instancesto annotate. Finally; we conclude with some thoughts on the future of crowdsourcing in …,Foundations and Trends® in Computer Graphics and Vision,2016,19
Leveraging the wisdom of the crowd for fine-grained recognition,Jia Deng; Jonathan Krause; Michael Stark; Li Fei-Fei,Fine-grained recognition concerns categorization at sub-ordinate levels; where thedistinction between object classes is highly local. Compared to basic level recognition; fine-grained categorization can be more challenging as there are in general less data and fewerdiscriminative features. This necessitates the use of a stronger prior for feature selection. Inthis work; we include humans in the loop to help computers select discriminative features.We introduce a novel online game called “Bubbles” that reveals discriminative featureshumans use. The player's goal is to identify the category of a heavily blurred image. Duringthe game; the player can choose to reveal full details of circular regions (“bubbles”); with acertain penalty. With proper setup the game generates discriminative bubbles with assuredquality. We next propose the “BubbleBank” representation that uses the human selected …,IEEE transactions on pattern analysis and machine intelligence,2016,19
Improving image classification with location context,Kevin Tang; Manohar Paluri; Li Fei-Fei; Rob Fergus; Lubomir Bourdev,Abstract With the widespread availability of cellphones and cameras that have GPScapabilities; it is common for images being uploaded to the Internet today to have GPScoordinates associated with them. In addition to research that tries to predict GPScoordinates from visual features; this also opens up the door to problems that areconditioned on the availability of GPS coordinates. In this work; we tackle the problem ofperforming image classification with location context; in which we are given the GPScoordinates for images in both the train and test phases. We explore different ways ofencoding and extracting features from the GPS coordinates; and show how to naturallyincorporate these features into a Convolutional Neural Network (CNN); the current state-of-the-art for most image classification and recognition problems. We also show how it is …,Proceedings of the IEEE international conference on computer vision,2015,19
What do we see in a glance of a scene,L Fei-Fei; A Iyer; C Koch; P Perona,Abstract What exactly do we see when we glance at a natural scene? And does what we seechange as the glance becomes longer? We asked naïve subjects to report what they saw inbriefly presented photographs. Our subjects received no specific information as to thecontent of each stimulus; and were asked to report what they saw in free-form text. Thus; ourparadigm differs from previous studies where subjects were either cued before a stimuluswas presented; and/or were probed with multiple-choice questions. Our experimentconsisted of two stages. First; a group of 22 native-English speaking subjects were shown100 novel gray-scale photographs foveally. The photographs contained a broad sample ofreal-life scenarios both indoor and outdoor. Each presentation time was chosen at randomin the set of 7 possible times (from 27ms to 500ms). A perceptual mask followed each …,Journal of Vision,2007,19
Scene graph generation by iterative message passing,Danfei Xu; Yuke Zhu; Christopher B Choy; Li Fei-Fei,Abstract Understanding a visual scene goes beyond recognizing individual objects inisolation. Relationships between objects also constitute rich semantic information about thescene. In this work; we explicitly model the objects and their relationships using scenegraphs; a visually-grounded graphical structure of an image. We propose a novel endto-endmodel that generates such structured scene representation from an input image. The modelsolves the scene graph inference problem using standard RNNs and learns to iterativelyimproves its predictions via message passing. Our joint inference model can take advantageof contextual cues to make better predictions on objects and their relationships. Theexperiments show that our model significantly outperforms previous methods on generatingscene graphs using Visual Genome dataset and inferring support relations with NYU …,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2017,18
Building a large-scale multimodal knowledge base system for answering visual queries,Yuke Zhu; Ce Zhang; Christopher Ré; Li Fei-Fei,Abstract: The complexity of the visual world creates significant challenges forcomprehensive visual understanding. In spite of recent successes in visual recognition;today's vision systems would still struggle to deal with visual queries that require a deeperreasoning. We propose a knowledge base (KB) framework to handle an assortment of visualqueries; without the need to train new classifiers for new tasks. Building such a large-scalemultimodal KB presents a major challenge of scalability. We cast a large-scale MRF into aKB representation; incorporating visual; textual and structured data; as well as their diverserelations. We introduce a scalable knowledge base construction system that is capable ofbuilding a KB with half billion variables and millions of parameters in a few hours. Oursystem achieves competitive results compared to purpose-built models on standard …,arXiv preprint arXiv:1507.05670,2015,18
Large scale visual recognition challenge,Jia Deng; Alex Berg; Sanjeev Satheesh; Hao Su; Aditya Khosla; Fei-Fei Li,*,www. image-net. org/challenges/LSVRC/2012,2012,18
Multi-view object categorization and pose estimation,Silvio Savarese; Li Fei-Fei,Abstract Object and scene categorization has been a central topic of computer visionresearch in recent years. The problem is a highly challenging one. A single object may showtremendous variability in appearance and structure under various photometric andgeometric conditions. In addition; members of the same class may differ from each other dueto various degrees of intra-class variability. Recently; researchers have proposed newmodels towards the goal of: i) finding a suitable representation that can efficiently capturethe intrinsic three-dimensional and multi-view nature of object categories; ii) takingadvantage of this representation to help the recognition and categorization task. In thisChapter we will review recent approaches aimed at tackling this challenging problem andfocus on the work by Savarese & Fei-Fei [54; 55]. In [54; 55] multi-view object models are …,*,2010,18
Good exemplars of natural scene categories elicit clearer patterns than bad exemplars but not greater BOLD activity,Ana Torralbo; Dirk B Walther; Barry Chai; Eamon Caddigan; Li Fei-Fei; Diane M Beck,Within the range of images that we might categorize as a “beach”; for example; some will bemore representative of that category than others. Here we first confirmed that humans couldcategorize “good” exemplars better than “bad” exemplars of six scene categories and thenexplored whether brain regions previously implicated in natural scene categorizationshowed a similar sensitivity to how well an image exemplifies a category. In a behavioralexperiment participants were more accurate and faster at categorizing good than badexemplars of natural scenes. In an fMRI experiment participants passively viewed blocks ofgood or bad exemplars from the same six categories. A multi-voxel pattern classifier trainedto discriminate among category blocks showed higher decoding accuracy for good than badexemplars in the PPA; RSC and V1. This difference in decoding accuracy cannot be …,PloS one,2013,17
Towards viewpoint invariant 3d human pose estimation,Albert Haque; Boya Peng; Zelun Luo; Alexandre Alahi; Serena Yeung; Li Fei-Fei,Abstract We propose a viewpoint invariant model for 3D human pose estimation from asingle depth image. To achieve this; our discriminative model embeds local regions into alearned viewpoint invariant feature space. Formulated as a multi-task learning problem; ourmodel is able to selectively predict partial poses in the presence of noise and occlusion. Ourapproach leverages a convolutional and recurrent network architecture with a top-downerror feedback mechanism to self-correct previous pose estimates in an end-to-end manner.We evaluate our model on a previously published depth dataset and a newly collectedhuman pose dataset containing 100 K annotated depth images from extreme viewpoints.Experiments show that our model achieves competitive performance on frontal views whileachieving state-of-the-art performance on alternate viewpoints.,European Conference on Computer Vision,2016,16
Understanding the 3D layout of a cluttered room from multiple images,Sid Yingze Bao; Axel Furlan; Li Fei-Fei; Silvio Savarese,We present a novel framework for robustly understanding the geometrical and semanticstructure of a cluttered room from a small number of images captured from differentviewpoints. The tasks we seek to address include: i) estimating the 3D layout of the room-that is; the 3D configuration of floor; walls and ceiling; ii) identifying and localizing all theforeground objects in the room. We jointly use multiview geometry constraints and imageappearance to identify the best room layout configuration. Extensive experimentalevaluation demonstrates that our estimation results are more complete and accurate inestimating 3D room structure and recognizing objects than alternative state-of-the-artalgorithms. In addition; we show an augmented reality mobile application to highlight thehigh accuracy of our method; which may be beneficial to many computer vision …,Applications of Computer Vision (WACV); 2014 IEEE Winter Conference on,2014,16
Visual scenes are categorized by function.,Michelle R Greene; Christopher Baldassano; Andre Esteva; Diane M Beck; Li Fei-Fei,Abstract How do we know that a kitchen is a kitchen by looking? Traditional models positthat scene categorization is achieved through recognizing necessary and sufficient featuresand objects; yet there is little consensus about what these may be. However; scenecategories should reflect how we use visual information. Therefore; we test the hypothesisthat scene categories reflect functions; or the possibilities for actions within a scene. Ourapproach is to compare human categorization patterns with predictions made by bothfunctions and alternative models. We collected a large-scale scene category distance matrix(5 million trials) by asking observers to simply decide whether 2 images were from the sameor different categories. Using the actions from the American Time Use Survey; we mappedactions onto each scene (1.4 million trials). We found a strong relationship between …,Journal of Experimental Psychology: General,2016,15
Efficient euclidean projections onto the intersection of norm balls,Adams Wei Yu; Hao Su; Li Fei-Fei,Abstract: Using sparse-inducing norms to learn robust models has received increasingattention from many fields for its attractive properties. Projection-based methods have beenwidely applied to learning tasks constrained by such norms. As a key building block of thesemethods; an efficient operator for Euclidean projection onto the intersection of $\ell_1 $ and$\ell_ {1; q} $ norm balls $(q= 2\text {or}\infty) $ is proposed in this paper. We prove that theprojection can be reduced to finding the root of an auxiliary function which is piecewisesmooth and monotonic. Hence; a bisection algorithm is sufficient to solve the problem. Weshow that the time complexity of our solution is $ O (n+ g\log g) $ for $ q= 2$ and $ O (n\logn) $ for $ q=\infty $; where $ n $ is the dimensionality of the vector to be projected and $ g $is the number of disjoint groups; we confirm this complexity by experimentation. Empirical …,arXiv preprint arXiv:1206.4638,2012,15
Audio-visual speaker localization using graphical models,Akash Kushal; Mandar Rahurkar; Li Fei-Fei; Jean Ponce; Thomas Huang,In this work we propose an approach to combine audio and video modalities for persontracking using graphical models. We demonstrate a principled and intuitive framework forcombining these modalities to obtain robustness against occlusion and change inappearance. We further exploit the temporal correlations that exist for a moving objectbetween adjacent frames to account for the cases where having both modalities might stillnot be enough; eg; when the person being tracked is occluded and not speaking.Improvement in tracking results is shown at each step and compared with manuallyannotated ground truth,Pattern Recognition; 2006. ICPR 2006. 18th International Conference on,2006,15
Basic level category structure emerges gradually across human ventral visual cortex,Marius Cătălin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,Objects can be simultaneously categorized at multiple levels of specificity ranging from verybroad (“natural object”) to very distinct (“Mr. Woof”); with a mid-level of generality (basiclevel:“dog”) often providing the most cognitively useful distinction between categories. It isunknown; however; how this hierarchical representation is achieved in the brain. Usingmultivoxel pattern analyses; we examined how well each taxonomic level (superordinate;basic; and subordinate) of real-world object categories is represented acrossoccipitotemporal cortex. We found that; although in early visual cortex objects are bestrepresented at the subordinate level (an effect mostly driven by low-level feature overlapbetween objects in the same category); this advantage diminishes compared to the basiclevel as we move up the visual hierarchy; disappearing in object-selective regions of …,Journal of cognitive neuroscience,2015,14
RGB-W: When vision meets wireless,Alexandre Alahi; Albert Haque; Li Fei-Fei,Abstract Inspired by the recent success of RGB-D cameras; we propose the enrichment ofRGB data with an additional “quasi-free” modality; namely; the wireless signal emitted byindividuals' cell phones; referred to as RGB-W. The received signal strength acts as a roughproxy for depth and a reliable cue on a person's identity. Although the measured signals arenoisy; we demonstrate that the combination of visual and wireless data significantlyimproves the localization accuracy. We introduce a novel image-driven representation ofwireless data which embeds all received signals onto a single image. We then evaluate theability of this additional data to (i) locate persons within a sparsitydriven framework and to (ii)track individuals with a new confidence measure on the data association problem. Oursolution outperforms existing localization methods. It can be applied to the millions of …,2015 IEEE International Conference on Computer Vision (ICCV),2015,14
Collecting a large-scale dataset of fine-grained cars,Jonathan Krause; Jia Deng; Michael Stark; Li Fei-Fei,In this work we introduce a large-scale; fine-grained dataset of cars. This dataset; consistingof 197 classes and 16;185 images; represents an order of magnitude increase in size overthe only existing fine-grained car dataset [7](14 classes; 1;904 images) and is comparable insize to the largest fine-grained datasets publicly available [9; 3]. The goals of this work aretwofold: 1) to describe the difficulties encountered when collecting such a dataset and 2) topresent baseline performance for two state-of-the-art methods.,*,2013,14
Dense-captioning events in videos,Ranjay Krishna; Kenji Hata; Frederic Ren; Li Fei-Fei; Juan Carlos Niebles,Abstract Most natural videos contain numerous events. For example; in a video of a “manplaying a piano”; the video might also contain “another man dancing” or “a crowd clapping”.We introduce the task of dense-captioning events; which involves both detecting anddescribing events in a video. We propose a new model that is able to identify all events in asingle pass of the video while simultaneously describing the detected events with naturallanguage. Our model introduces a variant of an existing proposal module that is designed tocapture both short as well as long events that span minutes. To capture the dependenciesbetween the events in a video; our model introduces a new captioning module that usescontextual information from past and future events to jointly describe all events. We alsointroduce ActivityNet Captions; a large-scale benchmark for dense-captioning events …,Proceedings of the IEEE International Conference on Computer Vision,2017,13
Pinpointing the peripheral bias in neural scene-processing networks during natural viewing,Christopher Baldassano; Li Fei-Fei; Diane M Beck,Abstract Peripherally presented stimuli evoke stronger activity in scene-processing regionsthan foveally presented stimuli; suggesting that scene understanding is driven largely byperipheral information. We used functional MRI to investigate whether functional connectivityevoked during natural perception of audiovisual movies reflects this peripheral bias. Foreach scene-sensitive region—the parahippocampal place area (PPA); retrosplenial cortex;and occipital place area—we computed two measures: the extent to which its activity couldbe predicted by V1 activity (connectivity strength) and the eccentricities within V1 to which itwas most closely related (connectivity profile). Scene regions were most related toperipheral voxels in V1; but the detailed nature of this connectivity varied within andbetween these regions. The retrosplenial cortex showed the most consistent peripheral …,Journal of vision,2016,13
Web image prediction using multivariate point processes,Gunhee Kim; Li Fei-Fei; Eric P Xing,Abstract In this paper; we investigate a problem of predicting what images are likely toappear on the Web at a future time point; given a query word and a database of historicalimage streams that potentiates learning of uploading patterns of previous user images andassociated metadata. We address such a Web image prediction problem at both a collectivegroup level and an individual user level. We develop a predictive framework based on themultivariate point process; which employs a stochastic parametric model to solve therelations between image occurrence and the covariates that influence it; in a flexible;scalable; and globally optimal way. Using Flickr datasets of more than ten million images of40 topics; our empirical results show that the proposed algorithm is more successful inpredicting unseen Web images than other candidate methods; including forecasting on …,Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,2012,12
Hierarchical mixture of classification experts uncovers interactions between brain regions,Bangpeng Yao; Dirk Walther; Diane Beck; Li Fei-Fei,Abstract The human brain can be described as containing a number of functional regions.For a given task; these regions; as well as the connections between them; play a key role ininformation processing in the brain. However; most existing multi-voxel pattern analysisapproaches either treat multiple functional regions as one large uniform region or severalindependent regions; ignoring the connections between regions. In this paper; we proposeto model such connections in an Hidden Conditional Random Field (HCRF) framework;where the classifier of one region of interest (ROI) makes predictions based on not only itsvoxels but also the classifier predictions from ROIs that it connects to. Furthermore; wepropose a structural learning method in the HCRF framework to automatically uncover theconnections between ROIs. Experiments on fMRI data acquired while human subjects …,Advances in Neural Information Processing Systems,2009,12
Human–object interactions are more than the sum of their parts,Christopher Baldassano; Diane M Beck; Li Fei-Fei,Abstract Understanding human–object interactions is critical for extracting meaning fromeveryday visual scenes and requires integrating complex relationships between humanpose and object identity into a new percept. To understand how the brain builds theserepresentations; we conducted 2 fMRI experiments in which subjects viewed humansinteracting with objects; noninteracting human–object pairs; and isolated humans andobjects. A number of visual regions process features of human–object interactions; includingobject identity information in the lateral occipital complex (LOC) and parahippocampal placearea (PPA); and human pose information in the extrastriate body area (EBA) and posteriorsuperior temporal sulcus (pSTS). Representations of human–object interactions in someregions; such as the posterior PPA (retinotopic maps PHC1 and PHC2) are well predicted …,Cerebral Cortex,2017,11
Discovering the signatures of joint attention in child-caregiver interaction,Guido Pusiol; Laura Soriano; Michael C Frank; Li Fei-Fei,Abstract Joint attention—when child and caregiver share attention to an object or location—is an important part of early language learning. Identifying when two people are in jointattention is an important practical question for analyzing large-scale video datasets; inaddition; identifying reliable cues to joint attention may provide insights into how childrenaccomplish this feat. We use techniques from computer vision to identify features related tojoint attention from both egocentric and fixedcamera videos of children and caregiverinteracting with objects. We find that the presence of caregivers' faces in the child'segocentric view and the motion of objects in the fixed camera both correlate with human-annotated joint attention. We use a classifier to predict joint attention using these featuresand find some initial success; in addition; classifier performance is substantially increased …,Proceedings of the Annual Meeting of the Cognitive Science Society,2014,11
Using deep learning and google street view to estimate the demographic makeup of the us,Timnit Gebru; Jonathan Krause; Yilun Wang; Duyun Chen; Jia Deng; Erez Lieberman Aiden; Li Fei-Fei,Abstract: The United States spends more than $1 B each year on the American CommunitySurvey (ACS); a labor-intensive door-to-door study that measures statistics relating to race;gender; education; occupation; unemployment; and other demographic factors. Although acomprehensive source of data; the lag between demographic changes and theirappearance in the ACS can exceed half a decade. As digital imagery becomes ubiquitousand machine vision techniques improve; automated data analysis may provide a cheaperand faster alternative. Here; we present a method that determines socioeconomic trendsfrom 50 million images of street scenes; gathered in 200 American cities by Google StreetView cars. Using deep learning-based computer vision techniques; we determined themake; model; and year of all motor vehicles encountered in particular neighborhoods …,arXiv preprint arXiv:1702.06683,2017,10
To err is human: Correlating fMRI decoding and behavioral errors to probe the neural representation of natural scene categories,Dirk B Walther; Diane M Beck; Li Fei-Fei,Summary New multivariate methods for the analysis of functional magnetic resonanceimaging (fMRI) data have enabled us to decode neural representations of visual informationwith unprecedented fidelity. But how do we know if humans make use of the information thatwe decode from the fMRI data for their behavioral response? In this chapter; we propose amethod for correlating the errors from fMRI decoding with the errors made by subjects in abehavioral task. High correlations suggest that subjects use information that is closelyrelated to the content of the fMRI signal to make their behavioral response. We demonstratethe viability of this method using the example of natural scene categorization. Humans areextremely efficient at categorizing natural scenes (such as forests; highways; or beaches);despite the fact that different classes of natural scenes often share similar image statistics …,Visual population codes–Toward a common multivariate framework for cell recording and functional imaging,2012,10
Cs231n convolutional neural networks for visual recognition; 2016,Andrej Karpathy; FF Li; J Johnson,*,URL http://cs231n. github. io,*,9
A hierarchical approach for generating descriptive image paragraphs,Jonathan Krause; Justin Johnson; Ranjay Krishna; Li Fei-Fei,Abstract Recent progress on image captioning has made it possible to generate novelsentences describing images in natural language; but compressing an image into a singlesentence can describe visual content in only coarse detail. While one new captioningapproach; dense captioning; can potentially describe images in finer levels of detail bycaptioning many regions within an image; it in turn is unable to produce a coherent story foran image. In this paper we overcome these limitations by generating entire paragraphs fordescribing images; which can tell detailed; unified stories. We develop a model thatdecomposes both images and paragraphs into their constituent parts; detecting semanticregions in images and using a hierarchical recurrent neural network to reason aboutlanguage. Linguistic analysis confirms the complexity of the paragraph generation task …,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017,8
Motion segment decomposition of RGB-D sequences for human behavior understanding,Maxime Devanne; Stefano Berretti; Pietro Pala; Hazem Wannous; Mohamed Daoudi; Alberto Del Bimbo,Abstract In this paper; we propose a framework for analyzing and understanding humanbehavior from depth videos. The proposed solution first employs shape analysis of thehuman pose across time to decompose the full motion into short temporal segmentsrepresenting elementary motions. Then; each segment is characterized by human motionand depth appearance around hand joints to describe the change in pose of the body andthe interaction with objects. Finally; the sequence of temporal segments is modeled througha Dynamic Naive Bayes classifier; which captures the dynamics of elementary motionscharacterizing human behavior. Experiments on four challenging datasets evaluate thepotential of the proposed approach in different contexts; including gesture or activityrecognition and online activity detection. Competitive results in comparison with state-of …,Pattern Recognition,2017,8
Two distinct scene-processing networks connecting vision and memory,Christopher Baldassano; Andre Esteva; Li Fei-Fei; Diane M Beck,A number of regions in the human brain are known to be involved in processing naturalscenes; but the field has lacked a unifying framework for understanding how these differentregions are organized and interact. We provide evidence from functional connectivity andmeta-analyses for a new organizational principle; in which scene processing relies upon twodistinct networks that split the classically defined parahippocampal place area (PPA). Thefirst network of strongly connected regions consists of the occipital place area/transverseoccipital sulcus and posterior PPA; which contain retinotopic maps and are not stronglycoupled to the hippocampus at rest. The second network consists of the caudal inferiorparietal lobule; retrosplenial complex; and anterior PPA; which connect to the hippocampus(especially anterior hippocampus); and are implicated in both visual and nonvisual tasks …,eNeuro,2016,8
GENIE TRECVID2011 Multimedia Event Detection: Late Fusion Approaches to Combine Multiple Audio Visual features,AG Perera; Sangmin Oh; Matthew Leotta; Ilseo Kim; Byuungki Byun; Chin-Hui Lee; Scott McCloskey; Jingchen Liu; Ben Miller; Zhi Feng Huang; Arash Vahdat; Weilong Yang; Greg Mori; Kevin Tang; Daphne Koller; L Fei-Fei; Kang Li; Gang Chen; Jason Corso; Yun Fu; Rohini Srihari,Abstract: For TRECVID 2011 MED task; the GENIE system incorporated two late-fusionapproaches where multiple discriminative base-classifiers are built per feature; then;combined later through discriminative fusion techniques. All of our fusion and baseclassifiers are formulated as one-vs-all detectors per event class along with thresholdestimation capabilities during cross-validation. Total of five different types of features wereextracted from data; which include both audio or visual features: HOG3D; Object Bank; Gist;MFCC; and acoustic segment models (ASMs). Features such as HOG3D and MFCC are low-level features while Object Bank and ASMs are more semantic. In our work; event-specificfeature adaptations or manual annotations were deliberately avoided; to establish a strongbaseline results. Overall; the results were competitive in the MED11 evaluation; and …,*,2012,8
Visual Recognition: Computational Models and Human Psychophysics,Li Fei-Fei,Abstract Object and scene recognition is one of the most essential functionalities of humanvision. It is also of fundamental importance for machines to be able to learn and recognizemeaningful objects and scenes. In this thesis; we explore the following four aspects of objectand scene recognition. It is well known that humans can be “blind” even to major aspects ofnatural scenes when we attend elsewhere. The only tasks that do not need attention appearto be carried out in the early stages of the visual system. Contrary to this common belief; weshow that subjects can rapidly detect animals or vehicles in briefly presented novel naturalscenes while simultaneously performing another attentionally demanding task. Bycomparison; they are unable to discriminate large T's from L's; or bisected two-color disksfrom their mirror images under the same conditions. We explore this phenonmenon …,*,2005,8
Progressive neural architecture search,Chenxi Liu; Barret Zoph; Jonathon Shlens; Wei Hua; Li-Jia Li; Li Fei-Fei; Alan Yuille; Jonathan Huang; Kevin Murphy,Abstract: We propose a method for learning CNN structures that is more efficient thanprevious approaches: instead of using reinforcement learning (RL) or genetic algorithms(GA); we use a sequential model-based optimization (SMBO) strategy; in which we searchfor architectures in order of increasing complexity; while simultaneously learning a surrogatefunction to guide the search; similar to A* search. On the CIFAR-10 dataset; our method findsa CNN structure with the same classification accuracy (3.41% error rate) as the RL methodof Zoph et al.(2017); but 2 times faster (in terms of number of models evaluated). It alsooutperforms the GA method of Liu et al.(2017); which finds a model with worse performance(3.63% error rate); and takes 5 times longer. Finally we show that the model we learned onCIFAR also works well at the task of ImageNet classification. In particular; we match the …,arXiv preprint arXiv:1712.00559,2017,7
Viewpoint invariant 3d human pose estimation with recurrent error feedback,Albert Haque; Boya Peng; Zelun Luo; Alexandre Alahi; Serena Yeung; F Li,Abstract. We propose a viewpoint invariant model for 3D human pose estimation from asingle depth image. To achieve viewpoint invariance; our deep discriminative modelembeds local regions into a learned viewpoint invariant feature space. Formulated as a multi-task learning problem; our model is able to selectively predict partial poses in the presenceof noise and occlusion. Our approach leverages a convolutional and recurrent network witha top-down error feedback mechanism to self-correct previous pose estimates in an end-to-end manner. We evaluate our model on a previously published depth dataset and a newlycollected human pose dataset containing 100K annotated depth images from extremeviewpoints. Experiments show that our model achieves competitive performance on frontalviews while achieving state-of-the-art performance on alternate viewpoints.,CoRR; abs/1603.07076,2016,7
Discovering voxel-level functional connectivity between cortical regions,C Baldasano; Marius Catalin Iordan; Diane M Beck; Li Fei-Fei,Abstract. Functional connectivity patterns are known to exist in the human brain at themillimeter scale; but the standard fMRI connectivity measure only computes functionalcorrelations at a coarse level. We present a method for identifying fine-grained functionalconnectivity between any two brain regions by simultaneously learning voxel-levelconnectivity maps over both regions. We show how to formulate this problem as aconstrained least-squares optimization; which can be solved using a trust region approach.Our method can automatically discover multiple correspondences between distinct voxelclusters in the two regions; even when these clusters have correlated timecourses. Wevalidate our method by identifying a known division in the lateral occipital complex usingonly functional connectivity; thus demonstrating that we can successfully learn subregion …,Machine Learning and Interpretation in NeuroImaging Workshop; NIPS,2012,7
Fine-Grained Car Detection for Visual Census Estimation.,Timnit Gebru; Jonathan Krause; Yilun Wang; Duyun Chen; Jia Deng; Li Fei-Fei,Abstract Targeted socio-economic policies require an accurate understanding of a country'sdemographic makeup. To that end; the United States spends more than 1 billion dollars ayear gathering census data such as race; gender; education; occupation and unemploymentrates. Compared to the traditional method of collecting surveys across many years which iscostly and labor intensive; data-driven; machine learningdriven approaches are cheaperand faster—with the potential ability to detect trends in close to real time. In this work; weleverage the ubiquity of Google Street View images and develop a computer vision pipelineto predict income; per capita carbon emission; crime rates and other city attributes from asingle source of publicly available visual data. We first detect cars in 50 million imagesacross 200 of the largest US cities and train a model to predict demographic attributes …,AAAI,2017,6
Characterizing and improving stability in neural style transfer,Agrim Gupta; Justin Johnson; Alexandre Alahi; Li Fei-Fei,Abstract Recent progress in style transfer on images has focused on improving the quality ofstylized images and speed of methods. However; real-time methods are highly unstableresulting in visible flickering when applied to videos. In this work we characterize theinstability of these methods by examining the solution set of the style transfer objective. Weshow that the trace of the Gram matrix representing style is inversely related to the stability ofthe method. Then; we present a recurrent convolutional network for real-time video styletransfer which incorporates a temporal consistency loss and overcomes the instability ofprior methods. Our networks can be applied at any resolution; do not require optical flow attest time; and produce high quality; temporally consistent stylized videos in real-time.,arXiv preprint arXiv:1705.02092,2017,6
Typicality sharpens category representations in object-selective cortex,Marius Cătălin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,Abstract The purpose of categorization is to identify generalizable classes of objects whosemembers can be treated equivalently. Within a category; however; some exemplars aremore representative of that concept than others. Despite long-standing behavioral effects;little is known about how typicality influences the neural representation of real-world objectsfrom the same category. Using fMRI; we showed participants 64 subordinate objectcategories (exemplars) grouped into 8 basic categories. Typicality for each exemplar wasassessed behaviorally and we used several multi-voxel pattern analyses to characterizehow typicality affects the pattern of responses elicited in early visual and object-selectiveareas: V1; V2; V3v; hV4; LOC. We found that in LOC; but not in early areas; typicalexemplars elicited activity more similar to the central category tendency and created …,NeuroImage,2016,6
Convolutional Neural Networks for Visual Recognition,Fei-Fei Li; Andrej Karpathy,*,*,2015,6
TRECVID 2012 GENIE: multimedia event detection and recounting,AG Amitha Perera; Sangmin Oh; P Megha; Tianyang Ma; Anthony Hoogs; Arash Vahdat; Kevin Cannons; Hossein Hajimirsadeghi; Greg Mori; Scott Mccloskey; Ben Miller; Sharath Venkatesha; Pedro Davalos; Pradipto Das; Chenliang Xu; Jason Corso; Rohini Srihari; Ilseo Kim; You-chi Cheng; Zhen Huang; Chin-hui Lee; Kevin Tang; L Fei-fei; Daphne Koller,Abstract Our MED 12 system is an extension of our MED 11 system [12]; and consists of acollection of low-level and high-level features; feature-specific classifiers built upon thosefeatures; and a fusion system that combines features both through mid-level kernel fusionand score fusion. We have incorporated large number of audio-visual features in our newsystem and incorporated diverse types of standard and newly developed event agentswhich learn the salient audio-visual characteristics of event classes. The combination ofadditional features and newly developed powerful event agents improve our MEDperformance substantially beyond our MED 11 results. In addition; our MER 12 submissionsreported recounting of specified clips for all five MER events and additionally provided MERresults for all the clips detected by MED system. Our MER system generated recounting of …,In TRECVID Workshop,2012,6
Scalable annotation of fine-grained categories without experts,Timnit Gebru; Jonathan Krause; Jia Deng; Li Fei-Fei,Abstract We present a crowdsourcing workflow to collect image annotations for visuallysimilar synthetic categories without requiring experts. In animals; there is a direct linkbetween taxonomy and visual similarity: eg a collie (type of dog) looks more similar to othercollies (eg smooth collie) than a greyhound (another type of dog). However; in syntheticcategories such as cars; objects with similar taxonomy can have very different appearance:eg a 2011 Ford F-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LLbut very different from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph basedcrowdsourcing algorithm to automatically group visually indistinguishable objects together.Using our workflow; we label 712;430 images by~ 1;000 Amazon Mechanical Turk workers;resulting in the largest fine-grained visual dataset reported to date with 2;657 categories …,Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,2017,5
Label Efficient Learning of Transferable Representations acrosss Domains and Tasks,Zelun Luo; Yuliang Zou; Judy Hoffman; Li F Fei-Fei,Abstract We propose a framework that learns a representation transferable across differentdomains and tasks in a data efficient manner. Our approach battles domain shift with adomain adversarial loss; and generalizes the embedding to novel task using a metriclearning-based approach. Our model is simultaneously optimized on labeled source dataand unlabeled or sparsely labeled data in the target domain. Our method shows compellingresults on novel classes within a new domain even when only a few labeled examples perclass are available; outperforming the prevalent fine-tuning approach. In addition; wedemonstrate the effectiveness of our framework on the transfer learning task from imageobject recognition to video action recognition.,Advances in Neural Information Processing Systems,2017,5
A hierarchical approach for generating descriptive image paragraphs,Jonathan Krause; Justin Johnson; Ranjay Krishna; Li Fei-Fei,Abstract: Recent progress on image captioning has made it possible to generate novelsentences describing images in natural language; but compressing an image into a singlesentence can describe visual content in only coarse detail. While one new captioningapproach; dense captioning; can potentially describe images in finer levels of detail bycaptioning many regions within an image; it in turn is unable to produce a coherent story foran image. In this paper we overcome these limitations by generating entire paragraphs fordescribing images; which can tell detailed; unified stories. We develop a model thatdecomposes both images and paragraphs into their constituent parts; detecting semanticregions in images and using a hierarchical recurrent neural network to reason aboutlanguage. Linguistic analysis confirms the complexity of the paragraph generation task …,arXiv preprint arXiv:1611.06607,2016,5
understanding recurrent networks. arXiv preprint,Andrej Karpathy; Justin Johnson; Fei-Fei Visualizing Li,*,arXiv preprint arXiv:1506.02078,2015,5
Integrating randomization and discrimination for classifying human-object interaction activities,Aditya Khosla; Bangpeng Yao; Li Fei-Fei,Abstract In this chapter we study the problem of classifying human–object interactionactivities in still images. The goal of our method is to explore fine image statistics and identifythe discriminative image patches for recognition. We achieve this goal by combining twoideas; discriminative feature mining and randomization. Discriminative feature mining allowsus to model the detailed information that distinguishes different classes of images; whilerandomization allows us to handle the huge feature space and prevent over-fitting. Wepropose a random forest with discriminative decision trees algorithm where every tree nodeis a discriminative classifier that is trained by combining the information in this node as wellas all upstream nodes. Besides human action recognition in still images; we also evaluateour method on subordinate categorization. Experimental results show that our method …,*,2014,5
Multi-level structured image coding on high-dimensional image representation,Li-Jia Li; Jun Zhu; Hao Su; Eric P Xing; Li Fei-Fei,Abstract Robust image representations such as classemes [1]; Object Bank (OB)[2]; spatialpyramid representation (SPM)[3] have been proposed; showing superior performance invarious high level visual recognition tasks. Our work is motivated by the need of exploringrich structural information encoded by these image representations. In this paper; wepropose a novel Multi-Level Structured Image Coding approach to uncover the structureembedded in representations with rich regular structural information by learning a structureddictionary from it. Specifically; we choose Object Bank [2] to demonstrate our algorithm sinceit encodes both semantics and spatial location as structural information. By using thelearned structured dictionary from Object Bank; we can compute a lower-dimensional andmore compact encoding of the image features while preserving and accentuating the rich …,Asian Conference on Computer Vision,2012,5
ImageNet: Constructing a large-scale image database,Li Fei-Fei; Jia Deng; Kai Li,Abstract Image dataset is a pivotal resource for vision research. We introduce here thepreview of a new dataset called “ImageNet”; a large-scale ontology of images built upon thebackbone of the WordNet structure. ImageNet aims to populate each of the majority of the80;000 synsets (concrete and countable nouns and their synonym sets) of WordNet with anaverage of 500–1000 clean images. This will result in tens of millions of annotated imagesorganized by the semantic hierarchy of WordNet. To construct ImageNet; we first collect alarge set of candidate images (about 10 thousands) from the Internet image search enginesfor each synset; which typically contains approximately 10% suitable images. We thendeploy an image annotation task on the online workers market Amazon Mechanical Turk. Toobtain a reliable rating; each image is evaluated by a dynamically determined number of …,Journal of Vision,2009,5
Categorization of good and bad examples of natural scene categories,Ana Torralbo; Barry Chai; Eamon Caddigan; Dirk Walther; Diane Beck; Li Fei-Fei,Abstract Despite the vast range of images that we might categorize as an example of aparticular natural scene category (eg; beach); human observers are able to quickly andefficiently categorize even briefly presented images of these scenes. However; within therange of images that we might categorize as a “beach”; for example; some will be morerepresentative of that category than others. We asked whether participants' ability tocategorize briefly presented scenes differed depending on whether the images were goodor bad examples of the scene. 3000 images from six categories (beaches; city streets;forests; highways; mountains and offices) were first rated by naïve subjects as good or badexamples of those categories. On the basis of these ratings; 50 good and 50 bad imageswere chosen from each category to be used in a categorization experiment; in which a …,Journal of Vision,2009,5
Mining discriminative adjectives and prepositions for natural scene recognition,Bangpeng Yao; Juan Carlos Niebles; Li Fei-Fei,This paper presents a method that considers not only patch appearances; but also patchrelationships in the form of adjectives and prepositions for natural scene recognition. Most ofthe existing scene categorization approaches only use patch appearances or co-occurrenceof patch appearances to determine the scene categories; but the relationships amongpatches remain ignored. Those relationships are; however; critical for recognition andunderstanding. For example; abeach'scene can be characterized by asky'regionabovesand'; and awater'region betweensky'andsand'. We believe that exploiting suchrelations between image regions can improve scene recognition. In our approach; eachimage is represented as a spatial pyramid; from which we obtain a collection of patchappearances with spatial layout information. We apply a feature mining approach to get …,Computer Vision and Pattern Recognition Workshops; 2009. CVPR Workshops 2009. IEEE Computer Society Conference on,2009,5
Can we see the shape of a mirror?,Silvio Savarese; Fei Fei Li; Pietro Perona,Abstract The three-dimensional shape of a surface may be perceived from a monocularstatic image. Contours; shading; texture gradients; perspective and occlusion are well-studied cues to this percept. When looking at the surface of a smooth reflecting object; suchas a well washed car; one additional cue is potentially available: the surrounding scene isreflected; and the deformation of this reflection is a function of the shape of the object'ssurface. Is this cue used by the human visual system? May it be used in isolation; ie whenother visual cues are not available?,Journal of Vision,2003,5
Fine-grained recognition in the wild: A multi-task domain adaptation approach,Timnit Gebru; Judy Hoffman; Li Fei-Fei,Abstract While fine-grained object recognition is an important problem in computer vision;current models are unlikely to accurately classify objects in the wild. These fully supervisedmodels need additional annotated images to classify objects in every new scenario; a taskthat is infeasible. However; sources such as e-commerce websites and field guides provideannotated images for many classes. In this work; we study fine-grained domain adaptationas a step towards overcoming the dataset shift between easily acquired annotated imagesand the real world. Adaptation has not been studied in the fine-grained setting whereannotations such as attributes could be used to increase performance. Our work uses anattribute based multi-task adaptation loss to increase accuracy from a baseline of 4.1% to19.1% in the semi-supervised adaptation case. Prior domain adaptation works have been …,2017 IEEE International Conference on Computer Vision (ICCV),2017,4
Visual semantic planning using deep successor representations,Yuke Zhu; Daniel Gordon; Eric Kolve; Dieter Fox; Li Fei-Fei; Abhinav Gupta; Roozbeh Mottaghi; Ali Farhadi,Abstract A crucial capability of real-world intelligent agents is their ability to plan a sequenceof actions to achieve their goals in the visual world. In this work; we address the problem ofvisual semantic planning: the task of predicting a sequence of actions from visualobservations that transform a dynamic environment from an initial state to a goal state. Doingso entails knowledge about objects and their affordances; as well as actions and theirpreconditions and effects. We propose learning these through interacting with a visual anddynamic environment. Our proposed solution involves bootstrapping reinforcement learningwith imitation learning. To ensure cross task generalization; we develop a deep predictivemodel based on successor representations. Our experimental results show near optimalresults across a wide range of tasks in the challenging THOR environment.,Proceedings of the IEEE International Conference on Computer Vision,2017,4
A glimpse far into the future: Understanding long-term crowd worker accuracy,Kenji Hata; Ranjay Krishna; Li Fei-Fei; Michael S Bernstein,ABSTRACT Microtask crowdsourcing is increasingly critical to the creation of extremelylarge datasets. As a result; crowd workers spend weeks or months repeating the exact sametasks—making it necessary to understand their behavior over these long periods of time. Weutilize three large; longitudinal datasets of nine million annotations collected from AmazonMechanical Turk to examine claims that workers fatigue or satisfice over these long periods;producing lower quality work. We find that; contrary to these claims; workers are extremelystable in their accuracy over the entire period. To understand whether workers set theiraccuracy based on the task's requirements for acceptance; we then perform an experimentwhere we vary the required accuracy for a large crowdsourcing task. Workers did not adjusttheir accuracy based on the acceptance threshold: workers who were above the …,CSCW: Computer-Supported Cooperative Work and Social Computing,2017,4
A glimpse far into the future: Understanding long-term crowd worker quality,Kenji Hata; Ranjay Krishna; Li Fei-Fei; Michael S Bernstein,Abstract: Microtask crowdsourcing is increasingly critical to the creation of extremely largedatasets. As a result; crowd workers spend weeks or months repeating the exact same tasks;making it necessary to understand their behavior over these long periods of time. We utilizethree large; longitudinal datasets of nine million annotations collected from AmazonMechanical Turk to examine claims that workers fatigue or satisfice over these long periods;producing lower quality work. We find that; contrary to these claims; workers are extremelystable in their quality over the entire period. To understand whether workers set their qualitybased on the task's requirements for acceptance; we then perform an experiment where wevary the required quality for a large crowdsourcing task. Workers did not adjust their qualitybased on the acceptance threshold: workers who were above the threshold continued …,arXiv preprint arXiv:1609.04855,2016,4
Natural scene categorization in the near absence of attention: further explorations,Fei Fei Li; Rufin VanRullen; Christof Koch; Pietro Perona,Abstract Subjects are able to detect quickly animals and vehicles in previously unseencluttered scenes presented peripherally even when their attention is distracted. They are;however; unable to discriminate rotated letters (T/L) and bisected color disks (Red/Green) inthe same conditions (Li et al; PNAS 02). We explore this phenomenon further by variationsof the original experiment.,Journal of Vision,2003,4
Vision-based classification of developmental disorders using eye-movements,Guido Pusiol; Andre Esteva; Scott S Hall; Michael Frank; Arnold Milstein; Li Fei-Fei,Abstract This paper proposes a system for fine-grained classification of developmentaldisorders via measurements of individuals' eye-movements using multi-modal visual data.While the system is engineered to solve a psychiatric problem; we believe the underlyingprinciples and general methodology will be of interest not only to psychiatrists but toresearchers and engineers in medical machine vision. The idea is to build features fromdifferent visual sources that capture information not contained in either modality. Using aneye-tracker and a camera in a setup involving two individuals speaking; we build temporalattention features that describe the semantic location that one person is focused on relativeto the other person's face. In our clinical context; these temporal attention features describe apatient's gaze on finely discretized regions of an interviewing clinician's face; and are …,International Conference on Medical Image Computing and Computer-Assisted Intervention,2016,3
Method and system for optimizing accuracy-specificity trade-offs in large scale visual recognition,*,As visual recognition scales up to ever larger numbers of categories; maintaining highaccuracy is increasingly difficult. Embodiment of the present invention include methods foroptimizing accuracy-specificity trade-offs in large scale recognition where object categoriesform a semantic hierarchy consisting of many levels of abstraction.,*,2015,3
TRECVID 2013 GENIE: Multimedia Event Detection and Recounting,Sangmin Oh; AG Amitha Perera; Ilseo Kim; Megha Pandey; Kevin Cannons; Hossein Hajimirsadeghi; Arash Vahdat; Greg Mori; Ben Miller; Scott McCloskey; You-Chi Cheng; Zhen Huang; Chin-Hui Lee; Chenliang Xu; Rohit Kumar; Wei Chen; Jason Corso; L Fei-Fei; Daphne Koller; Vignesh Ramanathan; Kevin Tang; Armand Joulin; Alexandre Alahi,Abstract Our MED 13 system is an extension of our MED 12 system [12; 13]; and consists ofa collection of lowlevel and high-level features; feature-specific classifiers built upon thosefeatures; and a fusion system that combines features both through mid-level kernel fusionand late fusion. Our MED submissions include total of 24 different configurations whichconsist of combinations of 2 submission timings (PS/AH); 3 training conditions (100/10/0Ex);and 4 types of feature conditions (Full/Visual/Audio/ASR). Our MER 13 submissions reportedrecounting for all five MER events. Our MER system combines evidences from multiple baseclassifiers; which are translated to texts and used to identify key frames. Multiple MER resultsare fused and presented to users as recounting for each detection.,Parade,2013,3
Mid-level features improve recognition of interactive activities,Kate Saenko; Ben Packer; C Chen; S Bandla; Y Lee; Yangqing Jia; J Niebles; D Koller; L Fei-Fei; K Grauman; Trevor Darrell,Abstract: We argue that mid-level representations can bridge the gap between existing low-level models; which are incapable of capturing the structure of interactive verbs; andcontemporary high-level schemes; which rely on the output of potentially brittle intermediatedetectors and trackers. We develop a novel descriptor based on generic object foregroundsegments our representation forms a histogram-of-gradient representation that is groundedto the frame of detected key-segments. Importantly; our method does not require objects tobe identi ed reliably in order to compute a ro-bust representation. We evaluate an integratedsystem including novel key-segment activity descriptors on a large-scale video datasetcontaining 48 common verbs; for which we present a comprehensive evaluation protocol.Our results con rm that a descriptor de ned on mid-level primitives operating at a higher …,*,2012,3
Method for Implementing a High-Level Image Representation for Image Analysis,*,Robust low-level image features have been proven to be effective representations for avariety of visual recognition tasks such as object recognition and scene classification; butpixels; or even local image patches; carry little semantic meanings. For high-level visualtasks; such low-level image representations are potentially not enough. The presentinvention provides a high-level image representation where an image is represented as ascale-invariant response map of a large number of pre-trained generic object detectors;blind to the testing dataset or visual task. Leveraging on this representation; superiorperformances on high-level visual recognition tasks are achieved with relatively classifierssuch as logistic regression and linear SVM classifiers.,*,2012,3
Towards total scene understanding: Classification; annotation and segmentation in an automatic framework. 2009 IEEE CVPR,R Socher,*,*,2009,3
Natural Scene Categorization: from Humans to Computers,Li Fei-Fei,An outdoor scene; I think. reminded me aa city... like walkingin a park in new york orsomething. there seemed to be trees and a road and then this large skyscraper in thebackground … Reference: Li et al. 2002; Fei-Fei et al. 2005 … 1. How critical is attention innatural scene … 2. How does this compare to other recognition tasks … • #2: what can weperceive within a glance … What do people see in a glance … An outdoor scene; I think. remindedme a a city... like walkingin a park in new york or something. there seemed to be trees and aroad and then this large skyscraper in the background … Reference: Fei-Fei et al. CVPR 2005… Of all the sensory impressions proceeding to the brain; the visual experiences are the dominantones. Our perception of the world around us is based essentially on the messages that reachthe brain from our eyes. For a long time it was thought that the retinal image was …,Scene Understanding Symposium,2007,3
Visual recognition: computational models and human psychophysics,Fei-Fei Li,Object and scene recognition is one of the most essential functionalities of human vision. It isalso of fundamental importance for machines to be able to learn and recognize meaningfulobjects and scenes. In this thesis; we explore the following four aspects of object and scenerecognition. It is well known that humans can be" blind" even to major aspects of naturalscenes when we attend elsewhere. The only tasks that do not need attention appear to becarried out in the early stages of the visual system. Contrary to this common belief; we showthat subjects can rapidly detect animals or vehicles in briefly presented novel natural sceneswhile simultaneously performing another attentionally demanding task. By comparison; theyare unable to discriminate large T's from L's; or bisected two-color disks from their mirrorimages under the same conditions. We explore this phenonmenon further by removing …,*,2005,3
Visual Genome,Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma; Michael S Bernstein; Li Fei-Fei,*,*,*,3
Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems,James Harrison; Animesh Garg; Boris Ivanovic; Yuke Zhu; Silvio Savarese; Li Fei-Fei; Marco Pavone,Abstract: Model-free policy learning has enabled robust performance of complex tasks withrelatively simple algorithms. However; this simplicity comes at the cost of requiring an Oracleand arguably very poor sample complexity. This renders such methods unsuitable forphysical systems. Variants of model-based methods address this problem through the use ofsimulators; however; this gives rise to the problem of policy transfer from simulated to thephysical system. Model mismatch due to systematic parameter shift and unmodelleddynamics error may cause sub-optimal or unsafe behavior upon direct transfer. Weintroduce the Adaptive Policy Transfer for Stochastic Dynamics (ADAPT) algorithm thatachieves provably safe and robust; dynamically-feasible zero-shot transfer of RL-policies tonew domains with dynamics error. ADAPT combines the strengths of offline policy …,arXiv preprint arXiv:1707.04674,2017,2
Evidence for similar patterns of neural activity elicited by picture-and word-based representations of natural scenes,Manoj Kumar; Kara D Federmeier; Li Fei-Fei; Diane M Beck,Abstract A long-standing core question in cognitive science is whether different modalitiesand representation types (pictures; words; sounds; etc.) access a common store of semanticinformation. Although different input types have been shown to activate a shared network ofbrain regions; this does not necessitate that there is a common representation; as theneurons in these regions could still differentially process the different modalities. However;multi-voxel pattern analysis can be used to assess whether; eg; pictures and words evoke asimilar pattern of activity; such that the patterns that separate categories in one modalitytransfer to the other. Prior work using this method has found support for a common code; buthas two limitations: they have either only examined disparate categories (eg animals vs.tools) that are known to activate different brain regions; raising the possibility that the …,Neuroimage,2017,2
Knowledge acquisition for visual question answering via iterative querying,Yuke Zhu; Joseph J Lim; Li Fei-Fei,Abstract Humans possess an extraordinary ability to learn new skills and new knowledge forproblem solving. Such learning ability is also required by an automatic model to deal witharbitrary; open-ended questions in the visual world. We propose a neural-based approachto acquiring taskdriven information for visual question answering (VQA). Our modelproposes queries to actively acquire relevant information from external auxiliary data.Supporting evidence from either human-curated or automatic sources is encoded and storedinto a memory bank. We show that acquiring task-driven evidence effectively improvesmodel performance on both the Visual7W and VQA datasets; moreover; these queries offercertain level of interpretability in our iterative QA model.,The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017,2
Tackling over-pruning in variational autoencoders,Serena Yeung; Anitha Kannan; Yann Dauphin; Li Fei-Fei,Abstract: Variational autoencoders (VAE) are directed generative models that learn factoriallatent variables. As noted by Burda et al.(2015); these models exhibit the problem of factorover-pruning where a significant number of stochastic factors fail to learn anything andbecome inactive. This can limit their modeling power and their ability to learn diverse andmeaningful latent representations. In this paper; we evaluate several methods to addressthis problem and propose a more effective model-based approach called the epitomicvariational autoencoder (eVAE). The so-called epitomes of this model are groups of mutuallyexclusive latent factors that compete to explain the data. This approach helps preventinactive units since each group is pressured to explain the data. We compare theapproaches with qualitative and quantitative results on MNIST and TFD datasets. Our …,arXiv preprint arXiv:1706.03643,2017,2
Adversarially robust policy learning: Active construction of physically-plausible perturbations,Ajay Mandlekar; Yuke Zhu; Animesh Garg; Li Fei-Fei; Silvio Savarese,Abstract—Policy search methods in reinforcement learning have demonstrated success inscaling up to larger problems beyond toy examples. However; deploying these methods onreal robots remains challenging due to the large sample complexity required during learningand their vulnerability to malicious intervention. We introduce Adversarially Robust PolicyLearning (ARPL); an algorithm that leverages active computation of physically-plausibleadversarial examples during training to enable robust policy learning in the source domainand robust performance under both random and adversarial input perturbations. Weevaluate ARPL on four continuous control tasks and show superior resilience to changes inphysical environment dynamics parameters and environment state as compared to state-of-the-art robust policy learning methods. Code; data; and additional experimental results …,IEEE International Conference on Intelligent Robots and Systems (to appear),2017,2
Learning to Predict Human Behavior in Crowded Scenes,Alexandre Alahi; Vignesh Ramanathan; Kratarth Goel; Alexandre Robicquet; Amir A Sadeghian; Li Fei-Fei; Silvio Savarese,Abstract Pedestrians follow different trajectories to avoid obstacles and accommodate fellowpedestrians. Any autonomous vehicle navigating such a scene should be able to foresee thefuture positions of pedestrians and accordingly adjust its path to avoid collisions. Thisproblem of trajectory prediction can be viewed as a sequence generation task; where we areinterested in predicting the future trajectory of people based on their past positions.Following the recent success of Recurrent Neural Network (RNN) models for sequenceprediction tasks; we propose an LSTM model which can learn general human movementand predict their future trajectories. This is in contrast to traditional approaches which usehand-crafted functions such as Social Forces. We demonstrate the performance of ourmethod on several public datasets. Our model outperforms state-of-the-art methods on …,*,2017,2
Epitomic Variational Autoencoders,Serena Yeung; Anitha Kannan; Yann Dauphin; Li Fei-Fei,Abstract: In this paper; we propose epitomic variational autoencoder (eVAE); a probabilisticgenerative model of high dimensional data. eVAE is composed of a number of sparsevariational autoencoders calledepitome'such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposedmodel greatly overcomes the common problem in variational autoencoders (VAE) of modelover-pruning. We substantiate that eVAE is efficient in using its model capacity andgeneralizes better than VAE; by presenting qualitative and quantitative results on MNISTand TFD datasets.,*,2016,2
Locally-Optimized Inter-Subject Alignment of Functional Cortical Regions,Marius Cătălin Iordan; Armand Joulin; Diane M Beck; Li Fei-Fei,Abstract: Inter-subject registration of cortical areas is necessary in functional imaging (fMRI)studies for making inferences about equivalent brain function across a population. However;many high-level visual brain areas are defined as peaks of functional contrasts whosecortical position is highly variable. As such; most alignment methods fail to accurately mapfunctional regions of interest (ROIs) across participants. To address this problem; wepropose a locally optimized registration method that directly predicts the location of a seedROI on a separate target cortical sheet by maximizing the functional correlation betweentheir time courses; while simultaneously allowing for non-smooth local deformations inregion topology. Our method outperforms the two most commonly used alternatives(anatomical landmark-based AFNI alignment and cortical convexity-based FreeSurfer …,arXiv preprint arXiv:1606.02349,2016,2
Visual noise from natural scene statistics reveals human scene category representations,Michelle R Greene; Abraham P Botros; Diane M Beck; Li Fei-Fei,Abstract: Our perceptions are guided both by the bottom-up information entering our eyes;as well as our top-down expectations of what we will see. Although bottom-up visualprocessing has been extensively studied; comparatively little is known about top-downsignals. Here; we describe REVEAL (Representations Envisioned Via EvolutionaryALgorithm); a method for visualizing an observer's internal representation of a complex; real-world scene; allowing us to; for the first time; visualize the top-down information in anobserver's mind. REVEAL rests on two innovations for solving this high dimensionalproblem: visual noise that samples from natural image statistics; and a computer algorithmthat collaborates with human observers to efficiently obtain a solution. In this work; wevisualize observers' internal representations of a visual scene category (street) using an …,arXiv preprint arXiv:1411.5331,2014,2
Object-centric spatial pooling for image classification,*,A method is provided for classifying an image. The method includes inferring locationinformation of an object of interest in an input representation of the image. The methodfurther includes determining foreground object features and background object features fromthe input representation of the image. The method additionally includes pooling theforeground object features separately from the background object features using the locationinformation to form a new representation of the image. The new representation is differentthan the input representation of the image. The method also includes classifying the imagebased on the new representation of the image.,*,2014,2
Bag of words models,Svetlana Lazebnik; A Torralba; L Fei-Fei; D Lowe; C Szurka,*,Dostopno na: http://cs. nyu. edu/~ fergus/teaching/vision_2012/9_BoW. pdf,2011,2
Recognizing and learning object categories: Year 2009,Li Fei-Fei,モーダル間の共起関係を考慮した階層的トピック軌跡モデルによる映像認識検索 (IBIS2010(情報論的学習理論ワークショップ)),ICCV2009 tutorial,2009,2
Variational shift invariant probabilistic PCA for face recognition,Jilin Tu; Aleksandar Ivanovic; Xun Xu; Li Fei-Fei; Thomas Huang,While PCA learns a subspace that captures the variations of the data; it assumes thecollected data is well pre-processed (ie; the pictures for faces are aligned by eye corners);this usually introduces a huge mount of manual labor for human. While people have beendeveloping automatic eye alignment tools for such purpose; detecting eyes with robustnessand accuracy is still an open problem for research. We propose to learn PCA while at thesame time eliminating the mis-alignment in the data. We formulate the PCA model in agenerative framework; and introduce the mis-alignment as a hidden variable in the model. Anovel variational message passing (J. Winn and C. Bishop; 2004) update rules is thenderived to learn the parameters. The experiments show that the performance of PCA basedface recognition is significantly improved by our algorithm when misalignments exist,Pattern Recognition; 2006. ICPR 2006. 18th International Conference on,2006,2
Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States,Timnit Gebru; Jonathan Krause; Yilun Wang; Duyun Chen; Jia Deng; Erez Lieberman Aiden; Li Fei-Fei,Abstract The United States spends more than $250 million each year on the AmericanCommunity Survey (ACS); a labor-intensive door-to-door study that measures statisticsrelating to race; gender; education; occupation; unemployment; and other demographicfactors. Although a comprehensive source of data; the lag between demographic changesand their appearance in the ACS can exceed several years. As digital imagery becomesubiquitous and machine vision techniques improve; automated data analysis may becomean increasingly practical supplement to the ACS. Here; we present a method that estimatessocioeconomic characteristics of regions spanning 200 US cities by using 50 million imagesof street scenes gathered with Google Street View cars. Using deep learning-basedcomputer vision techniques; we determined the make; model; and year of all motor …,Proceedings of the National Academy of Sciences,2017,1
Neural task programming: Learning to generalize across hierarchical tasks,Danfei Xu; Suraj Nair; Yuke Zhu; Julian Gao; Animesh Garg; Li Fei-Fei; Silvio Savarese,Abstract: In this work; we propose a novel robot learning framework called Neural TaskProgramming (NTP); which bridges the idea of few-shot learning from demonstration andneural program induction. NTP takes as input a task specification (eg; video demonstrationof a task) and recursively decomposes it into finer sub-task specifications. Thesespecifications are fed to a hierarchical neural program; where bottom-level programs arecallable subroutines that interact with the environment. We validate our method in threerobot manipulation tasks. NTP achieves strong generalization across sequential tasks thatexhibit hierarchal and compositional structures. The experimental results show that NTPlearns to generalize well to-wards unseen tasks with increasing lengths; variable topologies;and changing objectives. Subjects: Artificial Intelligence (cs. AI); Learning (cs. LG); …,arXiv preprint arXiv:1710.01813,2017,1
Unsupervised visual-linguistic reference resolution in instructional videos,De-An Huang; Joseph J Lim; Li Fei-Fei; Juan Carlos Niebles,Abstract We propose an unsupervised method for reference resolution in instructionalvideos; where the goal is to temporally link an entity (eg;“dressing”) to the action (eg;“mixyogurt”) that produced it. The key challenge is the inevitable visual-linguistic ambiguitiesarising from the changes in both visual appearance and referring expression of an entity inthe video. This challenge is amplified by the fact that we aim to resolve references with nosupervision. We address these challenges by learning a joint visuallinguistic model; wherelinguistic cues can help resolve visual ambiguities and vice versa. We verify our approachby learning our model unsupervisedly using more than two thousand unstructured cookingvideos from YouTube; and show that our visual-linguistic model can substantially improveupon state-of-the-art linguistic only model on reference resolution in instructional videos.,arXiv preprint arXiv:1703.02521,2017,1
Jointly Learning Energy Expenditures and Activities using Egocentric Multimodal Signals,Katsuyuki Nakamura; Serena Yeung; Alexandre Alahi; Li Fei-Fei,Abstract Physiological signals such as heart rate can provide valuable information about anindividual's state and activity. However; existing work on computer vision has not yetexplored leveraging these signals to enhance egocentric video understanding. In this work;we propose a model for reasoning on multimodal data to jointly predict activities and energyexpenditures. We use heart rate signals as privileged self-supervision to derive energyexpenditure in a training stage. A multitask objective is used to jointly optimize the two tasks.Additionally; we introduce a dataset that contains 31 hours of egocentric video augmentedwith heart rate and acceleration signals. This study can lead to new applications such as avisual calorie counter.,IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2017,1
Toward More Gender Diversity in CS through an Artificial Intelligence Summer Program for High School Girls,Marie E Vachovsky; Grace Wu; Sorathan Chaturapruek; Olga Russakovsky; Richard Sommer; Li Fei-Fei,Abstract The field of computer science suffers from a lack of diversity. The Stanford ArtificialIntelligence Laboratory's Outreach Summer (SAILORS); a two-week non-residential freesummer program; recruits high school girls to computer science; specifically to ArtificialIntelligence (AI). The program was organized by graduate student and professor volunteers.The goals of the pilot program are to increase interest in AI; contextualize technicallyrigorous AI concepts through societal impact; and address barriers that could discourage10th grade girls from pursuing computer science. In this paper we describe the curriculumdesigned to achieve these goals. Survey results show students had a statistically significantincrease in technical knowledge; interest in pursuing careers in AI; and confidence insucceeding in AI and computer science. Additionally; survey results show that the majority …,Proceedings of the 47th ACM Technical Symposium on Computing Science Education,2016,1
Vision-Based Hand Hygiene Monitoring in Hospitals.,Serena Yeung; Alexandre Alahi; Albert Haque; Boya Peng; Zelun Luo; Amit Singh; Terry Platchek; Arnold Milstein; Fei-Fei Li,Abstract Recent progress in developing cost-effective depth sensors has enabled new AI-assisted solutions such as assisted driving vehicles and smart spaces. Machine learningtechniques have been successfully applied on these depth signals to perceive meaningfulinformation about human behavior. In this work; we propose to deploy depth sensors inhospital settings and use computer vision methods to enable AI-assisted care. We aim toreduce visually-identifiable human errors such as hand hygiene compliance; one of theleading causes of Health Care-Associated Infection (HCAI) in hospitals.,AMIA,2016,1
Two distinct scene processing networks connecting vision and memory,Christopher Baldassano; Andre Esteva; Diane Beck; Li Fei-Fei,*,Journal of vision,2015,1
Visual census: Using cars to study people and society,Timnit Gebru; Jonathan Krause; Yilun Wang; Duyun Chen; Jia Deng; Li Fei-Fei,The cars people own can provide significant personal information: by knowing that theperson in Fig. 1 drives a Prius we can guess that he or she is probably from San Franciscoand earns an income of approximately $72 k/year. A few pioneering works by Zhou et al.;Ordonez et al.; and Naik et al. have started to apply visual scene analysis techniques to infercharacteristics of neighborhoods and cities [8; 4; 6; 7]. In this work; we are also interested inusing images to understand cities; neighborhoods and the demographic makeup of theirinhabitants. However; instead of using global image statistics; we achieve this goal bydetecting and classifying cars on the street (Fig. 2). 95% of American households own cars[1]; and as seen in Fig. 1 cars give a lot of information about individuals as well asneighborhoods. Our contributions are two-fold. First; we offer the largest fine-grained …,*,2015,1
Affordances Provide a Fundamental Categorization Principle for Visual Scenes,Michelle R Greene; Christopher Baldassano; Andre Esteva; Diane M Beck; Li Fei-Fei,Abstract: How do we know that a kitchen is a kitchen by looking? Relatively little is knownabout how we conceptualize and categorize different visual environments. Traditionalmodels of visual perception posit that scene categorization is achieved through therecognition of a scene's objects; yet these models cannot account for the mounting evidencethat human observers are relatively insensitive to the local details in an image.Psychologists have long theorized that the affordances; or actionable possibilities of astimulus are pivotal to its perception. To what extent are scene categories created fromsimilar affordances? Using a large-scale experiment using hundreds of scene categories;we show that the activities afforded by a visual scene provide a fundamental categorizationprinciple. Affordance-based similarity explained the majority of the structure in the human …,arXiv preprint arXiv:1411.5340,2014,1
Supervoxel parcellation of visual cortex connectivity,Christopher Baldassano; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2014,1
Social Role Recognition for Human Event Understanding,Vignesh Ramanathan; Bangpeng Yao; Li Fei-Fei,Abstract We deal with the problem of recognizing social roles played by people in an event.Social roles are governed by human interactions; and form a fundamental component ofhuman event description. We focus on a weakly supervised setting; where we are providedwith different videos belonging to an event class; without training role labels. Since socialroles are described by the interaction between people in an event; we propose aConditional Random Field to model the inter-role interactions; along with person-specificsocial descriptors. We develop tractable variational inference to simultaneously infer modelweights; as well as role assignment to all people in the videos. We also present a novelYouTube social roles dataset with ground truth role annotations; and introduce annotationson a subset of videos from the TRECVID-MED11 event kits for evaluation purposes. The …,*,2014,1
Oddness at a glance: Unraveling the time course of typical and atypical scene perception,Abraham Botros; Michelle Greene; Li Fei-Fei,This shows two girls seated around a round white table in what may be a fast-foodrestaurant. They are asian. The girl on the left is wearing a black polka-dotted skirt with whitedots. THe skirt is to her knee. The girl on the right is wearing short. The scene is red color inthe background. There is a low wall blocking the table and the girls from the rest of therestaurant.,Journal of Vision,2013,1
Real-world objects acquire basic-level advantage in occipito-temporal cortex,Marius Cãtãlin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,Page 1. Ideal Sub. Ideal Basic Ideal Super. * ns ** ** * V1 * ns *** ** V2 * *** * V3v * ns *** **hV4 * *** * TOS * *** * PPA * * *** RSC * * *** FFA * *** * LOC 0.1 0.2 0.3 0 C O R R ELATIO N W IT H ID EAL MAT R IX (R ) * * SIMILAR IT Y MATRIX 0.1 0.2 0.3 0 Percentile of r 0100 Ideal Subordinate Ideal Basic Ideal Superordinate ns *** ** V1 ** * * V2 * * V3v ** ** *hV4 *** ** TOS *** ** PPA ** ** RSC ** ** * FFA *** ** LOC 0.03 0.06 0.09 0.12 0.00 WITHINCA T EG O R Y SIMILAR IT Y (R ) 0.03 0.06 0.09 0.12 0.00 Subordinate Basic Superordinate*** ** 0.06 -0.03 BET W EEN C A T EG O R Y SIMILAR IT Y (R ) -0.06 -0.03 *** ** 0.15 0.15 * **** 0.18 0.18 *** ** 0.09 -0.09 ns ns ns Real-world objects acquire basic-level advantage inoccipito-temporal cortex Marius Cãtãlin Iordan …,Bay Area Vision Research Day (BAVRD),2013,1
Neural Representations of Object Categories at Multiple Taxonomic Levels,Marius Cătălin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2012,1
A simple method of voltage flicker estimation based on interharmonics,Fei-Fei Li; Hong-Geng Yang; Jin Hui; Mao-Qing Ye,This paper proposes a new method to estimate voltage flicker using the discrete Fouriertransform; which obtains the parameters of voltage flicker while estimating interharmonics.The proposed method obtains the amplitude; frequency and phase angle of each frequencycomponent using the odd points interpolation correction method; which reduces the leakagewhile interpolating; based on the specified signal processing recommendations byIEC61000-4-7. According to the concept of voltage flicker; the double-side property ofinterharmonic pairs converted by voltage flicker is obtained. Considering the maximumperceptible frequency range of the voltage flicker; the frequency range of interharmonicspairs can be obtained. According to the electric power distortion component symbiosismechanism; we can differentiate the interharmonic pairs from the common interharmonic …,Power System Protection and Control,2011,1
Translation Invariance of Natural Scene Categories,Marius Catalin Iordan; Christopher Baldassano; Dirk B Walther; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2011,1
Decoding objects undergoing contextual violations,Christopher Baldassano; Marius Catalin Iordan; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2011,1
Finding “good” features for natural scene classification,Eamon Caddigan; Dirk B Walther; Barry Chai; Diane M Beck; Li Fei-Fei,Finding “good” features for natural scene classification … Eamon Caddigan1;2; Dirk B.Walther2; Barry Chai3; Diane M. Beck1;2; Li Fei-Fei3;4 … 1Department of Psychology; Universityof Illinois at Urbana-Champaign; 2Beckman Institute; University of Illinois atUrbana-Champaign; 3Computer Science Department; Princeton University; 4PsychologyDepartment; Princeton University … Fei-Fei; L.; & Perona; P. (2005). A Bayesian HierarchicalModel for Learning Natural Scene Categories. IEEE Comp. Vis. Patt. Recog … Fei-Fei; L.;VanRullen; R.; Koch; C.; & Perona; P. (2002). Rapid natural scene categorization in the nearabsence of attention. Proc. Natl. Acad. Sci … Hoiem; D.; Efros; AA; & Hebert; M. (2005). GeometricContext from a Single Image. ICCV … Lazebnik; S.; Schmid; C. & Ponce; J. (2006). BeyondBags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories …,Journal of Vision,2009,1
Searchlight analysis reveals brain areas involved in scene categorization,Dirk Walther; Eamon Caddigan; Diane Beck; Li Fei-Fei,Abstract Our ability to categorize natural scenes is essential for visual tasks such asnavigation or the recognition of objects in their natural environment. Although differentclasses of natural scenes often share similar image statistics; human subjects are extremelyefficient at categorizing natural scenes. In order to map out the brain regions involved inscene categorization; we use multivariate pattern recognition to analyze the fMRI activationwithin a small spherical region (the searchlight; Kriegeskorte et al. 2006) that is positioned atevery possible location in the brain. From local activity patterns in each searchlight; weattempt to predict the scene category that the subject viewed during the experiment. Such ananalysis allows us to generate a spatial map of those brain regions producing the highestclassification accuracy. Furthermore; we can generate similar maps of the correlation of …,Journal of Vision,2009,1
Variational transform invariant mixture of probabilistic pca,Jilin Tu; Yun Fu; Alexandar Ivanovic; Thomas S Huang; Li Fei-Fei,In many video-based object recognition applications; the object appearances are acquiredby visual tracking or detection and are inconsistent due to misalignments. We believe themisalignments can be removed if we can reduce the inconsistency in the objectappearances caused by misalignments through clustering the objects in appearance; spaceand time domain simultaneously. We therefore propose to learn Transform InvariantMixtures of Probabilistic PCA (TIMPPCA) model from the data while at the same timeeliminating the misalignments. The model is formulated in a generative framework; and themisalignments are considered as hidden variables in the model. Variational EM updaterules are then derived based on Variational Message Passing (VMP) techniques. Theproposed TIMP-PCA is applied to improve head pose estimation performance and to …,Applications of Computer Vision; 2008. WACV 2008. IEEE Workshop on,2008,1
One Step Beyond Histogram: Image Representation by MarkOV Stationary Features,L Fei-Fei,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Proc. of IEEE Computer Vision and Pattern Recognition; 2008,2008,1
5 Integration or erasure?,Vincanne Adams; Fei-Fei Li,As most readers of this volume will already know; Tibetan medicine has been integrative;from the outset. Tibetan medicine; or what is sometimes called Rgyud bzhimedicine;emerged out of a multi-century effort to integrate elements of Indian; Persian; Chinese; andindigenous shamanic with Buddhist scriptural insights on the relationship between healthand the body (Meyer 1981; 1992; Dummer 1988). Moreover; since at least the seventeenthcentury; Tibetan medicine was based; at a minimum; on the textual forms of the Four Tantras(Rgyud bzhi) but also the important commentaries written by skilled medical practitioners inthe centuries before and after (Zhurkar; Lodri Gyalbo; Desi Sangye Gyatso; and the like)which surely modified these texts (intentionally or not) in the process of selectiveelaboration; exclusion; and clarification in relation to folk and village-level beliefs and …,Tibetan Medicine in the Contemporary World: Global Politics of Medical Knowledge and Practice,2008,1
OPTIMOL: a framework for online picture collection via incremental model learning,Li-Jia Li; Juan Carlos Niebles; Li Fei-Fei,Abstract OPTIMOL (a framework for Online Picture collecTion via Incremental MOdelLearning) is a novel; automatic dataset collecting and model learning system for objectcategorization. Our algorithm mimics the human learning process in such a way that; startingfrom a few training examples; the more confident data you incorporate in the training data;the more reliable models can be learnt. Our system uses the Internet as the (nearly)unlimited resource for images. The learning and image collection processes are done via aniterative and incremental scheme. The goal of this work is to use this tremendous webresource to learn robust object category models in order to detect and search for objects inreal-world scenes.,PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE,2007,1
Detection of objects in natural scenes with minimal or no attention,Fei Fei Li; Rufin VanRullen; Christof Koch; Pietro Perona,Abstract Attention plays a critical role in modulating visual information that eventuallyreaches our visual awareness. What can we see when we do not pay attention? Change-blindness experiments have demonstrated that one can be “blind” even to changes in majoraspects of a natural scene. The only tasks that need minimal or no attention appear to bethose carried out in the early stages of the visual system. Contrary to this common belief; wereport that subjects can rapidly detect highly variable objects (eg animals or vehicles) inbriefly presented novel natural scenes while simultaneously performing another attentionallydemanding task (a five-letter form discrimination task). By comparison; they are unable todiscriminate large 'T's from 'L's or a bisected two-color disk from its mirror image under thesame condition. This ability does not depend on the fact that our subjects are extensively …,Journal of Vision,2002,1
Learning semantic relationships for better action retrieval in images (Supplementary),Vignesh Ramanathan; Congcong Li; Jia Deng; Wei Han; Zhen Li; Kunlong Gu; Yang Song; Samy Bengio; Chuck Rossenberg; Li Fei-Fei,The original Stanford 40 actions dataset [5] has a carefully chosen set of 40 actions whichare mutually exclusive of each other. This makes the dataset less applicable for large scalesettings such as ours. Nevertheless; in order to demonstrate results on this dataset; weextend it with 41 additional action labels as explained below. Setup We introduce 41 newaction labels to this dataset. The additional actions are chosen such that they are impliedbyone or more of the original 40 actions. The newly added labels are shown in Tab. 2. Againsteach of the original 40 actions; we show the set of newly added actions which are implied bythis original action. We follow the experimental protocol form Deng et al.[2] and “relabel” asubset of the images to the newly added actions. More precisely; we relabel 50% of theimages belonging to an original action to one of the newly introduced actions which is …,*,*,1
Tool Detection and Operative Skill Assessment in Surgical Videos Using Region-Based Convolutional Neural Networks,Amy Jin; Serena Yeung; Jeffrey Jopling; Jonathan Krause; Dan Azagury; Arnold Milstein; Li Fei-Fei,Abstract: Five billion people in the world lack access to quality surgical care. Surgeon skillvaries dramatically; and many surgical patients suffer complications and avoidable harm.Improving surgical training and feedback would help to reduce the rate of complications; halfof which have been shown to be preventable. To do this; it is essential to assess operativeskill; a process that currently requires experts and is manual; time consuming; andsubjective. In this work; we introduce an approach to automatically assess surgeonperformance by tracking and analyzing tool movements in surgical videos; leveraging region-based convolutional neural networks. In order to study this problem; we also introduce anew dataset; m2cai16-tool-locations; which extends the m2cai16-tool dataset with spatialbounds of tools. While previous methods have addressed tool presence detection; ours is …,arXiv preprint arXiv:1802.08774,2018,*
Emergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation,Nick Haber; Damian Mrowca; Li Fei-Fei; Daniel LK Yamins,Abstract: Infants are experts at playing; with an amazing ability to generate novel structuredbehaviors in unstructured environments that lack clear extrinsic reward signals. We seek toreplicate some of these abilities with a neural network that implements curiosity-drivenintrinsic motivation. Using a simple but ecologically naturalistic simulated environment inwhich the agent can move and interact with objects it sees; the agent learns a world modelpredicting the dynamic consequences of its actions. Simultaneously; the agent learns to takeactions that adversarially challenge the developing world model; pushing the agent toexplore novel and informative interactions with its environment. We demonstrate that thispolicy leads to the self-supervised emergence of a spectrum of complex behaviors; includingego motion prediction; object attention; and object gathering. Moreover; the world model …,arXiv preprint arXiv:1802.07461,2018,*
Learning to Play with Intrinsically-Motivated Self-Aware Agents,Nick Haber; Damian Mrowca; Li Fei-Fei; Daniel LK Yamins,Abstract: Infants are experts at playing; with an amazing ability to generate novel structuredbehaviors in unstructured environments that lack clear extrinsic reward signals. We seek tomathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulatedenvironment in which an agent can move and interact with objects it sees; we propose a"world-model" network that learns to predict the dynamic consequences of the agent'sactions. Simultaneously; we train a separate explicit" self-model" that allows the agent totrack the error map of its own world-model; and then uses the self-model to adversariallychallenge the developing world-model. We demonstrate that this policy causes the agent toexplore novel and informative interactions with its environment; leading to the generation …,arXiv preprint arXiv:1802.07442,2018,*
MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels,Lu Jiang; Zhengyuan Zhou; Thomas Leung; Li-Jia Li; Li Fei-Fei,Abstract: Recent studies have discovered that deep networks are capable of memorizing theentire data even when the labels are completely random. Since deep models are trained onbig data where labels are often noisy; the ability to overfit noise can lead to poorperformance. To overcome the overfitting on corrupted training data; we propose a noveltechnique to regularize deep networks in the data dimension. This is achieved by learning aneural network called MentorNet to supervise the training of the base network; namely;StudentNet. Our work is inspired by curriculum learning and advances the theory by learninga curriculum from data by neural networks. We demonstrate the efficacy of MentorNet onseveral benchmarks. Comprehensive experiments show that it is able to significantlyimprove the generalization performance of the state-of-the-art deep networks on …,arXiv preprint arXiv:1712.05055,2017,*
Graph Distillation for Action Detection with Privileged Information,Zelun Luo; Lu Jiang; Jun-Ting Hsieh; Juan Carlos Niebles; Li Fei-Fei,Abstract: In this work; we propose a technique that tackles the video understanding problemunder a realistic; demanding condition in which we have limited labeled data and partiallyobserved training modalities. Common methods such as transfer learning do not takeadvantage of the rich information from extra modalities potentially available in the sourcedomain dataset. On the other hand; previous work on cross-modality learning only focuseson a single domain or task. In this work; we propose a graph-based distillation method thatincorporates rich privileged information from a large multi-modal dataset in the sourcedomain; and shows an improved performance in the target domain where data is scarce.Leveraging both a large-scale dataset and its extra modalities; our method learns a bettermodel for temporal action detection and action classification without needing to have …,arXiv preprint arXiv:1712.00108,2017,*
Convolutional neural networks best predict representational dissimilarity in scene-selective cortex: comparing computational; object and functional models,Iris Groen; Michelle Greene; Christopher Baldassano; Li Fei-Fei; Diane Beck; Christopher Baker,*,Journal of Vision,2017,*
Method for Implementing a High-Level Image Representation for Image Analysis,*,Robust low-level image features have been proven to be effective representations for avariety of visual recognition tasks such as object recognition and scene classification; butpixels; or even local image patches; carry little semantic meanings. For high-level visualtasks; such low-level image representations are potentially not enough. The presentinvention provides a high-level image representation where an image is represented as ascale-invariant response map of a large number of pre-trained generic object detectors;blind to the testing dataset or visual task. Leveraging on this representation; superiorperformances on high-level visual recognition tasks are achieved with relatively classifierssuch as logistic regression and linear SVM classifiers.,*,2017,*
Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring Hand Hygiene Compliance,Albert Haque; Michelle Guo; Alexandre Alahi; Serena Yeung; Zelun Luo; Alisha Rege; Jeffrey Jopling; Lance Downing; William Beninati; Amit Singh; Terry Platchek; Arnold Milstein; Li Fei-Fei,Abstract: One in twenty-five patients admitted to a hospital will suffer from a hospital acquiredinfection. If we can intelligently track healthcare staff; patients; and visitors; we can betterunderstand the sources of such infections. We envision a smart hospital capable ofincreasing operational efficiency and improving patient care with less spending. In thispaper; we propose a non-intrusive vision-based system for tracking people's activity inhospitals. We evaluate our method for the problem of measuring hand hygiene compliance.Empirically; our method outperforms existing solutions such as proximity-based techniquesand covert in-person observational studies. We present intuitive; qualitative results thatanalyze human movement patterns and conduct spatial analytics which convey ourmethod's interpretability. This work is a first step towards a computer-vision based smart …,arXiv preprint arXiv:1708.00163,2017,*
Learning to learn from noisy web videos,Serena Yeung; Vignesh Ramanathan; Olga Russakovsky; Liyue Shen; Greg Mori; Li Fei-Fei,Abstract Understanding the simultaneously very diverse and intricately fine-grained set ofpossible human actions is a critical open problem in computer vision. Manually labelingtraining videos is feasible for some action classes but doesn't scale to the full long-taileddistribution of actions. A promising way to address this is to leverage noisy data from webqueries to learn new actions; using semi-supervised or “webly-supervised” approaches.However; these methods typically do not learn domain-specific knowledge; or rely oniterative hand-tuned data labeling policies. In this work; we instead propose a reinforcementlearning-based formulation for selecting the right examples for training a classifier from noisyweb search results. Our method uses Q-learning to learn a data labeling policy on a smalllabeled training dataset; and then uses this to automatically label noisy web data for new …,arXiv preprint arXiv:1706.02884,2017,*
Unsupervised camera localization in crowded spaces,Alexandre Alahi; Judson Wilson; Li Fei-Fei; Silvio Savarese,Existing camera networks in public spaces such as train terminals or malls can help socialrobots to navigate crowded scenes. However; the localization of the cameras is required; ie;the positions and poses of all cameras in a unique reference. In this work; we estimate therelative location of any pair of cameras by solely using noisy trajectories observed from eachcamera. We propose a fully unsupervised learning technique using unlabelled pedestriansmotion patterns captured in crowded scenes. We first estimate the pairwise cameraparameters by optimally matching single-view pedestrian tracks using social awareness.Then; we show the impact of jointly estimating the network parameters. This is done byformulating a nonlinear least square optimization problem; leveraging a continuousapproximation of the matching function. We evaluate our approach in real-world …,Robotics and Automation (ICRA); 2017 IEEE International Conference on,2017,*
Adversarially robust policy learning through active construction of physically-plausible perturbations,Ajay Mandlekar; Yuke Zhu; Animesh Garg; Li Fei-Fei; Silvio Savarese,Abstract Policy search methods in reinforcement learning have demonstrated success inscaling up to larger problem sizes beyond toy examples. However; deploying these methodson real robots remains challenging due to the large sample complexity required duringlearning and their vulnerability to malicious intervention. We introduce Adversarially RobustPolicy Learning (ARPL); an algorithm that leverages active computation of physically-plausible adversarial examples during training to enable sample-efficient policy learning inthe source domain and robust performance under both random and adversarial inputperturbations. We evaluate ARPL on four continuous control tasks and show superiorresilience to changes in physical environment dynamics parameters and environment stateas compared to state-of-the-art robust policy learning methods.,IEEE Int’l Conf. on Intelligent Robots and Systems (IROS),2017,*
Distinct contributions of functional and deep neural network features to scene representation in brain and behavior,Iris Groen; Michelle R Greene; Christopher Baldassano; Li Fei-Fei; Diane M Beck; Chris I Baker,Real-world scenes are rich; heterogeneous stimuli that contain inherent correlationsbetween many visual and semantic features; making it difficult to determine how differentscene properties contribute to neural representations. Here; we assessed the uniquecontributions of three behaviorally relevant feature spaces by a) selecting stimuli for whichinherent correlations were minimized a priori and b) partitioning the neural varianceattributed to each individual feature space. We found that while scene categorizationbehavior is best explained by a functional feature space reflecting potential actions inscenes; cortical responses in scene-selective areas are best explained by mid-and high-level layers of computational deep neural network models (DNNs). While other regions ofextrastriate cortex represented some functional features; our findings reveal a striking …,bioRxiv,2017,*
Tracking Millions of Humans in Crowded Spaces,Alexandre Alahi; Vignesh Ramanathan; Li Fei-Fei,Abstract In this chapter; we present all the technical details that lead to successfullyunderstanding the mobility of more than a hundred million individuals in crowded trainterminals over the course of two years. We cover the full spectrum of an intelligent systemthat detects and tracks humans in high density crowds using a camera network. We sharedetailed insights on how to address the occlusion problem with sparsity promoting priorsand discrete combinatorial optimization that models social interactions.,*,2017,*
Categorization influences detection: A perceptual advantage for representative exemplars of natural scene categories,Eamon Caddigan; Heeyoung Choo; Li Fei-Fei; Diane M Beck,Abstract Traditional models of recognition and categorization proceed from registering low-level features; perceptually organizing that input; and linking it with stored representations.Recent evidence; however; suggests that this serial model may not be accurate; with objectand category knowledge affecting rather than following early visual processing. Here; weshow that the degree to which an image exemplifies its category influences how easily it isdetected. Participants performed a two-alternative forced-choice task in which they indicatedwhether a briefly presented image was an intact or phase-scrambled scene photograph.Critically; the category of the scene is irrelevant to the detection task. We nonetheless foundthat participants “see” good images better; more accurately discriminating them from phase-scrambled images than bad scenes; and this advantage is apparent regardless of …,Journal of vision,2017,*
Connectionist Temporal Modeling for Weakly Supervised Action Labeling 153,O Sener; A Zamir; S Savarese; A Saxena; Y Song; LP Morency; R Davis; K Tang; L Fei-Fei; D Koller; GW Taylor; R Fergus; Y LeCun; C Bregler; S Venugopalan; M Rohrbach; J Donahue; R Mooney; T Darrell; K Saenko,Abstract. Joint image filters can leverage the guidance image as a prior and transfer thestructural details from the guidance image to the target image for suppressing noise orenhancing spatial resolution. Existing methods rely on various kinds of explicit filterconstruction or handdesigned objective functions. It is thus difficult to understand; improve;and accelerate them in a coherent framework. In this paper; we propose a learning-basedapproach to construct a joint filter based on Convolutional Neural Networks. In contrast toexisting methods that consider only the guidance image; our method can selectively transfersalient structures that are consistent in both guidance and target images. We show that themodel trained on a certain type of data; eg; RGB and depth images; generalizes well forother modalities; eg; Flash/Non-Flash and RGB/NIR images. We validate the …,Computer Vision–ECCV 2016: 14th European Conference; Amsterdam; The Netherlands; October 11–14; 2016; Proceedings,2016,*
Visual and Semantic Neural Representations For Animate and Inanimate Object,Manoj Kumar; Kara Federmeier; Li Fei-Fei; Diane Beck,*,Journal of Vision,2016,*
Pushing the Boundaries of Fine-Grained Object Classification with fMRI Decoding in Human Occipito-Temporal Cortex,Clara Fannjiang; Marius Cătălin Iordan; Diane Beck; Li Fei-Fei,*,Journal of vision,2015,*
Category Boundaries and Typicality Warp the Neural Representation Space of Real-World Object Categories,Marius Cătălin Iordan; Michelle Greene; Diane Beck; Li Fei-Fei,*,Journal of vision,2015,*
Functions Provide a Fundamental Categorization Principle for Scenes,Michelle Greene; Christopher Baldassano; Andre Esteva; Diane Beck; Li Fei-Fei,*,Journal of vision,2015,*
SentenceRacer: A Game with a Purpose for Image Sentence Annotation,Kenji Hata; Sherman Leung; Ranjay Krishna; Michael S Bernstein; Li Fei-Fei,Abstract: Recently datasets that contain sentence descriptions of images have enabledmodels that can automatically generate image captions. However; collecting these datasetsare still very expensive. Here; we present SentenceRacer; an online game that gathers andverifies descriptions of images at no cost. Similar to the game hangman; players compete touncover words in a sentence that ultimately describes an image. SentenceRacer bothgenerates and verifies that the sentences are accurate descriptions. We show thatSentenceRacer generates annotations of higher quality than those generated on AmazonMechanical Turk (AMT).,arXiv preprint arXiv:1508.07053,2015,*
Abstracts Fourth Biennial Conference on Resting State Brain Connectivity September 11–13; 2014 Boston/Cambridge; Massachusetts; USA,SA Anteraper; C Triantafyllou; MR Geddes; AT Mattfeld; J Gabrieli; S Whitfield-Gabrieli; PR Baldwin; T Lal; K Collins; S Mathew; J Murrough; R Salas; EB Beall; MJ Lowe; RF Casseb; GC Beltramini; M Albuquerque; G Castellano; MC França Jr; E Damaraju; EA Allen; VD Calhoun; Z Ding; X Wu; R Xu; VL Morgan; AW Anderson; JC Gore; B Erem; A Akhondi-Asl; O Afacan; SK Warfield; Michael D Fox; Tianyi Qian; Joseph R Madsen; Danhong Wang; Manling Ge; Huan-cong Zuo; Bo Hong; Hesheng Liu; AM Golestani; JJ Chen; DA Handwerker; J Gonzalez-Castillo; C Chang; PA Bandettini; A Hoffmann; R Sladky; M Spies; D Pfabigan; M Küblböck; A Höflich; K Paul; A Hummer; GS Kranz; C Lamm; R Lanzenberger; C Windischberger; N Honnorat; H Eavani; TD Satterthwaite; RE Gur; RC Gur; C Davatzikos; J Jang; J Gabriel Castrillon; C Preibisch; V Riedl; AM Wohlschläger; J Jovicich; L Minati; R Marchitelli; GB Frisoni; Pharmacog Consortium; P Kohn; J Czarapata; M Gregory; S Kippenhan; N Turner; KF Berman; H-L Lee; J Assländer; P LeVan; J Hennig; D Linsley; S MacEvoy; D Mastrovito; S Hanson; C Hanson; Andrew Michael; Mathew Anderson; Robyn Miller; Tülay Adalı; Vince Calhoun; LD Nickerson; SM Smith; CF Beckmann; A Razi; ML Seghier; G Rees; KJ Friston; GM Rojas; JA Fuentes; M Gálvez; DS Margulies; S Saperstein; R Sekuler; JW Bohland; Sadia Shakil; Shella D Keilholz; Chin-Hui Lee; H Shou; A Eloyan; MB Nebel; AF Mejia; JJ Pekar; SH Mostofsky; B Caffo; MA Lindquist; C Crainiceanu; LCT Herrera; HFB Ozelo; A Alessio; MS Oliveira; M Cordeiro; RJM Covolan; G Castellano; Alexandra Touroutoglou; Joseph Andreano; Lisa Feldman Barrett; Bradford C Dickerson; V Vuksanović; P Hövel; X Miao; H Gu; L Yan; H Lu; DJ Wang; XJ Zhou; Y Zhuo; Y Yang; R Yuan; X Di; P Taylor; S Gohel; YH Tsai; BB Biswal; Tanzil M Arefin; A Mechling; SB Hamida,Background: Multiple cortical regions comprise the stop network including inferior frontalcortex (IFC); bilateral insula and supplementary motor area [1]. There is a paucity ofevidence from functional connectivity MRI (fcMRI) supporting the putative role of subthalamicnucleus (STN) in inhibitory control due to the technical limitations of imaging a smallsubcortical structure. Resting state fcMRI of STN using a small sample has been reported [2];but because of the lack of whole-brain coverage; inferior prefrontal regions were notinvestigated. Given the relationship between STN and the stop network [3]; we hypothesizedthat functional connectivity exists between IFC and STN. Methods: 51 normal volunteerswere imaged using a 3T Siemens Tim Trio (Siemens Healthcare; Erlangen; Germany) withthe product 32 Channel (32 Ch) coil. High-resolution (2 mm 3) resting-state scans were …,Brain Connectivity,2014,*
Visual And Semantic Representations Of Scenes,Manoj Kumar Kumar; Kara D Federmeier; Li Fei-Fei; Diane M Beck,*,Journal of Vision,2014,*
Visual categorization is automatic and obligatory: Evidence from Stroop-like,MR Greene; L Fei-Fei,Human observers categorize visual stimuli with remarkable efficiency—a result that has ledto the suggestion that object and scene categorization may be automatic processes. Wetested this hypothesis by presenting observers with a modified Stroop paradigm in whichobject or scene words were presented over images of objects or scenes. Terms were eithercongruent or incongruent with the images. Observers classified the words as being object orscene terms while ignoring images. Classifying a word on an incongruent image came at acost for both objects and scenes. Furthermore; automatic processing was observed for entry-level scene categories; but not superordinate-level categories; suggesting that not all rapidcategorizations are automatic. Taken together; we have demonstrated that entry-level visualcategorization is an automatic and obligatory process.,*,2014,*
Large-scale Video Classiﬁcation with Convolutional Neural Networks,Andrej Karpathy; George Toderici; Sanketh Shetty; Thomas Leung; Rahul Sukthankar; Li Fei-Fei,Abstract Convolutional Neural Networks (CNNs) have been established as a powerful classof models for image recognition problems. Encouraged by these results; we provide anextensive empirical evaluation of CNNs on large-scale video classification using a dataset of1 million YouTube videos belonging to 487 classes. We study multiple approaches forextending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multi-resolution; foveated architecture as a promisingway of regularizing the learning problem and speeding up training. Our best spatio-temporalnetworks display significant performance improvements compared to strong feature-basedbaselines (55.3% to 63.9%); but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our …,*,2014,*
Social Role Discovery in Human Events (Open Access),Vignesh Ramanathan; Bangpeng Yao; Li Fei-Fei,Abstract: We deal with the problem of recognizing social roles played by people in an event.Social roles are governed by human interactions; and form a fundamental component ofhuman event description. We focus on a weakly supervised setting; where we are provideddifferent videos belonging to an event class; without training role labels. Since social rolesare described by the interaction between people in an event; we propose a ConditionalRandom Field to model the inter-role interactions; along with person specific socialdescriptors. We develop tractable variational inference to simultaneously infer modelweights; as well as role assignment to all people in the videos. We also present a novelYouTube social roles dataset with ground truth role annotations; and introduce annotationson a subset of videos from the TRECVID-MED11 event kits for evaluation purposes. The …,*,2013,*
Discovering mental representations of complex natural scenes,Michelle Greene; Abraham Botros; Diane Beck; Li Fei-Fei,*,Journal of Vision,2013,*
Typicality Sharpens Object Representations in Object-Selective Cortex,Marius Cătălin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2013,*
Computer Vision: From 3d Reconstruction to Visual Recognition,Li Fei-Fei; Silvio Savarese,*,*,2013,*
Simple line drawings,Dirk B Walthera; Barry Chaib; Eamon Caddigan; Diane M Beck; Li Fei-Fei; Mathias Eitz; Mathias Eitz; Rui Hu; William T Freeman; Timo Ojala; Lewis D Griffin; Michael Crosier; Jie Zhang; Jonathan Masci; Moez Baccouche; David Marr; Simon Haykin; Yizhe Zhang; Yoshua Bengio; Xing Yan; Rui Hu,abstract={Humans are remarkably efficient at categorizing natural scenes. In fact; scenecategories can be decoded from functional MRI (fMRI) data throughout the ventral visualcortex; including the primary visual cortex; the parahippocampal place area (PPA); and theretrosplenial cortex (RSC). Here we ask whether; and where; we can still decode scenecategory if we reduce the scenes to mere lines. We collected fMRI data while participantsviewed photographs and line drawings of beaches; city streets; forests; highways;mountains; and offices. Despite the marked difference in scene statistics; we were able todecode scene category from fMRI data for line drawings just as well as from activity for colorphotographs; in primary visual cortex through PPA and RSC. Even more remarkably; in PPAand RSC; error patterns for decoding from line drawings were very similar to those from …,Proceedings of the,2013,*
INTERNAL REPRESENTATIONS OF REAL-WORLD SCENE CATEGORIES,Michelle Greene; Abraham Botros; Diane Beck; Li Fei-Fei,*,JOURNAL OF COGNITIVE NEUROSCIENCE,2013,*
NATURAL STIMULI ACQUIRE BASIC-LEVEL ADVANTAGE IN OBJECT-SELECTIVE CORTEX,Marius Catalin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,*,JOURNAL OF COGNITIVE NEUROSCIENCE,2013,*
Shifting Weights: Adapting Object Detectors from Image to Video (Author's Manuscript),Kevin Tang; Vignesh Ramanathan; Li Fei-Fei; Daphne Koller,Abstract: Typical object detectors trained on images perform poorly on video; as there is aclear distinction in domain between the two types of data. In this paper; we tackle theproblem of adapting object detectors learned from images to work well on videos. We treatthe problem as one of unsupervised domain adaptation; in which we are given labeled datafrom the source domain (image); but only unlabeled data from the target domain (video). Ourapproach; self-paced domain adaptation; seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples; starting with theeasiest first. At each iteration; the algorithm adapts by considering an increased number oftarget domain examples; and a decreased number of source domain examples. To discovertarget domain examples from the vast amount of video data; we introduce a simple; robust …,*,2012,*
Automatic basic-level object and scene categorization,Michelle R Greene; Li Fei-Fei,METHOD We presented 12 naive observers with scene and object category namessuperimposed on images of scenes and objects. A total of 100 object categories and 100scene categories were used in this experiment. Object images were shown over coloured 1/fnoise to equate the visual complexity and viewing angle of the object and scene images.Observers classified the words as being a type of object or a type of scene; while ignoringthe image; and were instructed to respond as quickly and accurately as possible. Imagesremained on screen until response. Observers completed 200 trials with object images andnames; and 200 trials of scene images and names. Half of the trials were congruent (eg; theword “chair” on a picture of a chair; or the word “kitchen” on the picture of a kitchen; seeFigure 1) and half of the trials were incongruent (eg; the word “lamp” on the picture of a …,Visual Cognition,2012,*
Special Editors' Introduction to the Special Issue on Award-Winning Papers from the IEEE Conference on Computer Vision and Pattern Recognition 2010 (CVPR 20...,Trevor Darrell; David Hogg; David Jacobs,• “Visual Event Recognition in Videos by Learning from Web Data;” LixinDuan; Dong Xu; IvorWai- Hung Tsang; and JieboLuo (Best Student Paper). • “Efficient Computation of RobustLow-Rank Matrix Approximations using the L1 Norm;” Anders Eriksson and Anton van den Hengel(Best Paper). • “Recognizing Human-Object Interactions in Still Images by Modeling the MutualContext of Objects and Human Poses;” Bangpeng Yao and Li Fei-Fei (Best Paper HonorableMention). The issue also contains extended versions of six of the eight other papers recommendedfor consideration for an award. All papers have been through the normal reviewing process forTPAMI. We received 1;724 submissions to the conference; a substantial increase from previousyears. To select papers from these submissions; we invited 45 well-known vision researchersto act as Areas Chairs (ACs) and recruited an expert team of 718 reviewers from the …,IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,*
Neural Representation of Human-Object Interactions,Christopher Baldassano; Diane M Beck; Li Fei-Fei,*,Journal of Vision,2012,*
A large-scale taxonomy of real-world scenes,Michelle Greene; Li Fei-Fei,*,Journal of Vision,2012,*
The role of attention in the perception of good and bad exemplars of natural scene categories,Eamon Caddigan; Li Fei-Fei; Diane Beck,*,Journal of Vision,2012,*
Tuesday; June 15; 2010,Yong Jae Lee; Kristen Grauman; Bangpeng Yao; Li Fei-Fei; Leonid Karlinsky; Michael Dinerstein; Daniel Harari; Shimon Ullman; Shai Bagon; Ori Brostovski; Meirav Galun; Michal Irani; Chang Huang; Ram Nevatia; Zdenek Kalal; Jiří Matas; Krystian Mikolajczyk,The following topics are dealt with: object recognition; image color analysis; image texture;illumination; reflectance; shape recognition; image matching; computational photography;image motion analysis; optimisation methods; stereo image processing; computer vision;image denoising; learning methods; statistical methods; video signal processing; imagesegmentation; face recognition; image retrieval; video retrieval; and coding methods.,*,2012,*
The effect of load shedding on the constant of dynamic security region critical hyper plane,Liu Huai-dong; Li Fei-fei; Liu Wen-shuang; Wang Jian; Wang An-ying,Due to its specific advantages; the model of probabilistic insecurity index is being appliedmore and more widely in the security control of power system. Based on the fact that theboundary of DSR can be expressed by hyper plane and the migration law of the hyperplane; lots of emulations on the New England 4-genarator 11-bus system are performed.Through the results of the emulations; it is found that the relationship between DSR hyperplane constant a 0 and the amount of load to be shed p is approximately quadraticpolynomial. The comparison between the emulation date and fitting curve of the emulationdate indicates that this relationship is effective and precise.,Innovative Smart Grid Technologies-Asia (ISGT Asia); 2012 IEEE,2012,*
Calculation of Flicker Parameters Using Voltage Interharmonics,Feifei Li; Maoqing Ye,This paper proposed a new method to estimate voltage flicker using the discrete Fouriertransform; which hope to obtain the parameters of voltage flicker while estimatinginterharmoncis. The proposed method obtains the amplitude; frequency and phase angle ofeach frequency component using the odd points interpolation correction method; whichreducing the leakage while interpolating; based on the specified signal processingrecommendations by IEC61000-4-7. According to the concept of voltage flicker; the double-band property of interharmonic-pair converted by voltage flicker was obtained. Consideringvoltage flicker perceptible frequency range; the interharmonic-pair frequency range can beobtained. According to the electric power distortion component symbiosis mechanism; wecan differentiate interharmonic-pair and common interharmonics components; and obtain …,Power and Energy Engineering Conference (APPEEC); 2012 Asia-Pacific,2012,*
Natural scenes are robust to bubbling,Eamon Caddigan; Li Fei-Fei; Diane Beck,• Radii for circles centered on these points were chosen to maximize the coverage of theimage• Circles could not overlap• No radius could fall below 5% of the image width• Keeponly displays providing> 80% coverage.• All circles were uniformly scaled to reach desiredlevel of occlusion (20%; 40%; 60%; 80%),Journal of Vision,2011,*
IEEE Computer Vision and Pattern Recognition (CVPR),Corporate Dono,CVPR is the premiere annual Computer Vision event comprising the main CVPR conferenceand 27 co-located workshops and short courses. With its high quality and low cost; itprovides an exceptional value for students; academics and industry. Attendance for the 2011event was limited to 1500 people for the main conference plus 50 workshop only attendeesand approximately 50 exhibitors and volunteers.,Workshop/Tutorials: Mon/Fri/Sat June,2011,*
Research of calculation of flicker values based on inter-harmonics measurement,Fei-Fei Li; Hong-Geng Yang; Jin Hui; Mao-Qing Ye; Zhi Chen,This paper proposed a new method to calculate the voltage flicker values based on inter-harmonics measurement. In this method; the concept of" AM-inter-harmonic pair" and" PM-inter-harmonic pair" are proposed to distinguish the modulation wave and general inter-harmonics which will cause flicker based on the voltage distortion theory. The proposedmethod obtains the parameters of each frequency component using the odd pointsinterpolation correction method; which reducing the leakage at interpolating; based on thespecified signal processing recommendations by IEC61000-4-7. Considering the voltageflicker perceptible frequency range and the signal transform equations; we can estimate theparameters of flicker signal. According to the theory of IEC flicker meter; we can calculate theinstantaneous flicker sensation level caused by each frequency component or the total …,Advanced Technology of Electrical Engineering and Energy,2011,*
A method of voltage flicker estimation based on interharmonic pairs,Fei-fei Li; Mao-qing Ye; Hong-geng Yang; Jin Hui,This paper proposed a new method to estimate voltage flicker using the discrete Fouriertransform (DFT). Based on the specified signal processing recommendations by IEC61000-4-7; the proposed method obtains the amplitude; frequency and phase angle of eachharmonic/interharmonic using the odd points interpolation correction method. According tothe electric power distortion component symbiosis mechanism; the detected interharmonicsin pairs are transformed into a flicker signal. Combined with the double-side property; theinterharmonics in pairs can be extracted. The parameters of flicker signal are obtained bythe signal transform equations. The simulation results indicate that it is easy and quick toestimate the parameters by detecting the interharmonics signal directly and estimating theflicker signal indirectly; and the odd points interpolation correction method can separate …,Electricity Distribution (CICED); 2010 China International Conference on,2010,*
INTERNET VISION,N Snavely; I Simon; M Goesele; R Szeliski; SM Seitz; B Kaneva; J Sivic; A Torralba; S Avidan; WT Freeman; Z Stone; T Zickler; T Darrell; T Mei; XS Hua; TL Berg; A Sorokin; G Wang; DA Forsyth; D Hoiem; I Endres; A Farhadi; R Fergus; L Fei-Fei; P Perona; A Zisserman; BC Russell; J Yuen,MEMBERSHIP Check out the many features available through the IEEE Membership Portal.PUBLICATIONS Find IEEE articles by using the search features of IEEE Xplore SERVICES TheIEEE offers many services to Members; as well as other groups. STANDARDS The IEEE is theleader in the development of many industry standards. CONFERENCES Search for the idealIEEE Conference; on the subject of your choice CAREERS/JOBS Find your next job throughthis IEEE service … Find the following information on our website. How to Subscribe JournalDescription History Current Issue Special Issue Schedule Recent Highlights The PublicationProcess Information for Authors Reader Opinions and Suggestions,Proceedings of the IEEE,2010,*
fMRI Decoding of Natural Scene Categories from Line Drawings,Dirk Walther; Barry Chai; Eamon Caddigan; Diane Beck; Li Fei-Fei,Abstract Using full color photographs of natural scenes; we have previously shown thatinformation about scene category is contained in patterns of fMRI activity in theparahippocampal place area (PPA); the retrosplenial cortex (RSC); the lateral occipitalcomplex (LOC); and primary visual cortex (V1)(Walther et al. J. Neurosc. 2009). If theseregions are involved in representing category; then it should be the case that we coulddecode scene category for any natural scene image that participants can categorize;including simple line drawings. In keeping with this prediction; we found that we can decodescene category from fMRI activity patterns for novel line drawing pictures just as well as fromactivity for color photographs; in V1 through PPA. Even more remarkably; a decoder trainedon fMRI activity elicited by color photographs was able to predict the correct scene …,Journal of Vision,2010,*
The Good; the Bad; and the Scrambled: A Perceptual Advantage for Good Examples of Natural Scene Categories,Eamon Caddigan; Dirk B Walther; Li Fei-Fei; Diane M Beck,Abstract Recent research has shown that participants are better able to categorize brieflypresented natural scene images that have been rated as “good” exemplars of their category;and that this is reflected in the distributed patterns of neural activation obtained through fMRI(Torralbo; et al.; 2009). The effect of typicality on categorization/decision processes is welldocumented (see Rosch; 1978); but it is possible that such effects may also reflectdifferences in perception. Here we asked whether subjects might actually 'see'goodexemplars of a category better than bad exemplars. We asked subjects to simply reportwhether a very briefly presented (19 ms–60 ms) image was intact or scrambled. Imagesdrawn from six natural scene categories (beaches; city streets; forests; highways; mountainsand offices) were rated as either “good” or “bad” exemplars of their categories. These …,Journal of Vision,2010,*
Human search strategies are informed by complex target distribution statistics,DB Walther; E Caddigan; L Fei-Fei; DM Beck,METHODS Thirty-three students from Duke University were divided randomly into threegroups of 11. Each trial presented a search array of 40 items on a cloudy background.Targets were perfect ''T''shapes and the remaining items were distractor ''L''shapes that werenot perfect Ts. Each trial had 0Á12 targets; with target prevalence manipulated betweengroups (see Figure 1A). Participants clicked on each T they found then clicked a buttonlabelled ''Done''; ending the trial. Feedback was given after each trial to provide participantsthe search environment statistics; regardless of their performance. Participants received 15points for each target found and the experiment ended at 2000 points.,Object Perception; Attention; and Memory (OPAM) 2010 Conference Report 18th Annual Meeting; St. Louis; Missouri; MO; USA,2010,*
Decoding of natural scene categories from transformed images using distributed patterns of fMRI activity,Eamon Caddigan; Dirk B Walther; Li Fei-Fei; Diane Beck,Page 1. Decoding of natural scene categories from transformed images using distributedpatterns of fMRI activity Eamon Caddigan1;*; Dirk B. Walther1; Li Fei-Fei2; Diane Beck11University of Illinois at Urbana-Champaign; 2Princeton University; *ecaddiga@uiuc.eduPerceiving natural scene categories is fast (Potter et al. 1976; Thorpe et al. 1996; Fei-Fei etal. 2007) and can be done with only little attention (Fei-Fei et al. 2002). How and where isthis information represented in the brain? We investigate the contribution of several visualareas—V1; lateral occipital complex (LOC); fusiform face area (FFA) and parahippocampalplace area (PPA)—to scene categorization. Introduction T raining Images Test Image Guess“beach” Pattern Recognition Alg. (eg; SVM) Train Test Pattern Recognition - Linear SupportVector Machine (SVM) - Strict leave-one-run-out cross-validation …,Journal of Vision,2008,*
Decoding scene categories using distributed patterns of fMRI activity,Eamon Caddigan; Dirk B Walther; Li Fei-Fei; Diane Beck,Page 1. Decoding scene categories using distributed patterns of fMRI activity Eamon Caddigan1;*;Dirk B. Walther1; Li Fei-Fei2; Diane Beck1 1University of Illinois at Urbana-Champaign; 2PrincetonUniversity; *ecaddiga@uiuc.edu Perceiving natural scene categories is fast (Potter et al. 1976;Thorpe et al. 1996; Fei-Fei et al. 2007) and can be done with only little attention (Fei-Fei et al.2002). How and where is this information represented in the brain? We investigate the contributionof several visual areas—V1; lateral occipital complex (LOC); fusiform face area (FFA) andparahippocampal place area (PPA)—to scene categorization. Introduction T raining Images TestImage Guess “beach” Pattern Recognition Alg. (eg; SVM) Train Test Pattern Recognition - LinearSupport Vector Machine (SVM) - Strict leave-one-run-out cross-validation - Separate fMRI dataand separate image sets for training and testing …,Perception ECVP abstract,2008,*
Decoding distributed patterns of activity associated with natural scene categorization,Eamon Caddigan; Dirk Walther; Justas Birgiolas; Jonathan Weissman; Diane Beck; Li Fei-Fei,Abstract Human observers are able to quickly and efficiently perceive the content of naturalscenes (Potter; 1976). Previous studies have examined the time course of this rapidclassification (Thorpe et al; 1996) as well as the brain regions activated when subjectscategorize natural scenes (Epstein & Higgins; 2006). Using statistical pattern recognitionalgorithms similar to those employed by Cox and Savoy (2002) to decode the neural statesassociated with object categories; we asked whether we can identify and discriminatedistributed patterns of fMRI activity associated with particular natural scene categories(beaches; mountains; forests; tall buildings; highways; and industrial scenes). fMRI data wasaquired while subjects viewed 100 images from each of six categories; in 6 blocks of 10images each of the same category; organized into 10 runs. A subset of the voxels was …,Journal of Vision,2007,*
Task-set switching with natural scenes: Measuring the cost of deploying top,DB Walther; L Fei-Fei,In many everyday situations; we bias our perception from the top down; based on a task oran agenda. Frequently; this entails shifting attention to a specific attribute of a particularobject or scene. To explore the cost of shifting top-down attention to a different stimulusattribute; we adopt the task-set switching paradigm; in which switch trials are contrasted withrepeat trials in mixed-task blocks and with single-task blocks. Using two tasks that relate tothe content of a natural scene in a gray-level photograph and two tasks that relate to thecolor of the frame around the image; we were able to distinguish switch costs with andwithout shifts of attention. We found a significant cost in reaction time of 23–31 ms forswitches that require shifting attention to other stimulus attributes; but no significant switchcost for switching the task set within an attribute. We conclude that deploying top-down …,*,2007,*
Measuring the cost of deploying top-down visual attention,Dirk Walther; Li Fei-Fei; Christof Koch,Abstract In many everyday situations; we bias our perception using task-dependentinformation. To explore the cost involved in shifting top-down attention to a new task; weadopted a task-switching paradigm; in which 'switch'and 'repeat'trials in mixed task blocksare contrasted with single task blocks. We use two visual tasks in our paradigm: objectdetection in cluttered gray-level natural scenes ('animal'vs.'non-animal'and 'vehicle'vs.'non-vehicle'); and discriminating the color of the frame enclosing these images('orange'vs.'purple'or 'blue'; and 'blue'vs.'orange'or 'purple'). We distinguished switch costswith (eg switching from detecting orange among purple/blue distracters to detecting animalsin natural scenes) and without top-down attention shifts. We found significant switch costs inreaction time of 20ms for switching from a color task to an object detection task (p< 0.05) …,Journal of Vision,2006,*
Using,Bangpeng Yao; Li Fei-fei,Abstract—Detecting objects in cluttered scenes and estimating articulated human body partsfrom 2D images are two challenging problems in computer vision. The difficulty isparticularly pronounced in activities involving human-object interactions (eg; playing tennis);where the relevant objects tend to be small or only partially visible and the human bodyparts are often self-occluded. We observe; however; that objects and human poses canserve as mutual context to each other—recognizing one facilitates the recognition of theother. In this paper; we propose a mutual context model to jointly model objects and humanposes in human-object interaction activities. In our approach; object detection provides astrong prior for better human pose estimation; while human pose estimation improves theaccuracy of detecting the objects that interact with the human. On a six-class sports data …,*,2005,*
3D Object Representations for Fine-Grained Categorization: Supplementary Material,Jonathan Krause; Michael Stark; Jia Deng; Li Fei-Fei,In Tab. 1 we give the classes and number of images in each class for BMW-10. In Tab. 2 we dothe same for car- 197. A coarse category distribution for car-197 is given in Fig. 1. The trainingand test splits for each dataset are fixed and have equal size … Class Num. Images BMW 3Series Sedan 2007 53 BMW 3 Series Sedan 2009 53 BMW 3 Series Sedan 2012 50 BMW 5Series Sedan 2007 50 BMW 5 Series Sedan 2008 52 BMW 5 Series Sedan 2011 50 BMW M3Sedan 2008 51 BMW M5 Sedan 2007 52 BMW ActiveHybrid 5 Sedan 2011 50 BMW Alpina B7Sedan 2011 51 … Table 1: Class list and image count for BMW-10 … Here we give additionalexperimental details for the cat- egorization experiments. Details already given in the main textare not repeated here. As input into our 3D geometry classifier; we extracted HOG [1] featureson a fixed grid of size 340 × 192 with a spacing of 16 pixels. We use the implementation …,IJCV,2004,*
Rapid natural scene categorization requires little or no attention,FF Li; R Van Rullen; C Koch; P Perona,*,JOURNAL OF COGNITIVE NEUROSCIENCE,2002,*
Lecture 13: k-‐means and mean-‐shift clustering,Juan Carlos Niebles; Fei-Fei Li,– Whole is greater than sum of its parts … – RelaPonships among parts can yield newproperPes/features … • Psychologists idenPfied series of factors that predispose set … “I standat the window and see a house; trees; sky. Theoretically I might say there were 327 brightnessesand nuances of colour. Do I have "327"? No. I have sky; house … • These factors make intuiPvesense; but are very difficult to translate into algorithms … • Goal: idenPfy groups of pixels thatgo together … Mean Shift: A Robust Approach toward Feature Space Analysis; PAMI 2002 …Mean Shift: A Robust Approach toward Feature Space Analysis; PAMI 2002 … • We could labelevery pixel in the image according to which … – ie; segment the image based on the intensityfeature … • What if the image isn't quite so simple … • Now how to determine the three main… • Goal: choose three “centers” as the representaPve intensiPes;,Psychologische Forschung,1923,*
Scaling Human-Object Interaction Recognition through Zero-Shot Learning,Liyue Shen; Serena Yeung; Judy Hoffman; Greg Mori; Li Fei-Fei,Abstract Recognizing human object interactions (HOI) is an important part of distinguishingthe rich variety of human action in the visual world. While recent progress has been made inimproving HOI recognition in the fully supervised setting; the space of possible human-object interactions is large and it is impractical to obtain labeled training data for allinteractions of interest. In this work; we tackle the challenge of scaling HOI recognition to thelong tail of categories through a zero-shot learning approach. We introduce a factorizedmodel for HOI detection that disentangles reasoning on verbs and objects; and at test-timecan therefore produce detections for novel verb-object pairs. We present experiments on therecently introduced large-scale HICODET dataset; and show that our model is able to bothperform comparably to state-of-the-art in fully-supervised HOI detection; while …,*,*,*
Thursday; 1 February,Helene Intraub; Mary C Potter; Timothy Brady; Russell Epstein; Li Fei-Fei; Moshe Bar; Leila Reddy,When testing the early time course of scene cognition questions often arise as to the locus ofan effect (eg;" Did this occur during perception or memory?"). These questions becomeparamount if we think of scene cognition solely in terms of a visual representation of thestimulus (usually a picture). In contrast; if we think of it in terms of a surrounding 3D spatialframework in which an input (eg; visual; haptic; imagined; etc.) is set; then the questionschange. Taking boundary extension (BE) as an example; I will report new research in whicha fleeting interruption (42 ms; commensurate with a saccade) during presentation of a viewwas sufficient to elicit BE errors. For example; the pre-interrupt view was remembered asbeing slightly more panoramic than the currently visible view--although they were identical. Iwill argue that a 3D spatial model of scene perception provides a more parsimonious …,*,*,*
Inferring and Executing Programs for Visual Reasoning Supplementary Material,Justin Johnson; Bharath Hariharan; Laurens van der Maaten; Judy Hoffman; Li Fei-Fei; C Lawrence Zitnick; Ross Girshick,In all experiments our program generator is an LSTM sequence-to-sequence model [9]. Itcomprises two learned recurrent neural networks: the encoder receives the naturallanguagequestion as a sequence of words; and summarizes the question as a fixed-length vector; thedecoder receives this fixed-length vector as input and produces the predicted program as asequence of functions. The encoder and decoder do not share weights. The encoderconverts the discrete words of the input question to vectors of dimension 300 using alearned word embedding layer; the resulting sequence of vectors is then processed with atwo-layer LSTM using 256 hidden units per layer. The hidden state of the second LSTMlayer at the final timestep is used as the input to the decoder network. At each timestep thedecoder network receives both the function from the previous timestep (or a special< …,*,*,*
Best of both worlds: Human-machine collaboration for object annotation,Fei-Fei Li; Li-Jia Li; Olga Russakovsky,Page 1. Best of both worlds: Human-machine collaboration for object annotation Fei-Fei Li(Stanford U.) Li-Jia Li (Snapchat) Olga Russakovsky (Stanford U.) CVPR 2015 Page 2. BackpackPage 3. Backpack Flute Strawberry Traffic light Bathing cap Matchstick Racket Sea lion Page4. Large-scale recognition Page 5. Large-scale recognition Need benchmark datasets Page 6.PASCAL VOC 2005-2012 Classification: person; motorcycle Detection Segmentation PersonMotorcycle Action: riding bicycle Everingham; Van Gool; Williams; Winn and Zisserman. ThePASCAL Visual Object Classes (VOC) Challenge. IJCV 2010. 20 object classes 22;591 imagesPage 7. Large Scale Visual Recognition Challenge (ILSVRC) 2010-2014 20 object classes 22;591images 200 object classes 517;840 images DET 1000 object classes 1;431;167 imagesCLS-LOC Person http://image-net.org/challenges/LSVRC/ Person Dog …,*,*,*
Hierarchical Task Generalization with Neural Programs,Danfei Xu; Yuke Zhu; Animesh Garg; Julian Gao; Li Fei-Fei; Silvio Savarese,Abstract—In this work; we propose a novel modeling framework for hierarchical tasks calledNeural Task Programming (NTP). We bridge the idea of meta-learning and neuralprogramming. NTP takes as input a task specification and recursively decomposes it intofiner subtask specifications. These specifications are fed to a hierarchical neural program;where bottomlevel programs are callable subroutines to interact with the environment. Wevalidate our method in two robot manipulation tasks. NTP achieves strong generalizationresults across sequential tasks that exhibit a hierarchal and compositional structure. Theexperimental results show that NTP learns to generalize well towards unseen tasks withincreasing lengths; variable topologies; and changing objectives.,*,*,*
Depth-Based Activity Recognition in ICUs Using Convolutional and Recurrent Neural Networks,Rishab Mehra; Gabriel M Bianconi; Serena Yeung; Li Fei-Fei,Abstract Currently; activity detection in Intensive Care Units (ICUs) is performed manually bytrained personnel-primarily nurses-who log the activities as they happen. This process isboth expensive and time consuming. Our goal is to design a system which automaticallygives an annotated list of all activities that occurred in the ICU over the day. In the future; wealso aim at providing a system which informs the doctor about the health status of patientsand unusual activity occurrences. Overall; this system will reduce the monitoring workload oftrained personnel; and lead to a quicker and safer recovery of the patient. In order to designthis system; we installed depth sensors in ICUs to create a novel dataset; and performedactivity recognition on it using Convolution Neural Networks and Long Short Term MemoryUnits. We were able to achieve high classification accuracy for a restricted dataset …,*,*,*
BURNED: Towards Efficient and Accurate Burn Prognosis Using Deep Learning,Orion Despo; Serena Yeung; Jeffrey Jopling; Brian Pridgen; Cliff Sheckter; Sara Silberstein; Li Fei-Fei; Arnold Milstein,Abstract Early excision of burns saves lives. Like most treatments; this relies on havingaccurate early-stage burn depth diagnosis. Unfortunately; even considering the experts;there exists a lack of accuracy among the burn community in terms of early diagnosis. Usingstate of the art deep learning principles; this project aims to improve upon this. First; wecreated the largest dataset to date of segmented and labeled burns. Second; with minimaltraining; we show that we can accurately discriminate burnt skin from the rest of an image;paving the way for the calculation of clinically important metrics; such as the percent of thebody which is burned (TBSA). Furthermore; we show there is large potential to do evenbetter. We then show that provided we make the necessary data and model adjustments; wecan extend this to accurately segment and classify different burn depths. In total; this …,*,*,*
Social Role Discovery in Human Events,Social Role Model; Vignesh Ramanathan; Bangpeng Yao; Li Fei-Fei,Page 1. Bride Priest Brides maid Grooms man Results: Role Clusters Social Role Model Results:Clustering Accuracy Social Role Discovery in Human Events Vignesh Ramanathan; BangpengYao and Li Fei-Fei {vigneshr; bangpeng; feifeili}@cs.stanford.edu Computer Science Department;Stanford University Introduction i v Ψ Σ Ψ Σ bride groom priest grooms men brides maid b'dayperson parent friends guest presenter recipient host distributor instructor presenter • Color of crossrepresents ground-truth role for wrong assignments - Unary feature weight - Interaction featureweight - Social role assignment - Reference role assignment Jointly infer by variational inferenceMethod Birthday Wedding Award Function Physical Training prior 29.32% 20.17% 62.97% 65.93%K-means 33.88% 29.43% 31.97% 57.67% Only unary 38.25% 39.22% 69.31% 76.69% Interactionas context 41.53% 38.83% 77.75% 77.91 …,*,*,*
Lecture 10: Multi-view geometry,Fei-Fei Li,Page 1. Lecture 10 - Fei-Fei Li Lecture 10: Multi-view geometry Professor Fei-Fei Li StanfordVision Lab 24-Oct-11 1 Page 2. Lecture 10 - Fei-Fei Li 24-Oct-11 2 First; a reminder of theFundamental Matrix: The Fundamental Matrix Song http://danielwedge.com/fmatrix/ O O' p p'P 0pFp T =′ F = Fundamental Matrix (Faugeras and Luong; 1992) Page 3. Lecture 10 - Fei-FeiLi What we will learn today? • Rectification for stereo vision • Correspondence problem (ProblemSet 2 (Q3)) • Active stereo vision systems • Structure from motion 24-Oct-11 3 Reading: [HZ]Chapters: 4; 9; 11 [FP] Chapters: 10 Page 4. Lecture 10 - Fei-Fei Li What we will learn today? •Rectification for stereo vision • Correspondence problem (Problem Set 2 (Q3)) • Active stereovision systems • Structure from motion 24-Oct-11 4 Reading: [HZ] Chapters: 4; 9; 11 [FP] Chapters:10 Page 5. Lecture 10 - Fei-Fei Li 24-Oct-11 5 Two eyes help! O 2 O 1 p 2 p 1 …,*,*,*
Lecture 18: Human Motion Recognition,Fei-Fei Li,Page 1. Lecture 18: Human Motion Recognition Professor Fei-Fei Li Stanford Vision LabLecture 18 - Fei-Fei Li 7-Mar-11 1 Page 2. What we will learn today? • Introduction • Motionclassification using template matching il ifi i i i l • Motion classification using spatio-temporalfeatures • Motion classification using pose estimation Lecture 18 - Fei-Fei Li 7-Mar-11 2Page 3. search sport analysis Human sport analysis surveillance Motion Analysis hi synthesisbiomechanics sign language Lecture 18 - Fei-Fei Li 7-Mar-11 3 game interfaces motioncapture gg Page 4. It's challenging… Variation in Appearance Variation in Pose Occlusion& clutter Lecture 18 - Fei-Fei Li 7-Mar-11 4 Adapted from http://luthuli.cs.uiuc.edu/~daf/tutorial.html Variation in view-point Page 5. It's challenging… It s challenging… • No clearaction/movement classes – Unless in specific domain …,*,*,*
Weakly Supervised Simultaneous Tracking and Action Recognition,Juan Carlos Niebles; Kevin Tang; Fei-Fei Li,*,*,*,*
WEEK DATE TOPIC INSTRUCTOR Chromatin and chromosome dynamics,F Li,Course goal and format: The goal of this course is to provide in depth look at selectedprocesses and structures in the nucleus; and to promote critical assessment of researchthrough reading and discussion of primary research articles. For each week (except Midtermweeks); Wednesday class will be a lecture; and during the Thursday class we will discuss aresearch paper. Before each class; you will be expected to read the reading assignmentsand the research paper to participate in active discussion of the subject during the class.Prerequisites:,*,*,*
IEEE Computer Society Publishing Services Staff,Chris Bishop; Andrew Blake; David Fleet; Zoubin Ghahramani; Eric Grimson; Daphne Koller; David Kriegman; Jitendra Malik; Gerard Medioni; Ramin Zabih; Kalle Åström; Shai Avidan; Francis Bach; Mikhail Belkin; Karsten Borgwardt; Richard Bowden; Chris Bregler; Michael Brown; Thomas Brox; Miguel A Carreira-Perpinan; Robert Collins; Tim Cootes; Fernando De la Torre; Inderjit Dhillon; Gal Elidan; Vittorio Ferrari; Andrew William Fitzgibbon; François Fleuret; Hiroshi Ishikawa; Jiaya Jia; Rong Jin; Christoph Lampert; Gert Lanckriet; Ivan Laptev; Hugo Larochelle; Erik Learned-Miller; Kyoung Mu Lee; Fei-Fei Li; Davide Maltoni; R Manmatha; Yasuyuki Matsushita; Greg Mori; Yael Moses; Tomas Pajdla; Maja Pantic; Haesun Park; Deva Ramanan; Ian Reid; Stephan Roth; Carsten Rother; Sudeep Sarkar; Tobias Scheffer; Clayton Scott; Cristian Sminchisescu; Amos Storkey; Peter Sturm; Charles Sutton; Ben Taskar; Yee Whye Teh; Massimo Tistarelli; Antonio Torralba; Andrea Vedaldi; René Vidal; Ying Wu; Eric P Xing; Huan Xu; Ruigang Yang; Jieping Ye; Lihi Zelnik; Jun Zhu; Sing Bing Kang; Amir Globerson; Stan Sclaroff; Tinne Tuytelaars; David A Forsyth; Max Welling,The IEEE Computer Society is an association of people with professional interest in the fieldof computers. All members of the IEEE are eligible for membership in the Computer Society;as are members of certain professional societies and other computer professionals. ComputerSociety members will receive this Transactions in print and online upon payment of the annualSociety membership fee ($56 for IEEE members; $125 for all others) plus an annual subscriptionfee of $95. For additional membership and subscription information; visit our Web site athttp://computer.org/subscribe; send email to help@computer.org; or write to IEEE ComputerSociety; 10662 Los Vaqueros Circle; Los Alamitos; CA 90720 USA. Individual subscription copiesof Transactions are for personal use only … IEEE TRANSACTIONS ON PATTERN ANALYSIS& MACHINE INTELLIGENCE is published monthly by the IEEE Computer Society. IEEE …,*,*,*
Sources,Li Fei-Fei; Rob Fergus,Page 1. 6 7 7 7 7 Instructor: Daphna Weinshall; daphna@cs.huji.ac.il Visual Object Recognition -67777 V is u a l O b je c t R e c o g n itio n - 6 7 7 7 7 Instructor: Daphna Weinshall;daphna@cs.huji.ac.il Office: Ross 211 Office hours: Sunday 12:00-13:00 1 Page 2. 6 7 7 7 7Sources ▪ Recognizing and Learning Object Categories ICCV 2005 short courses Li Fei-Fei(UIUC); Rob Fergus (Oxford-MIT); Antonio Torralba (MIT) http://people.csail.mit.edu/torralba/iccv2005 ▪ Object Recognition (UPenn; CSE399b Spring 2007) V is u a l O b je c t R e c o g nitio n - 6 7 7 7 7 2 Jianbo Shi (UPenn) ▪ Recognition and Matching based on local invariantfeatures Cordelia Schmid (INRIA); David Lowe (UBC) ▪ Visual Object Recognition AAAI tutorial;July 2008 Bastian Leibe (ETH Zurich); Kristen Grauman (U Texas; Austin) Page 3. 6 7 7 7 7 Whatis Visual Recognition About ▪ Finding and recognizing objects in a picture …,Instructor,*,*
Part 1: Bag-of-words models,Li Fei-Fei,China is forecasting a trade surplus of $90 bn (£ 51bn) to $100 bn this year; a threefoldincrease on 2004's $32 bn. The Commerce Ministry said the surplus would be created byaround us is based essentially on the messages that reach the brain from our eyes. For along time it was thought that the retinal image was transmitted point by point to visual centersin the brain; the cerebral cortex was aitk hi h th sensory; brain; visual; perception; Ministrysaid the surplus would be created by a predicted 30% jump in exports to $750 bn; comparedwith a 18% rise in imports to $660 bn. The figures are likely to further annoy the US; whichhas long argued that Chi' tf il hl db,*,*,*
Supplementary Section: Video Event Understanding using Natural Language Descriptions,Vignesh Ramanathan; Percy Liang; Li Fei-Fei,The optimization problem solved to learn our model with Posterior Regularization (PR) isshown again in Eq. 1. To recount; a; r are the action and role assignments to all the humantracklets across all videos and Q (a; r) is the corresponding probability of the assignments.The CRF loglikelihood of the assignments is given by L (a; r; w); where w are the weights tobe learnt. min w; Q; δ≥ 0; η≥ 0,*,*,*
Real-world objects acquire basic-level advantage in object-selective cortex,Marius Cãtãlin Iordan; Michelle R Greene; Diane M Beck; Li Fei-Fei,Page 1. Ideal Sub. Ideal Basic Ideal Super. * ns ** ** * V1 * ns *** ** V2 * *** * V3v * ns *** **hV4 * *** * TOS * *** * PPA * * *** RSC * * *** FFA * *** * LOC 0.1 0.2 0.3 0 C O R R ELATIO N W IT H ID EAL MAT R IX (R ) * * SIMILAR IT Y MATRIX 0.1 0.2 0.3 0 Percentile of r 0100 Ideal Subordinate Ideal Basic Ideal Superordinate ns *** ** V1 ** * * V2 * * V3v ** ** *hV4 *** ** TOS *** ** PPA ** ** RSC ** ** * FFA *** ** LOC 0.03 0.06 0.09 0.12 0.00 WITHINCA T EG O R Y SIMILAR IT Y (R ) 0.03 0.06 0.09 0.12 0.00 Subordinate Basic Superordinate*** ** 0.06 -0.03 BET W EEN C A T EG O R Y SIMILAR IT Y (R ) -0.06 -0.03 *** ** 0.15 0.15 * **** 0.18 0.18 *** ** 0.09 -0.09 ns ns ns Real-world objects acquire basic-level advantage inobject-selective cortex Marius Cãtãlin Iordan …,*,*,*
Egocentric Multi-Modal Dataset with Visual and Physiological Signals,Katsuyuki Nakamura; Alexandre Alahi; Serena Yeung; Li Fei-Fei,Abstract We introduce a dataset of egocentric video augmented with physiological heart rateand acceleration signals. To the best of our knowledge; our work is the first to leveragevisual information with physiological signals. We believe this dataset will have great valueby enabling new work directions in egocentric video understanding; ranging from activitydetection to video summarization.,*,*,*
Olga Russakovsky,O Russakovsky; J Deng; H Su; J Krause; S Satheesh; S Ma; Z Huang; A Karpathy; A Khosla; M Bernstein; AC Berg; L Fei-Fei; L Fei-Fei,5000 Forbes Ave; EDSH 216; Pittsburgh; PA 15213 http://cs.cmu.edu/~orussakoolgarus@cmu.edu … Fall 2017 Assistant Professor; Princeton University; Department of ComputerScience … 2015-2017 Postdoctoral fellow; Carnegie Mellon University; Robotics Institute.Advisors: Prof. Deva Ramanan and Prof. Abhinav Gupta … Ph.D. in computer science; StanfordUniversity; 2015. Advisor: Prof. Fei-Fei Li Thesis: Scaling Up Object Detection MS in computerscience; distinction in research; Stanford University; 2007. Advisor: Prof. Serafim BatzoglouThesis: Algorithms for Training Conditional Log-Linear Models BS in mathematics withdistinction; Stanford University; 2007 … 100 Leading Global Thinkers; Foreign PolicyMagazine; 2015. Awarded for founding and directing the Stanford AI Lab's outreach programSAILORS MIT EECS Rising Star award; 2013. Awarded annually to “about 40 …,*,*,*
Perceptual Losses for Real-Time Style Transfer and Super-Resolution: Supplementary Material,Justin Johnson; Alexandre Alahi; Li Fei-Fei,Our style transfer networks use the architecture shown in Table 1 and our superresolutionnetworks use the architecture shown in Table 2. In these tables “C× H× W conv” denotes aconvolutional layer with C filters size H× W which is immediately followed by spatial batchnormalization [1] and a ReLU nonlinearity. Our residual blocks each contain two 3× 3convolutional layers with the same number of filters on both layer. We use the residual blockdesign of Gross and Wilber [2](shown in Figure 1); which differs from that of He et al [3] inthat the ReLU nonlinearity following the addition is removed; this modified design was foundin [2] to perform slightly better for image classification. Layer Activation size Input 3× 256×256 32× 9× 9 conv; stride 1 32× 256× 256 64× 3× 3 conv; stride 2 64× 128× 128 128× 3× 3conv; stride 2 128× 64× 64 Residual block; 128 filters 128× 64× 64 Residual block; 128 …,*,*,*
DISCOVERING VOXEL-LEVEL FUNCTIONAL CONNECTIVITY BETWEEN CORTICAL REGIONS,C BALDASSANO MC IORDAN DM BECK; L FEI-FEI,Page 1. DISCOVERING VOXEL-LEVEL FUNCTIONAL CONNECTIVITY BETWEEN CORTICALREGIONS C. BALDASSANO MC IORDAN DM BECK L. FEI-FEI Attention and Perception LabComputer Science Department; Stanford University Beckman Institute and Psychology Department;University of Illinois at Urbana-Champaign Page 2. Precuneus Amygdala Thalamus Lateraloccipital complex LOC Connectivity 02 Subregion Connectivity Zhang et al. 2008 Kim et al. 2010Roy et al. 2009 Margulies et al. 2007 Margulies et al. 2009 Heinzle & Haynes 2011 Page 3.Precuneus Amygdala Thalamus Lateral occipital complex LOC Connectivity 02 SubregionConnectivity Zhang et al. 2008 Kim et al. 2010 Roy et al. 2009 Margulies et al. 2007 Margulieset al. 2009 Heinzle & Haynes 2011 GOAL: Fine-Grained Connectivity Voxel-level MapsSymmetrical Few Data Points Needed Page 4. no voxel-level connectivity …,*,*,*
Supplementary Material: Hierarchical Semantic Indexing for Large Scale Image Retrieval,Jia Deng; Alexander C Berg; Li Fei-Fei,Remark 1.1. We first provide proofs and constructions for probability vectors for non-overlapping categories (Lemma 1.4–1.12); ie x∈ RK;∑ i xi= 1; 0≤ xi≤ 1 for i= 1;...; K. Weuse∆ K− 1 to denote the set of all such vectors. In Lemma 1.15; we show extension to thegeneral case where x∈ RK; 0≤ xi≤ 1 for i= 1;...; K (but does not necessarily sum to one).We use∆ K− 1 to denote the set of all such vectors.,*,*,*
Supplementary material for Best of both worlds: human-machine collaboration for object annotation,Olga Russakovsky; Li-Jia Li2 Li Fei-Fei,We provide additional details about our principled human-in-the-loop framework forefficiently detecting all objects in an image. Section 2 provides details about how userfeedback is used in calculating probabilities (referenced in Section 4.3 in main paper). Wefirst show how each question posed to users affects multiple probability estimates about theimage. We then discuss probability updates in some special cases: in case of open-endedquestions (such as asking the user to draw a bounding box) and in case of computing theprobabilities of there being more class instances or new objects in the image.,*,*,*
Proceedings. Tenth IEEE International Conference on Computer Vision,JS Franco; E Boyer,In this paper; we investigate what can be inferred from several silhouette probability maps; inmultiview silhouette cue fusion. To this aim; we propose a new framework for multiviewsilhouette cue fusion. This framework work uses a space occupancy grid as a probabilistic3D representation of scene contents. Such a representation is of great interest for variouscomputer vision applications in perception;...,*,*,*
Fine-Grained Recognition without Part Annotations: Supplementary Material,Jonathan Krause; Hailin Jin; Jianchao Yang; Li Fei-Fei,In the main text we showed that large gains from using a VGGNet [5] architecture on theCUB-2011 [6] dataset. We show a similar comparison on the cars-196 [3] dataset in Tab. 1.As before; using a VGGNet architecture leads to large gains. Particularly striking is the gainfrom fine-tuning a VGGNet on cars-196–a basic R-CNN goes from 57.4% to 88.4% accuracyonly by fine-tuning; much larger than the already sizeable gain from fine-tuning a CaffeNet[2].,*,*,*
Best of both worlds: human-machine collaboration for object annotation (preliminary version),Olga Russakovsky; Li-Jia Li; Li Fei-Fei,Abstract Despite the recent boom in large-scale object detection; the long-standing goal oflocalizing every object in the image remains elusive. The current best object detectors canaccurately detect at most a few object instances per image. However; manually labelingobjects is quite expensive. This paper brings together the latest advancements in automaticobject detection and in crowd engineering into a principled framework for accurately andefficiently localizing the objects in an image. Our proposed image annotation systemseamlessly integrates multiple computer vision models with multiple sources of human inputin a Markov Decision Process. The system is light-weight and easily extensible; performsself-assessment; and is able to incorporate novel objects instead of relying on a limitedtraining set. It will be made publicly available.,*,*,*
Supplemental Material: Image Segmentation with Topic Random Field,Bin Zhao; Li Fei-Fei; Eric P Xing,The central challenge in using TRF is computing the posterior distribution of hiddenvariables given an image:(; c; z∣ x). In general; this distribution is intractable to computedue to the dependence between; c and z; once conditioned on some observations. Variousvariational inference algorithms have been proposed in the machine learning literature tosolve this problem. In this paper; we employ mean field variational inference to efficientlyobtain an approximation to this distribution. Specifically; mean field variational inferencealgorithm forms a factorized distribution of the latent variables; parameterized by freevariables known as variational parameters [1].(; z; c∣;;)=(∣),*,*,*
I ncremental,Li Fei-Fei,Page 1. Refe rence: L. -J. Li; G. W ang and L. F ei-F ei. OPTIMOL: automatic Objec t Pic turec ollecTion via Incremental MO del L earning . IEEE C omputer V ision and P a ttern Rec ognition(CVPR); M inneapolis; 2007. airplane a car c face f guitar r motorbike m watch w 76.0 14.0 0.35.3 0.3 0.3 4.8 1.0 94.5 0.3 4.5 0.3 0.3 0.3 0.5 1.4 82.9 3.7 0.5 0.5 11.5 2.2 4.9 5.6 60.4 13.30.2 13.3 1.0 2.0 1.0 5.0 89.0 1.0 2.0 0.3 5.5 0.3 5.5 1.0 67.3 20.5 1.7 5.5 17.7 11.0 5.5 5.0 53.6leopard l g Incremental Batch 050 100 150 200 250 300 350 Number of images ClassificationP e rformanc e (Dataset F e rgus et al. 2005) Incremental vs batch learning c omparison Numberof training images 10 20 30 40 50 60 70 80 90 100 020406080 100 120 140 160 180 BatchlearningIncremental learning Time(Sec) Detection rate 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 11 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 False alarm rate …,*,*,*
Free your Camera: 3D Indoor Scene Understanding from Arbitrary Camera Motion Supplementary Material,Axel Furlan; Stephen David Miller; Domenico G Sorrenti; Li Fei-Fei; Silvio Savarese,Here we briefly discuss the failure and success cases on the proposed dataset. In Table 1we present the classification accuracy for each sequence in the dataset achieved by thecompared methods; while in Figures 1; 2 and 3 we show the corresponding visual results.Figures are organized so that columns represents sequences (see the captions of theFigures); while rows are encoded with the following order:• First frame of the sequence.•Reconstruction result by [2].• Reconstruction result by [1].• Reconstruction result by theproposed method fed with VSLAM (projection).• Reconstruction result by the proposedmethod fed with VSLAM (3D model).• Reconstruction result by the proposed method fed withVSfM (projection).• Reconstruction result by the proposed method fed with VSfM (3D model).,*,*,*
Linking people in videos with “their” names using coreference resolution (Supplementary Material),Vignesh Ramanathan; Armand Joulin; Percy Liang; Li Fei-Fei,Page 1. Linking people in videos with “their” names using coreference resolution (SupplementaryMaterial) Vignesh Ramanathan∗; Armand Joulin†; Percy Liang†; Li Fei-Fei† ∗Department ofElectrical Engineering; Stanford University †Computer Science Department; Stanford University{vigneshr; armand; pliang; feifeili}@cs.stanford.edu 1 Episodes from the dataset The episodeswhich are part of Development and Test set are shown in Tab. 1. Development Set Test Set 1.Numb3rs 3x11 15. Highlander 5x14 2. Castle 1x03 16. Highlander 5x20 3. Highlander 5x02 17.Castle 1x09 4. Highlander 5x06 18. The Mentalist 1x19 5. The Mentalist 1x08 19. Californication1x01 6. The Mentalist 1x22 7. The Mentalist 3x11 8. The Good Wife 1x10 9. The Good Wife 1x2010. Twin Peaks 2x03 11. Desperate Housewives 1x04 12. 30 Rock 1x12 13. Sliders 4x02 14 …,*,*,*
Toáerráisáhuman: ácorrelatingáfMRIádecodingáandábehavioraláerrorsátoá probeátheáneuralárepresentationáofánaturalásceneácategoriesá,Dirk B Walther; Diane M Beck; Li Fei-Fei,Abstract New multivariate methods for the analysis of functional magnetic resonanceimaging (fMRI) data have enabled us to decode neural representations of visual informationwith unprecedented fidelity. But how do we know if humans make use of the information thatwe decode from the fMRI data for their behavioral response? In this chapter we propose amethod for correlating the errors from fMRI decoding with the errors made by subjects in abehavioral task. High correlations suggest that subjects use information that is closelyrelated to the content of the fMRI signal to make their behavioral response. We demonstratethe viability of this method using the example of natural scene categorization. Humans areextremely efficient at categorizing natural scenes (such as forests; highways; or beaches);despite the fact that different classes of natural scenes often share similar image statistics …,*,*,*
For λ> 1,Jia Deng; Jonathan Krause; Er C Berg; Li Fei-fei,Abstract We give a concrete example of discontinuous Φ (fλ) with respect to λ; ie; case (2) inSec. 4.2 of the main paper. Consider the simplest hierarchy: two leaf nodes a and b; plus aroot node c. Let the rewards ra= rb= 1and rc= 0. Let p (a| x)= Pr (Y= a| X= x) be the posteriorprobability of node a; which completely determines the posterior distribution. Assuming thatties are broken alphabetically; using Eqn. 6 in the main paper; it is easy to verify thefollowing. For 0≤ λ≤ 1; a; p (a| x)≥ 0.5 fλ (x)=,*,*,*
Action Classification: An Integration of Randomization and Discrimination in A Dense Feature Space,Bangpeng Yao; Aditya Khosla; Li Fei-Fei,Page 1. 1 Action Classification: An Integration of Randomization and Discrimination in A DenseFeature Space Computer Science Department; Stanford University {bangpeng;aditya86;feifeili}@cs.stanford.edu Bangpeng Yao; Aditya Khosla; and Li Fei-Fei Page 2. 2 • Action Classification& Intuition • Our Method • Our Results • Conclusion Outline Page 3. • Action Classification &Intuition • Our Method • Our Results • Conclusion Outline 3 Page 4. Action Classification 4 Objectclassification: Presence of parts and their spatial configurations. [Lazebnik et al; 2006] [Ferguset al; 2003] … Phoning RidingBike Running Page 5. Action Classification 5 • All images containhumans; Object classification: Presence of parts and their spatial configurations. [Lazebnik etal; 2006] [Fergus et al; 2003] … Page 6. Action Classification 6 • All images contain humans; •Large pose variation; Object classification …,*,*,*
School of Computer Science; Carnegie Mellon University 2 Computer Science Department; Stanford University,Gunhee Kim; Eric P Xing; Li Fei-Fei; Takeo Kanade,Theorem 1 (Submodularity on Anisotropic Diffusion). Suppose that the system undergoeslinear anisotropic diffusion. Let u (x; t; S) be the temperature at position x at time t whenidentical heat sources are attached to S (⊂ Ω). Then; the following statements hold for∀ x∈Ω;∀ t∈[0;∞].,*,*,*
Attention Influences the Neural Representation of Natural Scene Category,Eamon Caddigan; Audrey G Lustig; Li Fei-Fei; Diane M Beck,Page 1. Attention and Perception Lab Attention Influences the Neural Representation of NaturalScene Category Eamon Caddigan1;*; Audrey G. Lustig1;; Li Fei-Fei2; Diane M. Beck11University of Illinois at Urbana-Champaign; 2Stanford University; *ecaddiga@illinois.eduObservers can categorize images of natural scenes while engaged in a demanding task atfixation (Li et al.; 2002). Natural scene category can be “decoded” from patterns of fMRI activity(Walther et al.; 2009). fMRI activity can identify objects in unattended scenes (Peelen; Fei-Fei;Kastner; 2009). Can the category of unattended scenes be decoded from patterns of fMRI activity?Does attention affect natural scene category decoding accuracy? Introduction ParahippocampalPlace Area (PPA) Natural scene category was decoded from voxels in the PPA. Scene categorycan be decoded from PPA (Walther et al.; 2009) …,*,*,*
Decoding Perceived Natural Scene Categories from fMRI Activity in PPA and RSC,Dirk B Walther; Eamon Caddigan; Diane Beck; Li Fei-Fei,Functional magnetic resonance imaging (fMRI) has enabled understanding of manyphysiological processes in the brain. Traditionally; general linear models (GLMs) have beenused to contrast the BOLD activation for different experimental conditions [1]; treating eachvoxel independently. Given the highly interconnected nature of the brain; there is no a-priorireason to assume independence of voxels other than convenience of data analysis. Indeed;other groups have been able to use patterns of voxel activity to decode object identity [2; 3];orientation of gratings [4; 5]; or memory recall [6]. All these studies have used variants ofdiscriminant pattern classification algorithms; such as linear discriminant analysis (LDA) orsupport vector machines (SVM). Among the many important research results afforded bytraditional univariate analysis is the discovery of two visual areas with a general …,*,*,*
Construction and Analysis of a Large Scale Image Ontology,Li Fei-Fei; Hao Su; Minh Do; Kai Li; Jia Deng,J.Deng; W.Dong; R.Socher; L.-J.Li; K.Li and L.Fei-Fei. ImageNet: A Large-Scale HierarchicalImage Database. CVPR2009 … C.Fellbaum; WordNet: An Electronic Lexical Database;MITPress;1998 … L.von Ahn and L.Dabbish. Labeling images with a computer game. In CHI04;pages 319-326; 2004 … B.Russell; A.Torralba; K.Murphy; and W.Freeman. Labelme: A databaseand web-based tool for image annotation. IJCV; 77(1-3):157-173; May 2008 … B.Yao;X.Yang; and S.Zhu. Introduction to a large-scale general purpose ground truth database:Methodology; annotation tool and benchmarks. In EMMCVPR07; pages 169-183; 2007 …A.Torralba; R.Fergus; and W.Freeman. 80 million tiny images: A large data set for nonparametricobject and scene recognition. PAMI; 30(11):1958-1970; November 2008 … * synset name (#of synsets in the sub tree; average # of images per synset) … - Every node is a synonym …,*,*,*
TRECVID 2012 GENIE,Arash Vahdat; Kevin Cannons; Hossein Hajimirsadeghi; Greg Mori; Scott McCloskey; Ben Miller; Sharath Venkatesha; Pedro Davalos; Pradipto Das; Chenliang Xu; Jason Corso; Rohini Srihari; Ilseo Kim; You-Chi Cheng; Zhen Huang; Chin-Hui Lee; Kevin Tang; L Fei-Fei; Daphne Koller,Abstract Our MED 12 system is an extension of our MED 11 system [12]; and consists of acollection of lowlevel and high-level features; feature-specific classifiers built upon thosefeatures; and a fusion system that combines features both through mid-level kernel fusionand score fusion. We have incorporated large number of audio-visual features in our newsystem and incorporated diverse types of standard and newly developed event agentswhich learn the salient audio-visual characteristics of event classes. The combination ofadditional features and newly developed powerful event agents improve our MEDperformance substantially beyond our MED 11 results. In addition; our MER 12 submissionsreported recounting of specified clips for all five MER events and additionally provided MERresults for all the clips detected by MED system. Our MER system generated recounting of …,*,*,*
Unsupervised Learning of Human Action Categories,Juan Carlos Niebles; Hongcheng Wang; Li Fei-Fei,Imagine a video taken on a sunny beach; can a computer automatically tell what ishappening in the scene? Can it identify different human activities in the video; such as watersurfing; people walking and lying on the beach? To automatically classify or localizedifferent actions in video sequences is very useful for a variety of tasks; such as videosurveillance; objectlevel video summarization; video indexing; digital library organization;etc. However; it remains a challenging task for computers to achieve robust actionrecognition due to cluttered background; camera motion; occlusion; and geometric andphotometric variances of objects. For example; in a live video of a skating competition; theskater moves rapidly across the rink; and the camera also moves to follow the skater. Withmoving camera; non-stationary background; and moving target; few vision algorithms …,*,*,*
