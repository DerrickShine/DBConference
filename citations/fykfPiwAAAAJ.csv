Runtime measurements in the cloud: observing; analyzing; and reducing variance,Jörg Schad; Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Abstract One of the main reasons why cloud computing has gained so much popularity isdue to its ease of use and its ability to scale computing resources on demand. As a result;users can now rent computing nodes on large commercial clusters through several vendors;such as Amazon and rackspace. However; despite the attention paid by Cloud providers;performance unpredictability is a major issue in Cloud computing for (1) databaseresearchers performing wall clock experiments; and (2) database applications providingservice-level agreements. In this paper; we carry out a study of the performance variance ofthe most widely used Cloud infrastructure (Amazon EC2) from different perspectives. We useestablished microbenchmarks to measure performance variance in CPU; I/O; and network.And; we use a multi-node MapReduce application to quantify the impact on real …,Proceedings of the VLDB Endowment,2010,548
Hadoop++: Making a yellow elephant run like a cheetah (without it even noticing),Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Alekh Jindal; Yagiz Kargin; Vinay Setty; Jörg Schad,Abstract MapReduce is a computing paradigm that has gained a lot of attention in recentyears from industry and research. Unlike parallel DBMSs; MapReduce allows non-expertusers to run complex analytical tasks over very large data sets on very large clusters andclouds. However; this comes at a price: MapReduce processes tasks in a scan-orientedfashion. Hence; the performance of Hadoop---an open-source implementation ofMapReduce---often does not match the one of a well-configured parallel DBMS. In thispaper we propose a new type of system named Hadoop++: it boosts task performancewithout changing the Hadoop framework at all (Hadoop does not even'notice it'). To reachthis goal; rather than changing a working system (Hadoop); we inject our technology at theright places through UDFs only and affect Hadoop from inside. This has three important …,Proceedings of the VLDB Endowment,2010,464
Efficient big data processing in Hadoop MapReduce,Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Abstract This tutorial is motivated by the clear need of many organizations; companies; andresearchers to deal with big data volumes efficiently. Examples include web analyticsapplications; scientific applications; and social networks. A popular data processing enginefor big data is Hadoop MapReduce. Early versions of Hadoop MapReduce suffered fromsevere performance problems. Today; this is becoming history. There are many techniquesthat can be used with Hadoop MapReduce jobs to boost performance by orders ofmagnitude. In this tutorial we teach such techniques. First; we will briefly familiarize theaudience with Hadoop MapReduce and motivate its use for big data processing. Then; wewill focus on different data management techniques; going from job optimization to physicaldata organization like data layouts and indexes. Throughout this tutorial; we will highlight …,Proceedings of the VLDB Endowment,2012,189
iDM: A unified and versatile data model for personal dataspace management,Jens-Peter Dittrich; Marcos Antonio Vaz Salles,Abstract Personal Information Management Systems require a powerful and versatile datamodel that is able to represent a highly heterogeneous mix of data such as relational data;XML; file content; folder hierarchies; emails and email attachments; data streams; RSS feedsand dynamically computed documents; eg ActiveXML [3]. Interestingly; until now noapproach was proposed that is able to represent all of the above data in a single; powerfulyet simple data model. This paper fills this gap. We present the iMeMex Data Model (iDM) forpersonal information management. iDM is able to represent unstructured; semi-structuredand structured data inside a single model. Moreover; iDM is powerful enough to representgraph-structured data; intensional data as well as infinite data streams. Further; our modelenables to represent the structural information available inside files. As a consequence …,Proceedings of the 32nd international conference on Very large data bases,2006,171
Progressive Merge Join: A Generic and Non-Blocking Sort-Based Join Algorithm** This work has been supported by grant no. SE 553/2-2 from DFG.,Jens-Peter Dittrich; Bernhard Seeger; David Scot Taylor; Peter Widmayer,This chapter presents a generic technique called progressive merge join (PMJ) thateliminates the blocking behavior of sort-based join algorithms. The basic idea behind PMJ isto have the join produce results; as early as the external mergesort generates initial runs.Many state-of-the-art join techniques require the input relations to be almost fully sortedbefore the actual join processing starts. Thus; these techniques start producing first resultsonly after a considerable time has passed. This blocking behavior is a serious problemwhen consequent operators have to stop processing in order to wait for first results of thejoin. Furthermore; this behavior is not acceptable if the result of the join is visualized or/andrequires user interaction. These are typical scenarios for data mining applications. The off-time of existing techniques even increases with growing problem sizes.Progressive …,*,2002,124
Only Aggressive Elephants are Fast Elephants,Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz; Stefan Richter; Stefan Schuh; Alekh Jindal; Jörg Schad,Abstract Yellow elephants are slow. A major reason is that they consume their inputs entirelybefore responding to an elephant rider's orders. Some clever riders have trained their yellowelephants to only consume parts of the inputs before responding. However; the teachingtime to make an elephant do that is high. So high that the teaching lessons often do not payoff. We take a different approach. We make elephants aggressive; only this will make themvery fast. We propose HAIL (Hadoop Aggressive Indexing Library); an enhancement ofHDFS and Hadoop MapReduce that dramatically improves runtimes of several classes ofMapReduce jobs. HAIL changes the upload pipeline of HDFS in order to create differentclustered indexes on each data block replica. An interesting feature of HAIL is that wetypically create a win-win situation: we improve both data upload to HDFS and the …,Proceedings of the VLDB Endowment,2012,122
XXL-a library approach to supporting efficient implementations of advanced database queries,J v Bercken; Björn Blohsfeld; Jens-Peter Dittrich; Jürgen Krämer; Tobias Schäfer; Martin Schneider; Bernhard Seeger,Abstract Today's DBMS are still too inflexible to adapt fast enough to the query processingneeds of new applications [CW00]. Instead of using the cumbersome functionality of amonolithic DBMS; it is not uncommon that users implement their own functionality on top ofthe system. For such a scenario; the implementation would be substantially facilitatedthrough a powerful library. This paper introduces XXL (eXtensible and fleX-ible Library); ahigh-level; easy-to-use; platform independent Java library supporting the implementation ofnew query functionality. XXL provides framework implementations as well as toolboxeswhose applications are independent from the underlying data types and data structures. Weintroduce the most important concepts of XXL and discuss different application scenarioswhere XXL has been used recently. In particular; we show how an implementation of an …,Proc. of the Conf. on Very Large Databases (VLDB),2001,112
Data redundancy and duplicate detection in spatial join processing,J-P Dittrich; Bernhard Seeger,The partition-based spatial-merge join (PBSM) of JM Patel and DJ DeWitt (1996) and thesize separation spatial join (S/sup 3/J) of N. Koudas and KC Sevcik (1997) are considered tobe among the most efficient methods for processing spatial (intersection) joins on two ormore spatial relations. Neither method assumes the presence of pre-existing spatial indiceson the relations. In this paper; we propose several improvements to these join algorithms. Inparticular; we deal with the impact of data redundancy and duplicate detection on theperformance of these methods. For PBSM; we present a simple and inexpensive onlinemethod to detect duplicates in the response set. There is no longer any need to eliminateduplicates in a final sorting phase; as was originally suggested. We also investigate theimpact of different internal algorithms on the total run-time of PBSM. For S/sup 3/J; we …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,100
Trojan data layouts: right shoes for a running elephant,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract MapReduce is becoming ubiquitous in large-scale data analysis. Several recentworks have shown that the performance of Hadoop MapReduce could be improved; forinstance; by creating indexes in a non-invasive manner. However; they ignore the impact ofthe data layout used inside data blocks of Hadoop Distributed File System (HDFS). In thispaper; we analyze different data layouts in detail in the context of MapReduce and arguethat Row; Column; and PAX layouts can lead to poor system performance. We propose anew data layout; coined Trojan Layout; that internally organizes data blocks into attributegroups according to the workload in order to improve data access times. A salient feature ofTrojan Layout is that it fully preserves the fault-tolerance properties of MapReduce. Weimplement our Trojan Layout idea in HDFS 0.20. 3 and call the resulting system Trojan …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,99
A dataspace odyssey: The iMeMex personal dataspace management system,Lukas Blunschi; Jens-Peter Dittrich; Olivier René Girard; Shant Kirakos Karakashian; Marcos Antonio Vaz Salles,ABSTRACT A Personal Dataspace includes all data pertaining to a user on all his localdisks and on remote servers such as network drives; email and web servers. This data isrepresented by a heterogeneous mix of files; emails; bookmarks; music; pictures; calendar;personal information streams and so on. We demonstrate a new breed of system that is ableto handle the entire Personal Dataspace of a user. Our system; named iMeMex (integratedmemex); is a first implementation of a Personal DataSpace Management System (PDSMS).Visions for this type of systems have been proposed recently [13; 10; 12; 17]. We showcasehow iMeMex allows dataspace navigation across data source/file boundaries; how iMeMexoffers rich contextual information on query results and how our system returns best-effortresults.,CIDR,2007,97
iTrails: pay-as-you-go information integration in dataspaces,Marcos Antonio Vaz Salles; Jens-Peter Dittrich; Shant Kirakos Karakashian; Olivier René Girard; Lukas Blunschi,Abstract Dataspace management has been recently identified as a new agenda forinformation management [17; 22] and information integration [23]. In sharp contrast tostandard information integration architectures; a dataspace management system is a data-coexistence approach: it does not require any investments in semantic integration beforequerying services on the data are provided. Rather; a dataspace can be gradually enhancedover time by defining relationships among the data. Defining those integration semanticsgradually is termed pay-as-you-go information integration [17]; as time and effort (pay) areneeded over time (go) to provide integration semantics. The benefits are better query results(gain). This paper is the first to explore pay-as-you-go information integration in dataspaces.We provide a technique for declarative pay-as-you-go information integration named …,Proceedings of the 33rd international conference on Very large data bases,2007,93
Indexing moving objects using short-lived throwaway indexes,Jens Dittrich; Lukas Blunschi; Marcos Antonio Vaz Salles,Abstract With the exponential growth of moving objects data to the Gigabyte range; it hasbecome critical to develop effective techniques for indexing; updating; and querying thesemassive data sets. To meet the high update rate as well as low query response timerequirements of moving object applications; this paper takes a novel approach in movingobject indexing. In our approach we do not require a sophisticated index structure thatneeds to be adjusted for each incoming update. Rather we construct conceptually simpleshort-lived throwaway indexes which we only keep for a very short period of time (sub-seconds) in main memory. As a consequence; the resulting technique MOVIES supports atthe same time high query rates and high update rates and trades this for query resultstaleness. Moreover; MOVIES is the first main memory method supporting time …,International Symposium on Spatial and Temporal Databases,2009,67
GESS: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces,Jens-Peter Dittrich; Bernhard Seeger,Abstract The similarity join is an important operation for mining high-dimensional featurespaces. Given two data sets; the similarity join computes all tuples (x; y) that are within adistance &egr;. One of the most efficient algorithms for processing similarity-joins is theMultidimensional-Spatial Join (MSJ) by Koudas and Sevcik. In our previous work---pursuedfor the two-dimensional case---we found however that MSJ has several performanceshortcomings in terms of CPU and I/O cost as well as memory-requirements. Therefore; MSJis not generally applicable to high-dimensional data. In this paper; we propose a newalgorithm named Generic External Space Sweep (GESS). GESS introduces a modest rate ofdata replication to reduce the number of expensive distance computations. We present anew cost-model for replication; an I/O model; and an inexpensive method for duplicate …,Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,2001,66
RAFTing MapReduce: Fast recovery on the RAFT,Jorge-Arnulfo Quiane-Ruiz; Christoph Pinkel; Jörg Schad; Jens Dittrich,MapReduce is a computing paradigm that has gained a lot of popularity as it allows non-expert users to easily run complex analytical tasks at very large-scale. At such scale; taskand node failures are no longer an exception but rather a characteristic of large-scalesystems. This makes fault-tolerance a critical issue for the efficient operation of anyapplication. MapReduce automatically reschedules failed tasks to available nodes; which inturn recompute such tasks from scratch. However; this policy can significantly decreaseperformance of applications. In this paper; we propose a family of Recovery Algorithms forFast-Tracking (RAFT) MapReduce. As ease-of-use is a major feature of MapReduce; RAFTfocuses on simplicity and also non-intrusiveness; in order to be implementation-independent. To efficiently recover from task failures; RAFT exploits the fact that …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,61
Agile: Adaptive indexing for context-aware information filters,Jens-Peter Dittrich; Peter M Fischer; Donald Kossmann,Abstract Information filtering has become a key technology for modern information systems.The goal of an information filter is to route messages to the right recipients (possibly none)according to declarative rules called profiles. In order to deal with high volumes ofmessages; several index structures have been proposed in the past. The challengeaddressed in this paper is to carry out stateful information filtering in which profiles refer tovalues in a database or to previous messages. The difficulty is that database update streamsneed to be processed in addition to messages. This paper presents AGILE; a way to extendexisting index structures so that the indexes adapt to the message/update workload andshow good performance in all situations. Performance experiments show that AGILE isoverall the clear winner as compared to the best existing approaches. In extreme …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,58
Relax and let the database do the partitioning online,Alekh Jindal; Jens Dittrich,Abstract Vertical and Horizontal partitions allow database administrators (DBAs) toconsiderably improve the performance of business intelligence applications. However;finding and defining suitable horizontal and vertical partitions is a daunting task even forexperienced DBAs. This is because the DBA has to understand the physical query executionplans for each query in the workload very well to make appropriate design decisions. Tofacilitate this process several algorithms and advisory tools have been developed over thepast years. These tools; however; still keep the DBA in the loop. This means; the physicaldesign cannot be changed without human intervention. This is problematic in situationswhere a skilled DBA is either not available or the workload changes over time; eg due tonew DB applications; changed hardware; an increasing dataset size; or bursts in the …,International Workshop on Business Intelligence for the Real-Time Enterprise,2011,50
Towards a One Size Fits All Database Architecture.,Jens Dittrich; Alekh Jindal,ABSTRACT We propose a new type of database system coined OctopusDB. Our approachsuggests a unified; one size fits all data processing architecture for OLTP; OLAP; streamingsystems; and scan-oriented database systems. OctopusDB radically departs from existingarchitectures in the following way: it uses a logical event log as its primary storage structure.To make this approach efficient we introduce the concept of Storage Views (SV); iesecondary; alternative physical data representations covering all or subsets of the primarylog. OctopusDB (1) allows us to use different types of SVs for different subsets of the data;and (2) eliminates the need to use different types of database systems for differentapplications. Thus; based on the workload; OctopusDB emulates different types of systems(row stores; column stores; streaming systems; and more importantly; any hybrid …,CIDR,2011,44
iMeMex: A platform for personal dataspace management,Jens-Peter Dittrich,ABSTRACT Desktop computers provide thousands of different applications that query andstore data in hundreds of thousands of files of different formats. Those files are stored in thelocal filesystem and also in a number of remote data sources; such as network shares or asattachements to emails. To handle this heterogeneous and distributed mix of personalinformation; data processing logic is reinvented inside each application. This results in anunfortunate situation: most advanced data management functionality; such as complexqueries; backup and recovery; versioning; provenance tracking; among others; is (at leastpartially) performed by endusers in tedious; manual tasks. To solve these problems wepropose a software platform named iMeMex that brings physical and logical dataindependence to the desktop; freeing users from low-level data management …,Proceedings of Workshops of International ACM SIGIR Conference on Research and Development in Information Retrieval; ACM,2006,39
The repeatability experiment of SIGMOD 2008,Ioana Manolescu; Loredana Afanasiev; Andrei Arion; Jens Dittrich; Stefan Manegold; Neoklis Polyzotis; Karl Schnaitter; Pierre Senellart; Spyros Zoupanos; Dennis Shasha,Abstract SIGMOD 2008 was the first database conference that offered to test submitters'programs against their data to verify the experiments published. This paper discusses therationale for this effort; the community's reaction; our experiences; and advice for futuresimilar efforts.,ACM SIGMOD Record,2008,38
iMeMex: escapes from the personal information jungle,Jens-Peter Dittrich; Marcos Antonio Vaz Salles; Donald Kossmann; Lukas Blunschi,Abstract Modern computer work stations provide thousands of applications that store datain> 100.000 files on the file system of the underlying OS. To handle these files dataprocessing logic is reinvented inside each application. This results in a jungle of dataprocessing solutions and a jungle of data and file formats. For a user; it is extremely hard tomanage information in this jungle. Most of all it is impossible to use data distributed amongdifferent files and formats for combined queries; eg; join and union operations. To solve theproblems arising from file based data management; we present a software system callediMeMex as a unified solution to personal information management and integration. iMeMexis designed to integrate seamlessly into existing operating systems like Windows; Linux andMac OS X. Our system enables existing applications to gradually dispose file based …,Proceedings of the 31st international conference on Very large data bases,2005,34
Towards zero-overhead static and adaptive indexing in Hadoop,Stefan Richter; Jorge-Arnulfo Quiané-Ruiz; Stefan Schuh; Jens Dittrich,Abstract Hadoop MapReduce has evolved to an important industry standard for massiveparallel data processing and has become widely adopted for a variety of use-cases. Recentworks have shown that indexes can improve the performance of selective MapReduce jobsdramatically. However; one major weakness of existing approaches is high index creationcosts. We present HAIL (Hadoop Aggressive Indexing Library); a novel indexing approachfor HDFS and Hadoop MapReduce. HAIL creates different clustered indexes over terabytesof data with minimal; often invisible costs; and it dramatically improves runtimes of severalclasses of MapReduce jobs. HAIL features two different indexing pipelines; static indexingand adaptive indexing. HAIL static indexing efficiently indexes datasets while uploadingthem to HDFS. Thereby; HAIL leverages the default replication of Hadoop and enhances …,The VLDB journal,2014,32
MOVIES: indexing moving objects by shooting index images,Jens Dittrich; Lukas Blunschi; Marcos Antonio Vaz Salles,Abstract With the exponential growth of moving objects data to the Gigabyte range; it hasbecome critical to develop effective techniques for indexing; updating; and querying thesemassive data sets. To meet the high update rate as well as low query response timerequirements of moving object applications; this paper takes a novel approach in movingobject indexing. In our approach; we do not require a sophisticated index structure thatneeds to be adjusted for each incoming update. Rather; we construct conceptually simpleshort-lived index images that we only keep for a very short period of time (sub-seconds) inmain memory. As a consequence; the resulting technique MOVIES supports at the sametime high query rates and high update rates; trading this property for query result staleness.Moreover; MOVIES is the first main memory method supporting time-parameterized …,Geoinformatica,2011,32
The uncracked pieces in database cracking,Felix Martin Schuhknecht; Alekh Jindal; Jens Dittrich,Abstract Database cracking has been an area of active research in recent years. The coreidea of database cracking is to create indexes adaptively and incrementally as a side-product of query processing. Several works have proposed different cracking techniques fordifferent aspects including updates; tuple-reconstruction; convergence; concurrency-control;and robustness. However; there is a lack of any comparative study of these differentmethods by an independent group. In this paper; we conduct an experimental study ondatabase cracking. Our goal is to critically review several aspects; identify the potential; andpropose promising directions in database cracking. With this study; we hope to expand thescope of database cracking and possibly leverage cracking in database engines other thanMonetDB. We repeat several prior database cracking works including the core cracking …,Proceedings of the VLDB Endowment,2013,30
Intensional associations in dataspaces,Marcos Antonio Vaz Salles; Jens Dittrich; Lukas Blunschi,Dataspace applications necessitate the creation of associations among data items over time.For example; once information about people is extracted from sources on the Web;associations among them may emerge as a consequence of different criteria; such as theircity of origin or their elected hobbies. In this paper; we advocate a declarative approach tospecifying these associations. We propose that each set of associations be defined by anassociation trail. An association trail is a query-based definition of how items are connectedby intensional (ie; virtual) association edges to other items in the dataspace. We study theproblem of processing neighborhood queries over such intensional association graphs. Thenaive approach to neighborhood query processing over intensional graphs is to materializethe whole graph and then apply previous work on dataspace graph indexing to answer …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,26
Towards zero-overhead adaptive indexing in hadoop,Stefan Richter; Jorge-Arnulfo Quiané-Ruiz; Stefan Schuh; Jens Dittrich,Abstract: Several research works have focused on supporting index access in MapReducesystems. These works have allowed users to significantly speed up selective MapReducejobs by orders of magnitude. However; all these proposals require users to create indexesupfront; which might be a difficult task in certain applications (such as in scientific and socialapplications) where workloads are evolving or hard to predict. To overcome this problem; wepropose LIAH (Lazy Indexing and Adaptivity in Hadoop); a parallel; adaptive approach forindexing at minimal costs for MapReduce systems. The main idea of LIAH is to automaticallyand incrementally adapt to users' workloads by creating clustered indexes on HDFS datablocks as a byproduct of executing MapReduce jobs. Besides distributing indexing effortsover multiple computing nodes; LIAH also parallelises indexing with both map tasks …,arXiv preprint arXiv:1212.3480,2012,25
A comparison of knives for bread slicing,Alekh Jindal; Endre Palatinus; Vladimir Pavlov; Jens Dittrich,Abstract Vertical partitioning is a crucial step in physical database design in row-orienteddatabases. A number of vertical partitioning algorithms have been proposed over the lastthree decades for a variety of niche scenarios. In principle; the underlying problem remainsthe same: decompose a table into one or more vertical partitions. However; it is not clearhow good different vertical partitioning algorithms are in comparison to each other. In fact; itis not even clear how to experimentally compare different vertical partitioning algorithms. Inthis paper; we present an exhaustive experimental study of several vertical partitioningalgorithms. We categorize vertical partitioning algorithms along three dimensions. Wesurvey six vertical partitioning algorithms and discuss their pros and cons. We identify themajor differences in the use-case settings for different algorithms and describe how to …,Proceedings of the VLDB Endowment,2013,24
Bridging the gap between OLAP and SQL,Jens-Peter Dittrich; Donald Kossmann; Alexander Kreutz,Abstract In the last ten years; database vendors have invested heavily in order to extendtheir products with new features for decision support. Examples of functionality that has beenadded are top N [2]; ranking [13; 7]; spreadsheet computations [19]; grouping sets [14]; datacube [9]; and moving sums [15] in order to name just a few. Unfortunately; many modernOLAP systems do not use that functionality or replicate a great deal of it in addition to otherdatabase-related functionality. In fact; the gap between the functionality provided by anOLAP system and the functionality used from the underlying database systems has widenedin the past; rather than narrowed. The reasons for this trend are that SQL as a data definitionand query language; the relational model; and the client/server architecture of the currentgeneration of database products have fundamental shortcomings for OLAP. This paper …,Proceedings of the 31st international conference on Very large data bases,2005,24
Interesting-phrase mining for ad-hoc text analytics,Srikanta Bedathur; Klaus Berberich; Jens Dittrich; Nikos Mamoulis; Gerhard Weikum,Abstract Large text corpora with news; customer mail and reports; or Web 2.0 contributionsoffer a great potential for enhancing business-intelligence applications. We propose aframework for performing text analytics on such data in a versatile; efficient; and scalablemanner. While much of the prior literature has emphasized mining keywords or tags in blogsor social-tagging communities; we emphasize the analysis of interesting phrases. Theseinclude named entities; important quotations; market slogans; and other multi-word phrasesthat are prominent in a dynamically derived ad-hoc subset of the corpus; eg; being frequentin the subset but relatively infrequent in the overall corpus. We develop preprocessing andindexing methods for phrases; paired with new search techniques for the top-k mostinteresting phrases in ad-hoc subsets of the corpus. Our framework is evaluated using a …,Proceedings of the VLDB Endowment,2010,23
Efficient or Hadoop: why not both?,Jens Dittrich; Stefan Richter; Stefan Schuh,Abstract In this article; we give an overview of research related to Big Data processing inHadoop going on at the Information Systems Group at Saarland University. We discuss howto make Hadoop efficient. We briefly survey three of our projects in this context: Hadoop++;Trojan Layouts; and HAIL.,Datenbank-Spektrum,2013,22
On producing join results early,Jens-Peter Dittrich; Bernhard Seeger; David Scot Taylor; Peter Widmayer,Abstract Support for exploratory interaction with databases in applications such as datamining requires that the first few results of an operation be available as quickly as possible.We study the algorithmic side of what can and what cannot be achieved for processing joinoperations. We develop strategies that modify the strict two-phase processing of the sort-merge paradigm; intermingling join steps with selected merge phases of the sort. Wepropose an algorithm that produces early join results for a broad class of join problems;including many not addressed well by hash-based algorithms. Our algorithm has nosignificant increase in the number of I/O operations needed to complete the join compared tostandard sort-merge algorithms.,Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2003,22
javax. XXL: A prototype for a Library of Query processing Algorithms,Jochen Van den Bercken; Jens-Peter Dittrich; Bernhard Seeger,Abstract Therefore; index structures can easily be used in queries. A typical example is ajoin cursor which consumes the outputs of two underlying cursors. Most of our work ishowever not dedicated to the area of relational databases; but mainly refers to spatial andtemporal data. For spatial databases; for example; we provide several implementations ofspatial join algorithms [3]. The cursor-based processing is however the major advantage ofXXL in contrast to approaches like LEDA [6] and TPIE [7]. For more information on XXL seehttp://www. mathematik. uni-marburg. de/DBS/xxl. We will demonstrate the latest version ofXXL using examples to show its core functionality. We will concentrate on three key aspectsof XXL. Usage: We show how easily state-of-the-art spatial join-algorithms can beimplemented in XXL using data from different sources. Reuse: We will demonstrate how …,ACM SIGMOD Record,2000,21
A seven-dimensional analysis of hashing methods and its implications on query processing,Stefan Richter; Victor Alvarez; Jens Dittrich,Abstract Hashing is a solved problem. It allows us to get constant time access for lookups.Hashing is also simple. It is safe to use an arbitrary method as a black box and expect goodperformance; and optimizations to hashing can only improve it by a negligible delta. Why areall of the previous statements plain wrong? That is what this paper is about. In this paper wethoroughly study hashing for integer keys and carefully analyze the most common hashingmethods in a five-dimensional requirements space:(1) data-distribution;(2) load factor;(3)dataset size;(4) read/write-ratio; and (5) un/successful-ratio. Each point in that design spacemay potentially suggest a different hashing scheme; and additionally also a different hashfunction. We show that a right or wrong decision in picking the right hashing scheme andhash function combination may lead to significant difference in performance. To …,Proceedings of the VLDB Endowment,2015,20
From Personal Desktops to Personal Dataspaces: A Report on Building the iMeMex Personal Dataspace Management System.,Jens-Peter Dittrich; Lukas Blunschi; Markus Färber; Olivier René Girard; Shant Kirakos Karakashian; Marcos Antonio Vaz Salles,Abstract: We propose a new system that is able to handle the entire Personal Dataspace of auser. A Personal Dataspace includes all data pertaining to a user on all his disks and onremote servers such as network drives; email and web servers. This data is represented by aheterogeneous mix of files; emails; bookmarks; music; pictures; calendar data; personalinformation streams and so on. State-of-the-art tools such as desktop search engines anddesktop operating systems (including the upcoming Vista) are not enough as they neithersolve the problem of physical personal information independence (where is my data) norformat and data model independence (how is it stored and which application do I have touse in order to access that data). Our work builds on the visions presented in [DSKB05];which calls for a single system to manage the personal information jungle; and [FHM05] …,BTW,2007,19
WWHow! Freeing Data Storage from Cages,Alekh Jindal; Jorge-Arnulfo Quiané-Ruiz; Jens Dittrich,Abstract. Efficient data storage is a key component of data managing systems to achievegood performance. However; currently data storage is either heavily constrained by staticdecisions (eg fixed data stores in DBMSs) or left to be tuned and configured by users (egmanual data backup in File Systems). In this paper; we take a holistic view of data storageand envision a virtual storage layer. Our virtual storage layer provides a unified storageframework for several use-cases including personal; enterprise; and cloud storage.,*,2013,18
Dwarfs in the rearview mirror: how big are they really?,Jens Dittrich; Lukas Blunschi; Marcos Antonio Vaz Salles,Abstract Online-Analytical Processing (OLAP) has been a field of competing technologies forthe past ten years. One of the still unsolved challenges of OLAP is how to provide quickresponse times on any Terabyte-sized business data problem. Recently; a very clever multi-dimensional index structure termed Dwarf [26] has been proposed offering excellent queryresponse times as well as unmatched index compression rates. The proposed index seemsto scale well for both large data sets as well as high dimensions. Motivated by thesesurprisingly excellent results; we take a look into the rearview mirror. We have re-implemented the Dwarf index from scratch and make three contributions. First; wesuccessfully repeat several of the experiments of the original paper. Second; wesubstantially correct some of the experimental results reported by the inventors. Some of …,Proceedings of the VLDB Endowment,2008,17
An experimental comparison of thirteen relational equi-joins in main memory,Stefan Schuh; Xiao Chen; Jens Dittrich,Abstract Relational equi-joins are at the heart of almost every query plan. They have beenstudied; improved; and reexamined on a regular basis since the existence of the databasecommunity. In the past four years several new join algorithms have been proposed andexperimentally evaluated. Some of those papers contradict each other in their experimentalfindings. This makes it surprisingly hard to answer a very simple question: what is the fastestjoin algorithm in 2015? In this paper we will try to develop an answer. We start with an end-to-end black box comparison of the most important methods. Afterwards; we inspect theinternals of these algorithms in a white box comparison. We derive improved variants ofstate-of-the-art join algorithms by applying optimizations like~ software-write combinebuffers; various hash table implementations; as well as NUMA-awareness in terms of data …,Proceedings of the 2016 International Conference on Management of Data,2016,15
Raft at work: speeding-up mapreduce applications under task and node failures,Jorge-Arnulfo Quiané-Ruiz; Christoph Pinkel; Jörg Schad; Jens Dittrich,Abstract The MapReduce framework is typically deployed on very large computing clusterswhere task and node failures are no longer an exception but the rule. Thus; fault-tolerance isan important aspect for the efficient operation of MapReduce jobs. However; currentlyMapReduce implementations fully recompute failed tasks (subparts of a job) from thebeginning. This can significantly decrease the runtime performance of MapReduceapplications. We present an alternative system that implements RAFT ideas. RAFT is afamily of powerful and inexpensive Recovery Algorithms for Fast-Tracking MapReduce jobsunder task and node failures. To recover from task failures; RAFT exploits the intermediateresults persisted by MapReduce at several points in time. RAFT piggybacks checkpoints onthe task progress computation. To recover from node failures; RAFT maintains a per-map …,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,15
Main memory adaptive indexing for multi-core systems,Victor Alvarez; Felix Martin Schuhknecht; Jens Dittrich; Stefan Richter,Abstract Adaptive indexing is a concept that considers index creation in databases as a by-product of query processing; as opposed to traditional full index creation where the indexingeffort is performed up front before answering any queries. Adaptive indexing has received aconsiderable amount of attention; and several algorithms have been proposed over the pastfew years; including a recent experimental study comparing a large number of existingmethods. Until now; however; most adaptive indexing algorithms have been designed single-threaded; yet with multi-core systems already well established; the idea of designing parallelalgorithms for adaptive indexing is very natural. In this regard; and to the best of ourknowledge; only one parallel algorithm for adaptive indexing has recently appeared in theliterature: The parallel version of standard cracking. In this paper we describe three …,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,14
Fast aggregation of compressed data using full table scans,*,Methods and apparatus; including computer systems and program products; relating to aninformation management system and aggregating data by performing table scans. Ingeneral; in one aspect; the technique includes receiving a query for a response to a searchon a database; loading data from the database into memory; filtering the data based on thequery to generate a list of results; buffering at least one key figure corresponding to a result;buffering at least one dimension value corresponding to each key figure; aggregating thedimension values to generate an aggregate key; aggregating key figures corresponding tothe same aggregate key to generate one or more aggregate key figures; and displaying theresponse to the search on a display device. Loading the data may include compressing thedata. Filtering the data may be performed blockwise.,*,2007,12
On the surprising difficulty of simple things: the case of radix partitioning,Felix Martin Schuhknecht; Pankaj Khanchandani; Jens Dittrich,Abstract Partitioning a dataset into ranges is a task that is common in various applicationssuch as sorting [1; 6; 7; 8; 9] and hashing [3] which are in turn building blocks for almost anytype of query processing. Especially radix-based partitioning is very popular due to itssimplicity and high performance over comparison-based versions [6].,Proceedings of the VLDB Endowment,2015,11
How Achaeans Would Construct Columns in Troy,Alekh Jindal; Felix Martin Schuhknecht; Jens Dittrich; Karen Khachatryan; Alexander Bunte,ABSTRACT Column stores are becoming popular with data analytics in modern enterprises.However; traditionally; database vendors offer column stores as a different database productall together. As a result there is an all-or-none situation for column store features. To bridgethe gap; a recent effort introduced column store functionality in SQL server (a row store) bymaking deep seated changes in the database system. However; this approach is expensivein terms of time and effort. In addition; it is limited to SQL server. In this paper; we presentTrojan Columns; a novel technique for injecting column store functionality into a givenclosed source row-oriented commercial database system. Trojan Columns does not needaccess to the source code of the database system. Instead; it uses UDFs as a pluggablestorage layer to write and read data. Furthermore; Trojan Columns is transparent to users …,*,2013,11
A comparison of adaptive radix trees and hash tables,Victor Alvarez; Stefan Richter; Xiao Chen; Jens Dittrich,With prices of main memory constantly decreasing; people nowadays are more interested inperforming their computations in main memory; and leave high I/O costs of traditional disk-based systems out of the equation. This change of paradigm; however; represents newchallenges to the way data should be stored and indexed in main memory in order to beprocessed efficiently. Traditional data structures; like the venerable B-tree; were designed towork on disk-based systems; but they are no longer the way to go in main-memory systems;at least not in their original form; due to the poor cache utilization of the systems they run on.Because of this; in particular; during the last decade there has been a considerable amountof research on index data structures for main-memory systems. Among the most recent andmost interesting data structures for main-memory systems there is the recently-proposed …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,9
Bringing Precision to Desktop Search: A Predicate-based Desktop Search Architecture,Jens-Peter Dittrich; Cristian Duda; Bjorn Jarisch; Donald Kossmann; Marcos Antonio Vaz Salles,Google and other products have revolutionized the way we search for information on theInternet; Intranet; and on our desktop. However; the current generation of search productsdoes not exploit the structure and semantics of data; as defined by application programs (eg;Word or Excel) that generate the data. This paper shows how search technology can beenhanced with implicit predicates; in order to take into account the structure and semanticsdefined by applications. Better search results are produced with tolerable performanceoverhead; while at the same time maintaining the simplicity of the keyword search interface.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,9
Air: adaptive index replacement in hadoop,Stefan Schuh; Jens Dittrich,The Hadoop Distributed Filesystem has become the de-facto standard for storing largedatasets in data management systems such as Hadoop MapReduce; Hive; andStratosphere. Though HDFS was originally designed to support scan-oriented operations;recently several techniques for HDFS have been developed to allow for efficient indexing.One of these indexing techniques is aggressive indexing; ie HDFS replicas are immediatelyindexed at upload time before touching any disk-creating multiple clustered indexes almostfor free on the way. A second technique is adaptive indexing; ie HDFS blocks are onlyindexed on demand as a side effect of query processing. Though these techniques provideimpressive speed-ups in terms of query processing; they totally ignored the costs involvedwith storing a large number of replicas of a particular dataset. The HDFS-variants of …,Data Engineering Workshops (ICDEW); 2015 31st IEEE International Conference on,2015,7
iMeMex: From Search to Information Integration and Back.,Jens Dittrich; Marcos Antonio Vaz Salles; Lukas Blunschi,Abstract In this paper; we report on lessons learned while building iMeMex; the firstincarnation of a Personal Dataspace Management System (PDSMS). In contrast totraditional keyword search engines; users may not only search their collections with iMeMexbut also semantically integrate them over time. As a consequence; the system may improveon precision and recall of queries in a pay-as-you-go fashion. We discuss the impact ofseveral conceptual and architectural decisions made in iMeMex and outline open researchchallenges in combining search and information integration in order to build personaldataspace management systems.,IEEE Data Eng. Bull.,2009,7
Elephant; do not forget everything! efficient processing of growing datasets,Jorg Schad; Jorge-Arnulfo Quianee-Ruiz; Jens Dittrich,MapReduce has become quite popular to analyse very large datasets. Nevertheless; userstypically have to run their MapReduce jobs over the whole dataset every time the dataset isappended by new records. Some researchers have proposed to reuse the intermediate dataproduced by previous MapReduce jobs. However; existing works still have to read the wholedataset in order to identify which parts of the dataset changed. Furthermore; storingintermediate results is not suitable in some cases; because it can lead to a very high storageoverhead. In this paper; we propose Itchy; a MapReduce-based system that employes a setof different techniques to efficiently deal with growing datasets. Itchy uses an optimizer toautomatically choose the right technique to process a MapReduce job. The beauty of Itchy isthat it does not have to read the whole dataset again to deal with new records. In more …,Cloud Computing (CLOUD); 2013 IEEE Sixth International Conference on,2013,6
Merging partial query results into a single result,*,A method and system for executing an information retrieval query in a multiserver computingenvironment is disclosed. The method and system employ a technique in which the query isdistributed among each of a plurality of partial index servers in the multiserver environment;and a subset of results is calculated for each of the plurality of partial index servers. Then;the subset of results are merged in one logical index server to generate a merged result.,*,2008,6
Adding structure to web search with itrails [position paper],Marcos Antonio Vaz Salles; Jens Dittrich; Lukas Blunschi,We would like to discuss with workshop participants the iTrails framework for pay-as-you-goinformation integration; which was recently presented at VLDB 2007 [1]. iTrails allows usersto provide mini-mappings on their data that sharply increase the quality of search results.The core idea is to extend the semantics of a standard graphical search engine such that thequality of search results approaches the quality of a full-blown information integrationsystem. In contrast to [1]; this paper shows how iTrails can be used to tackle the challengesof adding structured information support to web search engines. We will show how iTrailsenriches a web search engine with a powerful query rewriting mechanism enabling thisengine to perform not only search but also integration of structured information.,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,6
Paper bricks: an alternative to complete-story peer reviewing,Jens Dittrich,Abstract The peer review system as used in several computer science communities hasseveral flaws including long review times; overloaded reviewers; as well as fostering ofniche topics. These flaws decrease quality; lower impact; slowdown the innovation process;and lead to frustration of authors; readers; and reviewers. In order to fix this; we propose anew peer review system termed paper bricks. Paper bricks has several advantages over theexisting system including shorter publications; better competition for new ideas; as well asan accelerated innovation process. Furthermore; paper bricks may be implemented withminimal change to the existing peer review systems.,ACM SIGMOD Record,2011,5
Operational analytics data management systems,Alexander Böhm; Jens Dittrich; Niloy Mukherjee; Ippokratis Pandis; Rajkumar Sen,Abstract Prior to mid-2000s; the space of data analytics was mainly confined within the areaof decision support systems. It was a long era of isolated enterprise data ware housescurating information from live data sources and of business intelligence software used toquery such information. Most data sets were small enough in volume and static enoughinvelocity to be segregated in warehouses for analysis. Data analysis was not ad-hoc; itrequired pre-requisite knowledge of underlying data access patterns for the creation ofspecialized access methods (eg covering indexes; materialized views) in order to efficientlyexecute a set of few focused queries.,Proceedings of the VLDB Endowment,2016,3
RUMA has it: rewired user-space memory access is possible!,Felix Martin Schuhknecht; Jens Dittrich; Ankur Sharma,Abstract Memory management is one of the most boring topics in database research. It playsa minor role in tasks like free-space management or efficient space usage. Here and therewe also realize its impact on database performance when worrying about NUMA-awarememory allocation; data compacting; snapshotting; and defragmentation. But; overall; let'sface it: the entire topic sounds as exciting as' garbage collection'or'debugging a program formemory leaks'. What if there were a technique that would promote memory managementfrom a third class helper thingie to a first class citizen in algorithm and systems design? Whatif that technique turned the role of memory management in a database system (and anyother data processing system) upside-down? What if that technique could be identified as akey for re-designing various core algorithms with the effect of outperforming existing state …,Proceedings of the VLDB Endowment,2016,3
The Case for Small Data Management.,Jens Dittrich,ABSTRACT Exabytes of data; several hundred thousand TPC-C transactions per second ona single computing core; scale-up to hundreds of cores and a dozen Terabytes of mainmemory; scale-out to thousands of nodes with close to Petabyte-sized main memories; andmassively parallel query processing are a reality in data management. But; hold on asecond: for how many users exactly? How many users do you know that really have tohandle these kinds of massive datasets and extreme query workloads? On the other hand:how many users do you know that are fighting to handle relatively small datasets; say in therange of a few thousand to a few million rows per table? How come some of the mostpopular open source DBMS have hopelessly outdated optimizers producing inefficient queryplans? How come people don't care and love it anyway? Could it be that most of the …,CIDR,2015,3
Keyword Search on Application Data,Jens-Peter Dittrich; Cristian Duda; Björn Jarisch; Donald Kossmann; Marcos Antonio Vaz Salles,Abstract Google and other products have revolutionized the way we search for informationon the Internet; Intranet; and on our desktop. However; the current generation of searchproducts does not exploit the structure and semantics of data; as defined by applicationprograms (eg; Word or Excel) that generate the data. This paper shows how searchtechnology can be enhanced with implicit predicates; in order to take into account thestructure and semantics defined by applications. Better search results are produced withtolerable performance overhead; while at the same time maintaining the simplicity of the,*,2006,3
Accelerated query refinement by instant estimation of results,*,Methods and apparatus; including computer systems and program products; for processingqueries for which a solution requires that an information management system performlogical operations on a data repository. In general; in one aspect; the techniques feature amethod of executing queries on a data repository. That method includes receiving a query;adapted for execution on a data set in the data repository; defining a sample of the data set;where the sample is a subset of the data set; executing the query on the sample; generatingan estimate of a result of the execution of the query on the sample; and providing theestimate to a user interface. The method may further include defining an Nth sample; suchthat the Nth sample is larger than an (N− 1) th sample; and generating an Nth estimate of theresult based on the execution of the query on the Nth sample.,*,2005,3
An experimental evaluation and analysis of database cracking,Felix Martin Schuhknecht; Alekh Jindal; Jens Dittrich,Abstract Database cracking has been an area of active research in recent years. The coreidea of database cracking is to create indexes adaptively and incrementally as a sideproduct of query processing. Several works have proposed different cracking techniques fordifferent aspects including updates; tuple reconstruction; convergence; concurrency control;and robustness. Our 2014 VLDB paper “The Uncracked Pieces in DatabaseCracking”(PVLDB 7: 97–108; 2013/VLDB 2014) was the first comparative study of thesedifferent methods by an independent group. In this article; we extend our publishedexperimental study on database cracking and bring it to an up-to-date state. Our goal is tocritically review several aspects; identify the potential; and propose promising directions indatabase cracking. With this study; we hope to expand the scope of database cracking …,The VLDB Journal,2016,2
Die Umgedrehte Vorlesung–Chancen für die Informatiklehre,Jens Dittrich,Zusammenfassung Dieser Artikel beschreibt die Erfahrung des Autors mit einerUmgedrehten Datenbankvorlesung. Bei einer Umgedrehten Vorlesung findet dieWissensaneignung zunächst mit Hilfe von Lehrvideos statt. In der ursprünglichenVorlesungszeit wird das gelernte Material dann gemeinsam eingeübt. DiesesVorlesungsformat ist ein hybrides Format zwischen traditioneller Vorlesung und densogenannten MOOCs (Massive Open Online Courses).,Datenbank-Spektrum,2014,2
Managing personal information using iTrails,Jens Dittrich; Marcos Antonio Vaz Salles; Lukas Blunschi,ABSTRACT We would like to present and discuss the iTrails framework for pay-as-you-goinformation integration which was recently presented at VLDB 2007 [7]. iTrails allows usersto provide mini-mappings on their data that sharply increase the quality of search results.The core idea is to extend the semantics of a standard graphical search engine such that thequality of search results approaches the quality of a full-blown information integrationsystem. We would like to discuss the impact of iTrails on the CHI-side of PersonalInformation Management Tools. In particular; we would like to better understand how toprovide interfaces and tools that allow end-users to make use of our framework.,Proe of the 3rd Personal Information Management Workshop (PIM'08). New York: ACM,2008,2
iMeMex: A Unified Approach to Personal Information Management,JP Dittrich; D Kossmann,*,SNF project under contract,*,2
Runtime Fragility in Main Memory,Endre Palatinus; Jens Dittrich,Abstract In this paper we investigate the following problem: Given a database workload(tables and queries); which data layout (row; column or a suitable PAX-layout) should wechoose in order to get the best possible performance? We show that this is not an easyproblem. We explore careful combinations of various parameters that have an impact on theperformance including:(1) the schema;(2) the CPU architecture;(3) the compiler; and (4) theoptimization level. We include a CPU from each of the past 4 generations of Intel CPUs. Inaddition; we demonstrate the importance of taking variance into account when deciding onthe optimal storage layout. We observe considerable variance throughout ourmeasurements which makes it difficult to argue along means over different runs of anexperiment. Therefore; we compute confidence intervals for all measurements and exploit …,*,2016,1
Janiform intra-document analytics for reproducible research,Jens Dittrich; Patrick Bender,Abstract Peer-reviewed publication of research papers is a cornerstone of science.However; one of the many issues of our publication culture is that our publications onlypublish a summary of the final result of a long project. This means that we put well-polishedgraphs describing (some) of our experimental results into our publications. However; thealgorithms; input datasets; benchmarks; raw result datasets; as well as scripts that wereused to produce the graphs in the first place are rarely published and typically not availableto other researchers. Often they are only available when personally asking the authors. Inmany cases; however; they are not available at all. This means from a long workflow that ledto producing a graph for a research paper; we only publish the final result rather than theentire workflow. This is unfortunate and has been criticized in various scientific …,Proceedings of the VLDB Endowment,2015,1
Replicated data storage system and methods,*,When analyzing a large web log; for example; the web log may contain different fields like'visitDate'; 'adRevenue' and 'sourceIP' that may serve as filter conditions. In the existing HDFSand Hadoop MapReduce stack; this log file may be uploaded to HDFS using the HDFSclient. HDFS then partitions the file into logical blocks using a constant block size (the HDFSdefault is 64 MB). Each block is then physically stored three times (assuming the default replicationfactor of three). Each physical copy of a block is called a replica. Each replica will sit on a differentdata node. Therefore; at least two node failures may be tolerated by HDFS. Information on thedifferent replicas for an HDFS block is kept in a central name node directory … After uploadinghis log file to HDFS; one may run an actual Map Reduce job. Assuming a user is interested inall source IPs with a visit date from 2011; a map-only MapReduce program may be …,*,2015,1
Method of Storing and Accessing Data in a Database System,*,A method of storing and accessing data in a database system is disclosed. The databasesystem comprises at least one primary data source. The database system is associated withat least one adapted data structure that defines the physical data storage structures (eg; rowstorage and columnar storage) in which the data are stored. Data is allocated from the atleast one primary data source to the at least one adapted data structure in correlation with adatabase query received. For example; based on the data access patterns (eg; queries); thephysical data storage structures in which the data managed by the database system are tobe stored are dynamically determined.,*,2013,1
Method of Storing and Accessing Data in a Database System,*,A method of storing and accessing data in a database system is disclosed. The databasesystem comprises at least one primary data source. The database system is associated withat least one adapted data structure that defines the physical data storage structures (eg; rowstorage and columnar storage) in which the data are stored. Data is allocated from the atleast one primary data source to the at least one adapted data structure in correlation with adatabase query received. For example; based on the data access patterns (eg; queries); thephysical data storage structures in which the data managed by the database system are tobe stored are dynamically determined.,*,2013,1
Mosquito: another one bites the data upload stream,Stefan Richter; Jens Dittrich; Stefan Schuh; Tobias Frey,Abstract Mosquito is a lightweight and adaptive physical design framework for Hadoop.Mosquito connects to existing data pipelines in Hadoop MapReduce and/or HDFS; observesthe data; and creates better physical designs; ie indexes; as a byproduct. Our approach isminimally invasive; yet it allows users and developers to easily improve the runtime ofHadoop. We present three important use cases: first; how to create indexes as a byproductof data uploads into HDFS; second; how to create indexes as a byproduct of map tasks; andthird; how to execute map tasks as a byproduct of HDFS data uploads. These use cases mayeven be combined.,Proceedings of the VLDB Endowment,2013,1
A method of storing and accessing data in a database system,*,A method of storing and accessing data in a database system is disclosed. The databasesystem comprises at least one primary data source. The database system is associated withat least one adapted data structure that defines the physical data storage structures (eg; rowstorage and columnar storage) in which the data are stored. Data is allocated from the atleast one primary data source to the at least one adapted data structure in correlation with adatabase query received. For example; based on the data access patterns (eg; queries); thephysical data storage structures in which the data managed by the database system are tobe stored are dynamically determined.,*,2011,1
The iMeMex Dataspace Management System: Architecture; Concepts; and Lessons Learned,Jens Dittrich,Abstract The iMeMex Project was one of the first systems trying to build a so-calleddataspace management system. This tutorial presents the core concepts of iMeMex. Wediscuss system design concepts; dataspace modelling; dataspace indexing; dataspacequery processing; and pay-as-you-go information integration. We will present someimportant lessons learned from this project and also discuss ongoing and open researchchallenges.,British National Conference on Databases,2009,1
Automatic elimination of functional dependencies between columns,*,In business systems; one or more methods can be used to reduce an amount of redundantdata. In one implementation; a method to reduce redundancy within a data model in adatabase; in which the data model is represented by at least one table; includes determininga number of distinct values of partial keys in a table. Each partial key represents at least onerow in the table. The method includes reordering one or more columns of the table bycardinality of partial keys; in which the cardinality of a partial key represents a number ofdistinct values of the partial key. The method further includes determining whether pairs ofpartial keys are functionally dependent and eliminating one or more columns havingfunctional dependencies from the table.,*,2008,1
Introduction to Dataspaces,Jens-Peter Dittrich,*,*,2006,1
The Case for Automatic Database Administration using Deep Reinforcement Learning,Ankur Sharma; Felix Martin Schuhknecht; Jens Dittrich,Abstract: Like any large software system; a full-fledged DBMS offers an overwhelmingamount of configuration knobs. These range from static initialisation parameters like buffersizes; degree of concurrency; or level of replication to complex runtime decisions likecreating a secondary index on a particular column or reorganising the physical layout of thestore. To simplify the configuration; industry grade DBMSs are usually shipped with variousadvisory tools; that provide recommendations for given workloads and machines. However;reality shows that the actual configuration; tuning; and maintenance is usually still done by ahuman administrator; relying on intuition and experience. Recent work on deepreinforcement learning has shown very promising results in solving problems; that requiresuch a sense of intuition. For instance; it has been applied very successfully in learning …,arXiv preprint arXiv:1801.05643,2018,*
Accelerating Analytical Processing in MVCC using Fine-Granular High-Frequency Virtual Snapshotting,Ankur Sharma; Felix Martin Schuhknecht; Jens Dittrich,Abstract: Efficient transactional management is a delicate task. As systems face transactionsof inherently different types; ranging from point updates to long running analyticalcomputations; it is hard to satisfy their individual requirements with a single processingcomponent. Unfortunately; most systems nowadays rely on such a single component thatimplements its parallelism using multi-version concurrency control (MVCC). While MVCCparallelizes short-running OLTP transactions very well; it struggles in the presence of mixedworkloads containing long-running scan-centric OLAP queries; as scans have to work theirway through large amounts of versioned data. To overcome this problem; we propose asystem; which reintroduces the concept of heterogeneous transaction processing: OLAPtransactions are outsourced to run on separate (virtual) snapshots while OLTP …,arXiv preprint arXiv:1709.04284,2017,*
Data processing in a multiple processor system to maintain multiple processor cache memory access coherency,*,A data processing system including multiple processors with a hierarchical cache structurecomprising multiple levels of cache between the processors and a main memory; wherein atleast one page mover is positioned closer to the main memory and is connected to thecache memories of the at least one shared cache level (L2; L3; L4); the main memory and tothe multiple processors to move data between the cache memories of the at least oneshared cache level; the main memory and the processors. In response to a request from oneof the processors; the at least one page mover fetches data of a storage area line-wise fromat least one of the following memories: the cache memories and the main memorymaintaining multiple processor cache memory access coherency.,*,2017,*
An Experimental Analysis of Different Key-Value Stores and Relational Databases,David Gembalczyk; Felix Martin Schuhknecht; Jens Dittrich,Nowadays; databases serve two main workloads: Online Transaction Processing (OLTP)and Online Analytic Processing (OLAP). For decades; relational databases dominated bothareas. With the hype on NoSQL databases; the picture has changed. Initially designed asinter-process hash tables handling OLTP requested; some key-value store vendors havestarted to tackle the area of OLAP as well. Therefore; in this performance study; we comparethe relational databases PostgreSQL; MonetDB; and HyPer with the key-value stores Redisand Aerospike in their write; read; and analytical capabilities. Based on the results; weinvestigate the reasons of the database's respective advantages and disadvantages.,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
Automatic Elimination Of Functional Dependencies Between Columns,*,In business systems; one or more methods can be used to reduce an amount of redundantdata. In one implementation; a method to reduce redundancy within a data model in adatabase; in which the data model is represented by at least one table; includes determininga number of distinct values of partial keys in a table. Each partial key represents at least onerow in the table. The method includes reordering one or more columns of the table bycardinality of partial keys; in which the cardinality of a partial key represents a number ofdistinct values of the partial key. The method further includes determining whether pairs ofpartial keys are functionally dependent and eliminating one or more columns havingfunctional dependencies from the table.,*,2012,*
Scheduling Strategies in a Main-Memory MapReduce Framework; Approach for countering Reduce side skew,Mahendiran Venkatachalapathy; Jens Dittrich; Jorge-Arnulfo Quiané-Ruiz,Zusammenfassung Over the past few decades; there is a multifold increase in the amount ofdigital data that is being generated. Various attempts are being made to process this vastamount of data in a fast and efficient manner. Hadoop-MapReduce is one such softwareframework that has gained popularity in the last few years. It provides a reliable and easierway to process huge amount of data in-parallel on large computing cluster. However;Hadoop always persists intermediate results to the local disk. As a result; Hadoop usuallysuffers from long execution runtimes as it typically pays a high I/O cost for running jobs. Thestate-of-the-art computing clusters have enough main memory capacity to hold terabytes ofdata in main memory. We have built M3R (Main Memory MapReduce) framework; aprototype for generic main memory-based data processing. M3R can execute …,*,2012,*
Automatic reduction of table memory footprint using column cardinality information,*,In a business system; one or more methods can be used to reduce an amount ofredundancy in the storage of data. One implementation includes a method of reducing amemory footprint of a database table having multiple rows and one or more columns; inwhich each of the one or more columns has a cardinality; and the cardinality is a totalnumber of different values in the rows of each column. The method includes comparing thecardinality with a total number of possible values in the rows of at least one column basedon a width of the column. The method also includes reducing the width of the column if thecardinality is less than a threshold based on the total number of possible values in the rowsof the column.,*,2010,*
Fast aggregation of compressed data using full table scans,*,Methods and apparatus; including computer systems and program products; relating to aninformation management system and aggregating data by performing table scans. Ingeneral; in one aspect; the technique includes receiving a query for a response to a searchon a database; loading data from the database into memory; filtering the data based on thequery to generate a list of results; buffering at least one key figure corresponding to a result;buffering at least one dimension value corresponding to each key figure; aggregating thedimension values to generate an aggregate key; aggregating key figures corresponding tothe sane aggregate key to generate one or more aggregate key figures; and displaying theresponse to the search on a display device. Loading the data may include compressing thedata. Filtering the data may be performed blockwise.,*,2010,*
Quality in Phrase Mining,Alekh Jindal; Jens Dittrich; Gerhard Weikum,Abstract Phrase snippets of large text corpora like news articles or web search results offergreat insight and analytical value. While much of the prior work is focussed on efficientstorage and retrieval of all candidate phrases; little emphasis has been laid on the quality ofthe result set. In this thesis; we define phrases of interest and propose a framework formining and post-processing interesting phrases. We focus on the quality of phrases anddevelop techniques to mine minimal-length maximal-informative sequences of words. Thetechniques developed are streamed into a post-processing pipeline and include exact andapproximate match-based merging; incomplete phrase detection with filtering; and heuristics-based phrase classification. The strategies aim to prune the candidate set of phrases downto the ones being meaningful and having rich content. We characterize the phrases with …,*,2009,*
Scalable phrase mining for ad-hoc text analytics,Srikanta Bedathur; Klaus Berberich; Jens Dittrich; Nikos Mamoulis; Gerhard Weikum,Abstract Large text corpora with news; customer mail and reports; or Web 2.0 contributionsoffer a great potential for enhancing business-intelligence applications. We propose aframework for performing text analytics on such data in a versatile; efficient; and scalablemanner. While much of the prior literature has emphasized mining keywords or tags in blogsor social-tagging communities; we emphasize the analysis of interesting phrases. Theseinclude named entities; important quotations; market slogans; and other multi-word phrasesthat are prominent in a dynamically derived ad-hoc subset of the corpus; eg; being frequentin the subset but relatively infrequent in the overall corpus. The ad-hoc subset may bederived by means of a keyword query against the corpus; or by focusing on a particular timeperiod. We investigate alternative definitions of phrase interestingness; based on the …,*,2009,*
Adding structure to web search with itrails [position paper],Marcos Antonio Vaz Salles; Jens Dittrich; Lukas Blunschi,Abstract We would like to discuss with workshop participants the iTrails framework for pay-as-you-go information integration; which was recently presented at VLDB 2007 [1]. iTrailsallows users to provide mini-mappings on their data that sharply increase the quality ofsearch results. The core idea is to extend the semantics of a standard graphical searchengine such that the quality of search results approaches the quality of a full-blowninformation integration system. In contrast to [1]; this paper shows how iTrails can be used totackle the challenges of adding structured information support to web search engines. Wewill show how iTrails enriches a web search engine with a powerful query rewritingmechanism enabling this engine to perform not only search but also integration of structuredinformation.,Proceedings of the 2008 IEEE 24th International Conference on Data Engineering Workshop,2008,*
Automatic reduction of table memory footprint using column cardinality information,*,In a business system; one or more methods can be used to reduce an amount ofredundancy in the storage of data. One implementation includes a method of reducing amemory footprint of a database table having multiple rows and one or more columns; inwhich each of the one or more columns has a cardinality; and the cardinality is a totalnumber of different values in the rows of each column. The method includes comparing thecardinality with a total number of possible values in the rows of at least one column basedon a width of the column. The method also includes reducing the width of the column if thecardinality is less than a threshold based on the total number of possible values in the rowsof the column.,*,2007,*
iMeMex: A dataspace odyssey: The iMeMex personal dataspace management system,J-P Dittrich,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,3rd Biennial Conf. on Innovative Data Systems Research (CIDR); California; USA; 2007,2007,*
Integration of Data Stream Management into an eHealth Digital Library,Charalampos Dimitropoulos; Yannis Ioannidis; Jens-Peter Dittrich; Peter Fischer; Donald Kossmann; Gert Brettlecker; Hans-J Schek; Heiko Schuldt,Recent trends in ubiquitous and pervasive computing; together with new sensortechnologies; wireless communication standards; powerful mobile devices and wearablecomputers strongly support novel types of applications. Especially in healthcare; tele-monitoring applications will make use of this new technology in order to improve the qualityof treatment and care for patients and the elderly. In particular; if we consider our agingsociety; the amount of elderly people suffering from one or more chronic diseases isincreasing. Tele-monitoring applications enable healthcare institutions to take care of theirpatients while they are out of hospital; which is especially useful for managing variouschronic diseases as well as for measuring the effects of treatments under real-life conditions.A similar but even more comprehensive application is the support for the elderly and for …,DELOS Research Activities 2005,2005,*
Management of and Access to Virtual Electronic Health Records,Robert Penz; Raimund Vogl; Wilhelm Hasselbring; Ulrike Steffens; Charalampos Dimitropoulos; Yannis Ioannidis; Jens-Peter Dittrich; Peter Fischer; Donald Kossmann; Christoph Langguth; Thomas Schabetsberger; Hans-Jörg Schek; Heiko Schuldt; Michael Springmann,Research Objectives The realization of these goals first requires the availability ofappropriate (web) services in order to access relevant data managed by specializedapplications which are hosted by different healthcare organizations. In addition; commonstandards have to be supported to integrate these legacy applications (eg; the PACSapplication where the X-rays of Y and Z are stored) or; alternatively; dedicated services areneeded to transform the format of the data retrieved from one application so as to make isavailable for other; subsequent services. Second; an infrastructure to combine theseservices into processes is needed that is highly dependable and reliable. Physicians mustbe given the guarantee that the system and data is always available (ie; by means oflaboratory,*,2005,*
Management of and Access to Virtual Electronic Health Records,Charalampos Dimitropoulos; Jens-Peter Dittrich; Peter Fischer; Wilhelm Hasselbring; Yannis Ioannidis; Donald Kossmann; Christoph Langguth; Robert Penz; Thomas Schabetsberger; Hans-Jörg Schek; Heiko Schuldt; Michael Springmann; Ulrike Steffens; Raimund Vogl,1 Introduction eHealth digital libraries contain electronic artifacts that are generated bydifferent healthcare providers (family doctors; laboratories; hospitals; etc.). An importantobservation is that this information is not stored at one central instance but rather under thecontrol of the organization where data has been produced. The electronic health record ofpatients therefore consists of a set of distributed artifacts and cannot be materialized fororganizational reasons. Rather; the electronic patient record is a virtual entity and has to begenerated by composing the required artifacts each time it is accessed (Figure 1). The virtualintegration of an electronic patient record is done by encompassing services provided byspecialized application systems (eg; CIS–clinical information systems; or PACS–picturearchiving and communication system) into processes. A process to access a virtual …,*,2005,*
Generische Joinverarbeitung am Beispiel des Similarity Join,Jens-Peter Dittrich,Zusammenfassung Der iA hnlichkeitsverbund (engl.: Similarity Join) ist eine bedeutende Grundoperation fiur z ahlreiche Data Mining A nwendungen wie C lustering und O utlierDetection. Ein Similarity Join z weier Mengen hochdimensionaler Punkte R und S berechnetalle Tupel (r; s); die fiur eine gegebene Distan z funktion einen A bstand ε nichtiuberschreiten. A lle bekannten e ffiz ienten Similarity Join Verfahren miussen dieEingaberelationen vor der eigentlichen Joinberechnung z uniachst sortieren. Wiahrenddieser Sortieroperation produ z ieren diese Joinverfahren keine Ergebnistupel undblockieren den Daten fl u ß innerhalb des A nfragebaums. Diese Verfahren sind somitungeeignet fiur wichtige A nwendungen wie z. B. online Data Mining.,*,2002,*
iMeMex: A Platform for Personal Dataspace Management,Marcos Antonio Vaz Salles; Jens-Peter Dittrich,ABSTRACT Desktop computers provide thousands of different applications that query andstore data in hundreds of thousands of files of different formats. Those files are stored in thelocal filesystem and also in a number of remote data sources; such as network shares or asattachments to emails. To handle this heterogeneous and distributed mix of personalinformation; data processing logic is re-invented inside each application. This results in anundesirable situation: most advanced data management functionality; such as complexqueries; backup and recovery; versioning; provenance tracking; among others; is (at leastpartially) performed by end-users in tedious; manual tasks. To solve these problems wepropose a software platform that brings physical and logical data independence to thedesktop; freeing users from low-level data management activities. Unlike current …,*,*,*
