Extracting information nuggets from disaster-related messages in social media.,Muhammad Imran; Shady Elbassuoni; Carlos Castillo; Fernando Diaz; Patrick Meier,ABSTRACT Microblogging sites such as Twitter can play a vital role in spreading informationduring “natural” or man-made disasters. But the volume and velocity of tweets posted duringcrises today tend to be extremely high; making it hard for disaster-affected communities andprofessional emergency responders to process the information in a timely manner.Furthermore; posts tend to vary highly in terms of their subjects and usefulness; frommessages that are entirely off-topic or personal in nature; to messages containing criticalinformation that augments situational awareness. Finding actionable information canaccelerate disaster response and alleviate both property and human losses. In this paper;we describe automatic methods for extracting information from microblog posts. Specifically;we focus on extracting valuable “information nuggets”; brief; self-contained information …,Iscram,2013,172
Natural language questions for the web of data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Maya Ramanath; Volker Tresp; Gerhard Weikum,Abstract The Linked Data initiative comprises structured databases in the Semantic-Webdata model RDF. Exploring this heterogeneous data by structured query languages istedious and error-prone even for skilled users. To ease the task; this paper presents amethodology for translating natural language questions into structured SPARQL queriesover linked-data sources. Our method is based on an integer linear program to solve severaldisambiguation tasks jointly: the segmentation of questions into phrases; the mapping ofphrases to semantic entities; classes; and relations; and the construction of SPARQL triplepatterns. Our solution harnesses the rich type system provided by knowledge bases in theweb of linked data; to constrain our semantic-coherence objective function. We presentexperiments on both the question translation and the resulting query answering.,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,142
Practical extraction of disaster-relevant information from social media,Muhammad Imran; Shady Elbassuoni; Carlos Castillo; Fernando Diaz; Patrick Meier,Abstract During times of disasters online users generate a significant amount of data; someof which are extremely valuable for relief efforts. In this paper; we study the nature of social-media content generated during two different natural disasters. We also train a model basedon conditional random fields to extract valuable information from such content. We evaluateour techniques over our two datasets through a set of carefully designed experiments. Wealso test our methods over a non-disaster dataset to show that our extraction model is usefulfor extracting information from socially-generated content in general.,Proceedings of the 22nd International Conference on World Wide Web,2013,103
Language-model-based ranking for queries on RDF-graphs,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Marcin Sydow; Gerhard Weikum,Abstract The success of knowledge-sharing communities like Wikipedia and the advances inautomatic information extraction from textual and Web sources have made it possible tobuild large" knowledge repositories" such as DBpedia; Freebase; and YAGO. Thesecollections can be viewed as graphs of entities and relationships (ER graphs) and can berepresented as a set of subject-property-object (SPO) triples in the Semantic-Web datamodel RDF. Queries can be expressed in the W3C-endorsed SPARQL language or bysimilarly designed graph-pattern search. However; exact-match query semantics often fallshort of satisfying the users' needs by returning too many or too few results. Therefore; IR-style ranking models are crucially needed. In this paper; we propose a language-model-based approach to ranking the results of exact; relaxed and keyword-augmented graph …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,86
Keyword search over RDF graphs,Shady Elbassuoni; Roi Blanco,Abstract Large knowledge bases consisting of entities and relationships between them havebecome vital sources of information for many applications. Most of these knowledge basesadopt the Semantic-Web data model RDF as a representation model. Querying theseknowledge bases is typically done using structured queries utilizing graph-patternlanguages such as SPARQL. However; such structured queries require some expertise fromusers which limits the accessibility to such data sources. To overcome this; keyword searchmust be supported. In this paper; we propose a retrieval model for keyword queries overRDF graphs. Our model retrieves a set of subgraphs that match the query keywords; andranks them based on statistical language models. We show that our retrieval modeloutperforms the-state-of-the-art IR and DB models for keyword search over structured …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,74
Ming: mining informative entity relationship subgraphs,Gjergji Kasneci; Shady Elbassuoni; Gerhard Weikum,Abstract Many modern applications are faced with the task of knowledge discovery in entity-relationship graphs; such as domain-specific knowledge bases or social networks. Miningan" informative" subgraph that can explain the relations between k (>= 2) given entities ofinterest is a frequent knowledge discovery scenario on such graphs. We present MING; aprincipled method for extracting an informative subgraph for given query nodes. MING buildson a new notion of informativeness of nodes. This is used in a random-walk-with-restartsprocess to compute the informativeness of entire subgraphs.,Proceedings of the 18th ACM conference on Information and knowledge management,2009,67
Searching RDF Graphs with SPARQL and Keywords.,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities like Wikipedia and theadvances in automated information extraction from Web pages enable the construction oflarge knowledge bases with facts about entities and their relationships. The facts can berepresented in the RDF data model; as so-called subject-property-object triples; and canthus be queried by structured query languages like SPARQL. In principle; this allows precisequerying in the database spirit. However; RDF data may be highly diverse and queries mayreturn way too many results; so that ranking by informativeness measures is crucial to avoidoverwhelming users. Moreover; as facts are extracted from textual contexts or havecommunity-provided annotations; it can be beneficial to consider also,IEEE Data Eng. Bull.,2010,59
Robust question answering over the web of linked data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Gerhard Weikum,Abstract Knowledge bases and the Web of Linked Data have become important assets forsearch; recommendation; and analytics. Natural-language questions are a user-friendlymode of tapping this wealth of knowledge and data. However; question answeringtechnology does not work robustly in this setting as questions have to be translated intostructured queries and users have to be careful in phrasing their questions. This paperadvocates a new approach that allows questions to be partially translated into relaxedqueries; covering the essential but not necessarily all aspects of the user's input. Tocompensate for the omissions; we exploit textual sources associated with entities andrelational facts. Our system translates user questions into an extended form of structuredSPARQL queries; with text predicates attached to triple patterns. Our solution is based on …,Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,2013,54
Query relaxation for entity-relationship search,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract Entity-relationship-structured data is becoming more important on the Web. Forexample; large knowledge bases have been automatically constructed by informationextraction from Wikipedia and other Web sources. Entities and relationships can berepresented by subject-property-object triples in the RDF model; and can then be preciselysearched by structured query languages like SPARQL. Because of their Boolean-matchsemantics; such queries often return too few or even no results. To improve recall; it is thusdesirable to support users by automatically relaxing or reformulating queries in such a waythat the intention of the original user query is preserved while returning a sufficient numberof ranked results. In this paper we describe comprehensive methods to relax SPARQL-liketriple-pattern queries in a fully automated manner. Our framework produces a set of …,Extended Semantic Web Conference,2011,48
Matching task profiles and user needs in personalized web search,Julia Luxenburger; Shady Elbassuoni; Gerhard Weikum,Abstract Personalization has been deemed one of the major challenges in informationretrieval with a significant potential for providing better search experience to individualusers. Especially; the need for enhanced user models better capturing elements such asusers' goals; tasks; and contexts has been identified. In this paper; we introduce a statisticallanguage model for user tasks representing different granularity levels of a user profile;ranging from very specific search goals to broad topics. We propose a personalizationframework that selectively matches the actual user information need with relevant past usertasks; and allows to dynamically switch the course of personalization from re-finding veryprecise information to biasing results to general user interests. In the extreme; our model isable to detect when the user's search and browse history is not appropriate for aiding the …,Proceedings of the 17th ACM conference on Information and knowledge management,2008,43
Deep answers for naturally asked questions on the web of data,Mohamed Yahya; Klaus Berberich; Shady Elbassuoni; Maya Ramanath; Volker Tresp; Gerhard Weikum,Abstract We present DEANNA; a framework for natural language question answering overstructured knowledge bases. Given a natural language question; DEANNA translatesquestions into a structured SPARQL query that can be evaluated over knowledge basessuch as Yago; Dbpedia; Freebase; or other Linked Data sources. DEANNA analyzesquestions and maps verbal phrases to relations and noun phrases to either individualentities or semantic classes. Importantly; it judiciously generates variables for target entitiesor classes to express joins between multiple triple patterns. We leverage the semantic typesystem for entities and use constraints in jointly mapping the constituents of the question torelations; classes; and entities. We demonstrate the capabilities and interface of DEANNA;which allows advanced users to influence the translation process and to see how the …,Proceedings of the 21st international conference on World Wide Web,2012,38
NAGA: harvesting; searching and ranking knowledge,Gjergji Kasneci; Fabian M Suchanek; Georgiana Ifrim; Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract The presence of encyclopedic Web sources; such as Wikipedia; the Internet MovieDatabase (IMDB); World Factbook; etc. calls for new querying techniques that are simpleand yet more expressive than those provided by standard keyword-based search engines.Searching for explicit knowledge needs to consider inherent semantic structures involvingentities and relationships. In this demonstration proposal; we describe a semantic searchsystem named NAGA. NAGA operates on a knowledge graph; which contains millions ofentities and relationships derived from various encyclopedic Web sources; such as the onesabove. NAGA's graph-based query language is geared towards expressing queries withadditional semantic information. Its scoring model is based on the principles of generativelanguage models; and formalizes several desiderata such as confidence; informativeness …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,37
Emotion recognition from text based on automatically generated rules,Shadi Shaheen; Wassim El-Hajj; Hazem Hajj; Shady Elbassuoni,With the growth of the Internet community; textual data has proven to be the main tool ofcommunication in human-machine and human-human interaction. This communication isconstantly evolving towards the goal of making it as human and real as possible. One way ofhumanizing such interaction is to provide a framework that can recognize the emotionspresent in the communication or the emotions of the involved users in order to enrich userexperience. For example; by providing insights to users for personal preferences andautomated recommendations based on their emotional state. In this work; we propose aframework for emotion classification in English sentences where emotions are treated asgeneralized concepts extracted from the sentences. We start by generating an intermediateemotional data representation of a given input sentence based on its syntactic and …,Data Mining Workshop (ICDMW); 2014 IEEE International Conference on,2014,26
Task-aware search personalization,Julia Luxenburger; Shady Elbassuoni; Gerhard Weikum,Abstract Search personalization has been pursued in many ways; in order to provide betterresult rankings and better overall search experience to individual users [5]. However; blindlyapplying personalization to all user queries; for example; by a background model derivedfrom the user's long-term query-and-click history; is not always appropriate for aiding theuser in accomplishing her actual task. User interests change over time; a user sometimesworks on very different categories of tasks within a short timespan; and history-basedpersonalization may impede a user's desire of discovering new topics. In this paper wepropose a personalization framework that is selective in a twofold sense. First; it selectivelyemploys personalization techniques for queries that are expected to benefit from priorhistory information; while refraining from undue actions otherwise. Second; we introduce …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,24
CATE: context-aware timeline for entity illustration,Tran Anh Tuan; Shady Elbassuoni; Nicoleta Preda; Gerhard Weikum,Abstract Wikipedia has become one of the most authoritative information sources on theWeb. Each article in Wikipedia provides a portrait of a certain entity. However; such a portraitis far from complete. An informative portrait of an entity should also reveal the context theentity belongs to. For example; for a person; major historical; political and cultural events thatcoincide with her life are important and should be included in that person's portrait. Similarly;the person's interactions with other people are also important. All this information should besummarized and presented in an appealing and interactive visual interface that enablesusers to quickly scan the entity's portrait. We demonstrate CATE which is a system thatutilizes Wikipedia to create a portrait of a given entity of interest. We provide a visualizationtool that summarizes the important events related to the entity. The novelty of our …,Proceedings of the 20th international conference companion on World wide web,2011,16
Adaptive personalization of web search,Shady Elbassuoni; Julia Luxenburger; Gerhard Weikum,Abstract An often stated problem in the state-of-the-art web search is its lack of useradaptation; as all users are presented with the same search results for a given query string.A user submitting an ambiguous query such as” java” with a strong interest in traveling mightappreciate finding pages related to the Indonesian island Java. However; if the same usersearched for programming tutorials a few minutes ago; the situation would be completelydifferent; and call for programming-related results. Furthermore suppose our sample usersearches for” java hashmap”. Again imposing her interest into traveling might this time havethe contrary effect and even harm the result quality. Thus the effectiveness of apersonalization of web search shows high variance in performance depending on the query;the user and the search context. To this end; carefully choosing the right personalization …,*,2008,15
ROXXI: Reviving witness dOcuments to eXplore eXtracted Information,Shady Elbassuoni; Katja Hose; Steffen Metzger; Ralf Schenkel,Abstract In recent years; there has been considerable research on information extractionand constructing RDF knowledge bases. In general; the goal is to extract all relevantinformation from a corpus of documents; store it into an ontology; and answer future queriesbased only on the created knowledge base. Thus; the original documents becomedispensable. On the one hand; an ontology is a convenient and non-redundant structuredsource of information; based on which specific queries can be answered efficiently. On theother hand; many users doubt the correctness of facts and ontology subgraphs presented tothem as query results without proof. Instead; users often wish to verify the obtained facts orsubgraphs by reading about them in context; ie; in a document relating the facts andproviding background information. In this demo; we present ROXXI; a system operating …,Proceedings of the VLDB Endowment,2010,12
Personalizing the Search for Knowledge.,Minko Dudev; Shady Elbassuoni; Julia Luxenburger; Maya Ramanath; Gerhard Weikum,ABSTRACT Recent work on building semantic search engines has given rise to large graph-based knowledge repositories and facilities for querying them and more importantly; rankingthe results. While the ranking provided may prove to be acceptable in general; for a trulysatisfactory search experience; it is necessary to tailor the results according to the user'sinterest. In this paper; we address the issue of personalizing query results in the specificsetting of graph-based knowledge bases. In particular; we address two important issues: i)construction of the user profile based on the inference of the user's interest and ii) a formalmodel for personalized scoring which incorporates the user's interest. Preliminaryexperimental results show that our techniques are indeed promising.,PersDB,2008,12
S3K: seeking statement-supporting top-K witnesses,Steffen Metzger; Shady Elbassuoni; Katja Hose; Ralf Schenkel,Abstract Traditional information retrieval techniques based on keyword search help toidentify a ranked set of relevant documents; which often contains many documents in the topranks that do not meet the user's intention. By considering the semantics of the keywordsand their relationships; both precision and recall can be improved. Using an ontology andmapping keywords to entities/concepts and identifying the relationship between them thatthe user is interested in; allows for retrieving documents that actually meet the user'sintention. In this paper; we present a framework that enables semantic-aware documentretrieval. User queries are mapped to semantic statements based on entities and theirrelationships. The framework searches for documents expressing these statements indifferent variations; eg; synonymous names for entities or different textual expressions for …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,10
RDF Xpress: a flexible expressive RDF search engine,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract We demonstrate RDF Xpress; a search engine that enables users to effectivelyretrieve information from large RDF knowledge bases or Linked Data Sources. RDF Xpressprovides a search interface where users can combine triple patterns with keywords to formqueries. Moreover; RDF Xpress supports automatic query relaxation and returns a rankedlist of diverse query results.,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,8
Calories Prediction from Food Images.,Manal Chokr; Shady Elbassuoni,Abstract Calculating the amount of calories in a given food item is now a common task. Wepropose a machine-learning-based approach to predict the amount of calories from foodimages. Our system does not require any input from the user; except from an image of thefood item. We take a pragmatic approach to accurately predict the amount of calories in afood item and solve the problem in three phases. First; we identify the type of the food item inthe image. Second; we estimate the size of the food item in grams. Finally; by taking intoconsideration the output of the first two phases; we predict the amount of calories in thephotographed food item. All these three phases are based purely on supervisedmachinelearning. We show that this pipelined approach is very effective in predicting theamount of calories in a food item as compared to baseline approaches which directly …,AAAI,2017,3
The impact of digital technology on health of populations affected by humanitarian crises: Recent innovations and current gaps,Sandra Mesmar; Reem Talhouk; Chaza Akik; Patrick Olivier; Imad H Elhajj; Shady Elbassuoni; Sarah Armoush; Joumana Kalot; Madeline Balaam; Aline Germani; Hala Ghattas,Abstract Digital technology is increasingly used in humanitarian action and promises toimprove the health and social well-being of populations affected by both acute andprotracted crises. We set out to (1) review the current landscape of digital technologies usedby humanitarian actors and affected populations;(2) examine their impact on health and well-being of affected populations; and (3) consider the opportunities for and challenges faced byusers of these technologies. Through a systematic search of academic databases andreports; we identified 50 digital technologies used by humanitarian actors; and/orpopulations affected by crises. We organized them according to the stage of thehumanitarian cycle that they were used in; and the health outcomes or determinants ofhealth they affected. Digital technologies were found to facilitate communication …,Journal of public health policy,2016,3
Effective Searching of RDF Knowledge Bases,Shady Elbassuoni,Eine Vielzahl aktueller Anwendungen basiert auf RDF-Daten als essentiellerInformationsquelle. Daher sind Modelle und Algorithmen zur effizienten Suche in RDF-Wissensdatenbanken ein entscheidender Aspekt; der über Erfolg und Nichterfolgentscheidet. Derartige Datenbanken bestehen aus einer großen Menge von Subjekt-Prädikat-Objekt-Tripeln (SPO-Tripeln); wobei Subjekt und Objekt Entitäten darstellen undPrädikate Beziehungen zwischen diesen Entitäten beschreiben. Suchanfragen werden inder Regel durch Verwendung des W3C Anfragestandards SPARQL oder ähnlichstrukturierte Anfragesprachen formuliert und basieren auf Tripel-Patterns. Werden nurexakte Treffer in die Ergebnismenge übernommen; wird das Informationsbedürfnis desNutzers häufig nicht befriedigt; wenn zu wenige oder zu viele Ergebnisse ausgegeben …,*,2012,3
Acquiring reliable ratings from the crowd,Beatrice Valeri; Shady Elbassuoni; Sihem Amer-Yahia,Abstract We address the problem of acquiring reliable ratings of items such as restaurants ormovies from the crowd. We propose a crowdsourcing platform that takes into considerationthe workers' skills with respect to the items being rated and assigns workers the best items torate. Our platform focuses on acquiring ratings from skilled workers and for items that onlyhave a few ratings. We evaluate the effectiveness of our system using a real-world datasetabout restaurants.,Third AAAI Conference on Human Computation and Crowdsourcing,2015,2
Methodical Evaluation of Arabic Word Embeddings,Mohammed Elrazzaz; Shady Elbassuoni; Khaled Shaban; Chadi Helwe,Abstract Many unsupervised learning techniques have been proposed to obtain meaningfulrepresentations of words from text. In this study; we evaluate these various techniques whenused to generate Arabic word embeddings. We first build a benchmark for the Arabiclanguage that can be utilized to perform intrinsic evaluation of different word embeddings.We then perform additional extrinsic evaluations of the embeddings based on two NLPtasks.,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2017,1
Arabic Corpora for Credibility Analysis.,Ayman Al Zaatari; Rim El Ballouli; Shady Elbassuoni; Wassim El-Hajj; Hazem M Hajj; Khaled B Shaban; Nizar Habash; Emad Yahya,Abstract A significant portion of data generated on blogging and microblogging websites isnon-credible as shown in many recent studies. To filter out such non-credible information;machine learning can be deployed to build automatic credibility classifiers. However; as inthe case with most supervised machine learning approaches; a sufficiently large andaccurate training data must be available. In this paper; we focus on building a public Arabiccorpus of blogs and microblogs that can be used for credibility classification. We focus onArabic due to the recent popularity of blogs and microblogs in the Arab World and due to thelack of any such public corpora in Arabic. We discuss our data acquisition approach andannotation process; provide rigid analysis on the annotated data and finally report someresults on the effectiveness of our data for credibility classification.,LREC,2016,1
TopoText: Interactive Digital Mapping of Literary Text,Randa El Khatib; Julia El Zini; David Wrisley; Mohamad Jaber; Shady Elbassuoni,Abstract We demonstrate TopoText; an interactive tool for digital mapping of literary text.TopoText takes as input a literary piece of text such as a novel or a biography article andautomatically extracts all place names in the text. The identified places are then geoparsedand displayed on an interactive map. TopoText calculates the number of times a place wasmentioned in the text; which is then reflected on the map allowing the end-user to grasp theimportance of the different places within the text. It also displays the most frequent wordsmentioned within a specified proximity of a place name in context or across the entire text.This can also be faceted according to part of speech tags. Finally; TopoText keeps thehuman in the loop by allowing the end-user to disambiguate places and to provide specificplace annotations. All extracted information such as geolocations; place frequencies; as …,Proceedings of COLING 2016; the 26th International Conference on Computational Linguistics: System Demonstrations,2016,1
Effective searching of RDF knowledge graphs,Hiba Arnaout; Shady Elbassuoni,Abstract RDF knowledge graphs are typically searched using triple-pattern queries. Often;triple-pattern queries will return too many or too few results; making it difficult for users to findrelevant answers to their information needs. To remedy this; we propose a generalframework for effective searching of RDF knowledge graphs. Our framework extends boththe searched knowledge graph and triple-pattern queries with keywords to allow users toform a wider range of queries. In addition; it provides result ranking based on statisticalmachine translation; and performs automatic query relaxation to improve query recall.Finally; we also define a notion of result diversity in the setting of RDF data and providemechanisms to diversify RDF search results using Maximal Marginal Relevance. Weevaluate the effectiveness of our retrieval framework using various carefully-designed …,Journal of Web Semantics,2017,*
Customizing Travel Packages with Interactive Composite Items,Manish Singh; Ria Mae Borromeo; Anas Hosami; Sihem Amer-Yahia; Shady Elbassuoni,We examine the applicability of Composite Items (CIs) for generating customized travelpackages consisting of Points of Interest (POIs) in a given city. CIs have been shown toserve complex information needs such as selecting books for a reading club; identifying aset of products for a promotion; or planning a city tour. In the travel domain; a synthesizedview of travel options in a city can be provided with a set of cohesive CIs; each of which iscovering a different region in the city. In this paper; we attempt to understand the benefit ofletting users customize travel packages; and examine the relationship betweencustomization and personalization. For personalization; we gather user preferences on POIfeatures when available or on latent topics extracted from POI tags. For customization; wedevelop a framework within which a user interacts with proposed travel packages and the …,Data Science and Advanced Analytics (DSAA); 2017 IEEE International Conference on,2017,*
Website Navigation Behavior Analysis for Bot Detection,Rabih Haidar; Shady Elbassuoni,Detecting bots is an important goal for most website admins. In this paper; we propose anovel machine learning bot detection approach based on local website navigation behavior.While machine learning has been used before for bot detection; most existing approachesrely on general hypotheses based on statistical analysis over multiple websites and are thuseasy to counter. In our work; we build a website-specific hypothesis or classifier based onthe actual navigation data of the website. The advantages of our approach is that it can begenerally used to detect any type of bots and is difficult to counter unless website-specificbots are designed as well. Our classifier uses a Two-Class Boosted Decision Treeclassification model and can be periodically re-trained to learn new hypotheses as botsevolve. We tested our approach on two real-world websites and achieved an accuracy of …,Data Science and Advanced Analytics (DSAA); 2017 IEEE International Conference on,2017,*
CCS Coding of Discharge Diagnoses via Deep Neural Networks,Chadi Helwe; Shady Elbassuoni; Mirabelle Geha; Eveline Hitti; Carla Makhlouf Obermeyer,Abstract A standard procedure in the medical domain is to code discharge diagnoses into aset of manageable categories known as the CCS codes. This is typically done by firstmanually coding the discharge diagnoses into the standard ICD codes and then using a one-to-one mapping between ICD and CCS codes. In this paper; we study the applicability ofdeep learning to perform automatic coding of discharge diagnoses into CCS codes. Inparticular; we build an LSTM network combined with a dense neural network that usesmedically-trained word embeddings to code discharge diagnoses into single-level CCScodes. We also investigate the advantage of mapping discharge diagnoses into UMLSconcepts before coding is carried out. Experimental results based on a large dataset ofmanually coded discharge diagnoses show that our deep-learning model outperforms the …,Proceedings of the 2017 International Conference on Digital Health,2017,*
CAT: Credibility Analysis of Arabic Content on Twitter,Rim El Ballouli; Wassim El-Hajj; Ahmad Ghandour; Shady Elbassuoni; Hazem Hajj; Khaled Shaban,Abstract Data generated on Twitter has become a rich source for various data mining tasks.Those data analysis tasks that are dependent on the tweet semantics; such as sentimentanalysis; emotion mining; and rumor detection among others; suffer considerably if the tweetis not credible; not real; or spam. In this paper; we perform an extensive analysis oncredibility of Arabic content on Twitter. We also build a classification model (CAT) toautomatically predict the credibility of a given Arabic tweet. Of particular originality is theinclusion of features extracted directly or indirectly from the author's profile and timeline. Totrain and test CAT; we annotated for credibility a data set of 9;000 Arabic tweets that aretopic independent. CAT achieved consistent improvements in predicting the credibility of thetweets when compared to several baselines and when compared to the state-of-the-art …,Proceedings of the Third Arabic Natural Language Processing Workshop,2017,*
Result Diversity for RDF Search,Hiba Arnaout; Shady Elbassuoni,Abstract RDF repositories are typically searched using triple-pattern queries. Often; triple-pattern queries will return,Proceedings of the International Joint Conference on Knowledge Discovery; Knowledge Engineering and Knowledge Management,2016,*
Credibility Models For Arabic Content On Twitter,Reem El-ballouli; M. sc; Wassim El Hajj; Shady Elbassuoni; Hazem Hajj; Nizar Habash; Khaled Bashir Shaban,Microblogging websites such as Twitter have gained popularity as an effective and quickmeans of expressing opinions; sharing news and promoting information and updates. As aresult; data generated on Twitter has become a vital and rich source for tasks such assentiment mining or newsgathering. However; a significant portion of such data is eitherbiased; untruthful; spam or non-credible in general. Consequently; filtering out non-credibletweets when performing data analyses tasks on Twitter becomes a crucial task. In this work;we present a credibility model for content on Twitter. Unlike previous work that focused onEnglish content or factual tweets; our work analyses the credibility of any tweet type andtargets Arabic tweets; a challenging language for NLP in general. We focus on Arabic tweetsdue to the recent popularity of Twitter in the Arab world and due to the presence of a large …,Qatar Foundation Annual Research Conference,2014,*
Context-Aware Timeline for Entity Exploration,Anh Tuan Tran; Gerhard Weikum; Nicoleta Preda; Shady Elbassuoni; Martin Theobald,Abstract With millions of articles in multiple languages; Wikipedia has become the de-factosource of reference on the Internet today. Each article on Wikipedia contains encyclopedicinformation about various topics (people; events; inventions; etc.) and implicitly representsan entity. Extracting the most important facts about such entity will help users to find desiredinformation more quickly and effectively. However; this task is challenging due to theincomplete and noisy nature of Wikipedia articles. This calls for a mechanism to detect andsummarize the most important information about an entity on Wikipedia. This thesisproposes and implements CATE (Context-Aware Timeline for Entity Exploration); aframework that utilizes Wikipedia to summarize and visualize the important aspects ofentities in a timeline fashion. Such a system will help users to draw quickly an informative …,*,2011,*
Language-model-based ranking in entity-relation graphs,Shady Elbassuoni; Maya Ramanath; Gerhard Weikum,Abstract We propose a language-model-based ranking approach for SPARQL-like querieson entity-relationship graphs. Our ranking model supports exact matching; approximatestructure matching; and approximate matching with text predicates. We show theeffectiveness of our model through examples.,Proceedings of the First International Workshop on Keyword Search on Structured Data,2009,*
QULOSUS: Querying Linked Open Data using Pseudo-SPARQL Queries,Raji Ghawi; Shady Elbassuoni,*,*,*,*
MS in Computer Science,George M Turkiyyah; H Haidar; Ahmad Dhaini; Shady Elbassuoni; Mohamad Jaber; Loa Aoude; Ahmad El Hajj; Mustafa Hamam; Fatima Makki; Mohamed Nassar; Salim Ramadan; Hussam Ramlaoui; Mageda Sharafeddin; Amine Sobh,In addition to the university requirements for graduate study in the Faculty of Arts andSciences; students must complete 21 credits and a thesis (thesis option); or 27 credits and aproject (project option); as detailed below. For both options; the student must take 9 creditsin Theory (CMPS 356); Systems (CMPS 372 or CMPS 374); and Software (CMPS 363). Theremaining credits (12 for the thesis option and 18 for the project option) are normally CMPScourses numbered 300 and above to be taken in coordination with the student's advisor. Formore information about the program; visit http://www. cs. aub. edu. lb/.,*,*,*
Data Engineering,Shady Elbassuoni; Maya Ramanath; Ralf Schenkel; Gerhard Weikum,Searching RDF Graphs with SPARQL and Keywords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . ............................Shady Elbassuoni; Maya Ramanath; Ralf Schenkel and GerhardWeikum 16 Weighted Set-Based String Similarity . . . . . . . . . . . . . . . . . . . . . .Marios Hadjieleftheriouand Divesh Srivastava 25 Search-As-You-Type: Opportunities and Challenges . . . . . . . . . . . .. . . . . . . . . . . . . . . .Chen Li and Guoliang Li 37 Query Results Ready; NowWhat? . . . . . . . . . . . .… VLDB 2010 Conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . 79 ICDE 2011 Conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .back cover … Editor-in-Chief David B. Lomet Microsoft ResearchOne Microsoft Way Redmond; WA 98052; USA lomet@microsoft.com … Associate Editors SihemAmer-Yahia Yahoo! Research 111 40th St; 17th floor New York; NY 10018,*,*,*
Authors’ Addresses Gjergji Kasneci Max-Planck Institute for Informatics Campus E1. 4 66123 Saarbrücken,Shady Elbassuoni; Gerhard Weikum,Many modern applications are faced with the task of knowledge discovery in largeentityrelationship (ER) graphs; such as domain-specific knowledge bases or socialnetworks. An important building block of many knowledge discovery tasks is that of findingthe “closest” relationships between k≥ 2 given entities. We have investigated this kind ofknowledge discovery task in our previous work [22]. A more general knowledge discoveryscenario on ER graphs is that of mining the most “informative” subgraph for k (≥ 2) givenentities of interest (ie query entities). Intuitively; this would be the subgraph that best explainsthe relations between the k given query entities. This knowledge discovery scenario is moregeneral than the one of [22] in that its focus is on whole subgraphs (and not on trees).Furthermore; in this scenario; the semantics plays a crucial role. Our notion of …,*,*,*
