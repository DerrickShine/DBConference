Sagas,Hector Garcia-Molina; Kenneth Salem,Abstract Long lived transactions (LLTs) hold on to database resources for relatively longperiods of time; significantly delaying the termination of shorter and more commontransactions. To alleviate these problems we propose the notion of a saga. A LLT is a saga ifit can be written as a sequence of transactions that can be interleaved with othertransactions. The database management system guarantees that either all the transactionsin a saga are successfully completed or compensating transactions are run to amend apartial execution. Both the concept of saga and its implementation are relatively simple; butthey have the potential to improve performance significantly. We analyze the variousimplementation issues related to sagas; including how they can be run on an existingsystem that does not directly support them. We also discuss techniques for database and …,ACM Sigmod Record,1987,1489
Main memory database systems: An overview,Hector Garcia-Molina; Kenneth Salem,Main memory database systems (MMDBs) store their data in main physical memory andprovide very high-speed access. Conventional database systems are optimized for theparticular characteristics of disk storage mechanisms. Memory resident systems; on theother hand; use different optimizations to structure and organize data; as well as to make itreliable. The authors survey the major memory residence optimizations and briefly discusssome of the MMDBs that have been designed or implemented.,IEEE Transactions on knowledge and data engineering,1992,698
Adaptive control of virtualized resources in utility computing environments,Pradeep Padala; Kang G Shin; Xiaoyun Zhu; Mustafa Uysal; Zhikui Wang; Sharad Singhal; Arif Merchant; Kenneth Salem,Abstract Data centers are often under-utilized due to over-provisioning as well as time-varying resource demands of typical enterprise applications. One approach to increaseresource utilization is to consolidate applications in a shared infrastructure usingvirtualization. Meeting application-level quality of service (QoS) goals becomes a challengein a consolidated environment as application resource needs differ. Furthermore; for multi-tier applications; the amount of resources needed to achieve their QoS goals might bedifferent at each tier and may also depend on availability of resources in other tiers. In thispaper; we develop an adaptive resource control system that dynamically adjusts theresource shares to individual tiers in order to meet application-level QoS goals whileachieving high resource utilization in the data center. Our control system is developed …,ACM SIGOPS Operating Systems Review,2007,670
Disk Striping,Kenneth Salem; Hector Garcia-Molina,Just like parallel processing elements can substantially speed up computationally intensivetasks; concurrent transfer of data in and out of memory can speed up data intensive tasks. Inthis paper we study one general purpose facility for achieving parallel data motion: diskstriping. A group of disks is striped if each data block is multiplexed across all the disks.Since each subblock is in a different device; input and output can proceed in parallel. Withthe help of an analytical model; we investigate the effect of striping on disk service times andits advantages and limitations in one of a set representative applications; file processing. Wealso explore several possible enhancements to striping: immediate reading; ordered blocks;and matched disks.,International Conference on Data Engineering; February 5-7; 1986; Bonaventure Hotel; Los Angeles; California; USA.,1986,471
Automatic virtual machine configuration for database workloads,Ahmed A Soror; Umar Farooq Minhas; Ashraf Aboulnaga; Kenneth Salem; Peter Kokosielis; Sunil Kamath,Abstract Virtual machine monitors are becoming popular tools for the deployment ofdatabase management systems and other enterprise software. In this article; we consider acommon resource consolidation scenario in which several database management systeminstances; each running in a separate virtual machine; are sharing a common pool ofphysical computing resources. We address the problem of optimizing the performance ofthese database management systems by controlling the configurations of the virtualmachines in which they run. These virtual machine configurations determine how the sharedphysical resources will be allocated to the different database system instances. Weintroduce a virtualization design advisor that uses information about the anticipatedworkloads of each of the database systems to recommend workload-specific …,ACM Transactions on Database Systems (TODS),2010,220
Lazy database replication with snapshot isolation,Khuzaima Daudjee; Kenneth Salem,Abstract Snapshot isolation is a popular transactional isolation level in database systems.Several replication techniques based on snapshot isolation have recently been proposed.These proposals; however; do not fully leverage the local concurrency controls that providesnapshot isolation. Furthermore; guaranteeing snapshot isolation in lazy replicated systemsmay result in transaction inversions; which happen when transactions see stale data. Strongsnapshot isolation; which is provided in centralized database servers; avoids transactioninversions but is expensive to provide in a lazy replicated system. In this paper; we showhow snapshot isolation can be maintained in lazy replicated systems while taking fulladvantage of the local concurrency controls. We propose strong session snapshot isolation;a correctness criterion that prevents transaction inversions. We show how strong session …,Proceedings of the 32nd international conference on Very large data bases,2006,178
Adaptive block rearrangement,Sedat Akyürek; Kenneth Salem,Abstract An adaptive technique for reducing disk seek times is described. The techniquecopies frequently referenced blocks from their original locations to reserved space near themiddle of the disk. Reference frequencies need not be known in advance. Instead; they areestimated by monitoring the stream of arriving requests. Trace-driven simulations show thatseek times can be cut substantially by copying only a small number of blocks using thistechnique. The technique has been implemented by modifying a UNIX device driver. Nomodifications are required to the file system that uses the driver.,ACM Transactions on Computer Systems (TOCS),1995,163
Modeling long-running activities as nested sagas,Hector Garcia-Molina; Dieter Gawlick; Johannes Klein; Karl Kleissner; Kenneth Salem,*,Data Engineering,1991,160
Concurrency control and recovery for global procedures in federated database systems,Rafael Alonso; Hector Garcia-Molina; Kenneth Salem,ABSTRACT A federated database system is a collection of autonomous databasescooperating for mutual beneﬁt. Global procedures can access several databases; butcontrolling concurrent database accesses by them is problematic. In particular; eachdatabase has its own {possibly different) concurrency control mechanism; and it must remainindependent from the global controls. Furthermore; it may be difﬁcult for the global controlsto know exactly what granules (eg; records or pages) are touched by each database access.In this paper we discuss the concurrency control problem for federated database systems;and suggest two mechanisms that may satisfy the requirements.,IEEE Data Eng. Bull.,1987,132
How to roll a join: Asynchronous incremental view maintenance,Kenneth Salem; Kevin Beyer; Bruce Lindsay; Roberta Cochrane,Abstract Incremental refresh of a materialized join view is often less expensive than a full;non-incremental refresh. However; it is still a potentially costly atomic operation. This paperpresents an algorithm that performs incremental view maintenance as a series of small;asynchronous steps. The size of each step can be controlled to limit contention between therefresh process and concurrent operations that access the materialized view or theunderlying relations. The algorithm supports point-in-time refresh; which allows amaterialized view to be refreshed to any time between the last refresh and the present.,ACM SIGMOD Record,2000,119
Altruistic locking,Kenneth Salem; Hector Garcia-Molina; Jeannie Shands,Abstract Long-lived transactions (LLTs) hold on to database resources for relatively longperiods of time; significantly delaying the completion of shorter and more commontransactions. To alleviate this problem we propose an extension to two-phase locking; calledaltruistic locking; whereby LLTs can release their locks early. Transactions that access thisreleased data are said to run in the wake of the LLT and must follow special locking rules.Like two-phase locking; altruistic locking is easy to implement and guarantees serializability.,ACM Transactions on Database Systems (TODS),1994,107
Deploying database appliances in the cloud.,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath,Page 1. 本翻译论文源于厦门大学计算机系数据库实验室林子雨老师的云数据库技术资料专区http://dblab.xmu.edu.cn/cloud_database_view 第1 页/共12 页翻译：厦门大学计算机系教师林子雨http://www.cs.xmu.edu.cn/linziyu Deploying Database Appliances in the Cloud Ashraf Aboulnaga;Kenneth Salem; Ahmed A. Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath …,IEEE Data Eng. Bull.,2009,98
Checkpointing memory-resident databases,Kenneth Salem; Hector Garcia-Molina,A database system is considered in which a main-memory database system holds all data insemiconductor memory; and for recovery purposes a backup copy of the database ismaintained in secondary storage. The checkpointer is the component of the crash recoverymanager responsible for maintaining the backup copy. Ideally; the checkpointer shouldmaintain an almost-up-to-date backup while interfering as little as possible with the system'stransaction processing activities. Several algorithms for maintaining such a backupdatabase are presented and compared using an analytic model. The results show somesignificant performance differences among the algorithms and illustrate some of the tradeoffsthat are available in designing such a checkpointer.,Data Engineering; 1989. Proceedings. Fifth International Conference on,1989,93
Coordinating multi-transaction activities,Hector Garcia-Molina; Dieter Gawlick; Johannes Klein; Karl Kleissner; Kenneth Salem,Data processing applications must often execute collections of related transactions. Wepropose a model for structuring and coordinating these multi-transaction activities. Themodel includes mechanisms for communication between transactions; for compensatingtransactions after an activity has failed; for dynamic creation and binding of activities; and forcheck pointing the progress of an activity. THIS IS A REVISED VERSION OF TECHNICALREPORTCS-TR-297-90; DEPARTMENT OF COMPUTER SCIENCE; PRINCETONUNIVERSITY; FEBRUARY 1990. A SHORTER VERSION OF THIS PAPER APPEAREDAS``Modeling Long-Running Activities as NestedSagas;''IN Database Engineering; Vol. 14;No. 1; March; 1991.,*,1990,89
System M: A transaction processing testbed for memory resident data,Kenneth Salem; Hector Garcia-Molina,System M is an experimental transaction processing testbed that runs on top of the Machoperating system. Its database is stored in primary memory. The structure and algorithmsused in System M are described. The checkpointer is the component that periodicallysweeps memory and propagates updates to a backup database copy on disk. Severaldifferent checkpointing (and logging) algorithms were implemented; and their performancewas experimentally evaluated.,IEEE Transactions on Knowledge and Data Engineering,1990,79
Lazy database replication with ordering guarantees,Khuzaima Daudjee; Kenneth Salem,Lazy replication is a popular technique for improving the performance and availability ofdatabase systems. Although there are concurrency control techniques; which guaranteeserializability in lazy replication systems; these techniques result in undesirable transactionorderings. Since transactions may see stale data; they may be serialized in an order differentfrom the one in which they were submitted. Strong serializability avoids such problems; but itis very costly to implement. We propose a generalized form of strong serializability that issuitable for use with lazy replication. In addition to having many of the advantages of strongserializability; it can be implemented more efficiently. We show how generalized strongserializability can be implemented in a lazy replication system; and we present the results ofa simulation study that quantifies the strengths and limitations of the approach.,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,77
Remusdb: Transparent high availability for database systems,Umar Farooq Minhas; Shriram Rajagopalan; Brendan Cully; Ashraf Aboulnaga; Kenneth Salem; Andrew Warfield,Abstract In this paper; we present a technique for building a high-availability (HA) databasemanagement system (DBMS). The proposed technique can be applied to any DBMS withlittle or no customization; and with reasonable performance overhead. Our approach isbased on Remus; a commodity HA solution implemented in the virtualization layer; that usesasynchronous virtual machine state replication to provide transparent HA and failovercapabilities. We show that while Remus and similar systems can protect a DBMS; databaseworkloads incur a performance overhead of up to 32% as compared to an unprotectedDBMS. We identify the sources of this overhead and develop optimizations that mitigate theproblems. We present an experimental evaluation using two popular database systems andindustry standard benchmarks showing that for certain workloads; our optimized …,The VLDB Journal,2013,72
Second-Tier Cache Management Using Write Hints.,Xuhui Li; Ashraf Aboulnaga; Kenneth Salem; Aamer Sachedina; Shaobo Gao,Abstract Storage servers; as well as storage clients; typically have large memories in whichthey cache data blocks. This creates a two-tier cache hierarchy in which the presence of afirst-tier cache (at the storage client) makes it more difficult to manage the second-tier cache(at the storage server). Many techniques have been proposed for improving themanagement of second-tier caches; but none of these techniques use the information that isprovided by writes of data blocks from the first tier to help manage the second-tier cache. Inthis paper; we illustrate how the information contained in writes from the first tier can be usedto improve the performance of the second-tier cache. In particular; we argue that there arevery different reasons why storage clients write data blocks to storage servers (eg; cleaningdirty blocks vs. limiting the time to recover from failure). These different types of writes can …,FAST,2005,64
A language for manipulating arrays,Arunprasad P Marathe; Kenneth Salem,Abstract This paper describes the Array Manipulation Language (AML); an algebra formultidimensional array data. AML is generic; in the sense that it can be customized tosupport a wide variety of domain-speci c operations on arrays. AML expressions can betreated declaratively and subjected to rewrite optimizations. To illustrate this; several rewriterules that exploit the structural properties of the AML operations are presented. Sometechniques for e cient evaluation of AML expressions are also discussed.,dim,1997,64
Query processing techniques for arrays,Arunprasad P Marathe; Kenneth Salem,Abstract Arrays are a common and important class of data. At present; database systems donot provide adequate array support: arrays can neither be easily defined nor convenientlymanipulated. Further; array manipulations are not optimized. This paper describes alanguage called the Array Manipulation Language (AML); for expressing arraymanipulations; and a collection of optimization techniques for AML expressions. In the AMLframework for array manipulation; arbitrary externally-defined functions can be applied toarrays in a structured manner. AML can be adapted to different application domains bychoosing appropriate external function definitions. This paper concentrates on arraysoccurring in databases of digital images such as satellite or medical images. AML queriescan be treated declaratively and subjected to rewrite optimizations. Rewriting minimizes …,The VLDB Journal—The International Journal on Very Large Data Bases,2002,61
Altruistic locking: A strategy for coping with long lived transactions,Kenneth Salem; Hector Garcia-Molina; Rafael Alonso,ABSTRA CT Long lived transactions (LLTs) hold on to database resources for relatively longperiods of time; significantly delaying the completion of shorter and more commontransactions. To alleviate these problems we propose an extension to two-phase locking;called altruistic locking; whereby LLTs can release their locks early. Transactions thataccess this released but uncommitted data run in the wake of the LLT and must followspecial locking and commit rules. Additional performance improvements can be obtained ifthe LLT predeclares its access set (this is only an option). Altruistic locking guaranteesserializability and allows transactions to access the database in any order. In addition topresenting the protocol; we discuss how LLTs can be automatically analyzed in order toextract the access pattern information used by altruistic locking.,*,1989,55
System and method for asynchronous view maintenance,*,A method and system for refreshing materialized join views includes asynchronouslyevaluating changes to the base tables; ie; evaluating changes for an intended time tintendedat an evaluation time teval that is later than the intended time. Changes made to the basetable between the intended time tintended and evaluation time teval are recursivelycompensated for. The results of the evaluating and compensating are then applied to thematerialized view to refresh the view.,*,2003,53
The DEC: processing scientific data over the Internet,Chungmin Chen; Kenneth Salem; Miron Livny,We present the Distributed Batch Controller (DBC); a system built to support batchprocessing of large scientific datasets. The DBC implements a federation of autonomousworkstation pools; which may be widely distributed. Individual batch jobs are executed usingidle workstations in these pools. Input data are staged to the pool before processing begins.We describe the architecture and implementation of the DBC; and present the results ofexperiments in which it is used to perform image compression.,Distributed Computing Systems; 1996.; Proceedings of the 16th International Conference on,1996,49
Database systems on virtual machines: How much do you lose?,Umar Farooq Minhas; Jitendra Yadav; Ashraf Aboulnaga; Kenneth Salem,Virtual machine technologies offer simple and practical mechanisms to address manymanageability problems in database systems. For example; these technologies allow forserver consolidation; easier deployment; and more flexible provisioning. Therefore;database systems are increasingly being run on virtual machines. This offers manyopportunities for researchers in self-managing database systems; but it is also important tounderstand the cost of virtualization. In this paper; we present an experimental study of theoverhead of running a database workload on a virtual machine. We show that the averageoverhead is less than 10%; and we present details of the different causes of this overhead.Our study shows that the manageability benefits of virtualization come at an acceptable cost.,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,44
The presumed-either two-phase commit protocol,Gopi K.  Attaluri; Kenneth Salem,This paper describes the presumed-either two-phase commit protocol. Presumed-eitherexploits log piggybacking to reduce the cost of committing transactions. If timelypiggybacking occurs; presumed-either combines the performance advantages of presumed-abort and presumed-commit. Otherwise; presumed-either behaves much like the widely-used presumed-abort protocol.,IEEE Transactions on Knowledge and Data Engineering,2002,40
Management of partially safe buffers,Sedat Akyurek; Kenneth Salem,Safe RAM is RAM which has been made as reliable as a disk. We consider the problem ofbuffer management in partially safe buffers; ie; buffers which contain both safe RAM andvolatile RAM. Buffer management techniques for partially safe buffers explicitly consider thesafety of memory in deciding which data to place in the buffer; where to place it; and when tocopy updates back to the disk. We present techniques for managing such buffers and studytheir performance using trace-driven simulations.,IEEE Transactions on Computers,1995,37
Optimization of query streams using semantic prefetching,Ivan T Bowman; Kenneth Salem,Abstract Streams of relational queries submitted by client applications to database serverscontain patterns that can be used to predict future requests. We present the Scalpel system;which detects these patterns and optimizes request streams using context-based predictionsof future requests. Scalpel uses its predictions to provide a form of semantic prefetching;which involves combining a predicted series of requests into a single request that can beissued immediately. Scalpel's semantic prefetching reduces not only the latencyexperienced by the application but also the total cost of query evaluation. We describe howScalpel learns to predict optimizable request patterns by observing the application's requeststream during a training phase. We also describe the types of query pattern rewrites thatScalpels cost-based optimizer considers. Finally; we present empirical results that show …,ACM Transactions on Database Systems (TODS),2005,36
Database virtualization: A new frontier for database tuning and physical design,Ahmed A Soror; Ashraf Aboulnaga; Kenneth Salem,Resource virtualization is currently being employed at all levels of the IT infrastructure toimprove provisioning and manageability; with the goal of reducing total cost of ownership.This means that database systems will increasingly be run in virtualized environments;inside virtual machines. This has many benefits; but it also introduces new tuning andphysical design problems that are of interest to the database research community. In thispaper; we discuss how virtualization can benefit database systems; and we present thetuning problems it introduces; which relate to setting the new" tuning knobs" that controlresource allocation to virtual machines in the virtualized environment. We present aformulation of the visualization design problem; which focuses on setting resource allocationlevels for different database workloads statically at deployment and configuration time. An …,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,31
Dynamic feedback control of resources in computing environments,*,A method for controlling resource allocation is provided. The method includes determining aservice metric associated with a first application; wherein the first application is associatedwith one or more virtual machines. The method further includes comparing the service metricto an application specific service level goal associated with the first application andmodifying a resource allocation associated with the first application at one or more of thevirtual machines.,*,2014,30
Compact access control labeling for efficient secure XML query evaluation,Huaxin Zhang; Ning Zhang; Kenneth Salem; Donghui Zhuo,Abstract Fine-grained access controls for XML define access privileges at the granularity ofindividual XML nodes. In this paper; we present a fine-grained access control mechanism forXML data. This mechanism exploits the structural locality of access rights as well ascorrelations among the access rights of different users to produce a compact physicalencoding of the access control data. This encoding can be constructed using a single passover a labeled XML database. It is block-oriented and suitable for use in secondary storage.We show how this access control mechanism can be integrated with a next-of-kin (NoK)XML query processor to provide efficient; secure query evaluation. The key idea is that thestructural information of the nodes and their encoded access controls are stored together;allowing the access privileges to be checked efficiently. Our evaluation shows that the …,Data & Knowledge Engineering,2007,30
Accordion: Elastic scalability for database systems supporting distributed transactions,Marco Serafini; Essam Mansour; Ashraf Aboulnaga; Kenneth Salem; Taha Rafiq; Umar Farooq Minhas,Abstract Providing the ability to elastically use more or fewer servers on demand (scale outand scale in) as the load varies is essential for database management systems (DBMSes)deployed on today's distributed computing platforms; such as the cloud. This requiressolving the problem of dynamic (online) data placement; which has so far been addressedonly for workloads where all transactions are local to one sever. In DBMSes where ACIDtransactions can access more than one partition; distributed transactions represent a majorperformance bottleneck. Scaling out and spreading data across a larger number of serversdoes not necessarily result in a linear increase in the overall system throughput; becausetransactions that used to access only one server may become distributed. In this paper wepresent Accordion; a dynamic data placement system for partition-based DBMSes that …,Proceedings of the VLDB Endowment,2014,29
Crash recovery mechanisms for main storage database systems,Kenneth Salem; Hector Garcia-Molina,*,*,1986,29
Elastic scale-out for partition-based database systems,Umar Farooq Minhas; Rui Liu; Ashraf Aboulnaga; Kenneth Salem; Jonathan Ng; Sean Robertson,An important goal for database systems today is to provide elastic scale-out; ie; the ability togrow and shrink processing capacity on demand; with varying load. Database systems aredifficult to scale since they are stateful-they manage a large database; and it is importantwhen scaling to multiple server machines to provide mechanisms so that these machinescan collaboratively manage the database and maintain its consistency. Databasepartitioning is often used to solve this problem; with each server machine being responsiblefor one partition. In this paper; we propose that the flexibility provided by a partitioned;shared nothing parallel database system can be exploited to provide elastic scale-out. Theidea is to start with a small number of server machines that manage all partitions; and toelastically scale out by dynamically adding new server machines and redistributing …,Data Engineering Workshops (ICDEW); 2012 IEEE 28th International Conference on,2012,28
Query processing techniques for arrays,Arunprasad P Marathe; Kenneth Salem,Abstract Arrays are an appropriate data model for images; gridded output fromcomputational models; and other types of data. This paper describes an approach to arrayquery processing. Queries are expressed in AML; a logical algebra that is easily extendedwith user-defined functions to support a wide variety of array operations. For example;compression; filtering; and algebraic operations on images can be described. We show howAML expressions involving such operations can be treated declaratively and subjected touseful rewrite optimizations. We also describe a plan generator that produces efficientiterator-based plans from rewritten AML expressions.,ACM SIGMOD Record,1999,28
Workload-aware storage layout for database systems,Oguzhan Ozmen; Kenneth Salem; Jiri Schindler; Steve Daniel,Abstract The performance of a database system depends strongly on the layout of databaseobjects; such as indexes or tables; onto the underlying storage devices. A good layout willboth balance the I/O workload generated by the database system and avoid theperformance-degrading interference that can occur when concurrently accessed objects arestored on the same volume. In current practice; layout is typically guided by heuristics andrules of thumb; such as separating indexes and tables or striping all objects across all of theavailable storage devices. However; these guidelines may give poor results. In this paper;we address the problem of generating an optimized layout of a given set of databaseobjects. Our layout optimizer goes beyond generic guidelines by making use of a descriptionof the database system's I/O activity. We formulate the layout problem as a non-linear …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,27
Storage workload estimation for database management systems,Oguzhan Ozmen; Kenneth Salem; Mustafa Uysal; M Attar,Abstract Modern storage systems are sophisticated. Simple direct-attached storage devicesare giving way to storage systems that are shared; flexible; virtualized and network-attached.Today; storage systems have their own administrators; who use specialized tools andexpertise to configure and manage storage resources. Although the separation of storagemanagement and database management has many advantages; it also introducesproblems. Database physical design and storage configuration are closely related tasks;and the separation makes it more difficult to achieve a good end-to-end design. In thispaper; we attempt to close this gap by addressing the problem of predicting the storageworkload that will be generated by a database management system. Specifically; we showhow to translate a database workload description; together with a database physical …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,27
Disk striping,Hector Garcia-Molina; Kenneth Salem,*,International Conference on Data Engineering,1986,24
Hybrid storage management for database systems,Xin Liu; Kenneth Salem,Abstract The use of flash-based solid state drives (SSDs) in storage systems is growing.Adding SSDs to a storage system not only raises the question of how to manage the SSDs;but also raises the question of whether current buffer pool algorithms will still workeffectively. We are interested in the use of hybrid storage systems; consisting of SSDs andhard disk drives (HDDs); for database management. We present cost-aware replacementalgorithms; which are aware of the difference in performance between SSDs and HDDs; forboth the DBMS buffer pool and the SSDs. In hybrid storage systems; the physical accesspattern to the SSDs depends on the management of the DBMS buffer pool. We studied theimpact of buffer pool caching policies on SSD access patterns. Based on these studies; wedesigned a cost-adjusted caching policy to effectively manage the SSD. We implemented …,Proceedings of the VLDB Endowment,2013,21
Data management with massive memory: A summary,Hector Garcia-Molina; Robert Abbott; Christopher Clifton; Carl Staelin; Kenneth Salem,The Massive Memory Machine Project (MMM) was started at Princeton around 1983. Yearby year; semiconductor memory was becoming cheaper (and still is) and memory chipdensities were increasing dramatically. We were interested in studying how availability ofvery large amounts of memory would change the way data intensive problems could besolved. We believed that just like parallel computer architectures could speed up scientificcomputations; computer architectures and software that exploited massive main memoriescould yield oder-of-magm'tude performance improvements on many important computations.By the end of 1985 we had received joint funding from the US National Science Foundationand the Defense Advanced Research Projects Agency. This formal project has two maingoals. One was to study key data intensive applications and to develop strategies for …,Workshop on Parallel Database Systems,1990,18
High performance transaction processing with memory resident data,Hector Garcia-Molina; Kenneth Salem,*,Proceedings of the 2nd International Workshop on High Performance Transaction Systems,1987,18
Automatic tuning of the multiprogramming level in Sybase SQL Anywhere,Mohammed Abouzour; Kenneth Salem; Peter Bumbulis,This paper looks at the problem of automatically tuning the database servermultiprogramming level to improve database server performance under varying workloads.We describe two tuning algorithms that were considered and how they performed underdifferent workloads. We then present the hybrid approach that we have successfullyimplemented in SQL Anywhere 12. We found that the hybrid approach yielded betterperformance than each of the algorithms separately.,Data Engineering Workshops (ICDEW); 2010 IEEE 26th International Conference on,2010,17
CLIC: CLient-Informed Caching for Storage Servers.,Xin Liu; Ashraf Aboulnaga; Kenneth Salem; Xuhui Li,Abstract Traditional caching policies are known to perform poorly for storage server caches.One promising approach to solving this problem is to use hints from the storage clients tomanage the storage server cache. Previous hinting approaches are ad hoc; in that apredefined reaction to specific types of hints is hard-coded into the caching policy. With adhoc approaches; it is difficult to ensure that the best hints are being used; and it is difficult toaccommodate multiple types of hints and multiple client applications. In this paper; wepropose CLient-Informed Caching (CLIC); a generic hint-based policy for managing storageserver caches. CLIC automatically interprets hints generated by storage clients andtranslates them into a server caching policy. It does this without explicit knowledge of theapplication-specific hint semantics. We demonstrate using trace-based simulation of …,FAST,2009,16
Virtualization and databases: state of the art and research challenges,Ashraf Aboulnaga; Cristiana Amza; Kenneth Salem,Abstract There is currently a lot of interest in resource virtualization as an importanttechnique for addressing the problems of manageability; reliability; and security in computersystems. Resource virtualization decouples the user's perception of hardware and softwareresources from the actual implementation of these resources. It adds a flexible andprogrammable layer of software between user applications (such as database systems) andthe resources that they use. This layer of software maps the virtual resources perceived bythe applications to real physical resources. An example of this layer of software is a virtualmachine monitor; which partitions the resources of a machine (CPU; disk; memory; network;etc.) into multiple virtual machines; and independent operating systems and applicationscan be installed on each virtual machine. The power of resource virtualization comes from …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,16
Crash recovery for memory-resident databases,Kenneth Salem; Hector Garcia-Molina,*,*,1987,16
Placing replicated data to reduce seek delays,Sedat Akyurek; Kenneth Salem,Abstract In many environments; seek time is a major component of the disk access time. Inthis paper we introduce the idea of replicating data on a disk to reduce the average seektime. Our focus is on the problem of placing replicated data. We present several techniquesfor replica placement and evaluate their performance using trace-driven simulations. 1Introduction Random access time is a major factor that degrades the performance of disks.As improvements in CPU and memory speeds continue to outpace improvements in diskspeeds; the importance of reducing random access times increases [Bitton 87]. Typically;seek time constitutes nearly half of the random access time; the rest being rotational latencyand channel contention [Bitton 88]. Average seek time can be reduced significantly byreplicating some data on the disk and spreading the replicas across the disk's surface. To …,Proc. USENIX File System Conference,1991,13
Managing geo-replicated data in multi-datacenters,Divyakant Agrawal; Amr El Abbadi; Hatem A Mahmoud; Faisal Nawab; Kenneth Salem,Abstract Over the past few years; cloud computing and the growth of global large scalecomputing systems have led to applications which require data management across multipledatacenters. Initially the models provided single row level transactions with eventualconsistency. Although protocols based on these models provide high availability; they arenot ideal for applications needing a consistent view of the data. There has been now agradual shift to provide transactions with strong consistency with Google's Megastore andSpanner. We propose protocols for providing full transactional support while replicating datain multi-datacenter environments. First; an extension of Megastore is presented; which usesoptimistic concurrency control. Second; a contrasting method is put forward; which usesgossip-based protocol for providing distributed transactions across datacenters. Our aim …,International Workshop on Databases in Networked Information Systems,2013,11
Probabilistic diagnosis of hot spots,Kenneth Salem; Daniel Barbara; Richard J Lipton,The authors present several techniques to identify; or diagnose; hot spots in a database. Allof them are probabilistic in the sense that they will classify the items as hot or cold andexhibit a non-zero probability of false diagnoses. Each technique is analysed to identify thetradeoffs of time and space involved in maintaining a low probability of false diagnosis. Eachof the techniques is presented. The analyses of the techniques is considered to determinehow likely they are to diagnose without error. The techniques are compared. A numericalcomparison based on the analyses is included.,Data Engineering; 1992. Proceedings. Eighth International Conference on,1992,11
NoSE: Schema design for NoSQL applications,Michael Joseph Mior; Kenneth Salem; Ashraf Aboulnaga; Rui Liu,Database design is critical for high performance in relational databases and a myriad oftools exist to aid application designers in selecting an appropriate schema. While theproblem of schema optimization is also highly relevant for NoSQL databases; existing toolsfor relational databases are inadequate in that setting. Application designers wishing to usea NoSQL database instead rely on rules of thumb to select an appropriate schema. Wepresent a system for recommending database schemas for NoSQL applications. Our cost-based approach uses a novel binary integer programming formulation to guide the mappingfrom the application's conceptual data model to a database schema. We implemented aprototype of this approach for the Cassandra extensible record store. Our prototype; theNoSQL Schema Evaluator (NoSE) is able to capture rules of thumb used by expert …,IEEE Transactions on Knowledge and Data Engineering,2017,10
Dax: a widely distributed multitenant storage service for dbms hosting,Rui Liu; Ashraf Aboulnaga; Kenneth Salem,Abstract Many applications hosted on the cloud have sophisticated data management needsthat are best served by a SQL-based relational DBMS. It is not difficult to run a DBMS in thecloud; and in many cases one DBMS instance is enough to support an application'sworkload. However; a DBMS running in the cloud (or even on a local server) still needs away to persistently store its data and protect it against failures. One way to achieve this is toprovide a scalable and reliable storage service that the DBMS can access over a network.This paper describes such a service; which we call DAX. DAX relies on multi-masterreplication and Dynamo-style flexible consistency; which enables it to run in multiple datacenters and hence be disaster tolerant. Flexible consistency allows DAX to control theconsistency level of each read or write operation; choosing between strong consistency at …,Proceedings of the VLDB Endowment,2013,10
Towards adaptive costing of database access methods,Ye Qin; Kenneth Salem; Anil K Goel,Most database query optimizers use cost models to identify good query execution plans.Inaccuracies in the cost models can cause query optimizers to select poor plans. In thispaper; we consider the problem of accurately estimating the I/O costs of database accessmethods; such as index scans. We present some experimental results which show thatexisting analytical I/O cost models can be very inaccurate. We also present a simple analysiswhich shows that larger cost estimation errors can cause the query optimizer to make largermistakes in plan selection. We propose the use of an adaptive black-box statistical costestimation methodology to achieve better estimates.,Data Engineering Workshop; 2007 IEEE 23rd International Conference on,2007,9
Adaptive block rearrangement under UNIX,Sedat Akyürek; Kenneth Salem,Abstract An adaptive UNIX disk device driver is described. To reduce seek times; the drivercopies frequently-referenced blocks from their original locations to reserved space near thecenter of the disk. Block reference frequencies need not be known in advance. Instead; theyare estimated by monitoring the stream of arriving requests. Measurements show that theadaptive driver reduces seek times and response times substantially.© 1997 by John Wiley& Sons; Ltd.,Software: Practice and Experience,1997,9
Services for a workflow management system,Hector Garcia-Molina; Kenneth Salem,Services for a Workflow Management System Hector Garcia-Molina y Stanford University KennethSalem z University of Maryland Abstract This paper represents an effort to understand the problemof workflow management. Our goal is not to propose yet another model or system. Instead; wegive examples of the types of services that a workflow management system may provide. 1 IntroductionTraditionally; database management systems (DBMSs) have had a very “data centered” viewof the world. An application program is simply an entity that issues a transaction. It starts interactingwith the DBMS by issuing a “begin work” command; then emitting a series requests to accessand modify data; and finally ending with either an “abort work” or a “commit work” command.After that; if the application issues other transactions; it is considered to be a differentprogram. It is of no concern to the DBMS what happens to the program if its work is …,Data Engineering,1994,9
A Taxonomy of Partitioned Replicated Cloud-based Database Systems.,Divy Agrawal; Amr El Abbadi; Kenneth Salem,Abstract The advent of the cloud computing paradigm has given rise to many innovative andnovel proposals for managing large-scale; fault-tolerant and highly available datamanagement systems. This paper proposes a taxonomy of large scale partitioned replicatedtransactional databases with the goal of providing a principled understanding of the growingspace of scalable and highly available database systems. The taxonomy is based on therelationship between transaction management and replica management. We illustratespecific instances of the taxonomy using several recent partitioned replicated databasesystems.,IEEE Data Eng. Bull.,2015,8
Semantic prefetching of correlated query sequences,Ivan T Bowman; Kenneth Salem,We present a system that optimizes sequences of related client requests by combining smallrequests into larger ones; thus reducing per-request overhead. The system predictsupcoming requests and their parameter values based on past observations; and prefetchesresults that are expected to be needed. We describe how the system makes its predictionsand how it uses them to optimize the request stream. We also characterize the benefits withseveral experiments.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,8
Sagas,Héctor García-Molina; Kenneth Salem,*,Readings in database systems (2nd ed.),1994,8
Implementing extended transaction models using transaction groups,Kenneth Salem,*,*,1993,8
Inferring a serialization order for distributed transactions,Khuzaima Daudjee; Kenneth Salem,Data partitioning is often used to scale-up a database system. In a centralized databasesystem; the serialization order of commited update transactions can be inferred from thedatabase log. To achieve this in a shared-nothing distributed database; the serializationorder of update transactions must be inferred from multiple database logs. We describe atechnique to generate a single stream of updates from logs of multiple database systems.This single stream represents a valid serialization order of update transactions at the sitesover which the database is partitioned.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,7
A pure lazy technique for scalable transaction processing in replicated databases,Khuzaima Daudjee; Kenneth Salem,Recently; there have been proposals for scaling-up a database system using lazyreplication. In these proposals; system scale-up is achieved through the addition ofsecondary sites which hold replicas of the database at a primary site. The addition of moresecondary sites improves system performance by allowing read-only transactions to beserviced at the secondary sites. However; the scalability of the distributed system is limitedby the single primary site which services the update workload. As the workload scales-up;an increasing update load is placed on the primary site; which suffers from performancedegradation. We address this problem by proposing a pure lazy solution for scaling-up thedatabase system. Our techniques provide scalability and avoid transaction inversions; whichcan occur when transactions access stale replicas.,Parallel and Distributed Systems; 2005. Proceedings. 11th International Conference on,2005,7
Coordinating multi-transaction activities,Hector G Molina; Dieter Gawlick; Johannes Klein; Karl Kleissner; Kenneth Salem,Search all the public and authenticated articles in CiteULike. Include unauthenticated resultstoo (may include "spam") Enter a search phrase. You can also specify a CiteULike article id(123456);. a DOI (doi:10.1234/12345678). or a PubMed ID (pmid:12345678). Click Help foradvanced usage. CiteULike; Group: ecoo-pe; Search; Register; Log in …,*,1990,6
Psalm: Cardinality estimation in the presence of fine-grained access controls,Huaxin Zhang; Ihab F Ilyas; Kenneth Salem,In database systems that support fine-grained access controls; each user has access rightsthat determine which tuples are accessible and which are inaccessible. Queries areanswered as if the inaccessible tuples are not present in the database. Thus; users withdifferent access rights may get different answers to a given query. To process queriesefficiently in the presence of fine-grained access controls; the database system needsaccurate estimates of the number of tuples that are both accessible according to the accessrights of the submitting user and relevant according to the selection predicates in the query.In this paper; we present PSALM; a sampling-based cardinality estimation technique for usein the presence of fine-grained access controls. Our technique exploits the fact that accessrights are relatively static and are common to all queries that are evaluated on behalf of a …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,5
Constrained dynamic physical database design,Hannes Voigt; Wolfgang Lehner; Kenneth Salem,Abstract Physical design has always been an important part of database administration.Today's commercial database management systems offer physical design tools; whichrecommend a physical design for a given workload. However; these tools work only withstatic workloads and ignore the fact that workloads; and physical designs; may change overtime. Research has now begun to focus on dynamic physical design; which can account fortime-varying workloads. In this paper; we consider a dynamic but constrained approach tophysical design. The goal is to recommend dynamic physical designs that reflect majorworkload trends but that are not tailored too closely to the details of the input workloads. Toachieve this; we constrain the number of changes that are permitted in the recommendeddesign. In this paper we present our definition of the constrained dynamic physical design …,SMDB’08,2008,5
Mr-cdf: Managing multi-resolution scientific data,Kenneth Salem,Abstract: MR-CDF is a system for managing multi-resolution scientific data sets. It is anextension of the popular CDF (Common Data Format) system. MR-CDF provides a simplefunctional interface to client programs for storage and retrieval of data. Data is stored so thatlow resolution versions of the data can be provided quickly. Higher resolutions are alsoavailable; but not as quickly. By managing data with MR-CDF; an application can berelieved of the low-level details of data management; and can easily trade data resolution forimproved access time.,*,1993,5
Non-deterministic queue operations,Hector Garcia-Molina; Kenneth Salem,Abstract Queues play a central role in transaction processing systems. We present atransaction model that allows signifkant concurrency improvements for extended queueoperations such as non-blocking dequeue; priority dequeue; non-blocking enqueue; andothers.,Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1991,5
Towards Dynamic Green-Sizing for Database Servers.,Mustafa Korkmaz; Alexey Karyakin; Martin Karsten; Kenneth Salem,ABSTRACT This paper presents two techniques for reducing the power consumed bydatabase servers. Both techniques are intended primarily for transactional workloads onservers with memory-resident databases. The first technique is databasemanaged dynamicprocessor voltage and frequency scaling (DVFS). We show that a DBMS can exploit itsknowledge of the workload and performance constraints to obtain power savings that aremore than twice as large as the power savings achieved when DVFS is managed by theoperating system. The second technique is rank-aware memory allocation; the goal of whichis to power memory that the database system needs and avoid powering memory it does notneed. We present experiments that show rank-aware allocation allows unneeded memory tomove to low-power states; reducing memory power consumption.,ADMS@ VLDB,2015,4
Materialized views for eventually consistent record stores,Changjiu Jin; Rui Liu; Kenneth Salem,Distributed; replicated keyed-record stores are often used by applications that place apremium on high availability and scalability. Such systems provide fast access to storedrecords given a primary key value; but access without the primary key may be very slow andexpensive. This problem can be addressed using materialized views. Materialized viewsredundantly store records; or parts of records; and the redundant copies can be organizedand distributed differently than the originals; eg; according to the value of a secondary key.In this paper; we consider the problem of supporting materialized views in multi-master;eventually consistent keyed-record stores. Incremental maintenance of materialized views ischallenging in such systems because there no single master server responsible forserializing the updates to each record. We present a decentralized technique for …,Data Engineering Workshops (ICDEW); 2013 IEEE 29th International Conference on,2013,4
Two-phase clustering of large datasets,Najmeh Joze-Hkajavi; Kenneth Salem,Abstract This paper addresses the problem of single-pass clustering of large; multi-dimensional datasets. A general two-phase approach to this problem is de ned. In the rstphase; an in-memory summary of the data set is constructed using a single pass through thedata. The second phase then uses any clustering algorithm to cluster the summary data.Most clustering algorithms for large datasets are two-phase. Two techniques for producing in-memory summaries are compared. The rst is based on a previously-proposed data structurecalled a cluster-feature tree; or CF-tree. The second; simpler technique uses randomsampling. Experiments with skewed arti cial data sets are used to show that samplingproduces clusters that are at least as accurate as those produced with CF-trees; and thatsampling is much faster. An important issue when sampling is the sample size; which …,*,1998,4
Coordinating activities through extended sagas: a summary,Hector Garcia-Molina; Dieter Gawlick; Johannes Klein; Karl Kleissner; Kenneth Salem,The authors describe an environment designed to support activities such as a purchaseorder. They propose a simple set of services which would be useful for describing andexecuting activities. In an implementation; an underlying system would provide theseservices for activities; much as an operating system provides a set of services for processes.The environment consists of a system call interface (Create; Bind; Commit; Abort;CompensationBind; Send; Receive) which programs can use to request services. Theseservices are designed to support some of the important requirements of data processingactivities; including concurrency; modularity; fault tolerance; rollback; and communication.,Compcon Spring'91. Digest of Papers,1991,4
Failure recovery in memory resident transaction processing systems,KM Salem,Abstract A main memory transaction processing system holds a complete copy of itsdatabase in semiconductor memory. We present and compare; in a common framework; anumber of strategies for recovery management in main memory transaction processingsystems. These include strategies for asynchronously checkpointing the primary (mainmemory) database copy; and for maintaining a transaction log. Though they are not directlyconcerned with recovery management; we also consider strategies for updating the primarydatabase; since they affect the performance of the recovery manager. The recoverystrategies are compared using an analytic performance model and a testbedimplementation. The model computes two performance metrics: processor overhead andrecovery time. Processor overhead measures the impact of a recovery strategy during …,*,1989,4
Report: 4th Int'l Workshop on Self-Managing Database Systems (SMDB 2009).,Ashraf Aboulnaga; Kenneth Salem,The Fourth International Workshop on Self-Managing Database Systems took place onMarch 29th; 2009 in Shanghai; China; on the day before ICDE. The SMDB workshops bringtogether researchers and practitioners interested in making data management systemseasier to deploy and operate effectively. Topics of interest range from “traditional”management issues; such as database physical design; system tuning; and resourceallocation to more recent challenges around scalable and highly-available data services incloud computing environments. The SMDB Workshops are sponsored by the IEEE TCDEWorkgroup on Self-Managing Database Systems. The SMDB 2009 program began with akeynote presentation by James Hamilton; Vice President and Distinguished Engineer withAmazon Web Services. This was followed by five technical papers; presented in two …,IEEE Data Eng. Bull.,2009,3
Lazy database replication with freshness guarantees,Khuzaima Daudjee; Kenneth Salem,ABSTRACT Lazy replication is a popular technique for improving the performance andavailability of database systems. Although there are concurrency control techniques whichguarantee serializability in lazy replication systems; these techniques do not providefreshness guarantees. Since transactions may see stale data; they may be serialized in anorder different from the one in which they were submitted. Strong serializability avoids suchproblems; but it is very costly to implement. In this paper; we propose a generalized form ofstrong serializability that is suitable for use with lazy replication. It has many of theadvantages of strong serializability; but can be implemented more efficiently. We show howgeneralized strong serializability can be implemented in a lazy replication system; and wepresent the results of a simulation study that quantifies the strengths and limitations of the …,*,2002,3
System M: A Transaction Processing System for Memory Resident Data,Hector Garcia-Molina; Kenneth Salem,*,*,1988,3
Write Amplification: An Analysis of In-Memory Database Durability Techniques,Jaemyung Kim; Kenneth Salem; Khuzaima Daudjee,Abstract Modern in-memory database systems perform transactions an order of magnitudefaster than conventional database systems. While in-memory database systems can readthe database without I/O; database updates can generate a substantial amount of I/O; sinceupdates must normally be written to persistent secondary storage to ensure that they aredurable. In this paper we present a study of storage managers for in-memory databasesystems; with the goal of characterizing their I/O efficiency. We model the storage efficiencyof two classes of storage managers: those that perform in-place updates in secondarystorage; and those that use copy-on-write. Our models allow us to make meaningful;quantitative comparisons of storage managers' I/O efficiencies under a variety of conditions.,Proceedings of the 3rd VLDB Workshop on In-Memory Data Mangement and Analytics,2015,1
Database high availability using SHADOW systems,Jaemyung Kim; Kenneth Salem; Khuzaima Daudjee; Ashraf Aboulnaga; Xin Pan,Abstract Hot standby techniques are widely used to implement highly available databasesystems. These techniques make use of two separate copies of the database; an active copyand a backup that is managed by the standby. The two database copies are storedindependently and synchronized by the database systems that manage them. However;database systems deployed in computing clouds often have access to reliable persistentstorage that can be shared by multiple servers. In this paper we consider how hot standbytechniques can be improved in such settings. We present SHADOW systems; a novelapproach to hot standby high availability. Like other database systems that use sharedstorage; SHADOW systems push the task of managing database replication out of thedatabase system and into the underlying storage service; simplifying the database system …,Proceedings of the Sixth ACM Symposium on Cloud Computing,2015,1
Edgex: Edge replication for web applications,Hemant Saxena; Kenneth Salem,Global Web applications face the problem of high network latency due to their need tocommunicate with distant data centers. Many applications use edge networks for cachingimages; CSS; java script; and other static content in order to avoid some of this networklatency. However; for updates and for anything other than static content; communication withthe data center is still required; and can dominate application request latencies. One way toaddress this problem is to push more of the web application; as well the database on whichit depends; from the remote data center towards the edge of the network. In this paper; wepresent preliminary work in this direction. Specifically; we present an edge-aware dynamicdata replication architecture for relational database systems supporting Web applications.Our objective is to allow dynamic content to be served from the edge of the network; with …,Cloud Computing (CLOUD); 2015 IEEE 8th International Conference on,2015,1
Latency amplification: Characterizing the impact of web page content on load times,Catalin-Alexandru Avram; Kenneth Salem; Bernard Wong,Web users like sites that load quickly. Longer web page load times translate to reduced usersatisfaction and loss of revenue and mindshare. The time required to load a given web pageis difficult to predict because it is a complex function of many factors; such as the latenciesassociated with the network requests used to retrieve that content from remote servers.However; one of the most important factors is the page content; including the scripts; images;style sheets and other objects that are present on the page. In this paper we propose asimple metric for characterizing the content of a web page in terms of its impact on pageloading times. This metric; called the latency amplification factor (LAF); characterizes thecontent of a web page in terms of how it affects the page load time. The LAF of a web pagecan be estimated quickly and easily; and we describe a lightweight method for doing so …,Reliable Distributed Systems Workshops (SRDSW); 2014 IEEE 33rd International Symposium on,2014,1
Safety and Domain Independence.,Rodney W Topor,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,1
Report on the Second International Workshop on Self-Managing Database Systems (SMDB 2007).,Anastassia Ailamaki; Surajit Chaudhuri; Sam Lightstone; Guy M Lohman; Patrick Martin; Kenneth Salem; Gerhard Weikum,Information management systems are growing rapidly in scale and complexity; while skilleddatabase administrators are becoming rarer and more expensive. Increasingly; the total costof ownership of information management systems is dominated by the cost of people; ratherthan hardware or software costs. This economic dynamic dictates that information systems ofthe future be more automated and simpler to use; with most administration tasks transparentto the user.,IEEE Data Eng. Bull.,2007,1
Dynamic histograms for non-stationary updates,Elizabeth Lam; Kenneth Salem,In this paper; we address the problem of incrementally maintaining a histogram in responseto a non-stationary update process. In relational database systems; this problem can occurwhenever relations model time-varying activities. We present a simple update model that isgeneral enough to describe both stationary and non-stationary update processes; and weuse it to show that existing histogram maintenance techniques can perform poorly whenupdates are non-stationary. We describe several techniques for solving this problem; andwe use the update model to demonstrate that these techniques can effectively handle abroad range of update processes; including non-stationary ones.,Database Engineering and Application Symposium; 2005. IDEAS 2005. 9th International,2005,1
Processing TOVS Polar Pathfinder data using the distributed batch controller,James Duff; Kenneth M Salem; Axel Schweiger; Miron Livny,The distributed batch controller (DBC) supports scientific batch data processing. Batch jobsare distributed by the DBC over a collection of computing resources. Since these resourcesmay be widely scattered the DBC is well suited for collaborative research efforts whoseresources may not be centrally located. The DBC provides its users with centralizedmonitoring and control of distributed batch jobs. Version 1 of the DBC is currently being usedby the TOVS Polar Pathfinder project to generate Arctic atmospheric temperature andhumidity profiles. Profile generating jobs are distributed and executed by the DBC onworkstation clusters located at several sites across the US. This paper describes the dataprocessing requirements of the TOVS Polar Pathfinder project; and how the DBC is beingused to meet them. It also describes Version 2 of the DBC. DBC V2 is implemented in …,Earth Observing Systems II,1997,1
Data Engineering,Divy Agrawal; Amr El Abbadi; Kenneth Salem; Manuel Bravo; Nuno Diegues; Jingna Zeng; Paolo Romano; Luıs Rodrigues; Philip A Bernstein; Sudipto Das; Justin Levandoski; Sudipta Sengupta; Ryan Stutsman; Rui Wang; Tudor-Ioan Salomie; Gustavo Alonso; Bettina Kemme; Ivan Brondino; José Pereira; Ricardo Vilaça; Francisco Cruz; Rui Oliveira; Yousuf Ahmad,Members of the Technical Committee on Data Engineering (TCDE) have voted for a newTCDE. The turnout for the election was higher than in past elections; which demonstrates; Ithink; two things. One; past TCDE Chair Kyu-Young Whang's initiative to enlargemembership has resulted in a larger overall TCDE membership; and hence a largerelectorate. Two; we had two strong candidates in Xiaofang Zhou and Erich Neuhold; whichgenerate more interest than in the past. The outcome is that Xiaofang Zhou is the new chairof the TCDE. I want to congratulate Xiaofang on his election victory. I am confident thatXiaofang will be a fine TCDE chair; and I look forward to working with him going forward.Xiaofang's letter as TC Chair appears on page 2 of the current issue. The TCDE Chair is theone empowered to appoint the TCDE Executive Committee. The new committee is shown …,*,1995,1
Checkpointing and logging for memory-resident databases,Kenneth Salem; Hector Garcia-Molina,*,*,1989,1
Concurrency Controls for Global Procedures in Federated Database Systems,Rafael Alonso; Hector Garcia-Molina; Kenneth Salem,*,*,1987,1
Checkpointing Memory—,Kenneth Salem; H Garcia-Molina,Abstract A main memory database system holds all data in semiconductor memory. Forrecovery purposes; a backup copy of the database is maintained in secondary storage. Thecheckpointer is the component of the crash recovery manager responsible for maintainingthe backup copy. Ideally; the checkpointer should maintain an almost—up—to—date backupwhile interfering as little as possible with the system's transaction processing activities. Wepresent several algorithms for maintaining such a backup database; and compare themusing an analytic model. Our results show some signiﬁcant performance differences amongthe algorithms; and illustrate some of the tradeoffs that are available in designing such acheckpointer.,*,*,1
An analysis of memory power consumption in database systems,Alexey Karyakin; Kenneth Salem,Abstract The growing appetite for in-memory computing is increasing memory's share of totalserver power consumption. However; memory power consumption in databasemanagement systems is not well understood. This paper presents an empiricalcharacterization of memory power consumption in database systems; for both analytical andtransactional workloads. Our results indicate that memory power optimization will beeffective only if it can reduce back-ground power through more aggressive use of low powermemory idle states.,Proceedings of the 13th International Workshop on Data Management on New Hardware,2017,*
Integrating SSD Caching into Database Systems.,Xin Liu; Kenneth Salem,Abstract Flash-based solid state storage devices (SSDs) are now becoming commonplacein server environments. In this paper; we consider the use of SSDs as a persistent second-tier cache for database systems. We argue that it is desirable to change the behavior of thedatabase system's buffer cache when a second-tier SSD cache is used; so that the buffercache is aware of which pages are in the SSD cache. We propose such an SSD-awarebuffer cache manager; called GD2L. An interesting side effect of SSD-aware buffer cachemanagement is that the rate with which a page will be evicted or written from the buffercache will change when that page is moved into or out of the second-tier SSD cache. Wealso propose a technique; called CAC; for managing the contents of the second-tier cache.CAC is aware that moving pages into or out of the SSD cache will change their physical …,IEEE Data Eng. Bull.,2014,*
Fine-Grained Database Access Control,Kenneth Salem,Fine-Grained Relational Access Controls• control instance objects; not (just) schemaobjects• typically; control access to tuples• two models: predicate-based: for each user U andeach relation R; define a predicate (over the attributes of R) that determines accessibility ofeach R tuple to U. Example: Oracle VPD label-based: add a column of access rights labelsto each table; and associate access rights labels with each user. Accessibility of each tupleto U is determined by the tuple's label and U's label. Example: IBM DB2 LBAC,*,2007,*
Hints for Multi-Tier Caching,Kenneth Salem,My plan for CS497: 1. Caching Hints • a (systems) research project 2. Database Access Controland Privacy • a very hot topic • access control and privacy issues from DBMS perspective • onerelated project at UW … Caching Hints Xuhui Li (UW) Ashraf Aboulnaga (UW) Shaobo Gao(UW) Aamer Sachedina (IBM Toronto) Matt Huras (IBM Toronto) … DBMS Access Controls HuaxinZhang (UW) Ihab Ilyas(UW) … • buffer manager chooses a victim according to a replacementpolicy • new page overwrites the victim in the cache … • buffered copy of the page is updated• copy-back policy means that backing storage is not updated. (Why?) … 1. copy dirty victimback to storage 2. overwrite the victim in the cache … Problem: write latencies on the criticalpath of query execution Solution: clean dirty pages asynchronously (wrt. query execution); priorto eviction … Problem: read latencies on the critical path of query execution Solution …,*,2007,*
Adaptive block rearrangement under UNIX,Sedat Akyurek; Kenneth Salem,*,SOFTWARE-PRACTICE & EXPERIENCE,1997,*
Processing TOVS Polar Pathfinder Data Using the Distributed Batch Controller [3117-20],J Duff; KM Salem; A Schweiger; M Livny,*,PROCEEDINGS-SPIE THE INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING,1997,*
Non-deterministic queue operations,Hector Garciamolina; Kenneth Salem,Abstract Queues play a central role in transaction processing systems. We present atransaction model that allows significant concurrency improvements for extended queueoperations such as non-blocking dequeue; priority dequeue; non-blocking enqueue; andothers.,Journal of Computer and System Sciences,1995,*
Adaptive Block Rearrangement Under UNIX,Kenneth Salem,Abstract An adaptive UNIX disk device driver is described. To reduce seek times; the drivercopies frequentlyreferenced blocks from their original locations to reserved space near thecenter of the disk. Block reference frequencies need not be known in advance. Instead; theyare estimated by monitoring the stream of arriving requests. Measurements show that theadaptive driver reduces seek times and response times substantially.,*,1994,*
Placing Replicated Data to Reduce Seek Delays y,Kenneth Salem,Abstract In many environments; seek time is a major component of the disk access time. Inthis paper we introduce the idea of replicating data on a disk to reduce the average seektime. Our focus is on the problem of placing replicated data. We present several techniquesfor replica placement and evaluate their performance using trace-driven simulations.,*,1991,*
Modeling Long-Running Activities as Nested Sagas Hector Garcia-Molina Dept. of Computer Science; Princeton University; Princeton; NJ 08544 Dieter Gawlick; Joh...,Kenneth Salem,*,A Quarterly Bulletin of the Computer Society of the IEEE Technical Committee on Data Engineering,1991,*
Database-Managed CPU Performance Scaling for Improved Energy Efficiency,Mustafa Korkmaz; Martin Karsten; Kenneth Salem; Semih Salihoglu,ABSTRACT Dynamic voltage and frequency scaling (DVFS) is a technique for adjusting thespeed and power consumption of processors; allowing performance to be traded forreduced power consumption. Since CPUs are typically the largest consumers of power inmodern servers; DVFS can have a significant impact on overall server power consumption.Modern operating systems include DVFS governors; which interact with the processor tomanage performance and power consumption according to some system-level policy. In thispaper; we argue that for database servers; DVFS can be managed more effectively by thedatabase management system. We present a power-aware database request schedulingalgorithm called POLARIS. Unlike operating system governors; POLARIS is aware ofdatabase units of work and database performance targets; and can achieve a better …,*,*,*
Renormalization of NoSQL Database Schemas,Michael J Mior; Kenneth Salem,ABSTRACT Applications using a NoSQL database may duplicate attribute values in multiplephysical structures in order to provide additional access paths. This is necessary in order tomeet performance goals but introduces complications. Typically; the NoSQL DBMS has nounderstanding of applicationlevel denormalization. This means that if a user wishes to issuead-hoc queries; it is necessary to understand this denormalization and to manually selectwhich physical structures to use for a given query. Furthermore; if application requirementschange; this often forces a change in the schema. This can impact existing application codewhich is no longer valid under the previous physical design. We propose an algorithm forreconstructing a normalized logical schema from a denormalized physical design. Ourmethod begins with a description of a denormalized physical schema along with …,*,*,*
What is Virtualization?,Ashraf Aboulnaga Cristiana Amza; Kenneth Salem,Page 1. 1 Virtualization and Databases: State of the Art and Research Challenges AshrafAboulnaga Cristiana Amza Kenneth Salem University of Waterloo University of Toronto Universityof Waterloo What is Virtualization? ● Separating the abstract view of a computing resource orservice from the implementation of this resource or service ● A layer of indirection betweenabstract view and implementation − Hides implementation details C tl if bttitil t ti − Controls mappingfrom abstract view to implementation "any problem in computer science can be solved with anotherlayer of indirection" – David Wheeler Page 2. 2 App 1 App 3 App 2 Example: Virtual MachinesMachine CPU CPU Mem Operating System Virtual Machine CPU CPU Mem Net Physical MachineLayer of Indirection (VMM) Example: Virtual Storage App 1 App 3 App 2 Virtual Disk MachineCPU CPU Mem Operating System Physical Storage …,*,*,*
论文翻译: Deploying Database Appliances in the Cloud,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath,*,*,*,*
Report on the Fourth International Workshop on Self-Managing Database Systems (SMDB 2009),Ashraf Aboulnaga; Kenneth Salem,The Fourth International Workshop on Self-Managing Database Systems took place onMarch 29th; 2009 in Shanghai; China; on the day before ICDE. The SMDB workshops bringtogether researchers and practitioners interested in making data management systemseasier to deploy and operate effectively. Topics of interest range from “traditional”management issues; such as database physical design; system tuning; and resourceallocation to more recent challenges around scalable and highly-available data services incloud computing environments. The SMDB Workshops are sponsored by the IEEE TCDEWorkgroup on Self-Managing Database Systems. The SMDB 2009 program began with akeynote presentation by James Hamilton; Vice President and Distinguished Engineer withAmazon Web Services. This was followed by five technical papers; presented in two …,*,*,*
Distributed Scientific Data Processing Using the DBC1,James Duff; Kenneth Salem; Miron Livny,Abstract The Distributed Batch Controller (DBC) supports scienti c batch data processing.The DBC distributes batch jobs to one or more pools of workstations and monitors andcontrols their execution. The pools themselves may be geographically distributed; and neednot be dedicated to processing batch jobs. We describe the use of the DBC in a large scientic data processing application; namely the generation of atmospheric temperature andhumidity pro les from satellite data. This application shows that the DBC can be an e ectiveplatform for distributed batch processing. It also highlights several design andimplementation issues that arise in distributed data processing systems.,*,*,*
Data Engineering,Ashraf Aboulnaga; Kenneth Salem; Ahmed A Soror; Umar Farooq Minhas; Peter Kokosielis; Sunil Kamath; Elisa Bertino; Federica Paci; Rodolfo Ferrini; Ning Shang; Kevin Beyer; Vuk Ercegovac; Rajasekar Krishnamurthy; Sriram Raghavan; Jun Rao; Frederick Reiss; Eugene J Shekita; David Simmen; Sandeep Tata; Shivakumar Vaithyanathan; Huaiyu Zhu,The Data Engineering Bulletin The Bulletin of the Technical Committee on Data Engineeringis published quarterly and is distributed to all TC members. Its scope includes the design;implementation; modelling; theory and application of database systems and theirtechnology. Letters; conference information; and news should be sent to the Editor-in-Chief.Papers for each issue are solicited by and should be sent to the Associate Editorresponsible for the issue. Opinions expressed in contributions are those of the authors anddo not necessarily reflect the positions of the TC on Data Engineering; the IEEE ComputerSociety; or the authors' organizations. The Data Engineering Bulletin web site is at http://tab.computer. org/tcde/bull_about. html.,*,*,*
Management of Partially-Safe Bu ers,Kenneth Salem,Abstract Safe RAM is RAM which has been made as reliable as a disk. We consider theproblem of bu er management in partially-safe bu ers; ie; bu ers which contain both safeRAM and volatile RAM. Bu er management techniques for partially-safe bu ers explicitlyconsider the safety of memory in deciding which data to place in the bu er; where to place it;and when to copy updates back to the disk. We present techniques for managing such buers and study their performance using trace-driven simulations.,*,*,*
Immediate Propagate Deferred Apply for Incremental Maintenance of Materialized Views,Kevin S Beyer; Roberta Cochrane; Hamid Pirahesh Richard Sidle; Jayavel Shanmugasundaram; С Mohan; Kenneth Salem,*,*,*,*
Fast Access to Large Databases,Kenneth Salem,*,Conference Proceedings,*,*
Department of Computer Science University of Waterloo Waterloo; Ontario N2L 3G1 Canada fapmarathe; kmsalemg@ db. uwaterloo. ca,Arunprasad P Marathe; Kenneth Salem,*,dim,*,*
Department of Computer Science University of Waterloo Waterloo; Ontario N2L 3G1 Canada,Arunprasad P Marathe; Kenneth Salem,*,dim,*,*
PSALM: Accurate Sampling for Cardinality Estimation in a Multi-user Environment,Huaxin Zhang; Ihab F Ilyas; Kenneth Salem,Abstract In database systems that support fine-grained access controls; each user hasaccess rights that determine which tuples are accessible and which are inaccessible.Queries are answered as if the inaccessible tuples are not present in the database. Thus;users with different access rights may get different answers to a given query. To processqueries efficiently in the presence of fine-grained access controls; the database systemneeds accurate estimates of the number of tuples that are both accessible according to theaccess rights of the submitting user and relevant according to the selection predicates in thequery. In this paper we present sampling-based cardinality estimation techniques for use inthe presence of fine-grained access controls. These techniques exploit the fact that accessrights are relatively static and are common to all queries that are evaluated on behalf of a …,*,*,*
T o hase) l2sterinB of DarBe HatasetsS,NaWmeh Johe-KhaWavi; Kenneth Salem,Abstract This paper addresses the problem of single-pass clustering of large; multi-dimensional datasets. A general two-phase approach to this problem is defined. In the firstphase; an in-memory summary of the data set is constructed using a single pass through thedata. The second phase then uses any clustering algorithm to cluster the summary data.Most clustering algorithms for large datasets are two-phase. Two techniques for producing in-memory summaries are compared. The first is based on a previously-proposed datastructure called a cluster-feature tree; or CF-tree. The second; simpler technique usesrandom sampling. Experiments with skewed artificial data sets are used to show thatsampling produces clusters that are at least as accurate as those produced with CF-trees;and that sampling is much faster. An important issue when sampling is the sample size …,*,*,*
