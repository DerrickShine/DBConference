RDF-3X: a RISC-style engine for RDF,Thomas Neumann; Gerhard Weikum,Abstract RDF is a data representation format for schema-free structured information that isgaining momentum in the context of Semantic-Web corpora; life sciences; and also Web 2.0platforms. The" pay-as-you-go" nature of RDF and the flexible pattern-matching capabilitiesof its query language SPARQL entail efficiency and scalability challenges for complexqueries including long join paths. This paper presents the RDF-3X engine; animplementation of SPARQL that achieves excellent performance by pursuing a RISC-stylearchitecture with a streamlined architecture and carefully designed; puristic data structuresand operations. The salient points of RDF-3X are: 1) a generic solution for storing andindexing RDF triples that completely eliminates the need for physical-design tuning; 2) apowerful yet simple query processor that leverages fast merge joins to the largest …,Proceedings of the VLDB Endowment,2008,532
The RDF-3X engine for scalable management of RDF data,Thomas Neumann; Gerhard Weikum,Abstract RDF is a data model for schema-free structured information that is gainingmomentum in the context of Semantic-Web data; life sciences; and also Web 2.0 platforms.The" pay-as-you-go" nature of RDF and the flexible pattern-matching capabilities of its querylanguage SPARQL entail efficiency and scalability challenges for complex queries includinglong join paths. This paper presents the RDF-3X engine; an implementation of SPARQL thatachieves excellent performance by pursuing a RISC-style architecture with streamlinedindexing and query processing. The physical design is identical for all RDF-3X databasesregardless of their workloads; and completely eliminates the need for index tuning byexhaustive indexes for all permutations of subject-property-object triples and their binaryand unary projections. These indexes are highly compressed; and the query processor …,The VLDB Journal—The International Journal on Very Large Data Bases,2010,460
HyPer: A hybrid OLTP&OLAP main memory database system based on virtual memory snapshots,Alfons Kemper; Thomas Neumann,The two areas of online transaction processing (OLTP) and online analytical processing(OLAP) present different challenges for database architectures. Currently; customers withhigh rates of mission-critical transactions have split their data into two separate systems; onedatabase for OLTP and one so-called data warehouse for OLAP. While allowing for decenttransaction rates; this separation has many disadvantages including data freshness issuesdue to the delay caused by only periodically initiating the Extract Transform Load-datastaging and excessive resource consumption due to maintaining two separate informationsystems. We present an efficient hybrid system; called HyPer; that can handle both OLTPand OLAP simultaneously by using hardware-assisted replication mechanisms to maintainconsistent snapshots of the transactional data. HyPer is a main-memory database system …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,388
Efficiently compiling efficient query plans for modern hardware,Thomas Neumann,Abstract As main memory grows; query performance is more and more determined by theraw CPU costs of query processing itself. The classical iterator style query processingtechnique is very simple and exible; but shows poor performance on modern CPUs due tolack of locality and frequent instruction mispredictions. Several techniques like batchoriented processing or vectorized tuple processing have been proposed in the past toimprove this situation; but even these techniques are frequently out-performed by hand-written execution plans. In this work we present a novel compilation strategy that translates aquery into compact and efficient machine code using the LLVM compiler framework. Byaiming at good code and data locality and predictable branch layout the resulting codefrequently rivals the performance of hand-written C++ code. We integrated these …,Proceedings of the VLDB Endowment,2011,262
Scalable join processing on very large RDF graphs,Thomas Neumann; Gerhard Weikum,Abstract With the proliferation of the RDF data format; engines for RDF query processing arefaced with very large graphs that contain hundreds of millions of RDF triples. This paperaddresses the resulting scalability problems. Recent prior work along these lines hasfocused on indexing and other physical-design issues. The current paper focuses on joinprocessing; as the fine-grained and schema-relaxed use of RDF often entails star-and chain-shaped join queries with many input streams from index scans. We present two contributionsfor scalable join processing. First; we develop very light-weight methods for sidewaysinformation passing between separate joins at query run-time; to provide highly effectivefilters on the input streams of joins. Second; we improve previously proposed algorithms forjoin-order optimization by more accurate selectivity estimations for very large RDF graphs …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,240
Efficient top-k querying over social-tagging networks,Ralf Schenkel; Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane X Parreira; Gerhard Weikum,Abstract Online communities have become popular for publishing and searching content; aswell as for finding and connecting to other users. User-generated content includes; forexample; personal blogs; bookmarks; and digital photos. These items can be annotated andrated by different users; and these social tags and derived user-specific scores can beleveraged for searching relevant content and discovering subjectively interesting items.Moreover; the relationships among users can also be taken into consideration for rankingsearch results; the intuition being that you trust the recommendations of your close friendsmore than those of your casual acquaintances. Queries for tag or keyword combinations thatcompute and rank the top-k results thus face a large variety of options that complicate thequery processing and pose efficiency challenges. This paper addresses these issues by …,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,180
Massively parallel sort-merge joins in main memory multi-core database systems,Martina-Cezara Albutiu; Alfons Kemper; Thomas Neumann,Abstract Two emerging hardware trends will dominate the database system technology inthe near future: increasing main memory capacities of several TB per server and massivelyparallel multi-core processing. Many algorithmic and control techniques in current databasetechnology were devised for disk-based systems where I/O dominated the performance. Inthis work we take a new look at the well-known sort-merge join which; so far; has not been inthe focus of research in scalable massively parallel multi-core data processing as it wasdeemed inferior to hash joins. We devise a suite of new massively parallel sort-merge(MPSM) join algorithms that are based on partial partition-based sorting. Contrary toclassical sort-merge joins; our MPSM algorithms do not rely on a hard to parallelize finalmerge step to create one complete sort order. Rather they work on the independently …,Proceedings of the VLDB Endowment,2012,165
Characteristic sets: Accurate cardinality estimation for RDF queries with multiple joins,Thomas Neumann; Guido Moerkotte,Accurate cardinality estimates are essential for a successful query optimization. This is notonly true for relational DBMSs but also for RDF stores. An RDF database consists of a set oftriples and; hence; can be seen as a relational database with a single table with threeattributes. This makes RDF rather special in that queries typically contain many self joins.We show that relational DBMSs are not well-prepared to perform cardinality estimation inthis context. Further; there are hardly any special cardinality estimation methods for RDFdatabases. To overcome this lack of appropriate cardinality estimation methods; weintroduce characteristic sets together with new cardinality estimation methods based uponthem. We then show experimentally that the new methods are-in the RDF context-highlysuperior to the estimation methods employed by commercial DBMSs and by the open …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,130
The adaptive radix tree: ARTful indexing for main-memory databases,Viktor Leis; Alfons Kemper; Thomas Neumann,Main memory capacities have grown up to a point where most databases fit into RAM. Formain-memory database systems; index structure performance is a critical bottleneck.Traditional in-memory data structures like balanced binary search trees are not efficient onmodern hardware; because they do not optimally utilize on-CPU caches. Hash tables; alsooften used for main-memory indexes; are fast but only support point queries. To overcomethese shortcomings; we present ART; an adaptive radix tree (trie) for efficient indexing inmain memory. Its lookup performance surpasses highly tuned; read-only search trees; whilesupporting very efficient insertions and deletions as well. At the same time; ART is veryspace efficient and solves the problem of excessive worst-case space consumption; whichplagues most radix trees; by adaptively choosing compact and efficient data structures for …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,120
Characterization of a novel 21-kb deletion; CFTRdele2; 3 (21 kb); in the CFTR gene: a cystic fibrosis mutation of Slavic origin common in Central and East Europe,Thilo Dörk; Milan Macek Jr; Frauke Mekus; Burkhard Tümmler; John Tzountzouris; Teresa Casals; Alice Krebsová; Monika Koudová; Iva Sakmaryová; Milan Macek Sr; Vecaron Vávrová; Dana Zemková; Evgeny Ginter; Nica V Petrova; Tatiana Ivaschenko; Vladislav Baranov; Michal Witt; Andrzej Pogorzelski; Jerzy Bal; Cesary Zékanowsky; K Wagner; M Stuhrmann; I Bauer; HH Seydewitz; T Neumann; S Jakubiczka; C Kraus; B Thamm; M Nechiporenko; L Livshits; N Mosse; G Tsukerman; L Kadási; M Ravnik-Glavač; D Glavač; R Komel; K Vouk; V Kučinskas; A Krumina; M Teder; S Kocheva; GD Efremov; T Onay; B Kirdar; G Malone; M Schwarz; Z Zhou; KJ Friedman; S Carles; M Claustres; D Bozon; C Verlingue; C Férec; M Tzetis; E Kanavakis; H Cuppens; C Bombieri; PF Pignatti; F Sangiuolo; A Jordanova; J Kusic; D Radojkovič; Jadranka Sertić; D Richter; A Stavljenić Rukavina; E Bjorck; B Strandvik; H Cardoso; M Montgomery; B Nakielna; D Hughes; X Estivill; I Aznarez; E Tullis; L-C Tsui; J Zielenski,Abstract. We report a large genomic deletion of the cystic fibrosis transmembraneconductance regulator (CFTR) gene; viz.; a deletion that is frequently observed in Centraland Eastern Europe. The mutation; termed CFTR dele2; 3 (21 kb); deletes 21;080 bpspanning introns 1–3 of the CFTR gene. Transcript analyses have revealed that this deletionresults in the loss of exons 2 and 3 in epithelial CFTR mRNA; thereby producing apremature termination signal within exon 4. In order to develop a simple polymerase chainreaction assay for this allele; we defined the end-points of the deletion at the DNA sequencelevel. We next screened for this mutation in a representative set of European and European-derived populations. Some 197 CF patients; including seven homozygotes; bearing thismutation have been identified during the course of our study. Clinical evaluation of CFTR …,Human genetics,2000,117
x-RDF-3X: fast querying; high update rates; and consistency for RDF databases,Thomas Neumann; Gerhard Weikum,Abstract The RDF data model is gaining importance for applications in computationalbiology; knowledge sharing; and social communities. Recent work on RDF engines hasfocused on scalable performance for querying; and has largely disregarded updates. Inaddition to incremental bulk loading; applications also require online updates with flexiblecontrol over multi-user isolation levels and data consistency. The challenge lies in meetingthese requirements while retaining the capability for fast querying. This paper presents acomprehensive solution that is based on an extended deferred-indexing method withintegrated versioning. The version store enables time-travel queries that are efficientlyprocessed without adversely affecting queries on the current data. For flexible consistency;transactional concurrency control is provided with options for either snapshot isolation or …,Proceedings of the VLDB Endowment,2010,113
A time machine for text search,Klaus Berberich; Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract Text search over temporally versioned document collections such as web archiveshas received little attention as a research problem. As a consequence; there is no scalableand principled solution to search such a collection as of a specified time. In this work; weaddress this shortcoming and propose an efficient solution for time-travel text search byextending the inverted file index to make it ready for temporal search. We introduceapproximate temporal coalescing as a tunable method to reduce the index size withoutsignificantly affecting the quality of results. In order to further improve the performance oftime-travel queries; we introduce two principled techniques to trade off index size for itsperformance. These techniques can be formulated as optimization problems that can besolved to near-optimality. Finally; our approach is evaluated in a comprehensive series of …,Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,2007,107
Morsel-driven parallelism: a NUMA-aware query evaluation framework for the many-core age,Viktor Leis; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract With modern computer architecture evolving; two problems conspire against thestate-of-the-art approaches in parallel query execution:(i) to take advantage of many-cores;all query work must be distributed evenly among (soon) hundreds of threads in order toachieve good speedup; yet (ii) dividing the work evenly is difficult even with accurate datastatistics due to the complexity of modern out-of-order cores. As a result; the existingapproaches for plan-driven parallelism run into load balancing and context-switchingbottlenecks; and therefore no longer scale. A third problem faced by many-core architecturesis the decentralization of memory controllers; which leads to Non-Uniform Memory Access(NUMA). In response; we present the morsel-driven query execution framework; wherescheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,93
Exploiting social relations for query expansion and result ranking,Matthias Bender; Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane Xavier Parreira; Ralf Schenkel; Gerhard Weikum,Online communities have recently become a popular tool for publishing and searchingcontent; as well as for finding and connecting to other users that share common interests.The content is typically user-generated and includes; for example; personal blogs;bookmarks; and digital photos. A particularly intriguing type of content is user-generatedannotations (tags) for content items; as these concise string descriptions allow forreasonings about the interests of the user who created the content; but also about the userwho generated the annotations. This paper presents a framework to cast the different entitiesof such networks into a unified graph model representing the mutual relationships of users;content; and tags. It derives scoring functions for each of the entities and relations. We haveperformed an experimental evaluation on two real-world datasets (crawled from deli. cio …,Data engineering workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,93
Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products,Guido Moerkotte; Thomas Neumann,Abstract Two approaches to derive dynamic programming algorithms for constructing jointrees are described in the literature. We show analytically and experimentally that these twovariants exhibit vastly diverging runtime behaviors for different query graphs. Morespecifically; each variant is superior to the other for one kind of query graph (chain or clique);but fails for the other. Moreover; neither of them handles star queries well. This motivates usto derive an algorithm that is superior to the two existing algorithms because it adapts to thesearch space implied by the query graph.,Proceedings of the 32nd international conference on Very large data bases,2006,86
Exploiting hardware transactional memory in main-memory databases,Viktor Leis; Alfons Kemper; Thomas Neumann,So far; transactional memory-although a promising technique-suffered from the absence ofan efficient hardware implementation. The upcoming Haswell microarchitecture from Intelintroduces hardware transactional memory (HTM) in mainstream CPUs. HTM allows forefficient concurrent; atomic operations; which is also highly desirable in the context ofdatabases. On the other hand HTM has several limitations that; in general; prevent a one-to-one mapping of database transactions to HTM transactions. In this work we devise severalbuilding blocks that can be used to exploit HTM in main-memory databases. We show thatHTM allows to achieve nearly lock-free processing of database transactions by carefullycontrolling the data layout and the access patterns. The HTM component is used fordetecting the (infrequent) conflicts; which allows for an optimistic; and thus very low …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,81
The mixed workload CH-benCHmark,Richard Cole; Florian Funke; Leo Giakoumakis; Wey Guy; Alfons Kemper; Stefan Krompass; Harumi Kuno; Raghunath Nambiar; Thomas Neumann; Meikel Poess; Kai-Uwe Sattler; Michael Seibold; Eric Simon; Florian Waas,Abstract While standardized and widely used benchmarks address either operational or real-time Business Intelligence (BI) workloads; the lack of a hybrid benchmark led us to thedefinition of a new; complex; mixed workload benchmark; called mixed workload CH-benCHmark. This benchmark bridges the gap between the established single-workloadsuites of TPC-C for OLTP and TPC-H for OLAP; and executes a complex mixed workload: atransactional workload based on the order entry processing of TPC-C and a correspondingTPC-H-equivalent OLAP query suite run in parallel on the same tables in a single databasesystem. As it is derived from these two most widely used TPC benchmarks; the CH-benCHmark produces results highly relevant to both hybrid and classic single-workloadsystems.,Proceedings of the Fourth International Workshop on Testing Database Systems,2011,72
Dynamic programming strikes back,Guido Moerkotte; Thomas Neumann,Abstract Two highly efficient algorithms are known for optimally ordering joins while avoidingcross products: DPccp; which is based on dynamic programming; and Top-Down PartitionSearch; based on memoization. Both have two severe limitations: They handle only (1)simple (binary) join predicates and (2) inner joins. However; real queries may containcomplex join predicates; involving more than two relations; and outer joins as well as othernon-inner joins. Taking the most efficient known join-ordering algorithm; DPccp; as a startingpoint; we first develop a new algorithm; DPhyp; which is capable to handle complex joinpredicates efficiently. We do so by modeling the query graph as a (variant of a) hypergraphand then reason about its connected subgraphs. Then; we present a technique to exploit thiscapability to efficiently handle the widest class of non-inner joins dealt with so far. Our …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,67
Fast serializable multi-version concurrency control for main-memory database systems,Thomas Neumann; Tobias Mühlbauer; Alfons Kemper,Abstract Multi-Version Concurrency Control (MVCC) is a widely employed concurrencycontrol mechanism; as it allows for execution modes where readers never block writers.However; most systems implement only snapshot isolation (SI) instead of full serializability.Adding serializability guarantees to existing SI implementations tends to be prohibitivelyexpensive. We present a novel MVCC implementation for main-memory database systemsthat has very little overhead compared to serial execution with single-version concurrencycontrol; even when maintaining serializability guarantees. Updating data in-place andstoring versions as before-image deltas in undo buffers not only allows us to retain the highscan performance of single-version systems but also forms the basis of our cheap and fine-grained serializability validation mechanism. The novel idea is based on an adaptation of …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,62
How good are query optimizers; really?,Viktor Leis; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark (JOB) and experimentally revisit the main componentsin the classic query optimizer architecture using a complex; real-world data set and realisticmulti-join queries. We investigate the quality of industrial-strength cardinality estimators andfind that all estimators routinely produce large errors. We further show that while estimatesare essential for finding a good join order; query performance is unsatisfactory if the queryengine relies too heavily on these estimates. Using another set of experiments that measurethe impact of the cost model; we find that it has much less influence on query performancethan the cardinality estimates. Finally; we investigate plan enumeration techniquescomparing exhaustive dynamic programming with heuristic algorithms and find that …,Proceedings of the VLDB Endowment,2015,58
Novel mutations in the gene SALL4 provide further evidence for acro-renal-ocular and Okihiro syndromes being allelic entities; and extend the phenotypic spectrum,W Borozdin; MJ Wright; RCM Hennekam; MC Hannibal; YJ Crow; TE Neumann; J Kohlhase,METHODS Patients Venous blood was collected from patients and unaffected relatives afterobtaining their informed consent. Genetic analysis Genomic DNA was prepared fromperipheral lymphocytes by routine procedures. Mutation analysis of SALL4 exons 1–4(complete coding region) was performed as described. 5 Mutations were confirmed by asecond; independent PCR; and direct sequencing of amplicons. In one family; the exactmutation was not readable from the direct sequencing,Journal of medical genetics,2004,56
Social Wisdom for Search and Recommendation.,Ralf Schenkel; Tom Crecelius; Mouna Kacimi; Thomas Neumann; Josiane Xavier Parreira; Marc Spaniol; Gerhard Weikum,Abstract Social-tagging communities offer great potential for smart recommendation and“socially enhanced” searchresult ranking. Beyond traditional forms of collaborativerecommendation that are based on the item-user matrix of the entire community; a specificopportunity of social communities is to reflect the different degrees of friendships and mutualtrust; in addition to the behavioral similarities among users. This paper presents a frameworkfor harnessing such social relations for search and recommendation. The framework isimplemented in the SENSE prototype system; and its usefulness is demonstrated inexperiments with an excerpt of the librarything community data.,IEEE Data Eng. Bull.,2008,49
Compacting transactional data in hybrid OLTP&OLAP databases,Florian Funke; Alfons Kemper; Thomas Neumann,Abstract Growing main memory sizes have facilitated database management systems thatkeep the entire database in main memory. The drastic performance improvements that camealong with these in-memory systems have made it possible to reunite the two areas of onlinetransaction processing (OLTP) and online analytical processing (OLAP): An emerging classof hybrid OLTP and OLAP database systems allows to process analytical queries directly onthe transactional data. By offering arbitrarily current snapshots of the transactional data forOLAP; these systems enable real-time business intelligence. Despite memory sizes ofseveral Terabytes in a single commodity server; RAM is still a precious resource: Since freememory can be used for intermediate results in query processing; the amount of memorydetermines query performance to a large extent. Consequently; we propose the …,Proceedings of the VLDB Endowment,2012,48
Instant loading for main memory databases,Tobias Mühlbauer; Wolf Rödiger; Robert Seilbeck; Angelika Reiser; Alfons Kemper; Thomas Neumann,Abstract eScience and big data analytics applications are facing the challenge of efficientlyevaluating complex queries over vast amounts of structured text data archived in networkstorage solutions. To analyze such data in traditional disk-based database systems; it needsto be bulk loaded; an operation whose performance largely depends on the wire speed ofthe data source and the speed of the data sink; ie; the disk. As the speed of networkadapters and disks has stagnated in the past; loading has become a major bottleneck. Thedelays it is causing are now ubiquitous as text formats are a preferred storage format forreasons of portability. But the game has changed: Ever increasing main memory capacitieshave fostered the development of in-memory database systems and very fast networkinfrastructures are on the verge of becoming economical. While hardware limitations for …,Proceedings of the VLDB Endowment,2013,45
TPC-H analyzed: Hidden messages and lessons learned from an influential benchmark,Peter Boncz; Thomas Neumann; Orri Erling,Abstract The TPC-D benchmark was developed almost 20 years ago; and even though itscurrent existence as TPC-H could be considered superseded by TPC-DS; one can still learnfrom it. We focus on the technical level; summarizing the challenges posed by the TPC-Hworkload as we now understand them; which we call “choke points”. We identify 28 differentsuch choke points; grouped into six categories: Aggregation Performance; JoinPerformance; Data Access Locality; Expression Calculation; Correlated Subqueries andParallel Execution. On the meta-level; we make the point that the rich set of choke-pointsfound in TPC-H sets an example on how to design future DBMS benchmarks.,Technology Conference on Performance Evaluation and Benchmarking,2013,44
Active knowledge: dynamically enriching RDF knowledge bases by web services,Nicoleta Preda; Gjergji Kasneci; Fabian M Suchanek; Thomas Neumann; Wenjun Yuan; Gerhard Weikum,Abstract The proliferation of knowledge-sharing communities and the advances ininformation extraction have enabled the construction of large knowledge bases using theRDF data model to represent entities and relationships. However; as the Web and its latentlyembedded facts evolve; a knowledge base can never be complete and up-to-date. On theother hand; a rapidly increasing suite of Web services provide access to timely and high-quality information; but this is encapsulated by the service interface. We propose to leveragethe information that could be dynamically obtained from Web services in order to enrich RDFknowledge bases on the fly whenever the knowledge base does not suffice to answer a userquery. To this end; we develop a sound framework for appropriately generating queries toencapsulated Web services and efficient algorithms for query execution and result …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,43
The linked data benchmark council: a graph and RDF industry benchmarking effort,Renzo Angles; Peter Boncz; Josep Larriba-Pey; Irini Fundulaki; Thomas Neumann; Orri Erling; Peter Neubauer; Norbert Martinez-Bazan; Venelin Kotsev; Ioan Toma,Abstract The Linked Data Benchmark Council (LDBC) is an EU project that aims to developindustry-strength benchmarks for graph and RDF data management systems. It includes thecreation of a non-profit LDBC organization; where industry players and academia cometogether for managing the development of benchmarks as well as auditing and publishingofficial results. We present an overview of the project including its goals and organization;and describe its process and design methodology for benchmark development. Weintroduce so-called" choke-point" based benchmark development through which expertsidentify key technical challenges; and introduce them in the benchmark workload. Finally;we present the status of two benchmarks currently in development; one targeting graph datamanagement systems using a social network data case; and the other targeting RDF …,ACM SIGMOD Record,2014,41
Exploiting the query structure for efficient join ordering in SPARQL queries.,Andrey Gubichev; Thomas Neumann,ABSTRACT The join ordering problem is a fundamental challenge that has to be solved byany query optimizer. Since the high-performance RDF systems are often implemented astriple stores (ie; they represent RDF data as a single table with three attributes; at leastconceptually); the query optimization strategies employed by such systems are oftenadopted from relational query optimization. In this paper we show that the techniquesborrowed from traditional SQL query optimization (such as Dynamic Programming algorithmor greedy heuristics) are not immediately capable of handling large SPARQL queries. Weintroduce a new join ordering algorithm that performs a SPARQL-tailored querysimplification. Furthermore; we present a novel RDF statistical synopsis that accuratelyestimates cardinalities in large SPARQL queries. Our experiments show that this …,EDBT,2014,41
Preventing bad plans by bounding the impact of cardinality estimation errors,Guido Moerkotte; Thomas Neumann; Gabriele Steidl,Abstract Query optimizers rely on accurate estimations of the sizes of intermediate results.Wrong size estimations can lead to overly expensive execution plans. We first define the q-error to measure deviations of size estimates from actual sizes. The q-error enables thederivation of two important results:(1) We provide bounds such that if the q-error is smallerthan this bound; the query optimizer constructs an optimal plan.(2) If the q-error is boundedby a number q; we show that the cost of the produced plan is at most a factor of q 4 worsethan the optimal plan. Motivated by these findings; we next show how to find the bestapproximation under the q-error. These techniques can then be used to build synopsis forsize estimates. Finally; we give some experimental results where we apply the developedtechniques.,Proceedings of the VLDB Endowment,2009,40
A reproducible benchmark for p2p retrieval,Thomas Neumann; Matthias Bender; Sebastian Michel; Gerhard Weikum; Philippe Bonnet; Ioana Manolescu,Abstract With the growing popularity of information retrieval (IR) in distributed systems and inparticular {P2P} Web search; a huge number of protocols and prototypes have beenintroduced in the literature. However; nearly every paper considers a different benchmark forits experimental evaluation; rendering their mutual comparison and the quantification ofperformance improvements an impossible task. We present a standardized; generalpurpose benchmark for {P2P IR} systems that finally makes this possible. We start bypresenting a detailed requirement analysis for such a standardized benchmark frameworkthat allows for reproducible and comparable experimental setups without sacrificingflexibility to suit different system models. We further suggest Wikipedia as a publicly-available and all-purpose document corpus and finally introduce a simple but yet flexible …,Untitled Event,2006,38
Locality-sensitive operators for parallel main-memory database clusters,Wolf Rodiger; Tobias Muhlbauer; Philipp Unterbrunner; Angelika Reiser; Alfons Kemper; Thomas Neumann,The growth in compute speed has outpaced the growth in network bandwidth over the lastdecades. This has led to an increasing performance gap between local and distributedprocessing. A parallel database cluster thus has to maximize the locality of queryprocessing. A common technique to this end is to co-partition relations to avoid expensivedata shuffling across the network. However; this is limited to one attribute per relation and isexpensive to maintain in the face of updates. Other attributes often exhibit a fuzzy co-locationdue to correlations with the distribution key but current approaches do not leverage this. Inthis paper; we introduce locality-sensitive data shuffling; which can dramatically reduce theamount of network communication for distributed operators such as join and aggregation.We present four novel techniques:(i) optimal partition assignment exploits locality to …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,35
Near-optimal dynamic replication in unstructured peer-to-peer networks,Mauro Sozio; Thomas Neumann; Gerhard Weikum,Abstract Replicating data in distributed systems is often needed for availability andperformance. In unstructured peer-to-peer networks; with epidemic messaging for queryrouting; replicating popular data items is also crucial to ensure high probability of finding thedata within a bounded search distance from the requestor. This paper considers suchnetworks and aims to maximize the probability of successful search. Prior work along theselines has analyzed the optimal degrees of replication for data items with non-uniform butglobal request rates; but did not address the issue of where replicas should be placed andwas very very limited in the capabilities for handling heterogeneity and dynamics of networkand workload. This paper presents the integrated P2R2 algorithm for dynamic replicationthat addresses all these issues; and determines both the degrees of replication and the …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,35
Query simplification: graceful degradation for join-order optimization,Thomas Neumann,Abstract Join ordering is one of the most important; but also most challenging problems ofquery optimization. In general finding the optimal join order is NP-hard. Existing dynamicprogramming algorithms exhibit exponential runtime even for the restricted; but highlyrelevant class of star joins. Therefore; it is infeasible to find the optimal join order when thequery includes a large number of joins. Existing approaches for large queries switch togreedy heuristics or randomized algorithms at some point; which can degrade queryexecution performance by orders of magnitude. We propose a new paradigm for optimizinglarge queries: when a query is too complex to be optimized exactly; we simplify the query'sjoin graph until the optimization problem becomes tractable within a given time budget.During simplification; we apply safe simplifications before more risky ones. This way join …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,34
A combined framework for grouping and order optimization,Thomas Neumann; Guido Moerkotte,Abstract Since the introduction of cost-based query optimization by Selinger et al. in theirseminal paper; the performance-critical role of interesting orders has been recognized.Some algebraic operators change interesting orders (eg sort and select); while othersexploit them (eg merge join). Likewise; Wang and Cherniack (VLDB 2003) showed thatexisting groupings should be exploited to avoid redundant grouping operations. Ideally; thereasoning about interesting orderings and groupings should be integrated into oneframework. So far; no complete; correct; and efficient algorithm for ordering and groupinginference has been proposed. We fill this gap by proposing a general two-phase approachthat efficiently integrates the reasoning about orderings and groupings. Our experimentalresults show that with a modest increase of the time and space requirements of the …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,34
The more the merrier: Efficient multi-source graph traversal,Manuel Then; Moritz Kaufmann; Fernando Chirigati; Tuan-Anh Hoang-Vu; Kien Pham; Alfons Kemper; Thomas Neumann; Huy T Vo,Abstract Graph analytics on social networks; Web data; and communication networks hasbeen widely used in a plethora of applications. Many graph analytics algorithms are basedon breadth-first search (BFS) graph traversal; which is not only time-consuming for largedatasets but also involves much redundant computation when executed multiple times fromdifferent start vertices. In this paper; we propose Multi-Source BFS (MS-BFS); an algorithmthat is designed to run multiple concurrent BFSs over the same graph on a single CPU corewhile scaling up as the number of cores increases. MS-BFS leverages the properties ofsmall-world networks; which apply to many real-world graphs; and enables efficient graphtraversal that:(i) shares common computation across concurrent BFSs;(ii) greatly reduces thenumber of random memory accesses; and (iii) does not incur synchronization costs. We …,Proceedings of the VLDB Endowment,2014,32
An efficient framework for order optimization,Thomas Neumann; Guido Moerkotte,Since the introduction of cost-based query optimization; the performance-critical role ofinteresting orders has been recognized. Some algebraic operators change interestingorders (eg sort and select); while others exploit interesting orders (eg merge join). The twooperations performed by any query optimizer during plan generation are 1) computing theresulting order given an input order and an algebraic operator and 2) determining thecompatibility between a given input order and the required order a given algebraic operatorcan beneficially exploit. Since these two operations are called millions of times during plangeneration; they are highly performance-critical. The third crucial parameter is the spacerequirement for annotating every plan node with its output order. Lately; a powerfulframework for reasoning about orders has been developed; which is based on functional …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,32
CPU and cache efficient management of memory-resident databases,Holger Pirk; Florian Funke; Martin Grund; Thomas Neumann; Ulf Leser; Stefan Manegold; Alfons Kemper; Martin Kersten,Memory-Resident Database Management Systems (MRDBMS) have to be optimized for tworesources: CPU cycles and memory bandwidth. To optimize for bandwidth in mixedOLTP/OLAP scenarios; the hybrid or Partially Decomposed Storage Model (PDSM) hasbeen proposed. However; in current implementations; bandwidth savings achieved bypartial decomposition come at increased CPU costs. To achieve the aspired bandwidthsavings without sacrificing CPU efficiency; we combine partially decomposed storage withJust-in-Time (JiT) compilation of queries; thus eliminating CPU inefficient function calls.Since existing cost based optimization components are not designed for JiT-compiled queryexecution; we also develop a novel approach to cost modeling and subsequent storagelayout optimization. Our evaluation shows that the JiT-based processor maintains the …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,30
Massively parallel NUMA-aware hash joins,Harald Lang; Viktor Leis; Martina-Cezara Albutiu; Thomas Neumann; Alfons Kemper,Abstract Driven by the two main hardware trends increasing main memory and massivelyparallel multi-core processing in the past few years; there has been much research effort inparallelizing well-known join algorithms. However; the non-uniform memory access (NUMA)of these architectures to main memory has only gained limited attention in the design ofthese algorithms. We study recent proposals of main memory hash join implementations andidentify their major performance problems on NUMA architectures. We then develop aNUMA-aware hash join for massively parallel environments; and show how the specificimplementation details affect the performance on a NUMA system. Our experimentalevaluation shows that a carefully engineered hash join implementation outperformsprevious high performance hash joins by a factor of more than two; resulting in an …,*,2015,29
High-speed query processing over high-speed networks,Wolf Rödiger; Tobias Mühlbauer; Alfons Kemper; Thomas Neumann,Abstract Modern database clusters entail two levels of networks: connecting CPUs andNUMA regions inside a single server in the small and multiple servers in the large. The hugeperformance gap between these two types of networks used to slow down distributed queryprocessing to such an extent that a cluster of machines actually performed worse than asingle many-core server. The increased main-memory capacity of the cluster remained thesole benefit of such a scale-out. The economic viability of high-speed interconnects such asInfiniBand has narrowed this performance gap considerably. However; InfiniBand's highernetwork bandwidth alone does not improve query performance as expected when thedistributed query engine is left unchanged. The scalability of distributed query processing isimpaired by TCP overheads; switch contention due to uncoordinated communication; and …,Proceedings of the VLDB Endowment,2015,28
Taking the Edge off Cardinality Estimation Errors using Incremental Execution.,Thomas Neumann; César A Galindo-Legaria,Abstract: Query optimization is an essential ingredient for efficient query processing; assemantically equivalent execution alternatives can have vastly different runtime behavior.The query optimizer is largely driven by cardinality estimates when selecting executionalternatives. Unfortunately these estimates are largely inaccurate; in particular for complexpredicates or skewed data. We present an incremental execution framework to make thequery optimizer more resilient to cardinality estimation errors. The framework computes thesensitivity of execution plans relative to cardinality estimation errors; and if necessaryexecutes parts of the query to remove uncertainty. This technique avoids optimizationdecisions based upon gross misestimation; and makes query optimization (and thusprocessing) much more robust. We demonstrate the effectiveness of these techniques on …,BTW,2013,27
ScyPer: Elastic OLAP throughput on transactional data,Tobias Mühlbauer; Wolf Rödiger; Angelika Reiser; Alfons Kemper; Thomas Neumann,Abstract Ever increasing main memory sizes and the advent of multi-core parallelprocessing have fostered the development of in-core databases. Even the transactional dataof large enterprises can be retained in-memory on a single server. Modern in-coredatabases like our HyPer system achieve best-of-breed OLTP throughput that is sufficient forthe lion's share of applications. Remaining server resources are used for OLAP queryprocessing on the latest transactional data; ie; real-time business analytics. While OLTPperformance of a single server is sufficient; an increasing demand for OLAP throughput canonly be satisfied economically by a scale-out. In this work we present ScyPer; a Scale-out ofour HyPer main memory database system that horizontally scales out on shared-nothinghardware. With ScyPer we aim at (i) sustaining the superior OLTP throughput of a single …,Proceedings of the Second Workshop on Data Analytics in the Cloud,2013,25
Path Query Processing on Very Large RDF Graphs.,Andrey Gubichev; Thomas Neumann,ABSTRACT Finding the shortest path between two nodes in an RDF graph is a fundamentaloperation that allows to discover complex relationships between entities. In this paper weconsider the path queries over graphs from a database perspective. We provide the full-fledge database solution to execute path queries over very large RDF graphs. We presentlow-level techniques to speed-up shortest paths algorithms; and a robust method to estimateselectivities of path queries. We perform extended experiments on several large RDFcollections; including the UniProt collection; demonstrating that our approach outperformsthe path query capabilities of modern systems by a large margin.,WebDB,2011,25
Effective and robust pruning for top-down join enumeration algorithms,Pit Fender; Guido Moerkotte; Thomas Neumann; Viktor Leis,Finding the optimal execution order of join operations is a crucial task of today's cost-basedquery optimizers. There are two approaches to identify the best plan: bottom-up and top-down join enumeration. For both optimization strategies efficient algorithms have beenpublished. However; only the top-down approach allows for branch-and-bound pruning.Two pruning techniques can be found in the literature. We add six new ones. Combined;they improve performance roughly by an average factor of 2-5. Even more important; ourtechniques improve the worst case by two orders of magnitude. Additionally; we introduce anew; very efficient; and easy to implement top-down join enumeration algorithm. Thisalgorithm; together with our improved pruning techniques; yields a performance which is byan average factor of 6-9 higher than the performance of the original top-down …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,24
Compiling Database Queries into Machine Code.,Thomas Neumann; Viktor Leis,Abstract On modern servers the working set of database management systems becomesmore and more main memory resident. Slow disk accesses are largely avoided; and thus thein-memory processing speed of databases becomes an important factor. One very attractiveapproach for fast query processing is justin-time compilation of incoming queries. Byproducing machine code at runtime we avoid the overhead of traditional interpretationsystems; and by carefully organizing the code around register usage we minimize memorytraffic and get excellent performance. In this paper we show how queries can be brought intoa form suitable for efficient translation; and how the underlying code generation can beorchestrated. By carefully abstracting away the necessary plumbing infrastructure we canbuild a query compiler that is both maintainable and efficient. The effectiveness of the …,IEEE Data Eng. Bull.,2014,23
Distributed top-k aggregation queries at large,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments. Theoptimizations can be applied to all algorithms that fall into the frameworks of the prior TPUTand KLEE methods. The optimizations address three degrees of freedom: 1) hierarchicallygrouping input lists into top-k operator trees and optimizing the tree structure; 2) computingdata-adaptive scan depths for different input sources; and 3) data-adaptive sampling of asmall subset of input sources in scenarios with hundreds or thousands of query-relevantnetwork nodes. All optimizations are based on a statistical cost model that utilizes local …,Distributed and Parallel Databases,2009,23
FluxCapacitor: efficient time-travel text search,Klaus Berberich; Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract An increasing number of temporally versioned text collections is available todaywith Web archives being a prime example. Search on such collections; however; is often notsatisfactory and ignores their temporal dimension completely. Time-travel text search solvesthis problem by evaluating a keyword query on the state of the text collection as of a user-specified time point. This work demonstrates our approach to efficient time-travel text searchand its implementation in the FLUXCAPACITOR prototype.,Proceedings of the 33rd international conference on Very large data bases,2007,23
Benchmarking Hybrid OLTP&OLAP Database Systems.,Florian Funke; Alfons Kemper; Thomas Neumann,Abstract: Recently; the case has been made for operational or real-time BusinessIntelligence (BI). As the traditional separation into OLTP database and OLAP datawarehouse obviously incurs severe latency disadvantages for operational BI; hybridOLTP&OLAP database systems are being developed. The advent of the first generation ofsuch hybrid OLTP&OLAP database systems requires means to characterize theirperformance. While there are standardized and widely used benchmarks addressing eitherOLTP or OLAP workloads; the lack of a hybrid benchmark led us to the definition of a newmixed workload benchmark; called TPC-CH. This benchmark bridges the gap between theexisting single-workload suits: TPC-C for OLTP and TPC-H for OLAP. The newly proposedTPC-CH benchmark executes a mixed workload: A transactional workload based on the …,BTW,2011,22
Data blocks: Hybrid OLTP and OLAP on compressed storage using both vectorization and compilation,Harald Lang; Tobias Mühlbauer; Florian Funke; Peter A Boncz; Thomas Neumann; Alfons Kemper,Abstract This work aims at reducing the main-memory footprint in high performance hybridOLTP & OLAP databases; while retaining high query performance and transactionalthroughput. For this purpose; an innovative compressed columnar storage format for colddata; called Data Blocks is introduced. Data Blocks further incorporate a new light-weightindex structure called Positional SMA that narrows scan ranges within Data Blocks even ifthe entire block cannot be ruled out. To achieve highest OLTP performance; thecompression schemes of Data Blocks are very light-weight; such that OLTP transactions canstill quickly access individual tuples. This sets our storage scheme apart from those used inspecialized analytical databases where data must usually be bit-unpacked. Up to now; high-performance analytical systems use either vectorized query execution or just-in-time (JIT) …,Proceedings of the 2016 International Conference on Management of Data,2016,21
Transaction Processing in the Hybrid OLTP&OLAP Main-Memory Database System HyPer,Alfons Kemper; Thomas Neumann; Jan Finis; Florian Funke; Viktor Leis; Henrik Mühe; Tobias Mühlbauer; Wolf Rödiger,Abstract Two emerging hardware trends have re-initiated the development of in-coredatabase systems: ever increasing main-memory capacities and vast multi-core parallelprocessing power. Main-memory capacities of several TB allow to retain all transactionaldata of even the largest applications in-memory on one (or a few) servers. The vastcomputational power in combination with low data management overhead yieldsunprecedented transaction performance which allows to push transaction processing (awayfrom application servers) into the database server and still “leaves room” for additional queryprocessing directly on the transactional data. Thereby; the often postulated goal of real-timebusiness intelligence; where decision makers have access to the latest version of thetransactional state; becomes feasible. In this paper we will survey the HyPerScript …,*,2013,20
How to efficiently snapshot transactional data: Hardware or software controlled?,Henrik Mühe; Alfons Kemper; Thomas Neumann,Abstract The quest for real-time business intelligence requires executing mixed transactionand query processing workloads on the same current database state. However; asHarizopoulos et al.[6] showed for transactional processing; co-execution using classicalconcurrency control techniques will not yield the necessary performance--even in re-emerging main memory database systems. Therefore; we designed an in-memory databasesystem that separates transaction processing from OLAP query processing via periodicallyrefreshed snapshots. Thus; OLAP queries can be executed without any synchronization andOLTP transaction processing follows the lock-free; mostly serial processing paradigm of H-Store [8]. In this paper; we analyze different snapshot mechanisms: Hardware-supportedPage Shadowing; which lazily copies memory pages when changed by transactions …,Proceedings of the Seventh International Workshop on Data Management on New Hardware,2011,20
HyPer: Adapting Columnar Main-Memory Data Management for Transactional AND Query Processing.,Alfons Kemper; Thomas Neumann; Florian Funke; Viktor Leis; Henrik Mühe,Abstract Traditionally; business applications have separated their data into an OLTP datastore for high throughput transaction processing and a data warehouse for complex queryprocessing. This separation bears severe maintenance and data consistencydisadvantages. Two emerging hardware trends allow the consolidation of the two disparateworkloads onto the same database state on one system: the increasing main memorycapacities of several terabytes per server and multi-threaded processing based on multicoreparallelism. The prevalent data representation of hybrid OLTP&OLAP main memorydatabase systems is columnar in order to achieve best possible query executionperformance for OLAP applications. In order to shield the OLTP transaction processing fromlong-running queries without costly locking/latching; all queries are executed on an …,IEEE Data Eng. Bull.,2012,19
Executing Long-Running Transactions in Synchronization-Free Main Memory Database Systems.,Henrik Mühe; Alfons Kemper; Thomas Neumann,*,CIDR,2013,18
Accelerating queries with group-by and join by groupjoin,Guido Moerkotte; Thomas Neumann,ABSTRACT Most aggregation queries contain both group-by and join operators; and spend asignificant amount of time evalu- ating these two expensive operators. Merging them into oneoperator (the groupjoin) significantly speeds up query execution. We introduce two main equivalencesto allow for the merg- ing and prove their correctness. Furthermore; we show ex- perimentallythat these equivalences can significantly speed up TPC-H … Most aggregation queries containjoins. For these queries it is almost inevitable to feature a join followed by a grouping with anaggregation. In a hash-based implementation; this results in a cascade of two hash tables:First; a hash table is built and maintained to compute the join result; and then; a second hashtable is filled to compute the result of the aggregation. It is easy to see that in some cases (ie;when the group-by attributes and the join attributes are the same) it is sufficient to …,Proceedings of the VLDB Endowment,2011,18
One size fits all; again! the architecture of the hybrid oltp&olap database management system hyper,Alfons Kemper; Thomas Neumann,Abstract Real time business intelligence demands to execute OLAP queries on a current; up-to-date state of the transactional OLTP data. The currently exercised separation oftransaction processing on the OLTP database and BI query processing on the datawarehouse that is only periodically refreshed violates this goal. We propose to enhance thetransactional database with highly effective query processing capabilities. We contrastdifferent architectures proposed for achieving the real-time BI goal: versioning of the dataand thereby separating the query from the transactions workload; continuous DW refreshing;heterogeneous workload management; update staging by periodically merging the updatedelta into the queryable main database; update and query batching; and our newlydeveloped virtual memory snapshot mechanism based on hardware-supported …,International Workshop on Business Intelligence for the Real-Time Enterprise,2010,18
Hyper: Hybrid oltp&olap high performance database system,Alfons Kemper; Thomas Neumann,Abstract: The two areas of online transaction processing (OLTP) and online analyticalprocessing (OLAP) present different challenges for database architectures. Currently;customers with high rates of mission-critical transactions have split their data into twoseparate systems; one database for OLTP and one so-called data warehouse for OLAP.While allowing for decent transaction rates; this separation has many disadvantagesincluding data freshness issues due to the delay caused by only periodically init...»,*,2010,18
Efficient generation and execution of DAG-structured query graphs,Thomas Neumann,Abstract: Traditional database management systems use tree-structured query evaluationplans. While easy to implement; a tree-structured query evaluation plan is not expressiveenough for some optimizations like factoring common algebraic subexpressions or magicsets. These require directed acyclic graphs (DAGs); ie shared subplans. This work coversthe different aspects of DAG-structured query graphs. First; it introduces a novel frameworkto reason about sharing of subplans and thus DAG-structured query evaluation plans.Second; it describes the first plan generator capable of generating optimal DAG-structuredquery evaluation plans. Third; an efficient framework for reasoning about orderings andgroupings used by the plan generator is presented. And fourth; a runtime system capable ofexecuting DAG-structured query evaluation plans with minimal overhead is discussed …,*,2005,18
The linked data benchmark council project,Peter Boncz; Irini Fundulaki; Andrey Gubichev; Josep Larriba-Pey; Thomas Neumann,Abstract Despite the fast growth and increasing popularity; the broad field of RDF and Graphdatabase systems lacks an independent authority for developing benchmarks; and forneutrally assessing benchmark results through industry-strength auditing which would allowto quantify and compare the performance of existing and emerging systems. Inspired by theimpact of the Transaction Processing Performance Council (TPC) Benchmarks on relationaldatabases; the LDBC consortium formed by University and Industry researchers andpractitioners has recently launched a European Commision sponsored project that will offerthe first comprehensive set of open and vendor-independent benchmarks for RDF andGraph technologies. The consortium will incorporate the Linked Data Benchmark Council(LDBC) which will survive the project and will supervise the process of obtaining and …,Datenbank-Spektrum,2013,17
Scaling up mixed workloads: a battle of data freshness; flexibility; and scheduling,Iraklis Psaroudakis; Florian Wolf; Norman May; Thomas Neumann; Alexander Böhm; Anastasia Ailamaki; Kai-Uwe Sattler,Abstract The common “one size does not fit all” paradigm isolates transactional andanalytical workloads into separate; specialized database systems. Operational data isperiodically replicated to a data warehouse for analytics. Competitiveness of enterprisestoday; however; depends on real-time reporting on operational data; necessitating anintegration of transactional and analytical processing in a single database system. Themixed workload should be able to query and modify common data in a shared schema. Thedatabase needs to provide performance guarantees for transactional workloads; and; at thesame time; efficiently evaluate complex analytical queries. In this paper; we share ouranalysis of the performance of two main-memory databases that support mixed workloads;SAP HANA and HyPer; while evaluating the mixed workload CH-benCHmark. By …,Technology Conference on Performance Evaluation and Benchmarking,2014,16
Heterogeneity-Conscious Parallel Query Execution: Getting a better mileage while driving faster!,Tobias Mühlbauer; Wolf Rödiger; Robert Seilbeck; Alfons Kemper; Thomas Neumann,Abstract Physical and thermal restrictions hinder commensurate performance gains from theever increasing transistor density. While multi-core scaling helped alleviate dimmed or darksilicon for some time; future processors will need to become more heterogeneous. To thisend; single instruction set architecture (ISA) heterogeneous processors are a particularlyinteresting solution that combines multiple cores with the same ISA but asymmetricperformance and power characteristics. These processors; however; are no free lunch fordatabase systems. Mapping jobs to the core that fits best is notoriously hard for the operatingsystem or a compiler. To achieve optimal performance and energy efficiency; heterogeneityneeds to be exposed to the database system. In this paper; we provide a thorough study ofparallelized core database operators and TPC-H query processing on a heterogeneous …,Proceedings of the Tenth International Workshop on Data Management on New Hardware,2014,15
Efficient processing of window functions in analytical SQL queries,Viktor Leis; Kan Kundhikanjana; Alfons Kemper; Thomas Neumann,Abstract Window functions; also known as analytic OLAP functions; have been part of theSQL standard for more than a decade and are now a widely-used feature. Window functionsallow to elegantly express many useful query types including time series analysis; ranking;percentiles; moving averages; and cumulative sums. Formulating such queries in plain SQL-92 is usually both cumbersome and inefficient. Despite being supported by all majordatabase systems; there have been few publications that describe how to implement anefficient relational window operator. This work aims at filling this gap by presenting anefficient and general algorithm for the window operator. Our algorithm is optimized for high-performance main-memory database systems and has excellent performance on modernmulti-core CPUs. We show how to fully parallelize all phases of the operator in order to …,Proceedings of the VLDB Endowment,2015,14
ScyPer: A Hybrid OLTP&OLAP Distributed Main Memory Database System for Scalable Real-Time Analytics.,Tobias Mühlbauer; Wolf Rödiger; Angelika Reiser; Alfons Kemper; Thomas Neumann,Abstract: ScyPer is an abbreviation for Scaled-out HyPer; a version of the HyPer mainmemory hybrid OLTP&OLAP database system that horizontally scales out on sharednothingcommodity hardware. Our demo shows that ScyPer a) achieves a near-linear scale-out ofOLAP query throughput with the number of active nodes; b) sustains a constant OLTPthroughput; c) is resilient to node failures; and d) offers real-time analytical capabilitiesthrough market-leading query response times and periodically forked TX-consistent virtualmemory snapshots with sub-second lifetime durations.,BTW,2013,14
ANGIE: Active knowledge for interactive exploration,Nicoleta Preda; Fabian M Suchanek; Gjergji Kasneci; Thomas Neumann; Maya Ramanath; Gerhard Weikum,Abstract We present ANGIE; a system that can answer user queries by combiningknowledge from a local database with knowledge retrieved from Web services. If a userposes a query that cannot be answered by the local database alone; ANGIE calls theappropriate Web services to retrieve the missing information. This information is integratedseamlessly and transparently into the local database; so that the user can query and browsethe knowledge base while appropriate Web services are called automatically in thebackground.,Proceedings of the VLDB Endowment,2009,14
Flow-join: Adaptive skew handling for distributed joins over high-speed networks,Wolf Rödiger; Sam Idicula; Alfons Kemper; Thomas Neumann,Modern InfiniBand interconnects offer link speeds of several gigabytes per second and aremote direct memory access (RDMA) paradigm for zero-copy network communication. Bothare crucial for parallel database systems to achieve scalable distributed query processingwhere adding a server to the cluster increases performance. However; the scalability ofdistributed joins is threatened by unexpected data characteristics: Skew can cause a severeload imbalance such that a single server has to process a much larger part of the input thanits fair share and by this slows down the entire distributed query. We introduce Flow-Join; anovel distributed join algorithm that handles attribute value skew with minimal overhead.Flow-Join detects heavy hitters at runtime using small approximate histograms and adaptsthe redistribution scheme to resolve load imbalances before they impact the join …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,13
An evaluation of strict timestamp ordering concurrency control for main-memory database systems,Stephan Wolf; Henrik Mühe; Alfons Kemper; Thomas Neumann,Abstract With the fundamental change of hardware technology; main-memory databasesystems have emerged as the next generation of DBMS. Thus; new methods to executetransactions in a serial; lock-free mode have been investigated and successfully employed;for instance in H-Store or HyPer. Although these techniques allow for unprecedentedly highthroughput for suitable workloads; their throughput quickly diminishes once unsuitabletransactions; for instance those crossing partition borders; are encountered. Still; littleresearch concentrates on the overdue re-evaluation of traditional techniques; that do not relyon partitioning. This paper studies strict timestamp ordering (STO); a “good old” technique; inthe context of modern main-memory database systems built on commodity hardware withhigh memory capacities. We show that its traditional main drawback–slowing down reads …,*,2015,12
Metrics for measuring the performance of the mixed workload CH-benCHmark,Florian Funke; Alfons Kemper; Stefan Krompass; Harumi Kuno; Raghunath Nambiar; Thomas Neumann; Anisoara Nica; Meikel Poess; Michael Seibold,Abstract Advances in hardware architecture have begun to enable database vendors toprocess analytical queries directly on operational database systems without impeding theperformance of mission-critical transaction processing too much. In order to evaluate suchsystems; we recently devised the mixed workload CH-benCHmark; which combinestransactional load based on TPC-C order processing with decision support load based onTPC-H-like query suite run in parallel on the same tables in a single database system. Justas the data volume of actual enterprises tends to increase over time; an inherentcharacteristic of this mixed workload benchmark is that data volume increases duringbenchmark runs; which in turn may increase response times of analytic queries. For purelytransactional loads; response times typically do not depend that much on data volume; as …,Technology Conference on Performance Evaluation and Benchmarking,2011,12
Making SENSE: socially enhanced search and exploration,Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane Xavier Parreira; Ralf Schenkel; Gerhard Weikum,Abstract Online communities like Flickr; del. icio. us and YouTube have establishedthemselves as very popular and powerful services for publishing and searching contents;but also for identifying other users who share similar interests. In these communities; dataare usually annotated with carefully selected and often semantically meaningful tags;collaboratively chosen by the user who uploaded an item and other users who came acrossthe item. Items like urls or videos are typically retrieved by issueing queries that consist of aset of tags; returning items that have been frequently annotated with these tags. However;users often prefer a more personalized way of searching over such a'global'search;exploiting preferences of and connections between users. The SENSE system presented inthis demo supports hybrid personalization along two dimensions: in the social dimension …,Proceedings of the VLDB Endowment,2008,12
A central natural products pool-New approach in drug discovery strategies,C Koch,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Drug discovery from nature,1999,12
The ART of practical synchronization,Viktor Leis; Florian Scheibner; Alfons Kemper; Thomas Neumann,Abstract The performance of transactional database systems is critically dependent on theefficient synchronization of in-memory data structures. The traditional approach; fine-grainedlocking; does not scale on modern hardware. Lock-free data structures; in contrast; scalevery well but are extremely difficult to implement and often require additional indirections. Inthis work; we argue for a middle ground; ie; synchronization protocols that use locking; butonly sparingly. We synchronize the Adaptive Radix Tree (ART) using two such protocols;Optimistic Lock Coupling and Read-Optimized Write EXclusion (ROWEX). Both perform andscale very well while being much easier to implement than lock-free techniques.,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,11
On the optimal ordering of maps and selections under factorization,Thomas Neumann; Sven Helmer; Guido Moerkotte,The query optimizer of a database system is confronted with two aspects when handlinguser-defined functions (UDFs) in query predicates: the vast differences in evaluation costsbetween UDFs (and other functions) and multiple calls of the same (expensive) UDF Theformer is dealt with by ordering the evaluation of the predicates optimally; the latter byidentifying common subexpressions and thereby avoiding costly recomputation. Currentapproaches order n predicates optimally (neglecting factorization) in O (nlogn). Their resultmay deviate significantly from the optimal solution under factorization. We formalize theproblem of finding optimal orderings under factorization and prove that it is NP-hard.Furthermore; we show how to improve on the run time of the brute-force algorithm (whichcomputes all possible orderings) by presenting different enhanced algorithms. Although …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,11
A robust scheme for multilevel extendible hashing,Sven Helmer; Thomas Neumann; Guido Moerkotte,Abstract Dynamic hashing; while surpassing other access methods for uniformly distributeddata; usually performs badly for non-uniformly distributed data. We propose a robust schemefor multi-level extendible hashing; allowing efficient processing of skewed data as well asuniformly distributed data. In order to test our access method; we implemented it andcompared it to several existing hashing schemes. The results of the experimental evaluationdemonstrate the superiority of our approach in both index size and performance.,International Symposium on Computer and Information Sciences,2003,11
-Estimating the Output Cardinality of Partial Preaggregation with a Measure of Clusteredness,Sven Helmer; Thomas Neumann; Guido Moerkotte,This chapter introduces a new parameter; the clusteredness of data; and shows how it canbe used for estimating the output cardinality of a partial preaggregation operator. Estimatingthe output cardinality of partial preaggregation (PPA) accurately is a necessary prerequisitefor a query optimizer to reach a decision on applying it. Several factors influence the qualityof an early aggregation step: the number of groups; the number of tuples in the input; thedistributions of the group sizes; the size of the buffer; the buffer replacement strategy used bythe algorithm and; the clusteredness of the data. Previous analyses of PPA did not considerthe clusteredness of the data; but assumed randomized data. The quality of theapproximations was demonstrated by thorough experiments. The experimental results arevery promising; due to the high accuracy of the cardinality estimation based on the …,*,2003,11
Generating optimal DAG-structured query evaluation plans,Thomas Neumann; Guido Moerkotte,Abstract In many database queries relations are access multiple times during queryprocessing. In these cases query processing can be accelerated by sharing scan operatorsand possibly other operators based upon the common relations. The standard approach toachieve sharing works as follows. In a first phase; a non-shared tree-shaped plan isgenerated via a traditional plan generator. In a second phase; common instances of a scanare detected and shared. After that; other possible operators are shared. The result is anoperator DAG (directed acyclic graph). The limitation of this approach is obvious. As sharinginfluences plan costs; a separation of the optimization into two phases comprises the dangerof missing the optimal plan; since the first optimization phase does not know about sharing.We remedy this situation by (1) introducing a general framework for reasoning about …,Computer Science-Research and Development,2009,10
HyPer-sonic Combined Transaction AND Query Processing,Florian Funke; Alfons Kemper; Thomas Neumann,ABSTRACT In this demo we will prove that it is–against common belief–indeed possible tobuild a main-memory database system that achieves world-record transaction processingthroughput and best-of-breed OLAP query response times in one system in parallel on thesame database state. The two workloads of online transaction processing (OLTP) and onlineanalytical processing (OLAP) present different challenges for database architectures.Currently; users with high rates of mission-critical transactions have split their data into twoseparate systems; one database for OLTP and one so-called data warehouse for OLAP.While allowing for decent transaction rates; this separation has many disadvantagesincluding data freshness issues due to the delay caused by only periodically initiating theExtract Transform Load-data staging and excessive resource consumption due to …,Proceedings of the VLDB Endowment,2011,9
Unnesting arbitrary queries,Thomas Neumann; Alfons Kemper,SQL-99 allows for nested subqueries at nearly all places within a query. From a user's pointof view; nested queries can greatly simplify the formulation of complex queries. However;nested queries that are correlated with the outer queries frequently lead to dependent joinswith nested loops evaluations and thus poor performance. Existing systems therefore use anumber of heuristics to unnest these queries; ie; de-correlate them. These unnestingtechniques can greatly speed up query processing; but are usually limited to certain classesof queries. To the best of our knowledge no existing system can de-correlate queries in thegeneral case. We present a generic approach for unnesting arbitrary queries. As a result; thede-correlated queries allow for much simpler and much more efficient query evaluation.,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,8
DeltaNI: An efficient labeling scheme for versioned hierarchical data,Jan Finis; Robert Brunel; Alfons Kemper; Thomas Neumann; Franz Färber; Norman May,Abstract Main-memory database systems are emerging as the new backbone of businessapplications. Besides flat relational data representations also hierarchical ones are essentialfor these modern applications; therefore we devise a new indexing and versioning approachfor hierarchies that is deeply integrated into the relational kernel. We propose the DeltaNIindex as a versioned pendant of the nested intervals (NI) labeling scheme. The index isspace-and time-efficient and yields a gapless; fixed-size integer NI labeling for each versionwhile also supporting branching histories. In contrast to a naive NI labeling; it facilitates evencomplex updates of the tree structure. As many query processing techniques that work ontop of the NI labeling have already been proposed; our index can be used as a buildingblock for processing various kinds of queries. We evaluate the performance of the index …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,8
Fast approximation of steiner trees in large graphs,Andrey Gubichev; Thomas Neumann,Abstract Finding the minimum connected subtree of a graph that contains a given set ofnodes (ie; the Steiner tree problem) is a fundamental operation in keyword search in graphs;yet it is known to be NP-hard. Existing approximation techniques either make use of theheavy indexing of the graph; or entirely rely on online heuristics.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,8
Der Naturstoff‐Pool‐neuer Ansatz für die Wirkstoffsuche,Corinna Koch; Thomas Neumann,Abstract Mit dem Aufbau einer zentralen Sammlung von Naturstoffen; dem Naturstoff-Pool;hat das Hans-Knöll-Institut (HKI) in Jena zusammen mit Partnern aus Wissenschaft undWirtschaft einen richtungsweisenden Schritt für die Wirkstoffsuche in Deutschland getan. DiePrüfung von Naturstoffen in aktuellen molekularen Testsystemen mit geringemSubstanzbedarf soll helfen; neue Leitstrukturen für eine Wirkstoffentwicklung zu entdecken.,Nachrichten aus der Chemie,1997,8
On-the-fly token similarity joins in relational databases,Nikolaus Augsten; Armando Miraglia; Thomas Neumann; Alfons Kemper,Abstract Token similarity joins represent data items as sets of tokens; for example; stringsare represented as sets of q-grams (substrings of length q). Two items are consideredsimilar and match if their token sets have a large overlap. Previous work on similarity joins indatabases mainly focuses on expressing the overlap computation with relational operators.The tokens are assumed to preexist in the database; and the token generation cannot beexpressed as part of the query. Our goal is to efficiently compute token similarity joins on-the-fly; ie; without any precomputed tokens or indexes. We define tokenize; a new relationaloperator that generates tokens and allows the similarity join to be fully integrated intorelational databases. This allows us to (1) optimize the token generation as part of the queryplan;(2) provide the query optimizer with cardinality estimates for tokens;(3) choose …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,7
One DBMS for all: the Brawny Few and the Wimpy Crowd,Tobias Mühlbauer; Wolf Rödiger; Robert Seilbeck; Angelika Reiser; Alfons Kemper; Thomas Neumann,Abstract Shipments of smartphones and tablets with wimpy CPUs are outpacing brawny PCand server shipments by an ever-increasing margin. While high performance databasesystems have traditionally been optimized for brawny systems; wimpy systems havereceived only little attention; leading to poor performance and energy inefficiency on suchsystems. This demonstration presents HyPer; a high-performance hybrid OLTP&OLAP mainmemory database system that we optimized for both; brawny and wimpy systems. Theefficient compilation of transactions and queries into efficient machine code allows for highperformance; independent of the target platform. HyPer has a memory footprint of just a fewmegabytes; even though it supports the SQL-92 standard; a PL/SQL-like scripting language;and ACID-compliant transactions. It is the goal of this demonstration to showcase the …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,7
Surface plasmon fluorescence techniques for bioaffinity studies,Wolfgang Knoll; Amal Kasry; Jing Liu; Thomas Neumann; Lifang Niu; Hyeyoung Park; Harald Paulsen; Rudolf Robelek; Danfeng Yao; Fang Yu,Among the various sensing principles proposed for bioaffinity studies; optical evanescentwave techniques have gained the lead in popularity. Next to evanescent ellipsometry [1] andthe various optical waveguide platforms [2; 3]; surface plasmon resonance (SPR)spectroscopy [4–6]; in particular; has found widespread applications and has demonstratedits potential for the sensitive detection of bioanalytes in numerous examples [7–9]. Since itsintroduction as a method for bioaffinity studies in 1983 by Liedberg et al.[10] and since thepresentation of the first commercial instrument by Biacore in 1990 [11]; the number of paperspublished has grown exponentially to currently more than 1500 contributions each year(Figure 9.1). This success story is largely based on the fact that SPR represents a label-freedetection principle–the mere presence of the bound analyte slightly changes the optical …,*,2008,7
SQL-and Operator-centric Data Analytics in Relational Main-Memory Databases.,Linnea Passing; Manuel Then; Nina Hubig; Harald Lang; Michael Schreier; Stephan Günnemann; Alfons Kemper; Thomas Neumann,ABSTRACT Data volume and complexity continue to increase; as does the need for insightinto data. Today; data management and data analytics are most often conducted in separatesystems: database systems and dedicated analytics systems. This separation leads to time-and resource-consuming data transfer; stale data; and complex IT architectures. In thispaper we show that relational main-memory database systems are capable of executinganalytical algorithms in a fully transactional environment while still exceeding performanceof state-of-the-art analytical systems rendering the division of data management and dataanalytics unnecessary. We classify and assess multiple ways of integrating data analytics indatabase systems. Based on this assessment; we extend SQL with a non-appendingiteration construct that provides an important building block for analytical algorithms while …,EDBT,2017,6
Scaling HTM-supported database transactions to many cores,Viktor Leis; Alfons Kemper; Thomas Neumann,So far; transactional memory-although a promising technique-suffered from the absence ofan efficient hardware implementation. Intel's Haswell microarchitecture introduced hardwaretransactional memory (HTM) in mainstream CPUs. HTM allows for efficient concurrent;atomic operations; which is also highly desirable in the context of databases. On the otherhand; HTM has several limitations that; in general; prevent a one-to-one mapping ofdatabase transactions to HTM transactions. In this work; we devise several building blocksthat can be used to exploit HTM in main-memory databases. We show that HTM allows forachieving nearly lock-free processing of database transactions by carefully controlling thedata layout and the access patterns. The HTM component is used for detecting the(infrequent) conflicts; which allows for an optimistic; and thus very low-overhead …,IEEE Transactions on Knowledge and Data Engineering,2016,6
Supporting hierarchical data in SAP HANA,Robert Brunel; Jan Finis; Gerald Franz; Norman May; Alfons Kemper; Thomas Neumann; Franz Faerber,Managing hierarchies is an ever-recurring challenge for relational database systems.Through investigations of customer scenarios at SAP we found that today's RDBMSs stillleave a lot to be desired in order to meet the requirements of typical applications. Ourresearch puts a new twist on handling hierarchies in SQL-based systems. We present anapproach for modeling hierarchical data natively; and we extend the SQL language withexpressive constructs for creating; manipulating; and querying a hierarchy. The constructscan be evaluated efficiently by leveraging existing indexing and query processingtechniques. We demonstrate the feasibility of our concepts with initial measurements on aHANA-based prototype.,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,6
Engineering high-performance database engines,Thomas Neumann,Abstract Developing a database engine is both challenging and rewarding. Databaseengines are very complex software artifacts that have to scale to large data sizes and largehardware configurations; and developing such systems usually means choosing betweendifferent trade-offs at various points of development. This papers gives a survey over twodifferent database engines; the disk-based SPARQL-processing engine RDF-3X; and therelational main-memory engine HyPer. It discusses the design choices that were madeduring development; and highlights optimization techniques that are important for bothsystems.,Proceedings of the VLDB Endowment,2014,6
Optimizing distributed top-k queries,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract Top-k query processing is a fundamental building block for efficient ranking in alarge number of applications. Efficiency is a central issue; especially for distributed settings;when the data is spread across different nodes in a network. This paper introduces noveloptimization methods for top-k aggregation queries in such distributed environments thatcan be applied to all algorithms that fall into the frameworks of the prior TPUT and KLEEmethods. The optimizations address 1) hierarchically grouping input lists into top-k operatortrees and optimizing the tree structure; and 2) computing data-adaptive scan depths fordifferent input sources. The paper presents comprehensive experiments with two differentreal-life datasets; using the ns-2 network simulator for a packet-level simulation of a largeInternet-style network.,International Conference on Web Information Systems Engineering,2008,6
Indexing set-valued attributes with a multi-level extendible hashing scheme,Sven Helmer; Robin Aly; Thomas Neumann; Guido Moerkotte,Abstract We present an access method for set-valued attributes that is based on a multi-levelextendible hashing scheme. This scheme avoids exponential directory growth for skeweddata and thus generates a much smaller number of subqueries for query sets (so far fast-growing directories have prohibited hash-based index structures for set-valued retrieval).We demonstrate the advantages of our scheme over regular extendible hashing bothanalytically and experimentally. We also implemented a prototype and briefly summarize theresults of our experimental evaluation.,International Conference on Database and Expert Systems Applications,2007,6
Algebraic query optimization for distributed top-k queries,Thomas Neumann; Sebastian Michel,Abstract Distributed top-k query processing is increasingly becoming an essentialfunctionality in a large number of emerging application classes. This paper addresses theefficient algebraic optimization of top-k queries in wide-area distributed data repositorieswhere the index lists for the attribute values (or text terms) of a query are distributed across anumber of data peers and the computational costs include network latency; bandwidthconsumption; and local peer work. We use a dynamic programming approach to find theoptimal execution plan using compact data synopses for selectivity estimation that is thebasis for our cost model. The optimized query is executed in a hierarchical way involving asmall and fixed number of communication phases. We have performed experiments on realweb data that show the benefits of distributed top-k query optimization both in network …,Informatik-Forschung und Entwicklung,2007,6
SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval; Amsterdam; The Netherla...,W Kraaij; AP de Vries; CLA Clarke; N Fuhr; N Kando,*,Proceedings of the 6th annual international ACM SIGIR conference on Research and development in information retrieval; SIGIR,2007,6
Der Naturstoff-Pool. Ein neuartiges Konzept zur Wirkstoffsuche bringt Wirtschaft und Wissenschaft zusammen,C Koch; T Neumann; R Thiericke; S Grabley,*,Biospektrum,1997,6
Elasticity in cloud databases and their query processing,Goetz Graefe; Anisoara Nica; Knut Stolze; Thomas Neumann; Todd Eavis; Ilia Petrov; Elaheh Pourabbas; David Fekete,Abstract A central promise of cloud services is elastic; on-demand provisioning. Theprovisioning of data on temporarily available nodes is what makes elastic database servicesa hard problem. The essential task that enables elastic data services is bringing a node andits data up-to-date. Strategies for high availability do not satisfy the need in this contextbecause they bring nodes online and up-to-date by repeating history; eg; by log shipping.Nodes must become up-to-date and useful for query processing incrementally by key range.What is wanted is a technique such that in a newly added node; during each short period oftime; an additional small key range becomes up-to-date; until eventually the entire datasetbecomes up-to-date and useful for query processing; with overall update performancecomparable to a traditional high-availability strategy that carries the entire dataset forward …,International Journal of Data Warehousing and Mining (IJDWM),2013,5
The mainframe strikes back: elastic multi-tenancy using main memory database systems on a many-core server,Henrik Mühe; Alfons Kemper; Thomas Neumann,Abstract Contrary to recent trends in database systems research focussing on scaling outworkloads on a cluster of commodity computers; this demo will break grounds for scale-up.We show that an elastic multi-tenancy solution can be achieved by combining a many-coreserver with a low footprint main memory database system. Total transactional throughput forTPC-C like order-entry transactions reaches up to 2 million transactions per second on a 32core server while the number of tenants sharing a single server can be varied from a few tohundreds of separate tenants without diminishing total throughput. Contrary to commonbelief; a scale-up solution provides high flexibility for tenants with growing throughput needsand allows for simple sharing of common resources between different tenants whileminimizing hardware and computing overhead. We show that our approach can handle …,Proceedings of the 15th International Conference on Extending Database Technology,2012,5
Query optimization (in relational databases),Thomas Neumann,In general; the term Quadtree refers to a class of representations of geometric entities (suchas points; line segments; polygons; regions) in a space of two (or more) dimensions; thatrecursively decompose the space containing these entities into blocks until the data in eachblock satisfy some condition (with respect; for example; to the block size; the number of blockentities; the characteristics of the block entities; etc.). In a more restricted sense; the termQuadtree (Octree) refers to a tree data-structure in which each internal node has four (eight)children and is used for the representation of geometric entities in a two (three) dimensionalspace. The root of the tree represents the whole space/region. Each child of a noderepresents a subregion of the subregion of its parent. The subregions of the siblingsconstitute a partition of the parent's regions. Several variations of quadtrees are possible …,*,2009,5
Cardinality Estimation Done Right: Index-Based Join Sampling.,Viktor Leis; Bernharde Radke; Andrey Gubichev; Alfons Kemper; Thomas Neumann,ABSTRACT After four decades of research; today's database systems still suffer from poorquery execution plans. Bad plans are usually caused by poor cardinality estimates; whichhave been called the “Achilles Heel” of modern query optimizers. In this work we proposeindexbased join sampling; a novel cardinality estimation technique for main-memorydatabases that relies on sampling and existing index structures to obtain accurate estimates.Results on a real-world data set show that this approach significantly improves estimation aswell as overall plan quality. The additional sampling effort is quite low and can be configuredto match the desired application profile. The technique can be easily integrated into mostsystems.,CIDR,2017,4
High-performance geospatial analytics in hyperspace,Varun Pandey; Andreas Kipf; Dimitri Vorona; Tobias Mühlbauer; Thomas Neumann; Alfons Kemper,Abstract In the past few years; massive amounts of location-based data has been captured.Numerous datasets containing user location information are readily available to the public.Analyzing such datasets can lead to fascinating insights into the mobility patterns andbehaviors of users. Moreover; in recent times a number of geospatial data-driven companieslike Uber; Lyft; and Foursquare have emerged. Real-time analysis of geospatial data isessential and enables an emerging class of applications. Database support for geospatialoperations is turning into a necessity instead of a distinct feature provided by only a fewdatabases. Even though a lot of database systems provide geospatial support nowadays;queries often do not consider the most current database state. Geospatial queries areinherently slow given the fact that some of these queries require a couple of geometric …,Proceedings of the 2016 International Conference on Management of Data,2016,4
Indexing highly dynamic hierarchical data,Jan Finis; Robert Brunel; Alfons Kemper; Thomas Neumann; Norman May; Franz Faerber,Abstract Maintaining and querying hierarchical data in a relational database system is animportant task in many business applications. This task is especially challenging whenconsidering dynamic use cases with a high rate of complex; possibly skewed structuralupdates. Labeling schemes are widely considered the indexing technique of choice forhierarchical data; and many different schemes have been proposed. However; they cannothandle dynamic use cases well due to various problems which we investigate in this paper.We therefore propose our dynamic Order Indexes; which offer competitive queryperformance; unprecedented update efficiency; and robustness for highly dynamicworkloads.,Proceedings of the VLDB Endowment,2015,4
Get tracked: A triple store for rfid traceability data,Veneta Dobreva; Martina-Cezara Albutiu; Robert Brunel; Thomas Neumann; Alfons Kemper,Abstract Companies are increasingly employing Radio Frequency Identification (RFID)technologies to track their goods in business processes. The rapidly produced; largeamounts of events pose new challenges to modern databases: an efficient data stagingprocess and efficient querying mechanisms for traceability data. In this paper; inspired byrecent work on RDF triple stores; we present a scalable dedicated system for efficientstorage and fast querying of RFID data. We address the challenges as follows:(1) weincorporate elaborated indexing techniques leveraging the specifics of the data in order toenable efficient event processing;(2) the query engine takes advantage of the RFIDcharacteristics (eg; the monotonic increase of timestamps) to speed up query processing.Our experimental studies show that the RFID Triple Store can achieve both a significantly …,East European Conference on Advances in Databases and Information Systems,2012,4
Early grouping gets the skew,Sven Helmer; Thomas Neumann; Guido Moerkotte,Abstract: We propose a new algorithm for external grouping with large results. Our approachhandles skewed data gracefully and lowers the amount of random IO on disk considerably.Contrary to existing grouping algorithms; our new algorithm does not require the optimizer toemploy complicated or error-prone procedures adjusting the parameters prior to query planexecution. We implemented several variants of our algorithm as well as the most commonlyused algorithms for grouping and carried out extensive experiments on both synthetic andreal data. The results of these experiments reveal the dominance of our approach. In case ofheavily skewed data we outperform the other algorithms by a factor of two.,Technical reports,2002,4
Analytics on Fast Data: Main-Memory Database Systems versus Modern Streaming Systems.,Andreas Kipf; Varun Pandey; Jan Böttcher; Lucas Braun; Thomas Neumann; Alfons Kemper,ABSTRACT Today's streaming applications demand increasingly high event throughputrates and are often subject to strict latency constraints. To allow for more complex workloads;such as window-based aggregations; streaming systems need to support stateful eventprocessing. This introduces new challenges for streaming engines as the state needs to bemaintained in a consistent and durable manner and simultaneously accessed by complexqueries for real-time analytics. Modern streaming systems; such as Apache Flink; do notallow for efficiently exposing the state to analytical queries. Thus; data engineers are forcedto keep the state in external data stores; which significantly increases the latencies untilevents are visible to analytical queries. Proprietary solutions have been created to meet datafreshness constraints. These solutions are expensive; error-prone; and difficult to …,EDBT,2017,3
High-Performance Main-Memory Database Systems and Modern Virtualization: Friends or Foes?,Tobias Mühlbauer; Wolf Rödiger; Andreas Kipf; Alfons Kemper; Thomas Neumann,Abstract Virtualization owes its popularity mainly to its ability to consolidate software systemsfrom many servers into a single server without sacrificing the desirable isolation betweenapplications. This not only reduces the total cost of ownership; but also enables rapiddeployment of complex software and application-agnostic live migration between servers forload balancing; high-availability; and fault-tolerance. However; virtualization is no free lunch.To achieve isolation; virtualization environments need to add an additional layer ofabstraction between the bare metal hardware and the application. This inevitably introducesa performance overhead. High-performance main-memory database systems arespecifically susceptible to additional software abstractions as they are closely optimized andtuned for the underlying hardware. In this work; we analyze in detail how much overhead …,Proceedings of the Fourth Workshop on Data analytics in the Cloud,2015,3
Extending the MPSM Join.,Martina-Cezara Albutiu; Alfons Kemper; Thomas Neumann,Abstract: Hardware vendors are improving their (database) servers in two main aspects:(1)increasing main memory capacities of several TB per server; mostly with non-uniformmemory access (NUMA) among sockets; and (2) massively parallel multi-core processing.While there has been research on the parallelization of database operations; still manyalgorithmic and control techniques in current database technology were devised for disk-based systems where I/O dominated the performance. Furthermore; NUMA has only recentlycaught the community's attention. In [AKN12]; we analyzed the challenges that modernhardware poses to database algorithms on a 32-core machine with 1 TB of main memory(four NUMA partitions) and derived three rather simple rules for NUMA-affine scalable multi-core parallelization. Based on our findings; we developed MPSM; a suite of massively …,BTW,2013,3
A Framework for Reasoning about Share Equivalence and Its Integration into a Plan Generator.,Thomas Neumann; Guido Moerkotte,Abstract: Very recently; Cao et al. presented the MAPLE approach; which acceleratesqueries with multiple instances of the same relation by sharing their scan operator. Theprincipal idea is to derive; in a first phase; a non-shared tree-shaped plan via a traditionalplan generator. In a second phase; common instances of a scan are detected and shared byturning the operator tree into an operator DAG (directed acyclic graph). The limits of theirapproach are obvious.(1) Sharing more than scans is often possible and can lead toconsiderable performance benefits.(2) As sharing influences plan costs; a separation of theoptimization into two phases comprises the danger of missing the optimal plan; since the firstoptimization phase does not know about sharing. We remedy both points by introducing ageneral framework for reasoning about sharing: plans can be shared whenever they are …,BTW,2009,3
Smooth interpolating histograms with error guarantees,Thomas Neumann; Sebastian Michel,Abstract Accurate selectivity estimations are essential for query optimization decisionswhere they are typically derived from various kinds of histograms which condense valuedistributions into compact representations. The estimation accuracy of existing approachestypically varies across the domain; with some estimations being very accurate and somequite inaccurate. This is in particular unfortunate when performing a parametric search usingthese estimations; as the estimation artifacts can dominate the search results. We proposethe usage of linear splines to construct histograms with known error guarantees across thewhole continuous domain. These histograms are particularly well suited for using theestimates in parameter optimization. We show by a comprehensive performance evaluationusing both synthetic and real world data that our approach clearly outperforms existing …,British National Conference on Databases,2008,3
Main Memory Database Systems,Franz Faerber; Alfons Kemper; Per-Åke Larson; Justin Levandoski; Thomas Neumann; Andrew Pavlo,Abstract This article provides an overview of recent developments in mainmemory databasesystems. With growing memory sizes and memory prices dropping by a factor of 10 every 5years; data having a “primary home” in memory is now a reality. Main-memory databaseseschew many of the traditional architectural pillars of relational database systems thatoptimized for disk-resident data. The result of these memory-optimized designs are systemsthat feature several innovative approaches to fundamental issues (eg; concurrency control;query processing) that achieve orders of magnitude performance improvements overtraditional designs. Our survey covers five main issues and architectural choices that need tobe made when building a high performance main-memory optimized database: dataorganization and storage; indexing; concurrency control; durability and recovery …,Foundations and Trends® in Databases,2017,2
Order Indexes: supporting highly dynamic hierarchical data in relational main-memory database systems,Jan Finis; Robert Brunel; Alfons Kemper; Thomas Neumann; Norman May; Franz Faerber,Abstract Maintaining and querying hierarchical data in a relational database system is animportant task in many business applications. This task is especially challenging whenconsidering dynamic use cases with a high rate of complex; possibly skewed structuralupdates. Labeling schemes are widely considered the indexing technique of choice forhierarchical data; and many different schemes have been proposed. However; they cannothandle dynamic use cases well due to various problems; which we investigate in this paper.We therefore propose Order Indexes—a dynamic representation of the nested intervalsencoding—which offer competitive query performance; unprecedented update efficiency;and robustness for highly dynamic workloads.,The VLDB Journal,2017,2
Efficient batched distance and centrality computation in unweighted and weighted graphs,Manuel Then; Stephan Günnemann; Alfons Kemper; Thomas Neumann,Distance and centrality computations are important building blocks for modern graphdatabases as well as for dedicated graph analytics systems. Two commonly used centralitymetrics are the compute-intense closeness and betweenness centralities; which requirenumerous expensive shortest distance calculations. We propose batched algorithmexecution to run multiple distance and centrality computations at the same time and let themshare common graph and data accesses. Batched execution amortizes the high cost ofrandom memory accesses and presents new vectorization potential on modern CPUs andcompute accelerators. We show how batched algorithm execution can be leveraged tosignificantly improve the performance of distance; closeness; and betweenness centralitycalculations on unweighted and weighted graphs. Our evaluation demonstrates that …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,2
Parallel Array-Based Single-and Multi-Source Breadth First Searches on Large Dense Graphs.,Moritz Kaufmann; Manuel Then; Alfons Kemper; Thomas Neumann,ABSTRACT One of the fundamental algorithms in analytical graph databases is breadth-firstsearch (BFS). It is the basis of reachability queries; centrality computations; neighborhoodenumeration; and many other commonly-used algorithms. We take the idea of purely array-based BFSs introduced in the sequential multi-source MS-BFS algorithm and extend thisapproach to multi-threaded single-and multi-source BFSs. Replacing the typically usedqueues with fixed-sized arrays; we eliminate major points of contention which other BFSalgorithms experience. To ensure equal work distribution between threads; we co-optimizework stealing parallelization with a novel vertex labeling. Our BFS algorithms have excellentscaling behavior and take advantage of multi-core NUMA architectures.,EDBT,2017,2
Main-memory database systems,Alfons Kemper; Thomas Neumann,The recent advances in processor technology-soon hundreds of cores and terabytes ofDRAM in commodity servers-have spawned the academic as well as the industrial interest inmain-memory database technology. In this panel; we will discuss the virtues of differentarchitectural designs wrt transaction processing as well as OLAP query processing.,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,2
System R (R*) Optimizer,Mouna Kacimi; Thomas Neumann,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,2
Social recommendations at work,Tom Crecelius; Mouna Kacimi; Sebastian Michel; Thomas Neumann; Josiane X Parreira; Ralf Schenkel; Gerhard Weikum,Abstract Online communities have become popular for publishing and searching content;and also for connecting to other users. User-generated content includes; for example;personal blogs; bookmarks; and digital photos. Items can be annotated and rated bydifferent users; and users can connect to others that are usually friends and/or sharecommon interests.,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,2
Single phase construction of optimal DAG-structured QEPs,Thomas Neumann; Guido Moerkotte,Zusammenfassung Traditionally; database management systems use tree-structured queryevaluation plans. They are easy to implement but not expressive enough for someoptimizations like eliminating common algebraic subexpressions or magic sets. Theserequire directed acyclic graphs (DAGs); ie shared subplans. Existing approaches considerDAGs merely for special cases and not in full generality. We introduce a novel framework toreason about sharing of subplans and; thus; DAG-structured query evaluation plans. Then;we present the first plan generator capable of generating optimal DAG-structured queryevaluation plans. The experimental results show that with no or only a modest increase ofplan generation time; a major reduction of query execution time can be achieved forcommon queries.,*,2008,2
Optimizing ranked retrieval,Thomas Neumann,Abstract Ranked retrieval plays an important role in explorative querying; where the user isinterested in the top k results of complex ad-hoc queries. In such a scenario; response timesare very important; but at the same time; tuning techniques; such as materialized views; arehard to use. Therefore it would be highly desirable to exploit the top-k property of the queryto speed up the computation; reducing intermediate results and thus execution time. Wepresent a novel approach to optimize ad-hoc top-k queries; propagating the top-k naturedown the execution plan. Our experimental results support our claim that integrating top-kprocessing into algebraic optimization greatly reduces the query execution times andprovides strong evidence that the resulting execution plans are robust against statisticalmisestimations.,International Conference on Database and Expert Systems Applications,2007,2
Search for the Best but Expect the Worst-Distributed Top-k Queries over Decreasing Aggregated Scores.,Sebastian Michel; Thomas Neumann,ABSTRACT We consider distributed top-k queries in wide-area networks where the indexlists for the attribute values (or text terms) of a query are distributed across a number of datapeers. In contrast to existing work; we exclusively consider distributed top-k queries overdecreasing aggregated values. State-of-the-art distributed top-k algorithms usually dependon threshold propagation to reduce expensive data access across the network; but fail tocompute tight thresholds if the aggregation function is decreasing. Decreasing aggregationfunctions; however; occur naturally; for example when considering conjunctive queries. Ourproposed algorithms allow for efficient execution of these kind of queries; using acombination of threshold propagation and semijoin techniques. We demonstrate thesetechniques for the problem of top-k peer selection in a Peer-To-Peer Web search engine …,WebDB,2007,2
On the optimal ordering of maps; selections; and joins under factorization,Thomas Neumann; Sven Helmer; Guido Moerkotte,Abstract We examine the problem of producing the optimal evaluation order for queriescontaining joins; selections; and maps. Specifically; we look at the case where commonsubexpressions involving expensive UDF calls can be factored out. First; we show thatignoring factorization during optimization can lead to plans that are far off the best possibleplan: the difference in cost between the best plan considering factorization and the best plannot considering factorization can easily reach several orders of magnitude. Then; weintroduce optimization strategies that produce optimal left-deep and bushy plans whenfactorization is taken into account. Experiments (1) confirm that factorization is a critical issuewhen it comes to generating optimal plans and (2) we show that to consider factorizationdoes not make plan generation significantly more expensive.,British National Conference on Databases,2006,2
Biopolymers at Interfaces,VA Basiuk; M Malmsten,*,*,1998,2
Automatic algorithm transformation for efficient multi-snapshot analytics on temporal graphs,Manuel Then; Timo Kersten; Stephan Günnemann; Alfons Kemper; Thomas Neumann,Abstract Analytical graph algorithms commonly compute metrics for a graph at one point intime. In practice it is often also of interest how metrics change over time; eg; to find trends.For this purpose; algorithms must be executed for multiple graph snapshots. We presentSingle Algorithm Multiple Snapshots (SAMS); a novel approach to execute algorithmsconcurrently for multiple graph snapshots. SAMS automatically transforms graph algorithmsto leverage similarities between the analyzed graph snapshots. The automatictransformation interleaves algorithm executions on multiple snapshots; synergisticallyshares their graph accesses and traversals; and optimizes the algorithm's data layout. Thus;SAMS can amortize the cost of random data accesses and improve memory bandwidthutilization---two main cost factors in graph analytics. We extensively evaluate SAMS using …,Proceedings of the VLDB Endowment,2017,1
Automated query interface for hybrid relational architectures,Andreas Lübcke; Gunter Saake; Thomas Neumann,Abstract Basic goals of data management are the efficient storage and the efficient access todata. Thereby; Database Systems (DBSs) have to meet different requirements for differentapplications. With several optimization techniques for DBSs; data management is tailored fordifferent applications. One option for optimization is the physical design of DBSs usingsupport structures; which mainly improve access to data (eg; indexes). On the one hand;DBSs shall support multiple purposes for daily operation as well as forecasts (one systemfits all); on the other hand; different purposes imply differing requirements and design goals;which prevent an optimal design for each purpose; and increase the complexity of physicaldesign. In other words; both goals–multi-purpose DBSs and optimal design (for onepurpose) with feasible complexity–contradict each other (one system does not fit all) …,*,2017,1
Die Evolution des Hauptspeicher-Datenbanksystems HyPer: Von Transaktionen und Analytik zu Big Data sowie von der Forschung zum Technologietransfer,Alfons Kemper; Viktor Leis; Thomas Neumann,Zusammenfassung Wir beschreiben die evolutionäre Entwicklung des an der TUMentwickelten Hauptspeicher‐Datenbanksystems HyPer; das für heterogene Workloadsbestehend aus Transaktionen; analytischen Anfragen bis hin zu Big Data Explorationengleichermaßen konzipiert wurde. Der Vorteil eines solchen „all-in-one “Datenbanksystemsgegenüber bisher gebräuchlichen dedizierten Einzelsystemen besteht darin; dass dieDatenexploration auf dem jüngsten Datenbankzustand basiert und somit aktuell gültigeErkenntnisse liefert.,*,2017,1
Query optimization through the looking glass; and what we found running the Join Order Benchmark,Viktor Leis; Bernhard Radke; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark that works on real-life data riddled with correlations andintroduces 113 complex join queries. We experimentally revisit the main components in theclassic query optimizer architecture using a complex; real-world data set and realistic multi-join queries. For this purpose; we describe cardinality-estimate injection and extractiontechniques that allow us to compare the cardinality estimators of multiple industrial SQLimplementations on equal footing; and to characterize the value of having perfect cardinalityestimates. Our investigation shows that all industrial-strength cardinality estimators routinelyproduce large errors: though cardinality estimation using table samples solves the problemfor single-table queries; there are still no techniques in industrial systems that can deal …,The VLDB Journal,2017,1
Special issue: Modern hardware,Peter Boncz; Wolfgang Lehner; Thomas Neumann,While database systems have long enjoyed a “free ride” with ever-increasing clock cycles ofthe CPU; in the last decade this increase stalled. On the computational side; we have seenan ever-increasing number of cores as well as the advent of specialized computing unitsranging from GPUs via FPGA to chips with specific extensions. On the memory side; we notonly observe a significant growth of the capacity of main memory; but a continued largeperformance impact of RAM latency on data access cost; recently aggravated by increasingNUMA effects. Storage-wise we have witnessed the introduction of NAND devices (eg;SSDs) impacting the established role of magnetic disk drive. These advances taken togetherimpact current database architectures and ask for adjustments; extensions or even acomplete re-write in order to establish a scalable; affordable; and flexible foundation for …,The VLDB Journal,2016,1
Evaluation of parallel graph loading techniques,Manuel Then; Moritz Kaufmann; Alfons Kemper; Thomas Neumann,Abstract For many exploratory graph workloads; the initial loading and construction of thegraph data structures makes up a significant part of the total runtime. Still; this topic is hardlyanalyzed in literature and often neglected in systems and their evaluations. In this paper weanalyze the whole graph loading process; including parsing; dense vertex identifierrelabeling; and writing the final in-memory data structures. We present various loadingstrategies that take into consideration the properties of the input graph; eg; partitioning; andevaluate them through extensive experiments.,Proceedings of the Fourth International Workshop on Graph Data Management Experiences and Systems,2016,1
Effiziente Integration von Data-und Graph-Mining-Algorithmen in relationale Datenbanksysteme.,Manuel Then; Linnea Passing; Nina Hubig; Stephan Günnemann; Alfons Kemper; Thomas Neumann,TU München; Lehrstuhl für Datenbanksysteme; Boltzmannstraße 3; 85748 Garching bei München{then;passing;hubig;guennemann;kemper;neumann}@in.tum.de … Zusammenfassung. DieNutzung komplexer Algorithmen zur Analy- se oft hochdimensionaler Datensätze gerät im Kontextvon … Schlüsselwörter: Data Mining; Graph; SQL; HyPer; RDBMS … 1 Motivation Die gegenwärtigeDatenexplosion stellt stand-alone Data-Mining-Programme vor Schwierigkeiten: zur Analysegroßer Datenmengen sind sie durch ihre einge- schränkte Datenverwaltungsfunktionalität kaumgeeignet. Besonders da die zu analysierenden Daten in die Applikationen kopiert werdenmüssen; sind diese für sich ändernde Daten ineffizient. Im Gegensatz hierzu bieten(relationale) Datenbanksysteme eine effiziente und update-freundliche Datenspeicherung. LautAggarwal et al. [1] ist die nahtlose Integration von Data-Mining-Technologien in …,LWA,2015,1
Hochperformante analyse von graph-datenbanken,Moritz Kaufmann; Tobias Mühlbauer; Manuel Then; Andrey Gubichev; Alfons Kemper; Thomas Neumann,Ziel des ACM SIGMOD Programming Contest 2014 war es ein hochperformantes System fürdie Analyse von großen Graph-Daten zu entwickeln. Insbesondere die unregelmäßigenSpeicherzugriffsmuster und Kontrollflussverzweigungen von Graphalgorithmen stellendabei eine große Herausforderung dar; da diese bisher nicht effizient auf moderndensuperskalaren Mehrkern-Prozessoren ausgeführt werden können. Um diese Prozessorenoptimal auszulasten bedarf es zudem der Nutzung aller parallelen Ausführungseinheiten. Inder vorliegenden Arbeit präsentieren wir das Gewinnersystem des Wettbewerbs. Der Erfolgunseres Systems beruht; neben gutem Engineering; auf den folgenden Entwicklungen:(i)Daten-parallelisierte Graph-Breitensuche; welche Cache-Misses effizient amortisiert;(ii)Heuristiken zur Reduzierung des Suchraums bei Top-k-Anfragen;(iii) schnelles …,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,1
Bericht vom Herbsttreffen der GI-Fachgruppe Datenbanksysteme.,Alfons Kemper; Tobias Mühlbauer; Thomas Neumann; Angelika Reiser; Wolf Rödiger,Das diesjährige Herbsttreffen der GI-Fachgruppe Da- tenbanksysteme fand am 22. und 23. Novemberan der Technischen Universität München statt. Das Treffen hatte sich “Scalable Analytics” zumThema gesetzt und stand unter dem Motto “Industry meets Academia”. Mit über 80 Teilnehmernaus der deutschsprachigen Datenbank-Community war das Treffen gut besucht. Insgesamt 10Firmen sowie 36 Universitäten und For- schungseinrichtungen waren dabei vertreten. Vordie- sem Publikum haben Datenbänkler und Datenbänk- lerinnen aus Industrie und Forschungin insgesamt 14 Vorträgen neueste Entwicklungen und Technologien für skalierbare analytischeAnfragebearbeitung präsentiert. Im Folgenden wollen wir kurz von diesen berichten. Zum Auftaktgab es am ersten Tag einen Vortrag von Jens Teubner (ETH Zürich) zur “Ausnutzung modernerHardware für optimierte Datenverarbeitung”. Dabei stellte er heraus; dass Multi-CPU …,Datenbank-Spektrum,2013,1
The Linked Data Benchmark Council (LDBC),Irini Fundulaki; Josep Larriba Pey; David Dominguez-Sal; Ioan Toma; Dieter Fensel; Barry Bishop; Thomas Neumann; Orri Erling; Peter Neubauer; Paul Groth; Frank Van Harmelen; Peter Boncz,In the last years we have seen an explosion of massive amounts of graph shaped datacoming from a variery of applications that are related to social networks (Facebook; Twitter;blogs and other on-line media) and telecommunication networks. Furthermore; the W3CLinking Open Data Initiative [8] has boosted the publication and interlinkage of a largenumber of datasets on the Semantic Web [2] resulting to the Linked Open Data Cloud. Thesedatasets with billions of RDF triples such as Wikipedia [5]; US Census bureau [4]; CIA WorldFactbook [1]; DBPedia [5]; and government sites1 have been created and published online.Moreover; numerous datasets and vocabularies from e-science are published nowadays asRDF graphs most notably in life and earth sciences; astronomy [6][7][3] in order to facilitatecommunity annotation and interlinkage of both scientific and scholarly data of interest …,Proc. First Eur. Data Forum,2012,1
RDF-stores und rdf-query-engines,Thomas Neumann; Gerhard Weikum,Zusammenfassung RDF ist das Datenmodell der Semantic-Web-und Linked-Data-Initiativen; das zunehmend Verbreitung findet. Es zeichnet sich durch feinkörnigeStrukturierungsmöglichkeiten in Form binärer Relationen und durch flexible Typisierungohne die Notwendigkeit eines präskriptiven Schemas aus. Aus diesen Gründen sind dieeffiziente Speicherung und Anfrageauswertung auf RDF-Datenkollektionen schwierigeForschungsthemen. Dieser Artikel gibt einen Überblick über verschiedene Alternativen zurSpeicherung von RDF-Daten und diskutiert kurz die Probleme; die sich für die Indexierungsowie Anfrageoptimmierung und-ausführung ergeben.,Datenbank-Spektrum,2011,1
Faster join enumeration for complex queries,Guido Moerkotte; Thomas Neumann,Most existing join ordering algorithms concentrate on join queries with simple joinpredicates and inner joins only; where simple predicates are those that involve exactly tworelations. However; real queries may contain complex join predicates; ie predicatesinvolving more than two relations. We show how to handle complex join predicatesefficiently; by modeling the query graph as a hypergraph and reasoning about its connectedsubgraphs.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,1
Adaptive Geospatial Joins for Modern Hardware,Andreas Kipf; Harald Lang; Varun Pandey; Raul Alexandru Persa; Peter Boncz; Thomas Neumann; Alfons Kemper,Abstract: Geospatial joins are a core building block of connected mobility applications. Anespecially challenging problem are joins between streaming points and static polygons.Since points are not known beforehand; they cannot be indexed. Nevertheless; points needto be mapped to polygons with low latencies to enable real-time feedback. We present anadaptive geospatial join that uses true hit filtering to avoid expensive geometriccomputations in most cases. Our technique uses a quadtree-based hierarchical grid toapproximate polygons and stores these approximations in a specialized radix tree. Weemphasize on an approximate version of our algorithm that guarantees a user-definedprecision. The exact version of our algorithm can adapt to the expected point distribution byrefining the index. We optimized our implementation for modern hardware architectures …,arXiv preprint arXiv:1802.09488,2018,*
Equilibrium Selection in Coordination Games: An Experimental Study of the Role of Higher Order Beliefs in Strategic Decisions,Thomas Neumann; Bodo Vogt,Abstract The equilibrium selection in games with multiple equilibria; such as coordinationgames; depends on one player's beliefs about the other player's behavior; as such; theoutcome of the game depends on the players' expectations of one another's behavior. Thisstudy assessed the extent to which players' higher order beliefs influence the strategicchoices they make during\(2\times 2\) coordination games. Using a quadratic scoring rule;the players' higher order beliefs about the choices their opponent would make were directlyelicited in a laboratory experiment. The players' higher order beliefs were analyzed toascertain the extent to which players' depth of thinking influenced their strategic decisions. Inaddition; this study focused on the question of whether the players update their beliefs tobuild higher order beliefs. The findings of the study revealed that the average participant …,*,2018,*
LIGANDS FOR ANTIBODY AND FC-FUSION PROTEIN PURIFICATION BY AFFINITY CHROMOTOGRAPHY IV,*,Abstract: The present invention relates to the use; for affinity purification of an antibody or anfragment of an antibody; of a ligand-substituted matrix comprising a support material and atleast one ligand covalently bonded to the support material; the ligand being represented byformula (I),*,2017,*
HyPerInsight: Data Exploration Deep Inside HyPer,Nina Hubig; Linnea Passing; Maximilian E Schüle; Dimitri Vorona; Alfons Kemper; Thomas Neumann,Abstract Nowadays we are drowning in data of various varieties. For all these mixed typesand categories of data there exist even more different analysis approaches; often done insingle hand-written solutions. We propose to extend HyPer; a main memory databasesystem to a uniform data agent platform following the one system fits all approach for solvinga wide variety of data analysis problems. We achieve this by applying a flexible operatorconcept to a set of various important data exploration algorithms. With that; HyPer solvesanalytical questions using clustering; classification; association rule mining and graphmining besides standard HTAP (Hybrid Transaction and Analytical Processing) workloadson the same database state. It enables to approach the full variety and volume of HTAPextended for data exploration (HTAPx); and only needs knowledge of already introduced …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,*
Ultimatum bargaining over losses and gains–An experimental comparison,Thomas Neumann; Stephan Schosser; Bodo Vogt,Abstract Subjects in the loss domain tend to split payoffs equally when bargaining. Theultimatum game offers an ideal mechanism through which social scientists can investigatewhether equal splits are the consequence of the proposers' generosity or due to theiranticipation that the responders will reject lower offers. This paper experimentally comparesultimatum bargaining that takes place in a loss domain with that under a gains domain. Theresults reveal that; although responders do not expect more in the loss domain; proposersdo make higher offers. As such; proposers reach agreements more often in the loss domainthan they do in the gains domain; and responders receive higher payoffs.,Social science research,2017,*
Ligands for antibody and Fc-fusion protein purification by affinity chromotography IV,*,The present invention relates to the use; for affinity purification of an antibody or an fragmentof an antibody; of a ligand-substituted matrix comprising a support material and at least oneligand covalently bonded to the support material; the ligand being represented by formula (I)L-(Sp) v-Ar1—Am—Ar2 (I) wherein L; SP; Ar1; AM; Ar2 and v are defined herein.,*,2017,*
The Impact of Previous Action on Bargaining—An Experiment on the Emergence of Preferences for Fairness Norms,Thomas Neumann; Stephan Schosser; Bodo Vogt,Abstract The communication of participants to identify an acceptable bargaining outcome inthe Nash bargaining game is all about fairness norms. Participants introduce fairness normswhich yield a better outcome for themselves in order to convince the other participant of theirbargaining proposal. Typically; these fairness norms are in line with theoretical predictions;which support a wide variety of different but fair outcomes the participants can choose from.In this experiment; we play two treatments of the Nash bargaining game: in one treatment;the participants play a dictator game prior to bargaining; and in the other treatment they donot. We find that participants who have not played the dictator game intensively discuss theoutcome of the game and come to solutions closer to the equal split of the pie the longerthey chat. This effect vanishes as soon as the participants have previous experience from …,Games,2017,*
Monopedia: staying single is good enough--the hyper way for web scale applications,Maximilian E Schüle; Pascal Schliski; Thomas Hutzelmann; Tobias Rosenberger; Viktor Leis; Dimitri Vorona; Alfons Kemper; Thomas Neumann,Abstract In order to handle the database load for web scale applications; the conventionalwisdom is that a cluster of database servers and a caching layer are essential. In this work;we argue that modern main memory database systems are often fast enough to consolidatethis complex architecture into a single server (plus an additional fail over system). Todemonstrate this claim; we design the Monopedia Benchmark; a benchmark for web scaleapplications modeled after Wikipedia. Using this benchmark; we show that it is indeedpossible to run the database workload of one of the largest web sites in the world on a singledatabase server.,Proceedings of the VLDB Endowment,2017,*
Combining biomechanical and data-driven body surface models,Stefanie Gassel; Thomas Neumann; Markus Wacker,Abstract Statistical body shape modelling can be used to realistically generate complexmuscle deformation effects on the skin. However; purely data-driven models still ignore thebiomechanical nature of surface deformations. Reliable anatomically and biomechanicallyconsistent predictions are barely possible. Our research aims at combining the previouslyseparate paradigms-data-driven and simulation-driven 3D surface modeling-to a hybridbody shape model. Our first goal consists of synthesizing the skin surface from simulatedbiomechanical data. As a first step in this direction we show preliminary results of our modelof an elbow flexion motion with separate biceps and triceps muscle bulging that exhibitsbelievable muscular deformation effects on the skin surface while enabling singular controlover specific muscle regions. Our model is separately controllable in shape and pose and …,ACM SIGGRAPH 2017 Posters,2017,*
Efficient Batched Distance; Closeness and Betweenness Centrality Computation in Unweighted and Weighted Graphs,Manuel Then; Stephan Günnemann; Alfons Kemper; Thomas Neumann,Abstract Distance and centrality computations are important building blocks for moderngraph databases as well as for dedicated graph analytics systems. Two commonly usedcentrality metrics are the compute-intense closeness and betweenness centralities; whichrequire numerous expensive shortest distance calculations. We propose batched algorithmexecution to run multiple distance and centrality computations at the same time and let themshare common graph and data accesses. Batched execution amortizes the high cost ofrandom memory accesses and presents new vectorization potential on modern CPUs andcompute accelerators. We show how batched algorithm execution can be leveraged tosignificantly improve the performance of distance; closeness; and betweenness centralitycalculations on unweighted and weighted graphs. Our evaluation demonstrates that …,Datenbank-Spektrum,2017,*
The Complete Story of Joins (inHyPer),Thomas Neumann; Viktor Leis; Alfons Kemper,SQL has evolved into an (almost) fully orthogonal query language that allows (arbitrarilydeeply) nested subqueries in nearly all parts of the query. In order to avoid recursiveevaluation strategies which incur unbearable O (n2) runtime we need an extended relationalalgebra to translate such subqueries into non-standard join operators. This paperconcentrates on the non-standard join operators beyond the classical textbook inner joins;outer joins and (anti) semi joins. Their implementations in HyPer were covered in previouspublications which we refer to. In this paper we cover the new join operators mark-join andsingle-join at both levels: At the logical level we show the translation and reorderingpossibilities in order to e ectively optimize the resulting query plans. At the physical level wedescribe hash-based and block-nested loop implementations of these new joins. Based …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
The Complete Story of Joins (in HyPer).,Thomas Neumann; Viktor Leis; Alfons Kemper,Abstract: SQL has evolved into an (almost) fully orthogonal query language that allows(arbitrarily deeply) nested subqueries in nearly all parts of the query. In order to avoidrecursive evaluation strategies which incur unbearable O (n2) runtime we need an extendedrelational algebra to translate such subqueries into non-standard join operators. This paperconcentrates on the non-standard join operators beyond the classical textbook inner joins;outer joins and (anti) semi joins. Their implementations in HyPer were covered in previouspublications which we refer to. In this paper we cover the new join operators mark-join andsingle-join at both levels: At the logical level we show the translation and reorderingpossibilities in order to effectively optimize the resulting query plans. At the physical level wedescribe hash-based and block-nested loop implementations of these new joins. Based …,BTW,2017,*
Massively Parallel NUMA-Aware Hash Joins,Thomas Neumann; Alfons Kemper,Abstract. Driven by the two main hardware trends increasing main memory and massivelyparallel multi-core processing in the past few years; there has been much research effort inparallelizing well-known join algorithms. However; the non-uniform memory access (NUMA)of these architectures to main memory has only gained limited attention in the design ofthese algorithms. We study recent proposals of main memory hash join implementations andidentify their major performance problems on NUMA architectures. We then develop aNUMA-aware hash join for massively parallel environments; and show how the specificimplementation details affect the performance on a NUMA system. Our experimentalevaluation shows that a carefully engineered hash join implementation outperformsprevious high performance hash joins by a factor of more than two; resulting in an …,In Memory Data Management and Analysis: First and Second International Workshops; IMDM 2013; Riva del Garda; Italy; August 26; 2013; IMDM 2014; Hongzhou; China; September 1; 2014; Revised Selected Papers,2015,*
In Memory Data Management and Analysis,Arun Jagatheesan; Justin Levandoski; Thomas Neumann; Andrew Pavlo,Over the last 30 years; memory prices have been dropping by a factor of 10 every 5 years.The number of I/O operations per second (IOPS) in DRAM is far greater than other storagemedia such as hard disks and SSDs. DRAM is readily available in the market at better pricepoint in comparison to DRAM-alternatives. These trends make DRAM a better storage mediafor latency-sensitive data management applications. For example; mobile applicationsrequire low-latency responses to user requests. The “hot set” of large transactionalworkloads fit comfortably in memory. Many largescale web applications such as Facebookand Amazon manage most of their active data in main memory. With the emergence of sucha diverse pool of latency-sensitive applications coupled with dropping DRAM prices; it istimely to explore main-memory optimized data management platforms. In addition; almost …,*,2015,*
The Database Group at TUM,Alfons Kemper; Thomas Neumann,Abstract The database group at TUM was established in 1972 by Rudolf Bayer [4] whoretired in 2004. The group is reponsible for the foundational and advanced databasecurriculum of the department's 3500 undergraduate and graduate students. The groupconsists of two post-docs and around 15 doctoral students. In the following we will overviewthe research agenda of the last couple of years only; previous projects and publications canbe viewed on our web site.,ACM SIGMOD Record,2014,*
HyPer Beyond Software: Exploiting Modern Hardware for Main-Memory Database Systems,Florian Funke; Alfons Kemper; Tobias Mühlbauer; Thomas Neumann; Viktor Leis,Abstract In this paper; we survey the use of advanced hardware features for optimizing main-memory database systems in the context of our HyPer project. We exploit the virtual memorymanagement for snapshotting the transactional data in order to separate OLAP queries fromparallel OLTP transactions. The access behavior of database objects from simultaneousOLTP transactions is monitored using the virtual memory management component in orderto compact the database into hot and cold partitions. Utilizing many-core NUMA-organizeddatabase servers is facilitated by the morsel-driven adaptive parallelization and partitioningthat guarantees data locality wrt the processing core. The most recent HardwareTransactional Memory support of; eg; Intel's Haswell processor; can be used as the basis fora lock-free concurrency control scheme for OLTP transactions. Finally; we show how …,Datenbank-Spektrum,2014,*
Benchmarking Elastic Query Processing on Big Data,Dimitri Vorona; Florian Funke; Alfons Kemper; Thomas Neumann,Abstract Existing analytical query benchmarks; such as TPC-H; often assess databasesystem performance on on-premises hardware installations. On the other hand; somebenchmarks for cloud-based analytics deal with flexible infrastructure; but often focus onsimpler queries and semi-structured data. With our benchmark draft we attempt to bridge thegap by challenging analytical platforms to answer complex queries on structured businessdata while leveraging the elastic infrastructure of the cloud to satisfy performancerequirements.,Workshop on Big Data Benchmarks,2014,*
Nutzung von Verfahren der Bildanalyse zur Baugrundbeurteilung,Markus Wacker; Thomas Neumann; Jens Engel; Gunter Gräfe,Die Eigenschaften von Boden und Fels als Baugrund oder Baustoff müssen durchKennziffern erfasst werden. Grundlage dafür sind Stoffgesetze; mit denen das Verhaltenmathematisch beschrieben werden kann; sowie experimentelle Verfahren zur Durchführungvon Messungen; mit denen sich die erforderlichen Kennwerte bestimmen lassen. In Bezugauf die Praxis in den geotechnischen Versuchsanstalten haben sich spezielleUntersuchungsmethoden etabliert; wobei auch regionale Besonderheiten und Erfahrungeneine Rolle spielen.,Aktuelle Themen der Geotechnik,2014,*
Efficient Query Processing in RDF Databases.,Andrey Gubichev; Thomas Neumann; José Ramalho,Page 1. Efficient Query Processing in RDF Databases AUTORES: Andrey Gubichev Munich;Germany Thomas Neumann Munich; Germany José Ramalho Apresentação capítulo 5 Curitiba;10 de novembro de 2016 Page 2. Introdução Itens Abordados: • Análise das consultas SPARQL.• Tipos de indexação de dados RDF. • Formas de armazenamento. • Exemplos de sistemasRDF. Page 3. Introdução Considerações Iniciais: • Aumento expressivo na web nos últimosanos do Linked Data. • Existência de diversos conjuntos de dados contendo milhões de triplas:biomedicina (Uniprot; PubMed); bases de conhecimento (DBpedia; YAGO); governo(Data.gov); entretenimento (MusicBrainz); etc. Page 4. 31 bilhões de triplas publicadas onlineDe acordo com Linked Open Data existem mais de Page 5. Introdução Motivação: • Númerosexpressivos de recursos RDF indicando a …,*,2014,*
Vorstellung des Lehrstuhls für Datenbanksysteme der TUM,Alfons Kemper; Thomas Neumann,Zusammenfassung Der Lehrstuhl für Datenbanksysteme der Technischen UniversitätMünchen beschäftigt sich in der Forschung mit der Entwicklung; Optimierung undAnwendung „moderner “Datenbanktechnologie. Die Schwerpunkte der letzten Jahre lagenin der Entwicklung von Multi-Tenancy-fähigen Datenbanken; eScience-Datenbankanwendungen; Workload-Management für heterogene Anwendungen; RDF-Datenbanken und; insbesondere; Hauptspeicher-Datenbanken für hybride OLTP&OLAP-Anwendungen.,Datenbank-Spektrum,2013,*
LIGANDS FOR ANTIBODY AND Fc-FUSION PROTEIN PURIFICATION BY AFFINITY CHROMATOGRAPHY,*,The present invention relates to the use for affinity purification of an antibody or an fragmentof an antibody; of a ligand-substituted matrix comprising a support material and at least oneligand covalently bonded to the support material; the ligand being represented by formula (I)wherein Sp is a spacer group; v is 0 or 1; Am is an amide group—NR1—C (O)—; andwherein either NR1 is attached to Ar1 and—C (O)—is attached to Ar2; or—C (O)—isattached to Ar1 and NR1 is attached to Ar2; and R1 is hydrogen or C1 to C4 alkyl; preferablyhydrogen or methyl; and more preferably hydrogen; Ar1 is a divalent 5-or 6-memberedsubstituted or unsubstituted aromatic ring; Ar2 is 5-or 6-membered heterocyclic aromatic ringwhich is (a) attached to a further 5-or 6-membered aromatic ring via a single bond; or (b)fused to a further 5-or 6-membered aromatic ring as part of a multicyclic ring system; or (c) …,*,2013,*
VLDB Endowment,Michael Böhlen; Christoph Koch; Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu; Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; Dan Olteanu; Divesh Srivastava; Jens Teubner; Stefan Manegold; Peer Kröger; Stratis D Viglas,39th International Conference on Very Large Data Bases; Riva del Garda; Trento; Italy … Proceedingsof the 39th International Conference on … Very Large Data Bases; Riva del Garda; Trento; Italy… Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu;Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; DanOlteanu; Divesh Srivastava; Jens Teubner … The 39th International Conference on Very LargeData Bases; Riva del Garda; Trento; Italy … Permission to make digital or hard copies of portionsof this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyright for components of this work owned by others than VLDBEndowment must be honored. Abstracting with credit is permitted. To copy otherwise; to …,*,2013,*
Incentive Design,M Gunkel,*,*,2013,*
N3458: Simple Database Integration in C++ 11,Thomas Neumann,Many applications make use of relational database to store and query their data. However;existing database interfaces like ODBC and JDBC introduce a significant barrier betweenthe application logic and the database access itself. This makes database usageintimidating and cumbersome. Here; we propose a much simpler database integrationmechanisms which makes use of C++ 11 features to allow for a very natural interaction withthe database itself.,*,2012,*
The mainframe strikes back: multi tenancy in the Main memory database hyper on a TB-server.,Henrik Mühe; Alfons Kemper; Thomas Neumann,Abstract: Contrary to recent trends in database systems research focussing on scaling outworkloads on a cluster of commodity computers; this presentation will break grounds forscale-up. We show that an elastic multi-tenancy solution can be achieved by combining amany-core server with a low footprint main memory database system. Total transactionalthroughput for TPC-C like order-entry transactions reaches up to 2 million transactions persecond on a 32 core server while the number of tenants sharing a single server can bevaried from a few to hundreds of separate tenants without diminishing total throughput.Contrary to common belief; a scale-up solution provides high flexibility for tenants withgrowing throughput needs and allows for simple sharing of common resources betweendifferent tenants while minimizing hardware and computing overhead. We show that our …,IMDM,2011,*
HyPer: Die effiziente Reinkarnation des Schattenspeichers in einem Hauptspeicher-DBMS,Florian Funke; Alfons Kemper; Henrik Mühe; Thomas Neumann,Zusammenfassung HyPer ist ein modernes Hauptspeicher-Datenbanksystem; das dieHardware-unterstützte virtuelle Speicherverwaltung des Betriebssystems für dieDatenverwaltung und die Synchronisation zwischen OLTP-Transaktionen und OLAP-Anfragen effektiv ausnutzt. In Bezug auf die „in-core “Datenverwaltung werden dierelationalen Daten direkt; also ohne zusätzliche Indirektion durch eine DBMS-kontrolliertePuffer-und Seitenverwaltung; auf den virtuellen Adressraum des OLTP-Prozessesabgebildet. Dieser Prozess kann transaktionskonsistente Snapshots der Datenbankanlegen; indem ein neuer OLAP-Prozess abgespaltet wird (in Linux mit dem Systembefehlfork). Der copy on write-Mechanismus des Betriebssystems/Prozessors sorgt für dieKonsistenzerhaltung dieses Snapshots; indem Seiten mit sich ändernden Datenobjekten …,Datenbank-Spektrum,2011,*
Efficient Query Processing on Modern Hardware.,Thomas Neumann,ABSTRACT Most database systems translate a given query into an expression in a(physical) algebra; and then start evaluating this algebraic expression to produce the queryresult. The traditional way to execute these algebraic plans is the iterator model: Everyphysical algebraic operator conceptually produces a tuple stream from its input; and allowsfor iterating over this tuple stream. This is a very nice and simple interface; and allows foreasy combination of arbitrary operators; but it clearly comes from a time when queryprocessing was dominated by I/O and CPU consumption was less important: The iteratorinterface causes thousands of expensive function calls; degrades the branch prediction ofmodern CPUs; and ofter results in poor code locality and complex book-keeping. On modernhardware query processing can be improved considerably by processing tuples in a data …,Grundlagen von Datenbanken,2011,*
Query Optimization,Thomas Neumann,1. The meta heuristics like tabu search require both a description (ie; encoding) of a solutionand a local change mechanism that perturbates the currently considered solution. Give anexample for a possible encoding and multiple alternatives for change functions. How doesthe choice affect the behavior of the meta heuristic?,Statistics,2010,*
Working Group: Classification; Representation and Modeling.,S Das; C Koch; B König-Ries; Ander de Keijzer; V Markl; A Deshpande; M van Keulen; PJ Haas; IF Ilyas; T Neumann; D Olteanu; M Theobald; V Vassalos,KNAW Narcis. Back to search results. Publication Working Group: Classification;Representation and Modeling. (2009). Pagina-navigatie: Main …,*,2009,*
Coupling knowledge bases and web services for active knowledge,Nicoleta Preda; Fabian M Suchanek; Gjergji Kasneci; Thomas Neumann; Gerhard Weikum,Abstract We present ANGIE; a system that can answer user queries by combiningknowledge from a local database with knowledge retrieved from Web services. If a userposes a query that cannot be answered by the local database alone; ANGIE calls theappropriate Web services to retrieve the missing information. In ANGIE; Web services act asdynamic components of the knowledge base that deliver knowledge on demand. To theuser; this is fully transparent; the dynamically acquired knowledge is presented as if it werestored in the local knowledge base. We have developed a RDF based model for declarativedefinition of functions embedded in the local knowledge base. The results of available Webservices are cast into RDF subgraphs. Parameter bindings are automatically constructed byANGIE; services are invoked; and the semi-structured information returned by the …,*,2009,*
08421 Working Group: Classification; Representation and Modeling,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,Abstract This report briefly summarizes the discussions carried out in the working group onclassification; representation and modeling of uncertain data. The discussion was dividedinto two subgroups: the first subgroup studied how different representation and modelingalternatives currently proposed can fit in a bigger picture of theory and technologyinteraction; while the second subgroup focused on contrasting current systemimplementations and the reasons behind such diverse class of available prototypes. Wesummarize the findings of these two groups and the future steps suggested by groupmembers.,Dagstuhl Seminar Proceedings,2009,*
08421 Working Group: Classification; Representation and Modeling.,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,*,Uncertainty Management in Information Systems,2008,*
of Proceedings: Web Information Systems Engineering–WISE 2008: 9th International Conference,Thomas Neumann; Matthias Bender; Sebastian Michel; Ralf Schenkel; Peter Triantafillou; Gerhard Weikum,Abstract/Description: Top-k query processing is a fundamental building block for efficientranking in a large number of applications. Efficiency is a central issue; especially fordistributed settings; when the data is spread across different nodes in a network. This paperintroduces novel optimization methods for top-k aggregation queries in such distributedenvironments that can be applied to all algorithms that fall into the frameworks of the priorTPUT and KLEE methods. The optimizations address 1) hierarchically grouping input listsinto top-k operator trees and optimizing the tree structure; and 2) computing data-adaptivescan depths for different input sources. The paper presents comprehensive experiments withtwo different real-life datasets; using the ns-2 network simulator for a packet-level simulationof a large Internet-style network.,*,2008,*
of Proceedings: Sharing Data; Information and Knowledge: 25th British National Conference on Databases; BNCOD 25,Thomas Neumann; Sebastian Michel,Abstract/Description: Accurate selectivity estimations are essential for query optimizationdecisions where they are typically derived from various kinds of histograms which condensevalue distributions into compact representations. The estimation accuracy of existingapproaches typically varies across the domain; with some estimations being very accurateand some quite inaccurate. This is in particular unfortunate when performing a parametricsearch using these estimations; as the estimation artifacts can dominate the search results.We propose the usage of linear splines to construct histograms with known error guaranteesacross the whole continuous domain. These histograms are particularly well suited for usingthe estimates in parameter optimization. We show by a comprehensive performanceevaluation using both synthetic and real world data that our approach clearly outperforms …,*,2008,*
Efficient Top-K Peer Selection Algorithms for Minerva,Christian Langner,The usage of Peer-to-Peer (P2P) networks is well known from programs like Napster orGnutella where data is downloaded from other users all over the world. The idea behindPeer-to-Peer networks can also be applied to database system where a database isdistributed over multiple peers. Minerva [2] is a Peer-to-Peer Web search engine prototype;developed at the Max-Planck-Institute for Informatics. It allows the user to execute multitermqueries in a way similar to Google; Yahoo; or MSN. The actual data; however; it distributedaccross peers that are connected via a distributed hash table (DHT). Peers can executequeries on their local indices but in addition publish compact per-term descriptions of theirlocal content using the DHT. These per-term; per-peer descriptions express the suitability ofa peer wrt a particular term. If a peer is not satisfied with the results obtained from its local …,*,2007,*
of Proceedings: Database and Expert Systems Applications; 18th International Conference; DEXA 2007,Thomas Neumann,Abstract/Description: Ranked retrieval plays an important role in explorative querying; wherethe user is interested in the top k results of complex ad-hoc queries. In such a scenario;response times are very important; but at the same time; tuning techniques; such asmaterialized views; are hard to use. However; it would be highly desirable for the queryoptimizer to exploit the top-k property of the query; ie; to optimize query execution such thatthe top-k results are produced as fast as possible. We present a novel approach to optimizead-hoc top-k queries; extending the classical approach of equivalent rewrites by explicitlyexploiting the top-k nature of the queries for performance optimizations. Our experimentalresults support our claim that integrating top-k processing into algebraic optimization greatlyreduces the query execution times and provides strong evidence that the resulting …,*,2007,*
of Proceedings: Flexible and efficient information handling: 23rd British National Conference on Databases; BNCOD 23,Thomas Neumann; Sven Helmer; Guido Moerkotte,*,*,2006,*
Effiziente Generierung und Ausfuehrung von DAG-strukturierten Anfragegraphen,Thomas Neumann,Datenbanksysteme verwenden traditionell baumstrukturierte Pläne für dieAnfragebearbeitung. Einige Optimierungstechniken wie Faktorisierung lassen sich mitBäumen aber nicht gut formulieren. Eine attraktive Möglichkeit; die Pläne ausdrucksstärkerzu machen; ist die Verallgemeinerung von Bäumen zu gerichteten azyklischen Graphen(DAGs). Existierende Ansätze betrachten DAGs nur in Spezialfällen; sie sind nicht voll in dieAnfrageoptimierung integriert. Der hier vorgestellte Plangenerator ist der erste; dergenerisch optimale DAG-strukturierte Pläne erzeugt. Die experimentellen Ergebnissezeigen; dass die so erzeugten Pläne teilweise deutlich effizienter sind.,Ausgezeichnete Informatikdissertationen 2005,2006,*
Surface physics; low-dimensional systems; and related topics-Fluorescent dyes as a probe for the localized field of coupled surface plasmon-related resonances,M Kreiter; T Neumann; S Mittler; W Knoll; JR Sambles,*,Physical Review-Section B-Condensed Matter,2001,*
Pool of natural substances-New approach to the search for useful chemical entities,C Koch; T Neumann; R Thiericke; S Grabley,*,NACHRICHTEN AUS CHEMIE TECHNIK UND LABORATORIUM,1997,*
Efficient Synthesis of Branched Propargyl-and Allylsilanes,LF Titze; T Neumann; M Kajino; M Pretor,*,SYNTHESIS-STUTTGART-,1993,*
LeanStore: In-Memory Data Management Beyond Main Memory,Viktor Leis; Michael Haubenschild; Alfons Kemper; Thomas Neumann,Abstract—Disk-based database systems use buffer managers in order to transparentlymanage data sets larger than main memory. This traditional approach is effective atminimizing the number of I/O operations; but is also the major source of overhead incomparison with in-memory systems. To avoid this overhead; in-memory database systemstherefore abandon buffer management altogether; which makes handling data sets largerthan main memory very difficult. In this work; we revisit this fundamental dichotomy anddesign a novel storage manager that is optimized for modern hardware. Our evaluation;which is based on TPC-C and micro benchmarks; shows that our approach has littleoverhead in comparison with a pure in-memory system when all data resides in mainmemory. At the same time; like a traditional buffer manager; it is fully transparent and can …,*,*,*
Adaptive Execution of Compiled Queries,André Kohn; Viktor Leis; Thomas Neumann,Abstract—Compiling queries to machine code is a very efficient way for executing queries.One often overlooked problem with compilation is the time it takes to generate machinecode. Even with fast compilation frameworks like LLVM; generating machine code forcomplex queries often takes hundreds of milliseconds. Such durations can be a majordisadvantage for workloads that execute many complex; but quick queries. To solve thisproblem; we propose an adaptive execution framework; which dynamically switches frominterpretation to compilation. We also propose a fast bytecode interpreter for LLVM; whichcan execute queries without costly translation to machine code and dramatically reduces thequery latency. Adaptive execution is fine-grained; and can execute code paths of the samequery using different execution modes. Our evaluation shows that this approach achieves …,*,*,*
Risk attitude; beliefs; and information in Autoren: Berninghaus; Siegfried Haller; Sven Krüger; Tyll,Thomas Neumann; Stephan Schosser; Bodo Vogt,*,*,*,*
Justify!,Thomas Neumann; Linnea Passing,2PL is not recoverable. When being in the phase of slowly releasing the locks; anothertransaction can acquire the lock and work on it and commit before the first biggertransaction. If this transaction is aborted; there is no chance undoing the smaller one since italready committed although it is dependent on data of the larger transaction. This could befixed by using strong or strict 2PL because the write locks are held longer.,*,*,*
The Computational Database for Real World Awareness CompDB,Thomas Neumann,Summary: Two major hardware trends have a significant impact on the architecture ofdatabase management systems (DBMSs): First; main memory sizes continue to growsignificantly. Machines with 1 TB of main memory and more are readily available at arelatively low price. Second; the number of cores in a system continues to grow; fromcurrently 60 and more to hundreds in the near future. This trend offers radically newopportunities for both business and science. It promises to allow for information-at-your-fingertips; ie; large volumes of data can be analyzed and deeply explored online; in parallelto regular transaction processing. Currently; deep data exploration is performed outside ofthe database system which necessitates huge data transfers. This impedes the processingsuch that real-time interactive exploration is impossible. These new hardware capabilities …,*,*,*
Extending RFSOM with DeepFeatures,Sven Hellbach; Thomas Neumann; Mathias Klingner; Hans-Joachim Böhme,Klingner et al.[3] proposes an algorithm for posture estimation of a human body. Thealgorithm takes the approach from Haker et al.[2] and extends it using Generalized MatrixLearning Vector Quantization (GMLVQ). The original approach [2] uses only a threedimensional space; ie direct spatial coordinates; as feature space to fit a self-organizingfeature map (SOM) with a body-like topology. This leads to problems when individualregions of the person are in close proximity. Hence;[3] decided to add textural information bytraining prototypical description of the body parts texture using GMLVQ. A bunch of typicaltexture descriptors; like RGB; HSV; HOG; LBP; GLCM; are precomputed. Interpreting thematrix Ω describing the adaptive metric in GMLVQ as relevance matrix gives a weighting offeature combination to discriminate the individual body regions. The learning metric …,MiWoCI Workshop-2017,*,*
Deep Learning and LVQ Some first Results in Image Classification,Thomas Neumann; Sven Hellbach; Markus Wacker,Since the influential work by Krizhevsky; Sutskever; and Hinton [3]; deep convolutionalneural networks (ConvNets) have become the predominant approach for imageclassification; showing excellent performance especially when large amounts of trainingdata is available. Recent work in this area mostly focused on novel network architectures;new activation functions; or clever optimisation algorithms. However; the final classificationmodule on top of such a ConvNet essentially remained the same for years: a fully-connectedlayer with softmax activation; whose output is optimised with the cross-entropy loss. Differentloss functions were studied only very recently [2]. Villmann et al.[4] propose anotheralternative: their theoretical arguments prove that generalised learning vector quantisation(GLVQ) can be combined with an (arbitrarily deep) neural network. With a similar idea in …,MiWoCI Workshop-2017,*,*
Supplementary Materials Identification of Small Molecule Inhibitors of Tau Aggregation by Target-ing Monomeric Tau As a Potential Therapeutic Approach for Tauop...,Marcus Pickhardt; Thomas Neumann; Daniel Schwizer; Kari Callaway; Michele Vendruscolo; Dale Schenk; Peter St George-Hyslop; Eva M Mandelkow; Christopher M Dobson10; Lisa McConlogue; Eckhard Mandelkow; Gergely Tóth10,Fig.(S1). HTau2N4Rwt stock preparation; used as starting material fort the screen; showingthat the protein was monomeric. 10% SDS-PAGE of hTau2N4Rwt stock prepared in PBSbuffer at pH 7.4 with 1 mM DTT at 1 mg/ml (22 µM) concentration. hTau2N4Rwt appears as asingle band by Coomassie Blue staining. The purity of the protein is> 95%.,*,*,*
Demo Program Committee,Sihem Amer-Yahia; Arvind Arasu; Sunil Arvindam; Magdalena Balazinska; Fabio Casati; Malu Castellanos; Mariano Cilia; Brian F Cooper; Adina Crainiceanu; Abhinandan Das; Alin Dobra; Pablo Guerrero; Christian Konig; Georgia Koutrika; Wolfgang Lehner; Feifei Li; Ashwin Machanavajjhala; Thomas Neumann; Dan Olteanu; Carlos Ordonez; Peter Pietzuch; Adam Silberstein; Alkis Simitsis,Sihem Amer-Yahia; Qatar Computing Research Institute Arvind Arasu; Microsoft Research SunilArvindam; SAP Research; India Magdalena Balazinska; University of Washington FabioCasati; University of Trento; Italy Malu Castellanos; HP Labs; USA Mariano Cilia; IntelCorporation; Argentina Brian F Cooper; Google Adina Crainiceanu; US Naval Academy AbhinandanDas; Google Alin Dobra; University of Florida Javier Garcia-Garcia; UNAM University; MexicoPablo Guerrero; TU Darmstadt; Germany Melanie Herschel; Tubingen University ChristianKonig; Microsoft Research Georgia Koutrika; IBM Almaden Research Center WolfgangLehner; TU Dresden; Germany Feifei Li; Florida State University Ashwin Machanavajjhala; YahooResearch Thomas Neumann; TU Munchen Dan Olteanu; University of Oxford CarlosOrdonez; University of Houston Peter Pietzuch; Imperial College London Lin Qiao; IBM …,*,*,*
Database Implementation For Modern Hardware,Thomas Neumann,Page 1. 1 / 19 Database Implementation For Modern Hardware Thomas Neumann Page 2.2 / 19 Introduction Introduction Database Management Systems (DBMS) are extremely important• used in nearly all commercial data management • very large data sets • valuable data Keychallenges: • scalability to huge data sets • reliability • concurrency Results in very complexsoftware. Page 3. 3 / 19 Introduction About This Lecture Goals of this lecture • learning howto build a modern DBMS • understanding the internals of existing DBMSs • understanding theeffects of hardware behavior Rough structure of the lecture 1. the classical DBMS architecture2. efficient query processing 3. adapting the architecture to hardware trends Page 4. 4 / 19Introduction Literature • Theo Härder; Erhard Rahm: Datenbanksysteme - Konzepte undTechniken der Implementierung. Springer-Verlag; 2001 …,*,*,*
GI-Edition,Gunter Saake; Andreas Henrich; Wolfgang Lehner; Thomas Neumann; Veit Köppen,In den letzten Jahren hat es auf dem Gebiet des Datenmanagements große Veränderungengegeben. Dabei muss sich die Datenbankforschungsgemeinschaft insbesondere denHerausforderungen von „Big Data “stellen; welches die Analyse von riesigen Datenmengenunterschiedlicher Struktur mit kurzen Antwortzeiten erfordert. Neben klassisch strukturiertenDaten müssen moderne Datenbanksysteme und Anwendungen ebenfalls semistrukturierte;textuelle und andere multi-modale Daten sowie Datenströme in völlig neuenGrößenordnungen verwalten. Gleichzeitig müssen die Verarbeitungssysteme die Korrektheitund Konsistenz der Daten sicherstellen.,*,*,*
Authors’ Addresses Klaus Berberich Max-Planck-Institut für Informatik Stuhlsatzenhausweg 85 66123 Saarbrücken,Srikanta Bedathur; Thomas Neumann; Gerhard Weikum,Abstract Text search over temporally versioned document collections such as web archiveshas received little attention as a research problem. As a consequence; there is no scalableand principled solution to search such a collection as of a specified time t. In this work; weaddress this shortcoming and propose an efficient solution for time-travel text search byextending the inverted file index to make it ready for temporal search. We introduceapproximate temporal coalescing as a tunable method to reduce the index size withoutsignificantly affecting the quality of results. In order to further improve the performance oftime-travel queries; we introduce two principled techniques to trade off index size for itsperformance. These techniques can be formulated as optimization problems that can besolved to near-optimality. Finally; our approach is evaluated in a comprehensive series of …,*,*,*
آزمایشی,آزمایشی,Skip navigation …,*,*,*
HyPer: one DBMS for all,Tobias Mühlbauer; Florian Funke; Viktor Leis; Henrik Mühe; Wolf Rödiger; Alfons Kemper; Thomas Neumann,Page 1. HyPer: one DBMS for all Tobias Mühlbauer; Florian Funke; Viktor Leis; Henrik Mühe;Wolf Rödiger; Alfons Kemper; Thomas Neumann Technische Universität München New EnglandDatabase Summit 2014 http://www.hyper-db.com/ Page 2. One DBMS for all? OLTP and OLAPTraditionally; DBMSs are either optimized for OLTP or OLAP OLTP • high rate of mostly tinytransactions • high data access locality OLAP • few; but long-running transactions • scans largeparts of the database • must see a consistent database state during execution Conflict of interest:traditional solutions like 2PL would block OLTP However: Main memory DBMSs enable newoptions! OLAP OLTP Isolation: Snapshotting Page 3. One DBMS for all? Wimpy and brawnyHigh-performance DBMSs are optimized for brawny servers Brawny servers • predominantlyx86-64 arch • multiple sockets; many cores Wimpy devices …,*,*,*
