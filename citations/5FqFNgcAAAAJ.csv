Evaluation of entity resolution approaches on real-world match problems,Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract Despite the huge amount of recent research efforts on entity resolution (matching)there has not yet been a comparative evaluation on the relative effectiveness and efficiencyof alternate approaches. We therefore present such an evaluation of existingimplementations on challenging real-world match tasks. We consider approaches both withand without using machine learning to find suitable parameterization and combination ofsimilarity functions. In addition to approaches from the research community we also considera state-of-the-art commercial entity resolution implementation. Our results indicate significantquality and efficiency differences between different approaches. We also find that somechallenging resolution tasks such as matching product entities from online shops are notsufficiently solved with conventional approaches based on the similarity of attribute …,Proceedings of the VLDB Endowment,2010,226
Load balancing for mapreduce-based entity resolution,Lars Kolb; Andreas Thor; Erhard Rahm,The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. Inthe presence of skewed data; sophisticated redistribution approaches thus becomenecessary to achieve load balancing among all reduce tasks to be executed in parallel. Forthe complex problem of entity resolution; we propose and evaluate two approaches for suchskew handling and load balancing. The approaches support blocking techniques to reducethe search space of entity resolution; utilize a preprocessing MapReduce job to analyze thedata distribution; and distribute the entities of large blocks among multiple reduce tasks. Theevaluation on a real cloud infrastructure shows the value and effectiveness of the proposedload balancing approaches.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,145
Dedoop: efficient deduplication with Hadoop,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract We demonstrate a powerful and easy-to-use tool called Dedoop (< u> De</u>duplication with Ha< u> doop</u>) for MapReduce-based entity resolution (ER) of largedatasets. Dedoop supports a browser-based specification of complex ER workflowsincluding blocking and matching steps as well as the optional use of machine learning forthe automatic generation of match classifiers. Specified workflows are automaticallytranslated into MapReduce jobs for parallel execution on different Hadoop clusters. Toachieve high performance Dedoop supports several advanced load balancing strategies.,Proceedings of the VLDB Endowment,2012,111
Diving behaviour of whale sharks in relation to a predictable food pulse,Rachel T Graham; Callum M Roberts; James CR Smart,We present diving data for four whale sharks in relation to a predictable food pulse (reef fishspawn) and an analysis of the longest continuous fine-resolution diving record for aplanktivorous shark. Fine-resolution pressure data from a recovered pop-up archival satellitetag deployed for 206 days on a whale shark were analysed using the fast Fourier Transformmethod for frequency domain analysis of time-series. The results demonstrated that a free-ranging whale shark displays ultradian; diel and circa-lunar rhythmicity of diving behaviour.Whale sharks dive to over 979.5 m and can tolerate a temperature range of 26.4° C. Thewhale sharks made primarily diurnal deep dives and remained in relatively shallow watersat night. Whale shark diving patterns are influenced by a seasonally predictable food source;with shallower dives made during fish spawning periods.,Journal of the Royal Society Interface,2006,108
Multi-pass sorted neighborhood blocking with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract Cloud infrastructures enable the efficient parallel execution of data-intensive taskssuch as entity resolution on large datasets. We investigate challenges and possiblesolutions of using the MapReduce programming model for parallel entity resolution usingSorting Neighborhood blocking (SN). We propose and evaluate two efficient MapReduce-based implementations for single-and multi-pass SN that either use multiple MapReducejobs or apply a tailored data replication. We also propose an automatic data partitioningapproach for multi-pass SN to achieve load balancing. Our evaluation based on real-worlddatasets shows the high efficiency and effectiveness of the proposed approaches.,Computer Science-Research and Development,2012,105
Citation analysis of database publications,Erhard Rahm; Andreas Thor,Abstract We analyze citation frequencies for two main database conferences (SIGMOD;VLDB) and three database journals (TODS; VLDB Journal; Sigmod Record) over 10 years.The citation data is obtained by integrating and cleaning data from DBLP and GoogleScholar. Our analysis considers different comparative metrics per publication venue; inparticular the total and average number of citations as well as the impact factor which has sofar only been considered for journals. We also determine the most cited papers; authors;author institutions and their countries.,ACM Sigmod Record,2005,99
Convergent validity of bibliometric Google Scholar data in the field of chemistry—Citation counts for papers that were accepted by Angewandte Chemie International...,Lutz Bornmann; Werner Marx; Hermann Schier; Erhard Rahm; Andreas Thor; Hans-Dieter Daniel,Abstract Examining a comprehensive set of papers (n= 1837) that were accepted forpublication by the journal Angewandte Chemie International Edition (one of the primechemistry journals in the world) or rejected by the journal but then published elsewhere; thisstudy tested the extent to which the use of the freely available database Google Scholar(GS) can be expected to yield valid citation counts in the field of chemistry. Analyses ofcitations for the set of papers returned by three fee-based databases–Science CitationIndex; Scopus; and Chemical Abstracts–were compared to the analysis of citations foundusing GS data. Whereas the analyses using citations returned by the three fee-baseddatabases show very similar results; the results of the analysis using GS citation datadiffered greatly from the findings using citations from the fee-based databases. Our study …,Journal of informetrics,2009,93
MOMA-A Mapping-based Object Matching System.,Andreas Thor; Erhard Rahm,Object matching or object consolidation is a crucial task for data integration and datacleaning. It addresses the problem of identifying object instances in data sources referring tothe same real world entity. We propose a flexible framework called MOMA for mapping-based object matching. It allows the construction of match workflows combining the results ofseveral matcher algorithms on both attribute values and contextual information. The outputof a match task is an instance-level mapping that supports information fusion in P2P dataintegration systems and can be re-used for other match tasks. MOMA utilizes furthersemantic mappings of different cardinalities and provides merge and compose operators formapping combination. We propose and evaluate several strategies for both object matchingbetween different sources as well as for duplicate identification within a single data …,CIDR,2007,74
Instance-based matching of large life science ontologies,Toralf Kirsten; Andreas Thor; Erhard Rahm,Abstract Ontologies are heavily used in life sciences so that there is increasing value tomatch different ontologies in order to determine related conceptual categories. We proposea simple yet powerful methodology for instance-based ontology matching which utilizes theassociations between molecular-biological objects and ontologies. The approach can buildon many existing ontology associations for instance objects like sequences and proteinsand thus makes heavy use of available domain knowledge. Furthermore; the approach isflexible and extensible since each instance source with associations to the ontologies ofinterest can contribute to the ontology mapping. We study several approaches to determinethe instance-based similarity of ontology categories. We perform an extensive experimentalevaluation to use protein associations for different species to match between …,International Conference on Data Integration in the Life Sciences,2007,55
Data integration support for mashups,Andreas Thor David Aumueller Erhard Rahm; D Thor; E Aumueller,*,Workshops at the Twenty-Second AAAI Conference on Artificial Intelligence,2007,45
Link prediction for annotation graphs using graph summarization,Andreas Thor; Philip Anderson; Louiqa Raschid; Saket Navlakha; Barna Saha; Samir Khuller; Xiao-Ning Zhang,Abstract Annotation graph datasets are a natural representation of scientific knowledge.They are common in the life sciences where genes or proteins are annotated with controlledvocabulary terms (CV terms) from ontologies. The W3C Linking Open Data (LOD) initiativeand semantic Web technologies are playing a leading role in making such datasets widelyavailable. Scientists can mine these datasets to discover patterns of annotation. Whileontology alignment and integration across datasets has been explored in the context of thesemantic Web; there is no current approach to mine such patterns in annotation graphdatasets. In this paper; we propose a novel approach for link prediction; it is a preliminarytask when discovering more complex patterns. Our prediction is based on a complementarymethodology of graph summarization (GS) and dense subgraphs (DSG). GS can exploit …,International Semantic Web Conference,2011,44
Tailoring entity resolution for matching product offers,Hanna Köpcke; Andreas Thor; Stefan Thomas; Erhard Rahm,Abstract Product matching is a challenging variation of entity resolution to identifyrepresentations and offers referring to the same product. Product matching is highly difficultdue to the broad spectrum of products; many similar but different products; frequentlymissing or wrong values; and the textual nature of product titles and descriptions. Wepropose the use of tailored approaches for product matching based on a preprocessing ofproduct offers to extract and clean new attributes usable for matching. In particular; wepropose a new approach to extract and use so-called product codes to identify products anddistinguish them from similar product variations. We evaluate the effectiveness of theproposed approaches with challenging real-life datasets with product offers from onlineshops. We also show that the UPC information in product offers is often error-prone and …,Proceedings of the 15th International Conference on Extending Database Technology,2012,35
Instance-based matching of hierarchical ontologies.,Andreas Thor; Toralf Kirsten; Erhard Rahm,Abstract: We study an instance-based approach for matching hierarchical ontologies; suchas product catalogs. The motivation for utilizing instances is that metadata-based matchapproaches often suffer from semantic heterogeneity; eg ambiguous concept names;different concept granularities or incomparable categorizations. Our instance-based matchapproach matches categories based on the instances (eg products) assigned to them. Thisway we partly translate the ontology match problem into an instance match problem which isoften easier to solve; especially when instances carry globally unique object ids. Sinceconcepts of different ontologies rarely match 1: 1 we propose to determine correspondencesbetween sets of concepts. We experimentally evaluate the match approaches for realproduct catalogs.,BTW,2007,34
Learning-based approaches for matching web data entities,Hanna Kopcke; Andreas Thor; Erhard Rahm,Entity matching is a key task for data integration and especially challenging for Web data.Effective entity matching typically requires combining several match techniques and findingsuitable configuration parameters; such as similarity thresholds. The authors investigate towhat degree machine learning helps semi-automatically determine suitable match strategieswith a limited amount of manual training effort. They use a new framework; Fever; toevaluate several learning-based approaches for matching different sets of Web data entities.In particular; they study different approaches for training-data selection and how muchtraining is needed to find effective combined match strategies and configurations.,IEEE Internet Computing,2010,29
iFuice-Information Fusion utilizing Instance Correspondences and Peer Mappings.,Erhard Rahm; Andreas Thor; David Aumueller; Hong Hai Do; Nick Golovin; Toralf Kirsten,ABSTRACT We present a new approach to information fusion of web data sources. It isbased on peer-to-peer mappings between sources and utilizes correspondences betweentheir instances. Such correspondences are already available between many sources; eg inthe form of web links; and help combine the information about specific objects and support ahigh quality data fusion. Sources and mappings relate to a domain model to support asemantically focused information fusion. The iFuice architecture incorporates a mappingmediator offering both an interactive and a script-driven; workflow-like access to the sourcesand their mappings. The script programmer can use powerful generic operators to executeand manipulate mappings and their results. The paper motivates the new approach andoutlines the architecture and its main components; in particular the domain model; source …,WebDB,2005,29
Comparative evaluation of entity resolution approaches with fever,Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract We present FEVER; a new evaluation platform for entity resolution approaches. Themodular structure of the FEVER framework supports the incorporation or reconstruction ofmany previously proposed approaches for entity resolution. A distinctive feature of FEVER isthat it not only evaluates traditional measures such as precision and recall but also the effortfor configuring (eg; parameter tuning; training) a good entity resolution approach. FEVERthus strives for a fair comparative evaluation of different approaches by considering both theeffectiveness and configuration effort.,Proceedings of the VLDB Endowment,2009,28
The calculation of the single publication h index and related performance measures: A web application based on Google Scholar data,Andreas Thor; Lutz Bornmann,Purpose–The single publication h index has been introduced by Schubert as the h indexcalculated from the list of citing publications of one single publication. This paper aims tolook at the calculation of the single publication h index and related performance measures.Design/methodology/approach–In this paper a web application is presented where thesingle publication h index and related performance measures (the single publication mindex; h 2 lower; h 2 centre; and h 2 upper) can be automatically calculated for anypublication indexed by Google Scholar. Findings–The use of the application isdemonstrated by means of the citation performance of two publications. Originality/value–Tothe authors' knowledge this web application is the first instrument to automatically calculatethe single publication h index and related performance measures based on Google …,Online Information Review,2011,26
Introducing CitedReferencesExplorer (CRExplorer): A program for reference publication year spectroscopy with cited references standardization,Andreas Thor; Werner Marx; Loet Leydesdorff; Lutz Bornmann,Abstract We introduce a new tool–the CitedReferencesExplorer (CRExplorer; www.crexplorer. net)–which can be used to disambiguate and analyze the cited references (CRs)of a publication set downloaded from the Web of Science (WoS). The tool is especiallysuitable to identify those publications which have been frequently cited by the researchers ina field and thereby to study for example the historical roots of a research field or topic.CRExplorer simplifies the identification of key publications by enabling the user to work withboth a graph for identifying most frequently cited reference publication years (RPYs) and thelist of references for the RPYs which have been most frequently cited. A further focus of theprogram is on the standardization of CRs. It is a serious problem in bibliometrics that thereare several variants of the same CR in the WoS. In this study; CRExplorer is used to study …,Journal of Informetrics,2016,25
Don't match twice: redundancy-free similarity computation with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract To improve the effectiveness of pair-wise similarity computation; state-of-the-artapproaches assign objects to multiple overlapping clusters. This introduces redundant paircomparisons when similar objects share more than one cluster. We propose an approachthat eliminates such redundant comparisons and that can be easily integrated into existingMapReduce implementations. We evaluate the approach on a real cloud infrastructure andshow its effectiveness for all degrees of redundancy.,Proceedings of the Second Workshop on Data Analytics in the Cloud,2013,25
AWESOME: a data warehouse-based system for adaptive website recommendations,Andreas Thor; Erhard Rahm,Abstract Recommendations are crucial for the success of large websites. While there aremany ways to determine recommendations; the relative quality of these recommendersdepends on many factors and is largely unknown. We propose a new classification ofrecommenders and comparatively evaluate their relative quality for a sample web-site. Theevaluation is performed with AWESOME (Adaptive website recommendations); a new datawarehouse-based recommendation system capturing and evaluating user feedback onpresented recommendations. Moreover; we show how AWESOME performs an automaticand adaptive closed-loop website optimization by dynamically selecting the most promisingrecommenders based on continuously measured recommendation feedback. We proposeand evaluate several alternatives for dynamic recommender selection including a …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,24
How do computed ontology mappings evolve?-A case study for life science ontologies,Anika Gross; Michael Hartung; Andreas Thor; Erhard Rahm,Abstract. Mappings between related ontologies are increasingly used to support dataintegration and analysis tasks. Changes in the ontologies also require the adaptation ofontology mappings. So far the evolution of ontology mappings has received little attentionalbeit ontologies change continuously especially in the life sciences. We therefore analyzehow mappings between popular life science ontologies evolve for different matchalgorithms. We also evaluate which semantic ontology changes primarily affect themappings. Our results can be valuable for users working with ontology mappings; eg; onecan learn from past ontology/mapping changes and their correlation to estimate possiblemapping changes if new ontology versions become available.,Joint Workshop on Knowledge Evolution and Ontology Dynamics,2012,22
Dynamic Fusion of Web Data: Beyond Mashups,Erhard Rahm; Andreas Thor; David Aumüller,Page 1. Dynamic Fusion of Web Data: Beyond Mashups Erhard Rahm Andreas Thor; DavidAumüller http://dbs.uni-leipzig.de 24th September; 2007 Top VLDB '97 Pubs: Google Scholar'sTop-5 Page 2. Google Scholar's Top-5 (2) ... more GS quality problems Duplicates due to •Extraction errors (title; authors) • Different titles • Typos (author name) • Heterogeneous venuenames • Missing / additional authors (!) Heterogeneous venue names • How to query for "VLDB'97"? Page 3. Top VLDB'97 Pubs: MS Libra's Result ... similar problems Jennifer Widom receivedher Bachelors degree in 1982 and her Computer Science Ph.D. in 1987. searching for a specificpublication P (get full text) searching for all publications of an author A searching for allpublications of a venue V ☺ Page 4. Top VLDB '97 Publications: Desired Result Top VLDB '97Publications: Desired Result (2) Page 5. Agenda • Motivation …,Proc. of XSym07,2007,21
The application of bibliometrics to research evaluation in the humanities and social sciences: An exploratory study using normalized Google Scholar data for the pub...,Lutz Bornmann; Andreas Thor; Werner Marx; Hermann Schier,Abstract In the humanities and social sciences; bibliometric methods for the assessment ofresearch performance are (so far) less common. This study uses a concrete example in anattempt to evaluate a research institute from the area of social sciences and humanities withthe help of data from Google Scholar (GS). In order to use GS for a bibliometric study; wedeveloped procedures for the normalization of citation impact; building on the procedures ofclassical bibliometrics. In order to test the convergent validity of the normalized citationimpact scores; we calculated normalized scores for a subset of the publications based ondata from the Web of Science (WoS) and Scopus. Even if scores calculated with the help ofGS and the WoS/Scopus are not identical for the different publication types (consideredhere); they are so similar that they result in the same assessment of the institute …,Journal of the Association for Information Science and Technology,2016,20
Finding cross genome patterns in annotation graphs,Joseph Benik; Caren Chang; Louiqa Raschid; Maria-Esther Vidal; Guillermo Palma; Andreas Thor,Abstract Annotation graph datasets are a natural representation of scientific knowledge.They are common in the life sciences where concepts such as genes and proteins areannotated with controlled vocabulary terms from ontologies. Scientists are interested inanalyzing or mining these annotations; in synergy with the literature; to discover patterns.Further; annotated datasets provide an avenue for scientists to explore shared annotationsacross genomes to support cross genome discovery. We present a tool; PAnG (P atterns inAn notation G raphs); that is based on a complementary methodology of graphsummarization and dense subgraphs. The elements of a graph summary correspond to apattern and its visualization can provide an explanation of the underlying knowledge. Wepresent and analyze two distance metrics to identify related concepts in ontologies. We …,International Conference on Data Integration in the Life Sciences,2012,18
Block-based load balancing for entity resolution with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract The effectiveness and scalability of MapReduce-based implementations of complexdata-intensive tasks depend on an even redistribution of data between map and reducetasks. In the presence of skewed data; sophisticated redistribution approaches thus becomenecessary to achieve load balancing among all reduce tasks to be executed in parallel. Forthe complex problem of entity resolution with blocking; we propose BlockSplit; a loadbalancing approach that supports blocking techniques to reduce the search space of entityresolution. The evaluation on a real cloud infrastructure shows the value and effectivenessof the proposed approach.,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,18
From black box to white box at open access journals: predictive validity of manuscript reviewing and editorial decisions at Atmospheric Chemistry and Physics,Lutz Bornmann; Werner Marx; Hermann Schier; Andreas Thor; Hans-Dieter Daniel,Abstract More than 4;500 open access (OA) journals have now become established inscience. But doubts exist about the quality of the manuscript selection process forpublication in these journals. In this study we investigate the quality of the selection processof an OA journal; taking as an example the journal Atmospheric Chemistry and Physics(ACP). ACP is working with a new system of public peer review. We examine the predictivevalidity of the ACP peer review system; namely; whether the process selects the best of themanuscripts submitted. We have data for 1;111 manuscripts that went through the completeACP selection process in the years 2001 to 2006. The predictive validity was investigated onthe basis of citation counts for the later published manuscripts. The results of the citationanalysis confirm the predictive validity of the reviewers' ratings and the editorial decisions …,Research Evaluation,2010,18
Measuring relatedness between scientific entities in annotation datasets,Guillermo Palma; Maria-Esther Vidal; Eric Haag; Louiqa Raschid; Andreas Thor,Abstract Linked Open Data has made available a diversity of scientific collections wherescientists have annotated entities in the datasets with controlled vocabulary terms (CVterms) from ontologies. These semantic annotations encode scientific knowledge which iscaptured in annotation datasets. One can mine these datasets to discover relationships andpatterns between entities. Determining the relatedness (or similarity) between entitiesbecomes a building block for graph pattern mining; eg; identifying drug-drug relationshipscould depend on the similarity of the diseases (conditions) that are associated with eachdrug. Diverse similarity metrics have been proposed in the literature; eg; i) string-similaritymetrics; ii) path-similarity metrics; iii) topological-similarity metrics; all measure relatednessin a given taxonomy or ontology. In this paper; we consider a novel annotation similarity …,Proceedings of the International Conference on Bioinformatics; Computational Biology and Biomedical Informatics,2013,17
Learning-based entity resolution with MapReduce,Lars Kolb; Hanna Köpcke; Andreas Thor; Erhard Rahm,Abstract Entity resolution is a crucial step for data quality and data integration. Learning-based approaches show high effectiveness at the expense of poor efficiency. To reduce thetypically high execution times; we investigate how learning-based entity resolution can berealized in a cloud infrastructure using MapReduce. We propose and evaluate two efficientMapReduce-based strategies for pair-wise similarity computation and classifier applicationon the Cartesian product of two input sources. Our evaluation is based on real-worlddatasets and shows the high efficiency and effectiveness of the proposed approaches.,Proceedings of the third international workshop on Cloud data management,2011,16
Entity search strategies for mashup applications,Stefan Endrullis; Andreas Thor; Erhard Rahm,Programmatic data integration approaches such as mashups have become a viableapproach to dynamically integrate web data at runtime. Key data sources for mashupsinclude entity search engines and hidden databases that need to be queried via source-specific search interfaces or web forms. Current mashups are typically restricted to simplequery approaches such as using keyword search. Such approaches may need a highnumber of queries if many objects have to be found. Furthermore; the effectiveness of thequeries may be limited; ie; they may miss relevant results. We therefore propose moreadvanced search strategies that aim at finding a set of entities with high efficiency and higheffectiveness. Our strategies use different kinds of queries that are determined by source-specific query generators. Furthermore; the queries are selected based on the …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,14
An Evolutionbased Approach for Assessing Ontology Mappings-A Case Study in the Life Sciences.,Andreas Thor; Michael Hartung; Anika Gross; Toralf Kirsten; Erhard Rahm,Abstract: Ontology matching has been widely studied. However; the resulting ontologymappings can be rather unstable when the participating ontologies or utilized secondarysources (eg; instance sources; thesauri) evolve. We propose an evolution-based approachfor assessing ontology mappings by annotating their correspondences by information aboutsimilarity values for past ontology versions. These annotations allow us to assess thestability of correspondences over time and they can thus be used to determine better andmore robust ontology mappings. The approach is generic in that it can be appliedindependently from the utilized match technique. We define different stability measures andshow results of a first evaluation for the life science domain.,BTW,2009,14
Adaptive website recommendations with awesome,Andreas Thor; Nick Golovin; Erhard Rahm,Abstract Recommendations are crucial for the success of large websites. While there aremany ways to determine recommendations; the relative quality of these recommendersdepends on many factors and is largely unknown. We present the architecture andimplementation of AWESOME (Adaptive website recommendations); a data warehouse-based recommendation system. It allows the coordinated use of a large number ofrecommenders to automatically generate website recommendations. Recommendations aredynamically selected by efficient rule-based approaches utilizing continuously measureduser feedback on presented recommendations. AWESOME supports a completely automaticgeneration and optimization of selection rules to minimize website administration overheadand quickly adapt to changing situations. We propose a classification of recommenders …,The VLDB journal,2005,12
Evaluation of query generators for entity search engines,Stefan Endrullis; Andreas Thor; Erhard Rahm,Abstract: Dynamic web applications such as mashups need efficient access to web data thatis only accessible via entity search engines (eg product or publication search engines).However; most current mashup systems and applications only support simple keywordsearches for retrieving data from search engines. We propose the use of more powerfulsearch strategies building on so-called query generators. For a given set of entities querygenerators are able to automatically determine a set of search queries to retrieve theseentities from an entity search engine. We demonstrate the usefulness of query generators foron-demand web data integration and evaluate the effectiveness and efficiency of querygenerators for a challenging real-world integration scenario. Subjects: Databases (cs. DB);Information Retrieval (cs. IR) ACM classes: H. 3.3; H. 3.4 Cite as: arXiv: 1003.4418 [cs. DB …,arXiv preprint arXiv:1003.4418,2010,11
Toward an adaptive String Similarity Measure for Matching Product Offers.,Andreas Thor,Abstract: Product matching aims at identifying different product offers referring to the samereal-world product. Product offers are provided by different merchants and describe productsusing textual attributes such as offer title and description. String similarity measurestherefore play an important role for matching corresponding product offers. In this paper; wepropose an adaptive string similarity measure that automatically adjusts the relevance ofterms for the product matching. This adaptation is done step-by-step during the matchprocess and does not require training data. We demonstrate that this approach improves thematch quality in comparison to the generic TFIDF string similarity measure.,GI Jahrestagung (1),2010,9
Automatische mapping-verarbeitung auf webdaten,Andreas Thor,Innerhalb einer Website können Korrespondenzen zB als Webseitenempfehlungen;sogenannte Recommendations; verwendet werden. Ausgehend von den vielen in derLiteratur vorgeschlagenen Algorithmen zur Berechnung von Recommendations;sogenannte Recommender; stellt sich das Problem; welcher Recommender für welchenNutzer unter welchen Umständen am nützlichsten ist. Durch eine gezielte und optimierteAuswahl sollen dabei die besten Recommendations bestimmt werden. Mit AWESOME [4; 5]wird dazu ein Ansatz vorgestellt; der eine solche adaptive Bestimmung vonRecommendations zulässt. Durch die Aufzeichnung und Auswertung von Nutzer-Feedbackkönnen die Empfehlungen den Nutzerinteressen angepasst werden; so dass eineSteigerung der Recommendation-Qualität ermöglicht wird. Innerhalb der Arbeit werden …,*,2008,9
Mashup-Werkzeuge zur Ad-hoc-Datenintegration im Web.,David Aumüller; Andreas Thor,Mashup-Werkzeuge ermöglichen eine einfache Erstellung von Mashups; dh vonWebapplikationen; die Informationen aus verschiedenen Quellen kombinieren und inintegrierter Form wieder selbst als Datenquelle oder Dienst anbieten. Dieser Artikel gibteinen Überblick über den aktuellen Stand der Technik von Mashups und einschlägigenWerkzeugen und geht dabei speziell auf die Möglichkeiten zur Datenintegration ein.Eingangs werden Mashups und typische Anwendungsszenarien vorgestellt um sodann diewesentlichen funktionalen Komponenten einer Mashup-Anwendung zu charakterisieren.Dabei wird der Einsatz von Mashups zur Datenintegration betrachtet und mit bestehendenklassischen Datenintegrationsansätzen verglichen. Anschließend wird eine Reihe vonMashup-Werkzeugen vorgestellt und kategorisiert; wobei Tools zur Modellierung von …,Datenbank-Spektrum,2008,8
Which early works are cited most frequently in climate change research literature? A bibliometric approach based on Reference Publication Year Spectroscopy,Werner Marx; Robin Haunschild; Andreas Thor; Lutz Bornmann,Abstract This bibliometric analysis focuses on the general history of climate change researchand; more specifically; on the discovery of the greenhouse effect. First; the ReferencePublication Year Spectroscopy (RPYS) is applied to a large publication set on climatechange of 222;060 papers published between 1980 and 2014. The references cited thereinwere extracted and analyzed with regard to publications; which are cited most frequently.Second; a new method for establishing a more subject-specific publication set for applyingRPYS (based on the co-citations of a marker reference) is proposed (RPYS-CO). The RPYSof the climate change literature focuses on the history of climate change research in total.We identified 35 highly-cited publications across all disciplines; which include fundamentalearly scientific works of the nineteenth century (with a weak connection to climate change …,Scientometrics,2017,7
Wetsuit: an efficient mashup tool for searching and fusing web entities,Stefan Endrullis; Andreas Thor; Erhard Rahm,Abstract We demonstrate a new powerful mashup tool called WETSUIT (Web EnTity Searchand fUsIon Tool) to search and integrate web data from diverse sources and domain-specificentity search engines. WETSUIT supports adaptive search strategies to query sets ofrelevant entities with a minimum of communication overhead. Mashups can be composedusing a set of high-level operators based on the Java-compatible language Scala. Theoperator implementation supports a high degree of parallel processing; in particular astreaming of entities between all data transformation operations facilitating a fastpresentation of intermediate results. WETSUIT has already been applied to solvechallenging integration tasks from different domains.,Proceedings of the VLDB Endowment,2012,7
PAnG: finding patterns in annotation graphs,Philip Anderson; Andreas Thor; Joseph Benik; Louiqa Raschid; Maria Esther Vidal,Abstract Annotation graph datasets are a natural representation of scientific knowledge.They are common in the life sciences and health sciences; where concepts such as genes;proteins or clinical trials are annotated with controlled vocabulary terms from ontologies. Wepresent a tool; PAnG (Patterns in Annotation Graphs); that is based on a complementarymethodology of graph summarization and dense subgraphs. The elements of a graphsummary correspond to a pattern and its visualization can provide an explanation of theunderlying knowledge. Scientists can use PAnG to develop hypotheses and for exploration.,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,7
New features of CitedReferencesExplorer (CRExplorer),Andreas Thor; Werner Marx; Loet Leydesdorff; Lutz Bornmann,Abstract Recently; we introduced the CitedReferencesExplorer (CRExplorer at http://www.crexplorer. net). The program was primarily developed to identify those publications in afield; a topic or by a researcher which have been frequently cited. This Letter to the Editordescribes the features of the new release of the CRExplorer.,Scientometrics,2016,6
Cloudfuice: A flexible cloud-based data integration system,Andreas Thor; Erhard Rahm,Abstract The advent of cloud computing technologies shows great promise for webengineering and facilitates the development of flexible; distributed; and scalable webapplications. Data integration can notably benefit from cloud computing because integratingweb data is usually an expensive task. This paper introduces CloudFuice; a data integrationsystem that follows a mashup-like specification of advanced dataflows for data integration.CloudFuice's task-based execution approach allows for an efficient; asynchronous; andparallel execution of dataflows in the cloud and utilizes recent cloud-based web engineeringinstruments. We demonstrate and evaluate CloudFuice's applicability for mashup-baseddata integration in the cloud with the help of a first prototype implementation.,International Conference on Web Engineering,2011,6
Parallel Sorted Neighborhood Blocking with MapReduce,Lars Kolb; Andreas Thor; Erhard Rahm,Abstract: Cloud infrastructures enable the efficient parallel execution of data-intensive taskssuch as entity resolution on large datasets. We investigate challenges and possiblesolutions of using the MapReduce programming model for parallel entity resolution. Inparticular; we propose and evaluate two MapReduce-based implementations for SortedNeighborhood blocking that either use multiple MapReduce jobs or apply a tailored datareplication. Subjects: Distributed; Parallel; and Cluster Computing (cs. DC) Cite as: arXiv:1010.3053 [cs. DC](or arXiv: 1010.3053 v1 [cs. DC] for this version) Submission historyFrom: Lars Kolb [view email][v1] Fri; 15 Oct 2010 00: 28: 44 GMT (268kb; D),arXiv preprint arXiv:1010.3053,2010,6
Referenced Publication Year Spectroscopy (RPYS) and Algorithmic Historiography: The Bibliometric Reconstruction of Andr\'as Schubert's {\OE} uvre,Loet Leydesdorff; Lutz Bornmann; Jordan Comins; Werner Marx; Andreas Thor,Abstract: Referenced Publication Year Spectroscopy (RPYS) was recently introduced as amethod to analyze the historical roots of research fields and groups or institutions. RPYSmaps the distribution of the publication years of the cited references in a document set. Inthis study; we apply this methodology to the {\oe} uvre of an individual researcher on theoccasion of a Festschrift for Andr\'as Schubert's 70th birthday. We discuss the differentoptions of RPYS in relation to one another (eg Multi-RPYS); and in relation to the longer-term research program of algorithmic historiography (eg; HistCite) based on Schubert'spublications (n= 172) and cited references therein as a bibliographic domain inscientometrics. Main path analysis and Multi-RPYS of the citation network are used to showthe changes and continuities in Schubert's intellectual career. Diachronic and static …,arXiv preprint arXiv:1604.04705,2016,3
Determining similarity of scientific entities in annotation datasets,Guillermo Palma; Maria-Esther Vidal; Eric Haag; Louiqa Raschid; Andreas Thor,Abstract Linked Open Data initiatives have made available a diversity of scientific collectionswhere scientists have annotated entities in the datasets with controlled vocabulary termsfrom ontologies. Annotations encode scientific knowledge; which is captured in annotationdatasets. Determining relatedness between annotated entities becomes a building block forpattern mining; eg identifying drug–drug relationships may depend on the similarity of thetargets that interact with each drug. A diversity of similarity measures has been proposed inthe literature to compute relatedness between a pair of entities. Each measure exploits someknowledge including the name; function; relationships with other entities; taxonomicneighborhood and semantic knowledge. We propose a novel general-purpose annotationsimilarity measure called 'AnnSim'that measures the relatedness between two entities …,Database,2015,2
Exploiting Semantics from Ontologies and Shared Annotations to Find Patterns in Annotated Linked Open Data.,Guillermo Palma; Maria-Esther Vidal; Louiqa Raschid; Andreas Thor,Abstract. Linked Open Data initiatives have made available a diversity of collections thatdomain experts have annotated with controlled vocabulary terms from ontologies. Thechallenge is to explore these rich and complex annotated datasets; together with the domainsemantics captured within ontologies; to discover patterns of annotations across multipleconcepts that may lead to potential discoveries. We identify an annotation signaturebetween a pair of concepts based on shared annotations and ontological relatedness.Formally; an annotation signature is a partitioning of the edges that represent therelationships between shared annotations. A clustering algorithm named AnnSigClusteringis proposed to generate annotation signatures. Evaluation results over drug and genedatasets demonstrate the effectiveness of using annotation signatures to find patterns.,LISC@ ISWC,2013,2
How do Ontology Mappings Change in the Life Sciences?,Anika Gross; Michael Hartung; Andreas Thor; Erhard Rahm,Abstract: Mappings between related ontologies are increasingly used to support dataintegration and analysis tasks. Changes in the ontologies also require the adaptation ofontology mappings. So far the evolution of ontology mappings has received little attentionalbeit ontologies change continuously especially in the life sciences. We therefore analyzehow mappings between popular life science ontologies evolve for different matchalgorithms. We also evaluate which semantic ontology changes primarily affect themappings. We further investigate alternatives to predict or estimate the degree of futuremapping changes based on previous ontology and mapping transitions.,arXiv preprint arXiv:1204.2731,2012,2
Identifying seminal works most important for research fields: Software for the Reference Publication Year Spectroscopy (RPYS),Lutz Bornmann; Andreas Thor; Werner Marx; Loet Leydesdorff,Reference Publication Year Spectroscopy (RPYS) was proposed by Marx; Bornmann; Barth;and Leydesdorff (2014;[18]) to identify seminal publications in a research field which aremost important in a historical context. We refined our RPYS toolbox by adding some featuresto the existing programs and we developed two new routines. First; a direct comparison ofthe results of different RPYSs is now possible; because the software transforms the results ofthe RPYS into percentiles for standardization. Second; we added routines that facilitate theuser with retrieving the most-cited publications in specific years indicated by peaks in thespectrograms. Cited references can be aggregated across misspellings and variants. Forthis paper; two examples from the humanities and natural sciences are provided todemonstrate the functionalities and results of the programs. A more technical description …,Collnet Journal of Scientometrics and Information Management,2016,1
Exploiting Semantics from Ontologies and Shared Annotations to Partition Linked Data,Guillermo Palma; Maria-Esther Vidal; Louiqa Raschid; Andreas Thor,Abstract Linked Open Data initiatives have made available a diversity of collections thatdomain experts have annotated with controlled vocabulary terms from ontologies. Weidentify annotation signatures of linked data that associate semantically similar concepts;where similarity is measured in terms of shared annotations and ontological relatedness.Formally; an annotation signature is a partition or clustering of the links that represent therelationships between shared annotations. A clustering algorithm named AnnSigClusteringis proposed to generate annotation signatures. Evaluation results over drug and diseasedatasets demonstrate the effectiveness of using annotation signatures to identify patternsamong entities in the same cluster of a signature.,International Conference on Data Integration in the Life Sciences,2014,1
Cloud-Technologien in der Hochschullehre–Pflicht oder Kür?,Stefanie Scherzinger; Andreas Thor,Zusammenfassung Ein eigenes Themenheft zum Datenmanagement in der Cloud dient unsals Anlass; die Präsenz von Cloud-Themen in der akademischen Datenbanklehre zuerfassen. In diesem Artikel geben wir die Ergebnisse einer Umfrage innerhalb derFachgruppe Datenbanksysteme durch den Arbeitskreis Datenmanagement in der Cloudwieder. Dozentinnen und Dozenten von über zwanzig Hochschulen nahmen an derUmfrage teil. Es zeigt sich deutlich; dass sich das Thema „Cloud “in der Hochschullehrezunehmend etabliert; jedoch überwiegend als ergänzendes Angebot; und seltener in dergrundständigen Lehre verankert. Wir fassen die Ergebnisse unserer Umfrage zusammenund wagen Deutungsversuche.,Datenbank-Spektrum,2014,1
Exploration using signatures in annotation graph datasets,Louiqa Raschid; Guillermo Palma; Maria-Esther Vidal; Andreas Thor,Abstract The widespread development and adoption of ontologies to capture semanticdomain knowledge and the growth of annotation graph datasets has created manyopportunities for large scale Linked Data analytics. Ontologies are developed by domainexperts to capture knowledge specific to some domain. The biomedical community hastaken the lead in these activities. Every model organism database has genes and proteinsthat are widely annotated; eg; with controlled vocabulary (CV) terms from the Gene Ontology(GO). The NCI Thesaurus (NCIt) version 12.05 d has 93;788 terms and the LinkedCT datasetof clinical trial results circa September 2011 includes 142;207 drugs or interventions;167;012 conditions or diseases; and 166;890 links to DBPedia; DrugBank and Diseasome.At the opposite end of the domain spectrum; the Financial Industry Business Ontology …,*,2013,1
Data-warehouse-basierte architektur für adaptive online-recommendations,Andreas Thor; Erhard Rahm,Kurzfassung Aufgezeichnetes Nutzungsverhalten von Websites stellt Wissen bereit; wieNutzer innerhalb der Website navigieren. Mittels effizientem Wissensmanagement könnendie Nutzer bei der Navigation zB durch den Einsatz von Online-Recommendation-Systemenunterstützt werden. Die vorgestellte Arbeit zeigt eine Architektur; wie ein solchesRecommendation-System flexibel und adaptiv gestaltet werden kann.,Content-und Wissensmanagement,2003,1
Identifying single influential publications in a research field: New analysis opportunities of the CRExplorer,Andreas Thor; Lutz Bornmann; Werner Marx; Rüdiger Mutz,Abstract: Reference Publication Year Spectroscopy (RPYS) has been developed foridentifying the cited references (CRs) with the greatest influence in a given paper set (mostlysets of papers on certain topics or fields). The program CRExplorer (see www. crexplorer.net) was specifically developed by Thor; Marx; Leydesdorff; and Bornmann (2016a; 2016b)for applying RPYS to publication sets downloaded from Scopus or Web of Science. In thisstudy; we present some advanced methods which have been newly developed forCRExplorer. These methods are able to identify and characterize the CRs which have beeninfluential across a longer period (many citing years). The new methods are demonstrated inthis study using all the papers published in Scientometrics between 1978 and 2016. Theindicators N_TOP50; N_TOP25; and N_TOP10 can be used to identify those CRswhich …,arXiv preprint arXiv:1801.08720,2018,*
Datenbanksysteme II SS 2017–Übungsblatt 3,Erhard Rahm; M Junghanns; V Christen,Page 1. Universität Leipzig; Institut für Informatik Abteilung Datenbanken Prof. Dr. E. Rahm;V. Christen; M. Franke Ausgabe: Besprechungen: 12.06.2017 19.06.2017 21.06.201726.06.2017 28.06.2017 Datenbanksysteme II SS 2017 – Übungsblatt 5 1. Aufgabe(SQL:2003: Tabellendefinitionen; Anfragen) Für eine Universitätsanwendung seien uafolgende SQL:2003-Typen definiert: CREATE TYPE PersonT (Name VARCHAR (40); FakREF (FakultaetT)); CREATE TYPE StudentT UNDER PersonT (MatNr INT; HauptfachVARCHAR (40); Nebenfach VARCHAR (40); Bachelor BOOLEAN); CREATE TYPE ProfTUNDER PersonT (Buero BueroT; Besoldung CHAR(2); DRTitel VARCHAR(20);ForschGebiete VARCHAR(20) ARRAY[5]); CREATE TYPE BueroT (GebaeudeVARCHAR(40); Stockwerk INTEGER; Nummer INTEGER; Telefon INTEGER); …,*,2017,*
Datenbanksysteme für Business; Technologie und Web (BTW 2017); 17. Fachtagung des GI-Fachbereichs" Datenbanken und Informationssysteme"(DBIS); 6.-10. M...,Bernhard Mitschang; Norbert Ritter; Holger Schwarz; Meike Klettke; Andreas Thor; Oliver Kopp; Matthias Wieland,Mitschang; Bernhard and Ritter; Norbert and Schwarz; Holger and Klettke; Meike and Thor; Andreasand Kopp; Oliver and Wieland; Matthias (2017) Datenbanksysteme für Business; Technologieund Web (BTW 2017); 17. Fachtagung des GI-Fachbereichs "Datenbanken undInformationssysteme" (DBIS); 6.-10. März 2017; Stuttgart; Germany; Workshopband. LNI; P-266. GI … Full text not available from this repository … Official URL: https://www.gi.de/service/publikationen/lni/gi-edi … Dbis Repository is powered by EPrints 3 which is developed by the School ofElectronics and Computer Science at the University of Southampton. More information and softwarecredits.,*,2017,*
DIAL–Ein BigBlueButton-basiertes System für interaktive Live-Übertragungen von Vorlesungen,Luise Kaufmann; Tobias Welz; Andreas Thor,Dieser Beitrag präsentiert DIAL (Distributed InterActive Lecture); ein BigBlueButtonbasiertesSystem für interaktive Live-Übertragungen von Vorlesungen. DIAL erweitert dasKonferenzsystem BigBlueButton derart; dass Studierenden Zugang zum Chat-undUmfragesystem aus dem Stand-By ihres Endgerätes heraus ermöglicht wird; dh ohne dieNotwendigkeit einer permanenten Verbindung. Damit ermöglicht DIAL Teilnehmern einerLive-Übertragung die einfache und direkte Interaktion analog zum virtuellen Klassenzimmer.Der Beitrag beschreibt die Architektur und die prototypische Implementation von DIAL undpräsentiert Evaluationsergebnisse aus der praktischen Anwendung an der Hochschule fürTelekommunikation Leipzig.,Bildungsräume 2017,2017,*
Further steps in integrating the platforms of WoS and Scopus: Historiography with HistCite™ and main-path analysis,Loet Leydesdorff; Andreas Thor; Lutz Bornmann,The program HistCite™ enables an analyst to identify significant works on a given topicusing the citation links between them diachronically. However; using Scopus data fordrawing historiograms with HistCite™ has hitherto been a problem. In the new version of theprogram CRExplorer; one can translate citation data from Scopus to WoS formats (or viceversa) and then import the data into HistCite™. In this brief communication; we demonstratethese options using the papers of Eugene Garfield (1925-2017) in Scopus for main-pathanalysis. The two historiograms are considerably different: unlike the WoS set; thenetworked connections between the time lines are sparse in the representation of theScopus data; the secondary documents (eg; editorials in Current Contents) not processed inScopus; but included in WoS enrich the representation. Furthermore; HistCite™ has an …,El profesional de la información (EPI),2017,*
Digitalisierte Hochschuldidaktik: Qualitätssicherung von Prüfungen mit dem E-Assessment-Literacy-Tool EAs. LiT,Andreas Thor; Norbert Pengel; Heinz-Werner Wollersheim,Die Formulierung von Learning Outcomes und deren Transparenz gegenüberStudierenden ist Grundlage für eigenverantwortliche Lernprozesse undkompetenzorientierte Prüfungen (Constructive Alignment). Das in diesem Beitragpräsentierte E-Assessment-Literacy-Tool (EAs. LiT) unterstützt hochschuldidaktischfundiert bei der Formulierung von Learning Outcomes; der darauf basierenden Erstellungund Begutachtung von Aufgaben sowie der kriterienbasierten semi-automatischenZusammenstellung gleichwertiger E-Prüfungen.,Bildungsräume 2017,2017,*
Datenbanksysteme für Business; Technologie und Web (BTW 2015)-Workshopband,Norbert Ritter; Andreas Henrich; Wolfgang Lehner; Andreas Thor; Steffen Friedrich; Wolfram Wingerath,*,*,2015,*
Autoshard-a Java object mapper (not only) for hot spot data objects in nosql data stores,Stefanie Scherzinger; Andreas Thor,We demonstrate AutoShard; a ready-to-use object mapper for Java applications runningagainst NoSQL data stores. AutoShard's unique feature is its capability to gracefully shardhot spot data objects that are suffering under concurrent writes. By sharding data on thelevel of the logical schema; scalability bottlenecks due to write contention can be effectivelyavoided. Using AutoShard; developers can easily employ sharding in their web applicationby adding minimally intrusive annotations to their code. Our live experiments show thesignificant impact of sharding on both the write throughput and the execution time.,Datenbanksysteme für Business; Technologie und Web (BTW 2015),2015,*
Mining patterns from clinical trial annotated datasets by exploiting the NCI thesaurus,Joseph Benik; Guillermo Palma; Louiqa Raschid; Andreas Thor; Maria-Esther Vidal,Abstract Annotations of clinical trials with controlled vocabularies of drugs and diseases;encode scientific knowledge that can be mined to discover relationships between scientificconcepts. We present PAnG (Patterns in Annotation Graphs); a tool that relies on densesubgraphs; graph summarization and taxonomic distance metrics; computed using the NCIThesaurus; to identify patterns.,Proceedings of the 2012th International Conference on Posters & Demonstrations Track-Volume 914,2012,*
Google Megastore-eine skalierbare relationale Datenbank,Falk Stehmann; Andreas Thor,Der Stellenwert von Datenbanken in der heutigen Informationstechnologie undDienstleistungsbranche steigt ständig. Durch die zunehmende Migration vonDesktopanwendungen in die Cloud und die Verbreitung von sozialen Netzwerken erhöhtsich das Datenaufkommen sehr stark. Um in dem schnell wachsenden Markt des User-created Content mithalten zu können; ist die rasche Entwicklung von neuen Featuresentscheidend. Deshalb werden nicht nur die Anforderungen an Skalierbarkeit undPerformanz der Datenbanken immer größer; sondern sie sollen ebenso einebenutzerfreundliche Anwendungsentwicklung ermöglichen. Diese Eigenschaften zuvereinen bildet eine Herausforderung für das Design moderner Datenbanken. Zu Beginndieser Arbeit wird ein allgemeiner Ansatz vorgestellt; wie dieses Problem gelöst werden …,*,2012,*
AWESOME–A Data Warehouse-based System for Adaptive,Andreas Thor; Erhard Rahm,Abstract Recommendations are crucial for the success of large websites. While there aremany ways to determine recommendations; the relative quality of these recommendersdepends on many factors and is largely unknown. We propose a new classification ofrecommenders and comparatively evaluate their relative quality for a sample website. Theevaluation is performed with AWESOME (Adaptive website recommendations); a new datawarehouse-based recommendation system capturing and evaluating user feedback onpresented recommendations. Moreover; we show how AWESOME performs an automaticand adaptive closed-loop website optimization by dynamically selecting the most promisingrecommenders based on continuously measured recommendation feedback. We proposeand evaluate several alternatives for dynamic recommender selection including a …,Proceedings 2004 VLDB Conference: The 30th International Conference on Very Large Databases (VLDB),2004,*
Digitalisierte Hochschuldidaktik: Technologische Infrastruktu-ren für kompetenzorientierte E-Assessments,Norbert Pengel; Andreas Thor; Peter Seifert; Heinz-Werner Wollersheim,Zusammenfassung In diesem Beitrag werden mit dem E-Assessment-Literacy-Tool EAs. LiTund dem Peer-Assessment-Tool PAssT! zwei plattformunabhängige Werkzeuge präsentiert;die hochschuldidaktisch bereits bekannte Verfahren workflowbasiert abbilden und zurEtablierung hochschulübergreifender Qualitätsstandards im Bereich E-Assessmentbeitragen sollen. EAs. LiT unterstützt bei der Formulierung von Learning Outcomes(Constructive Alignment); der darauf basierenden Erstellung und Begutachtung vonAufgaben sowie der kriterienbasierten semi-automatischen Zusammenstellunggleichwertiger E-Assessments. PAssT! bildet für Lernformate; in denen forschungs-undarbeitsmethodische Kompetenzen sowie der Erwerb kollaborativer und kommunikativerKompetenzen im Hinblick auf wissenschaftliches Arbeiten zentral sind; den Workflow …,*,*,*
Abteilung Datenbanken,S Endrullis Wartner; E Peukert; S Raunich; D Aumüller; A Thor; S Kitschke; P Arnold; T Gröger Student,Die Verbreitung von Ontologien und Taxonomien in vielen Domänen führt zu einemwachsenden Bedürfnis; diese zu integrieren. Das Ziel der Ontologie-Integration ist dasZusammenfügen von mindestens zwei Ontologien (mergen genannt); um eine einheitlicheAnsicht auf diese zu bieten; wobei alle Informationen der Ausgangsontologien erhaltenbleiben sollen. Es wurde ein neuartiger Taxonomie-Merging-Algorithmus entwickelt; der alsEingabe zwei Taxonomien und ein Äquivalenz-Mapping zwischen diesen verarbeitet undautomatisch eine integrierte Taxonomie erzeugt. Der Ansatz wird durch die Zieltaxonomiebestimmt (target-driven); das heißt; dass die Ausgangstaxonomie in diese eingefügt wirdund dabei die Struktur der Zielontologie so weit wie möglich bewahrt wird. Es wurde darüberhinaus untersucht; inwieweit man den Algorithmus um zusätzliche Informationen; wie zB …,*,*,*
Kapitel 6: Schemamanagement,Andreas Thor,Page 1. 1 Datenintegration Datenintegration Kapitel 6: Schemamanagement Andreas ThorSommersemester 2008 Universität Leipzig Institut für Informatik http://dbs.uni-leipzig.de 2 Inhalt •Schemaintegration – Erzeugung eines globalen / integrierten Schemas • Schema Matching –Identifikation semantischer Korrespondenzen zwischen Schemata • Schema Mapping – Erstellungvon Transformationsanfragen auf Basis von Schema-Matching-Ergebnis • Model Management –Generische Manipulation von Metadaten Page 2. 3 Schema-Integration; -Matching; -Mapping •Integration der Metadaten (Schemata) ist Voraussetzung für Integration der Instanzdaten •Schema-Integration; -Matching und -Mapping wichtige Teilaspekte – Begriff “Schema Mapping”überladen (→ Details später) Schema 1 Schema 2 Mapping / Korrespondenzen Datenbank 1Datenbank 2 Transformationsregeln Integriertes Schema …,*,*,*
Kapitel 4: Architekturen von Integrationssystemen,Andreas Thor,– Welches sind die Topkunden? Welches sind die Topfilialen? – Welche Produkte hatten imletzten Jahr im Bereich Leipzig einen Umsatzrückgang um mehr als 10%? – Haben Fillialeneinen höheren Umsatz; die gemeinsam gekaufte Produkte zusammen stellen … •Problem: viele Unternehmen haben Unmengen an Daten; ohne daraus … • Data Warehouseist eine für Analysezwecke optimierte zentrale … Datenbank; die Daten aus mehreren; ia heterogenenQuellen … • A data warehouse is a subject-oriented; integrated; non-volatile; and time … –Subject-oriented: Verkäufe; Personen; Produkte; etc. (nicht task-orientied) … – Non-Volatile:Hält Daten unverändert über die Zeit … – Time-Variant: Vergleich von Daten über die Zeit …[In96] WH Inmon; Building the Data Warehouse; 2nd ed.; John Wiley 1996 … EinfacheQueries; Primärschlüsselzugriff; Schnelle Abfolgen von Selects/inserts/updates/deletes,*,*,*
Content-und Knowledge-Management in Peer-to-Peer-Systemen,Christian Helmchen; Andreas Thor,Ein Caddy arbeitet seit mehreren Jahren in einem Golfklub. In dieser Zeit hat er vielErfahrung gesammelt und kann somit den Golfern nützliche Tips an dem einen oderanderen Loch geben. Dies führt indirekt wiederum zu einer leichten Umsatzsteigerung desGolfklubs; da der Golfer bessere Ergebnisse erzielt und den Klub öfter besucht odervielleicht sogar ein paar Freunde mitbringt. Somit profitieren alle Beteiligten vom Wissen desCaddys. Bei einem einzelnen Caddy kann man zwar noch nicht von Knowlegde-Management sprechen; wenn aber alle Caddys des Golfklubs ihr Wissen untereinander aufeine festgelegte Art und Weise (ein Schwarzes Brett oder eine regelmäßige Besprechung)austauschen; dann kann man sagen; dass durch dieses Knowlegde-Management der Klub;die Caddys sowie auch die Golfer gleichermaßen profitieren.[CXO03],*,*,*
Kontextbasierte Datenintegration mit iFuice,Andreas Thor,Page 1. Kontextbasierte Datenintegration mit iFuice Andreas Thor http://dbs.uni-leipzig.de Page2. 2 Motivation ▪ Datenintegration: Finden gleicher Instanzen in verschiedenen Datenquellen →same-Mappings ▪ Beispiel: ▪ Konferenzen ▪ DBLP-ACM Konferenzname Jahr NummerierungPage 3. 3 Beispiel: Konferenzen ▪ Domänenspezifische Matcher ▪ zT aufwändig zu erstellen ▪Matching durch Verwendung weiterer Mappings ▪ Verwendung weiterer Mappings ▪ Bsp:Publikationen sind einfacher zu matchen Page 4. 4 Agenda ▪ Motivation ▪ iFuice ▪ Kontext ▪iFuice-Erweiterungen ▪ Joint Distribution ▪ iFuice-Matcher ▪ Beispiel: Neighbourhood-Matcher ▪Zusammenfassung & Ausblick Page 5. 5 Metadatenmodell Author Publication ConferenceAuthPub PubAuth PubConf ConfPub CoAuthor Domänen-Modell LDS PDS mapping (same: )Legend ▪ Logische Datenquelle (LDS) = Physische Datenquelle (PDS) + Objekttyp …,*,*,*
AutoShard–Declaratively Managing Hot Spot Data Objects in NoSQL Document Stores,Stefanie Scherzinger; Andreas Thor,ABSTRACT NoSQL document stores are becoming increasingly popular as backends inweb development. Not only do they scale out to large volumes of data; many systems areeven custom-tailored for this domain: NoSQL document stores like Google Cloud Datastorehave been designed to support massively parallel reads; and even guarantee strongconsistency in updating single data objects. However; strongly consistent updates cannot beimplemented arbitrarily fast in large-scale distributed systems. Consequently; data objectsthat experience high-frequent writes can turn into severe performance bottlenecks. In thispaper; we present AutoShard; a ready-to-use object mapper for Java applications runningagainst NoSQL document stores. AutoShard's unique feature is its capability to gracefullyshard hot spot data objects to avoid write contention. Using AutoShard; developers can …,*,*,*
Datenintegration,Michael Hartung; Andreas Thor,Page 1. 1 Datenintegration Datenintegration Kapitel 5: Anfrageverarbeitung Michael Hartungin Vertretung von Dr. Andreas Thor Wintersemester 2010/11 Universität Leipzig Institut fürInformatik http://dbs.uni-leipzig.de Page 2. 2 Inhalt • Multidatenbanksprachen am Beispiel vonSchemaSQL – Grundlegende Syntax inklusive Zugriff auf Metadaten – Horizontale Aggregationund dynamische Umstrukturierung • Global-as-View (GaV) und Local-as-View (LaV) – GaV:Modellierung und Anfragebearbeitung – LaV: Modellierung – Vergleich • LaV-Anfragebearbeitung– Query Containment – Answering Queries using Views – Quellen mit beschränktenAnfragemöglichkeiten Page 3. 3 Enge vs. lose Kopplung • Lose Kopplung – Kein festes Schema •Nutzer müssen Semantik der Quellen kennen • Integrierte Sichten helfen –Multidatenbanksprachen • Enge Kopplung – Globales / integriertes / föderiertes Schema …,*,*,*
Cloud Data Management,Andreas Thor,Page 1. 1 Cloud Data Management Kapitel 3: Cloud Data Stores Dr. Andreas ThorSommersemester 2011 Universität Leipzig Institut für Informatik http://dbs.uni-leipzig.de Page2. 2 Inhaltsverzeichnis • Datastores – CAP-Theorem; ACID-Eigenschaften; BASE-Ansatz –Kategorisierung; Eigenschaften • Key-Value Stores – File/Object Storage Services – Beispiele:Microsoft Azure Storage; Amazon S3 / Dynamo • Document Stores – Beispiele: MongoDB;CouchDB Page 3. 3 Motivation • Relationales Schema zu starr für viele Webanwendungen –Evolution: Änderung der Webanwendung führt meist zu Schemaänderung – Datenstruktur:Heterogene Informationsarten führen zu grossen; unübersichtlichen Schemas (ua einfachemengenwertige Attribute zB Tags) – Impedence Mismatch – Anfrage: Wartbarkeit komplexerSQL-Anfragen (viele Joins) • Vorteile relationaler Modellierung spielen geringere Rolle …,*,*,*
