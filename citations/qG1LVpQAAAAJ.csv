Probabilistically Bounded Staleness for Practical Partial Quorums,Peter Bailis; Shivaram Venkataraman; Michael J. Franklin; Joseph M. Hellerstein; Ion Stoica,Abstract Data store replication results in a fundamental trade-off between operation latencyand data consistency. In this paper; we examine this trade-off in the context of quorum-replicated data stores. Under partial; or non-strict quorum replication; a data store waits forresponses from a subset of replicas before answering a query; without guaranteeing thatread and write replica sets intersect. As deployed in practice; these configurations provideonly basic eventual consistency guarantees; with no limit to the recency of data returned.However; anecdotally; partial quorums are often" good enough" for practitioners given theirlatency benefits. In this work; we explain why partial quorums are regularly acceptable inpractice; analyzing both the staleness of data they return and the latency benefits they offer.We introduce Probabilistically Bounded Staleness (PBS) consistency; which provides …,Proceedings of the VLDB Endowment (PVLDB 2012),2012,156
Eventual consistency today: Limitations; extensions; and beyond,Peter Bailis; Ali Ghodsi,In a July 2000 conference keynote; Eric Brewer; now VP of engineering at Google and aprofessor at the University of California; Berkeley; publicly postulated the CAP (consistency;availability; and partition tolerance) theorem; which would change the landscape of howdistributed storage systems were architected. 8 Brewer's conjecture—based on hisexperiences building infrastructure for some of the first Internet search engines at Inktomi—states that distributed systems requiring alwayson; highly available operation cannotguarantee the illusion of coherent; consistent single-system operation in the presence ofnetwork partitions; which cut communication between active servers. Brewer's conjectureproved prescient: in the following decade; with the continued rise of large-scale Internetservices; distributed-system architects frequently dropped “strong” guarantees in favor of …,Queue,2013,153
Bolt-on causal consistency,Peter Bailis; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract We consider the problem of separating consistency-related safety properties fromavailability and durability in distributed data stores via the application of a" bolt-on" shimlayer that upgrades the safety of an underlying general-purpose data store. This shimprovides the same consistency guarantees atop a wide range of widely deployed but ofteninflexible stores. As causal consistency is one of the strongest consistency models thatremain available during system partitions; we develop a shim layer that upgrades eventuallyconsistent stores to provide convergent causal consistency. Accordingly; we leverage widelydeployed eventually consistent infrastructure as a common substrate for providing causalguarantees. We describe algorithms and shim implementations that are suitable for a largeclass of application-level causality relationships and evaluate our techniques using an …,SIGMOD 2013,2013,143
Highly Available Transactions: Virtues and Limitations,Peter Bailis; Aaron Davidson; Alan Fekete; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract To minimize network latency and remain online during server failures and networkpartitions; many modern distributed data storage systems eschew transactional functionality;which provides strong semantic guarantees for groups of multiple operations over multipledata items. In this work; we consider the problem of providing Highly Available Transactions(HATs): transactional guarantees that do not suffer unavailability during system partitions orincur high network latency. We introduce a taxonomy of highly available systems andanalyze existing ACID isolation and distributed data consistency guarantees to identifywhich can and cannot be achieved in HAT systems. This unifies the literature on weaktransactional isolation; replica consistency; and highly available systems. We analyticallyand experimentally quantify the availability and performance benefits of HATs---often two …,Proceedings of the VLDB Endowment,2013,107
Coordination Avoidance in Database Systems,Peter Bailis; Alan Fekete; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract Minimizing coordination; or blocking communication between concurrentlyexecuting operations; is key to maximizing scalability; availability; and high performance indatabase systems. However; uninhibited coordination-free execution can compromiseapplication correctness; or consistency. When is coordination necessary for correctness?The classic use of serializable transactions is sufficient to maintain correctness but is notnecessary for all applications; sacrificing potential scalability. In this paper; we develop aformal framework; invariant confluence; that determines whether an application requirescoordination for correct execution. By operating on application-level invariants overdatabase states (eg; integrity constraints); invariant confluence analysis provides anecessary and sufficient condition for safe; coordination-free execution. When …,Proceedings of the VLDB Endowment,2014,97
Scalable atomic visibility with RAMP transactions,Peter Bailis; Alan Fekete; Joseph M Hellerstein; Ali Ghodsi; Ion Stoica,Abstract Databases can provide scalability by partitioning data across several servers.However; multipartition; multioperation transactional access is often expensive; employingcoordination-intensive locking; validation; or scheduling mechanisms. Accordingly; manyreal-world systems avoid mechanisms that provide useful semantics for multipartitionoperations. This leads to incorrect behavior for a large class of applications includingsecondary indexing; foreign key enforcement; and materialized view maintenance. In thiswork; we identify a new isolation model—Read Atomic (RA) isolation—that matches therequirements of these use cases by ensuring atomic visibility: either all or none of eachtransaction's updates are observed by other transactions. We present algorithms for ReadAtomic Multipartition (RAMP) transactions that enforce atomic visibility while offering …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,72
The Network is Reliable: An informal survey of real-world communications failures,Peter Bailis; Kyle Kingsbury,“The network is reliable” tops Peter Deutsch's classic list;“Eight fallacies of distributedcomputing”(https://blogs. oracle. com/jag/resource/Fallacies. html);“all [of which] prove to befalse in the long run and all [of which] cause big trouble and painful learning experiences.”Accounting for and understanding the implications of network behavior is key to designingrobust distributed programs—in fact; six of Deutsch's “fallacies” directly pertain to limitationson networked communications. This should be unsurprising: the ability (and oftenrequirement) to communicate over a shared channel is a defining characteristic ofdistributed programs; and many of the key results in the field pertain to the possibility andimpossibility of performing distributed computations under particular sets of networkconditions. For example; the celebrated FLP impossibility result9 demonstrates the …,Queue,2014,61
The potential dangers of causal consistency and an explicit solution,Peter Bailis; Alan Fekete; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract Causal consistency is the strongest consistency model that is available in thepresence of partitions and provides useful semantics for human-facing distributed services.Here; we expose its serious and inherent scalability limitations due to write propagationrequirements and traditional dependency tracking mechanisms. As an alternative to classicpotential causality; we advocate the use of explicit causality; or application-defined happens-before relations. Explicit causality; a subset of potential causality; tracks only relevantdependencies and reduces several of the potential dangers of causal consistency.,Proceedings of the Third ACM Symposium on Cloud Computing,2012,61
Programming micro-aerial vehicle swarms with karma,Karthik Dantu; Bryan Kate; Jason Waterman; Peter Bailis; Matt Welsh,Abstract Research in micro-aerial vehicle (MAV) construction; control; and high-densitypower sources is enabling swarms of MAVs as a new class of mobile sensing systems. Forefficient operation; such systems must adapt to dynamic environments; cope with uncertaintyin sensing and control; and operate with limited resources. We propose a novel systemarchitecture based on a hive-drone model that simplifies the functionality of an individualMAV to a sequence of sensing and actuation commands with no in-field communication.This decision simplifies the hardware and software complexity of individual MAVs andmoves the complexity of coordination entirely to a central hive computer. We present Karma;a system for programming and managing MAV swarms. Through simulation and testbedexperiments we demonstrate how applications in Karma can run on limited resources; are …,9th Annual Conference on Embedded Networked Sensor Systems; Seattle; WA,2011,50
Dimetrodon: processor-level preventive thermal management via idle cycle injection,Peter Bailis; Vijay Janapa Reddi; Sanjay Gandhi; David Brooks; Margo Seltzer,Abstract Processor-level dynamic thermal management techniques have long targeted worst-case thermal margins. We examine the thermal-performance trade-offs in average-case;preventive thermal management by actively degrading application performance to achievelong-term thermal control. We propose Dimetrodon; the use of idle cycle injection; a flexible;per-thread technique; as a preventive thermal management mechanism and demonstrate itsefficiency compared to hardware techniques in a commodity operating system on realhardware under throughput and latency-sensitive real-world workloads. Compared toinflexible hardware techniques; Dimet-rodon achieves favorable trade-offs for temperaturereductions up to 30% due to rapid heat dissipation during short idle intervals.,Design Automation Conference (DAC); 2011 48th ACM/EDAC/IEEE,2011,44
The missing piece in complex analytics: Low latency; scalable model management and serving with velox,Daniel Crankshaw; Peter Bailis; Joseph E Gonzalez; Haoyuan Li; Zhao Zhang; Michael J Franklin; Ali Ghodsi; Michael I Jordan,Abstract: To support complex data-intensive applications such as personalizedrecommendations; targeted advertising; and intelligent services; the data managementcommunity has focused heavily on the design of systems to support training complex modelson large datasets. Unfortunately; the design of these systems largely ignores a criticalcomponent of the overall analytics process: the deployment and serving of models at scale.In this work; we present Velox; a new component of the Berkeley Data Analytics Stack. Veloxis a data management system for facilitating the next steps in real-world; large-scaleanalytics pipelines: online model management; maintenance; and serving. Velox providesend-user applications and services with a low-latency; intuitive interface to models;transforming the raw statistical models currently trained using existing offline large-scale …,CIDR 2015,2014,42
Quantifying eventual consistency with PBS,Peter Bailis; Shivaram Venkataraman; Michael J Franklin; Joseph M Hellerstein; Ion Stoica,Abstract Data store replication results in a fundamental trade-off between operation latencyand data consistency. At the weak end of the consistency spectrum is eventual consistencyproviding no limit to the staleness of data returned. However; anecdotally; eventualconsistency is often “good enough” for practitioners given its latency and availabilitybenefits. In this work; we explain why eventually consistent systems are regularly acceptablein practice; analyzing both the staleness of data they return and the latency benefits theyoffer. We introduce Probabilistically Bounded Staleness (PBS); a consistency model whichprovides expected bounds on data staleness with respect to both versions and wall clocktime. We derive a closed-form solution for versioned staleness as well as model real-timestaleness under Internet-scale production workloads for a large class of quorum …,The VLDB Journal,2014,33
Feral concurrency control: An empirical investigation of modern application integrity,Peter Bailis; Alan Fekete; Michael J Franklin; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract The rise of data-intensive" Web 2.0" Internet services has led to a range of popularnew programming frameworks that collectively embody the latest incarnation of the vision ofObject-Relational Mapping (ORM) systems; albeit at unprecedented scale. In this work; weempirically investigate modern ORM-backed applications' use and disuse of databaseconcurrency control mechanisms. Specifically; we focus our study on the common use offeral; or application-level; mechanisms for maintaining database integrity; which; across arange of ORM systems; often take the form of declarative correctness criteria; or invariants.We quantitatively analyze the use of these mechanisms in a range of open sourceapplications written using the Ruby on Rails ORM and find that feral invariants are the mostpopular means of ensuring integrity (and; by usage; are over 37 times more popular than …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,31
Consistency without borders,Peter Alvaro; Peter Bailis; Neil Conway; Joseph M Hellerstein,Abstract Distributed consistency is a perennial research topic; in recent years it has becomean urgent practical matter as well. The research literature has focused on enforcing variousflavors of consistency at the I/O layer; such as linearizability of read/write registers. Forpractitioners; strong I/O consistency is often impractical at scale; while looser forms of I/Oconsistency are difficult to map to application-level concerns. Instead; it is common fordevelopers to take matters of distributed consistency into their own hands; leading toapplication-specific solutions that are tricky to write; test and maintain. In this paper; weagitate for the technical community to shift its attention to approaches that lie between theextremes of I/O-level and application-level consistency. We ground our discussion in earlywork in the area; including our own experiences building programmer tools and …,Proceedings of the 4th annual Symposium on Cloud Computing,2013,28
MacroBase: Prioritizing attention in fast data,Peter Bailis; Edward Gan; Samuel Madden; Deepak Narayanan; Kexin Rong; Sahaana Suri,Abstract As data volumes continue to rise; manual inspection is becoming increasinglyuntenable. In response; we present MacroBase; a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient; accurate; andmodular analyses that highlight and aggregate important and unusual behavior; acting as asearch engine for fast data. MacroBase is able to deliver order-of-magnitude speedups overalternatives by optimizing the combination of explanation and classification tasks and byleveraging a new reservoir sampler and heavy-hitters sketch specialized for fast datastreams. As a result; MacroBase delivers accurate results at speeds of up to 2M events persecond per query on a single core. The system has delivered meaningful results inproduction; including at a telematics company monitoring hundreds of thousands of …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,22
HAT; not CAP: towards highly available transactions,Peter Bailis; Alan Fekete; Ali Ghodsi; Joseph M Hellerstein; Ion Stoica,Abstract While the CAP Theorem is often interpreted to preclude the availability oftransactions in a partition-prone environment; we show that highly available systems canprovide useful transactional semantics; often matching those of today's ACID databases. Wepropose Highly Available Transactions (HATs) that are available in the presence ofpartitions. HATs support many desirable ACID guarantees for arbitrary transactionalsequences of read and write operations and permit low-latency operation.,Proceedings of the 14th USENIX conference on Hot Topics in Operating Systems,2013,19
Positional communication and private information in honeybee foraging models,Peter Bailis; Radhika Nagpal; Justin Werfel,Abstract Honeybees coordinate foraging efforts across vast areas through a complex systemof advertising and recruitment. One mechanism for coordination is the waggle dance; amovement pattern which carries positional information about food sources. However; recentevidence suggests that recruited foragers may not use the dance's positional information tothe degree that has traditionally been believed. We model bee colony foraging to investigatethe value of sharing food source position information in different environments. We find thatin several environments; relying solely on private information about previously encounteredfood sources is more efficient than sharing information. Relying on private information leadsto a greater diversity of forage sites and can decrease over-harvesting of sources. This isbeneficial in environments with small quantities of nectar per flower; but may be …,Swarm Intelligence (ANTS 2010),2011,19
NoScope: optimizing neural network queries over video at scale,Daniel Kang; John Emmons; Firas Abuzaid; Peter Bailis; Matei Zaharia,Abstract Recent advances in computer vision---in the form of deep neural networks---havemade it possible to query increasing volumes of video data with high accuracy. However;neural network inference is computationally expensive at scale: applying a state-of-the-artobject detector in real time (ie; 30+ frames per second) to a single video requires a $4000GPU. In response; we present N o S cope; a system for querying videos that can reduce thecost of neural network video analysis by up to three orders of magnitude via inference-optimized model search. Given a target video; object to detect; and reference neuralnetwork; N o S cope automatically searches for and trains a sequence; or cascade; ofmodels that preserves the accuracy of the reference network but is specialized to the targetvideo and are therefore far less computationally expensive. N o S cope cascades two …,Proceedings of the VLDB Endowment,2017,12
PBS at work: Advancing data management with consistency metrics (demo),Peter Bailis; Shivaram Venkataraman; Michael J Franklin; Joseph M Hellerstein; Ion Stoica,*,SIGMOD,2013,9
Coordination avoidance in distributed databases,Peter David Bailis,Abstract The rise of Internet-scale geo-replicated services has led to upheaval in the designof modern data management systems. Given the availability; latency; and throughputpenalties associated with classic mechanisms such as serializable transactions; a broadclass of systems (eg;" NoSQL") has sought weaker alternatives that reduce the use ofexpensive coordination during system operation; often at the cost of application integrity.When can we safely forego the cost of this expensive coordination; and when must we paythe price?,*,2015,8
Prioritizing Attention in Fast Data: Principles and Promise,Peter Bailis; Edward Gan; Kexin Rong; Sahaana Suri,ABSTRACT While data volumes continue to rise; the capacity of human attention remainslimited. As a result; users need analytics engines that can assist in prioritizing attention inthis fast data that is too large for manual inspection. We present a set of design principles forthe design of fast data analytics engines that leverage the relative scarcity of humanattention and overabundance of data: return fewer results; prioritize iterative analysis; andfilter fast to compute less. We report on our early experiences employing these principles inthe design and deployment of MacroBase; an open source analysis engine for prioritizingattention in fast data. By combining streaming operators for feature transformation;classification; and data summarization; MacroBase provides users with interpretableexplanations of key behaviors; acting as a search engine for fast data.,CIDR,2017,6
Asynchronous complex analytics in a distributed dataflow architecture,Joseph E Gonzalez; Peter Bailis; Michael I Jordan; Michael J Franklin; Joseph M Hellerstein; Ali Ghodsi; Ion Stoica,Abstract: Scalable distributed dataflow systems have recently experienced widespreadadoption; with commodity dataflow engines such as Hadoop and Spark; and evencommodity SQL engines routinely supporting increasingly sophisticated analytics tasks (eg;support vector machines; logistic regression; collaborative filtering). However; thesesystems' synchronous (often Bulk Synchronous Parallel) dataflow execution model is atodds with an increasingly important trend in the machine learning community: the use ofasynchrony via shared; mutable state (ie; data races) in convex programming tasks; whichhas---in a single-node context---delivered noteworthy empirical performance gains andinspired new research into asynchronous algorithms. In this work; we attempt to bridge thisgap by evaluating the use of lightweight; asynchronous state transfer within a commodity …,arXiv preprint arXiv:1510.07092,2015,6
ASAP: prioritizing attention via time series smoothing,Kexin Rong; Peter Bailis,Abstract Time series visualization of streaming telemetry (ie; charting of key metrics such asserver load over time) is increasingly prevalent in modern data platforms and applications.However; many existing systems simply plot the raw data streams as they arrive; oftenobscuring large-scale trends due to small-scale noise. We propose an alternative: to betterprioritize end users' attention; smooth time series visualizations as much as possible toremove noise; while retaining large-scale structure to highlight significant deviations. Wedevelop a new analytics operator called ASAP that automatically smooths streaming timeseries by adaptively optimizing the trade-off between noise reduction (ie; variance) andtrend retention (ie; kurtosis). We introduce metrics to quantitatively assess the quality ofsmoothed plots and provide an efficient search strategy for optimizing these metrics that …,Proceedings of the VLDB Endowment,2017,5
Scalable Kernel Density Classification via Threshold-Based Pruning,Edward Gan; Peter Bailis,Abstract Density estimation forms a critical component of many analytics tasks includingoutlier detection; visualization; and statistical testing. These tasks often seek to classify datainto high and low-density regions of a probability distribution. Kernel Density Estimation(KDE) is a powerful technique for computing these densities; offering excellent statisticalaccuracy but quadratic total runtime. In this paper; we introduce a simple technique forimproving the performance of using a KDE to classify points by their density (densityclassification). Our technique; thresholded kernel density classification (tKDC); appliesthreshold-based pruning to spatial index traversal to achieve asymptotic speedups overnaïve KDE; while maintaining accuracy guarantees. Instead of exactly computing eachpoint's exact density for use in classification; tKDC iteratively computes density bounds …,SIGMOD,2017,3
ACIDRain: Concurrency-Related Attacks on Database-Backed Web Applications,Todd Warszawski; Peter Bailis,Abstract In theory; database transactions protect application data from corruption andintegrity violations. In practice; database transactions frequently execute under weakisolation that exposes programs to a range of concurrency anomalies; and programmersmay fail to correctly employ transactions. While low transaction volumes mask manypotential concurrency-related errors under normal operation; determined adversaries canexploit them programmatically for fun and profit. In this paper; we formalize a new kind ofattack on database-backed applications called an ACIDRain attack; in which an adversarysystematically exploits concurrency-related vulnerabilities via programmatically accessibleAPIs. These attacks are not theoretical: ACIDRain attacks have already occurred in ahandful of applications in the wild; including one attack which bankrupted a popular …,SIGMOD,2017,2
Finding Heavily-Weighted Features in Data Streams,Kai Sheng Tai; Vatsal Sharan; Peter Bailis; Gregory Valiant,Abstract: We introduce a new sub-linear space data structure---the Weight-Median Sketch---that captures the most heavily weighted features in linear classifiers trained over datastreams. This enables memory-limited execution of several statistical analyses over streams;including online feature selection; streaming data explanation; relative deltoid detection; andstreaming estimation of pointwise mutual information. In contrast with related sketches thatcapture the most commonly occurring features (or items) in a data stream; the Weight-Median Sketch captures the features that are most discriminative of one stream (or class)compared to another. The Weight-Median sketch adopts the core data structure used in theCount-Sketch; but; instead of sketching counts; it captures sketched gradient updates to themodel parameters. We provide a theoretical analysis of this approach that establishes …,arXiv preprint arXiv:1711.02305,2017,1
DAWNBench: An End-to-End Deep Learning Benchmark and Competition,Cody Coleman; Deepak Narayanan; Daniel Kang; Tian Zhao; Jian Zhang; Luigi Nardi; Peter Bailis; Kunle Olukotun; Chris Ré; Matei Zaharia,Abstract Despite considerable research on systems; algorithms and hardware to speed updeep learning workloads; there is no standard means of evaluating end-to-end deeplearning performance. Existing benchmarks measure proxy metrics; such as time to processone minibatch of data; that do not indicate whether the system as a whole will produce ahigh-quality result. In this work; we introduce DAWNBench; a benchmark and competitionfocused on end-to-end training time to achieve a state-of-the-art accuracy level; as well asinference time with that accuracy. Using time to accuracy as a target metric; we explore howdifferent optimizations; including choice of optimizer; stochastic depth; and multi-GPUtraining; affect end-to-end training performance. Our results demonstrate that optimizationscan interact in non-trivial ways when used in conjunction; producing lower speed-ups and …,Training,2017,1
DROP: Dimensionality Reduction Optimization for Time Series,Sahaana Suri; Peter Bailis,Abstract: Dimensionality reduction is critical in analyzing increasingly high-volume; high-dimensional time series. In this paper; we revisit a now-classic study of time seriesdimensionality reduction operators and find that for a given quality constraint; PrincipalComponent Analysis (PCA) uncovers representations that are over 2x smaller than thoseobtained via alternative techniques favored in the literature. However; as classicallyimplemented via Singular Value Decomposition (SVD); PCA is incredibly expensive forlarge datasets. Therefore; we present DROP; a dimensionality reduction optimizer for high-dimensional analytics pipelines that greatly reduces the cost of the PCA operation over timeseries datasets. We show that many time series are highly structured; hence a small numberof data points are sufficient to characterize the data set; which permits aggressive …,arXiv preprint arXiv:1708.00183,2017,*
There and Back Again: A General Approach to Learning Sparse Models,Vatsal Sharan; Kai Sheng Tai; Peter Bailis; Gregory Valiant,Abstract: We propose a simple and efficient approach to learning sparse models. Ourapproach consists of (1) projecting the data into a lower dimensional space;(2) learning adense model in the lower dimensional space; and then (3) recovering the sparse model inthe original space via compressive sensing. We apply this approach to Non-negative MatrixFactorization (NMF); tensor decomposition and linear classification---showing that it obtains$10\times $ compression with negligible loss in accuracy on real data; and obtains up to$5\times $ speedups. Our main theoretical contribution is to show the following result forNMF: if the original factors are sparse; then their projections are the sparsest solutions to theprojected NMF problem. This explains why our method works for NMF and shows aninteresting new property of random projections: they can preserve the solutions of non …,arXiv preprint arXiv:1706.08146,2017,*
SimDex: Exploiting Model Similarity in Exact Matrix Factorization Recommendations,Firas Abuzaid; Geet Sethi; Peter Bailis; Matei Zaharia,Abstract: We present SimDex; a new technique for serving exact top-K recommendations onmatrix factorization models that measures and optimizes for the similarity between users inthe model. Previous serving techniques presume a high degree of similarity (eg; L2 orcosine distance) among users and/or items in MF models; however; as we demonstrate; themost accurate models are not guaranteed to exhibit high similarity. As a result; brute-forcematrix multiply outperforms recent proposals for top-K serving on several collaborativefiltering tasks. Based on this observation; we develop SimDex; a new technique for servingmatrix factorization models that automatically optimizes serving based on the degree ofsimilarity between users; and outperforms existing methods in both the high-similarity andlow-similarity regimes. SimDexfirst measures the degree of similarity among users via …,arXiv preprint arXiv:1706.01449,2017,*
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,Peter Bailis; Kunle Olukoton; Christopher Ré; Matei Zaharia,Abstract: Despite incredible recent advances in machine learning; building machinelearning applications remains prohibitively time-consuming and expensive for all but thebest-trained; best-funded engineering organizations. This expense comes not from a needfor new and improved statistical models but instead from a lack of systems and tools forsupporting end-to-end machine learning application development; from data preparationand labeling to productionization and monitoring. In this document; we outline opportunitiesfor infrastructure supporting usable; end-to-end machine learning applications in the contextof the nascent DAWN (Data Analytics for What's Next) project at Stanford. Subjects: Learning(cs. LG); Databases (cs. DB); Machine Learning (stat. ML) Cite as: arXiv: 1705.07538 [cs.LG](or arXiv: 1705.07538 v1 [cs. LG] for this version) Submission history From: Peter …,arXiv preprint arXiv:1705.07538,2017,*
