Graph pattern matching: from intractable to polynomial time,Wenfei Fan; Jianzhong Li; Shuai Ma; Nan Tang; Yinghui Wu; Yunpeng Wu,Abstract Graph pattern matching is typically defined in terms of subgraph isomorphism;which makes it an np-complete problem. Moreover; it requires bijective functions; which areoften too restrictive to characterize patterns in emerging applications. We propose a class ofgraph patterns; in which an edge denotes the connectivity in a data graph within apredefined number of hops. In addition; we define matching based on a notion of boundedsimulation; an extension of graph simulation. We show that with this revision; graph patternmatching can be performed in cubic-time; by providing such an algorithm. We also developalgorithms for incrementally finding matches when data graphs are updated; withperformance guarantees for dag patterns. We experimentally verify that these algorithmsscale well; and that the revised notion of graph pattern matching allows us to identify …,Proceedings of the VLDB Endowment,2010,204
NADEEF: a commodity data cleaning system,Michele Dallachiesa; Amr Ebaid; Ahmed Eldawy; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Nan Tang,Abstract Despite the increasing importance of data quality and the rich theoretical andpractical contributions in all aspects of data cleaning; there is no single end-to-end off-the-shelf solution to (semi-) automate the detection and the repairing of violations wrt a set ofheterogeneous and ad-hoc quality constraints. In short; there is no commodity platformsimilar to general purpose DBMSs that can be easily customized and deployed to solveapplication-specific data quality problems. In this paper; we present NADEEF; an extensible;generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between aprogramming interface and a core to achieve generality and extensibility. The programminginterface allows the users to specify multiple types of data quality rules; which uniformlydefine what is wrong with the data and (possibly) how to repair it through writing code that …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,136
Adding regular expressions to graph reachability and pattern queries,Wenfei Fan; Jianzhong Li; Shuai Ma; Nan Tang; Yinghui Wu,It is increasingly common to find graphs in which edges bear different types; indicating avariety of relationships. For such graphs we propose a class of reachability queries and aclass of graph patterns; in which an edge is specified with a regular expression of a certainform; expressing the connectivity in a data graph via edges of various types. In addition; wedefine graph pattern matching based on a revised notion of graph simulation. On graphs inemerging applications such as social networks; we show that these queries are capable offinding more sensible information than their traditional counterparts. Better still; theirincreased expressive power does not come with extra complexity. Indeed;(1) we investigatetheir containment and minimization problems; and show that these fundamental problemsare in quadratic time for reachability queries and are in cubic time for pattern queries.(2) …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,112
Interaction between record matching and data repairing,Wenfei Fan; Shuai Ma; Nan Tang; Wenyuan Yu,Abstract Central to a data cleaning system are record matching and data repairing. Matchingaims to identify tuples that refer to the same real-world object; and repairing is to make adatabase consistent by fixing errors in the data by using integrity constraints. These aretypically treated as separate processes in current data cleaning systems; based on heuristicsolutions. This article studies a new problem in connection with data cleaning; namely theinteraction between record matching and data repairing. We show that repairing caneffectively help us identify matches; and vice versa. To capture the interaction; we provide auniform framework that seamlessly unifies repairing and matching operations to clean adatabase based on integrity constraints; matching rules; and master data. We give a fulltreatment of fundamental problems associated with data cleaning via matching and …,Journal of Data and Information Quality (JDIQ),2014,107
Towards certain fixes with editing rules and master data,Wenfei Fan; Jianzhong Li; Shuai Ma; Nan Tang; Wenyuan Yu,Abstract A variety of integrity constraints have been studied for data cleaning. While theseconstraints can detect the presence of errors; they fall short of guiding us to correct theerrors. Indeed; data repairing based on these constraints may not find certain fixes that areabsolutely correct; and worse; may introduce new errors when repairing the data. Wepropose a method for finding certain fixes; based on master data; a notion of certain regions;and a class of editing rules. A certain region is a set of attributes that are assured correct bythe users. Given a certain region and master data; editing rules tell us what attributes to fixand how to update them. We show how the method can be used in data monitoring andenrichment. We develop techniques for reasoning about editing rules; to decide whetherthey lead to a unique fix and whether they are able to fix all the attributes in a tuple …,Proceedings of the VLDB Endowment,2010,83
Multiple materialized view selection for XPath query rewriting,Nan Tang; Jeffrey Xu Yu; M Tamer Ozsu; Byron Choi; Kam-Fai Wong,We study the problem of answering XPATH queries using multiple materialized views.Despite the efforts on answering queries using single materialized view; answering queriesusing multiple views remains relatively new. We address two important aspects of thisproblem: multiple-view selection and equivalent multiple-view rewriting. With regards to thefirst problem; we propose an NFA-based approach (called VFILTER) to filter views thatcannot be used to answer a given query. We then present the criterion for multipleview/query answerability. Based on the output of VFILTER; we further propose a heuristicmethod to identify a minimal view set that can answer a given query. For the problem ofmultiple-view rewriting; we first refine the materialized fragments of each selected view (likepushing selection); we then join the refined fragments utilizing an encoding scheme …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,69
Katara: A data cleaning system powered by knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Classical approaches to clean data have relied on using integrity constraints;statistics; or machine learning. These approaches are known to be limited in the cleaningaccuracy; which can usually be improved by consulting master data and involving experts toresolve ambiguity. The advent of knowledge bases KBs both general-purpose and withinenterprises; and crowdsourcing marketplaces are providing yet more opportunities toachieve higher accuracy at a larger scale. We propose KATARA; a knowledge base andcrowd powered data cleaning system that; given a table; a KB; and a crowd; interprets tablesemantics to align it with the KB; identifies correct and incorrect data; and generates top-kpossible repairs for incorrect data. Experiments show that KATARA can be applied tovarious datasets and KBs; and can efficiently annotate data and suggest possible repairs.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,63
Towards certain fixes with editing rules and master data,Wenfei Fan; Jianzhong Li; Shuai Ma; Nan Tang; Wenyuan Yu,Abstract A variety of integrity constraints have been studied for data cleaning. While theseconstraints can detect the presence of errors; they fall short of guiding us to correct theerrors. Indeed; data repairing based on these constraints may not find certain fixes that areguaranteed correct; and worse still; may even introduce new errors when attempting torepair the data. We propose a method for finding certain fixes; based on master data; anotion of certain regions; and a class of editing rules. A certain region is a set of attributesthat are assured correct by the users. Given a certain region and master data; editing rulestell us what attributes to fix and how to update them. We show how the method can be usedin data monitoring and enrichment. We also develop techniques for reasoning about editingrules; to decide whether they lead to a unique fix and whether they are able to fix all the …,The VLDB Journal,2012,60
Bigdansing: A system for big data cleansing,Zuhair Khayyat; Ihab F Ilyas; Alekh Jindal; Samuel Madden; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Si Yin,Abstract Data cleansing approaches have usually focused on detecting and fixing errorswith little attention to scaling to big datasets. This presents a serious impediment since datacleansing often involves costly computations such as enumerating pairs of tuples; handlinginequality joins; and dealing with user-defined functions. In this paper; we presentBigDansing; a Big Data Cleansing system to tackle efficiency; scalability; and ease-of-useissues in data cleansing. The system can run on top of most common general purpose dataprocessing platforms; ranging from DBMSs to MapReduce-like frameworks. A user-friendlyprogramming interface allows users to express data quality rules both declaratively andprocedurally; with no requirement of being aware of the underlying distributed platform.BigDansing takes these rules into a series of transformations that enable distributed …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,52
Towards dependable data repairing with fixing rules,Nan Tang Jiannan Wang,*,Proceedings of the 2014 international conference on Management of data (SIGMOD),2014,49
Incremental detection of inconsistencies in distributed data,Wenfei Fan; Jianzhong Li; Nan Tang,This paper investigates incremental detection of errors in distributed data. Given adistributed database D; a set Σ of conditional functional dependencies (CFDs); the set V ofviolations of the CFDs in D; and updates ΔD to D; it is to find; with minimum data shipment;changes ΔV to V in response to ΔD. The need for the study is evident since real-life data isoften dirty; distributed and frequently updated. It is often prohibitively expensive torecompute the entire set of violations when D is updated. We show that the incrementaldetection problem is NP-complete for database D that is partitioned either vertically orhorizontally; even when Σ and D are fixed. Nevertheless; we show that it is bounded: thereexist algorithms to detect errors such that their computational cost and data shipment areboth linear in the size of ΔD and ΔV; independent of the size of the database D. We …,IEEE Transactions on Knowledge and Data Engineering,2014,44
Inferring data currency and consistency for conflict resolution,Wenfei Fan; Floris Geerts; Nan Tang; Wenyuan Yu,This paper introduces a new approach for conflict resolution: given a set of tuples pertainingto the same entity; it is to identify a single tuple in which each attribute has the latest andconsistent value in the set. This problem is important in data integration; data cleaning andquery answering. It is; however; challenging since in practice; reliable timestamps are oftenabsent; among other things. We propose a model for conflict resolution; by specifying datacurrency in terms of partial currency orders and currency constraints; and by enforcing dataconsistency with constant conditional functional dependencies. We show that identifyingdata currency orders helps us repair inconsistent data; and vice versa. We investigate anumber of fundamental problems associated with conflict resolution; and establish theircomplexity. In addition; we introduce a framework and develop algorithms for conflict …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,33
Materialized view selection in XML databases,Nan Tang; Jeffrey Xu Yu; Hao Tang; M Tamer Özsu; Peter Boncz,Abstract Materialized views; a rdbms silver bullet; demonstrate its efficacy in manyapplications; especially as a data warehousing/decison support system tool. The pivot ofplaying materialized views efficiently is view selection. Though studied for over thirty years inrdbms; the selection is hard to make in the context of xml databases; where both the semi-structured data and the expressiveness of xml query languages add challenges to the viewselection problem. We start our discussion on producing minimal xml views (in terms of size)as candidates for a given workload (a query set). To facilitate intuitionistic view selection; wepresent a view graph (called vcube) to structurally maintain all generated views. By basingour selection on vcube for materialization; we propose two view selection strategies;targeting at space-optimized and space-time tradeoff; respectively. We built our …,International Conference on Database Systems for Advanced Applications,2009,30
Answering XML queries using path-based indexes: a survey,Kam-Fai Wong; Jeffrey Xu Yu; Nan Tang,Abstract The problem of answering XML queries using path-based indexes is to find efficientmethods for accelerating the XML query with pre-designed index structures over the XMLdatabase. This problem received increasing interests and have been lucubrated in recentyears. Regular path expression is the core of the XML query languages eg; XPath andXQuery. Most of the state-of-the-art path-based XML indexes; therefore; hammer at how toefficiently answer the path-based XML queries. This paper surveys various approaches toindexing XML data proposed in the literature. We give a step by step analysis to show theevolution of index structures for XML path information; based on tree structures or morecommonly; directed labeled graphs. For each approach; we first present the specific issue itaims to tackle; and then the proposed solution presented. Furthermore; construction …,World Wide Web,2006,30
WIN: An efficient data placement strategy for parallel XML databases,Nan Tang; Guoren Wang; Jeffrey Xu Yu; Kam-Fai Wong; Ge Yu,The basic idea behind parallel database systems is to perform operations in parallel toreduce the response time and improve the system throughput. Data placement is a keyfactor on the overall performance of parallel systems. XML is semistructured data; traditionaldata placement strategies cannot serve it well. In this paper; we present the concept ofintermediary node I Node; and propose a novel workload-aware data placement WIN toeffectively decluster XML data; to obtain high intra query parallelism. The extensiveexperiments show that the speedup and scale up performance of WIN outperforms theprevious strategies.,Parallel and Distributed Systems; 2005. Proceedings. 11th International Conference on,2005,26
NADEEF: A generalized data cleaning system,Amr Ebaid; Ahmed Elmagarmid; Ihab F Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang; Si Yin,Abstract We present NADEEF; an extensible; generic and easy-to-deploy data cleaningsystem. NADEEF distinguishes between a programming interface and a core to achievegenerality and extensibility. The programming interface allows users to specify data qualityrules by writing code that implements predefined classes. These classes uniformly definewhat is wrong with the data and (possibly) how to fix it. We will demonstrate the followingfeatures provided by NADEEF.(1) Heterogeneity: The programming interface can be used toexpress many types of data quality rules beyond the well known CFDs (FDs); MDs and ETLrules.(2) Interdependency: The core algorithms can interleave multiple types of rules todetect and repair data errors.(3) Deployment and extensibility: Users can easily customizeNADEEF by defining new types of rules; or by extending the core.(4) Metadata …,Proceedings of the VLDB Endowment,2013,24
Big data cleaning,Nan Tang,Abstract Data cleaning is; in fact; a lively subject that has played an important part in thehistory of data management and data analytics; and it still is undergoing rapid development.Moreover; data cleaning is considered as a main challenge in the era of big data; due to theincreasing volume; velocity and variety of data in many applications. This paper aims toprovide an overview of recent work in different aspects of data cleaning: error detectionmethods; data repairing algorithms; and a generalized data cleaning system. It also includessome discussion about our efforts of data cleaning methods from the perspective of big data;in terms of volume; velocity and variety.,Asia-Pacific Web Conference,2014,23
Efficient distribution of full-fledged XQuery,Ying Zhang; Nan Tang; Peter Boncz,We investigate techniques to automatically decompose any XQuery query into subqueries;that can be executed near their data sources; ie; function-shipping. In this scenario; thesubqueries being executed remotely may have XML node-valued parameters or results; thatmust be shipped in some way. The main challenge addressed here is to ensure that thedecomposed queries properly respect XML node identity and preserve structural properties;when (parts of) XML nodes are sent over the network; effectively copying them. We start byprecisely characterizing the conditions; under which pass-by-value parameter passingcauses semantic differences between remote execution of an XQuery expression and itslocal execution. We then formulate a conservative strategy that effectively avoidsdecomposition in such cases. To broaden the possibilities of query distribution; we extend …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,23
Detecting Data Errors: Where are we and what needs to be done?,Ziawasch Abedjan; Xu Chu; Dong Deng; Raul Castro Fernandez; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Michael Stonebraker; Nan Tang,Abstract Data cleaning has played a critical role in ensuring data quality for enterpriseapplications. Naturally; there has been extensive research in this area; and many datacleaning algorithms have been translated into tools to detect and to possibly repair certainclasses of errors such as outliers; duplicates; missing values; and violations of integrityconstraints. Since different types of errors may coexist in the same data set; we often need torun more than one kind of tool. In this paper; we investigate two pragmatic questions:(1) arethese tools robust enough to capture most errors in real-world data sets? and (2) what is thebest strategy to holistically run multiple tools to optimize the detection effort? To answerthese two questions; we obtained multiple data cleaning tools that utilize a variety of errordetection techniques. We also collected five real-world data sets; for which we could …,Proceedings of the VLDB Endowment,2016,22
Proof positive and negative in data cleaning,Matteo Interlandi; Nan Tang,One notoriously hard data cleaning problem is; given a database; how to precisely capturewhich value is correct (ie; proof positive) or wrong (ie; proof negative). Although integrityconstraints have been widely studied to capture data errors as violations; the accuracy ofdata cleaning using integrity constraints has long been controversial. Overall they deem onefundamental problem: Given a set of data values that together forms a violation; there is noevidence of which value is proof positive or negative. Hence; it is known that integrityconstraints themselves cannot guide dependable data cleaning. In this work; we introducean automated method for proof positive and negative in data cleaning; based on Sherlockrules and reference tables. Given a tuple and reference tables; Sherlock rules tell us whatattributes are proof positive; what attributes are proof negative and (possibly) how to …,Data Engineering (ICDE); 2015 IEEE 31st International Conference on,2015,21
Conflict resolution with data currency and consistency,Wenfei Fan; Floris Geerts; Nan Tang; Wenyuan Yu,Abstract This article introduces a new approach for conflict resolution: given a set of tuplespertaining to the same entity; it identifies a single tuple in which each attribute has the latestand consistent value in the set. This problem is important in data integration; data cleaning;and query answering. It is; however; challenging since in practice; reliable time stamps areoften absent; among other things. We propose a model for conflict resolution by specifyingdata currency in terms of partial currency orders and currency constraints and by enforcingdata consistency with constant conditional functional dependencies. We show thatidentifying data currency orders helps us repair inconsistent data; and vice versa. Weinvestigate a number of fundamental problems associated with conflict resolution andestablish their complexity. In addition; we introduce a framework and develop algorithms …,Journal of Data and Information Quality (JDIQ),2014,16
Lightning fast and space efficient inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which join relational tables on inequality conditions; are used invarious applications. While there have been a wide range of optimization methods for joinsin database systems; from algorithms such as sort-merge join and band join; to variousindices such as B+-tree; R*-tree and Bitmap; inequality joins have received little attentionand queries containing such joins are usually very slow. In this paper; we introduce fastinequality join algorithms. We put columns to be joined in sorted arrays and we usepermutation arrays to encode positions of tuples in one sorted array wrt the other sortedarray. In contrast to sort-merge join; we use space efficient bit-arrays that enableoptimizations; such as Bloom filter indices; for fast computation of the join results. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; and a …,Proceedings of the VLDB Endowment,2015,15
Fast XML structural join algorithms by partitioning,Nan Tang; Jeffrey Xu Yu; Kam-Fai Wong; Jianxin Li,An XML structural join evaluates structural relationships (eg parent-child or ancestor-descendant) between XML elements. It serves as an important computation unit in XMLpattern matching. Several classical structural join algorithms have been proposed such asStack-tree join and XR-Tree join. In this paper; we consider to answer the problem ofstructural join by partitioning. The Dietz numbering scheme is used for encoding sincenodes with the Dietz encodings could be well distributed on a plane. We first extend therelationships between nodes to the relationships between partitions on a plane and obtainsome observations and properties about the relationships between partitions. We thenpropose a new partition-based method; named P-Join for structural join between ancestorand descendant nodes based on the properties derived from our observations. Moreover …,Journal of Research and Practice in Information Technology,2008,15
Fast reachability query processing,Jiefeng Cheng; Jeffrey Xu Yu; Nan Tang,Abstract Graph has great expressive power to describe the complex relationships amongdata objects; and there are large graph datasets available. In this paper; we focus ourselveson processing a primitive graph query. We call it reachability query. The reachability query;denoted A\rightsquigarrowD; is to find all elements of a type D that are reachable from someelements in another type A. The problem is challenging because the existing structural joinalgorithms; studied in XML query processing; cannot be directly applied to it; because thosetechniques make use of the tree-structure heavily. We propose a novel approach which canprocess reachability queries on the fly while keeping the space consumption small that isneeded to keep the required information for processing reachability queries. In brief; ourapproach is based on 2-hop labeling for a directed graph G which consumes O (| V| log| E …,International Conference on Database Systems for Advanced Applications,2006,15
Road to Freedom in Big Data Analytics.,Divy Agrawal; Sanjay Chawla; Ahmed K Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,ABSTRACT The world is fast moving towards a data-driven society where data is the mostvaluable asset. Organizations need to perform very diverse analytic tasks using various dataprocessing platforms. In doing so; they face many challenges; chiefly; platform dependence;poor interoperability; and poor performance when using multiple platforms. We presentRHEEM; our vision for big data analytics over diverse data processing platforms. RHEEMprovides a threelayer data processing and storage abstraction to achieve both platformindependence and interoperability across multiple platforms. In this paper; we discuss ourvision as well as present multiple research challenges that we need to address to achieve it.As a case in point; we present a data cleaning application built using some of the ideas ofRHEEM. We show how it achieves platform independence and the performance benefits …,EDBT,2016,13
The Data Civilizer System.,Dong Deng; Raul Castro Fernandez; Ziawasch Abedjan; Sibo Wang; Michael Stonebraker; Ahmed K Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Nan Tang,ABSTRACT In many organizations; it is often challenging for users to find relevant data forspecific tasks; since the data is usually scattered across the enterprise and ofteninconsistent. In fact; data scientists routinely report that the majority of their effort is spentfinding; cleaning; integrating; and accessing data of interest to a task at hand. In order todecrease the “grunt work” needed to facilitate the analysis of data “in the wild”; we presentDATA CIVILIZER; an end-to-end big data management system. DATA CIVILIZER has alinkage graph computation module to build a linkage graph for the data and a data discoverymodule which utilizes the linkage graph to help identify data that is relevant to user tasks. Italso uses the linkage graph to discover possible join paths that can then be used in a query.For the actual query execution; we use a polystore DBMS; which federates query …,CIDR,2017,12
Graph stream summarization: From big bang to big crunch,Nan Tang; Qing Chen; Prasenjit Mitra,Abstract A graph stream; which refers to the graph with edges being updated sequentially ina form of a stream; has important applications in cyber security and social networks. Due tothe sheer volume and highly dynamic nature of graph streams; the practical way of handlingthem is by summarization. Given a graph stream G; directed or undirected; the problem ofgraph stream summarization is to summarize G as SG with a much smaller (sublinear)space; linear construction time and constant maintenance cost for each edge update; suchthat SG allows many queries over G to be approximately conducted efficiently. The widelyused practice of summarizing data streams is to treat each stream element independently byeg; hash-or sample-based methods; without maintaining the connections (or relationships)between elements. Hence; existing methods can only solve ad-hoc problems; without …,Proceedings of the 2016 International Conference on Management of Data,2016,12
Rheem: Enabling multi-platform task execution,Divy Agrawal; Lamine Ba; Laure Berti-Equille; Sanjay Chawla; Ahmed Elmagarmid; Hossam Hammady; Yasser Idris; Zoi Kaoudi; Zuhair Khayyat; Sebastian Kruse; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Mohammed J Zaki,Abstract Many emerging applications; from domains such as healthcare and oil & gas;require several data processing systems for complex analytics. This demo paper showcasessystem; a framework that provides multi-platform task execution for such applications. Itfeatures a three-layer data processing abstraction and a new query optimization approachfor multi-platform settings. We will demonstrate the strengths of system by using real-worldscenarios from three different applications; namely; machine learning; data cleaning; anddata fusion.,Proceedings of the 2016 International Conference on Management of Data,2016,10
Extension of Conjugate π Bridge in Dye Molecules for Dye-Sensitized Solar Cells,Wei-Shen ZHAN; Rui LI; Shi PAN; Ying-Nan GUO; Yi ZHANG,Taking dye D5 molecules as the prototype; different types and different elemental quantitiesof conjugate π bridge was used to design D-π-A organic molecules. Density functionaltheory (DFT) and timedependent density functional theory (TDDFT) were adopted tosimulate the geometric structures; molecular orbital energy levels; and UV-Vis absorptionspectra of the molecules; with the aim of finding conjugate π bridge in the sensitizermolecules for dye-sensitized solar cells (DSSCs). The absorption spectra of the moleculesusing “methenyl chains”;“furan rings” or “thiophene rings”;“methenyl chains and furan rings”;or “methenyl chains and thiophene rings” as conjugate π bridge showed a graduallyincreasing red-shifting trend. With increases in the number of conjugate π bridge elements;the absorption spectrum showed an intense red-shift; which weakened gradually; under …,Acta Physico-Chimica Sinica,2013,10
CerFix: A system for cleaning data with certain fixes,Wenfei Fan; Jianzhong Li; Shuai Ma; Nan Tang; Wenyuan Yu,Abstract We present CerFix; a data cleaning system that finds certain fixes for tuples at thepoint of data entry; ie; fixes that are guaranteed correct. It is based on master data; editingrules and certain regions. Given some attributes of an input tuple that are validated (assuredcorrect); editing rules tell us what other attributes to fix and how to correct them with masterdata. A certain region is a set of attributes that; if validated; warrant a certain fix for the entiretuple. We demonstrate the following facilities provided by Cer-Fix:(1) a region finder toidentify certain regions;(2) a data monitor to find certain fixes for input tuples; by guidingusers to validate a minimal number of attributes; and (3) an auditing module to show whatattributes are fixed and where the correct values come from.,Proceedings of the VLDB Endowment,2011,10
A data placement strategy for parallel XML databases,G Wang; Nan Tang; Y Yu; Bing Sun; Ge Yu,This paper targets on parallel XML document partitioning strategies to process XML queriesin parallel. To describe the problem of XML data partitioning; a concept; intermediary node;is presented in this paper. By a set of intermediary nodes; an XML data tree can bepartitioned into a root-tree and a set of sub-trees. While the root-tree is duplicated over allthe nodes; the set of the sub-trees can be evenly partitioned over all the nodes based on theworkload of user queries. For the same XML data tree; there are a number of intermediarynodes sets; and different intermediary nodes sets will generate different partitions. It can beevaluated if a partitioning is good based on the workload of user queries. It is obviously anNP hard problem to choose an optimal partitioning. To solve this problem; this paperproposes a set of heuristic rules. Based on the idea described above; this paper designs …,Journal of Software,2006,10
NADEEF/ER: generic and interactive entity resolution,Si Yin Ahmed K. Elmagarmid; Ihab F. Ilyas; Mourad Ouzzani; Jorge-Arnulfo Quiané-Ruiz; Nan Tang,*,Proceedings of the 2014 international conference on Management of data (SIGMOD),2014,9
Data quality problems beyond consistency and deduplication,Wenfei Fan; Floris Geerts; Shuai Ma; Nan Tang; Wenyuan Yu,Abstract Recent work on data quality has primarily focused on data repairing algorithms forimproving data consistency and record matching methods for data deduplication. This paperaccentuates several other challenging issues that are essential to developing data cleaningsystems; namely; error correction with performance guarantees; unification of data repairingand record matching; relative information completeness; and data currency. We provide anoverview of recent advances in the study of these issues; and advocate the need fordeveloping a logical framework for a uniform treatment of these issues.,*,2013,9
Data placement and query processing based on RPE parallelisms,Yaxin Yu; Guoren Wang; Ge Yu; Gang Wu; Junan Hu; Nan Tang,The basic idea behind parallel database systems is to perform operations in parallel toreduce the response time and improve the system throughput. Data placement is a keyfactor on the performance of parallel database systems. This paper proposes two datapartition strategies to decluster XML documents with very large size; path schema basedpath instance balancing (PSPIB) strategy; in which all path instances with the same pathschema in a data tree are declustered evenly over all sites; and node schema based noderound-robin (NSNRR) strategy; in which all node objects with the same node schema in adata tree are declustered over all sites in a round-robin way. Accordingly; two queryprocessing algorithms are proposed based on the two partition methods; parallel pathmerge (PPM) algorithm and parallel pipelining path join (PPPJ) algorithm. The …,Computer Software and Applications Conference; 2003. COMPSAC 2003. Proceedings. 27th Annual International,2003,9
Interactive and deterministic data cleaning,Jian He; Enzo Veltri; Donatello Santoro; Guoliang Li; Giansalvatore Mecca; Paolo Papotti; Nan Tang,Abstract We present Falcon; an interactive; deterministic; and declarative data cleaningsystem; which uses SQL update queries as the language to repair data. Falcon does not relyon the existence of a set of pre-defined data quality rules. On the contrary; it encouragesusers to explore the data; identify possible problems; and make updates to fix them.Bootstrapped by one user update; Falcon guesses a set of possible sql update queries thatcan be used to repair the data. The main technical challenge addressed in this paperconsists in finding a set of sql update queries that is minimal in size and at the same timefixes the largest number of errors in the data. We formalize this problem as a search in alattice-shaped space. To guarantee that the chosen updates are semantically correct; Falconnavigates the lattice by interacting with users to gradually validate the set of sql update …,Proceedings of the 2016 International Conference on Management of Data,2016,8
Space-Economical partial gram indices for exact substring matching,Nan Tang; Lefteris Sidirourgos; Peter Boncz,Abstract Exact substring matching queries on large data collections can be answered usingq-gram indices; that store for each occurring q-byte pattern an (ordered) posting list with thepositions of all occurrences. Such gram indices are known to provide fast query responsetime and to allow the index to be created quickly even on huge disk-based datasets. Theirmain drawback is relatively large storage space; that is a constant multiple (typically> 2) ofthe original data size; even when compression is used. In this work; we study methods toconserve the scalable creation time and efficient exact substring query properties of gramindices; while reducing storage space. To this end; we first propose a partial gram indexbased on a reduction from the problem of omitting indexed q-grams to the set cover problem.While this method is successful in reducing the size of the index; it generates false …,Proceedings of the 18th ACM conference on Information and knowledge management,2009,8
Accelerating XML structural join by partitioning,Nan Tang; Jeffrey Xu Yu; Kam-Fai Wong; Kevin Lü; Jianxin Li,Abstract Structural join is the core part of XML queries and has a significant impact on theperformance of XML queries; several classical structural join algorithms have beenproposed such as Stack-tree join and XR-Tree join. In this paper; we consider to answer theproblem of structural join by partitioning. We first extend the relationships between nodes tothe relationships between partitions in the plane and get some observations. We thenpropose a new partition-based method P-Join for structural join. Based on P-Join; moreover;we present an enhanced partitioned-based spatial structural join algorithm PSSJ.,International Conference on Database and Expert Systems Applications,2005,8
Fast structural join with a location function,Nan Tang; Jeffrey Xu Yu; Kam-Fai Wong; Haifeng Jiang,Abstract A structural join evaluates structural relationship (parent-child or ancestor-descendant) between xml elements. It serves as an important computation unit in xml patternmatching; such as twig joins. There exists many work on efficient structural joins. Inparticular; indexes can expedite structural joins by skipping unmatchable elements. A typicaluse of indexes is to retrieve; for a given element; all its ancestor (or descendant) elementsfrom an indexed set. However we observed two possible limitations with such index probes;namely false hit and false locate. A false hit means that an index probe touches unnecessarydata besides real results; a false locate stands for a (wasted) probe that has zero answers.Obviously false hit and false locate can affect negatively the efficiency of structural joins. Inthis paper; we challenge ourselves to develop new structural join algorithm with no false …,International Conference on Database Systems for Advanced Applications,2006,7
Big RDF data cleaning,Nan Tang,Without a shadow of a doubt; data cleaning has played an important part in the history ofdata management and data analytics. Possessing high quality data has been proven to becrucial for businesses to do data driven decision making; especially within the informationage and the era of big data. Resource Description Framework (RDF) is a standard model fordata interchange on the semantic web. However; it is known that RDF data is dirty; sincemany of them are automatically extracted from the web. In this paper; we will first revisit dataquality problems appeared in RDF data. Although many efforts have been put to clean RDFdata; unfortunately; most of them are based on laborious manual evaluation. We will alsodescribe possible solutions that shed lights on (semi-) automatically cleaning (big) RDFdata.,Data Engineering Workshops (ICDEW); 2015 31st IEEE International Conference on,2015,6
Generating concise entity matching rules,Rohit Singh; Vamsi Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration and cleaning. In manyapplications; the users need to understand why two entities are considered a match; whichreveals the need for interpretable and concise EM rules. We model EM rules in the form ofGeneral Boolean Formulas (GBFs) that allows arbitrary attribute matching combined byconjunctions (∨); disjunctions (∧); and negations.(¬) GBFs can generate more conciserules than traditional EM rules represented in disjunctive normal forms (DNFs). We useprogram synthesis; a powerful tool to automatically generate rules (or programs) thatprovably satisfy a high-level specification; to automatically synthesize EM rules in GBFformat; given only positive and negative matching examples. In this demo; attendees willexperience the following features:(1) Interpretability--they can see and measure the …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,5
KATARA: Reliable data cleaning with knowledge bases and crowdsourcing,Xu Chu; John Morcos; Ihab F Ilyas; Mourad Ouzzani; Paolo Papotti; Nan Tang; Yin Ye,Abstract Data cleaning with guaranteed reliability is hard to achieve without accessingexternal sources; since the truth is not necessarily discoverable from the data at hand.Furthermore; even in the presence of external sources; mainly knowledge bases andhumans; effectively leveraging them still faces many challenges; such as aligningheterogeneous data sources and decomposing a complex task into simpler units that can beconsumed by humans. We present K atara; a novel end-to-end data cleaning systempowered by knowledge bases and crowdsourcing. Given a table; a kb; and a crowd; K atara(i) interprets the table semantics wrt the given kb;(ii) identifies correct and wrong data; and(iii) generates top-k possible repairs for the wrong data. Users will have the opportunity toexperience the following features of K atara:(1) Easy specification: Users can define a K …,Proceedings of the VLDB Endowment,2015,5
Projective Distribution of XQuery with Updates,Ying Zhang; Nan Tang; Peter Boncz,We investigate techniques to automatically decompose any XQuery query-includingupdating queries specified by the XQuery Update Facility (XQUF)-into subqueries; that canbe executed near their data sources; ie; function-shipping. The main challenge addressedhere is to ensure that the decomposed queries properly respect XML node identity andpreserve structural properties; when (parts of) XML nodes are sent over the network;effectively copying them. We start by precisely characterizing the conditions; under whichpass-by-value parameter passing causes semantic differences between remote execution ofan XQuery expression and its local execution. We then formulate a conservative strategythat effectively avoids decomposition in such cases. To broaden the possibilities of querydistribution; we extend the pass-by-value semantics to a pass-by-fragment semantics …,IEEE Transactions on Knowledge and Data Engineering,2010,5
Hierarchical indexing approach to support XPath queries,Nan Tang; Jeffrey Xu Yu; M Tamer Ozsu; Kam-Fai Wong,We study new hierarchical indexing approach to process XPATH queries. Here; ahierarchical index consists of index entries that are pairs of queries and their (full/partial)answers (called extents). With such an index; XPATH queries can be processed to extractthe results if they match the queries maintained in those index entries. Existing XML pathindexing approaches support either child-axis (/) only; or additional descendant-or-self-axis(//) but only in the query root. Different from them; we propose a novel indexing approach toprocess a large fragment of XPATH queries; which may use/;//; and wildcards (*). The keyissues are how to reduce the number of index entries and how to maintain non-overlappingextents among index entries. We show how to compress such index and how to evaluateXPATH queries on it. Experiments show the efficiency of our approaches.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,4
Answering XML twig queries with automata,Bing Sun; Bo Zhou; Nan Tang; Guoren Wang; Ge Yu; Fulin Jia,Abstract XML is emerging as a de facto standard for information representation and dataexchange over the web. Evaluation of twig queries; which allows users to find all occurrenceof a multiple branch pattern in an XML database; is a core and complicate operation for XMLquery processing. Performance of conventional evaluation approaches based on structuraljoin declines with the expansion of data size and query complexity. In this paper; a novelapproach is proposed to compute twig queries with matching twig query automata and pathschema trees. Moreover; we give the performance evaluation to demonstrate the highperformance of our approach.,Advanced Web Technologies and Applications,2004,4
Uguide: User-guided discovery of fd-detectable errors,Saravanan Thirumuruganathan; Laure Berti-Equille; Mourad Ouzzani; Jorge-Arnulfo Quiane-Ruiz; Nan Tang,Abstract Error detection is the process of identifying problematic data cells that are differentfrom their ground truth. Functional dependencies (FDs) have been widely studied in supportof this process. Oftentimes; it is assumed that FDs are given by experts. Unfortunately; it isusually hard and expensive for the experts to define such FDs. In addition; automatic dataprofiling over dirty data in order to find correct FDs is known to be a hard problem. In thispaper; we propose an end-to-end solution to detect FD-detectable errors from dirty data. Thebroad intuition is that given a dirty dataset; it is feasible to automatically find approximateFDs; as well as data that is possibly erroneous. Arguably; at this point; only experts canconfirm true FDs or true errors. However; in practice; experts never have enough budget tofind all errors. Hence; our problem is; given a limited budget of expert's time; which …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
A novel cost-based model for data repairing,Shuang Hao; Nan Tang; Guoliang Li; Jian He; Na Ta; Jianhua Feng,Integrity constraint based data repairing is an iterative process consisting of two parts: detectand group errors that violate given integrity constraints (ICs); and modify values inside eachgroup such that the modified database satisfies those ICs. However; most existing automaticsolutions treat the process of detecting and grouping errors straightforwardly (eg; violationsof functional dependencies using string equality); while putting more attention on heuristicsof modifying values within each group. In this paper; we propose a revised semantics ofviolations and data consistency wrt a set of ICs. The revised semantics relies on stringsimilarities; in contrast to traditional methods that use syntactic error detection using stringequality. Along with the revised semantics; we also propose a new cost model to quantify thecost of data repair by considering distances between strings. We show that the revised …,IEEE transactions on knowledge and data engineering,2017,3
Fast and scalable inequality joins,Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract Inequality joins; which is to join relations with inequality conditions; are used invarious applications. Optimizing joins has been the subject of intensive research rangingfrom efficient join algorithms such as sort-merge join; to the use of efficient indices such asB^+ B+-tree; R^* R∗-tree and Bitmap. However; inequality joins have received little attentionand queries containing such joins are notably very slow. In this paper; we introduce fastinequality join algorithms based on sorted arrays and space-efficient bit-arrays. We furtherintroduce a simple method to estimate the selectivity of inequality joins which is then used tooptimize multiple predicate queries and multi-way joins. Moreover; we study an incrementalinequality join algorithm to handle scenarios where data keeps changing. We haveimplemented a centralized version of these algorithms on top of PostgreSQL; a distributed …,The VLDB Journal,2017,3
Cleaning relations using knowledge bases,Shuang Hao; Nan Tang; Guoliang Li; Jian Li,We study the data cleaning problem of detecting and repairing wrong relational data; as wellas marking correct data; using well curated knowledge bases (KBs). We propose detectiverules (DRs); a new type of data cleaning rules that can make actionable decisions onrelational data; by building connections between a relation and a KB. The main invention isthat; a DR simultaneously models two opposite semantics of a relation using types andrelationships in a KB: the positive semantics that explains how attribute values are linked toeach other in correct tuples; and the negative semantics that indicates how wrong attributevalues are connected to other correct attribute values within the same tuples. Naturally; a DRcan mark correct values in a tuple if it matches the positive semantics. Meanwhile; a DR candetect/repair an error if it matches the negative semantics. We study fundamental …,Data Engineering (ICDE); 2017 IEEE 33rd International Conference on,2017,2
DeepER--Deep Entity Resolution,Muhammad Ebraheem; Saravanan Thirumuruganathan; Shafiq Joty; Mourad Ouzzani; Nan Tang,Abstract: Entity Resolution (ER) is a fundamental problem with many applications. Machinelearning (ML)-based and rule-based approaches have been widely studied for decades;with many efforts being geared towards which features/attributes to select; which similarityfunctions to employ; and which blocking function to use-complicating the deployment of anER system as a turn-key system. In this paper; we present DEEPER; a turn-key ER systempowered by deep learning (DL) techniques. The central idea is that distributedrepresentations and representation learning from DL can alleviate the above human effortsfor tuning existing ER systems. DeepER makes several notable contributions: encoding atuple as a distributed representation of attribute values; building classifiers using theserepresentations and a semantic aware blocking based on LSH; and learning and tuning …,arXiv preprint arXiv:1710.00597,2017,1
Methods and systems for data cleaning,*,A method for cleaning data stored in a database; the method comprising providing a set offixing rules. Each fixing rule incorporates a set of attribute values that capture an error in aplurality of semantically related attribute values; and a deterministic correction which isoperable to replace one of the set of attribute values with a correct attribute value to correctthe error. The method further comprises comparing at least two of the fixing rules with oneanother to check that the error correction carried out by one fixing rule is consistent with theerror correction carried out by another fixing rule.,*,2015,1
The data analytics group at the qatar computing research institute,George Beskales; Gautam Das; Ahmed K Elmagarmid; Ihab F Ilyas; Felix Naumann; Mourad Ouzzani; Paolo Papotti; Jorge Quiane-Ruiz; Nan Tang,The Qatar Computing Research Institute (QCRI); a member of Qatar Foundation forEducation; Science and Community Development; started its activities in early 2011. QCRI isfocusing on tackling large-scale computing challenges that address national priorities forgrowth and development and that have global impact in computing research. QCRI hascurrently five research groups working on different aspects of computing; these are: ArabicLanguage Technologies; Social Computing; Scientific Computing; Cloud Computing; andData Analytics. The data analytics group at QCRI; DA@ QCRI for short; has embarked in anambitious endeavour to become a premiere world-class research group by tackling diverseresearch topics related to data quality; data integration; information extraction; scientific datamanagement; and data mining. In the short time since its birth; DA@ QCRI has grown to …,ACM SIGMOD Record,2013,1
Design and control on antagonistic bionic joint driven by pneumatic muscles actuators,Haitao Yu; Wei Guo; Hongwei Tan; Mantian Li; Hegao Cai,*,Jixie Gongcheng Xuebao(Chinese Journal of Mechanical Engineering),2012,1
Efficient xpath query processing in native xml databases,Nan Tang,Abstract As XML (eXtensible Markup Language) becomes a universal medium for dataexchange over the Internet; efficient XML query processing is now the focus of considerableresearch and development activities. This thesis describes works toward efficient XML queryevaluation and optimization in native XML databases. A XML query can be decomposed to asequence of structural joins (eg; parent/child and ancestor/descendant) and content joins.Thus; structural join optimization is a key to improving join-based evaluation. We optimizestructural join with two orthogonal methods: partition-based method exploits the spatialspecialities of XML encodings by projecting them on a plane; and location-based methodimproves structural join by accurately pruning all irrelevant nodes; which cannot produceresults.,*,2008,1
Synthesizing entity matching rules by examples,Rohit Singh; Venkata Vamsikrishna Meduri; Ahmed Elmagarmid; Samuel Madden; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Armando Solar-Lezama; Nan Tang,Abstract Entity matching (EM) is a critical part of data integration. We study how to synthesizeentity matching rules from positive-negative matching examples. The core of our solution isprogram synthesis; a powerful tool to automatically generate rules (or programs) that satisfya given high-level specification; via a predefined grammar. This grammar describes aGeneral Boolean Formula (GBF) that can include arbitrary attribute matching predicatescombined by conjunctions (∧); disjunctions (∨) and negations (¬); and is expressiveenough to model EM problems; from capturing arbitrary attribute combinations to handlingmissing attribute values. The rules in the form of GBF are more concise than traditional EMrules represented in Disjunctive Normal Form (DNF). Consequently; they are moreinterpretable than decision trees and other machine learning algorithms that output deep …,Proceedings of the VLDB Endowment,2017,*
Entity Consolidation: The Golden Record Problem,Dong Deng; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract: Four key subprocesses in data integration are: data preparation (ie; transformingand cleaning data); schema integration (ie; lining up like attributes); entity resolution (ie;finding clusters of records that represent the same entity) and entity consolidation (ie;merging each cluster into a" golden record" which contains the canonical values for eachattribute). In real scenarios; the output of entity resolution typically contains multiple dataformats and different abbreviations for cell values; in addition to the omnipresent problem ofmissing data. These issues make entity consolidation challenging. In this paper; we studythe entity consolidation problem. Truth discovery systems can be used to solve this problem.They usually employ simplistic heuristics such as majority consensus (MC) or sourceauthority to determine the golden record. However; these techniques are not capable of …,arXiv preprint arXiv:1709.10436,2017,*
Dependable Data Repairing with Fixing Rules,Jiannan Wang; Nan Tang,Abstract One of the main challenges that data-cleaning systems face is to automaticallyidentify and repair data errors in a dependable manner. Though data dependencies (alsoknown as integrity constraints) have been widely studied to capture errors in data;automated and dependable data repairing on these errors has remained a notoriouslydifficult problem. In this work; we introduce an automated approach for dependably repairingdata errors; based on a novel class of fixing rules. A fixing rule contains an evidence pattern;a set of negative patterns; and a fact value. The heart of fixing rules is deterministic: given atuple; the evidence pattern and the negative patterns of a fixing rule are combined toprecisely capture which attribute is wrong; and the fact indicates how to correct this error. Westudy several fundamental problems associated with fixing rules and establish their …,Journal of Data and Information Quality (JDIQ),2017,*
A Demo of the Data Civilizer System,Raul Castro Fernandez; Dong Deng; Essam Mansour; Abdulhakim A Qahtan; Wenbo Tao; Ziawasch Abedjan; Ahmed Elmagarmid; Ihab F Ilyas; Samuel Madden; Mourad Ouzzani; Michael Stonebraker; Nan Tang,Abstract Finding relevant data for a specific task from the numerous data sources availablein any organization is a daunting task. This is not only because of the number of possibledata sources where the data of interest resides; but also due to the data being scattered allover the enterprise and being typically dirty and inconsistent. In practice; data scientists areroutinely reporting that the majority (more than 80%) of their effort is spent finding; cleaning;integrating; and accessing data of interest to a task at hand. We propose to demonstrateDATA CIVILIZER to ease the pain faced in analyzing data" in the wild". DATA CIVILIZER isan end-to-end big data management system with components for data discovery; dataintegration and stitching; data cleaning; and querying data from a large variety of storageengines; running in large enterprises.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Errata for Lightning Fast and Space Efficient Inequality Joins (PVLDB 8 (13): 2074--2085),Zuhair Khayyat; William Lucia; Meghna Singh; Mourad Ouzzani; Paolo Papotti; Jorge-Arnulfo Quiané-Ruiz; Nan Tang; Panos Kalnis,Abstract This is in response to recent feedback from some readers; which requires someclarifications regarding our IEJ oin algorithm published in [1]. The feedback revolves aroundfour points:(1) a typo in our illustrating example of the join process;(2) a naming error for theindex used by our algorithm to improve the bit array scan;(3) the sort order used in ouralgorithms; and (4) a missing explanation on how duplicates are handled by our self joinalgorithm.,Proceedings of the VLDB Endowment,2017,*
Interactive Data Repairing: the FALCON Dive,Enzo Veltri; Donatello Santoro; Giansalvatore Mecca; Paolo Papotti; Jian He; Gouliang Li; Nan Tang,Abstract. In this paper we discuss Falcon; an interactive; deterministic; and declarative datacleaning system. Unlike traditional rule-based system; Falcon does not rely on the existenceof a set of pre-defined data quality rules; but it encourages users to explore the data; identifypossible problems; and make updates to fix them. The main technical challenge consists infinding a set of rules; expressed as sql update queries; that are semantically correct and thatfixes the largest number of errors in the data. Falcon navigates the lattice by interacting withusers to gradually checking the correctness of a set of rules. We have conducted extensiveexperiments using both real-world and synthetic datasets to show that Falcon can effectivelycommunicate with users in data repairing.,25th Italian Symposium on Advanced Database Systems; SEBD 2017,2017,*
CYBER SECURITY PART 2 AUTO-IMMUNITY,A Elmagarmid; P Cochrane; M Ouzzani; WJ Al Marri; Q Malluhi; M Tang; WG Aref; Z Abedjan; X Chu; D Deng; RC Fernandez; IF Ilyas; P Papotti; M Stonebraker; N Tang; Z Khayyat; W Lucia; J Quiane-Ruiz; T Nan; Y Yu; QM Malluhi; TK Saha; HM Hammady; AK Elmagarmid; H Hammady; Z Fedorowicz; M Yakout; H Elmeleegy; Y Qi,*,US Patent App,2016,*
A System for Big Data Analytics over Diverse Data Processing Platforms,Jorge Quiane; Divy Agrawal; Sanjay Chawla; Ahmed Elmagarmid; Zoi Kaoudi; Mourad Ouzzani; Paolo Papotti; Nan Tang; Mohammed Zaki,Data analytics is at the core of any organization that wants to obtain measurable value fromits growing data assets. Data analytic tasks may range from simple to extremely complexpipelines; such as data extraction; transformation and loading; online analytical processing;graph processing; and machine learning (ML). Following the dictum “one size does not fitall”; academia and industry have embarked on a race of developing data processingplatforms for supporting all of these different tasks; eg; DBMSs and MapReduce-likesystems. Semantic completeness; high performance and scalability are key objectives ofsuch platforms. While there have been major achievements in these objectives; users arestill faced with many road-blocks. MOTIVATING EXAMPLE The first roadblock is thatapplications are tied to a single processing platform; making the migration of an …,Qatar Foundation Annual Research Conference Proceedings,2016,*
On Summarizing Graph Streams,Nan Tang; Qing Chen; Prasenjit Mitra,Abstract: Graph streams; which refer to the graph with edges being updated sequentially in aform of a stream; have wide applications such as cyber security; social networks andtransportation networks. This paper studies the problem of summarizing graph streams.Specifically; given a graph stream G; directed or undirected; the objective is to summarize Gas S with much smaller (sublinear) space; linear construction time and constantmaintenance cost for each edge update; such that S allows many queries over G to beapproximately conducted efficiently. Due to the sheer volume and highly dynamic nature ofgraph streams; summarizing them remains a notoriously hard; if not impossible; problem.The widely used practice of summarizing data streams is to treat each elementindependently by eg; hash-or sampling-based method; without keeping track of the …,arXiv preprint arXiv:1510.02219,2015,*
Data Science at QCRI,Divy Agrawal; Laure Berti; Hossam Hammady; Prasenjit Mitra; Mourad Ouzzani; Paolo Papotti; Jorge Quiane Ruiz; Nan Tang; Yin Ye; Si Yin; Mohamed Zaki," The Data Analytics group at QCRI has embarked on an ambitious endeavor to become apremiere world-class research group in Data Science by tackling diverse research topicsrelated to information extraction; data quality; data profiling; data integration; and datamining. We will present our ongoing projects to overcome different challenges encounteredin Big Data Curation; Big Data Fusion; and Big Data Analytics.(1) Big Data Curation: Due tocomplex processing and transformation layers; data errors proliferate rapidly and sometimesin an uncontrolled manner; thus compromising the value of information and impacting dataanalysis and decision making. While data quality problems can have crippling effects and noend-to-end off-the-shelf solutions to (semi-) automate error detection and correction existed;we built a commodity platform; NADEEF that can be easily customized and deployed to …,Qatar Foundation Annual Research Conference,2014,*
MonetDB,ML Kersten; PA Boncz; NJ Nes; S Manegold; KS Mullender; FE Groffen; MG Ivanova; Y Zhang; RA Goncalves; E Sidirourgos; E Liarou; S Idreos; JA deRijke; AP deVries; W Alink; R Cornacchia; JFP van deAkker; AR vanBallegooij; CA van deBerg; JR Castelo; J Flokstra; CA Galindo-Legaria; T Grust; S Héman; D Hiemstra; T Ianeva; JS Karlsson; M vanKeulen; S deKonink; JA List; N Mamoulis; GJ Molenaar; G Modena; S Göldner; AJ Pellenkoft; HGP Bosch; W Quak; G Ramirez Camps; J Rittinger; H Rode; W Scherphof; AR Schmidt; N Tang; J Teubner; C Treijtel; T Tsikrika; F Waas; THW Westerveld; MA Windhouwer; M Zukowski; A Gafriller; A Singh; A Scherpenisse; B Brodbeck; G deNijs; M Mayr; M Antonelli; M vanDinther; R Aly; R vanOs; S Mayer; S Kerschbaumer; T Ressel; T Schreiber,KNAW Narcis. Back to search results. Publication MonetDB (2005). Pagina-navigatie: Main. Savepublication: Save as MODS; Export to Mendeley; Save as EndNote; Export to RefWorks. Title;MonetDB. Author; ML Kersten (Martin); PA Boncz (Peter); NJ Nes (Niels); S. Manegold (Stefan);KS Mullender (Sjoerd); FE Groffen (Fabian); MG Ivanova (Milena); Y. Zhang (Ying); RA Goncalves(Romulo); E. Sidirourgos (Eleftherios); E. Liarou (Erietta); S. Idreos (Stratos); JA de Rijke (Arjen);AP de Vries (Arjen); W. Alink (Wouter); R. Cornacchia (Roberto); JFP van den Akker; AR vanBallegooij; CA van den Berg; JR Castelo; J. Flokstra; CA Galindo-Legaria; T. Grust; S. Héman(Sándor); D. Hiemstra; T. Ianeva; JS Karlsson; M. van Keulen; S. de Konink (Stefan …,*,2005,*
