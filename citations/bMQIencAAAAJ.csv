Efficient algorithms for processing XPath queries,Georg Gottlob; Christoph Koch; Reinhard Pichler,Abstract Our experimental analysis of several popular XPath processors reveals a strikingfact: Query evaluation in each of the systems requires time exponential in the size of queriesin the worst case. We show that XPath can be processed much more efficiently; and proposemain-memory algorithms for this problem with polynomial-time combined query evaluationcomplexity. Moreover; we show how the main ideas of our algorithm can be profitablyintegrated into existing XPath processors. Finally; we present two fragments of XPath forwhich linear-time query processing algorithms exist and another fragment with linear-space/quadratic-time query processing.,ACM Transactions on Database Systems (TODS),2005,611
Probabilistic databases,Dan Suciu; Dan Olteanu; Christopher Ré; Christoph Koch,Abstract Probabilistic databases are databases where the value of some attributes or thepresence of some records are uncertain and known only with some probability. Applicationsin many areas such as information extraction; RFID and scientific data management; datacleaning; data integration; and financial risk assessment produce large volumes of uncertaindata; which are best modeled and processed by a probabilistic database. This bookpresents the state of the art in representation formalisms and query processing techniquesfor probabilistic data. It starts by discussing the basic principles for representing largeprobabilistic databases; by decomposing them into tuple-independent tables; block-independent-disjoint tables; or U-databases. Then it discusses two classes of techniques forquery evaluation on probabilistic databases. In extensional query evaluation; the entire …,Synthesis Lectures on Data Management,2011,339
Fast and simple relational processing of uncertain data,Lyublena Antova; Thomas Jansen; Christoph Koch; Dan Olteanu,This paper introduces U-relations; a succinct and purely relational representation system foruncertain databases. U-relations support attribute-level uncertainty using verticalpartitioning. If we consider positive relational algebra extended by an operation forcomputing possible answers; a query on the logical level can be translated into; andevaluated as; a single relational algebra query on the U-relational representation. Thetranslation scheme essentially preserves the size of the query in terms of number ofoperations and; in particular; number of joins. Standard techniques employed in off-the-shelfrelational database management systems are effective for optimizing and processingqueries on U-relations. In our experiments we show that query evaluation on U-relationsscales to large amounts of data with high degrees of uncertainty.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,277
Monadic datalog and the expressive power of languages for web information extraction,Georg Gottlob; Christoph Koch,Abstract Research on information extraction from Web pages (wrapping) has seen muchactivity recently (particularly systems implementations); but little work has been done onformally studying the expressiveness of the formalisms proposed or on the theoreticalfoundations of wrapping. In this paper; we first study monadic datalog over trees as awrapping language. We show that this simple language is equivalent to monadic secondorder logic (MSO) in its ability to specify wrappers. We believe that MSO has the rightexpressiveness required for Web information extraction and propose MSO as a yardstick forevaluating and comparing wrappers. Along the way; several other results on the complexityof query evaluation and query containment for monadic datalog over trees are established;and a simple normal form for this language is presented. Using the above results; we …,Journal of the ACM (JACM),2004,240
-Path Queries on Compressed XML,Peter Buneman; Martin Grohe; Christoph Koch,This chapter presents a novel approach to querying XML by keeping compressedrepresentations of the tree structure of documents in main memory. Central to any XMLquery language is a path language such as XPath; which operates on the tree structure ofthe XML document. The chapter demonstrates that the tree structure can be effectivelycompressed and manipulated using techniques derived from symbolic model checking.Succinct representations of document tree structures based on sharing subtrees are highlyeffective. Compressed structures can be queried directly and efficiently through a process ofmanipulating selections of nodes and partial decompression. The chapter describes boththe theoretical and experimental properties of this technique and provides algorithms forquerying the compressed instances using node-selecting path query languages; such as …,*,2003,222
$ ${10^{(10^{6})}} $ $ worlds and beyond: efficient representation and processing of incomplete information,Lyublena Antova; Christoph Koch; Dan Olteanu,Abstract We present a decomposition-based approach to managing probabilisticinformation. We introduce world-set decompositions (WSDs); a space-efficient and completerepresentation system for finite sets of worlds. We study the problem of efficiently evaluatingrelational algebra queries on world-sets represented by WSDs. We also evaluate ourtechnique experimentally in a large census data scenario and show that it is both scalableand efficient.,The VLDB Journal,2009,186
The complexity of XPath query evaluation,Georg Gottlob; Christoph Koch; Reinhard Pichler,Abstract In this paper; we study the precise complexity of XPath 1.0 query processing. Eventhough heavily used by its incorporation into a variety of XML-related standards; the precisecost of evaluating an XPath query is not yet wellunderstood. The first polynomial-timealgorithm for XPath processing (with respect to combined complexity) was proposed onlyrecently; and even to this day all major XPath engines take time exponential in the size ofthe input queries. From the standpoint of theory; the precise complexity of XPath queryevaluation is open; and it is thus unknown whether the query evaluation problem can beparallelized. In this work; we show that both the data complexity and the query complexity ofXPath 1.0 fall into lower (highly parallelizable) complexity classes; but that the combinedcomplexity is PTIME-hard. Subsequently; we study the sources of this hardness and …,Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2003,169
Efficient processing of XPath queries,*,The disclosed teachings provide methods and systems for efficient evaluation of XPathqueries. In particular; the disclosed evaluation methods require only polynomial time withrespect to the total size of an input XPath query and an input XML document. Crucial for thenew methods is the notion of “context-value tables”. This idea can be further refined forqueries in Core XPath and XSLT Patterns so as to yield even a linear time evaluationmethod. Moreover; the disclosed methods can be used for improving existing methods andsystems for processing XPath expressions so to guarantee polynomial worst-casecomplexity.,*,2007,167
The Lixto data extraction project: back and forth between theory and practice,Georg Gottlob; Christoph Koch; Robert Baumgartner; Marcus Herzog; Sergio Flesca,Abstract We present the Lixto project; which is both a research project in database theoryand a commercial enterprise that develops Web data extraction (wrapping) and Web servicedefinition software. We discuss the project's main motivations and ideas; in particular the useof a logic-based framework for wrapping. Then we present theoretical results on monadicdatalog over trees and on Elog; its close relative which is used as the internal wrapperlanguage in the Lixto system. These results include both a characterization of the expressivepower and the complexity of these languages. We describe the visual wrapper specificationprocess in Lixto and various practical aspects of wrapping. We discuss work on thecomplexity of query languages for trees that was inseminated by our theoretical study oflogic-based languages for wrapping. Then we return to the practice of wrapping and the …,Proceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2004,159
MayBMS: a probabilistic database management system,Jiewen Huang; Lyublena Antova; Christoph Koch; Dan Olteanu,Abstract MayBMS is a state-of-the-art probabilistic database management system whichleverages the strengths of previous database research for achieving scalability. As a proof ofconcept for its ease of use; we have built on top of MayBMS a Web-based application thatoffers NBA-related information based on what-if analysis of team dynamics using dataavailable at www. nba. com.,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,146
XPath leashed,Michael Benedikt; Christoph Koch,Abstract This survey gives an overview of formal results on the XML query language XPath.We identify several important fragments of XPath; focusing on subsets of XPath 1.0. We thengive results on the expressiveness of XPath and its fragments compared to other formalismsfor querying trees; algorithms; and complexity bounds for evaluation of XPath queries; aswell as static analysis of XPath queries.,ACM Computing Surveys (CSUR),2009,146
Conjunctive queries over trees,Georg Gottlob; Christoph Koch; Klaus U Schulz,Abstract We study the complexity and expressive power of conjunctive queries overunranked labeled trees represented using a variety of structure relations such as“child”;“descendant”; and “following” as well as unary relations for node labels. We establisha framework for characterizing structures representing trees for which conjunctive queriescan be evaluated efficiently. Then we completely chart the tractability frontier of the problemand establish a dichotomy theorem for our axis relations; that is; we find all subset-maximalsets of axes for which query evaluation is in polynomial time and show that for all othercases; query evaluation is NP-complete. All polynomial-time results are obtainedimmediately using the proof techniques from our framework. Finally; we study theexpressiveness of conjunctive queries over trees and show that for each conjunctive …,Journal of the ACM (JACM),2006,135
Conditioning probabilistic databases,Christoph Koch; Dan Olteanu,Abstract Past research on probabilistic databases has studied the problem of answeringqueries on a static database. Application scenarios of probabilistic databases however ofteninvolve the conditioning of a database using additional information in the form of newevidence. The conditioning problem is thus to transform a probabilistic database of priorsinto a posterior probabilistic database which is materialized for subsequent queryprocessing or further refinement. It turns out that the conditioning problem is closely relatedto the problem of computing exact tuple confidence values. It is known that exact confidencecomputation is an NP-hard problem. This has led researchers to consider approximationtechniques for confidence computation. However; neither conditioning nor exact confidencecomputation can be solved using such techniques. In this paper we present efficient …,Proceedings of the VLDB Endowment,2008,133
Schema-based scheduling of event processors and buffer minimization for queries on structured data streams,Christoph Koch; Stefanie Scherzinger; Nicole Schweikardt; Bernhard Stegmaier,Abstract We introduce an extension of the XQuery language; FluX; that supports event-based query processing and the conscious handling of main memory buffers. Purely event-based queries of this language can be executed on streaming XML data in a very direct way.We then develop an algorithm that allows to efficiently rewrite XQueries into the event-basedFluX language. This algorithm uses order constraints from a DTD to schedule eventhandlers and to thus minimize the amount of buffering required for evaluating a query. Wediscuss the various technical aspects of query optimization and query evaluation within ourframework. This is complemented with an experimental evaluation of our approach.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,124
Monadic queries over tree-structured data,Georg Gottlob; Christoph Koch,Monadic query languages over trees currently receive considerable interest in the databasecommunity; as the problem of selecting nodes from a tree is the most basic and widespreaddatabase query problem in the context of XML. Partly a survey of recent work done by theauthors and their group on logical query languages for this problem and theirexpressiveness; this paper provides a number of new results related to the complexity ofsuch languages over so-called axis relations (such as" child" or" descendant") which aremotivated by their presence in the XPath standard or by their utility for data extraction(wrapping).,Logic in Computer Science; 2002. Proceedings. 17th Annual IEEE Symposium on,2002,122
MayBMS: Managing incomplete information with probabilistic world-set decompositions,Lyublena Antova; Christoph Koch; Dan Olteanu,Managing incomplete information is important in many real world applications. In thisdemonstration we present MayBMS-a system for representing and managing finite sets ofpossible worlds-that successfully combines expressiveness and efficiency. Some features ofMayBMS are: completeness of the representation system for finite world-sets; space-efficientrepresentation of large world-sets; scalable evaluation and support for full relational algebraqueries; and probabilistic extension of the representation system and the query language.MayBMS is implemented on top of PostgreSQL. It models incomplete data using the so-called world-set decompositions (WSDs)(Ruggles et al.; 2004). For this demonstration; weintroduce a probabilistic extension of world-sets and WSDs; where worlds or correlationsbetween worlds have probabilities. The main idea underlying probabilistic WSDs is to use …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,104
FluXQuery: An optimizing XQuery processor for streaming XML data,Christoph Koch; Stefanie Scherzinger; Nicole Schweikardt; Bernhard Stegmaier,XML has established itself as the ubiquitous format for data exchange on the Internet. Animminent development is that of streams of XML data being exchanged and queried. Datamanagement scenarios where XQuery [11] is evaluated on XML streams are becomingincreasingly important and realistic; eg in e-commerce settings. Naturally; query enginesemployed for stream processing are main-memory-based; yet contemporary XQueryengines consume main memory in large multiples of the actual size of the input documents(cf.[10; 8]). This excessive need for buffers has proven to be a serious scalability issue andsignificant research challenge [10; 9; 5; 3].,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,103
Query evaluation on compressed trees,Markus Frick; Martin Grohe; Christoph Koch,This paper studies the problem of evaluating unary (or node-selecting) queries on unrankedtrees compressed in a natural structure-preserving way; by the sharing of common subtrees.The motivation to study unary queries on unranked trees comes from the database field;where querying XML (Extensible Markup Language) documents; which can be consideredas unranked labeled trees; is an important task. We give algorithms and complexity resultsfor the evaluation of XPath and monadic datalog queries. Furthermore; we propose a newautomata-theoretic formalism for querying trees and give algorithms for evaluating queriesdefined by such automata.,Logic in Computer Science; 2003. Proceedings. 18th Annual IEEE Symposium on,2003,100
From complete to incomplete information and back,Lyublena Antova; Christoph Koch; Dan Olteanu,Abstract Incomplete information arises naturally in numerous data managementapplications. Recently; several researchers have studied query processing in the context ofincomplete information. Most work has combined the syntax of a traditional query languagelike relational algebra with a nonstandard semantics such as certain or ranked possibleanswers. There are now also languages with special features to deal with uncertainty.However; to the standards of the data management community; to date no languageproposal has been made that can be considered a natural analog to SQL or relationalalgebra for the case of incomplete information. In this paper we propose such a language;World-set Algebra; which satisfies the robustness criteria and analogies to relational algebrathat we expect. The language supports the contemplation on alternatives and can thus …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,99
Sprout: Lazy vs. eager query plans for tuple-independent probabilistic databases,Dan Olteanu; Jiewen Huang; Christoph Koch,A paramount challenge in probabilistic databases is the scalable computation ofconfidences of tuples in query results. This paper introduces an efficient secondary-storageoperator for exact computation of queries on tuple-independent probabilistic databases. Weconsider the conjunctive queries without self-joins that are known to be tractable on anytuple-independent database; and queries that are not tractable in general but becometractable on probabilistic databases restricted by functional dependencies. Our operator issemantically equivalent to a sequence of aggregations and can be naturally integrated intoexisting relational query plans. As a proof of concept; we developed an extension of thePostgreSQL 8.3. 3 query engine called SPROUT. We study optimizations that push or pullour operator or parts thereof past joins. The operator employs static information; such as …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,97
The complexity of XPath query evaluation and XML typing,Georg Gottlob; Christoph Koch; Reinhard Pichler; Luc Segoufin,Abstract We study the complexity of two central XML processing problems. The first is XPath1.0 query processing; which has been shown to be in PTIME in previous work. We prove thatboth the data complexity and the query complexity of XPath 1.0 fall into lower (highlyparallelizable) complexity classes; while the combined complexity is PTIME-hard.Subsequently; we study the sources of this hardness and identify a large and practicallyimportant fragment of XPath 1.0 for which the combined complexity is LOGCFL-completeand; therefore; in the highly parallelizable complexity class NC 2. The second problem is thecomplexity of validating XML documents against various typing schemes like DocumentType Definitions (DTDs); XML Schema Definitions (XSDs); and tree automata; both withrespect to data and to combined complexity. For data complexity; we prove that validation …,Journal of the ACM (JACM),2005,97
-Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage: A Tree Automata-based Approach,Christoph Koch,This chapter proposes a new; highly scalable; and efficient technique for evaluating node-selecting queries on XML trees; which is based on recent advances in the theory of treeautomata. It discusses an efficient processing of expressive node-selecting queries on XMLdata in secondary storage. The experiments demonstrate the immediate practical usefulnessof the approach of using tree automata for the evaluation of expressive node-selectingqueries on trees in secondary storage; but more experiments are in place and under way.The approach has a number of interesting properties that need to be studied and possiblyexploited in the future. Tree automata-based query processing lends itself to parallel queryprocessing. Multiple query evaluation: Tree-Marking Normal Form (TMNF) programs canevaluate several queries (each one defined by one IDB predicate) in one program. It will …,*,2003,88
XPath query evaluation: Improving time and space efficiency,Georg Gottlob; Christoph Koch; Reinhard Pichler,Contemporary XPath query engines evaluate queries in time exponential in the sizes ofinput queries; a fact that has gone unnoticed for a long time. Recently; the first main-memoryevaluation algorithm for XPath 1.0 with polynomial time combined complexity; ie; which runsin polynomial time both with respect to the size of the data and the queries; has beenpublished (cf.[G. Gottlob; et al.;(2002)]. We present several important improvements andextensions of that work; including new XPath processing algorithms with improved time andspace efficiency. Moreover; we define a very large and practically relevant fragment of XPathfor which a further optimized form of query evaluation is possible. Apart from its immediaterelevance for XPath query processing; our work also sheds new light at those features ofXPath 1.0 which are most costly relative to their practical usefulness.,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,87
DBToaster: higher-order delta processing for dynamic; frequently fresh views,Christoph Koch; Yanif Ahmad; Oliver Kennedy; Milos Nikolic; Andres Nötzli; Daniel Lupei; Amir Shaikhha,Abstract Applications ranging from algorithmic trading to scientific data analysis require real-time analytics based on views over databases receiving thousands of updates each second.Such views have to be kept fresh at millisecond latencies. At the same time; these viewshave to support classical SQL; rather than window semantics; to enable applications thatcombine current with aged or historical data. In this article; we present the DBToastersystem; which keeps materialized views of standard SQL queries continuously fresh as datachanges very rapidly. This is achieved by a combination of aggressive compilationtechniques and DBToaster's original recursive finite differencing technique whichmaterializes a query and a set of its higher-order deltas as views. These views support eachother's incremental maintenance; leading to a reduced overall view maintenance cost …,The VLDB Journal,2014,84
Scaling games to epic proportions,Walker White; Alan Demers; Christoph Koch; Johannes Gehrke; Rajmohan Rajagopalan,Abstract We introduce scalability for computer games as the next frontier for techniques fromdata management. A very important aspect of computer games is the artificial intelligence(AI) of non-player characters. To create interesting AI in games today; developers or playershave to create complex; dynamic behavior for a very small number of characters; but neitherthe game engines nor the style of AI programming enables intelligent behavior that scales toa very large number of non-player characters. In this paper we make a first step towards trulyscalable AI in computer games by modeling game AI as a data management problem. Wepresent a highly expressive scripting language SGL that provides game designers andplayers with a data-driven AI scheme for customizing behavior for individual non-playercharacters. We use sophisticated query processing and indexing techniques to efficiently …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,79
World-set decompositions: Expressiveness and efficient algorithms,Dan Olteanu; Christoph Koch; Lyublena Antova,Abstract Uncertain information is commonplace in real-world data management scenarios.The ability to represent large sets of possible instances (worlds) while supporting efficientstorage and processing is an important challenge in this context. The recent formalism ofworld-set decompositions (WSDs) provides a space-efficient representation for uncertaindata that also supports scalable processing. WSDs are complete for finite world-sets in thatthey can represent any finite set of possible worlds. For possibly infinite world-sets; we showthat a natural generalization of WSDs precisely captures the expressive power of c-tables.We then show that several important problems are efficiently solvable on WSDs while theyare NP-hard on c-tables. Finally; we give a polynomial-time algorithm for factorizing WSDs;ie an efficient algorithm for minimizing such representations.,Theoretical Computer Science,2008,75
Building efficient query engines in a high-level language,Yannis Klonatos; Christoph Koch; Tiark Rompf; Hassan Chafi,Abstract In this paper we advocate that it is time for a radical rethinking of database systemsdesign. Developers should be able to leverage high-level programming languages withouthaving to pay a price in efficiency. To realize our vision of abstraction without regret; wepresent LegoBase; a query engine written in the high-level programming language Scala.The key technique to regain efficiency is to apply generative programming: the Scala codethat constitutes the query engine; despite its high-level appearance; is actually a programgenerator that emits specialized; low-level C code. We show how the combination of high-level and generative programming allows to easily implement a wide spectrum ofoptimizations that are difficult to achieve with existing low-level query compilers; and how itcan continuously optimize the query engine. We evaluate our approach with the TPC-H …,Proceedings of the VLDB Endowment,2014,73
Approximate confidence computation in probabilistic databases,Dan Olteanu; Jiewen Huang; Christoph Koch,This paper introduces a deterministic approximation algorithm with error guarantees forcomputing the probability of propositional formulas over discrete random variables. Thealgorithm is based on an incremental compilation of formulas into decision diagrams usingthree types of decompositions: Shannon expansion; independence partitioning; and productfactorization. With each decomposition step; lower and upper bounds on the probability ofthe partially compiled formula can be quickly computed and checked against the allowederror.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,73
Causality in databases,Alexandra Meliou; Wolfgang Gatterbauer; Joseph Y Halpern; Christoph Koch; Katherine F Moore; Dan Suciu,Abstract Provenance is often used to validate data; by verifying its origin and explaining itsderivation. When searching for “causes” of tuples in the query results or in generalobservations; the analysis of lineage becomes an essential tool for providing suchjustifications. However; lineage can quickly grow very large; limiting its immediate use forproviding intuitive explanations to the user. The formal notion of causality is a more refinedconcept that identifies causes for observations based on user-defined criteria; and thatassigns to them gradual degrees of responsibility based on their respective contributions. Inthis paper; we initiate a discussion on causality in databases; give some simple definitions;and motivate this formalism through a number of example applications.,IEEE Data Eng. Bull.,2010,72
On the complexity of nonrecursive XQuery and functional query languages on complex values,Christoph Koch,Abstract This article studies the complexity of evaluating functional query languages forcomplex values such as monad algebra and the recursion-free fragment of XQuery. Weshow that monad algebra; with equality restricted to atomic values; is complete for the classTA [2 O (n); O (n)] of problems solvable in linear exponential time with a linear number ofalternations if the query is assumed to be part of the input. The monotone fragment of monadalgebra with atomic value equality but without negation is NEXPTIME-complete. For monadalgebra with deep value equality; that is; equality of complex values; we establish TA [2 O(n); O (n)] lower and exponential-space upper bounds. We also study a fragment of XQuery;Core XQuery; that seems to incorporate all the features of a query language on complexvalues that are traditionally deemed essential. A close connection between monad …,ACM Transactions on Database Systems (TODS),2006,72
Enhancing disjunctive logic programming systems by SAT checkers,Christoph Koch; Nicola Leone; Gerald Pfeifer,Abstract Disjunctive logic programming (DLP) with stable model semantics is a powerfulnonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLPis harder than with normal (∨-free) logic programs; because stable model checking—deciding whether a given model is a stable model of a propositional DLP program—is co-NP-complete; while it is polynomial for normal logic programs. This paper proposes a newtransformation Γ M (P); which reduces stable model checking to UNSAT—ie; to decidingwhether a given CNF formula is unsatisfiable. The stability of a model M of a program P thuscan be verified by calling a Satisfiability Checker on the CNF formula Γ M (P). Thetransformation is parsimonious (ie; no new symbol is added); and efficiently computable; asit runs in logarithmic space (and therefore in polynomial time). Moreover; the size of the …,Artificial Intelligence,2003,71
MayBMS: A system for managing large uncertain and probabilistic databases,Christoph Koch,Abstract MayBMS is a state-of-the-art probabilistic database management system that hasbeen built as an extension of Postgres; an open-source relational database managementsystem. MayBMS follows a principled approach to leveraging the strengths of previousdatabase research for achieving scalability. This chapter describes the main goals of thisproject; the design of query and update language; efficient exact and approximate queryprocessing; and algorithmic and systems aspects. Acknowledgments. My collaborators onthe MayBMS project are Dan Olteanu (Oxford University); Lyublena Antova (Cornell); JiewenHuang (Oxford); and Michaela Goetz (Cornell). Thomas Jansen and Ali Baran Sari arealumni of the MayBMS team. I thank Dan Suciu for the inspirational talk he gave at aDagstuhl seminar in February of 2005; which triggered my interest in probabilistic …,Managing and Mining Uncertain Data,2009,65
The DLV system for knowledge representation and reasoning,Nicola Leone; Gerald Pfeifer; Wolfgang Faber; Thomas Eiter; Georg Gottlob; Simona Perri; Francesco Scarcello,Abstract Disjunctive Logic Programming (DLP) is an advanced formalism for knowledgerepresentation and reasoning; which is very expressive in a precise mathematical sense: itallows to express every property of finite structures that is decidable in the complexity classΣP 2 (NPNP). Thus; under widely believed assumptions; DLP is strictly more expressive thannormal (disjunction-free) logic programming; whose expressiveness is limited to propertiesdecidable in NP. Importantly; apart from enlarging the class of applications which can beencoded in the language; disjunction often allows for representing problems of lowercomplexity in a simpler and more natural fashion. This paper presents the DLV system;which is widely considered the state-of-the-art implementation of disjunctive logicprogramming; and addresses several aspects. As for problem solving; we provide a …,In ACM Transaction on Computational Logic. To Appear,2005,65
Attribute grammars for scalable query processing on XML streams,Christoph Koch; Stefanie Scherzinger,Abstract We introduce the notion of XML Stream Attribute Grammars (XSAGs). XSAGs arethe first scalable query language for XML streams (running strictly in linear time withbounded memory consumption independent of the size of the stream) that allows for actualdata transformations rather than just document filtering. XSAGs are also relatively easy touse for humans. Moreover; the XSAG formalism provides a strong intuition for which queriescan or cannot be processed scalably on streams. We introduce XSAGs together with thenecessary language-theoretic machinery; study their theoretical properties such asexpressiveness and complexity; and discuss their implementation.,The VLDB Journal—The International Journal on Very Large Data Bases,2007,61
The DLV system,Nicola Leone; Gerald Pfeifer; Wolfgang Faber; Francesco Calimeri; Tina Dell’Armi; Thomas Eiter; Georg Gottlob; Giovambattista Ianni; Giuseppe Ielpa; Christoph Koch; Simona Perri; Axel Polleres,Abstract The development of the DLV system has started as a research projectfinanced byFWF (the Austrian Science Funds) in 1996; and has evolved into an internationalcollaboration over the years. Currently; the University of Calabria and TU Wien participate inthe project; supported by a scientific-technological collaboration between Italy and Austria.At the time of writing; the latest version of the system has been released on April 12; 2002.,European Workshop on Logics in Artificial Intelligence,2002,61
The dlv system: Model generator and application frontends,Simona Citrigno; Thomas Eiter; Wolfgang Faber; Georg Gottlob; Christoph Koch; Nicola Leone; Cristinel Mateis; Gerald Pfeifer; Francesco Scarcello,Abstract During the last years; much research has been done concerning semantics andcomplexity of Disjunctive Deductive Databases (DDDBs). While DDDBs| function-freedisjunctive logic programs with negation in rule bodies allowed| are now generallyconsidered a powerful tool for common-sense reasoning and knowledge representation;there has been a shortage of actual (let alone e cient) implementations (ST94; ADN97]). Thispaper presents a brief overview of the architecture of the dlv (datalog with disjunction)system system currently developed at TU Wien in the FWF project P11580-MAT\A QuerySystem for Disjunctive Deductive Databases"; especially focusing on the Model Generator{the\heart" of the dlv system {and the integrated frontends for diagnostic reasoning andSQL3.,Proceedings of the 12th Workshop on Logic Programming,1997,59
Tight lower bounds for query processing on streaming and external memory data,Martin Grohe; Christoph Koch; Nicole Schweikardt,Abstract We study a clean machine model for external memory and stream processing. Weshow that the number of scans of the external data induces a strict hierarchy (as long aswork space is sufficiently small; eg; polylogarithmic in the size of the input). We also showthat neither joins nor sorting are feasible if the product of the number r (n) of scans of theexternal memory and the size s (n) of the internal memory buffers is sufficiently small; eg; ofsize o(\sqrtn5). We also establish tight bounds for the complexity of XPath evaluation andfiltering.,Theoretical Computer Science,2007,58
Massively multi-query join processing in publish/subscribe systems,Mingsheng Hong; Alan J Demers; Johannes E Gehrke; Christoph Koch; Mirek Riedewald; Walker M White,Abstract There has been much recent interest in XML publish/subscribe systems. Somesystems scale to thousands of concurrent queries; but support a limited query language(usually a fragment of XPath 1.0). Other systems support more expressive languages; but donot scale well with the number of concurrent queries. In this paper; we propose a set of novelquery processing techniques; referred to as Massively Multi-Query Join Processingtechniques; for processing a large number of XML stream queries involving value joins overmultiple XML streams and documents. These techniques enable the sharing ofrepresentations of inputs to multiple joins; and the sharing of join computation. Ourtechniques are also applicable to relational event processing systems and publish/subscribesystems that support join queries. We present experimental results to demonstrate the …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,57
Querying the web reconsidered: Design principles for versatile web query languages,François Bry; Christoph Koch; Tim Furche; Sebastian Schaffert; Liviu Badea; Sacha Berger,Abstract A decade of experience with research proposals as well as standardized querylanguages for the conventional Web and the recent emergence of query languages for theSemantic Web call for a reconsideration of design principles for Web and Semantic Webquery languages. This chapter first argues that a new generation of versatile Web querylanguages is needed for solving the challenges posed by the changing Web: We callversatile those query languages able to cope with both Web and Semantic Web dataexpressed in any (Web or Semantic Web) markup language. This chapter further suggeststhat well-known referential transparency and novel answer-closedness are essentialfeatures of versatile query languages. Indeed; they allow queries to be considered like formsand answers like form-fillings in the spirit of the query-by-example paradigm. This chapter …,International Journal on Semantic Web and Information Systems (IJSWIS),2005,57
Incremental query evaluation in a ring of databases,Christoph Koch,Abstract This paper approaches the incremental view maintenance problem from analgebraic perspective. We construct the algebraic structure of a ring of databases and use itas the foundation of the design of a query calculus that allows to express powerfulaggregate queries. The query calculus inherits key properties of the ring; such as having anormal form of polynomials and being closed under computing inverses and delta queries.The k-th delta of a polynomial query of degree k without nesting is purely a function of theupdate; not of the database. This gives rise to a method of eliminating expensive queryoperators such as joins from programs that perform incremental view maintenance. Themain result is that; for non-nested queries; each individual aggregate value can beincrementally maintained using a constant amount of work. This is not possible for …,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,47
Better scripts; better games,Walker White; Christoph Koch; Johannes Gehrke; Alan Demers,The video game industry earned $8.85 billion in revenue in 2007; almost as much as moviesmade at the box office. Much of this revenue was generated by blockbuster titles created by largegroups of people. Though large development teams are not unheard of in the softwareindustry; game studios tend to have unique collections of developers. Software engineers makeup a relatively small portion of the game development team; while the majority of the team consistsof content creators such as artists; musicians; and designers … Since content creation is sucha major part of game development; game studios spend many resources developing tools tointegrate content into their software. For example; entry-level programmers typically make toolsto allow artists to manage assets or to allow designers to place challenges and rewards in thegame. These tools export information in a format usable by the software engineers; either …,Communications of the ACM,2009,44
Database research opportunities in computer games,Walker White; Christoph Koch; Nitin Gupta; Johannes Gehrke; Alan Demers,Abstract In this paper; we outline several ways in which the database community cancontribute to the development of technology for computer games. We outline the architectureof different types of computer games; and show how database technology plays a role intheir design. From this; we identify several new research directions to improve the utilizationof this technology in computer games.,ACM SIGMOD Record,2007,44
Approximating predicates and expressive queries on probabilistic databases,Christoph Koch,Abstract We study complexity and approximation of queries in an expressive querylanguage for probabilistic databases. The language studied supports the compositional useof confidence computation. It allows for a wide range of new use cases; such as thecomputation of conditional probabilities and of selections based on predicates that involvemarginal and conditional probabilities. These features have important applications in areassuch as data cleaning and the processing of sensor data. We establish techniques forefficiently computing approximate query results and for estimating the error incurred byqueries. The central difficulty is due to selection predicates based on approximated values;which may lead to the unreliable selection of tuples. A database may contain certainsingularities at which approximation of predicates cannot be achieved; however; the …,Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2008,43
XPath processing in a nutshell,Georg Gottlob; Christoph Koch; Reinhard Pichler,Abstract We provide a concise yet complete formal definition of the semantics of XPath 1 andsummarize efficient algorithms for processing queries in this language. Our presentation isintended both for the reader who is looking for a short but comprehensive formal account ofXPath as well as the software developer in need of material that facilitates the rapidimplementation of XPath engines.,ACM SIGMOD Record,2003,43
Sparse projections onto the simplex,Anastasios Kyrillidis; Stephen Becker; Volkan Cevher; Christoph Koch,Abstract Most learning methods with rank or sparsity constraints use convex relaxations;which lead to optimization with the nuclear norm or the l1-norm. However; several importantlearning applications cannot benefit from this approach as they feature these convex normsas constraints in addition to the non-convex rank and sparsity constraints. In this setting; wederive efficient sparse projections onto the simplex and its extension; and illustrate how touse them to solve high-dimensional learning problems in quantum tomography; sparsedensity estimation and portfolio selection with non-convex constraints.,International Conference on Machine Learning,2013,41
Query language support for incomplete information in the MayBMS system,Lyublena Antova; Christoph Koch; Dan Olteanu,Abstract MayBMS [4; 1; 3; 2] is a data management system for incomplete informationdeveloped at Saarland University. Its main features are a simple and compactrepresentation system for incomplete information and a language called I-SQL with explicitoperations for handling uncertainty. MayBMS is currently an extension of PostgreSQL andmanages both complete and incomplete data and evaluates I-SQL queries.,Proceedings of the 33rd international conference on Very large data bases,2007,40
Combined static and dynamic analysis for effective buffer minimization in streaming XQuery evaluation,Michael Schmidt; Stefanie Scherzinger; Christoph Koch,Effective buffer management is crucial for efficient in-memory and streaming XQueryprocessing. We propose a buffer management scheme which combines static and dynamicanalysis to keep main memory consumption low. Our approach relies on a technique that wecall active garbage collection and which actively purges buffers at runtime based on thecurrent status of query evaluation. We have built a prototype system for a practical fragmentof XQuery which employs our buffer management scheme. The experimental resultsdemonstrate the significant impact of combined static and dynamic analysis on reducingmain memory consumption and running time.,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,40
DBToaster: A SQL compiler for high-performance delta processing in main-memory databases,Yanif Ahmad; Christoph Koch,Abstract We present DBToaster; a novel query compilation framework for producing highperformance compiled query executors that incrementally and continuously answer standingaggregate queries using in-memory views. DBToaster targets applications that requireefficient main-memory processing of standing queries (views) fed by high-volume datastreams; recursively compiling view maintenance (VM) queries into simple C++ functions forevaluating database updates (deltas). While today's VM algorithms consider the impact ofsingle deltas on view queries to produce maintenance queries; we recursively considerdeltas of maintenance queries and compile to thoroughly transform queries into code.Recursive compilation successively elides certain scans and joins; and eliminatessignificant query plan interpreter overheads. In this demonstration; we walk through our …,Proceedings of the VLDB Endowment,2009,39
The homeostasis protocol: Avoiding transaction coordination through program analysis,Sudip Roy; Lucja Kot; Gabriel Bender; Bailu Ding; Hossein Hojjat; Christoph Koch; Nate Foster; Johannes Gehrke,Abstract Datastores today rely on distribution and replication to achieve improvedperformance and fault-tolerance. But correctness of many applications depends on strongconsistency properties--something that can impose substantial overheads; since it requirescoordinating the behavior of multiple nodes. This paper describes a new approach toachieving strong consistency in distributed systems while minimizing communicationbetween nodes. The key insight is to allow the state of the system to be inconsistent duringexecution; as long as this inconsistency is bounded and does not affect transactioncorrectness. In contrast to previous work; our approach uses program analysis to extractsemantic information about permissible levels of inconsistency and is fully automated. Wethen employ a novel homeostasis protocol to allow sites to operate independently …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,38
Rule-based multi-query optimization,Mingsheng Hong; Mirek Riedewald; Christoph Koch; Johannes Gehrke; Alan Demers,Abstract Data stream management systems usually have to process many long-runningqueries that are active at the same time. Multiple queries can be evaluated more efficientlytogether than independently; because it is often possible to share state and computation.Motivated by this observation; various Multi-Query Optimization (MQO) techniques havebeen proposed. However; these approaches suffer from two limitations. First; they focus onvery specialized workloads. Second; integrating MQO techniques for CQL-style streamengines and those for event pattern detection engines is even harder; as the processingmodels of these two types of stream engines are radically different. In this paper; we proposea rule-based MQO framework. This framework incorporates a set of new abstractions;extending their counterparts; physical operators; transformation rules; and streams; in a …,Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,2009,38
Scalable and adaptive online joins,Mohammed Elseidy; Abdallah Elguindy; Aleksandar Vitorovic; Christoph Koch,Abstract Scalable join processing in a parallel shared-nothing environment requires apartitioning policy that evenly distributes the processing load while minimizing the size ofstate maintained and number of messages communicated. Previous research proposesstatic partitioning schemes that require statistics beforehand. In an online or streamingenvironment in which no statistics about the workload are known; traditional staticapproaches perform poorly. This paper presents a novel parallel online dataflow joinoperator that supports arbitrary join predicates. The proposed operator continuously adjustsitself to the data dynamics through adaptive dataflow routing and state repartitioning. Theoperator is resilient to data skew; maintains high throughput rates; avoids blocking behaviorduring state repartitioning; takes an eventual consistency approach for maintaining its …,Proceedings of the VLDB Endowment,2014,36
Query rewriting with symmetric constraints,Christoph Koch,Abstract We address the problem of answering queries using expressive symmetric inter-schema constraints which allow to establish mappings between several heterogeneousinformation systems. This problem is of high relevance to data integration; as symmetricconstraints are essential for dealing with true concept mismatch and are generalizations ofthe kinds of mappings supported by both local-as-view and global-as-view approaches thatwere previously studied in the literature. Moreover; the flexibility gained by using suchconstraints for data integration is essential for virtual enterprise and e-commerceapplications. We first discuss resolution-based methods for computing maximally containedrewritings and characterize computability aspects. Then we propose an alternative butsemantically equivalent perspective based on a generalization of results relating to the …,AI Communications,2004,33
From XQuery to relational logics,Michael Benedikt; Christoph Koch,Abstract Predicate logic has long been seen as a good foundation for querying relationaldata. This is embodied in the correspondence between relational calculus and first-orderlogic; and can also be seen in mappings from fragments of the standard relational querylanguage SQL to extensions of first-order logic (eg with counting). A key question is what isthe analog to this correspondence for querying tree-structured data; as seen; for example; inXML documents. We formalize this as the question of the appropriate logical query languagefor defining transformations on tree-structured data. The predominant practitioner paradigmfor defining such transformations is top-down tree building. This is embodied by the XQueryquery language; which builds the output tree in parallel starting at the root; based onvariable bindings and nodeset queries in the XPath language. The goal of this article is to …,ACM Transactions on Database Systems (TODS),2009,30
System description: DLV,Tina Dell’Armi; Wolfgang Faber; Giuseppe Ielpa; Christoph Koch; Nicola Leone; Simona Perri; Gerald Pfeifer,Abstract DLV is an efficient Answer Set Programming (ASP) system implementing theconsistent answer set semantics [5] with various language enhancements like support forlogic programming with inheritance and queries; integer arithmetics; and various other built-in predicates.,International Conference on Logic Programming and Nonmonotonic Reasoning,2001,30
Data integration against multiple evolving autonomous schemata,Christoph Koch,Abstract Research in the area of data integration has resulted in approaches such asfederated and multidatabases; mediation; data warehousing; global information systems;and the model management/schema matching approach. Architecturally; approaches canbe categorized into those that integrate against a single global schema and those that donot; while on the level of inter-schema constraints; most work can be classied either as so-called global-as-view or as local-as-view integration. These approaches dier widely in theirstrengths and weaknesses. Federated databases have been found applicable inenvironments in which several autonomous information systems coexist {each with theirindividual schemata {and need to share data. However; this approach does not providesucient support for dealing with change of schemata and requirements. Other approaches …,*,2001,30
PIP: A database system for great and small expectations,Oliver Kennedy; Christoph Koch,Estimation via sampling out of highly selective join queries is well known to be problematic;most notably in online aggregation. Without goal-directed sampling strategies; samplesfalling outside of the selection constraints lower estimation efficiency at best; and causeinaccurate estimates at worst. This problem appears in general probabilistic databasesystems; where query processing is tightly coupled with sampling. By committing to a set ofsamples before evaluating the query; the engine wastes effort on samples that will bediscarded; query processing that may need to be repeated; or unnecessarily large numbersof samples.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,28
Cooperative update exchange in the Youtopia system,Łucja Kot; Christoph Koch,Abstract Youtopia is a platform for collaborative management and integration of relationaldata. At the heart of Youtopia is an update exchange abstraction: changes to the datapropagate through the system to satisfy user-specified mappings. We present a novelchange propagation model that combines a deterministic chase with human intervention.The process is fundamentally cooperative and gives users significant control over howmappings are repaired. An additional advantage of our model is that mapping cycles can bepermitted without compromising correctness. We investigate potential harmful interferencebetween updates in our model; we introduce two appropriate notions of serializability thatavoid such interference if enforced. The first is very general and related to classical final-state serializability; the second is more restrictive but highly practical and related to …,Proceedings of the VLDB Endowment,2009,28
Logic-based web information extraction,Georg Gottlob; Christoph Koch,Abstract The Web wrapping proble; ie; the problem of extracting structured information fromHTML documents; is one of great practical importance. The often observed informationoverload that users of the Web experience witnesses the lack of intelligent andencompassing Web services that provide high-quality collected and value-addedinforamtion. The Web wrapping problem has been addressed by a significant amount ofresearch work. Previous work can be classified into two categories; depending on whetherthe HTML input is regarded as a sequential character string (eg;[34; 27; 24; 30; 23]) or a pre-parsed document tree (for instance;[35; 25; 22; 29; 3; 2; 26]). The latter category of work thusassumes that systems may make use of an existing HTML parser as a front and.,ACM SIGMOD Record,2004,24
On probabilistic fixpoint and markov chain query languages,Daniel Deutch; Christoph Koch; Tova Milo,Abstract We study highly expressive query languages such as datalog; fixpoint; and while-languages on probabilistic databases. We generalize these languages such thatcomputation steps (eg datalog rules) can fire probabilistically. We define two possiblesemantics for such query languages; namely inflationary semantics where the results ofeach computation step are added to the current database and noninflationary queries thatinduce a random walk in-between database instances. We then study the complexity ofexact and approximate query evaluation under these semantics.,Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2010,23
Processing queries on tree-structured data efficiently,Christoph Koch,Abstract This is a survey of algorithms; complexity results; and general solution techniquesfor efficiently processing queries on tree-structured data. I focus on query languages thatcompute nodes or tuples of nodes—conjunctive queries; first-order queries; datalog; andXPath. I also point out a number of connections among previous results that have not beenobserved before.,Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2006,23
Yin-yang: concealing the deep embedding of DSLs,Vojin Jovanovic; Amir Shaikhha; Sandro Stucki; Vladimir Nikolaev; Christoph Koch; Martin Odersky,Abstract Deeply embedded domain-specific languages (EDSLs) intrinsically compromiseprogrammer experience for improved program performance. Shallow EDSLs complementthem by trading program performance for good programmer experience. We present Yin-Yang; a framework for DSL embedding that uses Scala macros to reliably translate shallowEDSL programs to the corresponding deep EDSL programs. The translation allows programprototyping and development in the user friendly shallow embedding; while thecorresponding deep embedding is used where performance is important. The reliability ofthe translation completely conceals the deep em-bedding from the user. For the DSL author;Yin-Yang automatically generates the deep DSL embeddings from their shallowcounterparts by reusing the core translation. This obviates the need for code duplication …,Acm Sigplan Notices,2014,21
Approximation schemes for many-objective query optimization,Immanuel Trummer; Christoph Koch,Abstract The goal of multi-objective query optimization (MOQO) is to find query plans thatrealize a good compromise between conflicting objectives such as minimizing executiontime and minimizing monetary fees in a Cloud scenario. A previously proposed exhaustiveMOQO algorithm needs hours to optimize even simple TPC-H queries. This is why wepropose several approximation schemes for MOQO that generate guaranteed near-optimalplans in seconds where exhaustive optimization takes hours. We integrated all MOQOalgorithms into the Postgres optimizer and present experimental results for TPC-H queries;we extended the Postgres cost model and optimize for up to nine conflicting objectives in ourexperiments. The proposed algorithms are based on a formal analysis of typical costfunctions that occur in the context of MOQO. We identify properties that hold for a broad …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,21
LINVIEW: incremental view maintenance for complex analytical queries,Milos Nikolic; Mohammed ElSeidy; Christoph Koch,Abstract Many analytics tasks and machine learning problems can be naturally expressedby iterative linear algebra programs. In this paper; we study the incremental viewmaintenance problem for such complex analytical queries. We develop a framework; calledLINVIEW; for capturing deltas of linear algebra programs and understanding theircomputational cost. Linear algebra operations tend to cause an avalanche effect whereeven very local changes to the input matrices spread out and infect all of the intermediateresults and the final view; causing incremental view maintenance to lose its performancebenefit over re-evaluation. We develop techniques based on matrix factorizations to containsuch epidemics of change. As a consequence; our techniques make incremental viewmaintenance of linear algebra practical and usually substantially cheaper than re …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,21
Multi-objective parametric query optimization,Immanuel Trummer; Christoph Koch,Abstract Classical query optimization compares query plans according to one cost metricand associates each plan with a constant cost value. In this paper; we introduce the Multi-Objective Parametric Query Optimization (MPQ) problem where query plans are comparedaccording to multiple cost metrics and the cost of a given plan according to a given metric ismodeled as a function that depends on multiple parameters. The cost metrics may forinstance include execution time or monetary fees; a parameter may represent the selectivityof a query predicate that is unspecified at optimization time. MPQ generalizes parametricquery optimization (which allows multiple parameters but only one cost metric) and multi-objective query optimization (which allows multiple cost metrics but no parameters). Weformally analyze the novel MPQ problem and show why existing algorithms are …,Proceedings of the VLDB Endowment,2014,20
An evaluation of checkpoint recovery for massively multiplayer online games,Marcos Vaz Salles; Tuan Cao; Benjamin Sowell; Alan Demers; Johannes Gehrke; Christoph Koch; Walker White,Abstract Massively multiplayer online games (MMOs) have emerged as an exciting newclass of applications for database technology. MMOs simulate long-lived; interactive virtualworlds; which proceed by applying updates in frames or ticks; typically at 30 or 60 Hz. Inorder to sustain the resulting high update rates of such games; game state is kept entirely inmain memory by the game servers. Nevertheless; durability in MMOs is usually achieved bya standard DBMS implementing ARIES-style recovery. This architecture limits scalability;forcing MMO developers to either invest in high-end hardware or to over-partition their virtualworlds. In this paper; we evaluate the applicability of existing checkpoint recoverytechniques developed for main-memory DBMS to MMO workloads. Our thoroughexperimental evaluation uses a detailed simulation model fed with update traces …,Proceedings of the VLDB Endowment,2009,20
Dynamic approaches to in-network aggregation,Oliver Kennedy; Christoph Koch; Al Demers,Collaboration between small-scale wireless devices depends on their ability to inferaggregate properties of all nearby nodes. The highly dynamic environment created bymobile devices introduces a silent failure mode that is disruptive to this kind of inference. Weaddress this problem by presenting techniques for extending existing unstructuredaggregation protocols to cope with failure modes introduced by mobile environments. Themodified protocols allow devices with limited connectivity to maintain estimates ofaggregates; despite\textit {unexpected} peer departures and arrivals.,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,20
DBToaster: Agile Views for a Dynamic Data Management System.,Oliver Kennedy; Yanif Ahmad; Christoph Koch,ABSTRACT This paper calls for a new breed of lightweight systems–dynamic datamanagement systems (DDMS). In a nutshell; a DDMS manages large dynamic datastructures with agile; frequently fresh views; and provides a facility for monitoring theseviews and triggering application-level events. We motivate DDMS with applications in large-scale data analytics; database monitoring; and high-frequency algorithmic trading. Wecompare DDMS to more traditional data management systems architectures. We present theDBToaster project; which is an ongoing effort to develop a prototype DDMS system. Wedescribe its architecture design; techniques for high-frequency incremental viewmaintenance; storage; scaling up by parallelization; and the various key challenges toovercome to make DDMS a reality.,CIDR,2011,19
XML prefiltering as a string matching problem,Christoph Koch; Stefanie Scherzinger; Michael Schmidt,We propose a new technique for the efficient search and navigation in XML documents andstreams. This technique takes string matching algorithms designed for efficient keywordsearch in flat strings into the second dimension; to navigate in tree structured data. Weconsider the important XML data management task of prefiltering XML documents (alsocalled XML projection) as an application for our approach. Different from existing prefilteringschemes; we usually process only fractions of the input and get by with very economicalconsumption of both main memory and processing time. Our experiments reveal that;already on low-complexity problems such as XPath filtering; in-memory query engines canexperience speed-ups by two orders of magnitude.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,19
Entangled queries: enabling declarative data-driven coordination,Nitin Gupta; Lucja Kot; Sudip Roy; Gabriel Bender; Johannes Gehrke; Christoph Koch,Abstract Many data-driven social and Web applications involve collaboration andcoordination. The vision of Declarative Data-Driven Coordination (D3C); proposed in Kot etal.[2010]; is to support coordination in the spirit of data management: to make it data-centricand to specify it using convenient declarative languages. This article introduces entangledqueries; a language that extends SQL by constraints that allow for the coordinated choice ofresult tuples across queries originating from different users or applications. It is nontrivial todefine a declarative coordination formalism without arriving at the general (NP-complete)Constraint Satisfaction Problem from AI. In this article; we propose an efficiently enforceablesyntactic safety condition that we argue is at the sweet spot where interesting declarativepower meets applicability in large-scale data management systems and applications. The …,ACM Transactions on Database Systems (TODS),2012,18
Efficient algorithms for descendant-only tree pattern queries,Michaela Götz; Christoph Koch; Wim Martens,Abstract Tree pattern matching is a fundamental problem that has a wide range ofapplications in Web data management; XML processing; and selective data dissemination.In this paper we develop efficient algorithms for the tree homeomorphism problem; ie; theproblem of matching a tree pattern with exclusively transitive (descendant) edges. We firstprove that deciding whether there is a tree homeomorphism is LOGSPACE-complete;improving on the current LOGCFL upper bound. Furthermore; we develop a practicalalgorithm for the tree homeomorphism decision problem that is both space-and time-efficient. The algorithm is in LOGDCFL and space consumption is strongly bounded; whilethe running time is linear in the size of the data tree. This algorithm immediately generalizesto the problem of matching the tree pattern against all subtrees of the data tree …,Information Systems,2009,18
Efficient algorithms for the tree homeomorphism problem,Michaela Götz; Christoph Koch; Wim Martens,Abstract Tree pattern matching is a fundamental problem that has a wide range ofapplications in Web data management; XML processing; and selective data dissemination.In this paper we develop efficient algorithms for the tree homeomorphism problem; ie; theproblem of matching a tree pattern with exclusively transitive (descendant) edges. We firstprove that deciding whether there is a tree homeomorphism is LOGSPACE-complete;improving on the current LOGCFL upper bound. As our main result we develop a practicalalgorithm for the tree homeomorphism decision problem that is both space-and timeefficient. The algorithm is in LOGCFL and space consumption is strongly bounded; while therunning time is linear in the size of the data tree. This algorithm immediately generalizes tothe problem of matching the tree pattern against all subtrees of the data tree; preserving …,International Symposium on Database Programming Languages,2007,18
On the role of composition in XQuery,Christoph Koch,ABSTRACT Nonrecursive XQuery is known to be hard for nondeterministic exponential time.Thus it is commonly believed that any algorithm for evaluating XQuery has to requireexponential amounts of working memory and doubly exponential time in the worst case. Inthis paper we present a property–the lack of a certain form of composition–that virtually allreal-world XQueries have and that allows for query evaluation in singly exponential time andpolynomial space. Still; we are able to show for an important special case–our nonrecursiveXQuery fragment restricted to atomic value equality–that the composition-free language isjust as expressive as the language with composition. Thus; under widely-held complexity-theoretic assumptions; the composition-free language is an exponentially less succinctversion of the language with composition.,*,2005,18
Database research in computer games,Alan Demers; Johannes Gehrke; Christoph Koch; Ben Sowell; Walker White,Abstract This tutorial presents an overview of the data management issues faced bycomputer games today. While many games do not use databases directly; they still have toprocess large amounts of data; and could benefit from the application of databasetechnology. Other games; such as massively multiplayer online games (MMOs); mustcommunicate with commercial databases and have their own unique challenges. In thistutorial we will present the state-of-the-art of data management in games that we learnedfrom our interaction with various game studios. We will show how the issues involvedmotivate current research; and illustrate several possibilities for future work. Our tutorial willstart with a description of data-driven design; which is the source of many of the datamanagement issues that games face. We will show some of the tools that game …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,17
Interpreting tree-to-tree queries,Michael Benedikt; Christoph Koch,Abstract We establish correspondences between top-down tree building query languagesand predicate logics. We consider the expressive power of the query language XQ; a cleancore of the practitioner's language XQuery. We show that all queries in XQ with only atomicequality are equivalent to “first-order interpretations”; an analog to first-order logic (FO) in thesetting of transformations of tree-structured data. When XQ is considered with deep equality;we find that queries can be translated into FO with counting (FO (Cnt)). We establish partialconverses to this; characterizing the subset of the FO resp. FO (Cnt) interpretations thatcorrespond to XQ. Finally; we study the expressive power of fragments of XQ and obtainpartial characterizations in terms of existential FO and a fragment of FO that is two-variable ifthe tree node labeling alphabet is assumed fixed.,International Colloquium on Automata; Languages; and Programming,2006,17
DLV-a system for declarative problem solving,Thomas Eiter; Wolfgang Faber; Christoph Koch; Nicola Leone; Gerald Pfeifer,Institut für Informationssysteme; TU Wien Favoritenstrasse 9-11 A-1040 Wien; Austria{leone;pfeifer}@dbai.tuwien.ac.at; {eiter;faber}@kr.tuwien.ac.at … European Organization forNuclear Research CERN EP Division CH-1211 Geneva; Switzerland christoph.koch@cern.ch… DLV is an efficient logic programming and non- monotonic reasoning (LPNMR) system withadvanced knowledge representation mechanisms and interfaces to classic relational databasesystems. Its core language is disjunctive datalog (function-free disjunctive logicprogramming) under the Answer Set Semantics with integrity constraints; both default and strong(or explicit) negation; and queries. Integer arithmetics and various built-in predicates are alsosup- ported. In addition DLV has several frontends; namely brave and cautious reasoning; abductivediagnosis; consistency-based diagnosis; a subset of SQL3; plan- ning with action …,arXiv preprint cs/0003036,2000,16
How to architect a query compiler,Amir Shaikhha; Yannis Klonatos; Lionel Parreaux; Lewis Brown; Mohammad Dashti; Christoph Koch,Abstract This paper studies architecting query compilers. The state of the art in querycompiler construction is lagging behind that in the compilers field. We attempt to remedy thisby exploring the key causes of technical challenges in need of well founded solutions; andby gathering the most relevant ideas and approaches from the PL and compilerscommunities for easy digestion by database researchers. All query compilers known to usare more or less monolithic template expanders that do the bulk of the compilation task inone large leap. Such systems are hard to build and maintain. We propose to use a stack ofmultiple DSLs on different levels of abstraction with lowering in multiple steps to make querycompilers easier to build and extend; ultimately allowing us to create more convincing andsustainable compiler-based data management systems. We attempt to derive our advice …,Proceedings of the 2016 International Conference on Management of Data,2016,15
An incremental anytime algorithm for multi-objective query optimization,Immanuel Trummer; Christoph Koch,Abstract Query plans offer diverse tradeoffs between conflicting cost metrics such asexecution time; energy consumption; or execution fees in a multi-objective scenario. It isconvenient for users to choose the desired cost tradeoff in an interactive process;dynamically adding constraints and finally selecting the best plan based on a continuouslyrefined visualization of optimal cost tradeoffs. Multi-objective query optimization (MOQO)algorithms must possess specific properties to support such an interactive process: First;they must be anytime algorithms; generating multiple result plan sets of increasing qualitywith low latency between consecutive results. Second; they must be incremental; meaningthat they avoid regenerating query plans when being invoked several times for the samequery but with slightly different user constraints. We present an incremental anytime …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,15
Automatic synthesis of out-of-core algorithms,Yannis Klonatos; Andres Nötzli; Andrej Spielmann; Christoph Koch; Victor Kuncak,Abstract We present a system for the automatic synthesis of efficient algorithms specializedfor a particular memory hierarchy and a set of storage devices. The developer provides twoindependent inputs: 1) an algorithm that ignores memory hierarchy and external storageaspects; and 2) a description of the target memory hierarchy; including its topology andparameters. Our system is able to automatically synthesize memory-hierarchy and storage-device-aware algorithms out of those specifications; for tasks such as joins and sorting. Theframework is extensible and allows developers to quickly synthesize custom out-of-corealgorithms as new storage technologies become available.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,15
On query algebras for probabilistic databases,Christoph Koch,Abstract This article proposes a core query algebra for probabilistic databases. In essence;this core is part of the query languages of most probabilistic database systems proposed sofar; but is sometimes hidden in complex language definitions. We give a formal definition ofthe algebra and illustrate it by examples. We then survey the current state of knowledgeregarding the expressive power and complexity of this core.,ACM Sigmod Record,2009,15
A compositional framework for complex queries over uncertain data,Michaela Götz; Christoph Koch,Abstract The ability to flexibly compose confidence computation with the operations ofrelational algebra is an important feature of probabilistic database query languages.Computing confidences is computationally hard; however; and has to be approximated inpractice. In a compositional query language; even very small errors caused byapproximation can lead to an entirely incorrect result: A selection operation on anapproximated probability can incorrectly keep or drop a tuple even if the probability valuehas been approximated to a very narrow confidence interval. In this paper; we study thequery evaluation problem for compositional query languages for probabilistic databaseswith particular focus on providing overall result quality guarantees in the face of approximateintermediate results. We present a framework for evaluating compositional queries based …,Proceedings of the 12th international conference on database theory,2009,14
Probabilistic databases,Dan Suciu,Abstract Many applications today need to manage large data sets with uncertainties. In thispaper we describe the foundations of managing data where the uncertainties are quantifiedas probabilities. We review the basic definitions of the probabilistic data model and presentsome fundamental theoretical results for query evaluation on probabilistic databases.,ACM SIGACT News,2008,14
The dlv system: Model generator and advanced frontends (system description),Simona Citrigno; Thomas Eiter; Wolfgang Faber; Georg Gottlob; Christoph Koch; Nicola Leone; Cristinel Mateis; Gerald Pfeifer; Francesco Scarcello,*,WLP,1997,14
Multiple query optimization on the D-Wave 2X adiabatic quantum computer,Immanuel Trummer; Christoph Koch,Abstract The D-Wave adiabatic quantum annealer solves hard combinatorial optimizationproblems leveraging quantum physics. The newest version features over 1000 qubits andwas released in August 2015. We were given access to such a machine; currently hosted atNASA Ames Research Center in California; to explore the potential for hard optimizationproblems that arise in the context of databases. In this paper; we tackle the problem ofmultiple query optimization (MQO). We show how an MQO problem instance can betransformed into a mathematical formula that complies with the restrictive input formataccepted by the quantum annealer. This formula is translated into weights on and betweenqubits such that the configuration minimizing the input formula can be found via a processcalled adiabatic quantum annealing. We analyze the asymptotic growth rate of the …,Proceedings of the VLDB Endowment,2016,13
Beyond isolation: Research opportunities in declarative data-driven coordination,Lucja Kot; Nitin Gupta; Sudip Roy; Johannes Gehrke; Christoph Koch,Abstract There are many database applications that require users to coordinate andcommunicate. Friends want to coordinate travel plans; students want to jointly enroll in thesame set of courses; and busy professionals want to coordinate their schedules. These tasksare difficult to program using existing abstractions provided by database systems because inaddition to the traditional ACID properties provided by the system they all require some typeof coordination between users. This is fundamentally incompatible with isolation in theclassical ACID properties. In this position paper; we argue that it is time for the databasecommunity to look beyond isolation towards principled and elegant abstractions that allowfor communication and coordination between some notion of (suitably generalized)transactions. This new area of declarative data-driven coordination (D3C) is motivated by …,ACM SIGMOD Record,2010,13
A compositional query algebra for second-order logic and uncertain databases,Christoph Koch,Abstract World-set algebra is a variable-free query language for uncertain databases. Itconstitutes the core of the query language implemented in MayBMS; an uncertain databasesystem. This paper shows that world-set algebra captures exactly second-order logic overfinite structures; or equivalently; the polynomial hierarchy. The proofs also imply that world-set algebra is closed under composition; a previously open problem.,Proceedings of the 12th International Conference on Database Theory,2009,13
Information extraction for the semantic web,Robert Baumgartner; Thomas Eiter; Georg Gottlob; Marcus Herzog; Christoph Koch,Abstract The World Wide Web represents a universe of knowledge and information.Unfortunately; it is not straightforward to query and access the desired information.Languages and tools for accessing; extracting; transforming; and syndicating the desiredinformation are required. The Web should be useful not merely for human consumption butadditionally for machine communication. Therefore; powerful and user-friendly tools basedon expressive languages for extracting and integrating information from various differentWeb sources; or in general; various heterogeneous sources are needed. The tutorial givesan introduction to Web technologies required in this context; and presents variousapproaches and techniques used in information extraction and integration. Moreover;sample applications in various domains motivate the discussed topics and providing data …,*,2005,13
Go meta! A case for generative programming and dsls in performance critical systems,Tiark Rompf; Kevin J Brown; HyoukJoong Lee; Arvind K Sujeeth; Manohar Jonnalagedda; Nada Amin; Georg Ofenbeck; Alen Stojanov; Yannis Klonatos; Mohammad Dashti; Christoph Koch; Markus Püschel; Kunle Olukotun,Abstract Most performance critical software is developed using very low-level techniques.We argue that this needs to change; and that generative programming is an effective avenueto enable the use of high-level languages and programming techniques in many suchcircumstances.,LIPIcs-Leibniz International Proceedings in Informatics,2015,10
Abstraction without regret in database systems building: a manifesto,Christoph Koch,Abstract It has been said that all problems in computer science can be solved by addinganother level of indirection; except for performance problems; which are solved by removinglevels of indirection. Compilers are our tools for removing levels of indirection automatically.However; we do not trust them when it comes to systems building. Most performance-criticalsystems are built in low-level programming languages such as C. Some of the downsides ofthis compared to using modern high-level programming languages are very well known:bugs; poor programmer productivity; a talent bottleneck; and cruelty to programminglanguage researchers. In the future we might even add suboptimal performance to this list. Inthis article; I argue that compilers can be competitive with and outperform human experts atlow-level database systems programming. Performance-critical database systems are a …,IEEE Data Engineering Bulletin,2014,10
The GCX system: dynamic buffer minimization in streaming XQuery evaluation,Christoph Koch; Stefanie Scherzinger; Michael Schmidt,Abstract In this demonstration; we present the main-memory based streaming XQueryengine GCX which implements novel buffer management strategies that combine static anddynamic analysis to keep main memory consumption low. Depending on the progress madein query evaluation; memory buffers are dynamically purged and minimized. In this demo;we show the various stages in evaluating a practical fragment of XQuery with GCX. Wepresent the major steps in static analysis and demonstrate the mechanisms of dynamicbuffer minimization. We apply our system to XML streams and demonstrate the significantimpact of our approach on reducing main memory consumption and running time.,Proceedings of the 33rd international conference on Very large data bases,2007,10
How to win a hot dog eating contest: Distributed incremental view maintenance with batch updates,Milos Nikolic; Mohammad Dashti; Christoph Koch,Abstract In the quest for valuable information; modern big data applications continuouslymonitor streams of data. These applications demand low latency stream processing evenwhen faced with high volume and velocity of incoming changes and the user's desire to askcomplex queries. In this paper; we study low-latency incremental computation of complexSQL queries in both local and distributed streaming environments. We develop a techniquefor the efficient incrementalization of queries with nested aggregates for batch updates. Weidentify the cases in which batch processing can boost the performance of incremental viewmaintenance but also demonstrate that tuple-at-a-time processing often can achieve betterperformance in local mode. Batch updates are essential for enabling distributed incrementalview maintenance and amortizing the cost of network communication and …,Proceedings of the 2016 International Conference on Management of Data,2016,9
On APIs for probabilistic databases,Lyublena Antova; Christoph Koch,Abstract. We study database application programming interfaces for uncertain andprobabilistic databases and present a programming model that is independent ofrepresentation details. Conceptually; we use the possible worlds semantics; and programsare independently evaluated in each world. We study a class of programs that appear to theuser as if they are running in a single world rather than on a set of possible worlds. Wepresent an algorithm for efficiently verifying this property. We discuss how updates can beimplemented in uncertain database management systems; and propose techniques foroptimizing database programs.,*,2008,9
Load balancing and skew resilience for parallel joins,Aleksandar Vitorovic; Mohammed Elseidy; Christoph Koch,We address the problem of load balancing for parallel joins. We show that the distribution ofinput data received and the output data produced by worker machines are both important forperformance. As a result; previous work; which optimizes either for input or output; standsineffective for load balancing. To that end; we propose a multi-stage load-balancingalgorithm which considers the properties of both input and output data through sampling ofthe original join matrix. To do this efficiently; we propose a novel category of equi-weighthistograms. To build them; we exploit state-of-the-art computational geometry algorithms forrectangle tiling. To our knowledge; we are the first to employ tiling algorithms for join load-balancing. In addition; we propose a novel; join-specialized tiling algorithm that hasdrastically lower time and space complexity than existing algorithms. Experiments show …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,8
Quantum databases,Sudip Roy; Lucja Kot; Christoph Koch,ABSTRACT We introduce quantum databases; a new database abstraction that allows todefer the making of choices in transactions until an application or user forces the choices byobservation. Conceptually; a transaction is in a quantum state–in one of many possibleworlds; which one is unknown–until fixed by observation. Practically; our abstractionenables late binding of values read from the database. This allows more transactions tosucceed in environments with high contention. This is particularly important for applicationsin which transactions compete for scarce physical resources represented by data items inthe database; such as seats in airline reservation systems or meeting slots in calendaringsystems. In such environments; deferral of the assignment of resources to consumers untilall constraints are available to the system will lead to more successful transactions …,Proc. CIDR,2013,8
The complexity of querying external memory and streaming data,Martin Grohe; Christoph Koch; Nicole Schweikardt,Abstract We review a recently introduced computation model for streaming and externalmemory data. An important feature of this model is that it distinguishes between sequentiallyreading (streaming) data from external memory (through main memory) and randomlyaccessing external memory data at specific memory locations; it is well-known that the latteris much more expensive in practice. We explain how a number of lower bound results areobtained in this model and how they can be applied for proving lower bounds for XML queryprocessing.,International Symposium on Fundamentals of Computation Theory,2005,8
A formal comparison of visual web wrapper generators,Georg Gottlob; Christoph Koch,Querying semi-structured data is relevant in two important contexts – first; where informationis to be retrieved from XML databases and documents; and second; where information is to beextracted from Web documents formatted in HTML or in similar display-oriented languages. AtTU Wien; much work has been dedicated to both aspects of querying semi-structured data inthe recent years. We have recognized that many query and extraction tasks are inherently monadic[17] and have; in particular; studied monadic datalog over trees [18]; proving among other thingsthat this language has the same expressive power as monadic second order logic; while its combinedcomplexity is much lower (size of the query times size of the database). It was shown that CoreXPath; the “clean logical kernel” of the well-known XPath language has the same low complexityas monadic datalog by translating Core XPath in linear time into a version of monadic …,International Conference on Current Trends in Theory and Practice of Computer Science,2006,7
Fine-grained disclosure control for app ecosystems,Gabriel M Bender; Lucja Kot; Johannes Gehrke; Christoph Koch,Abstract The modern computing landscape contains an increasing number of appecosystems; where users store personal data on platforms such as Facebook orsmartphones. APIs enable third-party applications (apps) to utilize that data. A key concernassociated with app ecosystems is the confidentiality of user data. In this paper; we developa new model of disclosure in app ecosystems. In contrast with previous solutions; our modelis data-derived and semantically meaningful. Information disclosure is modeled in terms of aset of distinguished security views. Each query is labeled with the precise set of securityviews that is needed to answer it; and these labels drive policy decisions.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,6
Compilation and synthesis in big data analytics,Christoph Koch,Abstract Databases and compilers are two long-established and quite distinct areas ofcomputer science. With the advent of the big data revolution; these two areas move closer; tothe point that they overlap and merge. Researchers in programming languages andcompiler construction want to take part in this revolution; and also have to respond to theneed of programmers for suitable tools to develop data-driven software for data-intensivetasks and analytics. Database researchers cannot ignore the fact that most big-dataanalytics is performed in systems such as Hadoop that run code written in general-purposeprogramming languages rather than query languages. To remain relevant; each communityhas to move closer to the other. In the first part of this keynote; I illustrate this current trendfurther; and describe a number of interesting and inspiring research efforts that are …,British National Conference on Databases,2013,5
System and method for scaling simulations and games,*,A system and method for modeling simulation and game artificial intelligence as a datamanagement problem are described. A scripting language provides game designers andplayers with a data-driven artificial intelligence scheme for customizing behavior forindividual agents. Query processing and indexing techniques efficiently execute largenumbers of agent scripts; thus providing a framework for games with a large number ofagents.,*,2013,5
Yin-yang: Transparent deep embedding of dsls,Vojin Jovanovic; Vladimir Nikolaev; Ngoc Duy Pham; Vlad Ureche; Sandro Stucki; Christoph Koch; Martin Odersky,Abstract Deep EDSLs intrinsically compromise programmer experience for improvedprogram performance. Shallow ED-SLs; complement them; by trading program performancefor good programmer experience. We present Yin-Yang; a library for DSL embedding thatuses compile-time metaprogramming to transparently; and reliably; transform shallow EDSLprograms to the equivalent deep EDSL programs. The transformation allows programprototyping and development with the shallow embedding; while the equivalent deepembedding is used where performance is of essence. Through language virtualization and aminimal interface Yin-Yang allows design of compiler agnostic EDSLs that perform domain-specific analysis at compile time while ED-SLs can be compiled at both run and compiletime. For run time compiled EDSLs Yin-Yang introduces guarded recompilation that …,*,2013,5
Entangled transactions,Nitin Gupta; Milos Nikolic; Sudip Roy; Gabriel Bender; Lucja Kot; Johannes Gehrke; Christoph Koch,ABSTRACT As the world becomes more interdependent and computing grows morecollaborative; there is a need for new abstractions and tools to help users work together. Werecently introduced entangled queries–a mechanism for information exchange betweendatabase queries [6]. In this paper; we introduce entangled transactions; units of work similarto traditional transactions that however do not run in isolation; but communicate with eachother via entangled queries. Supporting entangled transactions brings about many newchallenges; from an abstract model to an investigation of the unique systems issues thatarise during their implementation. We first introduce a novel semantic model for entangledtransactions that comes with analogues of the classical ACID properties. We then discussexecution models for entangled transactions and select a concrete design motivated by …,Proceedings of the VLDB Endowment,2011,5
Xml stream processing,Christoph Koch,XML access control refers to the practice of limiting access to (parts of) XML data to onlyauthorized users. Similar to access control over other types of data and resources; XMLaccess control is centered around two key problems:(i) the development of formal models forthe specification of access control policies over XML data; and (ii) techniques for efficientenforcement of access control policies over XML data.,*,2009,5
Explicit modeling of the semantics of large multi-layered object-oriented databases,Christoph Koch; Zsolt Kovacs; Jean-Marie Le Goff; Richard McClatchey; Paolo Petta; Tony Solomonides,Abstract Description-driven systems based on meta-objects are an increasingly popular wayto handle complexity in large-scale object-oriented database applications. Such systemsfacilitate the management of large amounts of data and provide a means to avoid databaseschema evolution in many settings. Unfortunately; the description-driven approach leads toa loss of simplicity of the schema; and additional software behaviour is required for themanagement of dependencies; description relationships; and other Design Patterns thatrecur across the schema. This leads to redundant implementations of software that cannotbe handled by using a framework-based approach. This paper presents an approach toaddress this problem which is based on the concept of an ontology of Design Patterns. Suchan ontology allows the convenient separation of the structure and the semantics of …,International Conference on Conceptual Modeling,2000,5
A Fast Randomized Algorithm for Multi-Objective Query Optimization,Immanuel Trummer; Christoph Koch,Abstract Query plans are compared according to multiple cost metrics in multi-objectivequery optimization. The goal is to find the set of Pareto plans realizing optimal cost tradeoffsfor a given query. So far; only algorithms with exponential complexity in the number of querytables have been proposed for multi-objective query optimization. In this work; we presentthe first algorithm with polynomial complexity in the query size. Our algorithm is randomizedand iterative. It improves query plans via a multi-objective version of hill climbing that appliesmultiple transformations in each climbing step for maximal efficiency. Based on a locallyoptimal plan; we approximate the Pareto plan set within the restricted space of plans withsimilar join orders. We maintain a cache of Pareto-optimal plans for each potentially usefulintermediate result to share partial plans that were discovered in different iterations. We …,Proceedings of the 2016 International Conference on Management of Data,2016,4
Maybms: A probabilistic database system. user manual,Christoph Koch; Dan Olteanu; Lyublena Antova; Jiewen Huang,The MayBMS system (note: MayBMS is read as “maybe-MS”; like DBMS) is a completeprobabilistic database management system that leverages robust relational databasetechnology: MayBMS is an extension of the Postgres server backend. MayBMS is opensource and the source code is available under the BSD license at http://maybms.sourceforge. net The MayBMS system has been under development since 2005. While thedevelopment has been carried out in an academic environment; care has been taken tobuild a robust; scalable system that can be reliably used in real applications. The academichomepage of the MayBMS project is at,*,2009,4
Squall: Scalable real-time analytics,Aleksandar Vitorovic; Mohammed Elseidy; Khayyam Guliyev; Khue Vu Minh; Daniel Espino; Mohammad Dashti; Yannis Klonatos; Christoph Koch,Abstract Squall is a scalable online query engine that runs complex analytics in a clusterusing skew-resilient; adaptive operators. Squall builds on state-of-the-art partitioningschemes and local algorithms; including some of our own. This paper presents the overviewof Squall; including some novel join operators. The paper also presents lessons learnedover the five years of working on this system; and outlines the plan for the proposed systemdemonstration.,Proceedings of the VLDB Endowment,2016,3
Parallelizing query optimization on shared-nothing architectures,Immanuel Trummer; Christoph Koch,Abstract Data processing systems offer an ever increasing degree of parallelism on thelevels of cores; CPUs; and processing nodes. Query optimization must exploit high degreesof parallelism in order not to gradually become the bottleneck of query evaluation. We showhow to parallelize query optimization at a massive scale. We present algorithms for parallelquery optimization in left-deep and bushy plan spaces. At optimization start; we divide theplan space for a given query into partitions of equal size that are explored in parallel byworker nodes. At the end of optimization; each worker returns the optimal plan in its partitionto the master which determines the globally optimal plan from the partition-optimal plans. Nosynchronization or data exchange is required during the actual optimization phase. Theamount of data sent over the network; at the start and at the end of optimization; as well as …,Proceedings of the VLDB Endowment,2016,3
Abstraction without regret for efficient data processing,Tiark Rompf; Nada Amin; Thierry Coppey; Mohammad Dashti; Manohar Jonnalagedda; Yannis Klonatos; Martin Odersky; Christoph Koch,*,Data-Centric Programming Workshop,2014,3
Coordination through querying in the Youtopia system,Nitin Gupta; Lucja Kot; Gabriel Bender; Sudip Roy; Johannes Gehrke; Christoph Koch,Abstract In a previous paper; we laid out the vision of declarative data-driven coordination(D3C) where users are provided with novel abstractions that enable them to communicateand coordinate through declarative specifications [3]. In this demo; we will show Youtopia; anovel database system which is our first attempt at implementing this vision. Youtopiaprovides coordination abstractions within the DBMS. Users submit queries that come withexplicit coordination constraints to be met by other queries in the system. Such queries areevaluated together; the system ensures that their joint execution results in the satisfaction ofall coordination constraints. That is; the queries coordinate their answers in the mannerspecified by the users.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,3
Scaling transportation simulations through declarative processing,A Demers; O Gao; J Gehrke; C Koch; M Vaz Salles; W White,*,Proceedings of the Next Generation Data Mining Summit NGDM,2009,3
A visual query language for complex-value databases,Christoph Koch,Abstract: In this paper; a visual language; VCP; for queries on complex-value databases isproposed. The main strength of the new language is that it is purely visual:(i) It has no notionof variable; quantification; partiality; join; pattern matching; regular expression; recursion; orany other construct proper to logical; functional; or other database query languages and (ii)has a very natural; strong; and intuitive design metaphor. The main operation is that ofcopying and pasting in a schema tree. We show that despite its simplicity; VCP preciselycaptures complex-value algebra without powerset; or equivalently; monad algebra withunion and difference. Thus; its expressive power is precisely that of the language that isusually considered to play the role of relational algebra for complex-value databases.,arXiv preprint cs/0602006,2006,3
Reasoning web,Norbert Eisinger; Jan Maluszynski,*,*,2005,3
Optimizing Queries Using a Meta-level Database,Christoph Koch,Abstract: Graph simulation (using graph schemata or data guides) has been successfullyproposed as a technique for adding structure to semistructured data. Design patterns fordescription (such as meta-classes and homomorphisms between schema layers); which areprominent in the object-oriented programming community; constitute a generalization of thisgraph simulation approach. In this paper; we show description applicable to a wide range ofdata models that have some notion of object (-identity); and propose to turn it into a datamodel primitive much like; say; inheritance. We argue that such an extension fills a practicalneed in contemporary data management. Then; we present algebraic techniques for queryoptimization (using the notions of described and description queries). Finally; in thesemistructured setting; we discuss the pruning of regular path queries (with nested …,arXiv preprint cs/0205060,2002,3
Quoted staged rewriting: a practical approach to library-defined optimizations,Lionel Parreaux; Amir Shaikhha; Christoph E Koch,Abstract Staging has proved a successful technique for programmatically removing codeabstractions; thereby allowing for faster program execution while retaining a high-levelinterface for the programmer. Unfortunately; techniques based on staging suffer from anumber of problems—ranging from practicalities to fundamental limitations—which haveprevented their widespread adoption. We introduce Quoted Staged Rewriting (QSR); anapproach that uses type-safe; pattern matching-enabled quasiquotes to defineoptimizations. The approach is “staged” in two ways: first; rewrite rules can execute arbitrarycode during pattern matching and code reconstruction; leveraging the power and flexibilityof staging; second; library designers can orchestrate the application of successive rewritingphases (stages). The advantages of using quasiquote-based rewriting are that library …,ACM SIGPLAN Notices,2017,2
Squid: type-safe; hygienic; and reusable quasiquotes,Lionel Parreaux; Amir Shaikhha; Christoph E Koch,Abstract Quasiquotes have been shown to greatly simplify the task of metaprogramming.This is in part because they hide the data structures of the intermediate representation (IR);instead allowing metaprogrammers to use the concrete syntax of the language theymanipulate. Scala has had â syntacticâ quasiquotes for a long time; but still misses astatically-typed version like in MetaOCaml; Haskell and F#. This safer flavor of quasiquoteshas been particularly useful for staging and domain-specific languages. In this paper wepresent Squid; a metaprogramming system for Scala that fills this gap. Squid quasiquotesare novel in three ways: they are the first statically-typed quasiquotes we know that allowcode inspection (via pattern matching); they are implemented purely as a macro library;without modifications to the compiler; and they are reusable in the sense that they can …,Proceedings of the 8th ACM SIGPLAN International Symposium on Scala,2017,2
Logic; Languages; and Rules for Web Data Extraction and Reasoning over Data,Georg Gottlob; Christoph Koch; Andreas Pieris,Abstract This paper gives a short overview of specific logical approaches to data extraction;data management; and reasoning about data. In particular; we survey theoretical results andformalisms that have been obtained and used in the context of the Lixto Project at TU Wien;the DIADEM project at the University of Oxford; and the VADA project; which is currentlybeing carried out jointly by the universities of Edinburgh; Manchester; and Oxford. We startwith a formal approach to web data extraction rooted in monadic second order logic andmonadic Datalog; which gave rise to the Lixto data extraction system. We then present somecomplexity results for monadic Datalog over trees and for XPath query evaluation. We furtherargue that for value creation and for ontological reasoning over data; we need existentialquantifiers (or Skolem terms) in rule heads; and introduce the Datalog^ ± family. We give …,International Conference on Language and Automata Theory and Applications,2017,2
Repairing conflicts among MVCC transactions,Mohammad Dashti; Sachin Basil John; Amir Shaikhha; Christoph Koch,Abstract: The optimistic variants of MVCC (Multi-Version Concurrency Control) avoidblocking concurrent transactions at the cost of having a validation phase. Upon failure in thevalidation phase; the transaction is usually aborted and restarted from scratch. The" abortand restart" approach becomes a performance bottleneck for the use cases with highcontention objects or long running transactions. In addition; restarting from scratch creates anegative feedback loop in the system; because the system incurs additional overhead thatmay create even further conflicts. In this paper; we propose a novel approach for conflictresolution in MVCC for in-memory databases. This low overhead approach summarizes thetransaction programs in the form of a dependency graph. The dependency graph alsocontains the constructs used in the validation phase of the MVCC algorithm. Then; in the …,arXiv preprint arXiv:1603.00542,2016,2
Probably Approximately Optimal Query Optimization,Immanuel Trummer; Christoph Koch,Abstract: Evaluating query predicates on data samples is the only way to estimate theirselectivity in certain scenarios. Finding a guaranteed optimal query plan is not a reasonableoptimization goal in those cases as it might require an infinite number of samples. Wetherefore introduce probably approximately optimal query optimization (PAO) where the goalis to find a query plan whose cost is near-optimal with a certain probability. We will justifywhy PAO is a suitable formalism to model scenarios in which predicate sampling andoptimization need to be interleaved. We present the first algorithm for PAO. Our algorithm isnon-intrusive and uses standard query optimizers and sampling components as sub-functions. It is generic and can be applied to a wide range of scenarios. Our algorithm isiterative and calculates in each iteration a query plan together with a region in the …,arXiv preprint arXiv:1511.01782,2015,2
Errata for building efficient query engines in a high-level language: PVLDB 7 (10): 853-864,Yannis Klonatos; Christoph Koch; Tiark Rompf; Hassan Chafi,Table 1 clarifies which optimizations are used in each evaluated flavor of LegoBase. HyPer-simulated is a configuration of the LegoBase codebase that mirrors the HyPer system asclosely as possible by just activating some of the main optimizations and deactivating others:using the push engine; with operator inlining active but data structure optimizations and datalayout transformations turned off. In addition; as noted in our paper (footnote 8); the actualHyPer system uses query plans generated by its own query optimizer while HyPer-simulateduses query plans from DBX. We use TPC-H queries and generated data at scaling factor 8 toevaluate the impact of our compilation techniques. For all evaluated systems; reported queryevaluation times only include the execution time of the query and exclude the time taken forquery optimization/compilation and loading the data into the main-memory data structures …,Proceedings of the VLDB Endowment,2014,2
08421 Abstracts Collection--Uncertainty Management in Information Systems,Christoph Koch; Birgitta König-Ries; Volker Markl; Maurice van Keulen,Abstract From October 12 to 17; 2008 the Dagstuhl Seminar 08421'Uncertainty Managementin Information Systems''was held in Schloss Dagstuhl~--~ Leibniz Center for Informatics. Theabstracts of the plenary and session talks given during the seminar as well as those of theshown demos are put together in this paper.,Dagstuhl Seminar Proceedings,2009,2
Multi-Agent Coordination of Distributed Event Data Processing,Christoph Koch; Paolo Petta,Abstract. In this paper; we present a multi-agent systems approach to distributed event dataprocessing as it is eg pervasive in scientific computing environments. The matching ofcapability descriptions in these domains; which requires the description of constraints overstreams of events; is computationally very hard. We discuss a coordination approach basedon the contract net protocol in combination with shallow descriptions of agent capabilities;requiring agents to be able to deal with exceptions that arise when constraints betweeninterfaces that were not enforced by the capability descriptions are violated. In this case;agents decommit from their contracts and are required to reiterate coordination. We motivateevent-data processing as an interesting application area for multiagent systems; propose acoordination approach; and finally formulate the question of how to structure a principled …,*,2000,2
A simple query facility for the objectivity/db persistent object manager,Christoph Koch,This document discusses the reasons that lead to the development of a query faciliy withinthe CRISTAL project; its design criteria; syntax; semantics; use; and restrictions. It isfurthermore intended to serve as a preliminary manual. The query facility is discussed in itsimmediately next development stage which should be finished within the next few weeks1.The query facility; which will be called OQL'in the following; satisfies a practical need. TheObjectivity/DB Persistent Object Manager; which is used at CERN in several projects for itslow-overhead support of object persistency; provides as its sole way to dynamically querydata without writing a specialized application program a facility called SQL++. The querylanguage of this facility has several restrictions which make it unsuited for mining practicalobject-oriented databases. Among other things; it does not allow to follow associations …,Unpublished draft; available via the author's homepage< URL: http://home. cern. ch/~ chkoch,1999,2
Unifying Analytic and Statically-Typed Quasiquotes,Lionel Parreaux; Antoine Voizard; Amir Shaikhha; Christoph Koch,In this paper; we tackle the problem of statically typing metaprograms: programs thatconstruct; deconstruct; rewrite and evaluate other programs. Well-typed metaprogramsshould not “go wrong;” and in particular they should not run into type mismatches andunbound variable errors at runtime; which could arise from erroneous combinations of theseoperations. We describe our solution in Squid; 1 a Scala macro library that makes use ofstructural subtyping and intersection types to express the contextual dependencies ofprogram fragments. We also formalize the essence of Squid as λ{}; a multi-stage calculuswith support for code pattern matching and rewriting; and prove its soundness as progressand type preservation. This work was motivated by the needs of real-worldmetaprogramming applications; we give several use cases inspired by our work on query …,Proceedings of the ACM on Programming Languages,2018,1
BIG data,Weike Pan; Qiang Yang; Charu Aggarwal; Christoph Koch,Big data has been an enabler for innovation; reconstruction; and advancement of mostsectors of our society; and it's receiving continuous and growing attention from researchersand practitioners in academia; industry; and government. There are; however; still lots ofchallenges spanning from theoretical foundations; systems; and technology to data policyand standards. This special issue focuses on how big data cuts across systems andapplications arenas; and the guest editors' introduction describes the five articles theyselected out of 30 submitted to cover a wide spectrum of interesting topics; including featureselection for big data analytics; astronomical image analysis; large-scale network prediction;online URL filtering; and massive transaction clustering.,IEEE Intelligent Systems,2017,1
Multi-objective parametric query optimization,Immanuel Trummer; Christoph Koch,Abstract Classical query optimization compares query plans according to one cost metricand associates each plan with a constant cost value. In this paper; we introduce the multi-objective parametric query optimization (MPQO) problem where query plans are comparedaccording to multiple cost metrics and the cost of a given plan according to a given metric ismodeled as a function that depends on multiple parameters. The cost metrics may; forinstance; include execution time or monetary fees; a parameter may represent the selectivityof a query predicate that is unspecified at optimization time. MPQO generalizes parametricquery optimization (which allows multiple parameters but only one cost metric) and multi-objective query optimization (which allows multiple cost metrics but no parameters). Weformally analyze the novel MPQO problem and show why existing algorithms are …,The VLDB Journal,2017,1
Push vs. Pull-Based Loop Fusion in Query Engines,Amir Shaikhha; Mohammad Dashti; Christoph Koch,Abstract: Database query engines use pull-based or push-based approaches to avoid thematerialization of data across query operators. In this paper; we study these two types ofquery engines in depth and present the limitations and advantages of each engine.Similarly; the programming languages community has developed loop fusion techniques toremove intermediate collections in the context of collection programming. We draw parallelsbetween the DB and PL communities by demonstrating the connection between pipelinedquery engines and loop fusion techniques. Based on this connection; we propose a newtype of pull-based engine; inspired by a loop fusion technique; which combines the benefitsof both approaches. Then we experimentally evaluate the various engines; in the context ofquery compilation; for the first time in a fair environment; eliminating the biasing impact of …,arXiv preprint arXiv:1610.09166,2016,1
Incremental view maintenance for collection programming,Christoph Koch; Daniel Lupei; Val Tannen,Abstract In the context of incremental view maintenance (IVM); delta query derivation is anessential technique for speeding up the processing of large; dynamic datasets. The goal isto generate delta queries that; given a small change in the input; can update thematerialized view more efficiently than via recomputation. In this work we propose the firstsolution for the efficient incrementalization of positive nested relational calculus (NRC+) onbags (with integer multiplicities). More precisely; we model the cost of NRC+ operators andclassify queries as efficiently incrementalizable if their delta has a strictly lower cost than fullre-evaluation. Then; we identify NRC+; a large fragment of NRC+ that is efficientlyincrementalizable and we provide a semantics-preserving translation that takes any NRC+query to a collection of IncNRC+ queries. Furthermore; we prove that incremental …,Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2016,1
Incremental View Maintenance for Nested-Relational Databases,Daniel Lupei; Christoph Koch; Val Tannen,Abstract Incremental view maintenance is an essential tool for speeding up the processingof large; locally changing workloads. Its fundamental challenge is to ensure that changesare propagated from input to output more efficiently than via recomputation. We formalizethis requirement for positive nested relational algebra (NRA+) on bags and we propose atransformation deriving deltas for any expression in the language. The main difficulty inmaintaining nested queries lies in the inability to express within NRA+ the efficient updatingof inner bags; ie; without completely replacing the tuples that contain them. To address thisproblem; we first show how to efficiently incrementalize IncNRA+; a large fragment of NRA+whose deltas never generate inner bag updates. We then provide a semantics-preservingtransformation that takes any nested query into a collection of IncNRA+ queries. This …,Computing Research Repository,2014,1
Incremental query evaluation,*,This application is the US national phase entry under 35 USC §371 of International Patent ApplicationNo. PCT/US10/39982 filed Jun. 25; 2010; entitled INCREMENTAL QUERY EVALUATION; whichin turn claims priority from US Provisional Application Ser. No. 61/220;256; filed Jun. 25;2009; entitled AGGRESSIVE COMPILATION FRAMEWORK FOR CONTINUOUS QUERY PROCESSINGON UPDATE STREAMS; both of which are incorporated by reference herein in their entirety forall purposes … This invention was made with Government support under Grant NumberFA9550-06-1-0111 awarded by the Air Force Office of Scientific Research. The United StatesGovernment has certain rights in the invention … The system and method of the present embodimentrelate generally to processing queries using incremental processing techniques … Modernrelational databases focus almost exclusively on providing flexible and extensible …,*,2014,1
08421 Executive Summary--Uncertainty Management in Information Systems,Christoph Koch; Birgitta König-Ries; Volker Markl; Maurice van Keulen,Computer science has long pretended that information systems are perfect mirror images of aperfect world. Database management systems; eg; work under the assumption that the data storedrepresent a correct subset of the real world. Of course; this idealized assumption is rarelytrue. Information systems contain … – wrong information caused; eg; by data entry errors: Thisis a common problem for instance in genomic databases – imprecise or falsely preciseinformation; eg; a measuring device will provide information with a certain precision only.Typically; information systems store the measured date; but do not store information about theconditions under which this data is true and the precision achieved. – incompleteinformation. A certain piece of information may not be available to the information system. – inconsistentinformation. Different information systems may contain contra- dictory information.,Dagstuhl Seminar Proceedings,2009,1
Path Queries on Compressed XML’,Christoph Koch,Abstract Central to any XML query language is a path language such as XPath whichoperates on the tree structure of the XML document. We demonstrate in this paper that thetree structure can be effectively compressed and ma-nipulated using techniques derivedfrom symbolic model checking. Specifically; we show first that succinct representations ofdocument tree structures based on sharing subtrees are highly effective. Second; we showthat compressed structures can be queried directly and efficiently through a process ofmanipulating selections of nodes and partial decompression. We study both the theoreticaland experimen-tal properties of this technique and provide algorithms for querying ourcompressed in-stances using node-selecting path query languages such as XPath. Webelieve the ability to store and manipulate large portions of the structure of very large XML …,Proceedings 2003 VLDB Conference: 29th International Conference on Very Large Databases (VLDB),2003,1
XPath Query Processing,Georg Gottlob; Christoph Koch,Abstract XPath 1 [4] is a practical language for selecting nodes from XML document treesand plays an essential role in other XML-related technologies such as XSLT and XQuery.Implementations of XPath need to scale well both with respect to the size of the XML dataand the growing size and intricacy of the queries (ie; combined complexity). Unfortunately;current XPath engines use query evaluation techniques that require time exponential in thesize of queries in the worst case [1]. The main aim of this tutorial is to show that and howXPath can be processed much more efficiently. We present an algorithm for this problemwith polynomial-time combined query evaluation complexity. Then we discuss variousimprovements over the basic evaluation algorithm; such as the context-restriction techniqueof [3]; which lead to better worst-case bounds. We provide an overview over XPath …,International Workshop on Database Programming Languages,2003,1
Xml in the Virtual Observatory,Robert Mann; Rob Baxter; Peter Buneman; Daragh Byrne; Rob Hutchison; Christoph Koch; Ted Wen; Martin Westhead,Abstract XML is the lingua franca in the Web (and Grid) services world and so will play amajor role in the construction of the Virtual Observatory. Its great advantages are its flexibilityplatform-independence ease of transformation and the wide variety of existing software thatcan process it. An obvious disadvantage in its use as an astronomical data format is itsverbosity; the number of bytes taken up writing the XML tags can easily outnumber thoseconstituting the actual astronomical data. This becomes prohibitively inefficient when largeamounts of data are stored in XML and the developers of VOTable sought to circumvent thisby allowing for the use of binary data either in the VOTable document itself or in an externalfile linked from it. The verbosity of XML in this regard is a problem in many other disciplinesand computer scientists are developing more generic solutions to that found in the …,IAU Joint Discussion,2003,1
Transaction Repair for Multi-Version Concurrency Control,Mohammad Dashti; Sachin Basil John; Amir Shaikhha; Christoph Koch,Abstract The optimistic variants of Multi-Version Concurrency Control (MVCC) avoidblocking concurrent transactions at the cost of having a validation phase. Upon failure in thevalidation phase; the transaction is usually aborted and restarted from scratch. The" abortand restart" approach becomes a performance bottleneck for use cases with high contentionobjects or long running transactions. In addition; restarting from scratch creates a negativefeedback loop in the system; because the system incurs additional overhead that may createeven more conflicts. In this paper; we propose a novel approach for conflict resolution inMVCC for in-memory databases. This low overhead approach summarizes the transactionprograms in the form of a dependency graph. The dependency graph also contains theconstructs used in the validation phase of the MVCC algorithm. Then; when encountering …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Solving the Join Ordering Problem via Mixed Integer Linear Programming,Immanuel Trummer; Christoph Koch,Abstract We transform join ordering into a mixed integer linear program (MILP). This allowsto address query optimization by mature MILP solver implementations that have evolvedover decades and steadily improved their performance. They offer features such as anytimeoptimization and parallel search that are highly relevant for query optimization. We present aMILP formulation for searching left-deep query plans. We use sets of binary variables torepresent join operands and intermediate results; operator implementation choices or thepresence of interesting orders. Linear constraints restrict value assignments to the onesrepresenting valid query plans. We approximate the cost of scan and join operations vialinear functions; allowing to increase approximation precision up to arbitrary degrees. Weintegrated a prototypical implementation of our approach into the Postgres optimizer and …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,*
Building Efficient Query Engines in a High-Level Language,Amir Shaikhha; Yannis Klonatos; Christoph Koch,Abstract: Abstraction without regret refers to the vision of using high-level programminglanguages for systems development without experiencing a negative impact onperformance. A database system designed according to this vision offers both increasedproductivity and high performance; instead of sacrificing the former for the latter as is thecase with existing; monolithic implementations that are hard to maintain and extend. In thisarticle; we realize this vision in the domain of analytical query processing. We presentLegoBase; a query engine written in the high-level language Scala. The key technique toregain efficiency is to apply generative programming: LegoBase performs source-to-sourcecompilation and optimizes the entire query engine by converting the high-level Scala codeto specialized; low-level C code. We show how generative programming allows to easily …,arXiv preprint arXiv:1612.05566,2016,*
XML-based Execution Plan Format (XEP),Christoph Koch,Execution plan analysis is one of the most common SQL tuning tasks performed byrelational database administrators and developers. Currently each database managementsystem (DBMS) provides its own execution plan format; which supports system-specificdetails for execution plans and contains inherent plan operators. This makes SQL tuning achallenging issue. Firstly; administrators and developers often work with more than oneDBMS and thus have to rethink among different plan formats. In addition; the analysis toolsof execution plans only support single DBMSs; or they have to implement separate logic tohandle each specific plan format of different DBMSs. To address these problems; this paperproposes an XML-based Execution Plan format (XEP); aiming to standardize therepresentation of execution plans of relational DBMSs. Two approaches are developed …,Open Journal of Databases (OJDB),2016,*
Special issue on best papers of VLDB 2013,Michael H Böhlen; Christoph Koch,VLDB is a premier annual international forum for data management; and it targets databaseresearchers; users; vendors; and application developers. Each year the conference coverscurrent issues in data management and database systems; which remain key technologicalcornerstones of emerging applications of the twenty-first century. VLDB 2013 took place atthe picturesque town of Riva del Garda in Italy. We received a total of 559 research papersubmissions; and of these 127 submissions were accepted for presentation at the VLDBconference. A committee that included Vanja Josifovski; Mohamed Mokbel; Dan Olteanu;Ken Salem; Divesh Srivastava; and Jens Teubner selected the best papers that have beensubmitted to VLDB 2013. The authors of these papers were invited to submit an extendedversion of their papers to a special issue of the VLDB Journal. The result of this process is …,The VLDB Journal,2015,*
A Study of Linguistic Drift On Le Temps Newspaper Corpus,Nicolas Bornand; Farah Bouassida; Malik Bougacha; Gil Brechbühler; Tao Lin; Cynthia Oeschger; Marc Schär; Jérémy Weber; Immanuel Trummer; Christoph Koch,Intuition helps us think that; through time; language is changing because of the variation ofthe vocabulary but also in the way words of this vocabulary are used. Along with discoveriesand civilization evolution; some words disappear and others emerge. Moreover; the 21stcentury is characterized by secondary source documents that influence us in our way ofexpressing ourselves. Thus; our study tries to define the features that could prove that thereis a linguistic drift and to quantify the changes with a metric. Our dataset is a french articlescorpus from 1840 to 1998 from two Swiss newspapers. We are provided with xml files ofOCRized articles. The fact that there is less articles during the first years is sometimes to beconsidered; as is the fact that for years 1917 to 1919 we just have one newspaper's articlesand that in 1998; we just have the articles of a few months. Within this project; we want to …,*,2015,*
Computer system and methods for performing data-driven coordination,*,A computer system and computer implemented method that obtains coordinated results fromat least two queries by utilizing context data of each query. Specifically; the computer systemand computer implemented method facilitates enhanced querying functionality by matchingentangled queries to achieve coordinated results.,*,2014,*
Sparse simplex projections for portfolio optimization,Anastasios Kyrillidis; Stephen Becker; Volkan Cevher; Christoph Koch,II. BACKGROUND Definition II.1. The PLk (w) operator keeps the k-largest entries of w (not inmagnitude) and sets the rest to zero. Definition II.2. The projectors onto the simplex and the extendedsimplex are given by (Pλ+ (w))i = [wi − τρ]+; and (Pλ(w))i = wi −τp; respectively where τρ := 1ρ (∑ρ i=1 wi − λ). For the simplex case; ρ := max{j : wj > 1 j (∑j i=1 wi − λ)}. III. MAIN RESULTSAlgorithm 1 suggests an greedy approach for Problem 1: We select the set S⋆ by naively projectingw as PLk (w). Theorem 1. Algorithm 1 exactly solves Problem 1 … Unfortunately; the GSSPfails for Problem 2. As a result; we propose Algorithm 2 for the Σk ∩ ∆λ case: We first select theindex of the largest element with the same sign as λ and then; greedily grow the index set byfinding the farthest element from the current mean; as adjusted by lambda. Theorem 2. Algorithm2 exactly solves Problem 2 … IV. EXPERIMENTS While many portfolio optimization …,Global Conference on Signal and Information Processing (GlobalSIP); 2013 IEEE,2013,*
" Offizielle" elektronische Version der Publikation (entsprechend ihrem Digital Object Identifier-DOI),E Serral Asensio; O Kovalenko; T Moser; S Biffl,The use of high-level abstraction models can facilitate and improve not only system developmentbut also runtime system evolution. This is the idea of this work; in which behavioural modelscreated at design time are also used at runtime to evolve system behaviour. These behaviouralmodels describe the routine tasks that users want to be automated by the system. However;users´ needs may change after system deployment; and the routine tasks automated by the systemmustevolve to adapt to these changes. To facilitate this evolution; the automation of the specified routinetasks is achieved by directly interpreting the models at runtime. This turns models into the primarymeans to understand and interact with the system behaviour associated with the routine tasksas well as to execute and modify it. Thus; we provide tools to allow the adaptation of this behaviourby modifying the models at runtime. This means that the system behaviour evolution is …,Multimedia Tools and Applications,2013,*
Loop elimination for database updates,Vadim Savenkov; Reinhard Pichler; Christoph Koch,Abstract The additional expressive power of procedural extensions of query and updatelanguages come at the expense of trading the efficient set-at-a-time processing of databaseengines for the much less efficient tuple-at-a-time processing of a procedural language. Inthis work; we consider the problem of rewriting for-loops with update statements intosequences of updates which do not use loops or cursors and which simultaneously carry outthe action of several loop iterations in a set-at-a-time manner. We identify idempotence asthe crucial condition for allowing such a rewriting. We formulate concrete rewrite rules forsingle updates in a loop and extend them to sequences of updates in a loop.,British National Conference on Databases,2013,*
VLDB Endowment,Michael Böhlen; Christoph Koch; Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu; Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; Dan Olteanu; Divesh Srivastava; Jens Teubner; Stefan Manegold; Peer Kröger; Stratis D Viglas,39th International Conference on Very Large Data Bases; Riva del Garda; Trento; Italy … Proceedingsof the 39th International Conference on … Very Large Data Bases; Riva del Garda; Trento; Italy… Ashraf Aboulnaga; Sihem Amer‐Yahia; Chee Yong Chan; Yanlei Diao; Ada Waichee Fu;Johannes Gehrke; Alon Halevy; Jayant Haritsa; Nikos Mamoulis; Thomas Neumann; DanOlteanu; Divesh Srivastava; Jens Teubner … The 39th International Conference on Very LargeData Bases; Riva del Garda; Trento; Italy … Permission to make digital or hard copies of portionsof this work for personal or classroom use is granted without fee provided that copies are notmade or distributed for profit or commercial advantage and that copies bear this notice and thefull citation on the first page. Copyright for components of this work owned by others than VLDBEndowment must be honored. Abstracting with credit is permitted. To copy otherwise; to …,*,2013,*
Guest editors' Introduction to the special section on the 27th international conference on data engineering (ICDE 2011),Serge Abiteboul; Christoph Koch; Kian-Lee Tan; Jian Pei,THE 27th International Conference on Data Engineering (ICDE 2011) was held inHannover; Germany; on 11-16 April. ICDE 2011 received 491 research manu- scripts; 26 industrialcontributions; and 67 demo propo- sals; for a total of over 584 submissions. The research programfeatured 98 papers; the industrial program included eight submitted papers; and 21 demonstrationproposals were selected for presentation to the con- ference audience. This special section consistsof journal versions of eight best papers among the 98 research papers accepted by the programcommittees. These papers have been significantly revised; improved; and extended beyondtheir initial version that appears in the proceedings of ICDE 2011. They went through a rigorousreview process to ensure the high quality and comply with TKDE publication standards. Thefirst paper; titled “Computing Structural Statistics by Keywords in Databases” by Lu Qin …,IEEE Transactions on Knowledge and Data Engineering,2012,*
Markov chain Monte Carlo and databases,Christoph Koch,Abstract Several currently ongoing research efforts aim to combine Markov Chain MonteCarlo (MCMC) with database management systems. The goal is to scale up themanagement of uncertain data in contexts where only MCMC is known to be applicable orwhere the range and flexibility of MCMC provides a compelling proposition for powerful andinteresting systems. This talk surveys recent work in this area and identifies open researchchallenges.,International Conference on Scalable Uncertainty Management,2010,*
Working Group: Classification; Representation and Modeling.,S Das; C Koch; B König-Ries; Ander de Keijzer; V Markl; A Deshpande; M van Keulen; PJ Haas; IF Ilyas; T Neumann; D Olteanu; M Theobald; V Vassalos,KNAW Narcis. Back to search results. Publication Working Group: Classification;Representation and Modeling. (2009). Pagina-navigatie: Main …,*,2009,*
Applications of Automata in XML Processing,Christoph Koch,Abstract XML is at once a document format and a semistructed data model; and has becomea de-facto standard for exchanging data on the Internet. XML documents can alternatively beviewed as labeled trees; and tree automata are natural mechanisms for a wide range ofprocessing tasks on XML documents. In this talk; I survey applications of automata in XMLprocessing with an emphasis on those directions of work that so far have had the greatestpractical impact. The talk will consist of three parts. In the first; I will discuss XML validation.The standard schema formalisms for XML; Document Type Definitions and XML Schema;are regular tree grammars at their core. These official standards of the World Wide WebConsortium are well-founded in automata theory and formal language theory; and aredesigned to incorporate special restrictions to facilitate the creation of automata for …,International Conference on Implementation and Application of Automata,2009,*
Uncertainty Management in Information Systems–Executive Summary by the Organizers,Christoph Koch; Birgitta König-Ries; Volker Markl; Maurice van Keulen,Computer science has long pretended that information systems are perfect mirror images ofa perfect world. Database management systems; eg; work under the assumption that thedata stored represent a correct subset of the real world. Of course; this idealized assumptionis rarely true. Information systems contain–wrong information caused; eg; by data entryerrors: This is a common problem for instance in genomic databases–imprecise or falselyprecise information; eg; a measuring device will provide information with a certain precisiononly. Typically; information systems store the measured date; but do not store informationabout the conditions under which this data is true and the precision achieved.–incompleteinformation. A certain piece of information may not be available to the information system.–inconsistent information. Different information systems may contain contradictory …,Uncertainty Management in Information Systems: Dagstuhl Seminar 08421,2009,*
Imprecision; Diversity and Uncertainty: Disentangling Threads in Uncertainty Management: 08421 Working Group,M Spiliopoulou; H-J Lenz; J Wijsen; M Renz; R Kruse; M Stern; C Koch; B König-Ries; V Markl,*,*,2009,*
Report of the Probabilistic Databases Benchmarking: 08421 Working Group,C Koch; C Re; D Olteanu; H-J Lenz; PJ Haas; JZ Pan; B König-Ries; V Markl,*,*,2009,*
Probabilistic Data Integration,C Koch; B König-Ries; V Markl; Maurice van Keulen,In data integration efforts such as in portal development; much development time is devotedto entity resolution. Often advanced similarity measurement techniques are used to removesemantic duplicates or solve other semantic conflicts. It proofs impossible; however; toautomatically get rid of all semantic problems. An often-used rule of thumb states that about90% of the development effort is devoted to semi-automatically resolving the remaining 10%hard cases. In an attempt to significantly decrease human effort at data integration time; wehave proposed an approach that strives for a'good enough'initial integration which storesany remaining semantic uncertainty and conflicts in a probabilistic XML database. Theremaining cases are to be resolved during use with user feedback. We conducted extensiveexperiments on the effects and sensitivity of rule denition; threshold tuning; and user …,*,2009,*
IMPrECISE: Good-is-good-enough Data Integration,C Koch; B König-Ries; V Markl; Maurice van Keulen,The IMPrECISE system is a probabilistic XML database system which supports near-automatic integration of XML documents. What is required of the user is to configure thesystem with a few simple knowledge rules allowing the system to sufficiently eliminatenonsense possibilities. We demonstrate the integration process under conditions withvarying degrees of confusion and different sets of rules. Even when an integrated documentstill contains much uncertainty; it can be queried effectively. The system produces asequence of possible result elements ranked by likelihood. User feedback on query resultsfurther reduces uncertainty which in a sense continues the semantic integration processincrementally. We demonstrate querying on integrated documents and measure answerquality with adapted precision and recall measures. The user feedback mechanism has …,*,2009,*
08421 Working Group: Classification; Representation and Modeling,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,Abstract This report briefly summarizes the discussions carried out in the working group onclassification; representation and modeling of uncertain data. The discussion was dividedinto two subgroups: the first subgroup studied how different representation and modelingalternatives currently proposed can fit in a bigger picture of theory and technologyinteraction; while the second subgroup focused on contrasting current systemimplementations and the reasons behind such diverse class of available prototypes. Wesummarize the findings of these two groups and the future steps suggested by groupmembers.,Dagstuhl Seminar Proceedings,2009,*
Parameterized Complexity of Queries,Christoph Koch,P/FDM [5–7] integrated a functional data model with the logic programming language Prologfor general-purpose computation. The data model can be seen as an Entity-Relationshipdiagram with sub-types; much like a UML Class Diagram. The idea was for the user to beable to define a computation over objects in the diagram; instead of just using it as a schemadesign aid. Later versions of P/FDM included a graphic interface [2; 4] to build queries inDAPLEX syntax by clicking on the diagram and filling in values from menus.,*,2009,*
Logical Foundations of Web Data Extraction,Christoph Koch,A language model assigns a probability to a piece of unseen text; based on some trainingdata. For example; a language model based on a big English newspaper archive isexpected to assign a higher probability to ''a bit of text''than to ''aw pit tov tags;''because thewords in the former phrase (or word pairs or word triples if so-called N-Gram Models areused) occur more frequently in the data than the words in the latter phrase. For informationretrieval; typical usage is to build a language model for each document. At search time; thetop ranked document is the one whose language model assigns the highest probability tothe query.,*,2009,*
08421 Working Group: Classification; Representation and Modeling.,Anish Das Sarma; Ander de Keijzer; Amol Deshpande; Peter J Haas; Ihab F Ilyas; Christoph Koch; Thomas Neumann; Dan Olteanu; Martin Theobald; Vasilis Vassalos,*,Uncertainty Management in Information Systems,2008,*
Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB); University of Vienna; Austria; September 23-27; 2007,Christoph Koch; Johannes Gehrke; Minos N Garofalakis; Divesh Srivastava; Karl Aberer; Anand Deshpande; Chee Yong Chan; Venkatesh Ganti; Carl-Christian Kanne; Wolfgang Klas; Erich J Neuhold,*,*,2007,*
MayBMS: A Possible Worlds Base Management System,Lyublena Antova; Christoph Koch; Dan Olteanu,Incomplete information is frequent in real-world applications. This is often the case in dataintegration scenarios; in scientific data collections; or whenever the information is acquiredusing human interaction and is erroneous or imperfect. The different interpretations ofincomplete information yield different possible worlds. A system for managing incompletedata faces the challenge of being able to represent large sets of possible worlds compactly;while at the same time supporting efficient processing of the data. Nevertheless; there hasbeen little research so far into expressive yet scalable systems for managing incompleteinformation. Most current representation models have at least one of two flaws; some of them(such as or-set relations and v-tables) are not strong enough to represent query answerswithin the same formalism; and other (such as the c-tables) are strong enough but …,*,2006,*
Building a Native XML-DBMS as a Term Project in a Database Systems Course,Christoph Koch; Dan Olteanu; Stefanie Scherzinger,This is to report on a database systems course the first author held in the summer semesterof 2005 at Saarland University; Saarbrücken; Germany. This course was an experiment inseveral respects. For one; we wanted to teach a systems course with a practical part inwhich students apply the material taught to build the core of a database managementsystem. Such a systems building effort seems to be quite common in top-tier US universities;but it is rare in Europe. One main reason for this is that European curricula often requirestudents to take many small courses per term. Students then cannot be required to invest thetime necessary for such a systems-building effort into an individual course. In Saarbrücken;this fortunately does not apply and students are expected to take only about two maincourses per term.(The database systems course in Saarbrücken is worth 9 points in the …,*,2006,*
Core Database Technology Program Committee,Anastassia Ailamaki; Gustavo Alonso; Walid Aref; Lars Arge; Brian Babcock; Mikael Berndtsson; Elisa Bertino; Claudio Bettini; Michael Boehlen; Anthony Bonner; Philippe Bonnet; Alex Buchmann; Tiziana Catarci; Surajit Chaudhuri; Peter Dadam; Amol Deshpande; Asuman Dogac; Christos Faloutsos; Elena Ferrari; Johann-Christoph Freytag; Dieter Gawlick; Johannes Gehrke; Torsten Grust; Ralf Hartmut Güting; Jayant Haritsa; Chris Jermaine; Christoph Koch; George Kollios; Mong Li Lee; Wolfgang Lindner; David Lomet; Hongjun Lu; Samuel Madden; Giansalvatore Mecca; Alberto Mendelzon; Rosa Meo; Tova Milo; Michele Missikoff; C Mohan; Mario Nascimento; Shojiro Nishio; Ed Omiecinski; Norman Paton; Torben Bach Pedersen; Calton Pu; Philippe Pucheral; Raghu Ramakrishnan; Thomas Rölleke; Ken Ross; Gunther Saake; Albrecht Schmidt; Marc Scholl; Bernhard Seeger,Committee Chair: Martin Kersten; CWI; The Netherlands … Serge Abiteboul; INRIA; France AnastassiaAilamaki; Carnegie Mellon University; USA Gustavo Alonso; ETH Zurich; Switzerland WalidAref; Purdue University; USA Lars Arge; Aarhus University; Denmark Brian Babcock; StanfordUniversity; USA Mikael Berndtsson; University of Skövde; Sweden Elisa Bertino; PurdueUniversity; USA Claudio Bettini; University of Milan; Italy Michael Boehlen; Free University ofBolzano/Bozen; Italy Peter Boncz; CWI; The Netherlands Anthony Bonner; University ofToronto; Canada Philippe Bonnet; University of Copenhagen; Denmark Alex Buchmann; Universityof Darmstadt; Germany Tiziana Catarci; University of Rome 'La Sapienza'; Italy SurajitChaudhuri; Microsoft; USA Vassilis Christophides; FORTH; Greece Peter Dadam; Universityof Ulm; Germany Amol Deshpande; University of California; Berkeley; USA Asuman …,VLDB 2005: 31st International Conference on Very Large Data Bases: Proceedings of the 31st International Conference on Very Large Data Bases; Trondheim; Norway; August 30-September 2; 2005,2005,*
Coordination issues in multi-agent event data processing,Christoph Koch; Paolo Petta,Abstract We present a multi-agent systems approach to distributed event-data processing asit is pervasive in scientific computing environments. The task investigated is the one ofconfiguration and execution of event-data processing pipelines to be assembled from singlecomputational services (agents) that perform an asynchronous mapping of streams of inputsto streams of outputs; where the specification is given by characterizing the final output of apipeline. Comprehensive declarative descriptions of the capabilities of single agents in suchsystems can be shown to be computationally intractable because of the complexity of themapping between inputs and outputs of individual agents. We therefore investigate theconsequences of circumventing this problem by publishing just the output capabilities ofagents; performing the transformation of output requirements to input requirements …,International Workshop on Engineering Societies in the Agents World,2000,*
On Information Integration in Large Scientific Collaborations,Christoph Koch; Paolo Petta; J Le Goff; Richard McClatchey,Abstract We discuss the requirements for information integration in large scientificcollaborations and arrive at the conclusion that an architecture is needed that follows thedeclarative paradigm for reasoning completeness; maintainability and reuse of previouslyencoded knowledge but does not take the classical approach of integrating all sourcesagainst a single common “global” information model. Instead; we propose a local-as-viewinfrastructure that allows to make integrated information from remote sources available toindividual (legacy) information systems across multiple different integration models. Wediscuss our architecture and compare it to previous approaches in the literature.,*,2000,*
Home> Authorities> Lab> Record# 252342,Christoph Koch; General Note,English: Français. Search; Personalize: Your alerts; Your baskets; Your searches. Youralerts; Your baskets; Your searches. Browse Collections; Help. guest :: login. Home >Authorities > Lab > Record #252342. Infoscience. Information; Usage statistics; Files. DATA.Formal Name (French). Laboratoire de théorie et applications d'analyse de données. FormalName (Engish). Data Analysis Theory and Applications Laboratory. Lab Manager. Koch;Christoph. Group ID. U12327. Author(s): See affiliated authors. Institute: IINFCOM. Faculty:IC. General Note: Members of DATA-unit. Linked Resource: http://data.epfl.ch/. Publications:See complete list of publications (130). View as: MARC | MARCXML. Back to search. Recordcreated 2018-01-27; last modified 2018-01-27 …,Group,*,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Immanuel Trummer; Christoph Koch,We propose a generalization of the classical database query optimization problem: multi-objective parametric query (MPQ) optimization. MPQ compares alternative processing plansaccording to multiple execution cost metrics. It also models missing pieces of information onwhich plan costs depend upon as parameters. Both features are crucial to model queryprocessing on modern data processing platforms.MPQ generalizes previously proposedquery optimization variants; such as multi-objective query optimization; parametric queryoptimization; and traditional query optimization. We show; however; that the MPQ problemhas different properties than prior variants and solving it requires novel methods. We presentan algorithm that solves the MPQ problem and finds; for a given query; the set of all relevantquery plans. This set contains all plans that realize optimal execution cost tradeoffs for …,Communications of the ACM,*,*
" The DLV System"; Poster: 8 th European Conference on Artificial Intelligence; JELIA 2002; Cosenza; Italy (eingeladen); 23.09. 2002-26.09. 2002; in:" Logics in Artifi...,N Leone; G Pfeifer; W Faber; F Calimeri; T DellArmi; T Eiter; G Gottlob,The development of the DLVSystem has started as a research project financed by FWF(Austrian Science Funds) in 1996; and has evolved into an international collaboration overthe years. Currently; the University of Calabria and TU Wien participate in the projectsupported by a scientific-technological collaboration between Italy and Austria. At the time ofwriting; the latest version of the system has been released on April 12; 2002.,*,*,*
Automata; MSO; and Decidability of S1S,Christoph Koch,*,*,*,*
MayBMS: A Probabilistic DBMS,Jiewen Huang; Lyublena Antova; Christoph Koch; Dan Olteanu,Page 1. MayBMS: A Probabilistic DBMS Jiewen Huang ∗;∗∗ ; Lyublena Antova ∗ ; ChristophKoch ∗ ; and Dan Olteanu ∗∗ ∗ Cornell University ∗∗ Oxford University Example: random graphsGoal: Compute the probability that a random graph contains a triangle. T uv bit p 1 2 1 .5 1 2 0.5 1 3 1 .5 1 3 0 .5 2 3 1 .5 2 3 0 .5 create table E as select Qu; Qv from (repair key (u;v) in T weightby p) Q where Q.bit = 1; 8 possible worlds; one has a triangle. E not given as symmetric relation;but as subset of total order. select conf() as triangle_prob from E e1; E e2; E e3 where e1.v =e2.u and e2.v = e3.v and e1.u = e3.u and e1.u < e2.u and e2.u < e3.v; triangle_prob 0.125 Example:hypothetical queries Suppose I buy a company and exactly one em- ployee leaves. Which skillsdo I gain for certain? CE CID EID Google Bob Google Joe Yahoo Dan Yahoo Bill Yahoo FredES EID Skill Bob Web Joe Web Dan Java Dan Web …,*,*,*
The Goblin Project: Position Paper,Christoph Koch,Abstract This paper summarizes my views on the following issues:(1) the inappropriatenessof a common meta-model which “integrates” heterogeneous data sources versus anapproach based on multiple conceptualizations;(2) the drawbacks of a single commonontology compared to an infrastructure in which many conceptualizations may co-exist;(3)the integration of problem-solving components into the information infrastructure;(4) the roleand scope of mediation;(5) the scope of a domain; and (6) some of my views on what thearchitecture implemented in the GOBLIN project should look like. This is a collection of threeindependent discussions (each one occupying one section); rather than an integratedpaper.,*,*,*
System Description: DLV,Wolfgang Faber; Giuseppe Ielpa; Christoph Koch; Nicola Leone; Simona Perri; Gerald Pfeifer,DLV is an efficient Answer Set Programming (ASP) system implementing the consistentanswer set semantics [5] with various language enhancements like support for logicprogramming with inheritance and queries; integer arithmetics; and various other built-inpredicates.,*,*,*
Citations; Certificates and Object References,Christoph Koch,There are two ways in which objects may refer to other objects. The first is common indatabases; and we shall refer to it as an object reference or oref. For example; if a databasecontains employees and departments; and we want to represent the fact that Joe works inthe Widget department; we place the appropriate oref–a pointer or foreign key value–into thedepartment field of the record associated with Joe. If; at some later time; we ask for themanager of Joe's department; we look for the most recent record associated with the Widgetdepartment. In all likelihood the database will only store the most recent version. Nowcompare this with the situation in which you are writing a paper for publication on the Web. Ifyou refer to another document; you often assume that the document is fixed. However; if thatdocument has several revisions or versions you can–and should–take care to insert a …,*,*,*
" Attribute Grammars for Scalable Query Processing on XML Streams"; Poster: The 9 th International Workshop on Data Base Programming Languages; Potsdam; D...,C Koch; S Scherzinger,We introduce the new notion of XML Stream Attribute Grammars (XSAGs). XSAGs are thefirst scalable query language for XML streams (running strictly in linear time with boundedmemory consumption independent of the size of the stream) the allows for actual datatransformations rather than just document filtering. XSAGs are also relatively easy to use forhumans. Moreover; the XSAG formalism provides a strong intuition for wich queries can orcannot be processed scalably on streams. We introduce XSAGs together with the necessarylanguage-theoretic machinery; study their theoretical properties such as theirexpressiveness and complexity; and discuss their implementation.,*,*,*
Combined Static and Dynamic Analysis for Effective Buffer Minimization in Streaming XQuery Evaluation,Christoph Koch; Michael Schmidt,Abstract Coming along with the proliferation of the internet and increasing bandwidths; therelevance of data stream processing has rapidly increased in recent years. Along the way;XML has evolved into a de facto standard format for data interchange. In the meantime;many applications have been developed that follow both trends as they have to deal withXML data streams. The efficient extraction of data from XML streams has become a non-trivial challenge; as streaming data may arrive at very high rates. In consequence for thegrowing importance; different languages have been proposed to query data from XMLdocuments; among them XQuery and XPath. Efficient evaluation of XQuery; which is morepowerful than XPath; has been subject to extensive studies throughout the years. NativeXML database management systems; which rely on a physical database; are ill-suited for …,*,*,*
Patterns for Multi-Layered System Architectures,P Brooks; C Koch; Z Kovacs; JM Le Goff; R McClatchey,*,*,*,*
Arb: An Implementation of Selecting Tree Automata for XML Query Processing,Christoph Koch,*,*,*,*
XPath Leashed (Invited Talk),Christoph Koch,*,PLAN-X 2007,*,*
