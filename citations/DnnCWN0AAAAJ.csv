Hogwild: A lock-free approach to parallelizing stochastic gradient descent,Feng Niu; Benjamin Recht; Christopher Ré; Stephen Wright,Abstract Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers haverecently proposed schemes to parallelize SGD; but all require performance-destroyingmemory locking and synchronization. This work aims to show using novel theoreticalanalysis; algorithms; and implementation that SGD can be implemented* without anylocking*. We present an update scheme called Hogwild which allows processors access toshared memory with the possibility of overwriting each other's work. We show that when theassociated optimization problem is sparse; meaning most gradient updates only modifysmall parts of the decision variable; then Hogwild achieves a nearly optimal rate ofconvergence. We demonstrate experimentally that Hogwild outperforms alternative …,Advances in Neural Information Processing Systems,2011,920
Efficient top-k query evaluation on probabilistic data,Christopher Re; Nilesh Dalvi; Dan Suciu,Modern enterprise applications are forced to deal with unreliable; inconsistent andimprecise information. Probabilistic databases can model such data naturally; but SQL queryevaluation on probabilistic databases is difficult: previous approaches have either restrictedthe SQL queries; or computed approximate probabilities; or did not scale; and it was shownrecently that precise query evaluation is theoretically hard. In this paper we describe a novelapproach; which computes and ranks efficiently the top-k answers to a SQL query on aprobabilistic database. The restriction to top-k answers is natural; since imprecisions in thedata often lead to a large number of answers of low quality; and users are interested only inthe answers with the highest probabilities. The idea in our algorithm is to run in parallelseveral Monte-Carlo simulations; one for each candidate answer; and approximate each …,Data Engineering; 2007. ICDE 2007. IEEE 23rd International Conference on,2007,419
Probabilistic databases,Dan Suciu; Dan Olteanu; Christopher Ré; Christoph Koch,Abstract Probabilistic databases are databases where the value of some attributes or thepresence of some records are uncertain and known only with some probability. Applicationsin many areas such as information extraction; RFID and scientific data management; datacleaning; data integration; and financial risk assessment produce large volumes of uncertaindata; which are best modeled and processed by a probabilistic database. This bookpresents the state of the art in representation formalisms and query processing techniquesfor probabilistic data. It starts by discussing the basic principles for representing largeprobabilistic databases; by decomposing them into tuple-independent tables; block-independent-disjoint tables; or U-databases. Then it discusses two classes of techniques forquery evaluation on probabilistic databases. In extensional query evaluation; the entire …,Synthesis Lectures on Data Management,2011,340
Event queries on correlated probabilistic streams,Christopher Ré; Julie Letchner; Magdalena Balazinksa; Dan Suciu,Abstract A major problem in detecting events in streams of data is that the data can beimprecise (eg RFID data). However; current state-ofthe-art event detection systems such asCayuga [14]; SASE [46] or SnoopIB [1]; assume the data is precise. Noise in the data can becaptured using techniques such as hidden Markov models. Inference on these modelscreates streams of probabilistic events which cannot be directly queried by existing systems.To address this challenge we propose Lahar1; an event processing system for probabilisticevent streams. By exploiting the probabilistic nature of the data; Lahar yields a much higherrecall and precision than deterministic techniques operating over only the most probabletuples. By using a novel static analysis and novel algorithms; Lahar processes data orders ofmagnitude more efficiently than a naïve approach based on sampling. In this paper; we …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,298
Incremental knowledge base construction using DeepDive,Christopher De Sa; Alex Ratner; Christopher Ré; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang,Abstract Populating a database with information from unstructured sources—also known asknowledge base construction (KBC)—is a long-standing problem in industry and researchthat encompasses problems of extraction; cleaning; and integration. In this work; wedescribe DeepDive; a system that combines database and machine learning ideas to helpdevelop KBC systems; and we present techniques to make the KBC process more efficient.We observe that the KBC process is iterative; and we develop techniques to incrementallyproduce inference results for KBC systems. We propose two methods for incrementalinference; based; respectively; on sampling and variational techniques. We also study thetrade-off space of these methods and develop a simple rule-based optimizer. DeepDiveincludes all of these contributions; and we evaluate DeepDive on five KBC systems …,The VLDB Journal,2017,251
The MADlib analytics library: or MAD skills; the SQL,Joseph M Hellerstein; Christoper Ré; Florian Schoppmann; Daisy Zhe Wang; Eugene Fratkin; Aleksander Gorajek; Kee Siong Ng; Caleb Welton; Xixuan Feng; Kun Li; Arun Kumar,Abstract MADlib is a free; open-source library of in-database analytic methods. It provides anevolving suite of SQL-based algorithms for machine learning; data mining and statistics thatrun at scale within a database engine; with no need for data import/export to other tools. Thegoal is for MADlib to eventually serve a role for scalable database systems that is similar tothe CRAN library for R: a community repository of statistical methods; this time written withscale and parallelism in mind. In this paper we introduce the MADlib project; including thebackground that led to its beginnings; and the motivation for its open-source nature. Weprovide an overview of the library's architecture and design patterns; and provide adescription of various statistical methods in that context. We include performance andspeedup results of a core design pattern from one of those methods over the Greenplum …,Proceedings of the VLDB Endowment,2012,236
Parallel stochastic gradient algorithms for large-scale matrix completion,Benjamin Recht; Christopher Ré,Abstract This paper develops Jellyfish; an algorithm for solving data-processing problemswith matrix-valued decision variables regularized to have low rank. Particular examples ofproblems solvable by Jellyfish include matrix completion problems and least-squaresproblems regularized by the nuclear norm or γ _2-norm. Jellyfish implements a projectedincremental gradient method with a biased; random ordering of the increments. This biasedordering allows for a parallel implementation that admits a speed-up nearly proportional tothe number of processors. On large-scale matrix completion tasks; Jellyfish is orders ofmagnitude more efficient than existing codes. For example; on the Netflix Prize data set;prior art computes rating predictions in approximately 4 h; while Jellyfish solves the sameproblem in under 3 min on a 12 core workstation.,Mathematical Programming Computation,2013,229
MYSTIQ: a system for finding more answers by using probabilities,Jihad Boulos; Nilesh Dalvi; Bhushan Mandhani; Shobhit Mathur; Chris Re; Dan Suciu,Abstract MystiQ is a system that uses probabilistic query semantics [3] to find answers inlarge numbers of data sources of less than perfect quality. There are many reasons why thedata originating from many different sources may be of poor quality; and therefore difficult toquery: the same data item may have different representation in different sources; the schemaalignments needed by a query system are imperfect and noisy; different sources maycontain contradictory information; and; in particular; their combined data may violate someglobal integrity constraints; fuzzy matches between objects from different sources may returnfalse positives or negatives. Even in such environment; users some-times want to askcomplex; structurally rich queries; using query constructs typically found in SQL queries:joins; subqueries; existential/universal quantifiers; aggregate and group-by queries: for …,Proceedings of the 2005 ACM SIGMOD international conference on Management of data,2005,221
Tuffy: Scaling up statistical inference in markov logic networks using an rdbms,Feng Niu; Christopher Ré; AnHai Doan; Jude Shavlik,Abstract Markov Logic Networks (MLNs) have emerged as a powerful framework thatcombines statistical and logical reasoning; they have been applied to many data intensiveproblems including information extraction; entity resolution; and text mining. Currentimplementations of MLNs do not scale to large real-world data sets; which is preventing theirwidespread adoption. We present Tuffy that achieves scalability via three novelcontributions:(1) a bottom-up approach to grounding that allows us to leverage the full powerof the relational optimizer;(2) a novel hybrid architecture that allows us to perform AI-stylelocal search efficiently using an RDBMS; and (3) a theoretical insight that shows when onecan (exponentially) improve the efficiency of stochastic local search. We leverage (3) to buildnovel partitioning; loading; and parallel algorithms. We show that our approach …,Proceedings of the VLDB Endowment,2011,215
An asynchronous parallel stochastic coordinate descent algorithm,Ji Liu; Stephen J Wright; Christopher Ré; Victor Bittorf; Srikrishna Sridhar,Abstract We describe an asynchronous parallel stochastic coordinate descent algorithm forminimizing smooth unconstrained or separably constrained functions. The method achievesa linear convergence rate on functions that satisfy an essential strong convexity property anda sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicoresystem can be expected if the number of processors is O (n1/2) in unconstrainedoptimization and O (n1/4) in the separable-constrained case; where n is the number ofvariables. We describe results from implementation on 40-core processors.,The Journal of Machine Learning Research,2015,194
Probabilistic databases: diamonds in the dirt,Nilesh Dalvi; Christopher Ré; Dan Suciu,We illustrate using an example from an information extraction system. The Purple Soxasystem at Yahoo! Research focuses on technologies to extract and manage structuredinformation from the Web related to a specific community. An example is the DbLifesystem14 that aggregates structured information about the database community from dataon the Web. The system extracts lists of database researchers together with structured;related information such as publications they authored; their coauthor relationships; talksthey have given; their current affiliations; and their professional services. Figure 1 (a)illustrates the researchers' affiliations; and Figure 1 (b) illustrates their professional activities.Although most researchers have a single affiliation; in the data in Figure 1 (a); the extractedaffiliations are not unique. This occurs because outdated/erroneous information is often …,Communications of the ACM,2009,183
Automatic optimization for MapReduce programs,Eaman Jahani; Michael J Cafarella; Christopher Ré,Abstract The MapReduce distributed programming framework has become popular; despiteevidence that current implementations are inefficient; requiring far more hardware than atraditional relational databases to complete similar tasks. MapReduce jobs are amenable tomany traditional database query optimizations (B+ Trees for selections; column-store-styletechniques for projections; etc); but existing systems do not apply them; substantiallybecause free-form user code obscures the true data operation being performed. Forexample; a selection in SQL is easily detected; but a selection in a MapReduce program isembedded in Java code along with lots of other program logic. We could ask theprogrammer to provide explicit hints about the program's data semantics; but one ofMapReduce's attractions is precisely that it does not ask the user for such information …,Proceedings of the VLDB Endowment,2011,175
Factoring nonnegative matrices with linear programs,Ben Recht; Christopher Re; Joel Tropp; Victor Bittorf,Abstract This paper describes a new approach for computing nonnegative matrixfactorizations (NMFs) with linear programming. The key idea is a data-driven model for thefactorization; in which the most salient features in the data are used to express the remainingfeatures. More precisely; given a data matrix X; the algorithm identifies a matrix C thatsatisfies X= CX and some linear constraints. The matrix C selects features; which are thenused to compute a low-rank NMF of X. A theoretical analysis demonstrates that thisapproach has the same type of guarantees as the recent NMF algorithm of Arora etal.~(2012). In contrast with this earlier work; the proposed method has (1) better noisetolerance;(2) extends to more general noise models; and (3) leads to efficient; scalablealgorithms. Experiments with synthetic and real datasets provide evidence that the new …,Advances in Neural Information Processing Systems,2012,173
Materialized views in probabilistic databases: for information exchange and query optimization,Christopher Ré; Dan Suciu,Abstract Views over probabilistic data contain correlations between tuples; and the currentapproach is to capture these correlations using explicit lineage. In this paper we propose analternative approach to materializing probabilistic views; by giving conditions under which aview can be represented by a block-independent disjoint (BID) table. Not all views can berepresented as BID tables and so we propose a novel partial representation that canrepresent all views but may not define a unique probability distribution. We then giveconditions on when a query's value on a partial representation will be uniquely defined. Weapply our theory to two applications: query processing using views and informationexchange using views. In query processing on probabilistic data; we can ignore the lineageand use materialized views to more efficiently answer queries. By contrast; if the view has …,Proceedings of the 33rd international conference on Very large data bases,2007,168
Approximate lineage for probabilistic databases,Christopher Ré; Dan Suciu,Abstract In probabilistic databases; lineage is fundamental to both query processing andunderstanding the data. Current systems sa Trio or Mystiq use a complete approach inwhich the lineage for a tuple t is a Boolean formula which represents all derivations of t. Inlarge databases lineage formulas can become huge: in one public database (the GeneOntology) we often observed 10MB of lineage (provenance) data for a single tuple. In thispaper we propose to use approximate lineage; which is a much smaller formula keepingtrack of only the most important derivations; which the system can use to process queriesand provide explanations. We discuss in detail two specific kinds of approximate lineage:(1)a conservative approximation called sufficient lineage that records the most importantderivations for each tuple; and (2) polynomial lineage; which is more aggressive and can …,Proceedings of the VLDB Endowment,2008,163
Large-scale deduplication with constraints using dedupalog,Arvind Arasu; Christopher Ré; Dan Suciu,We present a declarative framework for collective deduplication of entity references in thepresence of constraints. Constraints occur naturally in many data cleaning domains and canimprove the quality of deduplication. An example of a constraint is" each paper has a uniquepublication venue''; if two paper references are duplicates; then their associated conferencereferences must be duplicates as well. Our framework supports collective deduplication;meaning that we can dedupe both paper references and conference references collectivelyin the example above. Our framework is based on a simple declarative Datalog-stylelanguage with precise semantics. Most previous work on deduplication eitherignoreconstraints or use them in an ad-hoc domain-specific manner. We also presentefficient algorithms to support the framework. Our algorithms have precise theoretical …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,154
Managing uncertainty in social networks.,Eytan Adar; Christopher Re,Abstract Social network analysis (SNA) has become a mature scientific field over the last 50years and is now an area with massive commercial appeal and renewed research interest.In this paper; we argue that new methods for collecting social nework strucuture; and theshift in scale of these networks; introduces a greater degree of imprecision that requiresrethinking on how SNA techniques can be applied. We discuss a new area in datamanagement; probabilistic databases; whose main research goal is to provide tools tomanage and manipulate imprecise or uncertain data. We outline the application buildingblocks necessary to build a large scale social networking application and the extent to whichcurrent research in probabilisitc databases addresses these challenges.,IEEE Data Eng. Bull.,2007,125
Structured querying of Web text,Michael J Cafarella; Christopher Re; Dan Suciu; Oren Etzioni; Michele Banko,ABSTRACT The Web contains a huge amount of text that is currently beyond the reach ofstructured access tools. This unstructured data often contains a substantial amount of implicitstructure; much of which can be captured using information extraction (IE) algorithms. Bycombining an IE system with an appropriate data model and query language; we couldenable structured access to all of the Web's unstructured data. We propose a general-purpose query system called the extraction database; or ExDB; which supports SQL-likestructured queries over Web text. We also describe the technical challenges involved;motivated in part by our experiences with an early 90M-page prototype.,3rd Biennial Conference on Innovative Data Systems Research (CIDR); Asilomar; California; USA,2007,116
Towards a unified architecture for in-RDBMS analytics,Xixuan Feng; Arun Kumar; Benjamin Recht; Christopher Ré,Abstract The increasing use of statistical data analysis in enterprise applications has createdan arms race among database vendors to offer ever more sophisticated in-databaseanalytics. One challenge in this race is that each new statistical technique must beimplemented from scratch in the RDBMS; which leads to a lengthy and complexdevelopment process. We argue that the root cause for this overhead is the lack of a unifiedarchitecture for in-database analytics. Our main contribution in this work is to take a steptowards such a unified architecture. A key benefit of our unified architecture is thatperformance optimizations for analytics techniques can be studied generically instead of anad hoc; per-technique fashion. In particular; our technical contributions are theoretical andempirical studies of two key factors that we found impact performance: the order data is …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,115
A complete and efficient algebraic compiler for XQuery,Christopher Re; Jerome Simeon; Mary Fernandez,As XQuery nears standardization; more sophisticated XQuery applications are emerging;which often exploit the entire language and are applied to non-trivial XML sources. Wepropose an algebra and optimization techniques that are suitable for building an XQuerycompiler that is complete; correct; and efficient. We describe the compilation rules for thecomplete language into that algebra and present novel optimization techniques that addressthe needs of complex queries. These techniques include new query unnesting rewritingsand specialized join algorithms that account for XQuery's complex predicate semantics. Thealgebra and optimizations are implemented in the Galax XQuery engine; and yieldexecution plans that are up to three orders of magnitude faster than earlier versions ofGalax.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,98
Evaluating stream filtering for entity profile updates for TREC 2013 (KBA Track Overview),John R Frank; Steven J Bauer; Max Kleiman-Weiner; Daniel A Roberts; Nilesh Tripuraneni; Ce Zhang; Christopher Re; Ellen Voorhees; Ian Soboroff,Abstract: The Knowledge Base Acceleration (KBA) track in TREC 2013 expanded the entity-centric filtering evaluation from TREC KBA 2012. This track evaluates systems that filter atime-ordered corpus for documents and slot fills that would change an entity profile in apredefined list of entities. We doubled the size of the KBA streamcorpus to twelve thousandcontiguous hours and a billion documents from blogs; news; and Web content. Wequadrupled the number of entities as query topics from structured knowledge bases (KB);such as Wikipedia and Twitter. We also added a second task component: identifying entityslot values that change over the course of the stream. This Streaming Slot Filling (SSF)subtask focuses on natural language understanding and is a step toward decomposing theprofile update process undertaken by humans maintaining a knowledge base. A …,*,2013,95
The Beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,Communications of the ACM,2016,90
Worst-case optimal join algorithms,Hung Q Ngo; Ely Porat; Christopher Ré; Atri Rudra,Abstract Efficient join processing is one of the most fundamental and well-studied tasks indatabase research. In this work; we examine algorithms for natural join queries over manyrelations and describe a novel algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias; Grohe; and Marx; whogave bounds on the size of a full conjunctive query in terms of the sizes of the individualrelations in the body of the query. These bounds; however; are not constructive: they rely onShearer's entropy inequality which is information-theoretic. Thus; the previous results leaveopen the question of whether there exist algorithms whose running time achieve theseoptimal bounds. An answer to this question may be interesting to database practice; as weshow in this paper that any project-join plan is polynomially slower than the optimal …,Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems,2012,89
Global convergence of stochastic gradient descent for some non-convex matrix problems,Christopher De Sa; Kunle Olukotun; Christopher Ré,Abstract: Stochastic gradient descent (SGD) on a low-rank factorization is commonlyemployed to speed up matrix problems including matrix completion; subspace tracking; andSDP relaxation. In this paper; we exhibit a step size scheme for SGD on a low-rank least-squares problem; and we prove that; under broad sampling conditions; our methodconverges globally from a random starting point within $ O (\epsilon^{-1} n\log n) $ stepswith constant probability for constant-rank problems. Our modification of SGD relates it tostochastic power iteration. We also show experiments to illustrate the runtime andconvergence of the algorithm. Subjects: Learning (cs. LG); Optimization and Control (math.OC); Machine Learning (stat. ML) Cite as: arXiv: 1411.1134 [cs. LG](or arXiv: 1411.1134 v3[cs. LG] for this version) Submission history From: Christopher De Sa [view email][v1] Wed …,arXiv preprint arXiv:1411.1134,2014,84
Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features,Kun-Hsing Yu; Ce Zhang; Gerald J Berry; Russ B Altman; Christopher Ré; Daniel L Rubin; Michael Snyder,Abstract Lung cancer is the most prevalent cancer worldwide; and histopathologicalassessment is indispensable for its diagnosis. However; human evaluation of pathologyslides cannot accurately predict patients' prognoses. In this study; we obtain 2;186haematoxylin and eosin stained histopathology whole-slide images of lung adenocarcinomaand squamous cell carcinoma patients from The Cancer Genome Atlas (TCGA); and 294additional images from Stanford Tissue Microarray (TMA) Database. We extract 9;879quantitative image features and use regularized machine-learning methods to select the topfeatures and to distinguish shorter-term survivors from longer-term survivors with stage Iadenocarcinoma (P< 0.003) or squamous cell carcinoma (P= 0.023) in the TCGA data set.We validate the survival prediction framework with the TMA cohort (P< 0.036 for both …,Nature communications,2016,82
Query evaluation on probabilistic databases.,Christopher Ré; Nilesh N Dalvi; Dan Suciu,In this paper we consider the query evaluation problem: how can we evaluate SQL querieson probabilistic databases? Our discussion is restricted to single-block SQL queries usingstandard syntax; with a modified semantics: each tuple in the answer is associated with aprobability representing our confidence in that tuple belonging to the answer. We presenthere a short summary of the research done at the University of Washington into this problem.Consider the probabilistic database in Fig. 1. Productp contains three products; their namesand their prices are known; but we are unsure about their color and shape. Gizmo may bered and oval; or it may be blue and square; with probabilities p1= 0.25 and p2= 0.75respectively. Camera has three possible combinations of color and shape; and IPod hastwo. Thus; the table defines for each product a probability distribution on its colors and …,IEEE Data Eng. Bull.,2006,82
Dimmwitted: A study of main-memory statistical analytics,Ce Zhang; Christopher Ré,Abstract We perform the first study of the tradeoff space of access methods and replication tosupport statistical analytics using first-order methods executed in the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical analytics systems differ fromconventional SQL-analytics in the amount and types of memory incoherence that they cantolerate. Our goal is to understand tradeoffs in accessing the data in row-or column-orderand at what granularity one should share the model and data for a statistical task. We studythis new tradeoff space and discover that there are tradeoffs between hardware andstatistical efficiency. We argue that our tradeoff study may provide valuable information fordesigners of analytics engines: for each system we consider; our prototype engine can runat least one popular task at least 100× faster. We conduct our study across five …,Proceedings of the VLDB Endowment,2014,73
Efficient evaluation of having queries on a probabilistic database,Christopher Ré; Dan Suciu,Abstract We study the evaluation of positive conjunctive queries with Boolean aggregatetests (similar to HAVING queries in SQL) on probabilistic databases. Our motivation is tohandle aggregate queries over imprecise data resulting from information integration orinformation extraction. More precisely; we study conjunctive queries with predicateaggregates using MIN; MAX; COUNT; SUM; AVG or COUNT (DISTINCT) on probabilisticdatabases. Computing the precise output probabilities for positive conjunctive queries(without HAVING) is# P-hard; but is in P for a restricted class of queries called safe queries.Further; for queries without self-joins either a query is safe or its data complexity is# P-Hard;which shows that safe queries exactly capture tractable queries without self-joins. In thispaper; for each aggregate above; we find a class of queries that exactly capture efficient …,International Symposium on Database Programming Languages,2007,71
Materialization optimizations for feature selection workloads,Ce Zhang; Arun Kumar; Christopher Ré,Abstract There is an arms race in the data management industry to support statisticalanalytics. Feature selection; the process of selecting a feature set that will be used to build astatistical model; is widely regarded as the most critical step of statistical analytics. Thus; weargue that managing the feature selection process is a pressing data managementchallenge. We study this challenge by describing a feature selection language and asupporting prototype system that builds on top of current industrial R-integration layers. Fromour interactions with analysts; we learned that feature selection is an interactive human-in-the-loop process; which means that feature selection workloads are rife with reuseopportunities. Thus; we study how to materialize portions of this computation using not onlyclassical database materialization optimizations but also methods that have not …,ACM Transactions on Database Systems (TODS),2016,69
Distributed xquery,Christopher Re; Jim Brinkley; Kevin Hinshaw; Dan Suciu,XQuery is increasingly being used for ad-hoc integration of heterogeneous data sources thatare logically mapped to XML. For example; scientists need to query multiple scientificdatabases; which are distributed over a large geographic area; and it is possible to useXQuery for that. However; the language currently supports only the data shipping queryevaluation model (through the document () function): it fetches all data sources to a singleserver; then runs the query there. This is a major limitation for many applications; especiallywhen some data sources are very large; or when a data source is only a virtual XML viewover some other logical data model. We propose here a simple extension to XQuery thatallows query shipping to be expressed in the language; in addition to data shipping.,Workshop on Information Integration on the Web,2004,69
Hazy: making it easier to build and maintain big-data analytics,Arun Kumar; Feng Niu; Christopher Ré,The challenges are also tremendous. For one; more and more data comes in diverse forms:text; audio; video; OCR (optical character recognition); sensor data; etc. While existing data managementsystems predominantly assume that data has rigid; precise semantics; increasingly more data(albeit valuable) contains imprecision or inconsistency. For another; the proliferation ofever-evolving algorithms to gain insights from data (under names including machinelearning; data mining; and statistical analysis) can often be daunting to a developer with a particulardata set and specific goals: the developer not only has to keep up with the state of the art; butalso must expend significant development effort in experimenting with different algorithms …Many state-of-the-art approaches to both of these challenges are largely statistical and combinerich databases with software driven by statistical analysis and machine learning …,Queue,2013,68
Brainwash: A Data System for Feature Engineering.,Michael R Anderson; Dolan Antenucci; Victor Bittorf; Matthew Burgess; Michael J Cafarella; Arun Kumar; Feng Niu; Yongjoo Park; Christopher Ré; Ce Zhang,ABSTRACT A new generation of data processing systems; including web search; Google'sKnowledge Graph; IBM's Watson; and several different recommendation systems; combinerich databases with software driven by machine learning. The spectacular successes ofthese trained systems have been among the most notable in all of computing and havegenerated excitement in health care; finance; energy; and general business. But buildingthem can be challenging; even for computer scientists with PhD-level training. If thesesystems are to have a truly broad impact; building them must become easier. We exploreone crucial pain point in the construction of trained systems: feature engineering. Given thesheer size of modern datasets; feature developers must (1) write code with few effectiveclues about how their code will interact with the data and (2) repeatedly endure long …,CIDR,2013,67
Skew strikes back: New developments in the theory of join algorithms,Hung Q Ngo; Christopher Ré; Atri Rudra,Evaluating the relational join is one of the central algorithmic and most well-studiedproblems in database systems. A staggering number of variants have been consideredincluding Block-Nested loop join; Hash-Join; Grace; Sort-merge (see Grafe [17] for a survey;and [4; 7; 24] for discussions of more modern issues). Commercial database engines usefinely tuned join heuristics that take into account a wide variety of factors including theselectivity of various predicates; memory; IO; etc. This study of join queries notwithstanding;the textbook description of join processing is suboptimal. This survey describes recentresults on join algorithms that have provable worst-case optimality runtime guarantees. Wesurvey recent work and provide a simpler and unified description of these algorithms that wehope is useful for theory-minded readers; algorithm designers; and systems implementors …,ACM SIGMOD Record,2014,64
Using social media to measure labor market flows,Dolan Antenucci; Michael Cafarella; Margaret Levenstein; Christopher Ré; Matthew D Shapiro,ABSTRACT Social media enable promising new approaches to measuring economic activityand analyzing economic behavior at high frequency and in real time using informationindependent from standard survey and administrative sources. This paper uses data fromTwitter to create indexes of job loss; job search; and job posting. Signals are derived bycounting job-related phrases in Tweets such as “lost my job.” The social media indexes areconstructed from the principal components of these signals. The University of MichiganSocial Media Job Loss Index tracks initial claims for unemployment insurance at mediumand high frequencies and predicts 15 to 20 percent of the variance of the prediction error ofthe consensus forecast for initial claims. The social media indexes provide real-timeindicators of events such as Hurricane Sandy and the 2013 government shutdown …,*,2014,63
Optimizing statistical information extraction programs over evolving text,Fei Chen; Xixuan Feng; Christopher Re; Min Wang,Statistical information extraction (IE) programs are increasingly used to build real-world IEsystems such as Alibaba; CiteSeer; Kylin; and YAGO. Current statistical IE approachesconsider the text corpora underlying the extraction program to be static. However; many real-world text corpora are dynamic (documents are inserted; modified; and removed). As thecorpus evolves; and IE programs must be applied repeatedly to consecutive corpussnapshots to keep extracted information up to date. Applying IE from scratch to eachsnapshot may be inefficient: a pair of consecutive snapshots may change very little; butunaware of this; the program must run again from scratch. In this paper; we present CRFlex;a system that efficiently executes such repeated statistical IE; by recycling previous IE resultsto enable incremental update. As the first step; CRFlex focuses on statistical IE programs …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,59
Access methods for markovian streams,Julie Letchner; Christopher Re; Magdalena Balazinska; Matthai Philipose,Model-based views have recently been proposed as an effective method for querying noisysensor data. Commonly used models from the AI literature (eg; the hidden Markov model)expose to applications a stream of probabilistic and correlated state estimates computedfrom the sensor data. Many applications want to detect sophisticated patterns of states fromthese Markovian streams. Such queries are called event queries. In this paper; we present anew Markovian stream storage manager; Caldera. We develop and evaluate Caldera as acomponent of Lahar; a Markovian stream event query processing system developed inprevious work. At the heart of Caldera is a set of access methods for Markovian streams thatcan improve event query performance by orders of magnitude compared to existingtechniques; which must scan the entire stream. Our access methods use new adaptations …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,59
Energy-efficient abundant-data computing: The N3XT 1;000 x,Mohamed M Sabry Aly; Mingyu Gao; Gage Hills; Chi-Shuen Lee; Greg Pitner; Max M Shulaker; Tony F Wu; Mehdi Asheghi; Jeff Bokor; Franz Franchetti; Kenneth E Goodson; Christos Kozyrakis; Igor Markov; Kunle Olukotun; Larry Pileggi; Eric Pop; Jan Rabaey; Christopher Re; H-S Philip Wong; Subhasish Mitra,Next-generation information technologies will process unprecedented amounts of looselystructured data that overwhelm existing computing systems. N3XT improves the energyefficiency of abundant-data applications 1;000-fold by using new logic and memorytechnologies; 3D integration with fine-grained connectivity; and new architectures forcomputation immersed in memory.,Computer,2015,54
Manimal: relational optimization for data-intensive programs,Michael J Cafarella; Christopher Ré,Abstract The MapReduce distributed programming framework is very popular; but currentlylacks the optimization techniques that have been standard with relational database systemsfor many years. This paper proposes Manimal; which uses static code analysis to detectMapReduce program semantics and thereby enable wholly-automatic optimization ofMapReduce programs. For example; a programmer's map function that emits data onlywhen an if... statement holds true is essentially encoding a selection condition; codeanalysis can detect and characterize these conditions. If Manimal has an appropriate indexavailable; it can then alter MapReduce execution to use it. Manimal can address manydifferent optimization opportunities; including projections; structure-aware data compression;and others. However; this paper illustrates the system by focusing on one: efficient …,Procceedings of the 13th International Workshop on the Web and Databases,2010,50
Taming the wild: A unified analysis of hogwild-style algorithms,Christopher M De Sa; Ce Zhang; Kunle Olukotun; Christopher Ré,Abstract Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machinelearning problems. Researchers and industry have developed several techniques tooptimize SGD's runtime performance; including asynchronous execution and reducedprecision. Our main result is a martingale-based analysis that enables us to capture the richnoise models that may arise from such techniques. Specifically; we useour new analysis inthree ways:(1) we derive convergence rates for the convex case (Hogwild) with relaxedassumptions on the sparsity of the problem;(2) we analyze asynchronous SGD algorithmsfor non-convex matrix problems including matrix completion; and (3) we design and analyzean asynchronous SGD algorithm; called Buckwild; that uses lower-precision arithmetic. Weshow experimentally that our algorithms run efficiently for a variety of problems on …,Advances in neural information processing systems,2015,47
XQuery!: An XML query language with side effects,Giorgio Ghelli; Christopher Ré; Jérôme Siméon,Abstract As XML applications become more complex; there is a growing interest inextending XQuery with side-effect operations; notably XML updates. However; the presenceof side-effects is at odds with XQuery's declarative semantics in which evaluation order isunspecified. In this paper; we define “XQuery!”; an extension of XQuery 1.0 that supports first-class XML updates and user-level control over update application; preserving the benefits ofXQuery's declarative semantics when possible. Our extensions can be easily implementedwithin an existing XQuery processor and we show how to recover basic databaseoptimizations for such a language.,International Conference on Extending Database Technology,2006,46
Towards high-throughput Gibbs sampling at scale: A study across storage managers,Ce Zhang; Christopher Ré,Abstract Factor graphs and Gibbs sampling are a popular combination for Bayesianstatistical methods that are used to solve diverse problems including insurance risk models;pricing models; and information extraction. Given a fixed sampling method and a fixedamount of time; an implementation of a sampler that achieves a higher throughput ofsamples will achieve a higher quality than a lower-throughput sampler. We study how (andwhether) traditional data processing choices about materialization; page layout; and buffer-replacement policy need to be changed to achieve high-throughput Gibbs sampling forfactor graphs that are larger than main memory. We find that both new theoretical and newalgorithmic techniques are required to understand the tradeoff space for each choice. Onboth real and synthetic data; we demonstrate that traditional baseline approaches may …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,43
A machine reading system for assembling synthetic paleontological databases,Shanan E Peters; Ce Zhang; Miron Livny; Christopher Ré,Many aspects of macroevolutionary theory and our understanding of biotic responses toglobal environmental change derive from literature-based compilations of paleontologicaldata. Existing manually assembled databases are; however; incomplete and difficult toassess and enhance with new data types. Here; we develop and validate the quality of amachine reading system; PaleoDeepDive; that automatically locates and extracts data fromheterogeneous text; tables; and figures in publications. PaleoDeepDive performscomparably to humans in several complex data extraction and inference tasks andgenerates congruent synthetic results that describe the geological history of taxonomicdiversity and genus-level rates of origination and extinction. Unlike traditional databases;PaleoDeepDive produces a probabilistic database that systematically improves as …,PLoS one,2014,41
Emptyheaded: A relational engine for graph processing,Christopher R Aberger; Andrew Lamb; Susan Tu; Andres Nötzli; Kunle Olukotun; Christopher Ré,Abstract There are two types of high-performance graph processing engines: low-and high-level engines. Low-level engines (Galois; PowerGraph; Snap) provide optimized datastructures and computation models but require users to write low-level imperative code;hence ensuring that efficiency is the burden of the user. In high-level engines; users write inquery languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier touse but are orders of magnitude slower than the low-level graph engines. We presentEmptyHeaded; a high-level engine that supports a rich datalog-like query language andachieves performance comparable to that of low-level engines. At the core ofEmptyHeaded's design is a new class of join algorithms that satisfy strong theoreticalguarantees; but have thus far not achieved performance comparable to that of specialized …,ACM Transactions on Database Systems (TODS),2017,39
Asynchronous stochastic convex optimization,John C Duchi; Sorathan Chaturapruek; Christopher Ré,Abstract: We show that asymptotically; completely asynchronous stochastic gradientprocedures achieve optimal (even to constant factors) convergence rates for the solution ofconvex optimization problems under nearly the same conditions required for asymptoticoptimality of standard stochastic gradient procedures. Roughly; the noise inherent to thestochastic approximation scheme dominates any noise from asynchrony. We also giveempirical evidence demonstrating the strong performance of asynchronous; parallelstochastic optimization schemes; demonstrating that the robustness inherent to stochasticapproximation problems allows substantially faster parallel and asynchronous solutionmethods.,arXiv preprint arXiv:1508.00882,2015,38
Improvement in fast particle track reconstruction with robust statistics,MG Aartsen; R Abbasi; Yasser Abdou; M Ackermann; J Adams; JA Aguilar; M Ahlers; D Altmann; J Auffenberg; X Bai; M Baker; SW Barwick; V Baum; R Bay; JJ Beatty; S Bechet; J Becker Tjus; K-H Becker; ML Benabderrahmane; S BenZvi; P Berghaus; D Berley; E Bernardini; A Bernhard; DZ Besson; G Binder; D Bindig; M Bissok; E Blaufuss; J Blumenthal; David J Boersma; S Bohaichuk; C Bohm; D Bose; S Böser; Olga Botner; L Brayeur; H-P Bretz; AM Brown; R Bruijn; J Brunner; Michael Carson; J Casey; M Casier; D Chirkin; A Christov; B Christy; K Clark; F Clevermann; S Coenders; S Cohen; DF Cowen; AH Cruz Silva; M Danninger; J Daughhetee; JC Davis; M Day; C De Clercq; Sam De Ridder; P Desiati; KD De Vries; T DeYoung; JC Díaz-Vélez; M Dunkman; R Eagan; B Eberhardt; J Eisch; S Euler; PA Evenson; O Fadiran; AR Fazely; A Fedynitch; J Feintzeig; Tom Feusels; K Filimonov; C Finley; T Fischer-Wasels; S Flis; A Franckowiak; K Frantzen; T Fuchs; TK Gaisser; J Gallagher; L Gerhardt; L Gladstone; T Glüsenkamp; A Goldschmidt; G Golup; JG Gonzalez; JA Goodman; D Góra; DT Grandmont; D Grant; A Groß; C Ha; A Haj Ismail; P Hallen; Allan Hallgren; F Halzen; K Hanson; D Heereman; D Heinen; K Helbing; R Hellauer; S Hickford; GC Hill; KD Hoffman; R Hoffmann; A Homeier; K Hoshina; W Huelsnitz; PO Hulth; K Hultqvist; S Hussain; A Ishihara; E Jacobi; J Jacobsen; K Jagielski; GS Japaridze; K Jero; O Jlelati; B Kaminsky; A Kappes; T Karg; A Karle; JL Kelley; J Kiryluk; J Kläs; SR Klein; J-H Köhne; G Kohnen; H Kolanoski; L Köpke; C Kopper; S Kopper; DJ Koskinen; M Kowalski; M Krasberg; K Krings; G Kroll; J Kunnen; N Kurahashi; T Kuwabara; M Labare; H Landsman; MJ Larson; M Lesiak-Bzdak; M Leuermann; J Leute; J Lünemann,Abstract The IceCube project has transformed 1 km 3 of deep natural Antarctic ice into aCherenkov detector. Muon neutrinos are detected and their direction is inferred by mappingthe light produced by the secondary muon track inside the volume instrumented withphotomultipliers. Reconstructing the muon track from the observed light is challenging dueto noise; light scattering in the ice medium; and the possibility of simultaneously havingmultiple muons inside the detector; resulting from the large flux of cosmic ray muons. Thispaper describes work on two problems:(1) the track reconstruction problem; in which; givena set of observations; the goal is to recover the track of a muon; and (2) the coincident eventproblem; which is to determine how many muons are active in the detector during a timewindow. Rather than solving these problems by developing more complex physical …,Nuclear Instruments and Methods in Physics Research Section A: Accelerators; Spectrometers; Detectors and Associated Equipment,2014,38
Data programming: Creating large training sets; quickly,Alexander J Ratner; Christopher M De Sa; Sen Wu; Daniel Selsam; Christopher Ré,Abstract Large labeled training sets are the critical building blocks of supervised learningmethods and are key enablers of deep learning techniques. For some applications; creatinglabeled training sets is the most time-consuming and expensive part of applying machinelearning. We therefore propose a paradigm for the programmatic creation of training setscalled data programming in which users provide a set of labeling functions; which areprograms that heuristically label subsets of the data; but that are noisy and may conflict. Byviewing these labeling functions as implicitly describing a generative model for this noise;we show that we can recover the parameters of this model to" denoise" the generatedtraining set; and establish theoretically that we can recover the parameters of thesegenerative models in a handful of settings. We then show how to modify a discriminative …,Advances in Neural Information Processing Systems,2016,36
Toward a noncommutative arithmetic-geometric mean inequality: conjectures; case-studies; and consequences,Benjamin Recht; Christopher Ré,Abstract Randomized algorithms that base iteration-level decisions on samples from somepool are ubiquitous in machine learning and optimization. Examples include stochasticgradient descent and randomized coordinate descent. This paper makes progress attheoretically evaluating the difference in performance between sampling with-and without-replacement in such algorithms. Focusing on least means squares optimization; weformulate a noncommutative arithmetic-geometric mean inequality that would prove that theexpected convergence rate of without-replacement sampling is faster than that of with-replacement sampling. We demonstrate that this inequality holds for many classes ofrandom matrices and for some pathological examples as well. We provide a deterministicworst-case bound on the gap between the discrepancy between the two sampling models …,Conference on Learning Theory,2012,36
Felix: Scaling inference for markov logic with an operator-based approach,Feng Niu; Ce Zhang; Christopher Ré; Jude Shavlik,Abstract We examine how to scale up text-processing applications that are expressed in alanguage; Markov Logic; that allows one to express both logical and statistical rules. Ouridea is to exploit the observation that to build text-processing applications one must solve ahost of common subtasks; eg; named-entity extraction; relationship discovery; coreferenceresolution. For some subtasks; there are specialized algorithms that achieve both highquality and high performance. But current general-purpose statistical inference approachesare oblivious to these subtasks and so use a single algorithm independent of the subtasksthat they are performing. The result is that general purpose approaches have either lowerquality; performance; or both compared to the specialized approaches. To combat this; wepresent Felix. In Felix programs are expressed in Markov Logic but are executed using a …,ArXiv e-prints,2011,32
GYM: A multiround distributed join algorithm,Foto N Afrati; Manas R Joglekar; Christopher M Re; Semih Salihoglu; Jeffrey D Ullman,Abstract Multiround algorithms are now commonly used in distributed data processingsystems; yet the extent to which algorithms can benefit from running more rounds is not wellunderstood. This paper answers this question for several rounds for the problem ofcomputing the equijoin of n relations. Given any query Q with width w; intersection width iw;input size IN; output size OUT; and a cluster of machines with M=\Omega (IN\frac{1}{\epsilon}) memory available per machine; where\epsilon> 1 and w\ge 1 are constants;we show that: 1. Q can be computed in O (n) rounds with O (n (INw+ OUT) 2/M)communication cost with high probability. Q can be computed in O (log (n)) rounds with O (n(INmax (w; 3iw)+ OUT) 2/M) communication cost with high probability. Intersection width is anew notion we introduce for queries and generalized hypertree decompositions (GHDs) …,LIPIcs-Leibniz International Proceedings in Informatics,2017,29
Big data versus the crowd: Looking for relationships in all the right places,Ce Zhang; Feng Niu; Christopher Ré; Jude Shavlik,Abstract Classically; training relation extractors relies on high-quality; manually annotatedtraining data; which can be expensive to obtain. To mitigate this cost; NLU researchers haveconsidered two newly available sources of less expensive (but potentially lower quality)labeled data from distant supervision and crowd sourcing. There is; however; no studycomparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap; we empirically study how state-of-the-art techniques areaffected by scaling these two sources. We use corpus sizes of up to 100 million documentsand tens of thousands of crowd-source labeled examples. Our experiments show thatincreasing the corpus size for distant supervision has a statistically significant; positiveimpact on quality (F1 score). In contrast; human feedback has a positive and statistically …,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,2012,28
Large-scale extraction of gene interactions from full-text literature using DeepDive,Emily K Mallory; Ce Zhang; Christopher Ré; Russ B Altman,Abstract Motivation: A complete repository of gene–gene interactions is key forunderstanding cellular processes; human disease and drug response. These gene–geneinteractions include both protein–protein interactions and transcription factor interactions.The majority of known interactions are found in the biomedical literature. Interactiondatabases; such as BioGRID and ChEA; annotate these gene–gene interactions; however;curation becomes difficult as the literature grows exponentially. DeepDive is a trainedsystem for extracting information from a variety of sources; including text. In this work; weused DeepDive to extract both protein–protein and transcription factor interactions from over100 000 full-text PLOS articles. Methods: We built an extractor for gene–gene interactionsthat identified candidate gene–gene relations within an input sentence. For each …,Bioinformatics,2015,27
Feature engineering for knowledge base construction,Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang,Abstract: Knowledge base construction (KBC) is the process of populating a knowledgebase; ie; a relational database together with inference rules; with information extracted fromdocuments and structured sources. KBC blurs the distinction between two traditionaldatabase problems; information extraction and information integration. For the last severalyears; our group has been building knowledge bases with scientific collaborators. Using ourapproach; we have built knowledge bases that have comparable and sometimes betterquality than those constructed by human volunteers. In contrast to these knowledge bases;which took experts a decade or more human years to construct; many of our projects areconstructed by a single graduate student. Our approach to KBC is based on jointprobabilistic inference and learning; but we do not see inference as either a panacea or a …,arXiv preprint arXiv:1407.6439,2014,26
Managing probabilistic data with mystiq: The can-do; the could-do; and the can’t-do,Christopher Re; Dan Suciu,Abstract MystiQ is a system that allows users to define a probabilistic database; then toevaluate SQL queries over this database. MystiQ is a middleware: the data itself is stored ina standard relational database system; and MystiQ is providing the probabilistic semantics.The advantage of a middleware over a re-implementation from scratch is that it can leveragethe infrastructure of an existing database engine; eg indexes; query evaluation; queryoptimization; etc. Furthermore; MystiQ attempts to perform most of the probabilistic inferenceinside the relational database engine. MystiQ is currently available from mystiq. cs.washington. edu.,International Conference on Scalable Uncertainty Management,2008,26
Supporting workflow in a course management system,Chavdar Botev; Hubert Chao; Theodore Chao; Yim Cheng; Raymond Doyle; Sergey Grankin; Jon Guarino; Saikat Guha; Pei-Chen Lee; Dan Perry; Christopher Re; Ilya Rifkin; Tingyan Yuan; Dora Abdullah; Kathy Carpenter; David Gries; Dexter Kozen; Andrew Myers; David Schwartz; Jayavel Shanmugasundaram,Abstract CMS is a secure and scalable web-based course management system developedby the Cornell University Computer Science Department. The system was designed tosimplify; streamline; and automate many aspects of the workflow associated with running alarge course; such as course creation; importing students; management of studentworkgroups; online submission of assignments; assignment of graders; grading; handlingregrade requests; and preparation of final grades. In contrast; other course managementsystems of which we are aware provide only specialized solutions for specific components;such as grading. CMS is increasingly widely used for course management at CornellUniversity. In this paper we articulate the principles we followed in designing the system anddescribe the features that users found most useful.,ACM SIGCSE Bulletin,2005,26
Sub-sampled newton methods with non-uniform sampling,Peng Xu; Jiyan Yang; Farbod Roosta-Khorasani; Christopher Ré; Michael W Mahoney,Abstract We consider the problem of finding the minimizer of a convex function $ F:\mathbbR^ d\rightarrow\mathbb R $ of the form $ F (w)\defeq\sum_ {i= 1}^ n f_i (w)+ R (w) $ where alow-rank factorization of $\nabla^ 2 f_i (w) $ is readily available. We consider the regimewhere $ n\gg d $. We propose randomized Newton-type algorithms that exploit\textit {non-uniform} sub-sampling of $\{\nabla^ 2 f_i (w)\} _ {i= 1}^{n} $; as well as inexact updates; asmeans to reduce the computational complexity; and are applicable to a wide range ofproblems in machine learning. Two non-uniform sampling distributions based on {\it blocknorm squares} and {\it block partial leverage scores} are considered. Under certainassumptions; we show that our algorithms inherit a linear-quadratic convergence rate in $ w$ and achieve a lower computational complexity compared to similar existing methods. In …,Advances in Neural Information Processing Systems,2016,25
Stanford’s 2014 slot filling systems,Gabor Angeli; Sonal Gupta; Melvin Jose; Christopher D Manning; Christopher Ré; Julie Tibshirani; Jean Y Wu; Sen Wu; Ce Zhang,Abstract We describe Stanford's entry in the TACKBP 2014 Slot Filling challenge. Wesubmitted two broad approaches to Slot Filling: one based on the DeepDive framework (Niuet al.; 2012); and another based on the multi-instance multi-label relation extractor ofSurdeanu et al.(2012). In addition; we evaluate the impact of learned and hard-codedpatterns on performance for slot filling; and the impact of the partial annotations described inAngeli et al.(2014).,TAC KBP,2014,24
Asynchrony begets momentum; with an application to deep learning,Ioannis Mitliagkas; Ce Zhang; Stefan Hadjis; Christopher Ré,Asynchronous methods are widely used in deep learning; but have limited theoreticaljustification when applied to non-convex problems. We show that running stochasticgradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function;so it is applicable to deep learning systems. We observe that a standard queuing model ofasynchrony results in a form of momentum that is commonly used by deep learningpractitioners. This forges a link between queuing theory and asynchrony in deep learningsystems; which could be useful for systems builders. For convolutional neural networks; weexperimentally validate that the degree of asynchrony directly correlates with themomentum; confirming our main result. An important implication is that tuning the …,Communication; Control; and Computing (Allerton); 2016 54th Annual Allerton Conference on,2016,23
Incrementally maintaining classification using an RDBMS,M Levent Koc; Christopher Ré,Abstract The proliferation of imprecise data has motivated both researchers and thedatabase industry to push statistical techniques into relational database managementsystems (RDBMSes). We study strategies to maintain model-based views for a popularstatistical technique; classification; inside an RDBMS in the presence of updates (to the setof training examples). We make three technical contributions:(1) A strategy thatincrementally maintains classification inside an RDBMS.(2) An analysis of the abovealgorithm that shows that our algorithm is optimal among all deterministic algorithms (andasymptotically within a factor of 2 of a non-deterministic optimal strategy).(3) A novel hybrid-architecture based on the technical ideas that underlie the above algorithm which allows usto store only a fraction of the entities in memory. We apply our techniques to text …,Proceedings of the VLDB Endowment,2011,22
Joins via Geometric Resolutions: Worst Case and Beyond,Mahmoud Abo Khamis; Hung Q Ngo; Christopher Ré; Atri Rudra,Abstract We present a simple geometric framework for the relational join. Using thisframework; we design an algorithm that achieves the fractional hypertree-width bound;which generalizes classical and recent worst-case algorithmic results on computing joins. Inaddition; we use our framework and the same algorithm to show a series of what arecolloquially known as beyond worst-case results. The framework allows us to prove resultsfor data stored in BTrees; multidimensional data structures; and even multiple indices pertable. A key idea in our framework is formalizing the inference one does with an index as atype of geometric resolution; transforming the algorithmic problem of computing joins to ageometric problem. Our notion of geometric resolution can be viewed as a geometric analogof logical resolution. In addition to the geometry and logic connections; our algorithm can …,ACM Transactions on Database Systems (TODS),2016,21
Parallel feature selection inspired by group testing,Yingbo Zhou; Utkarsh Porwal; Ce Zhang; Hung Q Ngo; XuanLong Nguyen; Christopher Ré; Venu Govindaraju,Abstract This paper presents a parallel feature selection method for classification that scalesup to very high dimensions and large data sizes. Our original method is inspired by grouptesting theory; under which the feature selection procedure consists of a collection ofrandomized tests to be performed in parallel. Each test corresponds to a subset of features;for which a scoring function may be applied to measure the relevance of the features in aclassification task. We develop a general theory providing sufficient conditions under whichtrue features are guaranteed to be correctly identified. Superior performance of our methodis demonstrated on a challenging relation extraction task from a very large data set that haveboth redundant features and sample size in the order of millions. We present comprehensivecomparisons with state-of-the-art feature selection methods on a range of data sets; for …,Advances in Neural Information Processing Systems,2014,21
Management of data with uncertainties,Christopher Re; Dan Suciu,Since their invention in the early 70s; relational databases have been deterministic. Theywere designed to support applications sa accounting; inventory; customer care; andmanufacturing; and these applications require a precise semantics. Thus; database systemsare deterministic. A row is either in the database or is not; a tuple is either in the queryanswer or is not. The foundations of query processing and the tools that exists today formanaging data rely fundamentally on the assumption that the data is deterministic.Increasingly; today we need to manage data that is uncertain. The uncertainty can be in thedata itself; in the schema; in the mapping between different data instances; or in the userquery. We find increasingly large amounts of uncertain data in a variety of domains: in dataintegration; in scientific data; in information extracted automatically from text; in data from …,Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,2007,21
Understanding tables in context using standard NLP toolkits,Vidhya Govindaraju; Ce Zhang; Christopher Ré,Abstract Tabular information in text documents contains a wealth of information; and sotables are a natural candidate for information extraction. There are many cues buried in botha table and its surrounding text that allow us to understand the meaning of the data in atable. We study how natural-language tools; such as part-of-speech tagging; dependencypaths; and named-entity recognition; can be used to improve the quality of relation extractionfrom tables. In three domains we show that (1) a model that performs joint probabilisticinference across tabular and natural language features achieves an F1 score that is twice ashigh as either a puretable or pure-text system; and (2) using only shallower features or non-joint inference results in lower quality.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),2013,20
A framework for XML-based integration of data; visualization and analysis in a biomedical domain,Nathan Bales; James Brinkley; E Sally Lee; Shobhit Mathur; Christopher Re; Dan Suciu,Abstract Biomedical data are becoming increasingly complex and heterogeneous in nature.The data are stored in distributed information systems; using a variety of data models; andare processed by increasingly more complex tools that analyze and visualize them. Wepresent in this paper our framework for integrating biomedical research data and tools into aunique Web front end. Our framework is applied to the University of Washington's HumanBrain Project. Specifically; we present solutions to four integration tasks: definition ofcomplex mappings from relational sources to XML; distributed XQuery processing;generation of heterogeneous output formats; and the integration of heterogeneous datavisualization and analysis tools.,International XML Database Symposium,2005,20
Caffe con troll: Shallow ideas to speed up deep learning,Stefan Hadjis; Firas Abuzaid; Ce Zhang; Christopher Ré,Abstract We present Caffe con Troll (CcT); a fully compatible end-to-end version of thepopular framework Caffe with rebuilt internals. We built CcT to examine the performancecharacteristics of training and deploying general-purpose convolutional neural networksacross different hardware architectures. We find that; by employing standard batchingoptimizations for CPU training; we achieve a 6: 3× throughput improvement over Caffe onpopular networks like CaffeNet. Moreover; with these improvements; the end-to-end trainingtime for CNNs is directly proportional to the FLOPS delivered by the CPU; which enables usto efficiently train hybrid CPU-GPU systems for CNNs.,Proceedings of the Fourth Workshop on Data analytics in the Cloud,2015,19
Declarative framework for deduplication,*,A system; framework; and algorithms for data deduplication are described. A declarativelanguage; such as a Datalog-type logic language; is provided. Programs in the languagedescribe data to be deduplicated and soft and hard constraints that must/should be satisfiedby data deduplicated according to the program. To execute the programs; algorithms forperforming graph clustering are described.,*,2012,19
Efficient top-K query evaluation on probabilistic data,*,A novel approach that computes and efficiently ranks the top-k answers to a query on aprobabilistic database. The approach identifies the top-k answers; since imprecisions in thedata often lead to a large number of answers of low quality. The algorithm is used to runseveral Monte Carlo simulations in parallel; one for each candidate answer; andapproximates the probability of each only to the extent needed to correctly determine the top-k answers. The algorithm is provably optimal and scales to large databases. A more generalapplication can identify a number of top-rated entities of a group that satisfy a condition;based on a criteria or score computed for the entities. Also disclosed are severaloptimization techniques. One option is to rank the top-rated results; another option providesfor interrupting the iteration to return the number of top-rated entities that have thus far …,*,2010,19
Building a large-scale multimodal knowledge base system for answering visual queries,Yuke Zhu; Ce Zhang; Christopher Ré; Li Fei-Fei,Abstract: The complexity of the visual world creates significant challenges forcomprehensive visual understanding. In spite of recent successes in visual recognition;today's vision systems would still struggle to deal with visual queries that require a deeperreasoning. We propose a knowledge base (KB) framework to handle an assortment of visualqueries; without the need to train new classifiers for new tasks. Building such a large-scalemultimodal KB presents a major challenge of scalability. We cast a large-scale MRF into aKB representation; incorporating visual; textual and structured data; as well as their diverserelations. We introduce a scalable knowledge base construction system that is capable ofbuilding a KB with half billion variables and millions of parameters in a few hours. Oursystem achieves competitive results compared to purpose-built models on standard …,arXiv preprint arXiv:1507.05670,2015,18
GeoDeepDive: statistical inference using familiar data-processing languages,Ce Zhang; Vidhya Govindaraju; Jackson Borchardt; Tim Foltz; Christopher Ré; Shanan Peters,Abstract We describe our proposed demonstration of GeoDeepDive; a system that helpsgeoscientists discover information and knowledge buried in the text; tables; and figures ofgeology journal articles. This requires solving a host of classical data managementchallenges including data acquisition (eg; from scanned documents); data extraction; anddata integration. SIGMOD attendees will see demonstrations of three aspects of oursystem:(1) an end-to-end system that is of a high enough quality to perform novel geologicalscience; but is written by a small enough team so that each aspect can be manageablyexplained;(2) a simple feature engineering system that allows a user to write in familiar SQLor Python; and (3) the effect of different sources of feedback on result quality including expertlabeling; distant supervision; traditional rules; and crowd-sourced data. Our prototype …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,18
Transducing Markov sequences,Benny Kimelfeld; Christopher Re,Abstract A Markov sequence is a basic statistical model representing uncertain sequentialdata; and it is used within a plethora of applications; including speech recognition; imageprocessing; computational biology; radio-frequency identification (RFID); and informationextraction. The problem of querying a Markov sequence is studied under the conventionalsemantics of querying a probabilistic database; where queries are formulated as finite-statetransducers. Specifically; the complexity of two main problems is analyzed. The first problemis that of computing the confidence (probability) of an answer. The second is theenumeration of the answers in the order of decreasing confidence (with the generation of thetop-k answers as a special case); or in an approximate order thereof. In particular; it isshown that enumeration in any subexponential-approximate order is generally intractable …,Journal of the ACM (JACM),2014,17
Understanding cardinality estimation using entropy maximization,Christopher Ré; Dan Suciu,Abstract Cardinality estimation is the problem of estimating the number of tuples returned bya query; it is a fundamentally important task in data management; used in queryoptimization; progress estimation; and resource provisioning. We study cardinalityestimation in a principled framework: given a set of statistical assertions about the number oftuples returned by a fixed set of queries; predict the number of tuples returned by a newquery. We model this problem using the probability space; over possible worlds; thatsatisfies all provided statistical assertions and maximizes entropy. We call this the EntropyMaximization model for statistics (MaxEnt). In this article we develop the mathematicaltechniques needed to use the MaxEnt model for predicting the cardinality of conjunctivequeries.,ACM Transactions on Database Systems (TODS),2012,17
Ensuring rapid mixing and low bias for asynchronous Gibbs sampling,Christopher De Sa; Kunle Olukotun; Christopher Ré,Abstract Gibbs sampling is a Markov chain Monte Carlo technique commonly used forestimating marginal distributions. To speed up Gibbs sampling; there has recently beeninterest in parallelizing it by executing asynchronously. While empirical results suggest thatmany models can be efficiently sampled asynchronously; traditional Markov chain analysisdoes not apply to the asynchronous case; and thus asynchronous Gibbs sampling is poorlyunderstood. In this paper; we derive a better understanding of the two main challenges ofasynchronous Gibbs: bias and mixing time. We show experimentally that our theoreticalresults match practical outcomes.,JMLR workshop and conference proceedings,2016,16
An approximate; efficient LP solver for LP rounding,Srikrishna Sridhar; Stephen Wright; Christopher Re; Ji Liu; Victor Bittorf; Ce Zhang,Abstract Many problems in machine learning can be solved by rounding the solution of anappropriate linear program. We propose a scheme that is based on a quadratic programrelaxation which allows us to use parallel stochastic-coordinate-descent to approximatelysolve large linear programs efficiently. Our software is an order of magnitude faster thanCplex (a commercial linear programming solver) and yields similar solution quality. Ourresults include a novel perturbation analysis of a quadratic-penalty formulation of linearprogramming and a convergence result; which we use to derive running time and qualityguarantees.,Advances in Neural Information Processing Systems,2013,16
Repeatability & workability evaluation of SIGMOD 2009,Stefan Manegold; Ioana Manolescu; Loredana Afanasiev; Jianlin Feng; Gang Gou; Marios Hadjieleftheriou; Stavros Harizopoulos; Panos Kalnis; Konstantinos Karanasos; Dominique Laurent; Mihai Lupu; Nicola Onose; Christopher Ré; Virginie Sans; Pierre Senellart; Tianyi Wu; Dennis Shasha,Abstract SIGMOD 2008 was the first database conference that offered to test submitters'programs against their data to verify the repeatability of the experiments published [1]. Giventhe positive feedback concerning the SIGMOD 2008 repeatability initiative; SIGMOD 2009modified and expanded the initiative with a workability assessment.,ACM SIGMOD Record,2010,16
Approximation trade-offs in markovian stream processing: An empirical study,Julie Letchner; Christopher Ré; Magdalena Balazinska; Matthai Philipose,A large amount of the world's data is both sequential and imprecise. Such data is commonlymodeled as Markovian streams; examples include words/sentences inferred from raw audiosignals; or discrete location sequences inferred from RFID or GPS data. The rich semanticsand large volumes of these streams make them difficult to query efficiently. In this paper; westudy the effects-on both efficiency and accuracy-of two common stream approximations.Through experiments on a realworld RFID data set; we identify conditions under which theseapproximations can improve performance by several orders of magnitude; with only minimaleffects on query results. We also identify cases when the full rich semantics are necessary.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,16
Cyclades: Conflict-free asynchronous machine learning,Xinghao Pan; Maximilian Lam; Stephen Tu; Dimitris Papailiopoulos; Ce Zhang; Michael I Jordan; Kannan Ramchandran; Christopher Ré,Abstract We present Cyclades; a general framework for parallelizing stochastic optimizationalgorithms in a shared memory setting. Cyclades is asynchronous during model updates;and requires no memory locking mechanisms; similar to Hogwild!-type algorithms. UnlikeHogwild!; Cyclades introduces no conflicts during parallel execution; and offers a black-boxanalysis for provable speedups across a large family of algorithms. Due to its inherent cachelocality and conflict-free nature; our multi-core implementation of Cyclades consistentlyoutperforms Hogwild!-type algorithms on sufficiently sparse datasets; leading to up to 40%speedup gains compared to Hogwild!; and up to 5\times gains over asynchronousimplementations of variance reduction algorithms.,Advances in Neural Information Processing Systems,2016,15
Beyond worst-case analysis for joins with minesweeper,Hung Q Ngo; Dung T Nguyen; Christopher Re; Atri Rudra,Abstract We describe a new algorithm; Minesweeper; that is able to satisfy stronger runtimeguarantees than previous join algorithms (colloquially``beyond worst-case''guarantees) fordata in indexed search trees. Our first contribution is developing a framework to measurethis stronger notion of complexity; which we call" certificate complexity;" that extends notionsof Barbay et al. and Demaine et al.; a certificate is a set of propositional formulae thatcertifies that the output is correct. This notion captures a natural class of join algorithms. Inaddition; the certificate allows us to define a strictly stronger notion of runtime complexitythan traditional worst-case guarantees. Our second contribution is to develop a dichotomytheorem for the certificate-based notion of complexity. Roughly; we show that Minesweeperevaluates $\beta $-acyclic queries in time linear in the certificate plus the output size …,Proceedings of the 33rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2014,15
It’s All a Matter of Degree,Manas Joglekar; Christopher Ré,Abstract We optimize multiway equijoins on relational tables using degree information. Wegive a new bound that uses degree information to more tightly bound the maximum outputsize of a query. On real data; our bound on the number of triangles in a social network canbe up to 95 times tighter than existing worst case bounds. We show that using only aconstant amount of degree information; we are able to obtain join algorithms with a runningtime that has a smaller exponent than existing algorithms–for any database instance. Wealso show that this degree information can be obtained in nearly linear time; which yieldsasymptotically faster algorithms in the serial setting and lower communication costalgorithms in the MapReduce setting. In the serial setting; the data complexity of joinprocessing can be expressed as a function O (IN x+ OUT) in terms of input size IN and …,Theory of Computing Systems,2017,14
The business of the state,Mari Sako,OUTSOURCING IS BIG busi-ness; demand for which is generated in no small part bygovernments. In recent years; public sector outsourcing has outstripped private sectoroutsourcing; with public sector organizations around the world spending $10.3 billion in thethird quarter of 2013 on IT outsourcing and business process outsourcing; compared with$6.4 billion in the private sector. The top three spending governments are in the US; Britain;and Australia. 2 Outsourcing creates business opportunities for commercial firms in a widerange of sectors including IT; defense; security; detention and prison service; healthcare;transport; and shared services. But public-to-private transactions in government outsourcingare rife with reputational risks for corporations. We consider the following. What hasmotivated governments to outsource their operations and services? What is the logical …,Communications of the ACM,2014,14
Feature selection in enterprise analytics: a demonstration using an R-based data analytics system,Pradap Konda; Arun Kumar; Christopher Ré; Vaishnavi Sashikanth,Abstract Enterprise applications are analyzing ever larger amounts of data using advancedanalytics techniques. Recent systems from Oracle; IBM; and SAP integrate R with a dataprocessing system to support richer advanced analytics on large data. A key step inadvanced analytics applications is feature selection; which is often an iterative process thatinvolves statistical algorithms and data manipulations. From our conversations with datascientists and analysts at enterprise settings; we observe three key aspects about featureselection. First; feature selection is performed by many types of users; not just data scientists.Second; high performance is critical to perform feature selection processes on large data.Third; the provenance of the results and steps in feature selection processes needs to betracked for purposes of transparency and auditability. Based on our discussions with data …,Proceedings of the VLDB Endowment,2013,14
Ringtail: Feature Selection For Easier Nowcasting.,Dolan Antenucci; Michael J Cafarella; Margaret Levenstein; Christopher Ré; Matthew Shapiro,ABSTRACT In recent years; social media “nowcasting”—the use of online user activity topredict various ongoing real-world social phenomena—has become a popular researchtopic; yet; this popularity has not led to widespread actual practice. We believe a majorobstacle to widespread adoption is the feature selection problem. Typical nowcastingsystems require the user to choose a set of relevant social media objects; which is difficult;time-consuming; and can imply a statistical background that users may not have. Wepropose Ringtail; which helps the user choose relevant social media signals. It takes asingle user input string (eg; unemployment) and yields a number of relevant signals the usercan use to build a nowcasting model. We evaluate Ringtail on six different topics using acorpus of almost 6 billion tweets; showing that features chosen by Ringtail in a wholly …,WebDB,2013,14
Omnivore: An optimizer for multi-device deep learning on cpus and gpus,Stefan Hadjis; Ce Zhang; Ioannis Mitliagkas; Dan Iter; Christopher Ré,Abstract: We study the factors affecting training time in multi-device deep learning systems.Given a specification of a convolutional neural network; our goal is to minimize the time totrain this model on a cluster of commodity CPUs and GPUs. We first focus on the single-node setting and show that by using standard batching and data-parallel techniques;throughput can be improved by at least 5.5 x over state-of-the-art systems on CPUs. Thisensures an end-to-end training speed directly proportional to the throughput of a deviceregardless of its underlying hardware; allowing each node in the cluster to be treated as ablack box. Our second contribution is a theoretical and empirical study of the tradeoffsaffecting end-to-end training time in a multiple-device setting. We identify the degree ofasynchronous parallelization as a key factor affecting both hardware and statistical …,arXiv preprint arXiv:1606.04487,2016,13
Join processing for graph patterns: An old dog with new tricks,Dung Nguyen; Molham Aref; Martin Bravenboer; George Kollias; Hung Q Ngo; Christopher Ré; Atri Rudra,Abstract Join optimization has been dominated by Selinger-style; pairwise optimizers fordecades. But; Selinger-style algorithms are asymptotically suboptimal for applications ingraphic analytics. This sub-optimality is one of the reasons that many have advocatedsupplementing relational engines with specialized graph processing engines. Recently; newjoin algorithms have been discovered that achieve optimal worst-case run times for any joinor even so-called beyond worst-case (or instance optimal) run time guarantees forspecialized classes of joins. These new algorithms match or improve on those used inspecialized graph-processing systems. This paper asks can these new join algorithms allowrelational engines to close the performance gap with graph engines? We examine thisquestion for graph-pattern queries or join queries. We find that classical relational …,Proceedings of the GRADES'15,2015,13
Ws-membership-failure management in a web-services world,Werner Vogels; Chris Re,Abstract An important factor in the successful deployment of federated web-services-basedbusiness activities will be the ability to guarantee reliable distributed operation andexecution. Failure management is essential for any reliable distributed operation butespecially for the target areas of web-services; where the activities can be constructed out ofservices located at different enterprises; and are accessed over heterogeneous networkstopologies. This paper describes ws-membership; a coordination service that provides ageneric web-service interface for tracking registered web-services and for providingmembership monitoring information. A prototype membership service based on epidemicprotocol techniques has been implemented and is described in detail in this paper. Thespecification and implementation have been developed in the context of the Huygens …,In Intl. World Wide Web Conference (WWW,2003,13
General database statistics using entropy maximization,Raghav Kaushik; Christopher Ré; Dan Suciu,Abstract We propose a framework in which query sizes can be estimated from arbitrarystatistical assertions on the data. In its most general form; a statistical assertion states thatthe size of the output of a conjunctive query over the data is a given number. A very simpleexample is a histogram; which makes assertions about the sizes of the output of severalrange queries. Our model also allows much more complex assertions that include joins andprojections. To model such complex statistical assertions we propose to use the Entropy-Maximization (EM) probability distribution. In this model any set of statistics that is consistenthas a precise semantics; and every query has an precise size estimate. We show thatseveral classes of statistics can be solved in closed form.,International Symposium on Database Programming Languages,2009,12
Data programming with DDLite: Putting humans in a different part of the loop,Henry R Ehrenberg; Jaeho Shin; Alexander J Ratner; Jason A Fries; Christopher Ré,Abstract Populating large-scale structured databases from unstructured sources is a criticaland challenging task in data analytics. As automated feature engineering methods growincreasingly prevalent; constructing sufficiently large labeled training sets has become theprimary hurdle in building machine learning information extraction systems. In light of this;we have taken a new approach called data programming [7]. Rather than hand-labelingdata; in the data programming paradigm; users generate large amounts of noisy traininglabels by programmatically encoding domain heuristics as simple rules. Using this approachover more traditional distant supervision methods and fully supervised approaches usinglabeled data; we have been able to construct knowledge base systems more rapidly andwith higher quality. Since the ability to quickly prototype; evaluate; and debug these rules …,Proceedings of the Workshop on Human-In-the-Loop Data Analytics,2016,11
Challenges for event queries over markovian streams,Julie Letchner; Christopher Ré; Magdalena Balazinska; Matthai Philipose,Building applications on top of sensor data streams is challenging because sensor data isnoisy. A model-based view can reduce noise by transforming raw sensor streams intostreams of probabilistic state estimates; which smooth out errors and gaps. The authorspropose a novel model-based view; the Markovian stream; to represent correlatedprobabilistic sequences. Applications interested in evaluating event queries-extractingsophisticated state sequences-can improve robustness by querying a Markovian streamview instead of querying raw data directly. The primary challenge is to properly handle theMarkovian stream's correlations.,IEEE Internet Computing,2008,11
A collaborative infrastructure for scalable and robust news delivery,Werner Vogels; Christopher Re; Robbert Van Renesse; Ken Birman,In this paper we describe the model used for the NewsWire collaborative content deliverysystem. The system builds on the robustness and scalability of Astrolabe to weave a peer-to-peer infrastructure for real time delivery of news items. The goal of the system is to delivernews updates to hundreds of thousands of subscribers within tens of seconds of the momentof publishing. The system significantly reduces the compute and network load at thepublishers and guarantees delivery even in the face of publisher overload or denial ofservice attacks.,Distributed Computing Systems Workshops; 2002. Proceedings. 22nd International Conference on,2002,11
Holoclean: Holistic data repairs with probabilistic inference,Theodoros Rekatsinas; Xu Chu; Ihab F Ilyas; Christopher Ré,Abstract We introduce HoloClean; a framework for holistic data repairing driven byprobabilistic inference. HoloClean unifies qualitative data repairing; which relies on integrityconstraints or external data sources; with quantitative data repairing methods; whichleverage statistical properties of the input data. Given an inconsistent dataset as input;HoloClean automatically generates a probabilistic program that performs data repairing.Inspired by recent theoretical advances in probabilistic inference; we introduce a series ofoptimizations which ensure that inference over HoloClean's probabilistic model scales toinstances with millions of tuples. We show that HoloClean finds data repairs with an averageprecision of∼ 90% and an average recall of above∼ 76% across a diverse array of datasetsexhibiting different types of errors. This yields an average F1 improvement of more than 2 …,Proceedings of the VLDB Endowment,2017,10
Understanding and optimizing asynchronous low-precision stochastic gradient descent,Christopher De Sa; Matthew Feldman; Christopher Ré; Kunle Olukotun,Abstract Stochastic gradient descent (SGD) is one of the most popular numerical algorithmsused in machine learning and other domains. Since this is likely to continue for theforeseeable future; it is important to study techniques that can make it run fast on parallelhardware. In this paper; we provide the first analysis of a technique called Buck-wild! thatuses both asynchronous execution and low-precision computation. We introduce the DMGCmodel; the first conceptualization of the parameter space that exists when implementing low-precision SGD; and show that it provides a way to both classify these algorithms and modeltheir performance. We leverage this insight to propose and analyze techniques to improvethe speed of low-precision SGD. First; we propose software optimizations that can increasethroughput on existing CPUs by up to 11X. Second; we propose architectural changes …,Proceedings of the 44th Annual International Symposium on Computer Architecture,2017,10
Duncecap: Query plans using generalized hypertree decompositions,Susan Tu; Christopher Ré,Abstract Joins are central to data processing. However; traditional query plans for joins;which are based on choosing the order of pairwise joins; are provably suboptimal. Theyoften perform poorly on cyclic graph queries; which have become increasingly important tomodern data analytics. Other join algorithms exist: Yannakakis'; for example; operates onacyclic queries in runtime proportional to the input size plus the output size\cite{yannakakis}. More recently; Ngo et al. published a join algorithm that is optimal on worst-case inputs\cite {worst}. My contribution is to explore query planning using these joinalgorithms. In our approach; every query plan can be viewed as a generalized hypertreedecomposition (GHD). We score each GHD using the minimal fractional hypertree width;which Ngo et al. show allows us to bound its worst-case runtime. We benchmark our …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,10
Ajar: Aggregations and joins over annotated relations,Manas R Joglekar; Rohan Puttagunta; Christopher Ré,Abstract We study a class of aggregate-join queries with multiple aggregation operatorsevaluated over annotated relations. We show that straightforward extensions of standardmultiway join algorithms and generalized hypertree decompositions (GHDs) provide best-known runtime guarantees. In contrast; prior work uses bespoke algorithms and datastructures and does not match these guarantees. We extend the standard techniques byproviding a complete characterization of (1) the set of orderings equivalent to a givenordering and (2) the set of GHDs valid with respect to the given ordering; ie; GHDs thatcorrectly answer a given aggregate-join query when provided to (simple variants of)standard join algorithms. We show by example that previous approaches are incomplete.The key technical consequence of our characterizations is a decomposition of a valid …,Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2016,9
The mobilize center: an NIH big data to knowledge center to advance human movement research and improve mobility,Joy P Ku; Jennifer L Hicks; Trevor Hastie; Jure Leskovec; Christopher Ré; Scott L Delp,Abstract Regular physical activity helps prevent heart disease; stroke; diabetes; and otherchronic diseases; yet a broad range of conditions impair mobility at great personal andsocietal cost. Vast amounts of data characterizing human movement are available fromresearch labs; clinics; and millions of smartphones and wearable sensors; but integrationand analysis of this large quantity of mobility data are extremely challenging. The authorshave established the Mobilize Center (http://mobilize. stanford. edu) to harness these data toimprove human mobility and help lay the foundation for using data science methods inbiomedicine. The Center is organized around 4 data science research cores: biomechanicalmodeling; statistical learning; behavioral and social modeling; and integrative modeling.Important biomedical applications; such as osteoarthritis and weight management; will …,Journal of the American Medical Informatics Association,2015,9
Rapidly mixing gibbs sampling for a class of factor graphs using hierarchy width,Christopher M De Sa; Ce Zhang; Kunle Olukotun; Christopher Ré,Abstract Gibbs sampling on factor graphs is a widely used inference technique; which oftenproduces good empirical results. Theoretical guarantees for its performance are weak: evenfor tree structured graphs; the mixing time of Gibbs may be exponential in the number ofvariables. To help understand the behavior of Gibbs sampling; we introduce a new (hyper)graph property; called hierarchy width. We show that under suitable conditions on theweights; bounded hierarchy width ensures polynomial mixing time. Our study of hierarchywidth is in part motivated by a class of factor graph templates; hierarchical templates; whichhave bounded hierarchy width—regardless of the data used to instantiate them. Wedemonstrate a rich application from natural language processing in which Gibbs samplingprovably mixes rapidly and achieves accuracy that exceeds human volunteers.,Advances in neural information processing systems,2015,9
Stanford’s distantly supervised slot filling systems for KBP 2014,Gabor Angeli; Sonal Gupta; Melvin Johnson Premkumar; Christopher D Manning; Christopher Ré; Julie Tibshirani; Jean Y Wu; Sen Wu; Ce Zhang,Abstract We describe Stanford's entry in the TACKBP 2014 Slot Filling challenge. Wesubmitted two broad approaches to Slot Filling; both strongly based on the ideas of distantsupervision: one built on the Deep-Dive framework (Niu et al.; 2012); and another based onthe multi-instance multilabel relation extractor of Surdeanu et al.(2012). In addition; weevaluate the impact of learned and hard-coded patterns on performance for slot filling; andthe impact of the partial annotations described in Angeli et al.(2014).,Proceedings of the Seventh Text Analysis Conference,2014,9
Probabilistic management of OCR data using an RDBMS,Arun Kumar; Christopher Ré,Abstract The digitization of scanned forms and documents is changing the data sources thatenterprises manage. To integrate these new data sources with enterprise data; the currentstate-of-the-art approach is to convert the images to ASCII text using optical characterrecognition (OCR) software and then to store the resulting ASCII text in a relationaldatabase. The OCR problem is challenging; and so the output of OCR often contains errors.In turn; queries on the output of OCR may fail to retrieve relevant answers. State-of-the-artOCR programs; eg; the OCR powering Google Books; use a probabilistic model thatcaptures many alternatives during the OCR process. Only when the results of OCR arestored in the database; do these approaches discard the uncertainty. In this work; wepropose to retain the probabilistic models produced by OCR process in a relational …,Proceedings of the VLDB Endowment,2011,9
Implementing NOT EXISTS Predicates over a Probabilistic Database.,Ting-You Wang; Christopher Re; Dan Suciu,(%! & $/Systems for managing uncertain data need to support) ueries with negatedsubgoals; which are typically expressed in SBL through the NOT EXISTS predicate. Forexample; the user of an RFID tracking sys^ eled from a point A to a point C without goingthrough a point D. Such but only decrease its probability. In this paper; we present anapproach for supporting) ueries with NOT EXISTS in a probabilistic database managementsystem; by leveraging the existing) uery processing infras^ tructure. Our approach is to breakup the) uery into multiple; monotone) ueries; which can be evaluated in the current system;then to combine their probabilities by addition and subtraction to compute that of theoriginal) uery. We will also describe how this techni) ue was integrated with MystiB; and howwe incorporated the top^ k multi^ simulation and safe^ plans optimi ations.,QDB/MUD,2008,9
Yellowfin and the art of momentum tuning,Jian Zhang; Ioannis Mitliagkas; Christopher Ré,Abstract: Hyperparameter tuning is one of the big costs of deep learning. State-of-the-artoptimizers; such as Adagrad; RMSProp and Adam; make things easier by adaptively tuningan individual learning rate for each variable. This level of fine adaptation is understood toyield a more powerful method. However; our experiments; as well as recent theory by Wilsonet al.; show that hand-tuned stochastic gradient descent (SGD) achieves better results; at thesame rate or faster. The hypothesis put forth is that adaptive methods converge to differentminima (Wilson et al.). Here we point out another factor: none of these methods tune theirmomentum parameter; known to be very important for deep learning applications (Sutskeveret al.). Tuning the momentum parameter becomes even more important in asynchronous-parallel systems: recent theory (Mitliagkas et al.) shows that asynchrony introduces …,arXiv preprint arXiv:1706.03471,2017,8
Extracting databases from dark data with deepdive,Ce Zhang; Jaeho Shin; Christopher Ré; Michael Cafarella; Feng Niu,Abstract DeepDive is a system for extracting relational databases from dark data: the massof text; tables; and images that are widely collected and stored but which cannot beexploited by standard relational tools. If the information in dark data---scientific papers; Webclassified ads; customer service notes; and so on---were instead in a relational database; itwould give analysts access to a massive and highly-valuable new set of" big data" to exploit.DeepDive is distinctive when compared to previous information extraction systems in itsability to obtain very high precision and recall at reasonable engineering cost; in a numberof applications; we have used DeepDive to create databases with accuracy that meets thatof human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance; materials science; genomics; paleontologists; law …,Proceedings of the 2016 International Conference on Management of Data,2016,8
Old techniques for new join algorithms: A case study in RDF processing,Christopher R Aberger; Susan Tu; Kunle Olukotun; Christopher Ré,Recently there has been significant interest around designing specialized RDF engines; astraditional query processing mechanisms incur orders of magnitude performance gaps onmany RDF workloads. At the same time researchers have released new worst-case optimaljoin algorithms which can be asymptotically better than the join algorithms in traditionalengines. In this paper we apply worst-case optimal join algorithms to a standard RDFworkload; the LUBM benchmark; for the first time. We do so using two worst-case optimalengines:(1) LogicBlox; a commercial database engine; and (2) EmptyHeaded; our prototyperesearch engine with enhanced worst-case optimal join algorithms. We show that withoutany added optimizations both LogicBlox and EmptyHeaded outperform two state-of-the-artspecialized RDF engines; RDF-3X and TripleBit; by up to 6× on cyclic join queries-the …,Data Engineering Workshops (ICDEW); 2016 IEEE 32nd International Conference on,2016,8
Weighted SGD for ℓp regression with randomized preconditioning,Jiyan Yang; Yin-Lam Chow; Christopher Ré; Michael W Mahoney,Abstract In recent years; stochastic gradient descent (SGD) methods and randomized linearalgebra (RLA) algorithms have been applied to many large-scale problems in machinelearning and data analysis. SGD methods are easy to implement and applicable to a widerange of convex optimization problems. In contrast; RLA algorithms provide much strongerworst-case performance guarantees but are applicable to a narrower class of problems. Weaim to bridge the gap between these two classes of methods in solving constrainedoverdetermined linear regression problems—eg; ℓ 2 and ℓ 1 regression problems. Wepropose a hybrid algorithm named pwSGD that uses RLA techniques for preconditioningand constructing an importance sampling distribution; and then performs an SGD-likeiterative process with weighted sampling on the preconditioned system. By rewriting the …,*,2016,8
Effectively creating weakly labeled training examples via approximate domain knowledge,Sriraam Natarajan; Jose Picado; Tushar Khot; Kristian Kersting; Christopher Re; Jude Shavlik,Abstract One of the challenges to information extraction is the requirement of humanannotated examples; commonly called gold-standard examples. Many successfulapproaches alleviate this problem by employing some form of distant supervision; ie; lookinto knowledge bases such as Freebase as a source of supervision to create moreexamples. While this is perfectly reasonable; most distant supervision methods rely on ahand-coded background knowledge that explicitly looks for patterns in text. For example;they assume all sentences containing Person X and Person Y are positive examples of therelation married (X; Y). In this work; we take a different approach–we infer weakly supervisedexamples for relations from models learned by using knowledge outside the naturallanguage task. We argue that this method creates more robust examples that are …,*,2015,8
Lahar demonstration: Warehousing markovian streams,Julie Letchner; Christopher Ré; Magdalena Balazinska; Matthai Philipose,Abstract Lahar is a warehousing system for Markovian streams---a common class ofuncertain data streams produced via inference on probabilistic models. Example Markovianstreams include text inferred from speech; location streams inferred from GPS or RFIDreadings; and human activity streams inferred from sensor data. Lahar supports OLAP-stylequeries on Markovian stream archives by leveraging novel approximation and indexingtechniques that efficiently manipulate stream probabilities.,Proceedings of the VLDB Endowment,2009,8
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Christopher Ré; Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan,Abstract Machine learning seems to be eating the world with a new breed of high-value data-driven applications in image analysis; search; voice recognition; mobile; and officeproductivity products. To paraphrase Mike Stonebraker; machine learning is no longer azero-billion-dollar business. As the home of high-value; data-driven applications for overfour decades; a natural question for database researchers to ask is: what role should thedatabase community play in these new data-driven machine-learning-based applications?,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,7
Scimark–c#,Chris Re; Werner Vogels,*,htt p://rotor. cs. cornell. edu/SciMark,2004,7
Socratic learning: Correcting misspecified generative models using discriminative models,Paroma Varma; R Yu; Dan Iter; C De Sa; C Ré,Abstract A challenge in training discriminative models like neural networks is obtainingenough labeled training data. Recent approaches have leveraged generative models todenoise weak supervision sources that a discriminative model can learn from. Thesegenerative models directly encode the users' background knowledge. Therefore; thesemodels may be incompletely specified and fail to model latent classes in the data. Wepresent Socratic learning to systematically correct such generative model misspecificationby utilizing feedback from the discriminative model. We prove that under mild conditions;Socratic learning can recover features from the discriminator that informs the generativemodel about these latent classes. Experimentally; we show that without any hand-labeleddata; the corrected generative model improves discriminative performance by up to 4.47 …,arXiv preprint arXiv:1610.08123,2017,6
Wikipedia Knowledge Graph with DeepDive.,Thomas Palomares; Youssef Ahres; Juhana Kangaspunta; Christopher Ré,Abstract Despite the tremendous amount of information on Wikipedia; only a very smallamount is structured. Most of the information is embedded in unstructured text and extractingit is a non trivial challenge. In this paper; we propose a full pipeline built on top of DeepDiveto successfully extract meaningful relations from the Wikipedia text corpus. We evaluated thesystem by extracting company-founders and family relations from the text. As a result; weextracted more than 140;000 distinct relations with an average precision above 90%.,Wiki@ ICWSM,2016,6
Mindtagger: a demonstration of data labeling in knowledge base construction,Jaeho Shin; Christopher Ré; Michael Cafarella,Abstract End-to-end knowledge base construction systems using statistical inference areenabling more people to automatically extract high-quality domain-specific information fromunstructured data. As a result of deploying DeepDive framework across several domains; wefound new challenges in debugging and improving such end-to-end systems to constructhigh-quality knowledge bases. DeepDive has an iterative development cycle in which usersimprove the data. To help our users; we needed to develop principles for analyzing thesystem's error as well as provide tooling for inspecting and labeling various data products ofthe system. We created guidelines for error analysis modeled after our colleagues' bestpractices; in which data labeling plays a critical role in every step of the analysis. To enablemore productive and systematic data labeling; we created Mindtagger; a versatile tool that …,Proceedings of the VLDB Endowment,2015,6
Ringtail: a generalized nowcasting system,Dolan Antenucci; Erdong Li; Shaobo Liu; Bochun Zhang; Michael J Cafarella; Christopher Ré,Abstract Social media nowcasting--using online user activity to describe real-worldphenomena--is an active area of research to supplement more traditional and costly datacollection methods such as phone surveys. Given the potential impact of such research; wewould expect general-purpose nowcasting systems to quickly become a standard toolamong noncomputer scientists; yet it has largely remained a research topic. We believe amajor obstacle to widespread adoption is the nowcasting feature selection problem. Typicalnowcasting systems require the user to choose a handful of social media objects from a poolof billions of potential candidates; which can be a time-consuming and error-prone process.We have built RINGTAIL; a nowcasting system that helps the user by automaticallysuggesting high-quality signals. We demonstrate that RINGTALL can make nowcasting …,Proceedings of the VLDB Endowment,2013,6
Learning the structure of generative models without labeled data,Stephen H Bach; Bryan He; Alexander Ratner; Christopher Ré,Abstract: Curating labeled training data has become the primary bottleneck in machinelearning. Recent frameworks address this bottleneck with generative models to synthesizelabels at scale from weak supervision sources. The generative model's dependencystructure directly affects the quality of the estimated labels; but selecting a structureautomatically without any labeled data is a distinct challenge. We propose a structureestimation method that maximizes the $\ell_1 $-regularized marginal pseudolikelihood ofthe observed data. Our analysis shows that the amount of unlabeled data required to identifythe true structure scales sublinearly in the number of possible dependencies for a broadclass of models. Experiments on synthetic data show that our method is 100$\times $ fasterthan a maximum likelihood approach and selects $1/4$ as many extraneous …,arXiv preprint arXiv:1703.00854,2017,5
High performance parallel stochastic gradient descent in shared memory,Scott Sallinen; Nadathur Satish; Mikhail Smelyanskiy; Samantika S Sury; Christopher Ré,Stochastic Gradient Descent (SGD) is a popular optimization method used to train a varietyof machine learning models. Most of SGD work to-date has concentrated on improving itsstatistical efficiency; in terms of rate of convergence to the optimal solution. At the same time;as parallelism of modern CPUs continues to increase through progressively higher corecounts; it is imperative to understand the parallel hardware efficiency of SGD; which oftencomes at odds with its statistical efficiency. In this paper; we explore several modernparallelization methods of SGD on a shared memory system; in the context of sparse andconvex optimization problems. Specifically; we develop optimized parallel implementationsof several SGD algorithms; and show that their parallel efficiency is severely limited by inter-core communication. We propose a new; scalable; communication-avoiding; many-core …,Parallel and Distributed Processing Symposium; 2016 IEEE International,2016,5
SLiMFast: Guaranteed results for data fusion and source reliability,Theodoros Rekatsinas; Manas Joglekar; Hector Garcia-Molina; Aditya Parameswaran; Christopher Ré,Abstract We focus on data fusion; ie; the problem of unifying conflicting data from datasources into a single representation by estimating the source accuracies. We proposeSLiMFast; a framework that expresses data fusion as a statistical learning problem overdiscriminative probabilistic models; which in many cases correspond to logistic regression.In contrast to previous approaches that use complex generative models; discriminativemodels make fewer distributional assumptions over data sources and allow us to obtainrigorous theoretical guarantees. Furthermore; we show how SLiMFast enables incorporatingdomain knowledge into data fusion; yielding accuracy improvements of up to 50% over state-of-the-art baselines. Building upon our theoretical results; we design an optimizer thatobviates the need for users to manually select an algorithm for learning SLiMFast's …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,4
Parallel SGD: When does averaging help?,Jian Zhang; Christopher De Sa; Ioannis Mitliagkas; Christopher Ré,Abstract: Consider a number of workers running SGD independently on the same pool ofdata and averaging the models every once in a while--a common but not well understoodpractice. We study model averaging as a variance-reducing mechanism and describe twoways in which the frequency of averaging affects convergence. For convex objectives; weshow the benefit of frequent averaging depends on the gradient variance envelope. For non-convex objectives; we illustrate that this benefit depends on the presence of multiple globallyoptimal points. We complement our findings with multicore experiments on both syntheticand real data. Subjects: Machine Learning (stat. ML); Learning (cs. LG) Cite as: arXiv:1606.07365 [stat. ML](or arXiv: 1606.07365 v1 [stat. ML] for this version) Submission historyFrom: Jian Zhang [view email][v1] Thu; 23 Jun 2016 16: 23: 35 GMT (1188kb; D),arXiv preprint arXiv:1606.07365,2016,4
Approximation trade-offs in a Markovian stream warehouse: An empirical study,Julie Letchner; Magdalena Balazinska; Christopher Ré; Matthai Philipose,Abstract A large amount of the world's data is both sequential and low-level. Manyapplications need to query higher-level information (eg; words and sentences) that isinferred from these low-level sequences (eg; raw audio signals) using a model (eg; a hiddenMarkov model). This inference process is typically statistical; resulting in high-levelsequences that are imprecise. Once archived; these imprecise streams are difficult to queryefficiently because of their rich semantics and large volumes; forcing applications to sacrificeeither performance or accuracy. There exists little work; however; that characterizes thistrade-off space and helps applications make an appropriate choice. In this paper; we studythe effects–on both efficiency and accuracy–of various stream approximations such asignoring correlations; ignoring low-probability states; or retaining only the single most …,Information Systems,2014,4
A Two-pronged Progress in Structured Dense Matrix Vector Multiplication,Christopher De Sa; Albert Cu; Rohan Puttagunta; Christopher Ré; Atri Rudra,Abstract Matrix-vector multiplication is one of the most fundamental computing primitives.Given a matrix and a vector; it is known that in the worst case Θ (N 2) operations over F areneeded to compute Ab. Many types of structured matrices do admit faster multiplication.However; even given a matrix A that is known to have this property; it is hard in general torecover a representation of A exposing the actual fast multiplication algorithm. Additionally; itis not known in general whether the inverses of such structured matrices can be computedor multiplied quickly. A broad question is thus to identify classes of structured dense matricesthat can be represented with O (N) parameters; and for which matrix-vector multiplication(and ideally other operations such as solvers) can be performed in a sub-quadratic numberof operations. One such class of structured matrices that admit near-linear matrix-vector …,*,2018,3
Snorkel: Rapid training data creation with weak supervision,Alexander Ratner; Stephen H Bach; Henry Ehrenberg; Jason Fries; Sen Wu; Christopher Ré,Abstract: Labeling training data is increasingly the largest bottleneck in deploying machinelearning systems. We present Snorkel; a first-of-its-kind system that enables users to trainstate-of-the-art models without hand labeling any training data. Instead; users write labelingfunctions that express arbitrary heuristics; which can have unknown accuracies andcorrelations. Snorkel denoises their outputs without access to ground truth by incorporatingthe first end-to-end implementation of our recently proposed machine learning paradigm;data programming. We present a flexible interface layer for writing labeling functions basedon our experience over the past year collaborating with companies; agencies; and researchlabs. In a user study; subject matter experts build models 2.8 x faster and increase predictiveperformance an average 45.5% versus seven hours of hand labeling. We study the …,arXiv preprint arXiv:1711.10160,2017,3
Snorkel: Fast training set generation for information extraction,Alexander J Ratner; Stephen H Bach; Henry R Ehrenberg; Chris Ré,Abstract State-of-the art machine learning methods such as deep learning rely on large setsof hand-labeled training data. Collecting training data is prohibitively slow and expensive;especially when technical domain expertise is required; even the largest technologycompanies struggle with this challenge. We address this critical bottleneck with Snorkel; anew system for quickly creating; managing; and modeling training sets. Snorkel enablesusers to generate large volumes of training data by writing labeling functions; which aresimple functions that express heuristics and other weak supervision strategies. These user-authored labeling functions may have low accuracies and may overlap and conflict; butSnorkel automatically learns their accuracies and synthesizes their output labels.Experiments and theory show that surprisingly; by modeling the labeling process in this …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,3
Scan order in Gibbs sampling: Models in which it matters and bounds on how much,Bryan D He; Christopher M De Sa; Ioannis Mitliagkas; Christopher Ré,Abstract Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iterativelysamples variables from their conditional distributions. There are two common scan orders forthe variables: random scan and systematic scan. Due to the benefits of locality in hardware;systematic scan is commonly used; even though most statistical guarantees are only forrandom scan. While it has been conjectured that the mixing times of random scan andsystematic scan do not differ by more than a logarithmic factor; we show by counterexamplethat this is not the case; and we prove that that the mixing times do not differ by more than apolynomial factor under mild conditions. To prove these relative bounds; we introduce amethod of augmenting the state space to study systematic scan using conductance.,Advances in neural information processing systems,2016,3
Duncecap: Compiling worst-case optimal query plans,Adam Perelman; Christopher Ré,Abstract Modern data analytics workloads frequently involve complex join queries where thepairwise-join-based algorithms used by most RDBMS engines are suboptimal. In this study;we explore two algorithms that are asymptotically faster than pairwise algorithms for a largeclass of queries. The first is Yannakakis' classical algorithm for acyclic queries. The secondis a more recent algorithm which works for any query and which is optimal with respect to theworst-case size of the output. We introduce a query compiler; DunceCap; which uses thesetwo algorithms and variations on them to produce optimal query plans; and find that theseplans can outperform standard RDBMS algorithms as well as simple worst-case optimalalgorithms by an order of magnitude on a variety of queries.,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,3
A demonstration of cascadia through a digital diary application,Nodira Khoussainova; Evan Welbourne; Magdalena Balazinska; Gaetano Borriello; Garrett Cole; Julie Letchner; Yang Li; Christopher Ré; Dan Suciu; Jordan Walke,Abstract The Cascadia system provides RFID-based pervasive computing applications withan infrastructure for specifying; extracting and managing meaningful high-level events fromraw RFID data. Cascadia allows application developers and even users to specify events ofinterest using either a declarative query language or a graphical interface with an intuitivevisual language. Cascadia then effectively extracts these events from data in spite of theunreliability of RFID technology and the inherent ambiguity in event extraction.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,3
Applications of probabilistic constraints,Christopher Ré,Abstract Relational database systems are a successful platform to manage large amounts ofdata; but do not cope well with uncertainty. However; the amount of uncertain data isgrowing at an unprecedented rate from both traditional sources (eg integrating enterprisedata) and from next generation sources (eg information extraction). This trend has promptedthe database community to investigate a promising new technique; probabilistic databases;that natively handle uncertainty. In this nascent area; it is an open question whichtechniques from traditional database management apply. A remarkably useful technique instandard relational databases is to allow users to enrich the semantics of their data bydeclaring constraints. Two traditional uses of constraints are to prevent errors while updatingthe data and to optimize queries. More recently; constraints provided an elegant solution …,Technical Reprot TR2007-03-03; University of Washington; Seattle; Washington,2007,3
A Peformant XQuery to SQL Translator,Christopher Ré; James Brinkley; Dan Suciu,Abstract We describe a largely complete and efficient XQuery to SQL translation for XMLpublishing. Our translation supports the entire XQuery language; except for functions; ifstatements and upwards navigation axes. The system has three important properties. First; itpreserves the correct XQuery semantics. This is accomplished by first translating XQueryinto core-XQuery; using a complete XQuery implementation; Galax. Second; we optimize theresulting SQL queries. We develop a comprehensive framework for optimizing the XQuery toSQL translation; which is effective for a wide range of XQuery workloads. Third; ourtranslation is platform independent. Our system achieves high degree of efficiency on a widerange of relational systems. This paper reports an extensive experimental validation onseveral XQuery workloads; using MySQL; PostgreSQL; and SQL Server; and compares …,*,2006,3
Accelerated stochastic power iteration,Christopher De Sa; Bryan He; Ioannis Mitliagkas; Christopher Ré; Peng Xu,Abstract: Principal component analysis (PCA) is one of the most powerful tools in machinelearning. The simplest method for PCA; the power iteration; requires $\mathcal O (1/\Delta) $full-data passes to recover the principal component of a matrix with eigen-gap $\Delta $.Lanczos; a significantly more complex method; achieves an accelerated rate of $\mathcal O(1/\sqrt {\Delta}) $ passes. Modern applications; however; motivate methods that only ingesta subset of available data; known as the stochastic setting. In the online stochastic setting;simple algorithms like Oja's iteration achieve the optimal sample complexity $\mathcal O(\sigma^ 2/\Delta^ 2) $. Unfortunately; they are fully sequential; and also require $\mathcal O(\sigma^ 2/\Delta^ 2) $ iterations; far from the $\mathcal O (1/\sqrt {\Delta}) $ rate of Lanczos.We propose a simple variant of the power iteration with an added momentum term; that …,arXiv preprint arXiv:1707.02670,2017,2
Flipper: A systematic approach to debugging training sets,Paroma Varma; Dan Iter; Christopher De Sa; Christopher Ré,Abstract As machine learning methods gain popularity across different fields; acquiringlabeled training datasets has become the primary bottleneck in the machine learningpipeline. Recently generative models have been used to create and label large amounts oftraining data; albeit noisily. The output of these generative models is then used to train adiscriminative model of choice; such as logistic regression or a complex neural network.However; any errors in the generative model can propagate to the subsequent model beingtrained. Unfortunately; these generative models are not easily interpretable and aretherefore difficult to debug for users. To address this; we present our vision for Flipper; aframework that presents users with high-level information about why their training set isinaccurate and informs their decisions as they improve their generative model manually …,Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics,2017,2
Learning to Compose Domain-Specific Transformations for Data Augmentation,Alexander J Ratner; Henry Ehrenberg; Zeshan Hussain; Jared Dunnmon; Christopher Ré,Abstract Data augmentation is a ubiquitous technique for increasing the size of labeledtraining sets by leveraging task-specific data transformations that preserve class labels.While it is often easy for domain experts to specify individual transformations; constructingand tuning the more sophisticated compositions typically needed to achieve state-of-the-artresults is a time-consuming manual task in practice. We propose a method for automatingthis process by learning a generative sequence model over user-specified transformationfunctions using a generative adversarial approach. Our method can make use of arbitrary;non-deterministic transformation functions; is robust to misspecified user input; and is trainedon unlabeled data. The learned transformation model can then be used to perform dataaugmentation for any end discriminative model. In our experiments; we show the efficacy …,Advances in Neural Information Processing Systems,2017,2
Links between Join Processing and Convex Geometry.,Christopher Ré,This talk will survey some results on join processing that use inequalities from convexgeometry. Recently; Ngo; Porat; Rudra; and Ré (NPRR) discovered the first relational joinalgorithm with worst-case optimal running time [8]. Since the seminal System R project [12];the dominant database optimizer paradigm optimizes a join query by examining each pair ofjoins and then combining these estimates using dynamic programming. In contrast; NPRRexamines all relations at the same time. This change to the “one-join-at-a-time” dogma isimportant for performance: there are classes of queries for which any join-project plan isdestined to be slower than the best possible run time by a polynomial factor in size of thedata. NPRR's analysis makes a link from database theory to a pair of beautiful inequalities:one due to Atserias; Grohe; and Marx from computer science [2] and one due to Bollobás …,ICDT,2014,2
User manual of tuffy 0.3,AnHai Doan; Feng Niu; Christopher Ré; Jude Shavlik; Ce Zhang,Markov logic networks (MLNs)[7; 1] have emerged as a powerful framework that combinesstatistical and logical reasoning; they have been applied to many data intensive problemsincluding information extraction; entity resolution; text mining; and natural languageprocessing. Based on principled data management techniques; Tuffy is an MLN engine thatachieves scalability and orders of magnitude speedup compared to prior artimplementations. It is written in Java and relies on PostgreSQL. For a brief introduction toMLNs and the technical details of Tuffy; please see our VLDB 2011 paper [4].,*,2011,2
Coming Soon to a Lab Near You: Drag-and-Drop Virtual Worlds,C Re; T Res,A group of researchers at Microsoft hopes to transform the way scientists study complex;ever-changing systems; such as the global carbon cycle and information processing insidecells. To do so; they9re working to develop a suite of new software tools including novelprogramming languages that better represent biological systems and computer models thatwork across multiple scales; simulating carbon budgets at the levels of leaves; trees; andforests; for example. They9re also striving to make those tools simple to use; therebyextending the types of studies that can be done by researchers who aren9t full-timeprogrammers. Prototype versions of several of these tools are now up and running andbeing put through their paces by researchers at Microsoft.,Science,2011,2
AURA: Enabling attribute-based spatial search in RFID rich environments,Tejas A Bapat; K Selçuk Candan; V Snehith Cherukuri; Hari Sundaram,In this paper; we introduce AURA; a novel framework for enriching the physical environmentwith information about objects and activities in order to support searches in the physicalworld. The goal is to enable individuals to use the environment in which they function as aliving (short-term) memory of their activities and of the objects with which they interact in thisenvironment. In order to act as a memory; the physical environment must be transparentlyembedded with relevant information and made accessible by in-situ search mechanisms.We achieve this embedding through innovative algorithms that leverage a collection ofparasitic RFID tags distributed in the environment to act as a distributed storage cloud.Information about the activities of the users and objects with which they interact are encodedand stored; in a decentralized way; on these RFID tags to support attribute-based search …,Data Engineering; 2009. ICDE'09. IEEE 25th International Conference on,2009,2
Managing large-scale probabilistic databases,Christopher Ré,Abstract Modern applications are driven by data; and increasingly the data driving theseapplications are imprecise. The set of applications that generate imprecise data is diverse: Insensor database applications; the goal is to measure some aspect of the physical world(such as temperature in a region or a person's location). Such an application has no choicebut to deal with imprecision; as measuring the physical world is inherently imprecise. In dataintegration; consider two databases that refer to the same set of real-world entities; but theway in which they refer to those entities is slightly different. For example; one database maycontain an entity 'J. Smith'while the second database refers to 'John Smith'. In such ascenario; the large size of the data makes it too costly to manually reconcile all references inthe two databases. To lower the cost of integration; state-of-the-art approaches allow the …,*,2009,2
Query Containment of Tier-2 Queries over a Probabilistic Database.,Katherine F Moore; Vibhor Rastogi; Christopher Ré; Dan Suciu,Abstract. We study the containment problem for a query language over probabilisticrelational databases that allows queries like “is the probability that q1 holds greater than 0.2and the probability that q2 holds greater than 0.6?” where q1 and q2 are Booleanconjunctive queries. In addition to being a fundamental problem in its own right; thecontainment problem is the key problem that an optimizer must solve for many standardoptimizations (such as picking up an index or using a materialized view). Our main technicalresult is that the containment problem is decidable; and we give an EXPSPACE-algorithmbased on linear programming for it. We believe that we are the first to study the containmentproblem for any such probabilistic languages.,MUD,2009,2
An extended state-space markov chain model for self-organizing systems in non-well-mixed environments,N Napp; E Klavins,*,*,2007,2
Mind the gap: bridging multi-domain query workloads with EmptyHeaded,Christopher R Aberger; Andrew Lamb; Kunle Olukotun; Christopher Ré,Abstract Executing domain specific workloads from a relational data warehouse is anincreasingly popular task. Unfortunately; classic relational database management systems(RDBMS) are suboptimal in many domains (eg; graph and linear algebra queries); and it ischallenging to transfer data from an RDBMS to a domain specific toolkit in an efficientmanner. This demonstration showcases the EmptyHeaded engine: an interactive queryprocessing engine that leverages a novel query architecture to support efficient execution inmultiple domains. To enable a unified design; the EmptyHeaded architecture is built aroundrecent theoretical advancements in join processing and automated in-query datatransformations. This demonstration highlights the strengths and weaknesses of this noveltype of query processing architecture while showcasing its flexibility in multiple domains …,Proceedings of the VLDB Endowment,2017,1
ShortFuse: biomedical time series representations in the presence of structured information,Madalina Fiterau; Suvrat Bhooshan; Jason Fries; Charles Bournhonesque; Jennifer Hicks; Eni Halilaj; Christopher Ré; Scott Delp,Abstract: In healthcare applications; temporal variables that encode movement; health statusand longitudinal patient evolution are often accompanied by rich structured information suchas demographics; diagnostics and medical exam data. However; current methods do notjointly optimize over structured covariates and time series in the feature extraction process.We present ShortFuse; a method that boosts the accuracy of deep learning models for timeseries by explicitly modeling temporal interactions and dependencies with structuredcovariates. ShortFuse introduces hybrid convolutional and LSTM cells that incorporate thecovariates via weights that are shared across the temporal domain. ShortFuse outperformscompeting models by 3% on two biomedical applications; forecasting osteoarthritis-relatedcartilage degeneration and predicting surgical outcomes for cerebral palsy patients …,arXiv preprint arXiv:1705.04790,2017,1
A Relational Framework for Classifier Engineering,Benny Kimelfeld; Christopher Ré,Abstract In the design of analytical procedures and machine-learning solutions; a critical andtime-consuming task is that of feature engineering; for which various recipes and toolingapproaches have been developed. In this framework paper; we embark on theestablishment of database foundations for feature engineering. We propose a formalframework for classification in the context of a relational database. The goal of thisframework is to open the way to research and techniques to assist developers with the taskof feature engineering by utilizing the database's modeling and understanding of data andqueries; and by deploying the well studied principles of database management. As a firststep; we demonstrate the usefulness of this framework by formally defining three keyalgorithmic challenges. The first challenge is that of separability; which is the problem of …,Proceedings of the 36th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,2017,1
DAWNBench: An End-to-End Deep Learning Benchmark and Competition,Cody Coleman; Deepak Narayanan; Daniel Kang; Tian Zhao; Jian Zhang; Luigi Nardi; Peter Bailis; Kunle Olukotun; Chris Ré; Matei Zaharia,Abstract Despite considerable research on systems; algorithms and hardware to speed updeep learning workloads; there is no standard means of evaluating end-to-end deeplearning performance. Existing benchmarks measure proxy metrics; such as time to processone minibatch of data; that do not indicate whether the system as a whole will produce ahigh-quality result. In this work; we introduce DAWNBench; a benchmark and competitionfocused on end-to-end training time to achieve a state-of-the-art accuracy level; as well asinference time with that accuracy. Using time to accuracy as a target metric; we explore howdifferent optimizations; including choice of optimizer; stochastic depth; and multi-GPUtraining; affect end-to-end training performance. Our results demonstrate that optimizationscan interact in non-trivial ways when used in conjunction; producing lower speed-ups and …,Training,2017,1
Socratic Learning,Paroma Varma; Rose Yu; Dan Iter; Christopher De Sa; Christopher Ré,Abstract Modern machine learning techniques often use discriminative models that requirelarge amounts of labeled data. Since generating labeled training data sets is expensive; analternative approach is to use a generative model; which leverages a simple heuristic toweakly label data. Domain experts prefer using generative models because they “tell astory” about their data. Unfortunately; generative models are typically less accurate thandiscriminative models. Several recent approaches connect the two types of models to exploittheir strengths. In this setting; a misspecified generative model can hurt the performance ofsubsequent discriminative training. To address this issue; we propose a framework calledSocratic learning that automatically uses information from the discriminative model to correctgenerative model misspecification. This process also provides users with interpretable …,Neural Information Processing Systems. Future of Interactive Learning Machines Workshop,2016,1
Using Commonsense Knowledge to Automatically Create (Noisy) Training Examples from Text.,Sriraam Natarajan; Jose Picado; Tushar Khot; Kristian Kersting; Christopher Re; Jude W Shavlik,Abstract One of the challenges to information extraction is the requirement of humanannotated examples. Current successful approaches alleviate this problem by employingsome form of distant supervision ie; look into knowledge bases such as Freebase as asource of supervision to create more examples. While this is perfectly reasonable; mostdistant supervision methods rely on a hand coded background knowledge that explicitlylooks for patterns in text. In this work; we take a different approach–we create weaklysupervised examples for relations by using commonsense knowledge. The key innovation isthat this commonsense knowledge is completely independent of the natural language text.This helps when learning the full model for information extraction as against simply learningthe parameters of a known CRF or MLN. We demonstrate on two domains that this form of …,AAAI Workshop: Statistical Relational Artificial Intelligence,2013,1
Probabilistic databases: Diamonds in the dirt (extended version),Nilesh Dalvi; Christopher Ré; Dan Suciu,A wide range of applications have recently emerged that need to manage large; imprecisedata sets. The reasons for imprecision in data are as diverse as the applications themselves:in sensor and RFID data; imprecision is due to measurement errors [28; 66]; in informationextraction; imprecision comes from the inherent ambiguity in natural-language text [32; 40];and in business intelligence; imprecision is used to reduce the cost of data cleaning [12]. Insome applications; such as privacy; it is a requirement that the data be less precise. Forexample; imprecision is purposely inserted to hide sensitive attributes of individuals so thatthe data may be published [29; 55; 62]. Imprecise data has no place in traditional; precisedatabase applications like payroll and inventory; and so; current database managementsystems are not prepared to deal with it. In contrast; these newly emerging applications …,*,2008,1
Learning Invariance with Compact Transforms,Anna T Thomas; Albert Gu; Tri Dao; Atri Rudra; Christopher Ré,Abstract: The problem of building machine learning models that admit efficientrepresentations and also capture an appropriate inductive bias for the domain has recentlyattracted significant interest. Existing work for compressing deep learning pipelines hasexplored classes of structured matrices that exhibit forms of shift-invariance akin toconvolutions. We leverage the displacement rank framework to automatically learn thestructured class; allowing for adaptation to the invariances required for a given dataset whilepreserving asymptotically efficient multiplication and storage. In a setting with a small fixedparameter budget; our broad classes of structured matrices improve final accuracy by 5-7%on standard image classification datasets compared to conventional parameter constrainingmethods.,*,2018,*
A Formal Framework For Probabilistic Unclean Databases,Christopher De Sa; Ihab F Ilyas; Benny Kimelfeld; Christopher Re; Theodoros Rekatsinas,Abstract: Traditional modeling of inconsistency in database theory casts all possible"repairs" equally likely. Yet; effective data cleaning needs to incorporate statistical reasoning.For example; yearly salary of\$100 k and age of 22 are more likely than\$100 k and 122 andtwo people with same address are likely to share their last name (ie; a functionaldependency tends to hold but may occasionally be violated). We propose a formalframework for unclean databases; where two types of statistical knowledge are incorporated.The first represents a belief of how intended (clean) data is generated; and the secondrepresents a belief of how the actual database is realized through the introduction of noise.Formally; a Probabilistic Unclean Database (PUD) is a triple that consists of a probabilisticdatabase that we call the" intention"; a probabilistic data transformator that we call the" …,arXiv preprint arXiv:1801.06750,2018,*
Association of Omics Features with Histopathology Patterns in Lung Adenocarcinoma,Kun-Hsing Yu; Gerald J Berry; Daniel L Rubin; Christopher Ré; Russ B Altman; Michael Snyder,Summary Adenocarcinoma accounts for more than 40% of lung malignancy; andmicroscopic pathology evaluation is indispensable for its diagnosis. However; howhistopathology findings relate to molecular abnormalities remains largely unknown. Here;we obtained H&E-stained whole-slide histopathology images; pathology reports; RNAsequencing; and proteomics data of 538 lung adenocarcinoma patients from The CancerGenome Atlas and used these to identify molecular pathways associated withhistopathology patterns. We report cell-cycle regulation and nucleotide binding pathwaysunderpinning tumor cell dedifferentiation; and we predicted histology grade usingtranscriptomics and proteomics signatures (area under curve> 0.80). We built an integrativehistopathology-transcriptomics model to generate better prognostic predictions for stage I …,Cell systems,2017,*
Creating Robust Relation Extract and Anomaly Detect via Probabilistic Logic-Based Reasoning and Learning,Jude Shavlik; Chris Re; Sriraam Natarajan,Abstract: We consider a three-pronged approach to deep exploration and filtering of text.The first is development of a set of scalable state-of-the-art learning algorithms that arecapable of learning generalized probabilistic logic rules from noisy; incomplete data. Thesecond is a data management system that is widely accepted as the state-of-the art forknowledge base construction (KBC) and is highly scalable. The final direction is the designand adaptation of the scalable management and learning algorithms for the tasks of deepknowledge understanding such as knowledge-based population and anomaly detection. Inthis report; they organize and present their accomplishments (the approaches and theirintuitive; theoretical and empirical ramifications) from the DEFT cooperative agreement into3 main focus areas or research thrusts. Each of them is motivated and introduced …,*,2017,*
Report from the third workshop on Algorithms and Systems for MapReduce and Beyond (BeyondMR'16),Foto N Afrati; Jan Hidders; Christopher Ré; Jacek Sroka; Jeffrey Ullman,Abstract This report summarizes the presentations and discussions of the third workshop onAlgorithms and Systems for MapReduce and Beyond (BeyondMR'16). The BeyondMRworkshop was held in conjunction with the 2016 SIGMOD conference in San Francisco;California; USA on July 1; 2016. The goal of the workshop was to bring together researchersand practitioners to explore algorithms; computational models; architectures; languages andinterfaces for systems that need largescale parallelization and systems designed to supportefficient parallelization and fault tolerance. These include specialized programming anddata-management systems based on MapReduce and extensions; graph processingsystems; data-intensive workflow and dataflow systems. The program featured two very wellattended invited talks by Ion Stoica from AMPLab; University of California Berkeley and …,ACM SIGMOD Record,2017,*
LevelHeaded: Making Worst-Case Optimal Joins Work in the Common Case,Christopher R Aberger; Andrew Lamb; Kunle Olukotun; Christopher Ré,Abstract: Pipelines combining SQL-style business intelligence (BI) queries and linearalgebra (LA) are becoming increasingly common in industry. As a result; there is a growingneed to unify these workloads in a single framework. Unfortunately; existing solutions eithersacrifice the inherent benefits of exclusively using a relational database (eg logical andphysical independence) or incur orders of magnitude performance gaps compared tospecialized engines (or both). In this work we study applying a new type of query processingarchitecture to standard BI and LA benchmarks. To do this we present a new in-memoryquery processing engine called LevelHeaded. LevelHeaded uses worst-case optimal joinsas its core execution mechanism for both BI and LA queries. With LevelHeaded; we showhow crucial optimizations for BI and LA queries can be captured in a worst-case optimal …,arXiv preprint arXiv:1708.07859,2017,*
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,Peter Bailis; Kunle Olukoton; Christopher Ré; Matei Zaharia,Abstract: Despite incredible recent advances in machine learning; building machinelearning applications remains prohibitively time-consuming and expensive for all but thebest-trained; best-funded engineering organizations. This expense comes not from a needfor new and improved statistical models but instead from a lack of systems and tools forsupporting end-to-end machine learning application development; from data preparationand labeling to productionization and monitoring. In this document; we outline opportunitiesfor infrastructure supporting usable; end-to-end machine learning applications in the contextof the nascent DAWN (Data Analytics for What's Next) project at Stanford. Subjects: Learning(cs. LG); Databases (cs. DB); Machine Learning (stat. ML) Cite as: arXiv: 1705.07538 [cs.LG](or arXiv: 1705.07538 v1 [cs. LG] for this version) Submission history From: Peter …,arXiv preprint arXiv:1705.07538,2017,*
SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data,Jason Fries; Sen Wu; Alex Ratner; Christopher Ré,Abstract: We present SwellShark; a framework for building biomedical named entityrecognition (NER) systems quickly and without hand-labeled data. Our approach viewsbiomedical resources like lexicons as function primitives for autogenerating weaksupervision. We then use a generative model to unify and denoise this supervision andconstruct large-scale; probabilistically labeled datasets for training high-accuracy NERtaggers. In three biomedical NER tasks; SwellShark achieves competitive scores with state-of-the-art supervised benchmarks using no hand-labeled training data. In a drug nameextraction task using patient medical records; one domain expert using SwellShark achievedwithin 5.1% of a crowdsourced annotation approach--which originally utilized 20 teams overthe course of several weeks--in 24 hours. Subjects: Computation and Language (cs. CL) …,arXiv preprint arXiv:1704.06360,2017,*
Fonduer: Knowledge Base Construction from Richly Formatted Data,Sen Wu; Luke Hsiao; Xiao Cheng; Braden Hancock; Theodoros Rekatsinas; Philip Levis; Christopher Ré,Abstract: We introduce Fonduer; a knowledge base construction (KBC) framework for richlyformatted information extraction (RFIE); where entity relations and attributes are conveyedvia structural; tabular; visual; and textual expressions. Fonduer introduces a newprogramming model for KBC built around a unified data representation that accounts forthree challenging characteristics of richly formatted data:(1) prevalent document-levelrelations;(2) multimodality; and (3) data variety. Fonduer is the first KBC system for richlyformatted data and uses a human-in-the-loop paradigm for training machine learningsystems; referred to as data programming. Data programming softens the burden oftraditional supervision by only asking users to provide lightweight functions thatprogrammatically assign (potentially noisy) labels to the input data. Fonduer's unified data …,arXiv preprint arXiv:1703.05028,2017,*
Machine learning and deep analytics for biocomputing: call for better explainability,Dragutin Petkovic; Lester Kobzik; Christopher Re,The goals of this workshop are to discuss challenges in explainability of current MachineLeaning and Deep Analytics (MLDA) used in biocomputing and to start the discussion onways to improve it. We define explainability in MLDA as easy to use information explainingwhy and how the MLDA approach made its decisions. We believe that much greater effort isneeded to address the issue of MLDA explainability because of: 1) the ever increasing useand dependence on MLDA in biocomputing including the need for increased adoption bynon-MLD experts; 2) the diversity; complexity and scale of biocomputing data and MLDAalgorithms; 3) the emerging importance of MLDA-based decisions in patient care; in dailyresearch; as well as in the development of new costly medical procedures and drugs. Thisworkshop aims to: a) analyze and challenge the current level of explainability of MLDA …,*,2017,*
Inferring Generative Model Structure with Static Analysis,Paroma Varma; Bryan D He; Payal Bajaj; Nishith Khandwala; Imon Banerjee; Daniel Rubin; Christopher Ré,Abstract Obtaining enough labeled data to robustly train complex discriminative models is amajor bottleneck in the machine learning pipeline. A popular solution is combining multiplesources of weak supervision using generative models. The structure of these models affectsthe quality of the training labels; but is difficult to learn without any ground truth labels. Weinstead rely on weak supervision sources having some structure by virtue of being encodedprogrammatically. We present Coral; a paradigm that infers generative model structure bystatically analyzing the code for these heuristics; thus significantly reducing the amount ofdata required to learn structure. We prove that Coral's sample complexity scalesquasilinearly with the number of heuristics and number of relations identified; improving overthe standard sample complexity; which is exponential in n for learning n-th degree …,Advances in Neural Information Processing Systems,2017,*
Gaussian Quadrature for Kernel Features,Tri Dao; Christopher M De Sa; Christopher Ré,Abstract Kernel methods have recently attracted resurgent interest; showing performancecompetitive with deep neural networks in tasks such as speech recognition. The randomFourier features map is a technique commonly used to scale up kernel machines; butemploying the randomized feature map means that $ O (\epsilon^{-2}) $ samples arerequired to achieve an approximation error of at most $\epsilon $. We investigate somealternative schemes for constructing feature maps that are deterministic; rather than random;by approximating the kernel in the frequency domain using Gaussian quadrature. We showthat deterministic feature maps can be constructed; for any $\gamma> 0$; to achieve error$\epsilon $ with $ O (e^{\gamma}+\epsilon^{-1/\gamma}) $ samples as $\epsilon $ goes to 0.Our method works particularly well with sparse ANOVA kernels; which are inspired by the …,Advances in Neural Information Processing Systems,2017,*
AMELIE accelerates Mendelian patient diagnosis directly from the primary literature,Johannes Birgmeier; Maximilian Haeussler; Cole A Deisseroth; Karthik A Jagadeesh; Alexander J Ratner; Harendra Guturu; Aaron M Wenger; Peter D Stenson; David N Cooper; Christopher Re; Jonathan A Bernstein; Gill Bejerano,The diagnosis of Mendelian disorders requires labor-intensive literature research. Oursoftware system AMELIE (Automatic MEndelian LIterature Evaluation) greatly automates thisprocess. AMELIE parses hundreds of thousands of full text articles to find an underlyingdiagnosis to explain a patient9s phenotypes given the patient9s exome. AMELIE prioritizespatient candidate genes for their likelihood of causing the patient9s phenotypes. Diagnosisof singleton patients (without relatives9 exomes) is the most time-consuming scenario.AMELIE9s gene ranking method was tested on 215 singleton Mendelian patients with aclinical diagnosis. AMELIE ranked the causal gene among the top 2 in the majority (63%) ofcases. Examining AMELIE9s top 10 genes; amounting to 8% of 124 candidate genes withrare functional variants per patient; results in diagnosis for 95% of cases. Strikingly …,bioRxiv,2017,*
Socratic Learning: Augmenting Generative Models to Incorporate Latent Subsets in Training Data,Paroma Varma; Bryan He; Dan Iter; Peng Xu; Rose Yu; Christopher De Sa; Christopher Ré,Abstract A challenge in training discriminative models like neural networks is obtainingenough labeled training data. Recent approaches use generative models to combine weaksupervision sources; like user-defined heuristics or knowledge bases; to label training data.Prior work has explored learning accuracies for these sources even without ground truthlabels; but they assume that a single accuracy parameter is sufficient to model the behaviorof these sources over the entire training set. In particular; they fail to model latent subsets inthe training data in which the supervision sources perform differently than on average. Wepresent Socratic learning; a paradigm that uses feedback from a correspondingdiscriminative model to automatically identify these subsets and augments the structure ofthe generative model accordingly. Experimentally; we show that without any ground truth …,arXiv preprint arXiv:1610.08123,2016,*
Dark Data: Are we solving the right problems?,Michael Cafarella; Ihab F Ilyas; Marcel Kornacker; Tim Kraska; Christopher Ré,With the increasing urge of the enterprises to ingest as much data as they can in what'scommonly referred to as “Data Lakes”; the new environment presents serious challenges totraditional ETL models and to building analytic layers on top of well-understood globalschema. With the recent development of multiple technologies to support this “load-first”paradigm; even traditional enterprises have fairly large HDFS-based data lakes now. Theyhave even had them long enough that their first generation IT projects delivered on some;but not all; of the promise of integrating their enterprise's data assets. In short; we movedfrom no data to Dark data. Dark data is what enterprises might have in their possession;without the ability to access it or with limited awareness of what this data represents. Inparticular; business-critical information might still remain out of reach. This panel is about …,Data Engineering (ICDE); 2016 IEEE 32nd International Conference on,2016,*
High-speed statistical processing in a database,*,Numerically intensive statistical processing of data is implemented as an incrementalgradient method within the engine of a database system. Small user-defined functions in thedatabase system calculate an approximate gradient from one term of a linearly separabledefined cost resolvable from a single tuple of the database. In this way the optimized dataaccess of the database may be exploited for rapid statistical processing.,*,2015,*
Exploiting Correlations for Expensive Predicate Evaluation,Manas Joglekar; Hector Garcia-Molina; Aditya Parameswaran; Christopher Re,Abstract User Defined Function (UDFs) are used increasingly to augment query languageswith extra; application dependent functionality. Selection queries involving UDF predicatestend to be expensive; either in terms of monetary cost or latency. In this paper; we studyways to efficiently evaluate selection queries with UDF predicates. We provide a family oftechniques for processing queries at low cost while satisfying user-specified precision andrecall constraints. Our techniques are applicable to a variety of scenarios including whenselection probabilities of tuples are available beforehand; when this information is availablebut noisy; or when no such prior information is available. We also generalize our techniquesto more complex queries. Finally; we test our techniques on real datasets; and show thatthey achieve significant savings in UDF evaluations of up to $80\% $; while incurring only …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,*
A Database Framework for Classifier Engineering,Benny Kimelfeld; Christopher Ré,In the design of machine-learning solutions; a critical and often the most resourceful task isthat of feature engineering [7; 4]; for which recipes and tooling have been developed [3; 7].In this vision paper we embark on the establishment of database foundations for featureengineering. We propose a formal framework for classification; in the context of a relationaldatabase; towards investigating the application of database and knowledge management toassist with the task of feature engineering. We demonstrate the usefulness of this frameworkby formally defining two key algorithmic challenges within:(1) separability refers todetermining the existence of feature queries that agree with the given training examples;and (2) identifiability is the task of testing for the property of independence among features(given as queries). Moreover; we give preliminary results on these challenges; in the …,CEUR workshop proceedings,2015,*
Jedi: A Storage Manager for SIMD-aware; Worst-case Optimal Join Processing.,Christopher Ré,ABSTRACT This talk describes a new graph-pattern engine called Jedi. Using a recentsimplification of worst-case optimal join algorithms due to Ngo et al.; Jedi translates joinqueries into a series of set intersection and union operations. Such set operations areideally suited to modern CPUs that provides single-instruction; multiple data (SIMD)instructions. Using these ideas; we demonstrate that Jedi outperforms specialized graphengines by over an order of magnitude and relational systems by over two orders ofmagnitude on standard graph processing queries over real data.,EDBT/ICDT Workshops,2015,*
Bringing Paleontology's ‘Dark Data’to Light,Shanan E Peters; Christopher Ré; Miron Livny; Ce Czhang; Vidhya Govindaraju; Michael McClennen; John J Czaplewski,Paleontology is fundamentally based on the description and biological classification offossils; an enterprise that over the past four centuries has spawned countless fieldexpeditions; museum trips; and hundreds of thousands of publications. The construction ofdatabases that aggregate these descriptive data on fossils in a way that allows large-scale;synthetic questions to be addressed has greatly expanded the intellectual reach ofpaleontology; and has led to many important new insights into macroevolutionary andmacroecological processes. Nevertheless; paleontology; like many geoscience disciplines;remains comparatively data-limited; both in terms of the pace of discovery and description ofnew fossils and in terms of the ability of researchers to find; access; and utilize existing datain a large; widely disseminated; and heterogeneous literature. One of the largest …,The Paleontological Society Special Publications,2014,*
Tradeoffs in Main-Memory Statistical Analytics from Impala to DimmWitted.,Victor Bittorf; Marcel Kornacker; Christopher Ré; Ce Zhang,Recent years have seen a surge in main-memory SQL-style analytic solutions to quicklydeliver business critical information over massive data sets [1; 7; 14]. At the same time; thereis an arms race to offer increasingly sophisticated statistical analytics inspired by thesuccess of web search; voice recognition; and image analysis; eg; Google Brain [8];Facebook [6]; and Microsoft's Adam [2]. This talk describes the first author's experienceporting statistical analytics to Impala via MADlib and observations about research for high-performance main-memory analytics that may be relevant for systems like Impala. A majormotivation for Impala was to enable interactive SQL-analytics queries over data stored inHadoop. Impala achieves high performance through many techniques including as co-location of computation with data in HDFS; LLVM code generation [13]; and aggressive …,IMDM@ VLDB,2014,*
The Theory of Zeta Graphs with an Application to Random Networks.,Christopher Ré,ABSTRACT Social; biological; and cyberphysical networks generate some of the mostintriguing and valuable sources of data on the planet. For at least the last two decades;researchers have attempted to create formal (typically stochastic) models of these networks.We examine the database theory questions raised by these new models. We study a simpleextension of Erdös–Rényi models that we call Zeta graphs. Zeta graphs are related tomultiple-valued zeta functions; and we show that the expectation of a conjunctive query canbe written as a linear combination of multiple-valued zeta functions. For queries on graphs;we use our results to devise a complete decision procedure for whether the probability that aquery is true tends to 1 as the domain size tends to infinity. We apply our theory of Zetagraphs to describe the set of conjunctive graph queries that are true with probability 1 in …,ICDT,2014,*
Invited Talks,Richard J Doyle; Michel Dumontier; Haym Hirsh; David Jensen; Peter Karp; Claire Monteleoni; Zoen Obradovic; Christopher Re; Andrey Rzhetsky; Kiri L Wagstaff,Although discovery is ostensibly a process that operates on data in hand; in the context ofspace exploration it is natural to take a full lifecycle perspective that begins at the datacollection point of a sensor or instrument. At each phase of the data lifecycle; important stepscan be taken to both enable and assist the objective of scientific discovery. For example;data triage is concerned with efficient assessment of data while it is buffered at the collectionpoint; to address the harsh reality that for many emerging high-capacity sensor andinstruments; not all data can be captured. Data visualization provides an array of tools forabstracting large volumes of data; to gain insight and shape the next query. Model-to-datareconciliation is at the heart of hypothesis formation and refinement; and relates to intelligentsampling strategies for maximizing information gain at multiple levels:(1) extracting the …,2013 AAAI Fall Symposium Series,2013,*
A tutorial on trained systems: a new generation of data management systems?,Christopher Ré,Abstract A new generation of data processing systems; including web search; Google'sKnowledge Graph; IBM's DeepQA; and several different recommendation systems; combinerich databases with software driven by machine learning. The spectacular successes ofthese trained systems have been among the most notable in all of computing and havegenerated excitement in health care; finance; energy; and general business. But buildingthem can be challenging even for computer scientists with PhD-level training. This tutorialwill describe some of the recent progress on trained systems from both industrial andacademic systems. It will also contain a walkthrough of examples of trained systems that arein daily use by scientists in Geoscience; PaleoBiology; and English Literature. Papers;software; virtual machines that contain installations of our software; links to applications …,British National Conference on Databases,2013,*
Faust: Flexible Acquistion and Understanding System for Text,LL Voss; David E Wilkins; David Israel; Christopher Manning; Daniel Jurafsky; Daniel S Weld; Pedro Domingos; Jude Shavlik; Christopher Re; Andrew McCallum,Abstract: The vast majority of scientific and technical knowledge is expressed in natural-language (NL) texts. Our objective was to create an automated reading system that makesthe knowledge in NL texts accessible to any of an open-ended range of formal reasoningsystems. Our approach was based on large-scale statistical (probabilistic) joint inferenceover relational models. This vision involved a radical re-thinking of the architecture forMachine Reading systems. Descriptors:* NATURAL LANGUAGE; ARTIFICIALINTELLIGENCE; AUTOMATION; PROBABILITY; READING Subject Categories: LinguisticsCybernetics Distribution Statement: APPROVED FOR PUBLIC RELEASE DEFENSETECHNICAL INFORMATION CENTER 8725 John J. Kingman Road; Fort Belvoir; VA 22060-6218 1-800-CAL-DTIC (1-800-225-3842) ABOUT,*,2013,*
Probabilistic Web Data Management,Lei Chen; Ihab Ilyas; Christopher Re; Xiaofang Zhou,With the development of Web 2.0 technology; enormous data are generated every day.Among these data; there exist quite a lot uncertainty due to careless data entry; incompleteinformation; and inconsistency among different data description. Although significant efforthas been paid to find effective and efficient solutions for managing and mining generaluncertain data; little attention is paid to manage uncertain data on the Web. This specialissue is proposed to attract research attempts on handling uncertainty of the Web data. Thisspecial issue has attracted 12 submissions; after two rounds of very careful reviews bydomain experts; we accepted three excellent papers. These three papers present new ideasto address issues on Probabilistic Web Data Management. The first paper;“an efficientapproach to suggesting topically related Web queries using hidden topic model” …,World Wide Web,2013,*
Exploiting Correlations for Evaluating Complex Queries,Manas Joglekar; Hector Garcia-Molina; Aditya Parameswaran; Chris Re,ABSTRACT User Defined Function (UDFs) are used increasingly to augment querylanguages with extra; application dependent functionality. UDFs tend to be expensive; eitherin terms of monetary cost or latency. In this paper; we study ways to efficiently evaluateselection queries involving UDFs. We provide a family of techniques for processing queriesat low cost while satisfying user-specified precision and recall constraints. Our techniquesare applicable to a wide variety of scenarios; such as when selection probabilities of tuplesare available beforehand; when this information is available but noisy; or when no such priorinformation is available. We also generalize our techniques to more complex queries.Finally; we test our techniques on real datasets; and show that they achieve significantsavings in cost of up to 80%; while incurring only a small reduction in accuracy.,*,2013,*
Bootstrapping Knowledge Base Acceleration.,Tushar Khot; Ce Zhang; Jude W Shavlik; Sriraam Natarajan; Christopher Ré,Abstract The Streaming Slot Filler (SSF) task in TREC Knowledge Base Acceleration trackinvolves detecting changes to slot values (relations) over time. To handle this task; thesystem needs to extract relations to identify slot-filler values and detect novel values. Beingthe first attempt at KBA; the biggest challenge that we faced was the scale of the data. Wepresent the approach used by University of Wisconsin for the SSF task and the large scalechallenge. We used Elementary; a scalable statistical inference and learning system;developed in University of Wisconsin as our core system. We used Stanford NLP Toolkit togenerate parse trees; dependency graphs and named-entity recognition information. Thesewere then converted to features for the logistic regression learner of Elementary. To handlethe lack of early SSF training data; we used our existing Knowledge Base Population …,TREC,2013,*
USER MANUAL OF FELIX 0.2,Feng Niu; Christopher Ré; Jude Shavlik; Josh Slauson; Ce Zhang,Recent years have seen a surge of sophisticated statistical frameworks with the relationaldata model (via SQL/logic-like languages). Examples include MLNs and PRMs. While thismovement has demonstrated significant quality advantage in numerous applications onsmall datasets; efficiency and scalability have been a critical challenge to their deploymentin Enterprise settings. Felix addresses such performance challenges with an operator-basedapproach; where each operator performs a statistical algorithm with relational input/output.The key observations underlying Felix are 1. Task Decomposition: Conventionalapproaches to statistical-relational inference are monolithic in that; not only do they expresscomplex tasks in a “unified” language; they also attempt to solve the tasks by running ageneric algorithm (eg; random walk; greedy; or sampling) over the entire program. This …,*,2011,*
Report of the Probabilistic Databases Benchmarking: 08421 Working Group,C Koch; C Re; D Olteanu; H-J Lenz; PJ Haas; JZ Pan; B König-Ries; V Markl,*,*,2009,*
08421 Working Group: Report of the Probabilistic Databases Benchmarking,Christoph Koch; C Re; D Olteanu; HJ Lenz; PJ Haas; JZ Pan,Koch; C.; Re; C.; Olteanu; D.; Lenz; HJ; Haas; PJ; & Pan; JZ (2009). 08421 Working Group: Reportof the Probabilistic Databases Benchmarking. In C. Koch; B. König-Ries; V. Markl; & M. van Keulen(Eds.); Proceedings of Dagstuhl Seminar 08421 on Uncertainty Management in Information Systems(pp. -). (Dagstuhl Seminar Proceedings; No. 08421). Dagstuhl; Germany: Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik … Koch; C.; Re; C.; Olteanu; D.; Lenz; HJ; Haas; PJ; Pan; JZ/ 08421 Working Group: Report of the Probabilistic Databases Benchmarking … Proceedingsof Dagstuhl Seminar 08421 on Uncertainty Management in Information Systems. ed. / C.Koch; B. König-Ries; V. Markl; Maurice van Keulen. Dagstuhl; Germany : Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik; 2009. p. - (Dagstuhl Seminar Proceedings; No. 08421) …Koch; C; Re; C; Olteanu; D; Lenz; HJ; Haas; PJ & Pan; JZ 2009; 08421 Working Group …,Uncertainty Management in Information Systems: Dagstuhl Seminar 08421,2009,*
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Narasimhaiah Gorla; Yan Wah Lam,A longstanding vision in distributed systems is to build reliable systems from unreliablecomponents. An enticing formulation of this vision is Byzantine fault-tolerant (BFT) statemachine replication; in which a group of servers collectively act as a correct server even ifsome of the servers misbehave or malfunction in arbitrary (" Byzantine") ways. Despite thispromise; practitioners hesitate to deploy BFT systems at least partly because of theperception that BFT must impose high overheads.In this article; we present Zyzzyva; aprotocol that uses speculation to reduce the cost of BFT replication. In Zyzzyva; replicasreply to a client's request without first running an expensive three-phase commit protocol toagree on the order to process requests. Instead; they optimistically adopt the order proposedby a primary server; process the request; and reply immediately to the client. If the primary …,Communications of the ACM,2008,*
Systems aspects of probabilistic data management,Magdalena Balazinska; Christopher Ré; Dan Suciu,Abstract There has been a wide interest recently in managing probabilistic data [1; 2; 3; 4; 5;6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26]. But in order tofollow the rich literature on probabilistic databases one is often required to take a detour intoprobability theory; correlations; conditionals; Monte Carlo simulations; error bounds; topicsthat have been studied extensively in several areas of Computer Science and Mathematics.Because of that; it is often difficult to get to the algorithmic and systems level aspects ofprobabilistic data management. In this tutorial; we will distill these aspects from the; oftentheory-heavy literature on probabilistic databases. We will start by describing a realapplication at the University of Washington; using the RFID Ecosystem; we will show howprobabilities arise naturally; and why we need to cope with them. We will then describe …,Proceedings of the VLDB Endowment,2008,*
Managing Probabilistic Data with MystiQ: The Can-Do; the Could-Do; and the Can't-Do (Invited Talk),C Re; D Suciu,*,LECTURE NOTES IN COMPUTER SCIENCE,2008,*
An interview with Jan Paredaens,Jan Van den Bussche; Dirk Van Gucht,Jan Paredaens; one of the most influential members of the database theory community; hasreached the age of sixty. On this occasion; we have interviewed him about his researchcareer; the services he has done to the research community; what drives him; and his viewof the future. The interview happened in July 2007 and took place at Indiana University;Bloomington; a place which Jan has visited on numerous occasions to work with Dirk VanGucht; getting inspiration; finding topics to work on.,Jan Paredaens is turning 60 on the first of October; 2007. On the occasion of this birthday; we held a symposium at the University of Antwerp; on Friday 21 September 2007. The present book contains some of the scientific papers that were presented at the symposium; but contains also additional papers; written by scientific,2007,*
Orderings on Annotated Collections,Christopher Re; Dan Suciu; Val Tannen,We dedicate this contribution to Jan Paredaens on the occasion of his 60th birthday. Formany years Jan has been a wonderful scientific mentor and collaborator; an exceptionalhost and a valuable friend to both of us. Jan; many thanks and we wish you many happyreturns!,Jan Paredaens is turning 60 on the first of October; 2007. On the occasion of this birthday; we held a symposium at the University of Antwerp; on Friday 21 September 2007. The present book contains some of the scientific papers that were presented at the symposium; but contains also additional papers; written by scientific,2007,*
Materialized Views in Probabilistic Databases,Christopher Ré; Dan Suciu,ABSTRACT Views over probabilistic data contain correlations between tuples; and thecurrent approach is to capture these correlations using explicit lineage. In this paper wepropose an alternative approach to materializing probabilistic views; by giving conditionsunder which a view can be represented by a block-independent disjoint (BID) table. Not allviews can be represented as BID tables and so we propose a novel partial representationthat can represent all views but may not define a unique probability distribution. We thengive conditions on when a query's value on a partial representation will be uniquely defined.We apply our theory to two applications: query processing using views and informationexchange using views. In query processing on probabilistic data; we can ignore the lineageand use materialized views to more efficiently answer queries. By contrast; if the view has …,*,2007,*
Robust Statistics in IceCube Initial Muon Reconstruction,Mark Wellons; B Recht; C Ré,Abstract In the IceCube Neutrino Detector; muon tracks are reconstructed from the muon'slight emission. The initial track “linefit” reconstruction serves as a starting point for moresophisticated track fitting; using detailed knowledge of the ice and the detector. The newapproach described here leads to a substantial improvement of the accuracy in the initialtrack reconstruction for muons. Our approach is to couple simple physical models withrobust statistical techniques. Using the metric of median angular accuracy; a standard metricfor track reconstruction; this solution improves the accuracy in the reconstructed direction by13%,*,*,*
Global implementation of genomic medicine: We are not alone,C RE,Around the world; innovative genomic-medicine programs capitalize on singular capabilitiesarising from local health care systems; cultural or political milieus; and unusual selected riskalleles or disease burdens. Such individual ef orts might benef t from the sharing ofapproaches and lessons learned in other locales. The US National Human GenomeResearch Institute and the National Academy of Medicine recently brought together 25 ofthese groups to compare projects; to examine the current state of implementation anddesired near-term capabilities; and to identify opportunities for collaboration that promote theresponsible practice of genomic medicine. Ef orts to coalesce these groups around concretebut compelling signature projects should accelerate the responsible implementation ofgenomic medicine in ef orts to improve clinical care worldwide.,*,*,*
DIMACS Workshop on Distributed Optimization; Information Processing; and Learning,Slides Video; Michael Alan Chang; Amir Daneshmand; Mark Eisen; Shripad Gade; Konstantinos Gatsis; Davood Hajinezhad; Ai Kagawa; Fatemeh Kazemikordasiabi; Alec Koppel; Fatemeh Mansoori; Aryan Mokhtari; Accelerated Distributed Nesterov Gradient Descent; Guannan Qu; Haroon Raja; Ran Xin; Zhixiong Yang; Distributed Large-scale Optimization via Batch,Contacting the Center Document last modified on August 31; 2017.,*,*,*
Similarity-based LSTMs for Time Series Representation Learning in the Presence of Structured Covariates,Madalina Fiterau; Jason Fries; Eni Halilaj; Nopphon Siranart; Suvrat Bhooshan; Christopher Ré,Abstract Time series are prevalent in biomedical data; from high-frequency vital signals andlongitudinal health indicators to gait kinematics and activity monitoring. One typicalapproach for the analysis of this data; frequently encountered in the biomedical literature; isto extract a set of features by applying tools such as PCA. Such general purpose techniquesare insufficient to yield a complete view of time series; creating a need for more salientrepresentations; in a setting where; often; only a limited amount of labels and annotationsare available for this purpose. To address this issue; we introduce a deep learningframework which takes structured covariates into account when learning time seriesrepresentations. The lower layers of the deep architecture include an embedding whichcaptures parameters specific to the time series; such as frequency and amplitude; early in …,*,*,*
Efficient Evaluation of HAVING Queries on a Probabilistic Database University of Washington TR:# 2007-06-01,Christopher Ré; Dan Suciu,Abstract. We study the evaluation of positive conjunctive queries with Boolean aggregatetests (similar to HAVING queries in SQL) on probabilistic databases. Our motivation is tohandle aggregate queries over imprecise data resulting from information integration orinformation extraction. More precisely; we study conjunctive queries with predicateaggregates using MIN; MAX; COUNT; SUM; AVG or COUNT (DISTINCT) on probabilisticdatabases. Computing the precise output probabilities for positive conjunctive queries(without HAVING) is♯ P-hard; but is in P for a restricted class of queries called safe queries.Further; for queries without self-joins either a query is safe or its data complexity is♯ P-Hard;which shows that safe queries exactly capture tractable queries without self-joins. In thispaper; for each aggregate above; we find a class of queries that exactly capture efficient …,*,*,*
Scrambling to Close the Isotope Gap,C RE,Two reactors; one in the Netherlands and the other in Canada; produce 60% of the world9sradioactive molybdenum-99; which decays into technetium-99; a radioisotope used in morethan 30 million procedures a year worldwide for imaging everything from blood flow throughthe heart to bone cancer—and both reactors are decades beyond their intended lifeexpectancy. The situation isn9t just a problem for doctors and patients. Governments aroundthe world are working to phase out civilian uses of the technology to produce nearly all Mo-99 today because of concerns that the highly radioactive material used in the process couldbe diverted to make nuclear weapons. And finding replacement technologies to produce theMo-99; and companies willing to take the financial risk of generating it; is provingchallenging.,*,*,*
Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,Divy Agrawal; Magdalena Balazinska; Michael Cafarella; Michael Jordan; Tim Kraska; Raghu Ramakrishnan; Christopher Ré,Machine learning seems to be eating the world with a new breed of high-value data-drivenapplications in image analysis; search; voice recognition; mobile; and office productivityproducts. To paraphrase Mike Stonebraker; machine learning is no longer a zero-billion-dollar business. As the home of high-value; data-driven applications for over four decades; anatural question for database researchers to ask is: what role should the databasecommunity play in these new datadriven machine-learning-based applications? The last fewyears have seen increasing crossover between database research and machine learning.But is this crossover a wise choice for database research? What are the opportunities andthe costs of this approach to industry; to the future of database research; and to academics?Do database researchers have something to contribute to this trend? These two areas …,*,*,*
GYM: A Multiround Join Algorithm In MapReduce And Its Analysis,Foto Afrati; Manas Joglekar; Christopher Re; Semih Salihoglu; Jeffrey Ullman,ABSTRACT We study the problem of computing the join of n relations in multiple rounds ofMapReduce. We introduce a distributed and generalized version of Yannakakis's algorithm;called GYM. GYM takes as input any generalized hypertree decomposition (GHD) of a queryof width w and depth d; and computes the query in O (d+ log (n)) rounds and O (n (INw+OUT) 2,*,*,*
Felix: Exploiting Specialized Subtasks in Markov Logic Networks for Higher Efficiency and Quality,Feng Niu; Ce Zhang; Chris Ré; Jude Shavlik,Page 1. Template provided by: “posters4research.com” Felix: Exploiting Specialized Subtasksin Markov Logic Networks for Higher Efficiency and Quality Feng Niu; Ce Zhang; Chris Ré; andJude Shavlik University of Wisconsin-Madison Executive Summary Felix at Work SpecializedSubtasks in Felix ❖Machine Reading (MR) requires joint inference ➢Desirable to have a versatile;easy-to-use language ❖Markov logic network (MLN) is such a language ➢But current approachto inference is monolithic: one algorithm for entire program ➢Suboptimal scalability and quality❖Key observation: MLNs in MR may contain subtasks ➢Eg; NER; coreference; link prediction;etc. ❖Felix hypothesis: We can get higher scale and quality by exploiting those subtasks ➢Idea:solve subtasks with specialized algorithms ➢Preliminary results show dramatic improvement inboth efficiency and quality Example Plan in Felix Experiments …,*,*,*
Data Engineering,Eric Gribkoff; Dan Suciu; Guy Van den Broeck; Christopher Ré; Amir Abbas Sadeghian; Zifei Shan; Jaeho Shin; Feiran Wang; Sen Wu; Ce Zhang; Matthias Boehm; Douglas R Burdick; Alexandre V Evfimievski; Berthold Reinwald; Frederick R Reiss; Prithviraj Sen; Shirish Tatikonda; Yuanyuan Tian; Botong Huang; Nicholas WD Jarrett; Shivnath Babu; Sayan Mukherjee; Jun Yang,Modern knowledge bases such as Yago [14]; DeepDive [19]; and Google's Knowledge Vault[6] are constructed from large corpora of text by using some form of supervised informationextraction. The extracted data usually starts as a large probabilistic database; then itsaccuracy is improved by adding domain knowledge expressed as hard or soft constraints.Finally; the knowledge base can be queried using some general-purpose query language(SQL; or Sparql). A key technical challenge during the construction; refinement; andquerying of knowledge bases is probabilistic reasoning. Because of the size of the datainvolved; probabilistic reasoning in knowledge bases becomes a central data managementproblem. The number of random variables is very large; typically one for each fact in theknowledge base. Most systems today perform inference by using Markov Chain Monte …,*,*,*
Xixuan (Aaron) Feng xfeng@ cs. wisc. edu,Xixuan Feng; Arun Kumar; Ben Recht; Christopher Ré; Fei Chen; Christopher Ré; Min Wang; JM Hellerstein; C Ré; F Schoppmann; DZ Wang; E Fratkin; A Gorajek; KS Ng; C Welton; K Li; A Kumar; EMC Greenplum; CA San Mateo,Page 1. Xixuan (Aaron) Feng xfeng@cs.wisc.edu +1 (608) 695-9606 / +86 13466622880 1210W. Dayton St. Madison; WI 53706 Education University of Wisconsin-Madison Madison; WI MSin Computer Sciences (Databases); GPA: 3.9/4.0 Aug. 2013 (Expected) Certificate in TechnicalJapanese; GPA: 4.0/4.0 May. 2013 (Expected) Peking University Beijing; China BS (With Honor)in Computer Science and Technology Jul. 2009 GPA: 3.7/4.0; Major GPA: 3.9/4.0; Rank 5 outof 120+ Stanford University Stanford; CA Summer Session Undergraduate Student 2008 (3-month)GPA: 4.3/4.3; 1 out of 3 participants from Peking University Hong Kong University of Science andTechnology Hong Kong; China Exchange Student in Computer Engineering 2006 (4-month) GPA:11.82/12.00; 1 out of 10 participants referred by Peking University Selected Publications Towardsa Unified Architecture for in-RDBMS Analytics …,*,*,*
Commoditizing Big Data Analytics for the Enterprise: Experiences with a Recommendation System,Arun Kumar; Christopher Ré; Vaishnavi Sashikanth; Anand Srinivasan,*,*,*,*
Mathematical and Computational Framework for Matrix Completion with Nonuniform Sampling in Resource Constrained Environments,HA Schmitt; B Recht; C Ré,ABSTRACT Matrix completion concerns the problem of recovering a low rank matrix from agiven small fraction of its entries. It is a recurring problem in collaborative filtering;dimensionality reduction; and multi-class learning and has a long history in mathematics.While the general problem of finding the lowest rank matrix satisfying a set of equalityconstraints is NP-hard; there are quite general settings where it is possible to perfectlyrecover all of the missing entries of a low-rank matrix by solving a convex optimizationproblem. One of us (Recht) has shown how this convex programming heuristic can be usedto reconstruct most× matrices of rank from most collections of entries; provided that thenumber of entries exceeds log 2 for some small; positive numerical constant. This workextended mathematical results from compressive sensing; in particular building upon its …,*,*,*
WS-MEMBERSHIP,Werner Vogels; Chris Re,ABSTRACT An important factor in the successful deployment of federated web-services-based business activities will be the ability to guarantee reliable distributed operation andexecution. Failure management is essential for any reliable distributed operation butespecially for the target areas of web-services; where the activities can be constructed out ofservices located at different enterprises; and are accessed over heterogeneous networkstopologies. This paper describes ws-membership; a coordination service that provides ageneric web-service interface for tracking registered web-services and for providingmembership monitoring information. A prototype membership service based on epidemicprotocol techniques has been implemented and is described in detail in this paper. Thespecification and implementation have been developed in the context of the Huygens …,*,*,*
A Demonstration of ScenicPeex Through a Digital Diary Application,Nodira Khoussainova; Evan Welbourne; Magdalena Balazinska; Gaetano Borriello; Julie Letchner; Christopher Ré; Dan Suciu; Jordan Walke,Abstract ScenicPeex is a system that provides RFID-based pervasive computingapplications with an infrastructure for specifying; extracting and managing meaningful high-level events from raw RFID data. Scenic allows users to specify events of interest using agraphical interface and an intuitive visual language whereas PEEX effectively extracts theseevents from data in spite of the unreliability of RFID technology and the inherent ambiguity inevent extraction.,*,*,*
UW TR:# TR08-07-01,Julie Letchner; Christopher Ré; Magdalena Balazinska; Matthai Philipose,Abstract Model-based views have recently been proposed as an effective method forquerying noisy sensor data. Commonly used models from the AI literature (eg; the HiddenMarkov Model) expose to applications a stream of probabilistic and correlated stateestimates computed from the sensor data. Many applications want to detect sophisticatedpatterns of states from these Markovian streams. Such queries are called event queries. Inthis paper; we present a new system; Caldera; for processing event queries over storedMarkovian streams. At the heart of our system is a set of access methods for Markovianstreams that can improve event query performance by orders of magnitude compared toexisting techniques; which must scan the entire stream. These access methods use newadaptations of traditional B+ tree indexes; and a new index; called the Markov-chain …,*,*,*
Materialized Views in Probabilistic Databases For Information Exchange and Query Optimization (Full Version) University of Washington Technical Report# TR2007-...,Christopher Ré; Dan Suciu,Abstract Views over probabilistic data contain correlations between tuples; and the currentapproach is to capture these correlations using explicit lineage. In this paper we propose analternative approach to materializing probabilistic views; by giving conditions under which aview can be represented by a block-independent disjoint (BID) table. Not all views can berepresented as BID tables and so we propose a novel partial representation that canrepresent all views but may not define a unique probability distribution. We then giveconditions on when a query's value on a partial representation will be uniquely defined. Weapply our theory to two applications: query processing using views and informationexchange using views. In query processing on probabilistic data; we can ignore the lineageand use materialized views to more efficiently answer queries. By contrast; if the view has …,*,*,*
UW TR:# TR2008-03-02,Christopher Ré; Dan Suciu,Abstract In probabilistic databases; lineage is fundamental to both query processing andunderstanding the data. Current systems sa Trio or Mystiq use a complete approach inwhich the lineage for a tuple t is a Boolean formula which represents all derivations of t. Inlarge databases lineage formulas can become huge: in one public database (the GeneOntology) we often observed 10MB of lineage (provenance) data for a single tuple. In thispaper we propose to use approximate lineage; which is a much smaller formula keepingtrack of only the most important derivations; which the system can use to process queriesand provide explanations. We discuss in detail two specific kinds of approximate lineage:(1)a conservative approximation called sufficient lineage that records the most importantderivations for each tuple; and (2) polynomial lineage; which is more aggressive and can …,*,*,*
General Database Statistics Using Entropy Maximization TR: 2009-05-01,Raghav Kaushik; Christopher Ré; Dan Suciu,Abstract We propose a framework in which query sizes can be estimated from arbitrarystatistical assertions on the data. In its most general form; a statistical assertion states thatthe size of the output of a conjunctive query over the data is a given number. A very simpleexample is a histogram; which makes assertions about the sizes of the output of severalrange queries. Our model also allows much more complex assertions that include joins andprojections. To model such complex statistical assertions we propose to use the Entropy-Maximization (EM) probability distribution. In this model any set of statistics that is consistenthas a precise semantics; and every query has an precise size estimate. We show thatseveral classes of statistics can be solved in closed form.,*,*,*
