Data-intensive text processing with MapReduce,Jimmy Lin; Chris Dyer,Abstract Our world is being revolutionized by data-driven methods: access to large amountsof data has generated new insights and opened exciting new opportunities in commerce;science; and computing applications. Processing the enormous quantities of data necessaryfor these advances requires large clusters; making distributed computing paradigms morecrucial than ever. MapReduce is a programming model for expressing distributedcomputations on massive datasets and an execution framework for large-scale dataprocessing on clusters of commodity servers. The programming model provides an easy-to-understand abstraction for designing scalable algorithms; while the execution frameworktransparently handles many system-level details; ranging from scheduling tosynchronization to fault tolerance. This book focuses on MapReduce algorithm design …,Synthesis Lectures on Human Language Technologies,2010,688
Searching for SNPs with cloud computing,Ben Langmead; Michael C Schatz; Jimmy Lin; Mihai Pop; Steven L Salzberg,As DNA sequencing outpaces improvements in computer speed; there is a critical need toaccelerate tasks like alignment and SNP calling. Crossbow is a cloud-computing softwaretool that combines the aligner Bowtie and the SNP caller SOAPsnp. Executing in parallelusing Hadoop; Crossbow analyzes data comprising 38-fold coverage of the human genomein three hours using a 320-CPU cluster rented from a cloud computing service for about $85.Crossbow is available from http://bowtie-bio. sourceforge. net/crossbow/.,Genome Biology,2009,460
Web question answering: Is more always better?,Susan Dumais; Michele Banko; Eric Brill; Jimmy Lin; Andrew Ng,Abstract This paper describes a question answering system that is designed to capitalize onthe tremendous amount of data that is now available online. Most question answeringsystems use a wide variety of linguistic resources. We focus instead on the redundancyavailable in large corpora as an important resource. We use this redundancy to simplify thequery rewrites that we need to use; and to support answer mining from returned snippets.Our system performs quite well given the simplicity of the techniques being utilized.Experimental results show that question answering accuracy can be greatly improved byanalyzing more and more matching passages. Simple passage ranking and n-gramextraction techniques work well in our system making it efficient to use with many backendretrieval engines.,SIGIR,2002,406
Cloud computing and information policy: Computing in a policy cloud?,Paul T Jaeger; Jimmy Lin; Justin M Grimes,ABSTRACT Cloud computing is a computing platform that resides in a large data center andis able to dynamically provide servers with the ability to address a wide range of needs; fromscientific research to e-commerce. The provision of computing resources as if it were a utilitysuch as electricity; while potentially revolutionary as a computing service; presents manymajor problems of information policy; including issues of privacy; security; reliability; access;and regulation. This article explores the nature and potential of cloud computing; the policyissues raised; and research questions related to cloud computing and policy. Ultimately; thepolicy issues raised by cloud computing are examined as a part of larger issues of publicpolicy attempting to respond to rapid technological evolution.,Journal of Information Technology & Politics,2008,390
Quantitative evaluation of passage retrieval algorithms for question answering,Stefanie Tellex; Boris Katz; Jimmy Lin; Aaron Fernandes; Gregory Marton,Abstract Passage retrieval is an important component common to many question answeringsystems. Because most evaluations of question answering systems focus on end-to-endperformance; comparison of common components becomes difficult. To address thisshortcoming; we present a quantitative evaluation of various passage retrieval algorithms forquestion answering; implemented in a framework called Pauchok. We present threeimportant findings: Boolean querying schemes perform well in the question answering task.The performance differences between various passage retrieval algorithms vary with thechoice of document retriever; which suggests significant interactions between documentretrieval and passage retrieval. The best algorithms in our evaluation employ density-basedmeasures for scoring query terms. Our results reveal future directions for passage …,SIGIR,2003,375
Data-intensive question answering,Eric Brill; Jimmy Lin; Michele Banko; Susan Dumais; Andrew Ng,Microsoft Research Redmond participated for the first time in TREC this year; focusing onthe question answering track. There is a separate report in this volume on the MicrosoftResearch Cambridge submissions for the filtering and Web tracks (Robertson et al.; 2002).We have been exploring data-driven techniques for Web question answering; and modifiedour system somewhat for participation in TREC QA. We submitted two runs for the main QAtrack (AskMSR and AskMSR2). Data-driven methods have proven to be powerful techniquesfor natural language processing. It is still unclear to what extent this success can beattributed to specific techniques; versus simply the data itself. For example; Banko and Brill(2001) demonstrated that for confusion set disambiguation; a prototypical disambiguation-in-string-context problem; the amount of data used far dominates the learning method …,TREC,2001,373
Overview of the TREC 2007 Question Answering Track,Hoa Trang Dang; Diane Kelly; Jimmy Lin,Abstract The TREC 2007 question answering (QA) track contained two tasks: the main taskconsisting of series of factoid; list; and “Other” questions organized around a set of targets;and the complex; interactive question answering (ciQA) task. The main task differed fromprevious years in that the document collection comprised blogs in addition to newswiredocuments; requiring systems to process diverse genres of unstructured text. The evaluationof factoid and list responses distinguished between answers that were globally correct (withrespect to the document collection); and those that were only locally correct (with respect tothe supporting document but not to the overall document collection). The ciQA task provideda framework for participants to investigate interaction in the context of complex informationneeds. Standing in for surrogate users; assessors interacted with QA systems live over …,TREC,2007,314
WTF: The Who to Follow service at Twitter,Pankaj Gupta; Ashish Goel; Jimmy Lin; Aneesh Sharma; Dong Wang; Reza Zadeh,Abstract WTF (" Who to Follow") is Twitter's user recommendation service; which isresponsible for creating millions of connections daily between users based on sharedinterests; common connections; and other related factors. This paper provides anarchitectural overview and shares lessons we learned in building and running the serviceover the past few years. Particularly noteworthy was our design decision to process theentire Twitter graph in memory on a single server; which significantly reduced architecturalcomplexity and allowed us to develop and deploy the service in only a few months. At thecore of our architecture is Cassovary; an open-source in-memory graph processing enginewe built from scratch for WTF. Besides powering Twitter's user recommendations; Cassovaryis also used for search; discovery; promoted products; and other services as well. We …,WWW,2013,260
Evaluation of PICO as a knowledge representation for clinical questions,Xiaoli Huang; Jimmy Lin; Dina Demner-Fushman,Abstract The paradigm of evidence-based medicine (EBM) recommends that physiciansformulate clinical questions in terms of the problem/population; intervention; comparison;and outcome. Together; these elements comprise a PICO frame. Although this frameworkwas developed to facilitate the formulation of clinical queries; the ability of PICO structures torepresent physicians' information needs has not been empirically investigated. This paperevaluates the adequacy and suitability of PICO frames as a knowledge representation byanalyzing 59 real-world primary-care clinical questions. We discovered that only twoquestions in our corpus contain all four PICO elements; and that 37% of questions containboth intervention and outcome. Our study reveals prevalent structural patterns for the fourtypes of clinical questions: therapy; diagnosis; prognosis; and etiology. We found that the …,AMIA annual symposium proceedings,2006,246
Design patterns for efficient graph algorithms in MapReduce,Jimmy Lin; Michael Schatz,Abstract Graphs are analyzed in many important contexts; including ranking search resultsbased on the hyperlink structure of the world wide web; module detection of proteinproteininteraction networks; and privacy analysis of social networks. Many graphs of interest aredifficult to analyze because of their large size; often spanning millions of vertices and billionsof edges. As such; researchers have increasingly turned to distributed solutions. Inparticular; MapReduce has emerged as an enabling technology for large-scale graphprocessing. However; existing best practices for MapReduce graph algorithms havesignificant shortcomings that limit performance; especially with respect to partitioning;serializing; and distributing the graph. In this paper; we present three design patterns thataddress these issues and can be used to accelerate a large class of graph algorithms …,Proceedings of the Eighth Workshop on Mining and Learning with Graphs,2010,225
Pairwise document similarity in large collections with MapReduce,Tamer Elsayed; Jimmy Lin; Douglas W. Oard,Abstract This paper presents a MapReduce algorithm for computing pairwise documentsimilarity in large document collections. MapReduce is an attractive framework because itallows us to decompose the inner products involved in computing document similarity intoseparate multiplication and summation stages in a way that is well matched to efficient diskaccess patterns across several machines. On a collection consisting of approximately900;000 newswire articles; our algorithm exhibits linear growth in running time and space interms of the number of documents.,ACL,2008,224
Answering clinical questions with knowledge-based and statistical techniques,Dina Demner-Fushman; Jimmy Lin,The combination of recent developments in question-answering research and theavailability of unparalleled resources developed specifically for automatic semanticprocessing of text in the medical domain provides a unique opportunity to explore complexquestion answering in the domain of clinical medicine. This article presents a systemdesigned to satisfy the information needs of physicians practicing evidence-based medicine.We have developed a series of knowledge extractors; which employ a combination ofknowledge-based and statistical techniques; for automatically identifying clinically relevantaspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithmthat scores the relevance of citations with respect to structured representations of informationneeds; in accordance with the principles of evidence-based medicine. Starting with an …,Computational Linguistics,2007,224
Overview of the TREC-2011 Microblog Track,Iadh Ounis; Craig Macdonald; Jimmy Lin; Ian Soboroff,The Microblog track examines search tasks and evaluation methodologies for informationseeking behaviours in microblogging environments such as Twitter. It was first introduced in2011; addressing a real-time adhoc search task; whereby the user wishes to see the mostrecent but relevant information to the query. In particular; systems should respond to a queryby providing a list of relevant tweets ordered from newest to oldest; starting from the time thequery was issued. For TREC 2011; we used the newly-created Tweets2011 corpus. Thecorpus is comprised of 16M tweets over approximately two weeks; sampled courtesy ofTwitter. The corpus is designed to be a reusable; representative sample of the twittersphere.As the reusability of a test collection is paramount in a TREC track; these sampled tweetscan be obtained at any point in time (subjected to some caveats; discussed below). To …,Proceedings of the 20th Text REtrieval Conference (TREC 2011),2011,203
PubMed related articles: A probabilistic topic-based model for content similarity,Jimmy Lin; W John Wilbur,We present a probabilistic topic-based model for content similarity called pmra that underliesthe related article search feature in PubMed. Whether or not a document is about a particulartopic is computed from term frequencies; modeled as Poisson distributions. Unlike previousprobabilistic retrieval models; we do not attempt to estimate relevance–but rather our focusis,BMC bioinformatics,2007,182
Omnibase: Uniform access to heterogeneous data for question answering,Boris Katz; Sue Felshin; Deniz Yuret; Ali Ibrahim; Jimmy Lin; Gregory Marton; Alton Jerome McFarland; Baris Temelkuran,Abstract Although the World Wide Web contains a tremendous amount of information; thelack of uniform structure makes finding the right knowledge difficult. A solution is to turn theWeb into a “virtual database” and to access it through natural language. We built Omnibase;a system that integrates heterogeneous data sources using an object-property-value model.With the help of Omnibase; our Start natural language system can now access numerousheterogeneous data sources on the Web in a uniform manner; and answers millions of userquestions with high precision.,International Conference on Application of Natural Language to Information Systems,2002,156
Large-scale machine learning at Twitter,Jimmy Lin; Alek Kolcz,Abstract The success of data-driven solutions to difficult problems; along with the droppingcosts of storing and processing massive amounts of data; has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machinelearning tools into its existing Hadoop-based; Pig-centric analytics platform. We begin withan overview of this platform; which handles" traditional" data warehousing and businessintelligence tasks for the organization. The core of this work lies in recent Pig extensions toprovide predictive analytics capabilities that incorporate machine learning; focusedspecifically on supervised classification. In particular; we have identified stochastic gradientdescent techniques for online learning and ensemble methods as being highly amenable toscaling out to large amounts of data. In our deployed solution; common machine learning …,Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,2012,147
Earlybird: Real-time search at Twitter,Michael Busch; Krishna Gade; Brian Larson; Patrick Lok; Samuel Luckenbill; Jimmy Lin,The web today is increasingly characterized by social and real-time signals; which webelieve represent two frontiers in information retrieval. In this paper; we present Early bird;the core retrieval engine that powers Twitter's real-time search service. Although Early birdbuilds and maintains inverted indexes like nearly all modern retrieval engines; its indexstructures differ from those built to support traditional web search. We describe thesedifferences and present the rationale behind our design. A key requirement of real-timesearch is the ability to ingest content rapidly and make it searchable immediately; whileconcurrently supporting low-latency; high-throughput query evaluation. These demands aremet with a single-writer; multiple-reader concurrency model and the targeted use of memorybarriers. Early bird represents a point in the design space of real-time search engines that …,2012 IEEE 28th International Conference on Data Engineering,2012,144
Extracting structural paraphrases from aligned monolingual corpora,Ali Ibrahim; Boris Katz; Jimmy Lin,Abstract We present an approach for automatically learning paraphrases from alignedmonolingual corpora. Our algorithm works by generalizing the syntactic paths betweencorresponding anchors in aligned sentence pairs. Compared to previous work; structuralparaphrases generated by our algorithm tend to be much longer on average; and arecapable of capturing long-distance dependencies. In addition to a standalone evaluation ofour paraphrases; we also describe a question answering application currently underdevelopment that could immensely benefit from automatically-learned structuralparaphrases.,Proceedings of the second international workshop on Paraphrasing-Volume 16,2003,140
Question answering from the web using knowledge annotation and knowledge mining techniques,Jimmy Lin; Boris Katz,Abstract We present a strategy for answering fact-based natural language questions that isguided by a characterization of real-world user queries. Our approach; implemented in asystem called Aranea; extracts answers from the Web using two different techniques:knowledge annotation and knowledge mining. Knowledge annotation is an approach toanswering large classes of frequently occurring questions by utilizing semi\-structured andstructured Web sources. Knowledge mining is a statistical approach that leverages massiveamounts of Web data to overcome many natural language processing challenges. We haveintegrated these two different paradigms into a question answering system capable ofproviding users with concise answers that directly address their information needs.,Proceedings of the twelfth international conference on Information and knowledge management,2003,139
Information network or social network? The structure of the Twitter follow graph,Seth A Myers; Aneesh Sharma; Pankaj Gupta; Jimmy Lin,Abstract In this paper; we provide a characterization of the topological features of the Twitterfollow graph; analyzing properties such as degree distributions; connected components;shortest path lengths; clustering coefficients; and degree assortativity. For each of theseproperties; we compare and contrast with available data from other social networks. Theseanalyses provide a set of authoritative statistics that the community can reference. Inaddition; we use these data to investigate an often-posed question: Is Twitter a socialnetwork or an information network? The" follow" relationship in Twitter is primarily aboutinformation consumption; yet many follows are built on social ties. Not surprisingly; we findthat the Twitter follow graph exhibits structural characteristics of both an information networkand a social network. Going beyond descriptive characterizations; we hypothesize that …,Proceedings of the 23rd International Conference on World Wide Web,2014,135
Where is the cloud? Geography; economics; environment; and jurisdiction in cloud computing,Paul T Jaeger; Jimmy Lin; Justin M Grimes; Shannon N Simmons,Abstract Cloud computing–the creation of large data centers that can be dynamicallyprovisioned; configured; and reconfigured to deliver services in a scalable manner–placesenormous capacity and power in the hands of users. As an emerging new technology;however; cloud computing also raises significant questions about resources; economics; theenvironment; and the law. Many of these questions relate to geographical considerationsrelated to the data centers that underlie the clouds: physical location; available resources;and jurisdiction. While the metaphor of the cloud evokes images of dispersion; cloudcomputing actually represents centralization of information and computing resources in datacenters; raising the specter of the potential for corporate or government control overinformation if there is insufficient consideration of these geographical issues; especially …,First Monday,2009,129
Selectively using relations to improve precision in question answering,Boris Katz; Jimmy Lin,Abstract Despite the intuition that linguistically sophisti—cated techniques should bebeneﬁcial to question answering; real gains in performance have yet to be demonstratedempirically in a reliable man—ner. Systems built around sophisticated linguis—tic analysisgenerally perform worse than their linguistically—uninformed cousins. We believe that thekey to effective application of natural language processing technology is to selectivelyemploy it only when helpful; without abandoning simpler techniques. To this end; we identifytwo linguis—tic phenomena that current information extraction driven systems have difﬁcultywith; and demon—strate how syntactic processing can help. By in—dexing syntacticrelations that can be reliably ex—tracted from corpus text and matching questions withdocuments at the relation level; we demonstrate that syntactic analysis enables a …,Proceedings of the workshop on Natural Language Processing for Question Answering (EACL 2003),2003,129
What makes a good answer? The role of context in question answering,Jimmy Lin; Dennis Quan; Vineet Sinha; Karun Bakshi; David Huynh; Boris Katz; David R Karger,Abstract: Question answering systems have proven to be helpful to users because they canprovide succinct answers that do not require users to wade through a large number ofdocuments. However; despite recent advances in the underlying question answeringtechnology; the problem of designing effective interfaces has been largely unexplored. Weconducted a user study to investigate this area and discovered that; overall; users preferparagraph-sized chunks of text over just an exact phrase as the answer to their questions.Furthermore; users generally prefer answers embedded in context; regardless of theperceived reliability of the source documents. When researching a topic; increasing theamount of text returned to users significantly decreases the number of queries that they poseto the system; suggesting that users utilize supporting text to answer related questions …,Proceedings of the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2003),2003,121
Scaling big data mining infrastructure: The Twitter experience,Jimmy Lin; Dmitriy Ryaboy,Abstract The analytics platform at Twitter has experienced tremendous growth over the pastfew years in terms of size; complexity; number of users; and variety of use cases. In thispaper; we discuss the evolution of our infrastructure and the development of capabilities fordata mining on" big data". One important lesson is that successful big data mining in practiceis about much more than what most academics would consider data mining: life" in thetrenches" is occupied by much preparatory work that precedes the application of data miningalgorithms and followed by substantial effort to turn preliminary models into robust solutions.In this context; we discuss two topics: First; schemas play an important role in helping datascientists understand petabyte-scale data stores; but they're insufficient to provide anoverall" big picture" of the data available to generate insights. Second; we observe that a …,ACM SIGKDD Explorations Newsletter,2013,120
Multi-candidate reduction: Sentence compression as a tool for document summarization tasks,David Zajic; Bonnie J Dorr; Jimmy Lin; Richard Schwartz,Abstract This article examines the application of two single-document sentence compressiontechniques to the problem of multi-document summarization—a “parse-and-trim” approachand a statistical noisy-channel approach. We introduce the multi-candidate reduction (MCR)framework for multi-document summarization; in which many compressed candidates aregenerated for each source sentence. These candidates are then selected for inclusion in thefinal summary based on a combination of static and dynamic features. Evaluationsdemonstrate that sentence compression is a valuable component of a larger multi-documentsummarization framework.,Information Processing & Management,2007,118
Event structure and the encoding of arguments: the syntax of the Mandarin and English verb phrase,Jimmy Lin,This work presents a theory of linguistic representation that attempts to capture the syntacticstructure of verbs and their arguments. My framework is based on the assumption that theproper representation of argument structure is event structure. Furthermore; I develop thehypothesis that event structure is syntactic structure; and argue that verb meanings arecompositionally derived in the syntax from verbalizing heads; functional elements thatlicense eventive interpretations; and verbal roots; abstract concepts drawn fromencyclopedic knowledge. The overall goal of the enterprise is to develop a theory that isable to transparently relate the structure and meaning of verbal arguments. By hypothesis;languages share the same inventory of primitive building blocks and are governed by thesame set of constraints--all endowed by principles of Universal Grammar and subjected …,*,2004,113
Overview of the TREC-2014 Microblog Track,Jimmy Lin; Miles Efron; Yulu Wang; Garrick Sherman,Abstract: This year represents the fourth iteration of the TREC Microblog track; which hasbeen running since 2011. The track continued using the evaluation as a service model [8; 7];in which participants had access to the document collection only through an API. In additionto the temporally-anchored ad hoc retrieval task; which has been running since the inceptionof the track; we introduced a new task called tweet timeline generation (TTG); where the goalis to produce concise summaries about a particular topic for human consumption. Althoughthis overview covers both tasks; more emphasis is placed on the tweet timeline generationtask; which necessitated the development of a new evaluation methodology. We refer thereader to previous track overview papers [8; 12; 9] for details on the setup of the ad hoc task.Descriptors:* DATA MINING;* INFORMATION RETRIEVAL;* RANK ORDER STATISTICS …,*,2014,112
Smoothing techniques for adaptive online language models: topic tracking in tweet streams,Jimmy Lin; Rion Snow; William Morgan,Abstract We are interested in the problem of tracking broad topics such as" baseball" and"fashion" in continuous streams of short texts; exemplified by tweets from the microbloggingservice Twitter. The task is conceived as a language modeling problem where per-topicmodels are trained using hashtags in the tweet stream; which serve as proxies for topiclabels. Simple perplexity-based classifiers are then applied to filter the tweet stream fortopics of interest. Within this framework; we evaluate; both intrinsically and extrinsically;smoothing techniques for integrating" foreground" models (to capture recency) and"background" models (to combat sparsity); as well as different techniques for retaininghistory. Experiments show that unigram language models smoothed using a normalizedextension of stupid backoff and a simple queue for history retention performs well on the …,Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,2011,112
Answering Definition Questions Using Multiple Knowledge Sources,Wesley Hildebrandt; Boris Katz; Jimmy Lin,Abstract Definition questions represent a largely unexplored area of question answering—they are different from factoid questions in that the goal is to return as many relevant“nuggets” of information about a concept as possible. We describe a multi-strategy approachto answering such questions using a database constructed offline with surface patterns; aWebbased dictionary; and an off-the-shelf document retriever. Results are presented fromcomponent-level evaluation and from an endto-end evaluation of our implemented system atthe TREC 2003 Question Answering Track.,Proceedings of the 2004 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2004),2004,110
Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce,Jimmy Lin,Abstract This paper explores the problem of computing pairwise similarity on documentcollections; focusing on the application of" more like this" queries in the life sciences domain.Three MapReduce algorithms are introduced: one based on brute force; a second where theproblem is treated as large-scale ad hoc retrieval; and a third based on the Cartesianproduct of postings lists. Each algorithm supports one or more approximations that tradeeffectiveness for efficiency; the characteristics of which are studied experimentally. Resultsshow that the brute force algorithm is the most efficient of the three when exact similarity isdesired. However; the other two algorithms support approximations that yield large efficiencygains without significant loss of effectiveness.,Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,2009,108
MapReduce is good enough? If all you have is a hammer; throw away everything that's not a nail!,Jimmy Lin,Abstract Hadoop is currently the large-scale data analysis “hammer” of choice; but thereexist classes of algorithms that aren't “nails” in the sense that they are not particularlyamenable to the MapReduce programming model. To address this; researchers haveproposed MapReduce extensions or alternative programming models in which thesealgorithms can be elegantly expressed. This article espouses a very different position: thatMapReduce is “good enough;” and that instead of trying to invent screwdrivers; we shouldsimply get rid of everything that's not a nail. To be more specific; much discussion in theliterature surrounds the fact that iterative algorithms are a poor fit for MapReduce. Thesimple solution is to find alternative; noniterative algorithms that solve the same problem.This article captures my personal experiences as an academic researcher as well as a …,Big Data,2013,98
Overview of the TREC-2012 Microblog Track,Ian Soboroff; Iadh Ounis; Craig Macdonald; Jimmy Lin,The Microblog track examines search tasks and evaluation methodologies for informationseeking behaviours in microblogging environments such as Twitter. It was first introduced in2011; addressing a real-time adhoc search task; whereby the user wishes to see the mostrecent relevant information to the query. In 2012; the realtime adhoc task was changedslightly; and a new filtering task was added. The filtering task models a standing querywhere the user wishes to see relevant tweets as they are posted. For the second year of thetrack; we reused the Tweets2011 corpus; described below. The corpus is comprised of 16Mtweets distributed over two weeks; sampled courtesy of Twitter. The corpus was designed tobe a reusable; representative sample of the twittersphere–ie; both important and spamtweets were included. As the reusability of a test collection is paramount in TREC; we …,TREC,2012,95
Answer extraction; semantic clustering; and extractive summarization for clinical question answering,Dina Demner-Fushman; Jimmy Lin,Abstract This paper presents a hybrid approach to question answering in the clinical domainthat combines techniques from summarization and information retrieval. We tackle afrequently-occurring class of questions that takes the form" What is the best drug treatmentfor X?" Starting from an initial set of MEDLINE citations; our system first identifies the drugsunder study. Abstracts are then clustered using semantic classes from the UMLS ontology.Finally; a short extractive summary is generated for each abstract to populate the clusters.Two evaluations---a manual one focused on short answers and an automatic one focusedon the supporting abstract---demonstrate that our system compares favorably to PubMed;the search system most widely used by physicians today.,Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,2006,94
Web-scale computer vision using MapReduce for multimedia data mining,Brandyn White; Tom Yeh; Jimmy Lin; Larry Davis,Abstract This work explores computer vision applications of the MapReduce framework thatare relevant to the data mining community. An overview of MapReduce and common designpatterns are provided for those with limited MapReduce background. We discuss both thehigh level theory and the low level implementation for several computer vision algorithms:classifier training; sliding windows; clustering; bag-of-features; background subtraction; andimage registration. Experimental results for the k-means clustering and single Gaussianbackground subtraction algorithms are performed on a 410 node Hadoop cluster.,Proceedings of the Tenth International Workshop on Multimedia Data Mining,2010,90
A cascade ranking model for efficient ranked retrieval,Lidan Wang; Jimmy Lin; Donald Metzler,Abstract There is a fundamental tradeoff between effectiveness and efficiency whendesigning retrieval models for large-scale document collections. Effectiveness tends toderive from sophisticated ranking functions; such as those constructed using learning torank; while efficiency gains tend to arise from improvements in query evaluation and cachingstrategies. Given their inherently disjoint nature; it is difficult to jointly optimize effectivenessand efficiency in end-to-end systems. To address this problem; we formulate and develop anovel cascade ranking model; which unlike previous approaches; can simultaneouslyimprove both top k ranked effectiveness and retrieval efficiency. The model constructs acascade of increasingly complex ranking functions that progressively prunes and refines theset of candidate documents to minimize retrieval latency and maximize result set quality …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,89
Multi-perspective sentence similarity modeling with convolutional neural networks,Hua He; Kevin Gimpel; Jimmy Lin,Abstract Modeling sentence similarity is complicated by the ambiguity and variability oflinguistic expression. To cope with these challenges; we propose a model for comparingsentences that uses a multiplicity of perspectives. We first model each sentence using aconvolutional neural network that extracts features at multiple levels of granularity and usesmultiple types of pooling. We then compare our sentence representations at severalgranularities using multiple similarity metrics. We apply our model to three tasks; includingthe Microsoft Research paraphrase identification task and two SemEval semantic textualsimilarity tasks. We obtain strong performance on all tasks; rivaling or exceeding the state ofthe art without using external resources such as WordNet or parsers.,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,86
Fast data in the era of big data: Twitter's real-time related query suggestion architecture,Gilad Mishne; Jeff Dalton; Zhenghua Li; Aneesh Sharma; Jimmy Lin,Abstract We present the architecture behind Twitter's real-time related query suggestion andspelling correction service. Although these tasks have received much attention in the websearch literature; the Twitter context introduces a real-time" twist": after significant breakingnews events; we aim to provide relevant results within minutes. This paper provides a casestudy illustrating the challenges of real-time data processing in the era of" big data". We tellthe story of how our system was built twice: our first implementation was built on a typicalHadoop-based analytics stack; but was later replaced because it did not meet the latencyrequirements necessary to generate meaningful real-time results. The secondimplementation; which is the system deployed in production today; is a custom in-memoryprocessing engine specifically designed for the task. This experience taught us that the …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,84
The curse of Zipf and limits to parallelization: A look at the stragglers problem in MapReduce,Jimmy Lin,ABSTRACT This paper explores the problem of “stragglers” in Map-Reduce: a commonphenomenon where a small number of mappers or reducers takes significantly longer thanthe others to complete. The effects of these stragglers include unnecessarily long wall-clockrunning times and sub-optimal cluster utilization. In many cases; this problem cannot simplybe attributed to hardware idiosyncrasies; but is rather caused by the Zipfian distribution ofinput or intermediate data. I present a simple theoretical model that shows how suchdistributions impose a fundamental limit on the amount of parallelism that can be extractedfrom a large class of algorithms where all occurrences of the same element must beprocessed together. A case study in parallel ad hoc query evaluation highlights the severityof the stragglers problem. Fortunately; a simple modification of the input data cuts end-to …,7th Workshop on Large-Scale Distributed Systems for Information Retrieval,2009,84
REXTOR: a system for generating relations from natural language,Boris Katz; Jimmy Lin,Abstract This paper argues that a finite-state language model with a ternary expressionrepresentation is currently the most practical and suitable bridge between natural languageprocessing and information retrieval. Despite the theoretical computational inadequacies offinite-state grammars; they are very cost effective (in time and space requirements) andadequate for practical purposes. The ternary expressions that we use are not onlylinguistically-motivated; but also amenable to rapid large-scale indexing. REXTOR(Relations EXtracTOR) is an implementation of this model; in one uniform framework; thesystem provides two separate grammars for extracting arbitrary patterns of text and buildingternary expressions from them. These content representational structures serve as the inputto our ternary expressions indexer. This approach to natural language information …,Proceedings of the ACL-2000 workshop on Recent advances in natural language processing and information retrieval: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 11,2000,81
Learning to efficiently rank,Lidan Wang; Jimmy Lin; Donald Metzler,Abstract It has been shown that learning to rank approaches are capable of learning highlyeffective ranking functions. However; these approaches have mostly ignored the importantissue of efficiency. Given that both efficiency and effectiveness are important for real searchengines; models that are optimized for effectiveness may not meet the strict efficiencyrequirements necessary to deploy in a production environment. In this work; we present aunified framework for jointly optimizing effectiveness and efficiency. We propose new metricsthat capture the tradeoff between these two competing forces and devise a strategy forautomatically learning models that directly optimize the tradeoff metrics. Experimentsindicate that models learned in this way provide a good balance between retrievaleffectiveness and efficiency. With specific loss functions; learned models converge to …,Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,2010,78
The Web as a Resource for Question Answering: Perspectives and Challenges,Jimmy Lin,Abstract The vast amounts of information readily available on the World Wide Web can beeffectively used for question answering in two fundamentally different ways. In the federatedapproach; techniques for handling semistructured data are applied to access Web sourcesas if they were databases; allowing large classes of common questions to be answereduniformly. In the distributed approach; largescale text-processing techniques are used toextract answers directly from unstructured Web documents. Because the Web is orders ofmagnitude larger than any human-collected corpus; question answering systems cancapitalize on its unparalleled-levels of data redundancy. Analysis of real-world userquestions reveals that the federated and distributed approaches complement each othernicely; suggesting a hybrid approach in future question answering systems.,LREC,2002,77
Integrating Web-based and Corpus-based Techniques for Question Answering.,Boris Katz; Jimmy Lin; Daniel Loreto; Wesley Hildebrandt; Matthew Bilotti; Sue Felshin; Aaron Fernandes; Gregory Marton; Federico Mora,MIT CSAIL's entry in this year's TREC Question Answering track focused on integrating Web-based techniques with more traditional strategies based on document retrieval and named-entity detection. We believe that achieving high performance in the question answering taskrequires a combination of multiple strategies designed to capitalize on differentcharacteristics of various resources. The system we deployed for the TREC evaluation lastyear relied exclusively on the World Wide Web to answer factoid questions (Lin et al.; 2002).The advantages that the Web offers are well known and have been exploited by previoussystems (Brill et al.; 2001; Clarke et al.; 2001; Dumais et al.; 2002). The immense amount offreely available unstructured text provides data redundancy; which can be leveraged withsimple pattern matching techniques involving the expected answer formulations. In many …,TREC,2003,76
An exploration of the principles underlying redundancy-based factoid question answering,Jimmy Lin,Abstract The so-called “redundancy-based” approach to question answering represents asuccessful strategy for mining answers to factoid questions such as “Who shot AbrahamLincoln&quest;” from the World Wide Web. Through contrastive and ablation experimentswith Aranea; a system that has performed well in several TREC QA evaluations; this workexamines the underlying assumptions and principles behind redundancy-based techniques.Specifically; we develop two theses: that stable characteristics of data redundancy allowfactoid systems to rely on external “black box” components; and that despite embodying adata-driven approach; redundancy-based methods encode a substantial amount ofknowledge in the form of heuristics. Overall; this work attempts to address the broaderquestion of “what really matters” and to provide guidance for future researchers.,ACM Transactions on Information Systems (TOIS),2007,75
What works better for question answering: Stemming or morphological query expansion?,Matthew W Bilotti; Boris Katz; Jimmy Lin,ABSTRACT How do different information retrieval techniques affect the performance ofdocument retrieval in the context of question answering? An exploration of this question isour overall research goal. In this paper; we specifically examine strategies for coping withmorphological variation. This work quantitatively compares two different approaches tohandling term variation: applying a stemming algorithm at indexing time; and performingmorphological query expansion at retrieval time. We discovered that; compared to the no-stemming baseline; stemming results in lower recall; and morphological expansion yieldshigher recall. By separately weighting different term variants; we were able to achieve evenhigher recall; which opens the door to interesting question analysis algorithms forsophisticated query generation. Another significant contribution of our work is the …,Proceedings of the Information Retrieval for Question Answering (IR4QA) Workshop at SIGIR,2004,75
Automatically evaluating answers to definition questions,Jimmy Lin; Dina Demner-Fushman,Abstract Following recent developments in the automatic evaluation of machine translationand document summarization; we present a similar approach; implemented in a measurecalled Pourpre; for automatically evaluating answers to definition questions. Until now; theonly way to assess the correctness of answers to such questions involves manualdetermination of whether an information nugget appears in a system's response. The lack ofautomatic methods for scoring system output is an impediment to progress in the field; whichwe address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracksindicate that rankings produced by our metric correlate highly with official rankings; and thatPourpre outperforms direct application of existing metrics.,Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,2005,74
The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine,Jimmy Lin; Dina Demner-Fushman,Abstract Despite its intuitive appeal; the hypothesis that retrieval at the level of" concepts"should outperform purely term-based approaches remains unverified empirically. Inaddition; the use of" knowledge" has not consistently resulted in performance gains. Afteridentifying possible reasons for previous negative results; we present a novel framework for"conceptual retrieval" that articulates the types of knowledge that are important for informationseeking. We instantiate this general framework in the domain of clinical medicine based onthe principles of evidence-based medicine (EBM). Experiments show that an EBM-basedscoring algorithm dramatically outperforms a state-of-the-art baseline that employs only termstatistics. Ablation studies further yield a better understanding of the performancecontributions of different components. Finally; we discuss how other domains can benefit …,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,72
Will pyramids built of nuggets topple over?,Jimmy Lin; Dina Demner-Fushman,Abstract The present methodology for evaluating complex questions at TREC analyzesanswers in terms of facts called" nuggets". The official F-score metric represents theharmonic mean between recall and precision at the nugget level. There is an implicitassumption that some facts are more important than others; which is implemented in abinary split between" vital" and" okay" nuggets. This distinction holds important implicationsfor the TREC scoring model---essentially; systems only receive credit for retrieving vitalnuggets---and is a source of evaluation instability. The upshot is that for many questions inthe TREC testsets; the median score across all submitted runs is zero. In this work; weintroduce a scoring model based on judgments from multiple assessors that captures a morerefined notion of nugget importance. We demonstrate on TREC 2003; 2004; and 2005 …,Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,2006,71
Summingbird: A framework for integrating batch and online MapReduce computations,Oscar Boykin; Sam Ritchie; Ian O'Connell; Jimmy Lin,Abstract Summingbird is an open-source domain-specific language implemented in Scalaand designed to integrate online and batch MapReduce computations in a singleframework. Summingbird programs are written using dataflow abstractions such as sources;sinks; and stores; and can run on different execution platforms: Hadoop for batch processing(via Scalding/Cascading) and Storm for online processing. Different execution modesrequire different bindings for the dataflow abstractions (eg; HDFS files or message queuesfor the source) but do not require any changes to the program logic. Furthermore;Summingbird can operate in a hybrid processing mode that transparently integrates batchand online results to efficiently generate up-to-date aggregations over long time spans. Thelanguage was designed to improve developer productivity and address pain points in …,Proceedings of the VLDB Endowment,2014,70
Fast; easy; and cheap: Construction of statistical machine translation models with MapReduce,Christopher Dyer; Aaron Cordova; Alex Mont; Jimmy Lin,Abstract In recent years; the quantity of parallel training data available for statistical machinetranslation has increased far more rapidly than the performance of individual computers;resulting in a potentially serious impediment to progress. Parallelization of the model-building algorithms that process this data on computer clusters is fraught with challengessuch as synchronization; data exchange; and fault tolerance. However; the MapReduceprogramming paradigm has recently emerged as one solution to these issues: a powerfulfunctional abstraction hides system-level details from the researcher; allowing programs tobe transparently distributed across potentially very large clusters of commodity hardware.We describe MapReduce implementations of two algorithms used to estimate theparameters for two word alignment models and one phrase-based translation model; all …,Proceedings of the Third Workshop on Statistical Machine Translation,2008,70
Recounting the courts? Applying automated content analysis to enhance empirical legal research,Michael Evans; Wayne McIntosh; Jimmy Lin; Cynthia Cates,Abstract Political scientists in general and public law specialists in particular have onlyrecently begun to exploit text classification using machine learning techniques to enable thereliable and detailed content analysis of political/legal documents on a large scale. Thisarticle provides an overview and assessment of this methodology. We describe the basics oftext classification; suggest applications of the technique to enhance empirical legal research(and political science more broadly); and report results of experiments designed to test thestrengths and weaknesses of alternative approaches for classifying the positions andinterpreting the content of advocacy briefs submitted to the US Supreme Court. We find thatthe Wordscores method (introduced by Laver et al. 2003); and various models using a NaïveBayes classifier; perform well at accurately classifying the ideological direction of amicus …,Journal of Empirical Legal Studies,2007,69
Comparative Evaluation of a Natural Language Dialog Based System and a Menu Driven System for Information Access: a Case Study,Joyce Chai; Jimmy Lin; Wlodek Zadrozny; Yiming Ye; Margo Budzikowska; Veronika Horvath; Nanda Kambhatla; Catherine Wolf,Abstract This paper describes the evaluation of a natural language dialog based navigationsystem (HappyAssistant) that helps users access e-commerce sites to find relevantinformation about products and services. The prototype system leverages technologies innatural language processing and human computer interaction to create a faster and moreintuitive way of interacting with websites; especially for the less experienced users. Theresult of a comparative study shows that users prefer the natural language enablednavigation two to one over the menu driven navigation. In addition; the study confirmed theefficiency of using natural language dialog in terms of the number of clicks and the amountof time required to obtain the relevant information. In the case study; comparing to the menudriven system; the average number of clicks used in the natural language system was …,Content-Based Multimedia Information Access-Volume 2,2000,67
Askmsr: Question answering using the worldwide web,Michele Banko; Eric Brill; Susan Dumais; Jimmy Lin,*,Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases,2002,66
Building a reusable test collection for question answering,Jimmy Lin; Boris Katz,Abstract In contrast to traditional information retrieval systems; which return ranked lists ofdocuments that users must manually browse through; a question answering system attemptsto directly answer natural language questions posed by the user. Although such systemspossess language-processing capabilities; they still rely on traditional document retrievaltechniques to generate an initial candidate set of documents. In this article; the authorsargue that document retrieval for question answering represents a task different fromretrieving documents in response to more general retrospective information needs. Thus; toguide future system development; specialized question answering test collections must beconstructed. They show that the current evaluation resources have major shortcomings; toremedy the situation; they have manually created a small; reusable question answering …,Journal of the Association for Information Science and Technology,2006,65
Generative content models for structural analysis of medical abstracts,Jimmy Lin; Damianos Karakos; Dina Demner-Fushman; Sanjeev Khudanpur,Abstract The ability to accurately model the content structure of text is important for manynatural language processing applications. This paper describes experiments withgenerative models for analyzing the discourse structure of medical abstracts; whichgenerally follow the pattern of" introduction";" methods";" results"; and" conclusions". Wedemonstrate that Hidden Markov Models are capable of accurately capturing the structure ofsuch texts; and can achieve classification accuracy comparable to that of discriminativetechniques. In addition; generative approaches provide advantages that may make thempreferable to discriminative techniques such as Support Vector Machines under certainconditions. Our work makes two contributions: at the application level; we report goodperformance on an interesting task in an important domain; more generally; our results …,Proceedings of the Workshop on Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis,2006,63
On building a reusable Twitter corpus,Richard McCreadie; Ian Soboroff; Jimmy Lin; Craig Macdonald; Iadh Ounis; Dean McCullough,Abstract The Twitter real-time information network is the subject of research for informationretrieval tasks such as real-time search. However; so far; reproducible experimentation onTwitter data has been impeded by restrictions imposed by the Twitter terms of service. In thispaper; we detail a new methodology for legally building and distributing Twitter corpora;developed through collaboration between the Text REtrieval Conference (TREC) andTwitter. In particular; we detail how the first publicly available Twitter corpus-referred to asTweets2011-was distributed via lists of tweet identifiers and specialist tweet crawlingsoftware. Furthermore; we analyse whether this distribution approach remains robust overtime; as tweets in the corpus are removed either by users or Twitter itself. Tweets2011 wassuccessfully used by 58 participating groups for the TREC 2011 Microblog track; while …,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,62
The unified logging infrastructure for data analytics at Twitter,George Lee; Jimmy Lin; Chuang Liu; Andrew Lorek; Dmitriy Ryaboy,Abstract In recent years; there has been a substantial amount of work on large-scale dataanalytics using Hadoop-based platforms running on large clusters of commodity machines.A less-explored topic is how those data; dominated by application logs; are collected andstructured to begin with. In this paper; we present Twitter's production logging infrastructureand its evolution from application-specific logging to a unified" client events" log format;where messages are captured in common; well-formatted; flexible Thrift messages. Sincemost analytics tasks consider the user session as the basic unit of analysis; we pre-materialize" session sequences"; which are compact summaries that can answer a largeclass of common queries quickly. The development of this infrastructure has streamlined logcollection and data analysis; thereby improving our ability to rapidly experiment and …,Proceedings of the VLDB Endowment,2012,62
You Are Where You Edit: Locating Wikipedia Contributors through Edit Histories.,Michael D Lieberman; Jimmy J Lin,Abstract Whether knowingly or otherwise; Wikipedia contributors reveal their interests andexpertise through their contribution patterns. An analysis of Wikipedia edit histories showsthat it is often possible to associate contributors with relatively small geographic regions;usually corresponding to where they were born or where they presently live. For manycontributors; the geographic coordinates of pages they have edited are tightly clustered.Results suggest that a wealth of information about contributors can be gleaned from edithistories. This illustrates the efficacy of data mining on large; publicly-available datasets andraises potential privacy concerns.,ICWSM,2009,59
Knowledge extraction for clinical question answering: Preliminary results,Dina Demner-Fushman; Jimmy Lin,*,Proceedings of the AAAI-05 Workshop on Question Answering in Restricted Domains,2005,58
Single-document and multi-document summarization techniques for email threads using sentence compression,David M Zajic; Bonnie J Dorr; Jimmy Lin,Abstract We present two approaches to email thread summarization: collective messagesummarization (CMS) applies a multi-document summarization approach; while individualmessage summarization (IMS) treats the problem as a sequence of single-documentsummarization tasks. Both approaches are implemented in our general framework driven bysentence compression. Instead of a purely extractive approach; we employ linguistic andstatistical methods to generate multiple compressions; and then select from thosecandidates to produce a final summary. We demonstrate these ideas on the Enron emailcollection–a very challenging corpus because of the highly technical language.Experimental results point to two findings: that CMS represents a better approach to emailthread summarization; and that current sentence compression techniques do not improve …,Information Processing & Management,2008,57
Of Ivory and Smurfs: Loxodontan MapReduce experiments for web search,Jimmy Lin; Donald Metzler; Tamer Elsayed; Lidan Wang,Abstract: This paper describes Ivory; an attempt to build a distributed retrieval system aroundthe open-source Hadoop implementation of MapReduce. We focus on three noteworthyaspects of our work: a retrieval architecture built directly on the Hadoop Distributed FileSystem (HDFS); a scalable Map-Reduce algorithm for inverted indexing; and webpageclassification to enhance retrieval effectiveness. Descriptors:* DISTRIBUTED DATAPROCESSING;* INFORMATION RETRIEVAL;* DATA MANAGEMENT; METADATA; HIGHLEVEL ARCHITECTURE; CLIENT SERVER SYSTEMS; SYMPOSIA; ALGORITHMS,*,2009,56
The role of context in question answering systems,Jimmy Lin; Dennis Quan; Vineet Sinha; Karun Bakshi; David Huynh; Boris Katz; David R Karger,Abstract Despite recent advances in natural language question an-swering technology; theproblem of designing effective user interfaces has been largely unexplored. We conducted auser study to investigate the problem and discovered that overall; users prefer a paragraph-sized chunk of text over just an exact phrase as the answer to their questions. Fur-thermore;users generally prefer answers embedded in con-text; regardless of the perceived reliabilityof the source documents. When users research a topic; increasing the amount of textreturned to users significantly decreases the number of queries that they pose to the system;suggesting that users utilize supporting text to answer related ques-tions. We believe thatthese results can serve to guide future developments in question answering user interfaces.,CHI'03 extended abstracts on Human factors in computing systems,2003,55
Extracting answers from the web using knowledge annotation and knowledge mining techniques,Jimmy Lin; Aaron Fernandes; Boris Katz; Gregory Marton; Stefanie Tellex,Abstract: Aranea is a question answering system that extracts answers from the World WideWeb using knowledge annotation and knowledge mining techniques. Knowledgeannotation; which utilizes semistructured database techniques; is effective for answeringlarge classes of commonly occurring questions. Knowledge mining; which utilizes statisticaltechniques; can leverage the massive amounts of data available on the Web to overcomemany natural language processing challenges. Aranea integrates these two differentparadigms of question answering into a single framework. For the TREC evaluation; we alsoexplored the problem of answer projection; or finding supporting documents for our Web-derived answers from the AQUAINT corpus. Descriptors:* INFORMATION RETRIEVAL;*KNOWLEDGE BASED SYSTEMS; DATA BASES; STATISTICAL PROCESSES …,*,2006,54
Is searching full text more effective than searching abstracts?,Jimmy Lin,With the growing availability of full-text articles online; scientists and other consumers of thelife sciences literature now have the ability to go beyond searching bibliographic records(title; abstract; metadata) to directly access full-text content. Motivated by this emerging trend;I posed the following question: is searching full text more effective than searching abstracts?This question is answered by comparing text retrieval algorithms on MEDLINE® abstracts;full-text articles; and spans (paragraphs) within full-text articles using data from the TREC2007 genomics track evaluation. Two retrieval models are examined: bm25 and the rankingalgorithm implemented in the open-source Lucene search engine. Experiments show thattreating an entire article as an indexing unit does not consistently yield higher effectivenesscompared to abstract-only search. However; retrieval based on spans; or paragraphs …,BMC bioinformatics,2009,53
Answering Questions about Moving Objects in Surveillance Videos.,Boris Katz; Jimmy J Lin; Chris Stauffer; W Eric L Grimson,Abstract Current question answering systems succeed in many respects regardingquestions about textual documents. However; information exists in other media; whichprovides both opportunities and challenges for question answering. We present results inextending question answering capabilities to video footage captured in a surveillancesetting. Our prototype system; called Spot; can answer questions about moving objects thatappear within the video. We situate this novel application of vision and language technologywithin a larger framework designed to integrate language and vision systems under acommon representation. We believe that our framework will support the next generation ofmultimodal natural language information access systems.,New directions in question answering,2003,52
Natural language annotations for the Semantic Web,Boris Katz; Jimmy Lin; Dennis Quan,Abstract Because the ultimate purpose of the Semantic Web is to help users locate;organize; and process information; we strongly believe that it should be grounded in theinformation access method humans are most comfortable with—natural language. However;the Resource Description Framework (RDF); the foundation of the Semantic Web; wasdesigned to be easily processed by computers; not humans. To render RDF friendlier tohumans; we propose to augment it with natural language annotations; or metadata written ineveryday language. We argue that natural language annotations are not only intuitive andeffective; but can also accelerate the pace with which the Semantic Web is being adopted.We demonstrate the use of natural language annotations from within Haystack; an end userSemantic Web platform that also serves as a testbed for our ideas. In addition to a …,OTM Confederated International Conferences" On the Move to Meaningful Internet Systems",2002,51
Overview of the TREC 2006 ciQA task,Diane Kelly; Jimmy Lin,Abstract Growing interest in interactive systems for answering complex questions lead to thedevelopment of the complex; interactive QA (ciQA) task; introduced for the first time at TREC2006. This paper describes the rationale and design of the ciQA task and the evaluationresults. Thirty complex relationship questions based on five question templates wereinvestigated using the AQUAINT collection of newswire text. Interaction forms were theprimary vehicle for defining and capturing user-system interactions. In total; six groupsparticipated in the ciQA task and contributed ten different sets of interaction forms. Therewere two main findings: baseline IR techniques are competitive for complex QA andinteraction; at least as defined and implemented in this evaluation; did not appear to improveperformance by much.,ACM SIGIR Forum,2007,47
Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents.,Alan R Aronson; Dina Demner-Fushman; Susanne M Humphrey; Jimmy J Lin; Patrick Ruch; Miguel E Ruiz; Lawrence H Smith; Lorraine K Tanabe; W John Wilbur; Hongfang Liu,Abstract This paper represents a continuation of research into the retrieval and annotation oftextual genomics documents (both MEDLINE® citations and full text articles) for the purposeof satisfying biologists' real information needs. The overall approach taken here for both thead hoc retrieval and categorization tasks within the TREC genomics track in 2005 was onecombining the results of several NLP; statistical and ML methods; using a fusion method forad hoc retrieval and ensemble methods for categorization. The results show that fusionapproaches can improve the final outcome for the ad hoc and the categorization tasks; butthat care must be taken in order to take advantage of the strengths of the constituentmethods.,TREC,2005,47
Full-text indexing for optimizing selection operations in large-scale data analytics,Jimmy Lin; Dmitriy Ryaboy; Kevin Weil,Abstract MapReduce; especially the Hadoop open-source implementation; has recentlyemerged as a popular framework for large-scale data analytics. Given the explosion ofunstructured data begotten by social media and other web-based applications; we take theposition that any modern analytics platform must support operations on free-text fields as first-class citizens. Toward this end; this paper addresses one inefficient aspect of Hadoop-based processing: the need to perform a full scan of the entire dataset; even in cases whereit is clearly not necessary to do so. We show that it is possible to leverage a full-text index tooptimize selection operations on text fields within records. The idea is simple and intuitive:the full-text index informs the Hadoop execution engine which compressed data blockscontain query terms of interest; and only those data blocks are decompressed and …,Proceedings of the second international workshop on MapReduce and its applications,2011,44
Pairwise word interaction modeling with deep neural networks for semantic similarity measurement,Hua He; Jimmy Lin,Abstract Textual similarity measurement is a challenging problem; as it requiresunderstanding the semantics of input sentences. Most previous neural network models usecoarse-grained sentence modeling; which has difficulty capturing fine-grained word-levelinformation for semantic comparisons. As an alternative; we propose to explicitly modelpairwise word interactions and present a novel similarity focus mechanism to identifyimportant correspondences for better similarity measurement. Our ideas are implemented ina novel neural network architecture that demonstrates state-ofthe-art accuracy on threeSemEval tasks and two answer selection tasks.,Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2016,42
How do users find things with PubMed? Towards automatic utility evaluation with user simulations,Jimmy Lin; Mark D Smucker,Abstract In the context of document retrieval in the biomedical domain; this paper exploresthe complex relationship between the quality of initial query results and the overall utility ofan interactive retrieval system. We demonstrate that a content-similarity browsing tool cancompensate for poor retrieval results; and that the relationship between retrievalperformance and overall utility is non-linear. Arguments are advanced with user simulations;which characterize the relevance of documents that a user might encounter with differentbrowsing strategies. With broader implications to IR; this work provides a case study of howuser simulations can be exploited as a formative tool for automatic utility evaluation.Simulation-based studies provide researchers with an additional evaluation tool tocomplement interactive and Cranfield-style experiments.,Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,2008,42
Scaling populations of a genetic algorithm for job shop scheduling problems using MapReduce,Di-Wei Huang; Jimmy Lin,Inspired by Darwinian evolution; a genetic algorithm (GA) approach is one popular heuristicmethod for solving hard problems such as the Job Shop Scheduling Problem (JSSP); whichis one of the hardest problems lacking efficient exact solutions today. It is intuitive that thepopulation size of a GA may greatly affect the quality of the solution; but it is unclear what arethe effects of having population sizes that are significantly greater than typical experiments.The emergence of MapReduce; a framework running on a cluster of computers that aims toprovide large-scale data processing; offers great opportunities to investigate this issue. Inthis paper; a GA is implemented to scale the population using MapReduce. Experiments areconducted on a large cluster; and population sizes up to 10^ 7 are inspected. It is shown thatlarger population sizes not only tend to yield better solutions; but also require fewer …,Cloud Computing Technology and Science (CloudCom); 2010 IEEE Second International Conference on,2010,41
The START Multimedia Information System: Current Technology and Future Directions.,Boris Katz; Jimmy J Lin; Sue Felshin,Abstract To address the problem of information overload in today's world; we havedeveloped Start; a natural language question answering system that provides users withhigh-precision multimedia information access through the use of natural languageannotations. To address the difficulty of accessing large amounts of heterogeneous data; wehave developed Omnibase; which assists Start by integrating structured and semistructuredWeb databases into a single; uniformly structured “virtual database.” Our ultimate goal is todevelop a computer system that acts like a “smart reference librarian;” and we believe wehave laid a firm foundation for achieving our goal. This paper describes our currentimplemented system and discusses future research directions.,Multimedia Information Systems,2002,41
Methods for automatically evaluating answers to complex questions,Jimmy Lin; Dina Demner-Fushman,Abstract Evaluation is a major driving force in advancing the state of the art in languagetechnologies. In particular; methods for automatically assessing the quality of machineoutput is the preferred method for measuring progress; provided that these metrics havebeen validated against human judgments. Following recent developments in the automaticevaluation of machine translation and document summarization; we present a similarapproach; implemented in a measure called P OURPRE; an automatic technique forevaluating answers to complex questions based on n-gram co-occurrences betweenmachine output and a human-generated answer key. Until now; the only way to assess thecorrectness of answers to such questions involves manual determination of whether aninformation “nugget” appears in a system's response. The lack of automatic methods for …,Information Retrieval,2006,40
The role of a natural language conversational interface in online sales: a case study,Joyce Chai; Jimmy Lin; Wlodek Zadrozny; Yiming Ye; Margo Stys-Budzikowska; Veronika Horvath; Nanda Kambhatla; Catherine Wolf,Abstract This paper describes the evaluation of a natural language dialog-based navigationsystem (HappyAssistant) that helps users access e-commerce sites to find relevantinformation about products and services. The prototype system leverages technologies innatural language processing and human-computer interaction to create a faster and moreintuitive way of interacting with websites; especially for less experienced users. The result ofa comparative study shows that users prefer the natural language-enabled navigation two toone over the menu driven navigation. In addition; the study confirmed the efficiency of usingnatural language dialog in terms of the number of clicks and the amount of time required toobtain the relevant information. In the case study; as compared to the menu driven system;the average number of clicks used in the natural language system was reduced by 63.2 …,International Journal of Speech Technology,2001,40
No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity,Ferhan Ture; Tamer Elsayed; Jimmy Lin,Abstract This work explores the problem of cross-lingual pairwise similarity; where the task isto extract similar pairs of documents across two different languages. Solutions to thisproblem are of general interest for text mining in the multi-lingual context and have specificapplications in statistical machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from onelanguage into another; and then uses locality-sensitive hashing (LSH) to extract similarpairs. We show that effective cross-lingual pairwise similarity requires working with similaritythresholds that are much lower than in typical monolingual applications; making the problemquite challenging. We present a parallel; scalable MapReduce implementation of the sort-based sliding window algorithm; which is compared to a brute-force approach on German …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,39
Identification of user sessions with hierarchical agglomerative clustering,G Craig Murray; Jimmy Lin; Abdur Chowdhury,Abstract We introduce a novel approach to identifying Web search user sessions based onthe burstiness of users' activity. Our method is user-centered rather than population-centeredor system-centered and can be deployed in situations in which users choose to withholdpersonal content information. We adopt a hierarchical agglomerative clustering approachwith a stopping criterion that is statistically motivated by users' activities. An evaluationbased on extracts from AOL Search&&num; 8482; logs reveals that our algorithm achieves98% accuracy in identifying session boundaries compared to human judgments.,Proceedings of the Association for Information Science and Technology,2006,39
Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures,Nima Asadi; Jimmy Lin,Abstract This paper examines a multi-stage retrieval architecture consisting of a candidategeneration stage; a feature extraction stage; and a reranking stage using machine-learnedmodels. Given a fixed set of features and a learning-to-rank model; we explore effectiveness/efficiency tradeoffs with three candidate generation approaches: postings intersection withSvS; conjunctive query evaluation with WAND; and disjunctive query evaluation with WAND.We find no significant differences in end-to-end effectiveness as measured by NDCGbetween conjunctive and disjunctive WAND; but conjunctive query evaluation issubstantially faster. Postings intersection with SvS; while fast; yields substantially lower end-to-end effectiveness; suggesting that document and term frequencies remain important inthe initial ranking stage. These findings show that conjunctive WAND is the best overall …,Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,2013,38
Evaluation of NLP systems,Philip Resnik; Jimmy Lin,As the engineering branch of computational linguistics; natural language processing isconcerned with the creation of artifacts that accomplish tasks. The operative question inevaluating an NLP algorithm or system is therefore the extent to which it produces the resultsfor which it was designed. Because NLP encompasses an enormous range of differenttasks; each with its own particular criteria for assessing results; a single chapter onevaluation cannot hope to be comprehensive. In this chapter; therefore; we have selected anumber of basic issues; laying out some fundamental principles of NLP evaluation;describing several of the most common evaluation paradigms; and illustrating how theprinciples and paradigms apply in the context of two specific tasks; word sensedisambiguation and question answering. For a comprehensive treatment; we refer the …,The handbook of computational linguistics and natural language processing,2010,37
A paraphrase-based approach to machine translation evaluation,Grazia Russo-Lassner; Jimmy Lin; Philip Resnik,Abstract: We propose a novel approach to automatic machine translation evaluation basedon paraphrase identification. The quality of machine-generated output can be viewed as theextent to which the conveyed meaning matches the semantics of reference translations;independent of lexical and syntactic divergences. This idea is implemented ill linearregression models that attempt to capture human judgments of adequacy and fluency;based on features that have previously been shown to be effective for paraphraseidentification. We evaluated our model using the output of three different MT systems fromthe 2004 NIST Arabic-to-English MT evaluation. Results show that models employingparaphrase-based features correlate better with human judgments than models basedpurely on existing automatic MT metrics. Descriptors:* AUTOMATION;* SEMANTICS …,*,2005,37
Annotating the semantic web using natural language,Boris Katz; Jimmy Lin,Abstract Because the ultimate purpose of the Semantic Web is to help users better locate;organize; and process content; we believe that it should be grounded in the informationaccess method humans are most comfortable with---natural language. However; theResource Description Framework (RDF); the foundation of the Semantic Web; was designedto be easily processed by computers; not humans. To render RDF more friendly to humans;we propose to augment it with natural language annotations; or metadata written ineveryday language. We argue that natural language annotations; parsed into computer-readable representations; are not only intuitive and effective; but can also accelerate thepace with which the Semantic Web is being adopted. We believe that our technology canfacilitate a happy marriage between natural language technology and the Semantic Web …,Proceedings of the 2nd workshop on NLP and XML-Volume 17,2002,37
Evaluating real-time search over tweets,Ian Soboroff; Dean McCullough; Jimmy Lin; Craig Macdonald; Iadh Ounis; Richard McCreadie,Abstract Twitter1 offers a phenomenal platform for the social sharing of information. Wedescribe new resources that have been created in the context of the Text RetrievalConference (TREC) to support the academic study of Twitter as a real-time informationsource. We formalize an information seeking task—real-time search—and offer amethodology for measuring system effectiveness. At the TREC 2011 Microblog Track; 58research groups participated in the first ever evaluation of this task. We present data from theeffort to illustrate and support our methodology.,Proc. ICWSM,2012,36
A Study of" Churn" in Tweets and Real-Time Search Queries.,Jimmy J Lin; Gilad Mishne,Abstract The real-time nature of Twitter means that term distributions in tweets and in searchqueries change rapidly: the most frequent terms in one hour may look very different fromthose in the next. Informally; we call this phenomenon “churn”. Our interest in analyzingchurn stems from the perspective of real-time search. How do we “correctly” compute termstatistics; considering that the underlying distributions change rapidly? In this paper; wepresent an analysis of tweet and query churn on Twitter; as a first step to answering thisquestion. Analyses reveal interesting insights on the temporal dynamics of term distributionson Twitter and hold implications for the design of search systems.,ICWSM,2012,36
Scalable language processing algorithms for the masses: A case study in computing word co-occurrence matrices with MapReduce,Jimmy Lin,Abstract This paper explores the challenge of scaling up language processing algorithms toincreasingly large datasets. While cluster computing has been available in commercialenvironments for several years; academic researchers have fallen behind in their ability towork on large datasets. I discuss two barriers contributing to this problem: lack of a suitableprogramming model for managing concurrency and difficulty in obtaining access tohardware. Hadoop; an open-source implementation of Google's MapReduce framework;provides a compelling solution to both issues. Its simple programming model hides system-level details from the developer; and its ability to run on commodity hardware puts clustercomputing within the reach of many academic research groups. This paper illustrates thesepoints with a case study in building word cooccurrence matrices from large corpora. I …,Proceedings of the Conference on Empirical Methods in Natural Language Processing,2008,36
Crowdflow: Integrating machine learning with mechanical turk for speed-cost-quality flexibility,Alexander J Quinn; Benjamin B Bederson; Tom Yeh; Jimmy Lin,Abstract Humans and machines have competing strengths for tasks such as naturallanguage processing and image understanding. Whereas humans do these things naturallywith potentially high accuracy; machines offer greater speed and flexibility. CrowdFlow is ourtoolkit for a model for blending the two in order to attain tighter control over the inherenttradeoffs in speed; cost and quality. With CrowdFlow; humans and machines work togetherto do a set of tasks at a user-specified point in the tradeoff space. They work symbiotically;with the humans providing training data to the machine while the machine provides first cutresults to the humans to save effort in cases where the machine's answer was alreadycorrect. The CrowdFlow toolkit can be considered as a generalization of our other domain-specific efforts aimed at enabling cloud computing services using a variety of …,Better performance over iterations,2010,35
Noise-contrastive estimation for answer selection with deep neural networks,Jinfeng Rao; Hua He; Jimmy Lin,Abstract We study answer selection for question answering; in which given a question and aset of candidate answer sentences; the goal is to identify the subset that contains theanswer. Unlike previous work which treats this task as a straightforward pointwiseclassification problem; we model this problem as a ranking task and propose a pairwiseranking approach that can directly exploit existing pointwise neural network models as basecomponents. We extend the Noise-Contrastive Estimation approach with a triplet rankingloss function to exploit interactions in triplet inputs over the question paired with positive andnegative examples. Experiments on TrecQA and WikiQA datasets show that our approachachieves state-of-the-art effectiveness without the need for external knowledge sources orfeature engineering.,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,34
Gathering knowledge for a question answering system from heterogeneous information sources,Boris Katz; Jimmy Lin; Sue Felshin,Abstract Although vast amounts of information are available electronically today; no effectiveinformation access mechanism exists to provide humans with convenient informationaccess. A general; open-domain question answering system is a solution to this problem.We propose an architecture for a collaborative question answering system that contains fourprimary components: an annotations system for storing knowledge; a ternary expressionrepresentation of language; a transformational rule system for handling some complexitiesof language; and a collaborative mechanism by which ordinary users can contribute newknowledge by teaching the system new information. We have developed a initial prototype;called Webnotator; with which to test these ideas.,Proceedings of the workshop on Human Language Technology and Knowledge Management-Volume 2001,2001,34
Answering Multiple Questions on a Topic From Heterogeneous Resources.,Boris Katz; Matthew W Bilotti; Sue Felshin; Aaron Fernandes; Wesley Hildebrandt; Roni Katzir; Jimmy J Lin; Daniel Loreto; Gregory Marton; Federico Mora; Özlem Uzuner,MIT CSAIL's entry into this year's TREC Question Answering track focused on theconversational aspect of this year's task; on improving the coverage of our list and definitionsystems; and on an infrastructure to generalize our TREC-specific tools for other questionanswering tasks. While our overall architecture remained largely unchanged from last year;we have built on our strengths for each component: our web-based factoid engine wasadapted for input from a new web search engine; our list engine's knowledge baseexpanded from 150 to over 3000 lists; our definitional nugget extractor now has expandedand improved patterns with improved component precision and recall. Beyond their internalimprovements; these components were adapted to a larger conversational framework thatpassed information about the topic 1 to factoids and lists. Answer selection for definitional …,TREC,2004,33
Sticky notes for the semantic web,David R Karger; Boris Katz; Jimmy Lin; Dennis Quan,Abstract Computer-based annotation is increasing in popularity as a mechanism for revisingdocuments and sharing comments over the Internet. One reason behind this surge is thatviewpoints; summaries; and notes written by others are often helpful to readers. In particular;these types of annotations can help users locate or recall relevant documents. We believethat this model can be applied to the problem of retrieval on the Semantic Web. In this paper;we propose a generalized annotation environment that supports richer forms of descriptionsuch as natural language. We discuss how RDF can be used to model annotations and theconnections between annotations and the documents they describe. Furthermore; weexplore the idea of a question answering interface that allows retrieval based both on thetext of the annotations and the annotations associated metadata. Finally; we speculate on …,Proceedings of the 8th international conference on Intelligent user interfaces,2003,33
Temporal feedback for tweet search with non-parametric density estimation,Miles Efron; Jimmy Lin; Jiyin He; Arjen De Vries,Abstract This paper investigates the temporal cluster hypothesis: in search tasks where timeplays an important role; do relevant documents tend to cluster together in time? We explorethis question in the context of tweet search and temporal feedback: starting with an initial setof results from a baseline retrieval model; we estimate the temporal density of relevantdocuments; which is then used for result reranking. Our contributions lie in a method tocharacterize this temporal density function using kernel density estimation; with and withouthuman relevance judgments; and an approach to integrating this information into a standardretrieval model. Experiments on TREC datasets confirm that our temporal feedbackformulation improves search effectiveness; thus providing support for our hypothesis. Ourapproach out-performs both a standard baseline and previous temporal retrieval models …,Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,2014,32
Question answering techniques for the World Wide Web,Jimmy Lin; Boris Katz,*,EACL-2003 Tutorial,2003,32
PageRank without hyperlinks: Reranking with PubMed related article networks for biomedical text retrieval,Jimmy Lin,Graph analysis algorithms such as PageRank and HITS have been successful in Webenvironments because they are able to extract important inter-document relationships frommanually-created hyperlinks. We consider the application of these techniques to biomedicaltext retrieval. In the current PubMed® search interface; a MEDLINE® citation is connected toa number of related citations; which are in turn connected to other citations. Thus; aMEDLINE record represents a node in a vast content-similarity network. This article exploresthe hypothesis that these networks can be exploited for text retrieval; in the same manner ashyperlink graphs on the Web. We conducted a number of reranking experiments using theTREC 2005 genomics track test collection in which scores extracted from PageRank andHITS analysis were combined with scores returned by an off-the-shelf retrieval engine …,BMC bioinformatics,2008,31
Evaluation of resources for question answering evaluation,Jimmy Lin,Abstract Controlled and reproducible laboratory experiments; enabled by reusable testcollections; represent a well-established methodology in modern information retrievalresearch. In order to confidently draw conclusions about the performance of differentretrieval methods using test collections; their reliability and trustworthiness must first beestablished. Although such studies have been performed for ad hoc test collections;currently available resources for evaluating question answering systems have not beensimilarly analyzed. This study evaluates the quality of answer patterns and lists of relevantdocuments currently employed in automatic question answering evaluation; and concludesthat they are not suitable for post-hoc experimentation. These resources; created from runssubmitted by TREC QA track participants; do not produce fair and reliable assessments of …,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,30
Sentence compression as a component of a multi-document summarization system,David M Zajic; Bonnie Dorr; Jimmy Lin; Richard Schwartz,Abstract We applied a single-document sentencetrimming approach (Trimmer) to theproblem of multi-document summarization. Trimmer was designed with the intention ofcompressing a lead sentence into a space consisting of tens of characters. In our Multi-Document Trimmer (MDT); we use Trimmer to generate multiple trimmed candidates foreach sentence. Sentence selection is used to determine which trimmed candidates providethe best combination of topic coverage and brevity. We demonstrate that we were able toport Trimmer easily to this new problem. We also show that MDT generally ranked higher forrecall than for precision; suggesting that MDT is currently more successful at finding relevantcontent than it is at weeding out irrelevant content. Finally; we present an error analysis thatshows that; while sentence compressions is making space for additional sentences; more …,Proceedings of the 2006 document understanding workshop; New York,2006,29
Runtime optimizations for tree-based machine learning models,Nima Asadi; Jimmy Lin; Arjen P De Vries,Tree-based models have proven to be an effective solution for web ranking as well as othermachine learning problems in diverse domains. This paper focuses on optimizing theruntime performance of applying such models to make predictions; specifically usinggradient-boosted regression trees for learning to rank. Although exceedingly simpleconceptually; most implementations of tree-based models do not efficiently utilize modernsuperscalar processors. By laying out data structures in memory in a more cache-consciousfashion; removing branches from the execution flow using a technique called predication;and micro-batching predictions using a technique called vectorization; we are able to betterexploit modern processor architectures. Experiments on synthetic data and on threestandard learning-to-rank datasets show that our approach is significantly faster than …,IEEE Transactions on Knowledge and Data Engineering,2014,27
Ranking under temporal constraints,Lidan Wang; Donald Metzler; Jimmy Lin,Abstract This paper introduces the notion of temporally constrained ranked retrieval; which;given a query and a time constraint; produces the best possible ranked list within thespecified time limit. Naturally; more time should translate into better results; but the rankingalgorithm should always produce some results. This property is desirable from a number ofperspectives: to cope with diverse users and information needs; as well as to better managesystem load and variance in query execution times. We propose two temporally constrainedranking algorithms based on a class of probabilistic prediction models that can naturallyincorporate efficiency constraints: one that makes independent feature selection decisions;and the other that makes joint feature selection decisions. Experiments on three different testcollections show that both ranking algorithms are able to satisfy imposed time constraints …,Proceedings of the 19th ACM international conference on Information and knowledge management,2010,27
Is question answering better than information retrieval? Towards a task-based evaluation framework for question series,Jimmy Lin,Abstract This paper introduces a novel evaluation framework for question series andemploys it to explore the effectiveness of QA and IR systems at addressing users'information needs. The framework is based on the notion of recall curves; whichcharacterize the amount of relevant information contained within a fixed-length text segment.Although it is widely assumed that QA technology provides more efficient access toinformation than IR systems; our experiments show that a simple IR baseline is quitecompetitive. These results help us better understand the role of NLP technology in QAsystems and suggest directions for future research.,Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,2007,27
Low-latency; high-throughput access to static global resources within the Hadoop framework,Jimmy Lin; Anand Bahety; Shravya Konda; Samantha Mahindrakar,Abstract Hadoop is an open source implementation of Google's MapReduce programmingmodel that has recently gained popularity as a practical approach to distributed informationprocessing. This work explores the use of memcached; an open-source distributed in-memory object caching system; to provide low-latency; high-throughput access to staticglobal resources in Hadoop. Such a capability is essential to a large class of MapReducealgorithms that require; for example; querying language model probabilities; accessingmodel parameters in iterative algorithms; or performing joins across relational datasets.Experimental results on a simple demonstration application illustrate that memcachedprovides a feasible general-purpose solution for rapidly accessing global key-value pairsfrom within Hadoop programs. Our proposed architecture exhibits the desirable scaling …,University of Maryland; Tech. Rep,2009,26
Modeling actions of PubMed users with n-gram language models,Jimmy Lin; W John Wilbur,Abstract Transaction logs from online search engines are valuable for two reasons: First;they provide insight into human information-seeking behavior. Second; log data can be usedto train user models; which can then be applied to improve retrieval systems. This articlepresents a study of logs from PubMed®; the public gateway to the MEDLINE® database ofbibliographic records from the medical and biomedical primary literature. Unlike mostprevious studies on general Web search; our work examines user activities with a highly-specialized search engine. We encode user actions as string sequences and model thesesequences using n-gram language models. The models are evaluated in terms of perplexityand in a sequence prediction task. They help us better understand how PubMed userssearch for information and provide an enabler for improving users' search experience.,Information retrieval,2009,25
Multiple alternative sentence compressions for automatic text summarization,Nitin Madnani; David Zajic; Bonnie Dorr; Necip Fazil Ayan; Jimmy Lin,Abstract We perform multi-document summarization by generating compressed versions ofsource sentences as summary candidates and using weighted features of these candidatesto construct summaries. We combine a parse-and-trim approach with a novel technique forproducing multiple alternative compressions for source sentences. In addition; we use anovel method for tuning the feature weights that maximizes the change in the ROUGE-2score (∆ ROUGE) between the already existing summary state and the new state that resultsfrom the addition of the candidate under consideration. We also describe experiments usinga new paraphrase-based feature for redundancy checking. Finally; we present the results ofour DUC2007 submissions and some ideas for future work.,Proceedings of DUC,2007,25
TREC-2006 at Maryland: Blog; enterprise; legal and QA tracks,Douglas Oard; Tamer Elsayed; Jianqiang Wang; Yejun Wu; Pengyi Zhang; Eileen Abels; Jimmy Lin; Dagbert Soergel,Abstract: In TREC 2006; teams from the University of Maryland participated in the Blog track;the Expert Search task of the Enterprise track; the Complex Interactive Question Answeringtask of the Question Answering track; and the Legal track. This paper reports our results.Descriptors:* SEARCHING;* INFORMATION RETRIEVAL; SEMANTICS; INTERNET SubjectCategories: INFORMATION SCIENCE LINGUISTICS Distribution Statement: APPROVEDFOR PUBLIC RELEASE DEFENSE TECHNICAL INFORMATION CENTER 8725 John J.Kingman Road; Fort Belvoir; VA 22060-6218 1-800-CAL-DTIC (1-800-225-3842) ABOUT,*,2006,25
Fast candidate generation for real-time tweet search with bloom filter chains,Nima Asadi; Jimmy Lin,Abstract The rise of social media and other forms of user-generated content have createdthe demand for real-time search: against a high-velocity stream of incoming documents;users desire a list of relevant results at the time the query is issued. In the context of real-timesearch on tweets; this work explores candidate generation in a two-stage retrievalarchitecture where an initial list of results is processed by a second-stage rescorer toproduce the final output. We introduce Bloom filter chains; a novel extension of Bloom filtersthat can dynamically expand to efficiently represent an arbitrarily long and growing list ofmonotonically-increasing integers with a constant false positive rate. Using a collection ofBloom filter chains; a novel approximate candidate generation algorithm called BWand isable to perform both conjunctive and disjunctive retrieval. Experiments show that our …,ACM Transactions on Information Systems (TOIS),2013,24
Training efficient tree-based models for document ranking,Nima Asadi; Jimmy Lin,Abstract Gradient-boosted regression trees (GBRTs) have proven to be an effective solutionto the learning-to-rank problem. This work proposes and evaluates techniques for trainingGBRTs that have efficient runtime characteristics. Our approach is based on the simple ideathat compact; shallow; and balanced trees yield faster predictions: thus; it makes sense toincorporate some notion of execution cost during training to “encourage” trees with thesetopological characteristics. We propose two strategies for accomplishing this: the first; bydirectly modifying the node splitting criterion during tree induction; and the second; bystagewise tree pruning. Experiments on a standard learning-to-rank dataset show that thepruning approach is superior; one balanced setting yields an approximately 40% decreasein prediction latency with minimal reduction in output quality as measured by NDCG.,European Conference on Information Retrieval,2013,24
Navigating information spaces: A case study of related article search in PubMed,Jimmy Lin; Michael DiCuccio; Vahan Grigoryan; W John Wilbur,Abstract The concept of an “information space” provides a powerful metaphor for guiding thedesign of interactive retrieval systems. We present a case study of related article search; abrowsing tool designed to help users navigate the information space defined by results ofthe PubMed® search engine. This feature leverages content-similarity links that tieMEDLINE® citations together in a vast document network. We examine the effectiveness ofrelated article search from two perspectives: a topological analysis of networks generatedfrom information needs represented in the TREC 2005 genomics track and a query loganalysis of real PubMed users. Together; data suggest that related article search is a usefulfeature and that browsing related articles has become an integral part of how users interactwith PubMed.,Information Processing & Management,2008,24
Start and beyond,Boris Katz; Jimmy Lin,Abstract To address the problem of information overload in today's world; we havedeveloped Start; a natural language question answering system that provides users withmultimedia information access through the use of natural language annotations. In order toharness the potential of knowledge sources on the World Wide Web; we have developedOmnibase; a virtual database that provides uniform access to Web resources. Our ultimategoal is to develop a computer system that acts like a “smart reference librarian;” and to alarge extent; we have accomplished our goal. However; expanding our system's domain ofknowledge is a time-consuming task that requires trained individuals. This paper describesseveral research directions aimed at overcoming the limitations of our current technology.,Proceedings of 6th World Multiconference on Systemics; Cybernetics; and Informatics,2002,24
Report on the SIGIR 2015 workshop on reproducibility; inexplicability; and generalizability of results (RIGOR),Jaime Arguello; Matt Crane; Fernando Diaz; Jimmy Lin; Andrew Trotman,Abstract The SIGIR 2015 Workshop on Reproducibility; Inexplicability; and Generalizabilityof Results (RIGOR) took place on Thursday; August 13; 2015 in Santiago; Chile. The goal ofthe workshop was two fold. The first to provide a venue for the publication and presentationof negative results. The second was to provide a venue through which the authors of opensource search engines could compare performance of indexing and searching on the samecollections and on the same machines-encouraging the sharing of ideas and discoveries ina like-to-like environment. In total three papers were presented and seven systemsparticipated.,ACM SIGIR Forum,2016,23
A sentence-trimming approach to multi-document summarization,David Zajic; Bonnie Dorr; Richard Schwartz; Christof Monz; J Lin,Abstract We implemented an initial application of a sentence-trimming approach (Trimmer)to the problem of multi-document summarization in the MSE2005 and DUC2005 tasks.Sentence trimming was incorporated into a feature-based summarization system; calledMulti-Document Trimmer (MDT); by using sentence trimming as both a preprocessing stageand a feature for sentence ranking. We demonstrate that we were able to port Trimmer easilyto this new problem. Although the direct impact of sentence trimming was minimal comparedto other features used in the system; the interaction of the other features resulted in trimmedsentences accounting for nearly half of the selected summary sentences.,Proceedings of HLT/EMNLP 2005 Workshop on Text Summarization (HLT/EMNLP’05),2005,23
Summarization,Jimmy Lin,The values in the relations of a relational database are elements of one or more underlyingsets called domains. In practical applications; a domain may be infinite; eg; the set of naturalnumbers. In this case; the value of a relational calculus query when applied to such adatabase may be infinite; eg;{njn! 10}. A query Q is called finite if the value of Q whenapplied to any database is finite. Even when the database domains are finite; all that isnormally known about them is that they are some finite superset of the values that occur inthe database. In this case; the value of a relational calculus query may depend on such anunknown domain; eg;{xj 8yR (x; y)}. A query Q is called domain independent if the value of Qwhen applied to any database is the same for any two domains containing the databasevalues or; equivalently; if the value of Q when applied to a database contains only values …,*,2009,22
Assessor differences and user preferences in tweet timeline generation,Yulu Wang; Garrick Sherman; Jimmy Lin; Miles Efron,Abstract In information retrieval evaluation; when presented with an effectiveness differencebetween two systems; there are three relevant questions one might ask. First; are thedifferences statistically significant? Second; is the comparison stable with respect toassessor differences? Finally; is the difference actually meaningful to a user? This papertackles the last two questions about assessor differences and user preferences in thecontext of the newly-introduced tweet timeline generation task in the TREC 2014 Microblogtrack; where the system's goal is to construct an informative summary of non-redundanttweets that addresses the user's information need. Central to the evaluation methodology ishuman-generated semantic clusters of tweets that contain substantively similar information.We show that the evaluation is stable with respect to assessor differences in clustering …,Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,2015,21
Exploring large-data issues in the curriculum: A case study with MapReduce,Jimmy Lin,Abstract This paper describes the design of a pilot research and educational effort at theUniversity of Maryland centered around technologies for tackling Web-scale problems. In thecontext of a" cloud computing" initiative lead by Google and IBM; students and researchersare provided access to a computer cluster running Hadoop; an open-source Javaimplementation of Google's MapReduce framework. This technology provides anopportunity for students to explore large-data issues in the context of a course organizedaround teams of graduate and undergraduate students; in which they tackle open researchproblems in the human language technologies. This design represents one attempt tobridge traditional instruction with real-world; large-data research challenges.,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,2008,21
NScale: neighborhood-centric large-scale graph analytics in the cloud,Abdul Quamar; Amol Deshpande; Jimmy Lin,Abstract There is an increasing interest in executing complex analyses over large graphs;many of which require processing a large number of multi-hop neighborhoods or subgraphs.Examples include ego network analysis; motif counting; finding social circles; personalizedrecommendations; link prediction; anomaly detection; analyzing influence cascades; andothers. These tasks are not well served by existing vertex-centric graph processingframeworks; where user programs are only able to directly access the state of a single vertexat a time; resulting in high communication; scheduling; and memory overheads in executingsuch tasks. Further; most existing graph processing frameworks ignore the challenges inextracting the relevant portions of the graph that an analysis task is interested in; andloading those onto distributed memory. This paper introduces NScale; a novel end-to-end …,The VLDB Journal,2016,20
Anytime ranking for impact-ordered indexes,Jimmy Lin; Andrew Trotman,Abstract The ability for a ranking function to control its own execution time is useful formanaging load; reigning in outliers; and adapting to different types of queries. We propose asimple yet effective anytime algorithm for impact-ordered indexes that builds on a score-at-a-time query evaluation strategy. In our approach; postings segments are processed indecreasing order of their impact scores; and the algorithm early terminates when a specifiednumber of postings have been processed. With a simple linear model and a few trainingtopics; we can determine this threshold given a time budget in milliseconds. Experiments ontwo web test collections show that our approach can accurately control query evaluationlatency and that aggressive limits on execution time lead to minimal decreases ineffectiveness.,Proceedings of the 2015 International Conference on The Theory of Information Retrieval,2015,20
Indexing and retrieving natural language using ternary expressions,Jimmy Lin,Abstract Traditional information retrieval systems based on the" bag-of-words" paradigmcannot completely capture the semantic content of documents. Yet it is impossible withcurrent technology to build a practical information access system that fully analyzes andunderstands unrestricted natural language. However; if we avoid the most complex andprocessing-intensive natural language understanding techniques; we can construct a large-scale information access system which is capable of processing unrestricted text; largelyunderstanding it; and answering natural language queries with high precision. We believethat ternary expressions are the most suitable representational structure for such a system;they are expressive enough for information retrieval purposes; yet amenable to rapid large-scale indexing.,*,2001,20
Report on the Evaluation-as-a-Service (EaaS) expert workshop,Frank Hopfgartner; Allan Hanbury; Henning Müller; Noriko Kando; Simon Mercer; Jayashree Kalpathy-Cramer; Martin Potthast; Tim Gollub; Anastasia Krithara; Jimmy Lin; Krisztian Balog; Ivan Eggel,Abstract In this report; we summarize the outcome of the" Evaluation-as-a-Service" workshopthat was held on the 5th and 6th March 2015 in Sierre; Switzerland. The objective of themeeting was to bring together initiatives that use cloud infrastructures; virtual machines; APIs(Application Programming Interface) and related projects that provide evaluation ofinformation retrieval or machine learning tools as a service.,ACM SIGIR Forum,2015,19
Hone: Scaling down hadoop on shared-memory systems,K Ashwin Kumar; Jonathan Gluck; Amol Deshpande; Jimmy Lin,Abstract The underlying assumption behind Hadoop and; more generally; the need fordistributed processing is that the data to be analyzed cannot be held in memory on a singlemachine. Today; this assumption needs to be re-evaluated. Although petabyte-scale data-stores are increasingly common; it is unclear whether" typical" analytics tasks require morethan a single high-end server. Additionally; we are seeing increased sophistication inanalytics; eg; machine learning; which generally operates over smaller and more refineddatasets. To address these trends; we propose" scaling down" Hadoop to run on shared-memory machines. This paper presents a prototype runtime called Hone; intended to beboth API and binary compatible with standard (distributed) Hadoop. That is; Hone can takean existing Hadoop jar and efficiently execute it; without modification; on a multi-core …,Proceedings of the VLDB Endowment,2013,19
Combining statistical translation techniques for cross-language information retrieval,Ferhan Ture; Jimmy Lin; Douglas Oard,ABSTRACT Cross-language information retrieval today is dominated by techniques that relyprincipally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available intheir internal representations. This paper explores combination-of-evidence techniquesusing three types of statistical translation models: context-independent token translation;token translation using phrase-dependent contexts; and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-alignedtokens in parallel text; phrase-dependent translation is performed using aligned statisticalphrases; and sentence-dependent translation is performed using those same alignedphrases together with an n-gram language model. Experiments on retrieval of Arabic …,Proceedings of COLING 2012,2012,19
Putting the user in the loop: interactive maximal marginal relevance for query-focused summarization,Jimmy Lin; Nitin Madnani; Bonnie J Dorr,Abstract This work represents an initial attempt to move beyond" single-shot" summarizationto interactive summarization. We present an extension to the classic Maximal MarginalRelevance (MMR) algorithm that places a user" in the loop" to assist in candidate selection.Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 showthat interactively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactivesummarization.,Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,2010,19
Semantic clustering of answers to clinical questions,Jimmy Lin; Dina Demner-Fushman,Abstract Access to clinical evidence is a critical component of the practice of evidence-basedmedicine. Advanced retrieval systems can supplement precompiled secondary sources toassist physicians in making sound clinical decisions. This study explores one particularissue related to the design of such retrieval systems: the effective organization of searchresults to facilitate rapid understanding and synthesis of potentially relevant information. Wehypothesize that grouping retrieved MEDLINE® citations into semantically-coherent clusters;based on automatically-extracted interventions from the abstract text; represents an effectivestrategy for presenting results; compared to a traditional ranked list. Experiments with ourimplemented system appear to support this claim.,AMIA Annual Symposium Proceedings,2007,19
Different structures for evaluating answers to complex questions: Pyramids won't topple; and neither will human assessors,Hoa Trang Dang; Jimmy Lin,Abstract The idea of “nugget pyramids” has recently been introduced as a refinement to thenugget-based methodology used to evaluate answers to complex questions in the TREC QAtracks. This paper examines data from the 2006 evaluation; the first large-scale deploymentof the nugget pyramids scheme. We show that this method of combining judgments ofnugget importance from multiple assessors increases the stability and discriminative powerof the evaluation while introducing only a small additional burden in terms of manualassessment. We also consider an alternative method for combining assessor opinions;which yields a distinction similar to micro-and macro-averaging in the context ofclassification tasks. While the two approaches differ in terms of underlying assumptions; theirresults are nevertheless highly correlated.,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,2007,19
Toward reproducible baselines: The open-source ir reproducibility challenge,Jimmy Lin; Matt Crane; Andrew Trotman; Jamie Callan; Ishan Chattopadhyaya; John Foley; Grant Ingersoll; Craig Macdonald; Sebastiano Vigna,Abstract The Open-Source IR Reproducibility Challenge brought together developers ofopen-source search engines to provide reproducible baselines of their systems in acommon environment on Amazon EC2. The product is a repository that contains all codenecessary to generate competitive ad hoc retrieval baselines; such that with a single script;anyone with a copy of the collection can reproduce the submitted runs. Our vision is thatthese results would serve as widely accessible points of comparison in future IR research.This project represents an ongoing effort; but we describe the first phase of the challengethat was organized as part of a workshop at SIGIR 2015. We have succeeded modestly sofar; achieving our main goals on the Gov2 collection with seven open-source searchengines. In this paper; we describe our methodology; share experimental results; and …,European Conference on Information Retrieval,2016,18
Evaluation-as-a-Service: Overview and outlook,Allan Hanbury; Henning Müller; Krisztian Balog; Torben Brodt; Gordon V Cormack; Ivan Eggel; Tim Gollub; Frank Hopfgartner; Jayashree Kalpathy-Cramer; Noriko Kando; Anastasia Krithara; Jimmy Lin; Simon Mercer; Martin Potthast,Abstract: Evaluation in empirical computer science is essential to show progress and assesstechnologies developed. Several research domains such as information retrieval have longrelied on systematic evaluation to measure progress: here; the Cranfield paradigm ofcreating shared test collections; defining search tasks; and collecting ground truth for thesetasks has persisted up until now. In recent years; however; several new challenges haveemerged that do not fit this paradigm very well: extremely large data sets; confidential datasets as found in the medical domain; and rapidly changing data sets as often encountered inindustry. Also; crowdsourcing has changed the way that industry approaches problem-solving with companies now organizing challenges and handing out monetary awards toincentivize people to work on their challenges; particularly in the field of machine learning …,arXiv preprint arXiv:1512.07454,2015,18
NScale: neighborhood-centric analytics on large graphs,Abdul Quamar; Amol Deshpande; Jimmy Lin,Abstract There is an increasing interest in executing rich and complex analysis tasks overlarge-scale graphs; many of which require processing and reasoning about a large numberof multi-hop neighborhoods or subgraphs in the graph. Examples of such tasks include egonetwork analysis; motif counting in biological networks; finding social circles; personalizedrecommendations; link prediction; anomaly detection; analyzing influence cascades; and soon. These tasks are not well served by existing vertex-centric graph processing frameworkswhose computation and execution models limit the user program to directly access the stateof a single vertex; resulting in high communication; scheduling; and memory overheads inexecuting such tasks. Further; most existing graph processing frameworks also typicallyignore the challenges in extracting the relevant portions of the graph that an analysis task …,Proceedings of the VLDB Endowment,2014,18
Integrating web resources and lexicons into a natural language query system,Boris Katz; Deniz Yuret; Jimmy Lin; Sue Felshin; Rebecca Schulman; Adnan Ilik; Ali Ibrahim; Philip Osafo-Kwaako,The START system responds to natural language queries with answers in text; pictures; andother media. START's sentence-level natural language parsing relies on a number ofmechanisms to help it process the huge; diverse resources available on the World WideWeb. Blitz; a hybrid heuristic-and corpus-based natural language preprocessor enablesSTART to integrate a large and ever-changing lexicon of proper names; by using heuristicrules and precompiled tables of symbols to preprocess various highly regular and fixedexpressions into lexical tokens. LaMeTH; a content-based system for extracting informationfrom HTML documents; assists START by providing a uniform method of accessinginformation on the Web in real time. These mechanisms have considerably improvedSTARTS ability to analyze real-world sentences and answer queries through expansion …,Multimedia Computing and Systems; 1999. IEEE International Conference on,1999,18
Document vector representations for feature extraction in multi-stage document ranking,Nima Asadi; Jimmy Lin,Abstract We consider a multi-stage retrieval architecture consisting of a fast;“cheap”candidate generation stage; a feature extraction stage; and a more “expensive” rerankingstage using machine-learned models. In this context; feature extraction can beaccomplished using a document vector index; a mapping from document ids to documentrepresentations. We consider alternative organizations of such a data structure for efficientfeature extraction: design choices include how document terms are organized; how complexterm proximity features are computed; and how these structures are compressed. Inparticular; we propose a novel document-adaptive hashing scheme for compactly encodingterm ids. The impact of alternative designs on both feature extraction speed and memoryfootprint is experimentally evaluated. Overall; results show that our architecture is …,Information retrieval,2013,17
Data-Intensive Text Processing with MapReduce (Synthesis Lectures on Human Language Technologies),Jimmy Lin; Chris Dyer; G Hirst,*,Morgan and Claypool Publishers,2010,17
Weathering the storm: The policy implications of cloud computing,Justin M Grimes; Paul T Jaeger; Jimmy Lin,Throughout the history of computing; there have been sev-eral paradigm shifts from main-frames to mini computing to microprocessing to networked computers. On track to be thenext major paradigm shift is that of cloud computing. While the de nitions are still beingdebated (see: http://videos. techielife. com/what-is-cloud-computing/video-online/2008/11/13); fundamentally; cloud computing can be de-ned as a push in designingservices where information is stored and processed on the Internet (ie;\the cloud") usu-allyvia massive large scale data centers which can be ac-cessed remotely through variousclients and platforms [18; 2]. Cloud computing itself has often been referred to as aconglomeration of ideas such as Software as a Service; Web 2.0; grid computing; and utilitycomputing [2]. In essence; cloud computing is an umbrella concept which attempts to …,*,2009,17
The role of information retrieval in answering complex questions,Jimmy Lin,Abstract This paper explores the role of information retrieval in answering" relationship"questions; a new class complex information needs formally introduced in TREC 2005. Sinceinformation retrieval is often an integral component of many question answering strategies; itis important to understand the impact of different term-based techniques. Within a frameworkof sentence retrieval; we examine three factors that contribute to question answeringperformance: the use of different retrieval engines; relevance (both at the document andsentence level); and redundancy. Results point out the limitations of purely term-basedmethods to this challenging task. Nevertheless; IR-based techniques provide a strongbaseline on top of which more sophisticated language processing techniques can bedeployed.,Proceedings of the COLING/ACL on Main conference poster sessions,2006,17
Real-time twitter recommendation: Online motif detection in large dynamic graphs,Pankaj Gupta; Venu Satuluri; Ajeet Grewal; Siva Gurumurthy; Volodymyr Zhabiuk; Quannan Li; Jimmy Lin,Abstract We describe a production Twitter system for generating relevant; personalized; andtimely recommendations based on observing the temporally-correlated actions of eachuser's followings. The system currently serves millions of recommendations daily to tens ofmillions of mobile users. The approach can be viewed as a specific instance of the novelproblem of online motif detection in large dynamic graphs. Our current solution partitions thegraph across a number of machines; and with the construction of appropriate datastructures; motif detection can be translated into the lookup and intersection of adjacencylists in each partition. We conclude by discussing a generalization of the problem thatperhaps represents a new class of data management systems.,Proceedings of the VLDB Endowment,2014,16
Infrastructure for supporting exploration and discovery in web archives,Jimmy Lin; Milad Gholami; Jinfeng Rao,Abstract Web archiving initiatives around the world capture ephemeral web content topreserve our collective digital memory. However; unlocking the potential of web archivesrequires tools that support exploration and discovery of captured content. These tools needto be scalable and responsive; and to this end we believe that modern" big data"infrastructure can provide a solid foundation. We present Warcbase; an open-sourceplatform for managing web archives built on the distributed datastore HBase. Our systemprovides a flexible data model for storing and managing raw content as well as metadataand extracted knowledge. Tight integration with Hadoop provides powerful tools for analyticsand data processing. Relying on HBase for storage infrastructure simplifies the developmentof scalable and responsive applications. We describe a service that provides temporal …,Proceedings of the 23rd International Conference on World Wide Web,2014,16
A month in the life of a production news recommender system,Alan Said; Jimmy Lin; Alejandro Bellogín; Arjen de Vries,Abstract During the last decade; recommender systems have become a ubiquitous feature inthe online world. Research on systems and algorithms in this area has flourished; leading tonovel techniques for personalization and recommendation. The evaluation of recommendersystems; however; has not seen similar progress---techniques have changed little since theadvent of recommender systems; when evaluation methodologies were" borrowed" fromrelated research areas. As an effort to move evaluation methodology forward; this paperdescribes a production recommender system infrastructure that allows research systems tobe evaluated in situ; by real-world metrics such as user clickthrough. We present an analysisof one month of interactions with this infrastructure and share our findings.,Proceedings of the 2013 workshop on Living labs for information retrieval evaluation,2013,16
Looking inside the box: Context-sensitive translation for cross-language information retrieval,Ferhan Ture; Jimmy Lin; Douglas W Oard,Abstract Cross-language information retrieval (CLIR) today is dominated by techniques thatuse token-to-token mappings from bilingual dictionaries. Yet; state-of-the-art statisticaltranslation models (eg; using Synchronous Context-Free Grammars) are far richer; capturingmulti-term phrases; term dependencies; and contextual constraints on translation choice. Wepresent a novel CLIR framework that is able to reach inside the translation" black box" andexploit these sources of evidence. Experiments on the TREC-5/6 English-Chinese testcollection show this approach to be promising.,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,16
Pseudo test collections for learning web search ranking functions,Nima Asadi; Donald Metzler; Tamer Elsayed; Jimmy Lin,Abstract Test collections are the primary drivers of progress in information retrieval. Theyprovide yardsticks for assessing the effectiveness of ranking functions in an automatic; rapid;and repeatable fashion and serve as training data for learning to rank models. However;manual construction of test collections tends to be slow; labor-intensive; and expensive. Thispaper examines the feasibility of constructing web search test collections in a completelyunsupervised manner given only a large web corpus as input. Within our proposedframework; anchor text extracted from the web graph is treated as a pseudo query log fromwhich pseudo queries are sampled. For each pseudo query; a set of relevant and non-relevant documents are selected using a variety of web-specific features; including spamand aggregated anchor text weights. The automatically mined queries and judgments …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,16
Using visualizations to monitor changes and harvest insights from a global-scale logging infrastructure at Twitter,Krist Wongsuphasawat; Jimmy Lin,Logging user activities is essential to data analysis for internet products and services. Twitterhas built a unified logging infrastructure that captures user activities across all clients itowns; making it one of the largest datasets in the organization. This paper describeschallenges and opportunities in applying information visualization to log analysis at thismassive scale; and shows how various visualization techniques can be adapted to help datascientists extract insights. In particular; we focus on two scenarios:(1) monitoring andexploring a large collection of log events; and (2) performing visual funnel analysis on logdata with tens of thousands of event types. Two interactive visualizations were developed forthese purposes: we discuss design choices and the implementation of these systems; alongwith case studies of how they are being used in day-to-day operations at Twitter.,Visual Analytics Science and Technology (VAST); 2014 IEEE Conference on,2014,15
Evaluation as a service for information retrieval,Jimmy Lin; Miles Efron,Abstract How can we run large-scale; community-wide evaluations of information retrievalsystems if we lack the ability to distribute the document collection on which the task isbased? This was the challenge we faced in the TREC Microblog tracks over the past fewyears. In this paper; we present a novel evaluation methodology we dub" evaluation as aservice"; which was implemented at TREC 2013 to address restrictions on dataredistribution. The basic idea is that instead of distributing the document collection; we (thetrack organizers) provided a service API" in the cloud" with which participants couldaccomplish the evaluation task. We outline advantages as well as disadvantages of thisevaluation methodology; and discuss how the approach might be extended to otherevaluation scenarios.,ACM SIGIR Forum,2013,15
Twanchor text: a preliminary study of the value of tweets as anchor text,Gilad Mishne; Jimmy Lin,Abstract It is well known that anchor text plays an important role in search; providing signalsthat are often not present in the source document itself. The paper reports results of apreliminary investigation on the value of tweets and tweet conversations as anchor text. Weshow that using tweets as anchors improves significantly over using HTML anchors; andsignificantly increases recall of news item retrieval.,Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,2012,15
Elements of a computational model for multi‐party discourse: The turn‐taking behavior of Supreme Court justices,Timothy Hawes; Jimmy Lin; Philip Resnik,Abstract This work explores computational models of multi-party discourse; using transcriptsfrom US Supreme Court oral arguments. The turn-taking behavior of participants is treatedas a supervised sequence-labeling problem and modeled using first-and second-orderconditional random fields (CRFs). We specifically explore the hypothesis that discoursemarkers and personal references provide important features in such models. Results from asequence prediction experiment demonstrate that incorporating these two types of featuresyields significant improvements in accuracy. Our experiments are couched in the broadercontext of developing tools to support legal scholarship; although we see other naturallanguage processing applications as well.,Journal of the Association for Information Science and Technology,2009,15
Evaluating summaries and answers: two sides of the same coin?,Jimmy Lin; Dina Demner-Fushman,Abstract This paper discusses the convergence between question answering andmultidocument summarization; pointing out implications and opportunities for knowledgetransfer in both directions. As a case study in one direction; we discuss the recentdevelopment of an automatic method for evaluating definition questions based on n-gramoverlap; a commonlyused technique in summarization evaluation. In the other direction; themove towards topic-oriented summaries requires an understanding of relevance andtopicality; issues which have received attention in the question answering literature. It is ouropinion that question answering and multi-document summarization represent twocomplementary approaches to the same problem of satisfying complex user informationneeds. Although this points to many exciting opportunities for systembuilding; here we …,Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,2005,15
On building better mousetraps and understanding the human condition: Reflections on big data in the social sciences,Jimmy Lin,Over the past few years; we have seen the emergence of “big data”: disruptive technologiesthat have transformed commerce; science; and many aspects of society. Despite thetremendous enthusiasm for big data; there is no shortage of detractors. This article arguesthat many criticisms stem from a fundamental confusion over goals: whether the desiredoutcome of big data use is “better science” or “better engineering.” Critics point to therejection of traditional data collection and analysis methods; confusion between correlationand causation; and an indifference to models with explanatory power. From the perspectiveof advancing social science; these are valid reservations. I contend; however; that if the endgoal of big data use is to engineer computational artifacts that are more effective accordingto well-defined metrics; then whatever improves those metrics should be exploited without …,The ANNALS of the American Academy of Political and Social Science,2015,14
Temporal relevance profiles for tweet search,Jimmy Lin; Miles Efron,ABSTRACT When searching tweets; users may know something about the temporalcharacteristics of the information they're after. For example; based on external knowledge; asearcher might prefer more recent results or results within a particular time interval.However; most search applications do not allow the user to explicitly supply this information;and neither do most retrieval models have a mechanism to incorporate this additionalevidence. In this paper; we introduce the notion of a temporal relevance profile; which a userexplicitly includes alongside a keyword search query. We propose alternativerepresentations of temporal relevance profiles and how existing retrieval models might takeadvantage of this data. Oracle experiments on microblog track data from TREC 2011 and2012 empirically demonstrate that this approach has the potential to significantly increase …,SIGIR Workshop on Time-aware Information Access,2013,14
Dynamic cutoff prediction in multi-stage retrieval systems,J Shane Culpepper; Charles LA Clarke; Jimmy Lin,Abstract Modern multi-stage retrieval systems are comprised of a candidate generationstage followed by one or more reranking stages. In such an architecture; the quality of thefinal ranked list may not be sensitive to the quality of the initial candidate pool; especially interms of early precision. This provides several opportunities to increase retrieval efficiencywithout significantly sacrificing effectiveness. In this paper; we explore a new approach todynamically predicting the size of an initial result set in the candidate generation stage;which can directly affect the overall efficiency and effectiveness of the entire system.Previous work exploring this tradeoff has focused on global parameter settings that apply toall queries; even though optimal settings vary across queries. In contrast; we propose atechnique that makes a parameter prediction to maximize efficiency within an …,Proceedings of the 21st Australasian Document Computing Symposium,2016,13
Simple dynamic emission strategies for microblog filtering,Luchen Tan; Adam Roegiest; Charles LA Clarke; Jimmy Lin,Abstract Push notifications from social media provide a method to keep up-to-date on topicsof personal interest. To be effective; notifications must achieve a balance between pushingtoo much and pushing too little. Push too little and the user misses important updates; pushtoo much and the user is overwhelmed by unwanted information. Using data from the TREC2015 Microblog track; we explore simple dynamic emission strategies for microblog pushnotifications. The key to effective notifications lies in establishing and maintainingappropriate thresholds for pushing updates. We explore and evaluate multiple thresholdsetting strategies; including purely static thresholds; dynamic thresholds without userfeedback; and dynamic thresholds with daily feedback. Our best technique takes advantageof daily feedback in a simple yet effective manner; achieving the best known result …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,13
Overview of the TREC 2016 real-time summarization track,Jimmy Lin; Adam Roegiest; Luchen Tan; Richard McCreadie; Ellen Voorhees; Fernando Diaz,The TREC 2016 Real-Time Summarization (RTS) Track aims to explore techniques andsystems that automatically monitor streams of social media posts such as Twitter to keepusers up to date on topics of interest. We might think of these topics as “interest profiles”;specifying the user's prospective information needs. In real-time summarization; the goal isfor a system to “push”(ie; recommend or suggest) interesting and novel content to users in atimely fashion. For example; the user might be interested in poll results for the 2016 USpresidential elections and wishes to be notified whenever new results are published. Wecan imagine two methods for disseminating updates:• Scenario A: Push notifications. Assoon as the system identifies a relevant post; it is immediately sent to the user's mobiledevice via a push notification. At a high level; push notifications should be relevant (on …,Proceedings of the 25th text retrieval conference; TREC,2016,13
CWI and TU Delft at TREC 2013: Contextual suggestion; federated web search; KBA; and web tracks,Alejandro Bellogín; Gebrekirstos G Gebremeskel; Jiyin He; Jimmy Lin; Alan Said; Thaer Samar; Arjen P de Vries; Jeroen BP Vuurens,ABSTRACT This paper provides an overview of the work done at the Centrum Wiskunde &Informatica (CWI) and Delft University of Technology (TU Delft) for different tracks of TREC2013. We participated in the Contextual Suggestion Track; the Federated Web Search Track;the Knowledge Base Acceleration (KBA) Track; and the Web Ad-hoc Track. In the ContextualSuggestion track; we focused on filtering the entire ClueWeb12 collection to generaterecommendations according to the provided user profiles and contexts. For the FederatedWeb Search track; we exploited both categories from ODP and document relevance tomerge result lists. In the KBA track; we focused on the Cumulative Citation Recommendationtask where we exploited different features to two classification algorithms. For the Web track;we extended an ad-hoc baseline with a proximity model that promotes documents in …,Proceedings of the Text REtrieval Conference (TREC),2013,13
When close enough is good enough: approximate positional indexes for efficient ranked retrieval,Tamer Elsayed; Jimmy Lin; Donald Metzler,Abstract Previous research has shown that features based on term proximity are importantfor effective retrieval. However; they incur substantial costs in terms of larger invertedindexes and slower query execution times as compared to term-based features. This paperexplores whether term proximity features based on approximate term positions are aseffective as those based on exact term positions. We introduce the novel notion ofapproximate positional indexes based on dividing documents into coarse-grained bucketsand recording term positions with respect to those buckets. We propose differentapproaches to defining the buckets and compactly encoding bucket ids. In the context oflinear ranking functions; experimental results show that features based on approximate termpositions are able to achieve effectiveness comparable to exact term positions; but with …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,13
Syntactic sentence compression in the biomedical domain: facilitating access to related articles,Jimmy Lin; W John Wilbur,Abstract We explore a syntactic approach to sentence compression in the biomedicaldomain; grounded in the context of result presentation for related article search in thePubMed search engine. By automatically trimming inessential fragments of article titles; asystem can effectively display more results in the same amount of space. Our implementedprototype operates by applying a sequence of syntactic trimming rules over the parse treesof article titles. Two separate studies were conducted using a corpus of manuallycompressed examples from MEDLINE: an automatic evaluation using B leu and asummative evaluation involving human assessors. Experiments show that a syntacticapproach to sentence compression is effective in the biomedical domain and that thepresentation of compressed article titles supports accurate “interest judgments”; decisions …,Information Retrieval,2007,13
Deconstructing nuggets: the stability and reliability of complex question answering evaluation,Jimmy Lin; Pengyi Zhang,Abstract A methodology based on" information nuggets" has recently emerged as the defacto standard by which answers to complex questions are evaluated. After severalimplementations in the TREC question answering tracks; the community has gained a betterunderstanding of its many characteristics. This paper focuses on one particular aspect of theevaluation: the human assignment of nuggets to answer strings; which serves as the basis ofthe F-score computation. As a byproduct of the TREC 2006 ciQA task; identical answerstrings were independently evaluated twice; which allowed us to assess the consistency ofhuman judgments. Based on these results; we explored simulations of assessor behaviorthat provide a method to quantify scoring variations. Understanding these variations in turnlets researchers be more confident in their comparisons of systems.,Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,2007,13
A comparison of Document-at-a-Time and Score-at-a-Time query evaluation,Matt Crane; J Shane Culpepper; Jimmy Lin; Joel Mackenzie; Andrew Trotman,Abstract We present an empirical comparison between document-at-a-time (DaaT) andscore-at-a-time (SaaT) document ranking strategies within a common framework. Althoughboth strategies have been extensively explored; the literature lacks a fair; direct comparison:such a study has been difficult due to vastly different query evaluation mechanics and indexorganizations. Our work controls for score quantization; document processing; compression;implementation language; implementation effort; and a number of details; arriving at anempirical evaluation that fairly characterizes the performance of three specific techniques:WAND (DaaT); BMW (DaaT); and JASS (SaaT). Experiments reveal a number of interestingfindings. The performance gap between WAND and BMW is not as clear as the literaturesuggests; and both methods are susceptible to tail queries that may take orders of …,Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,2017,12
Distilling massive amounts of data into simple visualizations: Twitter case studies,Miguel Rios; Jimmy Lin,Twitter is a communications platform on which users can send short; 140-charactermessages; called “tweets”; to their “followers” via a number of mechanisms; including webclients; mobile clients; and SMS. As of March 2012; Twitter has over 140 million active usersworldwide; who collectively post over 340 million tweets per day. Particularly salient is thereal-time nature of these global conversations; which rapidly evolve to reflect breakingevents such as major earthquakes (eg; Japan; March 2011) and deaths of prominent figures(eg; Steve Jobs; October 2011). From this large user base we gather tens of terabytes ofdata per day; containing records of what users tweet (and when and where); which tweetsthey interact with; and a host of other activities. How do we derive insights from thesemassive amounts of data? This responsibility falls primarily to the data analytics group …,Workshop on Social Media Visualization at ICWSM,2012,12
Exploring the effectiveness of related article search in PubMed,Jimmy Lin; Michael Dicuccio; Vahan Grigoryan; W John Wilbur,CiteSeerX - Document Details (Isaac Councill; Lee Giles; Pradeep Teregowda):,College of Information Studies; University of Maryland; College Park,2007,12
Visualizing the" Pulse" of World Cities on Twitter.,Miguel Rios; Jimmy J Lin,Abstract We present a large-scale analysis of activity on Twitter in 50 major cities around theworld throughout all of 2012. Our study consists of two parts: First; we created heatmapvisualizations; through which periods of comparatively intense and sparse activity arereadily apparent—these visual patterns reflect diurnal cycles; cultural norms; and evenreligious practices. Second; we performed a cluster analysis of these activity patterns toidentify groupings of cities that are similar in the ways their inhabitants use Twitter. Notsurprisingly; cities cluster geographically; although we are able to identify cross-culturalsimilarities as well.,ICWSM,2013,11
Why not grab a free lunch?: mining large corpora for parallel sentences to improve translation modeling,Ferhan Ture; Jimmy Lin,Abstract It is well known that the output quality of statistical machine translation (SMT)systems increases with more training data. To obtain more parallel text for translationmodeling; researchers have turned to the web to mine parallel sentences; but most previousapproaches have avoided the difficult problem of pairwise similarity on cross-lingualdocuments and instead rely on heuristics. In contrast; we confront this challenge head onusing the MapReduce framework. On a modest cluster; our scalable end-to-end processingpipeline was able to automatically gather 5.8 m parallel sentence pairs from English andGerman Wikipedia. Augmenting existing bitext with these data yielded significantimprovements over a state-of-the-art baseline (2.39 BLEU points in the best case).,Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2012,11
Assessing the term independence assumption in blind relevance feedback,Jimmy Lin; G Craig Murray,Abstract When applying blind relevance feedback for ad hoc document retrieval; is itpossible to identify; a priori; the set of query terms that will most improve retrievalperformance? Can this complex problem be reduced into the simpler one of makingindependent decisions about the performance effects of each query term? Our experimentssuggest that; for the selection of terms for blind relevance feedback; the term independenceassumption may be empirically justified.,Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,2005,11
Reproducible experiments on lexical and temporal feedback for tweet search,Jinfeng Rao; Jimmy Lin; Miles Efron,Abstract “Evaluation as a service”(EaaS) is a new methodology for community-wideevaluations where an API provides the only point of access to the collection for completingthe evaluation task. Two important advantages of this model are that it enables reproducibleIR experiments and encourages sharing of pluggable open-source components. In thispaper; we illustrate both advantages by providing open-source implementations of lexicaland temporal feedback techniques for tweet search built on the TREC Microblog API. For themost part; we are able to reproduce results reported in previous papers and confirm theirgeneral findings. However; experiments on new test collections and additional analysesprovide a more nuanced look at the results and highlight issues not discussed in previousstudies; particularly the large variances in effectiveness associated with training/test splits.,European Conference on Information Retrieval,2015,10
Old dogs are great at new tricks: Column stores for IR prototyping,Hannes Mühleisen; Thaer Samar; Jimmy Lin; Arjen De Vries,Abstract We make the suggestion that instead of implementing custom index structures andquery evaluation algorithms; IR researchers should simply store document representationsin a column-oriented relational database and implement ranking models using SQL. Forrapid prototyping; this is particularly advantageous since researchers can explore newscoring functions and features by simply issuing SQL queries; without needing to writeimperative code. We demonstrate the feasibility of this approach by an implementation ofconjunctive BM25 using two modern column stores. Experiments on a web collection showthat a retrieval engine built in this manner achieves effectiveness and efficiency on par withcustom-built retrieval engines; but provides many additional advantages; including cleanerquery semantics; a simpler architecture; built-in support for error analysis; and the ability …,Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,2014,10
Do recommendations matter?: news recommendation in real life,Alan Said; Alejandro Bellogín; Jimmy Lin; Arjen de Vries,Abstract We present a study of how recommendations are received in real life by usersacross different news domains (traditional online newspapers; hobbyist websites; forums;etc.). Our analysis shows that readers of websites centered around specific topics aregenerally less likely to interact with recommendations than readers of traditional newswebsites.,Proceedings of the companion publication of the 17th ACM conference on Computer supported cooperative work & social computing,2014,10
Dynamic memory allocation policies for postings in real-time twitter search,Nima Asadi; Jimmy Lin; Michael Busch,Abstract We explore a real-time Twitter search application where tweets are arriving at a rateof several thousands per second. Real-time search demands that they be indexed andsearchable immediately; which leads to a number of implementation challenges. In thispaper; we focus on one aspect: dynamic postings allocation policies for index structures thatare completely held in main memory. The core issue can be characterized as a" GoldilocksProblem". Because memory remains today a scare resource; an allocation policy that is tooaggressive leads to inefficient utilization; while a policy that is too conservative is slow andleads to fragmented postings lists. We present a dynamic postings allocation policy thatallocates memory in increasingly-larger" slices" from a small number of large; fixed pools ofmemory. With an analytical model and experiments; we explore different settings that …,Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,2013,10
Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters,Nima Asadi; Jimmy Lin,Abstract Most modern web search engines employ a two-phase ranking strategy: acandidate list of documents is generated using a" cheap" but low-quality scoring function;which is then reranked by an" expensive" but high-quality method (usually machine-learned). This paper focuses on the problem of candidate generation for conjunctive queryprocessing in this context. We describe and evaluate a fast; approximate postings listintersection algorithms based on Bloom filters. Due to the power of modern learning-to-ranktechniques and emphasis on early precision; significant speedups can be achieved withoutloss of end-to-end retrieval effectiveness. Explorations reveal a rich design space whereeffectiveness and efficiency can be balanced in response to specific hardwareconfigurations and application scenarios.,Proceedings of the 21st ACM international conference on Information and knowledge management,2012,10
Automatic management of partitioned; replicated search services,Florian Leibert; Jake Mannix; Jimmy Lin; Babak Hamadani,Abstract Low-latency; high-throughput web services are typically achieved throughpartitioning; replication; and caching. Although these strategies and the general design oflarge-scale distributed search systems are well known; the academic literature providessurprisingly few details on deployment and operational considerations in productionenvironments. In this paper; we address this gap by sharing the distributed searcharchitecture that underlies Twitter user search; a service for discovering relevant accountson the popular microblogging service. Our design makes use of the principle that eliminatesthe distinction between failure and other anticipated service disruptions: as a result; mostoperational scenarios share exactly the same code path. This simplicity leads to greaterrobustness and fault-tolerance. Another salient feature of our architecture is its exclusive …,Proceedings of the 2nd ACM Symposium on Cloud Computing,2011,10
Computational linguistics for metadata building (CLiMB): using text mining for the automatic identification; categorization; and disambiguation of subject terms for im...,Judith L Klavans; Carolyn Sheffield; Eileen Abels; Jimmy Lin; Rebecca Passonneau; Tandeep Sidhu; Dagobert Soergel,Abstract In this paper; we present a system using computational linguistic techniques toextract metadata for image access. We discuss the implementation; functionality andevaluation of an image catalogers' toolkit; developed in the Computational Linguistics forMetadata Building (CLiMB) research project. We have tested components of the system;including phrase finding for the art and architecture domain; functional semantic labelingusing machine learning; and disambiguation of terms in domain-specific text vis a vis a richthesaurus of subject terms; geographic and artist names. We present specific results ondisambiguation techniques and on the nature of the ambiguity problem given the thesaurus;resources; and domain-specific text resource; with a comparison of domain-generalresources and text. Our primary user group for evaluation has been the cataloger expert …,Multimedia Tools and Applications,2009,10
A Menagerie of Tracks at Maryland: HARD; Enterprise; QA; and Genomics; Oh My!,Jimmy Lin; Eileen Abels; Dina Demner-Fushman; Douglas W Oard; Philip Wu; Yejun Wu,Abstract: This year; the University of Maryland participated in four separate tracks: HARD;enterprise; question answering; and genomics. Our HARD experiments involved a trainedintermediary who searched for documents on behalf of the user; created clarification formsmanually; and exploited user responses accordingly. The aim was to better understand thenature of single-iteration clarification dialogs and to develop an" ontology of clarifications"that can be leveraged to guide system development. For the enterprise track; we submittedofficial runs to the Known Item Search and the Discussion Search tasks. Documenttransformation to normalize dates and version numbers was found to be helpful; butsuppression of text quoted from earlier messages and expansion of the indexed terms for amessage based on subject line threading proved to not be. For the QA track; we …,*,2006,10
Language and compiler design for streaming applications,William Thies; Michael I Gordon; Michal Karczmarek; Jasper Lin; David Maze; Rodric M Rabbah; Saman Amarasinghe,Summary form only given. We characterize high-performance streaming applications as anew and distinct domain of programs that is becoming increasingly important. The StreamItlanguage provides novel high-level representations to improve programmer productivity andprogram robustness within the streaming domain. At the same time; the StreamIt compileraims to improve the performance of streaming applications via stream-specific analysis andoptimizations. We motivate; describe and justify the language features of StreamIt; whichinclude a structured model of streams; a messaging system for control; and a natural textualsyntax.,Parallel and Distributed Processing Symposium; 2004. Proceedings. 18th International,2004,10
The historical background,Viola Klein,*,Women: A feminist perspective. Palo Alto; California: Mayfield Publishing Company,1975,10
GraphJet: real-time content recommendations at twitter,Aneesh Sharma; Jerry Jiang; Praveen Bommannavar; Brian Larson; Jimmy Lin,Abstract This paper presents GraphJet; a new graph-based system for generating contentrecommendations at Twitter. As motivation; we trace the evolution of our formulation andapproach to the graph recommendation problem; embodied in successive generations ofsystems. Two trends can be identified: supplementing batch with real-time processing and abroadening of the scope of recommendations from users to content. Both of these trendscome together in Graph-Jet; an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements asimple API; but one that is sufficiently expressive to support a range of recommendationalgorithms based on random walks that we have refined over the years. Similar toCassovary; a previous graph recommendation engine developed at Twitter; GraphJet …,Proceedings of the VLDB Endowment,2016,9
Do multiple listeners to the public Twitter sample stream receive the same tweets,Jiaul H Paik; Jimmy Lin,ABSTRACT Do multiple listeners to the public Twitter sample stream receive the sametweets? Due to limitations on redistribution of Twitter data; the answer to this question isimportant for the replicability and reproducibility of research findings. A negative answercreates barriers for different research groups to evaluate algorithms and systems on thesame collection of tweets. We describe a pilot experiment in preparation for the TREC 2015Microblog track that answers this question in the affirmative; which means that an evaluationmethodology built on geographically dispersed research groups independently crawling theTwitter streaming API is feasible.,SIGIR Workshop on Temporal; Social and Spatially-Aware Information Access,2015,9
Partitioning strategies for spatio-textual similarity join,Jinfeng Rao; Jimmy Lin; Hanan Samet,Abstract Given a collection of geo-tagged objects with associated textual descriptors; thespatio-textual similarity join (STJoin) problem is to identify all pairs of similar objects that areclose in distance. This task; which is useful in localized recommendations and otherapplications; is challenging since computing the join is super-linear with respect to the sizeof the collection. In this paper; we explore partitioning strategies for tackling STJoin. Oneapproach is to start with a spatial data structure; traverse regions and apply a previousalgorithm for identifying similar pairs of textual documents called All-Pairs. An alternativeapproach is to construct a global index but partition postings spatially and modify the All-Pairs algorithm to prune candidates based on distance. We evaluate these approaches ontwo real-world datasets and find that when running in a single thread; both approaches …,Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data,2014,9
Cloud 9: A MapReduce library for hadoop,Jimmy Lin,*,*,2010,9
User simulations for evaluating answers to question series,Jimmy Lin,Abstract Recently; question series have become one focus of research in questionanswering. These series are comprised of individual factoid; list; and “other” questionsorganized around a central topic; and represent abstractions of user–system dialogs.Existing evaluation methodologies have yet to catch up with this richer task model; as theyfail to take into account contextual dependencies and different user behaviors. This paperpresents a novel simulation-based methodology for evaluating answers to question seriesthat addresses some of these shortcomings. Using this methodology; we examine twodifferent behavior models: a “QA-styled” user and an “IR-styled” user. Results suggest thatan off-the-shelf document retrieval system is competitive with state-of-the-art QA systems inthis task. Advantages and limitations of evaluations based on user simulations are also …,Information processing & management,2007,9
An exploration of evaluation metrics for mobile push notifications,Luchen Tan; Adam Roegiest; Jimmy Lin; Charles LA Clarke,Abstract How do we evaluate systems that filter social media streams and send usersupdates via push notifications on their mobile phones? Such notifications must be relevant;timely; and novel. In this paper; we explore various evaluation metrics for this task; focusingspecifically on measuring relevance. We begin with an analysis of metrics deployed at theTREC 2015 Microblog evaluations. A simple change to the metrics; reflecting a differentassumption; dramatically alters system rankings. Applying another metric; previously used inthe TREC Microblog evaluations; again yields different system rankings. We find littlecorrelation between a number of" reasonable" evaluation metrics; which suggests thatsystem effectiveness depends on how you measure it---an undesirable state in IRevaluation. However; we argue that existing evaluation metrics can be generalized into a …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,8
Interleaved evaluation for retrospective summarization and prospective notification on document streams,Xin Qian; Jimmy Lin; Adam Roegiest,Abstract We propose and validate a novel interleaved evaluation methodology for twocomplementary information seeking tasks on document streams: retrospectivesummarization and prospective notification. In the first; the user desires relevant and non-redundant documents that capture important aspects of an information need. In the second;the user wishes to receive timely; relevant; and non-redundant update notifications for astanding information need. Despite superficial similarities; interleaved evaluation methodsfor web ranking cannot be directly applied to these tasks; for example; existing techniquesdo not account for temporality or redundancy. Our proposed evaluation methodologyconsists of two components: a temporal interleaving strategy and a heuristic for creditassignment to handle redundancy. By simulating user interactions with interleaved results …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,8
Desiderata for exploratory search interfaces to web archives in support of scholarly activities,Andrew Jackson; Jimmy Lin; Ian Milligan; Nick Ruest,Abstract Web archiving initiatives around the world capture ephemeral web content topreserve our collective digital memory. In this paper; we describe initial experiences inproviding an exploratory search interface to web archives for humanities scholars and socialscientists. We describe our initial implementation and discuss our findings in terms ofdesiderata for such a system. It is clear that the standard organization of a search engineresults page (SERP); consisting of an ordered list of hits; is inadequate to support the needsof scholars. Shneiderman's mantra for visual information seeking (" overview first; zoom andfilter; then details-on-demand") provides a nice organizing principle for interface design; towhich we propose an addendum:" Make everything transparent". We elaborate on this byhighlighting the importance of the temporal dimension of web pages as well as issues …,Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries,2016,8
Learning to efficiently rank on big data,Lidan Wang; Jimmy Lin; Donald Metzler; Jiawei Han,Abstract Ranking in response to user queries is a central problem in information retrieval;data mining; and machine learning. In the era of" Big data"; traditional effectiveness-centricranking techniques tend to get more and more costly (requiring additional hardware andenergy costs) to sustain reasonable ranking speed on large data. The mentality ofcombating big data by throwing in more hardware/machines will quickly become highlyexpensive since data is growing at an extremely fast rate oblivious to any cost concerns fromus." Learning to efficiently rank" offers a cost-effective solution to ranking on large data (eg;billions of documents). That is; it addresses a critically important question--whether it ispossible to improve ranking effectiveness on large data without incurring (too much)additional cost?,Proceedings of the 23rd International Conference on World Wide Web,2014,8
Visual analytics of MOOCs at maryland,Zhengzheng Xu; Dan Goldwasser; Benjamin B Bederson; Jimmy Lin,Abstract We use visual analytics to explore participation in five MOOCs at the University ofMaryland. In some of these courses; our analysis reveals interesting clustering patterns ofstudent behavior. For other courses; visualizations provide" color" to help us betterunderstand the range of student behavior.,Proceedings of the first ACM conference on Learning@ scale conference,2014,8
Overview of the trec 2003 question-answering track,Wesley Hildebrandt; Boris Katz; Jimmy Lin,*,Proceedings of the 2004 Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2004),2004,8
Event Structure and the Encoding of Arguments: The Syntax of the English and Mandarin Verb Phrase,Jimmy Lin,*,*,2004,8
Content selection and curation for web archiving: The gatekeepers vs. the masses,Ian Milligan; Nick Ruest; Jimmy Lin,Abstract Any preservation effort must begin with an assessment of what content to preserve;and web archiving is no different. There have historically been two answers to the question"what should we archive?" The Internet Archive's broad entire-web crawls have beensupplemented by narrower domain-or topic-specific collections gathered by numerouslibraries. We can characterize this as content selection and curation by" gatekeepers". Incontrast; we have witnessed the emergence of another approach driven by" the masses"---we can archive pages that are contained in social media streams such as Twitter. Theinteresting question; of course; is how these approaches differ. We provide an answer to thisquestion in the context of a case study about the 2015 Canadian federal elections. Based onour analysis; we recommend a hybrid approach that combines an effort driven by social …,Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries,2016,7
UMD-TTIC-UW at SemEval-2016 Task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement,Hua He; John Wieting; Kevin Gimpel; Jinfeng Rao; Jimmy Lin,Abstract We describe an attention-based convolutional neural network for the Englishsemantic textual similarity (STS) task in the SemEval-2016 competition (Agirre et al.; 2016).We develop an attention-based input interaction layer and incorporate it into ourmultiperspective convolutional neural network (He et al.; 2015); using the PARAGRAM-PHRASE word embeddings (Wieting et al.; 2016) trained on paraphrase pairs. Without usingany sparse features; our final model outperforms the winning entry in STS2015 whenevaluated on the STS2015 data.,Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),2016,7
Exploiting representations from statistical machine translation for cross-language information retrieval,Ferhan Ture; Jimmy Lin,Abstract This work explores how internal representations of modern statistical machinetranslation systems can be exploited for cross-language information retrieval. We tackle twocore issues that are central to query translation: how to exploit context to generate moreaccurate translations and how to preserve ambiguity that may be present in the originalquery; thereby retaining a diverse set of translation alternatives. These two considerationsare often in tension since ambiguity in natural language is typically resolved by exploitingcontext; but effective retrieval requires striking the right balance. We propose two novelquery translation approaches: the grammar-based approach extracts translationprobabilities from translation grammars; while the decoder-based approach takesadvantage of n-best translation hypotheses. Both are context-sensitive; in contrast to a …,ACM Transactions on Information Systems (TOIS),2014,7
The impact of future term statistics in real-time tweet search,Yulu Wang; Jimmy Lin,Abstract In the real-time tweet search task operationalized in the TREC Microblogevaluations; a topic consists of a query Q and a time t; modeling the task where the userwishes to see the most recent but relevant tweets that address the information need. Tosimulate the real-time aspect of the task in an evaluation setting; many systems search overthe entire collection and then discard results that occur after the query time. This approach;while computationally efficient;“cheats” in that it takes advantage of term statistics fromdocuments not available at query time (ie; future information). We show; however; that suchresults are nearly identical to a “gold standard” method that builds a separate index for eachtopic containing only those documents that occur before the query time. The implications ofthis finding on evaluation; system design; and user task models are discussed.,European Conference on Information Retrieval,2014,7
Brute-Force Approaches to Batch Retrieval: Scalable Indexing with MapReduce; or Why Bother?,Tamer Elsayed; Ferhan Ture; Jimmy Lin,Abstract Modern information retrieval research has evolved a standard workflow thatinvolves first indexing a document collection and then running ad hoc queries sequentiallyto evaluate retrieval effectiveness using standard test collections. This paper explores howaspects of this workflow might change in a MapReduce cluster-based environment. First; wepresent and evaluate two algorithms for inverted indexing that take advantage of theprogramming model's sorting mechanism to different extents. The running times of bothalgorithms scale linearly in terms of collection size up to 102 million web pages. Second; weshow that it is possible to efficiently perform batch query evaluation with MapReduce byscanning all postings lists in parallel; as opposed to sequentially accessing each postingslist. Third; we explore an approach that forgoes inverted indexing altogether and simply …,*,2010,7
" Bag of words" is not enough for strength of evidence classification.,Jimmy Lin; Dina Demner-Fushman,Abstract Incorporation of evidence from clinical research requires critical appraisal of itsquality. Information retrieval systems can facilitate physicians' judgments by automaticallylabeling retrieved citations with their strength of evidence categories. Preliminary results ofsuch a text classification experiment involving MEDLINE® citations show that a “bag ofwords” approach is insufficient for accurate classification.,AMIA... Annual Symposium proceedings. AMIA Symposium,2005,7
Are degree achievements really achievements,Jimmy Lin,Abstract This paper; which builds of the work of Hay; Kennedy; and Levin (1999); examinesthe puzzling aspectual behavior of so-called degree achievements. Drawing evidence fromMandarin Chinese; I argue that degree achievements without difference values denotepunctual events; ie; they are true achievements. I present an analysis that is consistent withfacts from both English and Mandarin; to explain the complex aspectual behavior of degreeachievements; my account appeals to coercion operators that are licensed to resolve typeclashes. It differs from previous theories in that complex aspectual behavior arises frominteractions between predicates and other sentential elements; and not from propertiesinherent to the predicates themselves. Cross-linguistic differences can be attributed to theavailability of these operators; which is a parameter of Universal Grammar.,Proceedings of the 9th International Symposium on Chinese Languages and Linguistics; Taipei: Graduate Institute of Linguistics; National Taiwan University,2004,7
Blitz: A preprocessor for detecting context-independent linguistic structures,Boris Katz; Deniz Yuret; Jimmy Lin; Sue Felshin; Rebecca Schulman; Adnan Ilik,Abstract The flow of natural language is often broken by constructions which are difficult toanalyze with conventional linguistic parsers. To handle these constructions; which includenumbers; dates; addresses; etc.; and; to a lesser extent; proper nouns; natural languagesystems typically implement specialized new rules. This leads to a level of complexity whichrenders development and maintenance difficult. Analyzing and tokenizing theseconstructions with an independent preprocessor can alleviate the burden on already taxedsystems. Because these constructions have highly regular forms; and can be largelyunderstood in the absence of context; it is possible to shift the burden of processing awayfrom the primary parser; and onto a simpler; faster; non-linguistic preprocessor. This paperdescribes Blitz; a hybrid database-and heuristic-based natural language preprocessor …,Proceedings of the 5th Pacific Rim Conference on Artificial Intelligence (PRICAI’98),1998,7
Compressing and decoding term statistics time series,Jinfeng Rao; Xing Niu; Jimmy Lin,Abstract There is growing recognition that temporality plays an important role in informationretrieval; particularly for timestamped document collections such as tweets. This paperexamines the problem of compressing and decoding term statistics time series; or counts ofterms within a particular time window across a large document collection. Such data arelarge—essentially the cross product of the vocabulary and the number of time intervals—butare also sparse; which makes them amenable to compression. We explore various integercompression techniques; starting with a number of coding schemes that are well-known inthe information retrieval literature; and build toward a novel compression approach basedon Huffman codes over blocks of term counts. We show that our Huffman-based methodsare able to substantially reduce storage requirements compared to state-of-the-art …,European Conference on Information Retrieval,2016,6
Identifying duplicate and contradictory information in wikipedia,Sarah Weissman; Samet Ayhan; Joshua Bradley; Jimmy Lin,Abstract In this paper; we identify sentences in Wikipedia articles that are either identical orhighly similar by applying techniques for near-duplicate detection of web pages. This isaccomplished with a MapReduce implementation of minhash to identify sentences with highJaccard similarity; followed by a pass to generate sentence clusters. Based on manualexamination; we discovered that these clusters can be categorized into six different types:templates; identical sentences; copyediting; factual drift; references; and other. Two of thesecategories are particularly interesting: identical sentences quantify the extent to whichcontent in Wikipedia is copied and pasted; and near-duplicate sentences that statecontradictory facts point to quality issues in Wikipedia.,Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries,2015,6
Towards Efficient Large-Scale Feature-Rich Statistical Machine Translation,Vladimir Eidelman; Ke Wu; Ferhan Ture; Philip Resnik; Jimmy Lin,Abstract We present the system we developed to provide efficient large-scale feature-richdiscriminative training for machine translation. We describe how we integrate withMapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing asparse feature set. We report our findings on German-English and Russian-Englishtranslation; and discuss benefits; as well as obstacles; to tuning on larger development setsdrawn from the parallel training data.,Proceedings of the Eighth Workshop on Statistical Machine Translation,2013,6
Evaluating real-time search over tweets,Dean McCullough; Jimmy Lin; Craig Macdonald; Iadh Ounis; Richard McCreadie,Abstract Twitter offers a phenomenal platform for the social sharing of information. Wedescribe new resources that have been created in the context of the Text RetrievalConference (TREC) to support the academic study of Twitter as a real-time informationsource. We formalize an information seeking task—real-time search—and offer amethodology for measuring system effectiveness. At the TREC 2011 Microblog Track; 58research groups participated in the first ever evaluation of this task. We present data from theeffort to illustrate and support our methodology.,Sixth International AAAI Conference on Weblogs and Social Media,2012,6
Toward automatic facet analysis and need negotiation: Lessons from mediated search,Jimmy Lin; Philip Wu; Eileen Abels,Abstract This work explores the hypothesis that interactions between a trained humansearch intermediary and an information seeker can inform the design of interactive IRsystems. We discuss results from a controlled Wizard-of-Oz case study; set in the context ofthe TREC 2005 HARD track evaluation; in which a trained intermediary executed anintegrated search and interaction strategy based on conceptual facet analysis and informedby need negotiation techniques common in reference interviews. Having a human “in theloop” yielded large improvements over fully automated systems as measured by standardranked-retrieval metrics; demonstrating the value of mediated search. We present a detailedanalysis of the intermediary's actions to gain a deeper understanding of what worked andwhy. One contribution is a taxonomy of clarification types informed both by empirical …,ACM Transactions on Information Systems (TOIS),2008,6
Situated question answering in the clinical domain: selecting the best drug treatment for diseases,Dina Demner-Fushman; Jimmy Lin,Abstract Unlike open-domain factoid questions; clinical information needs arise within therich context of patient treatment. This environment establishes a number of constraints onthe design of systems aimed at physicians in real-world settings. In this paper; we describe aclinical question answering system that focuses on a class of commonly-occurringquestions:" What is the best drug treatment for X?"; where X can be any disease. To evaluateour system; we built a test collection consisting of thirty randomly-selected diseases from anexisting secondary source. Both an automatic and a manual evaluation demonstrate that oursystem compares favorably to PubMed; the search system most commonly-used byphysicians today.,Proceedings of the Workshop on Task-Focused Summarization and Question Answering,2006,6
Leveraging reusability: Cost-effective lexical acquisition for large-scale ontology translation,G Craig Murray; Bonnie J Dorr; Jimmy Lin; Jan Hajič; Pavel Pecina,Abstract Thesauri and ontologies provide important value in facilitating access to digitalarchives by representing underlying principles of organization. Translation of suchresources into multiple languages is an important component for providing multilingualaccess. However; the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper; wepresent an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing aprobabilistic phrase dictionary and translating a thesaurus of 56;000 concepts used tocatalogue a large archive of oral histories. Our experiments demonstrate a cost-effectivetechnique for accurate machine translation of large ontologies.,Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,2006,6
The roles of natural language and xml in the semantic web,Graham Wilcock; P Buitleaar; Antonio Pareja-Lora; Barrett Bryant; Jimmy Lin; Nancy Ide,*,Huang; CR; Lenders; W,2004,6
Estimating topical volume in social media streams,Praveen Bommannavar; Jimmy Lin; Anand Rajaraman,Abstract This paper tackles the problem of estimating the volume of social media posts (eg;tweets) that pertain to a particular topic. This task differs from related filtering and eventdetection applications in that the filtered content isn't meant for direct human consumption;but rather we are primarily interested in estimating the cardinality of relevant posts. Wepresent a simple yet effective technique for generating and curating keywords to create whatwe call" overlap filters"; which can be applied to a stream of social media posts. Ourapproach leverages human labeling and thus a crucial element of the work involvesminimizing the cost of human computation. On top of a" day zero" cold start algorithm; wedescribe a number of optimizations that take advantage of history to further reduce labelingcosts. Experimental results show that our overlap filters produce accurate volume …,Proceedings of the 31st Annual ACM Symposium on Applied Computing,2016,5
Scaling down distributed infrastructure on wimpy machines for personal Web archiving,Jimmy Lin,Abstract Warcbase is an open-source platform for storing; managing; and analyzing webarchives using modern" big data" infrastructure on commodity clusters---specifically; HBasefor storage and Hadoop for data analytics. This paper describes an effort to scale" down"Warcbase onto a Raspberry Pi; an inexpensive single-board computer about the size of adeck of playing cards. Apart from an interesting technology demonstration; such a designpresents new opportunities for personal web archiving; in enabling a low-cost; low-power;portable device that is able to continuously capture a user's web browsing history---not onlythe URLs of the pages that a user has visited; but the contents of those pages---and allowingthe user to revisit any previously-encountered page; as it appeared at that time. Experimentsshow that data ingestion throughput and temporal browsing latency are adequate with …,Proceedings of the 24th International Conference on World Wide Web,2015,5
On run diversity in evaluation as a service,Ellen M Voorhees; Jimmy Lin; Miles Efron,Abstract" Evaluation as a service"(EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot bedistributed. The basic idea is that evaluation organizers provide a service API through whichthe evaluation task can be completed. However; this concept violates some of the premisesof traditional pool-based collection building and thus calls into question the quality of theresulting test collection. In particular; the service API might restrict the diversity of runs thatcontribute to the pool: this might hamper innovation by researchers and lead to incompletejudgment pools that affect the reusability of the collection. This paper shows that thedistinctiveness of the retrieval runs used to construct the first test collection built using EaaS;the TREC 2013 Microblog collection; is not substantially different from that of the TREC-8 …,Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval,2014,5
Monoidify! monoids as a design principle for efficient mapreduce algorithms,Jimmy Lin,Abstract: It is well known that since the sort/shuffle stage in MapReduce is costly; localaggregation is one important principle to designing efficient algorithms. This short paperrepresents an attempt to more clearly articulate this design principle in terms of monoids;which generalizes the use of combiners and the in-mapper combining pattern. Subjects:Distributed; Parallel; and Cluster Computing (cs. DC); Databases (cs. DB); ProgrammingLanguages (cs. PL),arXiv preprint arXiv:1304.7544,2013,5
Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus,Hua He; Jimmy Lin; Adam Lopez,Abstract Translation models in statistical machine translation can be scaled to large corporaand arbitrarily-long phrases by looking up translations of source phrases “on the fly” in anindexed parallel corpus using suffix arrays. However; this can be slow because on-demandextraction of phrase tables is computationally expensive. We address this problem bydeveloping novel algorithms for general purpose graphics processing units (GPUs); whichenable suffix array queries for phrase lookup and phrase extraction to be massivelyparallelized. Compared to a highly-optimized; state-of-the-art serial CPU-basedimplementation; our techniques achieve at least an order of magnitude improvement interms of throughput. This work demonstrates the promise of massively parallel architecturesand the potential of GPUs for tackling computationallydemanding problems in statistical …,Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,2013,5
Summarization. Encyclopedia of Database Systems,Jimmy Lin; M Ozsu; L Liu,*,*,2009,5
PageRank without Hyperlinks: Reranking with related document networks,Jimmy Lin,Abstract Graph analysis algorithms such as PageRank and HITS have been successful inWeb environments because they are able to extract important inter-document relationshipsfrom manually-created hyperlinks. We consider the application of these algorithms to relateddocument networks comprised of automatically-generated content-similarity links.Specifically; this work tackles the problem of document retrieval in the biomedical domain; inthe context of the PubMed search engine. A series of reranking experiments demonstratethat incorporating evidence extracted from link structure yields significant improvements interms of standard ranked retrieval metrics. These results extend the applicability of linkanalysis algorithms to different environments.,*,2008,5
Computational linguistics for metadata building (CLiMB) Text mining for the automatic extraction of subject terms for image metadata,Judith Klavans; Tandeep Sidhu; Carolyn Sheffield; Dagobert Soergel; Jimmy Lin; Eileen Abels; Rebecca Passonneau,Abstract. In this paper; we present a fully-implemented system using computational linguistictechniques to apply automatic text mining for the extraction of metadata for image access.We describe the implementation of a workbench created for; and evaluated by; imagecatalogers. We discuss the current functionality and future goals for this image catalogers'toolkit; developed in the Computational Linguistics for Metadata Building (CLiMB) researchproject. 1 Our primary user group for initial phases of the project is the cataloger expert; infuture work we address applications for end users.,Proceedings from: VISAPP Workshop Metadata Mining for Image Understanding. Madeira; Portugal,2008,5
Multiple Alternative Sentence Compressions and Word-Pair Antonymy for Automatic Text Summarization and Recognizing Textual Entailment.,Saif Mohammad; Bonnie J Dorr; Melissa Egan; Nitin Madnani; David M Zajic; Jimmy J Lin,Abstract The University of Maryland participated in three tasks organized by the TextAnalysis Conference 2008 (TAC 2008):(1) the update task of text summarization;(2) theopinion task of text summarization; and (3) recognizing textual entailment (RTE). At the heartof our summarization system is Trimmer; which generates multiple alternative compressedversions of the source sentences that act as candidate sentences for inclusion in thesummary. For the first time; we investigated the use of automatically generated antonympairs for both text summarization and recognizing textual entailment. The UMD summariesfor the opinion task were especially effective in providing non-redundant information (rank 3out of a total 19 submissions). More coherent summaries resulted when using the antonymyfeature as compared to when not using it. On the RTE task; even when using only …,TAC,2008,5
Concept disambiguation for improved subject access using multiple knowledge sources,Tandeep Sidhu; Judith Klavans; Jimmy Lin,Abstract We address the problem of mining text for relevant image metadata. Our work issituated in the art and architecture domain; where highly specialized technical vocabularypresents challenges for NLP techniques. To extract high quality metadata; the problem ofword sense disambiguation must be addressed in order to avoid leading the searcher to thewrong image as a result of ambiguous—and thus faulty—metadata. In this paper; we presenta disambiguation algorithm that attempts to select the correct sense of nouns in textualdescriptions of art objects; with respect to a rich domain-specific thesaurus; the Art andArchitecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a dataset of 600 subject terms extracted from an online National Gallery of Art (NGA) collection ofimages and text. Our results showed that the use of external knowledge sources shows …,Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007).,2007,5
TREC 2007 ciQA Task: University of Maryland.,Nitin Madnani; Jimmy J Lin; Bonnie J Dorr,Information needs are complex; evolving; and difficult to express or capture (Taylor; 1962); afact that is often overlooked by modern information retrieval systems. TREC; through theHARD track; has been attempting to introduce elements of interaction into large-scaleevaluations in order to achieve high accuracy document retrieval (Allan; 2005). Previousresearch has shown that well-constructed clarification questions can yield a betterunderstanding of users' information needs and thereby improve retrieval performance (Lin etal.; 2006). Interactive question answering has recently become a focus of research in thecontext of complex QA. The topics in the ciQA task are substantially different from factoidquestions in that the information needs are complex; multi-faceted; and often not welldefined or expressed. To investigate the role of interaction in complex QA; we …,TREC,2007,5
Exploring the limits of single-iteration clarification dialogs,Jimmy Lin; Philip Wu; Dina Demner-Fushman; Eileen Abels,Abstract Single-iteration clarification dialogs; as implemented in the TREC HARD track;represent an attempt to introduce interaction into ad hoc retrieval; while preserving the manybenefits of large-scale evaluations. Although previous experiments have not conclusivelydemonstrated performance gains resulting from such interactions; it is unclear whether thesefindings speak to the nature of clarification dialogs; or simply the limitations of currentsystems. To probe the limits of such interactions; we employed a human intermediary toformulate clarification questions and exploit user responses. In addition to establishing aplausible upper bound on performance; we were also able to induce an" ontology ofclarifications" to characterize human behavior. This ontology; in turn; serves as the input to aregression model that attempts to determine which types of clarification questions are …,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,5
Representation of information needs and the elements of context: A case study in the domain of clinical medicine,Jimmy Lin; DD Fushman,Information seekers today have an ingrained habit of issuing extremely short queries toretrieval systems. Although the dominance of Web search contributes to reinforcement of thisbehavior; the inability for retrieval systems to take advantage of more expressive queryrepresentations is also a culprit. Short queries pose a challenge to designers of informationretrieval systems for a number of reasons: At the linguistic level; they often containpolysemous words that are difficult to disambiguate without appropriate contextual cues. At ahigher level; short queries do not provide clues regarding the broader activities that give riseto the user's information need. There is reason to believe that if a computer system wereable to solicit richer queries from a user; it might be able to build a better model of theinformation seeking process; thereby leading to higher retrieval performance—for …,ACM SIGIR 2005 Workshop on Information Retrieval in Context (IRiX),2005,5
Fine-grained lexical semantic representations and compositionally-derived events in Mandarin Chinese,Jimmy Lin,Abstract Current lexical semantic representations for natural language applications viewverbs as simple predicates over their arguments. These structures are too coarse-grained tocapture many important generalizations about verbal argument structure. In this paper; Ispecifically defend the following two claims: verbs have rich internal structure expressible interms of finer-grained primitives of meaning; and at least for some languages; verbalmeaning is compositionally derived from these primitive elements. I primarily presentevidence from Mandarin Chinese; whose verbal system is very different from that of English.Many empirical facts about the typology of verbs in Mandarin cannot be captured by a" flat"lexical semantic representation. These theoretical results hold important practicalconsequences for natural language processing applications.,Proceedings of the HLT-NAACL Workshop on Computational Lexical Semantics,2004,5
Viewing the Web as a Virtual Database for Question Answering.,Boris Katz; Sue Felshin; Jimmy J Lin; Gregory Marton,Abstract Although the World Wide Web contains a tremendous amount of information; thelack of intuitive information access methods and the paucity of uniform structure makefinding the right knowledge difficult. Our solution is to turn the Web into a “virtual database”and to access it through natural language. We have accomplished this by developing astylized relational framework; called the object-property-value model; which captures theregularity found in both natural language questions and Web resources. We have adoptedthis framework in START and Omnibase; two components of a system that understandsnatural language questions and responds with answers extracted on the fly fromheterogeneous and semistructured Web sources. Our system can answer millions ofquestions from hundreds of Web resources with high precision.,New Directions in Question Answering,2004,5
Better public policy through natural language information access,Boris Katz; Roger Hurwitz; Jimmy J Lin; Ozlem Uzuner,Abstract Federal agencies implement laws passed by the Congress by creating rules andregulations that can be applied in practice. During this process; staffs at the variousagencies may review past and current regulations and receive comments from stakeholdersand the public regarding the proposed regulations. Putting rulemaking online can increasethe public's awareness of the proposed rules and its participation in the process. It can alsofacilitate staff work. A key factor in realizing these benefits will be the availability of simple;intuitive; and timely access to the empowering legislation; the proposed rules andinformation regarding them. We propose to provide such access through an informationarchitecture that allows members of the public as well as staff and stakeholders to obtain thetexts and information they desire by using everyday language. Over the past decade; we …,Proceedings of the 2003 annual national conference on Digital government research,2003,5
Information Access Using Natural Language,Boris Katz; Sue Felshin; Luciano Castagnola; Aaron Fernandes; Ali Ibrahim; Jimmy Lin; Jerome McFarland; Alp Simsek; Baris Temelkuran,The Problem: With recent advances in computer and Internet technology; people haveaccess to more information than ever before. As the amount of information grows; so doesthe problem of finding what one is looking for. We believe that the most natural form ofcommunication and information access for humans is natural language. We propose toaddress the growing information access problem with a uniform natural language interface.Motivation: In December 1993; START (SynTactic Analysis using ReversibleTransformations) became the first natural language system available for question answeringon the World Wide Web. Since then START has been involved in dialogs with users all overthe world; answering millions of questions. As we added more and more information toSTART's knowledge base; we discovered the advantages of “virtual collaboration.” We …,Artificial Intelligence Laboratory; http://www. ai. mit. edu,2001,5
Talking to Your TV: Context-Aware Voice Search with Hierarchical Recurrent Neural Networks,Jinfeng Rao; Ferhan Ture; Hua He; Oliver Jojic; Jimmy Lin,Abstract We tackle the novel problem of navigational voice queries posed against anentertainment system; where viewers interact with a voice-enabled remote controller tospecify the TV program to watch. This is a difficult problem for several reasons: such queriesare short; even shorter than comparable voice queries in other domains; which offers feweropportunities for deciphering user intent. Furthermore; ambiguity is exacerbated byunderlying speech recognition errors. We address these challenges by integrating word-andcharacter-level query representations and by modeling voice search sessions to capture thecontextual dependencies in query sequences. Both are accomplished with a probabilisticframework in which recurrent and feedforward neural network modules are organized in ahierarchical manner. From a raw dataset of 32M voice queries from 2.5 M viewers on the …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,4
Online in-situ interleaved evaluation of real-time push notification systems,Adam Roegiest; Luchen Tan; Jimmy Lin,Abstract Real-time push notification systems monitor continuous document streams such associal media posts and alert users to relevant content directly on their mobile devices. Wedescribe a user study of such systems in the context of the TREC 2016 Real-TimeSummarization Track; where system updates are immediately delivered as pushnotifications to the mobile devices of a cohort of users. Our study represents; to ourknowledge; the first deployment of an interleaved evaluation framework for prospectiveinformation needs; and also provides an opportunity to examine user behavior in a realisticsetting. Results of our online in-situ evaluation are correlated against the results a moretraditional post-hoc batch evaluation. We observe substantial correlations between manyonline and batch evaluation metrics; especially for those that share the same basic …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,4
Experiments with Convolutional Neural Network Models for Answer Selection,Jinfeng Rao; Hua He; Jimmy Lin,Abstract In recent years; neural networks have been applied to many text processingproblems. One example is learning a similarity function between pairs of text; which hasapplications to paraphrase extraction; plagiarism detection; question answering; and ad hocretrieval. Within the information retrieval community; the convolutional neural network modelproposed by Severyn and Moschitti in a SIGIR 2015 paper has gained prominence. Thispaper focuses on the problem of answer selection for question answering: we attempt toreplicate the results of Severyn and Moschitti using their open-source code as well as toreproduce their results via a de novo (ie; from scratch) implementation using a completelydifferent deep learning toolkit. Our de novo implementation is instructive in ascertainingwhether reported results generalize across toolkits; each of which have their …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,4
The role of index compression in score-at-a-time query evaluation,Jimmy Lin; Andrew Trotman,Abstract This paper explores the performance of top k document retrieval with score-at-a-time query evaluation on impact-ordered indexes in main memory. To better understandexecution efficiency in the context of modern processor architectures; we examine the role ofindex compression on query evaluation latency. Experiments include compressing postingswith variable byte encoding; Simple-8b; variants of the QMX compression scheme; as wellas a condition that is less often considered—no compression. Across four web testcollections; we find that the highest query evaluation speed is achieved by simply leavingthe postings lists uncompressed; although the performance advantage over a state-of-the-artcompression scheme is relatively small and the index is considerably larger. We explain thisfinding in terms of the design of modern processor architectures: Index segments with …,Information Retrieval Journal,2017,4
Temporal query expansion using a continuous hidden markov model,Jinfeng Rao; Jimmy Lin,Abstract In standard formulations of pseudo-relevance feedback; document timestamps donot play a role in identifying expansion terms. Yet we know that when searching socialmedia posts such as tweets; relevant documents are bursty and usually occur in temporalclusters. The main insight of our work is that term expansions should be biased to draw fromdocuments that occur in bursty temporal clusters. This is formally captured by a continuoushidden Markov model (cHMM); for which we derive an EM algorithm for parameterestimation. Given a query; we estimate the parameters for a cHMM that best explains theobserved distribution of an initial set of retrieved documents; and then use Viterbi decodingto compute the most likely state sequence. In identifying expansion terms; we only selectdocuments from bursty states. Experiments on test collections from the TREC 2011 and …,Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,2016,4
A platform for streaming push notifications to mobile assessors,Adam Roegiest; Luchen Tan; Jimmy Lin; Charles LA Clarke,Abstract We present an assessment platform for gathering online relevance judgments formobile push notifications that will be deployed in the newly-created TREC 2016 Real-TimeSummarization (RTS) track. There is emerging interest in building systems that filter socialmedia streams such as tweets to identify interesting and novel content in real time; putativelyfor delivery to users' mobile phones. In our evaluation design; all participants subscribe tothe Twitter streaming API to identify relevant tweets with respect to a set of interest profiles.As the systems generate results; they are pushed in real time to our evaluation broker via aREST API. The broker then" routes" the tweets to assessors who have installed a customapp on their mobile phones. We detail the design of this platform and discuss a number ofchallenges that need to be tackled in this type of" Living Labs" setup. It is our goal that …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,4
Mr. mira: Open-source large-margin structured learning on mapreduce,Vladimir Eidelman; Ke Wu; Ferhan Ture; Philip Resnik; Jimmy Lin,Abstract We present an open-source framework for large-scale online structured learning.Developed with the flexibility to handle cost-augmented inference problems such asstatistical machine translation (SMT); our large-margin learner can be used with anydecoder. Integration with MapReduce using Hadoop streaming allows efficient scaling withincreasing size of training data. Although designed with a focus on SMT; the decoder-agnostic design of our learner allows easy future extension to other structured learningproblems such as sequence labeling and parsing.,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,2013,4
Document vector representations for feature extraction in multi-stage document ranking,Nima Asadi; Jimmy Lin,Abstract We consider a multi-stage retrieval architecture consisting of a fast;“cheap”candidate generation stage; a feature extraction stage; and a more “expensive” rerankingstage using machine-learned models. In this context; feature extraction can beaccomplished using a document vector index; a mapping from document ids to documentrepresentations. We consider alternative organizations of such a data structure for efficientfeature extraction: design choices include how document terms are organized; how complexterm proximity features are computed; and how these structures are compressed. Inparticular; we propose a novel document-adaptive hashing scheme for compactly encodingterm ids. The impact of alternative designs on both feature extraction speed and memoryfootprint is experimentally evaluated. Overall; results show that our architecture is …,*,2012,4
A cost-effective lexical acquisition process for large-scale thesaurus translation,Jimmy Lin; G Craig Murray; Bonnie J Dorr; Jan Hajič; Pavel Pecina,Abstract Thesauri and controlled vocabularies facilitate access to digital collections byexplicitly representing the underlying principles of organization. Translation of suchresources into multiple languages is an important component for providing multilingualaccess. However; the specificity of vocabulary terms in most thesauri precludes fully-automatic translation using general-domain lexical resources. In this paper; we present anefficient process for leveraging human translations to construct domain-specific lexicalresources. This process is illustrated on a thesaurus of 56;000 concepts used to catalog alarge archive of oral histories. We elicited human translations on a small subset of concepts;induced a probabilistic phrase dictionary from these translations; and used the resultingresource to automatically translate the rest of the thesaurus. Two separate evaluations …,Language resources and evaluation,2009,4
Leveraging recurrent phrase structure in large-scale ontology translation,G Craig Murray; Bonnie Dorr; Jimmy Lin; Jan Hajič; Pavel Pecina,Abstract. This paper presents a process for leveraging structural relationships and reusablephrases when translating large-scale ontologies. Digital libraries are becoming more andmore prevalent. An important step in providing universal access to such material is toprovide multi-lingual access to the underlying principles of organization via ontologies;thesauri; and controlled vocabularies. Machine translation of these resources requires highaccuracy and a deep vocabulary. Human input is often required; but full manual translationcan be slow and expensive. We report on a cost-effective approach to ontology translation.We describe our technique of prioritization; our process of collecting aligned translationsand generating a new lexicon; and the resulting improvement to translation system output.Our preliminary evaluation indicates that this technique provides significant cost savings …,Proceedings of the 11th Annual Conference of the European Association for Machine Translation,2006,4
Sentence trimming and selection: Mixing and matching,David M Zajic; Bonnie J Dorr; Jimmy Lin; John M Conroy; Dianne P O’leary; Judith D Schlesinger,Abstract We describe how components from two distinct multi-document summarizationsystems were combined. Twenty four possible combinations of components wereconsidered. We observed some contrasts between conservative and aggressive sentencecompression (ie; trimming) in the context of multidocument summarization.,Proceedings of DUC,2006,4
Umd/bbn at mse2005,David Zajic; Bonnie J Dorr; Jimmy Lin; Richard Schwartz; D Zajic; B Dorr; J Lin,Abstract We implemented an initial application of a sentence-trimming approach (Trimmer)to the problem of multi-document summarization in the MSE2005 task. Sentence trimmingwas incorporated into a feature-based summarization system; called Multi-DocumentTrimmer (MDT); by using sentence trimming as both a pre-processing stage and a feature forsentence ranking. We demonstrate that we were able to port Trimmer easily to this newproblem; although the impact of sentence trimming was minimal compared to other featuresused in the system. The performance of our system in the official MSE2005 task was aroundthe middle of the pack (16 out of 27). After some minor bug fixes and a simple correction(dateline removal) we obtained an improvement on a post-hoc run on the test data.,Proceedings of the MSE2005 track of the Association for Computational Linguistics Workshop on Intrinsic and Extrinsic Evaluation Meatures for MT and/or Summarization; Ann Arbor; MI,2005,4
Recounting the Courts? Applying Automated Content Analysis to Enhance Empirical Legal Research’(2007),Michael Evans; Wayne McIntosh; Jimmy Lin; Cynthia Cates,*,Journal of Empirical Legal Studies,*,4
Anserini: Enabling the Use of Lucene for Information Retrieval Research,Peilin Yang; Hui Fang; Jimmy Lin,Abstract Software toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrievalmodels over standard test collections. Efforts are generally directed toward better rankingand less attention is usually given to scalability and other operational considerations. On theother hand; Lucene has become the de facto platform in industry for building searchapplications (outside a small number of companies that deploy custom infrastructure).Compared to academic IR toolkits; Lucene can handle heterogeneous web collections atscale; but lacks systematic support for evaluation over standard test collections. This paperintroduces Anserini; a new information retrieval toolkit that aims to provide the best of bothworlds; to better align information retrieval practice and research. Anserini provides …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,3
In Vacuo and In Situ Evaluation of SIMD Codecs,Andrew Trotman; Jimmy Lin,Abstract The size of a search engine index and the time to search are inextricably relatedthrough the compression codec. This investigation examines this tradeoff using severalrelatively unexplored SIMD-based codecs including QMX; TurboPackV; and TurboPFor. Ituses (the non-SIMD) OPTPFor as a baseline. Four new variants of QMX are introduced andalso compared. Those variants include optimizations for space and for time. Experimentswere conducted on the TREC. gov2 collection using topics 701-850; in crawl order and inURL order. The results suggest that there is very little difference between these codecs; butthat the reference implementation of QMX performs well.,Proceedings of the 21st Australasian Document Computing Symposium,2016,3
Retrievability in API-Based Evaluation as a Service,Jiaul H Paik; Jimmy Lin,Abstract" Evaluation as a service"(EaaS) refers to a family of related evaluationmethodologies that enables community-wide evaluations and the construction of testcollections on documents that cannot be easily distributed. In the API-based approach; thebasic idea is that evaluation organizers provide a service API through which the evaluationtask can be completed; without providing access to the raw collection. One concern with thisevaluation approach is that the API introduces biases and limits the diversity of techniquesthat can be brought to bear on the problem. In this paper; we tackle the question of API biasusing the concept of retrievability. The raw data for our analyses come from a naturally-occurring experiment where we observed the same groups completing the same task withthe API and also with access to the raw collection. We find that the retrievability bias of …,Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,2016,3
Is big data a transient problem?,Jimmy Lin,What does the future hold for Big Data? The author states that it could be the samequalitatively; just bigger and better; or there might be fundamentally disruptive forces thatcompletely reshape the computing landscape. Trying to predict the future; of course; is aperilous exercise. At best; this article provides some deep insight on future developments inBig Data. At worst; it makes for an interesting cocktail conversation. Either way; it's worth therumination.,IEEE Internet Computing,2015,3
Gappy pattern matching on gpus for on-demand extraction of hierarchical translation grammars,Hua He; Jimmy Lin; Adam Lopez,Abstract Grammars for machine translation can be materialized on demand by findingsource phrases in an indexed parallel corpus and extracting their translations. Thisapproach is limited in practical applications by the computational expense of online lookupand extraction. For phrase-based models; recent work has shown that on-demand grammarextraction can be greatly accelerated by parallelization on general purpose graphicsprocessing units (GPUs); but these algorithms do not work for hierarchical models; whichrequire matching patterns that contain gaps. We address this limitation by presenting a novelGPU algorithm for on-demand hierarchical grammar extraction that is at least an order ofmagnitude faster than a comparable CPU algorithm when processing large batches ofsentences. In terms of end-to-end translation; with decoding on the CPU; we increase …,Transactions of the Association of Computational Linguistics,2015,3
Infrastructure support for evaluation as a service,Jimmy Lin; Miles Efron,Abstract How do we conduct large-scale community-wide evaluations for informationretrieval if we are unable to distribute the document collection? This was the challenge wefaced in organizing a task on searching tweets at the Text Retrieval Conference (TREC);since Twitter's terms of service forbid redistribution of tweets. Our solution; which we call"evaluation as a service"; was to provide an API through which the collection can beaccessed for completing the evaluation task. This paper describes the infrastructureunderlying the service and its deployment at TREC 2013. We discuss the merits of theapproach and potential applicability to other evaluation scenarios.,Proceedings of the 23rd International Conference on World Wide Web,2014,3
Supporting “distant reading” for web archives,Jimmy Lin; Kari Kraus; R Punzalan,In a recent essay on the stock footage libraries amassed by Hollywood studios in the firsthalf of the 20th century; Rick Prelinger—moving image archivist at the Internet Archive—laments that “archives often seem like a first-aid kit or a rusty tool; resources that we findreassuring but rarely use”(Prelinger 2012). Although he doesn't single them out by name;web archives are particularly vulnerable to this charge. User studies; access statistics; pageviews; and other metrics have in recent years told a consistent story: web content that hasbeen harvested and preserved by collecting institutions; universities; and otherorganizations often lies fallow; and like Prelinger's rusty tool may be notable more for itslatent potential than for having served any real purpose (Hockx-Yu 2013; Kamps 2013;Huurdeman et al 2013). While the reasons for neglect are myriad; this paper focuses on …,Proc. Digital Humanities,2014,3
An exploration of postings list contiguity in main-memory incremental indexing,Nima Asadi; Jimmy Lin,ABSTRACT For text retrieval systems; the assumption that all data structures reside in mainmemory is increasingly common. In this context; we present a novel incremental invertedindexing algorithm for web-scale collections that directly constructs compressed postingslists in memory. Designing efficient in-memory algorithms requires understanding modernprocessor architectures: in this paper; we explore the issue of postings list contiguity.Postings lists that occupy contiguous memory regions are preferred for retrieval; butmaintaining contiguity is costly in terms of speed and complexity. On the other hand;allowing discontiguous index segments simplifies index construction but decreases retrievalperformance. Understanding this tradeoff is our main contribution: We show that co-locatingsmall groups of inverted list segments yields query evaluation performance that is …,WSDM Workshop on Large-Scale and Distributed Systems for Information Retrieval,2014,3
Flat vs. hierarchical phrase-based translation models for cross-language information retrieval,Ferhan Ture; Jimmy Lin,Abstract Although context-independent word-based approaches remain popular for cross-language information retrieval; many recent studies have shown that integrating insightsfrom modern statistical machine translation systems can lead to substantial improvements ineffectiveness. In this paper; we compare flat and hierarchical phrase-based translationmodels for query translation. Both approaches yield significantly better results than either atoken-based or a one-best translation baseline on standard test collections. The choice ofmodel manifests interesting tradeoffs in terms of effectiveness; efficiency; and modelcompactness.,Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,2013,3
Fast; incremental inverted indexing in main memory for web-scale collections,Nima Asadi; Jimmy Lin,Abstract: For text retrieval systems; the assumption that all data structures reside in mainmemory is increasingly common. In this context; we present a novel incremental invertedindexing algorithm for web-scale collections that directly constructs compressed postingslists in memory. Designing efficient in-memory algorithms requires understanding modernprocessor architectures and memory hierarchies: in this paper; we explore the issue ofpostings lists contiguity. Naturally; postings lists that occupy contiguous memory regions arepreferred for retrieval; but maintaining contiguity increases complexity and slows indexing.On the other hand; allowing discontiguous index segments simplifies index construction butdecreases retrieval performance. Understanding this tradeoff is our main contribution: Wefind that co-locating small groups of inverted list segments yields query evaluation …,arXiv preprint arXiv:1305.0699,2013,3
UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory.,Tamer Elsayed; Nima Asadi; Lidan Wang; Jimmy J Lin; Donald Metzler,ABSTRACT Ivory is a web-scale retrieval engine we have been developing for the past twoyears; built around a cluster-based environment running Hadoop; the open-sourceimplementation of the MapReduce programming model. Building on successes last year atTREC; we explored two major directions this year: more sophisticated retrieval models andlarge-scale graph analysis for spam detection. We describe results of ad hoc retrievalexperiments with latent concept expansion and a greedily-learned linear ranking model.Although neither model is novel; our experiments provide some insight on the behavior ofthese two approaches at scale; on collections larger than those previously studied. We alsodiscuss our link-based spam filtering algorithm that operated on the entire web graph ofClueWeb09. Unfortunately; results in the spam track were worse than the baseline …,TREC,2010,3
The maryland modular method: An approach to doctoral education in information studies,Allison Druin; Paul T Jaeger; Jennifer Golbeck; Kenneth R Fleischmann; Jimmy Lin; Yan Qu; Ping Wang; Bo Xie,As the field of information studies has matured into a broad interdisciplinary and multi-disciplinary field of study; the expectations for and range of students seeking doctoraleducation have evolved significantly. The majority of information studies pedagogicalliterature; however; continues to focus on the master's level. Building on efforts of theCollege of Information Studies at the University of Maryland to develop a new doctoralprogram; this paper presents a modular approach to doctoral education. We argue for thevalue of designing doctoral education models that embrace the unique interdisciplinary andmulti-disciplinary nature of information studies; highlighting a combination of conceptuallenses and content modules as a way to conceive new approaches to doctoral educationthat foster students' ability to conduct research in their areas of interest while …,Journal of Education for Library and Information Science,2009,3
Computational linguistics for metadata building.,Judith L Klavans; Carolyn Sheffield; Jimmy J Lin; Tandeep Sidhu,ABSTRACT The first stage of CLiMB's processing pipeline segments text into topicalportions and associates these with relevant images. The next phase; Linguistic Analysis;consists of several subprocesses. A part-of-speech (POS) tagger labels the function of eachword within a sentence. Complete noun phrases are identified by the NP chunker based onpatterns. CLiMB uses the Stanford tagger,JCDL,2008,3
Computational linguistics for metadata building: Aggregating text processing technologies for enhanced image access,Judith Klavans; Carolyn Sheffield; Eileen Abels; Joan E Beaudoin; Laura Jenemann; Jimmy Lin; Tom Lippincott; Rebecca Passonneau; Tandeep Sidhu; Dagobert Soergel; Tae Yano,Abstract We present a system which applies text mining using computational linguistictechniques to automatically extract; categorize; disambiguate and filter metadata for imageaccess. Candidate subject terms are identified through standard approaches; novelsemantic categorization using machine learning and disambiguation using both WordNetand a domain specific thesaurus are applied. The resulting metadata can be manuallyedited by image catalogers or filtered by semi-automatic rules. We describe theimplementation of this workbench created for; and evaluated by; image catalogers. Wediscuss the system's current functionality; developed under the Computational Linguistics forMetadata Building (CLiMB) research project. The CLiMB Toolkit has been tested withseveral collections; including: Art Images for College Teaching (AICT); ARTStor; the …,*,2008,3
Recounting the courts? Toward A text-centered computational approach to understanding the dynamics of the judicial system,Michael Evans; Wayne McIntosh; Cynthia L Cates; Jimmy Lin,Abstract This paper explores the potential uses of computational linguistics techniques foranalyzing Supreme Court briefs and opinions. To do so; we focused on advocacydocuments associated with the two recent University of Michigan affirmative action cases(Gratz v. Bollinger and Grutter v. Bollinger). The cases attracted more than one hundredamicus briefs; which provide a rich textual database for such an exploratory study. The goalof our preliminary work is to model the linguistic contents of the arguments presented by thepetitioners and respondents; as captured in the original litigants' briefs and the amici briefssubmitted in these two cases. In particular; we are interested in the types of words andphrases used by both sides in order to forward their arguments. These linguistic cues mayprovide us with insight into the policy and ideological inclinations of the parties involved …,Annual Meeting of the Midwest Political Science Association; Chicago; IL,2005,3
Annotating the Semantic Web Using Natural Language,Boris Katz Jimmy Lin,Abstract Because the ultimate purpose of the Semantic Web is to help users better locate;organize; and process content; we believe that it should be grounded in the informationaccess method humans are most comfortable with—natural language. However; theResource Description Framework (RDF); the foundation of the Semantic Web; was designedto be easily processed by computers; not humans. To render RDF more friendly to humans;we propose to augment it with natural language annotations; or metadata written ineveryday language. We argue that natural language annotations; parsed intocomputerreadable representations; are not only intuitive and effective; but can alsoaccelerate the pace with which the Semantic Web is being adopted. We believe that ourtechnology can facilitate a happy marriage between natural language technology and the …,Proceedings of the 2nd Workshop on NLP and XML (NLPXML 2002) at COLING,2002,3
Honk: A PyTorch reimplementation of convolutional neural networks for keyword spotting,Raphael Tang; Jimmy Lin,Abstract: We describe Honk; an open-source PyTorch reimplementation of convolutionalneural networks for keyword spotting that are included as examples in TensorFlow. Thesemodels are useful for recognizing" command triggers" in speech-based interfaces (eg;" HeySiri"); which serve as explicit cues for audio recordings of utterances that are sent to thecloud for full speech recognition. Evaluation on Google's recently released SpeechCommands Dataset shows that our reimplementation is comparable in accuracy andprovides a starting point for future work on the keyword spotting task.,arXiv preprint arXiv:1710.06554,2017,2
Integrating lexical and temporal signals in neural ranking models for searching social media streams,Jinfeng Rao; Hua He; Haotian Zhang; Ferhan Ture; Royal Sequiera; Salman Mohammed; Jimmy Lin,Abstract: Time is an important relevance signal when searching streams of social mediaposts. The distribution of document timestamps from the results of an initial query can beleveraged to infer the distribution of relevant documents; which can then be used to rerankthe initial results. Previous experiments have shown that kernel density estimation is asimple yet effective implementation of this idea. This paper explores an alternative approachto mining temporal signals with recurrent neural networks. Our intuition is that neuralnetworks provide a more expressive framework to capture the temporal coherence ofneighboring documents in time. To our knowledge; we are the first to integrate lexical andtemporal signals in an end-to-end neural network architecture; in which existing neuralranking models are used to generate query-document similarity vectors that feed into a …,arXiv preprint arXiv:1707.07792,2017,2
Searching from mars,Jimmy Lin; Charles LA Clarke; Gaurav Baruah,jaNUaRy/fEbRUaRy 2016 79 space missions throughout history; it's hard to imagine howsuch an approach is sustainable for a permanent colony. Indeed; we're already movingaway from such rigid interactions: for example; personal Internet use is possible on theInternational Space Station today. Thus; we want to be able to search from Mars. Anothercategory of information needs will likely be scholarly search. An important goal of Marsmissions is to advance science; so our colonists will require access to all of the scientificliterature on Earth. For example; the colonists might want to publish about breakthroughs inhydroponics; and thus would need the Internet in exactly the same way that an Earthboundscientist would: looking up related work; reading papers; interacting with peers; and so on.Although it might be possible to have an Earthside co-author handle all these interactions …,IEEE Internet Computing,2016,2
Cumulative Citation Recommendation: A Feature-Aware Comparison of Approaches,Gebrekirstos G Gebremeskel; Jiyin He; Arjen P De Vries; Jimmy Lin,In this work; we conduct a feature-aware comparison of approaches to Cumulative CitationRecommendation (CCR); a task that aims to filter and rank a stream of documents accordingto their relevance to entities in a knowledge base. We conducted experiments starting with abig feature set; identified a powerful subset and applied it to comparing classification andlearning-to-rank algorithms. With few set of powerful features; we achieve betterperformance than the state-of-the-art. Surprisingly; our findings challenge the previouslyknown preference of learning-to-rank over classification: in our study; the CCR performanceof the classification approach outperforms that using learning-to-rank. This indicates thatcomparing two approaches is problematic due to the interplay between the approachesthemselves and the feature sets one chooses to use.,Database and Expert Systems Applications (DEXA); 2014 25th International Workshop on,2014,2
A parallel algorithm for computing incomplete information systems under big data,Jiang Lin; Mi Yunlong; Wang Tian,The lower and upper approximations are important concepts in rough set theory. Therefore;the computation of approximations is the basic for improving the massive data miningperformance. Classical approximation space algorithm is infeasible for massive data; muchless for massive data with missing information. To this end; through deep analysis of thecharacteristics of massive data with missing information; combining with the MapReduceprogramming model; a parallel algorithm for computing incomplete information systemsusing MapReduce is put forward to deal with the massive data with missing information. Theexperimental results demonstrate that the proposed parallel algorithm is effective.,Computer Engineering and Applications,2014,2
Overview of the TREC-2014 microblog track (notebook draft),Jimmy Lin; Miles Efron; Yulu Wang; S Garrick,This year represents the fourth iteration of the TREC Microblog track; which has beenrunning since 2011. The track continued using the “evaluation as a service” model [8; 7]; inwhich participants had access to the document collection only through an API. In addition tothe temporallyanchored ad hoc retrieval task; which has been running since the inception ofthe track; we introduced a new task called tweet timeline generation (TTG); where the goal isto produce concise “summaries” about a particular topic for human consumption. Althoughthis overview covers both tasks; more emphasis is placed on the tweet timeline generationtask; which necessitated the development of a new evaluation methodology. We refer thereader to previous track overview papers [8; 12; 9] for details on the setup of the ad hoc task.,Proceedings of TREC,2014,2
Runtime optimizations for prediction with tree-based models,Nima Asadi; Jimmy Lin; Arjen P de Vries,Abstract: Tree-based models have proven to be an effective solution for web ranking as wellas other problems in diverse domains. This paper focuses on optimizing the runtimeperformance of applying such models to make predictions; given an already-trained model.Although exceedingly simple conceptually; most implementations of tree-based models donot efficiently utilize modern superscalar processor architectures. By laying out datastructures in memory in a more cache-conscious fashion; removing branches from theexecution flow using a technique called predication; and micro-batching predictions using atechnique called vectorization; we are able to better exploit modern processor architecturesand significantly improve the speed of tree-based models over hard-coded if-else blocks.Our work contributes to the exploration of architecture-conscious runtime implementations …,arXiv preprint arXiv:1212.2287,2012,2
A Study of" Churn" in Tweets and Real-Time Search Queries (Extended Version),Jimmy Lin; Gilad Mishne,Abstract: The real-time nature of Twitter means that term distributions in tweets and in searchqueries change rapidly: the most frequent terms in one hour may look very different fromthose in the next. Informally; we call this phenomenon" churn". Our interest in analyzingchurn stems from the perspective of real-time search. Nearly all ranking functions; machine-learned or otherwise; depend on term statistics such as term frequency; documentfrequency; as well as query frequencies. In the real-time context; how do we compute thesestatistics; considering that the underlying distributions change rapidly? In this paper; wepresent an analysis of tweet and query churn on Twitter; as a first step to answering thisquestion. Analyses reveal interesting insights on the temporal dynamics of term distributionson Twitter and hold implications for the design of search systems.,arXiv preprint arXiv:1205.6855,2012,2
Special issue on cloud computing,Gregory Chockler; Eliezer Dekel; Joseph JaJa; Jimmy Lin,*,*,2011,2
Special Issue of the Journal of Parallel and Distributed Computing: Cloud Computing,Gregory Chockler; Eliezer Dekel; Joseph JaJa; Jimmy Lin,*,*,2009,2
Users' adjustments to unsuccessful queries in biomedical search,G Craig Murray; Jimmy Lin; John Wilbur; Zhiyong Lu,Abstract Biomedical researchers depend on on-line databases and digital libraries for up todate information. We introduce a pilot project aimed at characterizing adjustments made tobiomedical queries that improve search results. Specifically we focus on queries submittedto PubMed®; a large sophisticated search engine that facilitates Web access to abstracts ofarticles in over 5;200 biomedical journals. On average 2 million users search PubMed eachday. During their search; nearly 20% will experience a result page from one of their queriesthat has zero results. In some cases there really is no document or abstract that will satisfy aparticular query. However; in analyzing one month of queries submitted to PubMed; we findthat more often than not; queries that retrieved no results are queries that would retrievesomething relevant if they were constructed differently. This paper describes a new effort …,Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries,2009,2
Interfaces to support the scholarly exploration of text collections,Georg Apitz; Jimmy Lin,ABSTRACT The analysis of text collections forms the basis of scholarship in manydisciplines in the humanities and social sciences. Despite the growing availability ofelectronic texts; automated techniques have not been effectively exploited to support theactivities of scholars in these fields. We present a prototype search interface for exploringtext collections that places equal emphasis on content; what the document is about; andmetadata; the context that situates a piece of text. As a start; we focus on a selection of briefsand opinions from the US Supreme Court to support legal scholars.,ESI 2007,2007,2
Computational Linguistics for Metadata Building (CLiMB,Judith L Klavans; Eep Sidhu; Carolyn Sheffield; Dagobert Soergel; Jimmy Lin; Eileen Abels; Rebecca Passonneau,Abstract. In this paper; we present a fully-implemented system using computational linguistictechniques to apply automatic text mining for the extraction of metadata for image access.We describe the implementation of a workbench created for; and evaluated by; imagecatalogers. We discuss the current functionality and future goals for this image catalogers'toolkit; developed in the Computational Linguistics for Metadata Building (CLiMB) researchproject. 1 Our primary user group for initial phases of the project is the cataloger expert; infuture work we address applications for end users. 1 The Problem: Insufficient SubjectAccess to Images The CLiMB project addresses the existing gap in subject metadata forimages; particularly for the domains of art history; architecture; and landscape architecture.Within each of these domains; image collections are increasingly available online yet …,In Procedings of the OntoImage Workshop,2006,2
Sapere: From Keywords to Key Relations,Boris Katz; Jimmy Lin,*,*,2004,2
Annotating the World Wide Web,Boris Katz; Jimmy Lin; Sue Felshin,Motivation: Keyword search engines are popular because they provide results—often; toomany results! If we could understand; at least partially; the meaning of documents; ratherthan just recognizing the words; we could answer queries with much higher precision.Natural language is the most convenient and most intuitive method of information access;and people should be able to retrieve information using a system capable of understandingand answering natural language questions.,In MIT Artificial Intelligence Laboratory Research Abstracts (this volume); September,2001,2
Blitz: A Preprocessor for Heuristically Detecting Context-Independent Linguistic Structures,Boris Katz; Jimmy Lin; Sue Felshin,The Problem: The flow of natural language is often broken by constructions such asnumbers; dates; addresses; etc.; which are difficult to analyze with conventional linguisticparsers. Blitz; a heuristic-based natural language preprocessor; has been integrated into theSTART Natural Language System [1]; considerably improving START's ability to analyzereal-world sentences. Motivation: Real-world sentences are populated with numerousconstructions that do not submit neatly to regular linguistic parsing methods. To handlethese constructions; natural language systems typically implement specialized new rules.This leads to a level of complexity which renders development and maintenance difficult.Because these constructions have highly regular forms; and can be largely understood inthe absence of context; it is possible to shift the burden of processing away from the …,*,2001,2
Improving the Precision of Information Retrieval Systems Using Syntactic Relations,Boris Katz; Jimmy Lin; Sue Felshin,To test our ideas; we have built a system; called Sapere; which indexes information usingternary expressions as described above. Once data is stored in the system via indexedternary expressions; Sapere can accept queries and analyze them into ternary expressionsvia the same mechanism. If the representation of the user's query matches representationsstored in the system; then Sapere retrieves the associated original data and presents it to theuser. Impact: Although Sapere is slower than the simple keyword indexer; we believe thatthe potential to dramatically increase precision offsets the longer processing time. By usingsimplified NLP techiques which are rapid yet retain much of the intelligence of full NLP; andapplying them to the domain of IR; we should be able to improve on,*,2001,2
Relevance Feedback,Jimmy Lin,*,*,*,2
Graph algorithms,Fei Li,Page 1. Graph Algorithms Fei Li∗ • lecture time Thursday 7:20pm-10:00pm • location InnovationHall 208 • course webpage http://cs.gmu.edu/~lifei/teaching/cs684spring17/syllabus.pdf • credit3 • textbook (online access from GMU's library) Combinatorial Optimization; Theory and Algorithmsby Bernhard Korte and Jens Vygen (KV); 2012; 5th edition • prerequisites CS583 with grade A−or above; or instructor's permission • Instructor Fei Li (lifei@cs.gmu.edu; Room 5326; EngineeringBuilding) • instructor's office hours Monday 1:00pm-3:00pm • teaching assistant Ivan Avramovic(iavramo2@masonlive.gmu.edu; Room 5321; Engineering Build- ing) • TA's office hoursTuesday/Wednesday 11:00am-12:00pm • grading policy: – assignments (40%) – (1 or 2 people)a project (50%) *Department of Computer Science; George Mason University. Email:lifei@cs.gmu.edu. Room 5326; Engineering Building 1 Page 2 …,trees,*,2
A Comparison of Nuggets and Clusters for Evaluating Timeline Summaries,Gaurav Baruah; Richard McCreadie; Jimmy Lin,Abstract There is growing interest in systems that generate timeline summaries by filteringhigh-volume streams of documents to retain only those that are relevant to a particular eventor topic. Continued advances in algorithms and techniques for this task depend onstandardized and reproducible evaluation methodologies for comparing systems. However;timeline summary evaluation is still in its infancy; with competing methodologies currentlybeing explored in international evaluation forums such as TREC. One area of activeexploration is how to explicitly represent the units of information that should appear in a"good" summary. Currently; there are two main approaches; one based on identifyingnuggets in an external" ground truth"; and the other based on clustering system outputs. Inthis paper; by building test collections that have both nugget and cluster annotations; we …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,1
The Pareto Frontier of Utility Models as a Framework for Evaluating Push Notification Systems,Gaurav Baruah; Jimmy Lin,Abstract We propose a utility-based framework for the evaluation of push notificationsystems that monitor document streams for users' topics of interest. Our starting point is thatusers derive either positive utility (ie;" gain") or negative utility (ie;" pain") from consumingsystem updates. By separately keeping track of these quantities; we can measure systemeffectiveness in a gain vs. pain tradeoff space. The Pareto Frontier of evaluated systemsrepresents the state of the art: for each system on the frontier; no other system can offer moregain without more pain. Our framework has several advantages: it unifies three previousTREC evaluations; subsumes existing metrics; and provides more insightful analyses.Furthermore; our approach can easily accommodate more refined user models and isextensible to different information-seeking modalities.,Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval,2017,1
An Exploration of Serverless Architectures for Information Retrieval,Matt Crane; Jimmy Lin,Abstract Serverless architectures represent a new approach to designing applications in thecloud without having to explicitly provision or manage servers. The developer specifiesfunctions with well-defined entry and exit points; and the cloud provider handles all otheraspects of execution. In this paper; we explore a novel application of serverlessarchitectures to information retrieval and describe a search engine built in this manner withAmazon Web Services: postings lists are stored in the DynamoDB NoSQL store and thepostings traversal algorithm for query evaluation is implemented in the Lambda service. Theresult is a search engine that scales elastically with a pay-per-request model; in contrast to aserver-based model that requires paying for running instances even if there are no requests.We empirically assess the performance and economics of our serverless architecture …,Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval,2017,1
Finally; a downloadable test collection of tweets,Royal Sequiera; Jimmy Lin,ABSTRACT Due to Twi er's terms of service that forbid redistribution of content; creatingpublicly downloadable collections of tweets for research purposes has been a perpetualproblem for the research community. Some collections are distributed by making availablethe ids of the tweets that comprise the collection and providing tools to fetch the actualcontent; this approach has scalability limitations. In other cases; evaluation organizers haveset up APIs that provide access to collections for speci c tasks; without exposing theunderlying content. is is a workable solution; but di cult to sustain over the long term sincesomeone has to maintain the APIs. We have noticed that the non-pro t Internet Archive hasbeen making available for public download captures of the so-called Twi er “spritzer” stream;which is the same source as the Tweets2013 collection used in the TREC 2013 and 2014 …,Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval; ACM,2017,1
On the Reusability of Living Labs Test Collections:: A Case Study of Real-Time Summarization,Luchen Tan; Gaurav Baruah; Jimmy Lin,Abstract Information retrieval test collections are typically built using data from large-scaleevaluations in international forums such as TREC; CLEF; and NTCIR. Previous validationstudies on pool-based test collections for ad hoc retrieval have examined their reusability toaccurately assess the effectiveness of systems that did not participate in the originalevaluation. To our knowledge; the reusability of test collections derived from" living labs"evaluations; based on logs of user activity; has not been explored. In this paper; weperformed a" leave-one-out" analysis of human judgment data derived from the TREC 2016Real-Time Summarization Track and show that those judgments do not appear to bereusable. While this finding is limited to one specific evaluation; it does call into question thereusability of test collections built from living labs in general; and at the very least …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,1
The Lucene for information access and retrieval research (LIARR) workshop at SIGIR 2017,Leif Azzopardi; Matt Crane; Hui Fang; Grant Ingersoll; Jimmy Lin; Yashar Moshfeghi; Harrisen Scells; Peilin Yang; Guido Zuccon,As an empirical discipline; information access and retrieval research requires substantialsoftware infrastructure to index and search large collections. This workshop is motivated bythe desire to better align information retrieval research with the practice of building searchapplications from the perspective of open-source information retrieval systems. Our goal is topromote the use of Lucene for information access and retrieval research.,*,2017,1
An Exploration of Approaches to Integrating Neural Reranking Models in Multi-Stage Ranking Architectures,Zhucheng Tu; Matt Crane; Royal Sequiera; Junchen Zhang; Jimmy Lin,Abstract: We explore different approaches to integrating a simple convolutional neuralnetwork (CNN) with the Lucene search engine in a multi-stage ranking architecture. Ourmodels are trained using the PyTorch deep learning toolkit; which is implemented in C/C++with a Python frontend. One obvious integration strategy is to expose the neural networkdirectly as a service. For this; we use Apache Thrift; a software framework for buildingscalable cross-language services. In exploring alternative architectures; we observe thatonce trained; the feedforward evaluation of neural networks is quite straightforward.Therefore; we can extract the parameters of a trained CNN from PyTorch and import themodel into Java; taking advantage of the Java Deeplearning4J library for feedforwardevaluation. This has the advantage that the entire end-to-end system can be implemented …,arXiv preprint arXiv:1707.08275,2017,1
Exploring the Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-End Question Answering,Royal Sequiera; Gaurav Baruah; Zhucheng Tu; Salman Mohammed; Jinfeng Rao; Haotian Zhang; Jimmy Lin,Abstract: Most work on natural language question answering today focuses on answerselection: given a candidate list of sentences; determine which contains the answer.Although important; answer selection is only one stage in a standard end-to-end questionanswering pipeline. This paper explores the effectiveness of convolutional neural networks(CNNs) for answer selection in an end-to-end context using the standard TrecQA dataset.We observe that a simple idf-weighted word overlap algorithm forms a very strong baseline;and that despite substantial efforts by the community in applying deep learning to tackleanswer selection; the gains are modest at best on this dataset. Furthermore; it is unclear if aCNN is more effective than the baseline in an end-to-end context based on standardretrieval metrics. To further explore this finding; we conducted a manual user evaluation …,arXiv preprint arXiv:1707.07804,2017,1
In-Browser Interactive SQL Analytics with Afterburner,Kareem El Gebaly; Jimmy Lin,Abstract This demonstration explores the novel and unconventional idea of implementing ananalytical RDBMS in pure JavaScript so that it runs completely inside a browser with noexternal dependencies. Our prototype; called Afterburner; generates compiled query plansthat exploit two JavaScript features: typed arrays and asm. js. On the TPC-H benchmark; weshow that Afterburner achieves comparable performance to MonetDB running natively onthe same machine. This is an interesting finding in that it shows how far JavaScript has comeas an efficient execution platform. Beyond a mere technical curiosity; we demonstrate howour techniques can support interactive data exploration by automatically generatingmaterialized views from a backend that is then shipped to the browser to facilitatesubsequent interactions seamlessly and efficiently.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
In Defense of MapReduce,Jimmy Lin,Don't throw the MapReduce baby out with the bath water! MapReduce represents a specificinstance of a general class of data-parallel dataflow languages; in which computations areconceptualized as directed graphs; where vertices represent operations on records that flowalong the directed edges. From this perspective; MAP and REDUCE are the two operatorsthat MapReduce provides; which define particular configurations of the edges that flow intoand out of vertices and specify the computations that occur at the vertices themselves.,IEEE Internet Computing,2017,1
Distant Supervision for Topic Classification of Tweets in Curated Streams,Salman Mohammed; Nimesh Ghelani; Jimmy Lin,Abstract: We tackle the challenge of topic classification of tweets in the context of analyzing alarge collection of curated streams by news outlets and other organizations to deliverrelevant content to users. Our approach is novel in applying distant supervision based onsemi-automatically identifying curated streams that are topically focused (for example; onpolitics; entertainment; or sports). These streams provide a source of labeled data to traintopic classifiers that can then be applied to categorize tweets from more topically-diffusestreams. Experiments on both noisy labels and human ground-truth judgments demonstratethat our approach yields good topic classifiers essentially" for free"; and that topic classifierstrained in this manner are able to dynamically adjust for topic drift as news on Twitterevolves. Subjects: Information Retrieval (cs. IR); Social and Information Networks (cs. SI) …,arXiv preprint arXiv:1704.06726,2017,1
Mining the Temporal Statistics of ery Terms for Searching Social Media Posts,Jinfeng Rao; Ferhan Ture; Xing Niu; Jimmy Lin,ABSTRACT ere is an emerging consensus that time is an important indicator of relevance forsearching streams of social media posts. In a process similar to pseudo-relevance feedback;the distribution of document timestamps from the results of an initial query can be leveragedto infer the distribution of relevant documents; for example; using kernel density estimation.In this paper; we explore an alternative approach to mining relevance signals directly fromthe temporal statistics of query terms in the collection; without the need to perform an initialretrieval. We propose two approaches: a linear ranking model that combines featuresderived from temporal collection statistics of query terms and a regression-based methodthat a empts to directly predict the distribution of relevant documents from query termstatistics. Experiments on standard tweet test collections show that our proposed methods …,*,2017,1
Optimizing Nugget Annotations with Active Learning,Gaurav Baruah; Haotian Zhang; Rakesh Guttikonda; Jimmy Lin; Mark D Smucker; Olga Vechtomova,Abstract Nugget-based evaluations; such as those deployed in the TREC TemporalSummarization and Question Answering tracks; require human assessors to determinewhether a nugget is present in a given piece of text. This process; known as nuggetannotation; is labor-intensive. In this paper; we present two active learning techniques thatprioritize the sequence in which candidate nugget/sentence pairs are presented to anassessor; based on the likelihood that the sentence contains a nugget. Our approach buildson the recognition that nugget annotation is similar to high-recall retrieval; and we adaptproven existing solutions. Simulation experiments with four existing TREC test collectionsshow that our techniques yield far more matches for a given level of effort than baselines thatare typically deployed in previous nugget-based evaluations.,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,1
Dynamic Trade-Off Prediction in Multi-Stage Retrieval Systems,J Shane Culpepper; Charles LA Clarke; Jimmy Lin,Abstract: Modern multi-stage retrieval systems are comprised of a candidate generationstage followed by one or more reranking stages. In such an architecture; the quality of thefinal ranked list may not be sensitive to the quality of initial candidate pool; especially interms of early precision. This provides several opportunities to increase retrieval efficiencywithout significantly sacrificing effectiveness. In this paper; we explore a new approach todynamically predicting two different parameters in the candidate generation stage which candirectly affect the overall efficiency and effectiveness of the entire system. Previous workexploring this tradeoff has focused on global parameter settings that apply to all queries;even though optimal settings vary across queries. In contrast; we propose a technique whichmakes a parameter prediction that maximizes efficiency within a effectiveness envelope …,arXiv preprint arXiv:1610.02502,2016,1
Total recall: Blue sky on mars,Charles LA Clarke; Gordon V Cormack; Jimmy Lin; Adam Roegiest,Abstract There are presently plans to create permanent colonies on Mars so that humanitywill have a second home. These colonists will need search; email; entertainment; andindeed most services provided on the modern web. The primary challenge is networklatencies; since the two planets are anywhere from 4 to 24 light minutes apart. A recentarticle sketches out how we might develop search technologies for Mars based on physicallytransporting a cache of the web to Mars; to which updates are applied via predictive models.Within this general framework; we explore the problem of high-recall retrieval; such asconducting a scientific survey. We explore simple techniques for masking speed-of-lightdelays and find that" priming" the search process with a small Martian cache is sufficient tomask a moderate amount of network latency. Simulation experiments show that it is …,Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,2016,1
Sapphire: Querying RDF data made simple,Ahmed El-Roby; Khaled Ammar; Ashraf Aboulnaga; Jimmy Lin,Abstract There is currently a large amount of publicly accessible structured data available asRDF data sets. For example; the Linked Open Data (LOD) cloud now consists of thousandsof RDF data sets with over 30 billion triples; and the number and size of the data sets iscontinuously growing. Many of the data sets in the LOD cloud provide public SPARQLendpoints to allow issuing queries over them. These end-points enable users to retrievedata using precise and highly expressive SPARQL queries. However; in order to do so; theuser must have sufficient knowledge about the data sets that she wishes to query; that is; thestructure of data; the vocabulary used within the data set; the exact values of literals; theirdata types; etc. Thus; while SPARQL is powerful; it is not easy to use. An alternative toSPARQL that does not require as much prior knowledge of the data is some form of …,Proceedings of the VLDB Endowment,2016,1
Burst Detection in Social Media Streams for Tracking Interest Profiles in Real Time,Cody Buntain; Jimmy Lin,Abstract This work presents RTTBurst; an end-to-end system for ingesting descriptions ofuser interest profiles and discovering new and relevant tweets based on those interestprofiles using a simple model for identifying bursts in token usage. Our approach differs fromstandard retrieval-based techniques in that it primarily focuses on identifying noteworthymoments in the tweet stream; and? summarizes? those moments using selected tweets. Welay out the architecture of RTTBurst; our participation in and performance at the TREC 2015Microblog track; and a method for combining and potentially improving existing TRECsystems. Official results and post hoc experiments show that our simple targeted burstdetection technique is competitive with existing systems. Furthermore; we demonstrate thatour burst detection mechanism can be used to improve the performance of other systems …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,1
Afterburner: The Case for In-Browser Analytics,Kareem El Gebaly; Jimmy Lin,Abstract: This paper explores the novel and unconventional idea of implementing ananalytical RDBMS in pure JavaScript so that it runs completely inside a browser with noexternal dependencies. Our prototype; called Afterburner; generates compiled query plansthat exploit typed arrays and asm. js; two relatively recent advances in JavaScript. On a fewsimple queries; we show that Afterburner achieves comparable performance to MonetDBrunning natively on the same machine. This is an interesting finding in that it shows how farJavaScript has come as an efficient execution platform. Beyond a mere technical curiosity;we discuss how our techniques could support ubiquitous in-browser interactive analytics(potentially integrating with browser-based notebooks) and also present interestingopportunities for split-execution strategies where query operators are distributed between …,arXiv preprint arXiv:1605.04035,2016,1
Discovering key moments in social media streams,Cody Buntain; Jimmy Lin; Jennifer Golbeck,This paper introduces a general technique; called LABurst; for identifying key moments; ormoments of high impact; in social media streams without the need for domain-specificinformation or seed keywords. We leverage machine learning to model temporal patternsaround bursts in Twitter's unfiltered public sample stream and build a classifier to identifytokens experiencing these bursts. We show LABurst performs competitively with existingburst detection techniques while simultaneously providing insight into and detection ofunanticipated moments. To demonstrate our approach's potential; we compare two baselineevent-detection algorithms with our language-agnostic algorithm to detect key momentsacross three major sporting competitions: 2013 World Series; 2014 Super Bowl; and 2014World Cup. Our results show LABurst outperforms a time series analysis baseline and is …,Consumer Communications & Networking Conference (CCNC); 2016 13th IEEE Annual,2016,1
Sigir 2015 workshop on reproducibility; inexplicability; and generalizability of results (rigor),Jaime Arguello; Fernando Diaz; Jimmy Lin; Andrew Trotman,Many; if not most; published research papers in Information Retrieval (IR) describe thefollowing process: the authors identify an opportunity to improve on a particular IR task;implement an experimental system; and compare its performance against one or morebaselines (or a control condition; in the case of a user study). The quality of the research isjudged based on the magnitude of the improvement and whether the methodologicalchoices suggest external validity and generalizability; for example; whether the experimentalsetup is “realistic” or whether the baseline methods reflect the state of the art. Unfortunately;research demonstrating the failure to reproduce or generalize previous results does nothave a similar publication venue. This sort of result—often referred to as a 'negative result'—serves to control the quality of published research in a scientific discipline and to better …,Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,2015,1
Learning to discover key moments in social media streams,Cody Buntain; Jimmy Lin; Jennifer Golbeck,Abstract: This paper introduces LABurst; a general technique for identifying key moments; ormoments of high impact; in social media streams without the need for domain-specificinformation or seed keywords. We leverage machine learning to model temporal patternsaround bursts in Twitter's unfiltered public sample stream and build a classifier to identifytokens experiencing these bursts. We show LABurst performs competitively with existingburst detection techniques while simultaneously providing insight into and detection ofunanticipated moments. To demonstrate our approach's potential; we compare two baselineevent-detection algorithms with our language-agnostic algorithm to detect key momentsacross three major sporting competitions: 2013 World Series; 2014 Super Bowl; and 2014World Cup. Our results show LABurst outperforms a time series analysis baseline and is …,arXiv preprint arXiv:1508.00488,2015,1
The sum of all human knowledge in your pocket: Full-text searchable Wikipedia on a Raspberry Pi,Jimmy Lin,Abstract We demonstrate a prototype that takes advantage of open-source software to put afull-text searchable copy of Wikipedia on a Raspberry Pi; providing nearby devices access tocontent via wifi or bluetooth without requiring internet connectivity. This short paperarticulates the advantages of such a form factor and provides an evaluation of browsing andsearch capabilities. We believe that personal digital libraries on lightweight mobilecomputing devices represent an interesting research direction to pursue.,Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital Libraries,2015,1
On the feasibility and implications of self-contained search engines in the browser,Jimmy Lin,Abstract: JavaScript engines inside modern browsers are capable of running sophisticatedmulti-player games; rendering impressive 3D scenes; and supporting complex; interactivevisualizations. Can this processing power be harnessed for information retrieval? This paperexplores the feasibility of building a JavaScript search engine that runs completely self-contained on the client side within the browser---this includes building the inverted index;gathering terms statistics for scoring; and performing query evaluation. The design takesadvantage of the IndexDB API; which is implemented by the LevelDB key-value store insideGoogle's Chrome browser. Experiments show that although the performance of theJavaScript prototype falls far short of the open-source Lucene search engine; it is sufficientlyresponsive for interactive applications. This feasibility demonstration opens the door to …,arXiv preprint arXiv:1410.4500,2014,1
Mining texts for image terms: the CLiMB project,Judith L Klavans; Eileen Abels; Jimmy Lin; R Passonneau; C Sheffield; D Soergel,Under the hypothesis that searchers do not find images they seek partly due to inadequatesubject description in metadata fields; the CLiMB project was initiated to address this subjectmetadata gap by applying automatic and semi-automatic techniques to the identification;extraction; and thesaural linking of subject terms. The CLiMB Toolkit processes textassociated with an image through natural language processing (NLP); categorization usingmachine learning (ML); and disambiguation techniques to identify; filter; and normalize high-quality subject descriptors. Like Pastra et al. 2003 we use NLP techniques and domain-specific ontologies; although our focus is on associated texts such as art historical surveys orcuratorial essays rather than captions; unlike generic image search; such as in Google; weanalyze beyond,Digital Humanities 2009 conference (College Park; Maryland,2009,1
Human SNPs from short reads in hours using cloud compu ng,Ben Langmead; Michael C Schatz; Jimmy Lin; Mihai Pop; Steven L Salzberg,Abstract As growth in short read sequencing throughput vastly outpaces improvements inmicroprocessor speed; there is a crifical need to accelerate common tasks; such as shortread alignment and SNP calling; via large-‐scale parallelizafion. Crossbow is a software toolthat combines the speed of the short read aligner BowHe1 and the accuracy of theSOAPsnp2 consensus and SNP caller within a cloud compuHng environment. Crossbowaligns reads and makes highly accurate SNP calls from a dataset comprising 38-‐foldcoverage of the human genome in under 1 day on a local 40 core cluster; and under 3 hoursusing a 320-‐core cluster rented from Amazon's ElasHc Compute Cloud3 (EC2) service.Crossbow's ability to run on,*,2009,1
Action modeling: language models that predict query behavior,G Craig Murray; Jimmy Lin; Abdur Chowdhury,Abstract We present a novel language modeling approach to capturing the queryreformulation behavior of Web search users. Based on a framework that categorizes eightdifferent types of" user moves"(adding/removing query terms; etc.); we treat search sessionsas sequence data and build n-gram language models to capture user behavior. Weevaluated our models in a prediction task. The results suggest that useful patterns of activitycan be extracted from user histories. Furthermore; by examining prediction performanceunder different order n-gram models; we gained insight into the amount of history/contextthat is associated with different types of user actions. Our work serves as the basis for morerefined user models.,Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,2006,1
A computational framework for non-lexicalist semantics,Jimmy Lin,Abstract Under a lexicalist approach to semantics; a verb completely encodes its syntacticand semantic structures; along with the relevant syntax-to-semantics mapping; polysemy istypically attributed to the existence of different lexical entries. A lexicon organized in thisfashion contains much redundant information and is unable to capture cross-categorialmorphological derivations. The solution is to spread the" semantic load" of lexical entries toother morphemes not typically taken to bear semantic content. This approach follows currenttrends in linguistic theory; and more perspicuously accounts for alternations in argumentstructure. I demonstrate how such a framework can be computationally realized with afeature-based; agenda-driven chart parser for the Minimalist Program.,Proceedings of the Student Research Workshop at HLT-NAACL 2004,2004,1
自然语言和 XML 在语义网中的作用,Graham Wilcock; Paul Buitelaar; Antonio Pareja-Lora; Barrett Bryant; Jimmy Lin; Nancy Ide,*,计算语言学前瞻,2004,1
Sapere: Improving the Precision of Information Retrieval Systems Using Syntactic Relations,Boris Katz; Jimmy Lin,The Problem: Traditional information retrieval systems based on the “bag-of-words”paradigm cannot capture the semantic content of documents. While these systems arerelatively robust and have high recall; they suffer from very poor precision. On the otherhand; it is impossible with current technology to build a practical information access systemthat fully analyzes and understands unrestricted natural language. Existing natural languagesystems; despite their high precision; have low recall and lack robustness. Motivation: Bysimplifying the sophistication of natural language techniques applied to document analysis;a significant portion of semantic content can be captured while many of the intractablecomplexities of language can be ignored. This can result in a large-scale information accesssystem which is capable of processing unrestricted text; largely understanding it; and …,*,2002,1
Aranea: Mining Answers from the World Wide Web,Jimmy Lin; Boris Katz,The Problem: Users seeking answers to questions like “what Spanish explorer discoveredthe Mississippi River” should not have to peruse entire documents about the Spanishexplorers or early American history to obtain relevant information. Instead; a computersystem should automatically distill large amounts of text into a compact answer in responseto a question posed in natural language. Motivation: An enormous amount of textual data isfreely available on the World Wide Web; properly utilized; this vast repository of text canserve as a rich source of knowledge for answering fact-based questions. The sheer size ofthe Web is its primary asset; items of knowledge are stated multiple times; in multipledocuments; and in multiple ways. This unique characteristic; known as data redundancy [1];allows for the development of novel question answering techniques.,Artificial Intelligence Laboratory; MiT; In Proceedings of the 5th RIAO Conference on Computer Assisted Information Searching on the Internet (RIAO 97),1997,1
Query Driven Algorithm Selection in Early Stage Retrieval,Joel Mackenzie; J Shane Culpepper; Roi Blanco; Matt Crane; Charles LA Clarke; Jimmy Lin,ABSTRACT Large scale retrieval systems often employ cascaded ranking architectures; inwhich an initial set of candidate documents is iteratively refined and re-ranked byincreasingly sophisticated and expensive ranking models. In this paper; we propose aunified framework for predicting a range of performance-sensitive parameters based onminimizing end-to-end effectiveness loss. The framework does not require relevancejudgments for training; is amenable to predicting a wide range of parameters; allows for finetuned efficiencyeffectiveness trade-offs; and can be easily deployed in large scale searchsystems with minimal overhead. As a proof of concept; we show that the framework canaccurately predict a number of performance parameters on a query-by-query basis; allowingefficient and effective retrieval; while simultaneously minimizing the tail latency of an early …,*,2018,*
Search relevance using messages of a messaging platform,*,Abstract: A method and system for searching documents. The method can include:identifying a first set of messages of a messaging platform referencing a document;identifying message text of the first set of messages; associating the message text with thedocument in a storage repository; receiving; by a computer processor; a search requestincluding a search term provided by a client; calculating; by the computer processor; arelevance score for the document based at least on the search term and the message text;and providing; to the client and based on the relevance score; a reference to the documentin response to the search request.,*,2017,*
Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks,Salman Mohammed; Peng Shi; Jimmy Lin,Abstract: We examine the problem of question answering over knowledge graphs; focusingon simple questions that can be answered by the lookup of a single fact. Adopting astraightforward decomposition of the problem into entity detection; entity linking; relationprediction; and evidence combination; we explore simple yet strong baselines. On theSimpleQuestions dataset; we find that baseline LSTMs and GRUs plus a few heuristics yieldaccuracies that approach the state of the art; and techniques that do not use neural networksalso perform reasonably well. These results show that gains from sophisticated deeplearning techniques proposed in the literature are quite modest and that some previousmodels exhibit unnecessary complexity. Subjects: Computation and Language (cs. CL) Citeas: arXiv: 1712.01969 [cs. CL](or arXiv: 1712.01969 v1 [cs. CL] for this version) …,arXiv preprint arXiv:1712.01969,2017,*
An Experimental Analysis of the Power Consumption of Convolutional Neural Networks for Keyword Spotting,Raphael Tang; Weijie Wang; Zhucheng Tu; Jimmy Lin,Abstract: Nearly all previous work on small-footprint keyword spotting with neural networksquantify model footprint in terms of the number of parameters and multiply operations for aninference pass. These values are; however; proxy measures since empirical performance inactual deployments is determined by many factors. In this paper; we study the powerconsumption of a family of convolutional neural networks for keyword spotting on aRaspberry Pi. We find that both proxies are good predictors of energy usage; although thenumber of multiplies is more predictive than the number of parameters. We also confirm thatmodels with the highest accuracies are; unsurprisingly; the most power hungry.,arXiv preprint arXiv:1711.00333,2017,*
Deep Residual Learning for Small-Footprint Keyword Spotting,Raphael Tang; Jimmy Lin,Abstract: We explore the application of deep residual learning and dilated convolutions tothe keyword spotting task; using the recently-released Google Speech Commands Datasetas our benchmark. Our best residual network (ResNet) implementation significantlyoutperforms Google's previous convolutional neural networks in terms of accuracy. Byvarying model depth and width; we can achieve compact models that also outperformprevious small-footprint variants. To our knowledge; we are the first to examine theseapproaches for keyword spotting; and our results establish an open-source state-of-the-artreference to support the development of future speech-based interfaces.,arXiv preprint arXiv:1710.10361,2017,*
Quantization in Append-Only Collections,Salman Mohammed; Matt Crane; Jimmy Lin,Abstract Quantization; the pre-calculation and conversion to integers of term/documentweights in an inverted index; is a well studied aspect of search engines that substantiallyimproves retrieval efficiency. Previous work has considered the impact of quantization oneffectiveness-efficiency tradeoffs in retrieval; for example; exploring the relationship betweencollection size and quantization range in static web collections. We extend previous work toappend-only collections and examine whether quantization settings derived from prior timeperiods can be applied to future time periods. Experiments confirm that previous resultsgeneralize to a collection with different characteristics and with a different ranking function;and that in an append-only collection; we can use previous quantization settings in futuretime periods without substantial losses in either effectiveness or efficiency.,Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval,2017,*
The Ubiquity of Large Graphs and Surprising Challenges of Graph Processing: A User Survey,Siddhartha Sahu; Amine Mhedhbi; Semih Salihoglu; Jimmy Lin; M Tamer Özsu,Abstract: Graph processing is becoming increasingly prevalent across many applicationdomains. In spite of this prevalence; there is little research about how graphs are actuallyused in practice. We conducted an online survey aimed at understanding:(i) the types ofgraphs users have;(ii) the graph computations users run;(iii) the types of graph softwareusers use; and (iv) the major challenges users face when processing their graphs. Wedescribe the responses of the participants to our questions; highlighting common patternsand challenges. The participants' responses revealed surprising facts about graphprocessing in practice; which we hope can guide future research. Subjects: Databases (cs.DB) Cite as: arXiv: 1709.03188 [cs. DB](or arXiv: 1709.03188 v1 [cs. DB] for this version)Submission history From: Siddhartha Sahu [view email][v1] Sun; 10 Sep 2017 22: 25: 13 …,arXiv preprint arXiv:1709.03188,2017,*
Automatically Extracting High-Quality Negative Examples for Answer Selection in Question Answering,Haotian Zhang; Jinfeng Rao; Jimmy Lin; Mark D Smucker,Abstract We propose a heuristic called" one answer per document" for automaticallyextracting high-quality negative examples for answer selection in question answering.Starting with a collection of question-answer pairs from the popular TrecQA dataset; weidentify the original documents from which the answers were drawn. Sentences from thesesource documents that contain query terms (aside from the answers) are selected asnegative examples. Training on the original data plus these negative examples yieldsimprovements in effectiveness by a margin that is comparable to successive recentpublications on this dataset. Our technique is completely unsupervised; which means thatthe gains come essentially for free. We confirm that the improvements can be directlyattributed to our heuristic; as other approaches to extracting comparable amounts of …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,*
Event Detection on Curated Tweet Streams,Nimesh Ghelani; Salman Mohammed; Shine Wang; Jimmy Lin,Abstract We present a system for identifying interesting social media posts on Twitter anddelivering them to users' mobile devices in real time as push notifications. In our problemformulation; users are interested in broad topics such as politics; sports; and entertainment:our system processes tweets in real time to identify relevant; novel; and salient content.There are three interesting aspects to our work: First; instead of attempting to tame thecacophony of unfiltered tweets; we exploit a smaller; but still sizeable; collection of curatedtweet streams corresponding to the Twitter accounts of different media outlets. Second; weapply distant supervision to extract topic labels from curated streams that have a specificfocus; which can then be leveraged to build high-quality topic classifiers essentially" forfree". Finally; our system delivers content via Twitter direct messages; supporting in situ …,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,2017,*
Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives,Jimmy Lin; Ian Milligan; Jeremy Wiebe; Alice Zhou,Abstract Web archiving initiatives around the world capture ephemeral Web content topreserve our collective digital memory. However; unlocking the potential of Web archives forhumanities scholars and social scientists requires a scalable analytics infrastructure tosupport exploration of captured content. We present Warcbase; an open-source Webarchiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure; namely Hadoop; HBase; and Spark; that has been widelydeployed in industry. Warcbase provides two main capabilities: support for temporalbrowsing and a domain-specific language that allows scholars to interrogate Web archivesin several different ways. This work represents a collaboration between computer scientistsand historians; where we have engaged in iterative codesign to build tools for scholars …,Journal on Computing and Cultural Heritage (JOCCH),2017,*
Do We Need Specialized Graph Databases?: Benchmarking Real-Time Social Networking Applications,Anil Pacaci; Alice Zhou; Jimmy Lin; M Tamer Özsu,Abstract With the advent of online social networks; there is an increasing demand for storageand processing of graph-structured data. Social networking applications pose newchallenges to data management systems due to demand for real-time querying andmanipulation of the graph structure. Recently; several systems specialized systems for graph-structured data have been introduced. However; whether we should abandon matureRDBMS technology for graph databases remains an ongoing discussion. In this paper wepresent an graph database benchmarking architecture built on the existing LDBC SocialNetwork Benchmark. Our proposed architecture stresses the systems with an interactivetransactional workload to better simulate the real-time nature of social networkingapplications. Using this improved architecture; we evaluated a selection of specialized …,Proceedings of the Fifth International Workshop on Graph Data-management Experiences & Systems,2017,*
Efficient and Effective Tail Latency Minimization in Multi-Stage Retrieval Systems,Joel Mackenzie; J Shane Culpepper; Roi Blanco; Matt Crane; Charles LA Clarke; Jimmy Lin,Abstract: Scalable web search systems typically employ multi-stage retrieval architectures;where an initial stage generates a set of candidate documents that are then pruned and re-ranked. Since subsequent stages typically exploit a multitude of features of varying costsusing machine-learned models; reducing the number of documents that are considered ateach stage improves latency. In this work; we propose and validate a unified framework thatcan be used to predict a wide range of performance-sensitive parameters which minimizeeffectiveness loss; while simultaneously minimizing query latency; across all stages of amulti-stage search architecture. Furthermore; our framework can be easily applied in large-scale IR systems; can be trained without explicitly requiring relevance judgments; and cantarget a variety of different efficiency-effectiveness trade-offs; making it well suited to a …,arXiv preprint arXiv:1704.03970,2017,*
Ten Blue Links on Mars,Charles LA Clarke; Gordon V Cormack; Jimmy Lin; Adam Roegiest,Abstract This paper explores a simple question: How would we provide a high-qualitysearch experience on Mars; where the fundamental physical limit is speed-of-lightpropagation delays on the order of tens of minutes? On Earth; users are accustomed tonearly instantaneous responses from web services. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper; weformulate the searching from Mars problem as a tradeoff between" effort"(waiting forresponses from Earth) and" data transfer"(pre-fetching or caching data on Mars). Thecontribution of our work is articulating this design space and presenting two case studiesthat explore the effectiveness of baseline techniques; using publicly available data from theTREC Total Recall and Sessions Tracks. We intend for this research problem to be …,Proceedings of the 26th International Conference on World Wide Web,2017,*
Partitioning and Segment Organization Strategies for Real-Time Selective Search on Document Streams,Yulu Wang; Jimmy Lin,Abstract The basic idea behind selective search is to partition a collection into topicalclusters; and for each query; consider only a subset of the clusters that are likely to containrelevant documents. Previous work on web collections has shown that it is possible to retainhigh-quality results while considering only a small fraction of the collection. These studies;however; assume static collections where it is feasible to run batch clustering algorithms forpartitioning. In this work; we consider the novel formulation of selective search on documentstreams (specifically; tweets); where partitioning must be performed incrementally. In ourapproach; documents are partitioned into temporal segments and selective search isperformed within each segment: these segments can either be clustered using batch oronline algorithms; and at different temporal granularities. For efficiency; we take …,Proceedings of the Tenth ACM International Conference on Web Search and Data Mining,2017,*
The Ubiquity of Large Graphs and Surprising Challenges of Graph Processing,Siddhartha Sahu; Amine Mhedhbi; Semih Salihoglu; Jimmy Lin; M Tamer Özsu,ABSTRACT Graph processing is becoming increasingly prevalent across many applicationdomains. In spite of this prevalence; there is little research about how graphs are actuallyused in practice. We conducted an online survey aimed at understanding:(i) the types ofgraphs users have;(ii) the graph computations users run;(iii) the types of graph softwareusers use; and (iv) the major challenges users face when processing their graphs. Wedescribe the participants' responses to our questions highlighting common patterns andchallenges. We further reviewed user feedback in the mailing lists; bug reports; and featurerequests in the source repositories of a large suite of software products for processinggraphs. Through our review; we were able to answer some new questions that were raisedby participants' responses and identify specific challenges that users face when using …,Proceedings of the VLDB Endowment,2017,*
The Lambda and the Kappa,Jimmy Lin,SEPTEMBER/OCTOBER 2017 61 same database; everything would be hunky dory;right?(Short answer: it's an improvement but not a panacea; don't buy the hype completely.)Regardless; we've seen the emergence of hybrid transactional/analytical processing(HTAP); a brilliant term invented by Gartner in 2014. The pendulum has now swung theother way; back toward one-sizefits-all solutions.,IEEE Internet Computing,2017,*
An Insight Extraction System on BioMedical Literature with Deep Neural Networks,Hua He; Kris Ganjam; Navendu Jain; Jessica Lundin; Ryen White; Jimmy Lin,Abstract Mining biomedical text offers an opportunity to automatically discover importantfacts and infer associations among them. As new scientific findings appear across a largecollection of biomedical publications; our aim is to tap into this literature to automatebiomedical knowledge extraction and identify important insights from them. Towards thatgoal; we develop a system with novel deep neural networks to extract insights on biomedicalliterature. Evaluation shows our system is able to provide insights with competitive accuracyof human acceptance and its relation extraction component outperforms previous work.,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017,*
Portable In-Browser Data Cube Exploration,Kareem El Gebaly; Lukasz Golab; Jimmy Lin,ABSTRACT Data cubes; which summarize data across multiple dimensions; have been astaple of On Line Analytical Processing (OLAP) for well over a decade. While users typicallyaccess data cubes through data warehouse systems or business intelligence tools; wedemonstrate that data cubes can be explored effectively and efficiently inside a browser. Weprovide an overview of the two recent technologies that enable our portable data cubeexploration approach: 1) Afterburner; an in-browser relational database managementsystem; and 2) explanation tables; an information-theoretic technique for guided data cubeexploration.,*,2017,*
Prizm: A Wireless Access Point for Proxy-Based Web Lifelogging,Jimmy Lin; Zhucheng Tu; Michael Rose; Patrick White,Abstract We present Prizm; a prototype lifelogging device that comprehensively records auser's web activity. Prizm is a wireless access point deployed on a Raspberry Pi that isdesigned to be a substitute for the user's normal wireless access point. Prizm proxies allHTTP (S) requests from devices connected to it and records all activity it observes. Althoughthis particular design is not entirely novel; there are a few features that are unique to ourapproach; most notably the physical deployment as a wireless access point. Such apackage allows capture of activity from multiple devices; integration with web archiving forpreservation; and support for offline operation. This paper describes the design of Prizm; thecurrent status of our project; and future plans.,Proceedings of the first Workshop on Lifelogging Tools and Applications,2016,*
Rank-at-a-Time Query Processing,Ahmed Elbagoury; Matt Crane; Jimmy Lin,Abstract Query processing strategies for ranked retrieval have been studied for decades. Inthis paper we propose a new strategy; which we call rank-at-a-time query processing; thatevaluates documents in descending order of quantized scores and is able to directlycompute the final document ranking via a sequence of boolean intersections. We show thatsuch a strategy is equivalent to a second-order restricted composition of per-term scores.Rank-at-a-time query processing has the advantage that it is anytime score-safe; whichmeans that the retrieval algorithm can self-adapt to produce an exact ranking given anarbitrary latency constraint. Due to the combinatorial nature of compositions; however; anaive implementation is too slow to be of practical use. To address this issue; we introduce ahybrid variant that is able to reduce query latency to a point that is on par with state-of-the …,Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,2016,*
The Future of Big Data Is... JavaScript?,Jimmy Lin; Kareem El Gebaly,JavaScript is terrible in so many ways. Instead of recounting them all; we'll just refer to“WAT;” Gary Bernhardt's incredibly funny talk.1 Never- theless; we should cut Brendan Eich(the creator of JavaScript) some slack; considering that he developed the language in 10days. We're sure neither he nor anyone else expected JavaScript to become so successful andubiquitous. Any- where there's a browser; there's (almost always) JavaScript. Your connectedrefrigerator or toaster in the near future? It might have a browser for its user interface; which meansit'll likely run JavaScript. But we're getting ahead of ourselves. Let's start with a quick recap ofcomputing history to see how we got here … A Brief History In the mid-1990s; Sun Microsystems(gobbled up by Oracle in 2010) came up with a brilliant slo- gan; “write once; run anywhere;”to describe the promises of Java in offering seamless cross-plat- form development. In …,IEEE Internet Computing,2016,*
Sampling Strategies and Active Learning for Volume Estimation,Haotian Zhang; Jimmy Lin; Gordon V Cormack; Mark D Smucker,Abstract This paper tackles the challenge of accurately and efficiently estimating the numberof relevant documents in a collection for a particular topic. One real-world application isestimating the volume of social media posts (eg; tweets) pertaining to a topic; which isfundamental to tracking the popularity of politicians and brands; the potential sales of aproduct; etc. Our insight is to leverage active learning techniques to find all the" easy"documents; and then to use sampling techniques to infer the number of relevant documentsin the residual collection. We propose a simple yet effective technique for determining this"switchover" point; which intuitively can be understood as the" knee" in an effort vs. recallgain curve; as well as alternative sampling strategies beyond the knee. We show on severalTREC datasets and a collection of tweets that our best technique yields more accurate …,Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,2016,*
The Effects of Latency Penalties in Evaluating Push Notification Systems,Luchen Tan; Jimmy Lin; Adam Roegiest; Charles LA Clarke,Abstract: We examine the effects of different latency penalties in the evaluation of pushnotification systems; as operationalized in the TREC 2015 Microblog track evaluation. Thepurpose of this study is to inform the design of metrics for the TREC 2016 Real-TimeSummarization track; which is largely modeled after the TREC 2015 evaluation design.Subjects: Information Retrieval (cs. IR) Cite as: arXiv: 1606.03066 [cs. IR](or arXiv:1606.03066 v1 [cs. IR] for this version) Submission history From: Jimmy Lin [view email][v1]Thu; 9 Jun 2016 18: 58: 51 GMT (1886kb; D),arXiv preprint arXiv:1606.03066,2016,*
Evaluating Search Among Secrets.,Douglas W Oard; Katie Shilton; Jimmy J Lin,ABSTRACT Today's search engines are designed with a single fundamental goal: to help usfind the things we want to see. Paradoxically; the very fact that they do this well means thatthere are many collections that we are not allowed to search. Citizens are not allowed tosearch some government records because there may be intermixed information that needsto be protected. Scholars are not yet allowed to see much of the growing backlog ofunprocessed archival collections for similar reasons. These limitations; and many more; aredirect consequences of the fact that today's search engines are not designed to protectsensitive information. We need to change that by creating a new class of search algorithmsdesigned to effectively “search among secrets” by balancing the user's interest in findingrelevant content with the provider's interest in protecting sensitive content. This paper …,EVIA@ NTCIR,2016,*
The Feasibility of Brute Force Scans for Real-Time Tweet Search,Yulu Wang; Jimmy Lin,Abstract The real-time search problem requires making ingested documents immediatelysearchable; which presents architectural challenges for systems built around invertedindexing. In this paper; we explore a radical proposition: What if we abandon documentinversion and instead adopt an architecture based on brute force scans of documentrepresentations? In such a design;" indexing" simply involves appending the parsedrepresentation of an ingested document to an existing buffer; which is simple and fast. Quitesurprisingly; experiments with TREC Microblog test collections show that query evaluationwith brute force scans is feasible and performance compares favorably to a traditionalsearch architecture based on an inverted index; especially if we take advantage ofvectorized SIMD instructions and multiple cores in modern processor architectures. We …,Proceedings of the 2015 International Conference on The Theory of Information Retrieval,2015,*
Building a Self-Contained Search Engine in the Browser,Jimmy Lin,Abstract JavaScript engines inside modern web browsers are capable of runningsophisticated multi-player games; rendering impressive 3D scenes; and supportingcomplex; interactive visualizations. Can this processing power be harnessed for informationretrieval? This paper explores the feasibility of building a JavaScript search engine that runscompletely self-contained on the client side within the browser-this includes building theinverted index; gathering terms statistics for scoring; and performing query evaluation. Thedesign takes advantage of the IndexDB API; which is implemented by the LevelDB key{value store inside Google's Chrome browser. Experiments show that although theperformance of the JavaScript prototype falls far short of the open-source Lucene searchengine; it is sufficiently responsive for interactive applications. This feasibility …,Proceedings of the 2015 International Conference on The Theory of Information Retrieval,2015,*
Perspectives on Computational Social Science: On Building Better Mousetraps and Understanding the Human Condition: Reflections on Big Data in the Social Scie...,JIMMY LIN,*,Annals,2015,*
Knowledge Reduction Algorithms of Incomplete Information System in Massive Datasets,WANG Tian; JIANG Lin; MI Yun-long,Knowledge reduction for massive datasets has attracted many research interests in roughset theory. Traditional knowledge reduction algorithms of incomplete information systemassume that all the datasets can be loaded into the main memory; which are obviouslyinfeasible for large-scale datasets; especially for massive datasets with missing information.To this end; deeply analyze the characteristics of massive datasets with missing information;and allowthe missing attribute value to take all possible values. Then; by combining theparallel computations used in classical knowledge reduction algorithms with thediscernibility (indiscernibility) of the attributes; a knowledge reduction algorithm is designedfor incomplete information systems under MapReduce framework. The experimental resultsdemonstrate that this algorithm is effective and feasible; which can efficiently process …,Computer Technology and Development,2015,*
Developing an Open-Source Bibliometric Ranking Website Using Google Scholar Citation Profiles for Researchers in the Field of Biomedical Informatics.,Dean F Sittig; Allison B McCoy; Adam Wright; J Lin,Abstract We developed the Biomedical Informatics Researchers ranking website (rank.informatics-review. com) to overcome many of the limitations of previous scientificproductivity ranking strategies. The website is composed of four key components that worktogether to create an automatically updating ranking website:(1) list of biomedicalinformatics researchers;(2) Google Scholar scraper;(3) display page; and (4) updater. Thesite has been useful to other groups in evaluating researchers; such as tenure andpromotions committees in interpreting the various citation statistics reported by candidates.Creation of the Biomedical Informatics Researchers ranking website highlights the vastdifferences in scholarly productivity among members of the biomedical informatics researchcommunity.,Studies in health technology and informatics,2015,*
Column Stores as an IR Prototyping Tool,Hannes Mühleisen; Thaer Samar; Jimmy Lin; Arjen P De Vries,Abstract We make the suggestion that instead of implementing custom index structures andquery evaluation algorithms; IR researchers should simply store document representationsin a column-oriented relational database and write ranking models using SQL. For rapidprototyping; this is particularly advantageous since researchers can explore new rankingfunctions and features by simply issuing SQL queries; without needing to write imperativecode. We demonstrate the feasibility of this approach by an implementation of conjunctiveBM25 using MonetDB on a part of the ClueWeb12 collection.,European Conference on Information Retrieval,2014,*
CWI and TU Delft at TREC 2013: Contextual Suggestion; Federated Web Search; KBA; and Web Tracks,A Bellogín Kouki; GG Gebremeskel; J He; JJP Lin; A Said; T Samar; AP deVries; JBP Vuurens,htmlabstractThis paper provides an overview of the work done at the Centrum Wiskunde &Informatica (CWI) and Delft University of Technology (TU Delft) for different tracks of TREC2013. We participated in the Contextual Suggestion Track; the Federated Web Search Track;the Knowledge Base Acceleration (KBA) Track; and the Web Ad-hoc Track. In the ContextualSuggestion track; we focused on filtering the entire ClueWeb12 collection to generaterecommendations according to the provided user profiles and contexts. For the FederatedWeb Search track; we exploited both categories from ODP and document relevance tomerge result lists. In the KBA track; we focused on the Cumulative Citation Recommendationtask where we exploited different features to two classification algorithms. For the Web track;we extended an ad-hoc baseline with a proximity model that promotes documents in …,*,2014,*
Big Data=,HandS On,There are so many things you can do once you have data available in digitized format;which is why everyone seems to be talking about Big Data. By using the right set of tools tocapture and analyze this data; organizations of any size can gain tremendous businessbenefits. In this issue; we talk about how the automotive; healthcare; retail and other sectorscan harness the power of Big Data. Also discussed are Big Data Tools; and why should behiring a Big Data expert in your company,*,2013,*
Introduction to MapReduce,Jinoh Kim,Page 1. April 19; 2012 Jinoh Kim; Ph.D. Computer Science Department Lock Haven Universityof Pennsylvania Introduction to MapReduce Page 2. Research Areas 2 Datacenter EnergyManagement Exa-scale Computing Network Performance Estimation Healthcare SystemPerformance Optimization Network Security Network Mgmt. Page 3. Outline  Background MapReduce functions  MapReduce system aspects  Summary Page 4. Some Technical Terms Distributed computing  Parallel computing (parallelism)  Failure-prone  Data replicationPage 5. Distributed Computing?  Utilizes multiple computers over a network  Examples? SETI@home; web crawling (search engine); parallel rendering (computer graphics); and a lot! Distributed computing systems provide a platform to execute distributed computing jobs Examples?  Datacenter clusters: 100s~10000s computers …,*,2012,*
CLOUD COMPUTING AND INFORMATION POLICY 1 Cloud Computing and Information Policy: Computing in a Policy Cloud? Forthcoming in the Journal of Informat...,Paul T Jaeger; Jimmy Lin; Justin M Grimes,Abstract Cloud computing is a computing platform that resides in a large data center and isable to dynamically provide servers the ability to address a wide range of needs; rangingfrom scientific research to e-commerce. The provision of computing resources as if it were autility such as electricity; while potentially revolutionary as a computing service; presentsmany major problems of information policy; including issues of privacy; security; reliability;access; and regulation. This paper explores the nature and potential of cloud computing; thepolicy issues raised; and research questions related to cloud computing and policy.Ultimately; the policy issues raised by cloud computing are examined as a part of largerissues of public policy attempting to respond to rapid technological evolution.,*,2011,*
Cross-corpus relevance projection,Nima Asadi; Donald Metzler; Jimmy Lin,Document corpora are key components of information retrieval test collections. However; forcertain tasks; such as evaluating the effectiveness of a new retrieval technique or estimatingthe parameters of a learning to rank model; a corpus alone is not enough. For these tasks;queries and relevance judgments associated with the corpus are also necessary. However;researchers often find themselves in scenarios where they only have access to a corpus; inwhich case evaluation and learning to rank become challenging. Document corpora arerelatively straightforward to gather. On the other hand; obtaining queries and relevancejudgments for a given corpus is costly. In production environments; it may be possible toobtain low-cost relevance information using query and click logs. However; in moreconstrained research environments these options are not available; and relevance …,Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,2011,*
In-depth accounts and passing mentions in the news: connecting readers to the context of a news event,Earl J Wagner; Jimmy Lin,Abstract Software that models how types of news events unfold can extract information aboutspecific events and explain them to a news reader. This support can be useful when thebackground provided by an article is insufficient; if other news coverage exists from which anevent's history can be extracted. For extended sequences of related events; it is reasonableto expect that articles published after the sequence concludes include less backgroundcoverage of the sequence. Focusing on two stereotypical types of event sequences---kidnappings and corporate acquisitions--we distinguish between articles providing in-depthcoverage; those having multiple sentences mentioning the same event sequence; fromarticles making a passing mention in just one sentence. We find that; after an eventsequence concludes; passing mentions become more common and there are …,Proceedings of the 2011 iConference,2011,*
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,Ciprian Chelba; Paul Kantor; Brian Roark,The first part of the tutorial introduces operations on weighted automata such asdeterminization and intersection/composition as well as the corresponding OpenFst binariesand library-level APIs that implement those operations. We describe how to read FSTs fromsimple textual descriptions and combine them into larger and more complex machines; andoptimize them using simple command-line and library calls.,*,2009,*
Is there a cloud in your future? Applications of “cloud computing” to Web-scale problems,Jimmy Lin,Abstract IBM and Google recently committed a total of $30 million over two years to aninitiative on “cloud computing”; in collaboration with six universities across the country (seereferences). They are: Berkeley; Carnegie Mellon; MIT; Stanford; the University of Maryland;and the University of Washington. I am the leader of this initiative at the University ofMaryland; and to my knowledge the only participant from an iSchool (the rest are lead byfaculty in computer science departments).“Cloud computing” refers to technology forexploiting large computer clusters to tackle “Web-scale” information processing problems;where immense quantities of data make traditional sequential processing impractical.Specifically; this initiative focuses on Google's MapReduce programming paradigm; whichwas specifically designed for processing extremely large data sets (and indeed used by …,*,2008,*
Large-Scale Network Analysis to Improve Retrieval in the Biomedical Domain,Jimmy Lin,MEDLINE is the authoritative repository of abstracts from the primary literature in thebiomedical domain; maintained by the National Library of Medicine. It currently containsover 17 million records. PubMed [1] is a publicly accessible Web gateway to the system. It isconsidered one of the most important resources available to researchers in biology;medicine; biochemistry; etc. Currently; PubMed implements a Boolean search algorithm thatsorts results in reverse chronological order.,*,2007,*
TOIS reviewers January 2006 through May 2007,Gary Marchionini; Ahmed Abbasi; Eugene Agichtein; Khurshid Ahmad; Azzah Al-Maskari; Gianni Amati; Sihem Amer Yahia; Shlomo Argamon; Daniel Ashbrook; Paolo Atzeni; Michela Bacchin; Godmar Back; Antonio Badia; Andras Banczur; Bettina Berendt; Elisa Bertino; B Bhagyavati; Suresh Bhavnani; Devdutta Bhosale; David Bodoff; Paolo Boldi; Johan Bollen; Angela Bonifati; Pia Borlund; Jit Bose; Athman Bouguettaya; Michael Brinkmeier; Peter Brown; Peter Brusilovsky; Peter Bruza; Christopher Burges; Robin Burke; Ben Carterette; Arthur Cater; Kuiyu Chang; Hsin Hsi Chen; Zheng Chen; James Cheney; Pu Jen Cheng; Roger Chiang; Byron Choi; Tat Seng Chua; Charlie Clarke; Paul Clough; Mariano Consens; Gordon Cormack; Nick Craswell; Fabio Crestani; Carolyn Crouch; Silviu Petru Cucerzan; Hang Cui; Sally Jo Cunningham; Edward Cutrell; Pablo De La Fuente; Arjen De Vries; Anne Diekema; Sandor Dominich; Shyamala Doraisamy; Mark Dunlop; Georges Dupret; Miles Efron; Jeremy Ellman; Peter Enser; Gunes Erkan; Laura Fochtmann; Anders Fongen; Nigel Ford; Martin Franz; Xin Fu; Paolo Garza; Susan Gauch; Pierre Geneves; Henry Gladney; Melanie Gnasa; Andrew Goldberg; Marcos Goncalves; Cyril Goutte; David Grossman; Dennis Groth; Jacek Gwizdka; Stephanie Haas; Sanda Harabagiu; Donna Harman; Andreas Henrich; Djoerd Hiemstra; Lee Hollaar; Chun Nan Hsu; Fei Huang; Zan Huang; Mike Huhns; Carlos Hurtado; Keisuke Innoue; Panagiotis Ipeirotis; Bernard Jansen; Wang Jianqiang; Rong Jin; Marko Junkkari; Patrick Juola; Vinay Kakade; Jaap Kamps; In Ho Kang; Damianos Karakos; Vangelis Karkaletsis; Martin Kaszkiel; Siddharth Kaza; Jaana Kekäläinen; Diane Kelly; Benny Kimelfeld; Alek Kolcz; Joseph Konstan; Kui Lam Kwok; Abhimanyu Lad; Alberto Laender; Mounia Lalmas; Leah Larkey; Ray Larson; Nabil Layaida; Zhang Le; Dik Lun Lee; Dongwon Lee; Jochen Leidner; Gina Levow; Hang Li; Xin Li; Chin Yew Lin; Jimmy Lin; Tie Yan Liu; Zehua Liu; David Losada; Jie Lu; Yiming Ma; Inderjeet Mani; Murali Mani; Ioana Manolescu; Catherine Marshall; Mercedes Martinez; Yosi Mass; Paul McNamee; Sean McNee; Brahim Medjahed; Lokman Meho; Donald Metzler; Rada Mihalcea; Ruslan Mitkov; Bamshad Mobasher; Marina Mongiello; Ani Nenkova; Frank Neven; Dorbin Ng; Wilfred Ng,Marchionini; Gary; Abbasi; Ahmed; Agichtein; Eugene; Ahmad; Khurshid; Al-Maskari; Azzah;Amati; Gianni; Yahia; Sihem Amer; Argamon; Shlomo; Ashbrook; Daniel; Atzeni; Paolo;Bacchin; Michela; Back; Godmar; Badia; Antonio; Banczur; Andras; Berendt; Bettina; Bertino;Elisa; Bhagyavati; B.; Bhavnani; Suresh; Bhosale; Devdutta; Bodoff; David; Boldi; Paolo;Bollen; Johan; Bonifati; Angela; Borlund; Pia; Bose; Jit; Bouguettaya; Athman; Brinkmeier;Michael; Brown; Peter; Brusilovsky; Peter; Bruza; Peter; Burges; Christopher; Burke; Robin;Carterette; Ben; Cater; Arthur; Chang; Kuiyu; Chen; Hsin Hsi; Chen; Zheng; Cheney; James;Cheng; Pu Jen; Chiang; Roger; Choi; Byron; Chua; Tat Seng; Clarke; Charlie; Clough; Paul;Consens; Mariano; Cormack; Gordon; Craswell; Nick; Crestani; Fabio; Crouch … In: ACMTransactions on Information Systems; Vol. 25; No. 4; 15; 01.10.2007.,ACM Transactions on Information Systems,2007,*
Presentation schemes for component analysis in IR experiments,Paul Kantor; Jimmy Lin,Abstract Information retrieval research; at least as conceived by the SIGIR community; isfundamentally experimental in nature. As such; the presentation of results from controlled;reproducible experiments lies at the core of our work. Many reports follow the same generalformat: authors propose a new retrieval method; whose performance on some well-definedtask is compared against a baseline. Authors also report results from alternativeconfigurations; eg; variations in parameters; turning off (ablation) of different components;etc. The presentation of experimental results forms an integral part of the conferences andjournals that comprise the medium in which knowledge is disseminated.,ACM SIGIR Forum,2007,*
LAMP-TR-130 CS-TR-4787 UMIACS-TR-2006-11,Answering Relationship Questions; Jimmy Lin,*,*,2006,*
LAMP-TR-127 CS-TR-4771 UMIACS-TR-2005-71,Jimmy Lin; Dina Demner-Fushman,*,*,2005,*
LAMP-TR-118 CS-TR-4693 UMIACS-TR-2005-03,Jimmy Lin,Abstract Controlled and reproducible laboratory experiments; enabled by reusable testcollections; represent a well-established methodology in modern information retrievalresearch. In order to confidently draw conclusions about the performance of differentretrieval methods using test collections; their reliability and trustworthiness must first beestablished. Although such studies have been performed for ad hoc test collections;currently available resources for evaluating question answering systems have not beensimilarly analyzed. This study evaluates the quality of answer patterns and lists of relevantdocuments currently employed in automatic question answering evaluation; and concludesthat they are not suitable for post-hoc experimentation. These resources; created by poolingruns of TREC QA track participants; do not produce fair and reliable assessments of …,*,2005,*
START: a framework for facilitating e-rulemaking,Boris Katz; Roger Hurwitz; Jimmy J Lin; Ozlem Uzuner,Abstract Federal agencies implement laws passed by Congress by making rules andregulations that can be applied in practice. Stakeholders and members of the public usuallywant to know how proposed rules will affect them; so they can effectively respond to theproposals; during the comment period. While the stakeholders; like business and advocacygroups; can employ information specialists to get their answers; individuals will have to turnto the rulemaking agencies for such information. Consequently; for online rulemaking toencourage and support public participation; there will be need for information access that issimple and intuitive to use; comprehensive in the material covered; specific to the user'sneeds and timely.,Proceedings of the 2003 annual national conference on Digital government research,2003,*
Sticky Notes for the Semantic Web,David Karger Mit; David R Karger; Boris Katz; Jimmy Lin; Dennis Quan,Abstract Computer-based annotation is increasing in popularity as a mechanism for revisingdocuments and sharing comments over the Internet. One reason behind this surge is thatviewpoints; summaries; and notes written by others are often helpful to readers. In particular;these types of annotations can help users locate or recall relevant documents. We believethat this model can be applied to the problem of retrieval on the Semantic Web. In this paper;we propose a generalized annotation environment that supports richer forms of descriptionsuch as natural language. We discuss how RDF can be used to model annotations and theconnections between annotations and the documents they describe. Furthermore; weexplore the idea of a question answering interface that allows retrieval based both on thetext of the annotations and the annotations' associated metadata. Finally; we speculate on …,In Proceedings of the 8th International Conference on Intelligent User Interfaces,2003,*
Start and Beyond,Boris Katz Jimmy J Lin,*,*,2002,*
Gregory Marton Alton Jerome McFarland Baris Temelkuran Artificial Intelligence Laboratory 200 Technology Square; Cambridge; MA 02139,Boris Katz; Sue Felshin; Deniz Yuret; Ali Ibrahim; Jimmy Lin,Abstract Although the World Wide Web contains a tremendous amount of information; thelack of uniform structure makes finding the right knowledge difficult. A solution to thisproblem is to turn the Web into a “virtual database” and to access it through naturallanguage. We built a system called Omnibase that integrates heterogeneous data sourcesusing an object–attribute–value model. With the help of Omnibase; our Start naturallanguage question answering system can now answer millions of questions consulting datasources found on the World Wide Web.,*,2002,*
Incomplete Information-based Labour Force Market Game,JIANG Lin,This paper discusses the dynamic game of labour market in two stages; givesthe Perfect Bsyesian Equilibrium (PBE) of individuals in every game; and analysesthe lay-off and unemployment phenomena in reality.,Journal of Chongqing Institute of Technology Management,2002,*
2014 25th International Workshop on Database and Expert Systems Applications (DEXA)(2014),Gebrekirstos G Gebremeskel; Jiyin He; Arjen P de Vries; Jimmy Lin,*,*,*,*
Answering multiple questions on a topic from heterogeneous,Boris Katz; Matthew Bilotti; Sue Felshin; Aaron Fernandes; VVesley Hildebrandt; Roni Katzir; Jimmy Lin; Daniel Loreto; Gregory Marton; Federico Mora; Ozlem Uzuner,*,*,*,*
Evaluation of a Natural Language Dialog Based Web Navigation System--A Case Study,Joyce Chai; Jimmy Lin; Wlodek Zadrozny; Yiming Ye; Margo Budzikowska; Veronika Horvath; Nanda Kambhatla; Catherine Wolf,With the emergence of e-commerce; websites must accommodate both customer needs andbusiness requirements. Menu driven navigation and keyword search provided by mostcommercial sites have tremendous limitations. There is no way to balance the current needsand intentions of a user with the business requirements of the site. Often; as a result; usersare overwhelmed and frustrated by the lengthy interaction; because it's hard to preciselydescribe their intentions; eg buying" dark pants without cuffs". The solution lies; in ouropinion; in centering electronic commerce websites around natural language andmultimodal dialog. This claim is supported by results of a recent study we performed; andwhich we present in this paper.,*,*,*
Protecting Sensitive Email: Archival Views on Challenges and Opportunities,Katie Shilton; Amy Wickner; Douglas W Oard; Jimmy Lin,Email is a critical institutional and personal record; but including email in archival collectionscan raise privacy concerns. Semi-structured interviews with archivists describe challengessuch as interleaving of personal and institutional records; and donors' complex definitions ofsensitive information. Limited current solutions suggest potential technical interventions;such as identifying and filtering commonly sensitive information types; and analyzing thecontext and content of messages to find anomalous records. We discuss how these findingscontribute to privacy-sensitive search tools for email collections. Introduction:,*,*,*
Information Access (EVIA 2016); a Satellite Workshop of the NTCIR-12 Conference; June 7; 2016 Tokyo Japan,Emine Yilmaz; L Charles; A Clarke; Keynote Yiqun Liu; Seyyed Hadi Hashemi; Charles LA Clarke; Adriel Dean-Hall; Jaap Kamps; Julia Kiseleva; Tetsuya Sakai; Lifeng Shang; Camilla Sanvitto; Debasis Ganguly; Gareth JF Jones; Gabriella Pasi; Promoting Repeatability Through Open Runs; Ellen Voorhees; Shahzad Rajput; Ian Soboroff; Douglas W Oard; Katie Shilton; Jimmy Lin; Makoto P Kato; Virgil Pavlu; Takehiro Yamamoto; Hajime Morita,Page 1. i Proceedings of the Seventh International Workshop on Evaluating Information Access(EVIA 2016); a Satellite Workshop of the NTCIR-12 Conference; June 7; 2016 Tokyo JapanTable of Contents Preface Emine Yilmaz and Charles L. A. Clarke Keynote Yiqun Liu An EasterEgg Hunting Approach to Test Collection Building in Dynamic Domains Seyyed Hadi Hashemi;Charles LA Clarke; Adriel Dean-Hall; Jaap Kamps and Julia Kiseleva 1 - 8 On EstimatingVariances for Topic Set Size Design Tetsuya Sakai and Lifeng Shang 9 - 12 A Laboratory-BasedMethod for the Evaluation of Personalised Search Camilla Sanvitto; Debasis Ganguly; GarethJF Jones and Gabriella Pasi 13 - 16 Promoting Repeatability Through Open Runs EllenVoorhees; Shahzad Rajput and Ian Soboroff 17 - 20 Evaluating Search Among Secrets DouglasW. Oard; Katie Shilton and Jimmy Lin 21 - 24 …,*,*,*
本报告是基于 MINDS 研讨会的五份报告中的其中一份; 该研讨会由 Donna Harman (NIST) 领导并由国家情报总监办公室科学技术办公室下属的突破性技术办公室,Liz Liddy; Eduard Hovy; Jimmy Lin; John Prager; Dragomir Radev; Lucy Vanderwende; Ralph Weischedel,过去几年中产生的一系列发现与进展已经使自然语言处理规则发生了重大的转变.相比其他转变; 有一些转变具有更深刻的影响. 但是; 今天人们认为实际上每次转变都具有重大影响; 尽管这些转变在当时可能未被重视. 有些转变并不是方法或途径上的实际转变;而是转变的推动者; 它们导致规则发生作用的方式产生了重大变化. 在自然语言处理领域出现的最早期; 就有证明说明自然语言处理能够在真实端对端系统(虽然只是游戏) 中产生具有实际语言处理层次的运算系统; 包括Winograd 设计的SHRDLU 系统(1971) 和Woods 设计的LUNAR系统(1970). 每个系统都能够完成一项明确的任务; 例如在积木世界里摆弄积木或者回答关于从月球采集的样本的问题. 他们能够达到一些有限目标; 这是因为他们涉及了与人类交互过程中的所有语言处理层次; 包括形态学; 词法; 句法; 语义学; 话语以及语用学. 这些演示系统激发了新领域的产生; 但是还需要许多年才能出现可以在真实世界系统中运行的包含更复杂的处理 …,*,*,*
Optimization Techniques for “Scaling Down” Hadoop on Multi-Core; Shared-Memory Systems,K Ashwin Kumar Jonathan Gluck; Amol Deshpande; Jimmy Lin,ABSTRACT The underlying assumption behind Hadoop and; more generally; the need fordistributed processing is that the data to be analyzed cannot be held in memory on a singlemachine. Today; this assumption needs to be re-evaluated. Although petabyte-scaledatastores are increasingly common; it is unclear whether “typical” analytics tasks requiremore than a single high-end server. Additionally; we are seeing increased sophistication inanalytics; eg; machine learning; where we process smaller and more refined datasets. Toaddress these trends; we propose “scaling down” Hadoop to run on multi-core; shared-memory machines. This paper presents a prototype runtime called Hone (“Hadoop One”)that is API compatible with Hadoop. With Hone; we can take an existing Hadoop applicationand run it efficiently on a single server. This allows us to take existing MapReduce …,*,*,*
1 Summarization Factors,Jimmy Lin,To better understand summarization; it is helpful to enumerate its many dimensions—whatSparck Jones [19] calls “factors”. These factors provide a basis for understanding variousautomatic methods; and can be grouped into three broad categories: input; purpose; andoutput. What follows is meant to be an overview of important factors; and not intended to beexhaustive.,*,*,*
If All You Have is a Hammer; Throw Away Everything That’s Not a Nail!,Jimmy Lin,Abstract Hadoop is currently the large-scale data analysis ''hammer''of choice; but there existclasses of algorithms that aren't ''nails''in the sense that they are not particularly amenable tothe MapReduce programming model. To address this; researchers have proposedMapReduce extensions or alternative programming models in which these algorithms canbe elegantly expressed. This article espouses a very different position: that MapReduce is''good enough;''and that instead of trying to invent screwdrivers; we should simply get rid ofeverything that's not a nail. To be more specific; much discussion in the literature surroundsthe fact that iterative algorithms are a poor fit for MapReduce. The simple solution is to findalternative; noniterative algorithms that solve the same problem. This article captures mypersonal experiences as an academic researcher as well as a software engineer in a '' …,*,*,*
Organizing and accessing a comprehensive knowledge base using the World Wide Web,Boris Katz; JJ Lin,To address the problem of information overload in today's world; we have developedSTART; a natural language question answering system that provides users with high-precision information access through the use of natural language annotations. To addressthe difficulty of accessing large amounts of heterogeneous structured and semistructureddata; we have developed Omnibase; which assists START by integrating Web databasesinto a single; uniformly structured" virtual database." To address the sheer amount ofunstructured information available electronically; we have developed techniques fordistilling large amounts of free text into relations that capture the salient aspects of the text.The combination of natural language annotation technology; object-property-value datamodel; and relation extraction technology allows us to rapidly develop and deploy smart …,Integration of Knowledge Intensive Multi-Agent Systems; 2003. International Conference on,*,*
In Proceedings of the 5th Pacific Rim Conference on Artificial Intelligence (PRICAI 1998); November 1998; Singapore,Boris Katz Deniz Yuret Jimmy Lin; Sue Felshin Rebecca Schulman Adnan Ilik,Abstract The flow of natural language is often broken by constructions which are difficult toanalyze with conventional linguistic parsers. To handle these constructions; which includenumbers; dates; addresses; etc.; and; to a lesser extent; proper nouns; NL systems typicallyimplement specialized new rules. This leads to a level of complexity which rendersmaintenance or improvement difficult. Analyzing and tokenizing these constructions with anindependent preprocessor can alleviate the burden on already taxed systems. Becausethese constructions have highly regular forms; strict structure; and can be largely understoodin the absence of context; it is possible to shift the burden of processing away from theprimary parser; and onto a simpler; faster; non-linguistic preprocessor. This paper describesBlitz; a hybrid database-and heuristic-based natural language preprocessor; which has …,*,*,*
Why is Finding What You Want So Difficult?,Jimmy Lin,*,UNDERGRADUATE RESEARCH,*,*
Computational Linguistics for Metadata Building (CLiMB) Text Mining for the Automatic Extraction of Subject Terms for Image Metadata,Judith L Klavans Tandeep Sidhu; Carolyn Sheffield; Jimmy Lin; Eileen Abels; Rebecca Passonneau,Abstract. In this paper; we present a fully-implemented system using computational linguistictechniques to apply automatic text mining for the extraction of metadata for image access.We describe the implementation of a workbench created for; and evaluated by; imagecatalogers. We discuss the current functionality and future goals for this image catalogers'toolkit; developed in the Computational Linguistics for Metadata Building (CLiMB) researchproject. 6 Our primary user group for initial phases of the project is the cataloger expert; infuture work we address applications for end users.,Computational Linguistics for Metadata Building,*,*
Annotating the Semantic Web with Natural Language,Boris Katz; Jimmy Lin,The Problem: Semantic Web research focuses on imbuing ordinary Web pages with“semantics;” so that specially enabled software agents can help users better locate; collate;and compare data. Since natural language is the most intuitive information access methodfor humans; we believe that the Semantic Web should be “natural language enabled.”However; the application of natural language techniques to the Semantic Web is largelyunexplored. Motivation: The vision of the Semantic Web [1] grew out of the recognition thatalthough a wealth of information readily exists today in electronic form; it cannot be easilyprocessed by computers due to a lack of “semantics.” Because computers cannotunderstand the contents of documents; they are unable to generate new knowledge fromexisting documents. Because computers cannot related the contents of multiple …,*,*,*
The Bridge Project,Jake Beal; Nick Caldwell; Jimmy Lin; Justin Schmidt; Marc Spraragen; Patrick Winston,The Problem: We need better perception systems; both to support applications and tosupport research on intelligence. We believe that each subsystem of a better perceptionsystem will rely on contextual information from other perception subsystems and onknowledge of the evolving situation in the external world. Accordingly; we are building asystem; the Bridge System in which simple machine vision and natural languagesubsystems work together; sharing information; asking each other questions; and ultimately;solving problems that machine vision and natural language systems; working alone; cannot.Motivation: We humans are amazingly good at interpreting sensory data. Images we see orwords we hear are often noisy; ambiguous; and context dependent. In fact; there is muchevidence that our human perception of the world is partly hallucinated; based on our …,*,*,*
Natural Language Access to Visual Information,Boris Katz; Jerome McFarland; Jimmy Lin,Previous Work: Current research in video and image retrieval has concentrated on static;fixed libraries of video clips and still images. Furthermore; most techniques for video andimage retrieval focus on textual information that accompanies them; ie; speech transcriptionsof video or text captions of images. Naturally; such techniques are useless when textualinformation is not available. In contrast to approaches that derive from information retrieval;our research focuses on information access using raw data—non-textual data generated bycurrent vision systems. In addition; we focus on live; streaming vision data; as opposed to astatic collection of media that does not change over time. Approach: The Vision InterfacesGroup at the MIT AI Laboratory has developed a camera system capable of dynamicallytracking objects in an indoor environment. The cameras provide a continuous stream of …,*,*,*
MIT Artificial Intelligence Laboratory 545 Technology Square; Cambridge; MA 02139 USA,Boris Katz Deniz Yuret Jimmy Lin; Sue Felshin Rebecca Schulman Adnan Ilik,*,*,*,*
Mars Information Access Server,Boris Katz; Sue Felshin; Aaron Fernandes; Jimmy Lin; Jerome McFarland; Alp Simsek; Chloe Tergiman,The Jet Propulsion Laboratory has the dual interest of making it possible for the public tosend queries to Mars mission spacecraft while they are in operation; and of providingconvenient public access to information on their Web site about Mars and missions to Mars.Their data access needs serve as an excellent test bed for our technologies; and ourtechnologies can help them fulfill their data access needs. Approach: Our long-term goal isto build; in association with JPL; an information access server which will aid a future Marsmission rover in conversing with people concerning its activities on Mars. As our first steptoward this goal; we have created an information server which answers individual queries.,*,*,*
Generating Relations from Natural Language Using REXTOR,Boris Katz; Jimmy Lin; Sue Felshin,The Problem: Everyone wants to be able to find information rapidly and conveniently.Ideally; we would like to have a system with the understanding of a human being and theperfect memory of a computer. A user should be able to pose a question in plain English—for example;“what do frogs eat”—and get a sensible answer; such as “Adult frogs eat mainlyinsects and other small animals; including earthworms; minnows; and spiders.” A full naturallanguage processing (NLP) system should have the language comprehension of a human;but NLP has not yet advanced to that level. When applied to information retrieval (IR); NLPsystems suffer from poor recall because data cannot be parsed quickly or accuratelyenough; so that too little knowledge is correctly indexed.,*,*,*
