The adaptive radix tree: ARTful indexing for main-memory databases,Viktor Leis; Alfons Kemper; Thomas Neumann,Main memory capacities have grown up to a point where most databases fit into RAM. Formain-memory database systems; index structure performance is a critical bottleneck.Traditional in-memory data structures like balanced binary search trees are not efficient onmodern hardware; because they do not optimally utilize on-CPU caches. Hash tables; alsooften used for main-memory indexes; are fast but only support point queries. To overcomethese shortcomings; we present ART; an adaptive radix tree (trie) for efficient indexing inmain memory. Its lookup performance surpasses highly tuned; read-only search trees; whilesupporting very efficient insertions and deletions as well. At the same time; ART is veryspace efficient and solves the problem of excessive worst-case space consumption; whichplagues most radix trees; by adaptively choosing compact and efficient data structures for …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,120
Morsel-driven parallelism: a NUMA-aware query evaluation framework for the many-core age,Viktor Leis; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract With modern computer architecture evolving; two problems conspire against thestate-of-the-art approaches in parallel query execution:(i) to take advantage of many-cores;all query work must be distributed evenly among (soon) hundreds of threads in order toachieve good speedup; yet (ii) dividing the work evenly is difficult even with accurate datastatistics due to the complexity of modern out-of-order cores. As a result; the existingapproaches for plan-driven parallelism run into load balancing and context-switchingbottlenecks; and therefore no longer scale. A third problem faced by many-core architecturesis the decentralization of memory controllers; which leads to Non-Uniform Memory Access(NUMA). In response; we present the morsel-driven query execution framework; wherescheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,93
Exploiting hardware transactional memory in main-memory databases,Viktor Leis; Alfons Kemper; Thomas Neumann,So far; transactional memory-although a promising technique-suffered from the absence ofan efficient hardware implementation. The upcoming Haswell microarchitecture from Intelintroduces hardware transactional memory (HTM) in mainstream CPUs. HTM allows forefficient concurrent; atomic operations; which is also highly desirable in the context ofdatabases. On the other hand HTM has several limitations that; in general; prevent a one-to-one mapping of database transactions to HTM transactions. In this work we devise severalbuilding blocks that can be used to exploit HTM in main-memory databases. We show thatHTM allows to achieve nearly lock-free processing of database transactions by carefullycontrolling the data layout and the access patterns. The HTM component is used fordetecting the (infrequent) conflicts; which allows for an optimistic; and thus very low …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,81
How good are query optimizers; really?,Viktor Leis; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark (JOB) and experimentally revisit the main componentsin the classic query optimizer architecture using a complex; real-world data set and realisticmulti-join queries. We investigate the quality of industrial-strength cardinality estimators andfind that all estimators routinely produce large errors. We further show that while estimatesare essential for finding a good join order; query performance is unsatisfactory if the queryengine relies too heavily on these estimates. Using another set of experiments that measurethe impact of the cost model; we find that it has much less influence on query performancethan the cardinality estimates. Finally; we investigate plan enumeration techniquescomparing exhaustive dynamic programming with heuristic algorithms and find that …,Proceedings of the VLDB Endowment,2015,58
Massively parallel NUMA-aware hash joins,Harald Lang; Viktor Leis; Martina-Cezara Albutiu; Thomas Neumann; Alfons Kemper,Abstract Driven by the two main hardware trends increasing main memory and massivelyparallel multi-core processing in the past few years; there has been much research effort inparallelizing well-known join algorithms. However; the non-uniform memory access (NUMA)of these architectures to main memory has only gained limited attention in the design ofthese algorithms. We study recent proposals of main memory hash join implementations andidentify their major performance problems on NUMA architectures. We then develop aNUMA-aware hash join for massively parallel environments; and show how the specificimplementation details affect the performance on a NUMA system. Our experimentalevaluation shows that a carefully engineered hash join implementation outperformsprevious high performance hash joins by a factor of more than two; resulting in an …,*,2015,29
Effective and robust pruning for top-down join enumeration algorithms,Pit Fender; Guido Moerkotte; Thomas Neumann; Viktor Leis,Finding the optimal execution order of join operations is a crucial task of today's cost-basedquery optimizers. There are two approaches to identify the best plan: bottom-up and top-down join enumeration. For both optimization strategies efficient algorithms have beenpublished. However; only the top-down approach allows for branch-and-bound pruning.Two pruning techniques can be found in the literature. We add six new ones. Combined;they improve performance roughly by an average factor of 2-5. Even more important; ourtechniques improve the worst case by two orders of magnitude. Additionally; we introduce anew; very efficient; and easy to implement top-down join enumeration algorithm. Thisalgorithm; together with our improved pruning techniques; yields a performance which is byan average factor of 6-9 higher than the performance of the original top-down …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,24
Compiling Database Queries into Machine Code.,Thomas Neumann; Viktor Leis,Abstract On modern servers the working set of database management systems becomesmore and more main memory resident. Slow disk accesses are largely avoided; and thus thein-memory processing speed of databases becomes an important factor. One very attractiveapproach for fast query processing is justin-time compilation of incoming queries. Byproducing machine code at runtime we avoid the overhead of traditional interpretationsystems; and by carefully organizing the code around register usage we minimize memorytraffic and get excellent performance. In this paper we show how queries can be brought intoa form suitable for efficient translation; and how the underlying code generation can beorchestrated. By carefully abstracting away the necessary plumbing infrastructure we canbuild a query compiler that is both maintainable and efficient. The effectiveness of the …,IEEE Data Eng. Bull.,2014,23
Transaction Processing in the Hybrid OLTP&OLAP Main-Memory Database System HyPer,Alfons Kemper; Thomas Neumann; Jan Finis; Florian Funke; Viktor Leis; Henrik Mühe; Tobias Mühlbauer; Wolf Rödiger,Abstract Two emerging hardware trends have re-initiated the development of in-coredatabase systems: ever increasing main-memory capacities and vast multi-core parallelprocessing power. Main-memory capacities of several TB allow to retain all transactionaldata of even the largest applications in-memory on one (or a few) servers. The vastcomputational power in combination with low data management overhead yieldsunprecedented transaction performance which allows to push transaction processing (awayfrom application servers) into the database server and still “leaves room” for additional queryprocessing directly on the transactional data. Thereby; the often postulated goal of real-timebusiness intelligence; where decision makers have access to the latest version of thetransactional state; becomes feasible. In this paper we will survey the HyPerScript …,*,2013,20
HyPer: Adapting Columnar Main-Memory Data Management for Transactional AND Query Processing.,Alfons Kemper; Thomas Neumann; Florian Funke; Viktor Leis; Henrik Mühe,Abstract Traditionally; business applications have separated their data into an OLTP datastore for high throughput transaction processing and a data warehouse for complex queryprocessing. This separation bears severe maintenance and data consistencydisadvantages. Two emerging hardware trends allow the consolidation of the two disparateworkloads onto the same database state on one system: the increasing main memorycapacities of several terabytes per server and multi-threaded processing based on multicoreparallelism. The prevalent data representation of hybrid OLTP&OLAP main memorydatabase systems is columnar in order to achieve best possible query executionperformance for OLAP applications. In order to shield the OLTP transaction processing fromlong-running queries without costly locking/latching; all queries are executed on an …,IEEE Data Eng. Bull.,2012,19
Efficient processing of window functions in analytical SQL queries,Viktor Leis; Kan Kundhikanjana; Alfons Kemper; Thomas Neumann,Abstract Window functions; also known as analytic OLAP functions; have been part of theSQL standard for more than a decade and are now a widely-used feature. Window functionsallow to elegantly express many useful query types including time series analysis; ranking;percentiles; moving averages; and cumulative sums. Formulating such queries in plain SQL-92 is usually both cumbersome and inefficient. Despite being supported by all majordatabase systems; there have been few publications that describe how to implement anefficient relational window operator. This work aims at filling this gap by presenting anefficient and general algorithm for the window operator. Our algorithm is optimized for high-performance main-memory database systems and has excellent performance on modernmulti-core CPUs. We show how to fully parallelize all phases of the operator in order to …,Proceedings of the VLDB Endowment,2015,14
The ART of practical synchronization,Viktor Leis; Florian Scheibner; Alfons Kemper; Thomas Neumann,Abstract The performance of transactional database systems is critically dependent on theefficient synchronization of in-memory data structures. The traditional approach; fine-grainedlocking; does not scale on modern hardware. Lock-free data structures; in contrast; scalevery well but are extremely difficult to implement and often require additional indirections. Inthis work; we argue for a middle ground; ie; synchronization protocols that use locking; butonly sparingly. We synchronize the Adaptive Radix Tree (ART) using two such protocols;Optimistic Lock Coupling and Read-Optimized Write EXclusion (ROWEX). Both perform andscale very well while being much easier to implement than lock-free techniques.,Proceedings of the 12th International Workshop on Data Management on New Hardware,2016,11
Scaling HTM-supported database transactions to many cores,Viktor Leis; Alfons Kemper; Thomas Neumann,So far; transactional memory-although a promising technique-suffered from the absence ofan efficient hardware implementation. Intel's Haswell microarchitecture introduced hardwaretransactional memory (HTM) in mainstream CPUs. HTM allows for efficient concurrent;atomic operations; which is also highly desirable in the context of databases. On the otherhand; HTM has several limitations that; in general; prevent a one-to-one mapping ofdatabase transactions to HTM transactions. In this work; we devise several building blocksthat can be used to exploit HTM in main-memory databases. We show that HTM allows forachieving nearly lock-free processing of database transactions by carefully controlling thedata layout and the access patterns. The HTM component is used for detecting the(infrequent) conflicts; which allows for an optimistic; and thus very low-overhead …,IEEE Transactions on Knowledge and Data Engineering,2016,6
Cardinality Estimation Done Right: Index-Based Join Sampling.,Viktor Leis; Bernharde Radke; Andrey Gubichev; Alfons Kemper; Thomas Neumann,ABSTRACT After four decades of research; today's database systems still suffer from poorquery execution plans. Bad plans are usually caused by poor cardinality estimates; whichhave been called the “Achilles Heel” of modern query optimizers. In this work we proposeindexbased join sampling; a novel cardinality estimation technique for main-memorydatabases that relies on sampling and existing index structures to obtain accurate estimates.Results on a real-world data set show that this approach significantly improves estimation aswell as overall plan quality. The additional sampling effort is quite low and can be configuredto match the desired application profile. The technique can be easily integrated into mostsystems.,CIDR,2017,4
Fast multi-tier indexing supporting dynamic update,*,A method includes performing a lookup using a key into a root node of a multi-tier datastructure; to find a partition for performing an insert. A lookup for the key is performed on afirst level index that is part of a linked data structure. A payload or reference is added to thelinked data structure based on data structure criterion; otherwise the key and the payloadare added to the linked data structure if the key is not found. A new first level index is createdand added to the linked data structure upon the linked data structure remaining unchanged.The key and the payload or reference are added to the new index. Based on merge criterion;a new second level index is created and a portion of content from selected first level andsecond level indexes are merged for combining into the new second level index.,*,2016,4
Concurrent reads and inserts into a data structure without latching or waiting by readers,*,A method includes performing; by a data structure processor; concurrent read and writeoperations into a hierarchical data structure. Writers acquire latches on the hierarchical datastructure elements that the latches modify. The hierarchical data structure elements aredirectly accessed by readers without acquiring latches. A modify operation is executed by awriter for one or more levels of the hierarchical data structure. When removed portions of thehierarchical data structure are no longer referenced; tracking is performed by use of acombination of a global state value and a copied local state value. The global state valuetransitions through a non-repeating sequence of values. No longer referenced portions ofthe hierarchical data structure are tagged with the current global state value.,*,2016,2
Die Evolution des Hauptspeicher-Datenbanksystems HyPer: Von Transaktionen und Analytik zu Big Data sowie von der Forschung zum Technologietransfer,Alfons Kemper; Viktor Leis; Thomas Neumann,Zusammenfassung Wir beschreiben die evolutionäre Entwicklung des an der TUMentwickelten Hauptspeicher‐Datenbanksystems HyPer; das für heterogene Workloadsbestehend aus Transaktionen; analytischen Anfragen bis hin zu Big Data Explorationengleichermaßen konzipiert wurde. Der Vorteil eines solchen „all-in-one “Datenbanksystemsgegenüber bisher gebräuchlichen dedizierten Einzelsystemen besteht darin; dass dieDatenexploration auf dem jüngsten Datenbankzustand basiert und somit aktuell gültigeErkenntnisse liefert.,*,2017,1
Query optimization through the looking glass; and what we found running the Join Order Benchmark,Viktor Leis; Bernhard Radke; Andrey Gubichev; Atanas Mirchev; Peter Boncz; Alfons Kemper; Thomas Neumann,Abstract Finding a good join order is crucial for query performance. In this paper; weintroduce the Join Order Benchmark that works on real-life data riddled with correlations andintroduces 113 complex join queries. We experimentally revisit the main components in theclassic query optimizer architecture using a complex; real-world data set and realistic multi-join queries. For this purpose; we describe cardinality-estimate injection and extractiontechniques that allow us to compare the cardinality estimators of multiple industrial SQLimplementations on equal footing; and to characterize the value of having perfect cardinalityestimates. Our investigation shows that all industrial-strength cardinality estimators routinelyproduce large errors: though cardinality estimation using table samples solves the problemfor single-table queries; there are still no techniques in industrial systems that can deal …,The VLDB Journal,2017,1
Query Processing and Optimization in Modern Database Systems,Viktor Leis,Relational database management systems; which were designed decades ago; are still thedominant data processing platform. Since then; large DRAM capacities and servers withmany cores have fundamentally changed the hardware landscape. As a consequence;traditional database systems cannot exploit modern hardware e ectively anymore. Thispaper summarizes author's thesis; which focuses on the challenges posed by modernhardware for transaction processing; query processing; and query optimization. In particular;we present a concurrent transaction processing system based on hardware transactionalmemory and show how to synchronize data structures e ciently. We further design a parallelquery engine for many-core CPUs that supports the important relational operators includingjoin; aggregation; window functions; etc. Finally; we dissect the query optimization …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,1
Monopedia: staying single is good enough--the hyper way for web scale applications,Maximilian E Schüle; Pascal Schliski; Thomas Hutzelmann; Tobias Rosenberger; Viktor Leis; Dimitri Vorona; Alfons Kemper; Thomas Neumann,Abstract In order to handle the database load for web scale applications; the conventionalwisdom is that a cluster of database servers and a caching layer are essential. In this work;we argue that modern main memory database systems are often fast enough to consolidatethis complex architecture into a single server (plus an additional fail over system). Todemonstrate this claim; we design the Monopedia Benchmark; a benchmark for web scaleapplications modeled after Wikipedia. Using this benchmark; we show that it is indeedpossible to run the database workload of one of the largest web sites in the world on a singledatabase server.,Proceedings of the VLDB Endowment,2017,*
The Complete Story of Joins (in HyPer),Thomas Neumann; Viktor Leis; Alfons Kemper,SQL has evolved into an (almost) fully orthogonal query language that allows (arbitrarilydeeply) nested subqueries in nearly all parts of the query. In order to avoid recursiveevaluation strategies which incur unbearable O (n2) runtime we need an extended relationalalgebra to translate such subqueries into non-standard join operators. This paperconcentrates on the non-standard join operators beyond the classical textbook inner joins;outer joins and (anti) semi joins. Their implementations in HyPer were covered in previouspublications which we refer to. In this paper we cover the new join operators mark-join andsingle-join at both levels: At the logical level we show the translation and reorderingpossibilities in order to e ectively optimize the resulting query plans. At the physical level wedescribe hash-based and block-nested loop implementations of these new joins. Based …,Datenbanksysteme für Business; Technologie und Web (BTW 2017),2017,*
HyPer Beyond Software: Exploiting Modern Hardware for Main-Memory Database Systems,Florian Funke; Alfons Kemper; Tobias Mühlbauer; Thomas Neumann; Viktor Leis,Abstract In this paper; we survey the use of advanced hardware features for optimizing main-memory database systems in the context of our HyPer project. We exploit the virtual memorymanagement for snapshotting the transactional data in order to separate OLAP queries fromparallel OLTP transactions. The access behavior of database objects from simultaneousOLTP transactions is monitored using the virtual memory management component in orderto compact the database into hot and cold partitions. Utilizing many-core NUMA-organizeddatabase servers is facilitated by the morsel-driven adaptive parallelization and partitioningthat guarantees data locality wrt the processing core. The most recent HardwareTransactional Memory support of; eg; Intel's Haswell processor; can be used as the basis fora lock-free concurrency control scheme for OLTP transactions. Finally; we show how …,Datenbank-Spektrum,2014,*
Cubic Cost Functions and Major Market Structures,Oliver Nikutowski; Viktor Leis; Robert K Frhr von Weizsäcker,Abstract Virtually every microeconomic textbook presents a stepwise and detaileddiscussion of households' and firms' objective functions before both sides are broughttogether within the framework of different market structures. Undoubtedly; this is the highroad to standard microeconomic theory. But along this road the useful combination of total;marginal and welfare analysis and the dual relationship between production; cost and profitfunctions are not always applied. Moreover; at the end of the road a comparative summaryand a visualization of the implications of major market structures are missing. This is wherethis Web site wants to add: Taking representative cubic cost and linear demand functions asgiven it combines total; marginal and welfare analysis for the cases of perfect competition;monopoly and monopsony within a single Java Applet. It illustrates the price dependence …,The Journal of Economic Education,2013,*
On the Performance and Pruning Power of Different Join Enumeration Strategies,Viktor Leis,Abstract: To find the optimal join order two different generative join enumeration strategieshave been proposed. The most commonly used one is dynamic programming whichproceeds bottom-up. The alternative is top down enumeration with memoization. For bothstrategies algorithms exist that enumerate only solutions without cartesian products; which isa commonly used heuristics. With top-down enumeration it is possible to further improveoptimization time by pruning the search space while still obtaining th...»,*,2011,*
LeanStore: In-Memory Data Management Beyond Main Memory,Viktor Leis; Michael Haubenschild; Alfons Kemper; Thomas Neumann,Abstract—Disk-based database systems use buffer managers in order to transparentlymanage data sets larger than main memory. This traditional approach is effective atminimizing the number of I/O operations; but is also the major source of overhead incomparison with in-memory systems. To avoid this overhead; in-memory database systemstherefore abandon buffer management altogether; which makes handling data sets largerthan main memory very difficult. In this work; we revisit this fundamental dichotomy anddesign a novel storage manager that is optimized for modern hardware. Our evaluation;which is based on TPC-C and micro benchmarks; shows that our approach has littleoverhead in comparison with a pure in-memory system when all data resides in mainmemory. At the same time; like a traditional buffer manager; it is fully transparent and can …,*,*,*
Adaptive Execution of Compiled Queries,André Kohn; Viktor Leis; Thomas Neumann,Abstract—Compiling queries to machine code is a very efficient way for executing queries.One often overlooked problem with compilation is the time it takes to generate machinecode. Even with fast compilation frameworks like LLVM; generating machine code forcomplex queries often takes hundreds of milliseconds. Such durations can be a majordisadvantage for workloads that execute many complex; but quick queries. To solve thisproblem; we propose an adaptive execution framework; which dynamically switches frominterpretation to compilation. We also propose a fast bytecode interpreter for LLVM; whichcan execute queries without costly translation to machine code and dramatically reduces thequery latency. Adaptive execution is fine-grained; and can execute code paths of the samequery using different execution modes. Our evaluation shows that this approach achieves …,*,*,*
HyPer: one DBMS for all,Tobias Mühlbauer; Florian Funke; Viktor Leis; Henrik Mühe; Wolf Rödiger; Alfons Kemper; Thomas Neumann,Page 1. HyPer: one DBMS for all Tobias Mühlbauer; Florian Funke; Viktor Leis; Henrik Mühe;Wolf Rödiger; Alfons Kemper; Thomas Neumann Technische Universität München New EnglandDatabase Summit 2014 http://www.hyper-db.com/ Page 2. One DBMS for all? OLTP and OLAPTraditionally; DBMSs are either optimized for OLTP or OLAP OLTP • high rate of mostly tinytransactions • high data access locality OLAP • few; but long-running transactions • scans largeparts of the database • must see a consistent database state during execution Conflict of interest:traditional solutions like 2PL would block OLTP However: Main memory DBMSs enable newoptions! OLAP OLTP Isolation: Snapshotting Page 3. One DBMS for all? Wimpy and brawnyHigh-performance DBMSs are optimized for brawny servers Brawny servers • predominantlyx86-64 arch • multiple sockets; many cores Wimpy devices …,*,*,*
