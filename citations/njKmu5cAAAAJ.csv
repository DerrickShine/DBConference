Roadrunner: Towards automatic data extraction from large web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract The paper investigates techniques for extracting data from HTML sites through theuse of automatically generated wrappers. To automate the wrapper generation and the dataextraction process; the paper develops a novel technique to compare HTML pages andgenerate a wrapper based on their similarities and differences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach.,VLDB,2001,1373
To weave the web,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abstract The paper discusses the issue of views in the Web context. We introduce a set oftools and languages for managing and restructuring data coming from the World Wide Web.We present a speci c data model; called the Araneus Data Model; inspired to the structurestypically present in Web sites. The model allows us to describe the scheme of a Webhypertext; in the spirit of databases. Based on the data model; we develop two languages tosupport a sophisticate view de nition process: the rst; called Ulixes; is used to build databaseviews of the Web; which can then be analyzed and integrated using database techniques;the second; called Penelope; allows the de nition of derived Web hypertexts from relationalviews. This can be used to generate hypertextual views over the Web.,VLDB,1997,322
Design and maintenance of data-intensive web sites,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abstract A methodology for designing and maintaining large Web sites is introduced. Itwould be especially useful if data to be published in the site are managed using a DBMS.The design process is composed of two intertwined activities: database design andhypertext design. Each of these is further divided in a conceptual phase and a logical phase;based on specific data models; proposed in our project. The methodology strongly supportssite maintenance: in fact; the various models provide a concise description of the sitestructure; they allow to reason about the overall organization of pages in the site andpossibly to restructure it.,International Conference on Extending Database Technology,1998,261
Automatic annotation of data extracted from large Web sites.,Luigi Arlotta; Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,ABSTRACT Data extraction from web pages is performed by software modules calledwrappers. Recently; some systems for the automatic generation of wrappers have beenproposed in the literature. These systems are based on unsupervised inference techniques:taking as input a small set of sample pages; they can produce a common wrapper to extractrelevant data. However; due to the automatic nature of the approach; the data extracted bythese wrappers have anonymous names. In the framework of our ongoing projectRoadRunner; we have developed a prototype; called Labeller; that automatically annotatesdata extracted by automatically generated wrappers. Although Labeller has been developedas a companion system to our wrapper generator; its underlying approach has a generalvalidity and therefore it can be applied together with other wrapper generator systems …,WebDB,2003,170
Semistructured and structured data in the web: Going back and forth,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,As a consequence of the explosion of the World Wide Web 11]; an increasing amount ofinformation is stored in repositories organized according to loose structures; usually ashypertextual documents; and data access is based on browsing and information retrievaltechniques. Due to their intuitive nature; browsing and searching present severe limitations8]. Also; they o er little or no support to a global view of information in the Web; nor to theactual extraction and manipulation of data: speci ce ort is required to use Web data as inputto subsequent computations or to correlate values in Web pages. In this paper; we presentthe approach to the management of Web data as attacked in the Araneus project carried outby the database group at Universit a di Roma Tre. Our approach is based on ageneralization of the notion of view to the Web framework. In fact; in traditional databases …,SIGMOD Record,1997,170
The Araneus Web-based management system,Giansalvatore Mecca; Paolo Atzeni; Alessandro Masci; Giuseppe Sindoni; Paolo Merialdo,The paper describes the ARANEUS Wel-Base Management System [l; 5; 4; 61; a systemdeveloped at Universitb di Roma Tre; which represents a proposal towards the definition ofa new kind of data-repository; designed to manage Web data in the database style. We calla Web-Base a collection of data of heterogeneous nature; and more specifically:(i) highlystructured data; such as the ones typically stored in relational or objectoriented databasesystems;(G) semistructured data; in the Web style. We can simplify by saying that itincorporates both databases and Web sites. A Web-Base Management System (WBMS) is asystem for managing such Web-bases. More specifically; it should provide functionalities forboth database and Web site management. It is natural to think of it as an evolution ofordinary DBMSs; in the sense that it will play in future generation Web-based Information …,ACM SIGMOD Record,1998,162
Clustering web pages based on their structure,Valter Crescenzi; Paolo Merialdo; Paolo Missier,Abstract Several techniques have been recently proposed to automatically generate Webwrappers; ie; programs that extract data from HTML pages; and transform them into a morestructured format; typically in XML. These techniques automatically induce a wrapper from aset of sample pages that share a common HTML template. An open issue; however; is howto collect suitable classes of sample pages to feed the wrapper inducer. Presently; the pagesare chosen manually. In this paper; we tackle the problem of automatically discovering themain classes of pages offered by a site by exploring only a small yet representative portionof it. We propose a model to describe abstract structural features of HTML pages. Based onthis model; we have developed an algorithm that accepts the URL of an entry point to atarget Web site; visits a limited yet representative number of pages; and produces an …,Data & Knowledge Engineering,2005,103
Design and development of data-intensive web sites: The Araneus approach,Paolo Merialdo; Paolo Atzeni; Giansalvatore Mecca,Abstract Data-intensive Web sites are large sites based on a back-end database; with afairly complex hypertext structure. The paper develops two main contributions:(a) a specificdesign methodology for data-intensive Web sites; composed of a set of steps and designtransformations that lead from a conceptual specification of the domain of interest to theactual implementation of the site;(b) a tool called H omer; conceived to support the sitedesign and implementation process; by allowing the designer to move through the varioussteps of the methodology; and to automate the generation of the code needed to implementthe actual site. Our approach to site design is based on a clear separation between severaldesign activities; namely database design; hypertext design; and presentation design. Allthese activities are carried on by using high-level models; all subsumed by an extension …,ACM Transactions on Internet Technology (TOIT),2003,93
RoadRunner: automatic data extraction from data-intensive web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Data extraction from HTML pages is performed by software modules; usually calledwrappers. Roughly speaking; a wrapper identifies and extracts relevant pieces of text insidea web page; and reorganizes them in a more structured format. In the literature there is anumber of systems to (semi-) automatically generate wrappers for HTML pages [1]. We haverecently investigated for original approaches that aims at pushing further the level ofautomation of the wrapper generation process. Our main intuition is that; in a dataintensiveweb site; pages can be classified in a small number of classes; such that pages belonging tothe same class share a rather tight structure. Based on this observation; we have studied annovel technique; we call the matching technique [2]; that automatically generates a commonwrapper by exploiting similarities and differences among pages of the same class. In …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,89
Araneus in the Era of XML.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni,A large body of research has been recently motivated by the attempt to extend databasemanipulation techniques to data on the Web (see [13] for a survey). Most of these researchefforts–which range from the definition of Web query languages and the relatedoptimizations; to systems for Web site development and management; and to integrationtechniques–started before XML was introduced; and therefore have strived for a long time tohandle the highly heterogeneous nature of HTML pages. In the meanwhile; Web datasources have evolved from small; home-made collections of HTML pages into complexplatforms for distributed data access and application development; and XML promises toimpose itself as a more appropriate format for this new breed of Web sites. XML brings dataon the Web closer to databases; since; differently from HTML; it is based on a clean …,IEEE Data Eng. Bull.,1999,82
Probabilistic models to reconcile complex data from inaccurate data sources,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several techniques have been developed to extract and integrate data from websources. However; web data are inherently imprecise and uncertain. This paper addressesthe issue of characterizing the uncertainty of data extracted from a number of inaccuratesources. We develop a probabilistic model to compute a probability distribution for theextracted values; and the accuracy of the sources. Our model considers the presence ofsources that copy their contents from other sources; and manages the misleadingconsensus produced by copiers. We extend the models previously proposed in the literatureby working on several attributes at a time to better leverage all the available evidence. Wealso report the results of several experiments on both synthetic and real-life data to show theeffectiveness of the proposed approach.,International Conference on Advanced Information Systems Engineering,2010,60
The (Short) Araneus Guide to Web-Site Development.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni; Valter Crescenzi; Valter Crescenzi,Web-site development has recently imposed itself as a new and challenging databaseproblem. This has justi ed a number of research proposals coming from the database area(eg; 5; 9; 4; 16; 15]) for data management in Web sites; other relevant works in the eld haveinvestigated the extension of design methodologies to these sites; and their interaction withdevelopment tools 6; 10]. Indeed; the notion of a Web site has recently evolved from a small;home-made collection of HTML pages into a number of di erent forms; including rathercomplex and sophisticated information system. Given the large number and diversity of Websites; we nd useful to classify them in categories; according to their complexity in terms ofdata and applications (ie; services); as shown in Figure 1.,WebDB (Informal Proceedings),1999,53
Efficient queries over web views,Giansalvatore Mecca; Alberto O Mendelzon; Paolo Merialdo,Abstract Large web sites are becoming repositories of structured information that can benefitfrom being viewed and queried as relational databases. However; querying these viewsefficiently requires new techniques. Data usually resides at a remote site and is organizedas a set of related HTML documents; with network access being a primary cost factor inquery evaluation. This cost can be reduced by exploiting the redundancy often found in sitedesign. We use a simple data model; a subset of the Araneus data model; to describe thestructure of a web site. We augment the model with link and inclusion constraints thatcapture the redundancies in the site. We map relational views of a site to a navigationalalgebra and show how to use the constraints to rewrite algebraic expressions; reducing thenumber of network accesses.,International Conference on Extending Database Technology,1998,44
The ARANEUS Guide to Web-Site Development.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni; Valter Crescenzi,Abstract Web sites are rapidly becoming a world-wide standard platform for informationsystem development. The paper reports on the work conducted in the last few years in theframework of the Araneus project in the eld; presenting models; tools; methodologies;techniques for Web design and development; as well as ideas coming from a number ofconcrete experiences in developing data-intensive Web sites using the system. We alsodiscuss research directions we are following to de ne a uni ed framework for data andapplication management on the Web.,SEBD,1999,42
Data-intensive web sites: design and maintenance,Paolo Atzeni; Paolo Merialdo; Giansalvatore Mecca,Abstract A methodology for designing and maintaining data–intensive Web sites isintroduced. Leveraging on ideas well established in the database field; the approach heavilyrelies on the use of models for the description of Web sites. The design process is composedof two intertwined activities: database design and hypertext design. Each of these is furtherdivided in a conceptual phase and a logical phase; based on specific data models. Themethodology strongly supports site maintenance: in fact; the various models provide aconcise description of the site structure; they allow to reason about the overall organizationof pages in the site and possibly to restructure it.,World Wide Web,2001,40
Automatic Web Information Extraction in the Road R unner System,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract This paper presents Road Runner; a research project that aims at developingsolutions for automatically extracting data from large HTML data sources. The target of ourresearch are data-intensive Web sites; ie; HTML-based sites with a fairly complex structure;that publish large amounts of data. The paper describes the top-level software architectureof the Road Runner System; and the novel research challenges posed by the attempt toautomate the information extraction process.,International Conference on Conceptual Modeling,2001,38
Wrapping-oriented classification of web pages,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Abstract Data extraction from HTML Web pages is performed by software programs calledwrapper. Writing wrappers is a costly and labor intensive task; recently several proposalhave attacked the problem of automatically generating wrappers. In this paper; we study aproblem related to the automation of the wrapping generation process: given a portion of aWeb site to wrap; we develop techniques to cluster its HTML pages into page classes withhomogeneous organization and layout; these classes can become the input to the wrappergeneration process. Also; once a wrapper library has been generated for a bunch of Websites; our techniques can be used in order to select; for any new page downloaded fromthese site; the right wrapper in the library. Based on the proposed techniques we havedeveloped a software prototype; and conducted several experiments on HTML pages …,Proceedings of the 2002 ACM symposium on Applied computing,2002,37
From Databases to Web-Bases:...,G Mecca; P Atzeni; P Merialdo; A Masci; G Sindoni,Abstract The ARANEUS project aims at developing tools for data-management on the WorldWide Web. Web-based information systems deal with data of heterogeneous nature; mainlydatabase data and HTML documents. We have implemented a system; called a Web-baseManagement system; for managing such repositories. The system is designed to supportseveral classes of applications:(i) high-level access to data in the Web;(ii) design;implementation and maintenance of Web sites;(iii) cooperative applications on the Web. Wediscuss the lessons learned from our experiences with the system; ranging from database-style query interfaces to popular Web sites; to the design and implementation of severalsites; among which an integrated Web museum; which correlates data coming from severalvirtual museums on the Web.,In Technical Report 34-1998. Dipartimento,1998,37
Web site evaluation: Methodology and case study,Paolo Atzeni; Paolo Merialdo; Giuseppe Sindoni,Abstract This paper presents a methodology for the assessment of Web site quality with aparticular emphasis public sector Web sites. In order to rate a site; a hierarchical model isproposed; which comprises several quality attributes. The methodology is designed for useby independent analysts who may have no knowledge of the technology underlying the sitenor any contact with the site managers. We used the methodology to assess the quality ofseveral Web sites of the Italian Public Administration. The results of the analysis arepresented as a case study.,International Conference on Conceptual Modeling,2001,29
Myocardial contrast enhancement after intravenous injection of sonicated albumin microbubbles: a transesophageal echocardiography dipyridamole study,Paolo Voci; Federico Bilotta; Paolo Merialdo; Luciano Agati,Myocardial opacification after intravenous injection of an echo-contrast agent is a major endpoint in contrast echocardiography; but it has not yet been obtained in human beings. Wepropose transesophageal contrast echocardiography as a clinical tool for the study ofmyocardial perfusion in human beings. Sonicated albumin microbubbles are brightultrasound reflectors that cross the pulmonary vasculature after intravenous injection andshow physiologic transit times through tissues. Transesophageal echocardiography usesideal transducer frequency and acoustic window for in vivo detection of sonicated albuminmicrobubbles. We have studied 11 patients receiving peripheral vein bolus injection ofsonicated albumin microbubbles during transesophageal echocardiography at baseline andduring dipyridamole infusion. Images were recorded on videotape and digitized off line …,Journal of the American Society of Echocardiography,1994,29
A framework for learning web wrappers from the crowd,Valter Crescenzi; Paolo Merialdo; Disheng Qiu,Abstract The development of solutions to scale the extraction of data from Web sources isstill a challenging issue. High accuracy can be achieved by supervised approaches but thecosts of training data; ie; annotations over a set of sample pages; limit their scalability.Crowd sourcing platforms are making the manual annotation process more affordable.However; the tasks demanded to these platforms should be extremely simple; to beperformed by non-expert people; and their number should be minimized; to contain thecosts. We introduce a framework to support a supervised wrapper inference system withtraining data generated by the crowd. Training data are labeled values generated by meansof membership queries; the simplest form of queries; posed to the crowd. We show that thecosts of producing the training data are strongly affected by the expressiveness of the …,Proceedings of the 22nd international conference on World Wide Web,2013,27
Extraction and integration of partially overlapping web sources,Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract We present an unsupervised approach for harvesting the data exposed by a set ofstructured and partially overlapping data-intensive web sources. Our proposal comes withina formal framework tackling two problems: the data extraction problem; to generateextraction rules based on the input websites; and the data integration problem; to integratethe extracted data in a unified schema. We introduce an original algorithm; WEIR; to solvethe stated problems and formally prove its correctness. WEIR leverages the overlappingdata among sources to make better decisions both in the data extraction (by pruning rulesthat do not lead to redundant information) and in the data integration (by reflecting localproperties of a source over the mediated schema). Along the way; we characterize theamount of redundancy needed by our algorithm to produce a solution; and present …,Proceedings of the VLDB Endowment,2013,26
Wrapper inference for ambiguous web pages,Valter Crescenzi; Paolo Merialdo,Several studies have concentrated on the generation of wrappers for web data sources. Aswrappers can be easily described as grammars; the grammatical inference heritage couldplay a significant role in this research field. Recent results have identified a new subclass ofregular languages; called prefix mark-up languages; that nicely abstract the structuresusually found in HTML pages of large web sites. This class has been proven to beidentifiable in the limit; and a PTIME unsupervised learning algorithm has been previouslydeveloped. Unfortunately; many real-life web pages do not fall in this class of languages. Inthis article we analyze the roots of the problem and we propose a technique to transformpages in order to bring them into the class of prefix mark-up languages. In this way; we havea practical solution without renouncing to the formal background defined within the …,Applied Artificial Intelligence,2008,21
A cooperative methodology to build conceptual models in medicine,Elena Galeazzi; Angelo Rossi Mori; Fabrizio Consorti; Anna Errera; Paolo Merialdo,Abstract We designed a methodology to perform distribute activities on conceptualmodelling among cooperating centers. Our methodology assigns responsibilities and tasksand regulates interactions preserving coherence; it passes through the construction ofunambiguous paraphrases to make explicit the context within the original sources; andthrough their compositional representation in an intermediate language. The process isintrinsically iterative; with continuous feedbacks and refinements; alternating analytic viewon details and synthetic view on regularities and structures. Our methodology is based onrequirements and experience made in the first GALEN project; and was applied in theGALEN-IN-USE project to coordinate modelling activities of three teams of surgeons inRome with activities of other partners; during the production of an extensive model of …,Studies in health technology and informatics,1997,21
Crawling programs for wrapper-based applications,Claudio Bertoli; Valter Crescenzi; Paolo Merialdo,Many large web sites provide pages containing highly valuable data. In order to extract datafrom these pages several methods and techniques have been developed to generate webwrappers; that is; programs that convert into a structured format the data embedded intoHTML pages. These techniques easy the burden of writing applications that make reuse ofdata from the web. However the generation of wrappers is just one of the ingredientsneeded to the development of such applications. A necessary yet underestimated task is thatof developing programs for driving a crawler towards the pages that contain the target data.We present a method and an associated tool to support this activity. Our method relies on adata model whose constructs allows a designer to define an intensional description of theorganization of data in a web site. Based on the model; we introduce the concepts of (i) …,Information Reuse and Integration; 2008. IRI 2008. IEEE International Conference on,2008,20
Efficiently Locating Collections of Web Pages to Wrap.,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo,Abstract: Many large web sites contain highly valuable information. Their pages aredynamically generated by scripts which retrieve data from a back-end database and embedthem into HTML templates. Based on this observation several techniques have beendeveloped to automatically extract data from a set of structurally homogeneous pages.These tools represent a step towards the automatic extraction of data from large web sites;but currently their input sample pages have to be manually collected. To scale the dataextraction process this task should be automated; as well. We present techniques toautomatically gathering structurally similar pages from large web sites. We have developedan algorithm that takes as input one sample page; and crawls the site to find pages similar instructure to the given page. The collected pages can feed an automatic wrapper …,WEBIST,2005,20
An automatic data grabber for large web sites,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo; Paolo Missier,Abstract We demonstrate a system to automatically grab data from data intensive web sites.The system first infers a model that describes at the intensional level the web site as acollection of classes; each class represents a set of structurally homogeneous pages; and itis associated with a small set of representative pages. Based on the model a library ofwrappers; one per class; is then inferred; with the help an external wrapper generator. Themodel; together with the library of wrappers; can thus be used to navigate the site andextract the data.,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,20
Fine-grain web site structure discovery,Valter Crescenzi; Paolo Merialdo; Paolo Missier,Abstract Several techniques have been recently proposed to automatically derive webwrappers; ie; programs that extract data from HTML pages; and transform them into a morestructured format; typically in XML syntax. These techniques automatically induce a wrapperfrom a set of sample pages that share a common HTML template. An open issue; however;is how to collect suitable classes of sample pages to feed the wrapper inducer. Presently;the pages are chosen manually. In this paper; we tackle the problem of automaticallydiscovering the main classes of pages offered by a site by exploring only a small;representative; portion of it. The web site model we propose describes the structure of thesite as a graph whose nodes are classes of pages that share a common structure; andwhose edges represent links among instances of the page classes. Using this model; we …,Proceedings of the 5th ACM international workshop on Web information and data management,2003,20
Ulixes: Building relational views over the web,Paolo Atzeni; Alessandro Masci; Giansalvatore Mecca; Paolo Merialdo; Elena Tabet,We consider structured Web sites; those sites in which structures are so tight and regularthat we can assimilate the site; from the logical viewpoint; to a conventional database. Wehave argued that; with respect to structured Web servers; it is possible to apply ideas fromtraditional database techniques; specifically with respect to design; query; and update [2].Here we focus on the querying process; which consists in associating a scheme with aserver and then use this scheme to pose queries in a high level query language. Todescribe the scheme; we use a specific data model; called the ARANEUS Data Model(ADM). We say that ADM is a page oriented model; in the sense that the main construct ofthe model is that of page scheme; used to describe the structure of sets of homogeneouspages in the server. ADM schemes are then offered to the user; who can query them by …,icde,1997,18
Flint: Google-basing the web,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several Web sites deliver a large number of pages; each publishing data about oneinstance of some real world entity; such as an athlete; a stock quote; a book. Even though itis easy for a human reader to recognize these instances; current search engines areunaware of them. Technologies for the Semantic Web aim at achieving this goal; however;so far they have been of little help in this respect; as semantic publishing is very limited. Wehave developed a system; called Flint; for automatically searching; collecting and indexingWeb pages that publish data representing an instance of a certain conceptual entity. Flinttakes as input a small set of labeled sample pages: it automatically infers a description of theunderlying conceptual entity and then searches the Web for other pages containing datarepresenting the same entity. Flint automatically extracts data from the collected pages …,Proceedings of the 11th international conference on Extending database technology: Advances in database technology,2008,17
Supporting the automatic construction of entity aware search engines,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Several web sites deliver a large number of pages; each publishing data about oneinstance of some real world entity; such as an athlete; a stock quote; a book. Although it iseasy for a human reader to recognize these instances; current search engines are unawareof them. Technologies for the Semantic Web aim at achieving this goal; however; so far theyhave been of little help in this respect; as semantic publishing is very limited. We havedeveloped a method to automatically search on the web for pages that publish datarepresenting an instance of a certain conceptual entity. Our method takes as input a smallset of sample pages: it automatically infers a description of the underlying conceptual entityand then searches the web for other pages containing data representing the same entity. Wehave implemented our method in a system prototype; which has been used to conduct …,Proceedings of the 10th ACM workshop on Web information and data management,2008,16
Knowledge Base Augmentation using Tabular Data.,Yoones A Sekhavat; Francesco Di Paolo; Denilson Barbosa; Paolo Merialdo,ABSTRACT Large linked data repositories have been built by leveraging semi-structureddata in Wikipedia (eg; DBpedia) and through extracting information from natural languagetext (eg; YAGO). However; the Web contains many other vast sources of linked data; such asstructured HTML tables and spreadsheets. Often; the semantics in such tables is hidden;preventing one from extracting triples from them directly. This paper describes a probabilisticmethod that augments an existing knowledge base with facts from tabular data byleveraging a Web text corpus and natural language patterns associated with relations in theknowledge base. A preliminary evaluation shows high potential for this technique inaugmenting linked data repositories.,LDOW,2014,14
Managing Web-based data-database models and transformations,Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,The paper considers the Araneus data model which employs database techniques andwrappers to extract data from and generate Web sites. The project features a logical modelthat abstracts physical aspects of Web sites. Araneus provides high-level descriptions ofpages that let us both extract data from the Web and generate Web sites from databases.,IEEE Internet Computing,2002,13
Redundancy-driven web data extraction and integration,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A large number of web sites publish pages containing structured information aboutrecognizable concepts; but these data are only partially used by current applications.Although such information is spread across a myriad of sources; the web scale implies arelevant redundancy. We present a domain independent system that exploits theredundancy of information to automatically extract and integrate data from the Web. Oursolution concentrates on sources that provide structured data about multiple instances fromthe same conceptual domain; eg; financial data; product information. Our proposal is basedon an original approach that exploits the mutual dependency between the data extractionand the data integration tasks. Experiments on a sample of 175;000 pages confirm thefeasibility and quality of the approach.,Procceedings of the 13th International Workshop on the Web and Databases,2010,11
Structure and Semantics of Data-IntensiveWeb Pages: An Experimental Study on their Relationships.,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo,Abstract: In data-intensive web sites pages are generated by scripts that embed data from abackend database into HTML templates. There is usually a relationship between thesemantics of the data in a page and its corresponding template. For example; in a web siteabout sports events; it is likely that pages with data about athletes are associated with atemplate that differs from the template used to generate pages about coaches or referees.This article presents a method to classify web pages according to the associated template.Given a web page; the goal of our method is to accurately find the pages that are about thesame topic. Our method leverages on a simple; yet effective model to abstract somestructural features of a web page. We present the results of an extensive experimentalanalysis that show the performance of our methods in terms of both recall and precision …,J. UCS,2008,11
Structures in the Web.,Paolo Atzeni; Alessandro Masci; Giansalvatore Mecca; Paolo Merialdo; Elena Tabet,Abstract. The paper discusses the issue of views in the Web context. We introduce a set oftools and languages for managing and restructuring data coming from the World Wide Web.We present a specific data model; called the Araneus Data Model; inspired to the structurestypically present in Web sites. The model allows us to describe the scheme of a Webhypertext; in the spirit of databases. Based on the data model; we develop the Araneus Viewlanguage; to support a sophisticate view definition process; the language has two maincomponents: the first; called Ulixes; is used to build database views of the Web; which canthen be analyzed and integrated using database techniques; the second; called Penelope;allows the definition of derived Web hypertexts from relational views. This can be used togenerate hypertextual views over the Web.,SEBD,1997,11
Crowdsourcing large scale wrapper inference,Valter Crescenzi; Paolo Merialdo; Disheng Qiu,Abstract We present a crowdsourcing system for large-scale production of accuratewrappers to extract data from data-intensive websites. Our approach is based on supervisedwrapper inference algorithms which demand the burden of generating training data toworkers recruited on a crowdsourcing platform. Workers are paid for answering simplequeries carefully chosen by the system. We present two algorithms: a single workeralgorithm (alf _ η ALF η) and a multiple workers algorithm (alfred). Both the algorithms dealwith the inherent uncertainty of the workers' responses and use an active learning approachto select the most informative queries. alfred estimates the workers' error rate to decide atruntime how many workers should be recruited to achieve a quality target. The system hasbeen fully implemented and tested: the experimental evaluation conducted with both …,Distributed and Parallel Databases,2015,10
HOMER: a model-based CASE tool for data-intensive Web sites,Paolo Merialdo; Paolo Atzeni; Marco Magnante; Giansalvatore Mecca; Marco Pecorone,We present HOMER; a CASE tool for building and maintaining complex; data-intensive Websites. In HOMER the processes of creation and maintenance of a Web site are completelybased on the adoption of suitable models; to describe the various aspects of the site(content~ navigation structure; presentation). The development of a site does not require anycode writing activity: based on the results of the design process; the system automaticallycreates programs to implement the site; statically and/or dynamically; as needed; also; thesystem does not depend on any specific tool or language: it has a modular architecture;which integrates external servers for specific tasks; finally; the system supports siteadministrators for several maintenance activities; which can involve changes over the site atdifferent levels. The site content is described both at the conceptual level; using the ER …,ACM SIGMOD Record,2000,10
Do we really need a new query language for XML?,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni,XML [XML98] has been proposed as a successor of HTML conceived to ease the task ofexchanging; manipulating and reusing data on the Internet (and in other frameworks). Themain advantage of XML with respect to its predecessor is a clear separation between theways in which data; structure (DTD) and layout (stylesheet) are encoded. At the same time;XML preserves and enhances the hypertextual linking mechanism [XLink98; XPointer98]that has been so successful for HTML. It is therefore presumable that we will very soonwitness the emergence of XML repositories on the Web; ie; large collections of Internet-available linked XML documents.The announced availability of these large repositoriesimposes to develop suitable techniques for querying and transforming XML data. Recentproposals of query languages for XML--such as; for example; XML-QL [Deu+ 98] and XQL …,QL,1998,10
A second generation of terminological systems is coming.,A Rossi Mori; Fabrizio Consorti; Elena Galeazzi; Paolo Merialdo,Abstract Diverse achievements by recent computer-based terminological systems areoutlining a new generation of systems (ie a" second generation"). We collected the relevantfeatures of various advanced terminological systems and we systematized these featuresinto four components of a unique framework. We review a set of systems according to ourframework; and we discuss how standardization activities can support the evolution ofcomputer-based terminological systems towards a complete set of new performances.,*,1997,10
Exploiting information redundancy to wring out structured data from the web,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A large number of web sites publish pages containing structured information aboutrecognizable concepts; but these data are only partially used by current applications.Although such information is spread across a myriad of sources; the web scale implies arelevant redundancy. We present a domain independent system that exploits theredundancy of information to automatically extract and integrate data from the Web. Oursolution concentrates on sources that provide structured data about multiple instances fromthe same conceptual domain; eg financial data; product information. Our proposal is basedon an original approach that exploits the mutual dependency between the data extractionand the data integration tasks. Experiments confirmed the quality and the feasibility of theapproach.,Proceedings of the 19th international conference on World wide web,2010,9
NGS: a framework for multi-domain query answering,Daniele Braga; Diego Calvanese; Alessandro Campi; Stefano Ceri; Florian Daniel; Davide Martinenghi; Paolo Merialdo; Riccardo Torlone,If we consider a query involving multiple domains; such as" find all database conferencesheld within six months in locations whose seasonal average temperature is 28degC and forwhich a cheap travel solution exists"; we note that (i) general-purpose search engines fail toanswer multi-domain queries and (ii) specific search services may cover one of suchdomains; but no general integration framework is readily available. Currently; the only wayto treat such cases is to separately query dedicated services and feed the result of onesearch as input to another; or to pairwise compare them by hand. This paper presents NGS;a framework providing fully automated support for cross-domain queries. In particular; NGS(a) integrates different kinds of services (search engines; web services; and wrapped webpages) into a global ontology; ie; a unified view of the concepts supported by the …,Data Engineering Workshop; 2008. ICDEW 2008. IEEE 24th International Conference on,2008,9
Integration of territorial maps in the vision system of an autonomous land vehicle,Paolo Merialdo; Pier Carlo Pecollo; Carlo S Regazzoni; Gianni Vernazza; Rodolfo Zunino,*,Intelligent Autonomous Systems 2; An International Conference,1989,9
SMART: a system supporting medical activities in real-time.,Domenico M Pisanelli; Fabrizio Consorti; Paolo Merialdo,Abstract This paper describes the system SMART whose goal is real-time assistance tophysicians who execute diagnostic or therapeutic protocols in a clinical context. SMART isable to retrieve a protocol from its knowledge base and to monitor its execution step by stepfor a single patient. Different protocols for different patients can be followed at the same timein a health care structure. The prototype realized supports the execution of protocols forevaluating surgical risks. It has been implemented according to the specifications given bythe 4th Surgical Clinic of" Policlinico Umberto I" and reflects the activities actually performedin that hospital. However; the protocol model defined is general purpose and we envisagean easy application to other contexts and therefore to the informatization of other protocols.,Studies in health technology and informatics,1997,8
Wrapper Generation Supervised by a Noisy Crowd.,Valter Crescenzi; Paolo Merialdo; Disheng Qiu,ABSTRACT We present solutions based on crowdsourcing platforms to support large-scaleproduction of accurate wrappers around data-intensive websites. Our approach is based onsupervised wrapper induction algorithms which demand the burden of generating thetraining data to the workers of a crowdsourcing platform. Workers are paid for answeringsimple membership queries chosen by the system. We present two algorithms: a singleworker algorithm (alfη) and a multiple workers algorithm (alfred). Both the algorithms dealwith the inherent uncertainty of the responses and use an active learning approach to selectthe most informative queries. alfred estimates the workers' error rate to decide at runtimehow many workers are needed. The experiments that we conducted on real and syntheticdata are encouraging: our approach is able to produce accurate wrappers at a low cost …,DBCrowd,2013,7
Automatic evaluation of relation extraction systems on large-scale,Mirko Bronzi; Zhaochen Guo; Filipe Mesquita; Denilson Barbosa; Paolo Merialdo,Abstract The extraction of relations between named entities from natural language text is alongstanding challenge in information extraction; especially in large-scale. A majorchallenge for the advancement of this research field has been the lack of meaningfulevaluation frameworks based on realistic-sized corpora. In this paper we propose aframework for large-scale evaluation of relation extraction systems based on an automaticannotator that uses a public online database and a large web corpus.,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,2012,7
Web content extraction: a metaanalysis of its past and thoughts on its future,Tim Weninger; Rodrigo Palacios; Valter Crescenzi; Thomas Gottron; Paolo Merialdo,Abstract In this paper; we present a meta-analysis of several Web content extractionalgorithms; and make recommendations for the future of content extraction on the Web. First;we find that nearly all Web content extractors do not consider a very large; and growing;portion of modernWeb pages. Second; it is well understood that wrapper induction extractorstend to break as theWeb changes;; heuristic/feature engineering extractors were thought tobe immune to a Web site's evolution; but we find that this is not the case: heuristic contentextractor performance also tends to degrade over time due to the evolution of Web site formsand practices. We conclude with recommendations for future work that address these andother findings.,ACM SIGKDD Explorations Newsletter,2016,6
Web-scale extension of RDF knowledge bases from templated websites,Lorenz Bühmann; Ricardo Usbeck; Axel-Cyrille Ngonga Ngomo; Muhammad Saleem; Andreas Both; Valter Crescenzi; Paolo Merialdo; Disheng Qiu,Abstract Only a small fraction of the information on the Web is represented as Linked Data.This lack of coverage is partly due to the paradigms followed so far to extract Linked Data.While converting structured data to RDF is well supported by tools; most approaches toextract RDF from semi-structured data rely on extraction methods based on ad-hocsolutions. In this paper; we present a holistic and open-source framework for the extractionof RDF from templated websites. We discuss the architecture of the framework and the initialimplementation of each of its components. In particular; we present a novel wrapperinduction technique that does not require any human supervision to detect wrappers for websites. Our framework also includes a consistency layer with which the data extracted by thewrappers can be checked for logical consistency. We evaluate the initial version of REX …,International Semantic Web Conference,2014,6
The RoadRunner Project: Towards automatic extraction of web data,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,ROADRUNNER is a research project that aims at developing solutions for automaticallyextracting data from large HTML data sources. The target of our research are data-intensiveWeb sites; ie; HTML-based sites that publish large amounts of data in a fairly complexstructure. In our view; we aim at ideally seeing the data extraction process of a data-intensive Web site as a black-box taking as input the URL of an entry point to the site (eg thehome page); and returning as output data extracted from HTML pages in the site in astructured database-like format. This paper describes the top-level software architecture ofthe ROADRUNNER System; which has been specifically designed to automatize the dataextraction process. Several components of the system have already been implemented; andpreliminary experiments show the feasibility of our ideas. Data-intensive Web sites …,Proceedings of the International,2002,6
Automatically building probabilistic databases from the web,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract A relevant number of web sites publish structured data about recognizableconcepts (such as stock quotes; movies; restau-rants; etc.). There is a great chance to createapplications that rely on a huge amount of data taken from the Web. We present anautomatic and domain independent system that performs all the steps required to benefitfrom these data: it discovers data intensive web sites containing information about an entityof interest; extracts and integrate the published data; and finally performs a probabilisticanalysis to characterize the impreciseness of the data and the accuracy of the sources. Theresults of the processing can be used to populate a probabilistic database.,Proceedings of the 20th international conference companion on World wide web,2011,5
Alfred: Crowd assisted data extraction,Valter Crescenzi; Paolo Merialdo; Disheng Qiu,Abstract The development of solutions to scale the extraction of data from Web sources isstill a challenging issue. High accuracy can be achieved by supervised approaches; but thecosts of training data; ie; annotations over a set of sample pages; limit their scalability.Crowdsourcing platforms are making the manual annotation process more affordable.However; the tasks demanded to these platforms should be extremely simple; to beperformed by non-expert people; and their number should be minimized; to contain thecosts. We demonstrate ALFRED; a wrapper inference system supervised by the workers of acrowdsourcing platform. Training data are labeled values generated by means ofmembership queries; the simplest form of queries; posed to the crowd. ALFRED includesseveral original features: it automatically selects a representative sample set from the …,Proceedings of the 22nd International Conference on World Wide Web,2013,4
Minimizing the Costs of the Training Data for Learning Web Wrappers.,Rolando Creo; Valter Crescenzi; Disheng Qiu; Paolo Merialdo,ABSTRACT Data extraction from the Web represents an important issue. Severalapproaches have been developed to bring the wrapper generation process at the webscale. Although they rely on different techniques and formalisms; they all learn a wrappergiven a set of sample pages. Unsupervised approaches require just a set of sample pages;supervised ones also need training data. Unfortunately; the accuracy obtained byunsupervised techniques is not sufficient for many applications. On the other hand; obtainingtraining data is not cheap at the web scale. This paper addresses the issue of minimizing thecosts of collecting training data for learning web wrappers. We show that two interleavedproblems affect this issue: the choice of the sample pages; and the expressiveness of thewrapper language. We propose a solution that leverages contributions in the field of …,VLDS,2012,4
Web data reconciliation: Models and experiences,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract An increasing number of web sites offer structured information about recognizableconcepts; relevant to many application domains; such as finance; sport; commercialproducts. However; web data is inherently imprecise and uncertain; and conflicting valuescan be provided by different web sources. Characterizing the uncertainty of web datarepresents an important issue and several models have been recently proposed in theliterature. This chapter illustrates state-of-the-art Bayesan models to evaluate the quality ofdata extracted from the Web and reports the results of an extensive application of the modelson real life web data. Experimental results show that for some applications even simpleapproaches can provide effective results; while sophisticated solutions are needed to obtaina more precise characterization of the uncertainty.,*,2012,4
Wrapper generation for overlapping web sources,Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Exploiting the huge amount of data available on the Web involves the generation ofwrappers to extract data from web pages. We argue that existing approaches for web dataextraction from data-intensive websites miss the opportunities related to the presence ofredundant information on the Web. We propose an innovative approach that aims at pushingfurther the level of automation of existing wrapper generation systems by leveraging theredundancy of data on the Web. An experimental evaluation of the proposed solution showsa relevant improvement for the precision of the extracted data; without a significant loss inthe recall.,Web Intelligence and Intelligent Agent Technology (WI-IAT); 2011 IEEE/WIC/ACM International Conference on,2011,4
Speaking Words of WISDOM: Web Intelligent Search based on DOMain ontologies.,Sonia Bergamaschi; Paolo Bouquet; Paolo Ciaccia; Paolo Merialdo,Abstract. in this paper we present the architecture of a system for searching and queryinginformation sources available on the web which was developed as part of a project calledWISDOM. key feature of our proposal is a distributed architecture based on (i) the peer-to-peer paradigm and (ii) the adoption of domain ontologies. at the lower level; we support astrong; ontology-based integration of the information content of a bunch of source peers;which form a so-called semantic peer. at the upper level; we provide a loose; mapping-based integration of a set of semantic peers. we then show how queries can be efficientlymanaged and distributed in such a two-layer scenario. 1 introduction the WISDOM (WebIntelligent Search based on DOMain ontologies) 1 project aims at studying; developing andexperimenting methods and techniques for searching and querying information sources …,SWAP,2005,4
Improving the expressiveness of ROADRUNNER.,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,*,SEBD,2004,4
A logical model for metadata in Web bases,P Atzeni; G Mecca; P Merialdo; G Sindoni,Abstract In systems that provide integrated management of both structured; database style;data and semistructured; Web-style; data; metadata may allow for the explicit managementof information about the structure and data content of pages. Unfortunately; the HTMLstandard doesn't offer any explicit framework for document metadata management. TheeXtensible Markup Language vice versa allows for the explicit description of the structure ofa document; but it lacks of database perspective. The aim of this paper is then to describe anapproach to the generation of database derived Web meta information that is based on alogical model for Web pages and to show how this approach can be effectively used for bothHTML and XML based systems.,ERCIM Workshop on metadata for Web databases,1998,4
A conceptual representation of clinical and managerial guidelines: the ATREUS workflow model.,Patrizia Grifoni; Daniela Luzi; Paolo Merialdo; Fabrizio L Ricci,Abstract In this paper we propose a workflow conceptual model able to represent clinicaland managerial activities within healthcare structures; the ATREUS model. This model uses:a) a graphical representation which models the activities and the events that activate them;b) a textual representation of information related to: a set of conditions used for the control ofactivity execution; the actors who undertake the activity; the resources and tools necessaryfor its enactment; the clinical and managerial data generated by the activity execution; c) astate diagram which allows the control of the activity execution. The model allowsmodularity; activity nesting and temporal flexibility using a top-down refinement ofprocesses. This model; unlike others; makes it possible to highlight the different types ofdecision involved in the performance of an activity.,Studies in health technology and informatics,1998,4
ATREUS: A model for the conceptual representation of a workflow,Patrizia Grifoni; Daniela Luzi; Paolo Merialdo; Fabrizio L Ricci,We propose a workflow conceptual model able to represent clinical and managerialactivities within health-care structures; the ATREUS model. This model was defined takinginto account the intrinsic difficulties and complexity of processes in the health-care domain.This model uses: hypergraphs to represent processes graphically; a textual representationwhich describes each activity; and a state diagram to control the activity development. Themodel allows a top-down refinement of processes.,Database and Expert Systems Applications; 1997. Proceedings.; Eighth International Workshop on,1997,4
Characterizing the uncertainty of web data: models and experiences,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract An increasing number of web sites offer structured information about recognizableconcepts; relevant to many application domains; such as finance; sport; commercialproducts. However; web data is inherently imprecise and uncertain; and conflicting valuescan be provided by different web sources. Characterizing the uncertainty of web datarepresents an important issue and several models have been recently proposed in theliterature. The paper illustrates state-of-the-art Bayesan models to evaluate the quality ofdata extracted from the Web and reports the results of an extensive application of the modelson real life web data. Our experimental results show that for some applications even simpleapproaches can provide effective results; while sophisticated solutions are needed to obtaina more precise characterization of the uncertainty.,Proceedings of the 2011 Joint WICOW/AIRWeb Workshop on Web Quality,2011,3
Efficient techniques for effective wrapper induction,Valter Crescenzi; Paolo Merialdo,Several studies have recently concentrated on the generation of wrappers for extracting datafrom Web data sources. The ROADRUNNER system aims at automating the tedious andexpensive process of writing wrappers in an unsupervised; domain-independent; andscalable manner. The system is based on a grammar inference algorithm; called MATCH;which has been designed in a sound theoretical framework. However; in its originaldefinition MATCH lacks in expressivity; that is; in many cases when MATCH runs over real-life Web pages; it is not able to produce a solution. In this paper we address the challengingissue of developing techniques that allow us to build upon MATCH an effective and efficientsystem; without renouncing to the original formal background. First; we analyze the mainlimitations of MATCH; then we illustrate the techniques we have developed to overcome …,Data Engineering Workshops; 2006. Proceedings. 22nd International Conference on,2006,3
Handling irregularities in roadrunner,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,*,ATEM-2004: The AAAI-04 Workshop on Adaptive Text Extraction and Mining; San Jose; CA,2004,3
Clinical protocol development using Inter/IntraNet technology: the FENARETE system.,A Errera; P Merialdo; A Orsano; G Sindoni; G Rumolo,Abstract In this work we present FENARETE; a software tool to design and distribute clinicalprotocols in an Inter/IntraNet framework. We consider a medical protocol as a clinicalbehaviour scheme; formally and clearly defined with sufficient details. Our work allows theknowledge content of any clinical protocol to be fully represented in a symbolic style. Acomputer based support tool that works as an interface between clinicians and the protocolknowledge base is regarded by the authors as a basic building block developing anintegrated environment for medical protocols design and management. The FENARETEapplication has been developed in Java and it is available for any Internet-linked machinewith a Java-compatible browser.,Studies in health technology and informatics,1997,3
A Method to define and Design Tools for Hypermedia Medical Reports Management,F Consorti; J Di Prospero; P Merialdo; G Sindoni,Abstract In this paper methods and tools for assisting physicians in creating and organizinglarge collections of hypermedia reports arc described. A method to automatically create ahypermedia network representing a collection of reports is defined. The design of ahypermedia report management system has been based on these concepts; and themethods; architecture and features of the system are illustrated. The medical report ismodeled as a hypermedia structured document. In order to improve report compiling andreading facilities; graphical models libraries of the investigated anatomical structures areintegrated in the hypermedia environment. In the system; the report collection is ahypermedia network offering an interesting approach to multimedia documents access.,STUDIES IN HEALTH TECHNOLOGY AND INFORMATICS,1996,3
The RoadRunner Web Data Extraction System.,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Extracting data from HTML text files and making them available to computer applications isbecoming of utmost importance for developing several emerging e-services. This paperpresents RoadRunner; a research project that aims at developing solutions for automaticallyextracting data from large HTML data sources. We concentrate on data-intensive Web sites;that is; sites that deliver large amounts of data through a complex graph of linked HTMLpages. The paper describes the top-level software architecture of the RoadRunner System;which has been specifically designed to automatize the data extraction process. The paperis organized as follows. First; Section 2 illustrates an overview of the project and gives anintuition of its key ideas. Then; Section 3 describes the overall architecture of theRoadRunner system. Section 4 concludes the paper discussing related works.,SEBD,2001,2
To weave the web,P Merialdo; P Atzeni; G Mecca,*,VLDB,1997,2
Browsing and querying multimedia report collections.,F Consorti; P Merialdo; G Sindoni,Abstract In this paper a system and new methodologies that enable efficient exploration ofdistributed collections of multimedia reports are described. A conceptual model for the reportand a method to semi-automatically create a hypermedia report network have been defined.The main issues addressed in this project were to exploit the textual component of a reportto give a more evident semantic meaning to the data produced during the relative exam andto provide users with new interaction paradigms based on Internet technologies.,Studies in health technology and informatics,1997,2
Crowdsourcing for data management,Valter Crescenzi; Alvaro AA Fernandes; Paolo Merialdo; Norman W Paton,Abstract Crowdsourcing provides access to a pool of human workers who can contributesolutions to tasks that are challenging for computers. Proposals have been made for the useof crowdsourcing in a wide range of data management tasks; including data gathering;query processing; data integration; and cleaning. We provide a classification of key featuresof these proposals and survey results to date; identifying recurring themes and open issues.,*,2017,1
In codice ratio: Scalable transcription of historical handwritten documents,Serena Ammirati; Donatella Firmani; Marco Maiorino; Paolo Merialdo; Elena Nieddu; Andrea Rossi,Abstract. Huge amounts of handwritten historical documents are being published by digitallibraries world wide. However; for these raw digital images to be really useful; they need tobe annotated with informative content. State-of-the-art Handwritten Text Recognition (HTR)approaches require an impressive training effort by expert paleographers. Our contributionis a scalable; end-to-end transcription work-flow–that we call In Codice Ratio–based on fine-grain segmentation of text elements into characters and symbols; with limited training effort.We provide a preliminary evaluation of In Codice Ratio over a corpus of letters by popeHonorii III; stored in the Vatican Secret Archive.,25th Italian Symposium on Advanced Database Systems (SEBD),2017,1
Accurate fact harvesting from natural language text in wikipedia with Lector,Matteo Cannaviccio; Denilson Barbosa; Paolo Merialdo,Abstract Many approaches have been introduced recently to automatically create oraugment Knowledge Graphs (KGs) with facts extracted from Wikipedia; particularly itsstructured components like the infoboxes. Although these structures are valuable; theyrepresent only a fraction of the actual information expressed in the articles. In this work; wequantify the number of highly accurate facts that can be harvested with high precision fromthe text of Wikipedia articles using information extraction techniques bootstrapped from theentities and relations already in a KG. Our experimental evaluation; which uses Freebase asreference KG; reveals we can augment several relations in the domain of people by morethan 10%; with facts whose accuracy are over 95%. Moreover; the vast majority of thesefacts are missing from the infoboxes; YAGO and DBpedia.,Proceedings of the 19th International Workshop on Web and Databases,2016,1
Contextual data extraction and instancebased integration,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract We propose a formal framework for an unsupervised approach tacking at the sametime two problems: The data extraction problem; for generating the extraction rules neededto gain data from web pages; and the data integration problem; to integrate the data comingfrom several sources. We motivate the approach by discussing its advantages with regard tothe traditional" waterfall approach"; in which data are wholly extracted before the integrationstarts without any mutual dependency between the two tasks. In this paper; we focus on datathat are exposed by structured and redundant web sources. We introduce novel polynomialalgorithms to solve the stated problems and present theoretical results on the properties ofthe solution generated by our approach. Finally; a preliminary experimental evaluation showthe applicability of our model with real-world websites.,Unknown Journal,2011,1
Data extraction and integration from imprecise web sources,Lorenzo Blanco; Mirko Bronzi; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Blanco; L; Bronzi; M; Crescenzi; V; Merialdo; P & Papotti; P 2009; Data extraction and integrationfrom imprecise web sources (Extended abstract). in 17th Italian Symposium on Advanced DatabaseSystems; SEBD 2009. pp. 229-236; 17th Italian Symposium on Advanced DatabaseSystems; SEBD 2009; Camogli; Genova; Italy; 6/21/09 … Blanco L; Bronzi M; Crescenzi V; MerialdoP; Papotti P. Data extraction and integration from imprecise web sources (Extendedabstract). In 17th Italian Symposium on Advanced Database Systems; SEBD 2009. 2009. p.229-236 … Powered by Pure; Scopus & Elsevier Fingerprint Engine™ © 2018 Elsevier BV.,17th Italian Symposium on Advanced Database Systems; SEBD 2009,2009,1
Harvesting Structurally Similar Pages.,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo,*,SEBD,2005,1
Managing web-based data [Z],Paolo Atzeni; Ginaselvatore Mecca; Paolo Merialdo,*,IEEE Internet Computing,2002,1
Back to Gold's Age: Bridging the Gap Between Traditional Grammar Inference and Web Information Extraction.,Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo,Since Gold's Theorem (1967); grammar inference for regular languages has been athoroughly studied topic; with an elegant theoretical background and well establishedtechniques. One of the main contributions of these works is the study of properties of thoselanguages for which the inference process can be performed in a completely automatic way;and of the relative algorithms. Recently; information extraction from Web sites has imposeditself as a relevant research field for the so called “Semantic Web”. Since extraction isperformed by wrappers; which are essentially grammar parsers for the HTML code of Webpages; grammar inference could in principle play a fundamental role in this field. However;despite 30 years of research; due to some limitations of the traditional framework; essentiallynone of the recent approaches to Web information extraction reuse theories and …,SEBD,2002,1
In Codice Ratio: Scalable Transcription of Vatican Registers,Donatella Firmani; Paolo Merialdo; Marco Maiorino,*,ERCIM NEWS,2017,*
It has become widely recognized that user feedback can play a fundamental role in facilitating information integration tasks; eg; the construction of integration schem...,Haoran Xu; Dandan Zhou; Yuqing Sun; Haiqi Sun; Reynold Cheng; Silviu Maniu; Pierre Senellart; Valter Crescenzi; Paolo Merialdo; Disheng Qiu; Vincenzo Della Mea; Eddy Maddalena; Stefano Mizzaro,Social tags take an important role in exploratory search. In collaborative tagging systems;users are allowed to annotate resources with tags. The significant challenges in suchsystems are the uncertainty of tag quality and the incomplete annotation on a large numberof resources. Based on the observation that these problems can be statistically negligibleafter receiving sufficient tags; we propose...,Distributed and Parallel Databases,2015,*
The Startup Ecosystem: a Quick Tour.,Paolo Merialdo,*,SEBD,2015,*
Web-Scale Extension of RDF Knowledge Bases from Templated Websites,Ricardo Usbeck12; Lorenz Bühmann; Axel-Cyrille Ngonga; Muhammad Saleem Ngomo; Andreas Both; Valter Crescenzi; Paolo Merialdo; Disheng Qiu,*,*,2014,*
Flint: From Web Pages to Probabilistic Semantic Data,Paolo Merialdo; Paolo Papotti,Abstract A large and increasing number of web sites publish structured data aboutrecognizable concepts (such as stock quotes; movies; restaurants). The great chance tocreate applications that rely on the huge amount of data taken from these sites has beendiscussed for more than a decade now; but in practice; only a small fraction of suchinformation is currently used. The main reason is that extracting and integrating web data ofgood quality is an expensive task; which often requires human intervention. In this chapter;we present the main results of the Flint project; which aims at developing automatic anddomain-independent tools to perform all the steps required to benefit from Web data:discovering data-intensive web sites containing information about entities of interest;extracting and integrating the published data; and performing a probabilistic analysis to …,*,2012,*
Probabilistic reconciliation of records from inaccurate web sources,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Abstract Web data are inherently imprecise and uncertain. This paper addresses the issue ofcharacterizing the uncertainty of data extracted from a number of inaccurate sources. Wedevelop a probabilistic model to compute a probability distribution for the extracted values;and the accuracy of the sources. Our model considers the presence of sources that copytheir contents from other sources; and manages the misleading consensus produced bycopiers. We extend the models previously proposed in the literature by working on severalattributes at a time to better leverage all the available evidence of copying.,18th Italian Symposium on Advanced Database Systems; SEBD 2010,2010,*
Searching entities on the web by sample,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,Blanco; L; Crescenzi; V; Merialdo; P & Papotti; P 2008; Searching entities on the web bysample. in SEBD 2008 - Proceedings of the 16th Italian Symposium on Advanced DatabaseSystems. pp. 406-413; 16th Italian Symposium on Advanced Database Systems; SEBD2008; Mondello; Palermo; Italy; 6/22/08 … Blanco L; Crescenzi V; Merialdo P; Papotti P. Searchingentities on the web by sample. In SEBD 2008 - Proceedings of the 16th Italian Symposium onAdvanced Database Systems. 2008. p. 406-413 … Powered by Pure; Scopus & Elsevier FingerprintEngine™ © 2017 Elsevier BV.,16th Italian Symposium on Advanced Database Systems; SEBD 2008,2008,*
NGS: A New Generation Search Engine Supporting Cross Domain Queries,Daniele Braga; Diego Calvanese; Alessandro Campi; Stefano Ceri; Florian Daniel; Davide Martinenghi; Paolo Merialdo; Riccardo Torlone,Abstract. This paper presents NGS; a framework providing fully automated support for cross-domain queries. In particular; NGS (a) integrates different kinds of services (search engines;web services; and wrapped web pages) into a global ontology; ie; a unified view of theconcepts supported by the available services;(b) covers query formulation aspects over theglobal ontology; and query rewriting in terms of the actual services; and (c) offers severaloptimization opportunities leveraging the characteristics of the different services at hand;based on several different cost metrics.,Proc. of the 16th Italian Conf. on Database Systems (SEBD 2008),2008,*
Prototype for the automatic inference of the schema of a data intensive web site,L Arlotta; L Blanco; V Crescenzi; D Lucia; F Martire; P Merialdo; A Pace; V Schiavoni,Abstract Questo rapporto descrive i prototipi Indesit e Alldesit per l'inferenza di uno schemadi siti Web. Indesit prende in input l'url di una pagina web e automaticamente costruisce unoschema locale del sito; attraverso lo schema vengono quindi raggiunte e scaricate tutte lepagine del sito simili in struttura alla pagina di input. Alldesit prende in input la home page diun sito quindi opera un'analisi delle pagine del sito al fine di costruire automaticamente unoschema del sito; minimizzando il numero di pagine scaricate. Lo schema del sito prodotto daAlldesit descrive il sito in termini di classi di pagine simili in struttura.,*,2006,*
Rapporto sulla architettura metodologica e funzionale di riferimento,S Bergamaschi; D Beneventano; L Cabibbo; P Ciaccia; V Crescenzi; P Merialdo,Abstract L'enorme quantita di dati e la crescente disponibilita di servizi presenti su Webrendono sempre piu importante lo sviluppo di infrastrutture e sistemi software chepermettano ai clienti collegati alla rete di “ricaricarsi” di dati di interesse per i propri bisogniinformativi. Il progetto Wisdomha come obiettivo principale la definizione di tecniche; estrumenti per la ricerca; la localizzazione e la fruizione personalizzata di risorse informativedisponibili su Web. Le soluzioni proposte sono basate su ontologie di dominio; e su unaarchitettura distribuita e decentralizzata. Obiettivo di questo documentoe la definizione diuna architettura metodologica e funzionale di riferimento al fine di garantire coerenza tra lesoluzioni che verranno studiate dalle varie unita di ricerca che partecipano al progetto.,*,2005,*
Experiences in XML data management.,Giansalvatore Mecca; Paolo Merialdo; Paolo Atzeni; Valter Crescenzi,A large body of research has been recently motivated by the attempt to extend databasemanipulation techniques to data on the Web (see [16] for a survey). Most of these researchefforts–which range from the definition of Web query languages and the relatedoptimizations; to systems for Web site development and management; and to integrationtechniques–started before XML was introduced; and therefore have strived for a long time tohandle the highly heterogeneous nature of HTML pages. In the meanwhile; Web datasources have evolved from small; home-made collections of HTML pages into complexplatforms for distributed data access and application development; and XML promises toimpose itself as a more appropriate format for this new breed of Web sites. XML brings dataon the Web closer to databases; since; differently from HTML; it is based on a clean …,SEBD,2000,*
Design and Maintenance of Web-Based Information Systems,Paolo Merialdo,4.1 Deriving Macroentities from leave nodes of is-a hierarchies.. 46 4.2 DerivingMacroentities from the root node of is-a hierarchies. 47 4.3 Entity PHYSICIAN participatestwo is-a hierarchies........ 48 4.4 Mapping entity PHYSICIAN of Figure 4.3 in a macroentitywith two roles............................... 48,*,1998,*
The Araneus Project: Extending Database Techniques to the World Wide Web.,Giansalvatore Mecca; Paolo Merialdo; Alessandro Masci; Giuseppe Sindoni,Abstract. The Araneus project aims at developing tools for datamanagement on the WorldWide Web. We have implemented a system; called a Web-base Management system; formanaging Web data. The system is designed to support several classes of applications:(i)highlevel access to data in the Web;(ii) design; implementation and maintenance of Websites;(iii) cooperative applications on the Web. We discuss the lessons learned from ourexperiences with the system; ranging from database-style query interfaces to popular Websites; to the design and implementation of several sites; among which an integrated Webmuseum; which correlates data coming from several virtual museums on the Web.,SEBD,1998,*
Automated recognition of cardiac structures in echocontrast perfusion studies,Paolo Merialdo; Federico Bilotta; Laura Fiorani; Giovanni Ettore Gigante,Myocardial perfusion information is crucial in the clinical evaluation of patients with coronaryartery disease. A software tool dedicated to the automated analysis of contrastechocardiographic images for the evaluation of myocardial perfusion is presented in thispaper. The system recognizes the internal border of the heart during the transit of anultrasound contrast bolus injection and draws regions of interest. For each region of interest;time intensity curves are derived in order to quantify segmental perfusion.,Computer-Based Medical Systems.; 1997. Proceedings.; Tenth IEEE Symposium on,1997,*
MrBrAQue: a multimedia medical report management system,Paolo Merialdo; Giuseppe Sindoni,The goal of the MrBrAQue (Medical Report BRowse And QUEry) project is to develop asystem that enables effective management of a large collection of multimedia documents.The tools for consulting the report collection are presented in this paper. Our proposal isbased on a paradigm; which we call orthogonal navigation; that allows a user to dynamicallybuild a hypermedia network of documents by means of queries over the documentcollection.,Multimedia Computing and Systems' 97. Proceedings.; IEEE International Conference on,1997,*
Echography with contrast media. State of the art and future perspectives,P Voci; C Marelli; P Merialdo; H Morris,1. Cardiologia. 1996 Oct;41(10):953-66. [Echography with contrast media. State of theart and future perspectives]. [Article in Italian]. Voci P(1); Marelli C; Merialdo P; MorrisH. Author information: (1)Istituto di Chirurgia del Cuore e dei Grossi Vasi; Universitàdegli Studi La Sapienza; Roma. PMID: 8983824 [Indexed for MEDLINE]. PublicationTypes: Review. MeSH terms. Blood Vessels/diagnostic imaging; Contrast Media*;Echocardiography; Doppler/methods; Echocardiography; Doppler; Color/methods;Humans; Ultrasonography/methods*. Substance. Contrast Media.,*,1996,*
In Codice Ratio: OCR of Handwritten Latin Documents using Deep Convolutional Networks,Donatella Firmani; Paolo Merialdo; Elena Nieddu; Simone Scardapane,Abstract. Automatic transcription of historical handwritten documents is a challengingresearch problem; requiring in general expensive transcriptions from expert paleographers.In Codice Ratio is designed to be an end-to-end architecture requiring instead limitedlabeling effort; whose aim is the automatic transcription of a portion of the Vatican SecretArchives (one of the largest historical libraries in the world). In this paper; we describe inparticular the design of our OCR component for Latin characters. To this end; we firstannotated a large corpus of Latin characters with a custom crowdsourcing platform.Leveraging over recent progresses in deep learning; we designed and trained a deepconvolutional network achieving an overall accuracy of 96% over the entire dataset; which isone of the highest results reported in the literature so far. Our training data are publicly …,*,*,*
Universit egli Stu ii Rom 5re Dipartimento di Informatica e Automazione Via della Vasca Navale; 84-00146 Roma; Italy.,Data-Intensive Web Sites; Paolo Atzeni; Giansalvatore Mecca; Paolo Merialdo,Abs TRA CTM any W eb sites include sig nifi cant and su b stantial pieces of information; inaw ay th at is often di cult to sh are; correlate and maintain. In many cases the mana g ementof a W eb site can g reatly b ene fi t from the adoption of met h ods and tec h niq ues b orro wed from the data b ase fi eld. T h is paper introduces a met h odolo gy for desig nin g andmaintainin g lar ge W eb sites b ased on the assumption th at data to be pu b lis h ed in thesite are mana g ed usin ga DB MS. W e see the process of desig nin gthe site as the result oftwo intert w ined activities: the database design and the hypertext design. E ac h of th ese isfurt h er divided in a conceptual design ph ase and a logical design ph ase; b ased onspecifi c data models. A ne w lo g ical data model; called ad m; is used to descri b et hestructure of a W e bh yperte x t. It is pa g e-oriented; in the sense th at the main construct is …,*,*,*
Universita degli Studi di Roma Tre,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo; Paolo Papotti,ABSTRACT An increasing number of web sites publish pages containing structuredinformation about recognizable concepts; relevant to specific application domains-such asfinance; sport; products. Although such information is spread across a myriad of sources; theweb scale implies a relevant redundancy; which is apparent both at the intensional level(many sources share a core set of attributes); and at the extensional level (many instancesoccur in a number of sources). The paper investigates novel; domain independenttechniques that exploit the redundancy of information among these sources to automaticallyextract and integrate data from the Web. The presented techniques have been implementedin a working prototype; which has been used to conduct a number of experiments on bothsynthetic and real-life web sites. Experimental results confirm the feasibility of the …,*,*,*
DEOS 2014 Chairs’ Welcome,Valter Crescenzi; Tim Furche; Paolo Merialdo; Giorgio Orsi,It is our great pleasure to welcome you to the WWW'14 Workshop on Data Extraction andObject Search–DEOS'14. This is the fourth installment of DEOS; co-located with WWW onApril 7th; the previous installments were held in Como 2010; in Vienna 2011; and in Oxford2013. Web data extraction is witnessing a renaissance. In an increasing number ofapplications such as price intelligence or predictive analytics; the value of data-drivenapproaches has been conclusively proven. However; the necessary data is often availableonly as HTML; eg; in form of online shops of competitors that can serve as sources forpricing and offer data. DEOS is a regular forum for researchers and practitioners in dataextraction and object search; to present and discuss ongoing work on data extraction andobject search for products; events; reviews; and other types of structured data on the web …,*,*,*
The 2010 IEEE International Symposium on Mining and Web (MAW) Organizing Committee,Kin Fun LI; Yanchun ZHANG; Hayato YAMANA; Shengrui WANG; Takahiro HARA; Laurence T YANG; Sonia Bergamaschi; Lifei Chen; Floriana Esposito; Sumio Fujita; Tomohiro Fukuhara; Hyoil Han; Yoshinori Hijikata; Michael Horie; Qingshan Jiang; Xue-Mei Jiang; Yuefeng Li; Jianguo Lu; Takuya Maekawa; Paolo Merialdo; Bamshad Mobasher; Kotaro Nakayama; Jianyun Nie; Chikashi Nobata; Rafael Parra-Hernandez; Keun Ho Ryu; Giovanni Maria Sacco; Kazuhiro Seki; Haojun Sun; Kosuke Takano; Giorgio Terracina; Masashi Toyoda; Jinmin Yang; Hwan-Seung Yong; Martine Wedlake; Koji Zettsu,The 2010 IEEE International Symposium on Mining and Web (MAW) … General Chairs: KinFun LI; University of Victoria; Canada Yanchun ZHANG; Victoria University; Australia ProgramChairs: Hayato YAMANA; Waseda University; Japan Shengrui WANG; Université deSherbrooke; Canada Steering Chair: Takahiro HARA; Osaka University; Japan InternationalLiaison: Laurence T. YANG; St. Francis Xavier University; Canada … Program Committee: SoniaBergamaschi Lifei Chen Floriana Esposito Sumio Fujita Tomohiro Fukuhara Hyoil Han YoshinoriHijikata Michael Horie Qingshan Jiang Xue-Mei Jiang Yuefeng Li Jianguo Lu Takuya MaekawaPaolo Merialdo Bamshad Mobasher Kotaro Nakayama Jianyun Nie Chikashi Nobata RafaelParra-Hernandez Keun Ho Ryu Giovanni Maria Sacco Kazuhiro Seki Haojun Sun Kosuke TakanoGiorgio Terracina Masashi Toyoda Jinmin Yang Hwan-Seung Yong Martine Wedlake …,*,*,*
Luc Moreau; University of Southampton; UK Boris Motik; University Karlsruhe; Germany Maurice Mulvenna; University of Ulster; UK Chivukula A. Murthy; Indian Stati...,Yasuhiko Kitamura; Michael Koch; Manuel Kolp; Donald H Kraft; Vipin Kumar; Raymond Yiu Keung Lau; Alain Leger; Chunhung Li; Juanzi Li; Xiaoming Li; Xuelong Li; Yuefeng Li; Bing Liu; Chao-Lin Liu; Chunnian Liu; Jie Lu; Rainer Malaka; Massimo Marchiori; Naohiro Matsumura; Mitsunori Matsushita; Robert Meersman; Sergey Melnik; Ernestina Menasalvas; Paolo Merialdo; Eva Millan,Ajith Abraham; Oklahoma State University; USA Anupriya Ankolekar; Carnegie MellonUniversity; USA Lora Aroyo; Eindhoven University of Technology; The Netherlands Jean-PaulBarthes; University of Compiegne; France Hendrik Blockeel; Katholieke Universiteit Leuven;Belgium Omar Boucelma; LSIS-CNRS; Marseille; France Peter Brezany; University ofVienna; Austria Peter Brusilovsky; University of Pittsburgh; USA Christoph Bussler; OracleCorporation; USA Cory Butz; University of Regina; Canada Lawrence Cavedon; StanfordUniversity; USA Keith CC Chan; Hong Kong Polytechnic University; Hong Kong DavidCheung; The University of Hong Kong; Hong Kong William Cheung; Hong Kong BaptistUniversity; Hong Kong Yiuming Cheung; Hong Kong Baptist University; Hong Kong NigelCollier; National Institute of Informatics; Japan Maarten de Rijke; University of …,*,*,*
The 2010 IEEE International Symposium on Mining and Web (MAW) Reviewers List,Teresa Basile; Sonia Bergamaschi; Lifei Chen; Floriana Esposito; Nicola Fanizzi; Sumio Fujita; Tomohiro Fukuhara; Hyoil Han; Takahiro Hara; Yoshinori Hijikata; Michael Horie; Kin Fun Li; Jianguo Lu; Takuya Maekawa; Paolo Merialdo; Jianyun Nie; Chikashi Nobata; Rafael Parra-Hernandez; Keun Ho Ryu; Giovanni Maria Sacco; Antonio Sala; Kazuhiro Seki; Serena Sorrentino; Haojun Sun; Shengrui Wang; Martine Wedlake; Hayato Yamana,A not-for-profit organization; IEEE is the world's largest technical professional organization dedicatedto advancing technology for the benefit of humanity. © Copyright 2017 IEEE - All rightsreserved. Use of this web site signifies your agreement to the terms and conditions.,*,*,*
The 2011 International Symposium on Mining and Web (MAW) Reviewers List,Sonia Bergamaschi; Lifei Chen; Floriana Esposito; Sumio Fujita; Tomohiro Fukuhara; Takahiro Hara; Masahiro Ito; Kin Fun Li; Jianguo Lu; Takuya Maekawa; Paolo Merialdo; Kotaro Nakayama; Rafael Parra-Hernandez; Shengrui Wang; Martine Wedlake; Hayato Yamana,A not-for-profit organization; IEEE is the world's largest technical professional organization dedicatedto advancing technology for the benefit of humanity. © Copyright 2017 IEEE - All rightsreserved. Use of this web site signifies your agreement to the terms and conditions.,*,*,*
Information Extraction by Convergent Boundary Classification/1 Aidan Finn and Nicholas Kushmerick IE Evaluation: Criticisms and Recommendations/7 A. Lavelli; M...,Yongzheng Zhang; Evangelos Milios; Nur Zincir-Heywood; Stephen Soderland; Oren Etzioni; Tal Shaked; Daniel S Weld; Un Yong Nahm; Raymond J Mooney; Kristina Lerman; Cenk Gazen; Steven Minton; Craig Knoblock; Valter Crescenzi; Giansalvatore Mecca; Paolo Merialdo; Farah Benamara; Doug Downey; Rakesh Gupta; Mykel J Kochenderfer; Cheng Niu; Wei Li; Rohini K Srihari; Ana-Maria Popescu; Alexander Yates; Rohit Joshi; Xiaoli Li; Sreeram Ramachandaran; Tze Yun Leong,IE Evaluation: Criticisms and Recommendations / 7 A. Lavelli; ME Califf; F. Ciravegna; D.Freitag; C. Giuliano; N. Kushmerick; and L. Romano … A Comparison of Keyword- andKeyterm-based Methods for Automatic Web Site Summarization / 15 Yongzheng Zhang; EvangelosMilios; and Nur Zincir-Heywood … The Use ofWeb-based Statistics to Validate Information Extraction/ 21 Stephen Soderland; Oren Etzioni; Tal Shaked; and Daniel S. Weld … Using Soft-MatchingMined Rules to Improve Information Extraction / 27 Un Yong Nahm and Raymond J. Mooney… Populating the SemanticWeb / 33 Kristina Lerman; Cenk Gazen; Steven Minton; and CraigKnoblock … Handling Irregularities in ROADRUNNER / 39 Valter Crescenzi; GiansalvatoreMecca; and Paolo Merialdo … A Model for Graded Levels of Generalizations in Intensional QueryAnswering / 45 Farah Benamara … Learning Text Patterns for Web Information …,*,*,*
Quadro Metodologico per lo Sviluppo di Siti Web,Paolo Atzeni; Paolo Merialdo,*,*,*,*
Strumenti avanzati di backup ed amministrazione remota per applicazioni web basate su database relazionale,Paolo Merialdo; Antonio Leonforte; Andrea Ruocco,Oggi più che mai i sistemi informativi sono sempre maggiormente orientati verso il web; lopossiamo dedurre dal fatto che le tecnologie a disposizione degli sviluppatori siano; oltreche numerose; anche così affermate e ben strutturate. Infatti la diffusione del web che siavuta nell'ultimo decennio ha spostato sempre di più l'attenzione su sistemi informativicostruiti intorno ad esso; che; pur costituendo un vasto panorama; fanno emergere unaparticolare tipologia di applicazioni; quelle centrate sui dati; ossia che offrono operazioniquali salvataggio; ricerca; modifica ed eliminazione degli stessi. A partire da componenti giàesistenti che permettono di generare applicazioni di questo tipo; il progetto in cui si inseriscequesta tesi si pone l'obiettivo di sviluppare un intero portale che permetta all'utente di gestireil loro ciclo di vita; quindi la generazione; il testing; la manutenzione; la possibilità di …,*,*,*
Automatically Generating Reports from Large Web Sites,Lorenzo Blanco; Valter Crescenzi; Paolo Merialdo,ABSTRACT Many large web sites contain highly valuable information. Their pages aredynamically generated by scripts which retrieve data from a back-end database and embedthem in HTML templates. Based on this observation several techniques have beendeveloped to automatically extract data from a set of structurally homogeneous pages.These tools represent a step towards the automatic extraction of data from large web sites;but currently their input sample pages have to be manually collected. To scale the dataextraction process this task should be automated; as well. We present techniques toautomatically gathering structurally similar pages from large web sites. We have developedan algorithm that takes as input one sample page; and crawls the site to find pages similar instructure to the given page. The algorithm minimizes the number of pages downloaded by …,*,*,*
The 2011 International Symposium on Mining and Web (MAW) Organizing Committee,Takahiro HARA; Kin Fun LI; Hayato YAMANA; Shengrui WANG; Sonia Bergamaschi; Lifei Chen; Floriana Esposito; Sumio Fujita; Tomohiro Fukuhara; Yoshinori Hijikata; Michael Horie; Qingshan Jiang; Xue-Mei Jiang; Yuefeng Li; Jianguo Lu; Takuya Maekawa; Paolo Merialdo; Bamshad Mobasher; Kotaro Nakayama; Jianyun Nie; Chikashi Nobata; Rafael Parra-Hernandez; Keun Ho Ryu; Giovanni Maria Sacco; Kazuhiro Seki; Haojun Sun; Kosuke Takano; Giorgio Terracina; Masashi Toyoda; Jinmin Yang; Hwan-Seung Yong; Martine Wedlake; Koji Zettsu,The 2011 International Symposium on Mining and Web (MAW) … Conference Chairs: TakahiroHARA; Osaka University; Japan Kin Fun LI; University of Victoria; Canada Hayato YAMANA;Waseda University; Japan Shengrui WANG; Université de Sherbrooke; Canada … ProgramCommittee: Sonia Bergamaschi Lifei Chen Floriana Esposito Sumio Fujita Tomohiro FukuharaYoshinori Hijikata Michael Horie Qingshan Jiang Xue-Mei Jiang Yuefeng Li Jianguo Lu TakuyaMaekawa Paolo Merialdo Bamshad Mobasher Kotaro Nakayama Jianyun Nie Chikashi NobataRafael Parra-Hernandez Keun Ho Ryu Giovanni Maria Sacco Kazuhiro Seki Haojun Sun KosukeTakano Giorgio Terracina Masashi Toyoda Jinmin Yang Hwan-Seung Yong Martine WedlakeKoji Zettsu,*,*,*
Universit a degli Studi di Roma Tre,Giansalvatore Mecca; P Atzeni; Paolo Merialdo; A Masci; G Sindoni,ABSTRACT Large web sites are becoming repositories of structured information that canbene t from being viewed and queried as relational databases. However; querying theseviews e ciently requires new techniques. Data usually resides at a remote site and isorganized as a set of related HTML documents; with network access being a primary costfactor in query evaluation. This cost can be reduced by exploiting the redundancy oftenfound in site design. We use a simple data model; a subset of the Araneus data model; todescribe the structure of a web site. We augment the model with link and inclusionconstraints that capture the redundancies in the site. We map relational views of a site to anavigational algebra and show how to use the constraints to rewrite algebraic expressions;reducing the number of network accesses. We show that similar techniques can be used …,*,*,*
Reingegnerizzazione del sito web del collegio didattico,Paolo Merialdo; Pierangelo Serio,Possa io fare della mia vita qualcosa di semplice e diritto; come un flauto di canna che il Signoreriempie di musica. (R.Tagore) … Un grazie particolare alla mia famiglia per avermi dato fiduciae la possibilità d'intraprendere il mio cammino … I Introduzione I.1 Il sito web del collegiodidattica.................................................... 1 I.2 Obbiettivi del lavoro...................................................................... 2 … 1 Scelte tecnologiche 1.1 L'architettura LAMP ................................................................. 3 1.1.1 Usare LAMP ................................................................... 5 1.1.2 Curva di Apprendimento… 2 Sicurezza e protezione; attacchi e difese per il web 2.1 Sicurezzadi rete.......................................................................... 19 2.2 Sicurezza del sistema operativo.................................................. 20 2.3 Sicurezza dell'applicazione … 3Ristrutturazione e Sviluppo 3.1 Controllo utilizzo codice e tabelle …,*,*,*
INTEGRAZIONE DI UNA PIATTAFORMA IPTV IN UN’ARCHITETTURA SOA,Sara Castellani; Paolo Merialdo; Alessandro Balzarelli; Alessandro Toscano,Internet è divenuto il mezzo di trasmissione e di ricezione di informazioni più diffuso almondo. Nel corso degli ultimi decenni la rete ha ampliato sempre più il suo spettro d‟azione; sia dal punto di vista della copertura geografica; che dal punto di vista della qualitàdel servizio fornito in termini di efficienza; affidabilità e sicurezza delle informazioniscambiate. Il miglioramento delle tecnologie di rete ha reso possibile il passaggio da unambiente di semplice divulgazione di contenuti testuali ad un pianeta complesso in cui èpossibile anche acquistare beni di prima necessità.,*,*,*
DIFA {Universit a della Basilicata 2 DIA {Universit a Roma Tre 3 DCI {Rutherford Appleton Lab.,P Atzeni; G Mecca; P Merialdo; G Sindoni,*,*,*,*
Universita degli Studi di Roma Tre,Valter Crescenzi; Paolo Merialdo; Paolo Missier,ABSTRACT Several techniques have been recently proposed to automatically generate webwrappers; ie; programs that extract data from HTML pages; and transform them into a morestructured format; typically in XML. These techniques automatically induce a wrapper from aset of sample pages that share a common HTML template. An open issue; however; is howto collect suitable classes of sample pages to feed the wrapper inducer. Presently; the pagesare chosen manually. In this paper; we tackle the problem of automatically discovering themain classes of pages offered by a site by exploring only a small yet representative portionof it. We propose a model to describe the structure of a web site as a graph: nodes areclasses of pages that share a common structure; edges represent links among instances ofthe page classes. Based on this model; we have developed an algorithm that accepts the …,*,*,*
E cient Queries over Web Views,Giansalvatore Mecca Alberto O Mendelzon; Paolo Merialdo,Abstract Large web sites are becoming repositories of structured information that can bene tfrom being viewed and queried as relational databases. However; querying these views eciently requires new techniques. Data usually resides at a remote site and is organized as aset of related HTML documents; with network access being a primary cost factor in queryevaluation. This cost can be reduced by exploiting the redundancy often found in sitedesign. We use a simple data model; a subset of the Araneus data model; to describe thestructure of a web site. We augment the model with link and inclusion constraints thatcapture the redundancies in the site. We map relational views of a site to a navigationalalgebra and show how to use the constraints to rewrite algebraic expressions; reducing thenumber of network accesses. We show that similar techniques can be used to maintain …,*,*,*
