Relational Databases for Querying XML Documents: Limitations and Opportunities,Shanmugasundaram Jayavel; Tufte Kristin; He Gang; Zhang Chun; DeWitt David; Naughton Jeffrey,*,Proceedings of the 25th VLDB Conference; Edinburgh; Scotland,1999,1644
On supporting containment queries in relational database management systems,Chun Zhang; Jeffrey Naughton; David DeWitt; Qiong Luo; Guy Lohman,Abstract Virtually all proposals for querying XML include a class of query we term“containment queries”. It is also clear that in the foreseeable future; a substantial amount ofXML data will be stored in relational database systems. This raises the question of how tosupport these containment queries. The inverted list technology that underlies much ofInformation Retrieval is well-suited to these queries; but should we implement thistechnology (a) in a separate loosely-coupled IR engine; or (b) using the native tables andquery execution machinery of the RDBMS? With option (b); more than twenty years of workon RDBMS query optimization; query execution; scalability; and concurrency control andrecovery immediately extend to the queries and structures that implement these newoperations. But all this will be irrelevant if the performance of option (b) lags that of (a) by …,Acm Sigmod Record,2001,1177
On the computation of multidimensional aggregates,Sameet Agarwal; Rakesh Agrawal; Prasad M Deshpande; Ashish Gupta; Jeffrey F Naughton; Raghu Ramakrishnan; Sunita Sarawagi,Abstract At the heart of OLAP or multidimensional data analysis applications is the ability tosimultaneously aggregate across many sets of dimensions. Computing multidimensionalaggregates is a performance bottleneck for these applications. We explore various schemesfor implementing multidimensional aggregation; in particular; the CUBE operator [1]proposed by Gray et al. This operator computes aggregates over all subsets of dimensionsspecified in the CUBE operation; and is equivalent to the union of a number of standardgroup-by operations. We show how the structure of CUBE computation can be viewed interms of a hierarchy of group-by operations; and present a class of sorting-based algorithmsthat overlap the computation of different group-by operations using the least possiblememory for each computation. Our algorithms seek to minimize the number of sorting …,VLDB,1996,849
Generalized search trees for database systems,Joseph M Hellerstein; Jeffrey F Naughton; Avi Pfeffer,Abstract This paper introduces the Generalized Search Tree (GiST); an index structuresupporting an extensible set of queries and data types. The GiST allows new data types tobe indexed in a manner supporting queries natural to the types; this is in contrast to previouswork on tree extensibility which only supported the traditional set of equality and rangepredicates. In a single data structure; the GiST provides all the basic search tree logicrequired by a database system; thereby unifying disparate structures such as Bt-trees and R-trees in a single piece of code; and opening the application of search trees to genera!extensibility. To illustrate the flexibility of the GiST; we provide simple methodimplementations that allow it to behave like a B+-tree; an R-tree; and an RD-tree; a newindex for data with set-valued attributes. We also present a preliminary performance …,*,1995,669
D. DeWitt; and J. Naughton;" The 007 Benchmark;",M Carey,*,Proceedings ofthe SIGMOD International Conference on Management ofData,*,668
An array-based algorithm for simultaneous multidimensional aggregates,Yihong Zhao; Prasad M Deshpande; Jeffrey F Naughton,Abstract Computing multiple related group-bys and aggregates is one of the core operationsof On-Line Analytical Processing (OLAP) applications. Recently; Gray et al.[GBLP95]proposed the “Cube” operator; which computes group-by aggregations over all possiblesubsets of the specified dimensions. The rapid acceptance of the importance of this operatorhas led to a variant of the Cube being proposed for the SQL standard. Several efficientalgorithms for Relational OLAP (ROLAP) have been developed to compute the Cube.However; to our knowledge there is nothing in the literature on how to compute the Cube forMultidimensional OLAP (MOLAP) systems; which store their data in sparse arrays ratherthan in tables. In this paper; we present a MOLAP algorithm to compute the Cube; andcompare it to a leading ROLAP algorithm. The comparison between the two is interesting …,ACM SIGMOD Record,1997,589
Shoring up persistent applications,Michael J Carey; David J DeWitt; Michael J Franklin; Nancy E Hall; Mark L McAuliffe; Jeffrey F Naughton; Daniel T Schuh; Marvin H Solomon; CK Tan; Odysseas G Tsatalos; Seth J White; Michael J Zwilling,Abstract SHORE (Scalable Heterogeneous Object REpository) is a persistent object systemunder development at the University of Wisconsin. SHORE represents a merger of object-oriented database and file system technologies. In this paper we give the goals andmotivation for SHORE; and describe how SHORE provides features of both technologies.We also describe some novel aspects of the SHORE architecture; including a symmetricpeer-to-peer server architecture; server customization through an extensible value-addedserver facility; and support for scalability on multiprocessor systems. An initial version ofSHORE is already operational; and we expect a release of Version 1 in mid-1994.,ACM SIGMOD Record,1994,583
Evaluating window joins over unbounded streams,Jaewoo Kang; Jeffrey F Naughton; Stratis D Viglas,We investigate algorithms for evaluating sliding window joins over pairs of unboundedstreams. We introduce a unit-time-basis cost model to analyze the expected performance ofthese algorithms. Using this cost model; we propose strategies for maximizing the efficiencyof processing joins in three scenarios. First; we consider the case where one stream is muchfaster than the other. We show that asymmetric combinations of join algorithms;(eg; hashjoin on one input; nested-loops join on the other) can outperform symmetric join algorithmimplementations. Second; we investigate the case where system resources are insufficientto keep up with the input streams. We show that we can maximize the number of join resulttuples produced in this case by properly allocating computing resources across the two inputstreams. Finally; we investigate strategies for maximizing the number of result tuples …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,485
Covering indexes for branching path queries,Raghav Kaushik; Philip Bohannon; Jeffrey F Naughton; Henry F Korth,Abstract In this paper; we ask if the traditional relational query acceleration techniques ofsummary tables and covering indexes have analogs for branching path expression queriesover tree-or graph-structured XML data. Our answer is yes---the forward-and-backwardindex already proposed in the literature can be viewed as a structure analogous to asummary table or covering index. We also show that it is the smallest such index that coversall branching path expression queries. While this index is very general; our experimentsshow that it can be so large in practice as to offer little performance improvement overevaluating queries directly on the data. Likening the forward-and-backward index to acovering index on all the attributes of several tables; we devise an index definition schemeto restrict the class of branching path expressions being indexed. The resulting index …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,422
Materialized view selection for multidimensional datasets,Amit Shukla; Prasad Deshpande; Jeffrey F Naughton,Abstract To fulfill the requirement of fast interactive multidimensional data analysis; databasesystems precompute aggregate views on some subsets of dimensions and theircorresponding hierarchies. However; the problem of what to precompute is difficult andintriguing. The leading existing algorithm; BPUS; has a running time that is polynomial in thenumber of views and is guaranteed to be within (0.63-f) of optimal; where f is the fraction ofavailable space consumed by the largest aggregate. Unfortunately; BPUS can beimpractically slow; and in some instances may miss good solutions due to the coarsegranularity at which it makes its decisions of what to precompute. In view of this; we study thestructure of the precomputation problem and show that under certain broad conditions onthe multidimensional data; an even simpler and faster algorithm; PBS; achieves the same …,VLDB,1998,365
Practical skew handling in parallel joins,David J DeWitt; Jeffrey F Naughton; Donovan A Schneider; Srinivasan Seshadri,Abstract We present an approach to dealing with skew in parallel joins in database systems.Our approach is easily implementable within current parallel DBMS; and performs well onskewed data without degrading the performance of the system on non-skewed data. Themain idea is to use multiple algorithms; each specialized for a different degree of skew; andto use a small sample of the relations being joined to determine which algorithm isappropriate. We developed; implemented; and experimented with four new skew-handlingparallel join algorithms; one; which we call virtual processor range partitioning; was the clearwinner in lhigh skew cases; while traditional hybrid hash join was the clear winner in lowerskew or no skew cases. We present experimental results from an implementation of all fouralgorithms on the Gamma parallel database machine. To our knowledge; these are the …,*,1992,365
The Asilomar report on database research,Phil Bernstein; Michael Brodie; Stefano Ceri; David DeWitt; Mike Franklin; Hector Garcia-Molina; Jim Gray; Jerry Held; Joe Hellerstein; HV Jagadish; Michael Lesk; Dave Maier; Jeff Naughton; Hamid Pirahesh; Mike Stonebraker; Jeff Ullman,Abstract The database research community is rightly proud of success in basic research;and its remarkable record of technology transfer. Now the field needs to radically broaden itsresearch focus to attack the issues of capturing; storing; analyzing; and presenting the vastarray of online data. The database research community should embrace a broader researchagenda—broadening the definition of database management to embrace all the content ofthe Web and other online data stores; and rethinking our fundamental assumptions in light oftechnology shifts. To accelerate this transition; we recommend changing the way researchresults are evaluated and presented. In particular; we advocate encouraging morespeculative and long-range work; moving conferences to a poster format; and publishing allresearch literature on the Web.,ACM Sigmod record,1998,339
Rate-based query optimization for streaming information sources,Stratis D Viglas; Jeffrey F Naughton,Abstract Relational query optimizers have traditionally relied upon table cardinalities whenestimating the cost of the query plans they consider. While this approach has been andcontinues to be successful; the advent of the Internet and the need to execute queries overstreaming sources requires a different approach; since for streaming inputs the cardinalitymay not be known or may not even be knowable (as is the case for an unbounded stream.)In view of this; we propose shifting from a cardinality-based approach to a rate-basedapproach; and give an optimization framework that aims at maximizing the output rate ofquery evaluation plans. This approach can be applied to cases where the cardinality-basedapproach cannot be used. It may also be useful for cases where cardinalities are known;because by focusing on rates we are able not only to optimize the time at which the last …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,330
-Maximizing the Output Rate of Multi-Way Join Queries over Streaming Information Sources,Stratis D Viglas; Jeffrey F Naughton; Josef Burger,Join algorithms have been extensively explored in the research literature; and at manypoints during the history of community one might have concluded that there was nothingmore to be discovered about them. The recent flurry of interest in streaming and adaptivejoin algorithms is one counter example. This chapter continues this investigation intostreaming and adaptive join algorithms; but with a new twist: by considering multi-waysymmetric join operators. The chapter shows that in many cases a multi-way join operatorcan produce its output in a streaming fashion and at a faster rate than any tree of binary joinoperators. Multi-way join operator has introduced interesting issues with respect to how tohandle memory overflow and how to choose a probing sequence within the join.Maximizingthe Output Rate of Multi-Way Join Queries over Streaming Information Sources Stratis D …,*,2003,329
Practical selectivity estimation through adaptive sampling,Richard J Lipton; Jeffrey F Naughton; Donovan A Schneider,Abstract Recently we have proposed an adaptive; random sampling algorithm for generalquery size estimation. In earlier work we analyzed the asymptotic efficiency and accuracy ofthe algorithm; in this paper we investigate its practicality as applied to selects and joins. First;we extend our previous analysis to provide significantly improved bounds on the amount ofsampling necessary for a given level of accuracy. Next; we provide “sanity bounds” to dealwith queries for which the underlying data is extremely skewed or the query result is verysmall. Finally; we report on the performance of the estimation algorithm as implemented in ahost language on a commercial relational system. The results are encouraging; even withthis loose coupling between the estimation algorithm and the DBMS.,ACM SIGMOD Record,1990,323
Sampling-based estimation of the number of distinct values of an attribute,Peter J Haas; Jeffrey F Naughton; S Seshadri; Lynne Stokes,Abstract We provide several new sampling-based estimators of the number of distinct valuesof an attribute in a relation. We compare these new estimators to estimators from thedatabase and statistical literature empirically; using a large number of attribute-valuedistributions drawn from a variety of real-world databases. This appears to be the firstextensive comparison of distinct-value estimators in either the database or statisticalliterature; and is certainly the first to use highlyskewed data of the sort frequentlyencountered in database applications. Our experiments indicate that a new “hybrid”estimator yields the highest precision on average for a given sampling fraction. Thisestimator explicitly takes into account the degree of skew in the data and combines a new“smoothed jackknife” estimator with an estimator due to Shlosser. We investigate how the …,VLDB,1995,313
On schema matching with opaque column names and data values,Jaewoo Kang; Jeffrey F Naughton,Abstract Most previous solutions to the schema matching problem rely in some fashion uponidentifying" similar" column names in the schemas to be matched; or by recognizingcommon domains in the data stored in the schemas. While each of these approaches isvaluable in many cases; they are not infallible; and there exist instances of the schemamatching problem for which they do not even apply. Such problem instances typically arisewhen the column names in the schemas and the data in the columns are" opaque" or verydifficult to interpret. In this paper we propose a two-step technique that works even in thepresence of opaque column names and data values. In the first step; we measure the pair-wise attribute correlations in the tables to be matched and construct a dependency graphusing mutual information as a measure of the dependency between attributes. In the …,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,271
Caching multidimensional queries using chunks,Prasad M Deshpande; Karthikeyan Ramasamy; Amit Shukla; Jeffrey F Naughton,Abstract Caching has been proposed (and implemented) by OLAP systems in order toreduce response times for multidimensional queries. Previous work on such caching hasconsidered table level caching and query level caching. Table level caching is more suitablefor static schemes. On the other hand; query level caching can be used in dynamic schemes;but is too coarse for “large” query results. Query level caching has the further drawback forsmall query results in that it is only effective when a new query is subsumed by a previouslycached query. In this paper; we propose caching small regions of the multidimensionalspace called “chunks”. Chunk-based caching allows fine granularity caching; and allowsqueries to partially reuse the results of previous queries with which they overlap. To facilitatethe computation of chunks required by a query but missing from the cache; we propose a …,ACM SIGMOD Record,1998,265
Estimating the selectivity of XML path expressions for internet scale applications,Ashraf Aboulnaga; Alaa R Alameldeen; Jeffrey F Naughton,Abstract Data on the Internet is increasingly presented in XML format. This allows for novelapplications that query all this data using some XML query language. All XML querylanguages use path expressions to navigate through the tree structure of the data.Estimating the selectivity of these path expressions is therefore essential for optimizingqueries in these languages. In this paper; we propose two techniques for capturing thestructure of complex large-scale XML data as would be handled by Internet-scaleapplications in a small amount of memory for estimating the selectivity of XML pathexpressions: summarized path trees and summarized Markov tables. We experimentallydemonstrate the accuracy of our proposed techniques; and explore the different situationsthat would favor one technique over the other. We also demonstrate that our proposed …,VLDB,2001,263
Storage estimation for multidimensional aggregates in the presence of hierarchies,Amit Shukla; Prasad M Deshpande; Jerey Naughton; K Ramaswamy,*,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,1996,260
The Niagara internet query system,Jeffrey F.  Naughton; David J.  DeWitt; David Maier; Ashraf Aboulnaga; Jianjun Chen; Leonidas Galanis; Jaewoo Kang; Rajasekar Krishnamurthy; Qiong Luo; Naveen Prakash; Ravishankar Ramamurthy; Jayavel Shanmugasundaram; Feng Tian; Kristin Tufte; Stratis Viglas; Yuan Wang; Chun Zhang; Bruce Jackson; Anurag Gupta; Rushan Chen,Abstract Recently; there has been a great deal of research into XML query languages toenable the execution of database-style queries over XML files. However; merely being anXML query-processing engine does not render a system suitable for querying the Internet. Auseful system must provide mechanisms to (a) find the XML files that are relevant to a givenquery; and (b) deal with remote data sources that either provide unpredictable data accessand transfer rates; or are infinite streams; or both. The Niagara Internet Query System wasdesigned from the bottom-up to provide these mechanisms. In this article we describe theoverall Niagara architecture; and how Niagara finds relevant XML documents by using acollaboration between the Niagara XML-QL query processor and the Niagara “text-in-context” XML search engine. The Niagara Internet Query System is public domain …,IEEE Data Eng. Bull.,2001,256
Cache conscious algorithms for relational query processing,Ambuj Shatdal; Chander Kant; Jeffrey F Naughton,The current main memory (DRAM) access speeds lag far behind CPU speeds. Cachememory; made of static RAM; is being used in today's architectures to bridge this gap. Itprovides access latencies of 2-4 processor cycles; in contrast to main memory whichrequires 15-25 cycles. Therefore; the performance of the CPU depends upon how well thecache can be utilized. We show that there are signiﬁcant beneﬁts in redesigning ourtraditional query processing algorithms so that they can make better use of the cache. Thenew algorithms run 8%—200% faster than the traditional ones.,*,1994,245
Middle-tier database caching for e-business,Qiong Luo; Sailesh Krishnamurthy; C Mohan; Hamid Pirahesh; Honguk Woo; Bruce G Lindsay; Jeffrey F Naughton,Abstract While scaling up to the enormous and growing Internet population withunpredictable usage patterns; E-commerce applications face severe challenges in cost andmanageability; especially for database servers that are deployed as those applications'backends in a multi-tier configuration. Middle-tier database caching is one solution to thisproblem. In this paper; we present a simple extension to the existing federated features inDB2 UDB; which enables a regular DB2 instance to become a DBCache without anyapplication modification. On deployment of a DBCache at an application server; arbitrarySQL statements generated from the unchanged application that are intended for a backenddatabase server; can be answered: at the cache; at the backend database server; or at bothlocations in a distributed manner. The factors that determine the distribution of workload …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,240
On the integration of structure indexes and inverted lists,Raghav Kaushik; Rajasekar Krishnamurthy; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract Several methods have been proposed to evaluate queries over a native XMLDBMS; where the queries specify both path and keyword constraints. These broadly consistof graph traversal approaches; optimized with auxiliary structures known as structureindexes; and approaches based on information-retrieval style inverted lists. We propose astrategy that combines the two forms of auxiliary indexes; and a query evaluation algorithmfor branching path expressions based on this strategy. Our technique is general andapplicable for a wide range of choices of structure indexes and inverted list join algorithms.Our experiments over the Niagara XML DBMS show the benefit of integrating the two formsof indexes. We also consider algorithmic issues in evaluating path expression queries whenthe notion of relevance ranking is incorporated. By integrating the above techniques with …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,230
Anonymization of set-valued data via top-down; local generalization,Yeye He; Jeffrey F Naughton,Abstract Set-valued data; in which a set of values are associated with an individual; iscommon in databases ranging from market basket data; to medical databases of patients'symptoms and behaviors; to query engine search logs. Anonymizing this data is important ifwe are to reconcile the conflicting demands arising from the desire to release the data forstudy and the desire to protect the privacy of individuals represented in the data.Unfortunately; the bulk of existing anonymization techniques; which were developed forscenarios in which each individual is associated with only one sensitive value; are not well-suited for set-valued data. In this paper we propose a top-down; partition-based approach toanonymizing set-valued data that scales linearly with the input size and scores well on aninformation-loss data quality metric. We further note that our technique can be applied to …,Proceedings of the VLDB Endowment,2009,216
A general technique for querying XML documents using a relational database system,Jayavel Shanmugasundaram; Eugene Shekita; Jerry Kiernan; Rajasekar Krishnamurthy; Efstratios Viglas; Jeffrey Naughton; Igor Tatarinov,Abstract There has been recent interest in using relational database systems to store andquery XML documents. Each of the techniques proposed in this context works by (a) creatingtables for the purpose of storing XML documents (also called relational schemageneration);(b) storing XML documents by shredding them into rows in the created tables;and (c) converting queries over XML documents into SQL queries over the created tables.Since relational schema generation is a physical database design issue--dependent onfactors such as the nature of the data; the query workload and availability of schemas--therehave been many techniques proposed for this purpose. Currently; each relational schemageneration technique requires its own query processor to efficiently convert queries overXML documents into SQL queries over the created tables. In this paper; we present an …,ACM SIGMOD Record,2001,209
The Lowell database research self-assessment,Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk; David Maier; Jeff Naughton; Hans Schek; Timos Sellis; Avi Silberschatz; Mike Stonebraker; Rick Snodgrass; Jeff Ullman; Gerhard Weikum; Jennifer Widom; Stan Zdonik,A group of senior database researchers gathers every few years to assess the state of databaseresearch and to point out problem areas that deserve additional focus. This article summarizesthe discussion and conclusions of the sixth such meeting in Lowell; MA; in May 2003. It followsa number of earlier reports with similar goals; including [1; 2; 5-7] … Continuing thistradition; 25 senior database researchers representing a broad cross section of the field in termsof research interests; affiliations; and geography gathered in Lowell for two days of intensivediscussion on where the database field is and where it should be going … Several importantobservations came out of this meeting. Information management continues to be a critical componentof most complex software systems. We recommend that database researchers increase theirfocus on the integration of text; data; code; and streams; fusion of information from …,Communications of the ACM,2005,203
Declarative information extraction using datalog with embedded extraction predicates,Warren Shen; AnHai Doan; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract In this paper we argue that developing information extraction (IE) programs usingDatalog with embedded procedural extraction predicates is a good way to proceed. First;compared to current ad-hoc composition using; eg; Perl or C++; Datalog provides a cleanerand more powerful way to compose small extraction modules into larger programs. Thus;writing IE programs this way retains and enhances the important advantages of currentapproaches: programs are easy to understand; debug; and modify. Second; once we writeIE programs in this framework; we can apply query optimization techniques to them. Thisgives programs that; when run over a variety of data sets; are more efficient than anymonolithic program because they are optimized based on the statistics of the data on whichthey are invoked. We show how optimizing such programs raises challenges specific to …,Proceedings of the 33rd international conference on Very large data bases,2007,197
An evaluation of non-equijoin algorithms,David J DeWitt; Jeffrey F Naughton; Donovan A Schneider,Note: OCR errors may be found in this Reference List extracted from the full text article. ACMhas opted to expose the complete List rather than only correct and linked references …{Con71} WJ Conover. Practical Nonparametric Statistics . John Wiley & Sons; New York; NY;1971 … {DG85} David J. DeWitt; Robert H. Gerber: Multiprocessor Hash-Based JoinAlgorithms. VLDB 1985: 151-164 … {KTMo83} Masaru Kitsuregawa; Hidehiko Tanaka; TohruMoto-Oka: Application of Hash to Data Base Machine and Its Architecture. New GenerationComput. 1(1): 63-74(1983) … {RE78} D. Ries and R. Epstein. Evaluation of distribution criteriafor distributed database systems. Technical Report UCB/ERL Tech. Rep. M78/22;UC-Berkeley; May 1978 … {Sto86} Michael Stonebraker: The Case for Shared Nothing. IEEEDatabase Eng. Bull. 9(1): 4-9(1986).,Proceedings of the 17th international conference on very large data bases,1991,191
Low-latency; concurrent checkpointing for parallel programs,Kai Li; Jeffrey F.  Naughton; James S.  Plank,Presents the results of an implementation of several algorithms for checkpointing andrestarting parallel programs on shared-memory multiprocessors. The algorithms arecompared according to the metrics of overall checkpointing time; overhead imposed by thecheckpointer on the target program; and amount of time during which the checkpointerinterrupts the target program. The best algorithm measured achieves its efficiency through avariation of copy-on-write; which allows the most time-consuming operations of thecheckpoint to be overlapped with the running of the program being checkpointed.,IEEE transactions on Parallel and Distributed Systems,1994,185
Parallel sorting on a shared-nothing architecture using probabilistic splitting,David J DeWitt; Jeffrey F Naughton; Donovan A Schneider,The authors consider the problem of external sorting in a shared-nothing multiprocessor. Acritical step in the algorithms the authors consider is to determine the range of sort keys to behandled by each processor. They consider two techniques for determining these ranges ofsort keys: exact splitting; using a parallel version of the algorithm proposed by Iyer; Ricard;and Varman; and probabilistic splitting; which uses sampling to estimate quantiles. Theypresent analytic results showing that probabilistic splitting performs better than exactsplitting. Finally; the authors present experimental results from an implementation of sortingprobabilistic splitting in the Gamma parallel database machine.,Parallel and distributed information systems; 1991.; proceedings of the first international conference on,1991,181
Adaptive parallel aggregation algorithms,Ambuj Shatdal; Jeffrey F Naughton,Abstract Aggregation and duplicate removal are common in SQL queries. However; in theparallel query processing literature; aggregate processing has received surprisingly littleattention; furthermore; for each of the traditional parallel aggregation algorithms; there is arange of grouping selectivities where the algorithm performs poorly. In this work; we proposenew algorithms that dynamically adapt; at query evaluation time; in response to observedgrouping selectivities. Performance analysis via analytical modeling and an implementationon a workstation-cluster shows that the proposed algorithms are able to perform well for allgrouping selectivities. Finally; we study the effect of data skew and show that for certain datasets the proposed algorithms can even outperform the best of traditional approaches.,ACM SIGMOD Record,1995,170
XML-to-SQL query translation literature: The state of the art and open problems,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract Recently; the database research literature has seen an explosion of publicationswith the goal of using an RDBMS to store and/or query XML data. The problems addressedand solved in this area are diverse. This diversity renders it difficult to know how the variousresults presented fit together; and even makes it hard to know what open problems remain.As a first step to rectifying this situation; we present a classification of the problem space anddiscuss how almost 40 papers fit into this classification. As a result of this study; we find thatsome basic questions are still open. In particular; for the XML publishing of relational dataand for “schema-based” shredding of XML documents into relations; there is no publishedalgorithm for translating even simple path expression queries (with the axis) into SQL whenthe XML schema is recursive.,International XML Database Symposium,2003,166
Real-time; concurrent checkpoint for parallel programs,Kai Li; Jeffery F Naughton; James S Plank,Abstract We have developed and implemented a checkpointing and restart algorithm forparallel programs running on commercial uniprocessors and shared-memorymultiprocessors. The algorithm runs concurrently with the target program; interrupts thetarget program for small; fixed amounts of time and is transparent to the checkpointedprogram and its compiler. The algorithm achieves its efficiency through a novel use ofaddress translation hardware that allows the most time-consuming operations of thecheckpoint to be overlapped with the running of the program being checkpointed.,ACM SIGPLAN Notices,1990,154
Combining keyword search and forms for ad hoc querying of databases,Eric Chu; Akanksha Baid; Xiaoyong Chai; AnHai Doan; Jeffrey Naughton,Abstract A common criticism of database systems is that they are hard to query for usersuncomfortable with a formal query language. To address this problem; form-based interfacesand keyword search have been proposed; while both have benefits; both also havelimitations. In this paper; we investigate combining the two with the hopes of creating anapproach that provides the best of both. Specifically; we propose to take as input a targetdatabase and then generate and index a set of query forms offline. At query time; a user witha question to be answered issues standard keyword search queries; but instead of returningtuples; the system returns forms relevant to the question. The user may then build astructured query with one of these forms and submit it back to the system for evaluation. Inthis paper; we address challenges that arise in form generation; keyword search over …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,148
Building a scaleable geo-spatial DBMS: technology; implementation; and evaluation,Jignesh Patel; JieBing Yu; Navin Kabra; Kristin Tufte; Biswadeep Nag; Josef Burger; Nancy Hall; Karthikeyan Ramasamy; Roger Lueder; Curt Ellmann; Jim Kupsch; Shelly Guo; Johan Larson; David De Witt; Jeffrey Naughton,Abstract This paper presents a number of new techniques for parallelizing geo-spatialdatabase systems and discusses their implementation in the Paradise object-relationaldatabase system. The effectiveness of these techniques is demonstrated using a variety ofcomplex geo-spatial queries over a 120 GB global geo-spatial data set.,ACM SIGMOD Record,1997,145
On the provenance of non-answers to queries over extracted data,Jiansheng Huang; Ting Chen; AnHai Doan; Jeffrey F Naughton,Abstract In information extraction; uncertainty is ubiquitous. For this reason; it is useful toprovide users querying extracted data with explanations for the answers they receive.Providing the provenance for tuples in a query result partially addresses this problem; in thatprovenance can explain why a tuple is in the result of a query. However; in some casesexplaining why a tuple is not in the result may be just as helpful. In this work we focus onproviding provenance-style explanations for non-answers and develop a mechanism forproviding this new type of provenance. Our experience with an information extractionprototype suggests that our approach can provide effective provenance information that canhelp a user resolve their doubts over non-answers to a query.,Proceedings of the VLDB Endowment,2008,135
Static optimization of conjunctive queries with sliding windows over infinite streams,Ahmed M Ayad; Jeffrey F Naughton,Abstract We define a framework for static optimization of sliding window conjunctive queriesover infinite streams. When computational resources are sufficient; we propose that the goalof optimization should be to find an execution plan that minimizes resource usage within theavailable resource constraints. When resources are insufficient; on the other hand; wepropose that the goal should be to find an execution plan that sheds some of the input load(by randomly dropping tuples) to keep resource usage within bounds while maximizing theoutput rate. An intuitive approach to load shedding suggests starting with the plan that wouldbe optimal if resources were sufficient and adding" drop boxes" to this plan. We find this tobe often times suboptimal-in many instances the optimal partial answer plan results fromadding drop boxes to plans that are not optimal in the unlimited resource case. In view of …,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,135
On the performance of object clustering techniques,Manolis M Tsangaris; Jeffrey F Naughton,Abstract We investigate the performance of some of the best-known object clusteringalgorithms on four different workloads based upon the tektronix benchmark. For all fourworkloads; stochastic clustering gave the best performance for a variety of performancemetrics. Since stochastic clustering is computationally expensive; it is interesting that forevery workload there was at least one cheaper clustering algorithm that matched or almostmatched stochastic clustering. Unfortunately; for each workload; the algorithm thatapproximated stochastic clustering was different. Our experiments also demonstrated thateven when the workload and object graph are fixed; the choice of the clustering algorithmdepends upon the goals of the system. For example; if the goal is to perform well ontraversals of small portions of the database starting with a cold cache; the important metric …,ACM SIGMOD Record,1992,135
Clustera: an integrated computation and data management system,David J DeWitt; Erik Paulson; Eric Robinson; Jeffrey Naughton; Joshua Royalty; Srinath Shankar; Andrew Krioukov,Abstract This paper introduces Clustera; an integrated computation and data managementsystem. In contrast to traditional cluster-management systems that target specific types ofworkloads; Clustera is designed for extensibility; enabling the system to be easily extendedto handle a wide variety of job types ranging from computationally-intensive; long-runningjobs with minimal I/O requirements to complex SQL queries over massive relational tables.Another unique feature of Clustera is the way in which the system architecture exploitsmodern software building blocks including application servers and relational databasesystems in order to realize important performance; scalability; portability and usabilitybenefits. Finally; experimental evaluation suggests that Clustera has good scale-upproperties for SQL processing; that Clustera delivers performance comparable to Hadoop …,Proceedings of the VLDB Endowment,2008,132
Form-based proxy caching for database-backed web sites,Qiong Luo; Jeffrey F Naughton,Abstract We explore a new proxy-caching framework that exploits the query semantics ofHTML forms. We identify a common class of form-based queries; and study tworepresentative caching schemes for them within this framework:(i) traditional passive querycaching; and (ii) active query caching; in which the proxy cache can service a request byevaluating a query over the contents of the cache. Results from our experimentalimplementation show that our form-based proxy is a general and flexible approach thatefficiently enables active caching schemes for database-backed web sites. Furthermore;handling query containment at the proxy yields significant performance advantages overpassive query caching; but extending the power of the active cache to do full semanticcaching appears to be less generally effective.,VLDB,2001,129
-Mixed Mode XML Query Processing,Alan Halverson; Josef Burger; Leonidas Galanis; Ameet Kini; Rajasekar Krishnamurthy; Ajith Nagaraja Rao; Feng Tian; Stratis D Viglas; Yuan Wang; Jeffrey F Naughton; David J DeWitt,Querying XML documents typically involves both tree-based navigation and patternmatching similar to that used in structured information retrieval domains. This chapter showsthat for good performance; a native XML query processing system should support queryplans that mix these two processing paradigms. The chapter describes the prototype nativeXML system; and reports on experiments demonstrating that even for simple queries; thereare a number of options for how to combine tree-based navigation and structural joins basedon information retrieval-style inverted lists; and that these options can have widely varyingperformance. It presents the ways of transparently using both techniques in a single system;and provides a cost model for identifying efficient combinations of the techniques. Thepreliminary experimental results prove the viability of the approach.Mixed Mode XML …,*,2003,126
K-Anonymization as Spatial Indexing: Toward Scalable and Incremental Anonymization,Tochukwu Iwuchukwu David J DeWitt AnHai; Doan Jeffrey F Naughton,*,*,*,125
Design and evaluation of alternative selection placement strategies in optimizing continuous queries,Jianjun Chen; David J DeWitt; Jeffrey F Naughton,We design and evaluate alternative selection placement strategies for optimizing a verylarge number of continuous queries in an Internet environment. Two grouping strategies;PushDown and PullUp; in which selections are either pushed below; or pulled above; joinsare proposed and investigated. While our earlier research has demonstrated that theincremental group optimization can significantly outperform an ungrouped approach; theresults from the paper show that different incremental group optimization strategies can havesignificantly different performance characteristics. Surprisingly; in our studies; PullUp; inwhich selections are pulled above joins; is often better and achieves an average 10 foldperformance improvement over PushDown (occasionally 100 times faster). Furthermore; arevised algorithm of PullUp; termed filtered PullUp is proposed that is able to further …,Data Engineering; 2002. Proceedings. 18th International Conference on,2002,123
The BUCKY object-relational benchmark,Michael J Carey; David J DeWitt; Jeffrey F Naughton; Mohammad Asgarian; Paul Brown; Johannes E Gehrke; Dhaval N Shah,Abstract According to various trade journals and corporate marketing machines; we are nowon the verge of a revolution—the object-relational database revolution. Since we believethat no one should face a revolution without appropriate armaments; this paper presentsBUCKY; a new benchmark for object-relational database systems. BUCKY is a query-oriented benchmark that tests many of the key features offered by object-relational systems;including row types and inheritance; references and path expressions; sets of atomic valuesand of references; methods and late binding; and user-defined abstract data types and theirmethods. To test the maturity of object-relational technology relative to relational technology;we provide both an object-relational version of BUCKY and a relational equivalent thereof(ie; a relational BUCKY simulation). Finally; we briefly discuss the initial performance …,ACM SIGMOD Record,1997,123
Query size estimation by adaptive sampling,Richard J Lipton; Jeffrey F Naughton,Abstract We present an adaptive; random sampling algorithm for estimating the size ofgeneral queries. The algorithm can be used for any query Q over a database D such that 1)for some n; the answer to Q can be partitioned into n disjoint subsets Q 1; Q 2;…; Q n; and 2)for 1≤ i≤ n; the size of Q i is bounded by some function b (D; Q); and 3) there is somealgorithm by which we can compute the size of Q i; where i is chosen randomly. We considerthe performance of the algorithm on three special cases of the algorithm: join queries;transitive closure queries; and general recursive Datalog queries.,Proceedings of the ninth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1990,121
Query execution techniques for caching expensive methods,Joseph M Hellerstein; Jeffrey F Naughton,Abstract Object-Relational and Object-Oriented DBMSs allow users to invoke time-consuming (" expensive") methods in their queries. When queries containing theseexpensive methods are run on data with duplicate values; time is wasted redundantlycomputing methods on the same value. This problem has been studied in the context ofprogramming languages; where" memoization" is the standard solution. In the databaseliterature; sorting has been proposed to deal with this problem. We compare theseapproaches along with a third solution; a variant of unary hybrid hashing which we callHybrid Cache. We demonstrate that Hybrid Cache always dominates memoization; andsignificantly outperforms sorting in many instances. This provides new insights into thetradeoff between hashing and sorting for unary operations. Additionally; our Hybrid …,ACM SIGMOD Record,1996,120
Selectivity and cost estimation for joins based on random sampling,Peter J Haas; Jeffrey F Naughton; S Seshadri; Arun N Swami,Abstract We compare the performance of sampling-based procedures for estimating theselectivity of a join. While some of the procedures have been proposed in the databaseliterature; their relative performance has never been analyzed. A main result of this paper isa partial ordering that compares the variability of the estimators for the different proceduresafter an arbitrary fixed number of sampling steps. Prior to the current work; it was alsounknown whether these fixed-step procedures could be extended to fixed-precisionprocedures that are both asymptotically consistent and asymptotically efficient. Our secondmain result is a general method for such an extension and a proof that the method is valid forall the procedures under consideration. We show that; under plausible assumptions onsampling costs; the partial ordering of the fixed-step procedures with respect to variability …,Journal of Computer and System Sciences,1996,118
Data independent recursion in deductive databases,Jeffrey F Naughton,Abstract Some recursive definitions in deductive database systems can be replaced byequivalent nonrecursive definitions. In this paper we give a linear-time algorithm that detectsmany such definitions; and specify a useful subset of recursive definitions for which thealgorithm is complete. Recent results by Gaifman et al. in “Proceedings; 2nd. IEEE Sympos.on Logic in Comput. Sci.; June 1987;” pp. 106–115. show that the general problem isundecidable. We consider two types of initialization of the recursively defined relation:arbitrary initialization; and initialization by a given nonrecursive rule. This extends earlierwork and is related to bounded tableau results by Sagiv in “Proceedings; ACM SIGACT-SIGMOD Sympos. on Principles of Database Systems; 1985;” pp. 171–180. Even if there isno equivalent nonrecursive definition; a modification of our algorithm can be used to …,Journal of Computer and System Sciences,1989,118
Extending RDBMSs to support sparse datasets using an interpreted attribute storage format,Jennifer L Beckmann; Alan Halverson; Rajasekar Krishnamurthy; Jeffrey F Naughton," Sparse" data; in which relations have many attributes that are null for most tuples; presentsa challenge for relational database management systems. If one uses the normal"horizontal" schema to store such data sets in any of the three leading commercial RDBMS;the result is tables that occupy vast amounts of storage; most of which is devoted to nulls. Ifone attempts to avoid this storage blowup by using a" vertical" schema; the storageutilization is indeed better; but query performance is orders of magnitude slower for certainclasses of queries. In this paper; we argue that the proper way to handle sparse data is notto use a vertical schema; but rather to extend the RDBMS tuple storage format to allow therepresentation of sparse attributes as interpreted fields. The addition of interpreted storageallows for efficient and transparent querying of sparse data; uniform access to all …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,115
Efficient evaluation of right-; left-; and multi-linear rules,Jeffrey F Naughton; Raghu Ramakrishnan; Yehoshua Sagiv; Jeffrey D Ullman,Abstract We present an algorithm for the efficient evaluation of a useful subset of recursivequeries. Like the magic sets transformation; the algorithm consists of a rewriting phasefollowed by semi-naive bottom-up evaluation of the resulting rules. We prove that on a widerange of recursions; this algorithm achieves a factor of &Ogr;(n) speedup over magic sets.Intuitively; the transformations in this algorithm achieve their performance by reducing thearity of the recursive predicates in the transformed rules.,ACM SIGMOD Record,1989,113
Checkpointing multicomputer applications,Kai Li; Jeffrey F Naughton; JS Planck,The authors present a checkpointing scheme that is transparent; imposes overhead onlyduring checkpoints; requires minimal message logging; and allows for quick resumption ofexecution from a checkpointed image. Since checkpointing multicomputer applicationsposes requirements different from those posed by checkpointing general distributedsystems; existing distributed checkpointing schemes are inadequate for multicomputercheckpointing. The proposed checkpointing scheme makes use of special properties ofmulticomputer interconnection networks to satisfy this set of requirements. The proposedalgorithm is efficient both when taking checkpoints and when recovering from checkpointedimages.,Reliable Distributed Systems; 1991. Proceedings.; Tenth Symposium on,1991,110
A stochastic approach for clustering in object bases,Manolis M Tsangaris; Jeffrey F Naughton,Abstract Object clustering has long been recognized as important to the performance ofobject bases; but in most work to date; it is not clear exactly what is being optimized or howoptimal are the solutions obtained. We give a rigorous treatment of a fundamental problemin clustering: given an object base and a probabilistic description of the expected accesspatterns; what is an optimal ob-Ject clustering; and how can this optimal clustering be foundor approximated? We present a system model for the clustering problem and discuss twomodels for access patterns in the system. For the first; exact optimal clustering strategies canbe found; for the second; we show that the problem is NP-complete; but that it is an instanceof a well-studied graph partitioning problem. We propose a new clustering algorithm basedupon Kernighan's heuristic graph partitioning algorithm; and present a preliminary …,ACM SIGMOD Record,1991,110
Toward a progress indicator for database queries,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,Abstract Many modern software systems provide progress indicators for long-running tasks.These progress indicators make systems more user-friendly by helping the user quicklyestimate how much of the task has been completed and when the task will finish. However;none of the existing commercial RDBMSs provides a non-trival progress indicator for long-running queries. In this paper; we consider the problem of supporting such progressindicators. After discussing the goals and challenges inherent in this problem; we present aset of techniques sufficient for implementing a simple yet useful progress indicator for a largesubset of RDBMS queries. We report an initial implementation of these techniques inPostgreSQL.,Proceedings of the 2004 ACM SIGMOD international conference on Management of data,2004,107
Efficient sampling strategies for relational database operations,Richard J Lipton; Jeffrey F Naughton; Donovan A Schneider; Sridhar Seshadri,Abstract Recently; we have proposed an adaptive; random-sampling algorithm for generalquery size estimation in databases. In an earlier work we analyzed the asymptotic efficiencyand accuracy of the algorithm; in this paper we investigate its practicality as applied to therelational database operations select; project; and join. We extend our previous analysis toprovide significantly improved bounds on the amount of sampling necessary for a givenlevel of accuracy. Also; we provide “sanity bounds” to deal with queries for which theunderlying data are extremely skewed or the query result is very small. We investigate howthe existence of indices can be used to generate more efficient sampling algorithms for theoperations of project and join. Finally; we report on the performance of the estimationalgorithm; both as implemented in “stand alone” C programs and as implemented in a …,Theoretical Computer Science,1993,106
A status report on the OO7 OODBMS benchmarking effort,Michael J Carey; David J DeWitt; Chander Kant; Jeffrey F Naughton,Abstract The OO7 Benchmark was first published in 1993; and has since found a home inthe marketing literature of various object-oriented database management system (OODBMS)vendors. The OO7 Benchmark (as published) was the initial result of an ongoing OODBMSperformance evaluation effort at the University of Wisconsin. This paper provides an updateon the status of the effort on two fronts: single-user and multi-user. On the single-user front;we review and critique the design of the initial OO7 Benchmark. We discuss some of itsfaults; the reasons for those faults; and things that might be done to correct them. On themulti-user front; we describe our current work on the development of a multi-user benchmarkfor OODBMSs. This effort includes changes and extensions to the OO7 database and thedesign of a family of interesting multi-user workloads.,ACM SIGPLAN Notices,1994,101
Active query caching for database web servers,Qiong Luo; Jeffrey F Naughton; Rajasekar Krishnamurthy; Pei Cao; Yunrui Li,Abstract A substantial portion of web traffic consists of queries to database web servers.Unfortunately; a common technique to improve web scalability; proxy caching; is ineffectivefor database web servers because existing web proxy servers cannot cache queries. Toaddress this problem; we modify a recently proposed enhanced proxy server; called anactive proxy; to enable Active Query Caching. Our approach works by having the serversend the proxy a query applet; which can process simple queries at the proxy. This enablesthe proxy server to share the database server workload as well as to reduce the networktraffic. We show both opportunities and limitations of this approach through a performancestudy.,International Workshop on the World Wide Web and Databases,2000,100
Corleone: hands-off crowdsourcing for entity matching,Chaitanya Gokhale; Sanjib Das; AnHai Doan; Jeffrey F Naughton; Narasimhan Rampalli; Jude Shavlik; Xiaojin Zhu,Abstract Recent approaches to crowdsourcing entity matching (EM) are limited in that theycrowdsource only parts of the EM workflow; requiring a developer to execute the remainingparts. Consequently; these approaches do not scale to the growing EM need at enterprisesand crowdsourcing startups; and cannot handle scenarios where ordinary users (ie; themasses) want to leverage crowdsourcing to match entities. In response; we propose thenotion of hands-off crowdsourcing (HOC)}; which crowdsources the entire workflow of a task;thus requiring no developers. We show how HOC can represent a next logical direction forcrowdsourcing research; scale up EM at enterprises and crowdsourcing startups; and openup crowdsourcing for the masses. We describe Corleone; a HOC solution for EM; whichuses the crowd in all major steps of the EM process. Finally; we discuss the implications …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,98
Simultaneous optimization and evaluation of multiple dimensional queries,Yihong Zhao; Prasad M Deshpande; Jeffrey F Naughton; Amit Shukla,Abstract Database researchers have made significant progress on several research issuesrelated to multidimensional data analysis; including the development of fast cubingalgorithms; efficient schemes for creating and maintaining precomputed group-bys; and thedesign of efficient storage structures for multidimensional data. However; to date there hasbeen little or no work on multidimensional query optimization. Recently; Microsoft hasproposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLEDB for OLAP defines Multi-Dimensional Expressions (MDX); which have the interesting andchallenging feature of allowing clients to ask several related dimensional queries in a singleMDX expression. In this paper; we present three algorithms to optimize multiple relateddimensional queries. Two of the algorithms focus on how to generate a global plan from …,ACM Sigmod Record,1998,97
Query size estimation by adaptive sampling,Richard J Lipton; Jeffrey F Naughton,Abstract We present an adaptive; random sampling algorithm for estimating the size ofgeneral queries. The algorithm can be used for any query D over a database D such that (1)for some n; the answer to L can be partitioned into n disjoint subsets L 1; L 2;...; L n; and (2)for 1≤ i≤ n; the size of L i; is bounded by some function b (D; L); and (3) there is somealgorithm by which we can compute the size of L i; where i is chosen randomly. We considerthe performance of the algorithm on three special cases of the algorithm: join queries;transitive closure queries; and general recursive Datalog queries.,Journal of Computer and System Sciences,1995,96
The beckman report on database research,Daniel Abadi; Rakesh Agrawal; Anastasia Ailamaki; Magdalena Balazinska; Philip A Bernstein; Michael J Carey; Surajit Chaudhuri; Jeffrey Dean; AnHai Doan; Michael J Franklin; Johannes Gehrke; Laura M Haas; Alon Y Halevy; Joseph M Hellerstein; Yannis E Ioannidis; HV Jagadish; Donald Kossmann; Samuel Madden; Sharad Mehrotra; Tova Milo; Jeffrey F Naughton; Raghu Ramakrishnan; Volker Markl; Christopher Olston; Beng Chin Ooi; Christopher Ré; Dan Suciu; Michael Stonebraker; Todd Walter; Jennifer Widom,Abstract Every few years a group of database researchers meets to discuss the state ofdatabase research; its impact on practice; and important new directions. This reportsummarizes the discussion and conclusions of the eighth such meeting; held October 14-15;2013 in Irvine; California. It observes that Big Data has now become a defining challenge ofour time; and that the database research community is uniquely positioned to address it; withenormous opportunities to make transformative impact. To do so; the report recommendssignificantly more attention to five research areas: scalable big/fast data infrastructures;coping with diversity in the data management landscape; end-to-end processing andunderstanding of data; cloud services; and managing the diverse roles of people in the datalife cycle.,ACM SIGMOD Record,2014,90
Information extraction challenges in managing unstructured data,AnHai Doan; Jeffrey F Naughton; Raghu Ramakrishnan; Akanksha Baid; Xiaoyong Chai; Fei Chen; Ting Chen; Eric Chu; Pedro DeRose; Byron Gao; Chaitanya Gokhale; Jiansheng Huang; Warren Shen; Ba-Quy Vuong,Abstract Over the past few years; we have been trying to build an end-to-end system atWisconsin to manage unstructured data; using extraction; integration; and user interaction.This paper describes the key information extraction (IE) challenges that we have run into;and sketches our solutions. We discuss in particular developing a declarative IE language;optimizing for this language; generating IE provenance; incorporating user feedback into theIE process; developing a novel wiki-based user interface for feedback; best-effort IE; pushingIE into RDBMSs; and more. Our work suggests that IE in managing unstructured data canopen up many interesting research challenges; and that these challenges can greatlybenefit from the wealth of work on managing structured data that has been carried out by thedatabase community.,ACM SIGMOD Record,2009,88
Argument reduction by factoring,Jeffrey F Naughton; Raghu Ramakrishnan; JD Ullman,*,Proceedings of the 15th international conference on Very large data bases,1989,84
A scalable hash ripple join algorithm,Gang Luo; Curt J Ellmann; Peter J Haas; Jeffrey F Naughton,Abstract Recently; Haas and Hellerstein proposed the hash ripple join algorithm in thecontext of online aggregation. Although the algorithm rapidly gives a good estimate for manyjoin-aggregate problem instances; the convergence can be slow if the number of tuples thatsatisfy the join predicate is small or if there are many groups in the output. Furthermore; ifmemory overflows (for example; because the user allows the algorithm to run to completionfor an exact answer); the algorithm degenerates to block ripple join and performance suffers.In this paper; we build on the work of Haas and Hellerstein and propose a new algorithmthat (a) combines parallelism with sampling to speed convergence; and (b) maintains goodperformance in the presence of memory overflow. Results from a prototype implementationin a parallel DBMS show that its rate of convergence scales with the number of …,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,83
Accurate estimation of the cost of spatial selections,Ashraf Aboulnaga; Jeffrey F Naughton,Optimizing queries that involve operations on spatial data requires estimating the selectivityand cost of these operations. In this paper; we focus on estimating the cost of spatialselections; or window queries; where the query windows and data objects are generalpolygons. Cost estimation techniques previously proposed in the literature only handlerectangular query windows over rectangular data objects; thus ignoring the very significantcost of exact geometry comparison (the refinement step in a" filter and refine" queryprocessing strategy). The cost of the exact geometry comparison depends on the selectivityof the filtering step and the average number of vertices in the candidate objects identified bythis step. In this paper; we introduce a new type of histogram for spatial data that capturesthe complexity and size of the spatial objects as well as their location. Capturing these …,Data Engineering; 2000. Proceedings. 16th International Conference on,2000,83
Recursive XML schemas; recursive XML queries; and relational storage: XML-to-SQL query translation,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Raghav Kaushik; Jeffrey F Naughton,We consider the problem of translating XML queries into SQL when XML documents havebeen stored in an RDBMS using a schema-based relational decomposition. Surprisingly;there is no published XML-to-SQL query translation algorithm for this scenario that handlesrecursive XML schemas. We present a generic algorithm to translate path expressionqueries into SQL in the presence of recursion in the schema and queries. This algorithmhandles a general class of XML-to-relational mappings; which includes all techniquesproposed in literature. Some of the salient features of this algorithm are:(i) It translates a pathexpression query into a single SQL query; irrespective of how complex the XML schemais;(ii) It uses the" with" clause in SQL99 to handle recursive queries even over nonrecursiveschemas;(iii) It reconstructs recursive XML subtrees with a single SQL query and (iv) It …,Data Engineering; 2004. Proceedings. 20th International Conference on,2004,82
Set containment joins: The good; the bad and the ugly,Karthikeyan Ramasamy; Jignesh M Patel; Jeffrey F Naughton; Raghav Kaushik,Abstract Efficient support for set-valued attributes is likely to grow in importance as object-relational database systems; which either support set-valued attributes or propose to do sosoon; begin to replace their purely relational predecessors. One of the most interesting andchallenging operations on set-valued attributes is the set containment join; because itprovides a concise and elegant way to express otherwise complex queries. Unfortunately;evaluating these joins is difficult; and naive approaches lead to algorithms that are veryexpensive. In this paper; we develop a new partition based algorithm for set containmentjoins: the Partitioning Set Join Algorithm (PSJ); which uses a replicating multilevelpartitioning scheme based on a combination of set elements and signatures. We present adetailed performance study with a complete implementation in the Paradise object …,VLDB,2000,82
A decidable class of bounded recursions,Jeffrey F Naughton; Yehoshua Sagiv,Abstract Detecting bounded recursions is a powerful optimization technique for recursionsdatabase query languages as bounded recursions can be replaced by equivalentnonrecursive definitions. The problem is of theoretical interest because by varying the classof recursions considered one can generate instances that vary from linearly decidable to NP-hard to undecidable. In this paper we review and clarify the existing definitions ofboundedness. We then specify a sample criterion that guarantees that the condition inVaughton [7] is necessary and sufficient for boundedness. The programs satisfying thiscriterion subsume and extend previously known decidable classes of bounded linearrecursions.,Proceedings of the sixth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1987,80
Turbocharging DBMS buffer pool using SSDs,Jaeyoung Do; Donghui Zhang; Jignesh M Patel; David J DeWitt; Jeffrey F Naughton; Alan Halverson,Abstract Flash solid-state drives (SSDs) are changing the I/O landscape; which has largelybeen dominated by traditional hard disk drives (HDDs) for the last 50 years. In this paper wepropose and systematically explore designs for using an SSD to improve the performance ofa DBMS buffer manager. We propose three alternatives that differ mainly in the way that theydeal with the dirty pages evicted from the buffer pool. We implemented these alternatives; aswell another recently proposed algorithm for this task (TAC); in SQL Server; and ranexperiments using a variety of benchmarks (TPC-C; E and H) at multiple scale factors. Ourempirical evaluation shows significant performance improvements of our methods over thedefault HDD configuration (up to 9.4 X); and up to a 6.8 X speedup over TAC.,Proceedings of the 2011 ACM SIGMOD International Conference on Management of data,2011,78
Estimating the size of generalized transitive closures,Richard J Lipton; Jeffrey F Naughton,Abstract We present a framework for the estimation of the size of binary recursively dennedrelations. We show how the framework can be used to provide estimating algorithms for thesize of the transitive closure and generalizations of the transitive closure; and also show thatfor bounded degree relations; the algorithm runs in linear time. Such estimating algorithmsare essential if database systems that support recursive relations or fixpoints are to be ableto optimize queries and avoid infeasible computations.,Proceedings of the 15th Int. Conf. on Very Large Data Bases,1989,78
Predicting query execution time: Are optimizer cost models really unusable?,Wentao Wu; Yun Chi; Shenghuo Zhu; Junichi Tatemura; Hakan Hacigümüs; Jeffrey F Naughton,Predicting query execution time is useful in many database management issues includingadmission control; query scheduling; progress monitoring; and system sizing. Recently theresearch community has been exploring the use of statistical machine learning approachesto build predictive models for this task. An implicit assumption behind this work is that thecost models used by query optimizers are insufficient for query execution time prediction. Inthis paper we challenge this assumption and show while the simple approach of scaling theoptimizer's estimated cost indeed fails; a properly calibrated optimizer cost model issurprisingly effective. However; even a well-tuned optimizer cost model will fail in thepresence of errors in cardinality estimates. Accordingly we investigate the novel idea ofspending extra resources to refine estimates for the query plan after it has been chosen …,Data Engineering (ICDE); 2013 IEEE 29th International Conference on,2013,77
One-sided recursions,Jeffrey F Naughton,Abstract The performance of database systems with recursive query languages can beimproved by recognizing simple; easily evaluable classes of recursions and usingalgorithms tailored to these classes whenever possible. In this paper we identify a usefulsubset of recursive definitions; the one-sided recursions. We show how to detect one-sidedrecursions; and give a schema to evaluate selections of the form “column= constant” on arelation defined by a one-sided recursion. Instantiating this schema produces evaluationalgorithms that have simple termination conditions; maintain minimal state information; anduse selections on the recursively defined relation to restrict the tuples examined during theevaluation. We show that there are no similar algorithms for many-sided recursions. We alsoprove that it is undecidable whether an arbitrary definition has an equivalent one-sided …,Journal of Computer and System Sciences,1991,74
Active caching for multi-dimensional data sets in relational database management system,*,An “active cache”; for use by On-Line Analytic Processing (OLAP) systems; that can not onlyanswer queries that match data stored in the cache; but can also answer queries that requireaggregation or other computation of the data stored in the cache … This invention relates ingeneral to database management systems performed by computers; and in particular; to an activecache approach to caching multi-dimensional data sets for an on-line analytical processing(OLAP) system that uses a relational database management system (RDBMS) … (Note: Thisapplication references a number of different publications as indicated throughout the specificationby reference numbers enclosed in brackets; eg; [x]. A list of these different publications orderedaccording to these reference numbers can be found in the “Detailed Description of the PreferredEmbodiment” in Section 9 entitled “References.” Each of these publications is …,*,2003,72
A relational approach to incrementally extracting and querying structure in unstructured data,Eric Chu; Akanksha Baid; Ting Chen; AnHai Doan; Jeffrey Naughton,Abstract There is a growing consensus that it is desirable to query over the structure implicitin unstructured documents; and that ideally this capability should be provided incrementally.However; there is no consensus about what kind of system should be used to support thiskind of incremental capability. We explore using a relational system as the basis for aworkbench for extracting and querying structure from unstructured data. As a proof ofconcept; we applied our relational approach to support structured queries over Wikipedia.We show that the data set is always available for some form of querying; and that as it isprocessed; users can pose a richer set of structured queries. We also provide examples ofhow we can incrementally evolve our understanding of the data in the context of therelational workbench.,Proceedings of the 33rd international conference on Very large data bases,2007,70
Generating Synthetic Complex-Structured XML Data.,Ashraf Aboulnaga; Jeffrey F Naughton; Chun Zhang,Abstract Synthetically generated data has always been important for evaluating andunderstanding new ideas in database research. In this paper; we describe a data generatorfor generating synthetic complex-structured XML data that allows for a high level of controlover the characteristics of the generated data. This data generator is certainly not theultimate solution to the problem of generating synthetic XML data; but we have found it veryuseful in our research on XML data management; and we believe that it can also be usefulto other researchers. Furthermore; we hope that this paper starts a discussion in the XMLcommunity about characterizing and generating XML data; and that it may serve as a firststep towards developing a commonly accepted XML data generator for our community.,WebDB,2001,68
On differentially private frequent itemset mining,Chen Zeng; Jeffrey F Naughton; Jin-Yi Cai,Abstract We consider differentially private frequent itemset mining. We begin by exploringthe theoretical difficulty of simultaneously providing good utility and good privacy in this task.While our analysis proves that in general this is very difficult; it leaves a glimmer of hope inthat our proof of difficulty relies on the existence of long transactions (that is; transactionscontaining many items). Accordingly; we investigate an approach that begins by truncatinglong transactions; trading off errors introduced by the truncation with those introduced by thenoise added to guarantee privacy. Experimental results over standard benchmarkdatabases show that truncating is indeed effective. Our algorithm solves the" classical"frequent itemset mining problem; in which the goal is to find all itemsets whose supportexceeds a threshold. Related work has proposed differentially private algorithms for the …,Proceedings of the VLDB Endowment,2012,66
Rescheduling transactions in a database system,*,A database system includes a transaction rescheduling mechanism that reorders pendingtransactions based on whether one or more of the pending transactions conflict with anytransactions that have been scheduled for execution. In one arrangement; the receivedpending transactions are stored in a first queue. The transactions that are scheduled forexecution are stored in a second queue. The transactions are moved from the first queue tothe second queue based on the reordering by the transaction rescheduling mechanism.,*,2006,65
Toward scalable keyword search over relational data,Akanksha Baid; Ian Rae; Jiexing Li; AnHai Doan; Jeffrey Naughton,Abstract Keyword search (KWS) over relational databases has recently received significantattention. Many solutions and many prototypes have been developed. This task requiresaddressing many issues; including robustness; accuracy; reliability; and privacy. Anemerging issue; however; appears to be performance related: current KWS systems haveunpredictable running times. In particular; for certain queries it takes too long to produceanswers; and for others the system may even fail to return (eg; after exhausting memory). Inthis paper we argue that as today's users have been" spoiled" by the performance of Internetsearch engines; KWS systems should return whatever answers they can produce quicklyand then provide users with options for exploring any portion of the answer space notcovered by these answers. Our basic idea is to produce answers that can be generated …,Proceedings of the VLDB Endowment,2010,64
The case for a wide-table approach to manage sparse relational data sets,Eric Chu; Jennifer Beckmann; Jeffrey Naughton,Abstract A" sparse" data set typically has hundreds or even thousands of attributes; but mostobjects have non-null values for only a small number of these attributes. A popular viewabout sparse data is that it arises merely as the result of poor schema design. In this paper;we argue that rather than being the result of inept schema design; storing a sparse data setin a single table is the right way to proceed. However; for this to be the case; RDBMSs mustprovide sparse data management facilities that go beyond the previously studiedrequirement of storing such data sets efficiently. In particular; an RDBMS must 1) enableusers to effectively build ad hoc queries over a very large number of attributes; and 2)support efficient evaluation of these queries over a wide; sparse table. We proposetechniques that provide these capabilities; and argue that the single-table approach is a …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,64
Multi-query SQL progress indicators,Gang Luo; Jeffrey F Naughton; S Yu Philip,Abstract Recently; progress indicators have been proposed for SQL queries in RDBMSs. Allpreviously proposed progress indicators consider each query in isolation; ignoring theimpact simultaneously running queries have on each other's performance. In this paper; weexplore a multi-query progress indicator; which explicitly considers concurrently runningqueries and even queries predicted to arrive in the future when producing its estimates. Wedemonstrate that multi-query progress indicators can provide more accurate estimates thansingle-query progress indicators. Moreover; we extend the use of progress indicatorsbeyond being a GUI tool and show how to apply multi-query progress indicators to workloadmanagement. We report on an initial implementation of a multi-query progress indicator inPostgreSQL and experiments with its use both for estimating remaining query execution …,International Conference on Extending Database Technology,2006,64
A non-blocking parallel spatial join algorithm,Gang Luo; Jeffrey F Naughton; Curt J Ellmann,Interest in incremental and adaptive query processing has led to the investigation of equijoinevaluation algorithms that are non-blocking. This investigation has yielded a number ofalgorithms; including the symmetric hash join; the XJoin; the Ripple Join; and their variants.However; to our knowledge no one has proposed a nonblocking spatial join algorithm. Inthis paper; we propose a parallel non-blocking spatial join algorithm that uses duplicateavoidance rather than duplicate elimination. Results from a prototype implementation in acommercial parallel object-relational DBMS show that it generates answer tuples steadilyeven in the presence of memory overflow; and that its rate of producing answer tuples scaleswith the number of processors. Also; when allowed to run to completion; its performance iscomparable with the state-of-the-art blocking parallel spatial join algorithm.,Data Engineering; 2002. Proceedings. 18th international conference on,2002,64
Updates for structure indexes,Raghav Kaushik; Jeffrey F Naughton; Philip Bohannon; Pradeep Shenoy,This chapter illustrates efficient update algorithms for structure indexes. The problem ofindexing path queries in semistructured/XML databases has received considerable attentionrecently; and several proposals have advocated the use of structured indexes as supportingdata structures for this problem. With the rapidly increasing popularity of XML for datarepresentation; there is a lot of interest in query processing over data that conforms to alabeled-tree or labeled-graph data model. This chapter focuses on instances of structureindexes that are based on the notion of graph bisimilarity. It proposes algorithms to updatethe bismulation partition for both kinds of updates and show how they extend to indexes. Thechapter presents experiments on two real world data sets that show that their updatealgorithms are an order of magnitude faster than dropping and rebuilding the index …,*,2002,63
Architecting a network query engine for producing partial results,Jayavel Shanmugasundaram; Kristin Tufte; David DeWitt; David Maier; Jeffrey F Naughton,Abstract The growth of the Internet has made it possible to query data in all corners of theglobe. This trend is being abetted by the emergence of standards for data representation;such as XML. In face of this exciting opportunity; however; existing query engines need to bechanged in order to use them to effectively query the Internet. One of the challenges isproviding partial results of query computation; based on the initial portion of the input;because it may be undesirable to wait for all of the input. This situation is due to (a) limiteddata transfer bandwidth (b) temporary unavailability of sites and (c) intrinsically long-runningqueries (eg; continual queries or triggers). A major issue in providing partial results isdealing with non-monotonic operators; such as sort; average; negation and nest; becausethese operators need to see all of their input before they can produce the correct output …,International Workshop on the World Wide Web and Databases,2000,63
Using shared virtual memory for parallel join processing,Ambuj Shatdal; Jeffrey F Naughton,Abstract In this paper; we show that shared virtual memory; in a shared-nothingmultiprocessor; facilitates the design and implementation of parallel join processingalgorithms that perform significantly better in the presence of skew than previously proposedparallel join processing algorithms. We propose two variants of an algorithm for parallel joinprocessing using shared virtual memory; and perform a detailed simulation to investigatetheir performance. The algorithm is unique in that it employs both the shared virtual memoryparadigm and the message-passing paradigm used by current shared-nothing paralleldatabase systems. The implementation of the algorithm requires few modifications toexisting shared-nothing parallel database systems.,ACM SIGMOD Record,1993,62
On energy management; load balancing and replication,Willis Lang; Jignesh M Patel; Jeffrey F Naughton,Abstract In this paper we investigate some opportunities and challenges that arise in energy-aware computing in a cluster of servers running data-intensive workloads. We leverage theinsight that servers in a cluster are often underutilized; which makes it attractive to considerpowering down some servers and redistributing their load to others. Of course; poweringdown servers naively will render data stored only on powered down servers inaccessible.While data replication can be exploited to power down servers without losing access to data;unfortunately; care must be taken in the design of the replication and server power downschemes to avoid creating load imbalances on the remaining" live" servers. Accordingly; inthis paper we study the interaction between energy management; load balancing; andreplication strategies for data-intensive cluster computing. In particular; we show that …,ACM SIGMOD Record,2010,61
Increasing the accuracy and coverage of SQL progress indicators,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,Recently; progress indicators have been proposed for long-running SQL queries inRDBMSs. Although the proposed techniques work well for a subset of SQL queries; they arepreliminary in the sense that (1) they cannot provide non-trivial estimates for some SQLqueries; and (2) the provided estimates can be rather imprecise in certain cases. In thispaper; we consider the problem of supporting non-trivial progress indicators for a widerclass of SQL queries with more precise estimates. We present a set of techniques inachieving this goal. We report an initial implementation of these techniques in PostgreSQL.,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,61
Integrating databases and workflow systems,Srinath Shankar; Ameet Kini; David J DeWitt; Jeffrey Naughton,Abstract There has been an information explosion in fields of science such as high energyphysics; astronomy; environmental sciences and biology. There is a critical need forautomated systems to manage scientific applications and data. Database technology is well-suited to handle several aspects of workflow management. Contemporary workflow systemsare built from multiple; separately developed components and do not exploit the full power ofDBMSs in handling data of large magnitudes. We advocate a holistic view of a WFMS thatincludes not only workflow modeling but planning; scheduling; data management andcluster management. Thus; it is worthwhile to explore the ways in which databases can beaugmented to manage workflows in addition to data. We present a language for modelingworkflows that is tightly integrated with SQL. Each scientific program in a workflow is …,ACM SIGMOD Record,2005,60
Efficient XML-to-SQL query translation: Where to add the intelligence?,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract We consider the efficiency of queries generated by XML to SQL translation. We firstshow that published XML-to-SQL query translation algorithms are suboptimal in that theyoften translate simple path expressions into complex SQL queries even when much simplerequivalent SQL queries exist. There are two logical ways to deal with this problem. Onecould generate suboptimal SQL queries using a fairly naive translation algorithm; and thenattempt to optimize the resulting SQL; or one could use a more intelligent translationalgorithm with the hopes of generating efficient SQL directly. We show that optimizing theSQL after it is generated is problematic; becoming intractable even in simple scenarios; bycontrast; designing a translation algorithm that exploits information readily available attranslation time is a promising alternative. To support this claim; we present a translation …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,59
Efficiently incorporating user feedback into information extraction and integration programs,Xiaoyong Chai; Ba-Quy Vuong; AnHai Doan; Jeffrey F Naughton,Abstract Many applications increasingly employ information extraction and integration (IE/II)programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise.Hence such programs often make many IE/II mistakes; and thus can significantly benefit fromuser feedback. Today; however; there is no good way to automatically provide and processsuch feedback. When finding an IE/II mistake; users often must alert the developer team (eg;via email or Web form) about the mistake; and then wait for the team to manually examinethe program internals to locate and fix the mistake; a slow; error-prone; and frustratingprocess. In this paper we propose a solution for users to directly provide feedback and forIE/II programs to automatically process such feedback. In our solution a developer U useshlog; a declarative IE/II language; to write an IE/II program P. Next; U writes declarative …,Proceedings of the 2009 ACM SIGMOD International Conference on Management of data,2009,58
A comparison of c-store and row-store in a common framework,Alan Halverson; Jennifer L Beckmann; Jeffrey F Naughton; David J Dewitt,Abstract Recently; a “column store” system called C-Store has shown signiﬁcantperformance ben» eﬁts by utilizing storage optimizations for a ieaclnnostly query workload.The authors oi'the C-Store paper compared their 0pti~ niizecl coluinn store to a coninieicialrow store RDBMS that is optimized tor a mixture of reads and writes; which obscures therelative beneﬁts of row and column stores. In this pa~ per; we describe two storageoptimizations for a row store architecture given a read-mostly query workload—“supertuples” and “column abstraction." We implement both our opti~ mizecl row store and C-Storein a common l'ramew0rl< in order to perform an “apples to-apples” comparison of theoptimizations in isolation and combination. We also develop a detailed cost model forsequential scans to break down time spent into three categories—disk I/O; iteration cost …,University of Wisconsin-Madison; Tech. Rep. TR1570,2006,58
Fixed-precision estimation of join selectivity,Peter J Haas; Jeffrey F Naughton; S Seshadri; Arun N Swami,Abstract We compare the performance of sampling-based procedures for estimation of theselectivity of an equijoin. While some of the procedures have been proposed in thedatabase sampling literature; their relative performance has never been analyzed. A mainresult of this paper is a partial ordering that compares the variability of the estimators for thedifferent procedures after an arbitrary fixed number of sampling steps. Prior to the currentwork; it was also unknown whether these fixed-step estimation procedures can be extendedto asymptotically efficient fixed-precision estimation procedures. Our second main result is ageneral method for such an extension and a proof that the method is valid for all theestimation procedures under consideration. Finally; we show that; under reasonableassumptions on sampling costs; the partial ordering on the variability of the fixed-step …,Proceedings of the twelfth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1993,57
On the relative cost of sampling for join selectivity estimation,Peter J Haas; Jeffrey F Naughton; Arun N Swami,Abstract We compare the cost of estimating the selectivity of a “star join” using samplingprocedure t-cross to the cost of simply computing the join and obtaining the exact answer.Our bounds and approximations for the relative cost of sampling show how this costdepends on the size of the input relations; the number of input relations; and the precisioncriterion used by the estimation procedure. We also demonstrate the deleterious effect ofdangling tuples and the mixed effect of data skew on the relative cost of sampling. Theseresults provide insight into when sampling should or should not be used for join selectivityestimation.,Proceedings of the thirteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1994,55
YAWN! (Yet Another Window on NAIL!),Katherine A.  Morris; Jeffrey F.  Naughton; Yatin P.  Saraiya; Jeffrey D.  Ullman; Allen Van  Gelder,*,IEEE Data Eng. Bull.,1987,55
Nested loops revisited,David J DeWitt; Jeffrey F Naughton; Joseph Burger,Four variants of parallel index nested loop algorithms are compared with the parallel hybridhash algorithm. The conclusions of experiments both with an analytic model and with animplementation in the Gamma parallel database system are that overall; parallel hybrid hashis the method of choice; but there are cases where nested loops with index wins big enoughthat systems could profit from implementing both algorithms. The experiments show furtherthat among the nested loop algorithms; one of them; subset nested loops with sorting; clearlydominates.,Parallel and Distributed Information Systems; 1993.; Proceedings of the Second International Conference on,1993,53
Materialized view selection for multi-cube data models,Amit Shukla; Prasad M Deshpande; Jeffrey F Naughton,Abstract OLAP applications use precomputation of aggregate data to improve queryresponse time. While this problem has been well-studied in the recent database literature; toour knowledge all previous work has focussed on the special case in which all aggregatesare computed from a single cube (in a star schema; this corresponds to there being a singlefact table). This is unfortunate; because many real world applications require aggregatesover multiple fact tables. In this paper; we attempt to fill this lack of discussion about theissues arising in multi-cube data models by analyzing these issues. Then we examineperformance issues by studying the precomputation problem for multi-cube systems. Weshow that this problem is significantly more complex than the single cube precomputationproblem; and that algorithms and cost models developed for single cube precomputation …,International Conference on Extending Database Technology,2000,51
Multiprocessor main memory transaction processing,Kai Li; Jeffrey F Naughton,Abstract In this paper we describe an experiment designed to evaluate the potentialtransaction processing system performance achievable through the combination of multipleprocessors and massive memories. The experiment consisted of the design andimplementation of a transaction processing kernel on stock multiprocessors. We found thatwith sufficient memory; multiple processors can greatly improve performance. A prototypeimplementation of the kernel on a pair of Firefly multiprocessors (each with five 1-MIPprocessors) runs the standard debit-credit benchmark at over 1000 transactions per second.,Proceedings of the first international symposium on Databases in parallel and distributed systems,2000,51
Sampling issues in parallel database systems,S Seshadri; Jeffrey F Naughton,Abstract Sampling has proven useful in database systems in applications including querysize estimation; and most recently; probabilistic parallel query evaluation algorithms. Inorder to apply the full power of modern multiprocessor database systems; samplingtechniques must (1) distribute the sampling workload evenly among the processors in thesystem; and (2) make use of all the data on the pages brought into main memory during thecourse of the sampling. In this paper we show how to achieve these two goals by provingthat for query size estimation;(1) stratified random sampling guarantees perfect loadbalancing without reducing the accuracy of the estimate; and that (2) for a given number ofI/O operations; page level sampling always produces a more accurate estimate than tuplelevel sampling. For probabilistic parallel query evaluation algorithms; high performance …,International Conference on Extending Database Technology,1992,48
Counting methods for cyclic relations,Ramsey W Haddad; Jeffrey F Naughton,Abstract In this paper we consider selections of the form “column= constant” on relationsdefined by linear recursive; two rule datalog programs. In general; counting methodsperform well on such queries. However; counting methods fail in the presence of cycles inthe database. We present an algorithm in the spirit of counting methods that correctly dealswith cyclic data and has the same asymptotic running time as counting methods. Thealgorithm; which is based on reducing a query on a database to a question aboutintersections of semi-linear sets; works by using efficient methods to construct theappropriate semi-linear sets from the database and query constant.,Proceedings of the seventh ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1988,47
Learning generalized linear models over normalized data,Arun Kumar; Jeffrey Naughton; Jignesh M Patel,Abstract Enterprise data analytics is a booming area in the data management industry. Manycompanies are racing to develop toolkits that closely integrate statistical and machinelearning techniques with data management systems. Almost all such toolkits assume that theinput to a learning algorithm is a single table. However; most relational datasets are notstored as single tables due to normalization. Thus; analysts often perform key-foreign keyjoins before learning on the join output. This strategy of learning after joins introducesredundancy avoided by normalization; which could lead to poorer end-to-end performanceand maintenance overheads due to data duplication. In this work; we take a step towardsenabling and optimizing learning over joins for a common class of machine learningtechniques called generalized linear models that are solved using gradient descent …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,46
Compiling separable recursions,Jeffrey F Naughton,Abstract In this paper we consider evaluating queries on relations defined by a combinationof recursive rules. We first define separable recursions. We then give a specialized algorithmfor evaluating selections on separable recursions. Like the Generalized Magic Sets andGeneralized Counting algorithms; thus algorithm uses selection constants to avoidexamining irrelevant portions of the database; however; on some simple recursions thisalgorithm is &Ogr;(n); whereas Generalized Magic Sets is &OHgr;(n 2) and GeneralizedCounting is &OHgr;(2 n),ACM SIGMOD Record,1988,46
Computation of multidimensional aggregates,Prasad M Deshpande; Sameet Agarwal; Jeffrey F Naughton; Raghu Ramakrishnan,*,*,1996,45
Bottom-up evaluation of logic programs,Jeffrey F Naughton; Raghu Ramakrishnan,*,Computational Logic-Essays in Honor of Alan Robinson,1991,44
Storage reclamation and reorganization in client-server persistent object stores,Voon-Fee Yong; Jeffrey F Naughton; Jie-Bing Yu,The authors develop and evaluate a number of storage reclamation algorithms for client-server persistent object stores. Experience with a detailed simulation and a prototypeimplementation in the Exodus storage manager shows that one of the proposed algorithms;the Incremental Partitioned Collector; is complete; maintains transaction semantics; and canbe run incrementally and concurrently with client applications. Furthermore; it cansignificantly improve subsequent system performance by reclustering data; rendering itattractive even for systems that choose not to support automatic storage reclamation.,Data Engineering; 1994. Proceedings. 10th International Conference,1994,43
Towards predicting query execution time for concurrent and dynamic database workloads,Wentao Wu; Yun Chi; Hakan Hacígümüş; Jeffrey F Naughton,Abstract Predicting query execution time is crucial for many database management tasksincluding admission control; query scheduling; and progress monitoring. While a number ofrecent papers have explored this problem; the bulk of the existing work either considersprediction for a single query; or prediction for a static workload of concurrent queries; whereby" static" we mean that the queries to be run are fixed and known. In this paper; weconsider the more general problem of dynamic concurrent workloads. Unlike most previouswork on query execution time prediction; our proposed framework is based on analyticmodeling rather than machine learning. We first use the optimizer's cost model to estimatethe I/O and CPU requirements for each pipeline of each query in isolation; and then use acombination queueing model and buffer pool model that merges the I/O and CPU …,Proceedings of the VLDB Endowment,2013,42
Aggregate aware caching for multi-dimensional queries,Prasad M Deshpande; Jeffrey F Naughton,Abstract To date; work on caching for OLAP workloads has focussed on using cachedresults from a previous query as the answer to another query. This strategy is effective whenthe query stream exhibits a high degree of locality. It unfortunately misses the dramaticperformance improvements obtainable when the answer to a query; while not immediatelyavailable in the cache; can be computed from data in the cache. In this paper; we considerthe common subcase of answering queries by aggregating data in the cache. In order to useaggregation in the cache; one must solve two subproblems:(1) determining when it ispossible to answer a query by aggregating data in the cache; and (2) determining the fastestpath for this aggregation; since there can be many. We present two strategies—a naive oneand a Virtual Count based strategy. The virtual count based method finds if a query is …,International Conference on Extending Database Technology,2000,42
Preventing equivalence attacks in updated; anonymized data,Yeye He; Siddharth Barman; Jeffrey F Naughton,In comparison to the extensive body of existing work considering publish-once; staticanonymization; dynamic anonymization is less well studied. Previous work; most notably m-invariance; has made considerable progress in devising a scheme that attempts to preventindividual records from being associated with too few sensitive values. We show; however;that in the presence of updates; even an m-invariant table can be exploited by a new type ofattack we call the “equivalence-attack.” To deal with the equivalence attack; we propose agraph-based anonymization algorithm that leverages solutions to the classic “min-cut/max-flow” problem; and demonstrate with experiments that our algorithm is efficient and effectivein preventing equivalence attacks.,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,41
On relational support for XML publishing: beyond sorting and tagging,Surajit Chaudhuri; Raghav Kaushik; Jeffrey F Naughton,Abstract In this paper; we study whether the need for efficient XML publishing brings anynew requirements for relational query engines; or if sorting query results in the relationalengine and tagging them in middleware is sufficient. We observe that the mismatch betweenthe XML data model and the relational model requires relational engines to be enhanced forefficiency. Specifically; they need to support relation valued variables. We discuss how suchsupport can be provided through the addition of an operator; GApply; with minimalextensions to existing relational engines. We discuss how the operator may be exposed inSQL syntax and provide a comprehensive study of optimization rules that govern thisoperator. We report the results of a preliminary performance evaluation showing thespeedup obtained through our approach and the effectiveness of our optimization rules.,Proceedings of the 2003 ACM SIGMOD international conference on Management of data,2003,41
Array-based evaluation of multi-dimensional queries in object-relational database systems,Yihong Zhao; Karthikeyan Ramasamy; Kristin Tufte; Jeffrey F Naughton,Since multi-dimensional arrays are a natural data structure for supporting multi-dimensionalqueries; and object-relational (O/R) database systems support multi-dimensional array ADTs(abstract data types); it is natural to ask if a multi-dimensional array-based ADT can be usedto improve O/R DBMS performance on multi-dimensional queries. As an initial step towardanswering this question; we have implemented a multi-dimensional array in the ParadiseO/R DBMS. In this paper; we describe the implementation of this compressed-array ADT andexplore its performance for queries including star-join consolidations and selections. Weshow that; in many cases; the array ADT can provide significantly higher performance thancan be obtained by applying techniques such as bitmap indices and star-join algorithms torelational tables.,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,41
On estimating the size of projections,Jeffrey F Naughton; S Seshadri,Abstract We present a new sampling algorithm for estimating the number of tuples in theprojection of a relation. The algorithm requires no assumptions about the distributions ofvalues in the attributes of the relation and converges faster and smoother than previoussampling algorithms for the problem. We give both a sound theoretical basis for thealgorithm and experimental data from an implementation of the algorithm.,International Conference on Database Theory,1990,40
Parallelizing OODBMS traversals: a performance evaluation,David J De Witt; Jeffrey F Naughton; John C Shafer; Shivakumar Venkataraman,Abstract In this paper we describe the design and implementation of ParSets; a means ofexploiting parallelism in the SHORE OODBMS. We used ParSets to parallelize the graphtraversal portion of the OO7 OODBMS benchmark; and present speedup and scaleup resultsfrom parallel SHORE running these traversals on a cluster of commodity workstationsconnected by a standard ethernet. For some OO7 traversals; SHORE achieved excellentspeedup and scaleup; for other OO7 traversals; only marginal speedup and scaleupoccurred. The characteristics of these traversals shed light on when the ParSet approach toparallelism can and cannot be applied to speed up an application.,The VLDB Journal—The International Journal on Very Large Data Bases,1996,39
Following the paths of XML data: An algebraic framework for XML query evaluation,Leonidas Galanis; Efstratios Viglas; David J DeWitt; Jeffrey F Naughton; David Maier,Abstract This paper introduces an algebraic framework for expressing and evaluatingqueries over XML data. It presents the underlying assumptions of the framework; describesthe input and output of the algebraic operators; and defines these operators and theirsemantics. It evaluates the framework with regard to other proposed XML query algebras.Examples show that this framework is flexible enough to capture queries expressed in Quilt;one of the dominant XML query languages. We have used this algebra in the context of anInternet query engine; in which it is used to formulate logical plans for XML-QL queries. Wedefine equivalence rules that provide opportunities for optimization; and give example casesthat point out the usefulness of these rules. 1,*,2001,38
-Locking Protocols for Materialized Aggregate Join Views,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,The maintenance of materialized aggregate join views is a well-studied problem. However;the published literature has largely ignored the issue of concurrency control. Clearlyimmediate materialized view maintenance with transactional consistency; if enforced bygeneric concurrency control mechanisms; can result in low levels of concurrency and highrates of deadlock. While this problem is superficially amenable to well-known techniquessuch as fine granularity locking and special lock modes for updates that are associative andcommutative. This chapter explores that these techniques do not fully solve the problem. Itextends previous high concurrency locking techniques to apply to materialized viewmaintenance; and shows how this extension can be implemented even in the presence ofindices on the materialized view. The V locking protocol is designed to support concurrent …,*,2003,37
OODB bulk loading revisited: The partitioned-list approach,Janet L Wiener; Jeffrey F Naughton,Abstract Object-oriented and object-relational databases (OODB) need to be able to load thevast quantities of data that OODB users bring to them. Loading OODB data is signi cantlymore complicated than loading relational data due to the presence of relationships; orreferences; in the data; the presence of these relationships means that naive loadingalgorithms are slow to the point of being unusable. In our previous work; we presented thelate-invsort algorithm; which performed signi cantly better than naive algorithms on all thedata sets we tested. Unfortunately; further experimentation with the late-invsort algorithmrevealed that for large data sets (ones in which a critical data structure of the load algorithmdoes not t in memory); the performance of late-invsort rapidly degrades to where it; too; isunusable. In this paper we propose a new algorithm; the partitioned-list algorithm; whose …,VLDB,1995,37
Minimizing function-free recursive inference rules,Jeffrey F Naughton,Abstract Recursive inference rules arise in recursive definitions in logic programmingsystems and in database systems with recursive query languages. Let D be a recursivedefinition of a relation t. D is considered minimal if for any predicate p in a recursive rule inD; p must appear in a recursive rule in any definition of t. It is shown that testing forminimality is; in general; undecidable. However; an efficient algorithm for a useful class ofrecursive rules is presented; and it is used to transform a recursive definition to a minimalrecursive definition. Evaluating the minimized definition avoids redundant computationwithout the overhead of caching intermediate results and run-time checking for duplicategoals.,Journal of the ACM (JACM),1989,37
End-biased samples for join cardinality estimation,Cristian Estan; Jeffrey F Naughton,We present a new technique for using samples to estimate join cardinalities. This technique;which we term" end-biased samples;" is inspired by recent work in network trafficmeasurement. It improves on random samples by using coordinated pseudo-randomsamples and retaining the sampled values in proportion to their frequency. We show thatend-biased samples always provide more accurate estimates than random samples with thesame sample size. The comparison with histograms is more interesting―while end-biasedhistograms are somewhat better than end-biased samples for uncorrelated data sets; end-biased samples dominate by a large margin when the data is correlated. Finally; wecompare end-biased samples to the recently proposed" skimmed sketches" and show thatneither dominates the other; that each has different and compelling strengths and …,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,36
The case for a structured approach to managing unstructured data,AnHai Doan; Jeff Naughton; Akanksha Baid; Xiaoyong Chai; Fei Chen; Ting Chen; Eric Chu; Pedro DeRose; Byron Gao; Chaitanya Gokhale; Jiansheng Huang; Warren Shen; Ba-Quy Vuong,Abstract: The challenge of managing unstructured data represents perhaps the largest datamanagement opportunity for our community since managing relational data. And yet we arerisking letting this opportunity go by; ceding the playing field to other players; ranging fromcommunities such as AI; KDD; IR; Web; and Semantic Web; to industrial players such asGoogle; Yahoo; and Microsoft. In this essay we explore what we can do to improve upon thissituation. Drawing on the lessons learned while managing relational data; we outline astructured approach to managing unstructured data. We conclude by discussing thepotential implications of this approach to managing other kinds of non-relational data; and tothe identify of our field.,arXiv preprint arXiv:0909.1783,2009,35
Method for determining the computability of data for an active multi-dimensional cache in a relational database management system,*,An “active cache”; for use by On-Line Anaytic Processing (OLAP) systems; that can not only answerqueries that match data stored in the cache; but can also answer queries that require aggregationor other computation of the data stored in the cache … This invention relates in general to databasemanagement systems performed by computers; and in particular; to an active cache approachto caching multi-dimensional data sets for an on-line analytical processing (OLAP) system thatuses a relational database management system (RDBMS) … (Note: This application referencesa number of different publications as indicated throughout the specification by reference numbersenclosed in brackets; eg; [x]. A list of these different publications ordered according to these referencenumbers can be found in the “Detailed Description of the Preferred Embodiment” in Section 9entitled “References.” Each of these publications is incorporated by reference herein.),*,2004,34
An efficient checkpointing method for multicomputers with wormhole routing,Kai Li; Jeffrey F Naughton; James S Plank,Abstract Efficient checkpointing and resumption of multicomputer applications is essential ifmulticomputers are to support time-sharing and the automatic resumption of jobs after asystem failure. We present a checkpointing scheme that is transparent; imposes overheadonly during checkpoints; requires minimal message logging; and allows for quickresumption of execution from a checkpointed image. Furthermore; the checkpointingalgorithm allows each processor p to continue running the application being checkpointedexcept during the time that p is actively taking a local snapshot; and requires no global stopor freeze of the multicomputer. Since checkpointing multicomputer applications posesrequirements different from those posed by checkpointing general distributed systems;existing distributed checkpointing schemes are inadequate for multicomputer …,International Journal of Parallel Programming,1991,34
Bulk loading into an OODB: A performance study,Janet Wiener; Jerey Naughton,Object-oriented database (OODB) users bring with them large quantities of legacy data(megabytes and even gigabIn addition; scientific OODB users continually generate newdata. All this data must be loaded into the OODB. Every relational database system has aload utility; but most OODBs do not. The process of loading data into an OODB iscomplicated by inter-object references; or relationships; in the data. These relationships areexpressed in the OODB as object identifiers; which are not known at the time the load data isgenerated; they may contain cycles; and there may be implicit system-maintained inverserelationships that must also be stored. W e introduce seven algorithms for loading data intoan OODB that examine different techniques for dealing with circular and inverserelationships. W e present a performance study based on both an analytic model and an …,*,1994,32
Model selection management systems: The next frontier of advanced analytics,Arun Kumar; Robert McCann; Jeffrey Naughton; Jignesh M Patel,Abstract John Boyd recognized in the 1960's the importance of situation awareness formilitary operations and introduced the notion of the OODA loop (Observe; Orient; Decide;and Act). Today we realize that many applications have to deal with situation awareness:Customer Relationship Management; Human Capital Management; Supply ChainManagement; patient care; power grid management; and cloud services management; aswell as any IoT (Internet of Things) related application; the list seems to be endless. Situationawareness requires applications to support the management of data; knowledge; processes;and other services such as social networking in an integrated way. These applicationsadditionally require high personalization as well as rapid and continuous evolution. Theymust provide a wide variety of operational and functional requirements; including real …,ACM SIGMOD Record,2016,30
Shared computation of user-defined metrics in an on-line analytic processing system,*,An On-Line Analytic Processing (OLAP) system computes complex expressions andaggregations in queries by re-using and sharing subparts of the expressions andaggregations. A dependency generation phase performed by the OLAP system identifiesdependencies among metrics based on the expressions; aggregations; and other metricsused by the metrics. An access plan generation phase performed by the OLAP systemgenerates an access plan based on the identified dependencies; wherein the access planensures that expressions; aggregations; and metrics are computed before they are needed;and that required values and intermediate results are passed up a tree structure of theaccess plan until they are used or consumed by some operator. An operator assignmentphase performed by the OLAP system generates operators based on the access plan …,*,2003,30
Modeling entity evolution for temporal record matching,Yueh-Hsuan Chiang; AnHai Doan; Jeffrey F Naughton,Abstract Temporal record matching recognizes that if the entities represented by the recordschange over time; approaches that use temporal information may do better than approachesthat do not. Any such temporal matching method relies at its heart on a temporal model thatcaptures information about how entities evolve. In their pioneering work; Li {\it et al.} used anefficiently computable model that simply tries to predict if an attribute is expected to changeover a given time interval. In our work; we propose and evaluate a more detailed model thatfocuses on the probability that a given attribute value reappears over time. The intuition hereis that an entity might change its attribute value in the way that is dependent on its pastvalues. In addition; our model considers sets of records (rather than simply pairs of records)to improve robustness and accuracy. Experimental results show that the resulting …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,27
GSLPI: A cost-based query progress indicator,Jiexing Li; Rimma V Nehme; Jeffrey Naughton,Progress indicators for SQL queries were first published in 2004 with the simultaneous andindependent proposals from Chaudhuri et al. and Luo et al. In this paper; we implement bothprogress indicators in the same commercial RDBMS to investigate their performance. Wesummarize common cases in which they are both accurate and cases in which they fail toprovide reliable estimates. Although there are differences in their performance; much morestriking is the similarity in the errors they make due to a common simplifying uniform futurespeed assumption. While the developers of these progress indicators were aware that thisassumption could cause errors; they neither explored how large the errors might be nor didthey investigate the feasibility of removing the assumption. To rectify this we propose a newquery progress indicator; similar to these early progress indicators but without the uniform …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,27
Method and apparatus for performing hash join,*,A parallel hash ripple join algorithm partitions tuples of two relations for localizedprocessing. The algorithm is non-blocking and may be performed in a parallel; multi-processor environment. At each processing node; the tuples are further partitioned such thatjoin operations may be performed as tuples are redistributed to each node during thepartitioning.,*,2006,27
Resource allocation and scheduling for mixed database workloads,Kurt P Brown; Michael J Carey; David J DeWitt; Manish Mehta; J Naughton,Some of the tradeoffs examined in this paper are complex and non-intuitive. As an example;consider the problem of memory allocation for a workload that consists of a single long-running query containing one or more joins; running concurrently with a stream of small;"rifle shot" transactions. Assume that each small transaction accesses a single tuple from acommon relation via an index lockup; and that the join query accesses separate relations; sothere is no data,*,1992,27
Simultaneous computation of multiple moving aggregates in a relational database management system,*,An On-Line Analytic Processing (OLAP) system identifies a plurality of simultaneouslycomputable moving aggregate functions in a query. The identified moving aggregatefunctions are then partitioned into sets that can be computed simultaneously based onequivalent sort expressions. Finally; the OLAP system generates an access plan thatexecutes the partitioned sets simultaneously.,*,2003,26
Schema matching using interattribute dependencies,Jaewoo Kang; Jeffrey F Naughton,Schema matching is one of the key challenges in information integration. It is a labor-intensive and time-consuming process. To alleviate the problem; many automated solutionshave been proposed. Most of the existing solutions mainly rely upon textual similarity of thedata to be matched. However; there exist instances of the schema matching problem forwhich they do not even apply. Such problem instances typically arise when the columnnames in the schemas and the data in the columns are opaque or very difficult to interpret. Inour previous work [36] we proposed a two-step technique to address this problem. In the firststep; we measure the dependencies between attributes within tables using an information-theoretic measure and construct a dependency graph for each table capturing thedependencies among attributes. In the second step; we find matching node pairs across …,IEEE Transactions on Knowledge and Data Engineering,2008,25
Cubing Algorithms; Storage Estimation; and Storage and Processing Alternatives for OLAP.,Prasad Deshpande; Jeffrey F.  Naughton; Karthikeyan Ramasamy; Amit Shukla; Kristin Tufte; Yihong Zhao,CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …,Data Engineering Bulletin,1997,24
Impact of disk corruption on open-source DBMS,Sriram Subramanian; Yupu Zhang; Rajiv Vaidyanathan; Haryadi S Gunawi; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Jeffrey F Naughton,Despite the best intentions of disk and RAID manufacturers; on-disk data can still becomecorrupted. In this paper; we examine the effects of corruption on database managementsystems. Through injecting faults into the MySQL DBMS; we find that in certain cases;corruption can greatly harm the system; leading to untimely crashes; data loss; or evenincorrect results. Overall; of 145 injected faults; 110 lead to serious problems. More detailedobservations point us to three deficiencies: MySQL does not have the capability to detectsome corruptions due to lack of redundant information; does not isolate corrupted data fromvalid data; and has inconsistent reactions to similar corruption scenarios. To detect andrepair corruption; a DBMS is typically equipped with an offline checker. Unfortunately; theMySQL offline checker is not comprehensive in the checks it performs; misdiagnosing …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,23
Putting XML query algebras into context,Stratis D Viglas; Leonidas Galanis; David J DeWitt; David Maier; JF Naughtonn,*,submitted for publication,2002,23
Exploring provenance in a distributed job execution system,Christine F Reilly; Jeffrey F Naughton,Abstract We examine provenance in the context of a distributed job execution system. It iscrucial to capture provenance information during the execution of a job in a distributedenvironment because often this information is lost once the job has finished. In this paper wediscuss the type of information that is available within a distributed job execution system;how to capture such information; and what the burdens on the user and system are whensuch information is captured. We identify what we think is the key data that must be capturedand discuss the collection of provenance in the Quill++ project of Condor. Our conclusion isthat it is possible to capture important provenance information in a distributed job executionsystem with relatively little intrusion on the user or the system.,International Provenance and Annotation Workshop,2006,22
Approximating streamingwindow joins under cpu limitations,Ahmed Ayad; Jeffrey Naughton; Stephen Wright; Utkarsh Srivastava,Data streaming systems face the possibility of having to shed load in the case of CPU ormemory resource limitations. We study the CPU limited scenario in detail. First; we proposea new model for the CPU cost. Then we formally state the problem of shedding load for thegoal of obtaining the maximum possible subset of the complete answer; and propose anonline strategy for semantic load shedding. Moving on to random load shedding; we discussrandom load shedding strategies that decouple the window maintenance and tupleproduction operations of the symmetric hash join; and prove that one of them—Probe-No-Insert—always dominates the previously proposed coin flipping strategy.,Data Engineering; 2006. ICDE'06. Proceedings of the 22nd International Conference on,2006,22
Efficient exception handling during access plan execution in an on-line analytic processing system,*,An On-Line Analytic Processing (OLAP) system converts queries into an operator treecomprised of a plurality of operators; wherein each of the operators is independentlyexecuted. When an exception is detected in one or more of the independently executedoperators; the exception is propagated to the remaining operators of the operator tree byrecycling empty data pages; piggybacking the detected exceptions on the recycled datapages; and sending the recycled data pages both upstream and downstream in the operatortree to the remaining operators of the operator tree. The propagated exceptions aredelivered to the remaining operators; without interrupting the operators; at one or morepoints at which the operator normally reads or writes data from its input stream. Finally; theoperators are terminated using a depth-first traversal of the operator tree.,*,2004,22
A comparison of three methods for join view maintenance in parallel RDBMS,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,In a typical data warehouse; materialized views are used to speed up query execution. Uponupdates to the base relations in the warehouse; these materialized views must also bemaintained. The need to maintain these materialized views can have a negative impact onperformance that is exacerbated in parallel RDBMSs; since simple single-node updates tobase relations can give rise to expensive all-node operations for materialized viewmaintenance. We present a comparison of three materialized join view maintenancemethods in a parallel RDBMS; which we refer to as the naive; auxiliary relation; and globalindex methods. The last two methods improve performance at the cost of using more space.The results of this study show that the method of choice depends on the environment; inparticular; the update activity on base relations and the amount of available storage …,Data Engineering; 2003. Proceedings. 19th International Conference on,2003,22
Shoring up persistent objects,M Carey; Dr DeWitt; Mr Franklin; Nu Hall; M McAuliffe; J Naughton; D Schuh; M Solomon; C Tan; O Tsatalos; S White; M Zwilling,*,Proc. of the 1994 SIGMOD Conference,1994,22
To join or not to join?: Thinking twice about joins before feature selection,Arun Kumar; Jeffrey Naughton; Jignesh M Patel; Xiaojin Zhu,Abstract Closer integration of machine learning (ML) with data processing is a booming areain both the data management industry and academia. Almost all ML toolkits assume that theinput is a single table; but many datasets are not stored as single tables due tonormalization. Thus; analysts often perform key-foreign key joins to obtain features from allbase tables and apply a feature selection method; either explicitly or implicitly; with the aimof improving accuracy. In this work; we show that the features brought in by such joins canoften be ignored without affecting ML accuracy significantly; ie; we can" avoid joins safely."We identify the core technical issue that could cause accuracy to decrease in some casesand analyze this issue theoretically. Using simulations; we validate our analysis andmeasure the effects of various properties of normalized data on accuracy. We apply our …,Proceedings of the 2016 International Conference on Management of Data,2016,21
On the complexity of privacy-preserving complex event processing,Yeye He; Siddharth Barman; Di Wang; Jeffrey F Naughton,Abstract Complex Event Processing (CEP) Systems are stream processing systems thatmonitor incoming event streams in search of userspecified event patterns. While CEPsystems have been adopted in a variety of applications; the privacy implications of eventpattern reporting mechanisms have yet to be studied-a stark contrast to the significantamount of attention that has been devoted to privacy for relational systems. In this paper wepresent a privacy problem that arises when the system must support desired patterns (thosethat should be reported if detected) and private patterns (those that should not be revealed).We formalize this problem; which we term privacy-preserving; utility maximizing CEP (PP-CEP); and analyze its complexity under various assumptions. Our results show that this is arich problem to study and shed some light on the difficulty of developing algorithms that …,Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2011,21
Memory management for scalable web data servers,Shivakumar Venkataraman; Miron Livny; Jeffrey F Naughton,Popular Web sites are already experiencing very heavy loads; and these loads will onlyincrease as the number of users accessing them grows. These loads create both CPU andI/O bottlenecks. One promising solution already being employed to eliminate the CPUbottleneck is to replace a single processor server with a cluster of servers. Our goal in thispaper is to develop buffer management algorithms that exploit the aggregate memorycapacity of the machines in such a server cluster to attack the I/O bottleneck. The keychallenge in designing such buffer management algorithms turns out to be controlling datareplication so as to achieve a good balance between intra-cluster network traffic and diskI/O. At one extreme; the straightforward application of client-server memory managementtechniques to this cluster architecture causes duplication in memory among the servers …,Data Engineering; 1997. Proceedings. 13th International Conference on,1997,21
On the complexity of join predicates,Jin-Yi Cai; Venkatesan T Chakaravarthy; Raghav Kaushik; Jeffrey F Naughton,Abstract We consider the complexity of join problems; focusing on equijoins; spatial-overlapjoins; and set-containment joins. We use a graph pebbling model to characterize these joinscombinatorially; by the length of their optimal pebbling strategies and computationally; bythe complexity of discovering these strategies. Our results show that equijoins are theeasiest of all joins; with optimal pebbling strategies that meet the lower bound over all joinproblems and that can be found in linear time. By contrast; spatial-overlap and set-containment joins are the hardest joins; with instances where optimal pebbling strategiesreach the upper bound over all join problems and with the problem of discovering optimalpebbling strategies being NP-complete. For set-containment joins; we show that discoveringthe optimal pebbling is also MAX-SNP-Complete. As a consequence; we show that …,Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2001,20
Auxiliary relation for materialized view,*,Auxiliary relations are used to maintain a materialized view. The materialized viewcomprises results of some operation performed between two or more base relations of aparallel database management system comprising a plurality of nodes. The two or morebase relations are distributed to the nodes according to some partitioning strategy. Eachauxiliary relation is partitioned according to the join attribute in the materialized viewdefinition. During join or other operations involving the relations; the auxiliary relations; notthe base relations; are used to construct join results and to maintain the materialized view.,*,2010,19
On the difficulty of finding optimal relational decompositions for xml workloads: A complexity theoretic perspective,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Jeffrey F Naughton,Abstract A key problem that arises in the context of storing XML documents in relationaldatabases is that of finding an optimal relational decomposition for a given set of XMLdocuments and a given set of XML queries over those documents. While there have been anumber of ad hoc solutions proposed for this problem; to our knowledge this paperrepresents a first step toward formalizing the problem and studying its complexity. It turns outthat to even define what one means by an optimal decomposition; one first needs to specifyan algorithm to translate XML queries to relational queries; and a cost model to evaluate thequality of the resulting relational queries. By examining an interesting problem embedded inchoosing a relational decomposition; we show that choices of different translation algorithmsand cost models result in very different complexities for the resulting optimization …,International Conference on Database Theory,2003,19
A software-defined networking based approach for performance management of analytical queries on distributed data stores,Pengcheng Xiong; Hakan Hacigumus; Jeffrey F Naughton,Abstract Nowadays data analytics applications are accessing more and more data fromdistributed data stores; creating a large amount of data traffic on the network. Therefore;distributed analytic queries are prone to suffer from poor performance when they encounternetwork contention; which can be quite common in a shared network. Typical distributedquery optimizers do not have a way to solve this problem because they treat the network asa black-box: they are unable to monitor it; let alone control it. With the new era of software-defined networking (SDN); we show how SDN can be effectively exploited for performancemanagement for analytical queries in distributed data store environments. More specifically;we present a group of methods to leverage SDN's visibility into and control of the network'sstate that enable distributed query processors to achieve performance improvements and …,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,18
Form-based proxy caching for database-backed web sites: keywords and functions,Qiong Luo; Jeffrey F Naughton; Wenwei Xue,Abstract Web caching proxy servers are essential for improving web performance andscalability; and recent research has focused on making proxy caching work for database-backed web sites. In this paper; we explore a new proxy caching framework that exploits thequery semantics of HTML forms. We identify two common classes of form-based queriesfrom real-world database-backed web sites; namely; keyword-based queries and function-embedded queries. Using typical examples of these queries; we study two representativecaching schemes within our framework:(i) traditional passive query caching; and (ii) activequery caching; in which the proxy cache can service a request by evaluating a query overthe contents of the cache. Results from our experimental implementation show that our form-based proxy is a general and flexible approach that efficiently enables active caching …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,18
Uncertainty aware query execution time prediction,Wentao Wu; Xi Wu; Hakan Hacigümüş; Jeffrey F Naughton,Abstract Predicting query execution time is a fundamental issue underlying many databasemanagement tasks. Existing predictors rely on information such as cardinality estimates andsystem performance constants that are difficult to know exactly. As a result; accurateprediction still remains elusive for many queries. However; existing predictors provide asingle; point estimate of the true execution time; but fail to characterize the uncertainty in theprediction. In this paper; we take a first step towards providing uncertainty information alongwith query execution time predictions. We use the query optimizer's cost model to representthe query execution time as a function of the selectivities of operators in the query plan aswell as the constants that describe the cost of CPU and I/O operations in the system. Bytreating these quantities as random variables rather than constants; we show that with low …,Proceedings of the VLDB Endowment,2014,17
Clocked adversaries for hashing,Richard J Lipton; Jeffrey F Naughton,Abstract A “clocked adversary” is a program that can time its operations and base itsbehavior on the results of those timings. While it is well known that hashing performs poorlyin the worst case; recent results have proven that; for reference-string programs; theprobability of falling into a bad case can be driven arbitrarily low. We show that this is nottrue for clocked adversaries. This emphasizes the limits on the appiicability of theorems onthe behavior of hashing schemes on reference string programs; and raises a novel set ofproblems dealing with optimality of and vulnerability to clocked adversaries.,Algorithmica,1993,17
Transparently Gathering Provenance with Provenance Aware Condor.,Christine F Reilly; Jeffrey F Naughton,Abstract We observed that the Condor batch execution system exposes a lot of informationabout the jobs that run in the system. This observation led us to explore whether this systeminformation could be used for provenance. The result of our explorations is ProvenanceAware Condor (PAC); a system that transparently gathers provenance while jobs run inCondor. Transparent provenance gathering requires that the application not be altered inorder to run in the provenance system. This requirement allows any application that can runin Condor to also run in PAC. Through SQL queries; PAC is able to answer a wide range ofquestions about the files used by a job and the machines that execute jobs.,Workshop on the Theory and Practice of Provenance,2009,16
Non-blocking parallel band join algorithm,*,A non-blocking parallel band join method and apparatus partitions tuples of two relations forlocalized processing. At each processing node; the tuples are further partitioned such thatjoin operations may be performed efficiently; as tuples are received by the node during thepartitioning.,*,2004,16
Parallel spatial join index,*,A parallel spatial join index is used for efficient join operations involving spatial data;including polygon data. The join operations between spatial data from two different tablesare performed in parallel in a multi-processor environment. Join indices are created; basedupon spatial object attributes from the two tables. Auxiliary relations are constructed from thetwo tables as well; and further include data for avoiding duplicate join results. R-trees indexthe auxiliary relations and B-trees index the join indices.,*,2004,16
Magellan: Toward building entity matching management systems,Pradap Konda; Sanjib Das; Paul Suganthan GC; AnHai Doan; Adel Ardalan; Jeffrey R Ballard; Han Li; Fatemah Panahi; Haojun Zhang; Jeff Naughton; Shishir Prasad; Ganesh Krishnan; Rohit Deep; Vijay Raghavendra,Abstract Entity matching (EM) has been a long-standing challenge in data management.Most current EM works focus only on developing matching algorithms. We argue that farmore efforts should be devoted to building EM systems. We discuss the limitations of currentEM systems; then present as a solution Magellan; a new kind of EM systems. Magellan isnovel in four important aspects.(1) It provides how-to guides that tell users what to do in eachEM scenario; step by step.(2) It provides tools to help users do these steps; the tools seek tocover the entire EM pipeline; not just matching and blocking as current EM systems do.(3)Tools are built on top of the data analysis and Big Data stacks in Python; allowing Magellanto borrow a rich set of capabilities in data cleaning; IE; visualization; learning; etc.(4)Magellan provides a powerful scripting environment to facilitate interactive …,Proceedings of the VLDB Endowment,2016,15
Tracking entities in the dynamic world: A fast algorithm for matching temporal records,Yueh-Hsuan Chiang; AnHai Doan; Jeffrey F Naughton,Abstract Identifying records referring to the same real world entity over time enableslongitudinal data analysis. However; difficulties arise from the dynamic nature of the world:the entities described by a temporal data set often evolve their states over time. While thestate of the art approach to temporal entity matching achieves high accuracy; this approachis computationally expensive and cannot handle large data sets. In this paper; we presentan approach that achieves equivalent matching accuracy but takes far less time. Our keyinsight is" static first; dynamic second." Our approach first runs an evidence-collection pass;grouping records without considering the possibility of entity evolution; as if the world were"static." Then; it merges clusters from the initial grouping by determining whether an entitymight evolve from the state described in one cluster to the state described in another …,Proceedings of the VLDB Endowment,2014,15
An overview of Quill: A passive operational data logging system for Condor,Jiansheng Huang; Ameet Kini; Erik Paulson; Christine Reilly; Eric Robinson; Srinath Shankar; Lakshmikant Shrinivas; David DeWitt; Jeffrey Naughton,*,Computer Sciences Technical Report; University of Wisconsin,2007,15
Transaction reordering and grouping for continuous data loading,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,Abstract With the increasing popularity of operational data warehousing; the ability to loaddata quickly and continuously into an RDBMS is becoming more and more important.However; in the presence of materialized join views; loading data concurrently into multiplebase relations of the same materialized join view can cause a severe deadlock problem. Tosolve this problem; we propose reordering the data to be loaded so that at any time; for anymaterialized join view; data is only loaded into one of its base relations. Also; for loadtransactions on the relations that contain “aggregate” attributes; we propose using pre-aggregation to reduce the number of SQL statements in the load transactions. Theadvantages of our methods are demonstrated through experiments with a commercialparallel RDBMS.,International Workshop on Business Intelligence for the Real-Time Enterprise,2006,15
The impact of data placement on memory management for multi-server OODBMS,Shivakumar Venkataraman; Miron Livny; Jeffrey F Naughton,We demonstrate the close relationship between data placement and memory managementfor symmetric multi-server OODBMS. We propose and investigate memory managementalgorithms for two data placement strategies; namely declustering and clustering. Through adetailed simulation; we show that by declustering the data most of the benefits of complexglobal memory management algorithms are realized by simple algorithms. In contrast weshow that when data is clustered; the simple algorithms perform poorly.,Data Engineering; 1995. Proceedings of the Eleventh International Conference on,1995,15
Dynamic memory hybrid hash join,D DeWitt; J Naughton,*,Computer Sciences Department; Technical Report; University of Wisconsin-Madison,1995,15
Concurrent real-time checkpoint for parallel programs,Kai Li; J Naughton; James Plank,*,Proceedings of the 2nd ACM SIGPLAN Symposium on Princiles & Practice of Parallel Programming,1990,15
Locking mechanism using a predefined lock for materialized views in a database system,*,A database system includes a locking mechanism for a materialized view defined on baserelations. In response to updates of a base relation; a predefined lock (eg; Y lock or IY lock)is placed on the materialized view. With the locking mechanism; transaction concurrency isenhanced by enabling concurrent updates of a materialized view by plural transactions incertain cases.,*,2006,14
Optimizing function-free recursive inference rules,Jeffrey F Naughton,Abstract Recursive inference rules arise in recursive definitions in logic programmingsystems and in database systems with recursive query languages. Let D be a recursivedefinition of a relation t. We say that D is minimal if for any predicate p in a recursive rule inD; p must appear in a recursive rule in any definition of t. We show that testing for minimalityis in general undecidable. However; we do present an efficient algorithm for a useful class ofrecursive rules; and show how to use it to transform a recursive definition to a minimalrecursive definition. Evaluating the optimized definition will avoid redundant computationwithout the overhead of caching intermediate results and run-time checking for duplicategoals.,*,1986,14
Redundancy in Function-Free Recursive Rules.,Jeffrey F Naughton,*,SLP,1986,14
Partial results in database systems,Willis Lang; Rimma V Nehme; Eric Robinson; Jeffrey F Naughton,Abstract As the size and complexity of analytic data processing systems continue to grow;the effort required to mitigate faults and performance skew has also risen. However; in someenvironments we have encountered; users prefer to continue query execution even in thepresence of failures (eg; the unavailability of certain data sources); and receive a" partial"answer to their query. We explore ways to characterize and classify these partial results; anddescribe an analytical framework that allows the system to perform coarse to fine-grainedanalysis to determine the semantics of a partial result. We propose that if the system isequipped with such a framework; in some cases it is better to return and explain partialresults than to attempt to avoid them.,Proceedings of the 2014 ACM SIGMOD international conference on Management of data,2014,13
Locking mechanism for views associated with B-tree indexes,*,A database system includes a locking mechanism for a view defined on one or more baserelations. A B-tree index is defined on the view. In response to updates of a base relation;predefined locks are placed on the view. With locking mechanisms according to someembodiments of the invention; concurrency of transactions that access the view and B-treeindex is enhanced by enabling concurrent updates of the materialized view by pluraltransactions in certain cases; while avoiding read-write and write-read conflicts.,*,2007,13
Set containment join operation in an object/relational database management system,*,A novel partition-based set containment join algorithm; known as Set Partitioning Algorithm(SPA); is performed by a relational database management system to aggressively partitionset-valued attributes into a very large number of partitions; in order to minimize the impact ofexcessive replication and improve performance.,*,2004,13
Remote load-sensitive caching for multi-server database systems,Shivakumar Venkataraman; Jeffrey F Naughton; Miron Livny,The recent dramatic improvements in the performance of commodity hardware has madeclusters of workstations or PCs an attractive and economical platform upon which to buildscalable database servers. These clusters have large aggregate memory capacities;however; since this global memory is distributed; good algorithms are necessary for memorymanagement; or this large aggregate memory will go underutilized. The goal of the study isto develop and evaluate buffer management algorithms for database clusters. We propose anew buffer management algorithm; remote load sensitive caching (RLS caching); that usesnovel techniques to combine data placement with a simple modification of standard clientserver page replacement algorithms to approximate a global LRU page replacement policy.Through an implementation in the SHORE database system; we evaluate the …,Data Engineering; 1998. Proceedings.; 14th International Conference on,1998,13
On the Performance of an Array-Based ADT for OLAP Workloads,Yihong Zhao; Kristin Tufte; Jeffrey F Naughton,Abstract There is currently a debate among OLAP vendors on the best way to provide OLAPfunctionality: Relational OLAP (ROLAP) vendors advocate using sophisticated front ends toprovide a multidimensional view of a standard relational database; whereasMultidimensional OLAP (MOLAP) vendors provide custom servers that generally store theirdata as arrays (instead of tables.) An important question in this debate is the relativeperformance of arrays vs. tables for common OLAP operations. To shed some light on thisquestion; we have implemented a MOLAP Abstract Data Type (ADT); which uses amultidimensional array as its principle storage mechanism; within the Paradise object-relational DBMS. Using this implementation; we have studied MOLAP and ROLAPperformance on the same DBMS platform (by using either the MOLAP ADT or standard …,*,1996,13
Processing aggregates in parallel database systems,Ambuj Shatdal; Jeffrey F Naughton,Abstract Aggregates are rife in real life SQL queries. However; in the parallel queryprocessing literature aggregate processing has received surprisingly little attention;furthermore; the way current parallel database systems do aggregate processing is far fromoptimal in many scenarios. We describe two hashing based algorithms for parallelevaluation of aggregates. A performance analysis via an analytical model and animplementation on the Intel Paragon multi-computer shows that each works well for someaggregation selectivities but poorly for the remaining. Fortunately; where one does poorlythe other does well and vice-versa. Thus; the two together cover all possible selectivities. Weshow how; using sampling; an optimizer can decide which of the two algorithms to use for aparticular query. Finally; we investigate the impact of data skew on the performance of …,*,1994,13
On load shedding in complex event processing,Yeye He; Siddharth Barman; Jeffrey F Naughton,Abstract: Complex Event Processing (CEP) is a stream processing model that focuses ondetecting event patterns in continuous event streams. While the CEP model has gainedpopularity in the research communities and commercial technologies; the problem ofgracefully degrading performance under heavy load in the presence of resource constraints;or load shedding; has been largely overlooked. CEP is similar to" classical" stream datamanagement; but addresses a substantially different class of queries. This unfortunatelyrenders the load shedding algorithms developed for stream data processing inapplicable. Inthis paper we study CEP load shedding under various resource constraints. We formalizebroad classes of CEP load-shedding scenarios as different optimization problems. Wedemonstrate an array of complexity results that reveal the hardness of these problems …,arXiv preprint arXiv:1312.4283,2013,12
Argument reduction by factoring,Jeffrey F Naughton; R Ramakrishnana; Yehoshua Sagiv; Jeffrey D Ullman,Abstract We identify a useful property of a program with respect to a predicate;calledfactoring. While we prove that detecting factorability is undecidable in general; weshow that for a large class of programs; the program obtained by applying the Magic Setstransformation is factorable with respect to the recursive predicate. When the factoringproperty holds; a simple optimization of the program generated by the Magic Setstransformation results in a new program that is never less efficient than the Magic Setsprogram and is often dramatically more efficient; due to the reduction of the arity of therecursive predicate. We show that the concept of factoring generalizes some previouslyidentified special cases of recursions; including separable recursions and right-and left-linear recursions.,Theoretical Computer Science,1995,12
Parsets for parallelizing OODBMS traversals: Implementation and performance,David J De Witt; Jeffrey F Naughton; John C Shafer; Shivakumar Venkataraman,Describes the design and implementation of ParSets; a means of exploiting parallelism inthe SHORE persistent object store. We used ParSets to create and parallelize the graphtraversals of the OO7 OODBMS benchmark; and present speedup and scaleup results fromparallel SHORE running these traversals on a cluster of commodity workstations connectedby a standard Ethernet. For some OO7 traversals; SHORE achieved excellent speedup andscaleup; for other OO7 traversals; only marginal speedup and scaleup occurred. Thecharacteristics of these traversals shed light on when the ParSet approach to parallelism canand cannot be applied to speed up an application.,Parallel and Distributed Information Systems; 1994.; Proceedings of the Third International Conference on,1994,12
On the expected size of recursive Datalog queries,S Seshadri; Jeffrey F Naughton,Abstract We present asymptotically exact expressions for the expected sizes of relationsdefined by two well-studied Datalog recursions; namely the “same generation” and“canonical factorable recursion”. We consider the size of the fixpoints of the recursivelydefined relations in the above programs; as well as the size of the fixpoints of the relationsdefined by the rewritten programs generated by the Magic Sets and Factoring rewritingalgorithms in response to selection queries. Our results show that even over relativelysparse base relations; the recursively defined relations are within a small constant factor oftheir worst-case size bounds; and that the Magic Sets rewriting algorithm on the averageproduces relations within a small constant factor of the corresponding bounds for therecursion without rewriting. The expected size of relations produced by the Factoring …,Proceedings of the tenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems,1991,12
Sampling-based query re-optimization,Wentao Wu; Jeffrey F Naughton; Harneet Singh,Abstract Despite of decades of work; query optimizers still make mistakes on" difficult"queries because of bad cardinality estimates; often due to the interaction of multiplepredicates and correlations in the data. In this paper; we propose a low-cost post-processingstep that can take a plan produced by the optimizer; detect when it is likely to have madesuch a mistake; and take steps to fix it. Specifically; our solution is a sampling-based iterativeprocedure that requires almost no changes to the original query optimizer or queryevaluation mechanism of the system. We show that this indeed imposes low overhead andcatches cases where three widely used optimizers (PostgreSQL and two commercialsystems) make large errors.,Proceedings of the 2016 International Conference on Management of Data,2016,11
Utility-maximizing event stream suppression,Di Wang; Yeye He; Elke Rundensteiner; Jeffrey F Naughton,Abstract Complex Event Processing (CEP) has emerged as a technology for monitoringevent streams in search of user specified event patterns. When a CEP system is deployed insensitive environments the user may wish to mitigate leaks of private information whileensuring that useful nonsensitive patterns are still reported. In this paper we consider how tosuppress events in a stream to reduce the disclosure of sensitive patterns while maximizingthe detection of nonsensitive patterns. We first formally define the problem of utility-maximizing event suppression with privacy preferences; and analyze its computationalhardness. We then design a suite of real-time solutions to solve this problem. Our firstsolution optimally solves the problem at the event-type level. The second solution; at theevent-instance level; further optimizes the event-type level solution by exploiting runtime …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,11
On Transactional Memory; Spinlocks; and Database Transactions.,Khai Q Tran; Spyros Blanas; Jeffrey F Naughton,ABSTRACT Currently; hardware trends include a move toward multicore processors; cheapand persistent variants of memory; and even sophisticated hardware support for mutualexclusion in the form of transactional memory. These trends; coupled with a growing desirefor extremely high performance on short database transactions; raise the question ofwhether the hardware primitives developed for mutual exclusion can be exploited to rundatabase transactions. In this paper; we present a preliminary exploration of this question.We conduct a set of experiments on both a hardware prototype and a simulator of a multi-core processor with Transactional Memory (TM.) Our results show that TM is attractive underlow contention workloads; while spinlocks can tolerate high contention workloads well; andthat in some cases these approaches can beat a simple implementation of a traditional …,ADMS@ VLDB,2010,11
Differentially private stochastic gradient descent for in-RDBMS analytics,Xi Wu; Arun Kumar; Kamalika Chaudhuri; Somesh Jha; Jeffrey F Naughton,Abstract: In-RDBMS data analysis has received considerable attention in the past decadeand has been widely used in sensitive domains to extract patterns in data using machinelearning. For these domains; there has also been growing concern about privacy; anddifferential privacy has emerged as the gold standard for private data analysis. However;while differentially private machine learning and in-RDBMS data analytics have beenstudied separately; little previous work has explored private learning in an in-RDBMSsystem. This work considers a specific algorithm---stochastic gradient descent (SGD) fordifferentially private machine learning---and explores how to integrate it into an RDBMSsystem. We find that previous solutions on differentially private SGD have characteristics thatrender them unattractive for an RDBMS implementation. To address this; we propose an …,arXiv preprint arXiv:1606.04722,2016,10
Distribution-based query scheduling,Yun Chi; Hakan Hacígümüş; Wang-Pin Hsiung; Jeffrey F Naughton,Abstract Query scheduling; a fundamental problem in database management systems; hasrecently received a renewed attention; perhaps in part due to the rise of the" database as aservice"(DaaS) model for database deployment. While there has been a great deal of workinvestigating different scheduling algorithms; there has been comparatively little workinvestigating what the scheduling algorithms can or should know about the queries to bescheduled. In this work; we investigate the efficacy of using histograms describing thedistribution of likely query execution times as input to the query scheduler. We propose anovel distribution-based scheduling algorithm; Shepherd; and show that Shepherdsubstantially outperforms state-of-the-art point-based methods through extensiveexperimentation with both synthetic and TPC workloads.,Proceedings of the VLDB Endowment,2013,10
Database support for matching: limitations and opportunities,Ameet Kini; Srinath Shankar; Jeffrey F Naughton; David J Dewitt,Abstract We define a match join of R and S with predicate θ to be a subset of the θ-join of Rand S such that each tuple of R and S contributes to at most one result tuple. Match joinsand their generalizations belong to a broad class of matching problems that have attracted agreat deal of attention in disciplines including operations research and theoretical computerscience. Instances of these problems arise in practice in resource allocation scenarios. Tothe best of our knowledge no one uses an RDBMS as a tool to help solve these problems;our goal in this paper is to explore whether or not this needs to be the case. We show thatthe simple approach of computing the full θ-join and then applying standard graph-matchingalgorithms to the result is ineffective for all but the smallest of problem instances. By contrast;a closer study shows that the DBMS primitives of grouping; sorting; and joining can be …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,10
Synopses for query optimization: A space-complexity perspective,Raghav Kaushik; Jeffrey F Naughton; Raghu Ramakrishnan; Venkatesan T Chakravarthy,Abstract Database systems use precomputed synopses of data to estimate the cost ofalternative plans during query optimization. A number of alternative synopsis structures havebeen proposed; but histograms are by far the most commonly used. While histograms haveproved to be very effective in (cost estimation for) single-table selections; queries with joinshave long been seen as a challenge; under a model where histograms are maintained forindividual tables; a celebrated result of Ioannidis and Christodoulakis [1991] observes thaterrors propagate exponentially with the number of joins in a query. In this article; we maketwo main contributions. First; we study the space complexity of using synopses for queryoptimization from a novel information-theoretic perspective. In particular; we offer evidencein support of histograms for single-table selections; including an analysis over data …,ACM Transactions on Database Systems (TODS),2005,10
Spatial join method and apparatus,*,A non-blocking parallel spatial join algorithm generates the spatial join result tuplescontinuously even in the event of memory overflow. The algorithm employs duplicateavoidance techniques to avoid the blocking and time-consuming removal of duplicates. Thealgorithm also uses parallelism to improve performance.,*,2004,10
Building XML statistics for the hidden web,Ashraf Aboulnaga; Jeffrey F Naughton,Abstract There have been several techniques proposed for building statistics for static XMLdata. However; very little work has been done in the area of building XML statistics for datasources that export XML views of data that is stored in relational or other databases. Forsuch data sources; we need statistics that are built in an on-line manner; by observing theXML queries to the data sources and their results. In this paper; we present a technique forbuilding on-line XML statistics by observing the XPath queries issued to a data source andtheir result sizes. These XPath queries select parts of the virtual XML document representingthe XML view of the data at the data source. We convert these XPath queries to a moreabstract and generalized form that we call annotated path expressions. We present atechnique for storing these annotated path expressions and information about their …,Proceedings of the twelfth international conference on Information and knowledge management,2003,10
A simple characterization of uniform boundedness for a class of recursions,Jeffrey F Naughton; Yehoshua Sagiv,Abstract Detecting bounded recursions is a powerful optimization technique for recursivedatabase query languages; as bounded recursions can be replaced by equivalentnonrecursive definitions. The problem is also of theoretical interest in that varying the classof recursions considered generates problem instances that vary from linearly decidable toNP-hard to undecidable. In this paper we review and clarify the existing definitions ofboundedness. We then specify a class of recursions C such that membership in Cguarantees that a certain simple condition is necessary and sufficient for boundedness. Weuse the notion of membership in C to unify and extend previous work on determiningdecidable classes of bounded recursions.,The Journal of logic programming,1991,10
Falcon: Scaling up hands-off crowdsourced entity matching to build cloud services,Sanjib Das; Paul Suganthan GC; AnHai Doan; Jeffrey F Naughton; Ganesh Krishnan; Rohit Deep; Esteban Arcaute; Vijay Raghavendra; Youngchoon Park,Abstract Many works have applied crowdsourcing to entity matching (EM). While promising;these approaches are limited in that they often require a developer to be in the loop. Assuch; it is difficult for an organization to deploy multiple crowdsourced EM solutions;because there are simply not enough developers. To address this problem; a recent workhas proposed Corleone; a solution that crowdsources the entire EM workflow; requiring nodevelopers. While promising; Corleone is severely limited in that it does not scale to largetables. We propose Falcon; a solution that scales up the hands-off crowdsourced EMapproach of Corleone; using RDBMS-style query execution and optimization over a Hadoopcluster. Specifically; we define a set of operators and develop efficient implementations. Wetranslate a hands-off crowdsourced EM workflow into a plan consisting of these operators …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,9
Rescheduling table scan transactions,*,A database system includes a rescheduler that performs rescheduling of transactions in ascheduling queue based on one or more criteria. One criterion is whether a first transactionin the queue can be performed as a synchronized scan of a table with a second transactionthat is currently executing. This enables the first and second transactions to share a segmentof a buffer storage pool.,*,2008,9
Locking mechanism for materialized views in a database system,*,A database system receives a transaction that causes a tuple to be integrated into a joinview. In response to receiving the transaction; a predefined first lock is placed on at least aportion of the join view; the predefined first lock conflicting with either a shared lock or anexclusive lock placed on the join view; but not conflicting with another predefined first lockplaced on the join view. Also; a latch or semaphore associated with a value of apredetermined attribute of the tuple is obtained before integrating the tuple into the join view.,*,2008,9
YAWN,Katherine Morris; Jeffrey F Naughton; Yatin P Saraiya; Jeffrey D Ullman; Allen Van Gelder,*,Yet another window on Nail,1987,9
Resource bricolage for parallel database systems,Jiexing Li; Jeffrey Naughton; Rimma V Nehme,Abstract Running parallel database systems in an environment with heterogeneousresources has become increasingly common; due to cluster evolution and increasinginterest in moving applications into public clouds. For database systems running in aheterogeneous cluster; the default uniform data partitioning strategy may overload some ofthe slow machines while at the same time it may under-utilize the more powerful machines.Since the processing time of a parallel query is determined by the slowest machine; such anallocation strategy may result in a significant query performance degradation. We take a firststep to address this problem by introducing a technique we call resource bricolage thatimproves database performance in heterogeneous environments. Our approach quantifiesthe performance differences among machines with various resources as they process …,Proceedings of the VLDB Endowment,2014,8
Toward industrial-strength keyword search systems over relational data,Akanksha Baid; Ian Rae; AnHai Doan; Jeffrey F Naughton,Keyword search (KWS) over relational data; where the answers are multiple tuplesconnected via joins; has received significant attention in the past decade. Numeroussolutions have been proposed and many prototypes have been developed. Building on thisrapid progress and on growing user needs; recently several RDBMS and Web companiesas well as academic research groups have started to examine how to build industrial-strength keywords search systems. This task clearly requires addressing many issues;including robustness; accuracy; reliability; and privacy; among others. A major emergingissue; however; appears to be performance related: current KWS systems haveunpredictable run time. In particular; for certain queries it takes too long to produce answers;and for others the system may even fail to return (eg; after exhausting memory). In this …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,8
The Niagara query system,J Naughton; D DeWitt; D Maier; J Chen; L Galanis; K Tufte; J Kang; Q Luo; N Prakash; F Tian,*,The IEEE Data Eng. Bull,2000,8
Space optimization in the bottom-up evaluation of logic programs,S Sudarshan; Divesh Srivastava; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of a logic program; all generated facts are usuallyassumed to be stored until the end of the evaluation. Considerable gains can be achievedby instead discarding facts that are no longer required: the space needed to evaluate theprogram is reduced; 1/0 costs may be reduced; and the costs of maintaining and accessingindices; eliminating duplicates etc. are reduced. Thus; discarding facts early could achievetime as well as space improvements. Given an evaluation method that is sound; completeand does not repeat derivation steps; we consider how facts can be discarded during theevaluation without compromising these properties. Our first contribution is to show that sucha space optimization technique has three distinct components. Informally; we must make allderivations that we can with each fact; detect all duplicate derivations of facts and try to …,ACM SIGMOD Record,1991,8
Minimizing expansions of recursions,Jeffrey F Naughton; Yehoshua Sagiv,This chapter discusses the language of function-free horn clauses; which is popularly knownas Datalog because it is the subset of Prolog that obeys the first-normal form assumptions ofrelational databases. Datalog has none of the extra-logical features present in Prolog; suchas the cut operator or a built-in procedural interpretation. However; Prolog notation is usedand the horn clauses are written as rules; with the positive literal in the head or consequent;and the conjunction of the negative literals as the body or antecedent. The terms predicateand relation are used interchangeably; and it is assumed that the relations in the system canbe separated into two types. The extensional database predicates appear in the head of norule and are defined completely by their extent; that is; by the tuples stored in the relationscorresponding to the predicate. The intensional database predicates are defined in terms …,*,1989,8
Operator and query progress estimation in microsoft sql server live query statistics,Kukjin Lee; Arnd Christian König; Vivek Narasayya; Bolin Ding; Surajit Chaudhuri; Brent Ellwein; Alexey Eksarevskiy; Manbeen Kohli; Jacob Wyant; Praneeta Prakash; Rimma Nehme; Jiexing Li; Jeff Naughton,Abstract We describe the design and implementation of the new Live Query Statistics (LQS)feature in Microsoft SQL Server 2016. The functionality includes the display of overall queryprogress as well as progress of individual operators in the query execution plan. Wedescribe the overall functionality of LQS; give usage examples and detail all areas where wehad to extend the current state-of-the-art to build the complete LQS feature. Finally; weevaluate the effect these extensions have on progress estimation accuracy with a series ofexperiments using a large set of synthetic and real workloads.,Proceedings of the 2016 International Conference on Management of Data,2016,7
Revisiting differentially private regression: Lessons from learning theory and their consequences,Xi Wu; Matthew Fredrikson; Wentao Wu; Somesh Jha; Jeffrey F Naughton,Abstract: Private regression has received attention from both database and securitycommunities. Recent work by Fredrikson et al.(USENIX Security 2014) analyzed thefunctional mechanism (Zhang et al. VLDB 2012) for training linear regression models overmedical data. Unfortunately; they found that model accuracy is already unacceptable withdifferential privacy when $\varepsilon= 5$. We address this issue; presenting an explicitconnection between differential privacy and stable learning theory through which asubstantially better privacy/utility tradeoff can be obtained. Perhaps more importantly; ourtheory reveals that the most basic mechanism in differential privacy; output perturbation; canbe used to obtain a better tradeoff for all convex-Lipschitz-bounded learning tasks. Sinceoutput perturbation is simple to implement; it means that our approach is potentially …,arXiv preprint arXiv:1512.06388,2015,7
Demonstration of Santoku: optimizing machine learning over normalized data,Arun Kumar; Mona Jalal; Boqun Yan; Jeffrey Naughton; Jignesh M Patel,Abstract Advanced analytics is a booming area in the data management industry and a hotresearch topic. Almost all toolkits that implement machine learning (ML) algorithms assumethat the input is a single table; but most relational datasets are not stored as single tablesdue to normalization. Thus; analysts often join tables to obtain a denormalized table. Also;analysts typically ignore any functional dependencies among features because ML toolkitsdo not support them. In both cases; time is wasted in learning over data with redundancy.We demonstrate Santoku; a toolkit to help analysts improve the performance of ML overnormalized data. Santoku applies the idea of factorized learning and automatically decideswhether to denormalize or push ML computations through joins. Santoku also exploitsdatabase dependencies to provide automatic insights that could help analysts with …,Proceedings of the VLDB Endowment,2015,7
Toward Progress Indicators on Steroids for Big Data Systems.,Jiexing Li; Rimma V Nehme; Jeffrey F Naughton,*,CIDR,2013,7
Issues in applying data mining to grid job failure detection and diagnosis,Lakshmikant Shrinivas; Jeffrey F Naughton,Abstract As grid computation systems become larger and more complex; manuallydiagnosing failures in jobs becomes impractical. Recently; machine-learning techniqueshave been proposed to detect a variety of application failures in grids. While this is apromising approach; there are many options as to how to apply machine learning to thisproblem; and it not always obvious which approaches are feasible or effective. We exploresome issues that arise when we try to apply existing implementations of data miningalgorithms to diagnose as well as predict job failures in grids. We demonstrate that a) it isfeasible to gather enough data in real-time to train useful classifier algorithms; using only asmall fraction of the grid's computational resources; b) it is important to choose the featuresused for classification with care; and c) it is useful to have both per-user and system-wide …,Proceedings of the 17th international symposium on High performance distributed computing,2008,7
Space optimization in deductive databases,Divesh Srivastava; S Sudarshan; Raghu Ramakrishnan; Jeffrey F Naughton,Abstract In the bottom-up evaluation of logic programs and recursively defined views ondatabases; all generated facts are usually assumed to be stored until the end of theevaluation. Discarding facts during the evaluation; however; can considerably improve theefficiency of the evaluation: the space needed to evaluate the program; the I/O costs; thecosts of maintaining and accessing indices; and the cost of eliminating duplicates may all bereduced. Given an evaluation method that is sound; complete; and does not repeatderivation steps; we consider how facts can be discarded during the evaluation withoutcompromising these properties. We show that every such space optimization method hascertain components; the first to ensure soundness and completeness; the second to avoidredundancy (ie; repetition of derivations); and the third to reduce “fact lifetimes”(ie; the …,ACM Transactions on Database Systems (TODS),1995,7
How to Forget the Past Without Repeating It.,Jeffrey F Naughton; Raghu Ramakrishnan,Abstract Bottom-up evaluation of deductive database programs has the advantage that itavoids repeated computation by storing all intermediate results and replacing recomputationby table lookup. However; in general; storing all intermediate results for the duration of acomputation wastes space. In this paper we propose an evaluation scheme that avoidsrecomputation; yet under fairly general conditions at any given time stores only a smallsubset of the facts generated. The results constitute a significant first step in compile-timegarbage collection for bottom-up evaluation of deductive database programs.,VLDB,1990,7
On Debugging Non-Answers in Keyword Search Systems.,Akanksha Baid; Wentao Wu; Chong Sun; AnHai Doan; Jeffrey F Naughton,ABSTRACT Handling non-answers is desirable in information retrieval systems. Current e-commerce websites usually try to suppress the somewhat dreaded message that no resultshave been found. Possible solutions include; for example; augmenting the data withsynonyms and common misspellings based on query logs. Nonetheless; this is onlyachievable if we can know the cause of the non-answers. Under the hood; most e-commercedata sits in some structured format. Debugging non-answers in the underlying KWS-Ssystems is therefore not trivial—non-answers in a KWS-S system could be a problem of thedata (eg; absence of some keywords); the schema (eg; missing key-foreign-key joins); ordue to empty join results from one of possibly several joins in the generated SQL queries. Sofar; we are unaware of any previous work that explores how to enable developers to …,EDBT,2015,6
JECB: A join-extension; code-based approach to OLTP data partitioning,Khai Q Tran; Jeffrey F Naughton; Bruhathi Sundarmurthy; Dimitris Tsirogiannis,Abstract Scaling complex transactional workloads in parallel and distributed systems is achallenging problem. When transactions span data partitions that reside in different nodes;significant overheads emerge that limit the throughput of these systems. In this paper; wepresent a low-overhead data partitioning approach; termed JECB; that can reduce thenumber of distributed transactions in complex database workloads such as TPC-E. Theproposed approach analyzes the transaction source code of the given workload and thedatabase schema to find a good partitioning solution. JECB leverages partitioning by key-foreign key relationships to automatically identify the best way to partition tables usingattributes from tables. We experimentally compare our approach with the state of the art data-partitioning techniques and show that over the benchmarks considered; JECB provides …,Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,2014,6
Transaction reordering with application to synchronized scans,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,Abstract Traditional workload management methods mainly focus on the current systemstatus while information about the interaction between queued and running transactions islargely ignored. An exception to this is the transaction reordering method; which reorders thetransaction sequence submitted to the RDBMS and improves the transaction throughput byconsidering both the current system status and information about the interaction betweenqueued and running transactions. The existing transaction reordering method onlyconsiders the reordering opportunities provided by analyzing the lock conflict informationamong multiple transactions. This significantly limits the applicability of the transactionreordering method. In this paper; we extend the existing transaction reordering method intoa general transaction reordering framework that can incorporate various factors as the …,Proceedings of the ACM 11th international workshop on Data warehousing and OLAP,2008,6
XML views as integrity constraints and their use in query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,The SQL queries produced in XML-to-SQL query translation are often unnecessarilycomplex; even for simple input XML queries. In this paper we argue that relational systemscan do a better job of XML-to-SQL query translation with the addition of a simple newconstraint; which we term the" lossless from XML" constraint. Intuitively; this constraint statesthat a given relational data set resulted from the shredding of an XML document thatconformed to a given schema. We illustrate the power of this approach by giving analgorithm that exploits the" lossless from XML" constraint to translate path expressionqueries into efficient SQL; even in the presence of recursive XML schemas. We argue thatthis approach is likely to be simpler and more effective than the current state of the art inoptimizing XML-to-SQL query translation; which involves identifying and declaring …,Data Engineering; 2005. ICDE 2005. Proceedings. 21st International Conference on,2005,6
Unraveling the duplicate-elimination problem in XML-to-SQL query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,Abstract We consider the scenario where existing relational data is exported as XML. In thiscontext; we look at the problem of translating XML queries into SQL. XML query languageshave two different notions of duplicates: node-identity based and value-based. Pathexpression queries have an implicit node-identity based duplicate elimination built intothem. On the other hand; SQL only supports value-based duplicate elimination. In this paper;using a simple path expression query we illustrate the problems that arise when we attemptto simulate the node-identity based duplicate elimination using value-based duplicateelimination in the SQL queries. We show how a general solution for this problem coveringthe class of views considered in published literature requires a fairly complex mechanism.,Proceedings of the 7th International Workshop on the Web and Databases: colocated with ACM SIGMOD/PODS 2004,2004,6
Souvenir purchase patterns of domestic tourists: Case study of Takayama city; Japan,M Nomura,*,Menomonie; Wisconsin: University of Wisconsin-Stout,2002,6
A methodology for formalizing model-inversion attacks,Xi Wu; Matthew Fredrikson; Somesh Jha; Jeffrey F Naughton,Confidentiality of training data induced by releasing machine-learning models; and hasrecently received increasing attention. Motivated by existing MI attacks and other previousattacks that turn out to be MI" in disguise;" this paper initiates a formal study of MI attacks bypresenting a game-based methodology. Our methodology uncovers a number of subtleissues; and devising a rigorous game-based definition; analogous to those in cryptography;is an interesting avenue for future work. We describe methodologies for two types of attacks.The first is for black-box attacks; which consider an adversary who infers sensitive valueswith only oracle access to a model. The second methodology targets the white-box scenariowhere an adversary has some additional knowledge about the structure of a model. For therestricted class of Boolean models and black-box attacks; we characterize model …,Computer Security Foundations Symposium (CSF); 2016 IEEE 29th,2016,5
Formulating global statistics for distributed databases,*,The present invention extends to methods; systems; and computer program products forformulating global statistics for parallel databases. In general; embodiments of the inventionmerge (combine) information in multiple compute node level histograms to create a globalhistogram for a table that is distributed across a number of compute nodes. Merging caninclude aligning histogram step boundaries across the compute node histograms. Mergingcan include aggregating histogram step-level information; such as; for example; equalityrows and average range rows (or alternately equality rows; range rows; and distinct rangerows); across the compute node histograms into a single global step. Merging can accountfor distinct values that do not appear at one or more compute nodes as well as distinctvalues that are counted at multiple compute nodes. A resulting global histogram can be …,*,2015,5
Providing a progress indicator in a database system,*,A database system is capable of processing a query and providing a progress indicator ofthe processing of the query based on at least a first cost factor. Initially; an estimate is set forthe first cost factor. During processing of the query; the estimate of the first cost factor isupdated to enable refinement of the progress indicator.,*,2012,5
Transaction reordering,Gang Luo; Jeffrey F Naughton; Curt J Ellmann; Michael W Watzke,Abstract Traditional workload management methods mainly focus on the current systemstatus while information about the interaction between queued and running transactions islargely ignored. This paper proposes using transaction reordering; a workload managementmethod that considers both the current system status and information about the interactionbetween queued and running transactions; to improve the transaction throughput in anRDBMS. Our main idea is to reorder the transaction sequence submitted to the RDBMS tominimize resource contention and to maximize resource sharing. The advantages of thetransaction reordering method are demonstrated through experiments with three commercialRDBMSs.,Data & Knowledge Engineering,2010,5
Anonymization techniques for large and dynamic data sets,Tochukwu Iwuchukwu,Abstract We make two main contributions to the problem of privacy in data publishing. First;we tackle the problem of anonymizing data sets that reside on secondary storage. Dataanonymization is a process by which data is transformed to protect the identities and othersensitive information regarding individuals in the data. One may view the preservation ofprivacy for larger-than-memory data sets as a scalability problem. Our solution to thisproblem takes advantage of parallels between data anonymization and indexing indatabases. We also show that database indexes; besides scalability; present other benefitslacking in previous data anonymization techniques. The dynamics of a data set introducesan entirely new set of challenges not typically present for static data sets. One of the mainchallenges involves the ability to anonymize and publish a changing data set without …,*,2007,5
Optimizing fixed-schema xml to sql query translation,Rajasekar Krishnamurthy; Raghav Kaushik; Jeffrey F Naughton,ABSTRACT Recently; there has been a lot of work on evaluating XML queries over datastored in relational database systems. The vast majority of this work has focused on thecases where either the relational schema is not fixed (so the problem is to find a goodrelational schema for a given XML workload) or the XML schema is not fixed (so the problemis to develop generic strategies for exporting XML views of relational data). While thesecases are interesting; in practice a third scenario; in which both the source relational andtarget XML schemas are fixed; seems highly relevant. We show that even in this highlyconstrained environment; there is a lot of freedom in the SQL that can be generated toevaluate a given XML query. Furthermore; we show through experiments with a commercialRDBMS that by exploiting the underlying relational constraints and the properties of a …,Proc of the VLDB Conf,2002,5
De Witt; Jeffrey F. Naughton; John C. Shafer; Shivakumar Venkataraman; Parallelizing OODBMS traversals: a performance evaluation,J David,*,The VLDB Journal—The International Journal on Very Large Data Bases,1996,5
The dec 007 benchmark,M Carey; D DeWitt; J Naughton,*,Proc. ACM SIGMOD Conf. on the Management of Data,1993,5
Parallel external sorting using probabilistic splitting,D DeWitt; Jeffrey F Naughton; Donovan A Schneider,*,Proceedings of the 1st Conference on Parallel and Distributed Information Systems,1991,5
Clocked adversaries for hashing,Richard J Lipton; Jeffrey F Naughton,Abstract A “clocked adversary” is a program that can time its operations and base itsbehavior on the results of those timings. While it is well known that hashing performs poorlyin the worst case; recent results have proven that for reference string programs; theprobability of falling into a bad case can be driven arbitrarily low. We show that this is nottrue for clocked adversaries. This emphasizes the limits on the applicability of theorems onthe behavior of hashing schemes on reference string programs; and raises a novel set ofproblems dealing with optimality of and vulnerability to clocked adversaries.,*,1989,5
Towards linear algebra over normalized data,Lingjiao Chen; Arun Kumar; Jeffrey Naughton; Jignesh M Patel,Abstract Providing machine learning (ML) over relational data is a mainstream requirementfor data analytics systems. While almost all ML tools require the input data to be presentedas a single table; many datasets are multi-table. This forces data scientists to join thosetables first; which often leads to data redundancy and runtime waste. Recent works on"factorized" ML mitigate this issue for a few specific ML algorithms by pushing ML throughjoins. But their approaches require a manual rewrite of ML implementations. Suchpiecemeal methods create a massive development overhead when extending such ideas toother ML algorithms. In this paper; we show that it is possible to mitigate this overhead byleveraging a popular formal algebra to represent the computations of many ML algorithms:linear algebra. We introduce a new logical data type to represent normalized data and …,Proceedings of the VLDB Endowment,2017,4
In-RDBMS inverted indexes revisited,Ian Rae; Alan Halverson; Jeffrey F Naughton,Every major open-source and commercial RDBMS offers some form of support for full-textsearch using inverted indexes. When providing this support; some developers haveimplemented specialized indexes that adapt techniques from the Information Retrieval (IR)community to work in a database setting; while others have opted to rely on the standardrelational query engine to process inverted index lookups. This choice is an important one;since the storage formats and algorithms used can vary greatly between a specialized indexand a relational index; but these alternatives have not been thoroughly compared in thesame system. Our work explores the differences in implementation and performance of threerepresentative environments for an in-RDBMS inverted index: an in-RDBMS IR engine; arow-oriented relational query engine; and a column-oriented relational query engine. We …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,4
Incremental loading of object databases,J Wiener; J Naughton,Object-oriented and object-relational databases (OODB) need to be able to load the vastquantities of data that OODB users bring to them. Loading OODB data is significantly morecomplicated than loading relational data due to the presence of relationships; or references;in the data. In our previous work; we presented algorithms for loading new objects that onlyshare relationships with other new objects. However; it is frequently the case that newobjects need to share relationships with objects already in database. In this paper wepropose using queries within the load data file to identify the existing objects and suggestusing parameterized functions to designate similar queries. We then propose a novelevaluation strategy for the queries that defers evaluation until all the queries can beevaluated together. All of the instantiations of a single query function can then be treated …,*,1996,4
Putting XML Query Algebras into Context; 2002,SD Viglas; L Galanis; DJ De-Witt; D Maier; JF Naughton,*,*,*,4
F.(1999). Relational databases for querying XML documents: limitations and opportunities,J Shanmugasundaram; K Tufte; C Zhang; G He; DJ DeWitt; J Naughton,*,25th International Conference on Very Large Data Bases,*,4
The Niagara Internet query system; 2000,J Naughton; D DeWitt; D Maier,*,*,*,4
The token distribution filter for approximate string membership checking,Chong Sun; Jeffrey Naughton,ABSTRACT A common application over web data is to find all the strings in a collection ofpages that match strings in a given dictionary. We consider the problem of extracting all thestrings or substrings in a document (or a page) that approximately match some string in agiven dictionary. The current state-of-art approach for this problem involves first applying anapproximate; fast filter; then applying a more expensive exact verification algorithm to thestrings that survive the filter. Many string filters; such as the length filter and prefix filter; havebeen proposed. However; we find many string filters are ineffective or inefficient in someproblem scenarios. In this paper; we propose a new filter; the TDF (token distribution filter).We conduct experiments on both synthetic and real data sets; and show that for a wide classof problems it performs better than previously proposed filters.,WebDB,2011,3
Locking mechanism employing a name lock for materialized views,*,A database system includes a locking mechanism for a materialized view defined on baserelations. In response to updates of a base relation; a name lock is placed on thematerialized view. The name lock is a type of exclusive lock and is associated with a datastructure containing a first parameter to identify a base relation of the materialized viewbeing updated; and a second parameter to indicate a number of transactions updating thebase relation. With locking mechanisms according to some embodiments of the invention;transaction concurrency is enhanced by enabling concurrent updates of a materialized viewby plural transactions in certain cases.,*,2006,3
TRAC: toward recency and consistency reporting in a database with distributed data sources,Jiansheng Huang; Jeffrey F Naughton; Miron Livny,Abstract Distributed computing environments; including workflows in computational grids;present challenges for monitoring; as the state of the system may be captured only in logsdistributed throughout the system. One approach to monitoring such systems is to" sniff"these distributed logs and to store their transformed content in a DBMS. This centralizes thestate and exposes it for querying; unfortunately; it also creates uncertainty with respect to therecency and consistency of the data. Previous related work has focused on allowing queriesto express currency and consistency constraints; which are then enforced by" pulling" datafrom the distributed sources on demand; or by requiring synchronous updates of acentralized data store. In some instances this is impossible due to legacy system issues orinefficient as the system scales to large numbers of processors. Accordingly; we propose …,Proceedings of the 32nd international conference on Very large data bases,2006,3
Relational Database for Querying XML Documents: Limitations and',J Shanmugasundaram; K Tufte; G He; C Zhang; D DeWitt; J Naughton,*,CLDB; Edinburgh; Scotland,1999,3
On the expected size of recursive datalog queries,S Seshadri; Jeffrey F Naughton,Abstract We present asymptotically exact expressions for the expected sizes of relationsdefined by three well-studied Datalog recursions; namely the" transitive closure;"" samegeneration;" and" canonical factorable recursion." We consider the size of the fixpoints of therecursively defined relations in the above programs; as well as the size of the fixpoints of therelations defined by the rewritten programs generated by the Magic Sets and Factoringrewriting algorithms in response to selection queries. Our results show that even overrelatively sparse base relations; the fixpoints of the recursively defined relations are within asmall constant factor of their worst-case size bounds; and that the Magic Sets rewritingalgorithm on the average produces relations whose fixpoints are within a small constantfactor of the corresponding bounds for the recursion without rewriting. The expected size …,Journal of Computer and System Sciences,1995,3
Ge neralized Search Tree for Database System,JF Naughton; A Pfefer,*,Proc 21th Very IJarge Data Base Conference; Zurich Switzerland,1995,3
How to forget the past without repeating it,Jeffrey F Naughton; Raghu Ramakrishnan,Abstract Bottom-up evaluation of deductive database programs has the advantage that itavoids repeated computations by storing all intermediate results and replacingrecomputation by table lookup. However; in general; storing all intermediate results for theduration of a computation wastes space. In this paper; we propose an evaluation schemethat avoids recomputation; yet for a fairly general class of programs at any given time storesonly a small subset of the facts generated. The results constitute a significant first step incompile-time garbage collection for bottom-up evaluation of deductive database programs.,Journal of the ACM (JACM),1994,3
De Witt; Jeffrey F. Naughton; John C. Shafer; Shivakumar Venkataraman; ParSets for parallelizing OODBMS traversals: implementation and performance,J David,*,Proceedings of the third international conference on on Parallel and distributed information systems,1994,3
ParSet design document,D DeWitt; J Naughton; J Shafer; S Venkataraman,*,Unpublished manuscript; November,1993,3
A unified approach to logic program evaluation,Jeffrey F Naughton; Raghu Ramakrishnan,*,*,1989,3
Zhang; c.; De-Witt; D.; and Naughton; J.(1999). Relational databases for querying XML documents: Limitations and opportunities,J Shanmugasundaram; K Tufte; G He,*,Proceedings of the 25th Conference on Very Large Databases (VillB),*,3
Jeffrey. F. Naughton and Donovan A. Schneider; September; 1991. An Evaluation of Non-Equijoin Algorithms,David J Dewitt,*,Proceedings of the 17th International Conference on Very Large Databases; Barcelona,*,3
Zhang. C.; He;. DJ; DeWitt; J. & Naughton; JF (1999). Relational databases for querying XML documents: Limitations and opportunities,J Shanmugasundaram; K Tufte,*,Proceedings of the International Conference on Very Large Data Bases (VLDB’99),*,3
J." Relational Databases for Querying XML Documents: Limitations and Opportunities;" 1999,J Shanmugasundaram; K He; G Zhang; C DeWitt; D Naughton,*,Proceedings of th 25th VLDB Conference,*,3
Bolt-on differential privacy for scalable stochastic gradient descent-based analytics,Xi Wu; Fengan Li; Arun Kumar; Kamalika Chaudhuri; Somesh Jha; Jeffrey Naughton,Abstract While significant progress has been made separately on analytics systems forscalable stochastic gradient descent (SGD) and private SGD; none of the major scalableanalytics frameworks have incorporated differentially private SGD. There are two inter-related issues for this disconnect between research and practice:(1) low model accuracydue to added noise to guarantee privacy; and (2) high development and runtime overheadof the private algorithms. This paper takes a first step to remedy this disconnect andproposes a private SGD algorithm to address both issues in an integrated manner. Incontrast to the white-box approach adopted by previous work; we revisit and use theclassical technique of output perturbation to devise a novel``bolt-on''approach to privateSGD. While our approach trivially addresses (2); it makes (1) even more challenging. We …,Proceedings of the 2017 ACM International Conference on Management of Data,2017,2
Rescheduling of modification operations for loading data into a database system,*,A method or apparatus for use with a database system that stores a join view associatedwith plural base relations includes receiving modification operations to modify at least two ofthe base relations of the join view; and re-ordering the received modification operations toavoid concurrent execution of modification operations of more than one of at least two baserelations.,*,2012,2
Approximate string membership checking: A multiple filter; optimization-based approach,Chong Sun; Jeffrey F Naughton; Siddharth Barman,We consider the approximate string membership checking (ASMC) problem of extracting allthe strings or sub strings in a document that approximately match some string in a givendictionary. To solve this problem; the current state-of-art approach involves first applying anapproximate; fast filter; then applying a more expensive exact verification algorithm to thestrings that pass the filter. Correspondingly; many string filters have been proposed. We notethat different filters are good at eliminating different strings; depending on the characteristicsof the strings in both the documents and the dictionary. We suspect that no single filter willdominate all other filters everywhere. Given an ASMC problem instance and a set of stringfilters; we need to select the optimal filter to maximize the performance. Furthermore; in ourexperiments we found that in some cases a sequence of filters dominates any of the filters …,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,2
Database Support for Weighted Match Joins,Ameet Kini; Jeffrey F Naughton,As relational database management systems are applied to non-traditional domains such asscientific data management; there is an increasing need to support queries with semanticsthat differ from those appropriate for traditional RDBMS applications. Two interesting ideascurrently being explored in the DBMS community are ranking query results (eg; top-kcomputations) and; more recently;" match joins." In this paper we combine these two ideasand study weighted match joins; in which (a) like match joins; each tuple joins with at mostone matching tuple; and (b) like top-k joins; the system attempts to provide a set of answertuples that maximizes a weight function. We explore exact and approximate strategies forevaluating weighted match joins. Using a prototype implementation in PostgreSQL; weexplore the performance characteristics of these strategies. Our results suggest that the …,Scientific and Statistical Database Management; 2007. SSBDM'07. 19th International Conference on,2007,2
K-relevance: a spectrum of relevance for data sources impacting a query,Jiansheng Huang; Jeffrey F Naughton,Abstract Applications ranging from grid management to sensor nets to web-basedinformation integration and extraction can be viewed as receiving data from some number ofautonomous remote data sources and then answering queries over this collected data. Insuch environments it is helpful to inform users which data sources are" relevant" to theirquery results. It is not immediately obvious what" relevant" should mean in this context; asdifferent users will have different requirements. In this paper; rather than proposing a singledefinition of relevance; we propose a spectrum of definitions; which we term" k-relevance";for k≥ 0. We give algorithms for identifying k-relevant data sources for relational queries andexplore their efficiency both analytically and experimentally. Finally; we explore the impact ofintegrity constraints (including dependencies) and materialized views on the problem of …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,2
Set Valued Attributes,Karthikeyan Ramasamy; Prasad M Deshpande,Abstract About three decades ago; when Codd (1970) invented the relational databasemodel; it took the database world by storm. The enterprises that adapted it early won a largecompetitive edge. The past two decades have witnessed tremendous growth of relationaldatabase systems; and today the relational model is by far the dominant data model and isthe foundation for leading DBMS products; including IBM DB2; Informix; Oracle; Sybase; andMicrosoft SQL server. Relational databases have become a multibillion-dollar industry.,*,2005,2
NIAGARA Query Engine,J Naughton; D DeWitt,*,*,2003,2
The Niagara internet query system (overview paper),Jeffrey Naughton; David DeWitt; David Maier,*,*,2000,2
Performance issues of multi-dimensional data analysis,Yihong Zhao; Jeffrey Naughton,Abstract This thesis investigates performance issues that arise in multi-dimensional analysis.It tries to solve three closely related problems in multi-dimensional data analysis bydesigning high performance storage structures and algorithms. Recently; On-Line AnalyticalProcessing (OLAP) has become the most widely used and studied database technology inmulti-dimensional data analysis. The key requirement of OLAP systems is fast responsetime. This requirement is the driving force and the primary motivation of this thesis. The basicidea is to exploit the special features of multi-dimensional queries to design highperformance algorithms for query processing. The first part of the thesis studies issuesrelated to array-based storage and designs array-based consolidation algorithms for multi-dimensional data. The second part contains the design and implementation of array …,*,1998,2
D. Dewitt; JF Naughton; M. Asgarian; P. Brown; JE Gerke; DN Shah;“The bucky object-relational benchmark”,M Carey,*,Proc. of the ACM SIGMOD Conference on Management of Data,1997,2
Hash join processing on shared memory multiprocessors,Ambuj Shatdal; Jeffrey F Naughton,Abstract While most scalable database systems are designed for a shared nothingarchitecture; advances in hardware technology suggest that in the near future sharedmemory multiprocessors (SMPs) will be capable of handling all but the largest applications.This raises important questions about how scalable database systems should bearchitected; in particular; do SMPs require specially targeted algorithms; or will algorithmsdeveloped for shared nothing hardware suffice? As a first step towards answering thequestion; with an implementation on an SGI Challenge multiprocessor; we investigate theperformance of hash join algorithms on SMPs. We first show that the shared nothingapproach; if ported to an SMP by using a transparent message passing library; yields poorperformance. However; we show that by using an optimized message passing library and …,*,1996,2
Using the SHORE Object-Oriented Database/File System Paradigm for Information Retrieval,Bradley S Rubin; Jeffrey F Naughton,Abstract The SHORE (Scaleable Heterogeneous Object Rlilpository) persistent object storeunder development at the University of Wisconsin—Madison is a hybrid of OODBMS and filesystem technologies. A primary goal of the SHORE project is to provide a system thatfacilitates the construction of high functionality data servers for a wide variety of applications.In this paper; we evaluate the suitability of the SHORE system for supporting theimplementation of an information retrieval server. Our experience with our implementation ofa prototype multimedia information server; Chrysalis; demonstrates that overall the SHOREpersistent object store functionality is better suited to supporting the construction of aninformation retrieval server than the traditional alternatives of ﬁle systems; relationaldatabase systems; or even current OODBMS. Chrysalis is the ﬁrst major SHORE …,*,1995,2
Benchmark,M Carey; D Dewitt; J Naughton,"To Render These Rights Secure": James Madison's Understanding of the Relationship of theConstitution to the Bill of Rights … James Madison on Constitutional Interpretation.... B. Nelson Ong ........... 17 … James Madison on Federalism: The Search for Abiding Principles......................... George Carey .......... 27 … Standing in the Need of Prayer? The Supreme Courton James Madison and Religious Liberty .... Jonathan K. Van Patten .. 59 … Original Intent Jurisprudenceand Madison's Detached Memoranda ....................... Robert L. Cord ........... 79 … RepublicanVirtue and The Federalist: A Consideration of the Wills Thesis … James MacGregorBums and the Liberal Critique of Madisonian Democracy ..................... Hrach Gregorian ........ 97… The Constitutional Convention and the Study of the American Founding ........................ MortonFrisch ............ 101,Proceedings of ACM SIGMOD Conference in Management of Data; Washington; USA,1993,2
Nested Loops revisited,DJ DeWitt J Naughton; J Burger,*,Proceedings of the Parallel and Distributed Information Systems,1993,2
The OO7 Benchmark,MJ Carey DJ DeWitt; Jerey F Naughton,*,Computer Sciences Dept.; Univ. of Wisconsin-Madison,1993,2
A Uniﬁed Approach to Logic Program Evaluation,J Naughton; R Ramakrishnan,Abstract The Prolog evaluation algorithm has become the standard for logic programevaluation; and bottom—up methods have long been considered impractical because theycompute irrelevant facts. Recently; however; bottom-up evaluation algorithms that retain thefocusing property of top—down evaluation have been proposed; and in view of thesealgorithms the choice between top-down and bottom-up nmthods needs to be re-examined.In order to motivate a closer look at bottom-up methods; we identify certain classes of logicprograms for which bottom-yup evaluation provides polynomial time evaluation algo-rithmswhere Prolog takes exponential time. We also demonstrate that techniques such aspredicate factoring can provide a further O (n) improvement; when they are applicable. Weargue that no one evaluation method is uniformly preferable; and suggest that a choice of …,*,1989,2
Benchmarking multi-rule recursion evaluation strategies,Jeffrey F Naughton,*,*,1988,2
D.; and Naughton; J.(1999)‘Relational Databases for Querying XML Documents: Limitations and opportunities’,J Shanmugasundaram; H Gang; K Tufte; C DeWitt Zhang,*,proceedings of the 25th international conference on Very Large Data Bases; Edinburgh; Scotland,*,2
The Lowell Database Research Self-Assessment Meeting; Lowell Massachusetts; 2003,S Abiteboul; R Agrawal; P Bernstein; M Carey; S Ceri; B Croft; D DeWitt; M Franklin; H Molina; D Gawlick; J Gray; L Haas; A Halevy; J Hellerstein; Y Ioannidis; M Kersten; M Pazzani; M Lesk; D Maier; J Naughton; H Schek; T Sellis; A Silberschatz; M Stonebraker; R Snodgrass; J Ullman; G Weikum; J Widom; S Zdonik,*,*,*,2
Th6 007 Benchmark,MJ Carey; DJ DeWitt; JF Naught0n,*,Proc. InG CO 00 Afan (WerneM I) OM; ACM SIGMOD; Washington; 객체지향 데이타베이스에서의 비용기반 버퍼 교체 알고리즘 U,*,2
Parallelizing OODBMS traversals: A performance evaluation; May 1995,D DeWitt; J Naughton; J Shafer; S Venkataraman,*,Submitted for Publication,*,2
D.'I ‘. Schuh; MH Solomon; CK. Tan; O. Tsatalos; S. White; and MJ Zwilling. Shoring up persistent objects,MJ Carey; DJ DeWitt; MJ Franklin; NE Hall; M McAuliffe; JF Naughton,*,Proceedings of ACM SIGMOD,*,2
When Lempel-Ziv-Welch Meets Machine Learning: A Case Study of Accelerating Machine Learning using Coding,Fengan Li; Lingjiao Chen; Arun Kumar; Jeffrey F Naughton; Jignesh M Patel; Xi Wu,Abstract: In this paper we study the use of coding techniques to accelerate machine learning(ML). Coding techniques; such as prefix codes; have been extensively studied and used toaccelerate low-level data processing primitives such as scans in a relational databasesystem. However; there is little work on how to exploit them to accelerate ML algorithms. Infact; applying coding techniques for faster ML faces a unique challenge: one needs toconsider both how the codes fit into the optimization algorithm used to train a model; and theinterplay between the model sstructure and the coding scheme. Surprisingly andintriguingly; our study demonstrates that a slight variant of the classical Lempel-Ziv-Welch(LZW) coding scheme is a good fit for several popular ML algorithms; resulting in substantialruntime savings. Comprehensive experiments on several real-world datasets show that …,arXiv preprint arXiv:1702.06943,2017,1
m-tables: Representing missing data,Bruhathi Sundarmurthy; Paraschos Koutris; Willis Lang; Jeffrey Naughton; Val Tannen,Abstract Representation systems have been widely used to capture different forms ofincomplete data in various settings. However; existing representation systems are notexpressive enough to handle the more complex scenarios of missing data that can occur inpractice: these could vary from missing attribute values; missing a known number of tuples;or even missing an unknown number of tuples. In this work; we propose a newrepresentation system called m-tables; that can represent many different types of missingdata. We show that m-tables form a closed; complete and strong representation systemunder both set and bag semantics and are strictly more expressive than conditional tablesunder both the closed and open world assumptions. We further study the complexity ofcomputing certain and possible answers in m-tables. Finally; we discuss how to" interpret" …,LIPIcs-Leibniz International Proceedings in Informatics,2017,1
Towards Interactive Debugging of Rule-based Entity Matching.,Fatemah Panahi; Wentao Wu; AnHai Doan; Jeffrey F Naughton,ABSTRACT Entity Matching (EM) identifies pairs of records referring to the same real-worldentity. In practice; this is often accomplished by employing analysts to iteratively design andmaintain sets of matching rules. An important task for such analysts is a “debugging” cycle inwhich they make a modification to the matching rules; apply the modified rules to a labeledsubset of the data; inspect the result; and then perhaps make another change. Our goal is tomake this process interactive by minimizing the time required to apply the modified rules. Wefocus on a common setting in which the matching function is a set of rules where each rule isin conjunctive normal form (CNF). We propose the use of “early exit” and “dynamicmemoing” to avoid unnecessary and redundant computations. These techniques create anew optimization problem; and accordingly we develop a cost model and study the …,EDBT,2017,1
Technical perspective: natural language to SQL translation by iteratively exploring a middle ground,Jeffrey F Naughton,A fundamental question in data management is how relational database managementsystems (RDBMSs) should be queried. Ideally; the query interface should be powerfulenough to express arbitrary queries; yet simple enough to learn that users require virtuallyno training. Natural language is an obvious and appealing approach–presumably mostusers already know at least one natural language and use it to “query” other humansconstantly. Unfortunately; employing natural language to query RDBMSs is highly nontrivial;and for the most part; not used. However; with the growing power and ubiquity of NaturalLanguage Processing (NLP) systems; it makes sense to redouble efforts in applying NLP todatabase querying. At the most basic level; relational database systems are queried usingSQL.(For that matter; most “NoSQL” systems are also queried using SQL.) SQL is very …,ACM SIGMOD Record,2016,1
Partial result classification,*,A query can be executed over incomplete data and produce a partial result. Moreover; thepartial result or portion thereof can be classified in accordance with a partial resulttaxonomy. In accordance with one aspect; the taxonomy can be defined in terms of datacorrectness and cardinality properties. Further; partial result analysis can be performed atvarious degrees of granularity. Classified partial result can be presented on a display deviceto allow user to view and optionally interact with the partial result.,*,2015,1
A Survey of the Existing Landscape of ML Systems,Arun Kumar; Robert McCann; Jeffrey Naughton; Jignesh M Patel,1. INTRODUCTION We present a detailed survey of the existing land- scape of ML systems. Wecategorize the existing and proposed ML systems into six major categories: (1) Packages of MLImplementations; (2) Systems with a Linear Algebra-based Language; (3) Model ManagementSystems; (4) Systems for Feature En- gineering; (5) Systems for Algorithm Selection; and (6)Systems for Parameter Tuning. For each cate- gory (or sub-category; wherever applicable); wedis- cuss a few prominent examples from both research and practice. Note that it is possible fora sys- tem to belong to more than one category; since it could potentially have multiple simultaneousgoals. Our categorization is not intended to be exhaustive. Rather; we aim to give a high-levelpicture of the kinds of functionalities that have been considered; and underscore the gaps thatexist to motivate our vision [26]. Table 1 summarizes the categories.,UW-Madison CS Tech. Rep. TR1827,2015,1
On Differentially Private Inductive Logic Programming,Chen Zeng; Eric Lantz; Jeffrey F Naughton; David Page,Abstract We consider differentially private inductive logic programming. We begin byformulating the problem of guarantee differential privacy to inductive logic programming; andthen prove the theoretical difficulty of simultaneously providing good utility and good privacyin this task. While our analysis proves that in general this is very difficult; it leaves a glimmerof hope in that when the size of the training data is large or the search tree for hypotheses is“short” and “narrow;” we might be able to get meaningful results. To prove our intuition; weimplement a differentially private version of Aleph; and our experimental results show thatour algorithm is able to produce accurate results for those two cases.,International Conference on Inductive Logic Programming,2013,1
We are drowning in a sea of least publishable units (LPUs),David J DeWitt; Ihab F Ilyas; Jeffrey Naughton; Michael Stonebraker,Abstract Our field is drowning in a sea of conference submissions. We assert that the sheernumber of papers has begun to seriously hurt the quality of the work that the field is doingand that the field is going to implode unless we take action to remedy the situation. In orderto improve the quality of the papers being published we must reduce the number beingsubmitted. This will require a change in the culture of our field where" more" is beingequated to" better" by both hiring and promotion committees. In this panel we will exploresome ideas for correcting the situation.,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,1
On optimal differentially private mechanisms for count-range queries,Chen Zeng; Jin-Yi Cai; Pinyan Lu; Jeffrey F Naughton,Abstract While there is a large and growing body of literature on differentially privatemechanisms for answering various classes of queries; to the best of our knowledge" count-range" queries have not been studied. These are a natural class of queries that ask" is thenumber of rows in a relation satisfying a given predicate between two integers θ 1 and θ 2?"Such queries can be viewed as a simple form of SQL" having" queries. We begin bydeveloping a provably optimal differentially private mechansim for count-range queries for asingle consumer. For count queries (in contrast to countrange queries); Ghosh et al.[9] haveprovided a differentially private mechanism that simultaneously maximizes utility for multipleconsumers. This raises the question of whether such a mechanism exists for count-rangequeries. We prove that the answer is no---for count range queries; no such mechanism …,Proceedings of the 16th International Conference on Database Theory,2013,1
Instrumenting a logic programming language to gather provenance from an information extraction application,Christine F Reilly; Yueh-Hsuan Chiang; Jeffrey F Naughton,Abstract Information extraction (IE) programs for the web consume and produce a lot of data.In order to better understand the program output; the developer and user often desire toknow the details of how the output was created. Provenance can be used to learn about thecreation of the output. We collect fine-grained provenance by leveraging ongoing work in theIE community to write IE programs in a logic programming language. The logic programminglanguage exposes the semantics of the program; allowing us to gather fine-grainedprovenance during program execution. We discuss a case study using a web-basedcommunity information management system; then present results regarding the performanceof queries over the provenance data gathered by our logic program interpreter. Our findingsshow that it is possible to gather useful fine-grained provenance during the execution of a …,Proceedings of the 21st International Conference on World Wide Web,2012,1
Database and XML Technologies: First International XML Database Symposium; XSYM 2003; Berlin; Germany; September 8; 2003; Proceedings,Zohra Bellahsène,Modern database systems enhance the capabilities of traditional database systems by theirability to handle any kind of data; including text; image; audio; and video. Today; databasesystemsareparticularlyrelevanttoth…; astheycanprovideinputtocontent generators for Web pages; and can handle queries issuedover the Internet. The eXtensible Markup Language (XML) is used in applications runningthe gamut from content management through publishing to Web services and e-commerce. Itis used as the universal communication language for exchanging music and graphics aswell as purchase orders and technical documentation. As database systems increasinglytalk to each other over the Web; there is a fa-growingdesiretouseXMLasthestandardexch….Asaresult; manyrelational database systems can export data as XML documents and importdata from XML d-uments and provide query and update capabilities for XML data. In …,*,2003,1
Towards building XML statistics for the hidden web,Ashraf Aboulnaga; Jeffrey F Naughton,Abstract There is currently a lot of interest in developing Internet query processors that canpose elaborate queries on XML data on the Web. Such query processors can query datasources that have static XML files; but they should also be able to query “hidden Web” datasources that export an XML view of data stored in a database. To optimize queries thatinvolve these hidden Web data sources; we need to have XML statistics that can be used toestimate the selectivity of queries posed to these sources. Since we can only access thedata at a hidden Web data source by issuing queries; we need to develop on-line XMLstatistics that are built by observing queries to a hidden Web data source and their resultsizes. In this paper; we assume that queries to a hidden Web data source are XPathselections from a virtual XML document that represents all the data at this source. We …,*,2003,1
Dealing with (un) structuredness in XML Data and Queries Using Relational Databases,Rajasekar Krishnamurthy; Jeffrey F Naughton; Jayavel Shanmugasundaram; Eugene Shekita,Abstract An XML database can contain documents with varying degrees of schemainformation. The queries can also range from fully specified structured SQL like queries topartially specified regular path expression queries. Relational databases are widely used tostore and query XML data and various schemes have been proposed to this end. Theyeither use DTDs or assume schemaless data. We show how more advanced schemainformation (like XMLSchema); even if partially available; can be used effectively to answerqueries. We also show how the interaction between the amount of schema informationavailable and the query workload plays an important role in choosing a decompositionstrategy into relational tables; suited to that workload. We propose a simple cost metric forthis purpose. Our experiments indicate that this metric can provide a reasonable choice …,DB seminar at Wise university,2001,1
IDB: Toward the scalable integration of queryable internet data sources,Jaewoo Kang; Mong Li Lee; Jeffrey F Naughton,Abstract As the number of databases accessible on the Web grows; the ability to executequeries spanning multiple heterogeneous queryable sources is becoming increasinglyimportant. To date; research in this area has focused on providing semantic completeness;and has generated solutions that work well when querying over a relatively small number ofdatabases that have static and well-defined schemas. Unfortunately; these solutions do notextend to the scale of the present Internet; let alone the Internet of the future. In this paper;we present an approach that makes the opposite tradeoff: it provides a scalable; unified viewover large numbers of queryable information sources by sacrificing some expressive powerin the set of queries supported. We have developed a prototype system; IDB; whichimplements this approach. The IDB system provides scalability through three main …,Technical Report,2000,1
Materialized view selection for multidimensional datasets,Amit Shukla,Abstract This dissertation describes techniques for speeding up Online AnalyticalProcessing or OLAP queries. OLAP systems allow users to quickly obtain the answers tocomplex business queries. Quickly answering these queries which aggregate large amountsof data; calls for various specialized techniques. One technique used by OLAP systems tospeed up multidimensional data analysis is to precompute aggregates on some subsets ofdimensions and their corresponding hierarchies. We first address the problem of efficientlyestimating aggregate sizes. Precomputation of aggregate data improves query responsetime. However; the decision of what and how much to precompute is a difficult one. It isfurther complicated by the fact that precomputation in the presence of hierarchies can resultin an unintuitively large increase in the amount of storage required by the database …,*,1999,1
Computation of Multidimensional Aggregates,Raghu Ramakrishnan,Abstract At the heart of OLAP or multidimensional data analysis applications is the ability tosimultaneously aggre-gate across many sets of dimensions. Computing multidimensionalaggregates is a performance bottleneck for these applications. We explore various schemesfor implementing multidimensional aggregation; in particular; the CUBE operator [1]proposed by Gray et al. This operator computes aggregates over all subsets of di-mensionsspecified in the CUBE operation; and is equivalent to the union of a number of standardgroup-by operations. We show how the structure of CUBE computation can be viewed interms of a hierarchy of group-by operations; and present a class of sorting-based algorithmsthat overlap the computation of different group-by operations using the least possiblememory for each computation. Our algorithms seek to minimize the number of sorting …,*,1997,1
Problems of Teaching Urdu in France as a Mixed Language ‘,Alain Desoulieres,*,Annual of Urdu Studies,1995,1
The OO7 Object-Oriented Database Benchmark,MJ Carey; DJ DeWitt; JF Naughton,*,Proceedings of the 1993 ACM SIGMOD Conference on Management of Data; Washihgton; DC,1993,1
The oo7 Benchmark: Current Status & Future Directions,Michael J Carey; David J DeWitt; Jeffrey F Naughton,*,HPTS,1993,1
An Evaluation of Non-Equijoin Algorithms,David J DeWitt Jeffrey F Naughton; Donovan A Schneider,Abstract A non-equijoin of relations R and S is a band join if the join predicate requiresvalues in the join attribute of R to fall within a speciﬁed band about the values in the joinattribute of S. We propose a new algorithm; termed a partitioned band join; for evaluatingband joins. We present a comparison between the partitioned band join algorithm and theclassical sort~ merge join algorithm (optimized for band joins) using both an analyticalmodel and an implementation on top of the WiSS storage system. The results show that thepartitioned band join algorithm outperforms sort~ merge unless memory is scarce and theoperands of the join are of equal size. We also describe a parallel implementation of thepartitioned band join on the Gamma database machine; and present data from speedup andscaleup experiments demonstrating that the partitioned band join is efficiently …,International Conference on Very Large Data Bases: September 3-6; 1991; Barcelona (Catalonia; Spain),1991,1
Technical Perspective: Optimized Wandering for Online Aggregation,Jeffrey F Naughton,There is a rich history in the DBMS research literature involving sampling to estimate theresults of queries faster than they can be computed exactly. A particularly interestingexample of this is “Online Aggregation” proposed by Hellerstein et al. in 1997 [2]. There theidea is to combine sampling with a creative and intuitive user interface. Briefly; when a querystarts to run; Online Aggregation will quickly present an estimate of the result of the query(based on data sampled up to that point) and will also present a confidence interval aroundthe estimate. As query execution continues; the estimate is refined; and the confidenceinterval shrinks. Hidden in this attractive idea; however; are some difficult challenges. As anexample; for queries that involve joins; the sampling process is in general slow; especially ifmost of the tuples from one relation participating in the join “match” with only a few tuples …,ACM SIGMOD Record,2017,*
Resource bricolage and resource selection for parallel database systems,Jiexing Li; Jeffrey F Naughton; Rimma V Nehme,Abstract Running parallel database systems in an environment with heterogeneousresources has become increasingly common; due to cluster evolution and increasinginterest in moving applications into public clouds. Performance differences among machinesin the same cluster pose new challenges for parallel database systems. First; for databasesystems running in a heterogeneous cluster; the default uniform data partitioning strategymay overload some of the slow machines; while at the same time it may underutilize themore powerful machines. Since the processing time of a parallel query is determined by theslowest machine; such an allocation strategy may result in a significant query performancedegradation. Second; since machines might have varying resources or performance;different choices of machines may lead to different costs or performance for executing the …,The VLDB Journal,2017,*
ACM SIGMOD Record Volume 45 Issue 3,Yanlei Diao; Vanessa Braganholo; Marco Brambilla; Chee Yong Chan; Rada Chirkova; Zackary Ives; Anastasios Kementsietsidis; Jeffrey Naughton; Frank Neven; Olga Papaemmanoui; Aditya Parameswaran; Anish Das Sarma; Alkis Simitsis; Wang-Chiew Tan; Nesime Tatbul; Marianne Winslett; Jun Yang,Google; Inc. (search). SIGN IN SIGN UP. ACM SIGMOD Record. Volume 45 Issue 3; September2016 table of contents. Editors: Yanlei Diao; University of Massachusetts Amherst. VanessaBraganholo; Universidade Federal Fluminense. Marco Brambilla; Politecnico di Milano.,*,2016,*
System and methods for predicting query execution time for concurrent and dynamic database workloads,*,Systems and methods for predicting query execution time for concurrent and dynamicdatabase workloads include decomposing each query into a sequence of query pipelinesbased on the query plan from a query optimizer; and predicting an execution time of eachpipeline with a progress predictor for a progress chart of query pipelines.,*,2016,*
Technical Perspective: Broadening and Deepening Query Optimization Yet Still Making Progress,Jeffrey F Naughton,Query optimization is a fundamental problem in data management. Simply put; mostdatabase query languages are declarative rather than imperative—that is; they specifyproperties the answer should satisfy; rather than give an algorithm to compute the answer.The best known and most widely used database query language—SQL—is a primeexample of a language for which optimization is essential.By" essential;" I mean thatdatabase optimization is not a matter of shaving 10% or even a factor of 2x from a query'sexecution time. In database query evaluation; the difference between a good plan and a bador even average plan can be multiple orders of magnitude—so successful queryoptimization makes the difference between a plan that runs quickly and one that neverfinishes at all. Accordingly; since the seminal papers in the 1970s; query optimization has …,ACM SIGMOD Record,2016,*
Resource Bricolage for Parallel DBMSs on Heterogeneous Clusters,Jiexing Li; Jeffrey Naughton; Rimma V Nehme,Abstract Running parallel database systems in an environment with heterogeneousresources has become increasingly common; due to cluster evolution and increasinginterest in moving applications into public clouds or shared infrastructures. For databasesystems running in a heterogeneous cluster; the default uniform data partitioning strategymay overload some of the slow machines while at the same time it may underutilize themore powerful machines. Since the processing time of a parallel query is determined by theslowest machine; such an allocation strategy may result in a significant query performancedegradation. We take a first step to address this problem by introducing a technique we callresource bricolage that improves database performance in heterogeneous environments.Our approach quantifies the performance differences among machines with various …,ACM SIGMOD Record,2016,*
Skew-aware storage and query execution on distributed database systems,*,Distributing rows of data in a distributed table distributed across a plurality of nodes. Amethod includes identifying skewed rows of a first table to be distributed in a distributeddatabase system. The skewed rows include a common data value in a column such that theskewed rows are skewed; according to a predetermined skew factor; with respect to otherrows in the first table not having the common data value. Non-skewed rows of the first tablethat are not skewed according to the skew factor are identified. The skewed rows of the firsttable are distributed across nodes in a non-deterministic fashion. The non-skewed rows ofthe first table are distributed across nodes in a deterministic fashion. The rows of the firsttable distributed across the nodes; whether distributed in a deterministic fashion or non-deterministic fashion; are stored in a single table at each of the nodes.,*,2016,*
Formulating global statistics for distributed databases,*,The present invention extends to methods; systems; and computer program products forformulating global statistics for parallel databases. In general; embodiments of the inventionmerge (combine) information in multiple compute node level histograms to create a globalhistogram for a table that is distributed across a number of compute nodes. Merging caninclude aligning histogram step boundaries across the compute node histograms. Mergingcan include aggregating histogram step-level information; such as; for example; equalityrows and average range rows (or alternately equality rows; range rows; and distinct rangerows); across the compute node histograms into a single global step. Merging can accountfor distinct values that do not appear at one or more compute nodes as well as distinctvalues that are counted at multiple compute nodes. A resulting global histogram can be …,*,2015,*
9 Minimizing Expansions of,JEFFREY F NAUGHTON,In recent years there has been a growing acknowledgement among database researchersthat relational query languages are not sufficiently powerful. One proposed alternative torelational query languages is query languages built upon horn-clause logic. While suchlogic-based query languages are more expressive than relational languages; this extraexpressive power does not come for free; as logicbased languages are more difficult toimplement efficiently. Particularly problematic for efficient implementation are recursivedefini-tions.,Algebraic Techniques: Resolution of Equations in Algebraic Structures,2014,*
DBMS: Lessons from the first 50 years; speculations for the next 50,Jeffrey F Naughton,Some of the major themes in DBMS research appeared in the computer science literature asearly as 50 years ago. The community has had a very productive time over the past 50 yearsexploring these themes; in the process contributing to a major software industry and creatinga large and vibrant research community. I will give a subjective and probably highly biasedview of these themes and why they have been so persistent; and speculate on how theymight continue to persist in the future. While we will probably continue to be productive overthe next 50 years as well; there are reasons for concern going forward. I will close with somespeculation on what we might do to deal with this.,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,*
Distributed data management,Jeffrey Naughton,*,International Conference on Management of Data: Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,*
RDBMS Index Support for Sparse Data Sets,Jennifer L Beckmann Eric Chu Jeffrey; F Naughton,Abstract Maintenance costs and storage overheads incurred by indexes often limit thenumber of indexes created per table in an RDBMS. For sparse data; where a table may havehundreds of attributes; indexing only a few attributes means that a vanishingly smallpercentage of attributes will have indexes; which unfortunately means that a table scan isthe only evaluation plan for almost all selection queries on that table. This paperdemonstrates that sparsity of the data actually enables index support for most; if not all;attributes in the data. Our approach leverages" sparse indexes;" which are partial indexesthat store only non-null values. Sparse indexes incur low maintenance costs and storageoverheads because most values in a sparse table are null. Properties of the data lead us totwo other contributions toward index support for sparse data: we show that sparse …,*,2006,*
Matchmaking in Database Systems,Ameet Kini; Srinath Shankar; D DeWitt; J Naughton,*,*,2005,*
The Management of ClassAds in RDBMS and XML Native Storage Systems,Youngsang Shin; Jeffrey F Naughton,ClassAd is a semi-structured data represented by ClassAd language [6] which has beendeveloped in the Condor project [9]. In Condor; most metadata and logs are denoted in thisform of data. Currently this data can be handled through the ClassAd library; which isdesigned for handling a main-memory resident data structure; and optimized for a relativelysmall amount of data and several functions. However; as metadata managed in Condor isgetting larger and more various complex querying functions are needed; the library shouldbe able to handle efficiently data in a disk; and be augmented with the hard coding forvarious additional querying functions. To effectively address these problems; RDBMS couldbe a serious alternative. Furthermore; since this language is semi-structured and easilytranslated into XML (eXtendible Markup Language)[10]; a XML native storage system …,*,2004,*
Cloning and characterization of neocarzinostatin gene cluster from Streptomyces carzinostaticus,Ju Yun Bae,*,*,2004,*
The Lowell Database Research Self Assessment,Stan Zdonik; Jennifer Widom; Gerhard Weikum; Jeff Ullman; Rick Snodgrass; Mike Stonebraker; Avi Silberschatz; Timos Sellis; Hans Schek; Jeff Naughton; David Maier; Serge Abiteboul; Rakesh Agrawal; Phil Bernstein; Mike Carey; Stefano Ceri; Bruce Croft; David DeWitt; Mike Franklin; Hector Garcia Molina; Dieter Gawlick; Jim Gray; Laura Haas; Alon Halevy; Joe Hellerstein; Yannis Ioannidis; Martin Kersten; Michael Pazzani; Mike Lesk,Abstract A group of senior database researchers gathers every few years to assess the stateof database research and to point out problem areas that deserve additional focus. Thisreport summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6;2003 in Lowell; Mass. It observes that information management continues to be a criticalcomponent of most complex software systems. It recommends that database researchersincrease focus on: integration of text; data; code; and streams; fusion of information fromheterogeneous data sources; reasoning about uncertain data; unsupervised data mining forinteresting correlations; information privacy; and self-adaptation and repair.,*,2003,*
Setting of Injection Molding Packing profile,Furong Gao; Xi Chen,Setting of Injection Molding Packing profile.,Advanced Molding and Mold Conference,2003,*
Close to Nature: Summer Camps in Early-twentieth-century America,Keith Mako Woodhouse,*,*,2003,*
Cloning and molecular characterizations of seven genes of fredericamycin biosynthetic cluster from streptomyces griseus,Bensheng Li,*,*,2003,*
Semistructured versus Structured Data-On the Difficulty of Finding Optimal Relational Decompositions for XML Workloads: A Complexity Theoretic Perspective,Rajasekar Krishnamurthy; Venkatesan T Chakaravarthy; Jeffrey F Naughton,*,Lecture Notes in Computer Science,2003,*
Paradise and Direct Broadcast Satellite: A Solution to Battlefield Data Dissemination for the 21st Century,David Dewitt; Jeffrey Naughton; San Dao,Abstract: This paper describes results from research under the Battlefield Aware DataDistribution (BADD) project. The goal of this research is to find new approaches to intelligentdata distribution from distributed servers to roaming clients whose needs evolve with time.The primary focus of this report is on the part of the problem associated with thedevelopment of satellite-based information dissemination network infrastructure and smartpush and filtering techniques. Research focused on possible ways to merge queries basedon more abstract ideas; such as through generalization in a concept hierarchy. A number ofissues in client cache management were also explored. Descriptors:* INFORMATIONTRANSFER;* SATELLITE COMMUNICATIONS;* CLIENT SERVER SYSTEMS;COMMUNICATIONS NETWORKS; SITUATIONAL AWARENESS. Subject Categories …,*,2001,*
Proceedings of the 2000 ACM SIGMOD-International Conference on Management of Data-May 16-18; 2000-Dallas; Texas-Foreword,M Dunham; J Naughton; WD Chen; N Koudas,*,*,2000,*
ACM SIGMOD Record Volume 29 Issue 2,Weidong Chen; Jeffrey Naughton; Philip A Bernstein,Google; Inc. (search). SIGN IN SIGN UP. ACM SIGMOD Record. Volume 29 Issue 2;June 2000 table of contents. Editors: Weidong Chen; Southern Methodist Univ.; Dallas;TX. Jeffrey Naughton; Univ. of Wisconsin-Madison; Madison.,*,2000,*
ISSAC 2000: 7-9 August 2000; University of St. Andrews; Scotland: Proceedings of the 2000 International Symposium on Symbolic and Algebraic Computation,Carlo Traverso; Weidong Chen; Jeffrey F Naughton; Philip A Bernstein,*,*,2000,*
Parallel and Distributed Information Systems,Jeffrey F Naughton; Gerhard Weikum,Abstract. A warehouse is a data repository containing integrated information for efficientquerying and analysis. Maintaining the consistency of warehouse data is challenging;especially if the data sources are autonomous and views of the data at the warehouse spanmultiple sources. Transactions containing multiple updates at one or more sources; eg;batch updates; complicate the consistency problem. In this paper we identity and discussthree fundamental transaction processing scenarios for data warehousing. We define fourlevels of consistency for warehouse data and present a new family of algorithms; the Strobefamily; that maintain consistency as the warehouse is updated; under the variouswarehousing scenarios. All of the algorithms are incremental and can handle a continuousand overlapping stream of updates from the sources. Our implementation shows that the …,*,1998,*
Array-Based Evaluation of Multi-Dimensional Queries in Object-Relational Database Systems Yihong Zhao Computer Sciences Department University of Wisconsin-...,Jeffrey F Naughton,Abstract Since multi-dimensional arrays are a natural data structure for supporting multi-dimensional queries; and object-relational database systems support multi-dimensionalarray ADTs; it is natural to ask if a multi-dimensional array-based ADT can be used toimprove O/R DBMS performance on multi-dimensional queries. As an initial step towardanswering this question; we have implemented a multi-dimensional array in the ParadiseObject-Relational DBMS. In this paper we describe the implementation of this compressed-array ADT; and explore its performance for queries including star-join consolidations andselections. We show that in many cases the array ADT can provide significantly higherperformance than can be obtained by applying techniques such as bitmap indices and star-join algorithms to relational tables.,*,1997,*
Array-Based Evaluation of Multi-Dimensional Queries in,Yihong Zhao; Karthikeyan Ramasamy; Kristin Tufte; Jeffrey F Naughton,*,*,1997,*
Building a Scalable Geo-Spatial DBMS: Technology; Implementation; and Evaluation,Jignesh Patel Jiebing; Jignesh Patel; Jiebing Yu; Navin Kabra; Kristin Tufte; Biswadeep Nag; Josef Burger; Nancy Hall; Karthikeyan Ramasamy; Roger Lueder; Curt Ellmann; Jim Kupsch; Shelly Guo; Johan Larson; David Dewitt; Jeffrey Naughton,Abstract This paper presents a number of new techniques for parallelizing geo-spatialdatabase systems and discusses their implementation in the Paradise object-relationaldatabase system. The effectiveness of these techniques is demonstrated using a variety ofcomplex geo-spatial queries over a 120 GB global geo-spatial data set. 1. Introduction andMotivation The past ten years have seen a great deal of research devoted to extendingrelational database systems to handle geo-spatial workloads; in fact; handling theseworkloads has been one of the driving forces for object-relational database technology.While researchers have always acknowledged the existence of very large data sets in thegeo-spatial domain; the vast majority of research to date has focused on language issues oruniprocessor query evaluation and indexing techniques. This is unfortunate; since the …,In Proceedings of the ACM SIGMOD Conference,1997,*
Hash Join Processing on Shared Memory l\/lultiprocessors,Ambuj Shatdal Jeffrey F Naughton,Abstract While most scalable database systems are designed for a shared nothingarchitecture; advances in hardware technology suggest that in the near future sharedmemory multiprocessors (Sl\/IPs) will be capable of handling all but the largest applications.This raises important questions about how scalable database systems should bearchitected; in particular; do SMPs require specially targeted algorithms; or will algorithmsdeveloped for shared nothing hardware sulfice? As a ﬁrst step towards answering thequestion; with an imple-mentation on an SGI Challenge multiprocessor; we investigate theperformance of hash join algorithms on SMPs. We first show that the shared nothingapproach; if ported to an Sl\/IP by using a transparent message passing library; yields poorperformance. However; we show that by using an optimized message passing library and …,*,1996,*
article no. 0044,Amihood Amir; Marc Andries; Egidio Astesiano; Brenda S Baker; Peter L Bartlett; Catriel Beeri; Gary Benson; Nader H Bshouty; Tsong Yueh Chen; Alessandra Cherubini; Richard Cleve; David P Dobkin; Andrzej Ehrenfeucht; Joost Engelfriet; David Eppstein; Shao C Fang; Martin Farach; Lance Fortnow; Michael Frazier; Zvi Galil; William Gasarch; Ricard GavaldaÁ; Gavin J Gibson; Sally Goldman; Sally A Goldman; Michael T Goodrich; Stephane Grumbach; Dimitrios Gunopulos; Peter J Haas; Thomas R Hancock; David Harel; Tirza Hirst; Juha Honkala; Giuseppe F Italiano; Sanjay Jain; Paris Kanellakis; Sampath Kannan; Ravi Krishnamurthy; Ajay D Kshemkalyani; Martin Kummer; Han La Poutre; Leonid Libkin; Ming Li; Philip M Long; Wolfgang Maass; Sleiman Matar; H David Mathias; Tova Milo; Nina Mishra; Shinichi Morishita; Jeffrey F Naughton; Noam Nisan; Jan Paredaens; Nicholas Pippenger; Leonard Pitt; Raghu Ramakrishnan; Sridhar Ramaswamy; Grzegorz Rozenberg; Pierluigi San Pietro; Robert E Schapire; Rainer Schuler; Linda M Sellie; S Seshadri; Arun Sharma; Oded Shmueli; Hans Ulrich Simon; Thomas H Spencer; Frank Stephan; Arun N Swami; Christino Tamon; Vijay K Vaishnavi; Jan Van den Bussche; Dirk van Gucht; Santosh S Venkatesh; Gottfried Vossen; Dan E Willard; Robert C Williamson; Limsoon Wong; Tomoyuki Yamakami; Chee Yap; Elena Zucca; David Zuckerman,*,journal of computer and system sciences,1996,*
Problems Related to the Teaching of Urdu in the Netherlands,Mohan K Gautam,*,Annual of Urdu Studies,1995,*
OODB Loading Revisited: The Partitioned-List Approach,J Wiener; J Naughton,Object-oriented and object-relational databases (OODB) need to be able to load the vastquantities of data that OODB users bring to them. Loading OODB data is significantly morecomplicated than loading relational data due to the presence of relationships; or references;in the data; the presence of these relationships means that naive loading algorithms areslow to the point of being unusable. In our previous work; we presented the late-invsortalgorithm; which performed significantly better than naive algorithms on all the data sets wetested. Unfortunately; further experimentation with the late-invsort algorithm revealed that forlarge data sets (ones in which a critical data structure of the load algorithm does not fit in theperformance of late-invsort rapidly degrades to where it; too; is unusable. In this paper wepropose a new algorithm; the partitioned-list algorithm; whose performance almost …,*,1995,*
Parsets for parallelizing oodbms traversals: a performance evaluation,David DeWitt; Jeffrey Naughton; John Shafer; Shivakumar Venkataraman,*,Proceedings of the 3rd International Conference on Parallel and Distributed Information Systems,1994,*
Shoring Up Persistent Applications,David J DeWitt; Seth J White,*,*,1994,*
1. Editorial Board,Serge Abiteboul; Paris Kanellakis; Ronald Fagin; Gabriel M Kuper; Moshe Y Vardi; Catriel Beeri; Yoram Kornatzky; Mariano P Consens; Alberto O Mendelzon; Peter Z Revesz; Ron van der Meyden; Richard J Lipton; Jeffrey F Naughton; Donovan A Schneider; S Seshadri,Read the latest articles of Theoretical Computer Science at ScienceDirect.com;Elsevier's leading platform of peer-reviewed scholarly literature.,Theoretical Computer Science,1993,*
Michael J. Carey,Jeffrey F Naughton,Abstract The OO7 Benchmark represents a comprehensive test of OODBMS performance. Inthis report we describe the benchmark and present performance results from itsimplementation in three OODB systems. It is our hope that the OO7 Benchmark will provideuseful insight for end-users evaluating the performance of OODB systems; we also hope thatthe research community will find that OO7 provides a database schema; instance; andworkload that is useful for evaluating new techniques and algorithms for OODBMSimplementation.,*,1993,*
The 007 Benchmark,Michael J Carey David J DeWitt; Jeffrey F Naughton,Abstract The 007 Benchmark represents a comprehensive test of OODBMS performance. Inthis report we describe the benchmark and present performance results from itsimplementation in three OODB systems. It is our hope that the 007 Benchmark will provideuseful insight for end-users evaluating the performance of OODB systems; we also hope thatthe research community will find that 007 provides a database schema; instance; andworkload that; is useful for evaluating new techniques and algorithms for OODBMSimplementation.,Proceedings,1993,*
The DC)? Benchmarl,Michael J Carey David J DeWitt; Jeffrey F Naughton,Abstract The OO7 Benchmark represents a comprehensive test of QODBMS performance. Inthis report we describe the benchmark and present performance results from itsimplementation in three OODB systems. It is our hope that the OO7 Benchmark will provideuseful insight for end-users evaluating the performance of OODB systems; we also hope thatthe research community will find that OO7 provides a database schema; instance; andworkload that is useful for evaluating new techniques and algorithms for OODBMSimplementation.,*,1993,*
Resource Allocation and Scheduling,Jeffrey F Naughton,Some of the tradeoffs examined in this paper are complex and non-intuitive. As an example;consider the problem of memory allocation for a workload that consists of a single long-running query containing one or more joins; running concurrently with a stream of small;“rifleshot" transactions. Assume that each small transaction accesses a single tuple from acommon relation via an index lookup; and that the join query accesses separate relations;so there is no data,*,1992,*
Q11 Estimating the Size of Projections,Jeffrey F Naughton; S Seshadri,Abstract We present a new sampling algorithm for estimating the number of tuples in theprojection of a relation. The algorithm requires no assumptions about the distribu-tions ofvalues in the attributes of the relation and converges faster and smoother than previoussampling algorithms for the problem. We give both a sound theoret-ical basis for thealgorithm and experimental data from an implementation of the algorithm.,*,1991,*
JOURNAL OF COMPUTER AND SYSTEM SClENCES 42; 399 (1991),WILLIAM AIELLO; ERIC BACH; RICHARD BEIGEL; MANFRED BROY; PAVOL DURIS; HERBERT EDEJSBRUNNER; ZVI GALIL; MAX GARZON; GIOVANNETTI ELIO; LEONIDAS J GLJIBAS; YURI GUREVICH; JOHAN HASTAD; ARKADY KANEVSKY; S KOSARAJU; CHRISTIAN LENGAUER; GIORGIO LEVI; MING LI; VIJAYA RAUACHANVRAN; ABHIRAM G RANADE; CELIA WRATHALL; ANDREW CHI-CHIH YAO; JEFFREY F NAUGHTON; FRIEDRICH OTTO; YECHEZKEL ZALCSTEIN,*,*,1991,*
A survey of Midwest employee recreation programs conducted outdoors,Kevin A Falkenberg,*,*,1986,*
Using Mathematical Induction to Design Computer Algorithms,Udi Manber,ABSTRACT An analogy between proving mathematical theorems and designing computeralgorithms is explored in this paper. This analogy provides an elegant methodology fordesigning algorithms; explaining their behavior; and understanding their key ideas. Thepaper identifies several mathematical proof techniques; mostly based on induction; andpresents analogous algorithm design techniques. Each of these techniques is illustrated byseveral examples of algorithms.,*,1986,*
Using Statistical Techniques to Find Predictive Relationships Between Variables,JH Halton,*,*,1981,*
Survey of the attitudes of superintendents; school board presidents; and principals toward health education by school district size,Janice Lee Thornberg,*,*,1979,*
A survey of parental attitudes concerning the acceptance of death education in the curriculum at Cashton High School; Cashton; Wisconsin,Gary Hanson,*,*,1978,*
A Survey of Practicing Pharmacists in Wisconsin,David Stewart Forbes,*,*,1971,*
An approach to the prescription pricing problem,William Shoulden Apple,*,*,1954,*
A survey of student lighting conditions at the University of Wisconsin,William Francis Cormack,*,*,1939,*
The clinical significance of high T waves in the electrocardiogram,Lester Paul Brillman,*,*,1938,*
A Clinical Survey of Cord Bladder,Henry Adolph Anderson,*,*,1937,*
Bulk Loading into an OODB: A Performance Study,Jeffrey F Naughton,Abstract As object-oriented databases (OODB) attract an increasingly large community ofusers; these users bring with them large quantities of legacy data (hundreds and thousandsof megabytes; and sometimes hundreds of gigabytes). In addition; scientific OODB userscontinually generate new data; often tens and hundreds of megabytes at a time. All this datamust be loaded into the OODB. Every relational database system has a load utility; butnearly all OODBs do not. The process of loading data into an OODB is complicated by inter-object references; or relationships; in the data. These relationships are expressed in theOODB as object identifiers; which are not known at the time the load data is generated; theymay contain cycles; and there may be implicit system-maintained inverse relationships thatmust also be stored. We introduce six algorithms for loading data into an OODB and …,*,*,*
Relational Query Processing,Jeffrey F Naughton,Abstract The current main memory (DRAM) access speeds lag far behind CPU speeds.Cache memory; made of static RAM; is being used in today's architectures to bridge this gap.It provides access latencies of 2–4 processor cycles; in contrast to main memory whichrequires 15-25 cycles. Therefore; the performance of the CPU depends upon how well thecache can be utilized. We show that there are significant benefits in redesigning ourtraditional query processing algorithms so that they can make better use of the cache. Thenew algorithms run 8%–200% faster than the traditional ones.,*,*,*
Haryadi S. Gunawi,Haryadi S Gunawi; Sriram Subramanian; Rajiv Vaidyanathan; Yupu Zhang; Andrea C Arpaci-Dusseau; Remzi H Arpaci-Dusseau; Jeffrey F Naughton; Cindy Rubio-Gonzalez; Ben Liblit; Abhishek Rajimwale; Vijayan Prabhakaran; Swetha Krishnan; Lakshmi N Bairavasundaram; Nitin Agrawal,My doctoral research centers around file and storage systems; specifically focusing on howto design a better framework to improve robustness. This research is motivated by threetrends that will dominate the storage systems of tomorrow: users are storing increasinglymassive amounts of data; storage software complexity is growing; and the use of cheap andless reliable hardware is increasing nowadays. The combination of these trends presents uswith a terrific challenge: How can we promise users that storage systems work robustly inspite of their massive software complexity and all the complex disk failures that can arise?My thesis work consists of three parts that deal with the challenges above. First; I analyzedcommodity file systems to understand how they handle various types of disk failures.Second; I prototyped a new reliability infrastructure for file systems. The goal of the new …,*,*,*
Janet L. Wiener,Jerey F Naughton,*,*,*,*
Michael J. Carey; David J. DeWitt; Michael J. Franklin,Mark L McAulie; Jerey F Naughton; Daniel T Schuh; Marvin H Solomon; CK Tan; Odysseas G Tsatalos; Seth J White; Michael J Zwilling,*,*,*,*
Program Vice-Chairs,Jeff Naughton; Sunita Sarawagi; Hank Korth; Arnie Rosenthal; Jeff Ullman; Hans Schek; Phil Bernstein; Donald Kossmann; Stavros Christodoulakis; Theo Haerder; Beng Chin Ooi; HV Jagadish; Gerhard Weikum,Page 1. xix Program Vice-Chairs Jeff Naughton; University of Wisconsin; USA Sunita Sarawagi;IBM Almaden; USA Hank Korth; Lucent - Bell Labs; USA Arnie Rosenthal; Mitre; USA Jeff Ullman;Stanford University; USA Hans Schek; ETH Zurich; Switzerland Phil Bernstein; Microsoft; USADonald Kossmann; University of Passau; Germany Stavros Christodoulakis; University of Crete;Greece Theo Haerder; University of Kaiserslautern; Germany Beng Chin Ooi; National Universityof Singapore; Singapore HV Jagadish; University of Illinois at Urbana-Champaign; USA AwardCommittee Members Hank Korth Donald Kossmann Arnie Rosenthal Gerhard Weikum,*,*,*
The OO7 Benchmark* Michael J. Carey David J. DeWitt Jeffrey F. Naughton Computer Sciences Department University of Wisconsin-Madison Version of January 21;...,Michael J Carey David J DeWitt,Abstract The OO7 Benchmark represents a comprehensive test of OODBMS performance. Inthis report we describe the benchmark and present performance results from itsimplementation in four OODB systems. It is our hope that the OO7 Benchmark will provideuseful insight for end-users evaluating the performance of OODB systems; we also hope thatthe research community will find that OO7 provides a database schema; instance; andworkload that is useful for evaluating new techniques and algorithms for OODBMSimplementation.,*,*,*
TheOO7 benchmark,MJ Carey; DJ DeWitt; JF Naughton,*,ACMSIGMODInt. Conf. OnManagementof Data,*,*
Leveraging Replication for Energy Management,Willis Lang; Jignesh M Patel; Jeffrey F Naughton,*,*,*,*
Heterogeneous and Fe&rated Databases Semantic Integration in Heterogeneous Databases Using Neural Networks,Wen-Syan Li; Chris Clifton; NB Uris; WA Gray; RF Churchhouse; Dirk Jonscher; Klaus R D&rich; Pipellned Parallelism; Waqar Hasan; Rajeev Motwani; HV Jagadish; Daniel Lieuwen; Rajeev Rastogi; Avi Silberschatz; S Sudarshan; Mukesh K Mohania; NL Sarda; C&u Galindo-Legaria; Arjan Pellet&oft; Martin Kersten; Eric Amiel; Marie-Jo Bellosta; Eric Dujardin; Eric Simon; Janet L Wiener; Jeffrey F Naughton; C Collet; T Coupaye; T Svensen,Semantic Integration in Heterogeneous Databases Using Neural Networks Wen-Syan Liand Chris Clifton.............................................................................................. 1 Providing DynamicSecurity Control in a Federated Database NB Uris; WA Gray and RF Churchhouse...................................................................... 13 An Approach for Building Secure Database FederationsDirk Jonscher and Klaus R. D&rich..................................................................................... 24,*,*,*
PART 1: FULL PAPERS,Constrained Physical Design Tuning; Nicolas Bruno; Surajit Chaudhuri; David DeWitt; Eric Robinson; Srinath Shankar; Erik Paulson; Jeffrey Naughton; Andrew Krioukov; Joshua Royalty; Michael Hay; Gerome Miklau; David Jensen; Don Towsley; Philipp Weis,*,*,*,*
YAWN!(YET ANOTHER WINDOW ON NAIL!) t,Jeffrey F Naughton; Yatin Saraiya; Allen Van Geldertt,ABSTRACT We present the important features of the NAIL! knowledge-base system. Thesein clude the “capture-rule” architecture for integrating different logic-evaluation strate gies;the NAIL! strategy-selection algorithm; and the NAIL! intermediate code for coupling with adatabase back end.,Data ri,*,*
On Optimal Differentially Private Mechanisms for Threshold and Range Queries,Chen Zeng; Jin-Yi Cai; Pinyan Lu; Jeffrey F Naughton,*,*,*,*
Program Co-Chairs,Jeffrey Naughton; Gerhard Weikum; Henry Korth; Chungmin Chen; Paul Attie; Weiyi Meng; Cyril Orji; Honesty Young; Eugene Shekita,*,*,*,*
Memory Management for Scalable Web Data Servers,Miron Livny; Jeffrey F Naughton,*,*,*,*
Bulletin of the Technical Committee on,PM Deshpande; JF Naughton; K Ramasamy; A Shukla; K Tufte; Y Zhao,*,*,*,*
On the Integration of Structure Indexes and Inverted Lists Paper Id: 616,Raghav Kaushik; Rajasekar Krishnamurthy; Jeffrey F Naughton; Raghu Ramakrishnan,Abstract We consider the problem of how to combine structure indexes and inverted lists toanswer queries over a native XML DBMS; where the queries specify both path and keywordconstraints. We augment the inverted list entries to integrate them with a given structureindex; and present novel algorithms for evaluating branching path expressions. Ourexperiments show the benefit of integrating the two forms of indexes. We also consider theproblem of evaluating path expression queries with (several alternative) extensions thatincorporate relevance ranking. By integrating the above techniques with the ThresholdAlgorithm proposed by Fagin et al.; we obtain instance optimal algorithms to push down topk computation.,*,*,*
Reading; MA; 1973. Kri P. Krishnan; Online Prediction Algorithms for Databases and Operating Systems;" Brown Univ. Ph. D. thesis; May; 1995; also available as Br...,LiN RJ Lipton; JF Naughton,*,*,*,*
Knu D. Knuth; The Art of Computer Programming; Vol. 3: Sorting and Searching; Addison-Wesley; Reading; MA; 1973. Kri P. Krishnan; Online Prediction Algorithms f...,LiN RJ Lipton; JF Naughton,Table 1: Table of grades for the di erent algorithms. A grade of x means that the algorithmdid well" as de ned in Section 7.4 for x of the queries. The right-most column presents resultsfor the random sampling strategy from Section 8. Total data structure memory usage in allcases is 389 bytes. lenient with negative patterns; since we expect them to occur lessfrequently. We are also less critical about under-estimating as opposed to over-estimating.Table 1 presents the grades of the di erent algorithms for positive single patterns; positivedouble patterns; and negative patterns; where a negative pattern is obtained by introducingtwo errors into a single positive pattern. We conclude from Table 1 and our discussion fromSections 7.1 7.3 that strategies I1; I,*,*,*
A Brief Note on Path Indexability,Raghav Kaushik; Jeffrey F Naughton,In [1]; a theory of indexability is developed. The authors essentially classify all indexingproblems as one of blocking up the data elements (from set I) into fixed size blocks for agiven query workload. The performance of the indexing scheme S is then measured throughtwo metrics: the access overhead a and the storage redundancy r. The access overhead fora given query Q is the minimum number of blocks of S that cover Q divided by the ideal cost;which is⌈| Q|/B⌉ where| Q| is the result size of Q and B is the fixed block size. The accessoverhead of the scheme itself can be measured by either the average or maximum over allqueries in the workload. The average storage redundancy of an indexing schemecorresponds to the average number of blocks in which an element of I occurs. The maximumstorage redundancy is the maximum number of blocks in which an element of I occurs …,*,*,*
On the Computation of Multidimensional Aggregates,Jeffrey F Naughton; Raghu Ramakrishnan; Sunita Sarawagi,Abstract At the heart of all OLAP or multidimensional data analysis applications is the abilityto simultaneously aggregate across many sets of dimensions. Computing multidimensionalaggregates is a performance bottleneck for these applications. This paper presents fastalgorithms for computing a collection of group bys. We focus on a special case of theaggregation problem-computation of the CUBE operator. The CUBE operator requirescomputing group-bys on all possible combinations of a list of attributes; and is equivalent tothe union of a number of standard group-by operations. We show how the structure of CUBEcomputation can be viewed in terms of a hierarchy of group-by operations. Our algorithmsextend sort-based and hashbased grouping methods with several. optimizations; likecombining common operations across multiple groupbys; caching; and using pre …,*,*,*
Set-Valued Attributes in O/R DBMS: Implementation Options and Performance Implications,Karthikeyan Ramasamy; Prasad M Deshpande; Jeffrey F Naughton; David Maier,ABSTRACT Although adding set-valued attributes to relational database managementsystems is certainly not a new idea; it is only recently that set-valued attributes have made itto the main-stream of the O/R DBMS world. All the major O/R DBMS vendors either currentlysupport or will support set-valued attributes in their universal servers; furthermore; set-valued attributes are a significant new component of the SQL3 standard. While sets havereceived a good deal of attention in the database modeling and language design literature;to date there has been very little written on implementation options for sets and how theseoptions impact performance. As a first step toward filling this gap we have implemented andanalyzed two broad categories of storage representations for set-valued attributes---Inlinedand External---in an object-relational DBMS. Our experiments show that external …,*,*,*
Data Engineering,PM Deshpande; JF Naughton; K Ramasamy; A Shukla; K Tufte; Y Zhao,Abstract “OLAP” or multi-dimensional analysis workloads present a number of interestingchallenges and opportunities for database developers and researchers. While the OLAPgoal of extremely fast response times is hard to meet in general; the structure of theunderlying multidimensional model (whether implemented by arrays or by tables) provides aframework that can be used to approach this performance goal for this class of queries. Inthis note we give an overview of our research into these problems.,*,*,*
