Discovering data quality rules,Fei Chiang; Renée J Miller,Abstract Dirty data is a serious problem for businesses leading to incorrect decision making;inefficient daily operations; and ultimately wasting both time and money. Dirty data oftenarises when domain constraints and business rules; meant to preserve data consistency andaccuracy; are enforced incompletely or not at all in application code. In this work; wepropose a new data-driven tool that can be used within an organization's data qualitymanagement process to suggest possible rules; and to identify conformant and non-conformant records. Data quality rules are known to be contextual; so we focus on thediscovery of context-dependent rules. Specifically; we search for conditional functionaldependencies (CFDs); that is; functional dependencies that hold only over a portion of thedata. The output of our tool is a set of functional dependencies together with the context in …,Proceedings of the VLDB Endowment,2008,222
Framework for evaluating clustering algorithms in duplicate detection,Oktie Hassanzadeh; Fei Chiang; Hyun Chul Lee; Renée J Miller,Abstract The presence of duplicate records is a major data quality concern in largedatabases. To detect duplicates; entity resolution also known as duplication detection orrecord linkage is used as a part of the data cleaning process to identify records thatpotentially refer to the same real-world entity. We present the Stringer system that providesan evaluation framework for understanding what barriers remain towards the goal of trulyscalable and general purpose duplication detection algorithms. In this paper; we useStringer to evaluate the quality of the clusters (groups of potential duplicates) obtained fromseveral unconstrained clustering algorithms used in concert with approximate jointechniques. Our work is motivated by the recent significant advancements that have madeapproximate join algorithms highly scalable. Our extensive evaluation reveals that some …,Proceedings of the VLDB Endowment,2009,148
Seeking stable clusters in the blogosphere,Nilesh Bansal; Fei Chiang; Nick Koudas; Frank Wm Tompa,Abstract The popularity of blogs has been increasing dramatically over the last couple ofyears. As topics evolve in the blogosphere; keywords align together and form the heart ofvarious stories. Intuitively we expect that in certain contexts; when there is a lot of discussionon a specific topic or event; a set of keywords will be correlated: the keywords in the set willfrequently appear together (pair-wise or in conjunction) forming a cluster. Note that suchkeyword clusters are temporal (associated with specific time periods) and transient. Astopics recede; associated keyword clusters dissolve; because their keywords no longerappear frequently together. In this paper; we formalize this intuition and present efficientalgorithms to identify keyword clusters in large collections of blog posts for specific temporalintervals. We then formalize problems related to the temporal properties of such clusters …,Proceedings of the 33rd international conference on Very large data bases,2007,84
A unified model for data and constraint repair,Fei Chiang; Renee J Miller,Integrity constraints play an important role in data design. However; in an operationaldatabase; they may not be enforced for many reasons. Hence; over time; data may becomeinconsistent with respect to the constraints. To manage this; several approaches haveproposed techniques to repair the data; by finding minimal or lowest cost changes to thedata that make it consistent with the constraints. Such techniques are appropriate for the oldworld where data changes; but schemas and their constraints remain fixed. In many modernapplications however; constraints may evolve over time as application or business ruleschange; as data is integrated with new data sources; or as the underlying semantics of thedata evolves. In such settings; when an inconsistency occurs; it is no longer clear if there isan error in the data (and the data should be repaired); or if the constraints have evolved …,Data Engineering (ICDE); 2011 IEEE 27th International Conference on,2011,54
Continuous data cleaning,Maksims Volkovs; Fei Chiang; Jaroslaw Szlichta; Renée J Miller,In declarative data cleaning; data semantics are encoded as constraints and errors arisewhen the data violates the constraints. Various forms of statistical and logical inference canbe used to reason about and repair inconsistencies (errors) in data. Recently; unifiedapproaches that repair both errors in data and errors in semantics (the constraints) havebeen proposed. However; both data-only approaches and unified approaches are by andlarge static in that they apply cleaning to a single snapshot of the data and constraints. Weintroduce a continuous data cleaning framework that can be applied to dynamic data andconstraint environments. Our approach permits both the data and its semantics to evolveand suggests repairs based on the accumulated evidence to date. Importantly; our approachuses not only the data and constraints as evidence; but also considers the past repairs …,Data Engineering (ICDE); 2014 IEEE 30th International Conference on,2014,48
Method and apparatus for automatic recommendation and selection of clustering indexes,*,A method; apparatus; and computer instructions for selecting and recommending ofclustering indexes are provided. Baseline run time cost estimates are calculated based ondatabase designs. Workload benefit of a candidate clustering design is calculated andcompared to the baseline costs. If the workload benefit outweighs the baseline costs;clustering dimension solutions originated from the record identifier (RID) based index areidentified. A clustering design is selected based on the identified clustering dimensionsolutions and the total number of recommended clustering dimensions for a given table.Based on the number of dimensions observed; either a multidimensional clustering solutionor a RID based clustering index is recommended.,*,2009,25
Combining quantitative and logical data cleaning,Nataliya Prokoshyna; Jaroslaw Szlichta; Fei Chiang; Renée J Miller; Divesh Srivastava,Abstract Quantitative data cleaning relies on the use of statistical methods to identify andrepair data quality problems while logical data cleaning tackles the same problems usingvarious forms of logical reasoning over declarative dependencies. Each of theseapproaches has its strengths: the logical approach is able to capture subtle data qualityproblems using sophisticated dependencies; while the quantitative approach excels atensuring that the repaired data has desired statistical properties. We propose a novelframework within which these two approaches can be used synergistically to combine theirrespective strengths. We instantiate our framework using (i) metric functional dependencies;a type of dependency that generalizes functional dependencies (FDs) to identifyinconsistencies in domains where only large differences in metric data are considered to …,Proceedings of the VLDB Endowment,2015,20
An XML index advisor for DB2,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Fei Chiang; Andrey Balmin; Kevin Beyer; Calisto Zuzarte,Abstract XML database systems are expected to handle increasingly complex queries overincreasingly large and highly structured XML databases. An important problem that needs tobe solved for these systems is how to choose the best set of indexes for a given workload.We have developed an XML Index Advisor that solves this XML index recommendationproblem and is tightly coupled with the query optimizer of the database system. We haveimplemented our XML Index Advisor for DB2. In this demonstration we showcase the newquery optimizer modes that we added to DB2; the index recommendation process; and theeffectiveness of the recommended indexes.,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,11
ACTIVE REPAIR OF DATA QUALITY RULES,Fei Chiang; Renée J Miller,Abstract: The use of data quality rules; which capture business rules and domain constraints;is central to most data quality processes. Poor data quality often arises when the data andthese rules (which are meant to preserve data integrity) become inconsistent. To resolveinconsistencies; organizations often implement specific; sometimes manual; sometimescomputer-aided; cleansing routines to fix the errors. This solution necessitates frequentrepetition of the data cleaning to resolve inconsistencies continually as the data evolves orgrows. It is important to recognize that modern organizations may be as dynamic as theirdata. The business rules; application domain constraints; and data semantics will evolve. Asbusiness policies change; as data is integrated with new sources; and as the underlyingdata evolves; it becomes necessary to manage; evolve; and repair both the data and the …,Proceedings of the 16th International Conference on Information Quality (ICIQ),2011,8
XML index recommendation with tight optimizer coupling,Iman Elghandour; Ashraf Aboulnaga; Daniel C Zilio; Fei Chiang; Andrey Balmin; Kevin Beyer; Calisto Zuzarte,XML database systems are expected to handle increasingly complex queries overincreasingly large and highly structured XML databases. An important problem that needs tobe solved for these systems is how to choose the best set of indexes for a given workload. Inthis paper; we present an XML Index Advisor that solves this XML index recommendationproblem and has the key characteristic of being tightly coupled with the query optimizer. Werely on the optimizer to enumerate index candidates and to estimate the benefit gained frompotential index configurations. We expand the set of candidate indexes obtained from thequery optimizer to include more general indexes that can be useful for queries other thanthose in the training workload. To recommend an index configuration; we introduce two newsearch algorithms. The first algorithm finds the best set of indexes for the specific training …,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,6
An evaluation of clustering algorithms in duplicate detection,Bilal Hussain; Oktie Hassanzadeh; Fei Chiang; Hyun Chul Lee; Renée J Miller,HC Lee Content Understanding & Personalization; LinkedIn E-mail: culee@ linkedin. comgration and cleaning of large databases. In this paper; we focus on a class of duplicatedetection algorithms that rely on clustering a similarity graph. Each node in the similaritygraph represents a record and the weight of an edge connecting two nodes reflects theamount of similarity between the corresponding records. The similarity graph can beefficiently constructed using state-of-the-art similarity join techniques. For duplicatedetection; a clustering algorithm over the similarity graph is used to produce sets of recordsthat are likely to represent the same entity. In this paper; we present a framework forevaluating the effectiveness of clustering algorithms for duplicate detection. We present theresults of our extensive evaluation of a wide range of clustering algorithms. Our …,Technical Report CSRG-620; University of Toronto; Department of Computer Science,2013,5
Autodict: Automated dictionary discovery,Fei Chiang; Periklis Andritsos; Erkang Zhu; Renée J Miller,An attribute dictionary is a set of attributes together with a set of common values of eachattribute. Such dictionaries are valuable in understanding unstructured or loosely structuredtextual descriptions of entity collections; such as product catalogs. Dictionaries provide thesupervised data for learning product or entity descriptions. In this demonstration; we willpresent AutoDict; a system that analyzes input data records; and discovers high qualitydictionaries using information theoretic techniques. To the best of our knowledge; AutoDict isthe first end-to-end system for building attribute dictionaries. Our demonstration willshowcase the different information analysis and extraction features within AutoDict; andhighlight the process of generating high quality attribute dictionaries.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,5
A data quality framework for customer relationship analytics,Fei Chiang; Siddharth Sitaramachandran,Abstract Poor data quality has become an increasingly pervasive problem for organizationsleading to operational inefficiency; increased costs; and missed opportunities. As highquality data is a prerequisite to trusted data analysis; we propose a framework that focuseson improving the data model to improve data quality. In particular; we show how changes tothe underlying data design can achieve key data quality properties. We conduct a casestudy that demonstrates the application of the framework to a customer relationshipmanagement (CRM) problem. Our evaluation shows that a set of CRM queries can beefficiently run over data sizes of up to 10 million records; and organizations can glean newinsights about customer preferences and activity.,International Conference on Web Information Systems Engineering,2015,3
Models for distributed; large scale data cleaning,Vincent J Maccio; Fei Chiang; Douglas G Down,Abstract Poor data quality is a serious and costly problem affecting organizations across allindustries. Real data is often dirty; containing missing; erroneous; incomplete; and duplicatevalues. Declarative data cleaning techniques have been proposed to resolve some of theseunderlying errors by identifying the inconsistencies and proposing updates to the data.However; much of this work has focused on cleaning data in static environments. Given theBig Data era; modern applications are operating in dynamic data environments where largescale data may be frequently changing. For example; consider data in sensor environmentswhere there is a frequent stream of data arrivals; or financial data of stock prices and tradingvolumes. Data cleaning in such dynamic environments requires understanding theproperties of the incoming data streams; and configuration of system parameters to …,Pacific-Asia Conference on Knowledge Discovery and Data Mining,2014,3
Index selection for XML database systems,*,A method; computer-implemented system; and computer program product for creatingindexes over XML data managed by a database system are provided. The method;computer-implemented system; and computer program product provide for receiving aworkload for the XML data; the workload including one or more database statements;utilizing an optimizer of the database system to enumerate a set of one or more pathexpressions by creating a virtual universal index based on the workload received andmatching a path expression to the virtual universal index; and recommending one or morepath expressions from the set of one or more candidate path expressions to create theindexes over the XML data.,*,2017,2
Repairing integrity rules for improved data quality,Fei Chiang; Yu Wang,Integrity constraints are the primary tool used to capture business rules and domainconstraints in data management systems. When these constraints are not strictly enforced;poor data quality often arises; as inconsistencies occur between the data and the set ofconstraints. To resolve these inconsistencies; organisations often implement specific;sometimes manual; cleansing routines to fix the errors. As modern systems are expected tohandle increasing amounts of highly heterogeneous data; often in dynamic dataenvironments where the data and the constraints may change; manual cleansing routinesare insufficient to handle this increased scale and heterogeneity. In this work; we present aset of new constraint repair operations that can be incorporated into a data quality tool thatprovides automated support for both data and constraint repair and management. Our …,International Journal of Information Quality,2014,2
An algebraic approach towards data cleaning,Ridha Khedri; Fei Chiang; Khair Eddin Sabri,Abstract There has been a proliferation in the amount of data being generated and collectedin the past several years. One of the leading factors contributing to this increased data scaleis cheaper commodity storage; making it easier for organisations to house large data storescontaining massive amounts of historical data. To effectively analyse these data sets; apreprocessing step is often required as most real data sets are inherently dirty andinconsistent. Existing data cleaning tools have focused on cleaning the errors at hand. In thispaper; we take a more formal approach and propose the use of information algebra as ageneral theory to describe structured data sets and data cleaning. We formally define thenotion of association rule; association function; and we present results relating theseconcepts. We also propose an algorithm for generating association rules from a given …,Procedia Computer Science,2013,2
Efficient discovery of ontology functional dependencies,Sridevi Baskaran; Alexander Keller; Fei Chiang; Lukasz Golab; Jaroslaw Szlichta,Abstract Functional Dependencies (FDs) define attribute relationships based on syntacticequality; and; when used in data cleaning; they erroneously label syntactically different butsemantically equivalent values as errors. We enhance dependency-based data cleaningwith Ontology Functional Dependencies (OFDs); which express semantic attributerelationships such as synonyms and is-a hierarchies defined by an ontology. Our technicalcontributions are twofold: 1) theoretical foundations for OFDs; including a set of sound andcomplete axioms and a linear-time inference procedure; and 2) an algorithm for discoveringOFDs (exact ones and ones that hold with some exceptions) from data that uses the axiomsto prune the exponential search space in the number of attributes. We demonstrate theefficiency of our techniques on real datasets; and we show that OFDs can significantly …,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,2017,1
Towards a Unified Framework for Data Cleaning and Data Privacy,Yu Huang; Fei Chiang,Abstract Data quality has become a pervasive challenge for organizations as they wranglewith large; heterogeneous datasets to extract value. Existing data cleaning solutions havefocused on scalable techniques to resolve inconsistencies quickly. However; given theproliferation of sensitive; confidential user information; data privacy concerns have largelyremained unexplored in data cleaning techniques. In this work; we present a new privacy-aware; data cleaning framework that aims to resolve data inconsistencies while minimizingthe amount of information disclosed. We present a set of data disclosure operations thatfacilitate the data cleaning process; and propose two information-theoretic measures forprivacy loss and data utility that are used to correct inconsistencies in the data.,International Conference on Web Information Systems Engineering,2015,1
CONDOR: A system for constraint discovery and repair,Joshua Segeren; Dhruv Gairola; Fei Chiang,Abstract We present CONDOR; a tool for managing constraints towards improved dataquality. As increasing amounts of heterogeneous data are being generated; integrityconstraints are the primary tool for enforcing data integrity. It is essential that an accurateand up-to-date set of constraints exist to validate that the correct application semantics arebeing enforced. We consider the widely used constraint; functional dependencies (FDs).CONDOR is an integrated system that identifies inconsistent data values (along withsuggestions for clean values); and generates repairs to both the data and/or FDs to resolveinconsistencies. We extend the set of FD repair operations proposed in past work; by (1)adding a set of attributes to an FD;(2) transforming an FD to a conditional functionaldependency (CFD); and (3) identifying redundant attributes in an FD. Our demonstration …,Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,2014,1
Automated dictionary discovery for the online marketplace,Fei Chiang; Renée J Miller,Abstract Shopping online has become a prolific activity as the number of online vendors andconsumers continue to rise each year. In 2009; almost $15 billion in goods and serviceswere ordered online by Canadians [1]. About 53% of these consumers' window shop'bydoing product research before actually making a purchase. Therefore; it is important thatonline vendors provide up-to-date and accurate product information to assist users inmaking educated decisions. In this poster; we present a tool that discovers product features;which will assist vendors and consumers to more accurately compare products in the onlinemarketplace,Proceedings of the 2012 iConference,2012,1
Quantifying duplication to improve data quality,Yu Huang; Fei Chiang; Albert Maier; Martin Petitclerc; Yannick Saillet; Damir Spisic; Calisto Zuzarte,Abstract Deduplication is a costly and tedious task that involves identifying duplicate recordsin a dataset. High duplication rates lead to poor data quality; where data ambiguity occurs asto whether two records refer to the same entity. Existing deduplication techniques compare aset of attribute values; and verify whether given similarity thresholds are satisfied. Whilepotential duplicate records are identified; these techniques do not provide users with anyinformation about the degree of duplication; ie; the varying levels of closeness among theattribute values and between records that define the duplicates. In this paper; we present aduplication metric that quantifies the level of duplication for an attribute value; and within anattribute. This metric can be used by analysts to understand the distribution and similarity ofvalues during the data cleaning process. We present a deduplication framework that …,Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering,2017,*
Privacy aware web services in the cloud,Farshad Rahimi Asl; Fei Chiang; Wenbo He; Reza Samavi,Data privacy and security continues to hinder wider adoption of cloud based web servicesfor small to medium businesses. Existing privacy aware systems for cloud environmentseither assume that web service providers are trustworthy and can adequately enforce aclient's privacy policies or adapt computationally expensive encryption techniques tominimize data security risks. In this paper; we propose; PASiC; a framework for PrivacyAware RESTful Web Services in the Cloud. PASiC provides lightweight data privacy featuresby allowing clients to define their specific privacy policies; obfuscation/encryption methodsand collaboratively engage with the service providers to enforce these policies. Ourframework is designed to facilitate integration with legacy systems. Our experimentalevaluation shows that PASiC safeguards sensitive data throughout the data staging …,Communications and Network Security (CNS); 2017 IEEE Conference on,2017,*
PARC: Privacy-Aware Data Cleaning,Dejun Huang; Dhruv Gairola; Yu Huang; Zheng Zheng; Fei Chiang,Abstract Poor data quality has become a persistent challenge for organizations as datacontinues to grow in complexity and size. Existing data cleaning solutions focus onidentifying repairs to the data to minimize either a cost function or the number of updates.These techniques; however; fail to consider underlying data privacy requirements that existin many real data sets containing sensitive and personal information. In this demonstration;we present PARC; a Privacy-AwaRe data Cleaning system that corrects datainconsistencies wrt a set of FDs; and limits the disclosure of sensitive values during thecleaning process. The system core contains modules that evaluate three key metrics duringthe repair search; and solves a multi-objective optimization problem to identify repairs thatbalance the privacy vs. utility tradeoff. This demonstration will enable users to understand …,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,2016,*
Unifying Data and Constraint Repairs,Fei Chiang; Siddharth Sitaramachandran,Abstract Integrity constraints play an important role in data design. However; in anoperational database; they may not be enforced for many reasons. Hence; over time; datamay become inconsistent with respect to the constraints. To manage this; severalapproaches have proposed techniques to repair the data by finding minimal or lowest costchanges to the data that make it consistent with the constraints. Such techniques areappropriate for applications where only the data changes; but schemas and their constraintsremain fixed. In many modern applications; however; constraints may evolve over time asapplication or business rules change; as data are integrated with new data sources or as theunderlying semantics of the data evolves. In such settings; when an inconsistency occurs; itis no longer clear if there is an error in the data (and the data should be repaired) or if the …,Journal of Data and Information Quality (JDIQ),2016,*
Data Driven Discovery of Attribute Dictionaries,Fei Chiang; Periklis Andritsos; Renée J Miller,Abstract Online product search engines such as Google and Yahoo shopping; rely onhaving extensive and complete product information to return accurate and timely searchresults. Given the expanding scope of products and updates to existing products; automatedtechniques are needed to ensure the underlying product dictionaries remain current andcomplete. Product search engines receive offers from merchants describing product specificattributes and characteristics. These offers normally contain structured attribute-value pairs;and unstructured (textual) descriptions describing product characteristics and features. Forexample; a laptop offer may contain attribute-value pairs such as “model-X42” and “RAM-8GB”; and a text description of the software; accessories; battery features; warranty; etc.Updating the product dictionaries using the textual descriptions is a more challenging …,*,2016,*
Data Quality Through Active Constraint Discovery and Maintenance,Fei Chiang,Abstract Although integrity constraints are the primary means for enforcing data integrity;there are cases in which they are not defined or are not strictly enforced. This leads toinconsistencies in the data; causing poor data quality. In this thesis; we leverage the powerof constraints to improve data quality. To ensure that the data conforms to the intendedapplication domain semantics; we develop two algorithms focusing on constraint discovery.The first algorithm discovers a class of conditional constraints; which hold over a subset ofthe relation; under specific conditional values. The second algorithm discovers attributedomain constraints; which bind specific values to the attributes of a relation for a givendomain. These two types of constraints have been shown to be useful for data cleaning.,*,2012,*
Refining Duplicate Detection for Improved Data Quality,Yu Huang; Fei Chiang,Abstract. Detecting duplicates is a pervasive data quality challenge that hindersorganizations from extracting value from their data sooner. The increased complexity andheterogeneity of modern datasets has lead to the presence of varying record formats;missing values; and evolving data semantics. As data is integrated; duplicates inevitablyoccur in the integrated instance. One of the challenges in deduplication is determiningwhether two values are sufficiently close to be considered equal. Existing similarity functionsoften rely on counting the number of required edits to transform one value to the other. Thisis insufficient in attribute domains; such as time; where small syntactic differences do notalways translate to'closeness'. In this paper; we propose a duplication detection framework;which adapts metric functional dependencies (MFDs) to improve the detection accuracy …,*,*,*
