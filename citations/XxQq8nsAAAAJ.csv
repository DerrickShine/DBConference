GPS: a graph processing system,Semih Salihoglu; Jennifer Widom,Abstract GPS (for Graph Processing System) is a complete open-source system wedeveloped for scalable; fault-tolerant; and easy-to-program execution of algorithms onextremely large graphs. This paper serves the dual role of describing the GPS system; andpresenting techniques and experimental results for graph partitioning in distributed graph-processing systems like GPS. GPS is similar to Google's proprietary Pregel system; withthree new features:(1) an extended API to make global computations more easily expressedand more efficient;(2) a dynamic repartitioning scheme that reassigns vertices to differentworkers during the computation; based on messaging patterns; and (3) an optimization thatdistributes adjacency lists of high-degree vertices across all compute nodes to improveperformance. In addition to presenting the implementation of GPS and its novel features …,*,2012,407
Upper and Lower Bounds on the Cost of a Map-Reduce Computation,Foto N Afrati; Anish Das Sarma; Semih Salihoglu; Jeffrey D Ullman,*,arXiv preprint arXiv:1206.4377,2012,132
Optimizing graph algorithms on pregel-like systems,Semih Salihoglu; Jennifer Widom,Abstract We study the problem of implementing graph algorithms efficiently on Pregel-likesystems; which can be surprisingly challenging. Standard graph algorithms in this settingcan incur unnecessary inefficiencies such as slow convergence or high communication orcomputation cost; typically due to structural properties of the input graphs such as largediameters or skew in component sizes. We describe several optimization techniques toaddress these inefficiencies. Our most general technique is based on the idea of performingsome serial computation on a tiny fraction of the input graph; complementing Pregel's vertex-centric parallelism. We base our study on thorough implementations of several fundamentalgraph algorithms; some of which have; to the best of our knowledge; not been implementedon Pregel-like systems before. The algorithms and optimizations we describe are fully …,Proceedings of the VLDB Endowment,2014,81
Simplifying scalable graph processing with a domain-specific language,Sungpack Hong; Semih Salihoglu; Jennifer Widom; Kunle Olukotun,Abstract Large-scale graph processing; with its massive data sets; requires distributedprocessing. However; conventional frameworks for distributed graph processing; such asPregel; use non-traditional programming models that are well-suited for parallelism andscalability but inconvenient for implementing non-trivial graph algorithms. In this paper; weuse Green-Marl; a Domain-Specific Language for graph analysis; to intuitively describegraph algorithms and extend its compiler to generate equivalent Pregel implementations.Using the semantic information captured by Green-Marl; the compiler applies a set oftransformation rules that convert imperative graph algorithms into Pregel's programmingmodel. Our experiments show that the Pregel programs generated by the Green-Marlcompiler perform similarly to manually coded Pregel implementations of the same …,Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization,2014,44
GYM: A multiround join algorithm in mapreduce,Foto Afrati; Manas Joglekar; Christopher Ré; Semih Salihoglu; Jeffrey D Ullman,Abstract: Multiround algorithms are now commonly used in distributed data processingsystems; yet the extent to which algorithms can benefit from running more rounds is not wellunderstood. This paper answers this question for a spectrum of rounds for the problem ofcomputing the equijoin of $ n $ relations. Specifically; given any query $ Q $ with width $\w$;{\em intersection width} $\iw $; input size $\mathrm {IN} $; output size $\mathrm {OUT} $;and a cluster of machines with $ M $ memory available per machine; we show that:,arXiv preprint arXiv:1410.4156,2014,28
Provenance-based refresh in data-oriented workflows,Robert Ikeda; Semih Salihoglu; Jennifer Widom,Abstract We consider a general workflow setting in which input data sets are processed by agraph of transformations to produce output results. Our goal is to perform efficient selectiverefresh of elements in the output data; ie; compute the latest values of specific outputelements when the input data may have changed. We explore how data provenance can beused to enable efficient refresh. Our approach is based on capturing one-level dataprovenance at each transformation when the workflow is run initially. Then at refresh timeprovenance is used to determine (transitively) which input elements are responsible forgiven output elements; and the workflow is rerun only on that portion of the data needed forrefresh. Our contributions are to formalize the problem setting and the problem itself; tospecify properties of transformations and provenance that are required for efficient refresh …,Proceedings of the 20th ACM international conference on Information and knowledge management,2011,27
Provenance-based debugging and drill-down in data-oriented workflows,Robert Ikeda; Junsang Cho; Charlie Fang; Semih Salihoglu; Satoshi Torikai; Jennifer Widom,Panda (for Provenance and Data); a system for data-oriented workflows that supportsdebugging and drill-down using logical provenance-provenance information stored at theprocessing-node level is demonstrated. In this demonstration; Panda is used to integrate;process; and analyze actual education data from multiple sources.,Data Engineering (ICDE); 2012 IEEE 28th International Conference on,2012,23
HelP: High-level primitives for large-scale graph processing,Semih Salihoglu; Jennifer Widom,Abstract Large-scale graph processing systems typically expose a small set of functions;such as the compute () function of Pregel; or the gather (); apply (); and scatter () functions ofPowerGraph. For some computations; these APIs are too low-level; yielding long andcomplex programs; but with shared coding patterns. Similar issues with the MapReduceframework have led to widely-used languages such as Pig Latin and Hive; which introducehigher-level primitives. We take an analogous approach for graph processing: we proposeHelP; a set of high-level primitives that capture commonly appearing operations in large-scale graph computations. Using our primitives we have implemented a large suite ofalgorithms; some of which we previously implemented with the APIs of existing systems. Ourexperience has been that implementing algorithms using our primitives is more intuitive …,Proceedings of Workshop on GRAph Data management Experiences and Systems,2014,18
Graft: A debugging tool for apache giraph,Semih Salihoglu; Jaeho Shin; Vikesh Khanna; Ba Quan Truong; Jennifer Widom,Abstract We address the problem of debugging programs written for Pregel-like systems.After interviewing Giraph and GPS users; we developed Graft. Graft supports the debuggingcycle that users typically go through:(1) Users describe programmatically the set of verticesthey are interested in inspecting. During execution; Graft captures the context information ofthese vertices across supersteps.(2) Using Graft's GUI; users visualize how the values andmessages of the captured vertices change from superstep to superstep; narrowing insuspicious vertices and supersteps.(3) Users replay the exact lines of the code vertex.compute () function that executed for the suspicious vertices and supersteps; by copyingcode that Graft generates into their development environments' line-by-line debuggers. Graftalso has features to construct end-to-end tests for Giraph programs. Graft is open-source …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,13
Designing good algorithms for MapReduce and beyond,Foto N Afrati; Magdalena Balazinska; Anish Das Sarma; Bill Howe; Semih Salihoglu; Jeffrey D Ullman,Abstract As MapReduce/Hadoop grows in importance; we find more exotic applicationsbeing written this way. Not every program written for this platform performs as well as wemight wish. There are several reasons why a MapReduce program can underperformexpectations. One is the need to balance the communication cost of transporting data fromthe mappers to the reducers against the computation done at the mappers and reducersthemselves. A second important issue is selecting the number of rounds of MapReduce. Athird issue is that of skew. If wall-clock time is important; then using many different reduce-keys and many compute nodes may minimize the time to finish the job. Yet if the data isuncooperative; and no provision is made to distribute the data evenly; much of the work isdone by a single node.,Proceedings of the Third ACM Symposium on Cloud Computing,2012,10
Anchor points algorithms for hamming and edit distance,Foto Afrati; Anish Das Sarma; Anand Rajaraman; Pokey Rule; Semih Salihoglu; Jeffrey Ullman,Algorithms for computing similarity joins in MapReduce were offered in [2]. Similarity joinsask to find input pairs that are within a certain distance d according to some distancemeasure. Here we explore the``anchor-points algorithm''of [2]. We continue looking atHamming distance; and show that the method of that paper can be improved; in particular; ifwe want to find strings within Hamming distance d; and anchor points are chosen so thatevery possible input is within Hamming distance k of some anchor point; then it is sufficientto send each input to all anchor points within distance (d/2)+ k; rather than d+ k as wassuggested in the earlier paper. This improves on the communication cost of the MapReducealgorithm; ie; reduces the amount of data transmitted among machines. Further; the sameholds for edit distance; provided inputs all have the same length n and either the length of …,*,2014,8
Interdomain routing as social choice,Ronny R Dakdouk; Semih Salihoglu; Hao Wang; Haiyong Xie; Yang Richard Yang,Interdomain routing is essential to both the stability and efficiency of the global Internet.However; most previous studies focus only on stability; and only on a special class of routingprotocols; namely BGP-type; path-vector protocols. In this paper; we conduct a systematicanalysis of interdomain routing considering optimality and implementation in strategicsettings. We adopt the novel perspective that an interdomain routing system is one whichdefines a social choice rule that aggregates individual preferences of all of the autonomoussystems (ASes) in a network to select interdomain routes with a set of desirable properties.An interdomain routing protocol; then; is a mechanism to implement the identifiedinterdomain routing social choice rule; when the ASes can adopt strategic actions. Bypointing out the incompatibility among the desirable properties of an interdomain routing …,Distributed Computing Systems Workshops; 2006. ICDCS Workshops 2006. 26th IEEE International Conference on,2006,8
SPARTex: a vertex-centric framework for RDF data analytics,Ibrahim Abdelaziz; Razen Harbi; Semih Salihoglu; Panos Kalnis; Nikos Mamoulis,Abstract A growing number of applications require combining SPARQL queries with genericgraph search on RDF data. However; the lack of procedural capabilities in SPARQL makes itinappropriate for graph analytics. Moreover; RDF engines focus on SPARQL queryevaluation whereas graph management frameworks perform only generic graphcomputations. In this work; we bridge the gap by introducing SPARTex; an RDF analyticsframework based on the vertex-centric computation model. In SPARTex; user-defined vertexcentric programs can be invoked from SPARQL as stored procedures. SPARTex allows theexecution of a pipeline of graph algorithms without the need for multiple reads/writes of inputdata and intermediate results. We use a cost-based optimizer for minimizing thecommunication cost. SPARTex evaluates queries that combine SPARQL and generic …,Proceedings of the VLDB Endowment,2015,6
Computing strongly connected components in pregel-like systems,Semih Salihoglu; Jennifer Widom,*,*,2013,6
Combining Vertex-centric Graph Processing with SPARQL for Large-scale RDF Data Analytics,Ibrahim Abdelaziz; Razen Harbi; Semih Salihoglu; Panos Kalnis,Modern applications require sophisticated analytics on RDF graphs that combine structuralqueries with generic graph computations. Existing systems support either declarativeSPARQL queries; or generic graph processing; but not both. We bridge the gap byintroducing Spartex; a versatile framework for complex RDF analytics. Spartex extendsSPARQL to combine seamlessly generic graph algorithms (eg; PageRank; Shortest Paths;etc.) with SPARQL queries. Spartex builds on existing vertex-centric graph processingframeworks; such as Graphlab or Pregel. It implements a generic SPARQL operator as avertex-centric program that interprets SPARQL queries and executes them efficiently using abuilt-in optimizer. In addition; any graph algorithm implemented in the underlying vertex-centric framework; can be executed in Spartex. We present various scenarios where our …,IEEE Transactions on Parallel and Distributed Systems,2017,5
Networks as vectos of their motif frequencies and 2-norm distance as a measure of similarity,Semih Salihoglu,ABSTRACT Previous studies have uncovered a list of interesting properties that exist in realnetworks by comparing them to synthetic networks. However; there has been less focus oncomparing real networks to other real networks. How do Autonomous Systems routingnetworks compare to social networks? How can we tell that one biological network looksdifferent from other biological networks? How can we quantify how similar two networks are?One way to capture the difference between two networks is to define an” appropriate” notionof distance. This project proposes using the 2-norm distance between vectors to capturenetwork similarity. In particular; we present the results of using the 2-norm distance as ameasure of similarity in two cases: a) when networks are represented as 4 dimensionalvectors based on their 2x2 stochastic Kronecker initiator matrices; b) when networks are …,*,2006,2
Graphflow: An Active Graph Database,Chathura Kankanamge; Siddhartha Sahu; Amine Mhedbhi; Jeremy Chen; Semih Salihoglu,Abstract Many applications detect the emergence or deletion of certain subgraphs in theirinput graphs continuously. In order to evaluate such continuous subgraph queries; theseapplications resort to inefficient or highly specialized solutions because existing graphdatabases are passive systems that only support one-time subgraph queries. Wedemonstrate Graphflow; a prototype active graph data-base that evaluates general one-timeand continuous subgraph queries. Graphflow supports the property graph data model andthe Cypher++ query language; which extends Neo4j's declarative Cypher language withsubgraph-condition-action triggers. At the core of Graphflow's query processor are two worst-case optimal join algorithms called Generic Join and our new Delta Generic Join algorithmfor one-time and continuous subgraph queries; respectively.,Proceedings of the 2017 ACM International Conference on Management of Data,2017,1
Delta Generic Join,Khaled Ammar; Frank McSherry; Semih Salihoglu,Abstract In this manuscript we describe a new incremental view maintenance (IVM)algorithm called Delta Generic Join (DeltaGJ) that is based on the recent worst-case optimalGeneric Join (GJ) algorithm. DeltaGJ translates an input subgraph query into several deltaqueries as done in some incremental view maintenance algorithms [2; 3] and evaluateseach one with a specific invocation of GJ. As we show; DeltaGJ is worstcase optimal underinsertion only workloads; an optimality guarantee that does not exist for existing IVMtechniques based on delta queries.,*,*,1
Algorithmic Aspects of Parallel Data Processing,Paraschos Koutris; Semih Salihoglu; Dan Suciu,Abstract In the last decade or so we have witnessed a growing interest in processing largedata sets on large distributed clusters. The idea was pioneered by the MapReduceframework; and has been widely adopted by several other systems; including PigLatin; Hive;Scope; U-SQL; Dremmel; Spark and Myria. A large part of the complex data analysisperformed by these systems consists of a sequence of relatively simple query operations;such as joining two or more tables. This survey discusses recent algorithmic developmentsfor distributed data processing. It uses a theoretical model of parallel processing called theMassively Parallel Computation (MPC) model; which is a simplification of the BSP modelwhere the only cost is given by the amount of communication and the number ofcommunication rounds. The survey studies several algorithms for multi-join queries; for …,Foundations and Trends® in Databases,2018,*
Distributed Evaluation of Subgraph Queries Using Worstcase Optimal LowMemory Dataflows,Khaled Ammar; Frank McSherry; Semih Salihoglu; Manas Joglekar,Abstract: We study the problem of finding and monitoring fixed-size subgraphs in acontinually changing large-scale graph. We present the first approach that (i) performs worst-case optimal computation and communication;(ii) maintains a total memory footprint linear inthe number of input edges; and (iii) scales down per-worker computation; communication;and memory requirements linearly as the number of workers increases; even onadversarially skewed inputs. Our approach is based on worst-case optimal join algorithms;recast as a data-parallel dataflow computation. We describe the general algorithm andmodifications that make it robust to skewed data; prove theoretical bounds on its resourcerequirements in the massively parallel computing model; and implement and evaluate it ongraphs containing as many as 64 billion edges. The underlying algorithm and ideas …,arXiv preprint arXiv:1802.03760,2018,*
The Ubiquity of Large Graphs and Surprising Challenges of Graph Processing,Siddhartha Sahu; Amine Mhedhbi; Semih Salihoglu; Jimmy Lin; M Tamer Özsu,ABSTRACT Graph processing is becoming increasingly prevalent across many applicationdomains. In spite of this prevalence; there is little research about how graphs are actuallyused in practice. We conducted an online survey aimed at understanding:(i) the types ofgraphs users have;(ii) the graph computations users run;(iii) the types of graph softwareusers use; and (iv) the major challenges users face when processing their graphs. Wedescribe the participants' responses to our questions highlighting common patterns andchallenges. We further reviewed user feedback in the mailing lists; bug reports; and featurerequests in the source repositories of a large suite of software products for processinggraphs. Through our review; we were able to answer some new questions that were raisedby participants' responses and identify specific challenges that users face when using …,Proceedings of the VLDB Endowment,2017,*
Massive-scale Processing of Record-oriented and Graph Data,Semih Salihoglu,Many data-driven applications perform computations on large volumes of data that do not fiton a single computer. These applications typically must use parallel shared-nothingdistributed software systems to perform their computations. This thesis addresseschallenges in large-scale distributed data processing with a particular focus on two primaryareas:(i) theoretical foundations for understanding the costs of distribution; and (ii)processing large-scale graph data. The first part of this thesis presents a theoreticalframework for the MapReduce system; to analyze the cost of distribution for differentproblems domains; and for evaluating the``goodness''of different algorithms. We identify afundamental tradeoff between the parallelism and communication costs of algorithms. Wefirst study the setting when computations are constrained to a single round of MapReduce …,*,2015,*
Computer Science & Economics Senior Thesis,Semih Salihoglu,Abstract Interdomain routing is an essential component of the Internet; which deals withconnecting two machines in the Internet through a set of links. Today this is done throughBGP; the de facto protocol of the Internet. Previous studies focus on the stability of BGP typeprotocols. The current thesis focuses on interdomain routing from a social choiceperspective and is divided into two main parts. In the first part we analyze interdomainrouting considering optimality. We view interdomain routing as a system; defining a socialchoice rule that aggregates individual preferences of autonomous systems (ASes) to selecta set of routing trees. An interdomain routing protocol; then; is a mechanism implementing asocial choice rule. We study the desirable properties of such social choice rules in theinterdomain routing context. By presenting major incompatibilities between certain …,*,2006,*
Database-Managed CPU Performance Scaling for Improved Energy Efficiency,Mustafa Korkmaz; Martin Karsten; Kenneth Salem; Semih Salihoglu,ABSTRACT Dynamic voltage and frequency scaling (DVFS) is a technique for adjusting thespeed and power consumption of processors; allowing performance to be traded forreduced power consumption. Since CPUs are typically the largest consumers of power inmodern servers; DVFS can have a significant impact on overall server power consumption.Modern operating systems include DVFS governors; which interact with the processor tomanage performance and power consumption according to some system-level policy. In thispaper; we argue that for database servers; DVFS can be managed more effectively by thedatabase management system. We present a power-aware database request schedulingalgorithm called POLARIS. Unlike operating system governors; POLARIS is aware ofdatabase units of work and database performance targets; and can achieve a better …,*,*,*
Algorithms and Systems for Massive-Scale Data Processing,Semih Salihoglu,Performing complex analyses on large-scale data is becoming one of the core challenges ofmany application domains. As the volume of data grows; and the speed with which new datais generated increases; applications need to perform their computations on highly-paralleland shared-nothing distributed systems. My thesis focuses on new theories; algorithms; andsystems that expand the capabilities of distributed large-scale data processing; with aparticular emphasis on two problems:(1) performing large-scale joins of record-orienteddata; and (2) processing large graphs. For join processing; my research started byformalizing a theoretical model for answering two questions within the context of theMapReduce system:(i) how difficult is it to parallelize different problems; and (ii) how optimalare existing algorithms for a given problem? Driven by the insights from studying the …,*,*,*
