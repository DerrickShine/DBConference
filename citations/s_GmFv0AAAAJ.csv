Large-scale matrix factorization with distributed stochastic gradient descent,Rainer Gemulla; Erik Nijkamp; Peter J Haas; Yannis Sismanis,Abstract We provide a novel algorithm to approximately factor large matrices with millions ofrows; millions of columns; and billions of nonzero elements. Our approach rests onstochastic gradient descent (SGD); an iterative stochastic optimization algorithm. We firstdevelop a novel" stratified" SGD variant (SSGD) that applies to general loss-minimizationproblems in which the loss function can be expressed as a weighted sum of" stratum losses."We establish sufficient conditions for convergence of SSGD using results from stochasticapproximation theory and regenerative process theory. We then specialize SSGD to obtain anew matrix-factorization algorithm; called DSGD; that can be fully distributed and run on web-scale datasets using; eg; MapReduce. DSGD can handle a wide variety of matrixfactorizations. We describe the practical techniques used to optimize performance in our …,Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,2011,428
Jaql: A scripting language for large scale semistructured data analysis,Kevin S Beyer; Vuk Ercegovac; Rainer Gemulla; Andrey Balmin; Mohamed Eltabakh; Carl-Christian Kanne; Fatma Ozcan; Eugene J Shekita,ABSTRACT This paper describes Jaql; a declarative scripting language for analyzing largesemistructured datasets in parallel using Hadoop's MapReduce framework. Jaql is currentlyused in IBM's InfoS-phere BigInsights [5] and Cognos Consumer Insight [9] products. Jaql'sdesign features are:(1) a flexible data model;(2) reusability;(3) varying levels of abstraction;and (4) scalability. Jaql's data model is inspired by JSON and can be used to representdatasets that vary from flat; relational tables to collections of semistructured documents. AJaql script can start without any schema and evolve over time from a partial to a rigidschema. Reusability is provided through the use of higher-order functions and by packagingrelated functions into modules. Most Jaql scripts work at a high level of abstraction forconcise specification of logical operations (eg; join); but Jaql's notion of physical …,Proceedings of VLDB conference,2011,221
CoHadoop: flexible data placement and its exploitation in Hadoop,Mohamed Y Eltabakh; Yuanyuan Tian; Fatma Özcan; Rainer Gemulla; Aljoscha Krettek; John McPherson,Abstract Hadoop has become an attractive platform for large-scale data analytics. In thispaper; we identify a major performance bottleneck of Hadoop: its lack of ability to colocaterelated data on the same set of nodes. To overcome this bottleneck; we introduceCoHadoop; a lightweight extension of Hadoop that allows applications to control where dataare stored. In contrast to previous approaches; CoHadoop retains the flexibility of Hadoop inthat it does not require users to convert their data to a certain format (eg; a relationaldatabase or a specific file format). Instead; applications give hints to CoHadoop that someset of files are related and may be processed jointly; CoHadoop then tries to colocate thesefiles for improved efficiency. Our approach is designed such that the strong fault toleranceproperties of Hadoop are retained. Colocation can be used to improve the efficiency of …,Proceedings of the VLDB Endowment,2011,216
Ricardo: integrating R and Hadoop,Sudipto Das; Yannis Sismanis; Kevin S Beyer; Rainer Gemulla; Peter J Haas; John McPherson,Abstract Many modern enterprises are collecting data at the most detailed level possible;creating data repositories ranging from terabytes to petabytes in size. The ability to applysophisticated statistical analysis methods to this data is becoming essential for marketplacecompetitiveness. This need to perform deep analysis over huge data repositories poses asignificant challenge to existing statistical software and data management systems. On theone hand; statistical software provides rich functionality for data analysis and modeling; butcan handle only limited amounts of data; eg; popular packages like R and SPSS operateentirely in main memory. On the other hand; data management systems-such asMapReduce-based systems-can scale to petabytes of data; but provide insufficient analyticalfunctionality. We report our experiences in building Ricardo; a scalable platform for deep …,Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,2010,197
Clausie: clause-based open information extraction,Luciano Del Corro; Rainer Gemulla,Abstract We propose ClausIE; a novel; clause-based approach to open informationextraction; which extracts relations and their arguments from natural language text. ClausIEfundamentally differs from previous approaches in that it separates the detectionof``useful''pieces of information expressed in a sentence from their representation in terms ofextractions. In more detail; ClausIE exploits linguistic knowledge about the grammar of theEnglish language to first detect clauses in an input sentence and to subsequently identify thetype of each clause according to the grammatical function of its constituents. Based on thisinformation; ClausIE is able to generate high-precision extractions; the representation ofthese extractions can be flexibly customized to the underlying application. ClausIE is basedon dependency parsing and a small set of domain-independent lexica; operates …,Proceedings of the 22nd international conference on World Wide Web,2013,185
On synopses for distinct-value estimation under multiset operations,Kevin Beyer; Peter J Haas; Berthold Reinwald; Yannis Sismanis; Rainer Gemulla,Abstract The task of estimating the number of distinct values (DVs) in a large dataset arisesin a wide variety of settings in computer science and elsewhere. We provide DV estimationtechniques that are designed for use within a flexible and scalable" synopsis warehouse"architecture. In this setting; incoming data is split into partitions and a synopsis is created foreach partition; each synopsis can then be used to quickly estimate the number of DVs in itscorresponding partition. By combining and extending a number of results in the literature; weobtain both appropriate synopses and novel DV estimators to use in conjunction with thesesynopses. Our synopses can be created in parallel; and can then be easily combined toyield synopses and DV estimates for arbitrary unions; intersections or differences ofpartitions. Our synopses can also handle deletions of individual partition elements. We …,Proceedings of the 2007 ACM SIGMOD international conference on Management of data,2007,126
Distributed matrix completion,Christina Teflioudi; Faraz Makari; Rainer Gemulla,We discuss parallel and distributed algorithms for large-scale matrix completion onproblems with millions of rows; millions of columns; and billions of revealed entries. Wefocus on in-memory algorithms that run on a small cluster of commodity nodes; even verylarge problems can be handled effectively in such a setup. Our DALS; ASGD; and DSGD++algorithms are novel variants of the popular alternating least squares and stochasticgradient descent algorithms; they exploit thread-level parallelism; in-memory processing;and asynchronous communication. We provide some guidance on the asymptoticperformance of each algorithm and investigate the performance of both our algorithms andpreviously proposed Map Reduce algorithms in large-scale experiments. We found thatDSGD++ outperforms competing methods in terms of overall runtime; memory …,Data Mining (ICDM); 2012 IEEE 12th International Conference on,2012,59
Mind the gap: Large-scale frequent sequence mining,Iris Miliaraki; Klaus Berberich; Rainer Gemulla; Spyros Zoupanos,Abstract Frequent sequence mining is one of the fundamental building blocks in datamining. While the problem has been extensively studied; few of the available techniques aresufficiently scalable to handle datasets with billions of sequences; such large-scale datasetsarise; for instance; in text mining and session analysis. In this paper; we propose MG-FSM; ascalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called" gap constraints"; which can be used to limit the output to a controlled set of frequentsequences. At its heart; MG-FSM partitions the input database in a way that allows us tomine each partition independently using any existing frequent sequence mining algorithm.We introduce the notion of w-equivalency; which is a generalization of the notion of a"projected database" used by many frequent pattern mining algorithms. We also present a …,Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,2013,43
A dip in the reservoir: Maintaining sample synopses of evolving datasets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Perhaps the most flexible synopsis of a database is a random sample of the data;such samples are widely used to speed up processing of analytic queries and data-miningtasks; enhance query optimization; and facilitate information integration. In this paper; westudy methods for incrementally maintaining a uniform random sample of the items in adataset in the presence of an arbitrary sequence of insertions and deletions. For" stable"datasets whose size remains roughly constant over time; we provide a novel samplingscheme; called" random pairing"(RP) which maintains a bounded-size uniform sample byusing newly inserted data items to compensate for previous deletions. The RP algorithm isthe first extension of the almost 40-year-old reservoir sampling algorithm to handledeletions. Experiments show that; when dataset-size fluctuations over time are not too …,Proceedings of the 32nd international conference on Very large data bases,2006,42
Fast integer compression using SIMD instructions,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract We study algorithms for efficient compression and decompression of a sequence ofintegers on modern hardware. Our focus is on universal codes in which the codeword lengthis a monotonically non-decreasing function of the uncompressed integer value; such codesare widely used for compressing" small integers". In contrast to traditional integercompression; our algorithms make use of the SIMD capabilities of modern processors byencoding multiple integer values at once. More specifically; we provide SIMD versions ofboth null suppression and Elias gamma encoding. Our experiments show that theseversions provide a speedup from 1.5 x up to 6.7 x for decompression; while maintaining asimilar compression performance.,Proceedings of the Sixth International Workshop on Data Management on New Hardware,2010,38
Sampling time-based sliding windows in bounded space,Rainer Gemulla; Wolfgang Lehner,Abstract Random sampling is an appealing approach to build synopses of large datastreams because random samples can be used for a broad spectrum of analytical tasks.Users are often interested in analyzing only the most recent fraction of the data stream inorder to avoid outdated results. In this paper; we focus on sampling schemes that samplefrom a sliding window over a recent time interval; such windows are a popular and highlycomprehensible method to model recency. In this setting; the main challenge is to guaranteean upper bound on the space consumption of the sample while using the allotted spaceefficiently at the same time. The difficulty arises from the fact that the number of items in thewindow is unknown in advance and may vary significantly over time; so that the samplingfraction has to be adjusted dynamically. We consider uniform sampling schemes; which …,Proceedings of the 2008 ACM SIGMOD international conference on Management of data,2008,37
Memory-efficient frequent-itemset mining,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract Efficient discovery of frequent itemsets in large datasets is a key component ofmany data mining tasks. In-core algorithms---which operate entirely in main memory andavoid expensive disk accesses---and in particular the prefix tree-based algorithm FP-growthare generally among the most efficient of the available algorithms. Unfortunately; theirexcessive memory requirements render them inapplicable for large datasets with manydistinct items and/or itemsets of high cardinality. To overcome this limitation; we propose twonovel data structures---the CFP-tree and the CFP-array---; which reduce memoryconsumption by about an order of magnitude. This allows us to process significantly largerdatasets in main memory than previously possible. Our data structures are based onstructural modifications of the prefix tree that increase compressability; an optimized …,Proceedings of the 14th International Conference on Extending Database Technology,2011,32
Maintaining bernoulli samples over evolving multisets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Random sampling has become a crucial component of modern data managementsystems. Although the literature on database sampling is large; there has been relativelylittle work on the problem of maintaining a sample in the presence of arbitrary insertions anddeletions to the underlying dataset. Most existing maintenance techniques apply either tothe insert-only case or to datasets that do not contain duplicates. In this paper; we provide ascheme that maintains a Bernoulli sample of an underlying multiset in the presence of anarbitrary stream of updates; deletions; and insertions. Importantly; the scheme never needsto access the underlying multiset. Such Bernoulli samples are easy to manipulate; and arewell suited to parallel processing environments. Our method can be viewed as anenhancement of the" counting sample" scheme developed by Gibbons and Matias for …,Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,2007,31
Finet: Context-aware fine-grained named entity typing,Luciano del Corro; Abdalghani Abujabal; Rainer Gemulla; Gerhard Weikum,Abstract We propose FINET; a system for detecting the types of named entities in shortinputs—such as sentences or tweets—with respect to WordNet's super fine-grained typesystem. FINET generates candidate types using a sequence of multiple extractors; rangingfrom explicitly mentioned types to implicit types; and subsequently selects the mostappropriate using ideas from word-sense disambiguation. FINET combats data scarcity andnoise from existing systems: It does not rely on supervision in its extractors and generatestraining data for type selection from WordNet and other resources. FINET supports the mostfine-grained type system so far; including types with no annotated training data. Ourexperiments indicate that FINET outperforms state-of-the-art methods in terms of recall;precision; and granularity of extracted types.,*,2015,27
Low-latency handshake join,Pratanu Roy; Jens Teubner; Rainer Gemulla,Abstract This work revisits the processing of stream joins on modern hardware architectures.Our work is based on the recently proposed handshake join algorithm; which is amechanism to parallelize the processing of stream joins in a NUMA-aware and hardware-friendly manner. Handshake join achieves high throughput and scalability; but it suffers froma high latency penalty and a non-deterministic ordering of the tuples in the physical resultstream. In this paper; we first characterize the latency behavior of the handshake join andthen propose a new low-latency handshake join algorithm; which substantially reduceslatency without sacrificing throughput or scalability. We also present a technique to generatepunctuated result streams with very little overhead; such punctuations allow the generationof correctly ordered physical output streams with negligible effect on overall throughput …,Proceedings of the VLDB Endowment,2014,25
A distributed algorithm for large-scale generalized matching,Faraz Makari Manshadi; Baruch Awerbuch; Rainer Gemulla; Rohit Khandekar; Julián Mestre; Mauro Sozio,Abstract Generalized matching problems arise in a number of applications; includingcomputational advertising; recommender systems; and trade markets. Consider; forexample; the problem of recommending multimedia items (eg; DVDs) to users such that (1)users are recommended items that they are likely to be interested in;(2) every user getsneither too few nor too many recommendations; and (3) only items available in stock arerecommended to users. State-of-the-art matching algorithms fail at coping with large real-world instances; which may involve millions of users and items. We propose the firstdistributed algorithm for computing near-optimal solutions to large-scale generalizedmatching problems like the one above. Our algorithm is designed to run on a small cluster ofcommodity nodes (or in a MapReduce environment); has strong approximation …,Proceedings of the VLDB Endowment,2013,23
Maintaining bounded-size sample synopses of evolving datasets,Rainer Gemulla; Wolfgang Lehner; Peter J Haas,Abstract Perhaps the most flexible synopsis of a database is a uniform random sample of thedata; such samples are widely used to speed up processing of analytic queries and data-mining tasks; enhance query optimization; and facilitate information integration. The ability tobound the maximum size of a sample can be very convenient from a system-design point ofview; because the task of memory management is simplified; especially when manysamples are maintained simultaneously. In this paper; we study methods for incrementallymaintaining a bounded-size uniform random sample of the items in a dataset in thepresence of an arbitrary sequence of insertions and deletions. For" stable" datasets whosesize remains roughly constant over time; we provide a novel sampling scheme; called"random pairing"(RP); that maintains a bounded-size uniform sample by using newly …,The VLDB Journal—The International Journal on Very Large Data Bases,2008,22
Distinct-value synopses for multiset operations,Kevin Beyer; Rainer Gemulla; Peter J Haas; Berthold Reinwald; Yannis Sismanis,Abstract The task of estimating the number of distinct values (DVs) in a large dataset arisesin a wide variety of settings in computer science and elsewhere. We provide DV estimationtechniques for the case in which the dataset of interest is split into partitions. We create foreach partition a synopsis that can be used to estimate the number of DVs in the partition. Bycombining and extending a number of results in the literature; we obtain both suitablesynopses and DV estimators. The synopses can be created in parallel; and can be easilycombined to yield synopses and DV estimates for" compound" partitions that are createdfrom the base partitions via arbitrary multiset union; intersection; or difference operations.Our synopses can also handle deletions of individual partition elements. We prove that ourDV estimators are unbiased; provide error bounds; and show how to select synopsis …,Communications of the ACM,2009,21
K-ary search on modern processors,Benjamin Schlegel; Rainer Gemulla; Wolfgang Lehner,Abstract This paper presents novel tree-based search algorithms that exploit the SIMDinstructions found in virtually all modern processors. The algorithms are a natural extensionof binary search: While binary search performs one comparison at each iteration; therebycutting the search space in two halves; our algorithms perform k comparisons at a time andthus cut the search space into k pieces. On traditional processors; this so-called k-ary searchprocedure is not beneficial because the cost increase per iteration offsets the cost reductiondue to the reduced number of iterations. On modern processors; however; multiple scalaroperations can be executed simultaneously; which makes k-ary search attractive. In thispaper; we provide two different search algorithms that differ in terms of efficiency andmemory access patterns. Both algorithms are first described in a platform independent …,Proceedings of the Fifth International Workshop on Data Management on New Hardware,2009,20
Reconstructing graphs from neighborhood data,Dóra Erdős; Rainer Gemulla; Evimaria Terzi,Abstract Consider a social network and suppose that we are only given the number ofcommon friends between each pair of users. Can we reconstruct the underlyingnetwork&quest; Similarly; consider a set of documents and the words that appear in them. Ifwe only know the number of common words for every pair of documents; as well as thenumber of common documents for every pair of words; can we infer which words appear inwhich documents&quest; In this article; we develop a general methodology for answeringquestions like these. We formalize these questions in what we call the R econstruct problem:given information about the common neighbors of nodes in a network; our goal is toreconstruct the hidden binary matrix that indicates the presence or absence of relationshipsbetween individual nodes. In fact; we propose two different variants of this problem: one …,ACM Transactions on Knowledge Discovery from Data (TKDD),2014,18
Linked bernoulli synopses: Sampling along foreign keys,Rainer Gemulla; Philipp Rösch; Wolfgang Lehner,Abstract Random sampling is a popular technique for providing fast approximate queryanswers; especially in data warehouse environments. Compared to other types of synopses;random sampling bears the advantage of retaining the dataset's dimensionality; it alsoassociates probabilistic error bounds with the query results. Most of the available samplingtechniques focus on table-level sampling; that is; they produce a sample of only a singledatabase table. Queries that contain joins over multiple tables cannot be answered withsuch samples because join results on random samples are often small and skewed. On thecontrary; schema-level sampling techniques by design support queries containing joins. Inthis paper; we introduce Linked Bernoulli Synopses; a schema-level sampling schemebased upon the well-known Join Synopses. Both schemes rely on the idea of maintaining …,International Conference on Scientific and Statistical Database Management,2008,18
Sampling algorithms for evolving datasets,Rainer Gemulla,Abstract Perhaps the most flexible synopsis of a database is a uniform random sample of thedata; such samples are widely used to speed up the processing of analytic queries and data-mining tasks; to enhance query optimization; and to facilitate information integration. Most ofthe existing work on database sampling focuses on how to create or exploit a randomsample of a static database; that is; a database that does not change over time. Theassumption of a static database; however; severely limits the applicability of thesetechniques in practice; where data is often not static but continuously evolving. In order tomaintain the statistical validity of the sample; any changes to the database have to beappropriately reflected in the sample. In this thesis; we study efficient methods forincrementally maintaining a uniform random sample of the items in a dataset in the …,*,2008,16
Method for maintaining a sample synopsis under arbitrary insertions and deletions,*,A method of incrementally maintaining a stable; bounded; uniform random sample S from adataset R; in the presence of arbitrary insertions and deletions to the dataset R; and withoutaccesses to the dataset R; comprises a random pairing method in which deletions areuncompensated until compensated by a subsequent insertion (randomly paired to thedeletion) by including the insertion's item into S if and only if the uncompensated deletion'sitem was removed from S (ie; was in S so that it could be removed). A method for resizing asample to a new uniform sample of increased size while maintaining a bound on the samplesize and balancing cost between dataset accesses and transactions to the dataset is alsodisclosed. A method for maintaining uniform; bounded samples for a dataset in the presenceof growth in size of the dataset is additionally disclosed.,*,2010,15
Fully Parallel Inference in Markov Logic Networks.,Kaustubh Beedkar; Luciano Del Corro; Rainer Gemulla,Abstract: Markov logic is a powerful tool for handling the uncertainty that arises in real-worldstructured data; it has been applied successfully to a number of data management problems.In practice; the resulting ground Markov logic networks can get very large; which poseschallenges to scalable inference. In this paper; we present the first fully parallelizedapproach to inference in Markov logic networks. Inference decomposes into a groundingstep and a probabilistic inference step; both of which can be cost-intensive. We propose aparallel grounding algorithm that partitions the Markov logic network based on itscorresponding join graph; each partition is ground independently and in parallel. Ourpartitioning scheme is based on importance sampling; which we use for parallel probabilisticinference; and is also well-suited to other; more efficient parallel inference techniques …,BTW,2013,14
Method for estimating the number of distinct values in a partitioned dataset,*,The task of estimating the number of distinct values (DVs) in a large dataset arises in a widevariety of settings in computer science and elsewhere. The present invention providessynopses for DV estimation in the setting of a partitioned dataset; as well as correspondingDV estimators that exploit these synopses. Whenever an output compound data partition iscreated via a multiset operation on a pair of (possibly compound) input partitions; thesynopsis for the output partition can be obtained by combining the synopses of the inputpartitions. If the input partitions are compound partitions; it is not necessary to access thesynopses for all the base partitions that were used to construct the input partitions. Superior(in certain cases near-optimal) accuracy in DV estimates is maintained; especially when thesynopsis size is small. The synopses can be created in parallel; and can also handle …,*,2011,14
Lemp: Fast retrieval of large entries in a matrix product,Christina Teflioudi; Rainer Gemulla; Olga Mykytiuk,Abstract We study the problem of efficiently retrieving large entries in the product of twogiven matrices; which arises in a number of data mining and information retrieval tasks. Wefocus on the setting where the two input matrices are tall and skinny; ie; with millions of rowsand tens to hundreds of columns. In such settings; the product matrix is large and itscomplete computation is generally infeasible in practice. To address this problem; wepropose the LEMP algorithm; which efficiently retrieves only the large entries in the productmatrix without actually computing it. LEMP maps the large-entry retrieval problem to a set ofsmaller cosine similarity search problems; for which existing methods can be used. We alsopropose novel algorithms for cosine similarity search; which are tailored to our setting. Ourexperimental study on large real-world datasets indicates that LEMP is up to an order of …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,13
A weakly supervised model for sentence-level semantic orientation analysis with multiple experts,Lizhen Qu; Rainer Gemulla; Gerhard Weikum,Abstract We propose the weakly supervised Multi-Experts Model (MEM) for analyzing thesemantic orientation of opinions expressed in natural language reviews. In contrast to mostprior work; MEM predicts both opinion polarity and opinion strength at the level of individualsentences; such fine-grained analysis helps to understand better why users like or dislikethe entity under review. A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength. For this reason; MEM is weakly supervised:It starts with potentially noisy indicators obtained from coarse-grained training data (ie;document-level ratings); a small set of diverse base predictors; and; if available; smallamounts of fine-grained training data. We integrate these noisy indicators into a unifiedprobabilistic framework using ideas from ensemble learning and graph-based semi …,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,2012,13
Derby/s: a DBMS for sample-based query answering,Anja Klein; Rainer Gemulla; Philipp Rösch; Wolfgang Lehner,Abstract Although approximate query processing is a prominent way to cope with therequirements of data analysis applications; current database systems do not provideintegrated and comprehensive support for these techniques. To improve this situation; wepropose an SQL extension---called SQL/S---for approximate query answering using randomsamples; and present a prototypical implementation within the engine of the open-sourcedatabase system Derby---called Derby/S. Our approach significantly reduces the requiredexpert knowledge by enabling the definition of samples in a declarative way; the choice ofthe specific sampling scheme and its parametrization is left to the system. SQL/S introducesnew DDL commands to easily define and administrate random samples subject to a givenset of optimization criteria. Derby/S automatically takes care of sample maintenance if the …,Proceedings of the 2006 ACM SIGMOD international conference on Management of data,2006,13
Deferred maintenance of disk-based random samples,Rainer Gemulla; Wolfgang Lehner,Abstract Random sampling is a well-known technique for approximate processing of largedatasets. We introduce a set of algorithms for incremental maintenance of large randomsamples on secondary storage. We show that the sample maintenance cost can be reducedby refreshing the sample in a deferred manner. We introduce a novel type of log file whichfollows the intuition that only a “sample” of the operations on the base data has to beconsidered to maintain a random sample in a statistically correct way. Additionally; wedevelop a deferred refresh algorithm which updates the sample by using fast sequential diskaccess only; and which does not require any main memory. We conducted an extensive setof experiments and found; that our algorithms reduce maintenance cost by several orders ofmagnitude.,International Conference on Extending Database Technology,2006,13
Lash: Large-scale sequence mining with hierarchies,Kaustubh Beedkar; Rainer Gemulla,Abstract We propose LASH; a scalable; distributed algorithm for mining sequential patternsin the presence of hierarchies. LASH takes as input a collection of sequences; eachcomposed of items from some application-specific vocabulary. In contrast to traditionalapproaches to sequence mining; the items in the vocabulary are arranged in a hierarchy:both input sequences and sequential patterns may consist of items from different levels ofthe hierarchy. Such hierarchies naturally occur in a number of applications including miningnatural-language text; customer transactions; error logs; or event sequences. LASH is thefirst parallel algorithm for mining frequent sequences with hierarchies; it is designed to scaleto very large datasets. At its heart; LASH partitions the data using a novel; hierarchy-awarevariant of item-based partitioning and subsequently mines each partition independently …,Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,2015,12
Shared-memory and shared-nothing stochastic gradient descent algorithms for matrix completion,Faraz Makari; Christina Teflioudi; Rainer Gemulla; Peter Haas; Yannis Sismanis,Abstract We provide parallel algorithms for large-scale matrix completion on problems withmillions of rows; millions of columns; and billions of revealed entries. We focus on in-memory algorithms that run either in a shared-memory environment on a powerful computenode or in a shared-nothing environment on a small cluster of commodity nodes; even verylarge problems can be handled effectively in these settings. Our ASGD; DSGD-MR;DSGD++; and CSGD algorithms are novel variants of the popular stochastic gradientdescent (SGD) algorithm; with the latter three algorithms based on a new “stratified SGD”approach. All of the algorithms are cache-friendly and exploit thread-level parallelism; in-memory processing; and asynchronous communication. We investigate the performance ofboth new and existing algorithms via a theoretical complexity analysis and a set of large …,Knowledge and Information Systems,2015,12
Werdy: Recognition and disambiguation of verbs and verb phrases with syntactic and semantic pruning,Luciano Del Corro; Rainer Gemulla; Gerhard Weikum,Abstract Word-sense recognition and disambiguation (WERD) is the task of identifying wordphrases and their senses in natural language text. Though it is well understood how todisambiguate noun phrases; this task is much less studied for verbs and verbal phrases. Wepresent Werdy; a framework for WERD with particular focus on verbs and verbal phrases.Our framework first identifies multi-word expressions based on the syntactic structure of thesentence; this allows us to recognize both contiguous and non-contiguous phrases. We thengenerate a list of candidate senses for each word or phrase; using novel syntactic andsemantic pruning techniques. We also construct and leverage a new resource of pairs ofsenses for verbs and their object arguments. Finally; we feed the so-obtained candidatesenses into standard word-sense disambiguation (WSD) methods; and boost their …,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),2014,11
Core: Context-aware open relation extraction with factorization machines,Fabio Petroni; Luciano Del Corro; Rainer Gemulla,Abstract We propose CORE; a novel matrix factorization model that leverages contextualinformation for open relation extraction. Our model is based on factorization machines andintegrates facts from various sources; such as knowledge bases or open informationextractors; as well as the context in which these facts have been observed. We argue thatintegrating contextual information—such as metadata about extraction sources; lexicalcontext; or type information—significantly improves prediction performance. Openinformation extractors; for example; may produce extractions that are unspecific orambiguous when taken out of context. Our experimental study on a large real-world datasetindicates that CORE has significantly better prediction performance than state-ofthe-artapproaches when contextual information is available.,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015,10
Reconstructing graphs from neighborhood data,Dora Erdos; Rainer Gemulla; Evimaria Terzi,Consider a social network and suppose that we are given the number of common friendsbetween each pair of users. Can we reconstruct the underlying network? Similarly; considera set of documents and the words that appear in them. If we know the number of commonwords for every pair of documents; as well as the number of common documents for everypair of words; can we infer which words appear in which documents? In this paper; wedevelop a general methodology for answering questions like the ones above. We formalizethese questions in what we call the Reconstruct problem: Given information about thecommon neighbors of nodes in a network; our goal is to reconstruct the hidden binary matrixthat indicates the presence or absence of relationships between individual nodes. Wepropose an effective and practical heuristic; which exploits properties of the singular …,Data Mining (ICDM); 2012 IEEE 12th International Conference on,2012,10
Designing random sample synopses with outliers,Philipp Rosch; Rainer Gemulla; Wolfgang Lehner,Random sampling is one of the most widely used means to build synopses of large datasetsbecause random samples can be used for a wide range of analytical tasks. Unfortunately;the quality of the estimates derived from a sample is negatively affected by the presence of"outliers" in the data. In this paper; we show how to circumvent this shortcoming byconstructing outlier-aware sample synopses. Our approach extends the well-known outlierindexing scheme to multiple aggregation columns.,Data Engineering; 2008. ICDE 2008. IEEE 24th International Conference on,2008,9
Systems and methods for large-scale randomized optimization for problems with decomposable loss functions,*,Systems and methods directed toward processing optimization problems using lossfunctions; wherein a loss function is decomposed into at least one stratum loss function; aloss is decreased for each stratum loss function to a predefined stratum loss thresholdindividually using gradient descent; and the overall loss is decreased to a predefinedthreshold for the loss function by appropriately ordering the processing of the strata andspending appropriate processing time in each stratum. Other embodiments and aspects arealso described herein.,*,2014,7
Closing the gap: Sequence mining at scale,Kaustubh Beedkar; Klaus Berberich; Rainer Gemulla; Iris Miliaraki,Abstract Frequent sequence mining is one of the fundamental building blocks in datamining. While the problem has been extensively studied; few of the available techniques aresufficiently scalable to handle datasets with billions of sequences; such large-scale datasetsarise; for instance; in text mining and session analysis. In this article; we propose MG-FSM; ascalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called “gap constraints”; which can be used to limit the output to a controlled set of frequentsequences. Both positional and temporal gap constraints; as well as appropriate maximalityand closedness constraints; are supported. At its heart; MG-FSM partitions the inputdatabase in a way that allows us to mine each partition independently using any existingfrequent sequence mining algorithm. We introduce the notion of ω-equivalency; which is …,ACM Transactions on Database Systems (TODS),2015,6
Exact and approximate maximum inner product search with lemp,Christina Teflioudi; Rainer Gemulla,Abstract We study exact and approximate methods for maximum inner product search; afundamental problem in a number of data mining and information retrieval tasks. Wepropose the LEMP framework; which supports both exact and approximate search withquality guarantees. At its heart; LEMP transforms a maximum inner product search problemover a large database of vectors into a number of smaller cosine similarity search problems.This transformation allows LEMP to prune large parts of the search space immediately andto select suitable search algorithms for each of the remaining problems individually. LEMP isable to leverage existing methods for cosine similarity search; but we also provide a numberof novel search algorithms tailored to our setting. We conducted an extensive experimentalstudy that provides insight into the performance of many state-of-the-art techniques …,ACM Transactions on Database Systems (TODS),2017,5
Non-uniformity issues and workarounds in bounded-size sampling,Rainer Gemulla; Peter J Haas; Wolfgang Lehner,Abstract A variety of schemes have been proposed in the literature to speed up queryprocessing and analytics by incrementally maintaining a bounded-size uniform sample froma dataset in the presence of a sequence of insertion; deletion; and update transactions.These algorithms vary according to whether the dataset is an ordinary set or a multiset andwhether the transaction sequence consists only of insertions or can include deletions andupdates. We report on subtle non-uniformity issues that we found in a number of these priorbounded-size sampling schemes; including some of our own. We provide workarounds thatcan avoid the non-uniformity problem; these workarounds are easy to implement and incurnegligible additional cost. We also consider the impact of non-uniformity in practice anddescribe simple statistical tests that can help detect non-uniformity in new algorithms.,The VLDB Journal,2013,4
Solving linear programs in mapreduce,Mahdi Ebrahimi; Gerhard Weikum; Rainer Gemulla,Abstract Most interesting discrete optimization problems are NP-hard; thus no efficientalgorithm to find optimal solution to such problems is likely to exist. Linear programmingplays a central role in design and analysis of many approximation algorithms. However;linear program instances in real-world applications grow enormously. In this thesis; we studythe Awerbuch-Khandekar parallel algorithm for approximating linear programs; providestrategies for efficient realization of the algorithm in MapReduce; and discuss methods toimprove its performance in practice. Further; we characterize numerical properties of thealgorithm by comparing it with partially-distributed optimization methods. Finally; weevaluate the algorithm on a weighted maximum satisfiability problem generated by SOFIEknowledge extraction framework on the complete Academic Corpus.,*,2011,4
Xml stream processing quality,Sven Schmidt; Rainer Gemulla; Wolfgang Lehner,Abstract Systems for selective dissemination of information (SDI) are used to efficiently filter;transform; and route incoming XML documents according to pre-registered XPath profiles tosubscribers. Recent work focuses on the efficient implementation of the SDI core/filteringengine. Surprisingly; all systems are based on the best effort principle: The resulting XMLdocument is delivered to the consumer as soon as the filtering engine has successfullyfinished. In this paper; we argue that a more specific Quality-of-Service consideration has tobe applied to this scenario. We give a comprehensive motivation of quality of service in SDI-systems; discuss the two most critical factors of XML document size and shape and XPathstructure and length; and finally outline our current prototype of a Quality-of-Service-basedSDI-system implementation based on a real-time operating system and an extention of …,International XML Database Symposium,2003,4
System and method for maintaining and utilizing Bernoulli samples over evolving multisets,*,One embodiment of the present invention provides a method for incrementally maintaining aBernoulli sample S with sampling rate q over a multiset R in the presence of update; delete;and insert transactions. The method includes processing items inserted into R usingBernoulli sampling and augmenting S with tracking counters during this processing. Itemsdeleted from R are processed by using the tracking counters and by removing newly deleteditems from S using a calculated probability while maintaining a degree of uniformity in S.,*,2012,3
Hierarchical group-based sampling,Rainer Gemulla; Henrike Berthold; Wolfgang Lehner,Abstract Approximate query processing is an adequate technique to reduce response timesand system load in cases where approximate results suffice. In database literature; samplinghas been proposed to evaluate queries approximately by using only a subset of the originaldata. Unfortunately; most of these methods consider either only certain problems arising dueto the use of samples in databases (eg data skew) or only join operations involving multiplerelations. We describe how well-known sampling techniques dealing with group-byoperations can be combined with foreign-key joins such that the join is computed after thegeneration of the sample. In detail; we show how senate sampling and small groupsampling can be combined efficiently with the idea of join synopses. Additionally; weintroduce different algorithms which maintain the sample if the underlying data changes …,British National Conference on Databases,2005,3
DESQ: Frequent sequence mining with subsequence constraints,Kaustubh Beedkar; Rainer Gemulla,Frequent sequence mining methods often make use of constraints to control whichsubsequences should be mined; eg; length; gap; span; regular-expression; and hierarchyconstraints. We show that many subsequence constraints-including and beyond thoseconsidered in the literature-can be unified in a single framework. In more detail; we proposea set of simple and intuitive" pattern expressions" to describe subsequence constraints andexplore algorithms for efficiently mining frequent subsequences under such generalconstraints. A unified treatment allows researchers to study jointly many types ofsubsequence constraints (instead of each one individually) and helps to improve usability ofpattern mining systems for practitioners.,Data Mining (ICDM); 2016 IEEE 16th International Conference on,2016,2
A distributed approximation algorithm for mixed packing-covering linear programs,Faraz Makari; Rainer Gemulla,Abstract Mixed packing-covering linear programs capture a simple but expressive subclassof linear programs. They commonly arise as linear programming relaxations of a numberimportant combinatorial problems; including various network design and generalizedmatching problems. In this paper; we propose an efficient distributed approximationalgorithm for solving mixed packing-covering problems which requires a poly-logarithmicnumber of passes over the input. Our algorithm is well-suited for parallel processing onGPUs; in shared-memory architectures; or on small clusters of commodity nodes. We reportresults of a case study for generalized bipartite matching problems.,NIPS 2013 Workshop on Big Learning. NIPS,2013,2
MinIE: minimizing facts in open information extraction,Kiril Gashteovski; Rainer Gemulla; Luciano Del Corro,Abstract The goal of Open Information Extraction (OIE) is to extract surface relations andtheir arguments from naturallanguage text in an unsupervised; domainindependent manner.In this paper; we propose MinIE; an OIE system that aims to provide useful; compactextractions with high precision and recall. MinIE approaches these goals by (1) representinginformation about polarity; modality; attribution; and quantities with semantic annotationsinstead of in the actual extraction; and (2) identifying and removing parts that are consideredoverly specific. We conducted an experimental study with several real-world datasets andfound that MinIE achieves competitive or higher precision and recall than most priorsystems; while at the same time producing shorter; semantically enriched extractions.,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,2017,1
What you will gain by rounding: Theory and algorithms for rounding rank,Stefan Neumann; Rainer Gemulla; Pauli Miettinen,When factorizing binary matrices; we often have to make a choice between using expensivecombinatorial methods that retain the discrete nature of the data and using continuousmethods that can be more efficient but destroy the discrete structure. Alternatively; we canfirst compute a continuous factorization and subsequently apply a rounding procedure toobtain a discrete representation. But what will we gain by rounding? Will this yield lowerreconstruction errors? Is it easy to find a low-rank matrix that rounds to a given binarymatrix? Does it matter which threshold we use for rounding? Does it matter if we allow foronly non-negative factorizations? In this paper; we approach these and further questions bypresenting and studying the concept of rounding rank. We show that rounding rank is relatedto linear classification; dimensionality reduction; and nested matrices. We also report on …,Data Mining (ICDM); 2016 IEEE 16th International Conference on,2016,1
Senti-LSSVM: Sentiment-oriented multi-relation extraction with latent structural SVM,Lizhen Qu; Yi Zhang; Rui Wang; Lili Jiang; Rainer Gemulla; Gerhard Weikum,Abstract Extracting instances of sentiment-oriented relations from user-generated webdocuments is important for online marketing analysis. Unlike previous work; we formulatethis extraction task as a structured prediction problem and design the correspondinginference as an integer linear program. Our latent structural SVM based model can learnfrom training corpora that do not contain explicit annotations of sentiment-bearingexpressions; and it can simultaneously recognize instances of both binary (polarity) andternary (comparative) relations with regard to entity mentions of interest. The empiricalevaluation shows that our approach significantly outperforms state-of-the-art systems acrossdomains (cameras and movies) and across genres (reviews and forum posts). The goldstandard corpus that we built will also be a valuable resource for the community.,Transactions of the Association for Computational Linguistics,2014,1
The magazine archive includes every article published in Communications of the ACM for over the past 50 years.,Robert M French; Murray Turoff,Decision support system"(DSS) has become a multifaceted term covering a wide range offunctions and uses; too many to list fully here. For example; systems might supportoperational; tactical; or strategic decision making. They might simply provide summaries ofdata; they might forecast future developments in the context of present circumstances or theymight simulate the future after some postulated action has been taken; they might takeaccount of uncertainties; and they might help the decision makers explore their ownperceptions and values. Further; they might be designed to work with individuals or withgroups; and the groups may work in the same time and place or at distant locations; workingperhaps asynchronously over the Internet. The systems may be built on large databases ormodels or both; or they may simply seek to organize and communicate results to …,Communications of the ACM,2007,1
On Multi-Relational Link Prediction with Bilinear Models,Yanjie Wang; Rainer Gemulla; Hui Li,Abstract: We study bilinear embedding models for the task of multi-relational link predictionand knowledge graph completion. Bilinear models belong to the most basic models for thistask; they are comparably efficient to train and use; and they can provide good predictionperformance. The main goal of this paper is to explore the expressiveness of and theconnections between various bilinear models proposed in the literature. In particular; asubstantial number of models can be represented as bilinear models with certain additionalconstraints enforced on the embeddings. We explore whether or not these constraints leadto universal models; which can in principle represent every set of relations; and whether ornot there are subsumption relationships between various models. We report results of anindependent experimental study that evaluates recent bilinear models in a common …,arXiv preprint arXiv:1709.04808,2017,*
On Some Problems of Rounding Rank,Stefan Neumann; Pauli Miettinen; Rainer Gemulla; Gerhard Weikum,Abstract This thesis is devoted to the study of the rounding rank problem: Given a binarymatrix and a real number; the rounding threshold; we want to find the real-valued matrix oflowest rank; that after rounding according to the given threshold results in the given inputmatrix. We call this rank the rounding rank.,*,2015,*
A Self-Portrayal of GI Junior Fellow Rainer Gemulla: Data Analysis at Scale,Rainer Gemulla,Abstract: My research focuses on methods to analyze and mine large datasets as well astheir practical realizations and applications. The key question of interest to me is: How canwe effectively and efficiently distill useful information from large; complex; and potentiallynoisy datasets? To approach this question; we are developing systems for scalable dataanalysis and data mining; for working with incomplete and noisy data; for data-intensiveoptimization; as well as for extracting structured information from natural-language text. Thisarticle highlights some of my work in these areas.,it–Information Technology,2015,*
Robust Principal Component Analysis as a Nonlinear Eigenproblem,Anastasia Podosinnikova; Matthias Hein; Rainer Gemulla,Abstract Principal Component Analysis (PCA) is a widely used tool for; eg; exploratory dataanalysis; dimensionality reduction and clustering. However; it is well known that PCA isstrongly affected by the presence of outliers and; thus; is vulnerable to both grossmeasurement error and adversarial manipulation of the data. This phenomenon motivatesthe development of robust PCA as the problem of recovering the principal components of theuncontaminated data. In this thesis; we propose two new algorithms; QRPCA and MDRPCA;for robust PCA components based on the projection-pursuit approach of Huber. While theresulting optimization problems are non-convex and non-smooth; we show that they can beefficiently minimized via the RatioDCA using bundle methods/accelerated proximal methodsfor the interior problem. The key ingredient for the most promising algorithm (QRPCA) is a …,*,2013,*
Hierarchisches gruppenbasiertes Sampling,Rainer Gemulla; Henrike Berthold; Wolfgang Lehner,Abstract In Zeiten wachsender Datenbankgrößen ist es unumgänglich; Anfragennäherungsweise auszuwerten um schnelle Antworten zu erhalten. Dieser Artikel stelltverschiedene Methoden vor; dieses Ziel zu erreichen; und wendet sich anschließend demSampling zu; welches mit Hilfe einer Stichprobe schnell zu adäquaten Ergebnissen führt.Enthalten Datenbankanfragen Verbund-oder Gruppierungsoperationen; so sinkt dieGenauigkeit vieler Sampling-Verfahren sehr stark; insbesondere werden vor allem kleineGruppen nicht erkannt. Dieser Artikel befasst sich mit hierarchischen gruppenbasiertemSampling; welches Sampling; Gruppierung und Verbundoperationen kombiniert.,Informatik-Forschung und Entwicklung,2005,*
