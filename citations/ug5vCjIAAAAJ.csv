BigBench: towards an industry standard benchmark for big data analytics,Ahmad Ghazal; Tilmann Rabl; Minqing Hu; Francois Raab; Meikel Poess; Alain Crolotte; Hans-Arno Jacobsen,Abstract There is a tremendous interest in big data by academia; industry and a large userbase. Several commercial and open source providers unleashed a variety of products tosupport big data storage and processing. As these products mature; there is a need toevaluate and compare the performance of these systems. In this paper; we presentBigBench; an end-to-end big data benchmark proposal. The underlying business model ofBigBench is a product retailer. The proposal covers a data model and synthetic datagenerator that addresses the variety; velocity and volume aspects of big data systemscontaining structured; semi-structured and unstructured data. The structured part of theBigBench data model is adopted from the TPC-DS benchmark; which is enriched with semi-structured and unstructured data components. The semi-structured part captures …,Proceedings of the 2013 ACM SIGMOD international conference on Management of data,2013,227
Energy cost; the key challenge of today's data centers: a power consumption analysis of TPC-C results,Meikel Poess; Raghunath Othayoth Nambiar,Abstract Historically; performance and price-performance of computer systems have beenthe key purchasing arguments for customers. With rising energy costs and increasing poweruse due to the ever-growing demand for computing power (servers; storage; networks);electricity bills have become a significant expense for today's data centers. In the very nearfuture; energy efficiency is expected to be one of the key purchasing arguments. Someperformance organizations; such as SPEC; have developed power benchmarks for singleservers (SPECpower_ssj2008); but so far; no benchmark exists that measures the powerconsumption of transaction processing systems. In this paper; we develop a powerconsumption model based on data readily available in the TPC-C full disclosure report ofpublished benchmarks. We verify our model with measurements taken from three fully …,Proceedings of the VLDB Endowment,2008,187
New TPC benchmarks for decision support and web commerce,Meikel Poess; Chris Floyd,Abstract For as long as there have been DBMS's and applications that use them; there hasbeen interest in the performance characteristics that these systems exhibit. This month'scolumn describes some of the recent work that has taken place in TPC; the TransactionProcessing Performance Council. TPC-A and TPC-B are obsolete benchmarks that youmight have heard about in the past. TPC-C V3. 5 is the current benchmark for OLTP systems.Introduced in 1992; it has been run on many hardware platforms and DBMS's. Indeed; theTPC web site currently lists 202 TPC-C benchmark results. Due to its maturity; TPC-C will notbe discussed in this article. We've asked two very knowledgeable individuals to write thisarticle. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd isthe chair of the TPC-W Subcommittee. We greatly appreciate their efforts. A wealth of …,ACM Sigmod Record,2000,186
Data compression in oracle,Meikel Poess; Dmitry Potapov,Abstract The Oracle RDBMS recently introduced an innovative compression technique forreducing the size of relational tables. By using a compression algorithm specificallydesigned for relational data; Oracle is able to compress data much more effectively thanstandard compression techniques. More significantly; unlike other compression techniques;Oracle incurs virtually no performance penalty for SQL queries accessing compressedtables. In fact; Oracle's compression may provide performance gains for queries accessinglarge amounts of data; as well as for certain data management operations like backup andrecovery. Oracle's compression algorithm is particularly well-suited for data warehouses:environments; which contains large volumes of historical data; with heavy query workloads.Compression can enable a data warehouse to store several times more raw data without …,Proceedings of the 29th international conference on Very large data bases-Volume 29,2003,119
Why you should run TPC-DS: a workload analysis,Meikel Poess; Raghunath Othayoth Nambiar; David Walrath,Abstract The Transaction Processing Performance Council (TPC) is completingdevelopment of TPC-DS; a new generation industry standard decision support benchmark.The TPC-DS benchmark; first introduced in the" The Making of TPC-DS"[9] paper at the 32nd International Conference on Very Large Data Bases (VLDB); has now entered the TPC's"Formal Review" phase for new benchmarks; companies and researchers alike can nowdownload the draft benchmark specification and tools for evaluation. The first paper [9] gavean overview of the TPC-DS data model; workload model; and execution rules. This paperdetails the characteristics of different phases of the workload; namely: database load; queryworkload and data maintenance; and also their impact to the benchmark's performancemetric. As with prior TPC benchmarks; this workload will be widely used by vendors to …,Proceedings of the 33rd international conference on Very large data bases,2007,90
The making of TPC-DS,Raghunath Othayoth Nambiar; Meikel Poess,Abstract For the last decade; the research community and the industry have used TPC-Dand its successor TPC-H to evaluate performance of decision support technology.Recognizing a paradigm shift in the industry the Transaction Processing PerformanceCouncil has developed a new Decision Support benchmark; TPC-DS; expected to bereleased this year. From an ease of benchmarking perspective it is similar to pastbenchmarks. However; it adjusts for new technology and new approaches the industry hasembarked on in recent years. This paper describes the main characteristics of TPC-DS;explains why some of the key decisions were made and which performance aspects ofdecision support system it measures.,Proceedings of the 32nd international conference on Very large data bases,2006,78
The mixed workload CH-benCHmark,Richard Cole; Florian Funke; Leo Giakoumakis; Wey Guy; Alfons Kemper; Stefan Krompass; Harumi Kuno; Raghunath Nambiar; Thomas Neumann; Meikel Poess; Kai-Uwe Sattler; Michael Seibold; Eric Simon; Florian Waas,Abstract While standardized and widely used benchmarks address either operational or real-time Business Intelligence (BI) workloads; the lack of a hybrid benchmark led us to thedefinition of a new; complex; mixed workload benchmark; called mixed workload CH-benCHmark. This benchmark bridges the gap between the established single-workloadsuites of TPC-C for OLTP and TPC-H for OLAP; and executes a complex mixed workload: atransactional workload based on the order entry processing of TPC-C and a correspondingTPC-H-equivalent OLAP query suite run in parallel on the same tables in a single databasesystem. As it is derived from these two most widely used TPC benchmarks; the CH-benCHmark produces results highly relevant to both hybrid and classic single-workloadsystems.,Proceedings of the Fourth International Workshop on Testing Database Systems,2011,72
MUDD: a multi-dimensional data generator,John M Stephens; Meikel Poess,Abstract Today's business intelligence systems consist of hundreds of processors with disksubsystems able to handle multiple Giga-bytes of IO-bandwidth. These systems usuallycontain terabytes of data. Evaluating database system performance of such systems oftenrequires generating synthetic data with well defined statistical properties. To simulatedifferent scenarios; it is important to vary statistical properties including row counts of tables.Foremost; in order to analyze large scale systems; data generators need to be able toproduce hundreds of terabytes of data in a timely fashion. In this paper we present MUDD; amulti-dimensional data generator. Originally designed for TPC-DS; a decision supportbenchmark being developed by the TPC; MUDD is able to generate up to 100 Terabyte offlat file data in hours; utilizing modern multi processor architectures; including clusters. Its …,ACM SIGSOFT Software Engineering Notes,2004,65
Tpc-ds; taking decision support benchmarking to the next level,Meikel Poess; Bryan Smith; Lubor Kollar; Paul Larson,Abstract TPC-DS is a new decision support benchmark currently under development by theTransaction Processing Performance Council (TPC). This paper provides a brief overview ofthe new benchmark. The benchmark models the decision support functions of a retailproduct supplier; including data loading; multiple types of queries and data maintenance.The database consists of multiple snowflake schemas with shared dimension tables; data isskewed; and the query set is large. Overall; the benchmark is considerably more realisticthan previous decision support benchmarks.,Proceedings of the 2002 ACM SIGMOD international conference on Management of data,2002,59
Benchmarking big data systems and the bigdata top100 list,Chaitanya Baru; Milind Bhandarkar; Raghunath Nambiar; Meikel Poess; Tilmann Rabl,Abstract “Big data” has become a major force of innovation across enterprises of all sizes.New platforms with increasingly more features for managing big datasets are beingannounced almost on a weekly basis. Yet; there is currently a lack of any means ofcomparability among such platforms. While the performance of traditional database systemsis well understood and measured by long-established institutions such as the TransactionProcessing Performance Council (TCP); there is neither a clear definition of the performanceof big data systems nor a generally agreed upon metric for comparing these systems. In thisarticle; we describe a community-based effort for defining a big data benchmark. Over thepast year; a Big Data Benchmarking Community has become established in order to fill thisvoid. The effort focuses on defining an end-to-end application-layer benchmark for …,Big Data,2013,54
Energy benchmarks: a detailed analysis,Meikel Poess; Raghunath Othayoth Nambiar; Kushagra Vaid; John M Stephens Jr; Karl Huppler; Evan Haines,Abstract In light of an increase in energy cost and energy consciousness industry standardorganizations such as Transaction Processing Performance Council (TPC); StandardPerformance Evaluation Corporation (SPEC) and Storage Performance Council (SPC) aswell as the US Environmental Protection Agency have developed tests to measure energyconsumption of computer systems. Although all of these consortia aim at standardizingpower consumption measurement using benchmarks; ultimately aiming to reduce overallpower consumption; and to aid in making purchase decisions; their methodologies differslightly. For instance; some organizations developed specialized benchmarks while othersadded energy metrics to existing benchmarks. In this paper we give a comprehensiveoverview of the currently available energy benchmarks followed by an in depth analysis …,Proceedings of the 1st International Conference on Energy-Efficient Computing and Networking,2010,49
Setting the direction for big data benchmark standards,Chaitanya Baru; Milind Bhandarkar; Raghunath Nambiar; Meikel Poess; Tilmann Rabl,Abstract The Workshop on Big Data Benchmarking (WBDB2012); held on May 8-9; 2012 inSan Jose; CA; served as an incubator for several promising approaches to define a big databenchmark standard for industry. Through an open forum for discussions on a number ofissues related to big data benchmarking—including definitions of big data terms; benchmarkprocesses and auditing—the attendees were able to extend their own view of big databenchmarking as well as communicate their own ideas; which ultimately led to the formationof small working groups to continue collaborative work in this area. In this paper; wesummarize the discussions and outcomes from this first workshop; which was attended byabout 60 invitees representing 45 different organizations; including industry and academia.Workshop attendees were selected based on their experience and expertise in the areas …,Technology Conference on Performance Evaluation and Benchmarking,2012,46
Generating thousand benchmark queries in seconds,Meikel Poess; John M Stephens Jr,Abstract The combination of an exponential growth in the amount of data managed by atypical business intelligence system and the increased competitiveness of a global economyhas propelled decision support systems (DSS) from the role of exploratory tools employedby a few visionary companies to become a core requirement for a competitive enterprise.That same maturation has often resulted in a selection process that requires an ever morecritical system evaluation and selection to be completed in an increasingly short period oftime. While there have been some advances in the generation of data sets for systemevaluation (see [3]); the quantification of query performance has often relied on models andmethodologies that were developed for systems that were more simplistic; less dynamic; andless central to a successful business. In this paper we present QGEN; a flexible; high …,Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,2004,45
TPC-H analyzed: Hidden messages and lessons learned from an influential benchmark,Peter Boncz; Thomas Neumann; Orri Erling,Abstract The TPC-D benchmark was developed almost 20 years ago; and even though itscurrent existence as TPC-H could be considered superseded by TPC-DS; one can still learnfrom it. We focus on the technical level; summarizing the challenges posed by the TPC-Hworkload as we now understand them; which we call “choke points”. We identify 28 differentsuch choke points; grouped into six categories: Aggregation Performance; JoinPerformance; Data Access Locality; Expression Calculation; Correlated Subqueries andParallel Execution. On the meta-level; we make the point that the rich set of choke-pointsfound in TPC-H sets an example on how to design future DBMS benchmarks.,Technology Conference on Performance Evaluation and Benchmarking,2013,44
Tuning servers; storage and database for energy efficient data warehouses,Meikel Poess; Raghunath Othayoth Nambiar,Undoubtedly; reducing power consumption is at the top of the priority list for system vendors;data center managers who are challenged by customers; analysts; and governmentagencies to implement green initiatives. Hardware and software vendors have developed anarray of power preserving techniques. On-demand-driven clock speeds for processors;energy efficient power supplies; and operating-system-controlled dynamic power modes arejust a few hardware examples. Software vendors have contributed to energy efficiency byimplementing power efficient coding methods; such as advanced compression and enablingapplications to take advantage of large memory caches. However; adoption of these power-preserving technologies in data centers is not straightforward; especially; for large; complexapplications such as data warehouses. Data warehouse workloads typically have …,Data Engineering (ICDE); 2010 IEEE 26th International Conference on,2010,37
The making of tpc-ds,Raghunath Othayoth; Meikel Poess,ABSTRACT For the last decade; the research community and the industry have used TPC-Dand its successor TPC-H to evaluate performance of decision support technology.Recognizing a paradigm shift in the industry the Transaction Processing PerformanceCouncil has developed a new Decision Support benchmark; TPC-DS; expected to bereleased this year. From an ease of benchmarking perspective it is similar to pastbenchmarks. However; it adjusts for new technology and new approaches the industry hasembarked on in recent years. This paper describes the main characteristics of TPC-DS;explains why some of the key decisions were made and which performance aspects ofdecision support system it measures.,PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES,2006,32
Large scale data warehouses on grid: Oracle database 10 g and HP proliant servers,Meikel Poess; Raghunath Othayoth Nambiar,Abstract Grid computing has the potential to drastically change enterprise computing as weknow it today. The main concept of grid computing is viewing computing as a utility. It shouldnot matter where data resides; or what computer processes a task. This concept has beenapplied successfully to academic research. It also has many advantages for commercialdata warehouse applications such as virtualization; flexible provisioning; reduced cost dueto commodity hardware; high availability and high scale-out. In this paper we show how alarge-scale; high-performing and scalable grid-based data warehouse can be implementedusing commodity hardware (industry-standard x86-based). Oracle Database 10g and theLinux operating system. We further demonstrate this architecture in a recently publishedTPC-H benchmark.,Proceedings of the 31st international conference on Very large data bases,2005,32
TPC-DI: the first industry benchmark for data integration,Meikel Poess; Tilmann Rabl; Hans-Arno Jacobsen; Brian Caufield,Abstract Historically; the process of synchronizing a decision support system with data fromoperational systems has been referred to as Extract; Transform; Load (ETL) and the toolssupporting such process have been referred to as ETL tools. Recently; ETL was replaced bythe more comprehensive acronym; data integration (DI). DI describes the process ofextracting and combining data from a variety of data source formats; transforming that datainto a unified data model representation and loading it into a data store. This is done in thecontext of a variety of scenarios; such as data acquisition for business intelligence; analyticsand data warehousing; but also synchronization of data between operational applications;data migrations and conversions; master data management; enterprise data sharing anddelivery of data services in a service-oriented architecture context; amongst others. With …,Proceedings of the VLDB Endowment,2014,26
Variations of the star schema benchmark to test the effects of data skew on query performance,Tilmann Rabl; Meikel Poess; Hans-Arno Jacobsen; Patrick O'Neil; Elizabeth O'Neil,Abstract The Star Schema Benchmark (SSB); now in its third revision; has been widely usedto evaluate the performance of database management systems when executing star schemaqueries. SSB; based on the well known industry standard benchmark TPC-H; shares someof its drawbacks; most notably; its uniform data distributions. Today's systems rely heavily onsophisticated cost-based query optimizers to generate the most efficient query executionplans. A benchmark that evaluates optimizer's capability to generate optimal execution plansunder all circumstances must provide the rich data set details on which optimizers rely(uniform and non-uniform distributions; data sparsity; etc.). This is also true for otherdatabase system parts; such as indices and operators; and ultimately holds for an end-to-end benchmark as well. SSB's data generator; based on TPC-H's dbgen; is not easy to …,Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering,2013,26
Discussion of BigBench: a proposed industry standard performance benchmark for big data,Chaitanya Baru; Milind Bhandarkar; Carlo Curino; Manuel Danisch; Michael Frank; Bhaskar Gowda; Hans-Arno Jacobsen; Huang Jie; Dileep Kumar; Raghunath Nambiar; Meikel Poess; Francois Raab; Tilmann Rabl; Nishkam Ravi; Kai Sachs; Saptak Sen; Lan Yi; Choonhan Youn,Abstract Enterprises perceive a huge opportunity in mining information that can be found inbig data. New storage systems and processing paradigms are allowing for ever larger datasets to be collected and analyzed. The high demand for data analytics and rapiddevelopment in technologies has led to a sizable ecosystem of big data processing systems.However; the lack of established; standardized benchmarks makes it difficult for users tochoose the appropriate systems that suit their requirements. To address this problem; wehave developed the BigBench benchmark specification. BigBench is the first end-to-end bigdata analytics benchmark suite. In this paper; we present the BigBench benchmark andanalyze the workload from technical as well as business point of view. We characterize thequeries in the workload along different dimensions; according to their functional …,Technology Conference on Performance Evaluation and Benchmarking,2014,20
Efficient update data generation for DBMS benchmarks,Michael Frank; Meikel Poess; Tilmann Rabl,Abstract It is without doubt that industry standard benchmarks have been proven to becrucial to the innovation and productivity of the computing industry. They are important to thefair and standardized assessment of performance across different vendors; different systemversions from the same vendor and across different architectures. Good benchmarks areeven meant to drive industry and technology forward. Since at some point; after allreasonable advances have been made using a particular benchmark even goodbenchmarks become obsolete over time. This is why standard consortia periodicallyoverhaul their existing benchmarks or develop new benchmarks. An extremely time andresource consuming task in the creation of new benchmarks is the development ofbenchmark generators; especially because benchmarks tend to become more and more …,Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering,2012,19
Introducing TPCx-HS: the first industry standard for benchmarking big data systems,Raghunath Nambiar; Meikel Poess; Akon Dey; Paul Cao; Tariq Magdon-Ismail; Andrew Bond,Abstract The designation Big Data has become a mainstream buzz phrase across manyindustries as well as research circles. Today many companies are making performanceclaims that are not easily verifiable and comparable in the absence of a neutral industrybenchmark. Instead one of the test suites used to compare performance of Hadoop basedBig Data systems is the TeraSort. While it nicely defines the data set and tasks to measureBig Data Hadoop systems it lacks a formal specification and enforcement rules that enablethe comparison of results across systems. In this paper we introduce TPCx-HS; the industry'sfirst industry standard benchmark; designed to stress both hardware and software that isbased on Apache HDFS API compatible distributions. TPCx-HS extends the workloaddefined in TeraSort with formal rules for implementation; execution; metric; result …,Technology Conference on Performance Evaluation and Benchmarking,2014,18
Parallel data generation for performance analysis of large; complex RDBMS,Tilmann Rabl; Meikel Poess,Abstract The exponential growth in the amount of data retained by today's systems isfostered by a recent paradigm shift towards cloud computing and the vast deployment ofdata-hungry applications; such as social media sites. At the same time systems arecapturing more sophisticated data. Running realistic benchmarks to test the performanceand robustness of these applications is becoming increasingly difficult; because of theamount of data that needs to be generated; the number of systems that need to generate thedata and the complex structure of the data. These three reasons are intrinsically connected.Whenever large amounts of data are needed; its generation process needs to be highlyparallel; in many cases across-systems. Since the structure of the data is becoming moreand more complex; its parallel generation is extremely challenging. Over the years there …,Proceedings of the Fourth International Workshop on Testing Database Systems,2011,17
A power consumption analysis of decision support systems,Meikel Poess; Raghunath Othayoth Nambiar,Abstract Enterprise data warehouses have been doubling every three years; demandinghigh compute power and storage capacities. The in-dustry is expected to meet suchcompute demands; but dealing with the dramatic increase in energy requirements will bechallenging. Energy efficiency has already become the top priority for system developersand data center managers. While system vendors focus on developing energy efficientsystems there is a huge demand for industry-standard workloads and processes to measureand analyze energy consumption for enterprise data warehouses. SPEC has developed apower benchmark for single servers (SPECpower_ssj2008); but so far; no benchmark existsthat measures the power consumption of large; complex systems. In this paper; we present asimple power consumption model for enterprise data warehouses based on the industry …,Proceedings of the first joint WOSP/SIPEW international conference on Performance engineering,2010,15
Performance evaluation and benchmarking,Raghunath Nambiar; Meikel Poess,The Transaction Processing Performance Council (TPC) is a non-profit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products; and to improve andmonitor the performance of their products under development. Many buyers use TPCbenchmark results as points of comparison when purchasing new computing systems. Theinformation technology landscape is evolving at a rapid pace; challenging industry expertsand researchers to develop innovative techniques for evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace with these rapid changes in technology. One vehicle …,*,2009,13
BigBench Specification V0. 1,Tilmann Rabl; Ahmad Ghazal; Minqing Hu; Alain Crolotte; Francois Raab; Meikel Poess; Hans-Arno Jacobsen,Abstract In this article; we present the specification of BigBench; an end-to-end big databenchmark proposal. BigBench models a retail product supplier. The benchmark proposalcovers a data model and a set of big data specific queries. BigBench's synthetic datagenerator addresses the variety; velocity and volume aspects of big data workloads. Thestructured part of the BigBench data model is adopted from the TPC-DS benchmark. Inaddition; the structured schema is enriched with semi-structured and unstructured datacomponents that are common in a retail product supplier environment. This specificationcontains the full query set as well as the data model.,*,2014,12
TPC benchmark roadmap 2012,Raghunath Nambiar; Meikel Poess; Andrew Masland; H Reza Taheri; Matthew Emmerton; Forrest Carman; Michael Majdalany,Abstract The TPC has played; and continues to play; a crucial role in providing the computerindustry with relevant standards for total system performance; price-performance and energyefficiency comparisons. Historically known for database-centric standards; the TPC is nowdeveloping standards for consolidation using virtualization technologies and multi-sourcedata integration; and exploring new ideas such as Big Data and Big Data Analytics to keeppace with rapidly changing industry demands. This paper gives a high level overview of thecurrent state of the TPC in terms of existing standards; standards under development andfuture outlook.,Technology Conference on Performance Evaluation and Benchmarking,2012,12
Metrics for measuring the performance of the mixed workload CH-benCHmark,Florian Funke; Alfons Kemper; Stefan Krompass; Harumi Kuno; Raghunath Nambiar; Thomas Neumann; Anisoara Nica; Meikel Poess; Michael Seibold,Abstract Advances in hardware architecture have begun to enable database vendors toprocess analytical queries directly on operational database systems without impeding theperformance of mission-critical transaction processing too much. In order to evaluate suchsystems; we recently devised the mixed workload CH-benCHmark; which combinestransactional load based on TPC-C order processing with decision support load based onTPC-H-like query suite run in parallel on the same tables in a single database system. Justas the data volume of actual enterprises tends to increase over time; an inherentcharacteristic of this mixed workload benchmark is that data volume increases duringbenchmark runs; which in turn may increase response times of analytic queries. For purelytransactional loads; response times typically do not depend that much on data volume; as …,Technology Conference on Performance Evaluation and Benchmarking,2011,12
Of snowstorms and bushy trees,Rafi Ahmed; Rajkumar Sen; Meikel Poess; Sunil Chakkappen,Abstract Many workloads for analytical processing in commercial RDBMSs are dominatedby snowstorm queries; which are characterized by references to multiple large fact tablesand their associated smaller dimension tables. This paper describes a technique for bushyjoin tree optimization for snowstorm queries in Oracle database system. This techniquegenerates bushy join trees containing subtrees that produce substantially reduced sets ofrows and; therefore; their joins with other subtrees are generally much more efficient thanjoins in the left-deep trees. The generation of bushy join trees within an existing commercialphysical optimizer requires extensive changes to the optimizer. Further; the optimizer willhave to consider a large join permutation search space to generate efficient bushy join trees.The novelty of the approach is that bushy join trees can be generated outside the physical …,Proceedings of the VLDB Endowment,2014,10
Transaction performance vs. Moore’s law: a trend analysis,Raghunath Nambiar; Meikel Poess,Abstract Intel co-founder Gordon E. Moore postulated in his famous 1965 paper that thenumber of components in integrated circuits had doubled every year from their invention in1958 until 1965; and then predicted that the trend would continue for at least ten years.Later; David House; an Intel colleague; after factoring in the increase in performance oftransistors; concluded that integrated circuits would double in performance every 18 months.Despite this trend in microprocessor improvements; your favored text editor continues to takethe same time to start and your PC takes pretty much the same time to reboot as it took 10years ago. Can this observation be made on systems supporting the fundamental aspects ofour information based economy; namely transaction processing systems? For over twodecades the Transaction Processing Performance Council (TPC) has been very …,Technology Conference on Performance Evaluation and Benchmarking,2010,10
A PDGF Implementation for TPC-H,Meikel Poess; Tilmann Rabl; Michael Frank; Manuel Danisch,Abstract With 182 benchmark results from 20 hardware vendors; TPC-H has establisheditself as the industry standard benchmark to measure performance of decision supportsystems. The release of TPC-H twelve years ago by the Transaction ProcessingPerformance Council's (TPC) was based on an earlier decision support benchmark; calledTPC-D; which was released 1994. TPC-H inherited TPC-D's data and query generators;DBgen and Qgen. As systems evolved over time; maintenance of these tools has become amajor burden for the TPC. DBgen and Qgen need to be ported on new hardwarearchitectures and adapted as the system grew in size to multiple terabytes. In this paper wedemonstrate how Parallel Data Generation Framework (PDGF); a generic data generator;developed at the University of Passau for massively parallel data generation; can be …,Technology Conference on Performance Evaluation and Benchmarking,2011,9
Parameter curation for benchmark queries,Andrey Gubichev; Peter Boncz,Abstract In this paper we consider the problem of generating parameters for benchmarkqueries so these have stable behavior despite being executed on datasets (real-world orsynthetic) with skewed data distributions and value correlations. We show that uniformrandom sampling of the substitution parameters is not well suited for such benchmarks;since it results in unpredictable runtime behavior of queries. We present our approach ofParameter Curation with the goal of selecting parameter bindings that have consistently low-variance intermediate query result sizes throughout the query plan. Our solution is illustratedwith IMDB data and the recently proposed LDBC Social Network Benchmark (SNB).,Technology Conference on Performance Evaluation and Benchmarking,2014,8
Keeping the TPC relevant!,Raghunath Nambiar; Meikel Poess,Abstract The Transaction Processing Performance Council (TPC) is a nonprofit organizationfounded in 1988 to define transaction processing and database benchmarks. Since then; theTPC has played a crucial role in providing the industry with relevant standards for totalsystem performance; price-performance; and energy-efficiency comparisons. TPCbenchmarks are widely used by database researchers and academia. Historically known fordatabase-centric standards; the TPC has developed a benchmark for virtualization and iscurrently developing a multisource data integration benchmark. The technology landscapeis changing at a rapid pace; challenging industry experts and researchers to developinnovative techniques for evaluating; measuring; and characterizing the performance ofmodern application systems. The Technology Conference series on Performance …,Proceedings of the VLDB Endowment,2013,8
Power Based Performance and Capacity Estimation Models for Enterprise Information Systems.,Meikel Poess; Raghunath Othayoth Nambiar,Abstract Historically; the performance and purchase price of enterprise information systemshave been the key arguments in purchasing decisions. With rising energy costs andincreasing power use due to the evergrowing demand for compute capacity (servers;storage; networks etc.); electricity bills have become a significant expense for todays datacenters. In the very near future; energy efficiency is expected to be one of the key purchasingarguments. Having realized this trend; the Transaction Processing Performance Council hasdeveloped the TPC-Energy specification. It defines a standard methodology for measuring;reporting and fairly comparing power consumption of enterprise information systems. Wideindustry adaption of TPC-Energy requires a large body of benchmark publications acrossmultiple software and hardware platforms; which could take several years. Meanwhile; we …,IEEE Data Eng. Bull.,2011,8
Big data benchmarking,Chaitan Baru; Milind Bhandarkar; Raghunath Nambiar; Meikel Poess; Tilmann Rabl,Abstract We provide a summary of the outcomes from the Workshop on Big DataBenchmarking (WBDB2012) held on May 8-9; 2012 in San Jose; CA. The workshopdiscussed a number of issues related to big data benchmarking definitions and benchmarkprocesses; and was attended by 60 invitees representing 45 different organizations fromindustry and academia. Attendees were selected based on their experience and expertise inone or more areas of big data; database systems; performance benchmarking; and big dataapplications. The participants concluded that there exists both a need and an opportunity fordefining benchmarks to capture the end-to-end aspects of big data applications. The metricsfor such benchmarks would need to include metrics for performance as well asprice/performance; and consider several costs including total system cost; setup cost; and …,Proceedings of the 2012 workshop on Management of big data systems,2012,7
Controlled SQL query evolution for decision support benchmarks,Meikel Poess,Abstract The synthesis of increased global competitiveness and the acceptance ofcommercially available multi purpose database management systems (DBMS) for decisionsupport applications requires an ever more critical system evaluation and selection to becompleted in a progressively short period of time. Designers of standard benchmarks;individual customer benchmarks and system stress tests alike are struggling to mastermindqueries that are both representative to the real world and execute in a reasonable time.Additionally; the enriched functionality of every new DBMS release amplifies the complexityof today's decision support systems calling for a novel approach in query generation forbenchmarks. This paper proposes a framework of so called query evolution rules that can beapplied to typical decision support queries; written in SQL92. Deployed in combination …,Proceedings of the 6th international workshop on Software and performance,2007,6
Big data benchmark compendium,Todor Ivanov; Tilmann Rabl; Meikel Poess; Anna Queralt; John Poelman; Nicolas Poggi; Jeffrey Buell,Abstract The field of Big Data and related technologies is rapidly evolving. Consequently;many benchmarks are emerging; driven by academia and industry alike. As thesebenchmarks are emphasizing different aspects of Big Data and; in many cases; coveringdifferent technical platforms and uses cases; it is extremely difficult to keep up with the paceof benchmark creation. Also with the combinations of large volumes of data; heterogeneousdata formats and the changing processing velocity; it becomes complex to specify anarchitecture which best suits all application requirements. This makes the investigation andstandardization of such systems very difficult. Therefore; the traditional way of specifying astandardized benchmark with pre-defined workloads; which have been in use for years inthe transaction and analytical processing systems; is not trivial to employ for Big Data …,Technology Conference on Performance Evaluation and Benchmarking,2015,5
Rapid development of data generators using meta generators in PDGF,Tilmann Rabl; Meikel Poess; Manuel Danisch; Hans-Arno Jacobsen,Abstract Generating data sets for the performance testing of database systems on aparticular hardware configuration and application domain is a very time consuming andtedious process. It is time consuming; because of the large amount of data that needs to begenerated and tedious; because new data generators might need to be developed orexisting once adjusted. The difficulty in generating this data is amplified by constantadvances in hardware and software that allow the testing of ever larger and morecomplicated systems. In this paper; we present an approach for rapidly developingcustomized data generators. Our approach; which is based on the Parallel Data GeneratorFramework (PDGF); deploys a new concept of so called meta generators. Meta generatorsextend the concept of column-based generators in PDGF. Deploying meta generators in …,Proceedings of the Sixth International Workshop on Testing Database Systems,2013,5
Tractor pulling on data warehouses,Martin L Kersten; Alfons Kemper; Volker Markl; Anisoara Nica; Meikel Poess; Kai-Uwe Sattler,Abstract Robustness of database systems under stress is hard to quantify; because there aremany factors involved; most notably the user expectation to perform a job within certainbounds of the user requirements. Nevertheless; robustness of database system is veryimportant to end users. In this paper we develop a database benchmark suite; inspired bytractor pulling; where robustness is measured as a system's ability to process data despite acontinuous increase in system load; as defined in terms of data volume; query volume andcomplexity. A functional evaluation is performed against several systems to highlight thebenchmark capabilities.,Proceedings of the Fourth International Workshop on Testing Database Systems,2011,5
Performance Evaluation; Measurement and Characterization of Complex Systems,Raghunath Nambiar; Meikel Poess,*,Lecture Notes in Computer Science,2011,5
Topics in Performance Evaluation; Measurement and Characterization: Third TPC Technology Conference; TPCTC 2011; Seattle; WA; USA; August 29-September 3...,Raghunath Nambiar; Meikel Poess,This book constitutes the proceedings of the Third Technology Conference on PerformanceEvaluation and Benchmarking; TPCTC 2011; held in conjunction with the 37th InternationalConference on Very Large Data Bases; VLDB 2011; in Seattle; August/September 2011.The 12 full papers and 2 keynote papers were carefully selected and reviewed fromnumerous submissions. The papers present novel ideas and methodologies in performanceevaluation; measurement; and characterization.,*,2012,4
From BigBench to TPCx-BB: standardization of a big data benchmark,Paul Cao; Bhaskar Gowda; Seetha Lakshmi; Chinmayi Narasimhadevara; Patrick Nguyen; John Poelman; Meikel Poess; Tilmann Rabl,Abstract With the increased adoption of Hadoop-based big data systems for the analysis oflarge volume and variety of data; an effective and common benchmark for big datadeployments is needed. There have been a number of proposals from industry andacademia to address this challenge. While most either have basic workloads (eg wordcounting); or port existing benchmarks to big data systems (eg TPC-H or TPC-DS); some arespecifically designed for big data challenges. The most comprehensive proposal amongthese is the BigBench benchmark; recently standardized by the Transaction ProcessingPerformance Council as TPCx-BB. In this paper; we discuss the progress made since theoriginal BigBench proposal to the standardized TPCx-BB. In addition; we will share thethought process went into creating the specification; challenges in navigating the …,Technology Conference on Performance Evaluation and Benchmarking,2016,3
Big Data Benchmarking,Tilmann Rabl; Kai Sachs; Meikel Poess; Chaitanya Baru; J Hans-Arno,Formed in 2012; the Big Data Benchmarking Community (BDBC) represents a major step infacilitating the development of benchmarks for objective comparisons of hardware andsoftware systems dealing with emerging big data applications. Led by Chaitanya Baru;Tilmann Rabl; Meikel Poess; Milind Bhandarkar; and Nambiar Raghunath; the BDBC hassuccessfully conducted five international Workshops on Big Data Benchmarking (WBDB).One strength of the WBDB is that it brings together practitioners and researchers; whichleads to a balance between industrial and academic contributions. It provides the rightenvironment in which to discuss the challenges and potential approaches to benchmark bigdata systems. The results of the WBDB have a high impact on the ongoing research in thedomain and WBDB itself is established as the leading event focusing on big data …,*,2015,3
TPC’s benchmark development model: making the first industry standard benchmark on big data a success,Meikel Poess,Abstract There are many questions to answer and hurdles to overcome before an idea for abenchmark becomes an industry standard. After all technical challenges are solved and aprototype benchmark is created; the question arises on how to turn the prototype into anindustry standard benchmark that has broad acceptance in the industry; is credible andsustainable over an extended period of time. The Transaction Processing PerformanceCouncil is one of the most recognized industry standard consortia for developing andmaintaining industry standard benchmarks. Its philosophy and strict rules have assuredacceptance; credibility and sustainability of its benchmarks for the last two decades. In thispaper the author shows how the TPC model for developing and maintaining benchmarkscan be applied to creating the first industry standard benchmark on Big Data.,*,2014,3
Specifying Big Data Benchmarks: First Workshop; WBDB 2012; San Jose; CA; USA; May 8-9; 2012 and Second Workshop; WBDB 2012; Pune; India; December 17-...,Tilmann Rabl; Meikel Poess; Chaitan Baru; Hans-Arno Jacobsen,This book constitutes the thoroughly refereed revised selected papers of the First Workshopon Big Data Benchmarks; WBDB 2012; held in San Jose; CA; USA; in May 2012 and theSecond Workshop on Big Data Benchmarks; WBDB 2012; held in Pune; India; in December2012. The 14 revised papers presented were carefully reviewed and selected from 60submissions. The papers are organized in topical sections on benchmarking; foundationsand tools; domain specific benchmarking; benchmarking hardware and end-to-end big databenchmarks.,*,2013,3
How to advance TPC benchmarks with dependability aspects,Raquel Almeida; Meikel Poess; Raghunath Nambiar; Indira Patil; Marco Vieira,Abstract Transactional systems are the core of the information systems of mostorganizations. Although there is general acknowledgement that failures in these systemsoften entail significant impact both on the proceeds and reputation of companies; thebenchmarks developed and managed by the Transaction Processing Performance Council(TPC) still maintain their focus on reporting bare performance. Each TPC benchmark has topass a list of dependability-related tests (to verify ACID properties); but not all benchmarksrequire measuring their performances. While TPC-E measures the recovery time of somesystem failures; TPC-H and TPC-C only require functional correctness of such recovery.Consequently; systems used in TPC benchmarks are tuned mostly for performance. In thispaper we argue that nowadays systems should be tuned for a more comprehensive suite …,Technology Conference on Performance Evaluation and Benchmarking,2010,3
Building Enterprise Class Real-Time Energy Efficient Decision Support Systems,Meikel Poess; Raghunath Nambiar,Abstract In today's highly competitive marketplace; companies have an insatiable need forup-to-the-second information about their business' operational state; while generatingTerabytes of data per day [2]. The ability to convert this data into meaningful businessinformation in a timely; cost effective manner is critical to their competitiveness. For many; itis no longer acceptable to move operational data into specialized analytical tools because ofthe delay this additional step would take. In certain cases they prefer to directly run querieson their operational data. To keep the response time of these queries low while data volumeincreases; IT departments are forced to buy faster processors or increase the number ofprocessors per system. At the same time they need to scale the I/O subsystem to keep theirsystems balanced. While processor performance has been doubling every two years in …,International Workshop on Business Intelligence for the Real-Time Enterprise,2010,3
Performance Evaluation and Benchmarking; Traditional to Big Data to Internet of Things,Raghunath Nambiar; Meikel Poess,The Transaction Processing Performance Council (TPC) is a non-profit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products; and to improve andmonitor the performance of their products under development. Many buyers use TPCbenchmark results as points of comparison when purchasing new computing systems. Theinformation technology landscape is evolving at a rapid pace; challenging industry expertsand researchers to develop innovative techniques for evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace with these rapid changes in technology. One vehicle …,Proceedings of the 7th TPCTC Conference. LNCS,2017,2
Performance Characterization and Benchmarking. Traditional to Big Data: 6th TPC Technology Conference; TPCTC 2014; Hangzhou; China; September 1--5; 2014....,Raghunath Nambiar; Meikel Poess,This book constitutes the refereed post-conference proceedings of the 6th TPC TechnologyConference; TPCTC 2014; held in Hangzhou; China; in September 2014. It contains 12selected peer-reviewed papers; a report from the TPC Public Relations Committee. Manybuyers use TPC benchmark results as points of comparison when purchasing newcomputing systems. The information technology landscape is evolving at a rapid pace;challenging industry experts and researchers to develop innovative techniques forevaluation; measurement and characterization of complex systems. The TPC remainscommitted to developing new benchmark standards to keep pace and one vehicle forachieving this objective is the sponsorship of the Technology Conference on PerformanceEvaluation and Benchmarking (TPCTC). Over the last five years TPCTC has been held …,*,2015,2
Performance Characterization and Benchmarking,Raghunath Nambiar; Meikel Poess,The Transaction Processing Performance Council (TPC) is a non-profit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products; and to improve andmonitor the performance of their products under development. Many buyers use TPCbenchmark results as points of comparison when purchasing new computing systems. Theinformation technology landscape is evolving at a rapid pace; challenging industry expertsand researchers to develop innovative techniques for evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace with these rapid changes in technology. One vehicle …,*,2014,2
Optimizing benchmark configurations for energy efficiency,Meikel Poess; Raghunath Nambiar; Kushagra Vaid,Abstract Historically compute server performance has been the most important pillar in theevaluation of datacenter efficiency; which can be measured using a variety of industrystandard benchmarks. With the introduction of industry standard servers; price-performancebecame the second pillar in the'efficiencyequation'. Today with an increased awareness inthe industry for power optimized designs and corporate initiatives to reduce carbonemissions; data center efficiency needs to incorporate yet another key element in thisequation: energy efficiency. Initial models based on'name-plate'power consumption havebeen used to estimate energy efficiency while recently industry standard consortia likeSPEC; TPC and SPC have started amalgating new energy metrics with their traditionalperformance metrics. TPC-Energy; enables the measuring and reporting of energy …,ACM SIGSOFT Software Engineering Notes,2011,2
Database are not toasters: A framework for comparing data warehouse appliances,Omer Trajman; Alain Crolotte; David Steinhoff; Raghunath Othayoth Nambiar; Meikel Poess,Abstract The success of Business Intelligence (BI) applications depends on two factors; theability to analyze data ever more quickly and the ability to handle ever increasing volumes ofdata. Data Warehouse (DW) and Data Mart (DM) installations that support BI applicationshave historically been built using traditional architectures either designed from the groundup or based on customized reference system designs. The advent of Data WarehouseAppliances (DA) brings packaged software and hardware solutions that addressperformance and scalability requirements for certain market segments. The differencesbetween DAs and custom installations make direct comparisons between them impracticaland suggest the need for a targeted DA benchmark. In this paper we review data warehouseappliances by surveying thirteen products offered today. We assess the common …,Technology Conference on Performance Evaluation and Benchmarking,2009,2
Analysis of TPC-DS: the first standard benchmark for SQL-based big data systems,Meikel Poess; Tilmann Rabl; Hans-Arno Jacobsen,Abstract The advent of Web 2.0 companies; such as Facebook; Google; and Amazon withtheir insatiable appetite for vast amounts of structured; semi-structured; and unstructureddata; triggered the development of Hadoop and related tools; eg; YARN; MapReduce; andPig; as well as NoSQL databases. These tools form an open source software stack tosupport the processing of large and diverse data sets on clustered systems to performdecision support tasks. Recently; SQL is resurrecting in many of these solutions; eg; Hive;Stinger; Impala; Shark; and Presto. At the same time; RDBMS vendors are adding Hadoopsupport into their SQL engines; eg; IBM's Big SQL; Actian's Vortex; Oracle's Big Data SQL;and SAP's HANA. Because there was no industry standard benchmark that could measurethe performance of SQL-based big data solutions; marketing claims were mostly based …,Proceedings of the 2017 Symposium on Cloud Computing,2017,1
Reinventing the TPC: from traditional to big data to internet of things,Raghunath Nambiar; Meikel Poess,Abstract The Transaction Processing Performance Council (TPC) has made significantcontributions to the industry and research with standards that encourage fair competition toaccelerate product development and enhancements. Technology disruptions are changingthe industry landscape faster than ever. This paper provides a high level summary of thehistory of the TPC and recent initiatives to make sure that it is a relevant organization in theage of digital transformation fueled by Big Data and the Internet of Things.,Technology Conference on Performance Evaluation and Benchmarking,2015,1
A Review of System Benchmark Standards and a Look Ahead Towards an Industry Standard for Benchmarking Big Data Workloads,Raghunath Nambiar; Meikel Poess,Abstract Industry standard benchmarks have played; and continue to play; a crucial role inthe advancement of the computing industry. Demands for them have existed since buyerswere first confronted with the choice between purchasing one system over another. Over theyears; industry standard benchmarks have proven critical to both buyers and vendors:buyers use benchmark results when evaluating new systems in terms of performance;price/performance; and energy efficiency; while vendors use benchmarks to demonstratecompetitiveness of their products and to monitor release-to-release progress of theirproducts under development. Historically; we have seen that industry standard benchmarksenable healthy competition that results in product improvements and the evolution of brandnew technologies. Over the past quarter-century; industry standard bodies like the …,*,2014,1
Selected Topics in Performance Evaluation and Benchmarking: 4th TPC Technology Conference; TPCTC 2012; Istanbul; Turkey; August 27; 2012; Revised Selected...,Raghunath Nambiar; Meikel Poess,This book constitutes the refereed proceedings of the 4th TPC Technology Conference;TPCTC 2012; held in Istanbul; Turkey; in August 2012. It contains 10 selected peer-reviewedpapers; 2 invited talks; a report from the TPC Public Relations Committee; and a report fromthe workshop on Big Data Benchmarking; WBDB 2012. The papers present novel ideas andmethodologies in performance evaluation; measurement; and characterization.,*,2013,1
Converting TPC-H Query Templates to Use DSQGEN for Easy Extensibility,John M Stephens; Meikel Poess,Abstract The ability to automatically generate queries that are not known a-priory is crucialfor ad-hoc benchmarks. TPC-H solves this problem with a query generator; QGEN; whichutilizes query templates to generate SQL queries. QGEN's architecture makes it difficult tomaintain; change or adapt to new types of query templates since every modification requirescode changes. DSQGEN; a generic query generator; originally written for the TPC-DSbenchmark; uses a query template language; which allows for easy modification andextension of existing query templates. In this paper we show how the current set of TPC-Hquery templates can be migrated to the template language of DSQGEN without any changeto comparability of published TPC-H results. The resulting query template model providesopportunities for easier enhancement and extension of the TPC-H workload; which we …,Technology Conference on Performance Evaluation and Benchmarking,2009,1
Setting the Direction for Big Data Benchmark Standards,Tilmann Rabl; Chaitanya Baru; Milind Bhandarkar; Meikel Poess; Raghunath Nambiar,• Audience: Who is the audience for this benchmark?• Application: What application shouldwe model?• Single benchmark spec: Is it possible to develop a single benchmark to capturecharacteristics of multiple applications?• Component vs. end-to-end benchmark. Is itpossible to factor out a set of benchmark “components”; which can be isolated and pluggedinto an end-to-end benchmark (s)?• Paper and Pencil vs Implementation-based. Should theimplementation be specification-driven or implementation-driven?• Reuse. Can we reuseexisting benchmarks?• Benchmark Data. Where do we get the data from?• Innovation orcompetition? Should the benchmark be for innovation or competition?,*,*,1
Performance Evaluation and Benchmarking for the Analytics Era: 9th Tpc Technology Conference; Tpctc 2017; Munich; Germany; August 28; 2017; Revised Selecte...,Raghunath Nambiar; Meikel Poess,This book constitutes the thoroughly refereed post-conference proceedings of the 8th TPCTechnology Conference; on Performance Evaluation and Benchmarking; TPCTC 2017; heldin conjunction with the43rd International Conference on Very Large Databases (VLDB 2017)in August/September 2017. The 12 papers presented were carefully reviewed and selectedfrom numeroussubmissions. The TPC remains committed to developing new benchmarkstandards to keep pace with these rapid changes in technology.,*,2018,*
Industry Standards for the Analytics Era: TPC Roadmap,Raghunath Nambiar; Meikel Poess,Abstract The Transaction Processing Performance Council (TPC) is a non-profit organizationfocused on developing data-centric benchmark standards and disseminating objective;verifiable performance data to industry. This paper provides a high-level summary of TPCbenchmark standards; technology conference initiative; and new development activities inprogress.,Technology Conference on Performance Evaluation and Benchmarking,2017,*
Big Data Benchmarking: 6th International Workshop; WBDB 2015; Toronto; ON; Canada; June 16-17; 2015 and 7th International Workshop; WBDB 2015; New Delhi;...,Tilmann Rabl; Raghunath Nambiar; Chaitanya Baru; Milind Bhandarkar; Meikel Poess; Saumyadipta Pyne,This book constitutes the thoroughly refereed post-workshop proceedings of the 6thInternational Workshop on Big Data Benchmarking; WBDB 2015; held in Toronto; ON;Canada; in June 2015 and the 7th International Workshop; WBDB 2015; held in New Delhi;India; in December 2015. The 8 full papers presented in this book were carefully reviewedand selected from 22 submissions. They deal with recent trends in big data and HPCconvergence; new proposals for big data benchmarking; as well as tooling and performanceresults.,*,2016,*
Erratum to: Performance Evaluation and Benchmarking,Raghunath Nambiar; Meikel Poess,1 Cisco Systems; Inc.; San Jose; CA; USA rnambiar@cisco.com 2 Oracle Corporation; RedwoodCity; CA; USA … Erratum to: R. Nambiar and M. Poess (Eds.): Performance Evaluation andBenchmarking; LNCS 10080; DOI: 10.1007/978-3-319-54334-5 … In the original version; thereis an error in the title on the cover and the inner title page. It must read “Internet of Things” …The updated original version of this book can be found at DOI: 10.1007/978-3-319-54334-5 ©Springer International Publishing AG 2017 R. Nambiar and M. Poess (Eds.): TPCTC 2016; LNCS10080; p. E1; 2017. DOI: 10.1007/978-3-319-54334-5_11,Technology Conference on Performance Evaluation and Benchmarking,2016,*
Advancing Big Data Benchmarks: Proceedings of the 2013 Workshop Series on Big Data Benchmarking; WBDB. cn; Xi'an; China; July16-17; 2013 and WBDB. us; S...,Tilmann Rabl; Nambiar Raghunath; Meikel Poess; Milind Bhandarkar; Hans-Arno Jacobsen; Chaitanya Baru,This book constitutes the thoroughly refereed joint proceedings of the Third and FourthWorkshop on Big Data Benchmarking. The third WBDB was held in Xi'an; China; in July2013 and the Fourth WBDB was held in San José; CA; USA; in October; 2013. The 15papers presented in this book were carefully reviewed and selected from 33 presentations.They focus on big data benchmarks; applications and scenarios; tools; systems and surveys.,*,2014,*
Specifying Big Data Benchmarks: First Workshop; WBDB 2012; San Jose; CA; USA; May 8-9; 2012; and Second Workshop; WBDB 2012; Pune; India; December 17-...,Meikel Poess; Chaitanya Baru; Hans-Arno Jacobsen; Tilmann Rabl,*,*,2014,*
TPC State of the Council 2013,Raghunath Nambiar; Meikel Poess; Andrew Masland; H Reza Taheri; Andrew Bond; Forrest Carman; Michael Majdalany,Abstract The TPC has played; and continues to play; a crucial role in providing the computerindustry and its customers with relevant standards for total system performance; price-performance; and energy efficiency comparisons. Historically known for database-centricstandards; the TPC is now developing benchmark standards for consolidation usingvirtualization technologies and multi-source data integration. The organization is alsoexploring new ideas such as Big Data and Big Data Analytics as well as an Expressbenchmark model to keep pace with rapidly changing industry demands. This paper gives ahigh level overview of the current state of the TPC in terms of existing standards; standardsunder development and future outlook.,Technology Conference on Performance Evaluation and Benchmarking,2013,*
Transaction Processing in the Hybrid OLTP&OLAP Main-Memory Database System HyPer,Alfons Kemper Thomas Neumann Jan Finis; Florian Funke; Viktor Leis Henrik Mühe Tobias Mühlbauer; Wolf Rödiger,Abstract Two emerging hardware trends have re-initiated the development of in-coredatabase systems: ever increasing main-memory capacities and vast multi-core parallelprocessing power. Main-memory capacities of several TB allow to retain all transactionaldata of even the largest applications in-memory on one (or a few) servers. The vastcomputational power in combination with low data management overhead yieldsunprecedented transaction performance which allows to push transaction processing (awayfrom application servers) into the database server and still “leaves room” for additional queryprocessing directly on the transactional data. Thereby; the often postulated goal of real-timebusiness intelligence; where decision makers have access to the latest version of thetransactional state; becomes feasible. In this paper we will survey the HyPerScript …,Data Engineering,2013,*
Incorporating Recovery from Failures into a Data Integration Benchmark,Len Wyatt; Brian Caufield; Marco Vieira; Meikel Poess,Abstract The proposed TPC-DI benchmark measures the performance of Data Integrationsystems (aka ETL systems) given the task of integrating data from an OLTP system and otherdata sources to create a data warehouse. This paper describes the scenario; structure andtiming principles used in TPC-DI. Although failure recovery is very important in realdeployments of Data Integration systems; certain complexities made it difficult to specify inthe benchmark. Hence failure recovery aspects have been scoped out of the current versionof TPC-DI. The issues around failure recovery are discussed in detail and some options aredescribed. Finally the audience is invited to offer additional suggestions.,Technology Conference on Performance Evaluation and Benchmarking,2012,*
Optimizing benchmark configurations for energy efficiency (abstracts only),Meikel Poess; Raghunath Nambiar; Kushagra Vaid,Abstract Historically compute server performance has been the most important pillar in theevaluation of datacenter efficiency; which can be measured using a variety of industrystandard benchmarks. With the introduction of industry standard servers; price-performancebecame the second pillar in the'efficiency equation'. Today with an increased awareness inthe industry for power optimized designs and corporate initiatives to reduce carbonemissions; data center efficiency needs to incorporate yet another key element in thisequation: energy efficiency. Initial models based on'name-plate'power consumption havebeen used to estimate energy efficiency while recently industry standard consortia likeSPEC; TPC and SPC have started amalgating new energy metrics with their traditionalperformance metrics. TPC-Energy; enables the measuring and reporting of energy …,ACM SIGMETRICS Performance Evaluation Review,2011,*
Performance Evaluation and Benchmarking: Second TPC Technology Conference; TPCTC 2010; Singapore; September 13-17; 2010. Revised Selected Papers,Raghunath Nambiar; Meikel Poess,This book constitutes the proceedings of the Second Technology Conference onPerformance Evaluation and Benchmarking; TPCTC 2010; held in conjunction with the 36thInternational Conference on Very Large Data Bases; VLDB 2010; in Singapore; September13-17; 2010. The 14 full papers and two keynote papers were carefully selected andreviewed from numerous submissions. This book considers issues such as appliance;business intelligence; cloud computing; complex event processing; database optimizations;data compression; energy and space efficiency; green computing; hardware innovations;high speed data generation; hybrid workloads; very large memory systems; andvirtualization.,*,2011,*
Industrial and Applications Sessions,Andreas Behm; Serge Rielau; Richard Swagerman; Conor Cunningham; Cesar Galindo-Legaria; Goetz Graefe; Walid Rjaibi; Paul Bird; Nagender Bandi; Chengyu Sun; Divyakant Agrawal; Amr El Abbadi; Sang K Cha; Changbin Song; Meikel Poess; John M Stephens Jr; Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan; Sougata Mukherjea; Bhuvan Bamba; Nick Koudas; Amit Marathe; Divesh Srivastava; Benoit Dageville; Dinesh Das; Karl Dias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin; Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; Arun Marathe; Vivek Narasayya; Manoj Syamala; Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W Warner; Vikas Arora; Susan Kotsovolos; Shankar Pal; Istvan Cseri; Oliver Seeliger; Gideon Schaller; Leo Giakoumakis; Vasili Zolotov; Ashraf Aboulnaga; Peter Haas; Mokhtar Kandil; Sam Lightstone; Guy Lohman; Volker Markl; Ivan Popivanov; Vijayshankar Raman; Marcus Fontoura; Eugene Shekita; Jason Zien; Sridhar Rajagopalan; Andreas Neumann; Bishwaranjan Bhattacharjee; Christof Bornhövd; Tao Lin; Stephan Haller; Joachim Schaper; Managing RDFI Data; Sudarshan S Chawathe; Venkat Krishnamurthyy; Sridhar Ramachandran; David Campbell; Toby Bloom; Ted Sharpe; Rakesh Nagarajan; Mushtaq Ahmed; Aditya Phatak; Raymie Stata; Patrick Hunt; William O'Connell; Ramesh Bhashyam; Roger MacNicol; Blaine French,PIVOT and UNPIVOT: Optimization and Execution Strategies in an RDBMS ConorCunningham; Cesar Galindo-Legaria; Goetz Graefe (Microsoft Corp.) … P*TIME: Highly ScalableOLTP DBMS for Managing Update-Intensive Stream Workload Sang K. Cha (Transact InMemory; Inc); Changbin Song (Seoul National Univ.) … Supporting Ontology-based SemanticMatching in RDBMS Souripriya Das; Eugene Chong; George Eadon; Jagannathan Srinivasan(Oracle Corp.) … Automatic SQL Tuning in Oracle 10g Benoit Dageville; Dinesh Das; KarlDias; Khaled Yagoub; Mohamed Zait; Mohamed Ziauddin (Oracle Corp.) … Database TuningAdvisor for Microsoft SQL Server 2005 Sanjay Agrawal; Surajit Chaudhuri; Lubor Kollar; ArunMarathe; Vivek Narasayya; Manoj Syamala (Microsoft Corp.) … Query Rewrite for XML in OracleXML DB Muralidhar Krishnaprasad; Zhen Liu; Anand Manikutty; James W. Warner; Vikas …,Proceedings of the Thirtieth International Conference on Very Large Data Bases: Toronto; Canada; August 31-September 3; 2004,2004,*
Selected topics in performance evaluation and benchmarking,Ist Anbul; R N Ambi Ar; Meikel Poess,Series/Report no.: Lecture notes in computer science;; 0302-9743;; 7755.; LNCS sublibrary.SL 2; Programming and software engineering.; Lecture notes in computer science;; 7755.;LNCS sublibrary.; SL 2;; Programming and software engineering.,*,*,*
Performance Evaluation and Benchmarking for the Analytics Era,Raghunath Nambiar; Meikel Poess,The Transaction Processing Performance Council (TPC) is a nonprofit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products; and to improve andmonitor the performance of their products under development. Many buyers use TPCbenchmark results as points of comparison when purchasing new computing systems. Theinformation technology landscape is evolving at a rapid pace; challenging industry expertsand researchers to develop innovative techniques for evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace with these rapid changes in technology. One vehicle …,*,*,*
Opportunities In Reducing Energy Consumption in DBMS,Meikel Poess,Current Trends: While historically performance and price-performance of informationsystems have been the key purchasing arguments; data center managers today; because ofthe steep increase in electricity cost of running a data center; are more conscious about theenergy rating of the systems they purchase. The same trend can be seen in benchmarksystems.[1] and [2] developed two analytical energy consumption models based onnameplate power rating of x86 systems; one for On Line Transaction Processing (OLTP) andone for Decision Support systems (DS). Both papers further presented two long term energyconsumption studies. These power estimation models were recently refined in [5] to takeenergy efficient components into consideration. Although estimated and based on TPCbenchmarks; the power estimates obtained from these models are very close to those of …,*,*,*
Selected Topics in Performance Evaluation and Benchmarking,Raghunath Nambiar Meikel Poess,The Transaction Processing Performance Council (TPC) is a non-profit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products and to improve and monitorthe performance of their products under development. Many buyers use TPC benchmarkresults as points of comparison when purchasing new computing systems. The informationtechnology landscape is evolving at a rapid pace; challenging industry experts andresearchers to develop innovative techniques for the evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace with these rapid changes in technology. One vehicle …,*,*,*
Performance Evaluation and Benchmarking,Meikel Poess,First established in August 1988; the Transaction Processing Performance Council (TPC)has shaped the landscape of modern transaction processing and database benchmarksover two decades. Now; the world is in the midst of an extraordinary information explosionled by rapid growth in the use of the Internet and connected devices. Both user-generateddata and enterprise data levels continue to grow exponentially. With substantialtechnological breakthroughs; Moore's law will continue for at least a decade; and the datastorage capacities and data transfer speeds will continue to increase exponentially. Thesehave challenged industry experts and researchers to develop innovative techniques toevaluate and benchmark both hardware and software technologies. As a result; the TPCheld its First Conference on Performance Evaluation and Benchmarking (TPCTC 2009) …,Lecture Notes in,*,*
Topics in Performance Evaluation; Measurement and Characterization,Raghunath Nambiar Meikel Poess,The Transaction Processing Performance Council (TPC) is a non-profit organizationestablished in August 1988. Over the years; the TPC has had a significant impact on thecomputing industry's use of industry-standard benchmarks. Vendors use TPC benchmarks toillustrate performance competitiveness for their existing products; and to improve andmonitor the performance of their products under development. Many buyers use TPCbenchmark results as points of comparison when purchasing new computing systems. Theinformation technology landscape is evolving at a rapid pace; challenging industry expertsand researchers to develop innovative techniques for evaluation; measurement; andcharacterization of complex systems. The TPC remains committed to developing newbenchmark standards to keep pace; and one vehicle for achieving this objective is the …,*,*,*
ADVCOMP 2009,Roger Barga; Gaspar Barreira; Radu Calinescu; Petre Dini; Wolfgang Gentzsch; Jianmin Jiang; Hans-Joachim Klein; Simon G Fabri; Jameleddine Hassine; Flavio Oquendo; Adithya Nagarajan; Meikel Poess; Kurt Rohloff; Kenneth P Camilleri; Marvin Bugeja; Sascha Opletal; Saïd Tazi; Witold Abramowicz; Sónia Maria Almeida da Luz; Vincenzo Ambriola; Renato Amorim; Basavaraj Anami; Stefan Andrei; Plamen Angelov; Simona Bernardi; Alexandra Bonnici; Pierre Borne; Marco C Campi; Juan Vicente Capella Hernández; Alessandro Casavola; Antonio Casimiro Costa; Tracey Cassar; Richard Chbeir; Rebeca Cortázar,Roger Barga; Microsoft Research; USA Gaspar Barreira; LIP / Portuguese Grid Initiative; PortugalRadu Calinescu; Oxford University; UK Petre Dini; Cisco Systems; Inc.; USA / ConcordiaUniversity; Canada Wolfgang Gentzsch; DEISA & OGF; Germany Jianmin Jiang; University ofBradford; UK Hans-Joachim Klein; Christian-Albrechts-Universitaet Kiel; Germany Simon G.Fabri; University of Malta - Msida; Malta Jameleddine Hassine; Cisco Systems; Inc.; CanadaFlavio Oquendo; European University of Brittany - UBS/VALORIA; France AdithyaNagarajan; Microsoft; USA Raghunath Nambiar; HP; USA Meikel Poess; Oracle; USA KurtRohloff; BBN Technologies; USA Kenneth P. Camilleri; University of Malta; MaltaKennethScerri; University of Malta; Malta Marvin Bugeja; University of Malta; Malta Sascha Opletal; UniversitätStuttgart; Germany Saïd Tazi; LAAS-CNRS; Université de Toulouse / Université …,*,*,*
Data Engineering,Andrew Krioukov; Christoph Goebel; Sara Alspaugh; Yanpei Chen; David Culler; Randy Katz; Willis Lang; Ramakrishnan Kandhan; Jignesh M Patel; Meikel Poess; Raghunath Nambiar,Abstract The variable and intermittent nature of many renewable energy sources makesintegrating them into the electric grid challenging and limits their penetration. The currentgrid requires expensive; largescale energy storage and peaker plants to match suchsupplies to conventional loads. We present an alternative solution; in which supply-followingloads adjust their power consumption to match the available renewable energy supply. Weshow Internet data centers running batched; data analytic workloads are well suited to besuch supply-following loads. They are large energy consumers; highly instrumented; agile;and contain much scheduling slack in their workloads. We explore the problem ofscheduling the workload to align with the time-varying available wind power. Usingsimulations driven by real life batch workloads and wind power traces; we demonstrate …,*,*,*
